<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-299" href="#">iccv2013-299</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</h1>
<br/><p>Source: <a title="iccv-2013-299-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Van_Den_Bergh_Online_Video_SEEDS_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Michael Van_Den_Bergh, Gemma Roig, Xavier Boix, Santiago Manen, Luc Van_Gool</p><p>Abstract: Superpixel and objectness algorithms are broadly used as a pre-processing step to generate support regions and to speed-up further computations. Recently, many algorithms have been extended to video in order to exploit the temporal consistency between frames. However, most methods are computationally too expensive for real-time applications. We introduce an online, real-time video superpixel algorithm based on the recently proposed SEEDS superpixels. A new capability is incorporated which delivers multiple diverse samples (hypotheses) of superpixels in the same image or video sequence. The multiple samples are shown to provide a strong cue to efficiently measure the objectness of image windows, and we introduce the novel concept of objectness in temporal windows. Experiments show that the video superpixels achieve comparable performance to state-of-the-art offline methods while running at 30 fps on a single 2.8 GHz i7 CPU. State-of-the-art performance on objectness is also demonstrated, yet orders of magnitude faster and extended to temporal windows in video.</p><p>Reference: <a title="iccv-2013-299-reference" href="../iccv2013_reference/iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ch ∗  Luc Van Gool1,2  Abstract Superpixel and objectness algorithms are broadly used as a pre-processing step to generate support regions and to speed-up further computations. [sent-4, score-0.517]
</p><p>2 We introduce an online, real-time video superpixel algorithm based on the recently proposed SEEDS superpixels. [sent-7, score-0.496]
</p><p>3 A new capability is incorporated which delivers multiple diverse samples (hypotheses) of superpixels in the same image or video sequence. [sent-8, score-0.549]
</p><p>4 The multiple samples are shown to provide a strong cue to efficiently measure the objectness of image windows, and we introduce the novel concept of objectness in temporal windows. [sent-9, score-1.227]
</p><p>5 Experiments show that the video superpixels achieve comparable performance to state-of-the-art offline methods while running at 30 fps on a single 2. [sent-10, score-0.524]
</p><p>6 State-of-the-art performance on objectness is also demonstrated, yet orders of magnitude faster and extended to temporal windows in video. [sent-12, score-0.798]
</p><p>7 Introduction  Many algorithms use superpixels or objectness scores to efficiently select areas which to analyze further. [sent-14, score-0.887]
</p><p>8 In terms of its still counterparts, it comes closest to the recently introduced SEEDS superpixels [15]. [sent-21, score-0.413]
</p><p>9 Similar to SEEDS, we define an objective function that prefers video superpixels to have a homogeneous color, and our video superpixels can be extracted efficiently. [sent-22, score-0.976]
</p><p>10 When starting off the partition of a new video frame, we exploit the hierarchical superpixel organization ofthe previous frame, the coarser levels of which serve as initialization. [sent-27, score-0.592]
</p><p>11 Moreover, we propose a method to extract multiple superpixel partitions with a value of the objective function close to that of the optimum. [sent-28, score-0.436]
</p><p>12 This allows us to introduce a new and highly efficient objectness measure, together with its natural extension to videos (a tube of bounding boxes spanning a time interval). [sent-30, score-0.764]
</p><p>13 We experimentally validate the video superpixel and objectness algorithms, where we use standard benchmarks where possible. [sent-33, score-1.013]
</p><p>14 Related Work In this section, we review previous work related to superpixels and objectness in videos, the two tasks tackled in this paper. [sent-36, score-0.887]
</p><p>15 Thus, our approach can be seen to add a third strand to video superpixel extraction, namely one that that moves the boundaries in an initial superpixel partition. [sent-45, score-0.908]
</p><p>16 [16, 17] proposed a benchmark to evaluate video superpixels and a framework for streaming video segmentation using the graph-based superpixel approach of [5]. [sent-47, score-1.027]
</p><p>17 The objectness measure was introduced by Alexe et al. [sent-52, score-0.563]
</p><p>18 To the best ofour knowledge, objectness throughout video shots has not been introduced before. [sent-54, score-0.688]
</p><p>19 It should not be confused with the recently introduced dynamic objectness [13], which extracts objectness within a frame by including instantaneous motion. [sent-55, score-1.151]
</p><p>20 SEEDS for stills Let s represent the superpixel partition of an image, such that s : {1, . [sent-62, score-0.483]
</p><p>21 yT she ∈ ∈SE SE,D wSh approach [e1 s5]e tf oorf extracting superpixels in stills serves as starting point for our video extension. [sent-74, score-0.546]
</p><p>22 SEEDS extracts superpixels by maximizing an objective function, thus enforcing the color histograms of superpixels to be each concentrated in a single bin. [sent-78, score-0.74]
</p><p>23 SEEDS for videos Our video approach propagates superpixels over multiple frames to build 3D spatio-temporal constructs. [sent-83, score-0.559]
</p><p>24 As time goes on, new video superpixels can appear and others may terminate. [sent-84, score-0.488]
</p><p>25 In the literature, this is controlled by constraining the number of superpixel tubes in the sequence. [sent-85, score-0.45]
</p><p>26 In order to fulfill both constraints, the termination of a superpixel implies the creation of a new one in the same frame. [sent-89, score-0.531]
</p><p>27 These are Ltehet partitions feotr owfh vicahli dth pea superpixels are contiguous blobs in all frames and that exhibit the correct superpixelper-frame and superpixel-rate behavior. [sent-92, score-0.487]
</p><p>28 set of pixels that belong to superpixel k, at frame t. [sent-95, score-0.52]
</p><p>29 To indicate all pixels of the video superpixel up to frame t, we use Atk:0. [sent-96, score-0.638]
</p><p>30 It maximizes the energy by exchanging pixels between superpixels at their boundaries. [sent-115, score-0.522]
</p><p>31 Both the pixel exchange between superpixels and their temporal propagation are regulated through blocks of pixels. [sent-120, score-0.722]
</p><p>32 aTyheer sb elaocchk simizee caot mthbei see 2co×nd2 layer (2 2 or 3 3) and the number of layers are chosen sru (c2h ×tha 2t tohre 3 image saunbdd tihveisi nounm abt ethre o highest layer approximately yields the prescribed number of superpixels per frame. [sent-125, score-0.476]
</p><p>33 Multiple pixel block exchanges between superpixels are considered, one after the other. [sent-137, score-0.533]
</p><p>34 The exchanged pixel blocks are adjacent to the superpixel boundaries. [sent-139, score-0.573]
</p><p>35 Let Bnt be a block of pixels of the current frame that belongs tto B the superpixel n, i. [sent-145, score-0.587]
</p><p>36 c kB Btn⊂ f Arom⊂ superpixel n to m iwnhcreetahesres e txhceh objective efu bnlcotcikon B, we can use one histogram intersection computation, rather than evaluating the complete energy function. [sent-150, score-0.467]
</p><p>37 A Ttmh:0u sis, higher nthtearntsheec tiinotner osfec Btiont too hthee superpixel ritp currently belongs to, the exchange is accepted, otherwise it is discarded. [sent-153, score-0.436]
</p><p>38 m Tehs eth faitrs tth one tiso gthraamt v oifde Bo superpixels are of similar size and that the blocks are much smaller than the video superpixels. [sent-161, score-0.623]
</p><p>39 This holds most of the time, since superpixels indeed tend to be of the same size, and the blocks are 379  defined to be at most one fourth of a superpixel in a frame,  and hence, are much smaller than superpixels extending on multiple frames in the video. [sent-162, score-1.288]
</p><p>40 According to the superpixel rate, some frames are selected to terminate and create superpixels. [sent-166, score-0.463]
</p><p>41 They allow to evaluate which termination and creation of superpixels yield higher energy using efficient intersection distances, as well. [sent-170, score-0.585]
</p><p>42 5 there is an illustration of the creation and termination of superpixels with the notation used. [sent-172, score-0.496]
</p><p>43 When a superpixel is terminated, its pixels at frame t are incorporated to a neighbor superpixel. [sent-173, score-0.52]
</p><p>44 (3)  We terminate the superpixel with higher intersection to its neighbor among all superpixels in the frame. [sent-179, score-0.837]
</p><p>45 ina |tAed, a new one should be created to fulfill the constraint of number of superpixels per frame (Sec. [sent-192, score-0.522]
</p><p>46 The candidates to form a new superpixel are blocks of pixels that belong to an existing video superpixel. [sent-195, score-0.678]
</p><p>47 Let Bnt ⊂ Atn:0 and Bmt ⊂ Atm:0 be blocks of superpixels Lcaentd Bidate⊂s tAo creaanted a new superpixel. [sent-196, score-0.505]
</p><p>48 In principle, the algorithm can run for an infinitely long video, since it generates the partition online, and in memory we only need the histograms of the video superpixels that propagate to the current frame. [sent-212, score-0.556]
</p><p>49 In the first frame of the video, the superpixels are initialized along a grid using the hierarchy of blocks. [sent-214, score-0.538]
</p><p>50 Like this, the superpixel structure can be propagated from the previous frame while discarding small details. [sent-218, score-0.502]
</p><p>51 Randomized SEEDS Some superpixel methods offer extra capabilities, such as the extraction of a hierarchy of superpixels [17]. [sent-222, score-0.821]
</p><p>52 In the next section we exploit it to design an objectness measure of temporal windows, though we expect that applications may not be limited to that one. [sent-224, score-0.67]
</p><p>53 6, we give an example of different partitions with the same number of superpixels, with similar energy value and which solutions have very similar accuracy according to the superpixel benchmarks. [sent-228, score-0.486]
</p><p>54 This shows that we can extract multiple samples of superpixel partitions from the same video, all of them of comparable quality. [sent-229, score-0.476]
</p><p>55 6 shows that when superimposing a diverse set of superpixel samples obtained with randomized SEEDS, the boundaries of the objects are preserved, and the boundaries due to over-segmentation fade away. [sent-249, score-0.59]
</p><p>56 In the following, we first define the measure of the objectness in a still image, and then we introduce how to extend it to temporal windows (tubes of bounding boxes). [sent-251, score-0.889]
</p><p>57 The objectness score is computed as the sum of the distances to the Objectness Measure for Still Images. [sent-252, score-0.569]
</p><p>58 We use O to represent the intersection of several superpixel samples of randomized SEEDS. [sent-253, score-0.561]
</p><p>59 O(i) takes value 1if all samples have a superpixel boundary at pixel i, and 0 otherwise. [sent-254, score-0.485]
</p><p>60 Thus, O is an image that indicates in which pixels the samples of randomized SEEDS agree that there is a superpixel boundary. [sent-255, score-0.569]
</p><p>61 We define the objectness score for a still image using O. [sent-256, score-0.59]
</p><p>62 Let X be the set of pixels inside the bounding box, Per(X) tLheet sXet boef pixels i onf t phiex perimeter hofe t bhoeu bounding b,o Pxe,r a(Xnd) XR,C(p) the pixels that are inside the bounding box and in the same row or column as pixel p. [sent-263, score-0.549]
</p><p>63 To the best of our knowledge, no earlier work has used multiple superpixel hypotheses to build an objectness score. [sent-267, score-0.915]
</p><p>64 Comparison of our online video superpixels method to the state-of-the-art (s-o-a). [sent-270, score-0.551]
</p><p>65 The temporal windows in shots allow for incorporating features and classifiers that exploit the spatio-temporal regions, and can easily be incorporated in any video application that uses bounding boxes. [sent-278, score-0.476]
</p><p>66 The aim of video objectness is to reduce these 1050 temporal windows to the 100-1000 most likely to contain an object. [sent-282, score-0.865]
</p><p>67 The video objectness score is proposed as a volumetric extension of Eq. [sent-283, score-0.687]
</p><p>68 In the first frame, all possible bounding boxes are extracted densely and ranked based on the objectness score for still images. [sent-285, score-0.744]
</p><p>69 In the subsequent frames, each bounding box is propagated in time by propagating the video superpixels that are completely inside the bounding box in the  first frame. [sent-286, score-0.789]
</p><p>70 The score is updated online as each new frame is added until the shot is finished, and accordingly, the ranking of the temporal windows is updated online as well. [sent-287, score-0.535]
</p><p>71 Experiments In this section we report experimental evaluation of the introduced online video superpixel method. [sent-289, score-0.602]
</p><p>72 Evaluation of Online Video SEEDS We report results of the online video superpixels on the Chen Xiph. [sent-300, score-0.572]
</p><p>73 To achieve the desired amount of temporal superpixels, we select the  number of superpixels per frame from a range between 200 and 600, and the superpixel rate from a range between 0 and 6. [sent-306, score-1.03]
</p><p>74 This results in a total number of video superpixels between 200 and 1086. [sent-307, score-0.488]
</p><p>75 Evolution of superpixel metrics as a function of the amount of randomization introduced in Eq. [sent-329, score-0.507]
</p><p>76 Evaluation of Randomized SEEDS We evaluate the accuracy of the randomized superpixel samples by analyzing the effect of different levels of randomization added in Eq. [sent-334, score-0.567]
</p><p>77 Evaluation of Video Objectness We report results of the video objectness measure on temporal windows to showcase the advantages of randomized SEEDS on video. [sent-348, score-1.014]
</p><p>78 We also report results of objectness mise between accuracy and efficiency. [sent-349, score-0.538]
</p><p>79 We report results of the objectness measure on PASCAL VOC07 [4]. [sent-351, score-0.562]
</p><p>80 We use our score with the randomized SEEDS to measure the objectness in still images, without  temporal propagation. [sent-355, score-0.847]
</p><p>81 In this way, we are able to compare it to s-o-a objectness measures [1, 11, 6, 14]. [sent-356, score-0.517]
</p><p>82 As baselines, we use the output of boundary detectors, instead of using randomized SEEDS, to compute our objectness score in still images. [sent-357, score-0.731]
</p><p>83 The objectness measure based on randomised SEEDS with 5 samples outperforms the one computed using only one sample, which emphasises the usefulness of using Randomized SEEDS. [sent-362, score-0.605]
</p><p>84 9b there are the results compared to s-o-a objectness measures in still images. [sent-366, score-0.538]
</p><p>85 It shows that our objectness method is competitive with the s-o-a, while being an order of magnitude faster. [sent-367, score-0.539]
</p><p>86 Also note that the presented objectness measure only uses superpixels, while the others rely on additional cues (e. [sent-368, score-0.541]
</p><p>87 We report results for our video objectness score using the Chen dataset [3] where we manually annotated object bounding boxes in the video sequences. [sent-376, score-0.98]
</p><p>88 In the video case, a stricter 50% criterion is used over the entire bounding box tube: the temporal window must overlap  at least 50% with the ground truth over the entire shot of the video. [sent-377, score-0.415]
</p><p>89 As these temporal objectness windows are presented as a novel concept, we compare our method to some baselines. [sent-379, score-0.747]
</p><p>90 Additionally, to show the usefulness of the video objectness score (noted as 3D edge in the figure), we compare with a method that  powbnejrasuctlipenroi5msp a mg eastpilhoeunsdi. [sent-381, score-0.711]
</p><p>91 Comparison of the objectness measure with sampling superpixels on PASCAL VOC07 to (a) baselines, (b) s-o-a, and (c) evalua-  tion of video objectness on the Chen dataset. [sent-389, score-1.546]
</p><p>92 ×  video objectness score (3D edge) there is an improvement in accuracy because the score is updated over time. [sent-392, score-0.739]
</p><p>93 It is interesting to note that the 1-sample-version benefits much more from the video objectness score than the 5-sample-version. [sent-394, score-0.687]
</p><p>94 The reason why is that the video objectness score can be seen as a form of multiple samples as well: the score is the sum over 25 samples in time. [sent-395, score-0.819]
</p><p>95 03s for the superpixel samples, 10−5s for the score computation r( 0th. [sent-398, score-0.43]
</p><p>96 Conclusions In this paper we have introduced a novel online video superpixel algorithm that is able to run in real-time, with accuracy comparable to offline methods. [sent-404, score-0.617]
</p><p>97 To achieve this, we have introduced novel concepts for temporal propagation, termination and creation of superpixels in time, using hierarchical block sizes and temporal histograms. [sent-405, score-0.871]
</p><p>98 We have demonstrated a new capability of our superpixel algorithm by efficiently extracting multiple diverse samples of superpixels. [sent-406, score-0.46]
</p><p>99 This allowed us to introduce a new, highly efficient objectness measure, together with its extension to video ob-  jectness. [sent-407, score-0.635]
</p><p>100 Finally, our experiments  have shown that both  the video superpixel and objectness algorithms match s-oa offline methods in terms of accuracy, but at much higher speeds. [sent-409, score-1.049]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('objectness', 0.517), ('superpixel', 0.378), ('superpixels', 0.37), ('seeds', 0.319), ('blocks', 0.135), ('bnt', 0.13), ('temporal', 0.129), ('video', 0.118), ('atn', 0.107), ('randomized', 0.104), ('windows', 0.101), ('bounding', 0.097), ('frame', 0.095), ('cbtn', 0.093), ('atm', 0.091), ('int', 0.076), ('catm', 0.074), ('catn', 0.074), ('hierarchy', 0.073), ('tubes', 0.072), ('climbing', 0.072), ('partition', 0.068), ('termination', 0.068), ('block', 0.067), ('exchanges', 0.066), ('online', 0.063), ('hill', 0.061), ('partitions', 0.058), ('exchange', 0.058), ('creation', 0.058), ('tube', 0.057), ('boxes', 0.057), ('atp', 0.056), ('bmt', 0.056), ('exchanging', 0.055), ('score', 0.052), ('terminate', 0.05), ('energy', 0.05), ('atk', 0.049), ('pixels', 0.047), ('injecting', 0.046), ('gbh', 0.046), ('randomization', 0.045), ('streaming', 0.043), ('samples', 0.04), ('intersection', 0.039), ('box', 0.039), ('boundary', 0.037), ('aated', 0.037), ('bsd', 0.037), ('catk', 0.037), ('cfurramreent', 0.037), ('oath', 0.037), ('stills', 0.037), ('videos', 0.036), ('offline', 0.036), ('frames', 0.035), ('metrics', 0.034), ('boundaries', 0.034), ('atq', 0.033), ('intermediary', 0.033), ('shot', 0.032), ('shots', 0.031), ('exchanged', 0.03), ('pixel', 0.03), ('per', 0.03), ('orders', 0.029), ('propagated', 0.029), ('nystrom', 0.029), ('supplementary', 0.028), ('hierarchical', 0.028), ('amount', 0.028), ('perimeter', 0.027), ('ntr', 0.027), ('fulfill', 0.027), ('undersegmentation', 0.027), ('sharon', 0.026), ('hj', 0.026), ('gpb', 0.026), ('layers', 0.026), ('layer', 0.025), ('starts', 0.024), ('measure', 0.024), ('usefulness', 0.024), ('blobs', 0.024), ('pascal', 0.023), ('canny', 0.023), ('magnitude', 0.022), ('cuts', 0.022), ('introduced', 0.022), ('baselines', 0.022), ('material', 0.022), ('capability', 0.021), ('hofe', 0.021), ('extracting', 0.021), ('report', 0.021), ('stream', 0.021), ('still', 0.021), ('seconds', 0.02), ('hypotheses', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="299-tfidf-1" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>Author: Michael Van_Den_Bergh, Gemma Roig, Xavier Boix, Santiago Manen, Luc Van_Gool</p><p>Abstract: Superpixel and objectness algorithms are broadly used as a pre-processing step to generate support regions and to speed-up further computations. Recently, many algorithms have been extended to video in order to exploit the temporal consistency between frames. However, most methods are computationally too expensive for real-time applications. We introduce an online, real-time video superpixel algorithm based on the recently proposed SEEDS superpixels. A new capability is incorporated which delivers multiple diverse samples (hypotheses) of superpixels in the same image or video sequence. The multiple samples are shown to provide a strong cue to efficiently measure the objectness of image windows, and we introduce the novel concept of objectness in temporal windows. Experiments show that the video superpixels achieve comparable performance to state-of-the-art offline methods while running at 30 fps on a single 2.8 GHz i7 CPU. State-of-the-art performance on objectness is also demonstrated, yet orders of magnitude faster and extended to temporal windows in video.</p><p>2 0.32365906 <a title="299-tfidf-2" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>Author: Matthias Reso, Jörn Jachalsky, Bodo Rosenhahn, Jörn Ostermann</p><p>Abstract: Superpixel algorithms represent a very useful and increasingly popular preprocessing step for a wide range of computer vision applications, as they offer the potential to boost efficiency and effectiveness. In this regards, this paper presents a highly competitive approach for temporally consistent superpixelsfor video content. The approach is based on energy-minimizing clustering utilizing a novel hybrid clustering strategy for a multi-dimensional feature space working in a global color subspace and local spatial subspaces. Moreover, a new contour evolution based strategy is introduced to ensure spatial coherency of the generated superpixels. For a thorough evaluation the proposed approach is compared to state of the art supervoxel algorithms using established benchmarks and shows a superior performance.</p><p>3 0.31891197 <a title="299-tfidf-3" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>Author: Abdelaziz Djelouah, Jean-Sébastien Franco, Edmond Boyer, François Le_Clerc, Patrick Pérez</p><p>Abstract: In this paper, we address the problem of object segmentation in multiple views or videos when two or more viewpoints of the same scene are available. We propose a new approach that propagates segmentation coherence information in both space and time, hence allowing evidences in one image to be shared over the complete set. To this aim the segmentation is cast as a single efficient labeling problem over space and time with graph cuts. In contrast to most existing multi-view segmentation methods that rely on some form of dense reconstruction, ours only requires a sparse 3D sampling to propagate information between viewpoints. The approach is thoroughly evaluated on standard multiview datasets, as well as on videos. With static views, results compete with state of the art methods but they are achieved with significantly fewer viewpoints. With multiple videos, we report results that demonstrate the benefit of segmentation propagation through temporal cues.</p><p>4 0.25796276 <a title="299-tfidf-4" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>Author: Yangqing Jia, Mei Han</p><p>Abstract: It is known that purely low-level saliency cues such as frequency does not lead to a good salient object detection result, requiring high-level knowledge to be adopted for successful discovery of task-independent salient objects. In this paper, we propose an efficient way to combine such high-level saliency priors and low-level appearance models. We obtain the high-level saliency prior with the objectness algorithm to find potential object candidates without the need of category information, and then enforce the consistency among the salient regions using a Gaussian MRF with the weights scaled by diverse density that emphasizes the influence of potential foreground pixels. Our model obtains saliency maps that assign high scores for the whole salient object, and achieves state-of-the-art performance on benchmark datasets covering various foreground statistics.</p><p>5 0.18499869 <a title="299-tfidf-5" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>Author: Anestis Papazoglou, Vittorio Ferrari</p><p>Abstract: We present a technique for separating foreground objects from the background in a video. Our method isfast, , fully automatic, and makes minimal assumptions about the video. This enables handling essentially unconstrained settings, including rapidly moving background, arbitrary object motion and appearance, and non-rigid deformations and articulations. In experiments on two datasets containing over 1400 video shots, our method outperforms a state-of-theart background subtraction technique [4] as well as methods based on clustering point tracks [6, 18, 19]. Moreover, it performs comparably to recent video object segmentation methods based on objectproposals [14, 16, 27], while being orders of magnitude faster.</p><p>6 0.14951317 <a title="299-tfidf-6" href="./iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation.html">383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</a></p>
<p>7 0.14947973 <a title="299-tfidf-7" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>8 0.14936909 <a title="299-tfidf-8" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>9 0.12872888 <a title="299-tfidf-9" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>10 0.12612364 <a title="299-tfidf-10" href="./iccv-2013-Estimating_the_3D_Layout_of_Indoor_Scenes_and_Its_Clutter_from_Depth_Sensors.html">144 iccv-2013-Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors</a></p>
<p>11 0.116025 <a title="299-tfidf-11" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>12 0.1148451 <a title="299-tfidf-12" href="./iccv-2013-Detecting_Curved_Symmetric_Parts_Using_a_Deformable_Disc_Model.html">110 iccv-2013-Detecting Curved Symmetric Parts Using a Deformable Disc Model</a></p>
<p>13 0.087655768 <a title="299-tfidf-13" href="./iccv-2013-Flattening_Supervoxel_Hierarchies_by_the_Uniform_Entropy_Slice.html">172 iccv-2013-Flattening Supervoxel Hierarchies by the Uniform Entropy Slice</a></p>
<p>14 0.081379801 <a title="299-tfidf-14" href="./iccv-2013-A_Unified_Video_Segmentation_Benchmark%3A_Annotation%2C_Metrics_and_Analysis.html">33 iccv-2013-A Unified Video Segmentation Benchmark: Annotation, Metrics and Analysis</a></p>
<p>15 0.079885721 <a title="299-tfidf-15" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>16 0.077903472 <a title="299-tfidf-16" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>17 0.073541693 <a title="299-tfidf-17" href="./iccv-2013-Coarse-to-Fine_Semantic_Video_Segmentation_Using_Supervoxel_Trees.html">76 iccv-2013-Coarse-to-Fine Semantic Video Segmentation Using Supervoxel Trees</a></p>
<p>18 0.073072061 <a title="299-tfidf-18" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>19 0.071868226 <a title="299-tfidf-19" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>20 0.071457401 <a title="299-tfidf-20" href="./iccv-2013-Holistic_Scene_Understanding_for_3D_Object_Detection_with_RGBD_Cameras.html">201 iccv-2013-Holistic Scene Understanding for 3D Object Detection with RGBD Cameras</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.165), (1, -0.034), (2, 0.141), (3, 0.021), (4, 0.042), (5, 0.048), (6, -0.086), (7, 0.071), (8, -0.015), (9, -0.09), (10, -0.009), (11, 0.13), (12, 0.06), (13, 0.029), (14, -0.097), (15, -0.078), (16, -0.04), (17, -0.072), (18, -0.146), (19, -0.047), (20, 0.056), (21, -0.147), (22, -0.094), (23, -0.037), (24, -0.117), (25, -0.018), (26, -0.08), (27, 0.025), (28, -0.122), (29, 0.028), (30, 0.008), (31, 0.051), (32, -0.055), (33, 0.064), (34, 0.163), (35, -0.119), (36, 0.193), (37, -0.055), (38, -0.07), (39, -0.02), (40, 0.064), (41, -0.074), (42, -0.064), (43, 0.042), (44, -0.03), (45, 0.006), (46, 0.129), (47, 0.048), (48, 0.146), (49, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96135116 <a title="299-lsi-1" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>Author: Michael Van_Den_Bergh, Gemma Roig, Xavier Boix, Santiago Manen, Luc Van_Gool</p><p>Abstract: Superpixel and objectness algorithms are broadly used as a pre-processing step to generate support regions and to speed-up further computations. Recently, many algorithms have been extended to video in order to exploit the temporal consistency between frames. However, most methods are computationally too expensive for real-time applications. We introduce an online, real-time video superpixel algorithm based on the recently proposed SEEDS superpixels. A new capability is incorporated which delivers multiple diverse samples (hypotheses) of superpixels in the same image or video sequence. The multiple samples are shown to provide a strong cue to efficiently measure the objectness of image windows, and we introduce the novel concept of objectness in temporal windows. Experiments show that the video superpixels achieve comparable performance to state-of-the-art offline methods while running at 30 fps on a single 2.8 GHz i7 CPU. State-of-the-art performance on objectness is also demonstrated, yet orders of magnitude faster and extended to temporal windows in video.</p><p>2 0.88765335 <a title="299-lsi-2" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>Author: Matthias Reso, Jörn Jachalsky, Bodo Rosenhahn, Jörn Ostermann</p><p>Abstract: Superpixel algorithms represent a very useful and increasingly popular preprocessing step for a wide range of computer vision applications, as they offer the potential to boost efficiency and effectiveness. In this regards, this paper presents a highly competitive approach for temporally consistent superpixelsfor video content. The approach is based on energy-minimizing clustering utilizing a novel hybrid clustering strategy for a multi-dimensional feature space working in a global color subspace and local spatial subspaces. Moreover, a new contour evolution based strategy is introduced to ensure spatial coherency of the generated superpixels. For a thorough evaluation the proposed approach is compared to state of the art supervoxel algorithms using established benchmarks and shows a superior performance.</p><p>3 0.72693884 <a title="299-lsi-3" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>Author: Abdelaziz Djelouah, Jean-Sébastien Franco, Edmond Boyer, François Le_Clerc, Patrick Pérez</p><p>Abstract: In this paper, we address the problem of object segmentation in multiple views or videos when two or more viewpoints of the same scene are available. We propose a new approach that propagates segmentation coherence information in both space and time, hence allowing evidences in one image to be shared over the complete set. To this aim the segmentation is cast as a single efficient labeling problem over space and time with graph cuts. In contrast to most existing multi-view segmentation methods that rely on some form of dense reconstruction, ours only requires a sparse 3D sampling to propagate information between viewpoints. The approach is thoroughly evaluated on standard multiview datasets, as well as on videos. With static views, results compete with state of the art methods but they are achieved with significantly fewer viewpoints. With multiple videos, we report results that demonstrate the benefit of segmentation propagation through temporal cues.</p><p>4 0.62420064 <a title="299-lsi-4" href="./iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation.html">383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</a></p>
<p>Author: Zhengxiang Wang, Rujie Liu</p><p>Abstract: This paper introduces to use semi-supervised learning for large scale image cosegmentation. Different from traditional unsupervised cosegmentation that does not use any segmentation groundtruth, semi-supervised cosegmentation exploits the similarity from both the very limited training image foregrounds, as well as the common object shared between the large number of unsegmented images. This would be a much practical way to effectively cosegment a large number of related images simultaneously, where previous unsupervised cosegmentation work poorly due to the large variances in appearance between different images and the lack ofsegmentation groundtruthfor guidance in cosegmentation. For semi-supervised cosegmentation in large scale, we propose an effective method by minimizing an energy function, which consists of the inter-image distance, the intraimage distance and the balance term. We also propose an iterative updating algorithm to efficiently solve this energy function, which decomposes the original energy minimization problem into sub-problems, and updates each image alternatively to reduce the number of variables in each subproblem for computation efficiency. Experiment results on iCoseg and Pascal VOC datasets show that the proposed cosegmentation method can effectively cosegment hundreds of images in less than one minute. And our semi-supervised cosegmentation is able to outperform both unsupervised cosegmentation as well asfully supervised single image segmentation, especially when the training data is limited.</p><p>5 0.54479569 <a title="299-lsi-5" href="./iccv-2013-Coarse-to-Fine_Semantic_Video_Segmentation_Using_Supervoxel_Trees.html">76 iccv-2013-Coarse-to-Fine Semantic Video Segmentation Using Supervoxel Trees</a></p>
<p>Author: Aastha Jain, Shuanak Chatterjee, René Vidal</p><p>Abstract: We propose an exact, general and efficient coarse-to-fine energy minimization strategy for semantic video segmentation. Our strategy is based on a hierarchical abstraction of the supervoxel graph that allows us to minimize an energy defined at the finest level of the hierarchy by minimizing a series of simpler energies defined over coarser graphs. The strategy is exact, i.e., it produces the same solution as minimizing over the finest graph. It is general, i.e., it can be used to minimize any energy function (e.g., unary, pairwise, and higher-order terms) with any existing energy minimization algorithm (e.g., graph cuts and belief propagation). It also gives significant speedups in inference for several datasets with varying degrees of spatio-temporal continuity. We also discuss the strengths and weaknesses of our strategy relative to existing hierarchical approaches, and the kinds of image and video data that provide the best speedups.</p><p>6 0.52857596 <a title="299-lsi-6" href="./iccv-2013-A_Unified_Video_Segmentation_Benchmark%3A_Annotation%2C_Metrics_and_Analysis.html">33 iccv-2013-A Unified Video Segmentation Benchmark: Annotation, Metrics and Analysis</a></p>
<p>7 0.52856612 <a title="299-lsi-7" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>8 0.50087667 <a title="299-lsi-8" href="./iccv-2013-Flattening_Supervoxel_Hierarchies_by_the_Uniform_Entropy_Slice.html">172 iccv-2013-Flattening Supervoxel Hierarchies by the Uniform Entropy Slice</a></p>
<p>9 0.49407521 <a title="299-lsi-9" href="./iccv-2013-Detecting_Curved_Symmetric_Parts_Using_a_Deformable_Disc_Model.html">110 iccv-2013-Detecting Curved Symmetric Parts Using a Deformable Disc Model</a></p>
<p>10 0.43430513 <a title="299-lsi-10" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>11 0.43012333 <a title="299-lsi-11" href="./iccv-2013-Motion-Aware_KNN_Laplacian_for_Video_Matting.html">275 iccv-2013-Motion-Aware KNN Laplacian for Video Matting</a></p>
<p>12 0.36796194 <a title="299-lsi-12" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>13 0.36117548 <a title="299-lsi-13" href="./iccv-2013-Space-Time_Tradeoffs_in_Photo_Sequencing.html">397 iccv-2013-Space-Time Tradeoffs in Photo Sequencing</a></p>
<p>14 0.354229 <a title="299-lsi-14" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<p>15 0.35037306 <a title="299-lsi-15" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>16 0.34727234 <a title="299-lsi-16" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>17 0.34384915 <a title="299-lsi-17" href="./iccv-2013-The_Interestingness_of_Images.html">416 iccv-2013-The Interestingness of Images</a></p>
<p>18 0.33769241 <a title="299-lsi-18" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>19 0.33711049 <a title="299-lsi-19" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>20 0.33164269 <a title="299-lsi-20" href="./iccv-2013-Estimating_the_3D_Layout_of_Indoor_Scenes_and_Its_Clutter_from_Depth_Sensors.html">144 iccv-2013-Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.056), (7, 0.013), (12, 0.275), (26, 0.128), (31, 0.028), (40, 0.013), (42, 0.078), (48, 0.022), (64, 0.073), (73, 0.028), (89, 0.15), (98, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88452256 <a title="299-lda-1" href="./iccv-2013-Target-Driven_Moire_Pattern_Synthesis_by_Phase_Modulation.html">413 iccv-2013-Target-Driven Moire Pattern Synthesis by Phase Modulation</a></p>
<p>Author: Pei-Hen Tsai, Yung-Yu Chuang</p><p>Abstract: This paper investigates an approach for generating two grating images so that the moir e´ pattern of their superposition resembles the target image. Our method is grounded on the fundamental moir e´ theorem. By focusing on the visually most dominant (1, −1)-moir e´ component, we obtain the phase smto ddoumlaintiaonnt c (o1n,s−tr1a)in-mt on the phase shifts bee otwbteaeinn the two grating images. For improving visual appearance of the grating images and hiding capability the embedded image, a smoothness term is added to spread information between the two grating images and an appearance phase function is used to add irregular structures into grating images. The grating images can be printed on transparencies and the hidden image decoding can be performed optically by overlaying them together. The proposed method enables the creation of moir e´ art and allows visual decoding without computers.</p><p>2 0.85090387 <a title="299-lda-2" href="./iccv-2013-POP%3A_Person_Re-identification_Post-rank_Optimisation.html">305 iccv-2013-POP: Person Re-identification Post-rank Optimisation</a></p>
<p>Author: Chunxiao Liu, Chen Change Loy, Shaogang Gong, Guijin Wang</p><p>Abstract: Owing to visual ambiguities and disparities, person reidentification methods inevitably produce suboptimal ranklist, which still requires exhaustive human eyeballing to identify the correct target from hundreds of different likelycandidates. Existing re-identification studies focus on improving the ranking performance, but rarely look into the critical problem of optimising the time-consuming and error-prone post-rank visual search at the user end. In this study, we present a novel one-shot Post-rank OPtimisation (POP) method, which allows a user to quickly refine their search by either “one-shot” or a couple of sparse negative selections during a re-identification process. We conduct systematic behavioural studies to understand user’s searching behaviour and show that the proposed method allows correct re-identification to converge 2.6 times faster than the conventional exhaustive search. Importantly, through extensive evaluations we demonstrate that the method is capable of achieving significant improvement over the stateof-the-art distance metric learning based ranking models, even with just “one shot” feedback optimisation, by as much as over 30% performance improvement for rank 1reidentification on the VIPeR and i-LIDS datasets.</p><p>3 0.7897796 <a title="299-lda-3" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>Author: Mohamed Elhoseiny, Babak Saleh, Ahmed Elgammal</p><p>Abstract: The main question we address in this paper is how to use purely textual description of categories with no training images to learn visual classifiers for these categories. We propose an approach for zero-shot learning of object categories where the description of unseen categories comes in the form of typical text such as an encyclopedia entry, without the need to explicitly defined attributes. We propose and investigate two baseline formulations, based on regression and domain adaptation. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the classifier parameters for new classes. We applied the proposed approach on two fine-grained categorization datasets, and the results indicate successful classifier prediction.</p><p>same-paper 4 0.78232473 <a title="299-lda-4" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>Author: Michael Van_Den_Bergh, Gemma Roig, Xavier Boix, Santiago Manen, Luc Van_Gool</p><p>Abstract: Superpixel and objectness algorithms are broadly used as a pre-processing step to generate support regions and to speed-up further computations. Recently, many algorithms have been extended to video in order to exploit the temporal consistency between frames. However, most methods are computationally too expensive for real-time applications. We introduce an online, real-time video superpixel algorithm based on the recently proposed SEEDS superpixels. A new capability is incorporated which delivers multiple diverse samples (hypotheses) of superpixels in the same image or video sequence. The multiple samples are shown to provide a strong cue to efficiently measure the objectness of image windows, and we introduce the novel concept of objectness in temporal windows. Experiments show that the video superpixels achieve comparable performance to state-of-the-art offline methods while running at 30 fps on a single 2.8 GHz i7 CPU. State-of-the-art performance on objectness is also demonstrated, yet orders of magnitude faster and extended to temporal windows in video.</p><p>5 0.7522552 <a title="299-lda-5" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>Author: Mihai Zanfir, Marius Leordeanu, Cristian Sminchisescu</p><p>Abstract: Human action recognition under low observational latency is receiving a growing interest in computer vision due to rapidly developing technologies in human-robot interaction, computer gaming and surveillance. In this paper we propose a fast, simple, yet powerful non-parametric Moving Pose (MP)frameworkfor low-latency human action and activity recognition. Central to our methodology is a moving pose descriptor that considers both pose information as well as differential quantities (speed and acceleration) of the human body joints within a short time window around the current frame. The proposed descriptor is used in conjunction with a modified kNN classifier that considers both the temporal location of a particular frame within the action sequence as well as the discrimination power of its moving pose descriptor compared to other frames in the training set. The resulting method is non-parametric and enables low-latency recognition, one-shot learning, and action detection in difficult unsegmented sequences. Moreover, the framework is real-time, scalable, and outperforms more sophisticated approaches on challenging benchmarks like MSR-Action3D or MSR-DailyActivities3D.</p><p>6 0.72723716 <a title="299-lda-6" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>7 0.69170141 <a title="299-lda-7" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>8 0.68261492 <a title="299-lda-8" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>9 0.66834068 <a title="299-lda-9" href="./iccv-2013-Efficient_Pedestrian_Detection_by_Directly_Optimizing_the_Partial_Area_under_the_ROC_Curve.html">136 iccv-2013-Efficient Pedestrian Detection by Directly Optimizing the Partial Area under the ROC Curve</a></p>
<p>10 0.66426092 <a title="299-lda-10" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>11 0.65419227 <a title="299-lda-11" href="./iccv-2013-YouTube2Text%3A_Recognizing_and_Describing_Arbitrary_Activities_Using_Semantic_Hierarchies_and_Zero-Shot_Recognition.html">452 iccv-2013-YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition</a></p>
<p>12 0.64989042 <a title="299-lda-12" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>13 0.6492756 <a title="299-lda-13" href="./iccv-2013-Beyond_Hard_Negative_Mining%3A_Efficient_Detector_Learning_via_Block-Circulant_Decomposition.html">61 iccv-2013-Beyond Hard Negative Mining: Efficient Detector Learning via Block-Circulant Decomposition</a></p>
<p>14 0.64913923 <a title="299-lda-14" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>15 0.64889324 <a title="299-lda-15" href="./iccv-2013-Translating_Video_Content_to_Natural_Language_Descriptions.html">428 iccv-2013-Translating Video Content to Natural Language Descriptions</a></p>
<p>16 0.64734989 <a title="299-lda-16" href="./iccv-2013-Handling_Occlusions_with_Franken-Classifiers.html">190 iccv-2013-Handling Occlusions with Franken-Classifiers</a></p>
<p>17 0.64708686 <a title="299-lda-17" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>18 0.64481574 <a title="299-lda-18" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>19 0.64080012 <a title="299-lda-19" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<p>20 0.63817728 <a title="299-lda-20" href="./iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F.html">316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
