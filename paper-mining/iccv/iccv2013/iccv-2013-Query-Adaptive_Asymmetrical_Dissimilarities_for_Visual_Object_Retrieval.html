<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-334" href="#">iccv2013-334</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</h1>
<br/><p>Source: <a title="iccv-2013-334-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Zhu_Query-Adaptive_Asymmetrical_Dissimilarities_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Cai-Zhi Zhu, Hervé Jégou, Shin'Ichi Satoh</p><p>Abstract: Visual object retrieval aims at retrieving, from a collection of images, all those in which a given query object appears. It is inherently asymmetric: the query object is mostly included in the database image, while the converse is not necessarily true. However, existing approaches mostly compare the images with symmetrical measures, without considering the different roles of query and database. This paper first measure the extent of asymmetry on large-scale public datasets reflecting this task. Considering the standard bag-of-words representation, we then propose new asymmetrical dissimilarities accounting for the different inlier ratios associated with query and database images. These asymmetrical measures depend on the query, yet they are compatible with an inverted file structure, without noticeably impacting search efficiency. Our experiments show the benefit of our approach, and show that the visual object retrieval task is better treated asymmetrically, in the spirit of state-of-the-art text retrieval.</p><p>Reference: <a title="iccv-2013-334-reference" href="../iccv2013_reference/iccv-2013-Query-Adaptive_Asymmetrical_Dissimilarities_for_Visual_Object_Retrieval_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Query-adaptive asymmetrical dissimilarities for visual object retrieval Cai-Zhi Zhu NII, Tokyo cai -z hi zhu @ nii ac . [sent-1, score-0.9]
</p><p>2 j p i  Abstract Visual object retrieval aims at retrieving, from a collection of images, all those in which a given query object appears. [sent-7, score-0.542]
</p><p>3 It is inherently asymmetric: the query object is mostly included in the database image, while the converse is not necessarily true. [sent-8, score-0.512]
</p><p>4 However, existing approaches mostly compare the images with symmetrical measures, without considering the different roles of query and database. [sent-9, score-0.57]
</p><p>5 Considering the standard bag-of-words representation, we then propose new asymmetrical dissimilarities accounting for the different inlier ratios associated with query and database images. [sent-11, score-1.279]
</p><p>6 These asymmetrical measures depend on the query, yet they are compatible with an inverted file structure, without noticeably impacting search efficiency. [sent-12, score-0.685]
</p><p>7 Introduction  The purpose of visual object retrieval is to search a specific object in large-scale image/video datasets. [sent-15, score-0.25]
</p><p>8 This difference is illustrated in Figure 1, where it appears that the two tasks mostly differ by how the query is defined. [sent-17, score-0.388]
</p><p>9 In object retrieval, a bounding box or a shape delimits the query entity, such as a person, place, or other object. [sent-18, score-0.442]
</p><p>10 In contrast, similar image search assumes that the query is the full image. [sent-19, score-0.376]
</p><p>11 This task is the visual counterpart of searching by query terms in textual information retrieval, where a few words or a short descriptions are compared with large textual documents. [sent-20, score-0.429]
</p><p>12 Early in the 60’s, the SMART system designed by Salton [20], considered text retrieval as an asymmetrical scenario. [sent-21, score-0.651]
</p><p>13 Similarly, state-of-the-art textual engines rely on asymmetrical measures, for instance by using different term weighting schemes for the query and database elements, such as in the Okapi [18, 19] method. [sent-22, score-1.084]
</p><p>14 In (a) object retrieval, the query is delimited by a bounding box or a shape, while in (b) similar image search, the query and database objects are of the same kind. [sent-25, score-0.896]
</p><p>15 This paper shows the importance of designing an asymmetrical dissimilarity measure for object retrieval, in order to better take into account the different inlier ratios between query objects and database images. [sent-26, score-1.358]
</p><p>16 Other techniques include improving the initial ranking list by exploiting spatial geometry [17, 21, 23] and query expansion [3]. [sent-37, score-0.47]
</p><p>17 All these approaches rely on a symmetrical metric to produce the initial ranking of the image collection, such as the ? [sent-38, score-0.28]
</p><p>18 As a result, the body of literature on asymmetrical metrics is limited. [sent-42, score-0.536]
</p><p>19 [8], who consider asymmetrical kernels for the problem of domain adaptation in image classification. [sent-44, score-0.536]
</p><p>20 Note that, although Bregman divergences such as Kullback-Leibler [9] are asymmetrical by construction, they do not reflect the underlying assumptions underpinning visual object recognition and lead to poor comparison results. [sent-46, score-0.634]
</p><p>21 We argue that symmetrical metrics are not optimal for judging the presence of query objects. [sent-48, score-0.551]
</p><p>22 This is because most of the area in the query image is useful: The bounding box or shape tightly delimits the relevant object. [sent-49, score-0.403]
</p><p>23 When the images are described by local descriptors, this leads to very different inlier ratios in the query and database images. [sent-53, score-0.623]
</p><p>24 First, we quantitatively analyze the different properties of the query and of the database images in visual object retrieval. [sent-56, score-0.523]
</p><p>25 We carried out our analysis on popular large-scale object retrieval datasets and the results show the extent to which this task is asymmetrical. [sent-57, score-0.204]
</p><p>26 Focusing on the standard BoW method, we then propose new query-adaptive asymmetrical dissimilarities. [sent-58, score-0.536]
</p><p>27 They are specially designed to take into account the asymmetry ofthe comparison underpinning visual object retrieval. [sent-59, score-0.264]
</p><p>28 They are defined on-the-fly for each query in order to account for the expected inlier ratio. [sent-60, score-0.509]
</p><p>29 Our method improves the initial ranking in comparison with a symmetrical baseline that already achieves state-of-the-art performance for the initial ranking. [sent-63, score-0.359]
</p><p>30 Section 2  introduces the datasets used through the paper to evaluate visual object retrieval, and illustrates the importance of asymmetry in this task. [sent-65, score-0.24]
</p><p>31 Section 3 describes our queryadaptive asymmetrical dissimilarities and how to calculate them with an inverted index. [sent-66, score-0.73]
</p><p>32 Object retrieval: an asymmetrical scenario This section shows that the asymmetry phenomenon is prevalent in visual object retrieval datasets. [sent-70, score-0.865]
</p><p>33 For this purpose, we first introduce three public benchmarks, which correspond to application scenarios where the query is an object instance. [sent-71, score-0.411]
</p><p>34 Finally, we analyze the asymmetry of inliers in query and database images in visual object recognition tasks and discuss the limitations of the symmetrical BoW in this context. [sent-73, score-0.932]
</p><p>35 A Region of Interest (ROI) is defined for each query image. [sent-79, score-0.349]
</p><p>36 A query topic may refer to a person, an object or a place. [sent-94, score-0.412]
</p><p>37 Each query topic consists of several query images and corresponding masks delimiting the ROI. [sent-95, score-0.758]
</p><p>38 For each query topic, the system has to return the 1000 video clips that are most likely to contain a recognizable instance of the query topic. [sent-96, score-0.747]
</p><p>39 As a result, the quality of the initial ranking (before spatial verification and query expansion) is critical. [sent-98, score-0.452]
</p><p>40 Similarly, a query image iis described∈ by a histogram vector Qi ∈ Rk computed from the descriptors appearing in the RO∈I. [sent-122, score-0.387]
</p><p>41 p-distance is computed between the query and all databases vectors to order the database images. [sent-126, score-0.454]
</p><p>42 The toy example in Figure 2 illustrates the drawback of using Equation 1 as a scoring method in an asymmetrical object retrieval scenario. [sent-142, score-0.739]
</p><p>43 In the first row, the object region in the query image is delimited by an ellipse. [sent-143, score-0.424]
</p><p>44 In this case, Image 2 is the correct answer and contains all the features of the query object. [sent-146, score-0.349]
</p><p>45 Such failures are frequent for small query objects like those of the Trecvid INS task, because the distance favors the selection of images described by a small number of features. [sent-151, score-0.349]
</p><p>46 2) with our asymmetrical dissimilarity in the third row (δ1 (Q, T1, ∞) = 1, δ1 (Q, T2, ∞) = 0). [sent-161, score-0.675]
</p><p>47 Statistical analysis of asymmetry In order to evaluate the extent of asymmetry in visual object retrieval, we consider the voting interpretation of the BoW framework [21, 5]. [sent-164, score-0.383]
</p><p>48 More specifically, a pair of features respectively from a query and test image is regarded as a match if these features are quantized to the same visual word. [sent-165, score-0.379]
</p><p>49 Inliers (Inl): Features belong to a matching pair (note that they may or may not correspond to a true match between the query and database images); 2. [sent-169, score-0.477]
</p><p>50 Query outliers (Qout): The query features (in the ROI) that do not correspond to any feature in the database image; 3. [sent-170, score-0.516]
</p><p>51 Database outliers (Dout): The features of the database that do not have any matching feature in the query ROI. [sent-171, score-0.493]
</p><p>52 k) component in the given query and database vectors. [sent-177, score-0.454]
</p><p>53 The unmatched features are then counted  Qil > Tjl)  as the outliers either of the query (if or of the database (if < ) images. [sent-180, score-0.493]
</p><p>54 Protocol to collect matching statistics from the histogram values Qil and Tjl : the bottom-right cell collects the inliers, while the top-right and bottom-left cells respectively correspond to the outliers of the query and database images. [sent-183, score-0.516]
</p><p>55 8erof  inliers/outliers when matching the query ROI with the corresponding relevant images of Oxford105K. [sent-186, score-0.349]
</p><p>56 Figure 3 shows the estimation of these quantities for a particular query image contained in the Oxford105K benchmark. [sent-191, score-0.393]
</p><p>57 Note that a joint average scheme [1, 24] is used for the TrecVid INS datasets: multiple images in each video clip and query topic are jointly quantized to form a single BoW vector by average pooling. [sent-195, score-0.373]
</p><p>58 By defining the inlier ratio as the number of matched feature points divided by the total number of features, we calculate the inlier ratio associated with the query and database sides as Inl/(Inl + Qout) and Inl/(Inl + Dout), respectively. [sent-197, score-0.76]
</p><p>59 As is to be expected for visual object recognition of small objects, Table 3 clearly shows that Qout << Dout, meaning that the inlier ratio is much higher in the queries than in the corresponding database images; i. [sent-199, score-0.374]
</p><p>60 , the features points of the query ROI are significantly more likely to be present in a database image than the inverse. [sent-201, score-0.454]
</p><p>61 Examples visualizing the asymmetrical inlier/outlier ratio on the query and database side on each benchmark. [sent-218, score-1.023]
</p><p>62 Asymmetrical dissimilarity The objective of the object retrieval task is to determine the existence of the query object, and it is inherently asymmetric: A appearing in B does not necessarily means that B also appears in A (see Figures 2 and 4). [sent-224, score-0.7]
</p><p>63 This is reflected in the asymmetry of the inlier ratio on the benchmarks. [sent-225, score-0.298]
</p><p>64 For this reason, we deem that the standard BoW scoring method is better adapted to the symmetrical similarity image search problem (without ROI), but is not optimal for visual object retrieval. [sent-230, score-0.368]
</p><p>65 In short, we argue that a symmetrical metric is designed for measuring a symmetrical similarity problem, while the asymmetry of visual object recognition requires an asymmetrical dissimilarity. [sent-231, score-1.154]
</p><p>66 This section describes asymmetrical dissimilarities that are specifically adapted to this task. [sent-232, score-0.677]
</p><p>67 Their design is motivated by the following observations:  •  The normalization severely penalizes the database images oinr mwahli czhat othne query object zise ss tmheall d atnabda corresponds to a small number of features (see Figure 2). [sent-233, score-0.493]
</p><p>68 After introducing our asymmetrical dissimilarities, we show how the computation is sped-up with an inverted file. [sent-263, score-0.61]
</p><p>69 Asymmetrical penalties We define our asymmetrical dissimilarity as follows: δp(Qi, Tj, w) = ? [sent-266, score-0.695]
</p><p>70 As one can deduce from Table 2, the values w and 1are penalties associated with the query and database (estimated) outliers, respectively. [sent-276, score-0.474]
</p><p>71 This means that we severely penalize features that are detected in the query object regions having no corresponding features in the database image. [sent-280, score-0.493]
</p><p>72 1, none of these choices is satisfactory, because none is adapted to the specific query and database. [sent-299, score-0.404]
</p><p>73 Instead, the next subsection introduces a querydependent method that automatically adapts the weight w to a given query and database. [sent-300, score-0.369]
</p><p>74 Query-adaptive dissimilarity The weight w reflects the different inlier ratios between the query and database images. [sent-303, score-0.762]
</p><p>75 Yet the parameter wopt highly depends on the dataset, for instance, wopt is 700, 300, 1500 and 700 for the Oxford105K, Oxford105K*, INS201 1and INS2012, respectively. [sent-307, score-0.233]
</p><p>76 In other terms, such a strategy implicitly assumes that the inlier ratio is constant across query and database images, which is not true in practice. [sent-308, score-0.607]
</p><p>77 We partially address this problem by automatically selecting w on-the-fly, at query time. [sent-309, score-0.349]
</p><p>78 , which has not impact on tphpei rnegla tthivee c ranking toefr tmhe w images, and setting w¯ = w+ 1, we re-define an equivalent dissimilarity measure as 1  δ1 (Qi, Tj, w¯ ) = ? [sent-320, score-0.232]
</p><p>79 is small) and have many matches with the query (? [sent-330, score-0.349]
</p><p>80 is large) will be regarded as similar to the query region. [sent-334, score-0.349]
</p><p>81 1 11770099  The benefit of this expression is that it automatically adapts  the dissimilarity function to 1) the database and to 2) the particular query Qi with the denominator. [sent-345, score-0.613]
</p><p>82 Speeding-up retrieval with an inverted index The direct calculation of the dissimilarities with Equations 4 or 5 requires one to access all the vector components. [sent-379, score-0.309]
</p><p>83 The symmetrical distances and our asymmetrical dissimilarities have comparable complexities. [sent-383, score-0.858]
</p><p>84 In order to compare our asymmetrical dissimilarities with a competitive baseline, we first optimized the choices involved in the baseline system for each dataset. [sent-403, score-0.745]
</p><p>85 In the experiment, we used a BoW baseline system without any re-ranking step, such as spatial re-ranking [17, 21] and query expansion [3], because we focus on improving the initial ranked accuracy, which is critical especially for difficult datasets. [sent-429, score-0.494]
</p><p>86 Most re-ranking algorithms, such as spatial verification [17, 21] or query expansion [3], require the short-list to be of sufficient quality to produce good results. [sent-430, score-0.417]
</p><p>87 In our experiments, we used the best configuration for each dataset and kept this choice consistent with our asymmetrical dissimilarities. [sent-445, score-0.536]
</p><p>88 Our dissimilarity consistently outperforms the symmetrical baseline: The improvement is of +5. [sent-454, score-0.341]
</p><p>89 88% on the Oxford105K, Oxford105K*, 11771100  tion 9: performance (vertical axis) of the δ1 asymmetrical dissimilarity. [sent-458, score-0.536]
</p><p>90 For the δ2 asymmetrical dissimilarity, we draw the same conclusions as above. [sent-467, score-0.536]
</p><p>91 However, as in the symmetrical case, the δ2 dissimilarity only slightly outperforms the corresponding δ1 on the INS2012 dataset (+1. [sent-468, score-0.341]
</p><p>92 Second, our asymmetrical method (Best δp) is consistently better than its symmetrical counterpart for the best choice (Best ? [sent-482, score-0.738]
</p><p>93 The scores of Best2 are reported for reference but are not directly comparable, as they generally include multiple  features, spatial verification or/and query expansion. [sent-488, score-0.374]
</p><p>94 Remark: For the sake ofcompleteness, the table also reports the best results (Best2) achieved by using, additionally, multiple features, spatial verification or other re-ranking schemes such as query expansion. [sent-509, score-0.405]
</p><p>95 Conclusions  This paper specifically addressed the asymmetrical phenomenon arising in an visual object retrieval scenario. [sent-516, score-0.72]
</p><p>96 This led us to propose new dissimilarities measures, adapted to the bag-of-words representation, that explicitly take into account this aspect to improve the retrieval quality. [sent-517, score-0.277]
</p><p>97 A key feature is to automatically adapt, per query, a parameter that reflects the different inlier ratios in the query and database images. [sent-520, score-0.642]
</p><p>98 Our dissimilarities come at not cost, as they are implemented with a vanilla inverted index like those used for symmetrical distances. [sent-521, score-0.396]
</p><p>99 To conclude, we believe that our method is fully compatible with the standard object retrieval architecture [2, 16], meaning that further refinements such as spatial re-ranking or query expansion can be seamlessly integrated with it. [sent-523, score-0.565]
</p><p>100 Total recall: Automatic query expansion with a generative feature model for object retrieval. [sent-556, score-0.431]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('asymmetrical', 0.536), ('query', 0.349), ('tjl', 0.286), ('tj', 0.247), ('qi', 0.225), ('qil', 0.215), ('symmetrical', 0.202), ('asymmetry', 0.145), ('dissimilarity', 0.139), ('qout', 0.125), ('dissimilarities', 0.12), ('inlier', 0.12), ('trecvid', 0.117), ('retrieval', 0.115), ('dout', 0.107), ('wopt', 0.107), ('database', 0.105), ('bow', 0.103), ('roi', 0.085), ('inverted', 0.074), ('min', 0.065), ('inliers', 0.062), ('inl', 0.059), ('baseline', 0.055), ('ranking', 0.054), ('qli', 0.054), ('ratios', 0.049), ('scoring', 0.049), ('equation', 0.048), ('ins', 0.046), ('quantities', 0.044), ('expansion', 0.043), ('nii', 0.041), ('impact', 0.039), ('outliers', 0.039), ('object', 0.039), ('egou', 0.038), ('appearing', 0.038), ('chum', 0.036), ('delimited', 0.036), ('delimiting', 0.036), ('delimits', 0.036), ('okapi', 0.036), ('trieval', 0.036), ('choices', 0.034), ('ratio', 0.033), ('idf', 0.033), ('robertson', 0.032), ('manning', 0.032), ('campaign', 0.032), ('schemes', 0.031), ('vocabulary', 0.03), ('clips', 0.03), ('quantization', 0.03), ('soft', 0.03), ('visual', 0.03), ('underpinning', 0.029), ('philbin', 0.029), ('file', 0.029), ('queries', 0.028), ('salton', 0.028), ('arandjelovi', 0.028), ('search', 0.027), ('benchmarks', 0.027), ('asymmetric', 0.027), ('clutter', 0.026), ('icmr', 0.026), ('perd', 0.026), ('datasets', 0.026), ('assignment', 0.026), ('isard', 0.025), ('verification', 0.025), ('och', 0.025), ('textual', 0.025), ('tokyo', 0.024), ('topic', 0.024), ('initial', 0.024), ('sivic', 0.024), ('extent', 0.024), ('swap', 0.024), ('correspond', 0.023), ('ranked', 0.023), ('ox', 0.022), ('adapted', 0.021), ('returned', 0.021), ('account', 0.021), ('adapts', 0.02), ('penalties', 0.02), ('appears', 0.02), ('instance', 0.019), ('compatible', 0.019), ('monotonically', 0.019), ('weighting', 0.019), ('zhu', 0.019), ('mostly', 0.019), ('expected', 0.019), ('parameter', 0.019), ('state', 0.018), ('vocabularies', 0.018), ('bounding', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="334-tfidf-1" href="./iccv-2013-Query-Adaptive_Asymmetrical_Dissimilarities_for_Visual_Object_Retrieval.html">334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</a></p>
<p>Author: Cai-Zhi Zhu, Hervé Jégou, Shin'Ichi Satoh</p><p>Abstract: Visual object retrieval aims at retrieving, from a collection of images, all those in which a given query object appears. It is inherently asymmetric: the query object is mostly included in the database image, while the converse is not necessarily true. However, existing approaches mostly compare the images with symmetrical measures, without considering the different roles of query and database. This paper first measure the extent of asymmetry on large-scale public datasets reflecting this task. Considering the standard bag-of-words representation, we then propose new asymmetrical dissimilarities accounting for the different inlier ratios associated with query and database images. These asymmetrical measures depend on the query, yet they are compatible with an inverted file structure, without noticeably impacting search efficiency. Our experiments show the benefit of our approach, and show that the visual object retrieval task is better treated asymmetrically, in the spirit of state-of-the-art text retrieval.</p><p>2 0.29957166 <a title="334-tfidf-2" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>Author: Basura Fernando, Tinne Tuytelaars</p><p>Abstract: In this paper we present a new method for object retrieval starting from multiple query images. The use of multiple queries allows for a more expressive formulation of the query object including, e.g., different viewpoints and/or viewing conditions. This, in turn, leads to more diverse and more accurate retrieval results. When no query images are available to the user, they can easily be retrieved from the internet using a standard image search engine. In particular, we propose a new method based on pattern mining. Using the minimal description length principle, we derive the most suitable set of patterns to describe the query object, with patterns corresponding to local feature configurations. This results in apowerful object-specific mid-level image representation. The archive can then be searched efficiently for similar images based on this representation, using a combination of two inverted file systems. Since the patterns already encode local spatial information, good results on several standard image retrieval datasets are obtained even without costly re-ranking based on geometric verification.</p><p>3 0.21455002 <a title="334-tfidf-3" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>Author: Xu Wang, Stefan Atev, John Wright, Gilad Lerman</p><p>Abstract: The problem of efficiently deciding which of a database of models is most similar to a given input query arises throughout modern computer vision. Motivated by applications in recognition, image retrieval and optimization, there has been significant recent interest in the variant of this problem in which the database models are linear subspaces and the input is either a point or a subspace. Current approaches to this problem have poor scaling in high dimensions, and may not guarantee sublinear query complexity. We present a new approach to approximate nearest subspace search, based on a simple, new locality sensitive hash for subspaces. Our approach allows point-tosubspace query for a database of subspaces of arbitrary dimension d, in a time that depends sublinearly on the number of subspaces in the database. The query complexity of our algorithm is linear in the ambient dimension D, allow- ing it to be directly applied to high-dimensional imagery data. Numerical experiments on model problems in image repatching and automatic face recognition confirm the advantages of our algorithm in terms of both speed and accuracy.</p><p>4 0.19508418 <a title="334-tfidf-4" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>Author: Dror Aiger, Efi Kokiopoulou, Ehud Rivlin</p><p>Abstract: We propose two solutions for both nearest neighbors and range search problems. For the nearest neighbors problem, we propose a c-approximate solutionfor the restricted version ofthe decisionproblem with bounded radius which is then reduced to the nearest neighbors by a known reduction. For range searching we propose a scheme that learns the parameters in a learning stage adopting them to the case of a set of points with low intrinsic dimension that are embedded in high dimensional space (common scenario for image point descriptors). We compare our algorithms to the best known methods for these problems, i.e. LSH, ANN and FLANN. We show analytically and experimentally that we can do better for moderate approximation factor. Our algorithms are trivial to parallelize. In the experiments conducted, running on couple of million im- ages, our algorithms show meaningful speed-ups when compared with the above mentioned methods.</p><p>5 0.1734778 <a title="334-tfidf-5" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>Author: Shiliang Zhang, Ming Yang, Xiaoyu Wang, Yuanqing Lin, Qi Tian</p><p>Abstract: Inverted indexes in image retrieval not only allow fast access to database images but also summarize all knowledge about the database, so that their discriminative capacity largely determines the retrieval performance. In this paper, for vocabulary tree based image retrieval, we propose a semantic-aware co-indexing algorithm to jointly San Antonio, TX 78249 . j dl@gmai l com qit ian@cs .ut sa . edu . The query embed two strong cues into the inverted indexes: 1) local invariant features that are robust to delineate low-level image contents, and 2) semantic attributes from large-scale object recognition that may reveal image semantic meanings. For an initial set of inverted indexes of local features, we utilize 1000 semantic attributes to filter out isolated images and insert semantically similar images to the initial set. Encoding these two distinct cues together effectively enhances the discriminative capability of inverted indexes. Such co-indexing operations are totally off-line and introduce small computation overhead to online query cause only local features but no semantic attributes are used for query. Experiments and comparisons with recent retrieval methods on 3 datasets, i.e., UKbench, Holidays, Oxford5K, and 1.3 million images from Flickr as distractors, manifest the competitive performance of our method 1.</p><p>6 0.15533218 <a title="334-tfidf-6" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<p>7 0.14960188 <a title="334-tfidf-7" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>8 0.14483285 <a title="334-tfidf-8" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>9 0.13529819 <a title="334-tfidf-9" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>10 0.12661545 <a title="334-tfidf-10" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>11 0.11006469 <a title="334-tfidf-11" href="./iccv-2013-EVSAC%3A_Accelerating_Hypotheses_Generation_by_Modeling_Matching_Scores_with_Extreme_Value_Theory.html">131 iccv-2013-EVSAC: Accelerating Hypotheses Generation by Modeling Matching Scores with Extreme Value Theory</a></p>
<p>12 0.10661311 <a title="334-tfidf-12" href="./iccv-2013-To_Aggregate_or_Not_to_aggregate%3A_Selective_Match_Kernels_for_Image_Search.html">419 iccv-2013-To Aggregate or Not to aggregate: Selective Match Kernels for Image Search</a></p>
<p>13 0.10236792 <a title="334-tfidf-13" href="./iccv-2013-Quantize_and_Conquer%3A_A_Dimensionality-Recursive_Solution_to_Clustering%2C_Vector_Quantization%2C_and_Image_Retrieval.html">333 iccv-2013-Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval</a></p>
<p>14 0.10142621 <a title="334-tfidf-14" href="./iccv-2013-3D_Sub-query_Expansion_for_Improving_Sketch-Based_Multi-view_Image_Retrieval.html">3 iccv-2013-3D Sub-query Expansion for Improving Sketch-Based Multi-view Image Retrieval</a></p>
<p>15 0.09881749 <a title="334-tfidf-15" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>16 0.090971105 <a title="334-tfidf-16" href="./iccv-2013-Fast_Neighborhood_Graph_Search_Using_Cartesian_Concatenation.html">159 iccv-2013-Fast Neighborhood Graph Search Using Cartesian Concatenation</a></p>
<p>17 0.086650454 <a title="334-tfidf-17" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<p>18 0.085283592 <a title="334-tfidf-18" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>19 0.076727673 <a title="334-tfidf-19" href="./iccv-2013-Learning_Coupled_Feature_Spaces_for_Cross-Modal_Matching.html">235 iccv-2013-Learning Coupled Feature Spaces for Cross-Modal Matching</a></p>
<p>20 0.073249824 <a title="334-tfidf-20" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.138), (1, 0.057), (2, -0.05), (3, -0.102), (4, 0.028), (5, 0.197), (6, 0.031), (7, -0.041), (8, -0.123), (9, 0.078), (10, 0.129), (11, -0.004), (12, 0.078), (13, 0.082), (14, 0.003), (15, -0.019), (16, 0.136), (17, -0.164), (18, 0.165), (19, -0.094), (20, -0.01), (21, -0.091), (22, -0.038), (23, -0.022), (24, -0.003), (25, 0.009), (26, 0.049), (27, -0.069), (28, 0.029), (29, -0.017), (30, -0.038), (31, 0.009), (32, 0.032), (33, -0.035), (34, 0.055), (35, -0.013), (36, -0.039), (37, 0.07), (38, -0.002), (39, 0.016), (40, 0.054), (41, 0.034), (42, -0.015), (43, 0.001), (44, 0.0), (45, 0.079), (46, 0.01), (47, -0.013), (48, 0.017), (49, -0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96919954 <a title="334-lsi-1" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>Author: Basura Fernando, Tinne Tuytelaars</p><p>Abstract: In this paper we present a new method for object retrieval starting from multiple query images. The use of multiple queries allows for a more expressive formulation of the query object including, e.g., different viewpoints and/or viewing conditions. This, in turn, leads to more diverse and more accurate retrieval results. When no query images are available to the user, they can easily be retrieved from the internet using a standard image search engine. In particular, we propose a new method based on pattern mining. Using the minimal description length principle, we derive the most suitable set of patterns to describe the query object, with patterns corresponding to local feature configurations. This results in apowerful object-specific mid-level image representation. The archive can then be searched efficiently for similar images based on this representation, using a combination of two inverted file systems. Since the patterns already encode local spatial information, good results on several standard image retrieval datasets are obtained even without costly re-ranking based on geometric verification.</p><p>same-paper 2 0.96369791 <a title="334-lsi-2" href="./iccv-2013-Query-Adaptive_Asymmetrical_Dissimilarities_for_Visual_Object_Retrieval.html">334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</a></p>
<p>Author: Cai-Zhi Zhu, Hervé Jégou, Shin'Ichi Satoh</p><p>Abstract: Visual object retrieval aims at retrieving, from a collection of images, all those in which a given query object appears. It is inherently asymmetric: the query object is mostly included in the database image, while the converse is not necessarily true. However, existing approaches mostly compare the images with symmetrical measures, without considering the different roles of query and database. This paper first measure the extent of asymmetry on large-scale public datasets reflecting this task. Considering the standard bag-of-words representation, we then propose new asymmetrical dissimilarities accounting for the different inlier ratios associated with query and database images. These asymmetrical measures depend on the query, yet they are compatible with an inverted file structure, without noticeably impacting search efficiency. Our experiments show the benefit of our approach, and show that the visual object retrieval task is better treated asymmetrically, in the spirit of state-of-the-art text retrieval.</p><p>3 0.8795591 <a title="334-lsi-3" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>Author: Dror Aiger, Efi Kokiopoulou, Ehud Rivlin</p><p>Abstract: We propose two solutions for both nearest neighbors and range search problems. For the nearest neighbors problem, we propose a c-approximate solutionfor the restricted version ofthe decisionproblem with bounded radius which is then reduced to the nearest neighbors by a known reduction. For range searching we propose a scheme that learns the parameters in a learning stage adopting them to the case of a set of points with low intrinsic dimension that are embedded in high dimensional space (common scenario for image point descriptors). We compare our algorithms to the best known methods for these problems, i.e. LSH, ANN and FLANN. We show analytically and experimentally that we can do better for moderate approximation factor. Our algorithms are trivial to parallelize. In the experiments conducted, running on couple of million im- ages, our algorithms show meaningful speed-ups when compared with the above mentioned methods.</p><p>4 0.84553099 <a title="334-lsi-4" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>Author: Xu Wang, Stefan Atev, John Wright, Gilad Lerman</p><p>Abstract: The problem of efficiently deciding which of a database of models is most similar to a given input query arises throughout modern computer vision. Motivated by applications in recognition, image retrieval and optimization, there has been significant recent interest in the variant of this problem in which the database models are linear subspaces and the input is either a point or a subspace. Current approaches to this problem have poor scaling in high dimensions, and may not guarantee sublinear query complexity. We present a new approach to approximate nearest subspace search, based on a simple, new locality sensitive hash for subspaces. Our approach allows point-tosubspace query for a database of subspaces of arbitrary dimension d, in a time that depends sublinearly on the number of subspaces in the database. The query complexity of our algorithm is linear in the ambient dimension D, allow- ing it to be directly applied to high-dimensional imagery data. Numerical experiments on model problems in image repatching and automatic face recognition confirm the advantages of our algorithm in terms of both speed and accuracy.</p><p>5 0.76926321 <a title="334-lsi-5" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>Author: Shiliang Zhang, Ming Yang, Xiaoyu Wang, Yuanqing Lin, Qi Tian</p><p>Abstract: Inverted indexes in image retrieval not only allow fast access to database images but also summarize all knowledge about the database, so that their discriminative capacity largely determines the retrieval performance. In this paper, for vocabulary tree based image retrieval, we propose a semantic-aware co-indexing algorithm to jointly San Antonio, TX 78249 . j dl@gmai l com qit ian@cs .ut sa . edu . The query embed two strong cues into the inverted indexes: 1) local invariant features that are robust to delineate low-level image contents, and 2) semantic attributes from large-scale object recognition that may reveal image semantic meanings. For an initial set of inverted indexes of local features, we utilize 1000 semantic attributes to filter out isolated images and insert semantically similar images to the initial set. Encoding these two distinct cues together effectively enhances the discriminative capability of inverted indexes. Such co-indexing operations are totally off-line and introduce small computation overhead to online query cause only local features but no semantic attributes are used for query. Experiments and comparisons with recent retrieval methods on 3 datasets, i.e., UKbench, Holidays, Oxford5K, and 1.3 million images from Flickr as distractors, manifest the competitive performance of our method 1.</p><p>6 0.76920605 <a title="334-lsi-6" href="./iccv-2013-Joint_Inverted_Indexing.html">221 iccv-2013-Joint Inverted Indexing</a></p>
<p>7 0.76081467 <a title="334-lsi-7" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>8 0.74519604 <a title="334-lsi-8" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>9 0.7337833 <a title="334-lsi-9" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<p>10 0.73231465 <a title="334-lsi-10" href="./iccv-2013-Fast_Neighborhood_Graph_Search_Using_Cartesian_Concatenation.html">159 iccv-2013-Fast Neighborhood Graph Search Using Cartesian Concatenation</a></p>
<p>11 0.73025757 <a title="334-lsi-11" href="./iccv-2013-To_Aggregate_or_Not_to_aggregate%3A_Selective_Match_Kernels_for_Image_Search.html">419 iccv-2013-To Aggregate or Not to aggregate: Selective Match Kernels for Image Search</a></p>
<p>12 0.69314706 <a title="334-lsi-12" href="./iccv-2013-3D_Sub-query_Expansion_for_Improving_Sketch-Based_Multi-view_Image_Retrieval.html">3 iccv-2013-3D Sub-query Expansion for Improving Sketch-Based Multi-view Image Retrieval</a></p>
<p>13 0.67920232 <a title="334-lsi-13" href="./iccv-2013-Quantize_and_Conquer%3A_A_Dimensionality-Recursive_Solution_to_Clustering%2C_Vector_Quantization%2C_and_Image_Retrieval.html">333 iccv-2013-Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval</a></p>
<p>14 0.64178675 <a title="334-lsi-14" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>15 0.55706513 <a title="334-lsi-15" href="./iccv-2013-Paper_Doll_Parsing%3A_Retrieving_Similar_Styles_to_Parse_Clothing_Items.html">306 iccv-2013-Paper Doll Parsing: Retrieving Similar Styles to Parse Clothing Items</a></p>
<p>16 0.54560179 <a title="334-lsi-16" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<p>17 0.52531481 <a title="334-lsi-17" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>18 0.5235104 <a title="334-lsi-18" href="./iccv-2013-EVSAC%3A_Accelerating_Hypotheses_Generation_by_Modeling_Matching_Scores_with_Extreme_Value_Theory.html">131 iccv-2013-EVSAC: Accelerating Hypotheses Generation by Modeling Matching Scores with Extreme Value Theory</a></p>
<p>19 0.51888865 <a title="334-lsi-19" href="./iccv-2013-SYM-FISH%3A_A_Symmetry-Aware_Flip_Invariant_Sketch_Histogram_Shape_Descriptor.html">368 iccv-2013-SYM-FISH: A Symmetry-Aware Flip Invariant Sketch Histogram Shape Descriptor</a></p>
<p>20 0.50867361 <a title="334-lsi-20" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.109), (7, 0.02), (12, 0.021), (13, 0.013), (26, 0.055), (27, 0.022), (30, 0.255), (31, 0.032), (42, 0.123), (64, 0.035), (73, 0.051), (89, 0.155), (98, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80178791 <a title="334-lda-1" href="./iccv-2013-Active_Visual_Recognition_with_Expertise_Estimation_in_Crowdsourcing.html">43 iccv-2013-Active Visual Recognition with Expertise Estimation in Crowdsourcing</a></p>
<p>Author: Chengjiang Long, Gang Hua, Ashish Kapoor</p><p>Abstract: We present a noise resilient probabilistic model for active learning of a Gaussian process classifier from crowds, i.e., a set of noisy labelers. It explicitly models both the overall label noises and the expertise level of each individual labeler in two levels of flip models. Expectation propagation is adopted for efficient approximate Bayesian inference of our probabilistic model for classification, based on which, a generalized EM algorithm is derived to estimate both the global label noise and the expertise of each individual labeler. The probabilistic nature of our model immediately allows the adoption of the prediction entropy and estimated expertise for active selection of data sample to be labeled, and active selection of high quality labelers to label the data, respectively. We apply the proposed model for three visual recognition tasks, i.e, object category recognition, gender recognition, and multi-modal activity recognition, on three datasets with real crowd-sourced labels from Amazon Mechanical Turk. The experiments clearly demonstrated the efficacy of the proposed model.</p><p>same-paper 2 0.77594912 <a title="334-lda-2" href="./iccv-2013-Query-Adaptive_Asymmetrical_Dissimilarities_for_Visual_Object_Retrieval.html">334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</a></p>
<p>Author: Cai-Zhi Zhu, Hervé Jégou, Shin'Ichi Satoh</p><p>Abstract: Visual object retrieval aims at retrieving, from a collection of images, all those in which a given query object appears. It is inherently asymmetric: the query object is mostly included in the database image, while the converse is not necessarily true. However, existing approaches mostly compare the images with symmetrical measures, without considering the different roles of query and database. This paper first measure the extent of asymmetry on large-scale public datasets reflecting this task. Considering the standard bag-of-words representation, we then propose new asymmetrical dissimilarities accounting for the different inlier ratios associated with query and database images. These asymmetrical measures depend on the query, yet they are compatible with an inverted file structure, without noticeably impacting search efficiency. Our experiments show the benefit of our approach, and show that the visual object retrieval task is better treated asymmetrically, in the spirit of state-of-the-art text retrieval.</p><p>3 0.75050169 <a title="334-lda-3" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>Author: Gang Hua, Chengjiang Long, Ming Yang, Yan Gao</p><p>Abstract: Active learning is an effective way of engaging users to interactively train models for visual recognition. The vast majority of previous works, if not all of them, focused on active learning with a single human oracle. The problem of active learning with multiple oracles in a collaborative setting has not been well explored. Moreover, most of the previous works assume that the labels provided by the human oracles are noise free, which may often be violated in reality. We present a collaborative computational model for active learning with multiple human oracles. It leads to not only an ensemble kernel machine that is robust to label noises, but also a principled label quality measure to online detect irresponsible labelers. Instead of running independent active learning processes for each individual human oracle, our model captures the inherent correlations among the labelers through shared data among them. Our simulation experiments and experiments with real crowd-sourced noisy labels demonstrated the efficacy of our model.</p><p>4 0.71668267 <a title="334-lda-4" href="./iccv-2013-Discriminative_Label_Propagation_for_Multi-object_Tracking_with_Sporadic_Appearance_Features.html">120 iccv-2013-Discriminative Label Propagation for Multi-object Tracking with Sporadic Appearance Features</a></p>
<p>Author: K.C. Amit Kumar, Christophe De_Vleeschouwer</p><p>Abstract: Given a set of plausible detections, detected at each time instant independently, we investigate how to associate them across time. This is done by propagating labels on a set of graphs that capture how the spatio-temporal and the appearance cues promote the assignment of identical or distinct labels to a pair of nodes. The graph construction is driven by the locally linear embedding (LLE) of either the spatio-temporal or the appearance features associated to the detections. Interestingly, the neighborhood of a node in each appearance graph is defined to include all nodes for which the appearance feature is available (except the ones that coexist at the same time). This allows to connect the nodes that share the same appearance even if they are temporally distant, which gives our framework the uncommon ability to exploit the appearance features that are available only sporadically along the sequence of detections. Once the graphs have been defined, the multi-object tracking is formulated as the problem of finding a label assignment that is consistent with the constraints captured by each of the graphs. This results into a difference of convex program that can be efficiently solved. Experiments are performed on a basketball and several well-known pedestrian datasets in order to validate the effectiveness of the proposed solution.</p><p>5 0.71558005 <a title="334-lda-5" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>Author: Basura Fernando, Tinne Tuytelaars</p><p>Abstract: In this paper we present a new method for object retrieval starting from multiple query images. The use of multiple queries allows for a more expressive formulation of the query object including, e.g., different viewpoints and/or viewing conditions. This, in turn, leads to more diverse and more accurate retrieval results. When no query images are available to the user, they can easily be retrieved from the internet using a standard image search engine. In particular, we propose a new method based on pattern mining. Using the minimal description length principle, we derive the most suitable set of patterns to describe the query object, with patterns corresponding to local feature configurations. This results in apowerful object-specific mid-level image representation. The archive can then be searched efficiently for similar images based on this representation, using a combination of two inverted file systems. Since the patterns already encode local spatial information, good results on several standard image retrieval datasets are obtained even without costly re-ranking based on geometric verification.</p><p>6 0.67428529 <a title="334-lda-6" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>7 0.67366111 <a title="334-lda-7" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>8 0.67042685 <a title="334-lda-8" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>9 0.66988939 <a title="334-lda-9" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>10 0.66963392 <a title="334-lda-10" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>11 0.6683166 <a title="334-lda-11" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>12 0.6681537 <a title="334-lda-12" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>13 0.66749007 <a title="334-lda-13" href="./iccv-2013-Learning_to_Rank_Using_Privileged_Information.html">248 iccv-2013-Learning to Rank Using Privileged Information</a></p>
<p>14 0.66697097 <a title="334-lda-14" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>15 0.66685897 <a title="334-lda-15" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>16 0.66668415 <a title="334-lda-16" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>17 0.66660964 <a title="334-lda-17" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>18 0.66657805 <a title="334-lda-18" href="./iccv-2013-Quadruplet-Wise_Image_Similarity_Learning.html">332 iccv-2013-Quadruplet-Wise Image Similarity Learning</a></p>
<p>19 0.66652846 <a title="334-lda-19" href="./iccv-2013-NEIL%3A_Extracting_Visual_Knowledge_from_Web_Data.html">285 iccv-2013-NEIL: Extracting Visual Knowledge from Web Data</a></p>
<p>20 0.66633487 <a title="334-lda-20" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
