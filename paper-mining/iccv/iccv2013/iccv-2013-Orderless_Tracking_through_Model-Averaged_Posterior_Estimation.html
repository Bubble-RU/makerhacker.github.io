<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>303 iccv-2013-Orderless Tracking through Model-Averaged Posterior Estimation</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-303" href="#">iccv2013-303</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>303 iccv-2013-Orderless Tracking through Model-Averaged Posterior Estimation</h1>
<br/><p>Source: <a title="iccv-2013-303-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Hong_Orderless_Tracking_through_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Seunghoon Hong, Suha Kwak, Bohyung Han</p><p>Abstract: We propose a novel offline tracking algorithm based on model-averaged posterior estimation through patch matching across frames. Contrary to existing online and offline tracking methods, our algorithm is not based on temporallyordered estimates of target state but attempts to select easyto-track frames first out of the remaining ones without exploiting temporal coherency of target. The posterior of the selected frame is estimated by propagating densities from the already tracked frames in a recursive manner. The density propagation across frames is implemented by an efficient patch matching technique, which is useful for our algorithm since it does not require motion smoothness assumption. Also, we present a hierarchical approach, where a small set of key frames are tracked first and non-key frames are handled by local key frames. Our tracking algorithm is conceptually well-suited for the sequences with abrupt motion, shot changes, and occlusion. We compare our tracking algorithm with existing techniques in real videos with such challenges and illustrate its superior performance qualitatively and quantitatively.</p><p>Reference: <a title="iccv-2013-303-reference" href="../iccv2013_reference/iccv-2013-Orderless_Tracking_through_Model-Averaged_Posterior_Estimation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 kr  Abstract We propose a novel offline tracking algorithm based on model-averaged posterior estimation through patch matching across frames. [sent-3, score-0.57]
</p><p>2 Contrary to existing online and offline tracking methods, our algorithm is not based on temporallyordered estimates of target state but attempts to select easyto-track frames first out of the remaining ones without exploiting temporal coherency of target. [sent-4, score-0.84]
</p><p>3 The posterior of the selected frame is estimated by propagating densities from the already tracked frames in a recursive manner. [sent-5, score-0.719]
</p><p>4 The density propagation across frames is implemented by an efficient patch matching technique, which is useful for our algorithm since it does not require motion smoothness assumption. [sent-6, score-0.484]
</p><p>5 Also, we present a hierarchical approach, where a small set of key frames are tracked first and non-key frames are handled by local key frames. [sent-7, score-0.597]
</p><p>6 Our tracking algorithm is conceptually well-suited for the sequences with abrupt motion, shot changes, and occlusion. [sent-8, score-0.492]
</p><p>7 We compare our tracking algorithm with existing techniques in real videos with such challenges and illustrate its superior performance qualitatively and quantitatively. [sent-9, score-0.239]
</p><p>8 Offline tracker is a reasonable option for tracking objects in such videos since more robust tracking results can be obtained by utilizing observations from the multiple frames regardless of their temporal order. [sent-12, score-0.695]
</p><p>9 However, most of existing online (and even most of offline) tracking algorithms are limited to processing frames in a temporal order. [sent-13, score-0.501]
</p><p>10 Note that tracking algorithms often fail eventually because a few intermediate frames are extremely challenging due to fast motion, shot changes, occlusion, shadow, and temporary appearance changes. [sent-14, score-0.492]
</p><p>11 While these methods are successful in handling various appearance changes, they all assume temporal smoothness of target motion; they often fail  to track objects in the presence of sudden changes in target and scene. [sent-19, score-0.396]
</p><p>12 It successfully tracks objects even with sudden changes of target location including shot changes in an online manner. [sent-21, score-0.336]
</p><p>13 However, it may not be able to recover from temporal failures as other online tracking algorithms, and may not be sufficiently robust to other kinds of challenges such as occlusion, background clutter, and appearance changes. [sent-22, score-0.322]
</p><p>14 Offline tracking [4, 6, 20, 22, 23] is an alternative option to handle abrupt motion, occlusion, and shot changes robustly since it can utilize entire frames within video at once. [sent-23, score-0.685]
</p><p>15 [22] formulates offline tracking as a global optimization problem, and solves it by dynamic programming efficiently. [sent-25, score-0.316]
</p><p>16 Note that dynamic programming estimates target state at each frame recursively, and still follows a predefined order, typically temporal order, of a sequence; the benefit of offline tracking is limited in practice. [sent-27, score-0.648]
</p><p>17 On the other hand, [20] proposes a bi-directional tracking algorithm, where a full trajectory of target is obtained by connecting a set of short trajectories with occlusion handling through the optimization with a discrete Hidden Markov Model (HMM). [sent-28, score-0.328]
</p><p>18 We propose an offline tracking algorithm based on model-averaged posterior estimation. [sent-30, score-0.512]
</p><p>19 esMainframRewmaoinrkgfroamfeosuralgorithRme ain gframes estimates its posterior sequentially by a variant of sequential Bayesian filtering. [sent-32, score-0.306]
</p><p>20 The posterior is represented with a  mixture model—mixture of potential tracking orders—and estimated by a weighted sum of multiple posteriors corresponding to the models. [sent-33, score-0.424]
</p><p>21 The observation in each frame is performed by a patch matching through hashing, which is appropriate for computing likelihoods without temporal smoothness assumption. [sent-34, score-0.289]
</p><p>22 Additionally, we present a hierarchical key frame based tracking algorithm, which exploits the temporal unorderedness of our algorithm and reduces computational cost significantly. [sent-35, score-0.471]
</p><p>23 The characteristics and benefits ofour tracking algorithm are summarized below: •  •  Our tracking algorithm does not have any temporal smoothness assumption, and is conceptually more robust to abrupt motion, occlusion, and shot changes of target than existing techniques. [sent-37, score-0.949]
</p><p>24 A hierarchical tracking approach is proposed for further efficiency, where a small number of key frames are tracked first and non-key frames are handled by nearby key frames. [sent-39, score-0.794]
</p><p>25 Suppose that frame t4 is selected for tracking in the 4th time step. [sent-167, score-0.34]
</p><p>26 (b) Tracking result of the frame t4 is determined by average estimate of the four chain models. [sent-169, score-0.224]
</p><p>27 Our objective is to estimate the posterior density functions P(xi) for all frames in a sequential but non-temporal order one-by-one in a greedy manner, where the next frame is selected for tracking based on the uncertainty score of each P(xi). [sent-172, score-0.936]
</p><p>28 , tk} is a set  of tracked frames sorted in the tracked order, and Rk = F \ Tk = {r1, . [sent-180, score-0.439]
</p><p>29 At the time step k + 1, where k frames are tracked, our tracking algorithm performs the following procedure: 1. [sent-184, score-0.418]
</p><p>30 Given target posterior density functions of all tracked frames P(xt) , ∀t ∈ Tk, it estimates the intermediate posterior density functions of all remaining frames P(xi) , ∀i ∈ Rk, recursively. [sent-185, score-1.257]
</p><p>31 Note that the new element tk+1 in Tk+1 also updates the posterior densities of remaining frames from the next iteration. [sent-189, score-0.462]
</p><p>32 Once a frame is inserted into the tracked list Tk, corresponding target posterior density would not change any more. [sent-190, score-0.659]
</p><p>33 Probabilistic Framework of Our Algorithm We present the main probabilistic framework ofour algorithm; we describe our posterior estimation technique called model-averaged posterior estimation and discuss how it is related to patch-based observation technique. [sent-193, score-0.442]
</p><p>34 Model-Averaged Posterior Estimation We first describe how target posterior density functions  are estimated for the remaining frames given k − 1 tracked 22229977  frames and their corresponding posterior density functions. [sent-196, score-1.239]
</p><p>35 Since our algorithm is not limited to temporally ordered estimation of posterior density functions, we employ a novel Bayesian formulation to handle a tracking scenario with an arbitrary sequence of frames. [sent-197, score-0.593]
</p><p>36 If the temporal order of frames is ignored, there are a number of possible ways to reach the kth tracking frame, tk, from the tracked k − 1 frames as illustrated in Figure 2(a). [sent-198, score-0.835]
</p><p>37 We can take any sequence generated by any subset of frames in Tk−1 1 as intermediate hops. [sent-199, score-0.223]
</p><p>38 Note that each potential path from t1 to tk is modeled by the first order Markov chain. [sent-200, score-0.592]
</p><p>39 So, tracking by a single model is risky especially when there are critical challenges in the model such as abrupt motion and shot changes. [sent-203, score-0.475]
</p><p>40 That is, tracking result of a frame is determined by average estimate of all possible chain models instead ofchoosing one, as illustrated in Figure 2(b). [sent-205, score-0.421]
</p><p>41 Let tk be the frame index selected for tracking in the kth time step. [sent-206, score-0.938]
</p><p>42 In principle, to estimate the posterior of xtk , we need to enumerate all chain models whose last nodes are tk  and calculate the average of their posteriors. [sent-207, score-1.372]
</p><p>43 We can estimate the posterior of xtk in a simple and recursive fashion, which is formally given by  P? [sent-208, score-0.731]
</p><p>44 (xt)dxt, (1) where t denotes a frame tracked before the kth time step, and Ztk is an observation variable of frame tk. [sent-212, score-0.407]
</p><p>45 Note that all chain models arriving at frame t are already averaged when calculating P? [sent-217, score-0.224]
</p><p>46 Once frame tk is selected and tracked, ) will no? [sent-219, score-0.703]
</p><p>47 By Bayesian model averaging strategy, the posterior of xt is given by  P? [sent-226, score-0.303]
</p><p>48 set,  only  one sequence  is generated from each  where ZSt is an observation variable corresponding to the frames in St, and ΩSt denotes a candidate chain model given by St. [sent-234, score-0.322]
</p><p>49 ,  (3)  where pt→tk denotes the path from the last node in ΩSt to tk and ? [sent-238, score-0.592]
</p><p>50 (xt)dxt, ∝  (7)  where the posterior density function for the kth tracking frame is now defined recursively. [sent-294, score-0.662]
</p><p>51 The prior of the path, P(pt→tk ), represents which path would be preferred to tracking frame tk, and is simply given by P(pt→tk)  ∝  k −1 1,  t ∈ Tk−1. [sent-295, score-0.354]
</p><p>52 (1) corresponds to density propagation process; given the target density at frame t ∈ Tk−1, we want to estimate ) through prediction and update steps, P(xtk |xt) and P? [sent-303, score-0.462]
</p><p>53 For a set of patches within target window corresponding to each sample xti in It, we obtain a voting result with respect to Itk as ? [sent-317, score-0.204]
</p><p>54 j=1  ctj  where is the center position of the jth patch within the target bounding box centered at xti, Kti is the number of patches within the bounding box, and atj is the offset from  ctj to xti. [sent-320, score-0.364]
</p><p>55 Since it relies only on patch matching and voting between frames, a density propagated from a single frame may cause drift problem. [sent-329, score-0.327]
</p><p>56 CSH searches entire image area with very low computational cost, and it is natural to effectively handle abrupt motion, shot changes, and occlusion of target without temporal smoothness assumption. [sent-334, score-0.479]
</p><p>57 Our patch-based voting algorithm is robust to local changes of target appearance such as partial occlusion and moderate non-rigid transformation. [sent-336, score-0.235]
</p><p>58 (11), matching between a pair of frames are to be computed  at most once throughout tracking. [sent-339, score-0.221]
</p><p>59 Note that the choice of density propagation technique in this work is orthogonal to our model-averaged posterior estimation framework for offline tracking. [sent-340, score-0.458]
</p><p>60 We now describe how to determine the next frame tk out of Rk−1 based on the uncertainty analysis for the rest of frames, which corresponds to the second and third steps of Section 2. [sent-345, score-0.723]
</p><p>61 Tracking result in a frame is likely to be reliable if its posterior density function has a clear mode, and we measure the uncertainty using entropy. [sent-346, score-0.465]
</p><p>62 For each frame r ∈ Rk−1, we compute the marginalized posterior probability of each block, which is given by P(Brm) =  ? [sent-351, score-0.322]
</p><p>63 We compute the entropy for every frame in Rk−1 and choose the frame with minimum entropy as tk = arg mrin H(xr), r ∈ Rk−1 . [sent-369, score-0.866]
</p><p>64 (14) By the above criterion, our algorithm tends to select easyto-track frames first regardless of their temporal order, and it helps to prevent the entire track from being corrupted by few tracking failures. [sent-370, score-0.549]
</p><p>65 After frame selection, we update the sets for tracked and remaining frames by Tk = Tk−1 ∪ {tk} and Rk = Rk−1 \ {tk}. [sent-371, score-0.468]
</p><p>66 Once a frame is inserted into Tk, its posterior does not change during remaining iterations any more. [sent-372, score-0.367]
</p><p>67 , our techniques does not rely on any temporal or spatial coherency of target and it is  ? [sent-381, score-0.24]
</p><p>68 ×  reasonable to track a subset of frames first and estimate the posteriors of the rest of frames based only on the tracked frames. [sent-383, score-0.577]
</p><p>69 Key Frame Selection Key frames should capture important characteristics of entire video, especially in case that the video contains a lot of variations inside such as shot changes, fast motion, and occlusion. [sent-388, score-0.295]
</p><p>70 We employ a similar idea in [8] to identify key frames from an input video. [sent-389, score-0.227]
</p><p>71 Our key frame selection technique first embeds all frames in a metric space, and selects a subset of frames by solving a metric facility location problem. [sent-390, score-0.575]
</p><p>72 Given the dissimilarity matrix D, all frames can be em-  bedded in a metric space by a non-linear manifold learning technique, and we employ Isomap [21] algorithm. [sent-395, score-0.201]
</p><p>73 Note that original dissimilarities (distances) between frames are preserved maximally through the manifold embedding; if the distance between two frames is small, they are likely to be located in a neighborhood. [sent-396, score-0.424]
</p><p>74 We find a subset of frames K ⊆ F, where |K| = κ, based on the following objective function:  K∗ = argK m⊆inFmv∈aFxum∈iKndE(u,v)  (17)  2Sequence partitioning is another idea to make our algorithm much faster, but we focus on this hierarchical approach in this paper. [sent-399, score-0.245]
</p><p>75 where F denotes the entire set of frames and dE is the distance in the embedded space. [sent-400, score-0.221]
</p><p>76 The selected frames by solving κ-center problem serve as anchor frames to the rest of frames in the local area in the embedded space. [sent-401, score-0.641]
</p><p>77 Density Propagation to Non-Key Frames After selecting key frames by the method described in Section 4. [sent-406, score-0.227]
</p><p>78 1, we perform the inference for the posterior of the key frames based on the procedure presented in Section 3. [sent-407, score-0.424]
</p><p>79 To propagate the density functions estimated in the  key frames to the non-key frames, we exploit the embedding result as described below. [sent-408, score-0.332]
</p><p>80 For each frame u ∈ F \ K, compute the posterior of each frame by a single hop density propagation, which is given by  P? [sent-416, score-0.552]
</p><p>81 ⊆K In step 2, we discard the key frames with negligible weights by setting their weights to 0, and re-normalize weights. [sent-423, score-0.227]
</p><p>82 ames to estimate the posterior for each frame in F \ K). [sent-430, score-0.322]
</p><p>83 All the sequences involve at least one critical challenges; animal has fast motion and motion blur, tennis is with abrupt location changes and pose variations, TUD, campus, accident contain severe occlusions, and the others involve shot changes and pose variations. [sent-436, score-0.529]
</p><p>84 We present the subsets of target windows by their tracked orders. [sent-440, score-0.21]
</p><p>85 Note that tracked order is not consistent with temporal order; we can observe that the proposed algorithm tends to track easy frames first. [sent-442, score-0.471]
</p><p>86 In each time step k, our tracker analyzes all the frames in Rk−1 to decide next  move; it measures uncertainty of every frame, and add the most confident one into Tk. [sent-446, score-0.26]
</p><p>87 It tends to choose frames in an increasing order of difficulty, as illustrated in Figure 3, which may not be same with temporal order. [sent-447, score-0.306]
</p><p>88 Note that, in Figure 3(a) and 3(c), visually similar frames to the initial frame have been selected at the beginning even with very different temporal locations. [sent-448, score-0.423]
</p><p>89 Quantitative and Qualitative Performance We compared our algorithm with the state-of-the-art tracking methods, which include L1 [17], L1-APG [3], SCM [25], ASLSA [10], MTT [24], MIL [2], IVT [18], FRAG [1], WLMC [14], and OTLE [6]. [sent-452, score-0.217]
</p><p>90 Examples of tracking failure are online trackers except OTLE, and WLMC is a specialized technique to handle abrupt motion of target; these two methods are more related to the proposed algorithm. [sent-454, score-0.428]
</p><p>91 It is probably because the algorithm is specialized for the sudden location changes of target but is not good enough to handle other variations such as occlusion in TUD and campus and background clutter in animal. [sent-464, score-0.303]
</p><p>92 The offline tracking algorithm, OTLE, is generally worse than ours. [sent-465, score-0.295]
</p><p>93 Other trackers have significant troubles to handle shot changes and abrupt motion of target. [sent-466, score-0.324]
</p><p>94 Some examples of tracking failure in psy are presented in Figure 5. [sent-468, score-0.27]
</p><p>95 Although our algorithm fails in some frames due to severe deformation or lighting changes, error propagation to other frames is minor since our algorithm tends to  postpone processing the failed frames and their influence is curbed by the model-averaged posterior estimation. [sent-469, score-0.902]
</p><p>96 Conclusion We presented a novel offline tracking algorithm based on model-averaged density estimation, where the posterior of a newly selected frame for tracking is estimated by a weighted mixture model. [sent-471, score-0.957]
</p><p>97 Our tracking algorithm is free from temporal smooth-  ness assumption, and tends to choose easy-to-track frames first and challenging frames last. [sent-480, score-0.724]
</p><p>98 So, it is conceptually  ro-  bust to various challenges such as abrupt motion, occlusion, and shot changes. [sent-481, score-0.269]
</p><p>99 To handle observations  out temporal coherency  efficiently with-  across frames, a patch matching  technique by hashing is employed. [sent-482, score-0.278]
</p><p>100 We evaluated the per-  formance of our algorithm qualitatively  and quantitatively,  and compared with the state-of-the-art tracking algorithms. [sent-483, score-0.217]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tk', 0.56), ('xtk', 0.516), ('frames', 0.201), ('posterior', 0.197), ('tracking', 0.197), ('ztk', 0.146), ('frame', 0.125), ('abrupt', 0.125), ('tracked', 0.119), ('xt', 0.106), ('density', 0.105), ('chain', 0.099), ('offline', 0.098), ('rk', 0.094), ('shot', 0.094), ('target', 0.091), ('temporal', 0.079), ('ctj', 0.073), ('psy', 0.073), ('coherency', 0.07), ('pt', 0.069), ('st', 0.067), ('fpm', 0.065), ('atj', 0.065), ('tud', 0.064), ('xr', 0.058), ('otle', 0.055), ('wlmc', 0.055), ('sequential', 0.055), ('xti', 0.05), ('accident', 0.048), ('campus', 0.047), ('changes', 0.045), ('boxing', 0.045), ('dxt', 0.045), ('tennis', 0.044), ('csh', 0.042), ('densities', 0.041), ('occlusion', 0.04), ('orderless', 0.04), ('voting', 0.039), ('uncertainty', 0.038), ('kth', 0.038), ('patch', 0.038), ('sudden', 0.037), ('motion', 0.037), ('brm', 0.036), ('gframes', 0.036), ('kti', 0.036), ('youngki', 0.036), ('zst', 0.036), ('propagation', 0.036), ('recursively', 0.033), ('path', 0.032), ('itk', 0.032), ('bayesian', 0.03), ('posteriors', 0.03), ('ordered', 0.029), ('uchida', 0.028), ('skating', 0.028), ('entropy', 0.028), ('sequences', 0.028), ('conceptually', 0.028), ('marginalize', 0.027), ('smoothness', 0.027), ('han', 0.027), ('animal', 0.026), ('key', 0.026), ('track', 0.026), ('tends', 0.026), ('ofour', 0.026), ('hashing', 0.026), ('pv', 0.025), ('bidirectional', 0.025), ('online', 0.024), ('kwak', 0.024), ('patches', 0.024), ('hierarchical', 0.024), ('remaining', 0.023), ('handle', 0.023), ('technique', 0.022), ('dissimilarities', 0.022), ('sequence', 0.022), ('challenges', 0.022), ('inserted', 0.022), ('korea', 0.022), ('tracker', 0.021), ('dance', 0.021), ('programming', 0.021), ('matching', 0.02), ('algorithm', 0.02), ('kwon', 0.02), ('embedded', 0.02), ('monte', 0.019), ('state', 0.019), ('selected', 0.018), ('markov', 0.018), ('estimates', 0.018), ('recursive', 0.018), ('carlo', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="303-tfidf-1" href="./iccv-2013-Orderless_Tracking_through_Model-Averaged_Posterior_Estimation.html">303 iccv-2013-Orderless Tracking through Model-Averaged Posterior Estimation</a></p>
<p>Author: Seunghoon Hong, Suha Kwak, Bohyung Han</p><p>Abstract: We propose a novel offline tracking algorithm based on model-averaged posterior estimation through patch matching across frames. Contrary to existing online and offline tracking methods, our algorithm is not based on temporallyordered estimates of target state but attempts to select easyto-track frames first out of the remaining ones without exploiting temporal coherency of target. The posterior of the selected frame is estimated by propagating densities from the already tracked frames in a recursive manner. The density propagation across frames is implemented by an efficient patch matching technique, which is useful for our algorithm since it does not require motion smoothness assumption. Also, we present a hierarchical approach, where a small set of key frames are tracked first and non-key frames are handled by local key frames. Our tracking algorithm is conceptually well-suited for the sequences with abrupt motion, shot changes, and occlusion. We compare our tracking algorithm with existing techniques in real videos with such challenges and illustrate its superior performance qualitatively and quantitatively.</p><p>2 0.14498839 <a title="303-tfidf-2" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>Author: Junliang Xing, Jin Gao, Bing Li, Weiming Hu, Shuicheng Yan</p><p>Abstract: Recently, sparse representation has been introduced for robust object tracking. By representing the object sparsely, i.e., using only a few templates via ?1-norm minimization, these so-called ?1-trackers exhibit promising tracking results. In this work, we address the object template building and updating problem in these ?1-tracking approaches, which has not been fully studied. We propose to perform template updating, in a new perspective, as an online incremental dictionary learning problem, which is efficiently solved through an online optimization procedure. To guarantee the robustness and adaptability of the tracking algorithm, we also propose to build a multi-lifespan dictionary model. By building target dictionaries of different lifespans, effective object observations can be obtained to deal with the well-known drifting problem in tracking and thus improve the tracking accuracy. We derive effective observa- tion models both generatively and discriminatively based on the online multi-lifespan dictionary learning model and deploy them to the Bayesian sequential estimation framework to perform tracking. The proposed approach has been extensively evaluated on ten challenging video sequences. Experimental results demonstrate the effectiveness of the online learned templates, as well as the state-of-the-art tracking performance of the proposed approach.</p><p>3 0.14269298 <a title="303-tfidf-3" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>Author: Aleksandr V. Segal, Ian Reid</p><p>Abstract: We propose a novel parametrization of the data association problem for multi-target tracking. In our formulation, the number of targets is implicitly inferred together with the data association, effectively solving data association and model selection as a single inference problem. The novel formulation allows us to interpret data association and tracking as a single Switching Linear Dynamical System (SLDS). We compute an approximate posterior solution to this problem using a dynamic programming/message passing technique. This inference-based approach allows us to incorporate richer probabilistic models into the tracking system. In particular, we incorporate inference over inliers/outliers and track termination times into the system. We evaluate our approach on publicly available datasets and demonstrate results competitive with, and in some cases exceeding the state of the art.</p><p>4 0.13967629 <a title="303-tfidf-4" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>Author: Stefan Duffner, Christophe Garcia</p><p>Abstract: In this paper, we present a novel algorithm for fast tracking of generic objects in videos. The algorithm uses two components: a detector that makes use of the generalised Hough transform with pixel-based descriptors, and a probabilistic segmentation method based on global models for foreground and background. These components are used for tracking in a combined way, and they adapt each other in a co-training manner. Through effective model adaptation and segmentation, the algorithm is able to track objects that undergo rigid and non-rigid deformations and considerable shape and appearance variations. The proposed tracking method has been thoroughly evaluated on challenging standard videos, and outperforms state-of-theart tracking methods designed for the same task. Finally, the proposed models allow for an extremely efficient implementation, and thus tracking is very fast.</p><p>5 0.13143498 <a title="303-tfidf-5" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>Author: Shuran Song, Jianxiong Xiao</p><p>Abstract: Despite significant progress, tracking is still considered to be a very challenging task. Recently, the increasing popularity of depth sensors has made it possible to obtain reliable depth easily. This may be a game changer for tracking, since depth can be used to prevent model drift and handle occlusion. We also observe that current tracking algorithms are mostly evaluated on a very small number of videos collectedandannotated by different groups. The lack of a reasonable size and consistently constructed benchmark has prevented a persuasive comparison among different algorithms. In this paper, we construct a unified benchmark dataset of 100 RGBD videos with high diversity, propose different kinds of RGBD tracking algorithms using 2D or 3D model, and present a quantitative comparison of various algorithms with RGB or RGBD input. We aim to lay the foundation for further research in both RGB and RGBD tracking, and our benchmark is available at http://tracking.cs.princeton.edu.</p><p>6 0.13026261 <a title="303-tfidf-6" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>7 0.11569139 <a title="303-tfidf-7" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>8 0.112985 <a title="303-tfidf-8" href="./iccv-2013-Latent_Space_Sparse_Subspace_Clustering.html">232 iccv-2013-Latent Space Sparse Subspace Clustering</a></p>
<p>9 0.11235911 <a title="303-tfidf-9" href="./iccv-2013-Online_Robust_Non-negative_Dictionary_Learning_for_Visual_Tracking.html">298 iccv-2013-Online Robust Non-negative Dictionary Learning for Visual Tracking</a></p>
<p>10 0.11001247 <a title="303-tfidf-10" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>11 0.10952643 <a title="303-tfidf-11" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>12 0.10789128 <a title="303-tfidf-12" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>13 0.10735475 <a title="303-tfidf-13" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>14 0.10095941 <a title="303-tfidf-14" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>15 0.097735122 <a title="303-tfidf-15" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>16 0.08988782 <a title="303-tfidf-16" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>17 0.088538587 <a title="303-tfidf-17" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<p>18 0.087762795 <a title="303-tfidf-18" href="./iccv-2013-Finding_the_Best_from_the_Second_Bests_-_Inhibiting_Subjective_Bias_in_Evaluation_of_Visual_Tracking_Algorithms.html">168 iccv-2013-Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms</a></p>
<p>19 0.086581685 <a title="303-tfidf-19" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>20 0.083171859 <a title="303-tfidf-20" href="./iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects.html">320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.165), (1, -0.041), (2, 0.023), (3, 0.071), (4, 0.004), (5, -0.005), (6, -0.068), (7, 0.131), (8, -0.031), (9, 0.155), (10, -0.073), (11, -0.114), (12, 0.036), (13, 0.063), (14, 0.025), (15, -0.034), (16, 0.005), (17, -0.017), (18, -0.056), (19, -0.031), (20, 0.007), (21, 0.001), (22, 0.013), (23, -0.035), (24, -0.028), (25, -0.006), (26, -0.01), (27, -0.027), (28, -0.013), (29, -0.007), (30, -0.002), (31, -0.004), (32, 0.005), (33, -0.02), (34, -0.003), (35, 0.027), (36, 0.049), (37, -0.03), (38, 0.042), (39, -0.008), (40, -0.005), (41, 0.043), (42, 0.007), (43, -0.04), (44, -0.023), (45, -0.046), (46, 0.039), (47, 0.053), (48, 0.088), (49, -0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97669995 <a title="303-lsi-1" href="./iccv-2013-Orderless_Tracking_through_Model-Averaged_Posterior_Estimation.html">303 iccv-2013-Orderless Tracking through Model-Averaged Posterior Estimation</a></p>
<p>Author: Seunghoon Hong, Suha Kwak, Bohyung Han</p><p>Abstract: We propose a novel offline tracking algorithm based on model-averaged posterior estimation through patch matching across frames. Contrary to existing online and offline tracking methods, our algorithm is not based on temporallyordered estimates of target state but attempts to select easyto-track frames first out of the remaining ones without exploiting temporal coherency of target. The posterior of the selected frame is estimated by propagating densities from the already tracked frames in a recursive manner. The density propagation across frames is implemented by an efficient patch matching technique, which is useful for our algorithm since it does not require motion smoothness assumption. Also, we present a hierarchical approach, where a small set of key frames are tracked first and non-key frames are handled by local key frames. Our tracking algorithm is conceptually well-suited for the sequences with abrupt motion, shot changes, and occlusion. We compare our tracking algorithm with existing techniques in real videos with such challenges and illustrate its superior performance qualitatively and quantitatively.</p><p>2 0.79839075 <a title="303-lsi-2" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>Author: Aleksandr V. Segal, Ian Reid</p><p>Abstract: We propose a novel parametrization of the data association problem for multi-target tracking. In our formulation, the number of targets is implicitly inferred together with the data association, effectively solving data association and model selection as a single inference problem. The novel formulation allows us to interpret data association and tracking as a single Switching Linear Dynamical System (SLDS). We compute an approximate posterior solution to this problem using a dynamic programming/message passing technique. This inference-based approach allows us to incorporate richer probabilistic models into the tracking system. In particular, we incorporate inference over inliers/outliers and track termination times into the system. We evaluate our approach on publicly available datasets and demonstrate results competitive with, and in some cases exceeding the state of the art.</p><p>3 0.75368178 <a title="303-lsi-3" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>Author: Ernesto Brau, Jinyan Guan, Kyle Simek, Luca Del Pero, Colin Reimer Dawson, Kobus Barnard</p><p>Abstract: Jinyan Guan† j guan1 @ emai l ari z ona . edu . Kyle Simek† ks imek@ emai l ari z ona . edu . Colin Reimer Dawson‡ cdaws on@ emai l ari z ona . edu . ‡School of Information University of Arizona Kobus Barnard‡ kobus @ s i sta . ari z ona . edu ∗School of Informatics University of Edinburgh for tracking an unknown and changing number of people in a scene using video taken from a single, fixed viewpoint. We develop a Bayesian modeling approach for tracking people in 3D from monocular video with unknown cameras. Modeling in 3D provides natural explanations for occlusions and smoothness discontinuities that result from projection, and allows priors on velocity and smoothness to be grounded in physical quantities: meters and seconds vs. pixels and frames. We pose the problem in the context of data association, in which observations are assigned to tracks. A correct application of Bayesian inference to multitarget tracking must address the fact that the model’s dimension changes as tracks are added or removed, and thus, posterior densities of different hypotheses are not comparable. We address this by marginalizing out the trajectory parameters so the resulting posterior over data associations has constant dimension. This is made tractable by using (a) Gaussian process priors for smooth trajectories and (b) approximately Gaussian likelihood functions. Our approach provides a principled method for incorporating multiple sources of evidence; we present results using both optical flow and object detector outputs. Results are comparable to recent work on 3D tracking and, unlike others, our method requires no pre-calibrated cameras.</p><p>4 0.74339527 <a title="303-lsi-4" href="./iccv-2013-Finding_the_Best_from_the_Second_Bests_-_Inhibiting_Subjective_Bias_in_Evaluation_of_Visual_Tracking_Algorithms.html">168 iccv-2013-Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms</a></p>
<p>Author: Yu Pang, Haibin Ling</p><p>Abstract: Evaluating visual tracking algorithms, or “trackers ” for short, is of great importance in computer vision. However, it is hard to “fairly” compare trackers due to many parameters need to be tuned in the experimental configurations. On the other hand, when introducing a new tracker, a recent trend is to validate it by comparing it with several existing ones. Such an evaluation may have subjective biases towards the new tracker which typically performs the best. This is mainly due to the difficulty to optimally tune all its competitors and sometimes the selected testing sequences. By contrast, little subjective bias exists towards the “second best” ones1 in the contest. This observation inspires us with a novel perspective towards inhibiting subjective bias in evaluating trackers by analyzing the results between the second bests. In particular, we first collect all tracking papers published in major computer vision venues in recent years. From these papers, after filtering out potential biases in various aspects, we create a dataset containing many records of comparison results between various visual trackers. Using these records, we derive performance rank- ings of the involved trackers by four different methods. The first two methods model the dataset as a graph and then derive the rankings over the graph, one by a rank aggregation algorithm and the other by a PageRank-like solution. The other two methods take the records as generated from sports contests and adopt widely used Elo’s and Glicko ’s rating systems to derive the rankings. The experimental results are presented and may serve as a reference for related research.</p><p>5 0.71894664 <a title="303-lsi-5" href="./iccv-2013-Slice_Sampling_Particle_Belief_Propagation.html">395 iccv-2013-Slice Sampling Particle Belief Propagation</a></p>
<p>Author: Oliver Müller, Michael Ying Yang, Bodo Rosenhahn</p><p>Abstract: Inference in continuous label Markov random fields is a challenging task. We use particle belief propagation (PBP) for solving the inference problem in continuous label space. Sampling particles from the belief distribution is typically done by using Metropolis-Hastings (MH) Markov chain Monte Carlo (MCMC) methods which involves sampling from a proposal distribution. This proposal distribution has to be carefully designed depending on the particular model and input data to achieve fast convergence. We propose to avoid dependence on a proposal distribution by introducing a slice sampling based PBP algorithm. The proposed approach shows superior convergence performance on an image denoising toy example. Our findings are validated on a challenging relational 2D feature tracking application.</p><p>6 0.71384668 <a title="303-lsi-6" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>7 0.69975388 <a title="303-lsi-7" href="./iccv-2013-Conservation_Tracking.html">87 iccv-2013-Conservation Tracking</a></p>
<p>8 0.69911933 <a title="303-lsi-8" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>9 0.6539892 <a title="303-lsi-9" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>10 0.65104681 <a title="303-lsi-10" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>11 0.64252925 <a title="303-lsi-11" href="./iccv-2013-Online_Robust_Non-negative_Dictionary_Learning_for_Visual_Tracking.html">298 iccv-2013-Online Robust Non-negative Dictionary Learning for Visual Tracking</a></p>
<p>12 0.63777518 <a title="303-lsi-12" href="./iccv-2013-Dynamic_Probabilistic_Volumetric_Models.html">128 iccv-2013-Dynamic Probabilistic Volumetric Models</a></p>
<p>13 0.63172549 <a title="303-lsi-13" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>14 0.62487161 <a title="303-lsi-14" href="./iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects.html">320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</a></p>
<p>15 0.61015481 <a title="303-lsi-15" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>16 0.59995288 <a title="303-lsi-16" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>17 0.58403045 <a title="303-lsi-17" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>18 0.54602021 <a title="303-lsi-18" href="./iccv-2013-Discriminative_Label_Propagation_for_Multi-object_Tracking_with_Sporadic_Appearance_Features.html">120 iccv-2013-Discriminative Label Propagation for Multi-object Tracking with Sporadic Appearance Features</a></p>
<p>19 0.53509724 <a title="303-lsi-19" href="./iccv-2013-The_Way_They_Move%3A_Tracking_Multiple_Targets_with_Similar_Appearance.html">418 iccv-2013-The Way They Move: Tracking Multiple Targets with Similar Appearance</a></p>
<p>20 0.53238237 <a title="303-lsi-20" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.082), (7, 0.018), (16, 0.012), (26, 0.068), (31, 0.027), (34, 0.011), (35, 0.01), (40, 0.013), (42, 0.078), (44, 0.25), (64, 0.138), (73, 0.062), (89, 0.117), (97, 0.017), (98, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79715759 <a title="303-lda-1" href="./iccv-2013-Orderless_Tracking_through_Model-Averaged_Posterior_Estimation.html">303 iccv-2013-Orderless Tracking through Model-Averaged Posterior Estimation</a></p>
<p>Author: Seunghoon Hong, Suha Kwak, Bohyung Han</p><p>Abstract: We propose a novel offline tracking algorithm based on model-averaged posterior estimation through patch matching across frames. Contrary to existing online and offline tracking methods, our algorithm is not based on temporallyordered estimates of target state but attempts to select easyto-track frames first out of the remaining ones without exploiting temporal coherency of target. The posterior of the selected frame is estimated by propagating densities from the already tracked frames in a recursive manner. The density propagation across frames is implemented by an efficient patch matching technique, which is useful for our algorithm since it does not require motion smoothness assumption. Also, we present a hierarchical approach, where a small set of key frames are tracked first and non-key frames are handled by local key frames. Our tracking algorithm is conceptually well-suited for the sequences with abrupt motion, shot changes, and occlusion. We compare our tracking algorithm with existing techniques in real videos with such challenges and illustrate its superior performance qualitatively and quantitatively.</p><p>2 0.73730445 <a title="303-lda-2" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>Author: Ping Wei, Nanning Zheng, Yibiao Zhao, Song-Chun Zhu</p><p>Abstract: Action recognition has often been posed as a classification problem, which assumes that a video sequence only have one action class label and different actions are independent. However, a single human body can perform multiple concurrent actions at the same time, and different actions interact with each other. This paper proposes a concurrent action detection model where the action detection is formulated as a structural prediction problem. In this model, an interval in a video sequence can be described by multiple action labels. An detected action interval is determined both by the unary local detector and the relations with other actions. We use a wavelet feature to represent the action sequence, and design a composite temporal logic descriptor to describe the action relations. The model parameters are trained by structural SVM learning. Given a long video sequence, a sequential decision window search algorithm is designed to detect the actions. Experiments on our new collected concurrent action dataset demonstrate the strength of our method.</p><p>3 0.70357472 <a title="303-lda-3" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>Author: Taegyu Lim, Seunghoon Hong, Bohyung Han, Joon Hee Han</p><p>Abstract: We propose an on-line algorithm to extract a human by foreground/background segmentation and estimate pose of the human from the videos captured by moving cameras. We claim that a virtuous cycle can be created by appropriate interactions between the two modules to solve individual problems. This joint estimation problem is divided into two subproblems, , foreground/background segmentation and pose tracking, which alternate iteratively for optimization; segmentation step generates foreground mask for human pose tracking, and human pose tracking step provides foreground response map for segmentation. The final solution is obtained when the iterative procedure converges. We evaluate our algorithm quantitatively and qualitatively in real videos involving various challenges, and present its outstandingperformance compared to the state-of-the-art techniques for segmentation and pose estimation.</p><p>4 0.67290926 <a title="303-lda-4" href="./iccv-2013-The_Interestingness_of_Images.html">416 iccv-2013-The Interestingness of Images</a></p>
<p>Author: Michael Gygli, Helmut Grabner, Hayko Riemenschneider, Fabian Nater, Luc Van_Gool</p><p>Abstract: We investigate human interest in photos. Based on our own and others ’psychological experiments, we identify various cues for “interestingness ”, namely aesthetics, unusualness and general preferences. For the ranking of retrieved images, interestingness is more appropriate than cues proposed earlier. Interestingness is, for example, correlated with what people believe they will remember. This is opposed to actual memorability, which is uncorrelated to both of them. We introduce a set of features computationally capturing the three main aspects of visual interestingness that we propose and build an interestingness predictor from them. Its performance is shown on three datasets with varying context, reflecting diverse levels of prior knowledge of the viewers.</p><p>5 0.65013391 <a title="303-lda-5" href="./iccv-2013-Incorporating_Cloud_Distribution_in_Sky_Representation.html">215 iccv-2013-Incorporating Cloud Distribution in Sky Representation</a></p>
<p>Author: Kuan-Chuan Peng, Tsuhan Chen</p><p>Abstract: Most sky models only describe the cloudiness ofthe overall sky by a single category or parameter such as sky index, which does not account for the distribution of the clouds across the sky. To capture variable cloudiness, we extend the concept of sky index to a random field indicating the level of cloudiness of each sky pixel in our proposed sky representation based on the Igawa sky model. We formulate the problem of solving the sky index of every sky pixel as a labeling problem, where an approximate solution can be efficiently found. Experimental results show that our proposed sky model has better expressiveness, stability with respect to variation in camera parameters, and geo-location estimation in outdoor images compared to the uniform sky index model. Potential applications of our proposed sky model include sky image rendering, where sky images can be generated with an arbitrary cloud distribution at any time and any location, previously impossible with traditional sky models.</p><p>6 0.64666277 <a title="303-lda-6" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>7 0.64530164 <a title="303-lda-7" href="./iccv-2013-Online_Robust_Non-negative_Dictionary_Learning_for_Visual_Tracking.html">298 iccv-2013-Online Robust Non-negative Dictionary Learning for Visual Tracking</a></p>
<p>8 0.64520854 <a title="303-lda-8" href="./iccv-2013-Cross-View_Action_Recognition_over_Heterogeneous_Feature_Spaces.html">99 iccv-2013-Cross-View Action Recognition over Heterogeneous Feature Spaces</a></p>
<p>9 0.6431005 <a title="303-lda-9" href="./iccv-2013-Finding_Actors_and_Actions_in_Movies.html">166 iccv-2013-Finding Actors and Actions in Movies</a></p>
<p>10 0.64282203 <a title="303-lda-10" href="./iccv-2013-Constant_Time_Weighted_Median_Filtering_for_Stereo_Matching_and_Beyond.html">88 iccv-2013-Constant Time Weighted Median Filtering for Stereo Matching and Beyond</a></p>
<p>11 0.63986248 <a title="303-lda-11" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>12 0.63917023 <a title="303-lda-12" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>13 0.63642311 <a title="303-lda-13" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>14 0.62875867 <a title="303-lda-14" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>15 0.62676728 <a title="303-lda-15" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>16 0.62569648 <a title="303-lda-16" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>17 0.62431836 <a title="303-lda-17" href="./iccv-2013-Video_Motion_for_Every_Visible_Point.html">441 iccv-2013-Video Motion for Every Visible Point</a></p>
<p>18 0.61295772 <a title="303-lda-18" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>19 0.60766608 <a title="303-lda-19" href="./iccv-2013-Volumetric_Semantic_Segmentation_Using_Pyramid_Context_Features.html">447 iccv-2013-Volumetric Semantic Segmentation Using Pyramid Context Features</a></p>
<p>20 0.60765034 <a title="303-lda-20" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
