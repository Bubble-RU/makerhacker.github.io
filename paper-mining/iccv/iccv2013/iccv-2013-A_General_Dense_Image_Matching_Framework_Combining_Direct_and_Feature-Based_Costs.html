<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-12" href="#">iccv2013-12</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</h1>
<br/><p>Source: <a title="iccv-2013-12-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Braux-Zin_A_General_Dense_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Jim Braux-Zin, Romain Dupont, Adrien Bartoli</p><p>Abstract: Dense motion field estimation (typically Romain Dupont1 romain . dupont @ cea . fr Adrien Bartoli2 adrien . bart o l @ gmai l com i . 2 ISIT, Universit e´ d’Auvergne/CNRS, France sions are explicitly modeled [32, 13]. Coarse-to-fine warping improves global convergence by making the assumption that optical flow, the motion of smaller structures is similar to the motion of stereo disparity and surface registration) is a key computer vision problem. Many solutions have been proposed to compute small or large displacements, narrow or wide baseline stereo disparity, but a unified methodology is still lacking. We here introduce a general framework that robustly combines direct and feature-based matching. The feature-based cost is built around a novel robust distance function that handles keypoints and “weak” features such as segments. It allows us to use putative feature matches which may contain mismatches to guide dense motion estimation out of local minima. Our framework uses a robust direct data term (AD-Census). It is implemented with a powerful second order Total Generalized Variation regularization with external and self-occlusion reasoning. Our framework achieves state of the art performance in several cases (standard optical flow benchmarks, wide-baseline stereo and non-rigid surface registration). Our framework has a modular design that customizes to specific application needs.</p><p>Reference: <a title="iccv-2013-12-reference" href="../iccv2013_reference/iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 fr 1 CEA, LIST, France Abstract Dense motion field estimation (typically Romain Dupont1 romain . [sent-3, score-0.189]
</p><p>2 Coarse-to-fine warping improves global convergence by making the assumption that optical flow, the motion of smaller structures is similar to the motion of stereo disparity and surface registration) is a key computer vision problem. [sent-8, score-0.746]
</p><p>3 It allows us to use putative feature matches which may contain mismatches to guide dense motion estimation out of local minima. [sent-12, score-0.528]
</p><p>4 Our framework uses a robust direct data term (AD-Census). [sent-13, score-0.226]
</p><p>5 Our framework achieves state of the art performance in several cases (standard optical flow benchmarks, wide-baseline stereo and non-rigid surface registration). [sent-15, score-0.754]
</p><p>6 Introduction A dense motion field, also called optical flow, is a very useful cue for problems such as tracking, segmentation, localization and reconstruction, or non-rigid surfaces registration. [sent-17, score-0.378]
</p><p>7 Optical flow estimation is an old computer vision problem. [sent-18, score-0.283]
</p><p>8 While early techniques were patch-based [19], current ones estimate dense flow fields with variational methods built upon the work by Horn and Schunk [16] by coupled minimization of a data term often based on the brightness constancy assumption and regularization. [sent-19, score-0.581]
</p><p>9 The research on non-rigid surface registration offers a good example of the duality of features and whole image information. [sent-31, score-0.293]
</p><p>10 Surface tracking frame to frame estimation of a deformation offers the best accuracy when using a direct pixel-wise cost [13, 12] but those methods, similarly to standard optical flow techniques are limited to small updates at each frame. [sent-32, score-0.841]
</p><p>11 On the other hand there is the problem of surface detection: estimation of the deformation from only one frame and the flat template image. [sent-33, score-0.276]
</p><p>12 This is usually done from feature matches which are first filtered to remove outliers and then fitted by a warp such as a Thin-Plate Spline [23]. [sent-34, score-0.509]
</p><p>13 –  –  Recent works [24] aim at initializing an image-based method from a feature-based warp but the two steps are still independent and if the warp is too inaccurate in some areas, the final step is unlikely to recover the true deformation. [sent-35, score-0.262]
</p><p>14 [3 1] propose a RANSAC approach to dense motion segmentation and estimation from feature matches. [sent-40, score-0.205]
</p><p>15 The output is then refined with a variational regularization. [sent-44, score-0.182]
</p><p>16 [9] inspired our work with a successful approach cou-  pling descriptor matching with variational optimization in the same process. [sent-47, score-0.268]
</p><p>17 Their method remains a reference in optical flow estimation for its robustness and accuracy, but we identified several limitations. [sent-48, score-0.587]
</p><p>18 It only uses custom descriptors computed densely or on a fixed spatial grid and the tight coupling between the feature matching and the cost function (see Section 1) prevents from using state of the art features. [sent-49, score-0.209]
</p><p>19 We propose a novel approach to widen the range of applicability of dense variational methods. [sent-52, score-0.244]
</p><p>20 This is achieved by combining direct and feature-based costs in one process with a focus on flexibility and robustness to easily exploit any descriptor and detector. [sent-53, score-0.282]
</p><p>21 Feature detection and matching is done only once on the full-resolution image pair but the feature matches guide the optical flow out of local minima during the whole optimization. [sent-54, score-0.91]
</p><p>22 1, a robust direct cost with occlusions handling in Section 2. [sent-56, score-0.333]
</p><p>23 2, and a novel feature-based cost able to handle unfiltered matches of point features or weakly localized segments in Section 2. [sent-57, score-0.629]
</p><p>24 [9] who proposed the first introduction of rich descriptors in variational optical flow computation through the addition of a new term in the cost function: CLDOF(u)  = Ccolor(u) + γ Cgradient(u) + α Csmooth(u)  (1) (2)  +β? [sent-78, score-0.855]
</p><p>25 ) dx  (4)  u(mia)tch  is the dis-  where for each feature match i∈ 1. [sent-91, score-0.192]
</p><p>26 proposed two types of features to associate a priori displacements to affected pixels: regions using SIFT [18] and color based descriptors, and points using Histogram of Oriented Gradients [11] or Geometric Blur [4] over a fine regular grid. [sent-100, score-0.287]
</p><p>27 Nearest neighbors in feature space are selected as potential matches ; this processus is represented by the term Cdesc (umatch) which does not intervene in the variational optimization. [sent-101, score-0.555]
</p><p>28 showed that while not increasing the best case accuracy, feature matches help drive optical flow estimation out of local minima when the common assumptions of coarse-tofine warping are violated. [sent-105, score-0.981]
</p><p>29 well localized (to translate matches to displacements) and to provide a match quality score to minimize the impact of outliers. [sent-109, score-0.402]
</p><p>30 This limits the scope of suitable features and 186  introduces a tight coupling between the feature matcher and the variational solver. [sent-110, score-0.321]
</p><p>31 The logic behind this choice was to let the variational solver “do the matching” by selecting the best feature amongst the neighbors in feature space. [sent-111, score-0.274]
</p><p>32 1  We use a variational model built around the recently introduced second order Total Generalized Variation [8] (TGV2) regularization which favors piecewise-affine displacement fields as detailed in Section 2. [sent-121, score-0.391]
</p><p>33 Contrary  to LDOF in (4), Cmatch is here a function of the distance of x + u(x) to the target feature and not the difference between u(x) and an a priori displacement. [sent-125, score-0.195]
</p><p>34 The two components ux and uy of the optical flow are independently regularized. [sent-132, score-0.502]
</p><p>35 TGV1 favors piecewise constant solutions while TGV2 favors piecewise affine solutions, more desirable for stereo and optical flow estimations. [sent-163, score-0.817]
</p><p>36 Any sufficiently smooth pixel-wise data term can be used, but complexity  constraints must be taken into account to keep reasonable performances as the direct cost is going to be evaluated hundreds of times at each pixel. [sent-174, score-0.322]
</p><p>37 In the following we derive the direct costs used in our experiments. [sent-176, score-0.186]
</p><p>38 We found in the scenarios tested with the AD-Census data-term that a value of 50% of the maximum direct cost value significantly increases robustness without deteriorating results in the absence of occlusions. [sent-194, score-0.278]
</p><p>39 Self-occlusions appear when a rigid scene is observed from different viewpoints or when a deformable surface folds. [sent-195, score-0.171]
</p><p>40 (11)  The direct data term is not to be trusted on occluded areas so we multiply it by 1 Pocc before including it in the global cost function (5). [sent-212, score-0.276]
</p><p>41 Ωρi(x)D(x,u(x),Fi) dx  (12)  D(x,u(x),Fi) =+ (c1Γ −σ[D c)f( Γxσ +[D ua(px(x), uF(ix)]),Fi)],  (13)  where ρi is the influence function of the feature, Γσ is a robust estimator, Df is the main feature distance, Dap the a priori match distance for weakly localized features and c a weighting factor. [sent-216, score-0.638]
</p><p>42 Given a point feature ilocated at xf = xf0 + dx, xf0 = floor(xf), dx = (1, 1)T − dx, its influence function ρi is defined for the four affecte−d pixels as: ρi ρi  (xf0) = dxxdxy | ρi ? [sent-221, score-0.342]
</p><p>43 It can be seen that at coarse levels, where several features affect each pixel, a vote takes place where the outvoted matches durably loose influence. [sent-242, score-0.317]
</p><p>44 At finer level, the regularization and the direct data term influences are more important and should converge to the desired optimum. [sent-243, score-0.253]
</p><p>45 As we will see, a priori matches are only needed for weakly-localized features so c = 1for point features. [sent-257, score-0.466]
</p><p>46 The segment matching algorithm proposed by Wang [28] is interesting because it does not rely on photometric similarities but encodes semi-global structure and is robust to wide-baseline perspective distortion. [sent-258, score-0.179]
</p><p>47 Segment matches lie on the same line but their endpoints are not guaranteed to be matched in both images ; in fact, due to occlusions or different image boundaries it is rarely the case. [sent-263, score-0.422]
</p><p>48 Given a segment feature defined by its endpoints Fi = (xib , xie ) :  Df(segment)(x,Fi) =? [sent-265, score-0.223]
</p><p>49 To make the influence of segment features vanish at the same rate as point features, the influence functions ρi are multiplied by s−1 at each upsampling step. [sent-271, score-0.361]
</p><p>50 Even though the segment matches constrain only one dimension, the remaining degree of freedom cannot be left fully unconstrained. [sent-273, score-0.382]
</p><p>51 We introduce the concept of a priori matches which makes the assumption of a linear mapping between segment matches. [sent-274, score-0.531]
</p><p>52 Given a segment = in I0 and its match =  Fi(0)  (xi(b0),xi(e0))  in I1, the a priori match defined by:  Fi(1)  xa(1p)  of the point  (xi(b1),xi(e1)) x(0) ∈ Fi(0) is  t =? [sent-275, score-0.337]
</p><p>53 However it is most of the time not far from the truth and can be used as a cue to guide the optical flow estimation at coarse levels. [sent-283, score-0.538]
</p><p>54 As shown in Table 1, a priori matches are not an accurate prior and degrade a lot the results if used as the sole constraint in either the small or wide-baseline settings. [sent-284, score-0.475]
</p><p>55 7%  only a priori matches no a priori matches  Table 1. [sent-292, score-0.858]
</p><p>56 Influence of the inclusion of a priori matches on the proportion of inlier depths values (error smaller than 5% of the depth range) when using segment features. [sent-293, score-0.531]
</p><p>57 This approach could be easily extended to other weakly localized features like regions or contours. [sent-298, score-0.166]
</p><p>58 We start by the standard optical flow benchmarks and then show promising results on wide-baseline stereo and non-rigid surface registration. [sent-301, score-0.754]
</p><p>59 5, α0 = 4, α1 = 1, 20 warp of 40 iterations each and a subsampling factor s = 0. [sent-303, score-0.167]
</p><p>60 The direct data term is AD-Census with a 3 3 window, μ0 = 1 and μ1 = 0. [sent-305, score-0.188]
</p><p>61 This is typically the case in frame-toframe optical flow benchmarks. [sent-312, score-0.502]
</p><p>62 At the time of writing, our method is the top ranked true 2D optical flow method in this benchmark as can be seen in Table 2. [sent-317, score-0.559]
</p><p>63 It means that even for relatively small displacements (frame-to-frame optical flow) our method outperforms state of the art. [sent-318, score-0.356]
</p><p>64 2 px  Time 200 s 3 min  3 4  MotionSLIC[ms] PR-Sceneflow  4. [sent-328, score-0.258]
</p><p>65 Methods 2 and 3 [33] are one-dimensional motion stereo estimation methods. [sent-355, score-0.22]
</p><p>66 Our method ranked 5 is the top true 2D optical flow method. [sent-356, score-0.502]
</p><p>67 The OpenCV FAST-SIFT matcher produced for each pair approximately 4000 matches in 4 seconds on a 6 2. [sent-378, score-0.336]
</p><p>68 40GHz Intel Xeon CPU and the variational estimat6io ×n t2o. [sent-379, score-0.182]
</p><p>69 We think it is interesting anyway to show that the presence of outliers in matches do not noticeably degrade the results. [sent-386, score-0.378]
</p><p>70 We use unfiltered SURF [3] matches for this experiment. [sent-387, score-0.375]
</p><p>71 The great resili=enc ∞e ofour algorithm to mismatches comes from the use of a non-convex robust estimator which penalizes matches which are not coherent with others. [sent-391, score-0.361]
</p><p>72 Wide-Baseline Stereo The increased robustness of our approach allows us to explore applications such as wide-baseline stereo, previously unreachable to optical flow techniques. [sent-394, score-0.551]
</p><p>73 We ran our algorithm on the herz j e su dataset with segment matches [28] and a feature weight γ = 5 while constraining the displacement vectors on the epipolar lines. [sent-397, score-0.588]
</p><p>74 For extreme perspective distortions, the AD-Census data term shows its limits but we see that the segment features greatly increase the convergence basin and allows for comparable results on most image pairs. [sent-399, score-0.186]
</p><p>75 Moreover, our continuous variational approach is faster than the Graph-Cut ones and is not restricted to one dimension. [sent-400, score-0.182]
</p><p>76 One can also note that our occlusion handler, although coming from the deformable surface field [13] is also suitable for rigid settings. [sent-401, score-0.207]
</p><p>77 Deformable Surface Detection Another interesting field of application where optical flow methods have so far been unadapted is the detection of de-  formable surfaces. [sent-404, score-0.538]
</p><p>78 Given a flat surface template and an image of this same surface with non-rigid deformation, the problem is to estimate the pixel correspondences between the two images. [sent-405, score-0.295]
</p><p>79 Non-rigid surface detection methods are feature-based and usually adopt a two step approach: features filtering and fitting of a warp (Thin-Plate Spline or Free-Form Deformation). [sent-407, score-0.297]
</p><p>80 To show the suitability of our method to deformable surface detection and produce some quantitative results, we generate a synthetic deformation using the Matlab toolbox from [22]. [sent-408, score-0.245]
</p><p>81 We obtain point matches using the SIFT detector and descriptor. [sent-409, score-0.28]
</p><p>82 We compare ourself with the feature-based method [24] which is used to filter the matches and then fit a standard Free-Form Deformation warp. [sent-410, score-0.28]
</p><p>83 We also add the LDOF optical flow method as the most robust optical flow method in the state of the art. [sent-411, score-1.042]
</p><p>84 In (b) we show an  example of the low-quality SURF matches used to demonstrate the robustness to outliers. [sent-423, score-0.329]
</p><p>85 The matches are represented by their equivalent motion vector. [sent-424, score-0.341]
</p><p>86 The table shows the percentage of outliers error greater than 5% of the depth range for our algorithm without matches / our algorithm with segment matches / Graph Cut with DAISY (results from [27]). [sent-521, score-0.714]
</p><p>87 Our results are even better with the unfiltered matches than with the filtered matches, which reveals a weakness of a separate step of outlier removal: it is difficult to find the balance between removing too many good matches or leaving mismatches. [sent-524, score-0.655]
</p><p>88 In this work we introduced a general framework allowing us to greatly extends the scope of applicability of variational optical flow techniques. [sent-525, score-0.684]
</p><p>89 We combined a modern powerful discontinuity preserving regularizer with a robust direct data term and features integration. [sent-526, score-0.34]
</p><p>90 We generalized and extended the model of [9] to support any point features and introduced the novel concept of a priori matches to enable the use of 191  FigurTem4p. [sent-527, score-0.532]
</p><p>91 Self-occlusion detection, rarely accounted for in optical flow estimation, further increases the robustness of our approach. [sent-541, score-0.551]
</p><p>92 This allowed  us to showcase state of the art results on standard narrowbaseline optical flow and wide-baseline stereo. [sent-542, score-0.565]
</p><p>93 Future work involves the improvement of each building block: higher-order regularization, richer direct data term and new features such as contours. [sent-544, score-0.225]
</p><p>94 References [1] Special session on robust optical flow, 2013. [sent-547, score-0.335]
</p><p>95 Recovering piecewise smooth multichannel images by minimization of convex functionals with total generalized variation penalty. [sent-583, score-0.254]
</p><p>96 Large displacement optical flow: descriptor matching in variational motion estimation. [sent-594, score-0.681]
</p><p>97 A variational approach to video registration with subspace constraints. [sent-610, score-0.267]
</p><p>98 An iterative image registration technique with an application to stereo vision. [sent-653, score-0.208]
</p><p>99 Feature-based deformable surface  [25] [26] [27] [28] [29] [30] [3 1] [32] [33] [34] [35] [36]  detection with self-occlusion reasoning. [sent-686, score-0.171]
</p><p>100 Pushing the limits of stereo using variational stereo estimation. [sent-693, score-0.428]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('matches', 0.28), ('px', 0.258), ('optical', 0.255), ('flow', 0.247), ('variational', 0.182), ('ldof', 0.163), ('priori', 0.149), ('census', 0.147), ('direct', 0.141), ('warp', 0.131), ('kitti', 0.13), ('surface', 0.129), ('cmatch', 0.127), ('stereo', 0.123), ('mia', 0.122), ('tch', 0.108), ('dx', 0.103), ('segment', 0.102), ('displacements', 0.101), ('displacement', 0.097), ('unfiltered', 0.095), ('influence', 0.093), ('cost', 0.088), ('registration', 0.085), ('localized', 0.079), ('surf', 0.077), ('deformation', 0.074), ('warping', 0.074), ('middlebury', 0.074), ('generalized', 0.066), ('brox', 0.066), ('occlusions', 0.066), ('regularization', 0.065), ('asift', 0.063), ('cdirect', 0.063), ('dxxdxy', 0.063), ('herz', 0.063), ('narrowbaseline', 0.063), ('pocc', 0.063), ('cea', 0.063), ('dense', 0.062), ('motion', 0.061), ('benchmark', 0.057), ('adrien', 0.056), ('looses', 0.056), ('matcher', 0.056), ('romain', 0.056), ('variation', 0.054), ('df', 0.054), ('opencv', 0.054), ('outliers', 0.052), ('warps', 0.05), ('weakly', 0.05), ('pock', 0.049), ('xa', 0.049), ('wills', 0.049), ('xib', 0.049), ('piecewise', 0.049), ('robustness', 0.049), ('fi', 0.048), ('descriptor', 0.047), ('favors', 0.047), ('term', 0.047), ('degrade', 0.046), ('feature', 0.046), ('smooth', 0.046), ('costs', 0.045), ('daisy', 0.045), ('disparity', 0.043), ('brightness', 0.043), ('minima', 0.043), ('chambolle', 0.043), ('mismatches', 0.043), ('tola', 0.043), ('match', 0.043), ('external', 0.042), ('deformable', 0.042), ('duality', 0.042), ('session', 0.042), ('matching', 0.039), ('total', 0.039), ('discontinuity', 0.039), ('endpoints', 0.039), ('horn', 0.039), ('spline', 0.039), ('robust', 0.038), ('preserving', 0.038), ('translates', 0.038), ('features', 0.037), ('sift', 0.037), ('template', 0.037), ('xf', 0.037), ('line', 0.037), ('subsampling', 0.036), ('upsampling', 0.036), ('xie', 0.036), ('field', 0.036), ('tv', 0.036), ('estimation', 0.036), ('descriptors', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="12-tfidf-1" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>Author: Jim Braux-Zin, Romain Dupont, Adrien Bartoli</p><p>Abstract: Dense motion field estimation (typically Romain Dupont1 romain . dupont @ cea . fr Adrien Bartoli2 adrien . bart o l @ gmai l com i . 2 ISIT, Universit e´ d’Auvergne/CNRS, France sions are explicitly modeled [32, 13]. Coarse-to-fine warping improves global convergence by making the assumption that optical flow, the motion of smaller structures is similar to the motion of stereo disparity and surface registration) is a key computer vision problem. Many solutions have been proposed to compute small or large displacements, narrow or wide baseline stereo disparity, but a unified methodology is still lacking. We here introduce a general framework that robustly combines direct and feature-based matching. The feature-based cost is built around a novel robust distance function that handles keypoints and “weak” features such as segments. It allows us to use putative feature matches which may contain mismatches to guide dense motion estimation out of local minima. Our framework uses a robust direct data term (AD-Census). It is implemented with a powerful second order Total Generalized Variation regularization with external and self-occlusion reasoning. Our framework achieves state of the art performance in several cases (standard optical flow benchmarks, wide-baseline stereo and non-rigid surface registration). Our framework has a modular design that customizes to specific application needs.</p><p>2 0.38358533 <a title="12-tfidf-2" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>Author: Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, Cordelia Schmid</p><p>Abstract: Optical flow computation is a key component in many computer vision systems designed for tasks such as action detection or activity recognition. However, despite several major advances over the last decade, handling large displacement in optical flow remains an open problem. Inspired by the large displacement optical flow of Brox & Malik [6], our approach, termed DeepFlow, blends a matching algorithm with a variational approach for optical flow. We propose a descriptor matching algorithm, tailored to the optical flow problem, that allows to boost performance on fast motions. The matching algorithm builds upon a multi-stage architecture with 6 layers, interleaving convolutions and max-pooling, a construction akin to deep convolutional nets. Using dense sampling, it allows to efficiently retrieve quasi-dense correspondences, and enjoys a built-in smoothing effect on descriptors matches, a valuable assetfor integration into an energy minimizationframework for optical flow estimation. DeepFlow efficiently handles large displacements occurring in realistic videos, and shows competitive performance on optical flow benchmarks. Furthermore, it sets a new state-of-the-art on the MPI-Sintel dataset [8].</p><p>3 0.36964157 <a title="12-tfidf-3" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>Author: Christoph Vogel, Konrad Schindler, Stefan Roth</p><p>Abstract: Estimating dense 3D scene flow from stereo sequences remains a challenging task, despite much progress in both classical disparity and 2D optical flow estimation. To overcome the limitations of existing techniques, we introduce a novel model that represents the dynamic 3D scene by a collection of planar, rigidly moving, local segments. Scene flow estimation then amounts to jointly estimating the pixelto-segment assignment, and the 3D position, normal vector, and rigid motion parameters of a plane for each segment. The proposed energy combines an occlusion-sensitive data term with appropriate shape, motion, and segmentation regularizers. Optimization proceeds in two stages: Starting from an initial superpixelization, we estimate the shape and motion parameters of all segments by assigning a proposal from a set of moving planes. Then the pixel-to-segment assignment is updated, while holding the shape and motion parameters of the moving planes fixed. We demonstrate the benefits of our model on different real-world image sets, including the challenging KITTI benchmark. We achieve leading performance levels, exceeding competing 3D scene flow methods, and even yielding better 2D motion estimates than all tested dedicated optical flow techniques.</p><p>4 0.28198019 <a title="12-tfidf-4" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<p>Author: Tae Hyun Kim, Hee Seok Lee, Kyoung Mu Lee</p><p>Abstract: Many state-of-the-art optical flow estimation algorithms optimize the data and regularization terms to solve ill-posed problems. In this paper, in contrast to the conventional optical flow framework that uses a single or fixed data model, we study a novel framework that employs locally varying data term that adaptively combines different multiple types of data models. The locally adaptive data term greatly reduces the matching ambiguity due to the complementary nature of the multiple data models. The optimal number of complementary data models is learnt by minimizing the redundancy among them under the minimum description length constraint (MDL). From these chosen data models, a new optical flow estimation energy model is designed with the weighted sum of the multiple data models, and a convex optimization-based highly effective and practical solution thatfinds the opticalflow, as well as the weights isproposed. Comparative experimental results on the Middlebury optical flow benchmark show that the proposed method using the complementary data models outperforms the state-ofthe art methods.</p><p>5 0.25225919 <a title="12-tfidf-5" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<p>Author: Marius Leordeanu, Andrei Zanfir, Cristian Sminchisescu</p><p>Abstract: Estimating a dense correspondence field between successive video frames, under large displacement, is important in many visual learning and recognition tasks. We propose a novel sparse-to-dense matching method for motion field estimation and occlusion detection. As an alternative to the current coarse-to-fine approaches from the optical flow literature, we start from the higher level of sparse matching with rich appearance and geometric constraints collected over extended neighborhoods, using an occlusion aware, locally affine model. Then, we move towards the simpler, but denser classic flow field model, with an interpolation procedure that offers a natural transition between the sparse and the dense correspondence fields. We experimentally demonstrate that our appearance features and our complex geometric constraintspermit the correct motion estimation even in difficult cases of large displacements and significant appearance changes. We also propose a novel classification method for occlusion detection that works in conjunction with the sparse-to-dense matching model. We validate our approach on the newly released Sintel dataset and obtain state-of-the-art results.</p><p>6 0.20360968 <a title="12-tfidf-6" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>7 0.20157874 <a title="12-tfidf-7" href="./iccv-2013-Multiple_Non-rigid_Surface_Detection_and_Registration.html">283 iccv-2013-Multiple Non-rigid Surface Detection and Registration</a></p>
<p>8 0.17604415 <a title="12-tfidf-8" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>9 0.16245642 <a title="12-tfidf-9" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>10 0.15609361 <a title="12-tfidf-10" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>11 0.14732467 <a title="12-tfidf-11" href="./iccv-2013-Robust_Non-parametric_Data_Fitting_for_Correspondence_Modeling.html">358 iccv-2013-Robust Non-parametric Data Fitting for Correspondence Modeling</a></p>
<p>12 0.14693122 <a title="12-tfidf-12" href="./iccv-2013-Improving_Graph_Matching_via_Density_Maximization.html">214 iccv-2013-Improving Graph Matching via Density Maximization</a></p>
<p>13 0.14631754 <a title="12-tfidf-13" href="./iccv-2013-EVSAC%3A_Accelerating_Hypotheses_Generation_by_Modeling_Matching_Scores_with_Extreme_Value_Theory.html">131 iccv-2013-EVSAC: Accelerating Hypotheses Generation by Modeling Matching Scores with Extreme Value Theory</a></p>
<p>14 0.1409345 <a title="12-tfidf-14" href="./iccv-2013-PM-Huber%3A_PatchMatch_with_Huber_Regularization_for_Stereo_Matching.html">304 iccv-2013-PM-Huber: PatchMatch with Huber Regularization for Stereo Matching</a></p>
<p>15 0.13982299 <a title="12-tfidf-15" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>16 0.13156667 <a title="12-tfidf-16" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>17 0.12906052 <a title="12-tfidf-17" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>18 0.12514891 <a title="12-tfidf-18" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>19 0.12461381 <a title="12-tfidf-19" href="./iccv-2013-Elastic_Net_Constraints_for_Shape_Matching.html">140 iccv-2013-Elastic Net Constraints for Shape Matching</a></p>
<p>20 0.1217397 <a title="12-tfidf-20" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.283), (1, -0.226), (2, -0.029), (3, 0.106), (4, -0.011), (5, 0.057), (6, -0.013), (7, -0.039), (8, 0.072), (9, -0.023), (10, -0.044), (11, 0.095), (12, 0.271), (13, -0.008), (14, 0.06), (15, -0.039), (16, -0.057), (17, 0.094), (18, 0.248), (19, 0.041), (20, 0.157), (21, 0.053), (22, -0.005), (23, -0.066), (24, 0.153), (25, -0.157), (26, 0.113), (27, 0.075), (28, -0.027), (29, -0.048), (30, -0.013), (31, -0.109), (32, -0.037), (33, -0.01), (34, -0.076), (35, -0.032), (36, 0.006), (37, -0.02), (38, 0.012), (39, -0.054), (40, 0.033), (41, -0.023), (42, 0.007), (43, -0.007), (44, 0.014), (45, -0.018), (46, -0.069), (47, -0.048), (48, 0.006), (49, -0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97655201 <a title="12-lsi-1" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>Author: Jim Braux-Zin, Romain Dupont, Adrien Bartoli</p><p>Abstract: Dense motion field estimation (typically Romain Dupont1 romain . dupont @ cea . fr Adrien Bartoli2 adrien . bart o l @ gmai l com i . 2 ISIT, Universit e´ d’Auvergne/CNRS, France sions are explicitly modeled [32, 13]. Coarse-to-fine warping improves global convergence by making the assumption that optical flow, the motion of smaller structures is similar to the motion of stereo disparity and surface registration) is a key computer vision problem. Many solutions have been proposed to compute small or large displacements, narrow or wide baseline stereo disparity, but a unified methodology is still lacking. We here introduce a general framework that robustly combines direct and feature-based matching. The feature-based cost is built around a novel robust distance function that handles keypoints and “weak” features such as segments. It allows us to use putative feature matches which may contain mismatches to guide dense motion estimation out of local minima. Our framework uses a robust direct data term (AD-Census). It is implemented with a powerful second order Total Generalized Variation regularization with external and self-occlusion reasoning. Our framework achieves state of the art performance in several cases (standard optical flow benchmarks, wide-baseline stereo and non-rigid surface registration). Our framework has a modular design that customizes to specific application needs.</p><p>2 0.9179613 <a title="12-lsi-2" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>Author: Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, Cordelia Schmid</p><p>Abstract: Optical flow computation is a key component in many computer vision systems designed for tasks such as action detection or activity recognition. However, despite several major advances over the last decade, handling large displacement in optical flow remains an open problem. Inspired by the large displacement optical flow of Brox & Malik [6], our approach, termed DeepFlow, blends a matching algorithm with a variational approach for optical flow. We propose a descriptor matching algorithm, tailored to the optical flow problem, that allows to boost performance on fast motions. The matching algorithm builds upon a multi-stage architecture with 6 layers, interleaving convolutions and max-pooling, a construction akin to deep convolutional nets. Using dense sampling, it allows to efficiently retrieve quasi-dense correspondences, and enjoys a built-in smoothing effect on descriptors matches, a valuable assetfor integration into an energy minimizationframework for optical flow estimation. DeepFlow efficiently handles large displacements occurring in realistic videos, and shows competitive performance on optical flow benchmarks. Furthermore, it sets a new state-of-the-art on the MPI-Sintel dataset [8].</p><p>3 0.87480366 <a title="12-lsi-3" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<p>Author: Marius Leordeanu, Andrei Zanfir, Cristian Sminchisescu</p><p>Abstract: Estimating a dense correspondence field between successive video frames, under large displacement, is important in many visual learning and recognition tasks. We propose a novel sparse-to-dense matching method for motion field estimation and occlusion detection. As an alternative to the current coarse-to-fine approaches from the optical flow literature, we start from the higher level of sparse matching with rich appearance and geometric constraints collected over extended neighborhoods, using an occlusion aware, locally affine model. Then, we move towards the simpler, but denser classic flow field model, with an interpolation procedure that offers a natural transition between the sparse and the dense correspondence fields. We experimentally demonstrate that our appearance features and our complex geometric constraintspermit the correct motion estimation even in difficult cases of large displacements and significant appearance changes. We also propose a novel classification method for occlusion detection that works in conjunction with the sparse-to-dense matching model. We validate our approach on the newly released Sintel dataset and obtain state-of-the-art results.</p><p>4 0.85782164 <a title="12-lsi-4" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<p>Author: Tae Hyun Kim, Hee Seok Lee, Kyoung Mu Lee</p><p>Abstract: Many state-of-the-art optical flow estimation algorithms optimize the data and regularization terms to solve ill-posed problems. In this paper, in contrast to the conventional optical flow framework that uses a single or fixed data model, we study a novel framework that employs locally varying data term that adaptively combines different multiple types of data models. The locally adaptive data term greatly reduces the matching ambiguity due to the complementary nature of the multiple data models. The optimal number of complementary data models is learnt by minimizing the redundancy among them under the minimum description length constraint (MDL). From these chosen data models, a new optical flow estimation energy model is designed with the weighted sum of the multiple data models, and a convex optimization-based highly effective and practical solution thatfinds the opticalflow, as well as the weights isproposed. Comparative experimental results on the Middlebury optical flow benchmark show that the proposed method using the complementary data models outperforms the state-ofthe art methods.</p><p>5 0.85150915 <a title="12-lsi-5" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>Author: Christoph Vogel, Konrad Schindler, Stefan Roth</p><p>Abstract: Estimating dense 3D scene flow from stereo sequences remains a challenging task, despite much progress in both classical disparity and 2D optical flow estimation. To overcome the limitations of existing techniques, we introduce a novel model that represents the dynamic 3D scene by a collection of planar, rigidly moving, local segments. Scene flow estimation then amounts to jointly estimating the pixelto-segment assignment, and the 3D position, normal vector, and rigid motion parameters of a plane for each segment. The proposed energy combines an occlusion-sensitive data term with appropriate shape, motion, and segmentation regularizers. Optimization proceeds in two stages: Starting from an initial superpixelization, we estimate the shape and motion parameters of all segments by assigning a proposal from a set of moving planes. Then the pixel-to-segment assignment is updated, while holding the shape and motion parameters of the moving planes fixed. We demonstrate the benefits of our model on different real-world image sets, including the challenging KITTI benchmark. We achieve leading performance levels, exceeding competing 3D scene flow methods, and even yielding better 2D motion estimates than all tested dedicated optical flow techniques.</p><p>6 0.67003906 <a title="12-lsi-6" href="./iccv-2013-Robust_Non-parametric_Data_Fitting_for_Correspondence_Modeling.html">358 iccv-2013-Robust Non-parametric Data Fitting for Correspondence Modeling</a></p>
<p>7 0.66491169 <a title="12-lsi-7" href="./iccv-2013-Two-Point_Gait%3A_Decoupling_Gait_from_Body_Shape.html">430 iccv-2013-Two-Point Gait: Decoupling Gait from Body Shape</a></p>
<p>8 0.62378871 <a title="12-lsi-8" href="./iccv-2013-Optimal_Orthogonal_Basis_and_Image_Assimilation%3A_Motion_Modeling.html">301 iccv-2013-Optimal Orthogonal Basis and Image Assimilation: Motion Modeling</a></p>
<p>9 0.58860546 <a title="12-lsi-9" href="./iccv-2013-Local_Signal_Equalization_for_Correspondence_Matching.html">255 iccv-2013-Local Signal Equalization for Correspondence Matching</a></p>
<p>10 0.58021057 <a title="12-lsi-10" href="./iccv-2013-Measuring_Flow_Complexity_in_Videos.html">263 iccv-2013-Measuring Flow Complexity in Videos</a></p>
<p>11 0.57396096 <a title="12-lsi-11" href="./iccv-2013-EVSAC%3A_Accelerating_Hypotheses_Generation_by_Modeling_Matching_Scores_with_Extreme_Value_Theory.html">131 iccv-2013-EVSAC: Accelerating Hypotheses Generation by Modeling Matching Scores with Extreme Value Theory</a></p>
<p>12 0.57204294 <a title="12-lsi-12" href="./iccv-2013-Multiple_Non-rigid_Surface_Detection_and_Registration.html">283 iccv-2013-Multiple Non-rigid Surface Detection and Registration</a></p>
<p>13 0.56357944 <a title="12-lsi-13" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>14 0.55995804 <a title="12-lsi-14" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<p>15 0.55759037 <a title="12-lsi-15" href="./iccv-2013-Elastic_Net_Constraints_for_Shape_Matching.html">140 iccv-2013-Elastic Net Constraints for Shape Matching</a></p>
<p>16 0.55689281 <a title="12-lsi-16" href="./iccv-2013-Modeling_Self-Occlusions_in_Dynamic_Shape_and_Appearance_Tracking.html">270 iccv-2013-Modeling Self-Occlusions in Dynamic Shape and Appearance Tracking</a></p>
<p>17 0.54797685 <a title="12-lsi-17" href="./iccv-2013-Fibonacci_Exposure_Bracketing_for_High_Dynamic_Range_Imaging.html">164 iccv-2013-Fibonacci Exposure Bracketing for High Dynamic Range Imaging</a></p>
<p>18 0.54492009 <a title="12-lsi-18" href="./iccv-2013-Nested_Shape_Descriptors.html">288 iccv-2013-Nested Shape Descriptors</a></p>
<p>19 0.53964424 <a title="12-lsi-19" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>20 0.50424945 <a title="12-lsi-20" href="./iccv-2013-Fix_Structured_Learning_of_2013_ICCV_paper_k2opt.pdf.html">171 iccv-2013-Fix Structured Learning of 2013 ICCV paper k2opt.pdf</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.05), (7, 0.011), (26, 0.054), (31, 0.045), (40, 0.012), (42, 0.086), (64, 0.043), (73, 0.378), (89, 0.221)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92257583 <a title="12-lda-1" href="./iccv-2013-Single-Patch_Low-Rank_Prior_for_Non-pointwise_Impulse_Noise_Removal.html">394 iccv-2013-Single-Patch Low-Rank Prior for Non-pointwise Impulse Noise Removal</a></p>
<p>Author: Ruixuan Wang, Emanuele Trucco</p><p>Abstract: This paper introduces a ‘low-rank prior’ for small oriented noise-free image patches: considering an oriented patch as a matrix, a low-rank matrix approximation is enough to preserve the texture details in the properly oriented patch. Based on this prior, we propose a single-patch method within a generalized joint low-rank and sparse matrix recovery framework to simultaneously detect and remove non-pointwise random-valued impulse noise (e.g., very small blobs). A weighting matrix is incorporated in the framework to encode an initial estimate of the spatial noise distribution. An accelerated proximal gradient method is adapted to estimate the optimal noise-free image patches. Experiments show the effectiveness of our framework in removing non-pointwise random-valued impulse noise.</p><p>2 0.91797197 <a title="12-lda-2" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>Author: Qiong Yan, Xiaoyong Shen, Li Xu, Shaojie Zhuo, Xiaopeng Zhang, Liang Shen, Jiaya Jia</p><p>Abstract: Color, infrared, and flash images captured in different fields can be employed to effectively eliminate noise and other visual artifacts. We propose a two-image restoration framework considering input images in different fields, for example, one noisy color image and one dark-flashed nearinfrared image. The major issue in such a framework is to handle structure divergence and find commonly usable edges and smooth transition for visually compelling image reconstruction. We introduce a scale map as a competent representation to explicitly model derivative-level confidence and propose new functions and a numerical solver to effectively infer it following new structural observations. Our method is general and shows a principled way for cross-field restoration.</p><p>3 0.87559807 <a title="12-lda-3" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>Author: Ernesto Brau, Jinyan Guan, Kyle Simek, Luca Del Pero, Colin Reimer Dawson, Kobus Barnard</p><p>Abstract: Jinyan Guan† j guan1 @ emai l ari z ona . edu . Kyle Simek† ks imek@ emai l ari z ona . edu . Colin Reimer Dawson‡ cdaws on@ emai l ari z ona . edu . ‡School of Information University of Arizona Kobus Barnard‡ kobus @ s i sta . ari z ona . edu ∗School of Informatics University of Edinburgh for tracking an unknown and changing number of people in a scene using video taken from a single, fixed viewpoint. We develop a Bayesian modeling approach for tracking people in 3D from monocular video with unknown cameras. Modeling in 3D provides natural explanations for occlusions and smoothness discontinuities that result from projection, and allows priors on velocity and smoothness to be grounded in physical quantities: meters and seconds vs. pixels and frames. We pose the problem in the context of data association, in which observations are assigned to tracks. A correct application of Bayesian inference to multitarget tracking must address the fact that the model’s dimension changes as tracks are added or removed, and thus, posterior densities of different hypotheses are not comparable. We address this by marginalizing out the trajectory parameters so the resulting posterior over data associations has constant dimension. This is made tractable by using (a) Gaussian process priors for smooth trajectories and (b) approximately Gaussian likelihood functions. Our approach provides a principled method for incorporating multiple sources of evidence; we present results using both optical flow and object detector outputs. Results are comparable to recent work on 3D tracking and, unlike others, our method requires no pre-calibrated cameras.</p><p>4 0.86063612 <a title="12-lda-4" href="./iccv-2013-Multiple_Non-rigid_Surface_Detection_and_Registration.html">283 iccv-2013-Multiple Non-rigid Surface Detection and Registration</a></p>
<p>Author: Yi Wu, Yoshihisa Ijiri, Ming-Hsuan Yang</p><p>Abstract: Detecting and registering nonrigid surfaces are two important research problems for computer vision. Much work has been done with the assumption that there exists only one instance in the image. In this work, we propose an algorithm that detects and registers multiple nonrigid instances of given objects in a cluttered image. Specifically, after we use low level feature points to obtain the initial matches between templates and the input image, a novel high-order affinity graph is constructed to model the consistency of local topology. A hierarchical clustering approach is then used to locate the nonrigid surfaces. To remove the outliers in the cluster, we propose a deterministic annealing approach based on the Thin Plate Spline (TPS) model. The proposed method achieves high accuracy even when the number of outliers is nineteen times larger than the inliers. As the matches may appear sparsely in each instance, we propose a TPS based match growing approach to propagate the matches. Finally, an approach that fuses feature and appearance information is proposed to register each nonrigid surface. Extensive experiments and evaluations demonstrate that the proposed algorithm achieves promis- ing results in detecting and registering multiple non-rigid surfaces in a cluttered scene.</p><p>same-paper 5 0.84588099 <a title="12-lda-5" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>Author: Jim Braux-Zin, Romain Dupont, Adrien Bartoli</p><p>Abstract: Dense motion field estimation (typically Romain Dupont1 romain . dupont @ cea . fr Adrien Bartoli2 adrien . bart o l @ gmai l com i . 2 ISIT, Universit e´ d’Auvergne/CNRS, France sions are explicitly modeled [32, 13]. Coarse-to-fine warping improves global convergence by making the assumption that optical flow, the motion of smaller structures is similar to the motion of stereo disparity and surface registration) is a key computer vision problem. Many solutions have been proposed to compute small or large displacements, narrow or wide baseline stereo disparity, but a unified methodology is still lacking. We here introduce a general framework that robustly combines direct and feature-based matching. The feature-based cost is built around a novel robust distance function that handles keypoints and “weak” features such as segments. It allows us to use putative feature matches which may contain mismatches to guide dense motion estimation out of local minima. Our framework uses a robust direct data term (AD-Census). It is implemented with a powerful second order Total Generalized Variation regularization with external and self-occlusion reasoning. Our framework achieves state of the art performance in several cases (standard optical flow benchmarks, wide-baseline stereo and non-rigid surface registration). Our framework has a modular design that customizes to specific application needs.</p><p>6 0.73142064 <a title="12-lda-6" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>7 0.73071039 <a title="12-lda-7" href="./iccv-2013-Joint_Noise_Level_Estimation_from_Personal_Photo_Collections.html">223 iccv-2013-Joint Noise Level Estimation from Personal Photo Collections</a></p>
<p>8 0.72191185 <a title="12-lda-8" href="./iccv-2013-Bayesian_Robust_Matrix_Factorization_for_Image_and_Video_Processing.html">60 iccv-2013-Bayesian Robust Matrix Factorization for Image and Video Processing</a></p>
<p>9 0.71988869 <a title="12-lda-9" href="./iccv-2013-Robust_Non-parametric_Data_Fitting_for_Correspondence_Modeling.html">358 iccv-2013-Robust Non-parametric Data Fitting for Correspondence Modeling</a></p>
<p>10 0.71071351 <a title="12-lda-10" href="./iccv-2013-A_New_Image_Quality_Metric_for_Image_Auto-denoising.html">23 iccv-2013-A New Image Quality Metric for Image Auto-denoising</a></p>
<p>11 0.6825006 <a title="12-lda-11" href="./iccv-2013-A_Robust_Analytical_Solution_to_Isometric_Shape-from-Template_with_Focal_Length_Calibration.html">27 iccv-2013-A Robust Analytical Solution to Isometric Shape-from-Template with Focal Length Calibration</a></p>
<p>12 0.67724216 <a title="12-lda-12" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<p>13 0.67656201 <a title="12-lda-13" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>14 0.67639387 <a title="12-lda-14" href="./iccv-2013-Exploiting_Reflection_Change_for_Automatic_Reflection_Removal.html">151 iccv-2013-Exploiting Reflection Change for Automatic Reflection Removal</a></p>
<p>15 0.66528648 <a title="12-lda-15" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>16 0.66442597 <a title="12-lda-16" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>17 0.66193807 <a title="12-lda-17" href="./iccv-2013-PM-Huber%3A_PatchMatch_with_Huber_Regularization_for_Stereo_Matching.html">304 iccv-2013-PM-Huber: PatchMatch with Huber Regularization for Stereo Matching</a></p>
<p>18 0.66077936 <a title="12-lda-18" href="./iccv-2013-Direct_Optimization_of_Frame-to-Frame_Rotation.html">115 iccv-2013-Direct Optimization of Frame-to-Frame Rotation</a></p>
<p>19 0.66022307 <a title="12-lda-19" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>20 0.65888911 <a title="12-lda-20" href="./iccv-2013-Modeling_Self-Occlusions_in_Dynamic_Shape_and_Appearance_Tracking.html">270 iccv-2013-Modeling Self-Occlusions in Dynamic Shape and Appearance Tracking</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
