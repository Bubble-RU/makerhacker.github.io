<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>135 iccv-2013-Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-135" href="#">iccv2013-135</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>135 iccv-2013-Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</h1>
<br/><p>Source: <a title="iccv-2013-135-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Meng_Efficient_Image_Dehazing_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Gaofeng Meng, Ying Wang, Jiangyong Duan, Shiming Xiang, Chunhong Pan</p><p>Abstract: unkown-abstract</p><p>Reference: <a title="iccv-2013-135-reference" href="../iccv2013_reference/iccv-2013-Efficient_Image_Dehazing_with_Boundary_Constraint_and_Contextual_Regularization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn  Abstract—Images captured in foggy weather conditions often suffer from bad visibility. [sent-6, score-0.215]
</p><p>2 In this paper, we propose an efficient regularization method to remove hazes from a single input image. [sent-7, score-0.271]
</p><p>3 Our method benefits much from an exploration on the inherent boundary constraint on the transmission function. [sent-8, score-0.662]
</p><p>4 Experimental results on a variety of haze images demonstrate the effectiveness and efficiency of the proposed method. [sent-12, score-0.289]
</p><p>5 INTRODUCTION When one takes a picture in foggy weather conditions, the obtained image often suffers from poor visibility. [sent-14, score-0.247]
</p><p>6 This is because the reflected light from these objects, before it reaches the camera, is attenuated in the air and further blended with the atmospheric light scattered by some aerosols (e. [sent-16, score-0.309]
</p><p>7 Early methods for haze removal mainly rely on additional depth information or multiple observations of the same scene. [sent-20, score-0.364]
</p><p>8 [11] notice that the airlight scattered by atmospheric particles is partially polarized. [sent-23, score-0.232]
</p><p>9 Based on this observation, they develop a quick method to reduce hazes by using two images taken through a polarizer at different angles. [sent-24, score-0.228]
</p><p>10 From left to right: (Top) the foggy image and the dehazing result by our method. [sent-32, score-0.668]
</p><p>11 (Bottom) the boundary constraint map and the recovered scene transmission. [sent-33, score-0.306]
</p><p>12 Under the assumption that the two functions are locally statistically uncorrelated, a haze image can be  broken into regions of constant albedo, from which the scene transmission can be inferred. [sent-38, score-0.76]
</p><p>13 Tan [13] proposes to enhance the visibility of a haze image by maximizing its local contrast. [sent-39, score-0.331]
</p><p>14 [5] present an interesting image prior - dark channel prior for single image dehazing. [sent-43, score-0.256]
</p><p>15 [7] model an image as a factorial Markov random field, in which the scene albedo and depth are two statistically independent latent layers. [sent-47, score-0.223]
</p><p>16 Following this idea, we begin our study in this paper by deriving an inherent boundary constraint on the scene transmission. [sent-52, score-0.276]
</p><p>17 This constraint, combined with a weighted L1−norm based contextual regularization between neighboring pixels, is formalized into an optimization problem to recover the  unknown transmission. [sent-53, score-0.194]
</p><p>18 Figure 1 illustrates an example of our dehazing result. [sent-55, score-0.567]
</p><p>19 Our second contribution is a new contextual regularization that enables us to incorporate a filter bank into image dehazing. [sent-59, score-0.211]
</p><p>20 The transmission function t(x) (0 ≤ t(x) ≤ 1) itrsa cnosmrreilssaiteodn . [sent-64, score-0.395]
</p><p>21 Fctuirotnhe tr( assuming xth)a ≤t th 1e) haze is homogenous, we can express t(x) as follows:  +  t(x) =  −  e−βd(x),  (2)  where β is the medium extinction coefficient, and d(x) is the scene depth. [sent-66, score-0.402]
</p><p>22 The goal of image dehazing is to recover the scene radiance J(x) from I(x) based on Eq. [sent-67, score-0.757]
</p><p>23 This requires us to estimate the transmission function t(x) and the global atmospheric light A. [sent-69, score-0.622]
</p><p>24 Once t(x) and A are estimated, the scene radiance can be recovered by:  J(x) =[maIx(x(t)( −x) A,? [sent-70, score-0.234]
</p><p>25 For each x, we require the extrapolation of J(x) cannot cross over the boundary of the radiance cube. [sent-73, score-0.296]
</p><p>26 Jb (x1) and Jb (x2) are the corresponding boundary constraint points. [sent-74, score-0.204]
</p><p>27 However, dehazing from a single image is highly underconstrained, since the number of unknowns is much greater than the number of available equations. [sent-79, score-0.544]
</p><p>28 (1), a pixel I(x) contaminated by fog will be “pushed” towards the global atmospheric light A (see Figure 2). [sent-83, score-0.289]
</p><p>29 (4)  Consider that the scene radiance of a given image is always bounded, that is, C0  ≤ J(x) ≤ C1,∀x  ∈ Ω,  (5)  where C0 and C1 are two constant vectors that are relevant to the given image. [sent-86, score-0.208]
</p><p>30 Consequently, for any x, a natural requirement is that the extrapolation of J(x) must be located in the radiance cube bounded by C0 and C1, as illustrated in Figure 2. [sent-87, score-0.23]
</p><p>31 The above requirement on J(x), in turn, imposes a boundary constraint on t(x). [sent-88, score-0.225]
</p><p>32 Suppose that the global atmospheric light A is given. [sent-89, score-0.227]
</p><p>33 Thus, for each x, we can compute the corresponding boundary constraint point Jb(x) (see Figure  2). [sent-90, score-0.204]
</p><p>34 (5), leading to the following boundary constraint on t(x): 0 ≤ tb(x) ≤ t(x) ≤ 1,  (6)  618  where tb(x) is the lower bound of t(x), given by  tb(x) = min? [sent-93, score-0.204]
</p><p>35 The boundary constraint of t(x) provides a new geometric perspective to the famous dark channel prior [5]. [sent-98, score-0.438]
</p><p>36 Let C0 = 0 and suppose the global atmospheric light A is brighter than any pixel in the haze image. [sent-99, score-0.54]
</p><p>37 (1) by assuming the pixel-wise dark channel of J(x) to be zero. [sent-101, score-0.212]
</p><p>38 Similarly, assuming that the transmission in a local image patch is constant, one can quickly derive the patch-wise transmission t˜(x) in He et al. [sent-102, score-0.842]
</p><p>39 It is worth noting that the boundary constraint is more fundamental. [sent-106, score-0.204]
</p><p>40 In most cases, the optimal global atmospheric light is a little darker than the brightest pixels in the image. [sent-107, score-0.318]
</p><p>41 In these cases, the dark channel prior will fail to those pixels, while the proposed boundary constraint still holds. [sent-111, score-0.438]
</p><p>42 It is also worthy to point out the commonly used constant assumption on the transmission within a local image patch is somewhat demanding. [sent-112, score-0.445]
</p><p>43 For this reason, the patch-wise transmission t˜(x) based on this assumption in [5] is often underestimated. [sent-113, score-0.42]
</p><p>44 The new patch-wise transmission is given as below: tˆ(x) =y m∈ωinxzm∈aωxytb(z). [sent-115, score-0.395]
</p><p>45 (9)  Fortunately, the above patch-wise transmission tˆ(x) can be conveniently computed by directly applying a morphological closing on tb(x). [sent-116, score-0.395]
</p><p>46 Figure 3 illustrates a comparison of the dehazing results by directly using the patch-wise transmissions derived from dark channel prior and the boundary constraint map, respectively. [sent-117, score-1.134]
</p><p>47 One can observe that the patchwise transmission from dark channel prior works not well in the bright sky region. [sent-118, score-0.697]
</p><p>48 In comparison, the new patch-wise transmission derived from the boundary constraint map can handle the bright sky region very well and also produces fewer halo artifacts. [sent-120, score-0.837]
</p><p>49 Based on this assumption, we have derived a patch-wise transmission from the boundary constraint. [sent-123, score-0.532]
</p><p>50 Image dehazing by directly using the patch-wise transmissions from dark channel prior and boundary constraint map, respectively. [sent-125, score-1.086]
</p><p>51 From left to right: (top) the foggy image, the dehazing result by dark channel prior and the dehazing result by boundary constraint. [sent-126, score-1.558]
</p><p>52 (bottom) the boundary constraint map, the patch-wise transmission from dark channel and the patch-wise transmission from boundary constraint map (C0 = (20, 20, 20)T, C1 = (300, 300, 300)T, δ = 1. [sent-127, score-1.41]
</p><p>53 patches with abrupt depth jumps, leading to significant halo artifacts in the dehazing results. [sent-129, score-0.795]
</p><p>54 When W(x, y) = 0, the corresponding contextual  constraint of t(x) between x and y will be canceled. [sent-134, score-0.187]
</p><p>55 Notice the facts that the depth jumps generally appear at the image edges, and that within local patches, pixels with a similar color often share a similar depth value. [sent-139, score-0.287]
</p><p>56 is the log-luminance channel of the image I(x), the exponent α > 0 controls the sensitivity to the luminance difference of two pixels and ? [sent-149, score-0.189]
</p><p>57 619  Integrating the weighted contextual constraints in the whole image domain leads to the following contextual regularization on t(x):  ? [sent-152, score-0.253]
</p><p>58 (3) requires to estimate an appropriate transmission function t(x) and the global atmospheric light A. [sent-197, score-0.622]
</p><p>59 We find an optimal transmission function t(x) by minimizing the following objective function:  2λ? [sent-210, score-0.415]
</p><p>60 t part is the data term, which measures the fidelity of t(x) to the patch-wise transmission tˆ(x) derived from the boundary constraint map, the second part models the contextual constraints of t(x), and λ is the regularization parameter for balancing the two terms. [sent-218, score-0.782]
</p><p>61 More specifically, we introduce the following auxiliary variables, denoted by uj (j ∈ ω) and convert (19) to a new cost function as below: ⎛ ⎞  λ2? [sent-221, score-0.205]
</p><p>62 Minimizing (20) for a fixed β can be performed by an alternating optimization with respect to uj and t. [sent-234, score-0.205]
</p><p>63 That is, we first solve for each optimal uj by fixing t, and then solve for an optimal t by fixing uj . [sent-235, score-0.496]
</p><p>64 620  Optimizing uj : With t fixed in (20), we solve for (j ∈ ω) by minimizing the following function:  ? [sent-238, score-0.205]
</p><p>65 22,  uj  (21)  The above problem consists of solving a series of independent 1D problems of the following forms, i. [sent-242, score-0.205]
</p><p>66 Figure 5 illustrates an example of the estimation process of scene transmission function. [sent-272, score-0.47]
</p><p>67 The intermediate estimations of t(x) and the final dehazing result are shown in the figure. [sent-275, score-0.544]
</p><p>68 Example Results Figure 6 illustrates some examples of our dehazing results and the recovered scene transmission functions. [sent-279, score-1.064]
</p><p>69 As can be seen from the results, our method can recover rich details of images with vivid color information in the haze regions. [sent-288, score-0.389]
</p><p>70 It should be pointed out that the estimated transmissions of the right three images in the figure cannot be regarded as a scaling version of the depth map, since the hazes in the images are not homogeneous. [sent-289, score-0.407]
</p><p>71 Actually, the transmission function reflects the density of the hazes in the captured scene. [sent-291, score-0.623]
</p><p>72 However,  the colors in the recovered images are often over saturated, since the method is not a physically based approach and the transmission may thus be underestimated. [sent-303, score-0.501]
</p><p>73 Moreover, some significant halo artifacts usually appear around the recovered sharp edges (e. [sent-304, score-0.226]
</p><p>74 In comparison, our method can improve the visuality of image structures in very dense haze regions while restoring the faithful colors. [sent-307, score-0.348]
</p><p>75 The halo artifacts in our results are also quite small. [sent-308, score-0.211]
</p><p>76 They estimate the atmospheric veil by applying a fast median filter to the minimum components of the observed image. [sent-313, score-0.191]
</p><p>77 The biggest advantage of their method is its linear complexity and can be implemented in real time, while the weakness is the dehazing results are not quite visually compelling. [sent-314, score-0.599]
</p><p>78 If the haze is very dense, the color information will be very faint and the transmission may thus be wrongly estimated, leading to erroneous enhancement on the image. [sent-316, score-0.792]
</p><p>79 For example, the hill enhanced by Fattal’s method in Figure 8 is too dark (bottom image) and some hazes still remain among the underbrush (top image). [sent-317, score-0.333]
</p><p>80 By exploiting the priors of natural images and depth statistics, they can factorize the image into its scene  albedo and depth via an EM algorithm. [sent-324, score-0.307]
</p><p>81 Moreover, the dehazing results also contain some halo artifacts. [sent-326, score-0.689]
</p><p>82 Based on the hue disparity between the original image and its semi-inverse, they can quickly identify the hazy regions and estimate the global airlight constant and the transmission map. [sent-330, score-0.486]
</p><p>83 However, due to the ambiguity between color and depth, pixel-wise haze detection is not robust and often suffers from large recognition errors. [sent-333, score-0.421]
</p><p>84 Therefore, some hazes in the images are not fully removed (e. [sent-334, score-0.228]
</p><p>85 In contrast, our method can well remove most hazes in the image and produce a clear image with vivid color information. [sent-337, score-0.299]
</p><p>86 As can be seen from the results, the both methods produce comparable results in regions with heavy hazes (e. [sent-340, score-0.228]
</p><p>87 Fewer hazes remain in the our dehazing results and the halo artifacts are also smaller. [sent-346, score-0.948]
</p><p>88 DISCUSSION AND CONCLUSION  In this paper, we have proposed an efficient method to remove hazes from a single image. [sent-353, score-0.228]
</p><p>89 From left to right: input haze images, Tan’s results, our results and the close-up patches of the results,  IFCigCuVre’0 89. [sent-357, score-0.289]
</p><p>90 d F irnom co l eofrt) to right: (top) input haze imag, Kratz et al. [sent-370, score-0.289]
</p><p>91 (Best viewed in color) much from an exploration on the inherent boundary constraint on the transmission function. [sent-376, score-0.642]
</p><p>92 In comparison with the state-of-the-arts, our method can generate quite visually pleasing results with faithful color and finer image details and structures. [sent-379, score-0.178]
</p><p>93 Single image dehazing often suffers from the problem of  ambiguity between image color and depth. [sent-380, score-0.676]
</p><p>94 From a geometric perspective of image dehazing, we have derived a boundary constraint on the transmission from the radiance cube of an image. [sent-385, score-0.802]
</p><p>95 Although the boundary constraint imposes a much weak constraint on the dehazing process, it proves to be surprisingly effective for the dehazing of most natural images, after combined with the contextual regularization. [sent-386, score-1.5]
</p><p>96 More generally, one can employ a tighter radiance envelop, not limited to a cubic shape, to provide a more accurate constraint on the transmissions. [sent-387, score-0.224]
</p><p>97 Another way to address the ambiguity problem is to adopt more sound constraints or develop new image priors, for example, using the scene geometry [2], or directly incorporating the available depth information [6] into the estimation of scene transmission. [sent-389, score-0.234]
</p><p>98 A fast semi-inverse approach to detect and remove the haze from a single image. [sent-401, score-0.289]
</p><p>99 Factorizing scene albedo and depth from a single foggy image. [sent-440, score-0.306]
</p><p>100 An investigation of dehazing effects on image and video coding. [sent-450, score-0.544]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dehazing', 0.544), ('transmission', 0.395), ('haze', 0.289), ('hazes', 0.228), ('uj', 0.205), ('atmospheric', 0.168), ('halo', 0.145), ('radiance', 0.132), ('foggy', 0.124), ('boundary', 0.112), ('ancuti', 0.11), ('kratz', 0.11), ('channel', 0.107), ('dark', 0.105), ('dj', 0.104), ('tarel', 0.104), ('transmissions', 0.104), ('contextual', 0.095), ('constraint', 0.092), ('wj', 0.079), ('depth', 0.075), ('fattal', 0.073), ('tb', 0.067), ('weather', 0.066), ('fog', 0.062), ('light', 0.059), ('faithful', 0.059), ('albedo', 0.055), ('narasimhan', 0.053), ('scene', 0.052), ('extrapolation', 0.052), ('bank', 0.05), ('recovered', 0.05), ('cube', 0.046), ('jb', 0.046), ('weighting', 0.044), ('regularization', 0.043), ('jumps', 0.042), ('visibility', 0.042), ('airlight', 0.041), ('brightest', 0.041), ('dehaze', 0.041), ('djt', 0.041), ('extinction', 0.041), ('factorial', 0.041), ('color', 0.04), ('filters', 0.039), ('sky', 0.038), ('quite', 0.035), ('ambiguity', 0.035), ('erroneous', 0.035), ('enhancement', 0.033), ('wij', 0.032), ('suffers', 0.032), ('tan', 0.032), ('artifacts', 0.031), ('colors', 0.031), ('vivid', 0.031), ('schechner', 0.031), ('enhancements', 0.031), ('bright', 0.03), ('pages', 0.03), ('pixels', 0.03), ('operators', 0.029), ('recover', 0.029), ('differential', 0.028), ('exponent', 0.027), ('neighboring', 0.027), ('kopf', 0.027), ('factorize', 0.027), ('splitting', 0.026), ('quickly', 0.026), ('restore', 0.026), ('patch', 0.026), ('luminance', 0.025), ('derived', 0.025), ('often', 0.025), ('pleasing', 0.024), ('constant', 0.024), ('brighter', 0.024), ('filter', 0.023), ('ic', 0.023), ('illustrates', 0.023), ('fixing', 0.023), ('compelling', 0.023), ('exploration', 0.023), ('scattered', 0.023), ('priors', 0.023), ('saturated', 0.023), ('prior', 0.022), ('imposes', 0.021), ('restoration', 0.021), ('operator', 0.021), ('medium', 0.02), ('inherent', 0.02), ('division', 0.02), ('visually', 0.02), ('constraints', 0.02), ('benefits', 0.02), ('optimal', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999917 <a title="135-tfidf-1" href="./iccv-2013-Efficient_Image_Dehazing_with_Boundary_Constraint_and_Contextual_Regularization.html">135 iccv-2013-Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</a></p>
<p>Author: Gaofeng Meng, Ying Wang, Jiangyong Duan, Shiming Xiang, Chunhong Pan</p><p>Abstract: unkown-abstract</p><p>2 0.12902126 <a title="135-tfidf-2" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>Author: Qiong Yan, Xiaoyong Shen, Li Xu, Shaojie Zhuo, Xiaopeng Zhang, Liang Shen, Jiaya Jia</p><p>Abstract: Color, infrared, and flash images captured in different fields can be employed to effectively eliminate noise and other visual artifacts. We propose a two-image restoration framework considering input images in different fields, for example, one noisy color image and one dark-flashed nearinfrared image. The major issue in such a framework is to handle structure divergence and find commonly usable edges and smooth transition for visually compelling image reconstruction. We introduce a scale map as a competent representation to explicitly model derivative-level confidence and propose new functions and a numerical solver to effectively infer it following new structural observations. Our method is general and shows a principled way for cross-field restoration.</p><p>3 0.091687307 <a title="135-tfidf-3" href="./iccv-2013-Modeling_Self-Occlusions_in_Dynamic_Shape_and_Appearance_Tracking.html">270 iccv-2013-Modeling Self-Occlusions in Dynamic Shape and Appearance Tracking</a></p>
<p>Author: Yanchao Yang, Ganesh Sundaramoorthi</p><p>Abstract: We present a method to track the precise shape of a dynamic object in video. Joint dynamic shape and appearance models, in which a template of the object is propagated to match the object shape and radiance in the next frame, are advantageous over methods employing global image statistics in cases of complex object radiance and cluttered background. In cases of complex 3D object motion and relative viewpoint change, self-occlusions and disocclusions of the object are prominent, and current methods employing joint shape and appearance models are unable to accurately adapt to new shape and appearance information, leading to inaccurate shape detection. In this work, we model self-occlusions and dis-occlusions in a joint shape and appearance tracking framework. Experiments on video exhibiting occlusion/dis-occlusion, complex radiance and background show that occlusion/dis-occlusion modeling leads to superior shape accuracy compared to recent methods employing joint shape/appearance models or employing global statistics.</p><p>4 0.068206556 <a title="135-tfidf-4" href="./iccv-2013-High_Quality_Shape_from_a_Single_RGB-D_Image_under_Uncalibrated_Natural_Illumination.html">199 iccv-2013-High Quality Shape from a Single RGB-D Image under Uncalibrated Natural Illumination</a></p>
<p>Author: Yudeog Han, Joon-Young Lee, In So Kweon</p><p>Abstract: We present a novel framework to estimate detailed shape of diffuse objects with uniform albedo from a single RGB-D image. To estimate accurate lighting in natural illumination environment, we introduce a general lighting model consisting oftwo components: global and local models. The global lighting model is estimated from the RGB-D input using the low-dimensional characteristic of a diffuse reflectance model. The local lighting model represents spatially varying illumination and it is estimated by using the smoothlyvarying characteristic of illumination. With both the global and local lighting model, we can estimate complex lighting variations in uncontrolled natural illumination conditions accurately. For high quality shape capture, a shapefrom-shading approach is applied with the estimated lighting model. Since the entire process is done with a single RGB-D input, our method is capable of capturing the high quality shape details of a dynamic object under natural illumination. Experimental results demonstrate the feasibility and effectiveness of our method that dramatically improves shape details of the rough depth input.</p><p>5 0.060492083 <a title="135-tfidf-5" href="./iccv-2013-A_Joint_Intensity_and_Depth_Co-sparse_Analysis_Model_for_Depth_Map_Super-resolution.html">18 iccv-2013-A Joint Intensity and Depth Co-sparse Analysis Model for Depth Map Super-resolution</a></p>
<p>Author: Martin Kiechle, Simon Hawe, Martin Kleinsteuber</p><p>Abstract: High-resolution depth maps can be inferred from lowresolution depth measurements and an additional highresolution intensity image of the same scene. To that end, we introduce a bimodal co-sparse analysis model, which is able to capture the interdependency of registered intensity . go l e i um . de . .t ities together with the knowledge of the relative positions between all views. Despite very active research in this area and significant improvements over the past years, stereo methods still struggle with noise, texture-less regions, repetitive texture, and occluded areas. For an overview of stereo methods, the reader is referred to [25]. and depth information. This model is based on the assumption that the co-supports of corresponding bimodal image structures are aligned when computed by a suitable pair of analysis operators. No analytic form of such operators ex- ist and we propose a method for learning them from a set of registered training signals. This learning process is done offline and returns a bimodal analysis operator that is universally applicable to natural scenes. We use this to exploit the bimodal co-sparse analysis model as a prior for solving inverse problems, which leads to an efficient algorithm for depth map super-resolution.</p><p>6 0.057610739 <a title="135-tfidf-6" href="./iccv-2013-A_Simple_Model_for_Intrinsic_Image_Decomposition_with_Depth_Cues.html">30 iccv-2013-A Simple Model for Intrinsic Image Decomposition with Depth Cues</a></p>
<p>7 0.055250928 <a title="135-tfidf-7" href="./iccv-2013-Image_Guided_Depth_Upsampling_Using_Anisotropic_Total_Generalized_Variation.html">209 iccv-2013-Image Guided Depth Upsampling Using Anisotropic Total Generalized Variation</a></p>
<p>8 0.055005595 <a title="135-tfidf-8" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>9 0.051416207 <a title="135-tfidf-9" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>10 0.049302109 <a title="135-tfidf-10" href="./iccv-2013-Incorporating_Cloud_Distribution_in_Sky_Representation.html">215 iccv-2013-Incorporating Cloud Distribution in Sky Representation</a></p>
<p>11 0.047508307 <a title="135-tfidf-11" href="./iccv-2013-Holistic_Scene_Understanding_for_3D_Object_Detection_with_RGBD_Cameras.html">201 iccv-2013-Holistic Scene Understanding for 3D Object Detection with RGBD Cameras</a></p>
<p>12 0.044627614 <a title="135-tfidf-12" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>13 0.044357762 <a title="135-tfidf-13" href="./iccv-2013-Efficient_Hand_Pose_Estimation_from_a_Single_Depth_Image.html">133 iccv-2013-Efficient Hand Pose Estimation from a Single Depth Image</a></p>
<p>14 0.043911431 <a title="135-tfidf-14" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>15 0.043895796 <a title="135-tfidf-15" href="./iccv-2013-Depth_from_Combining_Defocus_and_Correspondence_Using_Light-Field_Cameras.html">108 iccv-2013-Depth from Combining Defocus and Correspondence Using Light-Field Cameras</a></p>
<p>16 0.043705322 <a title="135-tfidf-16" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>17 0.041867305 <a title="135-tfidf-17" href="./iccv-2013-Characterizing_Layouts_of_Outdoor_Scenes_Using_Spatial_Topic_Processes.html">72 iccv-2013-Characterizing Layouts of Outdoor Scenes Using Spatial Topic Processes</a></p>
<p>18 0.040522981 <a title="135-tfidf-18" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>19 0.039694794 <a title="135-tfidf-19" href="./iccv-2013-Deblurring_by_Example_Using_Dense_Correspondence.html">103 iccv-2013-Deblurring by Example Using Dense Correspondence</a></p>
<p>20 0.039608356 <a title="135-tfidf-20" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.106), (1, -0.06), (2, -0.016), (3, -0.002), (4, -0.014), (5, -0.003), (6, -0.005), (7, -0.04), (8, -0.005), (9, -0.046), (10, -0.009), (11, -0.014), (12, 0.011), (13, 0.014), (14, -0.006), (15, -0.018), (16, -0.035), (17, -0.033), (18, -0.011), (19, 0.006), (20, -0.034), (21, 0.038), (22, -0.009), (23, -0.013), (24, -0.039), (25, 0.044), (26, 0.033), (27, -0.012), (28, 0.017), (29, -0.024), (30, 0.026), (31, 0.023), (32, 0.032), (33, 0.077), (34, -0.011), (35, 0.008), (36, -0.021), (37, 0.018), (38, 0.025), (39, 0.018), (40, -0.007), (41, 0.018), (42, 0.048), (43, -0.016), (44, -0.047), (45, 0.035), (46, 0.034), (47, 0.054), (48, -0.051), (49, -0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90421182 <a title="135-lsi-1" href="./iccv-2013-Efficient_Image_Dehazing_with_Boundary_Constraint_and_Contextual_Regularization.html">135 iccv-2013-Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</a></p>
<p>Author: Gaofeng Meng, Ying Wang, Jiangyong Duan, Shiming Xiang, Chunhong Pan</p><p>Abstract: unkown-abstract</p><p>2 0.75345665 <a title="135-lsi-2" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>Author: Qiong Yan, Xiaoyong Shen, Li Xu, Shaojie Zhuo, Xiaopeng Zhang, Liang Shen, Jiaya Jia</p><p>Abstract: Color, infrared, and flash images captured in different fields can be employed to effectively eliminate noise and other visual artifacts. We propose a two-image restoration framework considering input images in different fields, for example, one noisy color image and one dark-flashed nearinfrared image. The major issue in such a framework is to handle structure divergence and find commonly usable edges and smooth transition for visually compelling image reconstruction. We introduce a scale map as a competent representation to explicitly model derivative-level confidence and propose new functions and a numerical solver to effectively infer it following new structural observations. Our method is general and shows a principled way for cross-field restoration.</p><p>3 0.68219393 <a title="135-lsi-3" href="./iccv-2013-A_Simple_Model_for_Intrinsic_Image_Decomposition_with_Depth_Cues.html">30 iccv-2013-A Simple Model for Intrinsic Image Decomposition with Depth Cues</a></p>
<p>Author: Qifeng Chen, Vladlen Koltun</p><p>Abstract: We present a model for intrinsic decomposition of RGB-D images. Our approach analyzes a single RGB-D image and estimates albedo and shading fields that explain the input. To disambiguate the problem, our model estimates a number of components that jointly account for the reconstructed shading. By decomposing the shading field, we can build in assumptions about image formation that help distinguish reflectance variation from shading. These assumptions are expressed as simple nonlocal regularizers. We evaluate the model on real-world images and on a challenging synthetic dataset. The experimental results demonstrate that the presented approach outperforms prior models for intrinsic decomposition of RGB-D images.</p><p>4 0.67746937 <a title="135-lsi-4" href="./iccv-2013-A_Joint_Intensity_and_Depth_Co-sparse_Analysis_Model_for_Depth_Map_Super-resolution.html">18 iccv-2013-A Joint Intensity and Depth Co-sparse Analysis Model for Depth Map Super-resolution</a></p>
<p>Author: Martin Kiechle, Simon Hawe, Martin Kleinsteuber</p><p>Abstract: High-resolution depth maps can be inferred from lowresolution depth measurements and an additional highresolution intensity image of the same scene. To that end, we introduce a bimodal co-sparse analysis model, which is able to capture the interdependency of registered intensity . go l e i um . de . .t ities together with the knowledge of the relative positions between all views. Despite very active research in this area and significant improvements over the past years, stereo methods still struggle with noise, texture-less regions, repetitive texture, and occluded areas. For an overview of stereo methods, the reader is referred to [25]. and depth information. This model is based on the assumption that the co-supports of corresponding bimodal image structures are aligned when computed by a suitable pair of analysis operators. No analytic form of such operators ex- ist and we propose a method for learning them from a set of registered training signals. This learning process is done offline and returns a bimodal analysis operator that is universally applicable to natural scenes. We use this to exploit the bimodal co-sparse analysis model as a prior for solving inverse problems, which leads to an efficient algorithm for depth map super-resolution.</p><p>5 0.66690689 <a title="135-lsi-5" href="./iccv-2013-Image_Guided_Depth_Upsampling_Using_Anisotropic_Total_Generalized_Variation.html">209 iccv-2013-Image Guided Depth Upsampling Using Anisotropic Total Generalized Variation</a></p>
<p>Author: David Ferstl, Christian Reinbacher, Rene Ranftl, Matthias Ruether, Horst Bischof</p><p>Abstract: In this work we present a novel method for the challenging problem of depth image upsampling. Modern depth cameras such as Kinect or Time of Flight cameras deliver dense, high quality depth measurements but are limited in their lateral resolution. To overcome this limitation we formulate a convex optimization problem using higher order regularization for depth image upsampling. In this optimization an anisotropic diffusion tensor, calculated from a high resolution intensity image, is used to guide the upsampling. We derive a numerical algorithm based on a primaldual formulation that is efficiently parallelized and runs at multiple frames per second. We show that this novel upsampling clearly outperforms state of the art approaches in terms of speed and accuracy on the widely used Middlebury 2007 datasets. Furthermore, we introduce novel datasets with highly accurate groundtruth, which, for the first time, enable to benchmark depth upsampling methods using real sensor data.</p><p>6 0.65878248 <a title="135-lsi-6" href="./iccv-2013-Depth_from_Combining_Defocus_and_Correspondence_Using_Light-Field_Cameras.html">108 iccv-2013-Depth from Combining Defocus and Correspondence Using Light-Field Cameras</a></p>
<p>7 0.60821134 <a title="135-lsi-7" href="./iccv-2013-High_Quality_Shape_from_a_Single_RGB-D_Image_under_Uncalibrated_Natural_Illumination.html">199 iccv-2013-High Quality Shape from a Single RGB-D Image under Uncalibrated Natural Illumination</a></p>
<p>8 0.59556496 <a title="135-lsi-8" href="./iccv-2013-Illuminant_Chromaticity_from_Image_Sequences.html">207 iccv-2013-Illuminant Chromaticity from Image Sequences</a></p>
<p>9 0.59548777 <a title="135-lsi-9" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>10 0.59338188 <a title="135-lsi-10" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<p>11 0.58596528 <a title="135-lsi-11" href="./iccv-2013-Structured_Light_in_Sunlight.html">405 iccv-2013-Structured Light in Sunlight</a></p>
<p>12 0.58169788 <a title="135-lsi-12" href="./iccv-2013-Modeling_the_Calibration_Pipeline_of_the_Lytro_Camera_for_High_Quality_Light-Field_Image_Reconstruction.html">271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</a></p>
<p>13 0.56435382 <a title="135-lsi-13" href="./iccv-2013-Incorporating_Cloud_Distribution_in_Sky_Representation.html">215 iccv-2013-Incorporating Cloud Distribution in Sky Representation</a></p>
<p>14 0.56242239 <a title="135-lsi-14" href="./iccv-2013-SGTD%3A_Structure_Gradient_and_Texture_Decorrelating_Regularization_for_Image_Decomposition.html">364 iccv-2013-SGTD: Structure Gradient and Texture Decorrelating Regularization for Image Decomposition</a></p>
<p>15 0.5613125 <a title="135-lsi-15" href="./iccv-2013-A_Color_Constancy_Model_with_Double-Opponency_Mechanisms.html">5 iccv-2013-A Color Constancy Model with Double-Opponency Mechanisms</a></p>
<p>16 0.56090111 <a title="135-lsi-16" href="./iccv-2013-DCSH_-_Matching_Patches_in_RGBD_Images.html">101 iccv-2013-DCSH - Matching Patches in RGBD Images</a></p>
<p>17 0.5434081 <a title="135-lsi-17" href="./iccv-2013-Shape_Index_Descriptors_Applied_to_Texture-Based_Galaxy_Analysis.html">388 iccv-2013-Shape Index Descriptors Applied to Texture-Based Galaxy Analysis</a></p>
<p>18 0.52811718 <a title="135-lsi-18" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>19 0.52059489 <a title="135-lsi-19" href="./iccv-2013-Detecting_Irregular_Curvilinear_Structures_in_Gray_Scale_and_Color_Imagery_Using_Multi-directional_Oriented_Flux.html">112 iccv-2013-Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-directional Oriented Flux</a></p>
<p>20 0.51531279 <a title="135-lsi-20" href="./iccv-2013-A_Generalized_Low-Rank_Appearance_Model_for_Spatio-temporally_Correlated_Rain_Streaks.html">15 iccv-2013-A Generalized Low-Rank Appearance Model for Spatio-temporally Correlated Rain Streaks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.043), (7, 0.012), (25, 0.333), (26, 0.055), (31, 0.047), (35, 0.014), (40, 0.016), (42, 0.074), (48, 0.022), (64, 0.047), (73, 0.057), (89, 0.153), (98, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70776945 <a title="135-lda-1" href="./iccv-2013-Efficient_Image_Dehazing_with_Boundary_Constraint_and_Contextual_Regularization.html">135 iccv-2013-Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</a></p>
<p>Author: Gaofeng Meng, Ying Wang, Jiangyong Duan, Shiming Xiang, Chunhong Pan</p><p>Abstract: unkown-abstract</p><p>2 0.67291397 <a title="135-lda-2" href="./iccv-2013-Parallel_Transport_of_Deformations_in_Shape_Space_of_Elastic_Surfaces.html">307 iccv-2013-Parallel Transport of Deformations in Shape Space of Elastic Surfaces</a></p>
<p>Author: Qian Xie, Sebastian Kurtek, Huiling Le, Anuj Srivastava</p><p>Abstract: Statistical shape analysis develops methods for comparisons, deformations, summarizations, and modeling of shapes in given data sets. These tasks require afundamental tool called parallel transport of tangent vectors along arbitrary paths. This tool is essential for: (1) computation of geodesic paths using either shooting or path-straightening method, (2) transferring deformations across objects, and (3) modeling of statistical variability in shapes. Using the square-root normal field (SRNF) representation of parameterized surfaces, we present a method for transporting deformations along paths in the shape space. This is difficult despite the underlying space being a vector space because the chosen (elastic) Riemannian metric is non-standard. Using a finite-basis for representing SRNFs of shapes, we derive expressions for Christoffel symbols that enable parallel transports. We demonstrate this framework using examples from shape analysis of parameterized spherical sur- faces, in the three contexts mentioned above.</p><p>3 0.64835626 <a title="135-lda-3" href="./iccv-2013-A_Simple_Model_for_Intrinsic_Image_Decomposition_with_Depth_Cues.html">30 iccv-2013-A Simple Model for Intrinsic Image Decomposition with Depth Cues</a></p>
<p>Author: Qifeng Chen, Vladlen Koltun</p><p>Abstract: We present a model for intrinsic decomposition of RGB-D images. Our approach analyzes a single RGB-D image and estimates albedo and shading fields that explain the input. To disambiguate the problem, our model estimates a number of components that jointly account for the reconstructed shading. By decomposing the shading field, we can build in assumptions about image formation that help distinguish reflectance variation from shading. These assumptions are expressed as simple nonlocal regularizers. We evaluate the model on real-world images and on a challenging synthetic dataset. The experimental results demonstrate that the presented approach outperforms prior models for intrinsic decomposition of RGB-D images.</p><p>4 0.63597113 <a title="135-lda-4" href="./iccv-2013-Image_Segmentation_with_Cascaded_Hierarchical_Models_and_Logistic_Disjunctive_Normal_Networks.html">211 iccv-2013-Image Segmentation with Cascaded Hierarchical Models and Logistic Disjunctive Normal Networks</a></p>
<p>Author: Mojtaba Seyedhosseini, Mehdi Sajjadi, Tolga Tasdizen</p><p>Abstract: Contextual information plays an important role in solving vision problems such as image segmentation. However, extracting contextual information and using it in an effective way remains a difficult problem. To address this challenge, we propose a multi-resolution contextual framework, called cascaded hierarchical model (CHM), which learns contextual information in a hierarchical framework for image segmentation. At each level of the hierarchy, a classifier is trained based on downsampled input images and outputs of previous levels. Our model then incorporates the resulting multi-resolution contextual information into a classifier to segment the input image at original resolution. We repeat this procedure by cascading the hierarchical framework to improve the segmentation accuracy. Multiple classifiers are learned in the CHM; therefore, a fast and accurate classifier is required to make the training tractable. The classifier also needs to be robust against overfitting due to the large number of parameters learned during training. We introduce a novel classification scheme, called logistic dis- junctive normal networks (LDNN), which consists of one adaptive layer of feature detectors implemented by logistic sigmoid functions followed by two fixed layers of logical units that compute conjunctions and disjunctions, respectively. We demonstrate that LDNN outperforms state-of-theart classifiers and can be used in the CHM to improve object segmentation performance.</p><p>5 0.63522977 <a title="135-lda-5" href="./iccv-2013-Learning_CRFs_for_Image_Parsing_with_Adaptive_Subgradient_Descent.html">234 iccv-2013-Learning CRFs for Image Parsing with Adaptive Subgradient Descent</a></p>
<p>Author: Honghui Zhang, Jingdong Wang, Ping Tan, Jinglu Wang, Long Quan</p><p>Abstract: We propose an adaptive subgradient descent method to efficiently learn the parameters of CRF models for image parsing. To balance the learning efficiency and performance of the learned CRF models, the parameter learning is iteratively carried out by solving a convex optimization problem in each iteration, which integrates a proximal term to preserve the previously learned information and the large margin preference to distinguish bad labeling and the ground truth labeling. A solution of subgradient descent updating form is derived for the convex optimization problem, with an adaptively determined updating step-size. Besides, to deal with partially labeled training data, we propose a new objective constraint modeling both the labeled and unlabeled parts in the partially labeled training data for the parameter learning of CRF models. The superior learning efficiency of the proposed method is verified by the experiment results on two public datasets. We also demonstrate the powerfulness of our method for handling partially labeled training data.</p><p>6 0.57272208 <a title="135-lda-6" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>7 0.54101616 <a title="135-lda-7" href="./iccv-2013-Perceptual_Fidelity_Aware_Mean_Squared_Error.html">312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</a></p>
<p>8 0.52587336 <a title="135-lda-8" href="./iccv-2013-High_Quality_Shape_from_a_Single_RGB-D_Image_under_Uncalibrated_Natural_Illumination.html">199 iccv-2013-High Quality Shape from a Single RGB-D Image under Uncalibrated Natural Illumination</a></p>
<p>9 0.51063102 <a title="135-lda-9" href="./iccv-2013-Bayesian_Robust_Matrix_Factorization_for_Image_and_Video_Processing.html">60 iccv-2013-Bayesian Robust Matrix Factorization for Image and Video Processing</a></p>
<p>10 0.50939578 <a title="135-lda-10" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>11 0.50786471 <a title="135-lda-11" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>12 0.50739223 <a title="135-lda-12" href="./iccv-2013-Exploiting_Reflection_Change_for_Automatic_Reflection_Removal.html">151 iccv-2013-Exploiting Reflection Change for Automatic Reflection Removal</a></p>
<p>13 0.50646085 <a title="135-lda-13" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<p>14 0.50555927 <a title="135-lda-14" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>15 0.50554323 <a title="135-lda-15" href="./iccv-2013-Joint_Deep_Learning_for_Pedestrian_Detection.html">220 iccv-2013-Joint Deep Learning for Pedestrian Detection</a></p>
<p>16 0.50504547 <a title="135-lda-16" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<p>17 0.50490594 <a title="135-lda-17" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<p>18 0.50443017 <a title="135-lda-18" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>19 0.50442934 <a title="135-lda-19" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>20 0.50407279 <a title="135-lda-20" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
