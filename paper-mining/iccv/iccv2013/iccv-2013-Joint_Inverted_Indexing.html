<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>221 iccv-2013-Joint Inverted Indexing</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-221" href="#">iccv2013-221</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>221 iccv-2013-Joint Inverted Indexing</h1>
<br/><p>Source: <a title="iccv-2013-221-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Xia_Joint_Inverted_Indexing_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Yan Xia, Kaiming He, Fang Wen, Jian Sun</p><p>Abstract: Inverted indexing is a popular non-exhaustive solution to large scale search. An inverted file is built by a quantizer such as k-means or a tree structure. It has been found that multiple inverted files, obtained by multiple independent random quantizers, are able to achieve practically good recall and speed. Instead of computing the multiple quantizers independently, we present a method that creates them jointly. Our method jointly optimizes all codewords in all quantizers. Then it assigns these codewords to the quantizers. In experiments this method shows significant improvement over various existing methods that use multiple independent quantizers. On the one-billion set of SIFT vectors, our method is faster and more accurate than a recent state-of-the-art inverted indexing method.</p><p>Reference: <a title="iccv-2013-221-reference" href="../iccv2013_reference/iccv-2013-Joint_Inverted_Indexing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 An inverted file is built by a quantizer such as k-means or a tree structure. [sent-2, score-0.727]
</p><p>2 It has been found that multiple inverted files, obtained by multiple independent random quantizers, are able to achieve practically good recall and speed. [sent-3, score-0.248]
</p><p>3 Instead of computing the multiple quantizers independently, we present a method that creates them jointly. [sent-4, score-0.479]
</p><p>4 Our method jointly optimizes all codewords in all quantizers. [sent-5, score-0.524]
</p><p>5 On the one-billion set of SIFT vectors, our method is faster and more accurate than a recent state-of-the-art inverted indexing method. [sent-8, score-0.295]
</p><p>6 The inverted indexing is built as a codebook of the codewords. [sent-20, score-0.312]
</p><p>7 Given a query vector, the system finds the codeword of the query and checks those items in the list of this codeword. [sent-22, score-0.298]
</p><p>8 An effective way to improve the recall of inverted indexing methods is to use multiple quantizers (also known as multiple hash tables [8, 26]). [sent-25, score-0.876]
</p><p>9 In LSH each quantizer (hash table) is binning a low dimensional random subspace. [sent-29, score-0.475]
</p><p>10 In the spirit of hash, all the above multi-quantizer methods create the quantizers independently and randomly. [sent-35, score-0.5]
</p><p>11 The KLSH method [26] enjoys an advantage that its individual quantizer is (locally) optimal in the sense of minimizing quantization error [14]. [sent-36, score-0.522]
</p><p>12 But we point out that for KLSH, the codewords from different codebooks tend to be similar. [sent-37, score-0.499]
</p><p>13 This is because the k-means algorithm tends to put the codewords around densely distributed data even in different random trials. [sent-38, score-0.499]
</p><p>14 Actually, the codewords can be different from each other only because of the local optimality of the kmeans nature. [sent-39, score-0.528]
</p><p>15 The similar codewords would reduce the redundant coverage and limit the gain of multiple quantizers. [sent-40, score-0.499]
</p><p>16 Suppose we want two quantizers each with two codewords. [sent-43, score-0.479]
</p><p>17 If each quantizer is constructed randomly and independently as in KLSH, the two codewords in a quantizer will always be placed near the two peaks. [sent-44, score-1.448]
</p><p>18 The space partitioning of the two quantizers are almost the same (Fig. [sent-45, score-0.515]
</p><p>19 1 left), and the gain of having the  second quantizer is lost. [sent-46, score-0.446]
</p><p>20 Unless specified, in this paper we refer to LSH as a multiplequantizer method that uses inverted indexing, as it was described in [8, 2]. [sent-49, score-0.242]
</p><p>21 33440169  biiuitsdtonrc1c3c2c4data c1c3c2c4 quantizce1r 1qc2 quantizce1r 1qc2 quantizerc 32qc4 quantizer c23qc4  KLSH Joint Figure 1. [sent-50, score-0.446]
</p><p>22 We illustrate two quantizers with two codewords in each quantizer. [sent-53, score-0.978]
</p><p>23 Top: the locations of the four codewords (c1 to c4). [sent-54, score-0.499]
</p><p>24 Middle and Bottom: quantizer 1 and quantizer 2, with the partitioning boundaries. [sent-55, score-0.928]
</p><p>25 When a query (q) is near any partitioning boundary, the independent quantizers may miss many true neighbors, while the joint quantizers can retrieve more true neighbors because the query is well inside at least one partition. [sent-56, score-1.277]
</p><p>26 In the above example, we can instead generate all the four codewords jointly via a single pass of k-means clustering. [sent-57, score-0.551]
</p><p>27 These four codewords are distant from each other (Fig. [sent-58, score-0.499]
</p><p>28 Then we construct each quantizer by mutualexclusively selecting two codewords with some care, and the two resulting quantizers can be substantially different. [sent-60, score-1.439]
</p><p>29 Any query that lies near the partitioning boundary of one quantizer can still find its k-NNs through the other quantizer, and the recall is improved. [sent-61, score-0.598]
</p><p>30 With this motivation, in this paper we propose joint inverted indexing - a novel multiple-quantizer method. [sent-62, score-0.349]
</p><p>31 Rather than construct quantizers independently, we create them jointly. [sent-63, score-0.479]
</p><p>32 We first jointly optimize all the codewords of all quantizers. [sent-65, score-0.539]
</p><p>33 The optimized codewords are then assigned to the quantizers, with a consideration that the total distortion of all quantizers is small. [sent-66, score-1.129]
</p><p>34 When searching in one billion items for the first nearest neighbor of a query, our method takes less than ten milliseconds using a single thread and achieves over 90% recall in the first retrieved one hundred items (de-  tailed in Sec. [sent-69, score-0.227]
</p><p>35 In this paper, we focus on non-exhaustive search based on inverted indexing. [sent-80, score-0.235]
</p><p>36 Inverted indexing could be built using a quantizer like k-means [22], a binary tree [10, 7], hierarchical k-means [11, 24], or a lattice [1]. [sent-81, score-0.571]
</p><p>37 The k-means quantizer is (locally) optimal in terms of quantization distortion [14]. [sent-83, score-0.624]
</p><p>38 It is an attractive way to use multiple quantizers to improve search accuracy. [sent-84, score-0.514]
</p><p>39 For each single hash table, an inverted indexing system is built with each bin as an index, and each index has a list containing all the vectors in the bin. [sent-86, score-0.397]
</p><p>40 But as discussed in the introduction, the codewords in the multiple quantizers can be similar in random trials. [sent-95, score-0.978]
</p><p>41 This is actually a single quantizer method, whose quantizer is the Cartesian product of multiple subquantizers. [sent-97, score-0.926]
</p><p>42 The finer quantization, in conjunction with a softassignment scheme, has shown great superiority to a single k-means quantizer [3]. [sent-102, score-0.473]
</p><p>43 Formulation Denote the number of quantizers as L, and the number of codewords in each quantizer as K. [sent-109, score-1.424]
</p><p>44 Background It is well-known that a k-means quantizer attempts to minimize the quantization distortion [14]: ? [sent-113, score-0.624]
</p><p>45 -a Tlhieke d iasltgoortriitohnm in: (a1l)te crannati bvee olyp tuimpdizaetde tvhiea codewords {ck} and assign data to the clusters {Ck}. [sent-125, score-0.499]
</p><p>46 If all the codewords {clk | 1 ≤ k ≤ K, 1 ≤ la n≤d cLo}d are ridn. [sent-138, score-0.499]
</p><p>47 If it were not for the local optimality of k-means clustering, all the quantizers should be identical. [sent-141, score-0.479]
</p><p>48 Joint Codewords Optimization In (2) the codewords in one quantizer are unaware of those in other quantizers. [sent-142, score-0.945]
</p><p>49 So the independent optimization of any two quantizers may push the codewords towards similar positions due to the kmeans nature. [sent-143, score-1.007]
</p><p>50 To overcome this issue, we propose to optimize all the codewords jointly. [sent-144, score-0.514]
</p><p>51 We notice that in multiple inverted files, it is sufficient for a true datum to be correctly retrieved if it is covered by one of the L lists. [sent-145, score-0.283]
</p><p>52 To jointly optimize all codewords of all quantizers, we propose to minimize the sum of the smallest distortion of all data: ? [sent-150, score-0.655]
</p><p>53 A  =  naive way is to randomly assign the codewords in C without replacement2 to the L quantizers. [sent-178, score-0.535]
</p><p>54 This implies that the joint codewords optimization is an effective method for multiple quantizers. [sent-183, score-0.578]
</p><p>55 This is due to two nice properties of joint codewords optimization: (i) each x is expected to be accurately quantized at least once, in the sense of optimizing (4); (ii) the codewords among the quantizers are guaranteed to be sufficiently different from each other. [sent-184, score-1.531]
</p><p>56 Notice that the quantization distortion of any individual quantizer has not been considered in (4). [sent-189, score-0.624]
</p><p>57 For a better assignment, we consider the total distortion of all quantizers subject to a constraint. [sent-190, score-0.621]
</p><p>58 We illustrate L=3 quantizers with K=8 codewords in each quantizer. [sent-208, score-0.978]
</p><p>59 All L K codewords in all quantizers are generated simultaneously by k-means clustering. [sent-211, score-0.978]
</p><p>60 quantizer i sa constructed by randomly selecting one codeword from each group without replacement. [sent-219, score-0.559]
</p><p>61 The codewords in quantizer 1, 2, 3 are represented  by ? [sent-220, score-0.945]
</p><p>62 The objective function in (6) is the total distortion of all  ××  quantizers as in (2). [sent-225, score-0.6]
</p><p>63 The constraint in (7) means that the codewords must be mutual-exclusively selected from the fixed C. [sent-226, score-0.499]
</p><p>64 As such, the codewords must be subject to the jointly optimized codewords in (4). [sent-227, score-1.074]
</p><p>65 Because the set C has been fixed, the codewords cannot be arbitrarily updated (e. [sent-229, score-0.499]
</p><p>66 L K codewords are generated by a single pass oerfa t kio-mn. [sent-244, score-0.526]
</p><p>67 We divide the L K codewords into K groups (each group c doinvtiadiens t Le Lc×odKewords) (Fig. [sent-249, score-0.499]
</p><p>68 We group the codewords by a random projection tree [9]. [sent-252, score-0.526]
</p><p>69 A K-codeword quantizer is generated by randomly selecting one codeword from each of the K groups without replacement. [sent-254, score-0.559]
</p><p>70 We repeat this for L times and construct all the L quantizers (Fig. [sent-255, score-0.479]
</p><p>71 Discussions:  ×  Intuitively, to make a quantizer with small distortion, we expect the K codewords of this quantizer to be dispersed. [sent-257, score-1.391]
</p><p>72 So we group the codewords by a second pass of clustering (in Step 2, by a tree), and randomly select one codeword from each group to form a quantizer (in Step 3). [sent-258, score-1.085]
</p><p>73 As such, we can avoid the codewords in any quantizer from being too  Joint (rJaonidn-tas ign)99. [sent-259, score-0.945]
</p><p>74 We can expect such a quantizer to have smaller distortion than a quantizer that randomly selects from the whole set C (as in “Joint (rand-assign)”). [sent-274, score-1.016]
</p><p>75 Table 1 also gives the standard deviations (std) of total distortion in 10 random trails for both methods (all trails subject to the same constraint). [sent-277, score-0.242]
</p><p>76 We have also tried to optimize the total distortion by greedily swapping pairs of codewords between quantizers. [sent-279, score-0.635]
</p><p>77 For this database we build 3 quantizers with 8 codewords in each quantizer (L = 3, K = 8). [sent-288, score-1.424]
</p><p>78 We can see that in KLSH some codewords of different quantizers are very similar. [sent-289, score-0.978]
</p><p>79 4(a) we show the be33441192  c od siedsmiwemwilao lra rdsds  ×  quantizer 1 quantizer 2 quantizer 3 quantizer 1 quantizer 2 quantizer 3 (a) KLSH (b) Joint Figure 3. [sent-291, score-2.676]
</p><p>80 Top: the locations of all codewords in all quantizers. [sent-295, score-0.499]
</p><p>81 The codewords in quantizer 1, 2, 3 are represented by ? [sent-298, score-0.945]
</p><p>82 ), the cell where the query lies in each quantizer is shown by the partitioning boundary. [sent-306, score-0.55]
</p><p>83 havior of these quantizers to a typical query point: due to the similar codewords, the gain of the multiple quantizers is limited. [sent-308, score-1.026]
</p><p>84 In contrast, all the codewords are jointly optimized by our method (Fig. [sent-310, score-0.554]
</p><p>85 Both KLSH and our method should scan all codewords given a query. [sent-496, score-0.499]
</p><p>86 This can be time-consuming when the number of codewords is large, e. [sent-497, score-0.499]
</p><p>87 5 all methods use the same number of quantizers (L). [sent-512, score-0.479]
</p><p>88 5 shows that our joint quantizers outperform KLSH and other alternatives. [sent-520, score-0.533]
</p><p>89 This implies KLSH benefits from the k-means quantizers that give smaller distortion than the other independent quantizers. [sent-523, score-0.606]
</p><p>90 assign)” is a way that has jointly optimized codewords but has larger total distortion than the “Joint” way (Table 1). [sent-580, score-0.703]
</p><p>91 This implies the jointly optimized codewords are very essential for good performance. [sent-583, score-0.579]
</p><p>92 7 we investigate the recall versus the number of quantizers (L) at a given number N. [sent-588, score-0.513]
</p><p>93 Comparisons with Inverted Multi-Index [3] We also compare with inverted multi-index [3], a recent state-ofthe-art inverted indexing method. [sent-596, score-0.509]
</p><p>94 2, this method is actually a single quantizer method with finer quantization and soft-assignments. [sent-598, score-0.563]
</p><p>95 Because it can be more complicate to encode residual vectors in multiple quantizers (KLSH and ours), we simply use PQ to encode the original vector (it is observed that encoding the original vector is inferior [3]). [sent-608, score-0.531]
</p><p>96 Both KLSH and our method use L=16 quantizers here. [sent-615, score-0.479]
</p><p>97 But because the short lists in its finer quantizer are “too short”, this operation retrieves a very large number oflists and takes more time (cache-unfriendly). [sent-619, score-0.596]
</p><p>98 Each single quantizer uses one inverted file that stores all the vector IDs. [sent-626, score-0.683]
</p><p>99 On the contrary, the inverted multi-index method [3] is memory efficient as a single quantizer method. [sent-636, score-0.681]
</p><p>100 The training time is mainly on obtaining the L K codewords in the single pass of kmeans. [sent-651, score-0.526]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('codewords', 0.499), ('quantizers', 0.479), ('quantizer', 0.446), ('klsh', 0.326), ('inverted', 0.214), ('clk', 0.185), ('distortion', 0.102), ('codeword', 0.091), ('indexing', 0.081), ('quantization', 0.076), ('lsh', 0.073), ('query', 0.068), ('lists', 0.063), ('hash', 0.054), ('joint', 0.054), ('adc', 0.05), ('trails', 0.05), ('ck', 0.049), ('retrieves', 0.047), ('trees', 0.046), ('querying', 0.043), ('ckl', 0.043), ('hashing', 0.04), ('rkd', 0.038), ('items', 0.037), ('bytes', 0.036), ('partitioning', 0.036), ('recall', 0.034), ('jegou', 0.033), ('retrieval', 0.03), ('retrieved', 0.03), ('optimized', 0.03), ('binning', 0.029), ('kmeans', 0.029), ('ckld', 0.028), ('clkin', 0.028), ('fineness', 0.028), ('multiplequantizer', 0.028), ('paulev', 0.028), ('randassign', 0.028), ('files', 0.028), ('finer', 0.027), ('billion', 0.027), ('tree', 0.027), ('pass', 0.027), ('documents', 0.027), ('implies', 0.025), ('retrieve', 0.025), ('dasgupta', 0.025), ('consumes', 0.025), ('jointly', 0.025), ('nearest', 0.024), ('datum', 0.023), ('file', 0.023), ('randomly', 0.022), ('lattices', 0.022), ('neighbors', 0.022), ('search', 0.021), ('independently', 0.021), ('randomized', 0.021), ('subject', 0.021), ('std', 0.021), ('memory', 0.021), ('comparisons', 0.02), ('encoding', 0.02), ('product', 0.02), ('neighbor', 0.02), ('douze', 0.02), ('checks', 0.019), ('stoc', 0.019), ('total', 0.019), ('indyk', 0.018), ('thread', 0.018), ('cartesian', 0.018), ('compact', 0.018), ('competitors', 0.017), ('symposium', 0.017), ('pages', 0.017), ('dot', 0.017), ('built', 0.017), ('residual', 0.016), ('vectors', 0.016), ('true', 0.016), ('optimize', 0.015), ('counted', 0.015), ('substantially', 0.015), ('gmm', 0.015), ('checking', 0.015), ('engines', 0.015), ('list', 0.015), ('way', 0.014), ('pq', 0.014), ('smallest', 0.014), ('alternatives', 0.014), ('toy', 0.014), ('near', 0.014), ('actually', 0.014), ('embedding', 0.014), ('isard', 0.013), ('short', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="221-tfidf-1" href="./iccv-2013-Joint_Inverted_Indexing.html">221 iccv-2013-Joint Inverted Indexing</a></p>
<p>Author: Yan Xia, Kaiming He, Fang Wen, Jian Sun</p><p>Abstract: Inverted indexing is a popular non-exhaustive solution to large scale search. An inverted file is built by a quantizer such as k-means or a tree structure. It has been found that multiple inverted files, obtained by multiple independent random quantizers, are able to achieve practically good recall and speed. Instead of computing the multiple quantizers independently, we present a method that creates them jointly. Our method jointly optimizes all codewords in all quantizers. Then it assigns these codewords to the quantizers. In experiments this method shows significant improvement over various existing methods that use multiple independent quantizers. On the one-billion set of SIFT vectors, our method is faster and more accurate than a recent state-of-the-art inverted indexing method.</p><p>2 0.1509991 <a title="221-tfidf-2" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>Author: Zhuoyuan Chen, Ying Wu</p><p>Abstract: Sparsity models have recently shown great promise in many vision tasks. Using a learned dictionary in sparsity models can in general outperform predefined bases in clean data. In practice, both training and testing data may be corrupted and contain noises and outliers. Although recent studies attempted to cope with corrupted data and achieved encouraging results in testing phase, how to handle corruption in training phase still remains a very difficult problem. In contrast to most existing methods that learn the dictionaryfrom clean data, this paper is targeted at handling corruptions and outliers in training data for dictionary learning. We propose a general method to decompose the reconstructive residual into two components: a non-sparse component for small universal noises and a sparse component for large outliers, respectively. In addition, , further analysis reveals the connection between our approach and the “partial” dictionary learning approach, updating only part of the prototypes (or informative codewords) with remaining (or noisy codewords) fixed. Experiments on synthetic data as well as real applications have shown satisfactory per- formance of this new robust dictionary learning approach.</p><p>3 0.14810109 <a title="221-tfidf-3" href="./iccv-2013-Fast_Neighborhood_Graph_Search_Using_Cartesian_Concatenation.html">159 iccv-2013-Fast Neighborhood Graph Search Using Cartesian Concatenation</a></p>
<p>Author: Jing Wang, Jingdong Wang, Gang Zeng, Rui Gan, Shipeng Li, Baining Guo</p><p>Abstract: In this paper, we propose a new data structure for approximate nearest neighbor search. This structure augments the neighborhoodgraph with a bridge graph. We propose to exploit Cartesian concatenation to produce a large set of vectors, called bridge vectors, from several small sets of subvectors. Each bridge vector is connected with a few reference vectors near to it, forming a bridge graph. Our approach finds nearest neighbors by simultaneously traversing the neighborhood graph and the bridge graph in the best-first strategy. The success of our approach stems from two factors: the exact nearest neighbor search over a large number of bridge vectors can be done quickly, and the reference vectors connected to a bridge (reference) vector near the query are also likely to be near the query. Experimental results on searching over large scale datasets (SIFT, GIST andHOG) show that our approach outperforms stateof-the-art ANN search algorithms in terms of efficiency and accuracy. The combination of our approach with the IVFADC system [18] also shows superior performance over the BIGANN dataset of 1 billion SIFT features compared with the best previously published result.</p><p>4 0.14554463 <a title="221-tfidf-4" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>Author: Shiliang Zhang, Ming Yang, Xiaoyu Wang, Yuanqing Lin, Qi Tian</p><p>Abstract: Inverted indexes in image retrieval not only allow fast access to database images but also summarize all knowledge about the database, so that their discriminative capacity largely determines the retrieval performance. In this paper, for vocabulary tree based image retrieval, we propose a semantic-aware co-indexing algorithm to jointly San Antonio, TX 78249 . j dl@gmai l com qit ian@cs .ut sa . edu . The query embed two strong cues into the inverted indexes: 1) local invariant features that are robust to delineate low-level image contents, and 2) semantic attributes from large-scale object recognition that may reveal image semantic meanings. For an initial set of inverted indexes of local features, we utilize 1000 semantic attributes to filter out isolated images and insert semantically similar images to the initial set. Encoding these two distinct cues together effectively enhances the discriminative capability of inverted indexes. Such co-indexing operations are totally off-line and introduce small computation overhead to online query cause only local features but no semantic attributes are used for query. Experiments and comparisons with recent retrieval methods on 3 datasets, i.e., UKbench, Holidays, Oxford5K, and 1.3 million images from Flickr as distractors, manifest the competitive performance of our method 1.</p><p>5 0.11619718 <a title="221-tfidf-5" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>Author: Matthijs Douze, Jérôme Revaud, Cordelia Schmid, Hervé Jégou</p><p>Abstract: This paper makes two complementary contributions to event retrieval in large collections of videos. First, we propose hyper-pooling strategies that encode the frame descriptors into a representation of the video sequence in a stable manner. Our best choices compare favorably with regular pooling techniques based on k-means quantization. Second, we introduce a technique to improve the ranking. It can be interpreted either as a query expansion method or as a similarity adaptation based on the local context of the query video descriptor. Experiments on public benchmarks show that our methods are complementary and improve event retrieval results, without sacrificing efficiency.</p><p>6 0.10997096 <a title="221-tfidf-6" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>7 0.1050637 <a title="221-tfidf-7" href="./iccv-2013-Quantize_and_Conquer%3A_A_Dimensionality-Recursive_Solution_to_Clustering%2C_Vector_Quantization%2C_and_Image_Retrieval.html">333 iccv-2013-Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval</a></p>
<p>8 0.10429615 <a title="221-tfidf-8" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>9 0.086978093 <a title="221-tfidf-9" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>10 0.084331758 <a title="221-tfidf-10" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>11 0.081335127 <a title="221-tfidf-11" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>12 0.07798969 <a title="221-tfidf-12" href="./iccv-2013-A_General_Two-Step_Approach_to_Learning-Based_Hashing.html">13 iccv-2013-A General Two-Step Approach to Learning-Based Hashing</a></p>
<p>13 0.077071384 <a title="221-tfidf-13" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>14 0.074791797 <a title="221-tfidf-14" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<p>15 0.062380094 <a title="221-tfidf-15" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>16 0.061761029 <a title="221-tfidf-16" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>17 0.060878634 <a title="221-tfidf-17" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>18 0.058709469 <a title="221-tfidf-18" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<p>19 0.058670696 <a title="221-tfidf-19" href="./iccv-2013-Query-Adaptive_Asymmetrical_Dissimilarities_for_Visual_Object_Retrieval.html">334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</a></p>
<p>20 0.048173364 <a title="221-tfidf-20" href="./iccv-2013-To_Aggregate_or_Not_to_aggregate%3A_Selective_Match_Kernels_for_Image_Search.html">419 iccv-2013-To Aggregate or Not to aggregate: Selective Match Kernels for Image Search</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.083), (1, 0.03), (2, -0.045), (3, -0.061), (4, -0.022), (5, 0.152), (6, -0.003), (7, -0.025), (8, -0.109), (9, 0.049), (10, 0.026), (11, 0.035), (12, 0.024), (13, 0.022), (14, -0.009), (15, 0.037), (16, 0.054), (17, -0.047), (18, 0.045), (19, -0.021), (20, -0.008), (21, -0.025), (22, 0.007), (23, 0.025), (24, -0.027), (25, -0.023), (26, 0.016), (27, -0.025), (28, -0.004), (29, -0.025), (30, 0.018), (31, 0.006), (32, 0.013), (33, 0.023), (34, 0.018), (35, 0.017), (36, 0.006), (37, 0.005), (38, 0.037), (39, 0.057), (40, -0.009), (41, 0.073), (42, -0.05), (43, -0.015), (44, -0.015), (45, 0.024), (46, -0.007), (47, -0.104), (48, 0.035), (49, -0.005)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93782288 <a title="221-lsi-1" href="./iccv-2013-Joint_Inverted_Indexing.html">221 iccv-2013-Joint Inverted Indexing</a></p>
<p>Author: Yan Xia, Kaiming He, Fang Wen, Jian Sun</p><p>Abstract: Inverted indexing is a popular non-exhaustive solution to large scale search. An inverted file is built by a quantizer such as k-means or a tree structure. It has been found that multiple inverted files, obtained by multiple independent random quantizers, are able to achieve practically good recall and speed. Instead of computing the multiple quantizers independently, we present a method that creates them jointly. Our method jointly optimizes all codewords in all quantizers. Then it assigns these codewords to the quantizers. In experiments this method shows significant improvement over various existing methods that use multiple independent quantizers. On the one-billion set of SIFT vectors, our method is faster and more accurate than a recent state-of-the-art inverted indexing method.</p><p>2 0.76073074 <a title="221-lsi-2" href="./iccv-2013-Fast_Neighborhood_Graph_Search_Using_Cartesian_Concatenation.html">159 iccv-2013-Fast Neighborhood Graph Search Using Cartesian Concatenation</a></p>
<p>Author: Jing Wang, Jingdong Wang, Gang Zeng, Rui Gan, Shipeng Li, Baining Guo</p><p>Abstract: In this paper, we propose a new data structure for approximate nearest neighbor search. This structure augments the neighborhoodgraph with a bridge graph. We propose to exploit Cartesian concatenation to produce a large set of vectors, called bridge vectors, from several small sets of subvectors. Each bridge vector is connected with a few reference vectors near to it, forming a bridge graph. Our approach finds nearest neighbors by simultaneously traversing the neighborhood graph and the bridge graph in the best-first strategy. The success of our approach stems from two factors: the exact nearest neighbor search over a large number of bridge vectors can be done quickly, and the reference vectors connected to a bridge (reference) vector near the query are also likely to be near the query. Experimental results on searching over large scale datasets (SIFT, GIST andHOG) show that our approach outperforms stateof-the-art ANN search algorithms in terms of efficiency and accuracy. The combination of our approach with the IVFADC system [18] also shows superior performance over the BIGANN dataset of 1 billion SIFT features compared with the best previously published result.</p><p>3 0.76018453 <a title="221-lsi-3" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>Author: Dror Aiger, Efi Kokiopoulou, Ehud Rivlin</p><p>Abstract: We propose two solutions for both nearest neighbors and range search problems. For the nearest neighbors problem, we propose a c-approximate solutionfor the restricted version ofthe decisionproblem with bounded radius which is then reduced to the nearest neighbors by a known reduction. For range searching we propose a scheme that learns the parameters in a learning stage adopting them to the case of a set of points with low intrinsic dimension that are embedded in high dimensional space (common scenario for image point descriptors). We compare our algorithms to the best known methods for these problems, i.e. LSH, ANN and FLANN. We show analytically and experimentally that we can do better for moderate approximation factor. Our algorithms are trivial to parallelize. In the experiments conducted, running on couple of million im- ages, our algorithms show meaningful speed-ups when compared with the above mentioned methods.</p><p>4 0.72407782 <a title="221-lsi-4" href="./iccv-2013-Quantize_and_Conquer%3A_A_Dimensionality-Recursive_Solution_to_Clustering%2C_Vector_Quantization%2C_and_Image_Retrieval.html">333 iccv-2013-Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval</a></p>
<p>Author: Yannis Avrithis</p><p>Abstract: Inspired by the close relation between nearest neighbor search and clustering in high-dimensional spaces as well as the success of one helping to solve the other, we introduce a new paradigm where both problems are solved simultaneously. Our solution is recursive, not in the size of input data but in the number of dimensions. One result is a clustering algorithm that is tuned to small codebooks but does not need all data in memory at the same time and is practically constant in the data size. As a by-product, a tree structure performs either exact or approximate quantization on trained centroids, the latter being not very precise but extremely fast. A lesser contribution is a new indexing scheme for image retrieval that exploits multiple small codebooks to provide an arbitrarily fine partition of the descriptor space. Large scale experiments on public datasets exhibit state of the art performance and remarkable generalization.</p><p>5 0.71937233 <a title="221-lsi-5" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>Author: Masakazu Iwamura, Tomokazu Sato, Koichi Kise</p><p>Abstract: Approximate nearest neighbor search (ANNS) is a basic and important technique used in many tasks such as object recognition. It involves two processes: selecting nearest neighbor candidates and performing a brute-force search of these candidates. Only the former though has scope for improvement. In most existing methods, it approximates the space by quantization. It then calculates all the distances between the query and all the quantized values (e.g., clusters or bit sequences), and selects a fixed number of candidates close to the query. The performance of the method is evaluated based on accuracy as a function of the number of candidates. This evaluation seems rational but poses a serious problem; it ignores the computational cost of the process of selection. In this paper, we propose a new ANNS method that takes into account costs in the selection process. Whereas existing methods employ computationally expensive techniques such as comparative sort and heap, the proposed method does not. This realizes a significantly more efficient search. We have succeeded in reducing computation times by one-third compared with the state-of-the- art on an experiment using 100 million SIFT features.</p><p>6 0.71531379 <a title="221-lsi-6" href="./iccv-2013-To_Aggregate_or_Not_to_aggregate%3A_Selective_Match_Kernels_for_Image_Search.html">419 iccv-2013-To Aggregate or Not to aggregate: Selective Match Kernels for Image Search</a></p>
<p>7 0.70942265 <a title="221-lsi-7" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>8 0.70494884 <a title="221-lsi-8" href="./iccv-2013-Query-Adaptive_Asymmetrical_Dissimilarities_for_Visual_Object_Retrieval.html">334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</a></p>
<p>9 0.6951853 <a title="221-lsi-9" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>10 0.67150843 <a title="221-lsi-10" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>11 0.65910256 <a title="221-lsi-11" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>12 0.64836621 <a title="221-lsi-12" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<p>13 0.63206995 <a title="221-lsi-13" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>14 0.53707749 <a title="221-lsi-14" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<p>15 0.49895146 <a title="221-lsi-15" href="./iccv-2013-SIFTpack%3A_A_Compact_Representation_for_Efficient_SIFT_Matching.html">365 iccv-2013-SIFTpack: A Compact Representation for Efficient SIFT Matching</a></p>
<p>16 0.4950676 <a title="221-lsi-16" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>17 0.48559839 <a title="221-lsi-17" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>18 0.47638837 <a title="221-lsi-18" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>19 0.47619489 <a title="221-lsi-19" href="./iccv-2013-EVSAC%3A_Accelerating_Hypotheses_Generation_by_Modeling_Matching_Scores_with_Extreme_Value_Theory.html">131 iccv-2013-EVSAC: Accelerating Hypotheses Generation by Modeling Matching Scores with Extreme Value Theory</a></p>
<p>20 0.4630076 <a title="221-lsi-20" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.114), (7, 0.019), (13, 0.015), (26, 0.042), (27, 0.011), (31, 0.047), (42, 0.065), (64, 0.029), (73, 0.015), (76, 0.325), (89, 0.171), (98, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.74434417 <a title="221-lda-1" href="./iccv-2013-Fibonacci_Exposure_Bracketing_for_High_Dynamic_Range_Imaging.html">164 iccv-2013-Fibonacci Exposure Bracketing for High Dynamic Range Imaging</a></p>
<p>Author: Mohit Gupta, Daisuke Iso, Shree K. Nayar</p><p>Abstract: Exposure bracketing for high dynamic range (HDR) imaging involves capturing several images of the scene at different exposures. If either the camera or the scene moves during capture, the captured images must be registered. Large exposure differences between bracketed images lead to inaccurate registration, resulting in artifacts such as ghosting (multiple copies of scene objects) and blur. We present two techniques, one for image capture (Fibonacci exposure bracketing) and one for image registration (generalized registration), to prevent such motion-related artifacts. Fibonacci bracketing involves capturing a sequence of images such that each exposure time is the sum of the previous N(N > 1) exposures. Generalized registration involves estimating motion between sums of contiguous sets of frames, instead of between individual frames. Together, the two techniques ensure that motion is always estimated betweenframes of the same total exposure time. This results in HDR images and videos which have both a large dynamic range andminimal motion-relatedartifacts. We show, by results for several real-world indoor and outdoor scenes, that theproposed approach significantly outperforms several ex- isting bracketing schemes.</p><p>same-paper 2 0.73893362 <a title="221-lda-2" href="./iccv-2013-Joint_Inverted_Indexing.html">221 iccv-2013-Joint Inverted Indexing</a></p>
<p>Author: Yan Xia, Kaiming He, Fang Wen, Jian Sun</p><p>Abstract: Inverted indexing is a popular non-exhaustive solution to large scale search. An inverted file is built by a quantizer such as k-means or a tree structure. It has been found that multiple inverted files, obtained by multiple independent random quantizers, are able to achieve practically good recall and speed. Instead of computing the multiple quantizers independently, we present a method that creates them jointly. Our method jointly optimizes all codewords in all quantizers. Then it assigns these codewords to the quantizers. In experiments this method shows significant improvement over various existing methods that use multiple independent quantizers. On the one-billion set of SIFT vectors, our method is faster and more accurate than a recent state-of-the-art inverted indexing method.</p><p>3 0.66511333 <a title="221-lda-3" href="./iccv-2013-Discovering_Object_Functionality.html">118 iccv-2013-Discovering Object Functionality</a></p>
<p>Author: Bangpeng Yao, Jiayuan Ma, Li Fei-Fei</p><p>Abstract: Object functionality refers to the quality of an object that allows humans to perform some specific actions. It has been shown in psychology that functionality (affordance) is at least as essential as appearance in object recognition by humans. In computer vision, most previous work on functionality either assumes exactly one functionality for each object, or requires detailed annotation of human poses and objects. In this paper, we propose a weakly supervised approach to discover all possible object functionalities. Each object functionality is represented by a specific type of human-object interaction. Our method takes any possible human-object interaction into consideration, and evaluates image similarity in 3D rather than 2D in order to cluster human-object interactions more coherently. Experimental results on a dataset of people interacting with musical instruments show the effectiveness of our approach.</p><p>4 0.65992868 <a title="221-lda-4" href="./iccv-2013-Manipulation_Pattern_Discovery%3A_A_Nonparametric_Bayesian_Approach.html">260 iccv-2013-Manipulation Pattern Discovery: A Nonparametric Bayesian Approach</a></p>
<p>Author: Bingbing Ni, Pierre Moulin</p><p>Abstract: We aim to unsupervisedly discover human’s action (motion) patterns of manipulating various objects in scenarios such as assisted living. We are motivated by two key observations. First, large variation exists in motion patterns associated with various types of objects being manipulated, thus manually defining motion primitives is infeasible. Second, some motion patterns are shared among different objects being manipulated while others are object specific. We therefore propose a nonparametric Bayesian method that adopts a hierarchical Dirichlet process prior to learn representative manipulation (motion) patterns in an unsupervised manner. Taking easy-to-obtain object detection score maps and dense motion trajectories as inputs, the proposed probabilistic model can discover motion pattern groups associated with different types of objects being manipulated with a shared manipulation pattern dictionary. The size of the learned dictionary is automatically inferred. Com- prehensive experiments on two assisted living benchmarks and a cooking motion dataset demonstrate superiority of our learned manipulation pattern dictionary in representing manipulation actions for recognition.</p><p>5 0.6126259 <a title="221-lda-5" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>Author: Matthias Reso, Jörn Jachalsky, Bodo Rosenhahn, Jörn Ostermann</p><p>Abstract: Superpixel algorithms represent a very useful and increasingly popular preprocessing step for a wide range of computer vision applications, as they offer the potential to boost efficiency and effectiveness. In this regards, this paper presents a highly competitive approach for temporally consistent superpixelsfor video content. The approach is based on energy-minimizing clustering utilizing a novel hybrid clustering strategy for a multi-dimensional feature space working in a global color subspace and local spatial subspaces. Moreover, a new contour evolution based strategy is introduced to ensure spatial coherency of the generated superpixels. For a thorough evaluation the proposed approach is compared to state of the art supervoxel algorithms using established benchmarks and shows a superior performance.</p><p>6 0.56576765 <a title="221-lda-6" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>7 0.56552899 <a title="221-lda-7" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>8 0.56150663 <a title="221-lda-8" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>9 0.56047744 <a title="221-lda-9" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<p>10 0.56024504 <a title="221-lda-10" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>11 0.55820626 <a title="221-lda-11" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>12 0.55701149 <a title="221-lda-12" href="./iccv-2013-To_Aggregate_or_Not_to_aggregate%3A_Selective_Match_Kernels_for_Image_Search.html">419 iccv-2013-To Aggregate or Not to aggregate: Selective Match Kernels for Image Search</a></p>
<p>13 0.55684346 <a title="221-lda-13" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<p>14 0.55603015 <a title="221-lda-14" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>15 0.55530262 <a title="221-lda-15" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>16 0.55494291 <a title="221-lda-16" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>17 0.55453682 <a title="221-lda-17" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>18 0.5544908 <a title="221-lda-18" href="./iccv-2013-Class-Specific_Simplex-Latent_Dirichlet_Allocation_for_Image_Classification.html">73 iccv-2013-Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification</a></p>
<p>19 0.55437863 <a title="221-lda-19" href="./iccv-2013-SYM-FISH%3A_A_Symmetry-Aware_Flip_Invariant_Sketch_Histogram_Shape_Descriptor.html">368 iccv-2013-SYM-FISH: A Symmetry-Aware Flip Invariant Sketch Histogram Shape Descriptor</a></p>
<p>20 0.55433887 <a title="221-lda-20" href="./iccv-2013-Learning_to_Rank_Using_Privileged_Information.html">248 iccv-2013-Learning to Rank Using Privileged Information</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
