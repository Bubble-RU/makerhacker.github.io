<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>434 iccv-2013-Unifying Nuclear Norm and Bilinear Factorization Approaches for Low-Rank Matrix Decomposition</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-434" href="#">iccv2013-434</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>434 iccv-2013-Unifying Nuclear Norm and Bilinear Factorization Approaches for Low-Rank Matrix Decomposition</h1>
<br/><p>Source: <a title="iccv-2013-434-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Cabral_Unifying_Nuclear_Norm_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Ricardo Cabral, Fernando De_La_Torre, João P. Costeira, Alexandre Bernardino</p><p>Abstract: Low rank models have been widely usedfor the representation of shape, appearance or motion in computer vision problems. Traditional approaches to fit low rank models make use of an explicit bilinear factorization. These approaches benefit from fast numerical methods for optimization and easy kernelization. However, they suffer from serious local minima problems depending on the loss function and the amount/type of missing data. Recently, these lowrank models have alternatively been formulated as convex problems using the nuclear norm regularizer; unlike factorization methods, their numerical solvers are slow and it is unclear how to kernelize them or to impose a rank a priori. This paper proposes a unified approach to bilinear factorization and nuclear norm regularization, that inherits the benefits of both. We analyze the conditions under which these approaches are equivalent. Moreover, based on this analysis, we propose a new optimization algorithm and a “rank continuation ” strategy that outperform state-of-theart approaches for Robust PCA, Structure from Motion and Photometric Stereo with outliers and missing data.</p><p>Reference: <a title="iccv-2013-434-reference" href="../iccv2013_reference/iccv-2013-Unifying_Nuclear_Norm_and_Bilinear_Factorization_Approaches_for_Low-Rank_Matrix_Decomposition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract Low rank models have been widely usedfor the representation of shape, appearance or motion in computer vision problems. [sent-5, score-0.32]
</p><p>2 Traditional approaches to fit low rank models make use of an explicit bilinear factorization. [sent-6, score-0.418]
</p><p>3 However, they suffer from serious local minima problems depending on the loss function and the amount/type of missing data. [sent-8, score-0.451]
</p><p>4 Recently, these lowrank models have alternatively been formulated as convex problems using the nuclear norm regularizer; unlike factorization methods, their numerical solvers are slow and it is unclear how to kernelize them or to impose a rank a priori. [sent-9, score-1.387]
</p><p>5 This paper proposes a unified approach to bilinear factorization and nuclear norm regularization, that inherits the benefits of both. [sent-10, score-1.161]
</p><p>6 Moreover, based on this analysis, we propose a new optimization algorithm and a “rank continuation ” strategy that outperform state-of-theart approaches for Robust PCA, Structure from Motion and  Photometric Stereo with outliers and missing data. [sent-12, score-0.534]
</p><p>7 Low rank models have been widely used for learning representations of shape, appearance or motion in computer vision problems, under several noise assumptions and use of prior information [12, 14, 15, 35]. [sent-15, score-0.32]
</p><p>8 pt  ,  Bilinear factorization:  (1)  both bilinear factorization and nuclear norm regularization models. [sent-20, score-1.119]
</p><p>9 e D fu(e·) t od eintso intractability, cthtieo nra (nskee eco fonosttnraoitnet in (1) has typically been imposed by a factorization Z = UV? [sent-23, score-0.389]
</p><p>10 Unfortunately, this factorization approach has several caveats: The LS loss is highly susceptible to outliers; also, the presence of missing data in X results in local minima. [sent-36, score-0.657]
</p><p>11 While outliers can be addressed with robust loss functions [14, 22], factorization with missing data is an 1Bold capital letters denote matrices (e. [sent-37, score-0.733]
</p><p>12 ∗,  (3)  where λ is a trade-off parameter between the loss function and the low-rank regularization induced by the nuclear norm. [sent-73, score-0.54]
</p><p>13 These models have extended the use of low-rank priors to many applications where Z is low rank but its rank is not known a priori [9, 20, 40]. [sent-74, score-0.719]
</p><p>14 First, it is unclear how to impose a certain rank in Z: we show that adjusting λ so Z has a predetermined rank typically provides worse results than imposing it directly in (2). [sent-76, score-0.633]
</p><p>15 Second, the inability to access the factorization of Z in (3) hinders the use of the “kernel trick”. [sent-77, score-0.389]
</p><p>16 Motivated by the theoretical results in [6], we show that many nuclear norm regularized problems of the form (3) can be optimized with a bilinear factorization of Z = UV? [sent-82, score-1.187]
</p><p>17 by using the variational definition of the nuclear norm (see Fig. [sent-83, score-0.576]
</p><p>18 While this reformulation is known in the literature, this paper is the first to propose a unification of traditional bilinear factorization and nuclear norm approaches under one formulation. [sent-85, score-1.163]
</p><p>19 Our analysis is divided in two situations: when the output rank is unconstrained and when the output rank is known a priori. [sent-87, score-0.731]
</p><p>20 Previous Work Low-rank matrix factorization is a long standing problem in computer vision. [sent-91, score-0.442]
</p><p>21 The seminal factorization method for Structure from Motion of Tomasi and Kanade [35] has been extended to encompass non-rigid and articulated cases, as well as photometric stereo [5] and multiple bodies [12]. [sent-92, score-0.522]
</p><p>22 Unfortunately, in the presence of missing data or weights, the factorization problem becomes NP-Hard [17]. [sent-93, score-0.547]
</p><p>23 3 we provide a theoreticaljustification for this choice of algorithm for the factorization problem. [sent-112, score-0.389]
</p><p>24 However, this is not generalizable to several other computer vision problems modeled as lowrank factorization problems [5, 3 1, 34, 37]. [sent-116, score-0.519]
</p><p>25 Alternatively to bilinear factorization approaches, Candes and Recht [10] stated that the rank function, under broad conditions of incoherence, can be minimized by its convex surrogate, the nuclear norm. [sent-117, score-1.244]
</p><p>26 This result has extended the use of low-rank priors to many applications where the rank is not known a priori, e. [sent-118, score-0.363]
</p><p>27 Since the nuclear norm yields a SDP, several methods try to optimize it efficiently [8, 9, 16, 24] by exploiting a closed form solution of its proximal operator. [sent-122, score-0.645]
</p><p>28 However, these methods rely on a rank selection heuristic, which fails when the missing data pattern is not random. [sent-127, score-0.449]
</p><p>29 Recently, the nuclear norm has also been proposed to tackle the factorization problem when the rank is known a 22448899  priori. [sent-128, score-1.302]
</p><p>30 [2] proposed a weighted version of the nuclear norm for structure from motion. [sent-130, score-0.576]
</p><p>31 [13] proposed an element-wise factorization for projective reconstruction by relaxing the rank 4 constraint to nuclear norm  optimization. [sent-132, score-1.256]
</p><p>32 [41] extended [15] by adding a nuclear norm regularizer to V and the orthogonality constraints in U found in [5] for structure from motion. [sent-134, score-0.602]
</p><p>33 Results provided in this paper shows that when the output rank is known a priori, nuclear norm solutions typically provide worse reconstruction results than those obtained with bilinear factorization models. [sent-135, score-1.466]
</p><p>34 Soft and hard rank constraints In this section, we bridge the gap between factorization and nuclear norm approaches. [sent-137, score-1.256]
</p><p>35 This reformulation seems counterintuitive, as we changed the convex problem in (3) into a non-convex one, which may be prone to local minima (e. [sent-163, score-0.273]
</p><p>36 Theorem 1 (which we prove in Appendix A) immediately allows us to draw one conclusion: The factorization and the nuclear norm models in (2) and (3) are special cases of (6). [sent-170, score-0.994]
</p><p>37 Region of equivalence between factorization (6) and nuclear norm approaches (3) for a 100 100 random matrix and cLlSe lro nsso. [sent-181, score-1.018]
</p><p>38 mW aheppn foaacctohersiz a(t3i)on fo irs ain 1it0ia0li ×zed 1 0in0 trhaen dwomhite m area, i at isd equivalent to the result obtained with the nuclear norm (black line). [sent-182, score-0.576]
</p><p>39 When the rank is known a priori, better reconstruction results can be found in the grey area, by using factorization approaches. [sent-183, score-0.76]
</p><p>40 LS loss: the factorization approach in (2) corresponds to the case where λ = 0 and r is fixed, whilst the nuclear norm in (3) outputs an arbitrary rank k∗ as function of λ (the black line). [sent-184, score-1.256]
</p><p>41 A special case of Theorem 1 has been used to recommend the use of nuclear norm approaches in the machine learning community by Mazumder et al. [sent-187, score-0.576]
</p><p>42 However, their analysis is restricted to the LS loss and the case where the rank is not known a priori (i. [sent-189, score-0.512]
</p><p>43 That is, their output rank k is predetermined by a domain-specific constraint (e. [sent-195, score-0.379]
</p><p>44 Thus, we advocate the use of our unified model in (6) over the nuclear norm formulation in (3), based on two arguments: When the output rank is unconstrained, we show in Sec. [sent-198, score-0.932]
</p><p>45 When the output rank is known a priori, optimizing (6) is preferable to (2) and (3). [sent-201, score-0.413]
</p><p>46 As we will show in the experimental section, optimizing (6) is less prone to local minima than the unregularized problem (2). [sent-202, score-0.338]
</p><p>47 On the other hand, selecting λ in the nuclear norm model (3) such that the output rank k is the desired value typically leads to worse reconstructions than directly imposing r = k in (6). [sent-203, score-0.992]
</p><p>48 Factorization with unconstrained rank Nuclear norm models have extended the use of low-rank priors to many applications where Z is low rank but its exact value is not known a priori [9, 20, 40]. [sent-223, score-0.921]
</p><p>49 In this section, we propose an algorithm for solving (6) and show its complexity is lower than proximal methods [24] for optimizing the nuclear norm model in (3). [sent-224, score-0.655]
</p><p>50 Factorization with rank known a priori Many representations of shape, appearance or motion in computer vision problems yield models of a predetermined rank k [5, 12, 35]. [sent-310, score-0.806]
</p><p>51 In this case, we argue for the use of our model (6) instead of the nuclear norm approach in (3) and the unregularized model in (2). [sent-311, score-0.661]
</p><p>52 To understand why this is the case, let us consider the example of rank-k factorization of a matrix X under the LS loss with no missing data. [sent-312, score-0.71]
</p><p>53 3 shows that rank restrictions typically  lead to local minima when missing data are present. [sent-320, score-0.599]
</p><p>54 Thus, the unregularized factorization in (2) will be more prone to local minima than its regularized counterpart (6). [sent-325, score-0.72]
</p><p>55 4 that (6) always has a region with no local minima, we propose the following “rank continuation” strategy: we initialize (6) with a rank r ≥ k∗ matrix (i. [sent-328, score-0.344]
</p><p>56 Then, we use this solution as initialization to a new problem (6) where the dimensions r of U, V are decreased by one, until the desired rank is attained. [sent-333, score-0.352]
</p><p>57 Algorithm  2  Rank  continuation  Algorithm 2 Rank continuation  Input: X,W ∈ RM×N, output rank k, parameter λ, an optional eWstim ∈ate R of the output rank k∗ of (3) Initialize U, V randomly, with k∗ ≤ r ≤ min (M, N) Solve for Z in (6) with Alg. [sent-337, score-1.212]
</p><p>58 1 end for Output: Complete Matrix Z with rank k  UΣ121:r,  Σ121:rV? [sent-344, score-0.291]
</p><p>59 Rank continuation provides a deterministic optimization strategy that empirically is shown to find better local optima. [sent-346, score-0.373]
</p><p>60 1, we explore the application of Robust PCA, a typical scenario when the desired output rank is unconstrained and compare our factorization (6) to nuclear norm approaches; in Sec. [sent-353, score-1.354]
</p><p>61 2, we explore the applications of SfM, Non-rigid SfM and Photometric Stereo, typical scenarios where the output rank is known a priori. [sent-355, score-0.374]
</p><p>62 ∈  In this case, we compare our continuation approach to nuclear norm approaches and several factorization algorithms. [sent-359, score-1.243]
</p><p>63 Factorization with unconstrained rank In this section, we validated the lower computational complexity of the algorithm proposed in Sec. [sent-368, score-0.32]
</p><p>64 We compared to state-ofthe-art nuclear norm and Grassmann manifold methods: GRASTA [19], PRMF [36] and RPCA-IALM [24] in a synthetic and real data experiment for background modeling. [sent-370, score-0.646]
</p><p>65 We varied the dimension N and rank r and  tmhe a tsimure d th the y a tlogo krit hom ru anc. [sent-381, score-0.291]
</p><p>66 Since the background is common across many frames, the matrix concatenating all frames is a low rank matrix plus a sparse error matrix modeling the dynamic foreground. [sent-391, score-0.45]
</p><p>67 Factorization with known rank In this section, we empirically validated the “rank continuation” strategy proposed in Sec. [sent-439, score-0.384]
</p><p>68 5, in several synthetic and real data problems where the output rank is known a priori. [sent-440, score-0.477]
</p><p>69 We compared our method to state-of-the-art factorization approaches: the damped Newton in [4], the LRSDP formulations in [28] and the LS/L1 Wiberg methods in [15, 29]. [sent-441, score-0.435]
</p><p>70 For control, we also compared to two nuclear norm baselines: NN-SVD, obtained by solving (3) with the same λ used for other models and projecting to the desired rank with an SVD; NN-λ, obtained by tuning λ in (3) so the desired rank is obtained. [sent-444, score-1.222]
</p><p>71 Synthetic data We assessed the convergence performance of our continuation strategy using synthetic data. [sent-445, score-0.458]
</p><p>72 Comparison of convergence to empirical global minima (Min) for the LS and L1 losses in synthetic data. [sent-455, score-0.281]
</p><p>73 We show two representatives cases for the percentage of known entries (75% and 35%, the breakdown point for L2Wiberg methods), both for missing data patterns at random (M. [sent-470, score-0.33]
</p><p>74 The theoretical minimum number of entries to reconstruct the matrix is the same as the number of parameters minus factorization ambiguity Mr (N −r) (r 1), which for this case is 29. [sent-474, score-0.564]
</p><p>75 4(a) show that our deterministic continuation approach always reaches the empirical optima (found as the minimum of all runs of all meth-  ods), regardless ofthe number ofknown entries or pattern of missing data. [sent-479, score-0.614]
</p><p>76 SGDcuDFiatrlap acntfesofue rte2 6349124069S40i× z× e 172642076Outp 4 3t6rank Know4257n18 0% % entries we observe that L2-Wiberg is insensitive to initialization for a wide range of missing data entries. [sent-483, score-0.279]
</p><p>77 The baseline NNSVD performed poorly, showing that the estimation of the nuclear norm fits information in its additional degrees of freedom instead of representing it with the true rank. [sent-486, score-0.576]
</p><p>78 4(b) show that our continuation strategy no longer attains the empirical optima. [sent-488, score-0.35]
</p><p>79 We note that this is not surprising since the problem of factorization with missing data is NP-Hard. [sent-489, score-0.547]
</p><p>80 Our continuation method regained empirical optimality when only 2% of outliers were present in the data, suggesting a de-  pendency on the noise for the L1 case. [sent-491, score-0.329]
</p><p>81 Thus, continuation is a viable alternative to the memory expensive Wiberg method. [sent-493, score-0.278]
</p><p>82 Real data Next, we assessed the results of our continuation approach in real data sequences. [sent-494, score-0.304]
</p><p>83 Table 3 shows a comparison of average error over all observed entries for the continuation proposed in Alg. [sent-504, score-0.37]
</p><p>84 5, we observe that nuclear norm regularized approaches NN-SVD and NN-λ result in bad approximations when a rank restriction is imposed. [sent-508, score-0.899]
</p><p>85 While (c) (d) smooth out the image and (e) fails to reconstruct it, our continuation approach (f) is able to obtain reconstructions preserve finer details, such as the imperfections on the cheek or chin. [sent-517, score-0.334]
</p><p>86 As a control experiment, we also ran our continuation strategy for the unregularized case (λ = 0) on the Dino sequence with LS loss, which resulted  in a RMSE of 1. [sent-540, score-0.41]
</p><p>87 For the L1 loss, continuation outperforms the state-ofthe art in all datasets. [sent-544, score-0.278]
</p><p>88 1 or the problem can be reformulated as a different SDP [28] with a rank constraint, which can be tackled by our continuation strategy in Alg. [sent-550, score-0.616]
</p><p>89 Conclusion We developed a unified approach to matrix factorization and nuclear norm regularization, that inherits the benefits of both approaches. [sent-553, score-1.087]
</p><p>90 Based on this analysis, we proposed a deterministic “rank continuation” strategy that outperforms state-of-the-art approaches in several computer vision applications with outliers and missing data. [sent-554, score-0.304]
</p><p>91 Future work in factorization algorithms should optimize the unified model in (6), since it subsumes the traditional factorization and the nuclear norm regularized approaches. [sent-555, score-1.414]
</p><p>92 It should also focus into the theoretical understanding of the continuation model proposed in Sec. [sent-556, score-0.308]
</p><p>93 Proof of Theorem 1 To show this, we first note that (6) agrees with the following alternative formulation of the nuclear norm [32],  ? [sent-564, score-0.576]
</p><p>94 Damped newton algorithms for matrix factorization with missing data. [sent-608, score-0.641]
</p><p>95 Robust l1 norm factorization in the presence of outliers and missing data by alternative convex programming. [sent-723, score-0.805]
</p><p>96 Large-scale matrix factorization with missing data under additional constraints. [sent-758, score-0.6]
</p><p>97 On the Wiberg algorithm for factorization with missing components. [sent-763, score-0.547]
</p><p>98 Efficient algorithm for lowrank matrix factorization with missing components and performance  [31] [32]  [33] [34] [35] [36] [37] [38]  [39] [40] [41]  comparison of latest algorithms. [sent-769, score-0.664]
</p><p>99 Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. [sent-782, score-0.629]
</p><p>100 Shape and motion from image streams under orthography: a factorization method. [sent-800, score-0.418]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nuclear', 0.403), ('factorization', 0.389), ('rank', 0.291), ('continuation', 0.278), ('uv', 0.239), ('wiberg', 0.186), ('norm', 0.173), ('ls', 0.172), ('missing', 0.158), ('minima', 0.15), ('bilinear', 0.127), ('loss', 0.11), ('entries', 0.092), ('svd', 0.092), ('unregularized', 0.085), ('flatlining', 0.083), ('cabral', 0.073), ('prmf', 0.073), ('sdp', 0.073), ('synthetic', 0.07), ('grasta', 0.068), ('priori', 0.065), ('lowrank', 0.064), ('photometric', 0.064), ('prone', 0.064), ('alternated', 0.064), ('sfm', 0.062), ('lrsdp', 0.062), ('mzin', 0.061), ('alm', 0.059), ('theorem', 0.057), ('torre', 0.056), ('reconstructions', 0.056), ('mazumder', 0.055), ('sculpture', 0.055), ('matrix', 0.053), ('predetermined', 0.051), ('okatani', 0.051), ('outliers', 0.051), ('deterministic', 0.048), ('strategy', 0.047), ('known', 0.046), ('damped', 0.046), ('rm', 0.045), ('lagrange', 0.045), ('costeira', 0.044), ('stereo', 0.043), ('newton', 0.041), ('burer', 0.041), ('glashoff', 0.041), ('kernelizable', 0.041), ('portugal', 0.041), ('proceeded', 0.041), ('inherits', 0.041), ('proximal', 0.04), ('optimizing', 0.039), ('augmented', 0.038), ('optima', 0.038), ('convergence', 0.037), ('output', 0.037), ('la', 0.035), ('singular', 0.035), ('grey', 0.034), ('breakdown', 0.034), ('aguiar', 0.034), ('dino', 0.034), ('convex', 0.034), ('problems', 0.033), ('desired', 0.032), ('regularized', 0.032), ('buchanan', 0.03), ('nz', 0.03), ('xavier', 0.03), ('recht', 0.03), ('fernando', 0.03), ('ur', 0.03), ('multiplier', 0.03), ('theoretical', 0.03), ('prove', 0.029), ('completion', 0.029), ('corrupted', 0.029), ('initialization', 0.029), ('eriksson', 0.029), ('ameliorate', 0.029), ('unconstrained', 0.029), ('motion', 0.029), ('closed', 0.029), ('angst', 0.028), ('unified', 0.028), ('grassmann', 0.027), ('regularization', 0.027), ('irls', 0.026), ('assessed', 0.026), ('extended', 0.026), ('arguments', 0.026), ('attains', 0.025), ('reformulation', 0.025), ('vr', 0.025), ('matrices', 0.025), ('losses', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999875 <a title="434-tfidf-1" href="./iccv-2013-Unifying_Nuclear_Norm_and_Bilinear_Factorization_Approaches_for_Low-Rank_Matrix_Decomposition.html">434 iccv-2013-Unifying Nuclear Norm and Bilinear Factorization Approaches for Low-Rank Matrix Decomposition</a></p>
<p>Author: Ricardo Cabral, Fernando De_La_Torre, João P. Costeira, Alexandre Bernardino</p><p>Abstract: Low rank models have been widely usedfor the representation of shape, appearance or motion in computer vision problems. Traditional approaches to fit low rank models make use of an explicit bilinear factorization. These approaches benefit from fast numerical methods for optimization and easy kernelization. However, they suffer from serious local minima problems depending on the loss function and the amount/type of missing data. Recently, these lowrank models have alternatively been formulated as convex problems using the nuclear norm regularizer; unlike factorization methods, their numerical solvers are slow and it is unclear how to kernelize them or to impose a rank a priori. This paper proposes a unified approach to bilinear factorization and nuclear norm regularization, that inherits the benefits of both. We analyze the conditions under which these approaches are equivalent. Moreover, based on this analysis, we propose a new optimization algorithm and a “rank continuation ” strategy that outperform state-of-theart approaches for Robust PCA, Structure from Motion and Photometric Stereo with outliers and missing data.</p><p>2 0.27739555 <a title="434-tfidf-2" href="./iccv-2013-Partial_Sum_Minimization_of_Singular_Values_in_RPCA_for_Low-Level_Vision.html">310 iccv-2013-Partial Sum Minimization of Singular Values in RPCA for Low-Level Vision</a></p>
<p>Author: Tae-Hyun Oh, Hyeongwoo Kim, Yu-Wing Tai, Jean-Charles Bazin, In So Kweon</p><p>Abstract: Robust Principal Component Analysis (RPCA) via rank minimization is a powerful tool for recovering underlying low-rank structure of clean data corrupted with sparse noise/outliers. In many low-level vision problems, not only it is known that the underlying structure of clean data is low-rank, but the exact rank of clean data is also known. Yet, when applying conventional rank minimization for those problems, the objective function is formulated in a way that does not fully utilize a priori target rank information about the problems. This observation motivates us to investigate whether there is a better alternative solution when using rank minimization. In this paper, instead of minimizing the nuclear norm, we propose to minimize the partial sum of singular values. The proposed objective function implicitly encourages the target rank constraint in rank minimization. Our experimental analyses show that our approach performs better than conventional rank minimization when the number of samples is deficient, while the solutions obtained by the two approaches are almost identical when the number of samples is more than sufficient. We apply our approach to various low-level vision problems, e.g. high dynamic range imaging, photometric stereo and image alignment, and show that our results outperform those obtained by the conventional nuclear norm rank minimization method.</p><p>3 0.20269562 <a title="434-tfidf-3" href="./iccv-2013-Robust_Matrix_Factorization_with_Unknown_Noise.html">357 iccv-2013-Robust Matrix Factorization with Unknown Noise</a></p>
<p>Author: Deyu Meng, Fernando De_La_Torre</p><p>Abstract: Many problems in computer vision can be posed as recovering a low-dimensional subspace from highdimensional visual data. Factorization approaches to lowrank subspace estimation minimize a loss function between an observed measurement matrix and a bilinear factorization. Most popular loss functions include the L2 and L1 losses. L2 is optimal for Gaussian noise, while L1 is for Laplacian distributed noise. However, real data is often corrupted by an unknown noise distribution, which is unlikely to be purely Gaussian or Laplacian. To address this problem, this paper proposes a low-rank matrix factorization problem with a Mixture of Gaussians (MoG) noise model. The MoG model is a universal approximator for any continuous distribution, and hence is able to model a wider range of noise distributions. The parameters of the MoG model can be estimated with a maximum likelihood method, while the subspace is computed with standard approaches. We illustrate the benefits of our approach in extensive syn- thetic and real-world experiments including structure from motion, face modeling and background subtraction.</p><p>4 0.19831523 <a title="434-tfidf-4" href="./iccv-2013-Multi-view_3D_Reconstruction_from_Uncalibrated_Radially-Symmetric_Cameras.html">280 iccv-2013-Multi-view 3D Reconstruction from Uncalibrated Radially-Symmetric Cameras</a></p>
<p>Author: Jae-Hak Kim, Yuchao Dai, Hongdong Li, Xin Du, Jonghyuk Kim</p><p>Abstract: We present a new multi-view 3D Euclidean reconstruction method for arbitrary uncalibrated radially-symmetric cameras, which needs no calibration or any camera model parameters other than radial symmetry. It is built on the radial 1D camera model [25], a unified mathematical abstraction to different types of radially-symmetric cameras. We formulate the problem of multi-view reconstruction for radial 1D cameras as a matrix rank minimization problem. Efficient implementation based on alternating direction continuation is proposed to handle scalability issue for real-world applications. Our method applies to a wide range of omnidirectional cameras including both dioptric and catadioptric (central and non-central) cameras. Additionally, our method deals with complete and incomplete measurements under a unified framework elegantly. Experiments on both synthetic and real images from various types of cameras validate the superior performance of our new method, in terms of numerical accuracy and robustness.</p><p>5 0.15647943 <a title="434-tfidf-5" href="./iccv-2013-Bayesian_Robust_Matrix_Factorization_for_Image_and_Video_Processing.html">60 iccv-2013-Bayesian Robust Matrix Factorization for Image and Video Processing</a></p>
<p>Author: Naiyan Wang, Dit-Yan Yeung</p><p>Abstract: Matrix factorization is a fundamental problem that is often encountered in many computer vision and machine learning tasks. In recent years, enhancing the robustness of matrix factorization methods has attracted much attention in the research community. To benefit from the strengths of full Bayesian treatment over point estimation, we propose here a full Bayesian approach to robust matrix factorization. For the generative process, the model parameters have conjugate priors and the likelihood (or noise model) takes the form of a Laplace mixture. For Bayesian inference, we devise an efficient sampling algorithm by exploiting a hierarchical view of the Laplace distribution. Besides the basic model, we also propose an extension which assumes that the outliers exhibit spatial or temporal proximity as encountered in many computer vision applications. The proposed methods give competitive experimental results when compared with several state-of-the-art methods on some benchmark image and video processing tasks.</p><p>6 0.13658161 <a title="434-tfidf-6" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>7 0.11209662 <a title="434-tfidf-7" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>8 0.11176997 <a title="434-tfidf-8" href="./iccv-2013-The_Way_They_Move%3A_Tracking_Multiple_Targets_with_Similar_Appearance.html">418 iccv-2013-The Way They Move: Tracking Multiple Targets with Similar Appearance</a></p>
<p>9 0.10978488 <a title="434-tfidf-9" href="./iccv-2013-Non-convex_P-Norm_Projection_for_Robust_Sparsity.html">292 iccv-2013-Non-convex P-Norm Projection for Robust Sparsity</a></p>
<p>10 0.1002858 <a title="434-tfidf-10" href="./iccv-2013-GOSUS%3A_Grassmannian_Online_Subspace_Updates_with_Structured-Sparsity.html">182 iccv-2013-GOSUS: Grassmannian Online Subspace Updates with Structured-Sparsity</a></p>
<p>11 0.099034019 <a title="434-tfidf-11" href="./iccv-2013-Correntropy_Induced_L2_Graph_for_Robust_Subspace_Clustering.html">94 iccv-2013-Correntropy Induced L2 Graph for Robust Subspace Clustering</a></p>
<p>12 0.097498834 <a title="434-tfidf-12" href="./iccv-2013-Find_the_Best_Path%3A_An_Efficient_and_Accurate_Classifier_for_Image_Hierarchies.html">165 iccv-2013-Find the Best Path: An Efficient and Accurate Classifier for Image Hierarchies</a></p>
<p>13 0.097062498 <a title="434-tfidf-13" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>14 0.09434025 <a title="434-tfidf-14" href="./iccv-2013-An_Enhanced_Structure-from-Motion_Paradigm_Based_on_the_Absolute_Dual_Quadric_and_Images_of_Circular_Points.html">49 iccv-2013-An Enhanced Structure-from-Motion Paradigm Based on the Absolute Dual Quadric and Images of Circular Points</a></p>
<p>15 0.090017453 <a title="434-tfidf-15" href="./iccv-2013-Distributed_Low-Rank_Subspace_Segmentation.html">122 iccv-2013-Distributed Low-Rank Subspace Segmentation</a></p>
<p>16 0.081637092 <a title="434-tfidf-16" href="./iccv-2013-Robust_Tucker_Tensor_Decomposition_for_Effective_Image_Representation.html">362 iccv-2013-Robust Tucker Tensor Decomposition for Effective Image Representation</a></p>
<p>17 0.080810234 <a title="434-tfidf-17" href="./iccv-2013-Rank_Minimization_across_Appearance_and_Shape_for_AAM_Ensemble_Fitting.html">339 iccv-2013-Rank Minimization across Appearance and Shape for AAM Ensemble Fitting</a></p>
<p>18 0.075073116 <a title="434-tfidf-18" href="./iccv-2013-Single-Patch_Low-Rank_Prior_for_Non-pointwise_Impulse_Noise_Removal.html">394 iccv-2013-Single-Patch Low-Rank Prior for Non-pointwise Impulse Noise Removal</a></p>
<p>19 0.072009712 <a title="434-tfidf-19" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>20 0.067693308 <a title="434-tfidf-20" href="./iccv-2013-Efficient_Higher-Order_Clustering_on_the_Grassmann_Manifold.html">134 iccv-2013-Efficient Higher-Order Clustering on the Grassmann Manifold</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.157), (1, -0.056), (2, -0.068), (3, -0.013), (4, -0.111), (5, 0.052), (6, 0.018), (7, 0.028), (8, 0.101), (9, 0.001), (10, -0.004), (11, -0.028), (12, -0.104), (13, -0.001), (14, -0.009), (15, -0.0), (16, 0.006), (17, 0.074), (18, -0.03), (19, 0.075), (20, 0.007), (21, -0.007), (22, -0.125), (23, -0.073), (24, 0.013), (25, 0.058), (26, 0.139), (27, -0.106), (28, -0.01), (29, -0.048), (30, 0.124), (31, -0.017), (32, -0.056), (33, 0.098), (34, 0.076), (35, 0.021), (36, 0.013), (37, 0.135), (38, 0.114), (39, -0.033), (40, -0.1), (41, 0.012), (42, -0.06), (43, 0.193), (44, 0.011), (45, 0.182), (46, -0.178), (47, -0.098), (48, 0.132), (49, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97034657 <a title="434-lsi-1" href="./iccv-2013-Unifying_Nuclear_Norm_and_Bilinear_Factorization_Approaches_for_Low-Rank_Matrix_Decomposition.html">434 iccv-2013-Unifying Nuclear Norm and Bilinear Factorization Approaches for Low-Rank Matrix Decomposition</a></p>
<p>Author: Ricardo Cabral, Fernando De_La_Torre, João P. Costeira, Alexandre Bernardino</p><p>Abstract: Low rank models have been widely usedfor the representation of shape, appearance or motion in computer vision problems. Traditional approaches to fit low rank models make use of an explicit bilinear factorization. These approaches benefit from fast numerical methods for optimization and easy kernelization. However, they suffer from serious local minima problems depending on the loss function and the amount/type of missing data. Recently, these lowrank models have alternatively been formulated as convex problems using the nuclear norm regularizer; unlike factorization methods, their numerical solvers are slow and it is unclear how to kernelize them or to impose a rank a priori. This paper proposes a unified approach to bilinear factorization and nuclear norm regularization, that inherits the benefits of both. We analyze the conditions under which these approaches are equivalent. Moreover, based on this analysis, we propose a new optimization algorithm and a “rank continuation ” strategy that outperform state-of-theart approaches for Robust PCA, Structure from Motion and Photometric Stereo with outliers and missing data.</p><p>2 0.91025716 <a title="434-lsi-2" href="./iccv-2013-Partial_Sum_Minimization_of_Singular_Values_in_RPCA_for_Low-Level_Vision.html">310 iccv-2013-Partial Sum Minimization of Singular Values in RPCA for Low-Level Vision</a></p>
<p>Author: Tae-Hyun Oh, Hyeongwoo Kim, Yu-Wing Tai, Jean-Charles Bazin, In So Kweon</p><p>Abstract: Robust Principal Component Analysis (RPCA) via rank minimization is a powerful tool for recovering underlying low-rank structure of clean data corrupted with sparse noise/outliers. In many low-level vision problems, not only it is known that the underlying structure of clean data is low-rank, but the exact rank of clean data is also known. Yet, when applying conventional rank minimization for those problems, the objective function is formulated in a way that does not fully utilize a priori target rank information about the problems. This observation motivates us to investigate whether there is a better alternative solution when using rank minimization. In this paper, instead of minimizing the nuclear norm, we propose to minimize the partial sum of singular values. The proposed objective function implicitly encourages the target rank constraint in rank minimization. Our experimental analyses show that our approach performs better than conventional rank minimization when the number of samples is deficient, while the solutions obtained by the two approaches are almost identical when the number of samples is more than sufficient. We apply our approach to various low-level vision problems, e.g. high dynamic range imaging, photometric stereo and image alignment, and show that our results outperform those obtained by the conventional nuclear norm rank minimization method.</p><p>3 0.8104049 <a title="434-lsi-3" href="./iccv-2013-Robust_Matrix_Factorization_with_Unknown_Noise.html">357 iccv-2013-Robust Matrix Factorization with Unknown Noise</a></p>
<p>Author: Deyu Meng, Fernando De_La_Torre</p><p>Abstract: Many problems in computer vision can be posed as recovering a low-dimensional subspace from highdimensional visual data. Factorization approaches to lowrank subspace estimation minimize a loss function between an observed measurement matrix and a bilinear factorization. Most popular loss functions include the L2 and L1 losses. L2 is optimal for Gaussian noise, while L1 is for Laplacian distributed noise. However, real data is often corrupted by an unknown noise distribution, which is unlikely to be purely Gaussian or Laplacian. To address this problem, this paper proposes a low-rank matrix factorization problem with a Mixture of Gaussians (MoG) noise model. The MoG model is a universal approximator for any continuous distribution, and hence is able to model a wider range of noise distributions. The parameters of the MoG model can be estimated with a maximum likelihood method, while the subspace is computed with standard approaches. We illustrate the benefits of our approach in extensive syn- thetic and real-world experiments including structure from motion, face modeling and background subtraction.</p><p>4 0.78587461 <a title="434-lsi-4" href="./iccv-2013-Bayesian_Robust_Matrix_Factorization_for_Image_and_Video_Processing.html">60 iccv-2013-Bayesian Robust Matrix Factorization for Image and Video Processing</a></p>
<p>Author: Naiyan Wang, Dit-Yan Yeung</p><p>Abstract: Matrix factorization is a fundamental problem that is often encountered in many computer vision and machine learning tasks. In recent years, enhancing the robustness of matrix factorization methods has attracted much attention in the research community. To benefit from the strengths of full Bayesian treatment over point estimation, we propose here a full Bayesian approach to robust matrix factorization. For the generative process, the model parameters have conjugate priors and the likelihood (or noise model) takes the form of a Laplace mixture. For Bayesian inference, we devise an efficient sampling algorithm by exploiting a hierarchical view of the Laplace distribution. Besides the basic model, we also propose an extension which assumes that the outliers exhibit spatial or temporal proximity as encountered in many computer vision applications. The proposed methods give competitive experimental results when compared with several state-of-the-art methods on some benchmark image and video processing tasks.</p><p>5 0.6368953 <a title="434-lsi-5" href="./iccv-2013-Non-convex_P-Norm_Projection_for_Robust_Sparsity.html">292 iccv-2013-Non-convex P-Norm Projection for Robust Sparsity</a></p>
<p>Author: Mithun Das Gupta, Sanjeev Kumar</p><p>Abstract: In this paper, we investigate the properties of Lp norm (p ≤ 1) within a projection framework. We start with the (KpK T≤ equations of the neoctni-olnin efraarm optimization problem a thnde then use its key properties to arrive at an algorithm for Lp norm projection on the non-negative simplex. We compare with L1projection which needs prior knowledge of the true norm, as well as hard thresholding based sparsificationproposed in recent compressed sensing literature. We show performance improvements compared to these techniques across different vision applications.</p><p>6 0.57442957 <a title="434-lsi-6" href="./iccv-2013-Finding_Causal_Interactions_in_Video_Sequences.html">167 iccv-2013-Finding Causal Interactions in Video Sequences</a></p>
<p>7 0.57088399 <a title="434-lsi-7" href="./iccv-2013-Multi-view_3D_Reconstruction_from_Uncalibrated_Radially-Symmetric_Cameras.html">280 iccv-2013-Multi-view 3D Reconstruction from Uncalibrated Radially-Symmetric Cameras</a></p>
<p>8 0.57046586 <a title="434-lsi-8" href="./iccv-2013-Automatic_Kronecker_Product_Model_Based_Detection_of_Repeated_Patterns_in_2D_Urban_Images.html">55 iccv-2013-Automatic Kronecker Product Model Based Detection of Repeated Patterns in 2D Urban Images</a></p>
<p>9 0.54935873 <a title="434-lsi-9" href="./iccv-2013-The_Way_They_Move%3A_Tracking_Multiple_Targets_with_Similar_Appearance.html">418 iccv-2013-The Way They Move: Tracking Multiple Targets with Similar Appearance</a></p>
<p>10 0.53433424 <a title="434-lsi-10" href="./iccv-2013-Robust_Tucker_Tensor_Decomposition_for_Effective_Image_Representation.html">362 iccv-2013-Robust Tucker Tensor Decomposition for Effective Image Representation</a></p>
<p>11 0.52794051 <a title="434-lsi-11" href="./iccv-2013-SGTD%3A_Structure_Gradient_and_Texture_Decorrelating_Regularization_for_Image_Decomposition.html">364 iccv-2013-SGTD: Structure Gradient and Texture Decorrelating Regularization for Image Decomposition</a></p>
<p>12 0.52076554 <a title="434-lsi-12" href="./iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding.html">14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</a></p>
<p>13 0.48597875 <a title="434-lsi-13" href="./iccv-2013-GOSUS%3A_Grassmannian_Online_Subspace_Updates_with_Structured-Sparsity.html">182 iccv-2013-GOSUS: Grassmannian Online Subspace Updates with Structured-Sparsity</a></p>
<p>14 0.4627907 <a title="434-lsi-14" href="./iccv-2013-Enhanced_Continuous_Tabu_Search_for_Parameter_Estimation_in_Multiview_Geometry.html">141 iccv-2013-Enhanced Continuous Tabu Search for Parameter Estimation in Multiview Geometry</a></p>
<p>15 0.45530134 <a title="434-lsi-15" href="./iccv-2013-Learning_to_Rank_Using_Privileged_Information.html">248 iccv-2013-Learning to Rank Using Privileged Information</a></p>
<p>16 0.44570222 <a title="434-lsi-16" href="./iccv-2013-An_Enhanced_Structure-from-Motion_Paradigm_Based_on_the_Absolute_Dual_Quadric_and_Images_of_Circular_Points.html">49 iccv-2013-An Enhanced Structure-from-Motion Paradigm Based on the Absolute Dual Quadric and Images of Circular Points</a></p>
<p>17 0.43486896 <a title="434-lsi-17" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>18 0.43361199 <a title="434-lsi-18" href="./iccv-2013-A_Generalized_Low-Rank_Appearance_Model_for_Spatio-temporally_Correlated_Rain_Streaks.html">15 iccv-2013-A Generalized Low-Rank Appearance Model for Spatio-temporally Correlated Rain Streaks</a></p>
<p>19 0.42672756 <a title="434-lsi-19" href="./iccv-2013-Single-Patch_Low-Rank_Prior_for_Non-pointwise_Impulse_Noise_Removal.html">394 iccv-2013-Single-Patch Low-Rank Prior for Non-pointwise Impulse Noise Removal</a></p>
<p>20 0.40184811 <a title="434-lsi-20" href="./iccv-2013-Joint_Noise_Level_Estimation_from_Personal_Photo_Collections.html">223 iccv-2013-Joint Noise Level Estimation from Personal Photo Collections</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.053), (7, 0.011), (26, 0.061), (27, 0.018), (31, 0.081), (42, 0.12), (48, 0.011), (64, 0.026), (73, 0.045), (78, 0.011), (89, 0.138), (98, 0.325)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80689669 <a title="434-lda-1" href="./iccv-2013-Unsupervised_Domain_Adaptation_by_Domain_Invariant_Projection.html">435 iccv-2013-Unsupervised Domain Adaptation by Domain Invariant Projection</a></p>
<p>Author: Mahsa Baktashmotlagh, Mehrtash T. Harandi, Brian C. Lovell, Mathieu Salzmann</p><p>Abstract: Domain-invariant representations are key to addressing the domain shift problem where the training and test examples follow different distributions. Existing techniques that have attempted to match the distributions of the source and target domains typically compare these distributions in the original feature space. This space, however, may not be directly suitable for such a comparison, since some of the features may have been distorted by the domain shift, or may be domain specific. In this paper, we introduce a Domain Invariant Projection approach: An unsupervised domain adaptation method that overcomes this issue by extracting the information that is invariant across the source and target domains. More specifically, we learn a projection of the data to a low-dimensional latent space where the distance between the empirical distributions of the source and target examples is minimized. We demonstrate the effectiveness of our approach on the task of visual object recognition and show that it outperforms state-of-the-art methods on a standard domain adaptation benchmark dataset.</p><p>2 0.79680932 <a title="434-lda-2" href="./iccv-2013-Unbiased_Metric_Learning%3A_On_the_Utilization_of_Multiple_Datasets_and_Web_Images_for_Softening_Bias.html">431 iccv-2013-Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias</a></p>
<p>Author: Chen Fang, Ye Xu, Daniel N. Rockmore</p><p>Abstract: Many standard computer vision datasets exhibit biases due to a variety of sources including illumination condition, imaging system, and preference of dataset collectors. Biases like these can have downstream effects in the use of vision datasets in the construction of generalizable techniques, especially for the goal of the creation of a classification system capable of generalizing to unseen and novel datasets. In this work we propose Unbiased Metric Learning (UML), a metric learning approach, to achieve this goal. UML operates in the following two steps: (1) By varying hyperparameters, it learns a set of less biased candidate distance metrics on training examples from multiple biased datasets. The key idea is to learn a neighborhood for each example, which consists of not only examples of the same category from the same dataset, but those from other datasets. The learning framework is based on structural SVM. (2) We do model validation on a set of weakly-labeled web images retrieved by issuing class labels as keywords to search engine. The metric with best validationperformance is selected. Although the web images sometimes have noisy labels, they often tend to be less biased, which makes them suitable for the validation set in our task. Cross-dataset image classification experiments are carried out. Results show significant performance improvement on four well-known computer vision datasets.</p><p>same-paper 3 0.78574097 <a title="434-lda-3" href="./iccv-2013-Unifying_Nuclear_Norm_and_Bilinear_Factorization_Approaches_for_Low-Rank_Matrix_Decomposition.html">434 iccv-2013-Unifying Nuclear Norm and Bilinear Factorization Approaches for Low-Rank Matrix Decomposition</a></p>
<p>Author: Ricardo Cabral, Fernando De_La_Torre, João P. Costeira, Alexandre Bernardino</p><p>Abstract: Low rank models have been widely usedfor the representation of shape, appearance or motion in computer vision problems. Traditional approaches to fit low rank models make use of an explicit bilinear factorization. These approaches benefit from fast numerical methods for optimization and easy kernelization. However, they suffer from serious local minima problems depending on the loss function and the amount/type of missing data. Recently, these lowrank models have alternatively been formulated as convex problems using the nuclear norm regularizer; unlike factorization methods, their numerical solvers are slow and it is unclear how to kernelize them or to impose a rank a priori. This paper proposes a unified approach to bilinear factorization and nuclear norm regularization, that inherits the benefits of both. We analyze the conditions under which these approaches are equivalent. Moreover, based on this analysis, we propose a new optimization algorithm and a “rank continuation ” strategy that outperform state-of-theart approaches for Robust PCA, Structure from Motion and Photometric Stereo with outliers and missing data.</p><p>4 0.77049071 <a title="434-lda-4" href="./iccv-2013-Modeling_the_Calibration_Pipeline_of_the_Lytro_Camera_for_High_Quality_Light-Field_Image_Reconstruction.html">271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</a></p>
<p>Author: Donghyeon Cho, Minhaeng Lee, Sunyeong Kim, Yu-Wing Tai</p><p>Abstract: Light-field imaging systems have got much attention recently as the next generation camera model. A light-field imaging system consists of three parts: data acquisition, manipulation, and application. Given an acquisition system, it is important to understand how a light-field camera converts from its raw image to its resulting refocused image. In this paper, using the Lytro camera as an example, we describe step-by-step procedures to calibrate a raw light-field image. In particular, we are interested in knowing the spatial and angular coordinates of the micro lens array and the resampling process for image reconstruction. Since Lytro uses a hexagonal arrangement of a micro lens image, additional treatments in calibration are required. After calibration, we analyze and compare the performances of several resampling methods for image reconstruction with and without calibration. Finally, a learning based interpolation method is proposed which demonstrates a higher quality image reconstruction than previous interpolation methods including a method used in Lytro software.</p><p>5 0.76314306 <a title="434-lda-5" href="./iccv-2013-A_Learning-Based_Approach_to_Reduce_JPEG_Artifacts_in_Image_Matting.html">19 iccv-2013-A Learning-Based Approach to Reduce JPEG Artifacts in Image Matting</a></p>
<p>Author: Inchang Choi, Sunyeong Kim, Michael S. Brown, Yu-Wing Tai</p><p>Abstract: Single image matting techniques assume high-quality input images. The vast majority of images on the web and in personal photo collections are encoded using JPEG compression. JPEG images exhibit quantization artifacts that adversely affect the performance of matting algorithms. To address this situation, we propose a learning-based post-processing method to improve the alpha mattes extracted from JPEG images. Our approach learns a set of sparse dictionaries from training examples that are used to transfer details from high-quality alpha mattes to alpha mattes corrupted by JPEG compression. Three different dictionaries are defined to accommodate different object structure (long hair, short hair, and sharp boundaries). A back-projection criteria combined within an MRF framework is used to automatically select the best dictionary to apply on the object’s local boundary. We demonstrate that our method can produces superior results over existing state-of-the-art matting algorithms on a variety of inputs and compression levels.</p><p>6 0.7537275 <a title="434-lda-6" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>7 0.7044189 <a title="434-lda-7" href="./iccv-2013-A_Unified_Video_Segmentation_Benchmark%3A_Annotation%2C_Metrics_and_Analysis.html">33 iccv-2013-A Unified Video Segmentation Benchmark: Annotation, Metrics and Analysis</a></p>
<p>8 0.65855318 <a title="434-lda-8" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>9 0.65843332 <a title="434-lda-9" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<p>10 0.65495038 <a title="434-lda-10" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>11 0.6437332 <a title="434-lda-11" href="./iccv-2013-Unsupervised_Visual_Domain_Adaptation_Using_Subspace_Alignment.html">438 iccv-2013-Unsupervised Visual Domain Adaptation Using Subspace Alignment</a></p>
<p>12 0.61146545 <a title="434-lda-12" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>13 0.59458828 <a title="434-lda-13" href="./iccv-2013-Robust_Matrix_Factorization_with_Unknown_Noise.html">357 iccv-2013-Robust Matrix Factorization with Unknown Noise</a></p>
<p>14 0.5931769 <a title="434-lda-14" href="./iccv-2013-Joint_Learning_of_Discriminative_Prototypes_and_Large_Margin_Nearest_Neighbor_Classifiers.html">222 iccv-2013-Joint Learning of Discriminative Prototypes and Large Margin Nearest Neighbor Classifiers</a></p>
<p>15 0.59008753 <a title="434-lda-15" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>16 0.58880299 <a title="434-lda-16" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>17 0.58827859 <a title="434-lda-17" href="./iccv-2013-Enhanced_Continuous_Tabu_Search_for_Parameter_Estimation_in_Multiview_Geometry.html">141 iccv-2013-Enhanced Continuous Tabu Search for Parameter Estimation in Multiview Geometry</a></p>
<p>18 0.58353901 <a title="434-lda-18" href="./iccv-2013-Active_Visual_Recognition_with_Expertise_Estimation_in_Crowdsourcing.html">43 iccv-2013-Active Visual Recognition with Expertise Estimation in Crowdsourcing</a></p>
<p>19 0.58279043 <a title="434-lda-19" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>20 0.58262688 <a title="434-lda-20" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
