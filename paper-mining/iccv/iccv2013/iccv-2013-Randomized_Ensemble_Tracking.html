<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>338 iccv-2013-Randomized Ensemble Tracking</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-338" href="#">iccv2013-338</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>338 iccv-2013-Randomized Ensemble Tracking</h1>
<br/><p>Source: <a title="iccv-2013-338-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Bai_Randomized_Ensemble_Tracking_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Qinxun Bai, Zheng Wu, Stan Sclaroff, Margrit Betke, Camille Monnier</p><p>Abstract: We propose a randomized ensemble algorithm to model the time-varying appearance of an object for visual tracking. In contrast with previous online methods for updating classifier ensembles in tracking-by-detection, the weight vector that combines weak classifiers is treated as a random variable and the posterior distribution for the weight vector is estimated in a Bayesian manner. In essence, the weight vector is treated as a distribution that reflects the confidence among the weak classifiers used to construct and adapt the classifier ensemble. The resulting formulation models the time-varying discriminative ability among weak classifiers so that the ensembled strong classifier can adapt to the varying appearance, backgrounds, and occlusions. The formulation is tested in a tracking-by-detection implementation. Experiments on 28 challenging benchmark videos demonstrate that the proposed method can achieve results comparable to and often better than those of stateof-the-art approaches.</p><p>Reference: <a title="iccv-2013-338-reference" href="../iccv2013_reference/iccv-2013-Randomized_Ensemble_Tracking_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In contrast with previous online methods for updating classifier ensembles in tracking-by-detection, the weight vector that combines weak classifiers is treated as a random variable and the posterior distribution for the weight vector is estimated in a Bayesian manner. [sent-2, score-1.337]
</p><p>2 In essence, the weight vector is treated as a distribution that reflects the confidence among the weak classifiers used to construct and adapt the classifier ensemble. [sent-3, score-0.978]
</p><p>3 The resulting formulation models the time-varying discriminative ability among weak classifiers so that the ensembled strong classifier can adapt to the varying appearance, backgrounds, and occlusions. [sent-4, score-0.835]
</p><p>4 One common approach for detector training is to use a detector ensemble framework that linearly combines the weak classifiers with different associated weights, e. [sent-11, score-0.839]
</p><p>5 A larger weight implies that the corresponding weak classifier is more discriminative and thus more useful. [sent-14, score-0.703]
</p><p>6 To date, most previous efforts have focused on adapting offline ensemble algorithms into online mode. [sent-15, score-0.411]
</p><p>7 This strategy, despite its success in many online visual learning tasks, has limitations in the visual tracking domain. [sent-16, score-0.265]
</p><p>8 It weights the “reliability” of the weak classifiers within the box (green means high weight; blue means low weight). [sent-19, score-0.718]
</p><p>9 In the left image, the person is unoccluded; therefore, high weights are associated with weak classifiers that cover the body. [sent-20, score-0.641]
</p><p>10 Thus, the ensemble tracker can distinguish the tracked person from the distractors. [sent-22, score-0.546]
</p><p>11 tribution does not apply in tracking scenarios where the appearance of an object can undergo such significant changes that a negative example in the current frame looks more similar to the positive example identified in the past (Fig. [sent-23, score-0.295]
</p><p>12 Given the uncertainty in the appearance changes that may occur over time and the difficulty of estimating the nonstationary distribution ofthis observed data directly, we propose a method that models how the classifier weights evolve according to a non-stationary distribution. [sent-25, score-0.511]
</p><p>13 Second, many online self-learning methods update the weights of their classifiers by first computing the importance weights of the incoming data. [sent-26, score-0.583]
</p><p>14 We suggest that this is an inherent challenge for online self-learning methods and propose an approach for estimating the ensemble weights that is Bayesian and ensures that the update of the ensemble weights is smooth. [sent-29, score-0.92]
</p><p>15 Our method models the weights of the classifier ensem-  ble with a non-stationary distribution, where the weight vector is a random variable whose instantiation can be interpreted as a representation of the “hidden state” of the combined strong classifier. [sent-30, score-0.477]
</p><p>16 Our method detects an object of interest by inferring the posterior distribution of the ensemble 2040 y U. [sent-31, score-0.463]
</p><p>17 Copyright  weights and computing the expected output of the ensemble classifier with respect to this “hidden state”. [sent-33, score-0.58]
</p><p>18 We propose a classifier ensemble framework for tracking-by-detection that uses Bayesian estimation theory to estimate the non-stationary distribution of classifier weights. [sent-37, score-0.79]
</p><p>19 Our randomized classifier encodes the “relative reliability” among a pool of weak classifiers, which provides a probabilistic interpretation of which features of the object are relatively more discriminative. [sent-39, score-0.907]
</p><p>20 By integrating a performance measure of the weak classifiers with a fine-grained object representation, our ensemble tracker is able to identify the most informative local patches of the object and successfully interpret partial occlusion and detect ambiguities due  to distractors. [sent-41, score-1.265]
</p><p>21 Our experiments demonstrate that the method can detect an object in tracking scenarios where the object undergoes strong appearance changes, where it moves and deforms, where the background changes significantly, and where distracting objects appear and interact with the object of interest. [sent-43, score-0.385]
</p><p>22 This makes the ensemble weights for weak classifiers informative, indicating the spatial spread of “discriminative ability” over the object template. [sent-49, score-0.947]
</p><p>23 Avi-  dan [2], who was the first to explicitly apply ensemble methods to tracking-by-detection, extended the work of [5] by adopting the Adaboost algorithm to combine a set of weak classifiers maintained with an online update strategy. [sent-53, score-1.096]
</p><p>24 [8] was extended from the online boosting algorithm [15] by introducing feature selection from a maintained pool of features for weak classifiers. [sent-55, score-0.763]
</p><p>25 [3] who adopted Multiple Instance Learning in designing weak classifiers. [sent-58, score-0.396]
</p><p>26 In a different approach [18], Random Forests undergo online update to grow and/or discard decision trees during tracking. [sent-59, score-0.257]
</p><p>27 Our online ensemble method is most related with online boosting scheme, in the sense that we adopt weighted combination of weak classifiers. [sent-60, score-1.041]
</p><p>28 However, we characterize the ensemble weight vector as a random variable and evolve its distribution with recursive Bayesian estimation. [sent-61, score-0.509]
</p><p>29 As a result, the final strong classifier is an expectation of the ensemble with respect to the weight vector, which is approximated by an average of instantiations of the randomized ensemble. [sent-62, score-0.926]
</p><p>30 To the best of our knowledge, in the context of tracking-bydetection, we are the first to present such an online learning scheme that characterizes the uncertainty of a self-learning algorithm and enables a Bayesian update of the classifier. [sent-63, score-0.322]
</p><p>31 At each time step, our method starts with the pool of weak classifiers C = {mce1t , c2 , d· ·s ·t , tcsN }w, a d thisetri pboutoioln o fD wire(Dak) over fitehers weight {vecctor ,D·· a·n ,dc input ddaistatr x. [sent-67, score-0.771]
</p><p>32 oOnu rD imr(etDho)d o dvievrid tehse wthee gihn-t put x into a regular grid of small patches, and sends the feature extracted from each small patch to its corresponding weak classifier. [sent-68, score-0.396]
</p><p>33 At the same time, our method also samples the distribution Dir(D) to obtain M instantiations D(1), D(2), · · · , D(M) of the weight vector D (color maps in Fig. [sent-69, score-0.25]
</p><p>34 2), ·a·n·d , Dcombines them with the output of weak classifiers to yield M ensembles of weak classifiers fD(1) , fD(2) , · · · , fD(M) . [sent-70, score-1.228]
</p><p>35 These M ensembles can be interpreted as M, ·i·n·sta ,fntiations of the randomized classifier fD and are used to compute the approximation F of the ex-  pected output of the randomized classifier fD. [sent-71, score-0.84]
</p><p>36 Notation for our classification method  imation F is considered the output of the strong classifier created by our ensemble scheme for input data x. [sent-73, score-0.567]
</p><p>37 To evaluate new input data from the next frame, our method updates the distribution Dir(D) in a Bayesian manner by observing the agreement of each weak classifier with the strong classifier ensemble. [sent-74, score-1.066]
</p><p>38 The method also updates the pool of weak classifiers according to the output of the strong classifier. [sent-75, score-0.814]
</p><p>39 Initialization of the model is performed in the first frame of the image sequence, where the pool of weak classifiers is initialized with  the given ground truth and the Dirichlet prior for weight vector D is initialized uniformly. [sent-85, score-0.841]
</p><p>40 Each weak classifier ci of the pool C is a binary classifier and outputs a label 1or 0o ffo thr ee pacoho input ada btian. [sent-90, score-0.961]
</p><p>41 Given a weight vector D, we obtain an ensemble binary classifier fD of the pool by thresholding the linear combination of outputs ci (x) of all weak classifiers:  fD(x) =⎨⎧01 if? [sent-91, score-1.155]
</p><p>42 Step 3: Compute ensemble output F(x) by voting (Eq. [sent-97, score-0.354]
</p><p>43 To obtain the final ensemble classifier for input x, our method approximates Eq. [sent-132, score-0.479]
</p><p>44 Model Update Our method updates both the Dirichlet distribution of weight vectors and the pool of weak classifiers after the classification stage in each time step, so that the model can evolve. [sent-141, score-0.93]
</p><p>45 In fact, our online ensemble method as well as its update scheme does not enforce any constraints on the form of the weak classifiers, as long as each weak classifier is able to cast a vote for every input sample. [sent-143, score-1.499]
</p><p>46 The construction of weak classifiers and the mechanism of updating them can be chosen in an application-specific way. [sent-144, score-0.567]
</p><p>47 For each step, after performing the classification, our method obtains the labels of data predicted by our strong classifier F and the observation of performance of weak classifiers, that is, the prediction consistency of weak classifiers with respect to the strong classifier. [sent-145, score-1.292]
</p><p>48 Throughout this paper, we use the terms “consistency” or “consistent” to indicate agreement with the strong classifier F. [sent-146, score-0.304]
</p><p>49 Since the weight vector D is a measure of the “relative reliability” over the pool of classifiers, its posterior is a function of the “observation of relative reliability of each classifier. [sent-148, score-0.346]
</p><p>50 ” To formally represent it, we consider a performance measure of each weak classifier ci, which we denote as gi, while (g1, g2 , . [sent-149, score-0.603]
</p><p>51 Given a weight  vector D, gi should have an expectation proportional to the weight value di. [sent-153, score-0.39]
</p><p>52 Recall that the expectation of occurrence rate of a particular outcome in a multinomial distribution is just the distribution parameter for that outcome. [sent-154, score-0.413]
</p><p>53 Hence, if we regard a given weight vector D as multinomial parameter, gi could be regarded as the “rate of being a reliable classifier” as analogous to occurrence rate. [sent-155, score-0.28]
</p><p>54 Second, “positive” and “negative” weak classifiers should be evaluated symmetrically with respect to some neutral value; neutral values account for cases where the observations of classifier performance may be ambiguous or missing, e. [sent-177, score-0.774]
</p><p>55 We choose the following function: g : {1, 2, ·  · ·  , N} → [0, 2]  gi= g(i)  =1 + e2−siwi,  (11)  where si and wi denote the sign and weight respectively, whose values are determined by comparing the output of the voting classifier F with the output of the weak classifier ci. [sent-180, score-1.052]
</p><p>56 The weight wi sise tth toen b seet 1 t,o o tthheer margin accordingly indicating “goodness” of a positive weak classifier or “badness” of a negative weak classifier, as described in Algorithm-2. [sent-182, score-1.132]
</p><p>57 Note that the range of gi is a nonnegative real interval instead of the nonnegative integers in the conventional multinomial distribution. [sent-183, score-0.262]
</p><p>58 We now describe issues about the update of weak classifiers. [sent-202, score-0.485]
</p><p>59 : compute the sign si for each weak learner ci,  •  si=? [sent-207, score-0.396]
</p><p>60 : compute weight  wi,  ssii== − 11  End Step 3: update Dirichlet base distribution H via Eq. [sent-211, score-0.366]
</p><p>61 11 Step 4: update Dirichlet concentration parameter α by  gi  α? [sent-213, score-0.26]
</p><p>62 In our method, the normalized base distribution H characterizes the expected “relative reliability” of the weak classifiers. [sent-215, score-0.64]
</p><p>63 After multiplying H with the concentration parameter α, which characterizes our confidence in H, an “expected performance state” of each weak classifier could be obtained, i. [sent-216, score-0.725]
</p><p>64 Comparing it with 1 (the ncoeuutlrdal b vea olubeta oinfe tdh,e performance Cmomeaspuarrien),g we can d 1ec (itdhee whether a weak classifier is better than a random guess, i. [sent-219, score-0.603]
</p><p>65 ” By default, when the proportion of “good” weak classifiers is less than 50%, our system decides not to update the weak classifiers because the detected object is very likely occluded. [sent-222, score-1.257]
</p><p>66 This design turned out to be effective in helping our tracker recover from long-term full occlusions in our experiments. [sent-223, score-0.274]
</p><p>67 Experiments We tested our tracker on 28 video sequences, 27 of  ×  which are publicly available. [sent-225, score-0.274]
</p><p>68 Including gth beo exn intirtoe bounding ×bo4x ivtesnellfy, 21 additional weak classifiers are produced in these three scales. [sent-250, score-0.63]
</p><p>69 Each weak classifier corresponding to the local patch is a standard linear SVM, which is trained with its own buffer of 50 positive and 50 negative examples. [sent-252, score-0.642]
</p><p>70 The buffers and the weak classifiers are initialized with the ground truth bounding box and its shifted versions in the first frame. [sent-253, score-0.742]
</p><p>71 During tracking, whenever a new example is added to the buffer, the weak classifier is retrained. [sent-254, score-0.603]
</p><p>72 ) simple trackers that focus more on object representation, which include Compressive Tracker (CT) [21], Distribution Field (DF) [20] and 2044  Fragments-based tracker (Frag) [1]; iii. [sent-282, score-0.345]
</p><p>73 ) online-learning based trackers, which include the Multiple Instance Learning based tracker (MIL) [3] and Structured output tracker (Struck) [10]. [sent-283, score-0.575]
</p><p>74 The second baseline (OB) employs the same object representation and weak classifiers used in our tracker and the ensemble strategy is online boosting [15, 8, 3]. [sent-289, score-1.44]
</p><p>75 The implementation of our randomized ensember tracker (RET) employs 5000 samples drawn from the Dirichlet distribution. [sent-290, score-0.44]
</p><p>76 We also tested a deterministic version of our tracker (DET) that replaces the sampling step by using the mean distribution H directly (i. [sent-293, score-0.438]
</p><p>77 We considered TA and AOR, with ideal val-  ues equal to 1, as more informative metrics than ACLE, because when the tracker drifts the ACLE score can grow arbitrarily large. [sent-298, score-0.274]
</p><p>78 Our randomized ensemble tracker showed top/equivalently top performance on 14 out of 28 sequences. [sent-307, score-0.712]
</p><p>79 Our tracker attained high accuracy and robustness across diverse sequences; this is particularly good, considering that our method does not rely on motion prediction. [sent-308, score-0.274]
</p><p>80 By using a fine-grained representation and identifying the most discriminative local patches, our tracker is less likely to be affected by local drastic changes. [sent-312, score-0.274]
</p><p>81 The online boosting tracker evaluates the weak classifier by its error rate on training examples; such estimation makes sense only if the training data is generated from a fixed joint distribution and the label for the training data is given for sure. [sent-314, score-1.25]
</p><p>82 In contrast with OB, our method evaluates the performance of a weak classifier based upon its consistency, a completely different strategy, and the strong classifier is updated implicitly by Bayesian filtering the weighting vector (“hidden state”) smoothly. [sent-316, score-0.871]
</p><p>83 Therefore, our tracker is less vulnerable to a time-varying joint distribution. [sent-317, score-0.274]
</p><p>84 The randomized and deterministic variants of our ensemble trackers (RET, DET) are roughly comparable. [sent-318, score-0.535]
</p><p>85 However, we found that it is still less accurate than the randomized version, when the appearance of the object changes fast or undergoes a severe partial occlusion, such as in “skating2,” “ETH” and “walking”. [sent-320, score-0.296]
</p><p>86 The baseline methods are linear SVM (SVM) and online boosting (OB). [sent-327, score-0.266]
</p><p>87 RET and DET are the randomized and deterministic variants of our ensemble tracker formulation. [sent-328, score-0.772]
</p><p>88 ing conditions, we took snapshots of our learned ensemble classifier and show in Fig. [sent-337, score-0.518]
</p><p>89 3 the base distribution H of the Dirichlet distribution, which characterizes the relative importance of each weak classifier. [sent-338, score-0.64]
</p><p>90 3(a), the bounding box is not tight around the circuit board object, so patches in the box corners are actually background. [sent-343, score-0.296]
</p><p>91 3(b), an occlusion of the face causes the weak classifiers that account for the occluded region of the face to  disagree with the strong classifier. [sent-347, score-0.701]
</p><p>92 3(d), a pedestrian (shown on the top left corner) is completely occluded by a distracter (man with beigejacket), so the majority of the weak classifiers disagrees with the strong classifier and the weights for the weak classifiers in the corner of the bounding box are strengthened. [sent-352, score-1.732]
</p><p>93 We also witnessed a large variance of our RET tracker on “singer2” and “shaking” when non-smooth environment changes occur all the time. [sent-361, score-0.342]
</p><p>94 Discussion  We proposed a tracker that exploits a novel online randomized classifier ensemble method that naturally evolves the classifier in a Bayesian manner. [sent-364, score-1.265]
</p><p>95 Sample images with true detections (green), false alarms (red), and ground truth (yellow) and snapshots of base distribution H of Dirichlet distribution (greener means higher weight of the associated weak classifier and its higher discriminate ability, bluer means lower  weight). [sent-366, score-1.023]
</p><p>96 compute  deterministic optimal weights for the weak clas-  sifiers, we characterize their uncertainty by introducing the Dirichlet distribution, and draw random samples to form a randomized voting classifier. [sent-368, score-0.821]
</p><p>97 Our randomized ensemble tracker was tested in experiments on numerous tracking sequences, demonstating the robustness of our method compared to state-of-the-art approaches, even without motion prediction. [sent-369, score-0.838]
</p><p>98 Our framework is flexible, since our learning strategy does not restrict the type of weak classifier that can be used. [sent-370, score-0.63]
</p><p>99 In future work, we are interested in building a larger pool of weak classifiers and experimenting with different features. [sent-371, score-0.671]
</p><p>100 Since our method is general, we plan to apply it to other tasks where online classifier ensembles are used. [sent-373, score-0.413]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('weak', 0.396), ('tracker', 0.274), ('ensemble', 0.272), ('dirichlet', 0.253), ('classifier', 0.207), ('fd', 0.201), ('dir', 0.184), ('classifiers', 0.171), ('randomized', 0.166), ('online', 0.139), ('acle', 0.131), ('tracking', 0.126), ('gi', 0.116), ('pool', 0.104), ('distribution', 0.104), ('vtd', 0.104), ('weight', 0.1), ('ret', 0.096), ('boosting', 0.095), ('reliability', 0.089), ('update', 0.089), ('sequences', 0.08), ('saffari', 0.079), ('distracter', 0.078), ('det', 0.078), ('prost', 0.077), ('box', 0.077), ('weights', 0.074), ('expectation', 0.074), ('base', 0.073), ('gating', 0.07), ('ensembles', 0.067), ('characterizes', 0.067), ('multinomial', 0.064), ('bounding', 0.063), ('bayesian', 0.062), ('strong', 0.061), ('aor', 0.061), ('deterministic', 0.06), ('ob', 0.059), ('voting', 0.055), ('updates', 0.055), ('concentration', 0.055), ('distractors', 0.055), ('santner', 0.053), ('posterior', 0.053), ('gn', 0.051), ('tld', 0.05), ('patches', 0.049), ('ci', 0.047), ('instantiations', 0.046), ('polluted', 0.046), ('ta', 0.045), ('leistner', 0.044), ('draw', 0.043), ('liquor', 0.043), ('grabner', 0.043), ('nonnegative', 0.041), ('looks', 0.04), ('buffer', 0.039), ('snapshots', 0.039), ('occluded', 0.038), ('hi', 0.038), ('dy', 0.038), ('trackers', 0.037), ('underlined', 0.037), ('changes', 0.037), ('incoming', 0.036), ('agreement', 0.036), ('initialized', 0.035), ('searches', 0.035), ('rate', 0.035), ('occlusion', 0.035), ('instantiation', 0.035), ('boston', 0.035), ('struck', 0.035), ('godec', 0.035), ('object', 0.034), ('babenko', 0.034), ('evolve', 0.033), ('wi', 0.033), ('baseline', 0.032), ('unoccluded', 0.032), ('outcome', 0.032), ('witnessed', 0.031), ('dd', 0.03), ('undergoes', 0.03), ('reports', 0.03), ('board', 0.03), ('df', 0.03), ('compressive', 0.029), ('thresholding', 0.029), ('wiley', 0.029), ('undergo', 0.029), ('maintained', 0.029), ('appearance', 0.029), ('collins', 0.028), ('uncertainty', 0.027), ('strategy', 0.027), ('output', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="338-tfidf-1" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>Author: Qinxun Bai, Zheng Wu, Stan Sclaroff, Margrit Betke, Camille Monnier</p><p>Abstract: We propose a randomized ensemble algorithm to model the time-varying appearance of an object for visual tracking. In contrast with previous online methods for updating classifier ensembles in tracking-by-detection, the weight vector that combines weak classifiers is treated as a random variable and the posterior distribution for the weight vector is estimated in a Bayesian manner. In essence, the weight vector is treated as a distribution that reflects the confidence among the weak classifiers used to construct and adapt the classifier ensemble. The resulting formulation models the time-varying discriminative ability among weak classifiers so that the ensembled strong classifier can adapt to the varying appearance, backgrounds, and occlusions. The formulation is tested in a tracking-by-detection implementation. Experiments on 28 challenging benchmark videos demonstrate that the proposed method can achieve results comparable to and often better than those of stateof-the-art approaches.</p><p>2 0.31760961 <a title="338-tfidf-2" href="./iccv-2013-Handling_Occlusions_with_Franken-Classifiers.html">190 iccv-2013-Handling Occlusions with Franken-Classifiers</a></p>
<p>Author: Markus Mathias, Rodrigo Benenson, Radu Timofte, Luc Van_Gool</p><p>Abstract: Detecting partially occluded pedestrians is challenging. A common practice to maximize detection quality is to train a set of occlusion-specific classifiers, each for a certain amount and type of occlusion. Since training classifiers is expensive, only a handful are typically trained. We show that by using many occlusion-specific classifiers, we outperform previous approaches on three pedestrian datasets; INRIA, ETH, and Caltech USA. We present a new approach to train such classifiers. By reusing computations among different training stages, 16 occlusion-specific classifiers can be trained at only one tenth the cost of one full training. We show that also test time cost grows sub-linearly.</p><p>3 0.21715611 <a title="338-tfidf-3" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>Author: Shuran Song, Jianxiong Xiao</p><p>Abstract: Despite significant progress, tracking is still considered to be a very challenging task. Recently, the increasing popularity of depth sensors has made it possible to obtain reliable depth easily. This may be a game changer for tracking, since depth can be used to prevent model drift and handle occlusion. We also observe that current tracking algorithms are mostly evaluated on a very small number of videos collectedandannotated by different groups. The lack of a reasonable size and consistently constructed benchmark has prevented a persuasive comparison among different algorithms. In this paper, we construct a unified benchmark dataset of 100 RGBD videos with high diversity, propose different kinds of RGBD tracking algorithms using 2D or 3D model, and present a quantitative comparison of various algorithms with RGB or RGBD input. We aim to lay the foundation for further research in both RGB and RGBD tracking, and our benchmark is available at http://tracking.cs.princeton.edu.</p><p>4 0.19498636 <a title="338-tfidf-4" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>Author: Stefan Duffner, Christophe Garcia</p><p>Abstract: In this paper, we present a novel algorithm for fast tracking of generic objects in videos. The algorithm uses two components: a detector that makes use of the generalised Hough transform with pixel-based descriptors, and a probabilistic segmentation method based on global models for foreground and background. These components are used for tracking in a combined way, and they adapt each other in a co-training manner. Through effective model adaptation and segmentation, the algorithm is able to track objects that undergo rigid and non-rigid deformations and considerable shape and appearance variations. The proposed tracking method has been thoroughly evaluated on challenging standard videos, and outperforms state-of-theart tracking methods designed for the same task. Finally, the proposed models allow for an extremely efficient implementation, and thus tracking is very fast.</p><p>5 0.19177595 <a title="338-tfidf-5" href="./iccv-2013-Online_Robust_Non-negative_Dictionary_Learning_for_Visual_Tracking.html">298 iccv-2013-Online Robust Non-negative Dictionary Learning for Visual Tracking</a></p>
<p>Author: Naiyan Wang, Jingdong Wang, Dit-Yan Yeung</p><p>Abstract: This paper studies the visual tracking problem in video sequences and presents a novel robust sparse tracker under the particle filter framework. In particular, we propose an online robust non-negative dictionary learning algorithm for updating the object templates so that each learned template can capture a distinctive aspect of the tracked object. Another appealing property of this approach is that it can automatically detect and reject the occlusion and cluttered background in a principled way. In addition, we propose a new particle representation formulation using the Huber loss function. The advantage is that it can yield robust estimation without using trivial templates adopted by previous sparse trackers, leading to faster computation. We also reveal the equivalence between this new formulation and the previous one which uses trivial templates. The proposed tracker is empirically compared with state-of-the-art trackers on some challenging video sequences. Both quantitative and qualitative comparisons show that our proposed tracker is superior and more stable.</p><p>6 0.17311195 <a title="338-tfidf-6" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>7 0.16616403 <a title="338-tfidf-7" href="./iccv-2013-Finding_the_Best_from_the_Second_Bests_-_Inhibiting_Subjective_Bias_in_Evaluation_of_Visual_Tracking_Algorithms.html">168 iccv-2013-Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms</a></p>
<p>8 0.16424662 <a title="338-tfidf-8" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>9 0.14662394 <a title="338-tfidf-9" href="./iccv-2013-Efficient_Pedestrian_Detection_by_Directly_Optimizing_the_Partial_Area_under_the_ROC_Curve.html">136 iccv-2013-Efficient Pedestrian Detection by Directly Optimizing the Partial Area under the ROC Curve</a></p>
<p>10 0.14423452 <a title="338-tfidf-10" href="./iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects.html">320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</a></p>
<p>11 0.13407467 <a title="338-tfidf-11" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>12 0.12753691 <a title="338-tfidf-12" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>13 0.12659603 <a title="338-tfidf-13" href="./iccv-2013-Ensemble_Projection_for_Semi-supervised_Image_Classification.html">142 iccv-2013-Ensemble Projection for Semi-supervised Image Classification</a></p>
<p>14 0.1225846 <a title="338-tfidf-14" href="./iccv-2013-Real-Time_Body_Tracking_with_One_Depth_Camera_and_Inertial_Sensors.html">341 iccv-2013-Real-Time Body Tracking with One Depth Camera and Inertial Sensors</a></p>
<p>15 0.11807696 <a title="338-tfidf-15" href="./iccv-2013-Random_Forests_of_Local_Experts_for_Pedestrian_Detection.html">336 iccv-2013-Random Forests of Local Experts for Pedestrian Detection</a></p>
<p>16 0.11418691 <a title="338-tfidf-16" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>17 0.10998522 <a title="338-tfidf-17" href="./iccv-2013-CoDeL%3A_A_Human_Co-detection_and_Labeling_Framework.html">75 iccv-2013-CoDeL: A Human Co-detection and Labeling Framework</a></p>
<p>18 0.1082957 <a title="338-tfidf-18" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>19 0.10794749 <a title="338-tfidf-19" href="./iccv-2013-Sequential_Bayesian_Model_Update_under_Structured_Scene_Prior_for_Semantic_Road_Scenes_Labeling.html">386 iccv-2013-Sequential Bayesian Model Update under Structured Scene Prior for Semantic Road Scenes Labeling</a></p>
<p>20 0.10111054 <a title="338-tfidf-20" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.241), (1, -0.002), (2, 0.002), (3, -0.02), (4, 0.058), (5, -0.095), (6, -0.1), (7, 0.174), (8, -0.09), (9, 0.087), (10, -0.058), (11, -0.177), (12, 0.049), (13, 0.004), (14, 0.09), (15, -0.092), (16, 0.044), (17, 0.046), (18, -0.014), (19, 0.015), (20, -0.142), (21, 0.019), (22, -0.127), (23, 0.002), (24, -0.075), (25, 0.0), (26, 0.009), (27, 0.048), (28, 0.04), (29, -0.072), (30, -0.08), (31, -0.01), (32, -0.089), (33, 0.037), (34, -0.03), (35, -0.064), (36, 0.01), (37, -0.028), (38, -0.057), (39, 0.005), (40, 0.077), (41, 0.016), (42, -0.096), (43, 0.085), (44, 0.046), (45, 0.001), (46, 0.055), (47, 0.027), (48, 0.101), (49, -0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97546989 <a title="338-lsi-1" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>Author: Qinxun Bai, Zheng Wu, Stan Sclaroff, Margrit Betke, Camille Monnier</p><p>Abstract: We propose a randomized ensemble algorithm to model the time-varying appearance of an object for visual tracking. In contrast with previous online methods for updating classifier ensembles in tracking-by-detection, the weight vector that combines weak classifiers is treated as a random variable and the posterior distribution for the weight vector is estimated in a Bayesian manner. In essence, the weight vector is treated as a distribution that reflects the confidence among the weak classifiers used to construct and adapt the classifier ensemble. The resulting formulation models the time-varying discriminative ability among weak classifiers so that the ensembled strong classifier can adapt to the varying appearance, backgrounds, and occlusions. The formulation is tested in a tracking-by-detection implementation. Experiments on 28 challenging benchmark videos demonstrate that the proposed method can achieve results comparable to and often better than those of stateof-the-art approaches.</p><p>2 0.74708456 <a title="338-lsi-2" href="./iccv-2013-Finding_the_Best_from_the_Second_Bests_-_Inhibiting_Subjective_Bias_in_Evaluation_of_Visual_Tracking_Algorithms.html">168 iccv-2013-Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms</a></p>
<p>Author: Yu Pang, Haibin Ling</p><p>Abstract: Evaluating visual tracking algorithms, or “trackers ” for short, is of great importance in computer vision. However, it is hard to “fairly” compare trackers due to many parameters need to be tuned in the experimental configurations. On the other hand, when introducing a new tracker, a recent trend is to validate it by comparing it with several existing ones. Such an evaluation may have subjective biases towards the new tracker which typically performs the best. This is mainly due to the difficulty to optimally tune all its competitors and sometimes the selected testing sequences. By contrast, little subjective bias exists towards the “second best” ones1 in the contest. This observation inspires us with a novel perspective towards inhibiting subjective bias in evaluating trackers by analyzing the results between the second bests. In particular, we first collect all tracking papers published in major computer vision venues in recent years. From these papers, after filtering out potential biases in various aspects, we create a dataset containing many records of comparison results between various visual trackers. Using these records, we derive performance rank- ings of the involved trackers by four different methods. The first two methods model the dataset as a graph and then derive the rankings over the graph, one by a rank aggregation algorithm and the other by a PageRank-like solution. The other two methods take the records as generated from sports contests and adopt widely used Elo’s and Glicko ’s rating systems to derive the rankings. The experimental results are presented and may serve as a reference for related research.</p><p>3 0.74306738 <a title="338-lsi-3" href="./iccv-2013-Handling_Occlusions_with_Franken-Classifiers.html">190 iccv-2013-Handling Occlusions with Franken-Classifiers</a></p>
<p>Author: Markus Mathias, Rodrigo Benenson, Radu Timofte, Luc Van_Gool</p><p>Abstract: Detecting partially occluded pedestrians is challenging. A common practice to maximize detection quality is to train a set of occlusion-specific classifiers, each for a certain amount and type of occlusion. Since training classifiers is expensive, only a handful are typically trained. We show that by using many occlusion-specific classifiers, we outperform previous approaches on three pedestrian datasets; INRIA, ETH, and Caltech USA. We present a new approach to train such classifiers. By reusing computations among different training stages, 16 occlusion-specific classifiers can be trained at only one tenth the cost of one full training. We show that also test time cost grows sub-linearly.</p><p>4 0.72576815 <a title="338-lsi-4" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>Author: Siyu Tang, Mykhaylo Andriluka, Anton Milan, Konrad Schindler, Stefan Roth, Bernt Schiele</p><p>Abstract: People tracking in crowded real-world scenes is challenging due to frequent and long-term occlusions. Recent tracking methods obtain the image evidence from object (people) detectors, but typically use off-the-shelf detectors and treat them as black box components. In this paper we argue that for best performance one should explicitly train people detectors on failure cases of the overall tracker instead. To that end, we first propose a novel joint people detector that combines a state-of-the-art single person detector with a detector for pairs of people, which explicitly exploits common patterns of person-person occlusions across multiple viewpoints that are a frequent failure case for tracking in crowded scenes. To explicitly address remaining failure modes of the tracker we explore two methods. First, we analyze typical failures of trackers and train a detector explicitly on these cases. And second, we train the detector with the people tracker in the loop, focusing on the most common tracker failures. We show that our joint multi-person detector significantly improves both de- tection accuracy as well as tracker performance, improving the state-of-the-art on standard benchmarks.</p><p>5 0.70207232 <a title="338-lsi-5" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>Author: Shuran Song, Jianxiong Xiao</p><p>Abstract: Despite significant progress, tracking is still considered to be a very challenging task. Recently, the increasing popularity of depth sensors has made it possible to obtain reliable depth easily. This may be a game changer for tracking, since depth can be used to prevent model drift and handle occlusion. We also observe that current tracking algorithms are mostly evaluated on a very small number of videos collectedandannotated by different groups. The lack of a reasonable size and consistently constructed benchmark has prevented a persuasive comparison among different algorithms. In this paper, we construct a unified benchmark dataset of 100 RGBD videos with high diversity, propose different kinds of RGBD tracking algorithms using 2D or 3D model, and present a quantitative comparison of various algorithms with RGB or RGBD input. We aim to lay the foundation for further research in both RGB and RGBD tracking, and our benchmark is available at http://tracking.cs.princeton.edu.</p><p>6 0.68179852 <a title="338-lsi-6" href="./iccv-2013-Efficient_Pedestrian_Detection_by_Directly_Optimizing_the_Partial_Area_under_the_ROC_Curve.html">136 iccv-2013-Efficient Pedestrian Detection by Directly Optimizing the Partial Area under the ROC Curve</a></p>
<p>7 0.66694844 <a title="338-lsi-7" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>8 0.65736467 <a title="338-lsi-8" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>9 0.64799947 <a title="338-lsi-9" href="./iccv-2013-Online_Robust_Non-negative_Dictionary_Learning_for_Visual_Tracking.html">298 iccv-2013-Online Robust Non-negative Dictionary Learning for Visual Tracking</a></p>
<p>10 0.62871861 <a title="338-lsi-10" href="./iccv-2013-Orderless_Tracking_through_Model-Averaged_Posterior_Estimation.html">303 iccv-2013-Orderless Tracking through Model-Averaged Posterior Estimation</a></p>
<p>11 0.61595309 <a title="338-lsi-11" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>12 0.60949117 <a title="338-lsi-12" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>13 0.60406536 <a title="338-lsi-13" href="./iccv-2013-Random_Forests_of_Local_Experts_for_Pedestrian_Detection.html">336 iccv-2013-Random Forests of Local Experts for Pedestrian Detection</a></p>
<p>14 0.59800661 <a title="338-lsi-14" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>15 0.59032857 <a title="338-lsi-15" href="./iccv-2013-Conservation_Tracking.html">87 iccv-2013-Conservation Tracking</a></p>
<p>16 0.58875477 <a title="338-lsi-16" href="./iccv-2013-Slice_Sampling_Particle_Belief_Propagation.html">395 iccv-2013-Slice Sampling Particle Belief Propagation</a></p>
<p>17 0.57005751 <a title="338-lsi-17" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>18 0.56370568 <a title="338-lsi-18" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<p>19 0.55590922 <a title="338-lsi-19" href="./iccv-2013-Relative_Attributes_for_Large-Scale_Abandoned_Object_Detection.html">350 iccv-2013-Relative Attributes for Large-Scale Abandoned Object Detection</a></p>
<p>20 0.55182302 <a title="338-lsi-20" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.054), (7, 0.028), (12, 0.051), (21, 0.093), (26, 0.078), (31, 0.053), (34, 0.013), (40, 0.035), (42, 0.138), (48, 0.016), (64, 0.099), (73, 0.048), (89, 0.145), (95, 0.012), (97, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91083312 <a title="338-lda-1" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>Author: Qinxun Bai, Zheng Wu, Stan Sclaroff, Margrit Betke, Camille Monnier</p><p>Abstract: We propose a randomized ensemble algorithm to model the time-varying appearance of an object for visual tracking. In contrast with previous online methods for updating classifier ensembles in tracking-by-detection, the weight vector that combines weak classifiers is treated as a random variable and the posterior distribution for the weight vector is estimated in a Bayesian manner. In essence, the weight vector is treated as a distribution that reflects the confidence among the weak classifiers used to construct and adapt the classifier ensemble. The resulting formulation models the time-varying discriminative ability among weak classifiers so that the ensembled strong classifier can adapt to the varying appearance, backgrounds, and occlusions. The formulation is tested in a tracking-by-detection implementation. Experiments on 28 challenging benchmark videos demonstrate that the proposed method can achieve results comparable to and often better than those of stateof-the-art approaches.</p><p>2 0.88187361 <a title="338-lda-2" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>Author: Zhibin Hong, Xue Mei, Danil Prokhorov, Dacheng Tao</p><p>Abstract: Combining multiple observation views has proven beneficial for tracking. In this paper, we cast tracking as a novel multi-task multi-view sparse learning problem and exploit the cues from multiple views including various types of visual features, such as intensity, color, and edge, where each feature observation can be sparsely represented by a linear combination of atoms from an adaptive feature dictionary. The proposed method is integrated in a particle filter framework where every view in each particle is regarded as an individual task. We jointly consider the underlying relationship between tasks across different views and different particles, and tackle it in a unified robust multi-task formulation. In addition, to capture the frequently emerging outlier tasks, we decompose the representation matrix to two collaborative components which enable a more robust and accurate approximation. We show that theproposedformulation can be efficiently solved using the Accelerated Proximal Gradient method with a small number of closed-form updates. The presented tracker is implemented using four types of features and is tested on numerous benchmark video sequences. Both the qualitative and quantitative results demonstrate the superior performance of the proposed approach compared to several stateof-the-art trackers.</p><p>3 0.88087618 <a title="338-lda-3" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>Author: Junliang Xing, Jin Gao, Bing Li, Weiming Hu, Shuicheng Yan</p><p>Abstract: Recently, sparse representation has been introduced for robust object tracking. By representing the object sparsely, i.e., using only a few templates via ?1-norm minimization, these so-called ?1-trackers exhibit promising tracking results. In this work, we address the object template building and updating problem in these ?1-tracking approaches, which has not been fully studied. We propose to perform template updating, in a new perspective, as an online incremental dictionary learning problem, which is efficiently solved through an online optimization procedure. To guarantee the robustness and adaptability of the tracking algorithm, we also propose to build a multi-lifespan dictionary model. By building target dictionaries of different lifespans, effective object observations can be obtained to deal with the well-known drifting problem in tracking and thus improve the tracking accuracy. We derive effective observa- tion models both generatively and discriminatively based on the online multi-lifespan dictionary learning model and deploy them to the Bayesian sequential estimation framework to perform tracking. The proposed approach has been extensively evaluated on ten challenging video sequences. Experimental results demonstrate the effectiveness of the online learned templates, as well as the state-of-the-art tracking performance of the proposed approach.</p><p>4 0.87766635 <a title="338-lda-4" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>Author: Mihai Zanfir, Marius Leordeanu, Cristian Sminchisescu</p><p>Abstract: Human action recognition under low observational latency is receiving a growing interest in computer vision due to rapidly developing technologies in human-robot interaction, computer gaming and surveillance. In this paper we propose a fast, simple, yet powerful non-parametric Moving Pose (MP)frameworkfor low-latency human action and activity recognition. Central to our methodology is a moving pose descriptor that considers both pose information as well as differential quantities (speed and acceleration) of the human body joints within a short time window around the current frame. The proposed descriptor is used in conjunction with a modified kNN classifier that considers both the temporal location of a particular frame within the action sequence as well as the discrimination power of its moving pose descriptor compared to other frames in the training set. The resulting method is non-parametric and enables low-latency recognition, one-shot learning, and action detection in difficult unsegmented sequences. Moreover, the framework is real-time, scalable, and outperforms more sophisticated approaches on challenging benchmarks like MSR-Action3D or MSR-DailyActivities3D.</p><p>5 0.87404281 <a title="338-lda-5" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>Author: S. Karthikeyan, Vignesh Jagadeesh, Renuka Shenoy, Miguel Ecksteinz, B.S. Manjunath</p><p>Abstract: Eye movement studies have confirmed that overt attention is highly biased towards faces and text regions in images. In this paper we explore a novel problem of predicting face and text regions in images using eye tracking data from multiple subjects. The problem is challenging as we aim to predict the semantics (face/text/background) only from eye tracking data without utilizing any image information. The proposed algorithm spatially clusters eye tracking data obtained in an image into different coherent groups and subsequently models the likelihood of the clusters containing faces and text using afully connectedMarkov Random Field (MRF). Given the eye tracking datafrom a test image, itpredicts potential face/head (humans, dogs and cats) and text locations reliably. Furthermore, the approach can be used to select regions of interest for further analysis by object detectors for faces and text. The hybrid eye position/object detector approach achieves better detection performance and reduced computation time compared to using only the object detection algorithm. We also present a new eye tracking dataset on 300 images selected from ICDAR, Street-view, Flickr and Oxford-IIIT Pet Dataset from 15 subjects.</p><p>6 0.86764252 <a title="338-lda-6" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>7 0.86610019 <a title="338-lda-7" href="./iccv-2013-Person_Re-identification_by_Salience_Matching.html">313 iccv-2013-Person Re-identification by Salience Matching</a></p>
<p>8 0.86543357 <a title="338-lda-8" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>9 0.86529928 <a title="338-lda-9" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>10 0.86460561 <a title="338-lda-10" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>11 0.86324179 <a title="338-lda-11" href="./iccv-2013-Bayesian_Joint_Topic_Modelling_for_Weakly_Supervised_Object_Localisation.html">59 iccv-2013-Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation</a></p>
<p>12 0.86297995 <a title="338-lda-12" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>13 0.86230195 <a title="338-lda-13" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>14 0.86097598 <a title="338-lda-14" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>15 0.86090732 <a title="338-lda-15" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>16 0.8607589 <a title="338-lda-16" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>17 0.8602038 <a title="338-lda-17" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>18 0.86012173 <a title="338-lda-18" href="./iccv-2013-Incorporating_Cloud_Distribution_in_Sky_Representation.html">215 iccv-2013-Incorporating Cloud Distribution in Sky Representation</a></p>
<p>19 0.85989654 <a title="338-lda-19" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>20 0.85945714 <a title="338-lda-20" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
