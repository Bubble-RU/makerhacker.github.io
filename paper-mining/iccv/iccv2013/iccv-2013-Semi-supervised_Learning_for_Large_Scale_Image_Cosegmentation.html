<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-383" href="#">iccv2013-383</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</h1>
<br/><p>Source: <a title="iccv-2013-383-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Wang_Semi-supervised_Learning_for_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Zhengxiang Wang, Rujie Liu</p><p>Abstract: This paper introduces to use semi-supervised learning for large scale image cosegmentation. Different from traditional unsupervised cosegmentation that does not use any segmentation groundtruth, semi-supervised cosegmentation exploits the similarity from both the very limited training image foregrounds, as well as the common object shared between the large number of unsegmented images. This would be a much practical way to effectively cosegment a large number of related images simultaneously, where previous unsupervised cosegmentation work poorly due to the large variances in appearance between different images and the lack ofsegmentation groundtruthfor guidance in cosegmentation. For semi-supervised cosegmentation in large scale, we propose an effective method by minimizing an energy function, which consists of the inter-image distance, the intraimage distance and the balance term. We also propose an iterative updating algorithm to efficiently solve this energy function, which decomposes the original energy minimization problem into sub-problems, and updates each image alternatively to reduce the number of variables in each subproblem for computation efficiency. Experiment results on iCoseg and Pascal VOC datasets show that the proposed cosegmentation method can effectively cosegment hundreds of images in less than one minute. And our semi-supervised cosegmentation is able to outperform both unsupervised cosegmentation as well asfully supervised single image segmentation, especially when the training data is limited.</p><p>Reference: <a title="iccv-2013-383-reference" href="../iccv2013_reference/iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Different from traditional unsupervised cosegmentation that does not use any segmentation groundtruth, semi-supervised cosegmentation exploits the similarity from both the very limited training image foregrounds, as well as the common object shared between the large number of unsegmented images. [sent-5, score-1.959]
</p><p>2 For semi-supervised cosegmentation in large scale, we propose an effective method by minimizing an energy function, which consists of the inter-image distance, the intraimage distance and the balance term. [sent-7, score-0.832]
</p><p>3 We also propose an iterative updating algorithm to efficiently solve this energy function, which decomposes the original energy minimization problem into sub-problems, and updates each image  alternatively to reduce the number of variables in each subproblem for computation efficiency. [sent-8, score-0.497]
</p><p>4 Experiment results on iCoseg and Pascal VOC datasets show that the proposed cosegmentation method can effectively cosegment hundreds of images in less than one minute. [sent-9, score-1.034]
</p><p>5 And our semi-supervised cosegmentation is able to outperform both unsupervised cosegmentation as well asfully supervised single image segmentation, especially when the training data is limited. [sent-10, score-1.446]
</p><p>6 Introduction The problem of image cosegmentation is actively studied in recent computer vision community. [sent-12, score-0.683]
</p><p>7 Given a set of related images with the prior knowledge that they all contain a common object, the goal of cosegmentation is to automatically find this common object in each image and segment it as foreground. [sent-13, score-0.769]
</p><p>8 The original cosegmentation studies [24, 19, 20, 11, 26] could only handle just a pair of images. [sent-15, score-0.69]
</p><p>9 Recent studies [13, 5, 27, 21, 14, 25, 22] extend this limitation and can cosegment multiple images. [sent-16, score-0.328]
</p><p>10 [17, 15] have tried to cosegment hundreds of or even thousands of images, but they use clustering strategy that divides the large image set into multiple subsets, and then cosegment each subset separately. [sent-20, score-0.613]
</p><p>11 This may not be an optimal solution as it avoids to directly cosegment the whole image set, and the similarity information (about the common object) between images in different subsets is lost. [sent-21, score-0.408]
</p><p>12 In this paper, we try to cosegment a large number of images simultaneously, which is a much challenging task due to the large variance between different images. [sent-22, score-0.338]
</p><p>13 If some training image foregrounds are provided, it is possible to guide the cosegmentation task towards a correct direction. [sent-23, score-0.865]
</p><p>14 It 393 uses training image foregrounds for guidance in cosegmentation, and exploits the similarity from both the training images and unsegmented images. [sent-28, score-0.72]
</p><p>15 The intra-image distance considers spatial continuity within each unsegmented image. [sent-29, score-0.399]
</p><p>16 With these three terms, the resulting energy minimization problem can be formulated as a binary quadratic programming (QP) problem, which is able to effectively segment the foreground of each unsegmented image. [sent-31, score-0.749]
</p><p>17 To increase efficiency, we propose an iterative updating algorithm using the trust region idea to solve the energy function. [sent-33, score-0.342]
</p><p>18 That is, we update every image one by one alternatively in each iteration, by keeping the foregrounds of other images fixed and updating the foreground of each image as a sub-problem. [sent-34, score-0.479]
</p><p>19 Compared to updating all images simultaneously using only one iteration, this iterative updating algorithm can significantly reduce the number of variables in each sub-problem and therefore speed up the whole procedure. [sent-36, score-0.444]
</p><p>20 For cosegmenting hundreds of images, only less than one minute is required by using  this iterative updating algorithm. [sent-38, score-0.517]
</p><p>21 After solving the above mentioned accuracy and efficiency issues in cosegmenting a large number of images, our proposed semi-supervised method is more practical for real world applications than previous cosegmentation works. [sent-39, score-0.869]
</p><p>22 We propose an effective method for semi-supervised cosegmentation by minimizing an energy fiu-snucptieornv itsheadt consists of the inter-image distance, the intra-image distance and the balance term. [sent-41, score-0.832]
</p><p>23 We propose an efficient algorithm to solve the energy fWunec ptiroonp by i atnera eftifvieci updating, hwmhi ctho siso lavbele t htoe cosegment hundreds of image in less than one minute. [sent-42, score-0.436]
</p><p>24 We describe our energy function and the iterative updating algorithm in Section 3, and evaluate its performance in Section 4. [sent-45, score-0.305]
</p><p>25 Related Work Image Cosegmentation: The problem of cosegmentation is firstly proposed by Rother et al. [sent-48, score-0.684]
</p><p>26 [24], in which a common object shared by two images is segmented by measuring the similarity between their foreground histograms with L1-norm. [sent-49, score-0.265]
</p><p>27 The resulting foregrounds by cosegmentation would be helpful in many other applications. [sent-50, score-0.82]
</p><p>28 [24] show that the distance between an image pair measured by the cosegmented foregrounds can help improve image retrieval. [sent-52, score-0.251]
</p><p>29 Due to its usefulness in other computer vision applications, cosegmentation has been actively studied in recent years. [sent-55, score-0.683]
</p><p>30 Recent works [13, 5, 27, 21, 25] extend previous limitation of cosegmenting only two images, and can cosegment multiple images. [sent-57, score-0.507]
</p><p>31 The work in [17, 14, 22] also extend the foreground-background binary segmentation to multiple regions, which is able to cosegment multiple images with multiple objects. [sent-58, score-0.413]
</p><p>32 Another recent work in [16] tries to cosegment with multiple foregrounds, which would be a more challenging problem. [sent-59, score-0.294]
</p><p>33 All these works are unsupervised and limited to cosegment at most dozens of images simultaneously. [sent-60, score-0.422]
</p><p>34 For segmenting large scale dataset, [17, 15] use clustering strategy to divide the large image set into multiple subsets, and cosegment each subset separately. [sent-61, score-0.297]
</p><p>35 [18] trans-  fers segmentations from segmented images in the current source set to unsegmented images in the next target set by segmentation propagation, and finally segment the whole ImageNet dataset [8]. [sent-62, score-0.583]
</p><p>36 However, these works do not cosegment all images simultaneously, and may lose the similarity information between images in different subsets. [sent-63, score-0.382]
</p><p>37 These methods are unable to effectively and efficiently cosegment a large number of images, which would be benefit by semi-supervised learning. [sent-71, score-0.275]
</p><p>38 For this task, superpixels are firstly extracted from each image in pre-processing, so that the foreground/background label can be defined on each superpixel rather than on each pixel for computation efficiency. [sent-75, score-0.398]
</p><p>39 For each training image, the label of each superpixel can be easily determined by comparing the areas covered by foreground and background. [sent-76, score-0.393]
</p><p>40 For each unsegmented image, this task is formulated as predicting the label for each superpixel, then the final foreground can be found by selecting superpixels with foreground labels. [sent-77, score-0.734]
</p><p>41 A vector 푦푖 is used to represent the superpixel labels for an image 푋푖, with the dimension 푠푖 equal to the number of superpixels in this image. [sent-78, score-0.394]
</p><p>42 Each component 푦푖 (푗) in vector 푦푖 is a binary variable, with 1indicating the corresponding superpixel belongs to foreground and 0 for background. [sent-79, score-0.367]
</p><p>43 The determination of 푦푖 for each unsegmented image is formulated as an energy function minimization problem, which is then solved by an iterative updating algorithm. [sent-80, score-0.771]
</p><p>44 Like many previous works [24, 20, 11, 26, 21, 5, 15] , histogram descriptors are used to  ×  represent superpixels and the foregrounds of images, which can be either bag-of-words histogram with some local features, or color histogram based on pixel intensities. [sent-84, score-0.365]
</p><p>45 The superpixel histogram is represented by ℎ푖 (푗) ∈ 푅푑 for each superpixel 푗 in image 푋푖, and the foreground histogram of image 푋푖 can be calculated as ∑푗 푦푖 (푗) ⋅ ℎ푖 (푗), which can also be formulated as 퐻푖 ⋅ 푦푖, w∑here 퐻(푗푖 )is ⋅ a (푑 푠푖) matrix with each column correspondin∑g to ℎ푖 (푗i)s. [sent-85, score-0.694]
</p><p>46 1  Energy function definition  The proposed energy function is composed of three terms: the inter-image distance, the intra-image distance and the balance term, in which all unsegmented images are included. [sent-88, score-0.604]
</p><p>47 Therefore by solving the minimizing problem with this energy function, the superpixel labels of all unsegmented images can be calculated simultaneously. [sent-89, score-0.825]
</p><p>48 The inter-image distance measures the similarity of foregrounds between different images, including the similarity between unsegmented images and training images as well as that between pair-wise unsegmented images. [sent-90, score-1.112]
</p><p>49 Therefore both the training segmentation groundtruth and the similarity information shared between unsegmented images are explored in the inter-image distance. [sent-91, score-0.655]
</p><p>50 The intra-image distance considers the spatial consistency between two adjacent superpixels inside an unsegmented image. [sent-93, score-0.551]
</p><p>51 , foreground or background, by adding a penalty to the energy function in case that two adjacent superpixels are given different labels. [sent-96, score-0.345]
</p><p>52 Here 훼(푗, 푘) is the shared edge length between two superpixels 푗 and 푘, 푁(푗) is the set of adjacent superpixels of 푗, and 휃 is a constant value, which is set as the variance of the distance values between all superpixel histograms. [sent-99, score-0.568]
</p><p>53 The balance term prevents all superpixels belonging to the same label during the energy minimization procedure. [sent-100, score-0.366]
</p><p>54 By summing nth beese ca lthcurelea terms, t1he − w 푃hole energy function can be formulated as:  퐸  ×  = 퐸푖푛푡푒푟 + 휆1  ⋅  퐸푖푛푡푟푎  + 휆2  ⋅  퐸푏푎푙  (7)  where 휆1 and 휆2 are two trade-off parameters to control the proportion of each term in the energy function. [sent-103, score-0.304]
</p><p>55 2  Binary quadratic programming problem  Given the definition of the energy function, the minimization can be converted to a binary QP problem, by reformulating each of the three terms into suitable form. [sent-106, score-0.274]
</p><p>56 Its diagonal component 푀푖푖푛푡푟푎 (푗, 푗×) i푠s calculated as: 푀푖푖푛푡푟푎(푗,푗)  =  ∑ 푘∈∑푁(푗)  (푊푖(푗, 푘) + 푊푖(푘,푗))  (14)  and the off-diagonal component 푀푖푖푛푡푟푎 (푗, 푘) is calculated as follows if superpixel 푗 and 푘 are adjacent, or 0 otherwise. [sent-111, score-0.313]
</p><p>57 Iterative updating algorithm The binary QP problem has been studied extensively in the optimization literature [2, 23, 20], and Equation 18 can be easily solved using these methods when cosegmenting a small number of images. [sent-116, score-0.428]
</p><p>58 To increase efficiency, we propose an iterative updating algorithm using the trust region idea to solve  푌  396  this problem. [sent-118, score-0.244]
</p><p>59 The basic idea of this algorithm is to update every unsegmented image one by one alternatively in each iteration, by keeping the superpixel labels of other images fixed in updating the current image, and repeat this iteration until convergence. [sent-119, score-0.884]
</p><p>60 In this way, updating the superpixel labels of each image is decomposed as a sub-problem, where  the number of variables (superpixel labels) is significantly reduced and the optimization procedure can be accelerated. [sent-120, score-0.436]
</p><p>61 In the experiment of this paper, we simply relax the binary variable of each superpixel label 푦푖 (푗) from {0, 1} to [0, 1] . [sent-123, score-0.292]
</p><p>62 The resulting value is then rounded to binary value for superpixel labels. [sent-125, score-0.301]
</p><p>63 In the iterative updating algorithm, all sub-problems are solved individually to update the superpixel labels of the corresponding images. [sent-126, score-0.501]
</p><p>64 In two successive iterations, the only difference in updating each image 푋푖 of subproblem 퐸푖 is that the labels of other unsegmented ima∑ges 푦푗 would be changed, therefore only the first term 푀푖푖푗푛푡푒푟 ⋅ 푦푗) in vector 푉푖′ (Equation 22) of each s∑ub-problem is required to be re-calculated. [sent-127, score-0.626]
</p><p>65 As this term n∑eeds to sum over all other images, the complexity of updating all images grows quadratically with the number of images, which seems inefficient for large scale cosegmentation. [sent-128, score-0.259]
</p><p>66 Another advantage of the iterative updating algorithm is that it can also reduce the rounding error compared to directly solving energy function of Equation 18 (where the superpixel labels of all images need to be rounded simultaneously). [sent-132, score-0.673]
</p><p>67 This is because the rounding error of superpixel labels only occurs in the corresponding sub-problem and will be fixed in other sub-problems. [sent-133, score-0.299]
</p><p>68 The proposed iterative updating algorithm is similar to trust region graph cut in [24]. [sent-135, score-0.244]
</p><p>69 However, in our semi-supervised cosegmentation, the limited training images provide a good initialization and can guide the cosegmentation towards a correct direction for unsegmented images. [sent-138, score-1.135]
</p><p>70 Moreover, each sub-problem is approximated as a convex QP problem,  which makes the initialization for unsegmented images not important anymore. [sent-139, score-0.41]
</p><p>71 We simply set all initial superpixel labels as 1. [sent-140, score-0.277]
</p><p>72 iCoseg dataset is popularly used in previous cosegmentation works [1, 27, 25, 14], 397  Table 1. [sent-146, score-0.656]
</p><p>73 Cosegmentation accuracy comparison in iCoseg dataset  which contains 38 classes, each for an independent cosegmentation task. [sent-147, score-0.656]
</p><p>74 However, most classes contain only a few images, therefore we select 10 representative classes containing more images for our cosegmentation experiment, in which the number of images ranges from 18 to 40. [sent-148, score-0.77]
</p><p>75 For the representation of each superpixel and foreground, we use color histogram with RGB and Lab color channels. [sent-152, score-0.261]
</p><p>76 The intersection-over-union score is used to measure the cosegmentation accuracy, which is a standard evaluation metric in Pascal Challenges [9]. [sent-153, score-0.656]
</p><p>77 Three recent cosegmentation works [13, 17, 14] are compared in iCoseg dataset, which are implemented by their publicly available code with the default parameter  iterative updating algorithm. [sent-157, score-0.863]
</p><p>78 Note that although only 4 images are shown here as example, this is the intermediate result of cosegmenting all the 25 images in “Baseball” class in iCoseg dataset. [sent-158, score-0.293]
</p><p>79 In [17] and [14], images can be cosegmented into multiple regions, therefore we adjust the number of regions K from 2 to 9 and report the best one, for the foregroundbackground binary cosegmentation in this experiment. [sent-160, score-0.793]
</p><p>80 Table 1 shows the cosegmentation accuracy of each class and the average results. [sent-162, score-0.656]
</p><p>81 Cosegmentation dataset  accuracy  comparison  in VOC2012  ficult to determine the best K beforehand in unsupervised cosegmentation tasks. [sent-166, score-0.713]
</p><p>82 An example of the intermediate result during our iterative updating algorithm is shown in Figure1, and an analysis of the cosegmentation accuracy affected by the choice of parameters (휆1 and 휆2) can be found in the supplementary material. [sent-169, score-0.863]
</p><p>83 For [17] and [14], only the running time for their binary version is reported since more time is required for multiple regions cosegmentation (퐾 > 2). [sent-171, score-0.715]
</p><p>84 It should be noted that the running time shown in this table does not include superpixel extraction and histogram generation steps for all methods. [sent-175, score-0.281]
</p><p>85 In VOC2012 dataset, only [17] is compared since it can also cosegment images in large scale. [sent-176, score-0.315]
</p><p>86 For [13] and [14], the requirement on large memory and computation cost for cosegmenting hundreds of images is beyond our computation capability. [sent-177, score-0.316]
</p><p>87 The cosegmentation accuracy and running time are presented in Table 3 and 4 respectively. [sent-178, score-0.676]
</p><p>88 For cosegmenting hundreds of images, our method only requires less than one minute, which is much iCoseg  VOC201 2  cyaruAc0 0. [sent-180, score-0.276]
</p><p>89 We also try the cosegmentation experiments at the level of 1000 images. [sent-187, score-0.679]
</p><p>90 Due to the lack of enough images with groundtruth segmentation in VOC2012 dataset for the accuracy evaluation, we randomly select 1000 related images from its classification challenge set and only test the running time. [sent-188, score-0.236]
</p><p>91 Our method requires about 5 minutes for cosegmenting 1000 images, while [17] needs 60 − 70 minutes as reported 1in0 0th0e imir paper. [sent-189, score-0.259]
</p><p>92 Semi-supervised cosegmentation results Next, the cosegmentation experiment is performed in semi-supervised manner (denoted as “SemiSV”) and the result is compared with unsupervised learning (denoted as “UnSV”) as well as supervised learning (denoted as  “FullSV”). [sent-193, score-1.435]
</p><p>93 For supervised learning, each image is segmented individually with training images only, without considering the similarity of the common object shared between unsegmented images. [sent-194, score-0.617]
</p><p>94 For training image selection in this dataset, we notice that some images have large errors in the superpixel labels, which are determined according to the overlap with the foreground and background pixel labels. [sent-202, score-0.413]
</p><p>95 That is, the resulting foreground from the converted superpixel labels is significantly different from the original foreground, probably due to bad superpixel extraction. [sent-203, score-0.631]
</p><p>96 This result shows that semi-supervised learning will be most competent when the number of unsegmented images is far more than that of segmented ones, as concluded in [6]. [sent-207, score-0.482]
</p><p>97 With the fewest training images in VOC2012 dataset, the accuracy of “FullSV” is close to “UnSV”, which indicates that the similarity information from the common object between test images is competitive to that provided by the segmentation groundtruth of the 4 training images in this dataset. [sent-208, score-0.401]
</p><p>98 What’s more, the concrete information from the training images may be stained by the uncertainty of the unsegmented images, which worsens the final cosegmentation accuracy. [sent-212, score-1.128]
</p><p>99 Conclusion In this paper, we proposed a semi-supervised learning method for large scale images cosegmentation, where hundreds of images can be processed in less than one minute with competitive cosegmentation accuracy. [sent-214, score-0.872]
</p><p>100 Tricos: A tri-level class-discriminative cosegmentation method for image classification. [sent-241, score-0.656]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cosegmentation', 0.656), ('unsegmented', 0.37), ('cosegment', 0.275), ('superpixel', 0.233), ('cosegmenting', 0.213), ('foregrounds', 0.164), ('updating', 0.159), ('icoseg', 0.145), ('superpixels', 0.117), ('qp', 0.11), ('energy', 0.098), ('fullsv', 0.097), ('foreground', 0.095), ('semisv', 0.077), ('groundtruth', 0.077), ('hundreds', 0.063), ('segmentation', 0.059), ('cosegmented', 0.058), ('unsv', 0.058), ('unsupervised', 0.057), ('equation', 0.052), ('balance', 0.049), ('iterative', 0.048), ('training', 0.045), ('labels', 0.044), ('reformulated', 0.043), ('mukherjee', 0.043), ('minimization', 0.042), ('images', 0.04), ('calculated', 0.04), ('binary', 0.039), ('traninig', 0.039), ('segmented', 0.038), ('shared', 0.037), ('trust', 0.037), ('scribble', 0.037), ('formulated', 0.037), ('increased', 0.036), ('adjacent', 0.035), ('minute', 0.034), ('studies', 0.034), ('chai', 0.033), ('proportion', 0.032), ('supervised', 0.032), ('subproblem', 0.031), ('szie', 0.03), ('distance', 0.029), ('rother', 0.029), ('guidance', 0.029), ('quadratic', 0.029), ('rounded', 0.029), ('histogram', 0.028), ('firstly', 0.028), ('common', 0.028), ('actively', 0.027), ('similarity', 0.027), ('converted', 0.026), ('omitted', 0.026), ('dozens', 0.026), ('singh', 0.025), ('limited', 0.024), ('batra', 0.024), ('vicente', 0.024), ('pascal', 0.024), ('joulin', 0.024), ('try', 0.023), ('minutes', 0.023), ('rounding', 0.022), ('scale', 0.022), ('transductive', 0.022), ('programming', 0.022), ('term', 0.022), ('alternatively', 0.021), ('guillaumin', 0.021), ('label', 0.02), ('running', 0.02), ('whole', 0.019), ('limitation', 0.019), ('subsets', 0.019), ('simultaneously', 0.019), ('tries', 0.019), ('gool', 0.018), ('definition', 0.018), ('prevents', 0.018), ('classes', 0.017), ('lempitsky', 0.017), ('itera', 0.017), ('matchingincorporating', 0.017), ('competent', 0.017), ('zien', 0.017), ('hochbaum', 0.017), ('worsens', 0.017), ('beese', 0.017), ('iteration', 0.017), ('solved', 0.017), ('kim', 0.017), ('learning', 0.017), ('segment', 0.017), ('imagenet', 0.016), ('grows', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="383-tfidf-1" href="./iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation.html">383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</a></p>
<p>Author: Zhengxiang Wang, Rujie Liu</p><p>Abstract: This paper introduces to use semi-supervised learning for large scale image cosegmentation. Different from traditional unsupervised cosegmentation that does not use any segmentation groundtruth, semi-supervised cosegmentation exploits the similarity from both the very limited training image foregrounds, as well as the common object shared between the large number of unsegmented images. This would be a much practical way to effectively cosegment a large number of related images simultaneously, where previous unsupervised cosegmentation work poorly due to the large variances in appearance between different images and the lack ofsegmentation groundtruthfor guidance in cosegmentation. For semi-supervised cosegmentation in large scale, we propose an effective method by minimizing an energy function, which consists of the inter-image distance, the intraimage distance and the balance term. We also propose an iterative updating algorithm to efficiently solve this energy function, which decomposes the original energy minimization problem into sub-problems, and updates each image alternatively to reduce the number of variables in each subproblem for computation efficiency. Experiment results on iCoseg and Pascal VOC datasets show that the proposed cosegmentation method can effectively cosegment hundreds of images in less than one minute. And our semi-supervised cosegmentation is able to outperform both unsupervised cosegmentation as well asfully supervised single image segmentation, especially when the training data is limited.</p><p>2 0.28261772 <a title="383-tfidf-2" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>Author: Abdelaziz Djelouah, Jean-Sébastien Franco, Edmond Boyer, François Le_Clerc, Patrick Pérez</p><p>Abstract: In this paper, we address the problem of object segmentation in multiple views or videos when two or more viewpoints of the same scene are available. We propose a new approach that propagates segmentation coherence information in both space and time, hence allowing evidences in one image to be shared over the complete set. To this aim the segmentation is cast as a single efficient labeling problem over space and time with graph cuts. In contrast to most existing multi-view segmentation methods that rely on some form of dense reconstruction, ours only requires a sparse 3D sampling to propagate information between viewpoints. The approach is thoroughly evaluated on standard multiview datasets, as well as on videos. With static views, results compete with state of the art methods but they are achieved with significantly fewer viewpoints. With multiple videos, we report results that demonstrate the benefit of segmentation propagation through temporal cues.</p><p>3 0.27517629 <a title="383-tfidf-3" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>Author: Jian Sun, Jean Ponce</p><p>Abstract: In this paper, we address the problem of learning discriminative part detectors from image sets with category labels. We propose a novel latent SVM model regularized by group sparsity to learn these part detectors. Starting from a large set of initial parts, the group sparsity regularizer forces the model to jointly select and optimize a set of discriminative part detectors in a max-margin framework. We propose a stochastic version of a proximal algorithm to solve the corresponding optimization problem. We apply the proposed method to image classification and cosegmentation, and quantitative experiments with standard benchmarks show that it matches or improves upon the state of the art.</p><p>4 0.24651329 <a title="383-tfidf-4" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>Author: Jifeng Dai, Ying Nian Wu, Jie Zhou, Song-Chun Zhu</p><p>Abstract: Cosegmentation refers to theproblem ofsegmenting multiple images simultaneously by exploiting the similarities between the foreground and background regions in these images. The key issue in cosegmentation is to align common objects between these images. To address this issue, we propose an unsupervised learning framework for cosegmentation, by coupling cosegmentation with what we call “cosketch ”. The goal of cosketch is to automatically discover a codebook of deformable shape templates shared by the input images. These shape templates capture distinct image patterns and each template is matched to similar image patches in different images. Thus the cosketch of the images helps to align foreground objects, thereby providing crucial information for cosegmentation. We present a statistical model whose energy function couples cosketch and cosegmentation. We then present an unsupervised learning algorithm that performs cosketch and cosegmentation by energy minimization. Experiments show that our method outperforms state of the art methods for cosegmentation on the challenging MSRC and iCoseg datasets. We also illustrate our method on a new dataset called Coseg-Rep where cosegmentation can be performed within a single image with repetitive patterns.</p><p>5 0.23034121 <a title="383-tfidf-5" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>Author: Alon Faktor, Michal Irani</p><p>Abstract: Given a set of images which share an object from the same semantic category, we would like to co-segment the shared object. We define ‘good’ co-segments to be ones which can be easily composed (like a puzzle) from large pieces of other co-segments, yet are difficult to compose from remaining image parts. These pieces must not only match well but also be statistically significant (hard to compose at random). This gives rise to co-segmentation of objects in very challenging scenarios with large variations in appearance, shape and large amounts of clutter. We further show how multiple images can collaborate and “score each others ’ co-segments to improve the overall fidelity and accuracy of the co-segmentation. Our co-segmentation can be applied both to large image collections, as well as to very few images (where there is too little data for unsupervised learning). At the extreme, it can be applied even to a single image, to extract its co-occurring objects. Our approach obtains state-of-the-art results on benchmark datasets. We further show very encouraging co-segmentation results on the challenging PASCAL-VOC dataset. ”</p><p>6 0.14951317 <a title="383-tfidf-6" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>7 0.13779767 <a title="383-tfidf-7" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<p>8 0.13443638 <a title="383-tfidf-8" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>9 0.12611505 <a title="383-tfidf-9" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>10 0.091759533 <a title="383-tfidf-10" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>11 0.090638772 <a title="383-tfidf-11" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>12 0.078151241 <a title="383-tfidf-12" href="./iccv-2013-Estimating_the_3D_Layout_of_Indoor_Scenes_and_Its_Clutter_from_Depth_Sensors.html">144 iccv-2013-Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors</a></p>
<p>13 0.07335452 <a title="383-tfidf-13" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>14 0.063245118 <a title="383-tfidf-14" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>15 0.062940486 <a title="383-tfidf-15" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>16 0.061118398 <a title="383-tfidf-16" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>17 0.061008781 <a title="383-tfidf-17" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>18 0.05902794 <a title="383-tfidf-18" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>19 0.05750668 <a title="383-tfidf-19" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<p>20 0.053414289 <a title="383-tfidf-20" href="./iccv-2013-Decomposing_Bag_of_Words_Histograms.html">104 iccv-2013-Decomposing Bag of Words Histograms</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.13), (1, -0.0), (2, 0.048), (3, -0.006), (4, 0.026), (5, 0.032), (6, -0.1), (7, 0.079), (8, 0.003), (9, -0.104), (10, 0.024), (11, 0.134), (12, -0.007), (13, -0.06), (14, -0.058), (15, -0.019), (16, -0.027), (17, -0.054), (18, -0.098), (19, -0.114), (20, 0.146), (21, -0.093), (22, -0.133), (23, -0.024), (24, -0.036), (25, 0.05), (26, -0.044), (27, 0.083), (28, -0.049), (29, 0.112), (30, 0.06), (31, 0.146), (32, 0.106), (33, 0.114), (34, 0.042), (35, -0.097), (36, 0.033), (37, 0.05), (38, -0.082), (39, -0.143), (40, 0.014), (41, -0.075), (42, -0.013), (43, 0.049), (44, -0.208), (45, -0.108), (46, -0.01), (47, -0.037), (48, 0.093), (49, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91990006 <a title="383-lsi-1" href="./iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation.html">383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</a></p>
<p>Author: Zhengxiang Wang, Rujie Liu</p><p>Abstract: This paper introduces to use semi-supervised learning for large scale image cosegmentation. Different from traditional unsupervised cosegmentation that does not use any segmentation groundtruth, semi-supervised cosegmentation exploits the similarity from both the very limited training image foregrounds, as well as the common object shared between the large number of unsegmented images. This would be a much practical way to effectively cosegment a large number of related images simultaneously, where previous unsupervised cosegmentation work poorly due to the large variances in appearance between different images and the lack ofsegmentation groundtruthfor guidance in cosegmentation. For semi-supervised cosegmentation in large scale, we propose an effective method by minimizing an energy function, which consists of the inter-image distance, the intraimage distance and the balance term. We also propose an iterative updating algorithm to efficiently solve this energy function, which decomposes the original energy minimization problem into sub-problems, and updates each image alternatively to reduce the number of variables in each subproblem for computation efficiency. Experiment results on iCoseg and Pascal VOC datasets show that the proposed cosegmentation method can effectively cosegment hundreds of images in less than one minute. And our semi-supervised cosegmentation is able to outperform both unsupervised cosegmentation as well asfully supervised single image segmentation, especially when the training data is limited.</p><p>2 0.72118205 <a title="383-lsi-2" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>Author: Jifeng Dai, Ying Nian Wu, Jie Zhou, Song-Chun Zhu</p><p>Abstract: Cosegmentation refers to theproblem ofsegmenting multiple images simultaneously by exploiting the similarities between the foreground and background regions in these images. The key issue in cosegmentation is to align common objects between these images. To address this issue, we propose an unsupervised learning framework for cosegmentation, by coupling cosegmentation with what we call “cosketch ”. The goal of cosketch is to automatically discover a codebook of deformable shape templates shared by the input images. These shape templates capture distinct image patterns and each template is matched to similar image patches in different images. Thus the cosketch of the images helps to align foreground objects, thereby providing crucial information for cosegmentation. We present a statistical model whose energy function couples cosketch and cosegmentation. We then present an unsupervised learning algorithm that performs cosketch and cosegmentation by energy minimization. Experiments show that our method outperforms state of the art methods for cosegmentation on the challenging MSRC and iCoseg datasets. We also illustrate our method on a new dataset called Coseg-Rep where cosegmentation can be performed within a single image with repetitive patterns.</p><p>3 0.70354658 <a title="383-lsi-3" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>Author: Abdelaziz Djelouah, Jean-Sébastien Franco, Edmond Boyer, François Le_Clerc, Patrick Pérez</p><p>Abstract: In this paper, we address the problem of object segmentation in multiple views or videos when two or more viewpoints of the same scene are available. We propose a new approach that propagates segmentation coherence information in both space and time, hence allowing evidences in one image to be shared over the complete set. To this aim the segmentation is cast as a single efficient labeling problem over space and time with graph cuts. In contrast to most existing multi-view segmentation methods that rely on some form of dense reconstruction, ours only requires a sparse 3D sampling to propagate information between viewpoints. The approach is thoroughly evaluated on standard multiview datasets, as well as on videos. With static views, results compete with state of the art methods but they are achieved with significantly fewer viewpoints. With multiple videos, we report results that demonstrate the benefit of segmentation propagation through temporal cues.</p><p>4 0.57689583 <a title="383-lsi-4" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>Author: Michael Van_Den_Bergh, Gemma Roig, Xavier Boix, Santiago Manen, Luc Van_Gool</p><p>Abstract: Superpixel and objectness algorithms are broadly used as a pre-processing step to generate support regions and to speed-up further computations. Recently, many algorithms have been extended to video in order to exploit the temporal consistency between frames. However, most methods are computationally too expensive for real-time applications. We introduce an online, real-time video superpixel algorithm based on the recently proposed SEEDS superpixels. A new capability is incorporated which delivers multiple diverse samples (hypotheses) of superpixels in the same image or video sequence. The multiple samples are shown to provide a strong cue to efficiently measure the objectness of image windows, and we introduce the novel concept of objectness in temporal windows. Experiments show that the video superpixels achieve comparable performance to state-of-the-art offline methods while running at 30 fps on a single 2.8 GHz i7 CPU. State-of-the-art performance on objectness is also demonstrated, yet orders of magnitude faster and extended to temporal windows in video.</p><p>5 0.56345206 <a title="383-lsi-5" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>Author: Matthias Reso, Jörn Jachalsky, Bodo Rosenhahn, Jörn Ostermann</p><p>Abstract: Superpixel algorithms represent a very useful and increasingly popular preprocessing step for a wide range of computer vision applications, as they offer the potential to boost efficiency and effectiveness. In this regards, this paper presents a highly competitive approach for temporally consistent superpixelsfor video content. The approach is based on energy-minimizing clustering utilizing a novel hybrid clustering strategy for a multi-dimensional feature space working in a global color subspace and local spatial subspaces. Moreover, a new contour evolution based strategy is introduced to ensure spatial coherency of the generated superpixels. For a thorough evaluation the proposed approach is compared to state of the art supervoxel algorithms using established benchmarks and shows a superior performance.</p><p>6 0.5617916 <a title="383-lsi-6" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<p>7 0.54675919 <a title="383-lsi-7" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>8 0.48093122 <a title="383-lsi-8" href="./iccv-2013-Detecting_Curved_Symmetric_Parts_Using_a_Deformable_Disc_Model.html">110 iccv-2013-Detecting Curved Symmetric Parts Using a Deformable Disc Model</a></p>
<p>9 0.47330937 <a title="383-lsi-9" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>10 0.41323245 <a title="383-lsi-10" href="./iccv-2013-Learning_a_Dictionary_of_Shape_Epitomes_with_Applications_to_Image_Labeling.html">245 iccv-2013-Learning a Dictionary of Shape Epitomes with Applications to Image Labeling</a></p>
<p>11 0.40557131 <a title="383-lsi-11" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<p>12 0.39915431 <a title="383-lsi-12" href="./iccv-2013-Bounded_Labeling_Function_for_Global_Segmentation_of_Multi-part_Objects_with_Geometric_Constraints.html">63 iccv-2013-Bounded Labeling Function for Global Segmentation of Multi-part Objects with Geometric Constraints</a></p>
<p>13 0.38820273 <a title="383-lsi-13" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>14 0.37554565 <a title="383-lsi-14" href="./iccv-2013-Discriminatively_Trained_Templates_for_3D_Object_Detection%3A_A_Real_Time_Scalable_Approach.html">121 iccv-2013-Discriminatively Trained Templates for 3D Object Detection: A Real Time Scalable Approach</a></p>
<p>15 0.37162933 <a title="383-lsi-15" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>16 0.36870381 <a title="383-lsi-16" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>17 0.34850049 <a title="383-lsi-17" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>18 0.33650148 <a title="383-lsi-18" href="./iccv-2013-Bayesian_Joint_Topic_Modelling_for_Weakly_Supervised_Object_Localisation.html">59 iccv-2013-Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation</a></p>
<p>19 0.31646827 <a title="383-lsi-19" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>20 0.31592029 <a title="383-lsi-20" href="./iccv-2013-Progressive_Multigrid_Eigensolvers_for_Multiscale_Spectral_Segmentation.html">329 iccv-2013-Progressive Multigrid Eigensolvers for Multiscale Spectral Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.094), (4, 0.015), (7, 0.022), (19, 0.255), (26, 0.145), (31, 0.03), (35, 0.021), (42, 0.113), (48, 0.012), (64, 0.027), (73, 0.032), (89, 0.123)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75321263 <a title="383-lda-1" href="./iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation.html">383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</a></p>
<p>Author: Zhengxiang Wang, Rujie Liu</p><p>Abstract: This paper introduces to use semi-supervised learning for large scale image cosegmentation. Different from traditional unsupervised cosegmentation that does not use any segmentation groundtruth, semi-supervised cosegmentation exploits the similarity from both the very limited training image foregrounds, as well as the common object shared between the large number of unsegmented images. This would be a much practical way to effectively cosegment a large number of related images simultaneously, where previous unsupervised cosegmentation work poorly due to the large variances in appearance between different images and the lack ofsegmentation groundtruthfor guidance in cosegmentation. For semi-supervised cosegmentation in large scale, we propose an effective method by minimizing an energy function, which consists of the inter-image distance, the intraimage distance and the balance term. We also propose an iterative updating algorithm to efficiently solve this energy function, which decomposes the original energy minimization problem into sub-problems, and updates each image alternatively to reduce the number of variables in each subproblem for computation efficiency. Experiment results on iCoseg and Pascal VOC datasets show that the proposed cosegmentation method can effectively cosegment hundreds of images in less than one minute. And our semi-supervised cosegmentation is able to outperform both unsupervised cosegmentation as well asfully supervised single image segmentation, especially when the training data is limited.</p><p>2 0.75018787 <a title="383-lda-2" href="./iccv-2013-Learning_Slow_Features_for_Behaviour_Analysis.html">243 iccv-2013-Learning Slow Features for Behaviour Analysis</a></p>
<p>Author: Lazaros Zafeiriou, Mihalis A. Nicolaou, Stefanos Zafeiriou, Symeon Nikitidis, Maja Pantic</p><p>Abstract: A recently introduced latent feature learning technique for time varying dynamic phenomena analysis is the socalled Slow Feature Analysis (SFA). SFA is a deterministic component analysis technique for multi-dimensional sequences that by minimizing the variance of the first order time derivative approximation of the input signal finds uncorrelated projections that extract slowly-varying features ordered by their temporal consistency and constancy. In this paper, we propose a number of extensions in both the deterministic and the probabilistic SFA optimization frameworks. In particular, we derive a novel deterministic SFA algorithm that is able to identify linear projections that extract the common slowest varying features of two or more sequences. In addition, we propose an Expectation Maximization (EM) algorithm to perform inference in a probabilistic formulation of SFA and similarly extend it in order to handle two and more time varying data sequences. Moreover, we demonstrate that the probabilistic SFA (EMSFA) algorithm that discovers the common slowest varying latent space of multiple sequences can be combined with dynamic time warping techniques for robust sequence timealignment. The proposed SFA algorithms were applied for facial behavior analysis demonstrating their usefulness and appropriateness for this task.</p><p>3 0.72291368 <a title="383-lda-3" href="./iccv-2013-Saliency_Detection_via_Absorbing_Markov_Chain.html">371 iccv-2013-Saliency Detection via Absorbing Markov Chain</a></p>
<p>Author: Bowen Jiang, Lihe Zhang, Huchuan Lu, Chuan Yang, Ming-Hsuan Yang</p><p>Abstract: In this paper, we formulate saliency detection via absorbing Markov chain on an image graph model. We jointly consider the appearance divergence and spatial distribution of salient objects and the background. The virtual boundary nodes are chosen as the absorbing nodes in a Markov chain and the absorbed time from each transient node to boundary absorbing nodes is computed. The absorbed time of transient node measures its global similarity with all absorbing nodes, and thus salient objects can be consistently separated from the background when the absorbed time is used as a metric. Since the time from transient node to absorbing nodes relies on the weights on the path and their spatial distance, the background region on the center of image may be salient. We further exploit the equilibrium distribution in an ergodic Markov chain to reduce the absorbed time in the long-range smooth background regions. Extensive experiments on four benchmark datasets demonstrate robustness and efficiency of the proposed method against the state-of-the-art methods.</p><p>4 0.70905656 <a title="383-lda-4" href="./iccv-2013-The_Way_They_Move%3A_Tracking_Multiple_Targets_with_Similar_Appearance.html">418 iccv-2013-The Way They Move: Tracking Multiple Targets with Similar Appearance</a></p>
<p>Author: Caglayan Dicle, Octavia I. Camps, Mario Sznaier</p><p>Abstract: We introduce a computationally efficient algorithm for multi-object tracking by detection that addresses four main challenges: appearance similarity among targets, missing data due to targets being out of the field of view or occluded behind other objects, crossing trajectories, and camera motion. The proposed method uses motion dynamics as a cue to distinguish targets with similar appearance, minimize target mis-identification and recover missing data. Computational efficiency is achieved by using a Generalized Linear Assignment (GLA) coupled with efficient procedures to recover missing data and estimate the complexity of the underlying dynamics. The proposed approach works with tracklets of arbitrary length and does not assume a dynamical model a priori, yet it captures the overall motion dynamics of the targets. Experiments using challenging videos show that this framework can handle complex target motions, non-stationary cameras and long occlusions, on scenarios where appearance cues are not available or poor.</p><p>5 0.67166936 <a title="383-lda-5" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>Author: Zhuwen Li, Jiaming Guo, Loong-Fah Cheong, Steven Zhiying Zhou</p><p>Abstract: This paper addresses real-world challenges in the motion segmentation problem, including perspective effects, missing data, and unknown number of motions. It first formulates the 3-D motion segmentation from two perspective views as a subspace clustering problem, utilizing the epipolar constraint of an image pair. It then combines the point correspondence information across multiple image frames via a collaborative clustering step, in which tight integration is achieved via a mixed norm optimization scheme. For model selection, wepropose an over-segment and merge approach, where the merging step is based on the property of the ?1-norm ofthe mutual sparse representation oftwo oversegmented groups. The resulting algorithm can deal with incomplete trajectories and perspective effects substantially better than state-of-the-art two-frame and multi-frame methods. Experiments on a 62-clip dataset show the significant superiority of the proposed idea in both segmentation accuracy and model selection.</p><p>6 0.66399705 <a title="383-lda-6" href="./iccv-2013-Drosophila_Embryo_Stage_Annotation_Using_Label_Propagation.html">125 iccv-2013-Drosophila Embryo Stage Annotation Using Label Propagation</a></p>
<p>7 0.65817398 <a title="383-lda-7" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>8 0.65714407 <a title="383-lda-8" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>9 0.6539495 <a title="383-lda-9" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>10 0.6526404 <a title="383-lda-10" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>11 0.65105277 <a title="383-lda-11" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>12 0.64801699 <a title="383-lda-12" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>13 0.64691573 <a title="383-lda-13" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>14 0.64681065 <a title="383-lda-14" href="./iccv-2013-Slice_Sampling_Particle_Belief_Propagation.html">395 iccv-2013-Slice Sampling Particle Belief Propagation</a></p>
<p>15 0.64650363 <a title="383-lda-15" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>16 0.64566779 <a title="383-lda-16" href="./iccv-2013-A_Deformable_Mixture_Parsing_Model_with_Parselets.html">8 iccv-2013-A Deformable Mixture Parsing Model with Parselets</a></p>
<p>17 0.6448189 <a title="383-lda-17" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>18 0.64418644 <a title="383-lda-18" href="./iccv-2013-Learning_a_Dictionary_of_Shape_Epitomes_with_Applications_to_Image_Labeling.html">245 iccv-2013-Learning a Dictionary of Shape Epitomes with Applications to Image Labeling</a></p>
<p>19 0.64409649 <a title="383-lda-19" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>20 0.64352804 <a title="383-lda-20" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
