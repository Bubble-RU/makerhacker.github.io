<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-226" href="#">iccv2013-226</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</h1>
<br/><p>Source: <a title="iccv-2013-226-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Liu_Joint_Subspace_Stabilization_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Feng Liu, Yuzhen Niu, Hailin Jin</p><p>Abstract: Shaky stereoscopic video is not only unpleasant to watch but may also cause 3D fatigue. Stabilizing the left and right view of a stereoscopic video separately using a monocular stabilization method tends to both introduce undesirable vertical disparities and damage horizontal disparities, which may destroy the stereoscopic viewing experience. In this paper, we present a joint subspace stabilization method for stereoscopic video. We prove that the low-rank subspace constraint for monocular video [10] also holds for stereoscopic video. Particularly, the feature trajectories from the left and right video share the same subspace. Based on this proof, we develop a stereo subspace stabilization method that jointly computes a common subspace from the left and right video and uses it to stabilize the two videos simultaneously. Our method meets the stereoscopic constraints without 3D reconstruction or explicit left-right correspondence. We test our method on a variety of stereoscopic videos with different scene content and camera motion. The experiments show that our method achieves high-quality stabilization for stereoscopic video in a robust and efficient way.</p><p>Reference: <a title="iccv-2013-226-reference" href="../iccv2013_reference/iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Shaky stereoscopic video is not only unpleasant to watch but may also cause 3D fatigue. [sent-8, score-0.744]
</p><p>2 Stabilizing the left and right view of a stereoscopic video separately using a monocular stabilization method tends to both introduce undesirable vertical disparities and damage horizontal disparities, which may destroy the stereoscopic viewing experience. [sent-9, score-2.558]
</p><p>3 In this paper, we present a joint subspace stabilization method for stereoscopic video. [sent-10, score-1.36]
</p><p>4 We prove that the low-rank subspace constraint for monocular video [10] also holds for stereoscopic video. [sent-11, score-1.055]
</p><p>5 Particularly, the feature trajectories from the left and right video share the same subspace. [sent-12, score-0.485]
</p><p>6 Based on this proof, we develop a stereo subspace stabilization method that jointly computes a common subspace from the left and right video and uses it to stabilize the two videos simultaneously. [sent-13, score-1.362]
</p><p>7 Our method meets the stereoscopic constraints without 3D reconstruction or explicit left-right correspondence. [sent-14, score-0.624]
</p><p>8 We test our method on a variety of stereoscopic videos with different scene content and camera motion. [sent-15, score-0.718]
</p><p>9 The experiments show that our method achieves high-quality stabilization for stereoscopic video in a robust and efficient way. [sent-16, score-1.232]
</p><p>10 Thanks to the recent success of 3D movies, there is a resurgence of interests in stereoscopic video. [sent-19, score-0.565]
</p><p>11 Nowadays we have capable hardware for displaying and capturing stereoscopic video. [sent-20, score-0.565]
</p><p>12 However, the development in stereoscopic video processing is still in its infancy. [sent-21, score-0.695]
</p><p>13 This paper addresses an important stereoscopic video processing problem, video stabilization. [sent-22, score-0.825]
</p><p>14 Video stabilization is the problem of removing undesired camera shake from a video. [sent-23, score-0.627]
</p><p>15 It has been shown that a good video stabilization algorithm can significantly improve the visual quality of an amateur video, making it close to the level of a professional one [14]. [sent-24, score-0.667]
</p><p>16 Stabilization algorithms play an even more important role in stereoscopic video because the effect ofcamera shake is more pronounced in stereo [19]. [sent-25, score-0.786]
</p><p>17 In particular, the temporal jitter in a shaky stereoscopic video can cause an excessive demand on the accommodation-vergence linkage [11]. [sent-26, score-0.8]
</p><p>18 Applying a homography-based monocular stabilization method [20] to each view of a stereoscopic video is problematic as it often damages the original horizontal disparities and induces the vertical disparities. [sent-28, score-1.812]
</p><p>19 Existing 3D reconstruction-based stabilization methods can be well extended to handle stereoscopic video [3, 5, 14]. [sent-30, score-1.232]
</p><p>20 Once the 3D  camera motion and scene structure are estimated, we can smooth the camera motion and synthesize a new pair of left and right video to follow the smooth camera motion. [sent-31, score-0.608]
</p><p>21 In this work, we present a joint subspace stabilization method for stereoscopic video. [sent-34, score-1.36]
</p><p>22 This method handles disparity problems in video stabilization without 3D reconstruction or even explicit left-right correspondence estimation. [sent-35, score-0.84]
</p><p>23 According to Irani [10], the feature trajectories from a short monocular video lie in a low-rank subspace. [sent-36, score-0.446]
</p><p>24 We extend this to stereoscopic video and prove that the feature trajectories from the left and right video of a stereoscopic video share a common subspace spanned by a small number of eigentrajectories. [sent-37, score-2.12]
</p><p>25 In this way, no vertical disparities will be introduced and horizonal disparities will be smoothed. [sent-40, score-0.697]
</p><p>26 These two are the key properties of a successful stereoscopic video stabilization method. [sent-41, score-1.232]
</p><p>27 First, we prove that the low-rank subspace constraint for monocular video [10] also holds for stereo video with no increase in the rank. [sent-43, score-0.684]
</p><p>28 By combining the trajectories from the left and right video, our method is robust to  73  the insufficiency of long trajectories and to the degeneration in scene content, camera motion, and tracking error. [sent-46, score-0.598]
</p><p>29 Related Work Existing work on monocular video stabilization can be categorized into 2D and 3D reconstruction-based methods. [sent-48, score-0.764]
</p><p>30 If they are independently applied to the left and right view of a stereo video, they can often introduce vertical disparities which may damage the stereoscopic viewing experience. [sent-51, score-1.207]
</p><p>31 3D reconstruction-based video stabilization methods compute 3D models of the scene and use image-based rendering techniques to render the stabilized video [2, 3, 5, 14]. [sent-52, score-0.887]
</p><p>32 They can achieve similar stabilization effects to the 3D reconstruction-based methods but retain most of the robustness and efficiency of the 2D methods. [sent-59, score-0.537]
</p><p>33 Unlike 3D reconstruction-based methods, applying them independently to each view of a stereoscopic video often does not produce satisfying results as shown later on. [sent-60, score-0.718]
</p><p>34 There is limited work on video stabilization beyond monocular cameras. [sent-62, score-0.764]
</p><p>35 Their method uses similarity transformations and includes an additional term to account for the vertical disparities between left and right video. [sent-65, score-0.507]
</p><p>36 It shares the same limitations of the 3D reconstruction-based stabilization methods in terms of robustness and efficiency. [sent-70, score-0.537]
</p><p>37 Joint Subspace Video Stabilization  Like monocular video stabilization methods, our method first tracks and smooths feature trajectories from input video, and then synthesizes stabilized video guided by the smooth feature trajectories. [sent-75, score-1.242]
</p><p>38 Specifically, we first track feature trajectories by applying the KLT algorithm [21] to the left and right video separately. [sent-76, score-0.483]
</p><p>39 The stabilization task is to obtain two sets of new feature trajectories { xˆiL (t) , yˆLi (t)} and { ˆxiR(t) , ˆy Ri(t)} that guide the rendering o{ xfˆ a pair of( stt)}ab ainlidze {d xˆ videos. [sent-80, score-0.783]
</p><p>40 Below we first show that if an affine camera model is assumed, we can easily perform 3D reconstruction using matrix factorization for stereo video stabilization. [sent-87, score-0.468]
</p><p>41 This shows that for an affine camera, the displacement matrix from a stereoscopic video can be factored into a camera matrix E and scene matrices CL and CR. [sent-134, score-0.976]
</p><p>42 We can then perform stereoscopic video stabilization in the following steps. [sent-136, score-1.232]
</p><p>43 Estimate feature trajectories from the left and right video separately and assemble a joint trajectory displacement matrix according to Equation 1. [sent-138, score-0.735]
</p><p>44 Warp the left and right video guided by the smooth trajectories using content-preserving warping [14]. [sent-144, score-0.494]
</p><p>45 This approach to stereoscopic video stabilization clearly will not bring in disparity problems as it only smoothes the motion of the stereo camera rig. [sent-145, score-1.49]
</p><p>46 Perspective Stereo Video Stabilization We are inspired by the recent subspace video stabilization method for monocular video that handles a more general camera, i. [sent-149, score-1.108]
</p><p>47 This monocular stabilization method was based on the subspace observation that the feature trajectories of a short video imaged by a perspective camera lie in a low-rank subspace [10]. [sent-152, score-1.535]
</p><p>48 But it has been shown in [15], matrix E, called “eigen-trajectory matrix”, can be smoothed in the same way as the camera matrix for the affine camera, and the smooth trajectories can be obtained by composing matrix C and the smoothed eigen-trajectory matrix. [sent-155, score-0.532]
</p><p>49 In the following, we first prove that the subspace constraint is also valid for stereoscopic video imaged by perspective cameras. [sent-156, score-1.019]
</p><p>50 In fact, the trajectories from the left and right video share a common subspace. [sent-157, score-0.458]
</p><p>51 Accordingly, we can apply a similar approach for stereoscopic video captured by affine cameras to those captured by perspective cameras. [sent-159, score-0.824]
</p><p>52 nHo 2we ivse ar,n nit eisx interesting fto t see rthesaut ltthe frroe ims no increase in the rank when we go from a monocular video to a stereoscopic one. [sent-200, score-0.848]
</p><p>53 Furthermore, we note that the left and right video share the same lowdimensional subspace E. [sent-201, score-0.48]
</p><p>54 Rank 9 is also used in the monocular subspace stabilization method [15]. [sent-215, score-0.848]
</p><p>55 1  Summary: joint subspace stabilization  We now summarize our stereoscopic video stabilization method for perspective cameras as follows. [sent-218, score-2.118]
</p><p>56 As we estimate the eigen-trajectory matrix E in Equation 9 jointly from the trajectories of the left and right video, we call our method joint subspace video stabilization. [sent-219, score-0.735]
</p><p>57 Estimate feature trajectories from the left and right video separately and assemble a joint trajectory matrix as follows. [sent-221, score-0.676]
</p><p>58 2  MˆR  Disparity  We now examine how this joint subspace stabilization method works on disparities in a stereoscopic video. [sent-237, score-1.664]
</p><p>59 Particularly, if the input video has zero vertical disparities (assuming a rectified stereo rig and no noise in feature tracking), CRy = CLy. [sent-240, score-0.635]
</p><p>60 Then the output trajectories will have no vertical disparities either. [sent-241, score-0.585]
</p><p>61 Meanwhile, as horizontal disparities in a stereoscopic video typically change insignificantly temporally, smoothing will not change disparity magnitudes significantly. [sent-246, score-1.184]
</p><p>62 separate subspace  Equation 9 shows that the feature trajectories from the left and right video share the same subspace. [sent-250, score-0.755]
</p><p>63 This actually implies that ideally, applying the monocular subspace stabilization method [15] to each video separately, called separate subspace stabilization, will lead to the same stabilization result as our joint subspace method. [sent-251, score-2.043]
</p><p>64 However, separate  ×  subspace stabilization often cannot work well in practice. [sent-252, score-0.807]
</p><p>65 The subspaces separately estimated for the left and right video are often different when there are feature tracking errors or the set offeature trajectories for the two videos differ from each other significantly. [sent-253, score-0.608]
</p><p>66 The discrepancy between the left and right stabilization results will then be introduced. [sent-254, score-0.651]
</p><p>67 We performed a simulation to compare the joint subspace method with the separate subspace method. [sent-255, score-0.528]
</p><p>68 One concern with the joint subspace method is the possibly increasing subspace fitting error as only one subspace is estimated instead of two. [sent-279, score-0.686]
</p><p>69 We examined the fitting error for 20 stereoscopic videos, each of which has a frame size 640 360. [sent-280, score-0.613]
</p><p>70 Experiments We tested our approach on a collection of57 stereoscopic videos, ranging from 10 to 80 seconds, captured by a variety of stereoscopic cameras in many different scenes. [sent-293, score-1.16]
</p><p>71 We compared our joint subspace stabilization method against the separate subspace stabilization method of applying [15] independently to the left and right view. [sent-294, score-1.716]
</p><p>72 We did not compare our  method against any 2D or 3D stabilization methods because that would almost amount to comparing [15] against 2D or 3D methods which is already covered in [15]. [sent-296, score-0.537]
</p><p>73 We consider a result unsuccessful if either the algorithm fails to process the video or the resulting stereoscopic video is uncomfortable to watch on a stereoscopic display. [sent-299, score-1.418]
</p><p>74 In addition to the same two videos that failed our method, the separate subspace method failed on another 7 videos where the camera moves very quickly and there are not a sufficient number of long feature trajectories in at least one of the two views. [sent-303, score-0.79]
</p><p>75 Since the subspace constraint is applied to each view independently, the feature trajectories between two views can often be smoothed in-  consistently, which sometimes leads to vertical disparities, as described in Section 3. [sent-311, score-0.593]
</p><p>76 We examined the vertical and horizontal disparities as well as their second derivatives in the output videos. [sent-319, score-0.51]
</p><p>77 edu/ ˜f l iu/proj ect / j oint - subspace 78  (a) Input frame sequence (b) Separate subspace stabilization results (c) Joint subspace stabilization results  Figure 2. [sent-334, score-1.741]
</p><p>78 Stabilizing each view independently causes vertical disparities and drastically disturbs the  horizontal disparities, as shown in (b). [sent-337, score-0.483]
</p><p>79 the left and right view and thus be able to provide more feature trajectories for the robust statistics of the stabilization results. [sent-338, score-0.893]
</p><p>80 We then compute the horizontal and vertical disparities and their second derivatives. [sent-342, score-0.46]
</p><p>81 They were only used to evaluate the stabilization results. [sent-344, score-0.537]
</p><p>82 We found that our method is able to produce stereoscopic video with smaller vertical disparities. [sent-345, score-0.784]
</p><p>83 In particular, the average vertical disparity by the separate subspace method  ×  is 0. [sent-346, score-0.45]
</p><p>84 The average vertical disparities of the separate subspace method and our method are 1. [sent-352, score-0.663]
</p><p>85 The average second derivatives of horizontal disparities of the input videos and the output videos from both methods are 0. [sent-355, score-0.54]
</p><p>86 Since the horizontal disparities are inversely proportional to the depth and the jitters of the camera motion in depth is often small, so the jitters in most feature trajectories are small except for scene points close to the camera. [sent-357, score-0.806]
</p><p>87 We find that both the separate subspace andjoint subspace method reduce it from 1. [sent-359, score-0.484]
</p><p>88 That is, we did not include the 9 videos that failed the separate subspace method and had no stabilization results. [sent-363, score-0.917]
</p><p>89 These results show that our joint subspace method can successfully avoid introducing extra vertical disparities and smooth horizontal disparities. [sent-364, score-0.756]
</p><p>90 The reason that the separate subspace method does not produce significantly large vertical disparities or the second disparity derivatives is that these 48 videos have abundant long feature trajectories and thus can be handled by the separate subspace method, as discussed in Section 3. [sent-365, score-1.359]
</p><p>91 In particular, it is able to produce high-quality stabilization results without computing a 3D reconstruction and it is robust, efficient, and allows  a streaming implementation. [sent-384, score-0.596]
</p><p>92 However, our method still requires enough long feature trajectories for matrix factorization and has difficulties to handle videos with dominating scene motion, excessive shake, or strong motion blur, such as the examples shown in Figure 3. [sent-386, score-0.501]
</p><p>93 Conclusion This paper presented a joint subspace stabilization method for stereoscopic video. [sent-390, score-1.36]
</p><p>94 We proved that the subspace constraint is valid for stereoscopic video. [sent-391, score-0.797]
</p><p>95 We showed that we can stabilize stereoscopic video without any explicit correspondence computation. [sent-393, score-0.736]
</p><p>96 Moreover, our method is more robust to the presence of short trajectories than the monocular subspace stabilization method. [sent-394, score-1.04]
</p><p>97 We validate our method on a variety of stereoscopic videos with different scene content and camera motion. [sent-395, score-0.718]
</p><p>98 Our experiments show that our method is able to achieve high-quality stereoscopic video stabilization in a robust and efficient way. [sent-396, score-1.232]
</p><p>99 Auto-directed video stabilization with robust l1 optimal camera paths. [sent-467, score-0.73]
</p><p>100 3d cinematography principles and their applications to stereoscopic media processing. [sent-505, score-0.565]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stereoscopic', 0.565), ('stabilization', 0.537), ('disparities', 0.304), ('subspace', 0.214), ('trajectories', 0.192), ('video', 0.13), ('xr', 0.113), ('monocular', 0.097), ('disparity', 0.091), ('vertical', 0.089), ('trajectory', 0.081), ('factorization', 0.073), ('videos', 0.071), ('horizontal', 0.067), ('stereo', 0.064), ('camera', 0.063), ('perspective', 0.061), ('yr', 0.061), ('left', 0.061), ('reconstruction', 0.059), ('xir', 0.059), ('displacement', 0.059), ('cr', 0.057), ('separate', 0.056), ('right', 0.053), ('cl', 0.052), ('xl', 0.051), ('rolling', 0.049), ('klt', 0.046), ('joint', 0.044), ('stabilized', 0.044), ('shutter', 0.044), ('shaky', 0.043), ('matrix', 0.041), ('instantaneous', 0.04), ('motion', 0.04), ('failed', 0.039), ('affine', 0.038), ('gleicher', 0.038), ('smooth', 0.038), ('rank', 0.038), ('ur', 0.035), ('vr', 0.035), ('subspaces', 0.032), ('prove', 0.031), ('cameras', 0.03), ('smoothed', 0.03), ('jin', 0.03), ('yl', 0.03), ('proposition', 0.029), ('ccrl', 0.029), ('jitters', 0.029), ('xlyl', 0.029), ('xryr', 0.029), ('yrzr', 0.029), ('yuzhen', 0.029), ('assemble', 0.028), ('watch', 0.028), ('focal', 0.028), ('rendering', 0.027), ('feature', 0.027), ('smoothing', 0.027), ('shake', 0.027), ('viewing', 0.027), ('derivatives', 0.027), ('equation', 0.026), ('yri', 0.025), ('frame', 0.025), ('yli', 0.023), ('correspondence', 0.023), ('examined', 0.023), ('view', 0.023), ('ul', 0.023), ('errors', 0.023), ('share', 0.022), ('stabilizing', 0.022), ('xil', 0.022), ('vl', 0.022), ('jitter', 0.021), ('zr', 0.021), ('damage', 0.021), ('rig', 0.021), ('fl', 0.021), ('cause', 0.021), ('track', 0.02), ('matrices', 0.02), ('fps', 0.02), ('guided', 0.02), ('excessive', 0.02), ('zl', 0.02), ('scene', 0.019), ('separately', 0.019), ('go', 0.018), ('long', 0.018), ('constraint', 0.018), ('stabilize', 0.018), ('depth', 0.018), ('composing', 0.018), ('adobe', 0.017), ('worked', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="226-tfidf-1" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>Author: Feng Liu, Yuzhen Niu, Hailin Jin</p><p>Abstract: Shaky stereoscopic video is not only unpleasant to watch but may also cause 3D fatigue. Stabilizing the left and right view of a stereoscopic video separately using a monocular stabilization method tends to both introduce undesirable vertical disparities and damage horizontal disparities, which may destroy the stereoscopic viewing experience. In this paper, we present a joint subspace stabilization method for stereoscopic video. We prove that the low-rank subspace constraint for monocular video [10] also holds for stereoscopic video. Particularly, the feature trajectories from the left and right video share the same subspace. Based on this proof, we develop a stereo subspace stabilization method that jointly computes a common subspace from the left and right video and uses it to stabilize the two videos simultaneously. Our method meets the stereoscopic constraints without 3D reconstruction or explicit left-right correspondence. We test our method on a variety of stereoscopic videos with different scene content and camera motion. The experiments show that our method achieves high-quality stabilization for stereoscopic video in a robust and efficient way.</p><p>2 0.21168929 <a title="226-tfidf-2" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>Author: Karteek Alahari, Guillaume Seguin, Josef Sivic, Ivan Laptev</p><p>Abstract: We seek to obtain a pixel-wise segmentation and pose estimation of multiple people in a stereoscopic video. This involves challenges such as dealing with unconstrained stereoscopic video, non-stationary cameras, and complex indoor and outdoor dynamic scenes. The contributions of our work are two-fold: First, we develop a segmentation model incorporating person detection, pose estimation, as well as colour, motion, and disparity cues. Our new model explicitly represents depth ordering and occlusion. Second, we introduce a stereoscopic dataset with frames extracted from feature-length movies “StreetDance 3D ” and “Pina ”. The dataset contains 2727 realistic stereo pairs and includes annotation of human poses, person bounding boxes, and pixel-wise segmentations for hundreds of people. The dataset is composed of indoor and outdoor scenes depicting multiple people with frequent occlusions. We demonstrate results on our new challenging dataset, as well as on the H2view dataset from (Sheasby et al. ACCV 2012).</p><p>3 0.1849965 <a title="226-tfidf-3" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>Author: Ali Elqursh, Ahmed Elgammal</p><p>Abstract: The vast majority of work on motion segmentation adopts the affine camera model due to its simplicity. Under the affine model, the motion segmentation problem becomes that of subspace separation. Due to this assumption, such methods are mainly offline and exhibit poor performance when the assumption is not satisfied. This is made evident in state-of-the-art methods that relax this assumption by using piecewise affine spaces and spectral clustering techniques to achieve better results. In this paper, we formulate the problem of motion segmentation as that of manifold separation. We then show how label propagation can be used in an online framework to achieve manifold separation. The performance of our framework is evaluated on a benchmark dataset and achieves competitive performance while being online.</p><p>4 0.1648258 <a title="226-tfidf-4" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>Author: Feng Shi, Zhong Zhou, Jiangjian Xiao, Wei Wu</p><p>Abstract: Due to occlusions and objects ’ non-rigid deformation in the scene, the obtained motion trajectories from common trackers may contain a number of missing or mis-associated entries. To cluster such corrupted point based trajectories into multiple motions is still a hard problem. In this paper, we present an approach that exploits temporal and spatial characteristics from tracked points to facilitate segmentation of incomplete and corrupted trajectories, thereby obtain highly robust results against severe data missing and noises. Our method first uses the Discrete Cosine Transform (DCT) bases as a temporal smoothness constraint on trajectory projection to ensure the validity of resulting components to repair pathological trajectories. Then, based on an observation that the trajectories of foreground and background in a scene may have different spatial distributions, we propose a two-stage clustering strategy that first performs foreground-background separation then segments remaining foreground trajectories. We show that, with this new clustering strategy, sequences with complex motions can be accurately segmented by even using a simple trans- lational model. Finally, a series of experiments on Hopkins 155 dataset andBerkeley motion segmentation dataset show the advantage of our method over other state-of-the-art motion segmentation algorithms in terms of both effectiveness and robustness.</p><p>5 0.15337862 <a title="226-tfidf-5" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>Author: Heng Wang, Cordelia Schmid</p><p>Abstract: Recently dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets. This paper improves their performance by taking into account camera motion to correct them. To estimate camera motion, we match feature points between frames using SURF descriptors and dense optical flow, which are shown to be complementary. These matches are, then, used to robustly estimate a homography with RANSAC. Human motion is in general different from camera motion and generates inconsistent matches. To improve the estimation, a human detector is employed to remove these matches. Given the estimated camera motion, we remove trajectories consistent with it. We also use this estimation to cancel out camera motion from the optical flow. This significantly improves motion-based descriptors, such as HOF and MBH. Experimental results onfour challenging action datasets (i.e., Hollywood2, HMDB51, Olympic Sports and UCF50) significantly outperform the current state of the art.</p><p>6 0.14310683 <a title="226-tfidf-6" href="./iccv-2013-Camera_Alignment_Using_Trajectory_Intersections_in_Unsynchronized_Videos.html">68 iccv-2013-Camera Alignment Using Trajectory Intersections in Unsynchronized Videos</a></p>
<p>7 0.13915941 <a title="226-tfidf-7" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>8 0.13557179 <a title="226-tfidf-8" href="./iccv-2013-Robust_Subspace_Clustering_via_Half-Quadratic_Minimization.html">360 iccv-2013-Robust Subspace Clustering via Half-Quadratic Minimization</a></p>
<p>9 0.13545506 <a title="226-tfidf-9" href="./iccv-2013-Latent_Space_Sparse_Subspace_Clustering.html">232 iccv-2013-Latent Space Sparse Subspace Clustering</a></p>
<p>10 0.12611754 <a title="226-tfidf-10" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>11 0.11195546 <a title="226-tfidf-11" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>12 0.098052748 <a title="226-tfidf-12" href="./iccv-2013-PM-Huber%3A_PatchMatch_with_Huber_Regularization_for_Stereo_Matching.html">304 iccv-2013-PM-Huber: PatchMatch with Huber Regularization for Stereo Matching</a></p>
<p>13 0.097668134 <a title="226-tfidf-13" href="./iccv-2013-Rolling_Shutter_Stereo.html">363 iccv-2013-Rolling Shutter Stereo</a></p>
<p>14 0.094418727 <a title="226-tfidf-14" href="./iccv-2013-GOSUS%3A_Grassmannian_Online_Subspace_Updates_with_Structured-Sparsity.html">182 iccv-2013-GOSUS: Grassmannian Online Subspace Updates with Structured-Sparsity</a></p>
<p>15 0.088872932 <a title="226-tfidf-15" href="./iccv-2013-Measuring_Flow_Complexity_in_Videos.html">263 iccv-2013-Measuring Flow Complexity in Videos</a></p>
<p>16 0.082414873 <a title="226-tfidf-16" href="./iccv-2013-A_Unified_Rolling_Shutter_and_Motion_Blur_Model_for_3D_Visual_Registration.html">32 iccv-2013-A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration</a></p>
<p>17 0.081224322 <a title="226-tfidf-17" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>18 0.080737077 <a title="226-tfidf-18" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>19 0.078701399 <a title="226-tfidf-19" href="./iccv-2013-Minimal_Basis_Facility_Location_for_Subspace_Segmentation.html">264 iccv-2013-Minimal Basis Facility Location for Subspace Segmentation</a></p>
<p>20 0.074709594 <a title="226-tfidf-20" href="./iccv-2013-Distributed_Low-Rank_Subspace_Segmentation.html">122 iccv-2013-Distributed Low-Rank Subspace Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.147), (1, -0.087), (2, -0.015), (3, 0.129), (4, -0.079), (5, 0.103), (6, 0.039), (7, 0.043), (8, 0.149), (9, 0.107), (10, 0.075), (11, 0.01), (12, -0.036), (13, -0.019), (14, -0.068), (15, -0.048), (16, -0.02), (17, 0.023), (18, 0.018), (19, 0.069), (20, -0.097), (21, -0.018), (22, -0.048), (23, 0.144), (24, 0.028), (25, 0.035), (26, -0.061), (27, -0.079), (28, 0.034), (29, 0.071), (30, -0.034), (31, -0.036), (32, -0.008), (33, -0.058), (34, 0.091), (35, -0.035), (36, -0.061), (37, -0.038), (38, 0.014), (39, -0.035), (40, -0.008), (41, 0.019), (42, -0.043), (43, -0.0), (44, 0.052), (45, 0.017), (46, -0.049), (47, -0.059), (48, 0.002), (49, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93193656 <a title="226-lsi-1" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>Author: Feng Liu, Yuzhen Niu, Hailin Jin</p><p>Abstract: Shaky stereoscopic video is not only unpleasant to watch but may also cause 3D fatigue. Stabilizing the left and right view of a stereoscopic video separately using a monocular stabilization method tends to both introduce undesirable vertical disparities and damage horizontal disparities, which may destroy the stereoscopic viewing experience. In this paper, we present a joint subspace stabilization method for stereoscopic video. We prove that the low-rank subspace constraint for monocular video [10] also holds for stereoscopic video. Particularly, the feature trajectories from the left and right video share the same subspace. Based on this proof, we develop a stereo subspace stabilization method that jointly computes a common subspace from the left and right video and uses it to stabilize the two videos simultaneously. Our method meets the stereoscopic constraints without 3D reconstruction or explicit left-right correspondence. We test our method on a variety of stereoscopic videos with different scene content and camera motion. The experiments show that our method achieves high-quality stabilization for stereoscopic video in a robust and efficient way.</p><p>2 0.71032459 <a title="226-lsi-2" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>Author: Feng Shi, Zhong Zhou, Jiangjian Xiao, Wei Wu</p><p>Abstract: Due to occlusions and objects ’ non-rigid deformation in the scene, the obtained motion trajectories from common trackers may contain a number of missing or mis-associated entries. To cluster such corrupted point based trajectories into multiple motions is still a hard problem. In this paper, we present an approach that exploits temporal and spatial characteristics from tracked points to facilitate segmentation of incomplete and corrupted trajectories, thereby obtain highly robust results against severe data missing and noises. Our method first uses the Discrete Cosine Transform (DCT) bases as a temporal smoothness constraint on trajectory projection to ensure the validity of resulting components to repair pathological trajectories. Then, based on an observation that the trajectories of foreground and background in a scene may have different spatial distributions, we propose a two-stage clustering strategy that first performs foreground-background separation then segments remaining foreground trajectories. We show that, with this new clustering strategy, sequences with complex motions can be accurately segmented by even using a simple trans- lational model. Finally, a series of experiments on Hopkins 155 dataset andBerkeley motion segmentation dataset show the advantage of our method over other state-of-the-art motion segmentation algorithms in terms of both effectiveness and robustness.</p><p>3 0.69477898 <a title="226-lsi-3" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>Author: Ali Elqursh, Ahmed Elgammal</p><p>Abstract: The vast majority of work on motion segmentation adopts the affine camera model due to its simplicity. Under the affine model, the motion segmentation problem becomes that of subspace separation. Due to this assumption, such methods are mainly offline and exhibit poor performance when the assumption is not satisfied. This is made evident in state-of-the-art methods that relax this assumption by using piecewise affine spaces and spectral clustering techniques to achieve better results. In this paper, we formulate the problem of motion segmentation as that of manifold separation. We then show how label propagation can be used in an online framework to achieve manifold separation. The performance of our framework is evaluated on a benchmark dataset and achieves competitive performance while being online.</p><p>4 0.65660763 <a title="226-lsi-4" href="./iccv-2013-Camera_Alignment_Using_Trajectory_Intersections_in_Unsynchronized_Videos.html">68 iccv-2013-Camera Alignment Using Trajectory Intersections in Unsynchronized Videos</a></p>
<p>Author: Thomas Kuo, Santhoshkumar Sunderrajan, B.S. Manjunath</p><p>Abstract: This paper addresses the novel and challenging problem of aligning camera views that are unsynchronized by low and/or variable frame rates using object trajectories. Unlike existing trajectory-based alignment methods, our method does not require frame-to-frame synchronization. Instead, we propose using the intersections of corresponding object trajectories to match views. To find these intersections, we introduce a novel trajectory matching algorithm based on matching Spatio-Temporal Context Graphs (STCGs). These graphs represent the distances between trajectories in time and space within a view, and are matched to an STCG from another view to find the corresponding trajectories. To the best of our knowledge, this is one of the first attempts to align views that are unsynchronized with variable frame rates. The results on simulated and real-world datasets show trajectory intersections area viablefeatureforcamera alignment, and that the trajectory matching method performs well in real-world scenarios.</p><p>5 0.64261043 <a title="226-lsi-5" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>Author: Zhuwen Li, Jiaming Guo, Loong-Fah Cheong, Steven Zhiying Zhou</p><p>Abstract: This paper addresses real-world challenges in the motion segmentation problem, including perspective effects, missing data, and unknown number of motions. It first formulates the 3-D motion segmentation from two perspective views as a subspace clustering problem, utilizing the epipolar constraint of an image pair. It then combines the point correspondence information across multiple image frames via a collaborative clustering step, in which tight integration is achieved via a mixed norm optimization scheme. For model selection, wepropose an over-segment and merge approach, where the merging step is based on the property of the ?1-norm ofthe mutual sparse representation oftwo oversegmented groups. The resulting algorithm can deal with incomplete trajectories and perspective effects substantially better than state-of-the-art two-frame and multi-frame methods. Experiments on a 62-clip dataset show the significant superiority of the proposed idea in both segmentation accuracy and model selection.</p><p>6 0.56206936 <a title="226-lsi-6" href="./iccv-2013-A_Rotational_Stereo_Model_Based_on_XSlit_Imaging.html">28 iccv-2013-A Rotational Stereo Model Based on XSlit Imaging</a></p>
<p>7 0.55888015 <a title="226-lsi-7" href="./iccv-2013-Minimal_Basis_Facility_Location_for_Subspace_Segmentation.html">264 iccv-2013-Minimal Basis Facility Location for Subspace Segmentation</a></p>
<p>8 0.55359495 <a title="226-lsi-8" href="./iccv-2013-Measuring_Flow_Complexity_in_Videos.html">263 iccv-2013-Measuring Flow Complexity in Videos</a></p>
<p>9 0.53204787 <a title="226-lsi-9" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>10 0.53111571 <a title="226-lsi-10" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>11 0.53064072 <a title="226-lsi-11" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>12 0.52810532 <a title="226-lsi-12" href="./iccv-2013-PM-Huber%3A_PatchMatch_with_Huber_Regularization_for_Stereo_Matching.html">304 iccv-2013-PM-Huber: PatchMatch with Huber Regularization for Stereo Matching</a></p>
<p>13 0.48090053 <a title="226-lsi-13" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>14 0.47069469 <a title="226-lsi-14" href="./iccv-2013-Rolling_Shutter_Stereo.html">363 iccv-2013-Rolling Shutter Stereo</a></p>
<p>15 0.46024826 <a title="226-lsi-15" href="./iccv-2013-Latent_Space_Sparse_Subspace_Clustering.html">232 iccv-2013-Latent Space Sparse Subspace Clustering</a></p>
<p>16 0.45503914 <a title="226-lsi-16" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>17 0.43872949 <a title="226-lsi-17" href="./iccv-2013-Space-Time_Tradeoffs_in_Photo_Sequencing.html">397 iccv-2013-Space-Time Tradeoffs in Photo Sequencing</a></p>
<p>18 0.43447384 <a title="226-lsi-18" href="./iccv-2013-Optimal_Orthogonal_Basis_and_Image_Assimilation%3A_Motion_Modeling.html">301 iccv-2013-Optimal Orthogonal Basis and Image Assimilation: Motion Modeling</a></p>
<p>19 0.43346775 <a title="226-lsi-19" href="./iccv-2013-A_Unified_Rolling_Shutter_and_Motion_Blur_Model_for_3D_Visual_Registration.html">32 iccv-2013-A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration</a></p>
<p>20 0.43316215 <a title="226-lsi-20" href="./iccv-2013-Line_Assisted_Light_Field_Triangulation_and_Stereo_Matching.html">252 iccv-2013-Line Assisted Light Field Triangulation and Stereo Matching</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.041), (7, 0.021), (12, 0.015), (26, 0.048), (27, 0.013), (31, 0.051), (35, 0.012), (42, 0.098), (55, 0.228), (64, 0.05), (73, 0.061), (89, 0.202), (97, 0.012), (98, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86231107 <a title="226-lda-1" href="./iccv-2013-Joint_Optimization_for_Consistent_Multiple_Graph_Matching.html">224 iccv-2013-Joint Optimization for Consistent Multiple Graph Matching</a></p>
<p>Author: Junchi Yan, Yu Tian, Hongyuan Zha, Xiaokang Yang, Ya Zhang, Stephen M. Chu</p><p>Abstract: The problem of graph matching in general is NP-hard and approaches have been proposed for its suboptimal solution, most focusing on finding the one-to-one node mapping between two graphs. A more general and challenging problem arises when one aims to find consistent mappings across a number of graphs more than two. Conventional graph pair matching methods often result in mapping inconsistency since the mapping between two graphs can either be determined by pair mapping or by an additional anchor graph. To address this issue, a novel formulation is derived which is maximized via alternating optimization. Our method enjoys several advantages: 1) the mappings are jointly optimized rather than sequentially performed by applying pair matching, allowing the global affinity information across graphs can be propagated and explored; 2) the number of concerned variables to optimize is in linear with the number of graphs, being superior to local pair matching resulting in O(n2) variables; 3) the mapping consistency constraints are analytically satisfied during optimization; and 4) off-the-shelf graph pair matching solvers can be reused under the proposed framework in an ‘out-of-thebox’ fashion. Competitive results on both the synthesized data and the real data are reported, by varying the level of deformation, outliers and edge densities. ∗Corresponding author. The work is supported by NSF IIS1116886, NSF IIS-1049694, NSFC 61129001/F010403 and the 111 Project (B07022). Yu Tian Shanghai Jiao Tong University Shanghai, China, 200240 yut ian @ s j tu . edu .cn Xiaokang Yang Shanghai Jiao Tong University Shanghai, China, 200240 xkyang@ s j tu .edu . cn Stephen M. Chu IBM T.J. Waston Research Center Yorktown Heights, NY USA, 10598 s chu @u s . ibm . com</p><p>2 0.83645505 <a title="226-lda-2" href="./iccv-2013-A_Unified_Rolling_Shutter_and_Motion_Blur_Model_for_3D_Visual_Registration.html">32 iccv-2013-A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration</a></p>
<p>Author: Maxime Meilland, Tom Drummond, Andrew I. Comport</p><p>Abstract: Motion blur and rolling shutter deformations both inhibit visual motion registration, whether it be due to a moving sensor or a moving target. Whilst both deformations exist simultaneously, no models have been proposed to handle them together. Furthermore, neither deformation has been consideredpreviously in the context of monocularfullimage 6 degrees of freedom registration or RGB-D structure and motion. As will be shown, rolling shutter deformation is observed when a camera moves faster than a single pixel in parallax between subsequent scan-lines. Blur is a function of the pixel exposure time and the motion vector. In this paper a complete dense 3D registration model will be derived to accountfor both motion blur and rolling shutter deformations simultaneously. Various approaches will be compared with respect to ground truth and live real-time performance will be demonstratedfor complex scenarios where both blur and shutter deformations are dominant.</p><p>3 0.82834095 <a title="226-lda-3" href="./iccv-2013-Geometric_Registration_Based_on_Distortion_Estimation.html">183 iccv-2013-Geometric Registration Based on Distortion Estimation</a></p>
<p>Author: Wei Zeng, Mayank Goswami, Feng Luo, Xianfeng Gu</p><p>Abstract: Surface registration plays a fundamental role in many applications in computer vision and aims at finding a oneto-one correspondence between surfaces. Conformal mapping based surface registration methods conformally map 2D/3D surfaces onto 2D canonical domains and perform the matching on the 2D plane. This registration framework reduces dimensionality, and the result is intrinsic to Riemannian metric and invariant under isometric deformation. However, conformal mapping will be affected by inconsistent boundaries and non-isometric deformations of surfaces. In this work, we quantify the effects of boundary variation and non-isometric deformation to conformal mappings, and give the theoretical upper bounds for the distortions of conformal mappings under these two factors. Besides giving the thorough theoretical proofs of the theorems, we verified them by concrete experiments using 3D human facial scans with dynamic expressions and varying boundaries. Furthermore, we used the distortion estimates for reducing search range in feature matching of surface registration applications. The experimental results are consistent with the theoreticalpredictions and also demonstrate the performance improvements in feature tracking.</p><p>4 0.82223302 <a title="226-lda-4" href="./iccv-2013-Rolling_Shutter_Stereo.html">363 iccv-2013-Rolling Shutter Stereo</a></p>
<p>Author: Olivier Saurer, Kevin Köser, Jean-Yves Bouguet, Marc Pollefeys</p><p>Abstract: A huge fraction of cameras used nowadays is based on CMOS sensors with a rolling shutter that exposes the image line by line. For dynamic scenes/cameras this introduces undesired effects like stretch, shear and wobble. It has been shown earlier that rotational shake induced rolling shutter effects in hand-held cell phone capture can be compensated based on an estimate of the camera rotation. In contrast, we analyse the case of significant camera motion, e.g. where a bypassing streetlevel capture vehicle uses a rolling shutter camera in a 3D reconstruction framework. The introduced error is depth dependent and cannot be compensated based on camera motion/rotation alone, invalidating also rectification for stereo camera systems. On top, significant lens distortion as often present in wide angle cameras intertwines with rolling shutter effects as it changes the time at which a certain 3D point is seen. We show that naive 3D reconstructions (assuming global shutter) will deliver biased geometry already for very mild assumptions on vehicle speed and resolution. We then develop rolling shutter dense multiview stereo algorithms that solve for time of exposure and depth at the same time, even in the presence of lens distortion and perform an evaluation on ground truth laser scan models as well as on real street-level data.</p><p>same-paper 5 0.82196164 <a title="226-lda-5" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>Author: Feng Liu, Yuzhen Niu, Hailin Jin</p><p>Abstract: Shaky stereoscopic video is not only unpleasant to watch but may also cause 3D fatigue. Stabilizing the left and right view of a stereoscopic video separately using a monocular stabilization method tends to both introduce undesirable vertical disparities and damage horizontal disparities, which may destroy the stereoscopic viewing experience. In this paper, we present a joint subspace stabilization method for stereoscopic video. We prove that the low-rank subspace constraint for monocular video [10] also holds for stereoscopic video. Particularly, the feature trajectories from the left and right video share the same subspace. Based on this proof, we develop a stereo subspace stabilization method that jointly computes a common subspace from the left and right video and uses it to stabilize the two videos simultaneously. Our method meets the stereoscopic constraints without 3D reconstruction or explicit left-right correspondence. We test our method on a variety of stereoscopic videos with different scene content and camera motion. The experiments show that our method achieves high-quality stabilization for stereoscopic video in a robust and efficient way.</p><p>6 0.81434929 <a title="226-lda-6" href="./iccv-2013-A_New_Image_Quality_Metric_for_Image_Auto-denoising.html">23 iccv-2013-A New Image Quality Metric for Image Auto-denoising</a></p>
<p>7 0.80362922 <a title="226-lda-7" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>8 0.79420519 <a title="226-lda-8" href="./iccv-2013-Perceptual_Fidelity_Aware_Mean_Squared_Error.html">312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</a></p>
<p>9 0.77818209 <a title="226-lda-9" href="./iccv-2013-Estimating_the_3D_Layout_of_Indoor_Scenes_and_Its_Clutter_from_Depth_Sensors.html">144 iccv-2013-Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors</a></p>
<p>10 0.72729975 <a title="226-lda-10" href="./iccv-2013-Bayesian_Robust_Matrix_Factorization_for_Image_and_Video_Processing.html">60 iccv-2013-Bayesian Robust Matrix Factorization for Image and Video Processing</a></p>
<p>11 0.72480255 <a title="226-lda-11" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<p>12 0.72467291 <a title="226-lda-12" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>13 0.72420126 <a title="226-lda-13" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<p>14 0.72327763 <a title="226-lda-14" href="./iccv-2013-Robust_Non-parametric_Data_Fitting_for_Correspondence_Modeling.html">358 iccv-2013-Robust Non-parametric Data Fitting for Correspondence Modeling</a></p>
<p>15 0.72296959 <a title="226-lda-15" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>16 0.72233605 <a title="226-lda-16" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>17 0.72226703 <a title="226-lda-17" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>18 0.72210217 <a title="226-lda-18" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>19 0.72186899 <a title="226-lda-19" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>20 0.72185767 <a title="226-lda-20" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
