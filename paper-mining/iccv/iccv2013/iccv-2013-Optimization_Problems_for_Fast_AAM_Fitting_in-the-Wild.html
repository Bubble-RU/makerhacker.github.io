<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>302 iccv-2013-Optimization Problems for Fast AAM Fitting in-the-Wild</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-302" href="#">iccv2013-302</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>302 iccv-2013-Optimization Problems for Fast AAM Fitting in-the-Wild</h1>
<br/><p>Source: <a title="iccv-2013-302-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Tzimiropoulos_Optimization_Problems_for_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Georgios Tzimiropoulos, Maja Pantic</p><p>Abstract: We describe a very simple framework for deriving the most-well known optimization problems in Active Appearance Models (AAMs), and most importantly for providing efficient solutions. Our formulation results in two optimization problems for fast and exact AAM fitting, and one new algorithm which has the important advantage of being applicable to 3D. We show that the dominant cost for both forward and inverse algorithms is a few times mN which is the cost of projecting an image onto the appearance subspace. This makes both algorithms not only computationally realizable but also very attractive speed-wise for most current systems. Because exact AAM fitting is no longer computationally prohibitive, we trained AAMs in-the-wild with the goal of investigating whether AAMs benefit from such a training process. Our results show that although we did not use sophisticated shape priors, robust features or robust norms for improving performance, AAMs perform notably well and in some cases comparably with current state-ofthe-art methods. We provide Matlab source code for training, fitting and reproducing the results presented in this paper at ht tp ://ibug. . doc . i . a c . uk/resources. c</p><p>Reference: <a title="iccv-2013-302-reference" href="../iccv2013_reference/iccv-2013-Optimization_Problems_for_Fast_AAM_Fitting_in-the-Wild_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Optimization problems for fast AAM fitting in-the-wild  Georgios Tzimiropoulos 1. [sent-1, score-0.275]
</p><p>2 We show that the dominant cost for both forward and inverse algorithms is a few times mN which is the cost of projecting an image onto the appearance subspace. [sent-11, score-0.545]
</p><p>3 Because exact AAM fitting is no longer computationally prohibitive, we trained AAMs in-the-wild with the goal of investigating whether AAMs benefit from such  a training process. [sent-13, score-0.341]
</p><p>4 Our results show that although we did not use sophisticated shape priors, robust features or robust norms for improving performance, AAMs perform notably well and in some cases comparably with current state-ofthe-art methods. [sent-14, score-0.248]
</p><p>5 We provide Matlab source code for training, fitting and reproducing the results presented in this paper at ht tp ://ibug. [sent-15, score-0.301]
</p><p>6 They are statistical models of shape and appearance that can generate instances of a specific object class (e. [sent-23, score-0.161]
</p><p>7 faces) given a small number of model parameters which control shape and appearance variation. [sent-25, score-0.161]
</p><p>8 Recovering the shape parameters is important because it implies that the location of a set of landmarks (or fiducial points) has been detected in the Maja Pantic 1. [sent-27, score-0.128]
</p><p>9 The appearance model of the AAM was built using raw un-normalized pixel intensities as features. [sent-36, score-0.142]
</p><p>10 Neither sophisticated shape priors or robust norms were used during fitting nor robust image features were employed to build the AAM. [sent-37, score-0.476]
</p><p>11 Even without such sophisticated enhancements, AAM fitting produced satisfactory accuracy in landmark localization. [sent-38, score-0.367]
</p><p>12 To obtain these results, we simply trained the AAM in-the-wild (on the same database) and additionally for fitting and we used Fast-Forward algorithm, an exact but fast simultaneous algorithm. [sent-39, score-0.352]
</p><p>13 Hence, fitting AAMs robustly to new images has been the focus of extensive research over the past years. [sent-42, score-0.275]
</p><p>14 AAM fitting is an iterative process at each iteration of which an update of the current model parameters is esti-  mated. [sent-43, score-0.337]
</p><p>15 For example in [5], the relationship between the error image and the update is assumed lin593 ear and independent of the current model parameters. [sent-48, score-0.091]
</p><p>16 Other discriminative methods for fitting AAMs have been proposed in [11, 22, 20]. [sent-50, score-0.275]
</p><p>17 The second line of research for fitting AAMs is through non-linear least-squares [16]. [sent-51, score-0.275]
</p><p>18 AAM fitting is formulated as a Lukas-Kanade (LK) problem which can be solved iteratively using Gauss-Newton optimization. [sent-52, score-0.275]
</p><p>19 One of the major contributions of [16] is the so-called project-out inverse compositional algorithm (POIC). [sent-55, score-0.185]
</p><p>20 This combination results in an algorithm which is as efficient as regression-based approaches and is now considered the standard choice for fitting personspecific AAMs (i. [sent-57, score-0.275]
</p><p>21 AAMs trained for fitting face images of a specific subject which is known in advanced). [sent-59, score-0.331]
</p><p>22 AAMs trained for fitting face images of various subjects not known in advance). [sent-62, score-0.331]
</p><p>23 In contrast to POIC, the simultaneous inverse compositional (SIC) algorithm, proposed in [1], has been shown to perform robustly for the case of generic fitting [7]. [sent-63, score-0.532]
</p><p>24 However, the computational cost of the algorithm is almost prohibitive for most applications. [sent-64, score-0.103]
</p><p>25 Let n and m denote the number of the shape and appearance parameters of the AAM. [sent-65, score-0.161]
</p><p>26 Then, the dominant cost per iteration of SIC is on the order of (n + m)2N, where N is the number of pixels in the reference frame. [sent-66, score-0.126]
</p><p>27 We show that the cost for solving the exact AAM non-linear least squares problem with no approximations for both forward and inverse is significantly less than O((n + m)2N). [sent-76, score-0.308]
</p><p>28 (1)  As we show later on, using (1) reduces the dominant cost for both forward and inverse algorithms to nmN. [sent-79, score-0.34]
</p><p>29 n, which is the case for generic face alignment, tfoher mcos ? [sent-81, score-0.093]
</p><p>30 is n r,e wduhcicehd itso a ef cewas etifm oers g emnNer wc fhaicceh ailsi gthnecost of projecting an image onto the appearance subspace. [sent-83, score-0.128]
</p><p>31 Hence, our derivations shed further light on the different optimization problems that POIC and SIC solve. [sent-87, score-0.089]
</p><p>32 Additionally, the authors of [18] investigated  only the inverse case. [sent-88, score-0.113]
</p><p>33 As it is well known, the inverse compositional approach cannot be applied to 3D AAMs [23]. [sent-89, score-0.185]
</p><p>34 One of our main contributions is to show that (1) can be used to derive a forward additive update scheme and hence can be readily applied to 3D. [sent-90, score-0.245]
</p><p>35 Our second main contribution is to train AAMs in-thewild using the well-known LFPW database [3] and then fit using the proposed fast forward and inverse simultaneous algorithms, with the goal of investigating whether AAMs benefit from such a training process. [sent-92, score-0.248]
</p><p>36 These results are notable given that no shape prior was used, the employed appearance model was built using raw pixel intensities and no attempt to use more sophisticated image features (like Gabor filter responses as in [14] or SIFT features [12] as in [3]) was made. [sent-95, score-0.299]
</p><p>37 AAMs An AAM is defined by the shape, appearance and motion models. [sent-97, score-0.093]
</p><p>38 Learning the shape model requires consistently annotating a set of u landmarks [x1, y1, . [sent-98, score-0.128]
</p><p>39 Finally, PCA is applied on these shapes to obtain a 594  shape model defined by the mean shape and n shape eigenvectors {s0, S ∈ R{2u,n} }. [sent-106, score-0.243]
</p><p>40 (2)  Finally, in this work, to model similarity transforms the shape matrix S is appended with 4 similarity eigenvectors [16], all eigenvectors are re-orthonormalized, and then (2) is applied. [sent-110, score-0.146]
</p><p>41 Learning the appearance model requires removing shape variation from the texture. [sent-111, score-0.161]
</p><p>42 Finally, PCA is applied on the shape-free textures, to obtain the appearance model defined by the mean appearance and m appearance eigenvectors {A0, A ∈ R{N,m} }. [sent-113, score-0.318]
</p><p>43 The model captures appearance rvasr {iaAtio,nA Afor ∈ example d}u. [sent-114, score-0.093]
</p><p>44 (3)  Iˆ  We used piecewise affine warps W(x; p) as the motion  model in this work. [sent-117, score-0.145]
</p><p>45 Briefly, to define a piecewise affine warp, one first needs to triangulate the set of vertices of the given shapes. [sent-118, score-0.108]
</p><p>46 The collection of all affine warps defines a piecewise affine warp which is parameterized with respect to p. [sent-120, score-0.264]
</p><p>47 Finally, a model instance is synthesized to represent a test object by warping from the mean shape s0 to using the piecewise affine warp define by s0 and Please see [16, 5] for a detailed coverage of AAMs. [sent-121, score-0.24]
</p><p>48 Fitting AAMs Our approach to fitting AAMs is based on non-linear least-squares [16]. [sent-124, score-0.275]
</p><p>49 (4)  Because (4) is a non-linear function of p, the standard approach to proceed is to linearize with respect to the shape parameters p and then optimize iteratively in a GaussNewton fashion. [sent-128, score-0.106]
</p><p>50 In theforward case, the test image I linearized around the current estimate p, is a solution for a Δp is sought using least-squares, and p is  ×  +  updated in an additive fashion p ← p Δp. [sent-130, score-0.161]
</p><p>51 In the inverse case, the model {A0, A} is linearized around p = 0, a solutciaosne f,o thr a mΔopd eisl sought using least-squares, adndp p =i s0 updated in a compositional fashion p ← p ◦ Δp−1, where ◦ denotes tihne a composition aolf f atwshoio warps. [sent-131, score-0.284]
</p><p>52 pN◦otΔe pthat, applying tehneo iten-s verse compositional approach for piecewise affine warps is by no means straightforward. [sent-132, score-0.217]
</p><p>53 Please see [16] for a principled way of applying the inverse composition to AAMs. [sent-133, score-0.113]
</p><p>54 Following the seminal work of [16], inverse algorithms have gained increased popularity. [sent-134, score-0.138]
</p><p>55 The two most popular inverse algorithms are SIC and POIC. [sent-135, score-0.138]
</p><p>56 ∂W∂(xpk;p)  x and y gradients of Ai for the k−th pixel and ∈ R2×n is the Jacobian of the piecewise affine warp. [sent-139, score-0.108]
</p><p>57 SIC is slow because the cost for calculating Hsic is O((n + m)2N) [1]. [sent-146, score-0.194]
</p><p>58 POIC reduces this cost dramatically by decoupling shape and appearance by solving (6) in the subspace orthogonal to A. [sent-147, score-0.261]
</p><p>59 Fast algorithms for fitting AAMs Solving the exact problem in a simultaneous fashion as described above is not the only way for fitting AAMs. [sent-153, score-0.694]
</p><p>60 The solution of the inverse problem was originally proposed in [18]. [sent-155, score-0.113]
</p><p>61 Hence, our derivations shed further light on the different optimization problems that POIC and SIC solve. [sent-157, score-0.089]
</p><p>62 Fast-SIC first linearizes (the appearance model), and then projects out. [sent-167, score-0.131]
</p><p>63 This has the effect that the appearance terms Ac and AΔc immediately vanish. [sent-169, score-0.093]
</p><p>64 To readily see this notice that to calculate JI the cost is nN, and hence the cost for calculating PJI is nmN. [sent-199, score-0.342]
</p><p>65 Alternatively, one could avoid calculating PJI directly because JfTsic(I − A0) = JITP(I − A0), and P(I − A0) has a cost of mN(I. [sent-200, score-0.17]
</p><p>66 −H Aowever, theP c(oIs t− f oAr calculating ATJI= AT[Ix  Iy]∂∂Wp  (18)  is nmN (Ix and Iy are the gradients of I evaluated at ∂∂Wp) and calculating ATJI is necessary if we wish to efficiently calculate Hffw from Hffw = JITJI − (ATJI)T(ATJI). [sent-201, score-0.186]
</p><p>67 (19)  596  Note that the cost for calculating Hffw as above is n2N and comes from the first term (this is because ATJI ∈ Rm×n). [sent-202, score-0.17]
</p><p>68 An additional cost for the forward additive form∈u Rlation is that is evaluated at p and not at p = 0, but the cost for doing this can be negligible. [sent-203, score-0.267]
</p><p>69 An interesting observation following the above analysis is that, for both forward and inverse algorithms, the dominant computational cost comes from projecting out the ap-  ∂∂Wp  pearance subspace when calculating the Hessian. [sent-204, score-0.466]
</p><p>70 Fitting AAMs in-the-wild Simultaneous AAM fitting algorithms are known to perform well but their performance has not been previously assessed on recently collected in-the-wild data sets. [sent-209, score-0.3]
</p><p>71 In particular, we show that AAMs perform almost comparably to some state-of-the-art face alignment algorithms, even without using any priors (the fitting algorithms described above are used as is) and using raw pixel intensities as features. [sent-211, score-0.531]
</p><p>72 n, which is the case for generic face alignmciaelnlyt. [sent-213, score-0.093]
</p><p>73 This cost can be easily handled by current systems possibly allowing a close to real-time implementation. [sent-216, score-0.102]
</p><p>74 Another reason for ruling out AAMs from unconstrained face alignment experiments is the fact that AAMs are not considered robust. [sent-217, score-0.128]
</p><p>75 The problem with feature extraction is that it might slow down the speed of the fitting algorithm significantly especially when the dimensionality of the featured-based appearance model is large. [sent-222, score-0.392]
</p><p>76 The problem with robust norms is that scale parameters must be estimated (or percentage of outlier pixels must be predefined) and this task is not trivial. [sent-223, score-0.089]
</p><p>77 We propose a third orthogonal direction for fitting AAMs in unconstrained conditions which is via training AAMs inthe-wild. [sent-224, score-0.307]
</p><p>78 Landmarks were detected by  fitting the AAM using the Fast-SIC algorithm. [sent-229, score-0.275]
</p><p>79 (b) Reconstruction of the image from the appearance subspace. [sent-230, score-0.093]
</p><p>80 The appearance subspace is powerful because the AAM was built in the wild. [sent-231, score-0.116]
</p><p>81 This image was not seen during training, but similar images of unconstrained nature were used to train the shape and appearance model of an AAM. [sent-236, score-0.193]
</p><p>82 2 (b) shows the reconstruction of the image from the appearance subspace. [sent-238, score-0.093]
</p><p>83 As we may see the appearance model is powerful enough to reconstruct the texture almost perfectly. [sent-239, score-0.093]
</p><p>84 Fitting with a robust algorithm (Fast-SIC in this case) gives the fitting result of Fig. [sent-240, score-0.275]
</p><p>85 Results The main target of our experiments was not to prove that AAM fitting is state-of-the-art in face alignment but to show that robust fitting plus training in-the-wild improves AAM fitting performance dramatically. [sent-244, score-0.945]
</p><p>86 For this reason, we did not attempt to use sophisticated shape priors for regularization, nor we employed robust features/appearance models or robust norms for improving performance. [sent-245, score-0.201]
</p><p>87 To facilitate fitting, we used a multi-resolution fitting approach with m = 50, n = 3 at the lowest level and m = 200, n = 10 at the highest. [sent-249, score-0.275]
</p><p>88 (a) mean point-to-point error (Euclidean) normalized by the face size vs percentage of test images. [sent-255, score-0.117]
</p><p>89 For our experiments, we used the training set of LFPW to train the shape and appearance model of the AAM. [sent-259, score-0.161]
</p><p>90 In all cases, fitting was initialized by the face detector recently proposed in [24]. [sent-264, score-0.331]
</p><p>91 The first error measure that we used is the pointto-point error normalized by the face size as proposed in [24]. [sent-265, score-0.114]
</p><p>92 Similarly to [24], for this error measure, we produced the cumulative curve corresponding to the percentage of test images for which the error was less than a specific value. [sent-266, score-0.09]
</p><p>93 uk / re s ource s c for more details on our experimental setting (we provide source Matlab code for training, fitting and reproducing the results presented in this paper). [sent-272, score-0.301]
</p><p>94 Although both databases are inthe-wild, the faces of Helen seem to be much more natural, with more shape and appearance variation, and hence are more challenging to fit. [sent-285, score-0.232]
</p><p>95 Conclusions We described a very simple framework based on (1) for deriving the optimization problems and solutions for fast AAM fitting in both inverse (Fast-SIC) and forward (Fast-Forward) coordinate frames. [sent-291, score-0.537]
</p><p>96 Based on the proposed framework, exact AAM fitting is no longer computationally prohibitive. [sent-292, score-0.317]
</p><p>97 Then, we proposed a new direction for employing AAMs in unconstrained conditions by means of 598  599  training AAMs in-the-wild, and fitting using the proposed fast and exact algorithms. [sent-293, score-0.349]
</p><p>98 Our results show that although we did not use sophisticated shape priors, robust features  or robust norms for improving performance, AAMs perform almost comparably with current state-of-the-art methods. [sent-294, score-0.248]
</p><p>99 Efficient image inner products applied to active appearance models. [sent-410, score-0.144]
</p><p>100 Adaptive and constrained algorithms for inverse compositional active appearance model fitting. [sent-415, score-0.354]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('aams', 0.612), ('aam', 0.354), ('fitting', 0.275), ('poic', 0.231), ('sic', 0.228), ('atji', 0.115), ('inverse', 0.113), ('lfpw', 0.102), ('jacobian', 0.094), ('appearance', 0.093), ('calculating', 0.093), ('hffw', 0.092), ('jfsic', 0.092), ('wp', 0.077), ('cost', 0.077), ('forward', 0.076), ('ji', 0.072), ('compositional', 0.072), ('matthews', 0.072), ('shape', 0.068), ('warp', 0.064), ('pji', 0.061), ('fittings', 0.061), ('landmarks', 0.06), ('norms', 0.057), ('helen', 0.057), ('face', 0.056), ('affine', 0.055), ('comparably', 0.054), ('hager', 0.054), ('piecewise', 0.053), ('please', 0.053), ('ac', 0.052), ('active', 0.051), ('intensities', 0.049), ('dominant', 0.049), ('landmark', 0.048), ('hence', 0.048), ('imperial', 0.047), ('readily', 0.047), ('hfsic', 0.046), ('hsic', 0.046), ('jffw', 0.046), ('xpk', 0.046), ('ipn', 0.046), ('notable', 0.045), ('sophisticated', 0.044), ('fashion', 0.042), ('exact', 0.042), ('maja', 0.041), ('nmn', 0.041), ('alignment', 0.04), ('baker', 0.04), ('eigenvectors', 0.039), ('linearize', 0.038), ('linearizes', 0.038), ('tzimiropoulos', 0.038), ('update', 0.037), ('generic', 0.037), ('additive', 0.037), ('warps', 0.037), ('gross', 0.036), ('shed', 0.036), ('hessian', 0.035), ('argm', 0.035), ('simultaneous', 0.035), ('projecting', 0.035), ('pantic', 0.034), ('oar', 0.034), ('unconstrained', 0.032), ('priors', 0.032), ('percentage', 0.032), ('derivations', 0.03), ('linearized', 0.03), ('linearization', 0.03), ('entails', 0.029), ('error', 0.029), ('papers', 0.028), ('coordinate', 0.028), ('plugging', 0.027), ('saragih', 0.027), ('mpi', 0.027), ('sought', 0.027), ('iy', 0.026), ('reproducing', 0.026), ('prohibitive', 0.026), ('algorithms', 0.025), ('rn', 0.025), ('hf', 0.025), ('current', 0.025), ('plus', 0.024), ('slow', 0.024), ('london', 0.024), ('lk', 0.024), ('investigating', 0.024), ('belhumeur', 0.024), ('subspace', 0.023), ('optimization', 0.023), ('faces', 0.023), ('deriving', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="302-tfidf-1" href="./iccv-2013-Optimization_Problems_for_Fast_AAM_Fitting_in-the-Wild.html">302 iccv-2013-Optimization Problems for Fast AAM Fitting in-the-Wild</a></p>
<p>Author: Georgios Tzimiropoulos, Maja Pantic</p><p>Abstract: We describe a very simple framework for deriving the most-well known optimization problems in Active Appearance Models (AAMs), and most importantly for providing efficient solutions. Our formulation results in two optimization problems for fast and exact AAM fitting, and one new algorithm which has the important advantage of being applicable to 3D. We show that the dominant cost for both forward and inverse algorithms is a few times mN which is the cost of projecting an image onto the appearance subspace. This makes both algorithms not only computationally realizable but also very attractive speed-wise for most current systems. Because exact AAM fitting is no longer computationally prohibitive, we trained AAMs in-the-wild with the goal of investigating whether AAMs benefit from such a training process. Our results show that although we did not use sophisticated shape priors, robust features or robust norms for improving performance, AAMs perform notably well and in some cases comparably with current state-ofthe-art methods. We provide Matlab source code for training, fitting and reproducing the results presented in this paper at ht tp ://ibug. . doc . i . a c . uk/resources. c</p><p>2 0.58800268 <a title="302-tfidf-2" href="./iccv-2013-Rank_Minimization_across_Appearance_and_Shape_for_AAM_Ensemble_Fitting.html">339 iccv-2013-Rank Minimization across Appearance and Shape for AAM Ensemble Fitting</a></p>
<p>Author: Xin Cheng, Sridha Sridharan, Jason Saragih, Simon Lucey</p><p>Abstract: Active Appearance Models (AAMs) employ a paradigm of inverting a synthesis model of how an object can vary in terms of shape and appearance. As a result, the ability of AAMs to register an unseen object image is intrinsically linked to two factors. First, how well the synthesis model can reconstruct the object image. Second, the degrees of freedom in the model. Fewer degrees of freedom yield a higher likelihood of good fitting performance. In this paper we look at how these seemingly contrasting factors can complement one another for the problem of AAM fitting of an ensemble of images stemming from a constrained set (e.g. an ensemble of face images of the same person).</p><p>3 0.16729282 <a title="302-tfidf-3" href="./iccv-2013-Robust_Non-parametric_Data_Fitting_for_Correspondence_Modeling.html">358 iccv-2013-Robust Non-parametric Data Fitting for Correspondence Modeling</a></p>
<p>Author: Wen-Yan Lin, Ming-Ming Cheng, Shuai Zheng, Jiangbo Lu, Nigel Crook</p><p>Abstract: We propose a generic method for obtaining nonparametric image warps from noisy point correspondences. Our formulation integrates a huber function into a motion coherence framework. This makes our fitting function especially robust to piecewise correspondence noise (where an image section is consistently mismatched). By utilizing over parameterized curves, we can generate realistic nonparametric image warps from very noisy correspondence. We also demonstrate how our algorithm can be used to help stitch images taken from a panning camera by warping the images onto a virtual push-broom camera imaging plane.</p><p>4 0.13852699 <a title="302-tfidf-4" href="./iccv-2013-Pose-Free_Facial_Landmark_Fitting_via_Optimized_Part_Mixtures_and_Cascaded_Deformable_Shape_Model.html">321 iccv-2013-Pose-Free Facial Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Shape Model</a></p>
<p>Author: Xiang Yu, Junzhou Huang, Shaoting Zhang, Wang Yan, Dimitris N. Metaxas</p><p>Abstract: This paper addresses the problem of facial landmark localization and tracking from a single camera. We present a two-stage cascaded deformable shape model to effectively and efficiently localize facial landmarks with large head pose variations. For face detection, we propose a group sparse learning method to automatically select the most salient facial landmarks. By introducing 3D face shape model, we use procrustes analysis to achieve pose-free facial landmark initialization. For deformation, the first step uses mean-shift local search with constrained local model to rapidly approach the global optimum. The second step uses component-wise active contours to discriminatively refine the subtle shape variation. Our framework can simultaneously handle face detection, pose-free landmark localization and tracking in real time. Extensive experiments are conducted on both laboratory environmental face databases and face-in-the-wild databases. All results demonstrate that our approach has certain advantages over state-of-theart methods in handling pose variations1.</p><p>5 0.12349711 <a title="302-tfidf-5" href="./iccv-2013-Cascaded_Shape_Space_Pruning_for_Robust_Facial_Landmark_Detection.html">70 iccv-2013-Cascaded Shape Space Pruning for Robust Facial Landmark Detection</a></p>
<p>Author: Xiaowei Zhao, Shiguang Shan, Xiujuan Chai, Xilin Chen</p><p>Abstract: In this paper, we propose a novel cascaded face shape space pruning algorithm for robust facial landmark detection. Through progressively excluding the incorrect candidate shapes, our algorithm can accurately and efficiently achieve the globally optimal shape configuration. Specifically, individual landmark detectors are firstly applied to eliminate wrong candidates for each landmark. Then, the candidate shape space is further pruned by jointly removing incorrect shape configurations. To achieve this purpose, a discriminative structure classifier is designed to assess the candidate shape configurations. Based on the learned discriminative structure classifier, an efficient shape space pruning strategy is proposed to quickly reject most incorrect candidate shapes while preserve the true shape. The proposed algorithm is carefully evaluated on a large set of real world face images. In addition, comparison results on the publicly available BioID and LFW face databases demonstrate that our algorithm outperforms some state-of-the-art algorithms.</p><p>6 0.09571375 <a title="302-tfidf-6" href="./iccv-2013-Exemplar-Based_Graph_Matching_for_Robust_Facial_Landmark_Localization.html">149 iccv-2013-Exemplar-Based Graph Matching for Robust Facial Landmark Localization</a></p>
<p>7 0.077987924 <a title="302-tfidf-7" href="./iccv-2013-Robust_Face_Landmark_Estimation_under_Occlusion.html">355 iccv-2013-Robust Face Landmark Estimation under Occlusion</a></p>
<p>8 0.072178394 <a title="302-tfidf-8" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>9 0.071967095 <a title="302-tfidf-9" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>10 0.066900142 <a title="302-tfidf-10" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>11 0.064908236 <a title="302-tfidf-11" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>12 0.063605212 <a title="302-tfidf-12" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>13 0.060904033 <a title="302-tfidf-13" href="./iccv-2013-Abnormal_Event_Detection_at_150_FPS_in_MATLAB.html">34 iccv-2013-Abnormal Event Detection at 150 FPS in MATLAB</a></p>
<p>14 0.05982291 <a title="302-tfidf-14" href="./iccv-2013-Coupling_Alignments_with_Recognition_for_Still-to-Video_Face_Recognition.html">97 iccv-2013-Coupling Alignments with Recognition for Still-to-Video Face Recognition</a></p>
<p>15 0.05959814 <a title="302-tfidf-15" href="./iccv-2013-Modifying_the_Memorability_of_Face_Photographs.html">272 iccv-2013-Modifying the Memorability of Face Photographs</a></p>
<p>16 0.059484847 <a title="302-tfidf-16" href="./iccv-2013-Multiple_Non-rigid_Surface_Detection_and_Registration.html">283 iccv-2013-Multiple Non-rigid Surface Detection and Registration</a></p>
<p>17 0.056869127 <a title="302-tfidf-17" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>18 0.052153155 <a title="302-tfidf-18" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>19 0.051534809 <a title="302-tfidf-19" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>20 0.049509462 <a title="302-tfidf-20" href="./iccv-2013-Elastic_Net_Constraints_for_Shape_Matching.html">140 iccv-2013-Elastic Net Constraints for Shape Matching</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.14), (1, -0.04), (2, -0.069), (3, -0.044), (4, -0.031), (5, -0.06), (6, 0.135), (7, 0.071), (8, 0.017), (9, -0.003), (10, -0.036), (11, 0.059), (12, 0.031), (13, 0.004), (14, -0.025), (15, 0.009), (16, 0.047), (17, 0.031), (18, -0.032), (19, -0.089), (20, 0.064), (21, 0.059), (22, -0.053), (23, 0.088), (24, 0.057), (25, -0.076), (26, 0.109), (27, -0.032), (28, 0.013), (29, -0.125), (30, -0.017), (31, -0.107), (32, -0.012), (33, 0.049), (34, 0.184), (35, 0.112), (36, -0.102), (37, -0.076), (38, 0.088), (39, -0.087), (40, -0.102), (41, -0.084), (42, -0.304), (43, 0.215), (44, -0.064), (45, -0.016), (46, 0.065), (47, 0.265), (48, -0.19), (49, -0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94021159 <a title="302-lsi-1" href="./iccv-2013-Optimization_Problems_for_Fast_AAM_Fitting_in-the-Wild.html">302 iccv-2013-Optimization Problems for Fast AAM Fitting in-the-Wild</a></p>
<p>Author: Georgios Tzimiropoulos, Maja Pantic</p><p>Abstract: We describe a very simple framework for deriving the most-well known optimization problems in Active Appearance Models (AAMs), and most importantly for providing efficient solutions. Our formulation results in two optimization problems for fast and exact AAM fitting, and one new algorithm which has the important advantage of being applicable to 3D. We show that the dominant cost for both forward and inverse algorithms is a few times mN which is the cost of projecting an image onto the appearance subspace. This makes both algorithms not only computationally realizable but also very attractive speed-wise for most current systems. Because exact AAM fitting is no longer computationally prohibitive, we trained AAMs in-the-wild with the goal of investigating whether AAMs benefit from such a training process. Our results show that although we did not use sophisticated shape priors, robust features or robust norms for improving performance, AAMs perform notably well and in some cases comparably with current state-ofthe-art methods. We provide Matlab source code for training, fitting and reproducing the results presented in this paper at ht tp ://ibug. . doc . i . a c . uk/resources. c</p><p>2 0.88246959 <a title="302-lsi-2" href="./iccv-2013-Rank_Minimization_across_Appearance_and_Shape_for_AAM_Ensemble_Fitting.html">339 iccv-2013-Rank Minimization across Appearance and Shape for AAM Ensemble Fitting</a></p>
<p>Author: Xin Cheng, Sridha Sridharan, Jason Saragih, Simon Lucey</p><p>Abstract: Active Appearance Models (AAMs) employ a paradigm of inverting a synthesis model of how an object can vary in terms of shape and appearance. As a result, the ability of AAMs to register an unseen object image is intrinsically linked to two factors. First, how well the synthesis model can reconstruct the object image. Second, the degrees of freedom in the model. Fewer degrees of freedom yield a higher likelihood of good fitting performance. In this paper we look at how these seemingly contrasting factors can complement one another for the problem of AAM fitting of an ensemble of images stemming from a constrained set (e.g. an ensemble of face images of the same person).</p><p>3 0.63331062 <a title="302-lsi-3" href="./iccv-2013-Robust_Non-parametric_Data_Fitting_for_Correspondence_Modeling.html">358 iccv-2013-Robust Non-parametric Data Fitting for Correspondence Modeling</a></p>
<p>Author: Wen-Yan Lin, Ming-Ming Cheng, Shuai Zheng, Jiangbo Lu, Nigel Crook</p><p>Abstract: We propose a generic method for obtaining nonparametric image warps from noisy point correspondences. Our formulation integrates a huber function into a motion coherence framework. This makes our fitting function especially robust to piecewise correspondence noise (where an image section is consistently mismatched). By utilizing over parameterized curves, we can generate realistic nonparametric image warps from very noisy correspondence. We also demonstrate how our algorithm can be used to help stitch images taken from a panning camera by warping the images onto a virtual push-broom camera imaging plane.</p><p>4 0.49466899 <a title="302-lsi-4" href="./iccv-2013-Robust_Face_Landmark_Estimation_under_Occlusion.html">355 iccv-2013-Robust Face Landmark Estimation under Occlusion</a></p>
<p>Author: Xavier P. Burgos-Artizzu, Pietro Perona, Piotr Dollár</p><p>Abstract: Human faces captured in real-world conditions present large variations in shape and occlusions due to differences in pose, expression, use of accessories such as sunglasses and hats and interactions with objects (e.g. food). Current face landmark estimation approaches struggle under such conditions since theyfail toprovide aprincipled way ofhandling outliers. We propose a novel method, called Robust Cascaded Pose Regression (RCPR) which reduces exposure to outliers by detecting occlusions explicitly and using robust shape-indexed features. We show that RCPR improves on previous landmark estimation methods on three popular face datasets (LFPW, LFW and HELEN). We further explore RCPR ’s performance by introducing a novel face dataset focused on occlusion, composed of 1,007 faces presenting a wide range of occlusion patterns. RCPR reduces failure cases by half on all four datasets, at the same time as it detects face occlusions with a 80/40% precision/recall.</p><p>5 0.49247083 <a title="302-lsi-5" href="./iccv-2013-Pose-Free_Facial_Landmark_Fitting_via_Optimized_Part_Mixtures_and_Cascaded_Deformable_Shape_Model.html">321 iccv-2013-Pose-Free Facial Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Shape Model</a></p>
<p>Author: Xiang Yu, Junzhou Huang, Shaoting Zhang, Wang Yan, Dimitris N. Metaxas</p><p>Abstract: This paper addresses the problem of facial landmark localization and tracking from a single camera. We present a two-stage cascaded deformable shape model to effectively and efficiently localize facial landmarks with large head pose variations. For face detection, we propose a group sparse learning method to automatically select the most salient facial landmarks. By introducing 3D face shape model, we use procrustes analysis to achieve pose-free facial landmark initialization. For deformation, the first step uses mean-shift local search with constrained local model to rapidly approach the global optimum. The second step uses component-wise active contours to discriminatively refine the subtle shape variation. Our framework can simultaneously handle face detection, pose-free landmark localization and tracking in real time. Extensive experiments are conducted on both laboratory environmental face databases and face-in-the-wild databases. All results demonstrate that our approach has certain advantages over state-of-theart methods in handling pose variations1.</p><p>6 0.43089935 <a title="302-lsi-6" href="./iccv-2013-Cascaded_Shape_Space_Pruning_for_Robust_Facial_Landmark_Detection.html">70 iccv-2013-Cascaded Shape Space Pruning for Robust Facial Landmark Detection</a></p>
<p>7 0.41019666 <a title="302-lsi-7" href="./iccv-2013-Exemplar-Based_Graph_Matching_for_Robust_Facial_Landmark_Localization.html">149 iccv-2013-Exemplar-Based Graph Matching for Robust Facial Landmark Localization</a></p>
<p>8 0.37453094 <a title="302-lsi-8" href="./iccv-2013-SYM-FISH%3A_A_Symmetry-Aware_Flip_Invariant_Sketch_Histogram_Shape_Descriptor.html">368 iccv-2013-SYM-FISH: A Symmetry-Aware Flip Invariant Sketch Histogram Shape Descriptor</a></p>
<p>9 0.37279281 <a title="302-lsi-9" href="./iccv-2013-Modeling_Self-Occlusions_in_Dynamic_Shape_and_Appearance_Tracking.html">270 iccv-2013-Modeling Self-Occlusions in Dynamic Shape and Appearance Tracking</a></p>
<p>10 0.35216495 <a title="302-lsi-10" href="./iccv-2013-A_Method_of_Perceptual-Based_Shape_Decomposition.html">21 iccv-2013-A Method of Perceptual-Based Shape Decomposition</a></p>
<p>11 0.32949382 <a title="302-lsi-11" href="./iccv-2013-Modifying_the_Memorability_of_Face_Photographs.html">272 iccv-2013-Modifying the Memorability of Face Photographs</a></p>
<p>12 0.32732654 <a title="302-lsi-12" href="./iccv-2013-Detecting_Curved_Symmetric_Parts_Using_a_Deformable_Disc_Model.html">110 iccv-2013-Detecting Curved Symmetric Parts Using a Deformable Disc Model</a></p>
<p>13 0.32164904 <a title="302-lsi-13" href="./iccv-2013-Abnormal_Event_Detection_at_150_FPS_in_MATLAB.html">34 iccv-2013-Abnormal Event Detection at 150 FPS in MATLAB</a></p>
<p>14 0.31797066 <a title="302-lsi-14" href="./iccv-2013-Go-ICP%3A_Solving_3D_Registration_Efficiently_and_Globally_Optimally.html">185 iccv-2013-Go-ICP: Solving 3D Registration Efficiently and Globally Optimally</a></p>
<p>15 0.29352781 <a title="302-lsi-15" href="./iccv-2013-Automatic_Kronecker_Product_Model_Based_Detection_of_Repeated_Patterns_in_2D_Urban_Images.html">55 iccv-2013-Automatic Kronecker Product Model Based Detection of Repeated Patterns in 2D Urban Images</a></p>
<p>16 0.27421314 <a title="302-lsi-16" href="./iccv-2013-Shape_Anchors_for_Data-Driven_Multi-view_Reconstruction.html">387 iccv-2013-Shape Anchors for Data-Driven Multi-view Reconstruction</a></p>
<p>17 0.27186918 <a title="302-lsi-17" href="./iccv-2013-Coupling_Alignments_with_Recognition_for_Still-to-Video_Face_Recognition.html">97 iccv-2013-Coupling Alignments with Recognition for Still-to-Video Face Recognition</a></p>
<p>18 0.27153793 <a title="302-lsi-18" href="./iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding.html">14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</a></p>
<p>19 0.26841837 <a title="302-lsi-19" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>20 0.26819345 <a title="302-lsi-20" href="./iccv-2013-Shape_Index_Descriptors_Applied_to_Texture-Based_Galaxy_Analysis.html">388 iccv-2013-Shape Index Descriptors Applied to Texture-Based Galaxy Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.028), (7, 0.014), (26, 0.044), (31, 0.027), (42, 0.087), (64, 0.022), (73, 0.033), (89, 0.635), (98, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99926329 <a title="302-lda-1" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>Author: Qian-Yi Zhou, Stephen Miller, Vladlen Koltun</p><p>Abstract: We present an approach to reconstruction of detailed scene geometry from range video. Range data produced by commodity handheld cameras suffers from high-frequency errors and low-frequency distortion. Our approach deals with both sources of error by reconstructing locally smooth scene fragments and letting these fragments deform in order to align to each other. We develop a volumetric registration formulation that leverages the smoothness of the deformation to make optimization practical for large scenes. Experimental results demonstrate that our approach substantially increases the fidelity of complex scene geometry reconstructed with commodity handheld cameras.</p><p>2 0.99729592 <a title="302-lda-2" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>Author: Kevin Tang, Bangpeng Yao, Li Fei-Fei, Daphne Koller</p><p>Abstract: In this paper, we tackle the problem of combining features extracted from video for complex event recognition. Feature combination is an especially relevant task in video data, as there are many features we can extract, ranging from image features computed from individual frames to video features that take temporal information into account. To combine features effectively, we propose a method that is able to be selective of different subsets of features, as some features or feature combinations may be uninformative for certain classes. We introduce a hierarchical method for combining features based on the AND/OR graph structure, where nodes in the graph represent combinations of different sets of features. Our method automatically learns the structure of the AND/OR graph using score-based structure learning, and we introduce an inference procedure that is able to efficiently compute structure scores. We present promising results and analysis on the difficult and large-scale 2011 TRECVID Multimedia Event Detection dataset [17].</p><p>3 0.99666619 <a title="302-lda-3" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>Author: Dan Xie, Sinisa Todorovic, Song-Chun Zhu</p><p>Abstract: This paper presents an approach to localizing functional objects in surveillance videos without domain knowledge about semantic object classes that may appear in the scene. Functional objects do not have discriminative appearance and shape, but they affect behavior of people in the scene. For example, they “attract” people to approach them for satisfying certain needs (e.g., vending machines could quench thirst), or “repel” people to avoid them (e.g., grass lawns). Therefore, functional objects can be viewed as “dark matter”, emanating “dark energy ” that affects people ’s trajectories in the video. To detect “dark matter” and infer their “dark energy ” field, we extend the Lagrangian mechanics. People are treated as particle-agents with latent intents to approach “dark matter” and thus satisfy their needs, where their motions are subject to a composite “dark energy ” field of all functional objects in the scene. We make the assumption that people take globally optimal paths toward the intended “dark matter” while avoiding latent obstacles. A Bayesian framework is used to probabilistically model: people ’s trajectories and intents, constraint map of the scene, and locations of functional objects. A data-driven Markov Chain Monte Carlo (MCMC) process is used for inference. Our evaluation on videos of public squares and courtyards demonstrates our effectiveness in localizing functional objects and predicting people ’s trajectories in unobserved parts of the video footage.</p><p>4 0.99515486 <a title="302-lda-4" href="./iccv-2013-Deblurring_by_Example_Using_Dense_Correspondence.html">103 iccv-2013-Deblurring by Example Using Dense Correspondence</a></p>
<p>Author: Yoav Hacohen, Eli Shechtman, Dani Lischinski</p><p>Abstract: This paper presents a new method for deblurring photos using a sharp reference example that contains some shared content with the blurry photo. Most previous deblurring methods that exploit information from other photos require an accurately registered photo of the same static scene. In contrast, our method aims to exploit reference images where the shared content may have undergone substantial photometric and non-rigid geometric transformations, as these are the kind of reference images most likely to be found in personal photo albums. Our approach builds upon a recent method for examplebased deblurring using non-rigid dense correspondence (NRDC) [11] and extends it in two ways. First, we suggest exploiting information from the reference image not only for blur kernel estimation, but also as a powerful local prior for the non-blind deconvolution step. Second, we introduce a simple yet robust technique for spatially varying blur estimation, rather than assuming spatially uniform blur. Unlike the aboveprevious method, which hasproven successful only with simple deblurring scenarios, we demonstrate that our method succeeds on a variety of real-world examples. We provide quantitative and qualitative evaluation of our method and show that it outperforms the state-of-the-art.</p><p>same-paper 5 0.99436241 <a title="302-lda-5" href="./iccv-2013-Optimization_Problems_for_Fast_AAM_Fitting_in-the-Wild.html">302 iccv-2013-Optimization Problems for Fast AAM Fitting in-the-Wild</a></p>
<p>Author: Georgios Tzimiropoulos, Maja Pantic</p><p>Abstract: We describe a very simple framework for deriving the most-well known optimization problems in Active Appearance Models (AAMs), and most importantly for providing efficient solutions. Our formulation results in two optimization problems for fast and exact AAM fitting, and one new algorithm which has the important advantage of being applicable to 3D. We show that the dominant cost for both forward and inverse algorithms is a few times mN which is the cost of projecting an image onto the appearance subspace. This makes both algorithms not only computationally realizable but also very attractive speed-wise for most current systems. Because exact AAM fitting is no longer computationally prohibitive, we trained AAMs in-the-wild with the goal of investigating whether AAMs benefit from such a training process. Our results show that although we did not use sophisticated shape priors, robust features or robust norms for improving performance, AAMs perform notably well and in some cases comparably with current state-ofthe-art methods. We provide Matlab source code for training, fitting and reproducing the results presented in this paper at ht tp ://ibug. . doc . i . a c . uk/resources. c</p><p>6 0.99385154 <a title="302-lda-6" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>7 0.99288177 <a title="302-lda-7" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>8 0.99165195 <a title="302-lda-8" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>9 0.98424792 <a title="302-lda-9" href="./iccv-2013-3D_Scene_Understanding_by_Voxel-CRF.html">2 iccv-2013-3D Scene Understanding by Voxel-CRF</a></p>
<p>10 0.98082727 <a title="302-lda-10" href="./iccv-2013-Shufflets%3A_Shared_Mid-level_Parts_for_Fast_Object_Detection.html">390 iccv-2013-Shufflets: Shared Mid-level Parts for Fast Object Detection</a></p>
<p>11 0.97506344 <a title="302-lda-11" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>12 0.962641 <a title="302-lda-12" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>13 0.9617278 <a title="302-lda-13" href="./iccv-2013-Large-Scale_Multi-resolution_Surface_Reconstruction_from_RGB-D_Sequences.html">228 iccv-2013-Large-Scale Multi-resolution Surface Reconstruction from RGB-D Sequences</a></p>
<p>14 0.96117806 <a title="302-lda-14" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>15 0.95658261 <a title="302-lda-15" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>16 0.95453918 <a title="302-lda-16" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>17 0.95389748 <a title="302-lda-17" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>18 0.9535799 <a title="302-lda-18" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>19 0.95355678 <a title="302-lda-19" href="./iccv-2013-Active_MAP_Inference_in_CRFs_for_Efficient_Semantic_Segmentation.html">42 iccv-2013-Active MAP Inference in CRFs for Efficient Semantic Segmentation</a></p>
<p>20 0.95291042 <a title="302-lda-20" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
