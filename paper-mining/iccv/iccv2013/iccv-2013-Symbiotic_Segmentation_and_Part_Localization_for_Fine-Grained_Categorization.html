<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-411" href="#">iccv2013-411</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</h1>
<br/><p>Source: <a title="iccv-2013-411-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Chai_Symbiotic_Segmentation_and_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Yuning Chai, Victor Lempitsky, Andrew Zisserman</p><p>Abstract: We propose a new method for the task of fine-grained visual categorization. The method builds a model of the baselevel category that can be fitted to images, producing highquality foreground segmentation and mid-level part localizations. The model can be learnt from the typical datasets available for fine-grained categorization, where the only annotation provided is a loose bounding box around the instance (e.g. bird) in each image. Both segmentation and part localizations are then used to encode the image content into a highly-discriminative visual signature. The model is symbiotic in that part discovery/localization is helped by segmentation and, conversely, the segmentation is helped by the detection (e.g. part layout). Our model builds on top of the part-based object category detector of Felzenszwalb et al., and also on the powerful GrabCut segmentation algorithm of Rother et al., and adds a simple spatial saliency coupling between them. In our evaluation, the model improves the categorization accuracy over the state-of-the-art. It also improves over what can be achieved with an analogous system that runs segmentation and part-localization independently.</p><p>Reference: <a title="iccv-2013-411-reference" href="../iccv2013_reference/iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The method builds a model of the baselevel category that can be fitted to images, producing highquality foreground segmentation and mid-level part localizations. [sent-11, score-0.582]
</p><p>2 The model can be learnt from the typical datasets available for fine-grained categorization, where the only annotation provided is a loose bounding box around the instance (e. [sent-12, score-0.22]
</p><p>3 Both segmentation and part localizations are then used to encode the image content into a highly-discriminative visual signature. [sent-15, score-0.496]
</p><p>4 The model is symbiotic in that part discovery/localization is helped by segmentation and, conversely, the segmentation is helped by the detection (e. [sent-16, score-0.996]
</p><p>5 It also improves over what can be achieved with an analogous system that runs  segmentation and part-localization independently. [sent-23, score-0.219]
</p><p>6 Introduction Fine-grained visual categorization is the task of distinguishing between sub-ordinate categories, e. [sent-25, score-0.22]
</p><p>7 Several recent works have pointed out two aspects, which distinguish visual categorization at the subordinate level from that at the base level. [sent-28, score-0.304]
</p><p>8 First, in subordinate classification it often happens that two similar classes can only be distinguished by the appearance of localized and very subtle details (such as the color of the beak for bird classes or the shape of the petal edges for flower classes). [sent-29, score-0.318]
</p><p>9 Therefore, [5, 24, 32, 34, 35] focused on the localization of these discriminative image parts as a precursor to categorization. [sent-31, score-0.316]
</p><p>10 Once the discriminative parts are localized, they are encoded into separate parts of the visual signature, enabling the classifier to pick up on the fine differences in those parts. [sent-32, score-0.273]
</p><p>11 However, [10, 22, 24] demonstrated that at the sub-  ordinate category level, the background is seldom discriminative and it is beneficial to segment out the foreground and to discard the visual information in the background. [sent-35, score-0.282]
</p><p>12 [10] further demonstrated that increasing the accuracy of foreground segmentation at training time directly translates into an increase in accuracy of subordinate-level categorization at test time. [sent-36, score-0.609]
</p><p>13 In the light of all this evidence, it is natural to investigate the combination of part localization and foreground segmentation for fine-grained categorization, and their interaction in combination is the topic of this work. [sent-37, score-0.633]
</p><p>14 More interestingly, we demonstrate that the accuracy of fine-grained categorization can be further boosted if part localization and foreground segmentation are performed together, so that the outcomes of both processes aid each other. [sent-39, score-0.909]
</p><p>15 As a result, better segmentation can be obtained by taking into account part localizations, and, likewise, more semantically meaningful and discriminative parts can be learned and localized if foreground masks are taken into account. [sent-40, score-0.752]
</p><p>16 We implement this feedback loop via the energy minimization of a joint functional that incorporates the con-  sistency between part localization and foreground segmentation as one of the terms. [sent-41, score-0.72]
</p><p>17 The resulting symbiotic system achieves a better categorization performance compared to the system obtained by a mere concatenation of two visual 321  indicate the provided ground truth bounding box. [sent-42, score-0.884]
</p><p>18 Middle: GrabCut automatically segments the images using the outside of the given bounding box as background and a prior foreground saliency map for the region inside the bounding box. [sent-44, score-0.584]
</p><p>19 Bottom: our approach, which trains a symbiotic set of detector templates and saliency maps and applies them jointly to images. [sent-45, score-0.692]
</p><p>20 Overall, our symbiotic system outperforms the previous state-of-the-art on all datasets considered in our experiments (both the 2010 and 2011version of Caltech-UCSD Birds, and Stanford Dogs). [sent-50, score-0.524]
</p><p>21 This symbiotic system is the main contribution of the paper. [sent-51, score-0.524]
</p><p>22 Related  Work  There is a line of work stretching back over a decade on the interplay between segmentation and detection. [sent-56, score-0.256]
</p><p>23 In early works, object category detectors simply proposed foreground masks [4, 18]. [sent-57, score-0.297]
</p><p>24 Later methods used these masks to initialize graph-cuts based segmentations [7] that could take advantage of image specific color distributions, giving crisper and more accurate foreground segmentations [17, 19, 26]. [sent-58, score-0.442]
</p><p>25 In the poselet line of research [6] the detectors are for parts, rather than for entire categories, but again the poseletdetectors can predict foreground masks for object category detection and segmentation [9, 20]. [sent-59, score-0.526]
</p><p>26 Whether the parts arise from poselets [35] or are discovered from random initializations [33], there are benefits in comparing objects in finegrained visual categorization tasks at the part level where subtle discriminative features are more evident. [sent-60, score-0.606]
</p><p>27 We demonstrate, however, that the parts discovered in the absence of supervision are less discriminative than those discovered with the help of the segmentation process as is done in our method. [sent-61, score-0.414]
</p><p>28 It also accomplishes unsupervised learning of a deformable part model in order to find discriminative parts for fine-grained categorization. [sent-67, score-0.397]
</p><p>29 An earlier method had used the image as a bounding box for learning a deformable parts model for scene classification [23]. [sent-68, score-0.306]
</p><p>30 Again, neither of these use segmentation to aid the part learning and localization. [sent-69, score-0.333]
</p><p>31 In summary, although the synergy between segmentation and detection has long been recognized [16], the interplay between part localization and segmentation has not been investigated in the context of fine-grained categorization (to the best of our knowledge). [sent-70, score-0.894]
</p><p>32 bird) which includes a deformable part model W and a set S of saliency 322  maps each associated with a part or root of the DPM. [sent-76, score-0.576]
</p><p>33 The recovered part localizations p and the foreground segmentation f are then used to encode the image content into a highly-discriminative visual signature as discussed in the next section. [sent-79, score-0.762]
</p><p>34 With the introduction of a third (consistency) energy term EC that takes a pre-trained saliency model S we penalize the cases twakheesre a tphree foreground segmentation wf ea pnden tahlepart locations p do not agree. [sent-81, score-0.59]
</p><p>35 Deformable part model W = {wt}: here, we use a multicomponent eD pefaortrm maobdlee lP Wart =M {odwel} (:D hPeMre,) [w1e4 ]u consisting of several mixtures of parts, where each part is described by a HOG template and a geometric location prior. [sent-85, score-0.285]
</p><p>36 We denote the number of mixture components N, and the number of parts in each component M. [sent-86, score-0.214]
</p><p>37 We omit extra indices for different mixture components and use w0 to describe the root HOG template for each component. [sent-87, score-0.27]
</p><p>38 wt then denotes the parameters of the t-th part (the HOG template and the geometric prior). [sent-88, score-0.254]
</p><p>39 Saliency model S = {st}: we associate with the root and eSaaclhie part wt eofl Sthe = d {efso}r:m wabele a part amteod weilt an eex rtorao map st that indicates the foreground probability. [sent-89, score-0.705]
</p><p>40 Pixels of this saliency map thus have values between −1 and 1, with 1indicating a high cuhsa hnacvee eo vfa thluee pixel being foreground atnhd 1 − in1doithceartiwnigse a. [sent-90, score-0.377]
</p><p>41 Part localizations p = {pt}: this variable denotes the loPcaatriotn lo (cthalei bounding =b ox { pco}o:rd ithniaste vsa)r ioafb laell d deentoectetsed th parts in an image. [sent-93, score-0.449]
</p><p>42 The localization of a particular part template wt is denoted pt. [sent-95, score-0.41]
</p><p>43 The part localizations are shown as colored bounding boxes in the output images of Fig. [sent-96, score-0.45]
</p><p>44 t  where mt (pt, f) is a binary map {−1, 1} clipped from the segmentation mf)a sisk af by trhye m loapca {li−ze1d, part bounding b thoxe pt. [sent-119, score-0.484]
</p><p>45 This map is resized to the size of a saliency map st, which is denoted as θt. [sent-120, score-0.248]
</p><p>46 | |mt (pt, f) | |22 is constant for the reason that mt only con|t|amins pixel |v|alues of either −1 or 1and hence the squared norm piisx simply tshe o fnu eimthbeerr − −o1f pixels specified by tshqeu asriezed θt, and does not depend on pt and f. [sent-122, score-0.252]
</p><p>47 We optimize the cost function (1) using a blockcoordinate-descent pattern, that is, alternating between updating part localizations p while fixing the foreground segmentation f and color c, and vice versa. [sent-123, score-0.718]
</p><p>48 =0  D(pt,  wt,  p0) = R(pt, wt) + Qt (pt, p0)  (6)  R(pt, wt) is the HOG-template filter response map of the t-th root or part template. [sent-129, score-0.262]
</p><p>49 Qt is a quadratic function of the 323  relative location of the part and the root that penalizes the atypical geometric configurations. [sent-130, score-0.235]
</p><p>50 Assuming that part localizations p are fixed, the minimization mfinβEGC(f, c|I) + EC(p, f|S)  (8)  can be accomplished with an appropriately modified GrabCut algorithm. [sent-136, score-0.41]
</p><p>51 Recall that GrabCut alternates the color model updates and the segmentation updates. [sent-137, score-0.198]
</p><p>52 Let us now focus on the foreground segmentation update (given part localizations p and the color model c). [sent-139, score-0.718]
</p><p>53 Thus, the HOG templates for root filters are in the mixture components via latent SVM training (we use a separate unrelated dataset as a source of negative examples; and constrain the root filters to overlap with user-provided boxes by at least 70%). [sent-184, score-0.432]
</p><p>54 At the same time, we run GrabCut on all training examples (using bounding box annotations), and estimate the root saliency map s0 corresponding to root filters by averaging the segmentation masks (as detailed below). [sent-185, score-0.804]
</p><p>55 In [14], “interesting” parts are discovered greedily (as discussed in [14]) by covering the high-energy (large gradient magnitude) parts of the root HOG-template. [sent-189, score-0.415]
</p><p>56 In our case, we modify this interestingness measure by multiplying the HOG magnitude by the root saliency maps estimated for each component. [sent-190, score-0.276]
</p><p>57 In this way, we constrain the discovery process to parts which overlap substantially with the foreground (as estimated by a GrabCut). [sent-191, score-0.379]
</p><p>58 We come back to the issue of unsupervised part discovery in the experiments section. [sent-193, score-0.245]
</p><p>59 Mean accuracy (mA) performance on the three finegrained categorization datasets. [sent-206, score-0.259]
</p><p>60 Given the part localizatLioeansr nanindg gth teh eG sraabliCenuct segmentations ivoef anll t training images, we set the saliency mask for each part to be the pixel-wise mean of all segmentation masks cutou? [sent-212, score-0.727]
</p><p>61 The symbiotic model is fitted to images using 5 alternation iterations (the convergence is observed after 3 iterations in most cases). [sent-229, score-0.535]
</p><p>62 The symbiotic model outputs one binary segmentation and a set of detected part bounding boxes for a given image. [sent-235, score-0.88]
</p><p>63 , one feature vector, xSEG, for the foreground region in the segmentation, and a feature vector for each of the parts apart from the root template. [sent-238, score-0.423]
</p><p>64 the foreground and the box of each part) is encoded by: (1) LLC-encoded [29] Lab color histogram vector, and (2) Fisher vector [25] aggregating SIFT features (the implementation [11] was adopted). [sent-244, score-0.269]
</p><p>65 20992 dims each), no matter how many parts and mixture components are used. [sent-254, score-0.214]
</p><p>66 The models learned by the symbiotic system for the birds and dogs datasets can be seen in Fig. [sent-271, score-0.793]
</p><p>67 The relative importance of the model components, as well as the net effect of the “symbiosis” between the segmentation and part localization, are evaluated in Tab. [sent-274, score-0.285]
</p><p>68 In the table, we compare the categorization accuracy of the systems resulting from applying GrabCut alone or DPM 325  IDModel fittingDescriptormABirdsm11APmABirdsm10APmADogmsAP  Table2. [sent-276, score-0.225]
</p><p>69 975segmntaio  produced by the symbiotic model allow for more discriminative signatures than those produced with GrabCut alone (#3 vs. [sent-286, score-0.742]
</p><p>70 #2), while parts learned and localized by the symbiotic model are more discriminative than those learned and localized by DPM (#5 vs. [sent-287, score-0.739]
</p><p>71 Finally, categorization with full signatures produced by the symbiotic model is better than categorization based on the concatenation of segmentation-based and part-based signatures produced by GrabCut and DPM run independently (#7 vs #6). [sent-289, score-1.221]
</p><p>72 All these improvements are due to the fact that part localization and segmentation processes assist each other within the proposed symbiotic model. [sent-290, score-0.949]
</p><p>73 part localization alone, while keeping the rest of the parameters (initialization, feature encoding, etc. [sent-291, score-0.273]
</p><p>74 Likewise, the same improvement is observed for part localization, when the segmentation process is used to aid part discovery and fitting,  as opposed to using a DPM model on its own (line 5 vs line 4). [sent-294, score-0.557]
</p><p>75 The interaction between the segmentation and the part localization processes are further shown in Fig. [sent-296, score-0.476]
</p><p>76 3, we used the same deformable part model W (learned within the symbiotic dmeofodreml) bb ulet pevaarltua mteodd ilt Wwith (l eaanrdn wdit whoituhti nth teh help obfi othtiec segmentation process. [sent-300, score-0.824]
</p><p>77 In both cases, it can be seen how symbiosis between the part localization and the segmentation improve the performance of each process. [sent-303, score-0.491]
</p><p>78 We attribute this fact to a greater pose variability for dogs that is harder to cope with for the deformable parts model. [sent-305, score-0.332]
</p><p>79 At the same time, dogs have a nice roundish shape which makes them very appropriate for GrabCut (so that the aid from the parts localization is not needed in most cases). [sent-306, score-0.442]
</p><p>80 However, as discussed below, it might hurt the generalization in the categorization step, and es-  pecially since we keep the feature dimension of xPART the same. [sent-308, score-0.221]
</p><p>81 We have further evaluated the influence of the size of the deformable parts model on the categorization accuracy, namely N (number of mixture components) and M (the number of parts per component). [sent-311, score-0.555]
</p><p>82 While large N may also increase the data fragmentation within some subordinate classes, potentially having large N may also attribute different subordinate classes to different components, thus making the categorization easier. [sent-315, score-0.357]
</p><p>83 Overall, for the bird datasets, we chose N = 1and M = 4, while N = 2 and M = 4 seems to be more reasonable for the dogs dataset (each DPM mixture component is applied twice (once with mirroring and once without) during training and test). [sent-322, score-0.359]
</p><p>84 The loss in accuracy with higher number of mixture components indicates that the complexity of a bird pose does not justify more than one mixture component in our model. [sent-346, score-0.275]
</p><p>85 Only by combining segmentation and part localization (lines 6 and 7 in the table) can we see a consistent benefit from having part localization in the system. [sent-348, score-0.714]
</p><p>86 One natural question is whether the perfor-  mance of part localization is inherently limited or is this a problem with segmentation-supervised and, particularly, unsupervised part discovery? [sent-349, score-0.444]
</p><p>87 Apart from the bounding boxes, there are 15 part locations annotated per image. [sent-351, score-0.226]
</p><p>88 Thus, we first made use of the annotated head locations and trained a head detector (which was a mixture of HOG templates). [sent-357, score-0.259]
</p><p>89 4, the resulting systems were able to surpass the performance of the symbiotic system even when only using the trained head detector. [sent-367, score-0.587]
</p><p>90 Using ground –  truth head localizations, the gap in the achieved accuracy compared to the symbiotic system (and, naturally, all other systems evaluated on this task) becomes very large. [sent-368, score-0.587]
</p><p>91 Overall, our conclusion here is that part localization has  TabldG oeCcTt4a. [sent-369, score-0.273]
</p><p>92 The top two rows show the results if the head detector is trained using human annotation rather than unsupervised training, while the bottom rows show the accuracies if the head position is given even during test time. [sent-376, score-0.265]
</p><p>93 Conclusion We have introduced and symbiotic part localization fine-grained categorization. [sent-380, score-0.746]
</p><p>94 It also opens up new research questions: how can the model be extended from loose bounding box annotation to (even weaker) image level annotation? [sent-382, score-0.22]
</p><p>95 Top: part localizations using the symbiotically trained DPM, but fitted without the guidance of segmentation. [sent-403, score-0.39]
</p><p>96 Bottom: the same DPM model fitted with the help of segmentation (i. [sent-404, score-0.23]
</p><p>97 The last three columns show some failure cases where segmentations hurts part localization. [sent-408, score-0.196]
</p><p>98 Object detection and segmentation from joint embedding of parts and pixels. [sent-538, score-0.281]
</p><p>99 Weakly supervised discriminative localization and classification: a joint learning process. [sent-546, score-0.203]
</p><p>100 Scene recognition and weakly supervised object localization with deformable part-based models. [sent-557, score-0.222]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('symbiotic', 0.473), ('dpm', 0.234), ('grabcut', 0.212), ('localizations', 0.211), ('categorization', 0.193), ('foreground', 0.192), ('segmentation', 0.168), ('pt', 0.16), ('saliency', 0.158), ('localization', 0.156), ('birds', 0.144), ('signatures', 0.136), ('dogs', 0.125), ('edpm', 0.124), ('xseg', 0.124), ('root', 0.118), ('part', 0.117), ('parts', 0.113), ('bird', 0.104), ('egc', 0.1), ('xpart', 0.1), ('ux', 0.093), ('mt', 0.092), ('wt', 0.086), ('bounding', 0.08), ('segmentations', 0.079), ('discovery', 0.074), ('ec', 0.073), ('mixture', 0.07), ('subordinate', 0.068), ('deformable', 0.066), ('finegrained', 0.066), ('head', 0.063), ('masks', 0.062), ('fitted', 0.062), ('interplay', 0.055), ('unsupervised', 0.054), ('localized', 0.053), ('template', 0.051), ('annotation', 0.051), ('system', 0.051), ('symb', 0.05), ('symbiosis', 0.05), ('nx', 0.049), ('aid', 0.048), ('st', 0.048), ('box', 0.047), ('discriminative', 0.047), ('signature', 0.046), ('ox', 0.045), ('minimization', 0.044), ('energy', 0.043), ('base', 0.043), ('category', 0.043), ('discovered', 0.043), ('midlevel', 0.042), ('boxes', 0.042), ('loose', 0.042), ('modified', 0.038), ('hog', 0.038), ('rt', 0.038), ('locus', 0.037), ('synergy', 0.037), ('concatenation', 0.036), ('stanford', 0.036), ('resized', 0.036), ('encoding', 0.035), ('processes', 0.035), ('helped', 0.035), ('beak', 0.035), ('consistency', 0.035), ('mirroring', 0.034), ('detector', 0.034), ('lempitsky', 0.034), ('qt', 0.033), ('line', 0.033), ('alone', 0.032), ('bourdev', 0.032), ('components', 0.031), ('color', 0.03), ('translates', 0.03), ('schroff', 0.03), ('locations', 0.029), ('discussed', 0.028), ('chai', 0.028), ('discriminating', 0.028), ('poselet', 0.028), ('attribute', 0.028), ('flower', 0.028), ('map', 0.027), ('templates', 0.027), ('vedaldi', 0.027), ('produced', 0.027), ('poselets', 0.027), ('distinguishing', 0.027), ('saturation', 0.027), ('training', 0.026), ('unchanged', 0.026), ('branson', 0.026), ('wah', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="411-tfidf-1" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>Author: Yuning Chai, Victor Lempitsky, Andrew Zisserman</p><p>Abstract: We propose a new method for the task of fine-grained visual categorization. The method builds a model of the baselevel category that can be fitted to images, producing highquality foreground segmentation and mid-level part localizations. The model can be learnt from the typical datasets available for fine-grained categorization, where the only annotation provided is a loose bounding box around the instance (e.g. bird) in each image. Both segmentation and part localizations are then used to encode the image content into a highly-discriminative visual signature. The model is symbiotic in that part discovery/localization is helped by segmentation and, conversely, the segmentation is helped by the detection (e.g. part layout). Our model builds on top of the part-based object category detector of Felzenszwalb et al., and also on the powerful GrabCut segmentation algorithm of Rother et al., and adds a simple spatial saliency coupling between them. In our evaluation, the model improves the categorization accuracy over the state-of-the-art. It also improves over what can be achieved with an analogous system that runs segmentation and part-localization independently.</p><p>2 0.31496489 <a title="411-tfidf-2" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>Author: Ning Zhang, Ryan Farrell, Forrest Iandola, Trevor Darrell</p><p>Abstract: Recognizing objects in fine-grained domains can be extremely challenging due to the subtle differences between subcategories. Discriminative markings are often highly localized, leading traditional object recognition approaches to struggle with the large pose variation often present in these domains. Pose-normalization seeks to align training exemplars, either piecewise by part or globally for the whole object, effectively factoring out differences in pose and in viewing angle. Prior approaches relied on computationally-expensive filter ensembles for part localization and required extensive supervision. This paper proposes two pose-normalized descriptors based on computationally-efficient deformable part models. The first leverages the semantics inherent in strongly-supervised DPM parts. The second exploits weak semantic annotations to learn cross-component correspondences, computing pose-normalized descriptors from the latent parts of a weakly-supervised DPM. These representations enable pooling across pose and viewpoint, in turn facilitating tasks such as fine-grained recognition and attribute prediction. Experiments conducted on the Caltech-UCSD Birds 200 dataset and Berkeley Human Attribute dataset demonstrate significant improvements over state-of-art algorithms.</p><p>3 0.21820959 <a title="411-tfidf-3" href="./iccv-2013-Fine-Grained_Categorization_by_Alignments.html">169 iccv-2013-Fine-Grained Categorization by Alignments</a></p>
<p>Author: E. Gavves, B. Fernando, C.G.M. Snoek, A.W.M. Smeulders, T. Tuytelaars</p><p>Abstract: The aim of this paper is fine-grained categorization without human interaction. Different from prior work, which relies on detectors for specific object parts, we propose to localize distinctive details by roughly aligning the objects using just the overall shape, since implicit to fine-grained categorization is the existence of a super-class shape shared among all classes. The alignments are then used to transfer part annotations from training images to test images (supervised alignment), or to blindly yet consistently segment the object in a number of regions (unsupervised alignment). We furthermore argue that in the distinction of finegrained sub-categories, classification-oriented encodings like Fisher vectors are better suited for describing localized information than popular matching oriented features like HOG. We evaluate the method on the CU-2011 Birds and Stanford Dogs fine-grained datasets, outperforming the state-of-the-art.</p><p>4 0.19000311 <a title="411-tfidf-4" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>Author: Wei Xia, Csaba Domokos, Jian Dong, Loong-Fah Cheong, Shuicheng Yan</p><p>Abstract: Numerous existing object segmentation frameworks commonly utilize the object bounding box as a prior. In this paper, we address semantic segmentation assuming that object bounding boxes are provided by object detectors, but no training data with annotated segments are available. Based on a set of segment hypotheses, we introduce a simple voting scheme to estimate shape guidance for each bounding box. The derived shape guidance is used in the subsequent graph-cut-based figure-ground segmentation. The final segmentation result is obtained by merging the segmentation results in the bounding boxes. We conduct an extensive analysis of the effect of object bounding box accuracy. Comprehensive experiments on both the challenging PASCAL VOC object segmentation dataset and GrabCut50 image segmentation dataset show that the proposed approach achieves competitive results compared to previous detection or bounding box prior based methods, as well as other state-of-the-art semantic segmentation methods.</p><p>5 0.18745546 <a title="411-tfidf-5" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>Author: Yangqing Jia, Mei Han</p><p>Abstract: It is known that purely low-level saliency cues such as frequency does not lead to a good salient object detection result, requiring high-level knowledge to be adopted for successful discovery of task-independent salient objects. In this paper, we propose an efficient way to combine such high-level saliency priors and low-level appearance models. We obtain the high-level saliency prior with the objectness algorithm to find potential object candidates without the need of category information, and then enforce the consistency among the salient regions using a Gaussian MRF with the weights scaled by diverse density that emphasizes the influence of potential foreground pixels. Our model obtains saliency maps that assign high scores for the whole salient object, and achieves state-of-the-art performance on benchmark datasets covering various foreground statistics.</p><p>6 0.17914358 <a title="411-tfidf-6" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<p>7 0.17487647 <a title="411-tfidf-7" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>8 0.16450244 <a title="411-tfidf-8" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>9 0.16441616 <a title="411-tfidf-9" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>10 0.15963137 <a title="411-tfidf-10" href="./iccv-2013-Saliency_Detection_via_Dense_and_Sparse_Reconstruction.html">372 iccv-2013-Saliency Detection via Dense and Sparse Reconstruction</a></p>
<p>11 0.1516905 <a title="411-tfidf-11" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>12 0.15123172 <a title="411-tfidf-12" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>13 0.15047127 <a title="411-tfidf-13" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>14 0.14576715 <a title="411-tfidf-14" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>15 0.13796881 <a title="411-tfidf-15" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>16 0.1329881 <a title="411-tfidf-16" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>17 0.13238853 <a title="411-tfidf-17" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>18 0.12846836 <a title="411-tfidf-18" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>19 0.12289812 <a title="411-tfidf-19" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>20 0.12037687 <a title="411-tfidf-20" href="./iccv-2013-Building_Part-Based_Object_Detectors_via_3D_Geometry.html">66 iccv-2013-Building Part-Based Object Detectors via 3D Geometry</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.252), (1, 0.018), (2, 0.186), (3, -0.11), (4, 0.101), (5, -0.046), (6, -0.113), (7, 0.082), (8, -0.023), (9, -0.081), (10, 0.078), (11, 0.124), (12, -0.056), (13, -0.177), (14, -0.095), (15, -0.036), (16, 0.061), (17, 0.074), (18, 0.037), (19, -0.048), (20, 0.101), (21, 0.058), (22, -0.036), (23, 0.048), (24, 0.007), (25, 0.138), (26, -0.024), (27, -0.061), (28, 0.088), (29, 0.003), (30, 0.079), (31, -0.084), (32, -0.009), (33, -0.044), (34, -0.073), (35, 0.001), (36, -0.025), (37, 0.073), (38, 0.028), (39, 0.047), (40, -0.014), (41, -0.009), (42, 0.078), (43, -0.004), (44, 0.004), (45, 0.01), (46, 0.013), (47, -0.025), (48, 0.034), (49, -0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96556914 <a title="411-lsi-1" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>Author: Yuning Chai, Victor Lempitsky, Andrew Zisserman</p><p>Abstract: We propose a new method for the task of fine-grained visual categorization. The method builds a model of the baselevel category that can be fitted to images, producing highquality foreground segmentation and mid-level part localizations. The model can be learnt from the typical datasets available for fine-grained categorization, where the only annotation provided is a loose bounding box around the instance (e.g. bird) in each image. Both segmentation and part localizations are then used to encode the image content into a highly-discriminative visual signature. The model is symbiotic in that part discovery/localization is helped by segmentation and, conversely, the segmentation is helped by the detection (e.g. part layout). Our model builds on top of the part-based object category detector of Felzenszwalb et al., and also on the powerful GrabCut segmentation algorithm of Rother et al., and adds a simple spatial saliency coupling between them. In our evaluation, the model improves the categorization accuracy over the state-of-the-art. It also improves over what can be achieved with an analogous system that runs segmentation and part-localization independently.</p><p>2 0.82573533 <a title="411-lsi-2" href="./iccv-2013-Fine-Grained_Categorization_by_Alignments.html">169 iccv-2013-Fine-Grained Categorization by Alignments</a></p>
<p>Author: E. Gavves, B. Fernando, C.G.M. Snoek, A.W.M. Smeulders, T. Tuytelaars</p><p>Abstract: The aim of this paper is fine-grained categorization without human interaction. Different from prior work, which relies on detectors for specific object parts, we propose to localize distinctive details by roughly aligning the objects using just the overall shape, since implicit to fine-grained categorization is the existence of a super-class shape shared among all classes. The alignments are then used to transfer part annotations from training images to test images (supervised alignment), or to blindly yet consistently segment the object in a number of regions (unsupervised alignment). We furthermore argue that in the distinction of finegrained sub-categories, classification-oriented encodings like Fisher vectors are better suited for describing localized information than popular matching oriented features like HOG. We evaluate the method on the CU-2011 Birds and Stanford Dogs fine-grained datasets, outperforming the state-of-the-art.</p><p>3 0.79272473 <a title="411-lsi-3" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>Author: Ning Zhang, Ryan Farrell, Forrest Iandola, Trevor Darrell</p><p>Abstract: Recognizing objects in fine-grained domains can be extremely challenging due to the subtle differences between subcategories. Discriminative markings are often highly localized, leading traditional object recognition approaches to struggle with the large pose variation often present in these domains. Pose-normalization seeks to align training exemplars, either piecewise by part or globally for the whole object, effectively factoring out differences in pose and in viewing angle. Prior approaches relied on computationally-expensive filter ensembles for part localization and required extensive supervision. This paper proposes two pose-normalized descriptors based on computationally-efficient deformable part models. The first leverages the semantics inherent in strongly-supervised DPM parts. The second exploits weak semantic annotations to learn cross-component correspondences, computing pose-normalized descriptors from the latent parts of a weakly-supervised DPM. These representations enable pooling across pose and viewpoint, in turn facilitating tasks such as fine-grained recognition and attribute prediction. Experiments conducted on the Caltech-UCSD Birds 200 dataset and Berkeley Human Attribute dataset demonstrate significant improvements over state-of-art algorithms.</p><p>4 0.77012962 <a title="411-lsi-4" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>Author: Lingxi Xie, Qi Tian, Richang Hong, Shuicheng Yan, Bo Zhang</p><p>Abstract: As a special topic in computer vision, , fine-grained visual categorization (FGVC) has been attracting growing attention these years. Different with traditional image classification tasks in which objects have large inter-class variation, the visual concepts in the fine-grained datasets, such as hundreds of bird species, often have very similar semantics. Due to the large inter-class similarity, it is very difficult to classify the objects without locating really discriminative features, therefore it becomes more important for the algorithm to make full use of the part information in order to train a robust model. In this paper, we propose a powerful flowchart named Hierarchical Part Matching (HPM) to cope with finegrained classification tasks. We extend the Bag-of-Features (BoF) model by introducing several novel modules to integrate into image representation, including foreground inference and segmentation, Hierarchical Structure Learn- ing (HSL), and Geometric Phrase Pooling (GPP). We verify in experiments that our algorithm achieves the state-ofthe-art classification accuracy in the Caltech-UCSD-Birds200-2011 dataset by making full use of the ground-truth part annotations.</p><p>5 0.75683367 <a title="411-lsi-5" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>Author: Wei Xia, Csaba Domokos, Jian Dong, Loong-Fah Cheong, Shuicheng Yan</p><p>Abstract: Numerous existing object segmentation frameworks commonly utilize the object bounding box as a prior. In this paper, we address semantic segmentation assuming that object bounding boxes are provided by object detectors, but no training data with annotated segments are available. Based on a set of segment hypotheses, we introduce a simple voting scheme to estimate shape guidance for each bounding box. The derived shape guidance is used in the subsequent graph-cut-based figure-ground segmentation. The final segmentation result is obtained by merging the segmentation results in the bounding boxes. We conduct an extensive analysis of the effect of object bounding box accuracy. Comprehensive experiments on both the challenging PASCAL VOC object segmentation dataset and GrabCut50 image segmentation dataset show that the proposed approach achieves competitive results compared to previous detection or bounding box prior based methods, as well as other state-of-the-art semantic segmentation methods.</p><p>6 0.70346004 <a title="411-lsi-6" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>7 0.69976044 <a title="411-lsi-7" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>8 0.68481553 <a title="411-lsi-8" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<p>9 0.67275876 <a title="411-lsi-9" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>10 0.67224044 <a title="411-lsi-10" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>11 0.67211562 <a title="411-lsi-11" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>12 0.66689372 <a title="411-lsi-12" href="./iccv-2013-Detecting_Avocados_to_Zucchinis%3A_What_Have_We_Done%2C_and_Where_Are_We_Going%3F.html">109 iccv-2013-Detecting Avocados to Zucchinis: What Have We Done, and Where Are We Going?</a></p>
<p>13 0.66068757 <a title="411-lsi-13" href="./iccv-2013-Decomposing_Bag_of_Words_Histograms.html">104 iccv-2013-Decomposing Bag of Words Histograms</a></p>
<p>14 0.65098989 <a title="411-lsi-14" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<p>15 0.64296263 <a title="411-lsi-15" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>16 0.64145285 <a title="411-lsi-16" href="./iccv-2013-Shufflets%3A_Shared_Mid-level_Parts_for_Fast_Object_Detection.html">390 iccv-2013-Shufflets: Shared Mid-level Parts for Fast Object Detection</a></p>
<p>17 0.63773251 <a title="411-lsi-17" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>18 0.62899351 <a title="411-lsi-18" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>19 0.62896711 <a title="411-lsi-19" href="./iccv-2013-How_Do_You_Tell_a_Blackbird_from_a_Crow%3F.html">202 iccv-2013-How Do You Tell a Blackbird from a Crow?</a></p>
<p>20 0.61755615 <a title="411-lsi-20" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.07), (4, 0.027), (7, 0.017), (13, 0.015), (25, 0.158), (26, 0.128), (31, 0.057), (34, 0.04), (35, 0.022), (40, 0.012), (42, 0.081), (64, 0.061), (73, 0.035), (89, 0.17), (95, 0.013), (98, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88141239 <a title="411-lda-1" href="./iccv-2013-Efficient_Image_Dehazing_with_Boundary_Constraint_and_Contextual_Regularization.html">135 iccv-2013-Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</a></p>
<p>Author: Gaofeng Meng, Ying Wang, Jiangyong Duan, Shiming Xiang, Chunhong Pan</p><p>Abstract: unkown-abstract</p><p>2 0.86764443 <a title="411-lda-2" href="./iccv-2013-Learning_CRFs_for_Image_Parsing_with_Adaptive_Subgradient_Descent.html">234 iccv-2013-Learning CRFs for Image Parsing with Adaptive Subgradient Descent</a></p>
<p>Author: Honghui Zhang, Jingdong Wang, Ping Tan, Jinglu Wang, Long Quan</p><p>Abstract: We propose an adaptive subgradient descent method to efficiently learn the parameters of CRF models for image parsing. To balance the learning efficiency and performance of the learned CRF models, the parameter learning is iteratively carried out by solving a convex optimization problem in each iteration, which integrates a proximal term to preserve the previously learned information and the large margin preference to distinguish bad labeling and the ground truth labeling. A solution of subgradient descent updating form is derived for the convex optimization problem, with an adaptively determined updating step-size. Besides, to deal with partially labeled training data, we propose a new objective constraint modeling both the labeled and unlabeled parts in the partially labeled training data for the parameter learning of CRF models. The superior learning efficiency of the proposed method is verified by the experiment results on two public datasets. We also demonstrate the powerfulness of our method for handling partially labeled training data.</p><p>same-paper 3 0.86119866 <a title="411-lda-3" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>Author: Yuning Chai, Victor Lempitsky, Andrew Zisserman</p><p>Abstract: We propose a new method for the task of fine-grained visual categorization. The method builds a model of the baselevel category that can be fitted to images, producing highquality foreground segmentation and mid-level part localizations. The model can be learnt from the typical datasets available for fine-grained categorization, where the only annotation provided is a loose bounding box around the instance (e.g. bird) in each image. Both segmentation and part localizations are then used to encode the image content into a highly-discriminative visual signature. The model is symbiotic in that part discovery/localization is helped by segmentation and, conversely, the segmentation is helped by the detection (e.g. part layout). Our model builds on top of the part-based object category detector of Felzenszwalb et al., and also on the powerful GrabCut segmentation algorithm of Rother et al., and adds a simple spatial saliency coupling between them. In our evaluation, the model improves the categorization accuracy over the state-of-the-art. It also improves over what can be achieved with an analogous system that runs segmentation and part-localization independently.</p><p>4 0.85722566 <a title="411-lda-4" href="./iccv-2013-Image_Segmentation_with_Cascaded_Hierarchical_Models_and_Logistic_Disjunctive_Normal_Networks.html">211 iccv-2013-Image Segmentation with Cascaded Hierarchical Models and Logistic Disjunctive Normal Networks</a></p>
<p>Author: Mojtaba Seyedhosseini, Mehdi Sajjadi, Tolga Tasdizen</p><p>Abstract: Contextual information plays an important role in solving vision problems such as image segmentation. However, extracting contextual information and using it in an effective way remains a difficult problem. To address this challenge, we propose a multi-resolution contextual framework, called cascaded hierarchical model (CHM), which learns contextual information in a hierarchical framework for image segmentation. At each level of the hierarchy, a classifier is trained based on downsampled input images and outputs of previous levels. Our model then incorporates the resulting multi-resolution contextual information into a classifier to segment the input image at original resolution. We repeat this procedure by cascading the hierarchical framework to improve the segmentation accuracy. Multiple classifiers are learned in the CHM; therefore, a fast and accurate classifier is required to make the training tractable. The classifier also needs to be robust against overfitting due to the large number of parameters learned during training. We introduce a novel classification scheme, called logistic dis- junctive normal networks (LDNN), which consists of one adaptive layer of feature detectors implemented by logistic sigmoid functions followed by two fixed layers of logical units that compute conjunctions and disjunctions, respectively. We demonstrate that LDNN outperforms state-of-theart classifiers and can be used in the CHM to improve object segmentation performance.</p><p>5 0.84727716 <a title="411-lda-5" href="./iccv-2013-Parallel_Transport_of_Deformations_in_Shape_Space_of_Elastic_Surfaces.html">307 iccv-2013-Parallel Transport of Deformations in Shape Space of Elastic Surfaces</a></p>
<p>Author: Qian Xie, Sebastian Kurtek, Huiling Le, Anuj Srivastava</p><p>Abstract: Statistical shape analysis develops methods for comparisons, deformations, summarizations, and modeling of shapes in given data sets. These tasks require afundamental tool called parallel transport of tangent vectors along arbitrary paths. This tool is essential for: (1) computation of geodesic paths using either shooting or path-straightening method, (2) transferring deformations across objects, and (3) modeling of statistical variability in shapes. Using the square-root normal field (SRNF) representation of parameterized surfaces, we present a method for transporting deformations along paths in the shape space. This is difficult despite the underlying space being a vector space because the chosen (elastic) Riemannian metric is non-standard. Using a finite-basis for representing SRNFs of shapes, we derive expressions for Christoffel symbols that enable parallel transports. We demonstrate this framework using examples from shape analysis of parameterized spherical sur- faces, in the three contexts mentioned above.</p><p>6 0.83860946 <a title="411-lda-6" href="./iccv-2013-A_Simple_Model_for_Intrinsic_Image_Decomposition_with_Depth_Cues.html">30 iccv-2013-A Simple Model for Intrinsic Image Decomposition with Depth Cues</a></p>
<p>7 0.81999534 <a title="411-lda-7" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>8 0.81236827 <a title="411-lda-8" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>9 0.81208003 <a title="411-lda-9" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>10 0.80878961 <a title="411-lda-10" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>11 0.80712283 <a title="411-lda-11" href="./iccv-2013-Data-Driven_3D_Primitives_for_Single_Image_Understanding.html">102 iccv-2013-Data-Driven 3D Primitives for Single Image Understanding</a></p>
<p>12 0.80678582 <a title="411-lda-12" href="./iccv-2013-A_Deformable_Mixture_Parsing_Model_with_Parselets.html">8 iccv-2013-A Deformable Mixture Parsing Model with Parselets</a></p>
<p>13 0.80675161 <a title="411-lda-13" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>14 0.8042627 <a title="411-lda-14" href="./iccv-2013-Perceptual_Fidelity_Aware_Mean_Squared_Error.html">312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</a></p>
<p>15 0.80256367 <a title="411-lda-15" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<p>16 0.80243188 <a title="411-lda-16" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>17 0.80206758 <a title="411-lda-17" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>18 0.80163854 <a title="411-lda-18" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<p>19 0.80075449 <a title="411-lda-19" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>20 0.79952055 <a title="411-lda-20" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
