<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>121 iccv-2013-Discriminatively Trained Templates for 3D Object Detection: A Real Time Scalable Approach</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-121" href="#">iccv2013-121</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>121 iccv-2013-Discriminatively Trained Templates for 3D Object Detection: A Real Time Scalable Approach</h1>
<br/><p>Source: <a title="iccv-2013-121-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Rios-Cabrera_Discriminatively_Trained_Templates_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Reyes Rios-Cabrera, Tinne Tuytelaars</p><p>Abstract: In this paper we propose a new method for detecting multiple specific 3D objects in real time. We start from the template-based approach based on the LINE2D/LINEMOD representation introduced recently by Hinterstoisser et al., yet extend it in two ways. First, we propose to learn the templates in a discriminative fashion. We show that this can be done online during the collection of the example images, in just a few milliseconds, and has a big impact on the accuracy of the detector. Second, we propose a scheme based on cascades that speeds up detection. Since detection of an object is fast, new objects can be added with very low cost, making our approach scale well. In our experiments, we easily handle 10-30 3D objects at frame rates above 10fps using a single CPU core. We outperform the state-of-the-art both in terms of speed as well as in terms of accuracy, as validated on 3 different datasets. This holds both when using monocular color images (with LINE2D) and when using RGBD images (with LINEMOD). Moreover, wepropose a challenging new dataset made of12 objects, for future competing methods on monocular color images.</p><p>Reference: <a title="iccv-2013-121-reference" href="../iccv2013_reference/iccv-2013-Discriminatively_Trained_Templates_for_3D_Object_Detection%3A_A_Real_Time_Scalable_Approach_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 mx Abstract In this paper we propose a new method for detecting multiple specific 3D objects in real time. [sent-5, score-0.218]
</p><p>2 First, we propose to learn the templates in a discriminative fashion. [sent-8, score-0.575]
</p><p>3 Since detection of an object is fast, new objects can be added with very low cost, making our approach scale well. [sent-11, score-0.263]
</p><p>4 In our experiments, we easily handle 10-30 3D objects at frame rates above 10fps using a single CPU core. [sent-12, score-0.17]
</p><p>5 2nd row: it is running at 14fps detecting 30 objects simultaneously (dataset of [2]). [sent-30, score-0.244]
</p><p>6 The main advantages of template matching are that it can be implemented efficiently and that it works well also for objects with few discriminating features that are dominantly determined based on their overall shape. [sent-35, score-0.465]
</p><p>7 While initially applied mostly in a relatively clean 2D setting, it has been shown that, by using multiple tem2048  plates and robust representations, it can also be very successful for detecting 3D objects in more uncontrolled scenarios [5, 7]. [sent-36, score-0.183]
</p><p>8 Additionally, the use of templates also allows to transfer efficiently and accurately metadata that has been provided in an offline phase for the example images, to the object in the test image once it has been detected. [sent-37, score-0.654]
</p><p>9 Current template based methods use greedy or heuristic approaches to learn the templates [2, 5, 6, 7, 13], which often produce suboptimal results. [sent-41, score-0.884]
</p><p>10 They cannot scale easily [5, 6, 7, 13] or are not fast enough [13], or cannot handle heavy clutter [2]. [sent-42, score-0.158]
</p><p>11 In this paper we propose to combine techniques that are usually used for offline learning methods, with techniques used in online learning ones, to overcome these limitations. [sent-43, score-0.212]
</p><p>12 Discriminative learning is standard for category-level recognition, but not often used for specific objects probably because i) it’s often assumed all appearance changes can be modeled mathematically (e. [sent-45, score-0.201]
</p><p>13 Starting from example images, this usually means that a large number of templates is needed, each of them covering only a small range of viewpoints or viewing conditions. [sent-51, score-0.506]
</p><p>14 Regarding the latter, the main idea is that one wants a fast system able to learn by simply showing it images of the object to be detected and discriminative learning is not a straight forward step because of lack of intraclass features. [sent-52, score-0.161]
</p><p>15 This combination of online methods with learning based methods for specific object detection is our main contribution. [sent-59, score-0.291]
</p><p>16 But why not optimize the templates obtained via the online learning ? [sent-64, score-0.621]
</p><p>17 Related Work The detection ofinstances of a specific object was not addressed for a while, since it was considered an easier problem (compared to category-level object detection), and traditional methods such as [1, 8, 11] seemed to work well as long as the objects have enough texture. [sent-68, score-0.346]
</p><p>18 However, in fields  like robotics where a robot has to handle texture-poor objects and localize them in real-time, including their orientation, with high accuracy, and possibly their contours and segmentation, those traditional methods do not work well. [sent-69, score-0.238]
</p><p>19 The method does not require a time consuming training stage, and can handle untextured objects. [sent-73, score-0.177]
</p><p>20 It is designed to be robust to small image transformations by spreading gradient orientations on a regular grid. [sent-74, score-0.313]
</p><p>21 Using binary operations, evaluation ofthe templates is very fast. [sent-75, score-0.506]
</p><p>22 Another advantage of the DOT detector, is that the Hamming distance between templates can be used to create clusters grouped by OR’ing similar templates. [sent-76, score-0.566]
</p><p>23 However, the method becomes slow when handling several objects at the same time, and it is not template size invariant. [sent-78, score-0.465]
</p><p>24 However, when using thousands of templates, as needed for 3D  objects or when detecting multiple objects, its speed decreases, as demonstrated in [5], where it was compared to the LINE2D detector and was shown to be 100 times slower even when using already 4 pyramid levels for speeding up. [sent-81, score-0.332]
</p><p>25 As DOT, the LINE2D method of [5] does not require a time consuming training stage and can handle untextured 2049  objects. [sent-82, score-0.177]
</p><p>26 It is also based on dominant gradient orientations, it is robust to small image transformations and very robust to strong clutter. [sent-83, score-0.213]
</p><p>27 This is the type of templates that we mainly build on in this work (although the main idea can also be applied to other types of templates, e. [sent-84, score-0.506]
</p><p>28 However it is also slower and, as DOT, it is not scalable for detection of many objects simultaneously. [sent-90, score-0.268]
</p><p>29 They demonstrate how to build templates from 3D models, and how to estimate the 6 degrees-of-freedom pose accurately and in real-time. [sent-94, score-0.506]
</p><p>30 However, if we intend to handle the 15 objects it proposes simultaneously, the detection speed again falls to 0. [sent-98, score-0.343]
</p><p>31 In their test setup, they evaluate multi-object detection on a 30 objects dataset showing detections in the order of milliseconds. [sent-105, score-0.297]
</p><p>32 In their testing, they evaluate using the 50% intersection over union criterion, which we consider not precise enough for template matching methods. [sent-108, score-0.343]
</p><p>33 It can also handle many objects at the same time. [sent-110, score-0.17]
</p><p>34 It’s the only method that can handle 5 objects or more simultaneously and still run at several  frames per second. [sent-111, score-0.231]
</p><p>35 Moreover, even though the edgelets method runs in real time, it suffers an increase of testing time by 10 folds, when ambiguous objects are included. [sent-113, score-0.258]
</p><p>36 An example of such methods, trained to handle templates and metadata transfer, was proposed recently by Malisiewicz [9]. [sent-115, score-0.645]
</p><p>37 Only recently [14] proposed an efficient template learning scheme, albeit in a different context (finegrained classification). [sent-118, score-0.387]
</p><p>38 To this end, the authors propose to first cluster the training examples using HOG, then generate a DOT template for each training example and learn a mask for it based on a linear SVM, so as to remove background noise while at the same time keeping the relevant context information. [sent-120, score-0.543]
</p><p>39 The different templates are then combined into a strong classifier using boosting. [sent-121, score-0.588]
</p><p>40 Here, we build on this work, using the same ideas of applying a linear SVM to discriminatively train the templates and using boosting to combine multiple templates  into a cascade structure. [sent-123, score-1.235]
</p><p>41 More importantly, we focus on specific object detection instead of category-level detection. [sent-125, score-0.176]
</p><p>42 In the problem of specific object detection there is still room for improvement in terms of both speed, scalability and accuracy. [sent-128, score-0.176]
</p><p>43 Background and notations The LINE2D/LINEMOD method binarizes gradient orientations into a byte. [sent-130, score-0.21]
</p><p>44 However, the location of each gradient is taken into account to avoid the problem of accumulating many gradient orientations in a local area. [sent-132, score-0.317]
</p><p>45 In order to binarize the gradient orientations, it defines a range from [0 − 180◦] and it uses steps otaft o18n0s◦,/ it8 d e=fi 2e2s. [sent-133, score-0.154]
</p><p>46 A model or template T can then be defined as a pair T =A m(Oo,d ePl) ,o rw tehmerpe Oate eis T a c reanfer tehencne image noefd th aes object tTo d =ete (cOt, aPnd), P w specifies a region ninc eO i. [sent-139, score-0.391]
</p><p>47 m Taghee template can tthoe dne bteec compared pweicthif a region oatn nlo inca Oti. [sent-140, score-0.343]
</p><p>48 (2) By finding the maximum, gradients in the template and test image get better aligned. [sent-148, score-0.382]
</p><p>49 As in [12], we convert binary templates into weak classifiers. [sent-149, score-0.506]
</p><p>50 t (I) for a pool of templates T otn, we th bienna rbyui lrdes a strong fcl ? [sent-156, score-0.546]
</p><p>51 (4)  H(I) builds on T0 templates selected by AdaBoost. [sent-161, score-0.506]
</p><p>52 t (I)) are set automatically for each template using dth ien s ? [sent-163, score-0.343]
</p><p>53 Description of Our Method In our proposal we try to reconcile the advantages of traditional offline learning-based methods that use big annotated datasets with the advantages of online learning methods. [sent-166, score-0.168]
</p><p>54 Since acquiring a set of negative images is not costly at all, we use one to learn better templates with a linear SVM, building on the method of [12]. [sent-168, score-0.617]
</p><p>55 ii) Then we create a cascaded version to further speed up the detection process. [sent-169, score-0.269]
</p><p>56 iii) Finally, we further tune each template using the negative samples, i. [sent-170, score-0.433]
</p><p>57 Contrary to the template based methods of [5, 6, 7], we tune each of the templates separately because using the same parameters (number of regions) for every single template is suboptimal. [sent-173, score-1.237]
</p><p>58 DTT: Discriminatively Trained Templates Inspired by [12], we propose to learn the most impor-  tant elements of a template by using the weights of a linear SVM. [sent-178, score-0.472]
</p><p>59 This is done before learning a cascade structure to further speed up (see section 4. [sent-180, score-0.234]
</p><p>60 We use of the weights of a linear classifier to discriminate which regions of the templates are most important to compare with a test sample, and which are actually damaging the performance. [sent-188, score-0.669]
</p><p>61 The elements of the weights vector w that are negative are considered to be damaging the template performance, since those were generated mainly by support vectors from the negatives. [sent-189, score-0.543]
</p><p>62 Training the Templates We use a precomputed set of 10,000 negative samples, from 100 cluttered images that do not contain the objects we want to train. [sent-191, score-0.235]
</p><p>63 To apply the scheme above and select the most relevant elements of the  ×  templates as proposed in [12], we need a set ofpositive samples as well. [sent-192, score-0.573]
</p><p>64 Considering online learning, for each template ta of an image a (containing homogeneous background) we follow the next steps: 1. [sent-194, score-0.414]
</p><p>65 We capture the input image to learn the template ta. [sent-195, score-0.378]
</p><p>66 eHnetraet we emphasize stth aoft we idno nao 3t use t nhee regular grid when testing but only for selecting the most discriminative gradient orientations in the training process. [sent-198, score-0.35]
</p><p>67 Then we obtain all the strongest unique gradient orientations located in the cells (up to bn) that are above a threshold. [sent-200, score-0.251]
</p><p>68 This is done homogeneously on the input image (as opposed to the approach in [5, 12], where only the strongest top n0 gradient orientations are chosen). [sent-201, score-0.251]
</p><p>69 We do not select the top n0 gradient orientations, because it is possible that all the top n0 could be associated with the same gradient orientation. [sent-202, score-0.183]
</p><p>70 Then we find up to P nearest neighbors (NN) among the previously trained templates, that will be used as 205 1  additional positive examples for learning the linear SVM (if no templates were learnt yet, we use only the current one). [sent-205, score-0.645]
</p><p>71 For this, we compare templates using the similarity measurement of equation 1. [sent-206, score-0.537]
</p><p>72 We select a random set of a few hundred negatives from the large pool of negatives, (precomputed with the gradient spreading algorithm of [5]). [sent-210, score-0.213]
</p><p>73 Then we AND the template gradients of ta with each of the NN, to produce positive training vectors for the SVM, and with the selected negative subset, to produce negative training vectors. [sent-212, score-0.566]
</p><p>74 Only if the object makes it to the end of the cascade, we proceed to compare with the whole tuned look up table of cluster Ci as in the traditional template based methods. [sent-228, score-0.484]
</p><p>75 For the final clustered table, we tune each template ta to keep R±r bits. [sent-229, score-0.419]
</p><p>76 For each we learn a strong classifier and create a MIP cascade. [sent-239, score-0.177]
</p><p>77 Only hypothesis passing the cascade are compared with the tuned cluster table. [sent-240, score-0.203]
</p><p>78 In order  to create the dataset, we first determined an approximate location of the objects by using a calibration pattern. [sent-248, score-0.214]
</p><p>79 We trained about 1000 templates for each object, covering sizes [1. [sent-251, score-0.584]
</p><p>80 0], 360 degrees rotation, 30-85 degrees of tilt rotation and 45 degrees of in-plane rotation. [sent-253, score-0.172]
</p><p>81 We can train at 10fps while showing the object to the camera, and learning only a new template when this is 6% different from all saved ones. [sent-255, score-0.435]
</p><p>82 Moreover, if we only get the top N detections from each image, then comparing detections with other images is inaccurate. [sent-259, score-0.164]
</p><p>83 Since template based detectors try to find the best template fit, here we set a minimum bounding box overlap of 70% intersection over union. [sent-267, score-0.686]
</p><p>84 Inspecting the bounding boxes, and the associated template for each detection, we found out that 70% was a fair choice to represent the object. [sent-268, score-0.343]
</p><p>85 Detecting the 12 objects simultaneously, LINE2D runs at 1. [sent-270, score-0.199]
</p><p>86 25fps using a single core to detect the objects simultaneously, see Table 1. [sent-273, score-0.172]
</p><p>87 Since our method needs explicit templates for different views and sizes, we rotate each training image from the dataset and increase their sizes in steps of 12% to train in total about 32,000 templates for the whole set of 30 objects. [sent-289, score-1.124]
</p><p>88 This is because some templates appear to be subparts of bigger objects. [sent-299, score-0.539]
</p><p>89 We tested  our method on  template discriminatively  12 objects  comparing with the LINE2D baseline. [sent-329, score-0.538]
</p><p>90 One where we trained the  called DTT, and a second one, where each template is tuned, and then a a cascaded strong classifier for different  clusters is added. [sent-331, score-0.505]
</p><p>91 However to train our objects, we select the gradients and normals, using our discriminatively trained method. [sent-334, score-0.189]
</p><p>92 During detection, we use the clustered cascades to speed up detection, and the tuned tables, as described before. [sent-335, score-0.246]
</p><p>93 During training, we need on average 62msec for each template, plus about 24 seconds to create the speed up cascades. [sent-338, score-0.177]
</p><p>94 One of the standing problems is the need of several templates for training. [sent-345, score-0.506]
</p><p>95 using monocular color images Training discriminatively each ment in detection performance. [sent-352, score-0.207]
</p><p>96 template allows an increBy combining techniques methods with online learnand more accurate detector. [sent-354, score-0.414]
</p><p>97 Real-time learning and detection of 3d texture-less objects: A scalable approach. [sent-369, score-0.19]
</p><p>98 Dominant orientation templates for real-time detection of texture-less objects. [sent-402, score-0.643]
</p><p>99 Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes. [sent-412, score-0.247]
</p><p>100 Boosting binary masks and dominant orientation templates for efficient object detection. [sent-445, score-0.646]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('templates', 0.506), ('template', 0.343), ('hinterstoisser', 0.243), ('damen', 0.213), ('dtt', 0.152), ('ori', 0.145), ('linemod', 0.137), ('orientations', 0.135), ('objects', 0.122), ('cascade', 0.11), ('dot', 0.093), ('detection', 0.093), ('detections', 0.082), ('speed', 0.08), ('cascades', 0.08), ('ilic', 0.08), ('runs', 0.077), ('mip', 0.076), ('gradient', 0.075), ('discriminatively', 0.073), ('online', 0.071), ('steger', 0.069), ('textureless', 0.066), ('clutter', 0.063), ('detecting', 0.061), ('damaging', 0.061), ('simultaneously', 0.061), ('create', 0.06), ('weights', 0.06), ('testing', 0.059), ('auc', 0.058), ('drost', 0.056), ('tuned', 0.055), ('scalable', 0.053), ('spreading', 0.053), ('wtx', 0.053), ('offline', 0.053), ('negatives', 0.052), ('learnt', 0.051), ('transformations', 0.05), ('core', 0.05), ('rgbd', 0.049), ('speeds', 0.049), ('untextured', 0.048), ('manufacturing', 0.048), ('binarize', 0.048), ('object', 0.048), ('dominant', 0.048), ('handle', 0.048), ('training', 0.047), ('heavy', 0.047), ('metadata', 0.047), ('navab', 0.045), ('tune', 0.045), ('negative', 0.045), ('trained', 0.044), ('degrees', 0.044), ('orientation', 0.044), ('learning', 0.044), ('classifier', 0.042), ('monocular', 0.041), ('strongest', 0.041), ('rotation', 0.04), ('strong', 0.04), ('ideas', 0.04), ('gradients', 0.039), ('leuven', 0.039), ('cluster', 0.038), ('svm', 0.038), ('pyramid', 0.038), ('seconds', 0.037), ('robotics', 0.037), ('precomputed', 0.036), ('fua', 0.036), ('cascaded', 0.036), ('learn', 0.035), ('specific', 0.035), ('discriminative', 0.034), ('elements', 0.034), ('consuming', 0.034), ('lepetit', 0.034), ('sizes', 0.034), ('keeping', 0.033), ('cos', 0.033), ('select', 0.033), ('bits', 0.033), ('bigger', 0.033), ('cluttered', 0.032), ('faster', 0.032), ('malisiewicz', 0.032), ('location', 0.032), ('similarity', 0.031), ('argument', 0.031), ('clustered', 0.031), ('steps', 0.031), ('levels', 0.031), ('robot', 0.031), ('costly', 0.031), ('nn', 0.031), ('pruning', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="121-tfidf-1" href="./iccv-2013-Discriminatively_Trained_Templates_for_3D_Object_Detection%3A_A_Real_Time_Scalable_Approach.html">121 iccv-2013-Discriminatively Trained Templates for 3D Object Detection: A Real Time Scalable Approach</a></p>
<p>Author: Reyes Rios-Cabrera, Tinne Tuytelaars</p><p>Abstract: In this paper we propose a new method for detecting multiple specific 3D objects in real time. We start from the template-based approach based on the LINE2D/LINEMOD representation introduced recently by Hinterstoisser et al., yet extend it in two ways. First, we propose to learn the templates in a discriminative fashion. We show that this can be done online during the collection of the example images, in just a few milliseconds, and has a big impact on the accuracy of the detector. Second, we propose a scheme based on cascades that speeds up detection. Since detection of an object is fast, new objects can be added with very low cost, making our approach scale well. In our experiments, we easily handle 10-30 3D objects at frame rates above 10fps using a single CPU core. We outperform the state-of-the-art both in terms of speed as well as in terms of accuracy, as validated on 3 different datasets. This holds both when using monocular color images (with LINE2D) and when using RGBD images (with LINEMOD). Moreover, wepropose a challenging new dataset made of12 objects, for future competing methods on monocular color images.</p><p>2 0.30857113 <a title="121-tfidf-2" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>Author: Junliang Xing, Jin Gao, Bing Li, Weiming Hu, Shuicheng Yan</p><p>Abstract: Recently, sparse representation has been introduced for robust object tracking. By representing the object sparsely, i.e., using only a few templates via ?1-norm minimization, these so-called ?1-trackers exhibit promising tracking results. In this work, we address the object template building and updating problem in these ?1-tracking approaches, which has not been fully studied. We propose to perform template updating, in a new perspective, as an online incremental dictionary learning problem, which is efficiently solved through an online optimization procedure. To guarantee the robustness and adaptability of the tracking algorithm, we also propose to build a multi-lifespan dictionary model. By building target dictionaries of different lifespans, effective object observations can be obtained to deal with the well-known drifting problem in tracking and thus improve the tracking accuracy. We derive effective observa- tion models both generatively and discriminatively based on the online multi-lifespan dictionary learning model and deploy them to the Bayesian sequential estimation framework to perform tracking. The proposed approach has been extensively evaluated on ten challenging video sequences. Experimental results demonstrate the effectiveness of the online learned templates, as well as the state-of-the-art tracking performance of the proposed approach.</p><p>3 0.25791004 <a title="121-tfidf-3" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>Author: Jifeng Dai, Ying Nian Wu, Jie Zhou, Song-Chun Zhu</p><p>Abstract: Cosegmentation refers to theproblem ofsegmenting multiple images simultaneously by exploiting the similarities between the foreground and background regions in these images. The key issue in cosegmentation is to align common objects between these images. To address this issue, we propose an unsupervised learning framework for cosegmentation, by coupling cosegmentation with what we call “cosketch ”. The goal of cosketch is to automatically discover a codebook of deformable shape templates shared by the input images. These shape templates capture distinct image patterns and each template is matched to similar image patches in different images. Thus the cosketch of the images helps to align foreground objects, thereby providing crucial information for cosegmentation. We present a statistical model whose energy function couples cosketch and cosegmentation. We then present an unsupervised learning algorithm that performs cosketch and cosegmentation by energy minimization. Experiments show that our method outperforms state of the art methods for cosegmentation on the challenging MSRC and iCoseg datasets. We also illustrate our method on a new dataset called Coseg-Rep where cosegmentation can be performed within a single image with repetitive patterns.</p><p>4 0.23114681 <a title="121-tfidf-4" href="./iccv-2013-Online_Robust_Non-negative_Dictionary_Learning_for_Visual_Tracking.html">298 iccv-2013-Online Robust Non-negative Dictionary Learning for Visual Tracking</a></p>
<p>Author: Naiyan Wang, Jingdong Wang, Dit-Yan Yeung</p><p>Abstract: This paper studies the visual tracking problem in video sequences and presents a novel robust sparse tracker under the particle filter framework. In particular, we propose an online robust non-negative dictionary learning algorithm for updating the object templates so that each learned template can capture a distinctive aspect of the tracked object. Another appealing property of this approach is that it can automatically detect and reject the occlusion and cluttered background in a principled way. In addition, we propose a new particle representation formulation using the Huber loss function. The advantage is that it can yield robust estimation without using trivial templates adopted by previous sparse trackers, leading to faster computation. We also reveal the equivalence between this new formulation and the previous one which uses trivial templates. The proposed tracker is empirically compared with state-of-the-art trackers on some challenging video sequences. Both quantitative and qualitative comparisons show that our proposed tracker is superior and more stable.</p><p>5 0.17007256 <a title="121-tfidf-5" href="./iccv-2013-Learning_a_Dictionary_of_Shape_Epitomes_with_Applications_to_Image_Labeling.html">245 iccv-2013-Learning a Dictionary of Shape Epitomes with Applications to Image Labeling</a></p>
<p>Author: Liang-Chieh Chen, George Papandreou, Alan L. Yuille</p><p>Abstract: The first main contribution of this paper is a novel method for representing images based on a dictionary of shape epitomes. These shape epitomes represent the local edge structure of the image and include hidden variables to encode shift and rotations. They are learnt in an unsupervised manner from groundtruth edges. This dictionary is compact but is also able to capture the typical shapes of edges in natural images. In this paper, we illustrate the shape epitomes by applying them to the image labeling task. In other work, described in the supplementary material, we apply them to edge detection and image modeling. We apply shape epitomes to image labeling by using Conditional Random Field (CRF) Models. They are alternatives to the superpixel or pixel representations used in most CRFs. In our approach, the shape of an image patch is encoded by a shape epitome from the dictionary. Unlike the superpixel representation, our method avoids making early decisions which cannot be reversed. Our resulting hierarchical CRFs efficiently capture both local and global class co-occurrence properties. We demonstrate its quanti- tative and qualitativeproperties ofour approach with image labeling experiments on two standard datasets: MSRC-21 and Stanford Background.</p><p>6 0.15813395 <a title="121-tfidf-6" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<p>7 0.13379893 <a title="121-tfidf-7" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>8 0.13098809 <a title="121-tfidf-8" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>9 0.12567337 <a title="121-tfidf-9" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>10 0.12485518 <a title="121-tfidf-10" href="./iccv-2013-Human_Re-identification_by_Matching_Compositional_Template_with_Cluster_Sampling.html">205 iccv-2013-Human Re-identification by Matching Compositional Template with Cluster Sampling</a></p>
<p>11 0.10636665 <a title="121-tfidf-11" href="./iccv-2013-Multiple_Non-rigid_Surface_Detection_and_Registration.html">283 iccv-2013-Multiple Non-rigid Surface Detection and Registration</a></p>
<p>12 0.10499039 <a title="121-tfidf-12" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>13 0.1034959 <a title="121-tfidf-13" href="./iccv-2013-Learning_Graph_Matching%3A_Oriented_to_Category_Modeling_from_Cluttered_Scenes.html">237 iccv-2013-Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes</a></p>
<p>14 0.10325079 <a title="121-tfidf-14" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>15 0.10025109 <a title="121-tfidf-15" href="./iccv-2013-Saliency_Detection_via_Dense_and_Sparse_Reconstruction.html">372 iccv-2013-Saliency Detection via Dense and Sparse Reconstruction</a></p>
<p>16 0.097326778 <a title="121-tfidf-16" href="./iccv-2013-Handling_Occlusions_with_Franken-Classifiers.html">190 iccv-2013-Handling Occlusions with Franken-Classifiers</a></p>
<p>17 0.092669591 <a title="121-tfidf-17" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>18 0.091994412 <a title="121-tfidf-18" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>19 0.091071241 <a title="121-tfidf-19" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>20 0.090731941 <a title="121-tfidf-20" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.243), (1, -0.012), (2, -0.001), (3, -0.036), (4, 0.031), (5, -0.092), (6, -0.086), (7, 0.064), (8, -0.124), (9, 0.015), (10, 0.021), (11, -0.013), (12, -0.01), (13, 0.007), (14, 0.026), (15, -0.034), (16, 0.078), (17, 0.118), (18, 0.043), (19, -0.082), (20, 0.023), (21, 0.066), (22, -0.068), (23, 0.038), (24, -0.021), (25, 0.005), (26, -0.07), (27, 0.1), (28, -0.074), (29, 0.002), (30, -0.027), (31, 0.09), (32, 0.089), (33, 0.026), (34, 0.005), (35, -0.144), (36, -0.122), (37, -0.027), (38, -0.085), (39, -0.218), (40, -0.099), (41, 0.119), (42, 0.05), (43, 0.012), (44, -0.048), (45, -0.04), (46, -0.067), (47, 0.027), (48, -0.044), (49, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92092228 <a title="121-lsi-1" href="./iccv-2013-Discriminatively_Trained_Templates_for_3D_Object_Detection%3A_A_Real_Time_Scalable_Approach.html">121 iccv-2013-Discriminatively Trained Templates for 3D Object Detection: A Real Time Scalable Approach</a></p>
<p>Author: Reyes Rios-Cabrera, Tinne Tuytelaars</p><p>Abstract: In this paper we propose a new method for detecting multiple specific 3D objects in real time. We start from the template-based approach based on the LINE2D/LINEMOD representation introduced recently by Hinterstoisser et al., yet extend it in two ways. First, we propose to learn the templates in a discriminative fashion. We show that this can be done online during the collection of the example images, in just a few milliseconds, and has a big impact on the accuracy of the detector. Second, we propose a scheme based on cascades that speeds up detection. Since detection of an object is fast, new objects can be added with very low cost, making our approach scale well. In our experiments, we easily handle 10-30 3D objects at frame rates above 10fps using a single CPU core. We outperform the state-of-the-art both in terms of speed as well as in terms of accuracy, as validated on 3 different datasets. This holds both when using monocular color images (with LINE2D) and when using RGBD images (with LINEMOD). Moreover, wepropose a challenging new dataset made of12 objects, for future competing methods on monocular color images.</p><p>2 0.7442221 <a title="121-lsi-2" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>Author: Jifeng Dai, Ying Nian Wu, Jie Zhou, Song-Chun Zhu</p><p>Abstract: Cosegmentation refers to theproblem ofsegmenting multiple images simultaneously by exploiting the similarities between the foreground and background regions in these images. The key issue in cosegmentation is to align common objects between these images. To address this issue, we propose an unsupervised learning framework for cosegmentation, by coupling cosegmentation with what we call “cosketch ”. The goal of cosketch is to automatically discover a codebook of deformable shape templates shared by the input images. These shape templates capture distinct image patterns and each template is matched to similar image patches in different images. Thus the cosketch of the images helps to align foreground objects, thereby providing crucial information for cosegmentation. We present a statistical model whose energy function couples cosketch and cosegmentation. We then present an unsupervised learning algorithm that performs cosketch and cosegmentation by energy minimization. Experiments show that our method outperforms state of the art methods for cosegmentation on the challenging MSRC and iCoseg datasets. We also illustrate our method on a new dataset called Coseg-Rep where cosegmentation can be performed within a single image with repetitive patterns.</p><p>3 0.61738449 <a title="121-lsi-3" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>Author: Junliang Xing, Jin Gao, Bing Li, Weiming Hu, Shuicheng Yan</p><p>Abstract: Recently, sparse representation has been introduced for robust object tracking. By representing the object sparsely, i.e., using only a few templates via ?1-norm minimization, these so-called ?1-trackers exhibit promising tracking results. In this work, we address the object template building and updating problem in these ?1-tracking approaches, which has not been fully studied. We propose to perform template updating, in a new perspective, as an online incremental dictionary learning problem, which is efficiently solved through an online optimization procedure. To guarantee the robustness and adaptability of the tracking algorithm, we also propose to build a multi-lifespan dictionary model. By building target dictionaries of different lifespans, effective object observations can be obtained to deal with the well-known drifting problem in tracking and thus improve the tracking accuracy. We derive effective observa- tion models both generatively and discriminatively based on the online multi-lifespan dictionary learning model and deploy them to the Bayesian sequential estimation framework to perform tracking. The proposed approach has been extensively evaluated on ten challenging video sequences. Experimental results demonstrate the effectiveness of the online learned templates, as well as the state-of-the-art tracking performance of the proposed approach.</p><p>4 0.59300464 <a title="121-lsi-4" href="./iccv-2013-Online_Robust_Non-negative_Dictionary_Learning_for_Visual_Tracking.html">298 iccv-2013-Online Robust Non-negative Dictionary Learning for Visual Tracking</a></p>
<p>Author: Naiyan Wang, Jingdong Wang, Dit-Yan Yeung</p><p>Abstract: This paper studies the visual tracking problem in video sequences and presents a novel robust sparse tracker under the particle filter framework. In particular, we propose an online robust non-negative dictionary learning algorithm for updating the object templates so that each learned template can capture a distinctive aspect of the tracked object. Another appealing property of this approach is that it can automatically detect and reject the occlusion and cluttered background in a principled way. In addition, we propose a new particle representation formulation using the Huber loss function. The advantage is that it can yield robust estimation without using trivial templates adopted by previous sparse trackers, leading to faster computation. We also reveal the equivalence between this new formulation and the previous one which uses trivial templates. The proposed tracker is empirically compared with state-of-the-art trackers on some challenging video sequences. Both quantitative and qualitative comparisons show that our proposed tracker is superior and more stable.</p><p>5 0.58103883 <a title="121-lsi-5" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>Author: Jian Sun, Jean Ponce</p><p>Abstract: In this paper, we address the problem of learning discriminative part detectors from image sets with category labels. We propose a novel latent SVM model regularized by group sparsity to learn these part detectors. Starting from a large set of initial parts, the group sparsity regularizer forces the model to jointly select and optimize a set of discriminative part detectors in a max-margin framework. We propose a stochastic version of a proximal algorithm to solve the corresponding optimization problem. We apply the proposed method to image classification and cosegmentation, and quantitative experiments with standard benchmarks show that it matches or improves upon the state of the art.</p><p>6 0.56555986 <a title="121-lsi-6" href="./iccv-2013-Learning_a_Dictionary_of_Shape_Epitomes_with_Applications_to_Image_Labeling.html">245 iccv-2013-Learning a Dictionary of Shape Epitomes with Applications to Image Labeling</a></p>
<p>7 0.56465036 <a title="121-lsi-7" href="./iccv-2013-Human_Re-identification_by_Matching_Compositional_Template_with_Cluster_Sampling.html">205 iccv-2013-Human Re-identification by Matching Compositional Template with Cluster Sampling</a></p>
<p>8 0.56161988 <a title="121-lsi-8" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>9 0.55416745 <a title="121-lsi-9" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<p>10 0.53749835 <a title="121-lsi-10" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>11 0.52514231 <a title="121-lsi-11" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<p>12 0.51142794 <a title="121-lsi-12" href="./iccv-2013-Shufflets%3A_Shared_Mid-level_Parts_for_Fast_Object_Detection.html">390 iccv-2013-Shufflets: Shared Mid-level Parts for Fast Object Detection</a></p>
<p>13 0.49996075 <a title="121-lsi-13" href="./iccv-2013-Detecting_Curved_Symmetric_Parts_Using_a_Deformable_Disc_Model.html">110 iccv-2013-Detecting Curved Symmetric Parts Using a Deformable Disc Model</a></p>
<p>14 0.49880567 <a title="121-lsi-14" href="./iccv-2013-Content-Aware_Rotation.html">90 iccv-2013-Content-Aware Rotation</a></p>
<p>15 0.49253303 <a title="121-lsi-15" href="./iccv-2013-Modeling_Self-Occlusions_in_Dynamic_Shape_and_Appearance_Tracking.html">270 iccv-2013-Modeling Self-Occlusions in Dynamic Shape and Appearance Tracking</a></p>
<p>16 0.49021432 <a title="121-lsi-16" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>17 0.48291504 <a title="121-lsi-17" href="./iccv-2013-Multiple_Non-rigid_Surface_Detection_and_Registration.html">283 iccv-2013-Multiple Non-rigid Surface Detection and Registration</a></p>
<p>18 0.47863057 <a title="121-lsi-18" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>19 0.47747907 <a title="121-lsi-19" href="./iccv-2013-Data-Driven_3D_Primitives_for_Single_Image_Understanding.html">102 iccv-2013-Data-Driven 3D Primitives for Single Image Understanding</a></p>
<p>20 0.47581825 <a title="121-lsi-20" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.074), (6, 0.21), (7, 0.021), (12, 0.025), (26, 0.091), (31, 0.035), (42, 0.117), (48, 0.014), (64, 0.088), (73, 0.063), (89, 0.17), (98, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84730083 <a title="121-lda-1" href="./iccv-2013-Markov_Network-Based_Unified_Classifier_for_Face_Identification.html">261 iccv-2013-Markov Network-Based Unified Classifier for Face Identification</a></p>
<p>Author: Wonjun Hwang, Kyungshik Roh, Junmo Kim</p><p>Abstract: We propose a novel unifying framework using a Markov network to learn the relationship between multiple classifiers in face recognition. We assume that we have several complementary classifiers and assign observation nodes to the features of a query image and hidden nodes to the features of gallery images. We connect each hidden node to its corresponding observation node and to the hidden nodes of other neighboring classifiers. For each observation-hidden node pair, we collect a set of gallery candidates that are most similar to the observation instance, and the relationship between the hidden nodes is captured in terms of the similarity matrix between the collected gallery images. Posterior probabilities in the hidden nodes are computed by the belief-propagation algorithm. The novelty of the proposed framework is the method that takes into account the classifier dependency using the results of each neighboring classifier. We present extensive results on two different evaluation protocols, known and unknown image variation tests, using three different databases, which shows that the proposed framework always leads to good accuracy in face recognition.</p><p>2 0.84021437 <a title="121-lda-2" href="./iccv-2013-A_Fully_Hierarchical_Approach_for_Finding_Correspondences_in_Non-rigid_Shapes.html">11 iccv-2013-A Fully Hierarchical Approach for Finding Correspondences in Non-rigid Shapes</a></p>
<p>Author: Ivan Sipiran, Benjamin Bustos</p><p>Abstract: This paper presents a hierarchical method for finding correspondences in non-rigid shapes. We propose a new representation for 3D meshes: the decomposition tree. This structure characterizes the recursive decomposition process of a mesh into regions of interest and keypoints. The internal nodes contain regions of interest (which may be recursively decomposed) and the leaf nodes contain the keypoints to be matched. We also propose a hierarchical matching algorithm that performs in a level-wise manner. The matching process is guided by the similarity between regions in high levels of the tree, until reaching the keypoints stored in the leaves. This allows us to reduce the search space of correspondences, making also the matching process efficient. We evaluate the effectiveness of our approach using the SHREC’2010 robust correspondence benchmark. In addition, we show that our results outperform the state of the art.</p><p>same-paper 3 0.83214498 <a title="121-lda-3" href="./iccv-2013-Discriminatively_Trained_Templates_for_3D_Object_Detection%3A_A_Real_Time_Scalable_Approach.html">121 iccv-2013-Discriminatively Trained Templates for 3D Object Detection: A Real Time Scalable Approach</a></p>
<p>Author: Reyes Rios-Cabrera, Tinne Tuytelaars</p><p>Abstract: In this paper we propose a new method for detecting multiple specific 3D objects in real time. We start from the template-based approach based on the LINE2D/LINEMOD representation introduced recently by Hinterstoisser et al., yet extend it in two ways. First, we propose to learn the templates in a discriminative fashion. We show that this can be done online during the collection of the example images, in just a few milliseconds, and has a big impact on the accuracy of the detector. Second, we propose a scheme based on cascades that speeds up detection. Since detection of an object is fast, new objects can be added with very low cost, making our approach scale well. In our experiments, we easily handle 10-30 3D objects at frame rates above 10fps using a single CPU core. We outperform the state-of-the-art both in terms of speed as well as in terms of accuracy, as validated on 3 different datasets. This holds both when using monocular color images (with LINE2D) and when using RGBD images (with LINEMOD). Moreover, wepropose a challenging new dataset made of12 objects, for future competing methods on monocular color images.</p><p>4 0.80646032 <a title="121-lda-4" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>Author: Ali Elqursh, Ahmed Elgammal</p><p>Abstract: The vast majority of work on motion segmentation adopts the affine camera model due to its simplicity. Under the affine model, the motion segmentation problem becomes that of subspace separation. Due to this assumption, such methods are mainly offline and exhibit poor performance when the assumption is not satisfied. This is made evident in state-of-the-art methods that relax this assumption by using piecewise affine spaces and spectral clustering techniques to achieve better results. In this paper, we formulate the problem of motion segmentation as that of manifold separation. We then show how label propagation can be used in an online framework to achieve manifold separation. The performance of our framework is evaluated on a benchmark dataset and achieves competitive performance while being online.</p><p>5 0.76402253 <a title="121-lda-5" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<p>Author: Marius Leordeanu, Andrei Zanfir, Cristian Sminchisescu</p><p>Abstract: Estimating a dense correspondence field between successive video frames, under large displacement, is important in many visual learning and recognition tasks. We propose a novel sparse-to-dense matching method for motion field estimation and occlusion detection. As an alternative to the current coarse-to-fine approaches from the optical flow literature, we start from the higher level of sparse matching with rich appearance and geometric constraints collected over extended neighborhoods, using an occlusion aware, locally affine model. Then, we move towards the simpler, but denser classic flow field model, with an interpolation procedure that offers a natural transition between the sparse and the dense correspondence fields. We experimentally demonstrate that our appearance features and our complex geometric constraintspermit the correct motion estimation even in difficult cases of large displacements and significant appearance changes. We also propose a novel classification method for occlusion detection that works in conjunction with the sparse-to-dense matching model. We validate our approach on the newly released Sintel dataset and obtain state-of-the-art results.</p><p>6 0.76241755 <a title="121-lda-6" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>7 0.76219559 <a title="121-lda-7" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>8 0.75913084 <a title="121-lda-8" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>9 0.75442219 <a title="121-lda-9" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>10 0.75315976 <a title="121-lda-10" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>11 0.75291109 <a title="121-lda-11" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>12 0.75258213 <a title="121-lda-12" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>13 0.75242007 <a title="121-lda-13" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>14 0.75195307 <a title="121-lda-14" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>15 0.75180537 <a title="121-lda-15" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>16 0.75162709 <a title="121-lda-16" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>17 0.75093573 <a title="121-lda-17" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>18 0.75028914 <a title="121-lda-18" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>19 0.74972767 <a title="121-lda-19" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>20 0.74940491 <a title="121-lda-20" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
