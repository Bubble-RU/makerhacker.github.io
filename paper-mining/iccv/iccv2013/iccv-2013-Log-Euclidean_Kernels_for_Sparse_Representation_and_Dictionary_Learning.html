<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>257 iccv-2013-Log-Euclidean Kernels for Sparse Representation and Dictionary Learning</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-257" href="#">iccv2013-257</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>257 iccv-2013-Log-Euclidean Kernels for Sparse Representation and Dictionary Learning</h1>
<br/><p>Source: <a title="iccv-2013-257-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Li_Log-Euclidean_Kernels_for_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Peihua Li, Qilong Wang, Wangmeng Zuo, Lei Zhang</p><p>Abstract: The symmetric positive de?nite (SPD) matrices have been widely used in image and vision problems. Recently there are growing interests in studying sparse representation (SR) of SPD matrices, motivated by the great success of SR for vector data. Though the space of SPD matrices is well-known to form a Lie group that is a Riemannian manifold, existing work fails to take full advantage of its geometric structure. This paper attempts to tackle this problem by proposing a kernel based method for SR and dictionary learning (DL) of SPD matrices. We disclose that the space of SPD matrices, with the operations of logarithmic multiplication and scalar logarithmic multiplication de?ned in the Log-Euclidean framework, is a complete inner product space. We can thus develop a broad family of kernels that satis?es Mercer’s condition. These kernels characterize the geodesic distance and can be computed ef?ciently. We also consider the geometric structure in the DL process by updating atom matrices in the Riemannian space instead of in the Euclidean space. The proposed method is evaluated with various vision problems and shows notable per- formance gains over state-of-the-arts.</p><p>Reference: <a title="iccv-2013-257-reference" href="../iccv2013_reference/iccv-2013-Log-Euclidean_Kernels_for_Sparse_Representation_and_Dictionary_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 nite (SPD) matrices have been widely used in image and vision problems. [sent-6, score-0.284]
</p><p>2 Though the space of SPD matrices is well-known to form a Lie group that is a Riemannian manifold, existing work fails to take full advantage of its geometric structure. [sent-8, score-0.213]
</p><p>3 This paper attempts to tackle this problem by proposing a kernel based method for SR and dictionary learning (DL) of SPD matrices. [sent-9, score-0.19]
</p><p>4 We disclose that the space of SPD matrices, with the operations of logarithmic multiplication and scalar logarithmic multiplication de? [sent-10, score-0.397]
</p><p>5 ned in the Log-Euclidean framework, is a complete inner product space. [sent-11, score-0.155]
</p><p>6 We can thus develop a broad family of kernels that satis? [sent-12, score-0.149]
</p><p>7 These kernels characterize the geodesic distance and can be computed ef? [sent-14, score-0.159]
</p><p>8 We also consider the geometric structure in the DL process by updating atom matrices in the Riemannian space instead of in the Euclidean space. [sent-16, score-0.392]
</p><p>9 nite (SPD) matrices can be introduced in the imaging, pre-processing, feature extraction and representation processes, and have been widely adopted in many computer vision applications [4, 29, 27]. [sent-20, score-0.305]
</p><p>10 As to feature extraction, the covariance matrices that model the second-order statistics of image features also result in SPD matrices, which have been successfully applied to detection [27], recognition [18], and classi? [sent-29, score-0.21]
</p><p>11 Due to their wide applications, the investigations on learning methods for SPD matrices have recently received considerable research interests. [sent-31, score-0.149]
</p><p>12 It is known that the space of n n SPD matrices, denoted by Sknn+o,w isn n thoat a hlien esapra space n1 ×but n fo SPrmDs a aLtrieic group nthotaetd dis b a RSiemannian manifold [1]. [sent-33, score-0.141]
</p><p>13 Hence, the mathematical modeling in this space is different from what is commonly done in the Euclidean space [6, 3 1], and new operators for SPD matrices should be introduced for SPD matrix-based learning. [sent-34, score-0.248]
</p><p>14 In this work, we take sparse representation (SR) and dictionary learning (DL) [8] as examples, and focus on extending SR and DL to SPD matrix-based learning. [sent-35, score-0.164]
</p><p>15 Since conventional SR and DL methods are proposed for vector data in the Euclidean space rather than the SPD matrices in the Riemannian manifold, in order to use SR and DL for SPD matrix-based learning, we should consider the following issues in developing new operators in Sn+ . [sent-36, score-0.209]
</p><p>16 (1) In the Euicnlgide isasnu space tehvee l ionpeianrg cnoemwb oipneatriaotno rosfi anto Sm vectors can be naturally obtained using the conventional matrix operators; but it would be challenging to represent an SPD matrix as a linear combination of atom matrices since Sn+ is not a linear space. [sent-37, score-0.462]
</p><p>17 (3) The updating of dictionary atoms involves solving a constrained optimization problem in Sn+ and it is more appro1S+n is not a linear space with the operations of conventional matrix additSion and scalar-matrix multiplication. [sent-40, score-0.2]
</p><p>18 However, with the operations of  logarithmic multiplication and scalar logarithmic multiplication Sn+ is not loonglyar a hlimneiacr m space [ic1]a tbiount a alnsod a ccaolmarp l oegtae i nthnmeri cpr moduultcitp space as Sshown in section 2. [sent-41, score-0.406]
</p><p>19 In Tensor Spare Coding (TSC) [22], an SPD matrix is linearly decomposed as a set of of atom matrices and LogDet (or Bregman matrix) divergence [15] was adopted to measure the reconstruction error. [sent-51, score-0.433]
</p><p>20 Based on this framework, dictionary learning methods are further proposed to learn atom matrices [23]. [sent-52, score-0.449]
</p><p>21 In the generalized dictionary learning (GDL) algorithm [25], each SPD matrix is represented as a linear combination of rank-1 atom matrices; the error between one SPD matrix and its linear combination is evaluated by matrix Frobenius norm. [sent-53, score-0.405]
</p><p>22 Second, we can explicitly or implicitly map SPD matrices to some Reproducing Kernel Hilbert Space (RKHS), and use the kernel SR or DL framework for SPD matrix-  based learning. [sent-55, score-0.243]
</p><p>23 They adopted Stein kernel tmo map dth feo rS SPDR manadtri DcLes tno higher dimensional RKHS. [sent-59, score-0.094]
</p><p>24 This method is in contrast with those methods which directly embed SPD matrices into Euclidean space (LogE-SR) [11, 34] and achieves state-of-the-art performance compared to its counterparts. [sent-60, score-0.188]
</p><p>25 The main difference is that we develop a novel family of kernel functions based on the Log-Euclidean framework [1]. [sent-72, score-0.125]
</p><p>26 The proposed kernels characterize the geodesic distance and thus can accurately  ×  measure the reconstruction error; they also satisfy the Mercer’s condition under broad conditions. [sent-73, score-0.193]
</p><p>27 These are in contrast to the Stein kernel which is only an approximation of the geodesic distance and satis? [sent-74, score-0.147]
</p><p>28 Our work differs from them in that we disclose the inner product structure of Sn+, by which we can develop a broad variety of kernel foufn Sctions and the Gaussian kernel is a special case of ours. [sent-78, score-0.378]
</p><p>29 Log-Euclidean Kernel  This section starts with a brief introduction of LogEuclidean framework [1]; subsequently, we show that Sn+ Efourcmlisd an ni fnrnaemr product space; ebqauseedn on twheis ,s we design a family of kernel functions. [sent-80, score-0.178]
</p><p>30 In the Log-Euclidean framework, an operation of logarithmic multiplication ? [sent-87, score-0.171]
</p><p>31 ee identity matrix and with the inverse operation the regular matrix inverse. [sent-92, score-0.12]
</p><p>32 cTohme geodesics equipped wSith a bi-invariant metric are the left translates of the geodesics through the identity element, given by one-parameter subgroup exp(tV), where t ∈ R and V ∈ Sn. [sent-99, score-0.136]
</p><p>33 nally the geodesic distance between two SPD matrices S and T as follows: ρgeo(S, T) = ? [sent-101, score-0.202]
</p><p>34 Sn+ as a Complete Inner Product Space It is known that Sn+ is not a linear space with the operationIst iosf k tnhoew wconn tvheant Stional matrix addition and scalar-matrix multiplication but forms a Riemannian manifold [1, 3 1]. [sent-111, score-0.181]
</p><p>35 However, as shown in [1], in the Log-Euclidean framework it is endowed with a linear space structure with the logarithmic multiplication (1) and the following scalar logarithmic  multiplication [1]:  ×  λ ⊗ S = exp(λ log(S)) = Sλ  (3)  where λ is a real number. [sent-112, score-0.337]
</p><p>36 and ⊗ satisfy the conditions of a linear space, ewraithti tnhse identity m saattrisixfy being otnhed identity eale limneenart and regular matrix inversion operation as inverse mapping. [sent-114, score-0.113]
</p><p>37 Indeed, not only a linear space, Sn+ is also an inner productI space as dte osnclryib aed li by rth sep following corollary which is not disclosed previously: Corollary 1 With two operations ? [sent-115, score-0.247]
</p><p>38 log = tr(log(S) log(T))  (4)  is an inner product, where tr denotes the matrix trace, and Sn+ is a complete inner product space (Hilbert space). [sent-120, score-0.423]
</p><p>39 log = ≥0 0if, aanndd only bifv iSo iss thhaet identity matrix I. [sent-163, score-0.238]
</p><p>40 nite dimension and therefore it is a complete inner product space (Hilbert space) [26]. [sent-165, score-0.3]
</p><p>41 The norm induced by the inner product is expressed as  S? [sent-166, score-0.126]
</p><p>42 However, a linear space or a normed liner space is not necessarily an inner product space unless a function that satis? [sent-187, score-0.273]
</p><p>43 r oacneds sicngala dri logarithmic multiplication ⊗, unlike [1] twiohnich ? [sent-194, score-0.149]
</p><p>44 i annvodlv sceasl mapping hSmPiDc mmuatlrtiipcleics atotio logarithmic d [1o-] main, performing data processing therein and then mapping back to Sn+ again. [sent-195, score-0.1]
</p><p>45 kee:r n(1el); T T(h2e)  The tensor product φ1 ∗ φ? [sent-268, score-0.125]
</p><p>46 kernels, such as polynomial, exponential, radial basis, B-Spline kernels, or Fourier kernel etc. [sent-284, score-0.094]
</p><p>47 T Here we compare the proposed kernels with Stein kernel  =  >−  [24]. [sent-335, score-0.178]
</p><p>48 ne-Riemannian distance between the two matrices is dA (S, T) = ? [sent-342, score-0.149]
</p><p>49 1 shows the histogram computed from the SPD matrices used in texture classi? [sent-368, score-0.178]
</p><p>50 kernel under restricted co =nd eitxipo(n−, tβhdat is, β = 21, . [sent-375, score-0.094]
</p><p>51 The logarithm of SPD matrices can be computed through the eigen-decomposition. [sent-385, score-0.206]
</p><p>52 The logarithms of the involved SPD matrices can generally be computed beforehand because oftheir “decoupling” property either in the inner product or distance; in these cases, the complexity of the proposed kernels becomes O(n3) and is the same as that of the Stein kernel. [sent-394, score-0.359]
</p><p>53 While this method outperforms state-of-the-arts, the symmetric Stein divergence only approximates the Riemannian metric and the Stein kernel only satis? [sent-400, score-0.206]
</p><p>54 Let φ be th∈e fSunction that maps SPD matrices to RKHS, SR of Y can be formulated as the following kernelised LASSO problem [12]:  xm∈RinN? [sent-409, score-0.149]
</p><p>55 1 Minimaization of the above equation is similar to regular sparse coding in Euclidean space [10], and we use the method introduced in [12] for its solution. [sent-459, score-0.109]
</p><p>56 , M, the atom matrices can be obtained by learning method so that they have more powerful representation capability. [sent-465, score-0.374]
</p><p>57 First, suppose that the atom matrices Si ∈ Sn+ , i= 1, . [sent-498, score-0.353]
</p><p>58 , M, we compute its sparse vector xj as described in the previ-  ous section; then, let xj be ? [sent-505, score-0.097]
</p><p>59 xed, we update dictionary atom matrices Si, i= 1, . [sent-506, score-0.449]
</p><p>60 In the following, we illustrate the atom matrices update scheme using Gaussian kernel κg. [sent-510, score-0.447]
</p><p>61 Re-writing (7) in kernel function κ, we have the partial derivative of f(·) w. [sent-513, score-0.094]
</p><p>62 (8)  One may update log Sr instead of Sr, which is equivalent to transforming by logarithm the SPD matrices to Euclidean space in which atoms are updated. [sent-520, score-0.395]
</p><p>63 We thus instead update the atom matrices in the Lie group as follows: Sr = exp ? [sent-523, score-0.425]
</p><p>64 rst evaluate the performance of the  ×  proposed family of kernels on sparse representation without dictionary learning. [sent-532, score-0.321]
</p><p>65 As in [30], the training samples are adopted as atom matrices and the reconstruction errors are used for classi? [sent-533, score-0.353]
</p><p>66 Then we learn the atom matrices from the training data and the sparse codes obtained from the learned atom matrices are used for classi? [sent-535, score-0.753]
</p><p>67 cation  ×  mbye taho 43d ×in 4[330 c]o, vaanrdia tnhcee preprocessing ompetth thoed cilna s[1si2? [sent-552, score-0.296]
</p><p>68 2 shows the recognition accuracy of the proposed kernels with the regularization parameter λ = 10−3, where the recognition rates of RSR using Stein kernel [12] are also shown as baseline (red dash-dotted). [sent-555, score-0.232]
</p><p>69 ,Irte can ibneg seen thaat ta btoheu proposed kernels are clearly better than the Stein kernel on all datasets. [sent-560, score-0.178]
</p><p>70 cation rates of RSR that uses the Stein kernel [12] are shown as baseline (red dash-dotted line). [sent-619, score-0.444]
</p><p>71 TSC has unsatisfactory performance and we owe it to the linear representation of SPD matrices in the Euclidean space without use of the Riemannian metric. [sent-621, score-0.209]
</p><p>72 By using the Riemannian metric, LogE-SR has improved recognition rates but the sparse decomposition is performed in the logarithm domain rather than in the original Riemannian manifold. [sent-622, score-0.158]
</p><p>73 cation We employ the Brodatz dataset and follow the experimental setting in [22, 23, 12] for fair comparison. [sent-639, score-0.296]
</p><p>74 cation algorithm [17] but to testify the proposed method with closely related work. [sent-641, score-0.326]
</p><p>75 In the Brodatz dataset each class contains only one image and we use the mosaics of 5-texture (‘5c’ , ‘5m’, ‘5v’, ‘5v2’, ‘5v3’), 10texture (‘ 10’,‘ 10v’), and 16-texture (‘ 16c’, ‘ 16v’). [sent-642, score-0.09]
</p><p>76 Among the 64 covariance matrices per class, 5 are randomly selected for training and the remaining ones are for testing. [sent-646, score-0.21]
</p><p>77 Dictionary Learning To testify the effectiveness of the proposed dictionary learning method, we compare three methods: random sampling, K-Means clustering and dictionary learning. [sent-673, score-0.222]
</p><p>78 The K-Means clustering is performed in the LogEuclidean framework [1]: the covariance matrices are ? [sent-675, score-0.21]
</p><p>79 rst mapped to the linear space Sn by matrix logarithm, in which mthea clustering ilisn performed and the results are then mapped  ××  back to Sn+ . [sent-676, score-0.116]
</p><p>80 cation We use the Brodatz dataset and follow the experimental setting in [12]. [sent-678, score-0.296]
</p><p>81 T phiexe 5ls-d,i frmoemns wiohnicahl afe 5at×u5re voevcatroirasn ctoe compute the covariance matrix comprise grayscale intensity, and the 1st and 2nd partial derivatives with respect to spatial coordinates. [sent-681, score-0.096]
</p><p>82 We thus have 2200 covariance matrices in total for dictionary learning. [sent-683, score-0.306]
</p><p>83 It can be seen that the dictionary learning method is consistently superior to random dictionary and Log-E K-Means, particularly when the number of atom matrices are small. [sent-690, score-0.545]
</p><p>84 It is interesting to notice that the random dictionary is better than the learned dictionary via Log-E K-Means if the number of atom matrices are less than 80. [sent-691, score-0.545]
</p><p>85 We also observe that the performance of both random sampling and Log-E K-Means improves with the increase of atom matrix number. [sent-693, score-0.239]
</p><p>86 cation accuracy on the Brodatz dataset Scene Categorization We use the popular benchmark database Scene15 [16] for classi? [sent-696, score-0.296]
</p><p>87 In each image, we extract 8 8 covariance matrices at dense grids with a stride of t8r a pcitxe 8l×s. [sent-699, score-0.21]
</p><p>88 First, among covariance matrices ofall images, 50,000 ones are randomly chosen which  are used to obtain atom matrices. [sent-705, score-0.414]
</p><p>89 cation rates of the proposed method are over 18 percent higher than the random dictionary. [sent-712, score-0.383]
</p><p>90 We can also observe that the proposed method has over 8 percent, 4 percent, and 2 percent advantages over Log-E K-Means Clustering for 32, 64, and 128 atom matrices, respectively. [sent-713, score-0.237]
</p><p>91 From both of the above experiments, we observe that as atom matrix number grows, the performance gains of the dictionary learning over the other two methods gets smaller. [sent-714, score-0.357]
</p><p>92 As the current dictionary is generative without discriminative information, more powerful representational capability does not necessarily mean better discriminability. [sent-715, score-0.096]
</p><p>93 ndings and we think that the performance difference between the three methods will get smaller or even negligible as the atom matrix number becomes much larger. [sent-717, score-0.239]
</p><p>94 Conclusion  This paper presented a novel Riemannian metric based kernel method for SR and DL in Sn+. [sent-728, score-0.114]
</p><p>95 We disclosed that the space of SPD matrices is a complete inner product space, and developed a broad 11660077  family of p. [sent-734, score-0.404]
</p><p>96 Action recognition using sparse representation on covariance manifolds of optical ? [sent-828, score-0.129]
</p><p>97 Sparse coding and dictionary learning for symmetric positive de? [sent-838, score-0.166]
</p><p>98 Kernel methods on the riemannian manifold of symmetric positive de? [sent-854, score-0.236]
</p><p>99 Gabor feature based sparse representation for face recognition with gabor occlusion dictionary. [sent-990, score-0.096]
</p><p>100 Action recognition using sparse representation on covariance manifolds of optical ? [sent-998, score-0.129]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spd', 0.452), ('classi', 0.428), ('cation', 0.296), ('sn', 0.283), ('atom', 0.204), ('stein', 0.188), ('sr', 0.16), ('riemannian', 0.151), ('log', 0.15), ('matrices', 0.149), ('nite', 0.135), ('rsr', 0.12), ('dl', 0.098), ('dictionary', 0.096), ('kernel', 0.094), ('feret', 0.09), ('mosaics', 0.09), ('pn', 0.085), ('kernels', 0.084), ('logarithmic', 0.08), ('corollary', 0.08), ('brodatz', 0.08), ('inner', 0.073), ('tensor', 0.072), ('multiplication', 0.069), ('satis', 0.066), ('covariance', 0.061), ('tsc', 0.058), ('logarithm', 0.057), ('rates', 0.054), ('geodesic', 0.053), ('harandi', 0.053), ('product', 0.053), ('mercer', 0.053), ('rkhs', 0.053), ('euclidean', 0.048), ('sparse', 0.047), ('exp', 0.047), ('symmetric', 0.047), ('divergence', 0.045), ('hilbert', 0.044), ('geodesics', 0.044), ('rst', 0.042), ('bregman', 0.04), ('space', 0.039), ('manifold', 0.038), ('si', 0.036), ('de', 0.036), ('matrix', 0.035), ('bd', 0.034), ('broad', 0.034), ('percent', 0.033), ('family', 0.031), ('en', 0.03), ('boley', 0.03), ('disclose', 0.03), ('gdl', 0.03), ('morellas', 0.03), ('normed', 0.03), ('sandwiching', 0.03), ('sivalingam', 0.03), ('testify', 0.03), ('operations', 0.03), ('texture', 0.029), ('ned', 0.029), ('identity', 0.028), ('gabor', 0.028), ('bf', 0.027), ('bg', 0.027), ('nine', 0.027), ('arsigny', 0.027), ('pennec', 0.027), ('commutative', 0.027), ('cients', 0.027), ('xj', 0.025), ('group', 0.025), ('iss', 0.025), ('fillard', 0.025), ('logeuclidean', 0.025), ('rinn', 0.025), ('disclosed', 0.025), ('dsr', 0.025), ('yj', 0.024), ('pages', 0.023), ('lie', 0.023), ('coding', 0.023), ('proposition', 0.023), ('gains', 0.022), ('geo', 0.022), ('xed', 0.022), ('ds', 0.022), ('characterize', 0.022), ('operation', 0.022), ('representation', 0.021), ('diffusion', 0.021), ('xm', 0.021), ('operators', 0.021), ('coef', 0.02), ('therein', 0.02), ('metric', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999815 <a title="257-tfidf-1" href="./iccv-2013-Log-Euclidean_Kernels_for_Sparse_Representation_and_Dictionary_Learning.html">257 iccv-2013-Log-Euclidean Kernels for Sparse Representation and Dictionary Learning</a></p>
<p>Author: Peihua Li, Qilong Wang, Wangmeng Zuo, Lei Zhang</p><p>Abstract: The symmetric positive de?nite (SPD) matrices have been widely used in image and vision problems. Recently there are growing interests in studying sparse representation (SR) of SPD matrices, motivated by the great success of SR for vector data. Though the space of SPD matrices is well-known to form a Lie group that is a Riemannian manifold, existing work fails to take full advantage of its geometric structure. This paper attempts to tackle this problem by proposing a kernel based method for SR and dictionary learning (DL) of SPD matrices. We disclose that the space of SPD matrices, with the operations of logarithmic multiplication and scalar logarithmic multiplication de?ned in the Log-Euclidean framework, is a complete inner product space. We can thus develop a broad family of kernels that satis?es Mercer’s condition. These kernels characterize the geodesic distance and can be computed ef?ciently. We also consider the geometric structure in the DL process by updating atom matrices in the Riemannian space instead of in the Euclidean space. The proposed method is evaluated with various vision problems and shows notable per- formance gains over state-of-the-arts.</p><p>2 0.37045458 <a title="257-tfidf-2" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>Author: Hesamoddin Salehian, Guang Cheng, Baba C. Vemuri, Jeffrey Ho</p><p>Abstract: Symmetric positive-definite (SPD) matrices are ubiquitous in Computer Vision, Machine Learning and Medical Image Analysis. Finding the center/average of a population of such matrices is a common theme in many algorithms such as clustering, segmentation, principal geodesic analysis, etc. The center of a population of such matrices can be defined using a variety of distance/divergence measures as the minimizer of the sum of squared distances/divergences from the unknown center to the members of the population. It is well known that the computation of the Karcher mean for the space of SPD matrices which is a negativelycurved Riemannian manifold is computationally expensive. Recently, the LogDet divergence-based center was shown to be a computationally attractive alternative. However, the LogDet-based mean of more than two matrices can not be computed in closed form, which makes it computationally less attractive for large populations. In this paper we present a novel recursive estimator for center based on the Stein distance which is the square root of the LogDet di– vergence that is significantly faster than the batch mode computation of this center. The key theoretical contribution is a closed-form solution for the weighted Stein center of two SPD matrices, which is used in the recursive computation of the Stein center for a population of SPD matrices. Additionally, we show experimental evidence of the convergence of our recursive Stein center estimator to the batch mode Stein center. We present applications of our recursive estimator to K-means clustering and image indexing depicting significant time gains over corresponding algorithms that use the batch mode computations. For the latter application, we develop novel hashing functions using the Stein distance and apply it to publicly available data sets, and experimental results have shown favorable com– ∗This research was funded in part by the NIH grant NS066340 to BCV. †Corresponding author parisons to other competing methods.</p><p>3 0.28181154 <a title="257-tfidf-3" href="./iccv-2013-A_Novel_Earth_Mover%27s_Distance_Methodology_for_Image_Matching_with_Gaussian_Mixture_Models.html">25 iccv-2013-A Novel Earth Mover's Distance Methodology for Image Matching with Gaussian Mixture Models</a></p>
<p>Author: Peihua Li, Qilong Wang, Lei Zhang</p><p>Abstract: The similarity or distance measure between Gaussian mixture models (GMMs) plays a crucial role in contentbased image matching. Though the Earth Mover’s Distance (EMD) has shown its advantages in matching histogram features, its potentials in matching GMMs remain unclear and are not fully explored. To address this problem, we propose a novel EMD methodology for GMM matching. We ?rst present a sparse representation based EMD called SR-EMD by exploiting the sparse property of the underlying problem. SR-EMD is more ef?cient and robust than the conventional EMD. Second, we present two novel ground distances between component Gaussians based on the information geometry. The perspective from the Riemannian geometry distinguishes the proposed ground distances from the classical entropy- or divergence-based ones. Furthermore, motivated by the success of distance metric learning of vector data, we make the ?rst attempt to learn the EMD distance metrics between GMMs by using a simple yet effective supervised pair-wise based method. It can adapt the distance metrics between GMMs to speci?c classi?ca- tion tasks. The proposed method is evaluated on both simulated data and benchmark real databases and achieves very promising performance.</p><p>4 0.23913664 <a title="257-tfidf-4" href="./iccv-2013-Joint_Deep_Learning_for_Pedestrian_Detection.html">220 iccv-2013-Joint Deep Learning for Pedestrian Detection</a></p>
<p>Author: Wanli Ouyang, Xiaogang Wang</p><p>Abstract: Feature extraction, deformation handling, occlusion handling, and classi?cation are four important components in pedestrian detection. Existing methods learn or design these components either individually or sequentially. The interaction among these components is not yet well explored. This paper proposes that they should be jointly learned in order to maximize their strengths through cooperation. We formulate these four components into a joint deep learning framework and propose a new deep network architecture1. By establishing automatic, mutual interaction among components, the deep model achieves a 9% reduction in the average miss rate compared with the current best-performing pedestrian detection approaches on the largest Caltech benchmark dataset.</p><p>5 0.15128103 <a title="257-tfidf-5" href="./iccv-2013-Dictionary_Learning_and_Sparse_Coding_on_Grassmann_Manifolds%3A_An_Extrinsic_Solution.html">114 iccv-2013-Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution</a></p>
<p>Author: Mehrtash Harandi, Conrad Sanderson, Chunhua Shen, Brian Lovell</p><p>Abstract: Recent advances in computer vision and machine learning suggest that a wide range of problems can be addressed more appropriately by considering non-Euclidean geometry. In this paper we explore sparse dictionary learning over the space of linear subspaces, which form Riemannian structures known as Grassmann manifolds. To this end, we propose to embed Grassmann manifolds into the space of symmetric matrices by an isometric mapping, which enables us to devise a closed-form solution for updating a Grassmann dictionary, atom by atom. Furthermore, to handle non-linearity in data, we propose a kernelised version of the dictionary learning algorithm. Experiments on several classification tasks (face recognition, action recognition, dynamic texture classification) show that the proposed approach achieves considerable improvements in discrimination accuracy, in comparison to state-of-the-art methods such as kernelised Affine Hull Method and graphembedding Grassmann discriminant analysis.</p><p>6 0.10901572 <a title="257-tfidf-6" href="./iccv-2013-Nonparametric_Blind_Super-resolution.html">293 iccv-2013-Nonparametric Blind Super-resolution</a></p>
<p>7 0.10650744 <a title="257-tfidf-7" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>8 0.1044926 <a title="257-tfidf-8" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>9 0.099004522 <a title="257-tfidf-9" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>10 0.098372929 <a title="257-tfidf-10" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>11 0.097116306 <a title="257-tfidf-11" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>12 0.083686993 <a title="257-tfidf-12" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>13 0.082918778 <a title="257-tfidf-13" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>14 0.081915572 <a title="257-tfidf-14" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>15 0.080649756 <a title="257-tfidf-15" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>16 0.079698123 <a title="257-tfidf-16" href="./iccv-2013-Discriminant_Tracking_Using_Tensor_Representation_with_Semi-supervised_Improvement.html">119 iccv-2013-Discriminant Tracking Using Tensor Representation with Semi-supervised Improvement</a></p>
<p>17 0.076603033 <a title="257-tfidf-17" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>18 0.07624609 <a title="257-tfidf-18" href="./iccv-2013-Coupled_Dictionary_and_Feature_Space_Learning_with_Applications_to_Cross-Domain_Image_Synthesis_and_Recognition.html">96 iccv-2013-Coupled Dictionary and Feature Space Learning with Applications to Cross-Domain Image Synthesis and Recognition</a></p>
<p>19 0.075985238 <a title="257-tfidf-19" href="./iccv-2013-Parallel_Transport_of_Deformations_in_Shape_Space_of_Elastic_Surfaces.html">307 iccv-2013-Parallel Transport of Deformations in Shape Space of Elastic Surfaces</a></p>
<p>20 0.07373178 <a title="257-tfidf-20" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.132), (1, 0.046), (2, -0.06), (3, -0.023), (4, -0.152), (5, -0.016), (6, -0.033), (7, -0.007), (8, 0.0), (9, -0.085), (10, -0.026), (11, -0.056), (12, 0.02), (13, -0.052), (14, 0.039), (15, 0.042), (16, 0.027), (17, -0.022), (18, 0.031), (19, 0.019), (20, 0.013), (21, 0.066), (22, 0.105), (23, 0.006), (24, -0.083), (25, 0.09), (26, 0.061), (27, 0.099), (28, -0.183), (29, 0.184), (30, -0.124), (31, -0.097), (32, -0.066), (33, -0.008), (34, 0.161), (35, 0.222), (36, 0.097), (37, 0.126), (38, -0.169), (39, 0.066), (40, 0.036), (41, 0.04), (42, 0.07), (43, 0.025), (44, -0.042), (45, -0.023), (46, -0.114), (47, -0.055), (48, 0.042), (49, 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93211144 <a title="257-lsi-1" href="./iccv-2013-Log-Euclidean_Kernels_for_Sparse_Representation_and_Dictionary_Learning.html">257 iccv-2013-Log-Euclidean Kernels for Sparse Representation and Dictionary Learning</a></p>
<p>Author: Peihua Li, Qilong Wang, Wangmeng Zuo, Lei Zhang</p><p>Abstract: The symmetric positive de?nite (SPD) matrices have been widely used in image and vision problems. Recently there are growing interests in studying sparse representation (SR) of SPD matrices, motivated by the great success of SR for vector data. Though the space of SPD matrices is well-known to form a Lie group that is a Riemannian manifold, existing work fails to take full advantage of its geometric structure. This paper attempts to tackle this problem by proposing a kernel based method for SR and dictionary learning (DL) of SPD matrices. We disclose that the space of SPD matrices, with the operations of logarithmic multiplication and scalar logarithmic multiplication de?ned in the Log-Euclidean framework, is a complete inner product space. We can thus develop a broad family of kernels that satis?es Mercer’s condition. These kernels characterize the geodesic distance and can be computed ef?ciently. We also consider the geometric structure in the DL process by updating atom matrices in the Riemannian space instead of in the Euclidean space. The proposed method is evaluated with various vision problems and shows notable per- formance gains over state-of-the-arts.</p><p>2 0.8352347 <a title="257-lsi-2" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>Author: Hesamoddin Salehian, Guang Cheng, Baba C. Vemuri, Jeffrey Ho</p><p>Abstract: Symmetric positive-definite (SPD) matrices are ubiquitous in Computer Vision, Machine Learning and Medical Image Analysis. Finding the center/average of a population of such matrices is a common theme in many algorithms such as clustering, segmentation, principal geodesic analysis, etc. The center of a population of such matrices can be defined using a variety of distance/divergence measures as the minimizer of the sum of squared distances/divergences from the unknown center to the members of the population. It is well known that the computation of the Karcher mean for the space of SPD matrices which is a negativelycurved Riemannian manifold is computationally expensive. Recently, the LogDet divergence-based center was shown to be a computationally attractive alternative. However, the LogDet-based mean of more than two matrices can not be computed in closed form, which makes it computationally less attractive for large populations. In this paper we present a novel recursive estimator for center based on the Stein distance which is the square root of the LogDet di– vergence that is significantly faster than the batch mode computation of this center. The key theoretical contribution is a closed-form solution for the weighted Stein center of two SPD matrices, which is used in the recursive computation of the Stein center for a population of SPD matrices. Additionally, we show experimental evidence of the convergence of our recursive Stein center estimator to the batch mode Stein center. We present applications of our recursive estimator to K-means clustering and image indexing depicting significant time gains over corresponding algorithms that use the batch mode computations. For the latter application, we develop novel hashing functions using the Stein distance and apply it to publicly available data sets, and experimental results have shown favorable com– ∗This research was funded in part by the NIH grant NS066340 to BCV. †Corresponding author parisons to other competing methods.</p><p>3 0.74440885 <a title="257-lsi-3" href="./iccv-2013-A_Novel_Earth_Mover%27s_Distance_Methodology_for_Image_Matching_with_Gaussian_Mixture_Models.html">25 iccv-2013-A Novel Earth Mover's Distance Methodology for Image Matching with Gaussian Mixture Models</a></p>
<p>Author: Peihua Li, Qilong Wang, Lei Zhang</p><p>Abstract: The similarity or distance measure between Gaussian mixture models (GMMs) plays a crucial role in contentbased image matching. Though the Earth Mover’s Distance (EMD) has shown its advantages in matching histogram features, its potentials in matching GMMs remain unclear and are not fully explored. To address this problem, we propose a novel EMD methodology for GMM matching. We ?rst present a sparse representation based EMD called SR-EMD by exploiting the sparse property of the underlying problem. SR-EMD is more ef?cient and robust than the conventional EMD. Second, we present two novel ground distances between component Gaussians based on the information geometry. The perspective from the Riemannian geometry distinguishes the proposed ground distances from the classical entropy- or divergence-based ones. Furthermore, motivated by the success of distance metric learning of vector data, we make the ?rst attempt to learn the EMD distance metrics between GMMs by using a simple yet effective supervised pair-wise based method. It can adapt the distance metrics between GMMs to speci?c classi?ca- tion tasks. The proposed method is evaluated on both simulated data and benchmark real databases and achieves very promising performance.</p><p>4 0.52628148 <a title="257-lsi-4" href="./iccv-2013-Discriminant_Tracking_Using_Tensor_Representation_with_Semi-supervised_Improvement.html">119 iccv-2013-Discriminant Tracking Using Tensor Representation with Semi-supervised Improvement</a></p>
<p>Author: Jin Gao, Junliang Xing, Weiming Hu, Steve Maybank</p><p>Abstract: Visual tracking has witnessed growing methods in object representation, which is crucial to robust tracking. The dominant mechanism in object representation is using image features encoded in a vector as observations to perform tracking, without considering that an image is intrinsically a matrix, or a 2nd-order tensor. Thus approaches following this mechanism inevitably lose a lot of useful information, and therefore cannot fully exploit the spatial correlations within the 2D image ensembles. In this paper, we address an image as a 2nd-order tensor in its original form, and find a discriminative linear embedding space approximation to the original nonlinear submanifold embedded in the tensor space based on the graph embedding framework. We specially design two graphs for characterizing the intrinsic local geometrical structure of the tensor space, so as to retain more discriminant information when reducing the dimension along certain tensor dimensions. However, spatial correlations within a tensor are not limited to the elements along these dimensions. This means that some part of the discriminant information may not be encoded in the embedding space. We introduce a novel technique called semi-supervised improvement to iteratively adjust the embedding space to compensate for the loss of discriminant information, hence improving the performance of our tracker. Experimental results on challenging videos demonstrate the effectiveness and robustness of the proposed tracker.</p><p>5 0.51543099 <a title="257-lsi-5" href="./iccv-2013-Dictionary_Learning_and_Sparse_Coding_on_Grassmann_Manifolds%3A_An_Extrinsic_Solution.html">114 iccv-2013-Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution</a></p>
<p>Author: Mehrtash Harandi, Conrad Sanderson, Chunhua Shen, Brian Lovell</p><p>Abstract: Recent advances in computer vision and machine learning suggest that a wide range of problems can be addressed more appropriately by considering non-Euclidean geometry. In this paper we explore sparse dictionary learning over the space of linear subspaces, which form Riemannian structures known as Grassmann manifolds. To this end, we propose to embed Grassmann manifolds into the space of symmetric matrices by an isometric mapping, which enables us to devise a closed-form solution for updating a Grassmann dictionary, atom by atom. Furthermore, to handle non-linearity in data, we propose a kernelised version of the dictionary learning algorithm. Experiments on several classification tasks (face recognition, action recognition, dynamic texture classification) show that the proposed approach achieves considerable improvements in discrimination accuracy, in comparison to state-of-the-art methods such as kernelised Affine Hull Method and graphembedding Grassmann discriminant analysis.</p><p>6 0.481406 <a title="257-lsi-6" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>7 0.4708758 <a title="257-lsi-7" href="./iccv-2013-Robust_Tucker_Tensor_Decomposition_for_Effective_Image_Representation.html">362 iccv-2013-Robust Tucker Tensor Decomposition for Effective Image Representation</a></p>
<p>8 0.44750217 <a title="257-lsi-8" href="./iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning.html">212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</a></p>
<p>9 0.43457428 <a title="257-lsi-9" href="./iccv-2013-Joint_Deep_Learning_for_Pedestrian_Detection.html">220 iccv-2013-Joint Deep Learning for Pedestrian Detection</a></p>
<p>10 0.43373683 <a title="257-lsi-10" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>11 0.42812276 <a title="257-lsi-11" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>12 0.42642114 <a title="257-lsi-12" href="./iccv-2013-Parallel_Transport_of_Deformations_in_Shape_Space_of_Elastic_Surfaces.html">307 iccv-2013-Parallel Transport of Deformations in Shape Space of Elastic Surfaces</a></p>
<p>13 0.3960785 <a title="257-lsi-13" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>14 0.39316225 <a title="257-lsi-14" href="./iccv-2013-Nonparametric_Blind_Super-resolution.html">293 iccv-2013-Nonparametric Blind Super-resolution</a></p>
<p>15 0.38750204 <a title="257-lsi-15" href="./iccv-2013-Automatic_Kronecker_Product_Model_Based_Detection_of_Repeated_Patterns_in_2D_Urban_Images.html">55 iccv-2013-Automatic Kronecker Product Model Based Detection of Repeated Patterns in 2D Urban Images</a></p>
<p>16 0.37682685 <a title="257-lsi-16" href="./iccv-2013-From_Point_to_Set%3A_Extend_the_Learning_of_Distance_Metrics.html">177 iccv-2013-From Point to Set: Extend the Learning of Distance Metrics</a></p>
<p>17 0.35235864 <a title="257-lsi-17" href="./iccv-2013-Efficient_Higher-Order_Clustering_on_the_Grassmann_Manifold.html">134 iccv-2013-Efficient Higher-Order Clustering on the Grassmann Manifold</a></p>
<p>18 0.34479752 <a title="257-lsi-18" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>19 0.34406993 <a title="257-lsi-19" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>20 0.3362 <a title="257-lsi-20" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.049), (7, 0.027), (13, 0.011), (26, 0.09), (31, 0.041), (41, 0.241), (42, 0.113), (48, 0.021), (64, 0.028), (73, 0.031), (89, 0.121), (95, 0.051), (97, 0.036), (98, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77257937 <a title="257-lda-1" href="./iccv-2013-Log-Euclidean_Kernels_for_Sparse_Representation_and_Dictionary_Learning.html">257 iccv-2013-Log-Euclidean Kernels for Sparse Representation and Dictionary Learning</a></p>
<p>Author: Peihua Li, Qilong Wang, Wangmeng Zuo, Lei Zhang</p><p>Abstract: The symmetric positive de?nite (SPD) matrices have been widely used in image and vision problems. Recently there are growing interests in studying sparse representation (SR) of SPD matrices, motivated by the great success of SR for vector data. Though the space of SPD matrices is well-known to form a Lie group that is a Riemannian manifold, existing work fails to take full advantage of its geometric structure. This paper attempts to tackle this problem by proposing a kernel based method for SR and dictionary learning (DL) of SPD matrices. We disclose that the space of SPD matrices, with the operations of logarithmic multiplication and scalar logarithmic multiplication de?ned in the Log-Euclidean framework, is a complete inner product space. We can thus develop a broad family of kernels that satis?es Mercer’s condition. These kernels characterize the geodesic distance and can be computed ef?ciently. We also consider the geometric structure in the DL process by updating atom matrices in the Riemannian space instead of in the Euclidean space. The proposed method is evaluated with various vision problems and shows notable per- formance gains over state-of-the-arts.</p><p>2 0.69413102 <a title="257-lda-2" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>Author: Kristina Scherbaum, James Petterson, Rogerio S. Feris, Volker Blanz, Hans-Peter Seidel</p><p>Abstract: Face detection is an important task in computer vision and often serves as the first step for a variety of applications. State-of-the-art approaches use efficient learning algorithms and train on large amounts of manually labeled imagery. Acquiring appropriate training images, however, is very time-consuming and does not guarantee that the collected training data is representative in terms of data variability. Moreover, available data sets are often acquired under controlled settings, restricting, for example, scene illumination or 3D head pose to a narrow range. This paper takes a look into the automated generation of adaptive training samples from a 3D morphable face model. Using statistical insights, the tailored training data guarantees full data variability and is enriched by arbitrary facial attributes such as age or body weight. Moreover, it can automatically adapt to environmental constraints, such as illumination or viewing angle of recorded video footage from surveillance cameras. We use the tailored imagery to train a new many-core imple- mentation of Viola Jones ’ AdaBoost object detection framework. The new implementation is not only faster but also enables the use of multiple feature channels such as color features at training time. In our experiments we trained seven view-dependent face detectors and evaluate these on the Face Detection Data Set and Benchmark (FDDB). Our experiments show that the use of tailored training imagery outperforms state-of-the-art approaches on this challenging dataset.</p><p>3 0.6654321 <a title="257-lda-3" href="./iccv-2013-Abnormal_Event_Detection_at_150_FPS_in_MATLAB.html">34 iccv-2013-Abnormal Event Detection at 150 FPS in MATLAB</a></p>
<p>Author: Cewu Lu, Jianping Shi, Jiaya Jia</p><p>Abstract: Speedy abnormal event detection meets the growing demand to process an enormous number of surveillance videos. Based on inherent redundancy of video structures, we propose an efficient sparse combination learning framework. It achieves decent performance in the detection phase without compromising result quality. The short running time is guaranteed because the new method effectively turns the original complicated problem to one in which only a few costless small-scale least square optimization steps are involved. Our method reaches high detection rates on benchmark datasets at a speed of 140∼150 frames per soenc obnednc on average wsehtesn a computing on an ordinary desktop PC using MATLAB.</p><p>4 0.65899235 <a title="257-lda-4" href="./iccv-2013-A_Novel_Earth_Mover%27s_Distance_Methodology_for_Image_Matching_with_Gaussian_Mixture_Models.html">25 iccv-2013-A Novel Earth Mover's Distance Methodology for Image Matching with Gaussian Mixture Models</a></p>
<p>Author: Peihua Li, Qilong Wang, Lei Zhang</p><p>Abstract: The similarity or distance measure between Gaussian mixture models (GMMs) plays a crucial role in contentbased image matching. Though the Earth Mover’s Distance (EMD) has shown its advantages in matching histogram features, its potentials in matching GMMs remain unclear and are not fully explored. To address this problem, we propose a novel EMD methodology for GMM matching. We ?rst present a sparse representation based EMD called SR-EMD by exploiting the sparse property of the underlying problem. SR-EMD is more ef?cient and robust than the conventional EMD. Second, we present two novel ground distances between component Gaussians based on the information geometry. The perspective from the Riemannian geometry distinguishes the proposed ground distances from the classical entropy- or divergence-based ones. Furthermore, motivated by the success of distance metric learning of vector data, we make the ?rst attempt to learn the EMD distance metrics between GMMs by using a simple yet effective supervised pair-wise based method. It can adapt the distance metrics between GMMs to speci?c classi?ca- tion tasks. The proposed method is evaluated on both simulated data and benchmark real databases and achieves very promising performance.</p><p>5 0.63238573 <a title="257-lda-5" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>Author: Jungseock Joo, Shuo Wang, Song-Chun Zhu</p><p>Abstract: We present a part-based approach to the problem of human attribute recognition from a single image of a human body. To recognize the attributes of human from the body parts, it is important to reliably detect the parts. This is a challenging task due to the geometric variation such as articulation and view-point changes as well as the appearance variation of the parts arisen from versatile clothing types. The prior works have primarily focused on handling . edu . cn ???????????? geometric variation by relying on pre-trained part detectors or pose estimators, which require manual part annotation, but the appearance variation has been relatively neglected in these works. This paper explores the importance of the appearance variation, which is directly related to the main task, attribute recognition. To this end, we propose to learn a rich appearance part dictionary of human with significantly less supervision by decomposing image lattice into overlapping windows at multiscale and iteratively refining local appearance templates. We also present quantitative results in which our proposed method outperforms the existing approaches.</p><p>6 0.62911963 <a title="257-lda-6" href="./iccv-2013-GOSUS%3A_Grassmannian_Online_Subspace_Updates_with_Structured-Sparsity.html">182 iccv-2013-GOSUS: Grassmannian Online Subspace Updates with Structured-Sparsity</a></p>
<p>7 0.62664789 <a title="257-lda-7" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>8 0.62417686 <a title="257-lda-8" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>9 0.62077528 <a title="257-lda-9" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>10 0.62044632 <a title="257-lda-10" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>11 0.61918432 <a title="257-lda-11" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>12 0.61832821 <a title="257-lda-12" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>13 0.61825228 <a title="257-lda-13" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>14 0.61728126 <a title="257-lda-14" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>15 0.61530441 <a title="257-lda-15" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>16 0.61445576 <a title="257-lda-16" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>17 0.61442 <a title="257-lda-17" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>18 0.61406296 <a title="257-lda-18" href="./iccv-2013-Joint_Deep_Learning_for_Pedestrian_Detection.html">220 iccv-2013-Joint Deep Learning for Pedestrian Detection</a></p>
<p>19 0.61400157 <a title="257-lda-19" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>20 0.61249793 <a title="257-lda-20" href="./iccv-2013-Dictionary_Learning_and_Sparse_Coding_on_Grassmann_Manifolds%3A_An_Extrinsic_Solution.html">114 iccv-2013-Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
