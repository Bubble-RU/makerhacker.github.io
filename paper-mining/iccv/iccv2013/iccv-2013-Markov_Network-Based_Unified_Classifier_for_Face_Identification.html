<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>261 iccv-2013-Markov Network-Based Unified Classifier for Face Identification</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-261" href="#">iccv2013-261</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>261 iccv-2013-Markov Network-Based Unified Classifier for Face Identification</h1>
<br/><p>Source: <a title="iccv-2013-261-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Hwang_Markov_Network-Based_Unified_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Wonjun Hwang, Kyungshik Roh, Junmo Kim</p><p>Abstract: We propose a novel unifying framework using a Markov network to learn the relationship between multiple classifiers in face recognition. We assume that we have several complementary classifiers and assign observation nodes to the features of a query image and hidden nodes to the features of gallery images. We connect each hidden node to its corresponding observation node and to the hidden nodes of other neighboring classifiers. For each observation-hidden node pair, we collect a set of gallery candidates that are most similar to the observation instance, and the relationship between the hidden nodes is captured in terms of the similarity matrix between the collected gallery images. Posterior probabilities in the hidden nodes are computed by the belief-propagation algorithm. The novelty of the proposed framework is the method that takes into account the classifier dependency using the results of each neighboring classifier. We present extensive results on two different evaluation protocols, known and unknown image variation tests, using three different databases, which shows that the proposed framework always leads to good accuracy in face recognition.</p><p>Reference: <a title="iccv-2013-261-reference" href="../iccv2013_reference/iccv-2013-Markov_Network-Based_Unified_Classifier_for_Face_Identification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 kr st  Abstract We propose a novel unifying framework using a Markov network to learn the relationship between multiple classifiers in face recognition. [sent-9, score-0.445]
</p><p>2 We assume that we have several complementary classifiers and assign observation nodes to the features of a query image and hidden nodes to the features of gallery images. [sent-10, score-0.937]
</p><p>3 We connect each hidden node to its corresponding observation node and to the hidden nodes of other neighboring classifiers. [sent-11, score-0.545]
</p><p>4 For each observation-hidden node pair, we collect a set of gallery candidates that are most similar to the observation instance, and the relationship between the hidden nodes is captured in terms of the similarity matrix between the collected gallery images. [sent-12, score-1.102]
</p><p>5 Posterior probabilities in the hidden nodes are computed by the belief-propagation algorithm. [sent-13, score-0.197]
</p><p>6 We present extensive results on two different evaluation protocols, known and unknown image variation tests, using three different databases, which shows that the proposed framework always leads to good accuracy in face  recognition. [sent-15, score-0.199]
</p><p>7 Introduction When we use a face image as a query, we can retrieve several desired face images from a large image database. [sent-17, score-0.24]
</p><p>8 We calculate many similarities of the query image and the gallery images in the database, and the retrieved gallery images are ranked by similar orders. [sent-18, score-0.795]
</p><p>9 It is a one-to-many identification problem [8] and has many applications such as searching similar face images in a database and face tagging in images and videos. [sent-19, score-0.336]
</p><p>10 However, the traditional face recognition algorithms [10][14] have been developed for one-to-one verification [8], in particular, a biometric task. [sent-20, score-0.274]
</p><p>11 Recent successful face recognition methods have attempted to merge several classifiers using multiple feature sets of different characteristics, as in component-based methods, which extract features from separate spatial re-  Figure 1. [sent-21, score-0.395]
</p><p>12 These methods used the classifiers not only based on the different feature sets but also trained independently, and the similarity scores are merged with the predefined parameters. [sent-27, score-0.272]
</p><p>13 Note that these methods lead to good accuracy in face verification, but there is no specific framework for the  one-to-many identification problem. [sent-29, score-0.211]
</p><p>14 First, we assume that we have multiple classifiers that have complementary characteristics. [sent-31, score-0.217]
</p><p>15 We can unify the multiple classifiers based not on the predefined weight values but on a Markov network, as summarized in Figure 2. [sent-32, score-0.217]
</p><p>16 For this purpose, we assign one node of a Markov network to each classifier. [sent-33, score-0.161]
</p><p>17 At its paired hidden node, we retrieve the first n similar gallery 11995522  Figure 2. [sent-36, score-0.439]
</p><p>18 N multiple classifiers are deployed under (a) the traditional recognition framework and (b) the proposed recognition  framework using a Markov network. [sent-37, score-0.363]
</p><p>19 Observation node y is assigned by the feature of a query image and hidden node x is assigned by the feature of a gallery image. [sent-39, score-0.721]
</p><p>20 samples from the database, and their orders are made by the similarity scores for the query face image. [sent-42, score-0.214]
</p><p>21 The multiple classifiers have their own lists of retrieved gallery images, which are not identical in general, thereby complementing the neighbor classifiers. [sent-43, score-0.633]
</p><p>22 Because the hidden nodes are connected by the network lines, the relationship of the connected nodes is learned by the similarity scores between the neighbor classifiers, and the scores are calculated by concatenating the two gallery features of the neighbor classifiers. [sent-44, score-0.867]
</p><p>23 The posterior probability at each hidden node is easily computed by the belief-propagation algorithm. [sent-45, score-0.235]
</p><p>24 Our main contribution is the novel recognition framework that successfully organizes the classifier relationship using the similarity between the top ranked gallery images  of the corresponding classifiers. [sent-48, score-0.527]
</p><p>25 Its own node characteristics are iteratively propagated to other neighbor nodes. [sent-49, score-0.178]
</p><p>26 As a result, all nodes are correlated to others, which improves the recognition results in a one-to-many identification task. [sent-50, score-0.197]
</p><p>27 Related Work The performances of recent face recognition algorithms have gradually advanced, which is largely due to techniques that merge several classifiers of different characteristics [21][20][9]. [sent-56, score-0.469]
</p><p>28 In this paper, we use Gabor feature-based classifiers as representative classifiers, but the proposed framework is applicable to any other classifiers. [sent-58, score-0.239]
</p><p>29 [3] proposed a hybrid face recognition method that combines holistic and feature analysis-based approaches using a Markov random  field (MRF) model. [sent-60, score-0.177]
</p><p>30 On the other hand, we use the Markov network to apply the relationship between the multiple classifiers to the face recognition scheme and propose a similarity-based compatibility function between the neighbor classifiers for the identification task. [sent-68, score-0.908]
</p><p>31 However, in this paper, given a query image, y, we would like to identify the most similar gallery image, x, from the enrolled image set in a one-to-many face identification scenario. [sent-73, score-0.591]
</p><p>32 The belief-propagation algorithm iteratively updates the message mij from the node ito the node j,and the equation is as follows: mij(xj) =  ? [sent-81, score-0.294]
</p><p>33 =j  where mij (xj) is an element of the vector mij corresponding to the gallery candidate xj . [sent-86, score-0.465]
</p><p>34 The marginal probability bi  for gallery  xi  at node iis derived by  bi(xi) =  ? [sent-87, score-0.554]
</p><p>35 Node In [3], an image was divided into several patches and nodes were assigned to these patches under the Markov assumption, but in this paper, we assign the nodes to the feature vectors of the multiple classifiers. [sent-93, score-0.184]
</p><p>36 For example, the query feature and the gallery feature are assigned to the observation node, y, and the hidden node, x, respectively, and we design the parallel network without loops for simplicity, as shown in Figure 2 (b). [sent-94, score-0.59]
</p><p>37 In this respect, each classifier is influenced by two neighbor classifiers under the Markov assumption, and we can finally have N observation and hidden nodes pairs. [sent-95, score-0.543]
</p><p>38 A pair of the observation and the hidden nodes might be assumed as traditional face recognition, but the hidden nodes are connected by the network lines. [sent-96, score-0.648]
</p><p>39 At each hidden node, we collect n gallery candidates that are most similar to the observation feature. [sent-97, score-0.511]
</p><p>40 The hidden nodes have their own sets of gallery candidates retrieved from the database, and the n candidates ofeach node are not the same as the others because we assume that the multiple classifiers have different characteristics. [sent-98, score-0.978]
</p><p>41 For example, when assuming that there are two classifiers, we can have two sets of the  n first ranked gallery candidates, and the relationship between hidden node x1 and x2 can be evaluated for pairs of realizations, (x1p, x2q), 1 ≤ p ≤ n, 1 ≤ q ≤ n. [sent-99, score-0.6]
</p><p>42 In detail, x11, the first ranke)d, 1ele ≤me pnt ≤ ≤of n nt,he1 gallery ca nn. [sent-100, score-0.334]
</p><p>43 The similarity between instances x1p and x2q of different hidden nodes x1 and x2 is computed by comparing two concatenated features, = [fx1p fxp] and = [fxq fx2q], where x1p and x2q are from the retrieved gallery sets at the first and second hidden nodes, respectively. [sent-103, score-0.722]
</p><p>44 We generate the neighbor features, fxp and fxq, using the neighbor classifier (red), and we add them to the main features, fx1p and fx2q , (blue), in order. [sent-104, score-0.18]
</p><p>45 Compatibility Function  For a given observation, yi, the query-gallery compatibility function, Φ(xi , yi), is evaluated for n gallery candidates of xi, {xi1, . [sent-108, score-0.495]
</p><p>46 Th,e { gallery-gallery compatibility fgun act vioecn,Ψtor(x ini, xj), is evaluated for n n pairs of (xi , xj), where xi takes v)a,l uise on l{uxaite1 , . [sent-113, score-0.172]
</p><p>47 orrelation between two feature vectors as a measure of similarity, the n most similar gallery images are retrieved at each hidden node. [sent-122, score-0.476]
</p><p>48 We define the compatibility function between the hidden nodes iand j as, ψij(xi, xj) = exp(−|sij(xi, xj) − 1|2/2σ2),  (4)  where σ is a noise parameter. [sent-123, score-0.34]
</p><p>49 wH wowe neveeedr, xip and xjq are from different face classifiers, and we cannot directly compare their corresponding features, fxip and fxjq . [sent-126, score-0.35]
</p><p>50 To address this problem, we propose the concatenated features, and which version of fxip and fxjq . [sent-127, score-0.161]
</p><p>51 The similarity between iand j nodes is measured by the normalized correlation coefficient between the concatenated features:  fxip  fxjq,  sij(xip,xjq) =<  are an augmented  fxip,fxjq  >  /||fxip||||fxjq||. [sent-129, score-0.235]
</p><p>52 Given a query image at node i, n gallery images that are most similar to the query image, yi, are retrieved. [sent-132, score-0.577]
</p><p>53 The query-gallery compatibility function at node iis evaluated and represented by a column vector Φi. [sent-133, score-0.225]
</p><p>54 The gallery-gallery compatibility function between the gallery node iand the neighbor node j is computed for each pair of the candidates and is represented by a matrix, Ψij . [sent-134, score-0.779]
</p><p>55 (10)  As the posterior probability p(xi |y1, · · · , yN) is proportional to bi (xi) at the hidden n|yode, xi, we have a chance to use these N marginal probabilities such as b1(x1) , b2 (x2) , · · · , bN (xN). [sent-156, score-0.187]
</p><p>56 e 4, where the message that comes from the neigh-  bor node ito the current node j corresponds to the column Table 1. [sent-161, score-0.248]
</p><p>57 Each element in the matrix (blue area) is a measure of similarity between a gallery image and a gallery image of another node. [sent-169, score-0.694]
</p><p>58 If the two gallery images are similar enough to result in a large term in the matrix, the corresponding term in the column vector (red area) receives more emphasis. [sent-170, score-0.334]
</p><p>59 For example, suppose we would like to compare the query image and gallery  image 1. [sent-171, score-0.402]
</p><p>60 If gallery image 1 is somehow similar to another gallery image, say, image 2, then the idea is that we will use the similarity between image 2 and the query to update the similarity between image 1 and the query. [sent-172, score-0.788]
</p><p>61 In this protocol, the multiple classifiers are trained by the FRGC training image set, and the test sets consist of the FRGC test images whose conditions are similar to that of the training images. [sent-179, score-0.271]
</p><p>62 In the other protocol, as described in Table 2, we train the multiple classifiers using only the FRGC training set, and the two test sets are composed of XM2VTS (XvX) and BANCA images (BvB), respectively, where we expect to observe performance changes according to the un-trained image variations. [sent-185, score-0.244]
</p><p>63 In the end, face recognition accuracy is calculated by the 1st rank of the Cumulative Match Characteristics (CMC) curve as a measurement of one-to-many face identification [8]. [sent-187, score-0.345]
</p><p>64 Gabor LDA-based Classifier To evaluate the generalizability of the proposed recognition framework, we employ two different Gabor LDA-based classifiers such as the RSG classifiers, proposed in this paper, and the ECG classifiers [4], respectively. [sent-190, score-0.494]
</p><p>65 In this paper, we build the ten RSG classifiers as the multiple classifiers (N = 10). [sent-198, score-0.479]
</p><p>66 They finally merged the multiple classifiers and achieved the best result among the Gabor-  example, the BGL and ECG classifiers and ten RSG classifiers, are shown under the traditional and the proposed frameworks in the (a) CvC and (b) UvC tests. [sent-201, score-0.564]
</p><p>67 The average accuracy of combining different numbers of RSG classifiers under the traditional and proposed frameworks. [sent-204, score-0.247]
</p><p>68 The score fusion rule is the sum rule and the test protocol is the UvC test. [sent-205, score-0.223]
</p><p>69 In this paper, we employ twelve ECG classifiers (N = 12) for the full performance without consideration of the computational complexity. [sent-207, score-0.254]
</p><p>70 As shown in Figure 6, the average recognition rates of the RSG classifiers range from 63. [sent-211, score-0.253]
</p><p>71 75% (2nd classifier), but the BGL classifier and the ECG classifiers achieve an average of 72. [sent-213, score-0.272]
</p><p>72 Note that all single RSG classifiers of the unified framework improve the average accuracy by approximately 12–13% with the UvC test compared to the original single RSG classifiers. [sent-218, score-0.266]
</p><p>73 Moreover, all single RSG classifiers of the proposed method show better accuracy than the BGL and ECG classifiers, and similar improvement is also found in the CvC test. [sent-219, score-0.217]
</p><p>74 11995566  RSG classifiers as a function of the size of the retrieval images, n, in the UvC test. [sent-220, score-0.217]
</p><p>75 As shown in Figure 7, when the number of classifiers increases, the accuracy also increases in both the traditional and proposed methods. [sent-225, score-0.247]
</p><p>76 In this paper, we use the sum rule [7] to combine RSG classifiers for simplicity, but more complex combination algorithms may further improve system performance. [sent-227, score-0.267]
</p><p>77 The performances of the proposed method are compared with the Gabor-based classifiers such as the BGL and ECG classifiers. [sent-235, score-0.265]
</p><p>78 , MIN, MAX, and Sum) [7], RANK based fusion [11], likelihood rate (LR) based fusion [16], Gaussian mixture model-based LR (GMLR) method [13], and fisher classifier (LDA) based score fusion method [18]. [sent-243, score-0.244]
</p><p>79 When compared with the performances of the other Gabor-based methods such as BGL and ECG classifiers, as shown in Table 3, the twelve ECG classifiers fused by the LR method show an average of 85. [sent-254, score-0.302]
</p><p>80 21% average of the ten RSG classifiers fused by the sum rule. [sent-257, score-0.286]
</p><p>81 On the other hand, the proposed recognition framework works successfully for the RSG classifiers and ECG classifiers. [sent-263, score-0.275]
</p><p>82 For example, using the proposed recognition framework, the recognition rate of the RSG classifiers is boosted from 82. [sent-264, score-0.313]
</p><p>83 18% and the recognition rate of the ECG classifiers is boosted from 85. [sent-266, score-0.277]
</p><p>84 From this result, we can conclude that the proposed method efficiently combines multiple classifiers using the classifier relationship in the known variation test. [sent-269, score-0.334]
</p><p>85 CMC curves corresponding to the well-known recognition methods and the proposed framework using RSG classifiers and ECG classifiers in the (a) XvX test and (b) BvB test, respectively. [sent-275, score-0.519]
</p><p>86 sifiers merged by sum rule, denoted by RSG (Sum), the twelve ECG classifiers merged by the LR method, denoted by ECG (LR), and the proposed frameworks with the RSG and ECG classifiers, respectively, in the XvX and BvB tests. [sent-277, score-0.362]
</p><p>87 In detail, the RSG and ECG classifiers show similar recognition rates, approximately 82%, in the XvX test, but the ECG classifiers achieve a 7% better recognition rate than the RSG classifiers in the BvB test. [sent-280, score-0.723]
</p><p>88 Note that compared to the performances of the proposed method in the known variation test protocol (i. [sent-285, score-0.162]
</p><p>89 Conclusion We propose a novel face recognition framework, particularly for the one-to-many identification task, based on multiple classifiers connected by a Markov network. [sent-294, score-0.463]
</p><p>90 The Markov network probabilistically models the relationships between a query and gallery images and between neighboring gallery images. [sent-295, score-0.79]
</p><p>91 From the viewpoint of an observation-hidden node pair, we retrieve the most similar gallery images from the database using a query image face model. [sent-296, score-0.656]
</p><p>92 The statistical dependency between the hidden nodes is calculated by the similarities between the retrieved gallery images. [sent-297, score-0.589]
</p><p>93 We prove the good performances of the proposed framework using RSG classifiers and ECG classifiers, respectively. [sent-299, score-0.287]
</p><p>94 Moreover, we  have confirmed the generality of the proposed method with the known variation test and unknown variation test protocols which consist of three different databases: the FRGC, XM2VTS, and BANCA databases. [sent-300, score-0.182]
</p><p>95 A hybrid face recognition method using markov random fields. [sent-333, score-0.256]
</p><p>96 Face recognition system using extended curvature gabor classifier bunch for low-resolution face image. [sent-341, score-0.338]
</p><p>97 Face recognition system using multiple face model of hybrid fourier feature under uncontrolled illumination variation. [sent-350, score-0.201]
</p><p>98 Unconstrained face recognition using mrf priors and manifold traversing. [sent-462, score-0.183]
</p><p>99 Hierarchical ensemble of global and local classifiers for face recognition. [sent-484, score-0.337]
</p><p>100 Fusing gabor and lbp feature set for kernel-based face recognition. [sent-490, score-0.247]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rsg', 0.553), ('ecg', 0.415), ('gallery', 0.334), ('classifiers', 0.217), ('bgl', 0.19), ('uvc', 0.156), ('frgc', 0.147), ('gabor', 0.127), ('bvb', 0.121), ('xvx', 0.121), ('face', 0.12), ('compatibility', 0.118), ('node', 0.107), ('hidden', 0.105), ('banca', 0.104), ('nodes', 0.092), ('markov', 0.079), ('identification', 0.069), ('fxip', 0.069), ('fxjq', 0.069), ('query', 0.068), ('fusion', 0.063), ('protocol', 0.057), ('biometric', 0.057), ('classifier', 0.055), ('network', 0.054), ('xi', 0.054), ('xjq', 0.052), ('korea', 0.051), ('cmc', 0.049), ('performances', 0.048), ('lda', 0.048), ('cvc', 0.047), ('ver', 0.047), ('lr', 0.047), ('mij', 0.046), ('neighbor', 0.045), ('ten', 0.045), ('candidates', 0.043), ('hwang', 0.042), ('protocols', 0.041), ('xip', 0.04), ('xj', 0.039), ('kittler', 0.038), ('twelve', 0.037), ('retrieved', 0.037), ('recognition', 0.036), ('marginal', 0.035), ('fxik', 0.035), ('fxp', 0.035), ('fxq', 0.035), ('localities', 0.035), ('messer', 0.035), ('mki', 0.035), ('protocoldatabaseindividualimage', 0.035), ('message', 0.034), ('relationship', 0.032), ('fyi', 0.031), ('authentication', 0.031), ('marcel', 0.031), ('verification', 0.031), ('sij', 0.03), ('variation', 0.03), ('traditional', 0.03), ('observation', 0.029), ('merged', 0.029), ('rodrigues', 0.028), ('unknown', 0.027), ('test', 0.027), ('international', 0.027), ('database', 0.027), ('mrf', 0.027), ('rule', 0.026), ('similarity', 0.026), ('characteristics', 0.026), ('frameworks', 0.026), ('iand', 0.025), ('kee', 0.024), ('unexpected', 0.024), ('boosted', 0.024), ('uncontrolled', 0.024), ('bi', 0.024), ('ij', 0.024), ('shan', 0.024), ('sum', 0.024), ('generalizability', 0.024), ('posterior', 0.023), ('controlled', 0.023), ('concatenated', 0.023), ('ranked', 0.022), ('biometrics', 0.022), ('merge', 0.022), ('framework', 0.022), ('gestures', 0.022), ('dependency', 0.021), ('hybrid', 0.021), ('bg', 0.021), ('vr', 0.021), ('tests', 0.021), ('connected', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="261-tfidf-1" href="./iccv-2013-Markov_Network-Based_Unified_Classifier_for_Face_Identification.html">261 iccv-2013-Markov Network-Based Unified Classifier for Face Identification</a></p>
<p>Author: Wonjun Hwang, Kyungshik Roh, Junmo Kim</p><p>Abstract: We propose a novel unifying framework using a Markov network to learn the relationship between multiple classifiers in face recognition. We assume that we have several complementary classifiers and assign observation nodes to the features of a query image and hidden nodes to the features of gallery images. We connect each hidden node to its corresponding observation node and to the hidden nodes of other neighboring classifiers. For each observation-hidden node pair, we collect a set of gallery candidates that are most similar to the observation instance, and the relationship between the hidden nodes is captured in terms of the similarity matrix between the collected gallery images. Posterior probabilities in the hidden nodes are computed by the belief-propagation algorithm. The novelty of the proposed framework is the method that takes into account the classifier dependency using the results of each neighboring classifier. We present extensive results on two different evaluation protocols, known and unknown image variation tests, using three different databases, which shows that the proposed framework always leads to good accuracy in face recognition.</p><p>2 0.20023799 <a title="261-tfidf-2" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>Author: Renliang Weng, Jiwen Lu, Junlin Hu, Gao Yang, Yap-Peng Tan</p><p>Abstract: Over the past two decades, a number of face recognition methods have been proposed in the literature. Most of them use holistic face images to recognize people. However, human faces are easily occluded by other objects in many real-world scenarios and we have to recognize the person of interest from his/her partial faces. In this paper, we propose a new partial face recognition approach by using feature set matching, which is able to align partial face patches to holistic gallery faces automatically and is robust to occlusions and illumination changes. Given each gallery image and probe face patch, we first detect keypoints and extract their local features. Then, we propose a Metric Learned ExtendedRobust PointMatching (MLERPM) method to discriminatively match local feature sets of a pair of gallery and probe samples. Lastly, the similarity of two faces is converted as the distance between two feature sets. Experimental results on three public face databases are presented to show the effectiveness of the proposed approach.</p><p>3 0.17646554 <a title="261-tfidf-3" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>Author: Meng Yang, Luc Van_Gool, Lei Zhang</p><p>Abstract: Face recognition (FR) with a single training sample per person (STSPP) is a very challenging problem due to the lack of information to predict the variations in the query sample. Sparse representation based classification has shown interesting results in robust FR; however, its performance will deteriorate much for FR with STSPP. To address this issue, in this paper we learn a sparse variation dictionary from a generic training set to improve the query sample representation by STSPP. Instead of learning from the generic training set independently w.r.t. the gallery set, the proposed sparse variation dictionary learning (SVDL) method is adaptive to the gallery set by jointly learning a projection to connect the generic training set with the gallery set. The learnt sparse variation dictionary can be easily integrated into the framework of sparse representation based classification so that various variations in face images, including illumination, expression, occlusion, pose, etc., can be better handled. Experiments on the large-scale CMU Multi-PIE, FRGC and LFW databases demonstrate the promising performance of SVDL on FR with STSPP.</p><p>4 0.17168029 <a title="261-tfidf-4" href="./iccv-2013-Coupling_Alignments_with_Recognition_for_Still-to-Video_Face_Recognition.html">97 iccv-2013-Coupling Alignments with Recognition for Still-to-Video Face Recognition</a></p>
<p>Author: Zhiwu Huang, Xiaowei Zhao, Shiguang Shan, Ruiping Wang, Xilin Chen</p><p>Abstract: The Still-to-Video (S2V) face recognition systems typically need to match faces in low-quality videos captured under unconstrained conditions against high quality still face images, which is very challenging because of noise, image blur, lowface resolutions, varying headpose, complex lighting, and alignment difficulty. To address the problem, one solution is to select the frames of ‘best quality ’ from videos (hereinafter called quality alignment in this paper). Meanwhile, the faces in the selected frames should also be geometrically aligned to the still faces offline well-aligned in the gallery. In this paper, we discover that the interactions among the three tasks–quality alignment, geometric alignment and face recognition–can benefit from each other, thus should be performed jointly. With this in mind, we propose a Coupling Alignments with Recognition (CAR) method to tightly couple these tasks via low-rank regularized sparse representation in a unified framework. Our method makes the three tasks promote mutually by a joint optimization in an Augmented Lagrange Multiplier routine. Extensive , experiments on two challenging S2V datasets demonstrate that our method outperforms the state-of-the-art methods impressively.</p><p>5 0.15270856 <a title="261-tfidf-5" href="./iccv-2013-POP%3A_Person_Re-identification_Post-rank_Optimisation.html">305 iccv-2013-POP: Person Re-identification Post-rank Optimisation</a></p>
<p>Author: Chunxiao Liu, Chen Change Loy, Shaogang Gong, Guijin Wang</p><p>Abstract: Owing to visual ambiguities and disparities, person reidentification methods inevitably produce suboptimal ranklist, which still requires exhaustive human eyeballing to identify the correct target from hundreds of different likelycandidates. Existing re-identification studies focus on improving the ranking performance, but rarely look into the critical problem of optimising the time-consuming and error-prone post-rank visual search at the user end. In this study, we present a novel one-shot Post-rank OPtimisation (POP) method, which allows a user to quickly refine their search by either “one-shot” or a couple of sparse negative selections during a re-identification process. We conduct systematic behavioural studies to understand user’s searching behaviour and show that the proposed method allows correct re-identification to converge 2.6 times faster than the conventional exhaustive search. Importantly, through extensive evaluations we demonstrate that the method is capable of achieving significant improvement over the stateof-the-art distance metric learning based ranking models, even with just “one shot” feedback optimisation, by as much as over 30% performance improvement for rank 1reidentification on the VIPeR and i-LIDS datasets.</p><p>6 0.12691557 <a title="261-tfidf-6" href="./iccv-2013-Handling_Occlusions_with_Franken-Classifiers.html">190 iccv-2013-Handling Occlusions with Franken-Classifiers</a></p>
<p>7 0.11853907 <a title="261-tfidf-7" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>8 0.091943927 <a title="261-tfidf-8" href="./iccv-2013-Deep_Learning_Identity-Preserving_Face_Space.html">106 iccv-2013-Deep Learning Identity-Preserving Face Space</a></p>
<p>9 0.089239962 <a title="261-tfidf-9" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>10 0.08628203 <a title="261-tfidf-10" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>11 0.084846526 <a title="261-tfidf-11" href="./iccv-2013-Multi-stage_Contextual_Deep_Learning_for_Pedestrian_Detection.html">279 iccv-2013-Multi-stage Contextual Deep Learning for Pedestrian Detection</a></p>
<p>12 0.083218828 <a title="261-tfidf-12" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>13 0.074808232 <a title="261-tfidf-13" href="./iccv-2013-Hidden_Factor_Analysis_for_Age_Invariant_Face_Recognition.html">195 iccv-2013-Hidden Factor Analysis for Age Invariant Face Recognition</a></p>
<p>14 0.072568551 <a title="261-tfidf-14" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>15 0.068353213 <a title="261-tfidf-15" href="./iccv-2013-Find_the_Best_Path%3A_An_Efficient_and_Accurate_Classifier_for_Image_Hierarchies.html">165 iccv-2013-Find the Best Path: An Efficient and Accurate Classifier for Image Hierarchies</a></p>
<p>16 0.066594556 <a title="261-tfidf-16" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>17 0.065197125 <a title="261-tfidf-17" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>18 0.063596196 <a title="261-tfidf-18" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>19 0.062649861 <a title="261-tfidf-19" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>20 0.061018109 <a title="261-tfidf-20" href="./iccv-2013-Learning_Graph_Matching%3A_Oriented_to_Category_Modeling_from_Cluttered_Scenes.html">237 iccv-2013-Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.139), (1, 0.054), (2, -0.062), (3, -0.099), (4, 0.0), (5, -0.024), (6, 0.106), (7, 0.063), (8, 0.002), (9, -0.022), (10, -0.023), (11, -0.002), (12, 0.058), (13, 0.047), (14, 0.022), (15, 0.023), (16, -0.018), (17, -0.064), (18, 0.003), (19, 0.064), (20, -0.11), (21, -0.127), (22, -0.007), (23, -0.063), (24, 0.048), (25, 0.094), (26, -0.133), (27, 0.069), (28, -0.025), (29, -0.031), (30, -0.004), (31, -0.065), (32, -0.028), (33, 0.001), (34, -0.043), (35, -0.062), (36, -0.099), (37, 0.037), (38, 0.005), (39, -0.029), (40, 0.038), (41, -0.006), (42, 0.048), (43, -0.018), (44, -0.029), (45, 0.061), (46, 0.029), (47, 0.062), (48, 0.063), (49, -0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90875518 <a title="261-lsi-1" href="./iccv-2013-Markov_Network-Based_Unified_Classifier_for_Face_Identification.html">261 iccv-2013-Markov Network-Based Unified Classifier for Face Identification</a></p>
<p>Author: Wonjun Hwang, Kyungshik Roh, Junmo Kim</p><p>Abstract: We propose a novel unifying framework using a Markov network to learn the relationship between multiple classifiers in face recognition. We assume that we have several complementary classifiers and assign observation nodes to the features of a query image and hidden nodes to the features of gallery images. We connect each hidden node to its corresponding observation node and to the hidden nodes of other neighboring classifiers. For each observation-hidden node pair, we collect a set of gallery candidates that are most similar to the observation instance, and the relationship between the hidden nodes is captured in terms of the similarity matrix between the collected gallery images. Posterior probabilities in the hidden nodes are computed by the belief-propagation algorithm. The novelty of the proposed framework is the method that takes into account the classifier dependency using the results of each neighboring classifier. We present extensive results on two different evaluation protocols, known and unknown image variation tests, using three different databases, which shows that the proposed framework always leads to good accuracy in face recognition.</p><p>2 0.78988177 <a title="261-lsi-2" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>Author: Renliang Weng, Jiwen Lu, Junlin Hu, Gao Yang, Yap-Peng Tan</p><p>Abstract: Over the past two decades, a number of face recognition methods have been proposed in the literature. Most of them use holistic face images to recognize people. However, human faces are easily occluded by other objects in many real-world scenarios and we have to recognize the person of interest from his/her partial faces. In this paper, we propose a new partial face recognition approach by using feature set matching, which is able to align partial face patches to holistic gallery faces automatically and is robust to occlusions and illumination changes. Given each gallery image and probe face patch, we first detect keypoints and extract their local features. Then, we propose a Metric Learned ExtendedRobust PointMatching (MLERPM) method to discriminatively match local feature sets of a pair of gallery and probe samples. Lastly, the similarity of two faces is converted as the distance between two feature sets. Experimental results on three public face databases are presented to show the effectiveness of the proposed approach.</p><p>3 0.74998492 <a title="261-lsi-3" href="./iccv-2013-Coupling_Alignments_with_Recognition_for_Still-to-Video_Face_Recognition.html">97 iccv-2013-Coupling Alignments with Recognition for Still-to-Video Face Recognition</a></p>
<p>Author: Zhiwu Huang, Xiaowei Zhao, Shiguang Shan, Ruiping Wang, Xilin Chen</p><p>Abstract: The Still-to-Video (S2V) face recognition systems typically need to match faces in low-quality videos captured under unconstrained conditions against high quality still face images, which is very challenging because of noise, image blur, lowface resolutions, varying headpose, complex lighting, and alignment difficulty. To address the problem, one solution is to select the frames of ‘best quality ’ from videos (hereinafter called quality alignment in this paper). Meanwhile, the faces in the selected frames should also be geometrically aligned to the still faces offline well-aligned in the gallery. In this paper, we discover that the interactions among the three tasks–quality alignment, geometric alignment and face recognition–can benefit from each other, thus should be performed jointly. With this in mind, we propose a Coupling Alignments with Recognition (CAR) method to tightly couple these tasks via low-rank regularized sparse representation in a unified framework. Our method makes the three tasks promote mutually by a joint optimization in an Augmented Lagrange Multiplier routine. Extensive , experiments on two challenging S2V datasets demonstrate that our method outperforms the state-of-the-art methods impressively.</p><p>4 0.64684075 <a title="261-lsi-4" href="./iccv-2013-Hidden_Factor_Analysis_for_Age_Invariant_Face_Recognition.html">195 iccv-2013-Hidden Factor Analysis for Age Invariant Face Recognition</a></p>
<p>Author: Dihong Gong, Zhifeng Li, Dahua Lin, Jianzhuang Liu, Xiaoou Tang</p><p>Abstract: Age invariant face recognition has received increasing attention due to its great potential in real world applications. In spite of the great progress in face recognition techniques, reliably recognizingfaces across ages remains a difficult task. The facial appearance of a person changes substantially over time, resulting in significant intra-class variations. Hence, the key to tackle this problem is to separate the variation caused by aging from the person-specific features that are stable. Specifically, we propose a new method, calledHidden FactorAnalysis (HFA). This methodcaptures the intuition above through a probabilistic model with two latent factors: an identity factor that is age-invariant and an age factor affected by the aging process. Then, the observed appearance can be modeled as a combination of the components generated based on these factors. We also develop a learning algorithm that jointly estimates the latent factors and the model parameters using an EM procedure. Extensive experiments on two well-known public domain face aging datasets: MORPH (the largest public face aging database) and FGNET, clearly show that the proposed method achieves notable improvement over state-of-the-art algorithms.</p><p>5 0.64536411 <a title="261-lsi-5" href="./iccv-2013-Face_Recognition_via_Archetype_Hull_Ranking.html">154 iccv-2013-Face Recognition via Archetype Hull Ranking</a></p>
<p>Author: Yuanjun Xiong, Wei Liu, Deli Zhao, Xiaoou Tang</p><p>Abstract: The archetype hull model is playing an important role in large-scale data analytics and mining, but rarely applied to vision problems. In this paper, we migrate such a geometric model to address face recognition and verification together through proposing a unified archetype hull ranking framework. Upon a scalable graph characterized by a compact set of archetype exemplars whose convex hull encompasses most of the training images, the proposed framework explicitly captures the relevance between any query and the stored archetypes, yielding a rank vector over the archetype hull. The archetype hull ranking is then executed on every block of face images to generate a blockwise similarity measure that is achieved by comparing two different rank vectors with respect to the same archetype hull. After integrating blockwise similarity measurements with learned importance weights, we accomplish a sensible face similarity measure which can support robust and effective face recognition and verification. We evaluate the face similarity measure in terms of experiments performed on three benchmark face databases Multi-PIE, Pubfig83, and LFW, demonstrat- ing its performance superior to the state-of-the-arts.</p><p>6 0.62087423 <a title="261-lsi-6" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>7 0.61224639 <a title="261-lsi-7" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>8 0.60248357 <a title="261-lsi-8" href="./iccv-2013-Deep_Learning_Identity-Preserving_Face_Space.html">106 iccv-2013-Deep Learning Identity-Preserving Face Space</a></p>
<p>9 0.59600043 <a title="261-lsi-9" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>10 0.59262198 <a title="261-lsi-10" href="./iccv-2013-POP%3A_Person_Re-identification_Post-rank_Optimisation.html">305 iccv-2013-POP: Person Re-identification Post-rank Optimisation</a></p>
<p>11 0.58623379 <a title="261-lsi-11" href="./iccv-2013-Model_Recommendation_with_Virtual_Probes_for_Egocentric_Hand_Detection.html">267 iccv-2013-Model Recommendation with Virtual Probes for Egocentric Hand Detection</a></p>
<p>12 0.57435405 <a title="261-lsi-12" href="./iccv-2013-Hybrid_Deep_Learning_for_Face_Verification.html">206 iccv-2013-Hybrid Deep Learning for Face Verification</a></p>
<p>13 0.55145854 <a title="261-lsi-13" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>14 0.52726889 <a title="261-lsi-14" href="./iccv-2013-Complex_3D_General_Object_Reconstruction_from_Line_Drawings.html">84 iccv-2013-Complex 3D General Object Reconstruction from Line Drawings</a></p>
<p>15 0.52563363 <a title="261-lsi-15" href="./iccv-2013-Fast_High_Dimensional_Vector_Multiplication_Face_Recognition.html">158 iccv-2013-Fast High Dimensional Vector Multiplication Face Recognition</a></p>
<p>16 0.51317 <a title="261-lsi-16" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>17 0.48294213 <a title="261-lsi-17" href="./iccv-2013-Person_Re-identification_by_Salience_Matching.html">313 iccv-2013-Person Re-identification by Salience Matching</a></p>
<p>18 0.43917203 <a title="261-lsi-18" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>19 0.43763313 <a title="261-lsi-19" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>20 0.42401779 <a title="261-lsi-20" href="./iccv-2013-Modifying_the_Memorability_of_Face_Photographs.html">272 iccv-2013-Modifying the Memorability of Face Photographs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.056), (4, 0.012), (6, 0.328), (7, 0.018), (26, 0.072), (31, 0.041), (42, 0.121), (48, 0.01), (64, 0.027), (73, 0.025), (89, 0.14), (98, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.70591551 <a title="261-lda-1" href="./iccv-2013-A_Fully_Hierarchical_Approach_for_Finding_Correspondences_in_Non-rigid_Shapes.html">11 iccv-2013-A Fully Hierarchical Approach for Finding Correspondences in Non-rigid Shapes</a></p>
<p>Author: Ivan Sipiran, Benjamin Bustos</p><p>Abstract: This paper presents a hierarchical method for finding correspondences in non-rigid shapes. We propose a new representation for 3D meshes: the decomposition tree. This structure characterizes the recursive decomposition process of a mesh into regions of interest and keypoints. The internal nodes contain regions of interest (which may be recursively decomposed) and the leaf nodes contain the keypoints to be matched. We also propose a hierarchical matching algorithm that performs in a level-wise manner. The matching process is guided by the similarity between regions in high levels of the tree, until reaching the keypoints stored in the leaves. This allows us to reduce the search space of correspondences, making also the matching process efficient. We evaluate the effectiveness of our approach using the SHREC’2010 robust correspondence benchmark. In addition, we show that our results outperform the state of the art.</p><p>same-paper 2 0.6989699 <a title="261-lda-2" href="./iccv-2013-Markov_Network-Based_Unified_Classifier_for_Face_Identification.html">261 iccv-2013-Markov Network-Based Unified Classifier for Face Identification</a></p>
<p>Author: Wonjun Hwang, Kyungshik Roh, Junmo Kim</p><p>Abstract: We propose a novel unifying framework using a Markov network to learn the relationship between multiple classifiers in face recognition. We assume that we have several complementary classifiers and assign observation nodes to the features of a query image and hidden nodes to the features of gallery images. We connect each hidden node to its corresponding observation node and to the hidden nodes of other neighboring classifiers. For each observation-hidden node pair, we collect a set of gallery candidates that are most similar to the observation instance, and the relationship between the hidden nodes is captured in terms of the similarity matrix between the collected gallery images. Posterior probabilities in the hidden nodes are computed by the belief-propagation algorithm. The novelty of the proposed framework is the method that takes into account the classifier dependency using the results of each neighboring classifier. We present extensive results on two different evaluation protocols, known and unknown image variation tests, using three different databases, which shows that the proposed framework always leads to good accuracy in face recognition.</p><p>3 0.64104688 <a title="261-lda-3" href="./iccv-2013-Discriminatively_Trained_Templates_for_3D_Object_Detection%3A_A_Real_Time_Scalable_Approach.html">121 iccv-2013-Discriminatively Trained Templates for 3D Object Detection: A Real Time Scalable Approach</a></p>
<p>Author: Reyes Rios-Cabrera, Tinne Tuytelaars</p><p>Abstract: In this paper we propose a new method for detecting multiple specific 3D objects in real time. We start from the template-based approach based on the LINE2D/LINEMOD representation introduced recently by Hinterstoisser et al., yet extend it in two ways. First, we propose to learn the templates in a discriminative fashion. We show that this can be done online during the collection of the example images, in just a few milliseconds, and has a big impact on the accuracy of the detector. Second, we propose a scheme based on cascades that speeds up detection. Since detection of an object is fast, new objects can be added with very low cost, making our approach scale well. In our experiments, we easily handle 10-30 3D objects at frame rates above 10fps using a single CPU core. We outperform the state-of-the-art both in terms of speed as well as in terms of accuracy, as validated on 3 different datasets. This holds both when using monocular color images (with LINE2D) and when using RGBD images (with LINEMOD). Moreover, wepropose a challenging new dataset made of12 objects, for future competing methods on monocular color images.</p><p>4 0.62388563 <a title="261-lda-4" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>Author: Ali Elqursh, Ahmed Elgammal</p><p>Abstract: The vast majority of work on motion segmentation adopts the affine camera model due to its simplicity. Under the affine model, the motion segmentation problem becomes that of subspace separation. Due to this assumption, such methods are mainly offline and exhibit poor performance when the assumption is not satisfied. This is made evident in state-of-the-art methods that relax this assumption by using piecewise affine spaces and spectral clustering techniques to achieve better results. In this paper, we formulate the problem of motion segmentation as that of manifold separation. We then show how label propagation can be used in an online framework to achieve manifold separation. The performance of our framework is evaluated on a benchmark dataset and achieves competitive performance while being online.</p><p>5 0.58464539 <a title="261-lda-5" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<p>Author: Marius Leordeanu, Andrei Zanfir, Cristian Sminchisescu</p><p>Abstract: Estimating a dense correspondence field between successive video frames, under large displacement, is important in many visual learning and recognition tasks. We propose a novel sparse-to-dense matching method for motion field estimation and occlusion detection. As an alternative to the current coarse-to-fine approaches from the optical flow literature, we start from the higher level of sparse matching with rich appearance and geometric constraints collected over extended neighborhoods, using an occlusion aware, locally affine model. Then, we move towards the simpler, but denser classic flow field model, with an interpolation procedure that offers a natural transition between the sparse and the dense correspondence fields. We experimentally demonstrate that our appearance features and our complex geometric constraintspermit the correct motion estimation even in difficult cases of large displacements and significant appearance changes. We also propose a novel classification method for occlusion detection that works in conjunction with the sparse-to-dense matching model. We validate our approach on the newly released Sintel dataset and obtain state-of-the-art results.</p><p>6 0.54913634 <a title="261-lda-6" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>7 0.54344022 <a title="261-lda-7" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>8 0.54134196 <a title="261-lda-8" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>9 0.54120505 <a title="261-lda-9" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>10 0.5404886 <a title="261-lda-10" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>11 0.54025912 <a title="261-lda-11" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>12 0.54014206 <a title="261-lda-12" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>13 0.5398587 <a title="261-lda-13" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>14 0.53983104 <a title="261-lda-14" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>15 0.53962523 <a title="261-lda-15" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>16 0.53950739 <a title="261-lda-16" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>17 0.53914994 <a title="261-lda-17" href="./iccv-2013-Distributed_Low-Rank_Subspace_Segmentation.html">122 iccv-2013-Distributed Low-Rank Subspace Segmentation</a></p>
<p>18 0.53880423 <a title="261-lda-18" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>19 0.53876925 <a title="261-lda-19" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<p>20 0.5381189 <a title="261-lda-20" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
