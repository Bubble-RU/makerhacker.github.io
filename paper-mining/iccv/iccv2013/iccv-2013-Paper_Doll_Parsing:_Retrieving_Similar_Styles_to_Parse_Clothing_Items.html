<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>306 iccv-2013-Paper Doll Parsing: Retrieving Similar Styles to Parse Clothing Items</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-306" href="#">iccv2013-306</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>306 iccv-2013-Paper Doll Parsing: Retrieving Similar Styles to Parse Clothing Items</h1>
<br/><p>Source: <a title="iccv-2013-306-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Yamaguchi_Paper_Doll_Parsing_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Kota Yamaguchi, M. Hadi Kiapour, Tamara L. Berg</p><p>Abstract: Clothing recognition is an extremely challenging problem due to wide variation in clothing item appearance, layering, and style. In this paper, we tackle the clothing parsing problem using a retrieval based approach. For a query image, we find similar styles from a large database of tagged fashion images and use these examples to parse the query. Our approach combines parsing from: pre-trained global clothing models, local clothing models learned on theflyfrom retrieved examples, and transferredparse masks (paper doll item transfer) from retrieved examples. Experimental evaluation shows that our approach significantly outperforms state of the art in parsing accuracy.</p><p>Reference: <a title="iccv-2013-306-reference" href="../iccv2013_reference/iccv-2013-Paper_Doll_Parsing%3A_Retrieving_Similar_Styles_to_Parse_Clothing_Items_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract Clothing recognition is an extremely challenging problem due to wide variation in clothing item appearance, layering, and style. [sent-9, score-0.808]
</p><p>2 In this paper, we tackle the clothing parsing problem using a retrieval based approach. [sent-10, score-0.913]
</p><p>3 For a query image, we find similar styles from a large database of tagged fashion images and use these examples to parse the query. [sent-11, score-0.467]
</p><p>4 Our approach combines parsing from: pre-trained global clothing models, local clothing models learned on theflyfrom retrieved examples, and transferredparse masks (paper doll item transfer) from retrieved examples. [sent-12, score-2.074]
</p><p>5 Experimental evaluation shows that our approach significantly outperforms state of the art in parsing accuracy. [sent-13, score-0.276]
</p><p>6 In addition to style variation, individual clothing items also display many different appearance characteristics. [sent-22, score-1.069]
</p><p>7 In this paper, we take a data driven approach to clothing parsing. [sent-29, score-0.591]
</p><p>8 We first collect a large, complex, real world collection of outfit pictures from a social network focused on fashion, chictopia . [sent-30, score-0.21]
</p><p>9 Using a very small set of hand parsed images in combination with the text tags associated with each image in the collection, we can parse our large database accurately. [sent-32, score-0.46]
</p><p>10 Now, given a query image without any associated text, we can predict an accurate parse by retrieving similar outfits from our parsed collection, building local models from retrieved clothing items, and transferring inferred clothing items from the retrieved samples  to the query image. [sent-33, score-2.514]
</p><p>11 In each of these steps we take advantage of the relationship between clothing and body pose to constrain prediction and produce a more accurate parse. [sent-35, score-0.711]
</p><p>12 We call this paper doll parsing because it essentially transfers predictions from retrieved samples to the query, like laying paper cutouts of clothing items onto a paper doll. [sent-36, score-1.475]
</p><p>13 In particular, we propose a retrieval based approach to clothing parsing that combines: • Pre-trained global models of clothing items. [sent-38, score-1.525]
</p><p>14 • Local models of clothing items learned on the fly from rLeotrciaevl medo examples. [sent-39, score-0.96]
</p><p>15 • Parse mask predictions transferred from retrieved examples taos tkhe p query image. [sent-40, score-0.3]
</p><p>16 Clothing recognition is a challenging and societally important problem global sales for clothing total over a hundred billion dollars, much of which is conducted online. [sent-42, score-0.612]
</p><p>17 This is reflected in the growing interest in clothing related recognition papers [11, 10, 25, 16, 26, 7, 2, 4], perhaps boosted by recent advances in pose estimation [27, 3]. [sent-43, score-0.639]
</p><p>18 Many of these papers have focused on specific aspects of clothing recognition such as predicting attributes of clothing [7, 2, 4], outfit recommendation [15], or identifying aspects of socio-identity through clothing [18, 20]. [sent-44, score-1.872]
</p><p>19 We attack the problem of clothing parsing, assigning a semantic label to each pixel in the image where labels can –  3355 1192  be selected from background, skin, hair, or from a large set of clothing items (e. [sent-45, score-1.586]
</p><p>20 Effective solutions to clothing parsing could enable useful end-user applications such as pose independent clothing retrieval [26] or street to shop applications [16]. [sent-48, score-1.552]
</p><p>21 This problem is closely related to the general image parsing problem which has been approached successfully using related non-parametric methods [21, 14, 22]. [sent-49, score-0.276]
</p><p>22 However, we view the clothing parsing problem as suitable for specialized exploration because it deals with people, a category that has obvious significance. [sent-50, score-0.867]
</p><p>23 The clothing parsing problem is also special in that one can take advantage of body pose estimates during parsing, and we do so in all parts of our method. [sent-51, score-0.951]
</p><p>24 Previous state of the art on clothing parsing [26] performed quite well on the constrained parsing problem, where test images are parsed given user provided tags indicating depicted clothing items. [sent-52, score-1.926]
</p><p>25 However, they were less effective at unconstrained clothing parsing, where test images are parsed in the absense of any textual information. [sent-53, score-0.682]
</p><p>26 The training samples are used for learning feature transforms, building global clothing models, and adjusting parameters. [sent-58, score-0.64]
</p><p>27 com with associated metadata tags denoting characteristics such as color, clothing item, or occasion. [sent-62, score-0.692]
</p><p>28 From the remaining, we select pictures tagged with at least one clothing item and run a full-body pose detector [27], keeping those that have a person detection. [sent-64, score-0.969]
</p><p>29 This results in 339,797 pictures weakly annotated with clothing items and estimated pose. [sent-65, score-1.003]
</p><p>30 Though the annotations are not always complete users often do not label all depicted items, especially small items or accessories it is rare to find images where an annotated tag is not present. [sent-66, score-0.481]
</p><p>31 Use retrieved images and tags to parse the query. [sent-72, score-0.515]
</p><p>32 During parsing, we compute the parse in this fixed frame size then warp it back to the original image, assuming regions outside the bounding box are background. [sent-78, score-0.268]
</p><p>33 Our methods draw from a number of dense feature types (each parsing method uses some subset): RGB RGB color of the pixel. [sent-79, score-0.276]
</p><p>34 Style retrieval Our goal for retrieving similar pictures is two-fold: a) to predict depicted clothing items, and b) to obtain information helpful for parsing clothing items. [sent-90, score-1.616]
</p><p>35 Style descriptor We design a descriptor for style retrieval that is useful for finding styles with similar appearance. [sent-93, score-0.261]
</p><p>36 Skin-hair Detection is computed using logistic regression for skin, hair, background, and clothing at each pixel. [sent-98, score-0.676]
</p><p>37 Note that we do not include Pose Distance as a feature in the style descriptor, but instead use Skin-hair detection to indirectly include pose-dependent information in the representation since the purpose ofthe style descriptor is to find similar styles independent of pose. [sent-100, score-0.322]
</p><p>38 Tag prediction The retrieved samples are first used to predict clothing items potentially present in a query image. [sent-112, score-1.257]
</p><p>39 The purpose of tag prediction is to obtain a set of tags that might be relevant  to the query, while eliminating definitely irrelevant items for consideration. [sent-113, score-0.552]
</p><p>40 Each tag in the retrieved samples provides a vote weighted by the inverse of its distance from the query, which forms a confidence for presence of that item. [sent-117, score-0.296]
</p><p>41 While linear classification (clothing item classifiers trained on subsets of body parts, e. [sent-121, score-0.253]
</p><p>42 Since the goal here is only to eliminate obviously irrelevant items while keeping most potentially relevant items, we tune the threshold to give 0. [sent-125, score-0.348]
</p><p>43 Due to the skewed item distribution in the Fashionista dataset, we use the same threshold for all items to avoid over-fitting the predictive model. [sent-127, score-0.592]
</p><p>44 In the parsing stage, we always include background, skin, and hair in addition to the predicted clothing tags. [sent-128, score-0.935]
</p><p>45 Clothing parsing  Following tag prediction, we start to parse the image in a per-pixel fashion. [sent-130, score-0.611]
</p><p>46 Compute pixel-level confidence from three methods: global parse, nearest neighbor parse, and transferred parse. [sent-132, score-0.217]
</p><p>47 Pixel confidence Let us denote yi as the clothing item label at pixel i. [sent-138, score-0.997]
</p><p>48 The first step in parsing is to compute a confidence score of assigning clothing item lto yi. [sent-139, score-1.139]
</p><p>49 1  Global parse  The first term in our model is a global clothing likelihood, trained for each clothing item on the hand parsed Fashionista training split. [sent-144, score-1.779]
</p><p>50 The leftmost column shows query images with ground truth item annotation. [sent-146, score-0.302]
</p><p>51 The rest are  retrieved images with associated tags in the top 25. [sent-147, score-0.247]
</p><p>52 Transferred  parse  Combined (1+2+3)  Smoothed result  Labels are MAP assignments of the scoring functions. [sent-151, score-0.268]
</p><p>53 This could potentially increase confusion between similar item types, such as blazer and jacket since they usually do not appear together, in favor of better localization accuracy. [sent-158, score-0.366]
</p><p>54 2  Nearest neighbor parse  The second term in our model is also a logistic regression, but trained only on the retrieved nearest neighbor (NN) images. [sent-164, score-0.585]
</p><p>55 Here we learn a local appearance model for each  clothing item based on examples that are similar to the query, e. [sent-165, score-0.808]
</p><p>56 blazers that look similar to the query blazer because they were retrieved via style similarity. [sent-167, score-0.451]
</p><p>57 3  Transferred parse  The third term in our model is obtained by transferring the parse mask likelihoods estimated by the global parse Sglobal from the retrieved images to the query image (Figure 6 visualizes an example). [sent-177, score-1.089]
</p><p>58 , [21]) to overcome the difficulty in naively transferring deformable, often occluded clothing items pixel-wise. [sent-181, score-0.972]
</p><p>59 Our approach first computes an over-segmentation of both query and retrieved images using a fast and simple segmentation algorithm [9], then finds corresponding pairs of super-pixels between the query and each retrieved image based on pose and appearance: 1. [sent-182, score-0.51]
</p><p>60 Then, our transferred parse is computed as:  Stransfer(yi|xi,D) ≡Z1? [sent-189, score-0.337]
</p><p>61 si,rP(yi= l|xi,θlg) · 1[l ∈ τ(r)],  (5)  which is a mean of the global parse over the super-pixel in a retrieved image. [sent-193, score-0.435]
</p><p>62 Iterative label smoothing The combined confidence gives a rough estimate of item localization. [sent-213, score-0.344]
</p><p>63 However, it does not respect boundaries of actual clothing items since it is computed per-pixel. [sent-214, score-0.939]
</p><p>64 Therefore, we introduce an iterative smoothing stage that considers all pixels together to provide a smooth parse of an image. [sent-215, score-0.367]
</p><p>65 Following the approach of [19], we formulate this smoothing problem by considering the joint labeling of pixels Y ≡ {yi} and item appearance models Θ ≡ {θsl}, where θls ≡is { a m}o adnedl f ioter a la apbeple al. [sent-216, score-0.284]
</p><p>66 r aTnhcee goal iesl tso Θ Θfind ≡ ≡th {eθ optimal joint assignment Y∗ and item models Θ∗ for a given image. [sent-217, score-0.248]
</p><p>67 We start this problem by initializing the current predicted parsing Yˆ0 with the MAP assignment under the combined 33551236  Yˆ0  confidence S. [sent-218, score-0.396]
</p><p>68 Then, we treat as training data to build initial image-specific item models Θˆ0 (logistic regressions). [sent-219, score-0.217]
</p><p>69 Offline processing Our retrieval techniques require the large Paper Doll Dataset to be pre-processed (parsed), for building nearest neighbor models on the fly from retrieved samples and for transferring parse masks. [sent-234, score-0.614]
</p><p>70 Therefore, we estimate a clothing parse for each sample in the 339K image dataset, making use of pose estimates and the tags associated with the image by the photo owner. [sent-235, score-1.008]
</p><p>71 This parse makes use of the global clothing models (constrained to the tags associated with the image by the photo owner) and iterative smoothing parts of our approach. [sent-236, score-1.058]
</p><p>72 Although these training images are tagged, there are often clothing items missing in the annotation. [sent-237, score-0.939]
</p><p>73 To prevent this, we add an unknown item label with uniform probability and initialize together with the global clothing model at all samples. [sent-239, score-0.856]
</p><p>74 This effectively prevents the final estimated labeling Yˆ to mark missing items with incorrect labels. [sent-240, score-0.348]
</p><p>75 For an unseen query image, our full parsing pipeline takes 20 to 40 seconds, including pose estimation. [sent-242, score-0.409]
</p><p>76 Experimental results We evaluate parsing performance on the 229 testing sam-  ples from the Fashionista dataset. [sent-245, score-0.276]
</p><p>77 The task is to predict a label for every pixel where labels represent a set of 56 different categories a very large and challenging variety of clothing items. [sent-246, score-0.67]
</p><p>78 In addition, we also include foreground accuracy (See eqn 6) as a measure of how accurately each method is at parsing foreground regions (those pixels on the body, not on the background). [sent-248, score-0.408]
</p><p>79 Table 1 summarizes predictive performance of our parsing method, including a breakdown of how well the intermediate parsing steps perform. [sent-251, score-0.579]
</p><p>80 For comparison, we include the performance of previous state of the art on clothing parsing [26]. [sent-252, score-0.867]
</p><p>81 Figure 7 shows examples from our parsing method, with ground truth annotation and the method of [26]. [sent-265, score-0.276]
</p><p>82 We observe –  that our method produces a parse that respects the actual item boundary, even if some items are incorrectly labeled; e. [sent-266, score-0.833]
</p><p>83 However, often these confusions are due to high similarity in appearance between items and sometimes due to non-exclusivity in item types, i. [sent-269, score-0.588]
</p><p>84 Figure 8 plots F-1 scores for non-empty items (items predicted on the test set) comparing the method of [26] with our method. [sent-272, score-0.382]
</p><p>85 Our model outperforms the prior work on many items, especially major foreground items such as dress, jeans, coat, shorts, or skirt. [sent-273, score-0.403]
</p><p>86 This results in a significant boost in foreground accuracy and perceptually better parsing results. [sent-274, score-0.352]
</p><p>87 By design, our style descriptor is aimed at representing whole outfit style rather than specific details of the outfit. [sent-276, score-0.354]
</p><p>88 Consequently, small items like accessories tend to be less weighted during retrieval and are therefore poorly predicted during parsing. [sent-277, score-0.467]
</p><p>89 However, prediction of small items is inherently extremely challenging because they provide limited appearance information. [sent-278, score-0.384]
</p><p>90 flicting items from being predicted for the same image, such as dress and skirt, or boots and shoes which tend not to be  worn together. [sent-322, score-0.556]
</p><p>91 Our iterative smoothing is effectively reducing such confusion, but the parsing result sometimes contains one item split into two conflicting items. [sent-323, score-0.593]
</p><p>92 Lastly, we find it difficult to predict items with skin-like color or coarsely textured items (similar to issues reported in [26]). [sent-325, score-0.719]
</p><p>93 Because of the variation in lighting condition in pictures, it is very hard to distinguish between actual skin and clothing items that look like skin, e. [sent-326, score-0.992]
</p><p>94 Conclusion We describe a clothing parsing method based on nearest neighbor style retrieval. [sent-332, score-1.069]
</p><p>95 Our system combines: global parse models, nearest neighbor parse models, and transferred parse predictions. [sent-333, score-0.966]
</p><p>96 all accuracy and especially foreground parsing accuracy over previous work. [sent-338, score-0.331]
</p><p>97 It is our future work to resolve the confusion between very similar items and to incorporate higher level knowledge about outfits. [sent-339, score-0.393]
</p><p>98 Street-toshop: Cross-scenario clothing retrieval via parts alignment and auxiliary set. [sent-453, score-0.637]
</p><p>99 Finding things: Image parsing with regions and per-exemplar detectors. [sent-493, score-0.276]
</p><p>100 Who blocks who: Simultaneous clothing segmentation for grouping images. [sent-507, score-0.591]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('clothing', 0.591), ('items', 0.348), ('parsing', 0.276), ('parse', 0.268), ('item', 0.217), ('fashionista', 0.19), ('retrieved', 0.146), ('style', 0.13), ('shorts', 0.125), ('tags', 0.101), ('parsed', 0.091), ('jeans', 0.088), ('doll', 0.086), ('query', 0.085), ('yi', 0.078), ('jacket', 0.073), ('outfit', 0.071), ('transferred', 0.069), ('tag', 0.067), ('pictures', 0.064), ('pants', 0.063), ('logistic', 0.061), ('dress', 0.061), ('lg', 0.059), ('shoes', 0.059), ('skirt', 0.055), ('chapel', 0.055), ('foreground', 0.055), ('confidence', 0.055), ('blazer', 0.054), ('boots', 0.054), ('chictopia', 0.054), ('outfits', 0.054), ('sglobal', 0.054), ('shirts', 0.054), ('tights', 0.054), ('skin', 0.053), ('knn', 0.051), ('tagged', 0.049), ('pose', 0.048), ('sweater', 0.047), ('hill', 0.047), ('retrieval', 0.046), ('smoothing', 0.045), ('kiapour', 0.044), ('xi', 0.043), ('rgb', 0.043), ('accessories', 0.039), ('styles', 0.039), ('lab', 0.039), ('neighbor', 0.038), ('prediction', 0.036), ('body', 0.036), ('blazers', 0.036), ('snearest', 0.036), ('stransfer', 0.036), ('hair', 0.034), ('predicted', 0.034), ('nearest', 0.034), ('transferring', 0.033), ('pages', 0.032), ('iterative', 0.032), ('belt', 0.032), ('hadi', 0.032), ('assignment', 0.031), ('coat', 0.029), ('stony', 0.029), ('regressions', 0.029), ('shirt', 0.029), ('pixel', 0.029), ('predicting', 0.028), ('samples', 0.028), ('brook', 0.028), ('unc', 0.028), ('predictive', 0.027), ('label', 0.027), ('fashion', 0.026), ('tighe', 0.025), ('button', 0.025), ('vs', 0.025), ('boundary', 0.025), ('retrieving', 0.025), ('regression', 0.024), ('yamaguchi', 0.024), ('resolve', 0.023), ('predict', 0.023), ('descriptor', 0.023), ('crf', 0.023), ('sometimes', 0.023), ('bourdev', 0.023), ('confusion', 0.022), ('gallagher', 0.022), ('song', 0.022), ('pixels', 0.022), ('social', 0.021), ('ls', 0.021), ('global', 0.021), ('wear', 0.021), ('perceptually', 0.021), ('fly', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999869 <a title="306-tfidf-1" href="./iccv-2013-Paper_Doll_Parsing%3A_Retrieving_Similar_Styles_to_Parse_Clothing_Items.html">306 iccv-2013-Paper Doll Parsing: Retrieving Similar Styles to Parse Clothing Items</a></p>
<p>Author: Kota Yamaguchi, M. Hadi Kiapour, Tamara L. Berg</p><p>Abstract: Clothing recognition is an extremely challenging problem due to wide variation in clothing item appearance, layering, and style. In this paper, we tackle the clothing parsing problem using a retrieval based approach. For a query image, we find similar styles from a large database of tagged fashion images and use these examples to parse the query. Our approach combines parsing from: pre-trained global clothing models, local clothing models learned on theflyfrom retrieved examples, and transferredparse masks (paper doll item transfer) from retrieved examples. Experimental evaluation shows that our approach significantly outperforms state of the art in parsing accuracy.</p><p>2 0.25440192 <a title="306-tfidf-2" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>Author: Ming Shao, Liangyue Li, Yun Fu</p><p>Abstract: In this paper, we investigate the problem of recognizing occupations of multiple people with arbitrary poses in a photo. Previous work utilizing single person ’s nearly frontal clothing information and fore/background context preliminarily proves that occupation recognition is computationally feasible in computer vision. However, in practice, multiple people with arbitrary poses are common in a photo, and recognizing their occupations is even more challenging. We argue that with appropriately built visual attributes, co-occurrence, and spatial configuration model that is learned through structure SVM, we can recognize multiple people ’s occupations in a photo simultaneously. To evaluate our method’s performance, we conduct extensive experiments on a new well-labeled occupation database with 14 representative occupations and over 7K images. Results on this database validate our method’s effectiveness and show that occupation recognition is solvable in a more general case.</p><p>3 0.13956454 <a title="306-tfidf-3" href="./iccv-2013-A_Deformable_Mixture_Parsing_Model_with_Parselets.html">8 iccv-2013-A Deformable Mixture Parsing Model with Parselets</a></p>
<p>Author: Jian Dong, Qiang Chen, Wei Xia, Zhongyang Huang, Shuicheng Yan</p><p>Abstract: In this work, we address the problem of human parsing, namely partitioning the human body into semantic regions, by using the novel Parselet representation. Previous works often consider solving the problem of human pose estimation as the prerequisite of human parsing. We argue that these approaches cannot obtain optimal pixel level parsing due to the inconsistent targets between these tasks. In this paper, we propose to use Parselets as the building blocks of our parsing model. Parselets are a group of parsable segments which can generally be obtained by lowlevel over-segmentation algorithms and bear strong semantic meaning. We then build a Deformable Mixture Parsing Model (DMPM) for human parsing to simultaneously handle the deformation and multi-modalities of Parselets. The proposed model has two unique characteristics: (1) the possible numerous modalities of Parselet ensembles are exhibited as the “And-Or” structure of sub-trees; (2) to further solve the practical problem of Parselet occlusion or absence, we directly model the visibility property at some leaf nodes. The DMPM thus directly solves the problem of human parsing by searching for the best graph configura- tionfrom apool ofParselet hypotheses without intermediate tasks. Comprehensive evaluations demonstrate the encouraging performance of the proposed approach.</p><p>4 0.11763137 <a title="306-tfidf-4" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>Author: Jungseock Joo, Shuo Wang, Song-Chun Zhu</p><p>Abstract: We present a part-based approach to the problem of human attribute recognition from a single image of a human body. To recognize the attributes of human from the body parts, it is important to reliably detect the parts. This is a challenging task due to the geometric variation such as articulation and view-point changes as well as the appearance variation of the parts arisen from versatile clothing types. The prior works have primarily focused on handling . edu . cn ???????????? geometric variation by relying on pre-trained part detectors or pose estimators, which require manual part annotation, but the appearance variation has been relatively neglected in these works. This paper explores the importance of the appearance variation, which is directly related to the main task, attribute recognition. To this end, we propose to learn a rich appearance part dictionary of human with significantly less supervision by decomposing image lattice into overlapping windows at multiscale and iteratively refining local appearance templates. We also present quantitative results in which our proposed method outperforms the existing approaches.</p><p>5 0.11665285 <a title="306-tfidf-5" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>Author: Arash Vahdat, Greg Mori</p><p>Abstract: Gathering accurate training data for recognizing a set of attributes or tags on images or videos is a challenge. Obtaining labels via manual effort or from weakly-supervised data typically results in noisy training labels. We develop the FlipSVM, a novel algorithm for handling these noisy, structured labels. The FlipSVM models label noise by “flipping ” labels on training examples. We show empirically that the FlipSVM is effective on images-and-attributes and video tagging datasets.</p><p>6 0.095716424 <a title="306-tfidf-6" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>7 0.086021237 <a title="306-tfidf-7" href="./iccv-2013-Scene_Collaging%3A_Analysis_and_Synthesis_of_Natural_Images_with_Semantic_Layers.html">375 iccv-2013-Scene Collaging: Analysis and Synthesis of Natural Images with Semantic Layers</a></p>
<p>8 0.082958959 <a title="306-tfidf-8" href="./iccv-2013-Strong_Appearance_and_Expressive_Spatial_Models_for_Human_Pose_Estimation.html">403 iccv-2013-Strong Appearance and Expressive Spatial Models for Human Pose Estimation</a></p>
<p>9 0.073064327 <a title="306-tfidf-9" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>10 0.068315633 <a title="306-tfidf-10" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>11 0.067056216 <a title="306-tfidf-11" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>12 0.066429794 <a title="306-tfidf-12" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>13 0.065427318 <a title="306-tfidf-13" href="./iccv-2013-Learning_Coupled_Feature_Spaces_for_Cross-Modal_Matching.html">235 iccv-2013-Learning Coupled Feature Spaces for Cross-Modal Matching</a></p>
<p>14 0.064758837 <a title="306-tfidf-14" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>15 0.063519627 <a title="306-tfidf-15" href="./iccv-2013-Learning_CRFs_for_Image_Parsing_with_Adaptive_Subgradient_Descent.html">234 iccv-2013-Learning CRFs for Image Parsing with Adaptive Subgradient Descent</a></p>
<p>16 0.063385345 <a title="306-tfidf-16" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>17 0.061272196 <a title="306-tfidf-17" href="./iccv-2013-Monocular_Image_3D_Human_Pose_Estimation_under_Self-Occlusion.html">273 iccv-2013-Monocular Image 3D Human Pose Estimation under Self-Occlusion</a></p>
<p>18 0.060281921 <a title="306-tfidf-18" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>19 0.058720987 <a title="306-tfidf-19" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>20 0.057920512 <a title="306-tfidf-20" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.145), (1, 0.038), (2, -0.011), (3, -0.048), (4, 0.065), (5, 0.019), (6, -0.017), (7, -0.014), (8, -0.033), (9, 0.035), (10, 0.047), (11, 0.023), (12, -0.011), (13, -0.022), (14, -0.032), (15, 0.045), (16, 0.018), (17, -0.105), (18, 0.047), (19, -0.012), (20, 0.029), (21, -0.009), (22, 0.024), (23, -0.008), (24, -0.036), (25, 0.017), (26, 0.066), (27, 0.023), (28, 0.001), (29, -0.008), (30, -0.025), (31, 0.047), (32, 0.115), (33, -0.017), (34, 0.026), (35, 0.003), (36, -0.011), (37, 0.11), (38, -0.031), (39, -0.013), (40, 0.027), (41, -0.02), (42, -0.069), (43, -0.04), (44, 0.084), (45, 0.062), (46, 0.054), (47, -0.015), (48, -0.077), (49, -0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92776674 <a title="306-lsi-1" href="./iccv-2013-Paper_Doll_Parsing%3A_Retrieving_Similar_Styles_to_Parse_Clothing_Items.html">306 iccv-2013-Paper Doll Parsing: Retrieving Similar Styles to Parse Clothing Items</a></p>
<p>Author: Kota Yamaguchi, M. Hadi Kiapour, Tamara L. Berg</p><p>Abstract: Clothing recognition is an extremely challenging problem due to wide variation in clothing item appearance, layering, and style. In this paper, we tackle the clothing parsing problem using a retrieval based approach. For a query image, we find similar styles from a large database of tagged fashion images and use these examples to parse the query. Our approach combines parsing from: pre-trained global clothing models, local clothing models learned on theflyfrom retrieved examples, and transferredparse masks (paper doll item transfer) from retrieved examples. Experimental evaluation shows that our approach significantly outperforms state of the art in parsing accuracy.</p><p>2 0.63274986 <a title="306-lsi-2" href="./iccv-2013-A_Deformable_Mixture_Parsing_Model_with_Parselets.html">8 iccv-2013-A Deformable Mixture Parsing Model with Parselets</a></p>
<p>Author: Jian Dong, Qiang Chen, Wei Xia, Zhongyang Huang, Shuicheng Yan</p><p>Abstract: In this work, we address the problem of human parsing, namely partitioning the human body into semantic regions, by using the novel Parselet representation. Previous works often consider solving the problem of human pose estimation as the prerequisite of human parsing. We argue that these approaches cannot obtain optimal pixel level parsing due to the inconsistent targets between these tasks. In this paper, we propose to use Parselets as the building blocks of our parsing model. Parselets are a group of parsable segments which can generally be obtained by lowlevel over-segmentation algorithms and bear strong semantic meaning. We then build a Deformable Mixture Parsing Model (DMPM) for human parsing to simultaneously handle the deformation and multi-modalities of Parselets. The proposed model has two unique characteristics: (1) the possible numerous modalities of Parselet ensembles are exhibited as the “And-Or” structure of sub-trees; (2) to further solve the practical problem of Parselet occlusion or absence, we directly model the visibility property at some leaf nodes. The DMPM thus directly solves the problem of human parsing by searching for the best graph configura- tionfrom apool ofParselet hypotheses without intermediate tasks. Comprehensive evaluations demonstrate the encouraging performance of the proposed approach.</p><p>3 0.62756282 <a title="306-lsi-3" href="./iccv-2013-3D_Sub-query_Expansion_for_Improving_Sketch-Based_Multi-view_Image_Retrieval.html">3 iccv-2013-3D Sub-query Expansion for Improving Sketch-Based Multi-view Image Retrieval</a></p>
<p>Author: Yen-Liang Lin, Cheng-Yu Huang, Hao-Jeng Wang, Winston Hsu</p><p>Abstract: We propose a 3D sub-query expansion approach for boosting sketch-based multi-view image retrieval. The core idea of our method is to automatically convert two (guided) 2D sketches into an approximated 3D sketch model, and then generate multi-view sketches as expanded sub-queries to improve the retrieval performance. To learn the weights among synthesized views (sub-queries), we present a new multi-query feature to model the similarity between subqueries and dataset images, and formulate it into a convex optimization problem. Our approach shows superior performance compared with the state-of-the-art approach on a public multi-view image dataset. Moreover, we also conduct sensitivity tests to analyze the parameters of our approach based on the gathered user sketches.</p><p>4 0.59782225 <a title="306-lsi-4" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<p>Author: Shi Qiu, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: This paper proposes modeling the complex web image collections with an automatically generated graph structure called visual semantic complex network (VSCN). The nodes on this complex network are clusters of images with both visual and semantic consistency, called semantic concepts. These nodes are connected based on the visual and semantic correlations. Our VSCN with 33, 240 concepts is generated from a collection of 10 million web images. 1 A great deal of valuable information on the structures of the web image collections can be revealed by exploring the VSCN, such as the small-world behavior, concept community, indegree distribution, hubs, and isolated concepts. It not only helps us better understand the web image collections at a macroscopic level, but also has many important practical applications. This paper presents two application examples: content-based image retrieval and image browsing. Experimental results show that the VSCN leads to significant improvement on both the precision of image retrieval (over 200%) and user experience for image browsing.</p><p>5 0.57733953 <a title="306-lsi-5" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>Author: Ming Shao, Liangyue Li, Yun Fu</p><p>Abstract: In this paper, we investigate the problem of recognizing occupations of multiple people with arbitrary poses in a photo. Previous work utilizing single person ’s nearly frontal clothing information and fore/background context preliminarily proves that occupation recognition is computationally feasible in computer vision. However, in practice, multiple people with arbitrary poses are common in a photo, and recognizing their occupations is even more challenging. We argue that with appropriately built visual attributes, co-occurrence, and spatial configuration model that is learned through structure SVM, we can recognize multiple people ’s occupations in a photo simultaneously. To evaluate our method’s performance, we conduct extensive experiments on a new well-labeled occupation database with 14 representative occupations and over 7K images. Results on this database validate our method’s effectiveness and show that occupation recognition is solvable in a more general case.</p><p>6 0.56887436 <a title="306-lsi-6" href="./iccv-2013-Query-Adaptive_Asymmetrical_Dissimilarities_for_Visual_Object_Retrieval.html">334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</a></p>
<p>7 0.55923992 <a title="306-lsi-7" href="./iccv-2013-Example-Based_Facade_Texture_Synthesis.html">148 iccv-2013-Example-Based Facade Texture Synthesis</a></p>
<p>8 0.55630273 <a title="306-lsi-8" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>9 0.55345607 <a title="306-lsi-9" href="./iccv-2013-Pedestrian_Parsing_via_Deep_Decompositional_Network.html">311 iccv-2013-Pedestrian Parsing via Deep Decompositional Network</a></p>
<p>10 0.54023957 <a title="306-lsi-10" href="./iccv-2013-SYM-FISH%3A_A_Symmetry-Aware_Flip_Invariant_Sketch_Histogram_Shape_Descriptor.html">368 iccv-2013-SYM-FISH: A Symmetry-Aware Flip Invariant Sketch Histogram Shape Descriptor</a></p>
<p>11 0.5313493 <a title="306-lsi-11" href="./iccv-2013-Monocular_Image_3D_Human_Pose_Estimation_under_Self-Occlusion.html">273 iccv-2013-Monocular Image 3D Human Pose Estimation under Self-Occlusion</a></p>
<p>12 0.52702731 <a title="306-lsi-12" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>13 0.52172226 <a title="306-lsi-13" href="./iccv-2013-Scene_Collaging%3A_Analysis_and_Synthesis_of_Natural_Images_with_Semantic_Layers.html">375 iccv-2013-Scene Collaging: Analysis and Synthesis of Natural Images with Semantic Layers</a></p>
<p>14 0.51869142 <a title="306-lsi-14" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>15 0.4897871 <a title="306-lsi-15" href="./iccv-2013-Discovering_Object_Functionality.html">118 iccv-2013-Discovering Object Functionality</a></p>
<p>16 0.48906127 <a title="306-lsi-16" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>17 0.48216256 <a title="306-lsi-17" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>18 0.47608489 <a title="306-lsi-18" href="./iccv-2013-A_Non-parametric_Bayesian_Network_Prior_of_Human_Pose.html">24 iccv-2013-A Non-parametric Bayesian Network Prior of Human Pose</a></p>
<p>19 0.47477463 <a title="306-lsi-19" href="./iccv-2013-Learning_CRFs_for_Image_Parsing_with_Adaptive_Subgradient_Descent.html">234 iccv-2013-Learning CRFs for Image Parsing with Adaptive Subgradient Descent</a></p>
<p>20 0.47050202 <a title="306-lsi-20" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.115), (12, 0.014), (26, 0.118), (31, 0.041), (34, 0.017), (40, 0.014), (41, 0.017), (42, 0.087), (52, 0.24), (64, 0.034), (73, 0.032), (89, 0.147), (98, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81078994 <a title="306-lda-1" href="./iccv-2013-Paper_Doll_Parsing%3A_Retrieving_Similar_Styles_to_Parse_Clothing_Items.html">306 iccv-2013-Paper Doll Parsing: Retrieving Similar Styles to Parse Clothing Items</a></p>
<p>Author: Kota Yamaguchi, M. Hadi Kiapour, Tamara L. Berg</p><p>Abstract: Clothing recognition is an extremely challenging problem due to wide variation in clothing item appearance, layering, and style. In this paper, we tackle the clothing parsing problem using a retrieval based approach. For a query image, we find similar styles from a large database of tagged fashion images and use these examples to parse the query. Our approach combines parsing from: pre-trained global clothing models, local clothing models learned on theflyfrom retrieved examples, and transferredparse masks (paper doll item transfer) from retrieved examples. Experimental evaluation shows that our approach significantly outperforms state of the art in parsing accuracy.</p><p>2 0.71066797 <a title="306-lda-2" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>Author: Matthijs Douze, Jérôme Revaud, Cordelia Schmid, Hervé Jégou</p><p>Abstract: This paper makes two complementary contributions to event retrieval in large collections of videos. First, we propose hyper-pooling strategies that encode the frame descriptors into a representation of the video sequence in a stable manner. Our best choices compare favorably with regular pooling techniques based on k-means quantization. Second, we introduce a technique to improve the ranking. It can be interpreted either as a query expansion method or as a similarity adaptation based on the local context of the query video descriptor. Experiments on public benchmarks show that our methods are complementary and improve event retrieval results, without sacrificing efficiency.</p><p>3 0.68528068 <a title="306-lda-3" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>Author: Suyog Dutt Jain, Kristen Grauman</p><p>Abstract: The mode of manual annotation used in an interactive segmentation algorithm affects both its accuracy and easeof-use. For example, bounding boxes are fast to supply, yet may be too coarse to get good results on difficult images; freehand outlines are slower to supply and more specific, yet they may be overkill for simple images. Whereas existing methods assume a fixed form of input no matter the image, we propose to predict the tradeoff between accuracy and effort. Our approach learns whether a graph cuts segmentation will succeed if initialized with a given annotation mode, based on the image ’s visual separability and foreground uncertainty. Using these predictions, we optimize the mode of input requested on new images a user wants segmented. Whether given a single image that should be segmented as quickly as possible, or a batch of images that must be segmented within a specified time budget, we show how to select the easiest modality that will be sufficiently strong to yield high quality segmentations. Extensive results with real users and three datasets demonstrate the impact.</p><p>4 0.68231988 <a title="306-lda-4" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>Author: Matthias Reso, Jörn Jachalsky, Bodo Rosenhahn, Jörn Ostermann</p><p>Abstract: Superpixel algorithms represent a very useful and increasingly popular preprocessing step for a wide range of computer vision applications, as they offer the potential to boost efficiency and effectiveness. In this regards, this paper presents a highly competitive approach for temporally consistent superpixelsfor video content. The approach is based on energy-minimizing clustering utilizing a novel hybrid clustering strategy for a multi-dimensional feature space working in a global color subspace and local spatial subspaces. Moreover, a new contour evolution based strategy is introduced to ensure spatial coherency of the generated superpixels. For a thorough evaluation the proposed approach is compared to state of the art supervoxel algorithms using established benchmarks and shows a superior performance.</p><p>5 0.68076849 <a title="306-lda-5" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>Author: S. Karthikeyan, Vignesh Jagadeesh, Renuka Shenoy, Miguel Ecksteinz, B.S. Manjunath</p><p>Abstract: Eye movement studies have confirmed that overt attention is highly biased towards faces and text regions in images. In this paper we explore a novel problem of predicting face and text regions in images using eye tracking data from multiple subjects. The problem is challenging as we aim to predict the semantics (face/text/background) only from eye tracking data without utilizing any image information. The proposed algorithm spatially clusters eye tracking data obtained in an image into different coherent groups and subsequently models the likelihood of the clusters containing faces and text using afully connectedMarkov Random Field (MRF). Given the eye tracking datafrom a test image, itpredicts potential face/head (humans, dogs and cats) and text locations reliably. Furthermore, the approach can be used to select regions of interest for further analysis by object detectors for faces and text. The hybrid eye position/object detector approach achieves better detection performance and reduced computation time compared to using only the object detection algorithm. We also present a new eye tracking dataset on 300 images selected from ICDAR, Street-view, Flickr and Oxford-IIIT Pet Dataset from 15 subjects.</p><p>6 0.68074858 <a title="306-lda-6" href="./iccv-2013-Class-Specific_Simplex-Latent_Dirichlet_Allocation_for_Image_Classification.html">73 iccv-2013-Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification</a></p>
<p>7 0.67986202 <a title="306-lda-7" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>8 0.67970854 <a title="306-lda-8" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>9 0.67827564 <a title="306-lda-9" href="./iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation.html">383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</a></p>
<p>10 0.67795622 <a title="306-lda-10" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>11 0.67712718 <a title="306-lda-11" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>12 0.67682689 <a title="306-lda-12" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>13 0.67645788 <a title="306-lda-13" href="./iccv-2013-Learning_a_Dictionary_of_Shape_Epitomes_with_Applications_to_Image_Labeling.html">245 iccv-2013-Learning a Dictionary of Shape Epitomes with Applications to Image Labeling</a></p>
<p>14 0.67594379 <a title="306-lda-14" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>15 0.67573106 <a title="306-lda-15" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>16 0.67569274 <a title="306-lda-16" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>17 0.67566526 <a title="306-lda-17" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>18 0.67515498 <a title="306-lda-18" href="./iccv-2013-A_Convex_Optimization_Framework_for_Active_Learning.html">6 iccv-2013-A Convex Optimization Framework for Active Learning</a></p>
<p>19 0.6751501 <a title="306-lda-19" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>20 0.67483002 <a title="306-lda-20" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
