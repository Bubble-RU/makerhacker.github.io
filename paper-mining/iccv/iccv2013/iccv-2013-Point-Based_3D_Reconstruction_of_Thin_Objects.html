<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-319" href="#">iccv2013-319</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</h1>
<br/><p>Source: <a title="iccv-2013-319-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Ummenhofer_Point-Based_3D_Reconstruction_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Benjamin Ummenhofer, Thomas Brox</p><p>Abstract: 3D reconstruction deals with the problem of finding the shape of an object from a set of images. Thin objects that have virtually no volumepose a special challengefor reconstruction with respect to shape representation and fusion of depth information. In this paper we present a dense pointbased reconstruction method that can deal with this special class of objects. We seek to jointly optimize a set of depth maps by treating each pixel as a point in space. Points are pulled towards a common surface by pairwise forces in an iterative scheme. The method also handles the problem of opposed surfaces by means of penalty forces. Efficient optimization is achieved by grouping points to superpixels and a spatial hashing approach for fast neighborhood queries. We show that the approach is on a par with state-of-the-art methods for standard multi view stereo settings and gives superior results for thin objects.</p><p>Reference: <a title="iccv-2013-319-reference" href="../iccv2013_reference/iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Thin objects that have virtually no volumepose a special challengefor reconstruction with respect to shape representation and fusion of depth information. [sent-4, score-0.317]
</p><p>2 We seek to jointly optimize a set of depth maps by treating each pixel as a point in space. [sent-6, score-0.452]
</p><p>3 Points are pulled towards a common surface by pairwise forces in an iterative scheme. [sent-7, score-0.333]
</p><p>4 The method also handles the problem of opposed surfaces by means of penalty forces. [sent-8, score-0.271]
</p><p>5 Efficient optimization is achieved by grouping points to superpixels and a spatial hashing approach for fast neighborhood queries. [sent-9, score-0.343]
</p><p>6 We show that the approach is on a par with state-of-the-art methods for standard multi view stereo settings and gives superior results for thin objects. [sent-10, score-0.304]
</p><p>7 Introduction Image-based 3D reconstruction is the problem of infer-  ring the surface of real world objects solely from visual clues. [sent-12, score-0.293]
</p><p>8 To the best of our knowledge, none of them addresses the reconstruction of very thin objects, such as the street sign in Fig. [sent-14, score-0.466]
</p><p>9 Such thin objects are very problematic for contemporary reconstruction methods. [sent-17, score-0.415]
</p><p>10 However, grids cannot properly represent objects thinner than the voxel size, and the fixed grid comes with high memory requirements, which severely limits the resolution. [sent-20, score-0.205]
</p><p>11 In the case of an arbitrary thin object, the resolution required to represent the object leads to extreme memory requirements. [sent-21, score-0.304]
</p><p>12 Top: Two renderings of the reconstruction when ignoring opposed surfaces (left and center) and a photo ofthe scene (right). [sent-24, score-0.321]
</p><p>13 Our approach resolves collisions between points that represent different sides of thin objects. [sent-28, score-0.555]
</p><p>14 Almost all points from the correct side pass the depth test (left and center) and therefore lie on the correct side. [sent-29, score-0.353]
</p><p>15 The approach preserves the thin structure of the objects, as seen in the view from the top (right). [sent-30, score-0.304]
</p><p>16 Another popular surface representation is by triangle meshes [8, 9, 5]. [sent-32, score-0.228]
</p><p>17 In contrast to voxel grids, they only model the surface rather than the whole scene volume, and different parts of the scene can be represented by triangles of different size. [sent-33, score-0.299]
</p><p>18 This makes mesh based algorithms suitable for large scale reconstructions [9], and potentially also allows to handle thin objects. [sent-34, score-0.361]
</p><p>19 However, mesh representations  typically have problems with change in topology during surface evolution. [sent-35, score-0.298]
</p><p>20 [9] create a Delaunay tetrahedral mesh where tetrahedra are labeled as inside 996699  or outside. [sent-37, score-0.206]
</p><p>21 The initial surface triangles are the tetrahedron faces that connect two tetrahedra with opposite labels. [sent-38, score-0.456]
</p><p>22 In case of a thin sheet, none of the tetrahedra would be labeled as inside the object and the triangulated surface would miss the object. [sent-39, score-0.598]
</p><p>23 We argue that the best representation for thin objects is a point cloud representation with reference to a set of registered depth maps. [sent-40, score-0.879]
</p><p>24 Similar to Szeliski and Tonnesen [23] and Fua [4] we use forces to manipulate the orientation and position of points. [sent-44, score-0.249]
</p><p>25 We can avoid the latter, because we keep a reference of the points to the depth maps from which they originated and allow only for motion of points along their projection rays. [sent-46, score-0.59]
</p><p>26 Finally, they generate a subset of high quality depth maps  by fusing the information from multiple neighboring depth maps. [sent-49, score-0.589]
</p><p>27 A limitation of depth maps is the affinity to a specific camera. [sent-50, score-0.296]
</p><p>28 In contrast, our approach treats the values of all depth maps as a point cloud and jointly optimizes all points, improving all depth maps at the same time. [sent-52, score-0.961]
</p><p>29 The PMVS approach of Furukawa and Ponce [6] uses a patch representation similar to a point cloud and potentially can deal with thin objects. [sent-53, score-0.673]
</p><p>30 Like in our approach, the depth and the normal of the patches is optimized. [sent-55, score-0.293]
</p><p>31 A common challenge of point cloud representations is computational efficiency because, in contrast to voxel grids or meshes, the neighborhood structure is not explicit and may change. [sent-57, score-0.598]
</p><p>32 We use efficient data structures and a coarseto-fine optimization based on superpixels to handle large point clouds with millions of points. [sent-58, score-0.298]
</p><p>33 Moreover, we explicitly deal with a problem that is specific to thin objects: if the object is regarded from opposite viewpoints, points from different surfaces basically share the same position but have opposite normals. [sent-59, score-0.838]
</p><p>34 Noise in the measurements will lead to contradictive results, where invisible points occlude visible ones. [sent-60, score-0.261]
</p><p>35 We call this the prob-  lem of opposed surfaces and introduce a coupling term in our energy model that deals with this problem. [sent-61, score-0.371]
</p><p>36 An initial point cloud is computed via incremental bundle adjustment and a variant of semi-global matching [10]. [sent-63, score-0.449]
</p><p>37 The heart of the approach is an energy model that regularizes the point cloud and pulls the points to common surfaces with normals that are consistent with the viewing direction. [sent-64, score-0.941]
</p><p>38 Initial depth maps and camera parameters The initialization of our algorithm consists of a set of depth maps and the corresponding camera projection matrices. [sent-67, score-0.712]
</p><p>39 For each we compute a depth map with semi-global matching [10]. [sent-73, score-0.206]
</p><p>40 We accumulate a simple sum of abso-  lute differences photometric error over 14 neighboring images for 128 depth labels. [sent-76, score-0.33]
</p><p>41 Our SGM implementation uses 32 directions and an adaptive penalty for large depth label changes steered by the gradient magnitude of the image. [sent-77, score-0.392]
</p><p>42 Camera parameters and depth values yield a coarse estimate of the scene. [sent-78, score-0.206]
</p><p>43 The depth maps contain a large amount of outliers and noise. [sent-79, score-0.342]
</p><p>44 Energy model We represent the surfaces of a scene by a set of oriented points P. [sent-81, score-0.282]
</p><p>45 The points are initially given by the dense depth maps, iP. [sent-82, score-0.389]
</p><p>46 It con∈tai Pns c tohrer essuprfoancdes position pi ∈n oRne3 and its normal vector ni at this point. [sent-86, score-0.562]
</p><p>47 Surfaces covered by many pixels in the image are automatically represented at a higher resolution in the reconstruction Points generated from different depth maps are unlikely to agree on the same surface due to noise, wrong measurements and inaccurate camera poses. [sent-88, score-0.649]
</p><p>48 We treat the point cloud as a particle simulation and define an energy that pulls close points towards a common surface: E  = Esmooth + αEdata + βEcollision. [sent-89, score-0.734]
</p><p>49 (1)  Edata keeps the points close to their measured position p0, Esmooth and Ecollision define pairwise forces that pull the 997700  points to a common surface and push the points to resolve self intersections, respectively. [sent-90, score-0.928]
</p><p>50 Each point in the point cloud corresponds to a depth map. [sent-93, score-0.685]
</p><p>51 We denote the distance that the point P has been moved away from its original position p0 by u and optimize this quantity together with the surface normal n associated with this point. [sent-97, score-0.523]
</p><p>52 The data term penalizes points that diverge from their initial position:  Edata=P? [sent-98, score-0.186]
</p><p>53 The energy Esmooth defines pairwise interactions of points and reads  Esmooth=P? [sent-106, score-0.305]
</p><p>54 Tthhee s eunrfearcgyes m mdeefaisnuerde by the neighboring points Pj . [sent-114, score-0.234]
</p><p>55 The energy for two points Pi and Pj is minimal if the points lie on the respective planes defined by their position and normal. [sent-115, score-0.515]
</p><p>56 The second term in (4) weights the angle between the normal ni and the neighboring point’s position pj . [sent-126, score-0.674]
</p><p>57 Points directly behind or in front of a point Pi should have a high influence as they promote a different position for the surface described by pi and ni, while a point near the tangent plane describes a similar surface at a different position. [sent-127, score-0.975]
</p><p>58 2 shows the value of wij for varying positions of pj . [sent-129, score-0.403]
</p><p>59 The choice of  the smoothing radius r defines the size of the neighborhood and therefore directly influences the runtime as well as the topology of the reconstruction. [sent-130, score-0.407]
</p><p>60 The radius r also relates to the depth uncertainty of the initial depth maps and should be chosen accordingly. [sent-134, score-0.651]
</p><p>61 The function η restricts the computation of the smoothness force to points that belong to the same surface. [sent-135, score-0.203]
</p><p>62 Points with normals pointing in different directions shall not influence each other; hence we define  ηni,nj=? [sent-136, score-0.215]
</p><p>63 > 0  (6)  We use the density ρi to normalize the energy and make it independent of the point density. [sent-142, score-0.275]
</p><p>64 ∈P  A special problem that arises for the reconstruction of thin objects are inconsistencies between the front-face and the back-face of a thin object. [sent-149, score-0.719]
</p><p>65 Due to noise, points with normals pointing in different directions may occlude each other. [sent-150, score-0.439]
</p><p>66 To resolve this opposed surface problem, we introduce a penalty force:  Ecollision=P? [sent-151, score-0.374]
</p><p>67 )  The energy measures the truncated signed distance of points Pi to the surfaces defined by the neighboring points Pj . [sent-155, score-0.679]
</p><p>68 The energy becomes non-zero if the distance of the points is positive and the normals have different directions (the dot product of the normals is negative). [sent-156, score-0.529]
</p><p>69 Point pairs Pi, Pj with this configuration are in conflict because they occlude each other but belong to opposite surfaces of the object. [sent-157, score-0.289]
</p><p>70 Point cloud optimization The gradient of the global energy (1) defines the forces that are used in an iterative scheme to optimize the position and normal of the points. [sent-159, score-0.899]
</p><p>71 The energy is non-convex due to the non-convex dependency of the weights w on the variables ui and ni. [sent-160, score-0.202]
</p><p>72 We assume that a sufficient number of points is close enough to the actual surface to find a good local minimum. [sent-161, score-0.329]
</p><p>73 We use gradient descent for fixed values of w and ρ to optimize the points and update w and ρ after each iteration, which yields a fixed point iteration scheme for w and ρ. [sent-162, score-0.518]
</p><p>74 Weight wij with radius r = 1for varying positions of pj relative to pi. [sent-170, score-0.513]
</p><p>75 The weight is low (black) when pj is far away and when the point is ’beside’ pi describing a different part of the surface. [sent-172, score-0.678]
</p><p>76 The update scheme is  nuit ++11== nuti t− τ τ∂∂unit EE,  (9)  where ni and Eni are parameterized with spherical coordinates in an appropriate local coordinate frame. [sent-173, score-0.209]
</p><p>77 The gradient descent scheme is very slow since the time step size τ must be chosen small to achieve convergence. [sent-174, score-0.215]
</p><p>78 We found that a mixture of coordinate descent and gradient descent significantly speeds up convergence. [sent-175, score-0.391]
</p><p>79 The sign ambiguity is resolved by the fact that the surface must point towards the camera that observes it. [sent-180, score-0.403]
</p><p>80 The energy (1) with fixed density ρ and weight w is a sum of weighted and possibly truncated ? [sent-182, score-0.205]
</p><p>81 Sorting these intervals with respect to the coordinate ui allows us to quickly compute the minimum. [sent-186, score-0.195]
</p><p>82 The sorting can be aborted as soon as the sign of the derivative changes and the minimum is found. [sent-187, score-0.201]
</p><p>83 Let uˆit be the position on the ray where the energy for the point is minimal. [sent-188, score-0.406]
</p><p>84 m We efo trra calkl points and decrease ω by the factor 21 when the minimum and maximum is not altered for 80% of the points in the  last iteration. [sent-192, score-0.294]
</p><p>85 To resolve remaining collisions we add a last iteration using the coordinate descent scheme for the variables u with ω = 1. [sent-195, score-0.397]
</p><p>86 The line search of the coordinate descent scheme allows to find a state free of collisions for points where the penalty forces act too late. [sent-198, score-0.737]
</p><p>87 Runtime optimization Processing point clouds with millions of points is computationally expensive. [sent-201, score-0.349]
</p><p>88 The time complexity for updating a point cloud with N fully connected points is in O(N2). [sent-202, score-0.516]
</p><p>89 Fortunately, due to the limited support ofthe smoothing kernel (5), the complexity can be reduced to O(N) since only neighboring points within a radius r need to be considered. [sent-203, score-0.42]
</p><p>90 We optimize the superpixel point cloud until convergence and transfer the positions and normals to the original point cloud. [sent-216, score-0.634]
</p><p>91 The optimization result of the superpixel point cloud yields a good approximation of the solution and greatly reduces the number of iterations spent on the original problem with N points. [sent-217, score-0.369]
</p><p>92 Outlier removal Due to erroneous depth maps, the initial point cloud may contain a large number of outliers, i. [sent-220, score-0.614]
</p><p>93 , points that do not describe an actual surface of the scene. [sent-222, score-0.329]
</p><p>94 To detect these outliers, we compute for each point Pi how many points from other images support the corresponding surface. [sent-224, score-0.293]
</p><p>95 A point Pj supports a point Pi if the position pi is close to the tangent plane defined by pj and nj . [sent-225, score-0.926]
</p><p>96 The use of superpixels reduces the size of the point cloud and significantly speeds up the optimization. [sent-227, score-0.506]
</p><p>97 The sampling density adapts to the scene depth to create superpixels with approximately equal size in space. [sent-230, score-0.344]
</p><p>98 (11) sj is the disk radius that we also use for rendering the point. [sent-236, score-0.226]
</p><p>99 The disk radius is simply computed as sj = where ξ is the depth of the point and f is the focal length. [sent-237, score-0.542]
</p><p>100 The neighborhood N contains only points within a radIi. [sent-239, score-0.208]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pj', 0.315), ('thin', 0.304), ('cloud', 0.259), ('pi', 0.253), ('depth', 0.206), ('surface', 0.182), ('forces', 0.151), ('points', 0.147), ('esmooth', 0.139), ('surfaces', 0.135), ('energy', 0.123), ('descent', 0.115), ('tetrahedra', 0.112), ('reconstruction', 0.111), ('point', 0.11), ('radius', 0.11), ('normals', 0.109), ('collisions', 0.104), ('position', 0.098), ('grids', 0.097), ('superpixels', 0.096), ('uit', 0.093), ('maps', 0.09), ('wij', 0.088), ('neighboring', 0.087), ('ni', 0.087), ('normal', 0.087), ('edata', 0.086), ('ecollision', 0.085), ('ui', 0.079), ('occlude', 0.077), ('opposite', 0.077), ('opposed', 0.075), ('ray', 0.075), ('coordinate', 0.071), ('voxel', 0.071), ('pointing', 0.065), ('pmvs', 0.062), ('neighborhood', 0.061), ('penalty', 0.061), ('camera', 0.06), ('derivative', 0.06), ('disk', 0.06), ('topology', 0.059), ('originate', 0.058), ('pulls', 0.058), ('clouds', 0.057), ('mesh', 0.057), ('rendered', 0.056), ('sj', 0.056), ('resolve', 0.056), ('force', 0.056), ('runtime', 0.055), ('sign', 0.051), ('scheme', 0.051), ('gradient', 0.049), ('influences', 0.047), ('outliers', 0.046), ('meshes', 0.046), ('triangles', 0.046), ('hash', 0.046), ('optimize', 0.046), ('soon', 0.045), ('intervals', 0.045), ('sorting', 0.045), ('density', 0.042), ('directions', 0.041), ('bundle', 0.041), ('speeds', 0.041), ('truncated', 0.04), ('tangent', 0.04), ('smoothing', 0.04), ('hashing', 0.039), ('initial', 0.039), ('deals', 0.038), ('tetrahedral', 0.037), ('freiburg', 0.037), ('contradictive', 0.037), ('persist', 0.037), ('pointbased', 0.037), ('iburg', 0.037), ('pns', 0.037), ('piti', 0.037), ('miso', 0.037), ('lute', 0.037), ('ainond', 0.037), ('thieni', 0.037), ('thinner', 0.037), ('tohrer', 0.037), ('simulation', 0.037), ('act', 0.037), ('support', 0.036), ('dense', 0.036), ('defines', 0.035), ('millions', 0.035), ('brox', 0.035), ('benjamin', 0.035), ('steered', 0.035), ('octrees', 0.035), ('merrell', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="319-tfidf-1" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>Author: Benjamin Ummenhofer, Thomas Brox</p><p>Abstract: 3D reconstruction deals with the problem of finding the shape of an object from a set of images. Thin objects that have virtually no volumepose a special challengefor reconstruction with respect to shape representation and fusion of depth information. In this paper we present a dense pointbased reconstruction method that can deal with this special class of objects. We seek to jointly optimize a set of depth maps by treating each pixel as a point in space. Points are pulled towards a common surface by pairwise forces in an iterative scheme. The method also handles the problem of opposed surfaces by means of penalty forces. Efficient optimization is achieved by grouping points to superpixels and a spatial hashing approach for fast neighborhood queries. We show that the approach is on a par with state-of-the-art methods for standard multi view stereo settings and gives superior results for thin objects.</p><p>2 0.19167015 <a title="319-tfidf-2" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>Author: Michael Weinmann, Aljosa Osep, Roland Ruiters, Reinhard Klein</p><p>Abstract: In this paper, we present a novel, robust multi-view normal field integration technique for reconstructing the full 3D shape of mirroring objects. We employ a turntablebased setup with several cameras and displays. These are used to display illumination patterns which are reflected by the object surface. The pattern information observed in the cameras enables the calculation of individual volumetric normal fields for each combination of camera, display and turntable angle. As the pattern information might be blurred depending on the surface curvature or due to nonperfect mirroring surface characteristics, we locally adapt the decoding to the finest still resolvable pattern resolution. In complex real-world scenarios, the normal fields contain regions without observations due to occlusions and outliers due to interreflections and noise. Therefore, a robust reconstruction using only normal information is challenging. Via a non-parametric clustering of normal hypotheses derived for each point in the scene, we obtain both the most likely local surface normal and a local surface consistency estimate. This information is utilized in an iterative mincut based variational approach to reconstruct the surface geometry.</p><p>3 0.17648527 <a title="319-tfidf-3" href="./iccv-2013-Shape_Anchors_for_Data-Driven_Multi-view_Reconstruction.html">387 iccv-2013-Shape Anchors for Data-Driven Multi-view Reconstruction</a></p>
<p>Author: Andrew Owens, Jianxiong Xiao, Antonio Torralba, William Freeman</p><p>Abstract: We present a data-driven method for building dense 3D reconstructions using a combination of recognition and multi-view cues. Our approach is based on the idea that there are image patches that are so distinctive that we can accurately estimate their latent 3D shapes solely using recognition. We call these patches shape anchors, and we use them as the basis of a multi-view reconstruction system that transfers dense, complex geometry between scenes. We “anchor” our 3D interpretation from these patches, using them to predict geometry for parts of the scene that are relatively ambiguous. The resulting algorithm produces dense reconstructions from stereo point clouds that are sparse and noisy, and we demonstrate it on a challenging dataset of real-world, indoor scenes.</p><p>4 0.17452133 <a title="319-tfidf-4" href="./iccv-2013-Large-Scale_Multi-resolution_Surface_Reconstruction_from_RGB-D_Sequences.html">228 iccv-2013-Large-Scale Multi-resolution Surface Reconstruction from RGB-D Sequences</a></p>
<p>Author: Frank Steinbrücker, Christian Kerl, Daniel Cremers</p><p>Abstract: We propose a method to generate highly detailed, textured 3D models of large environments from RGB-D sequences. Our system runs in real-time on a standard desktop PC with a state-of-the-art graphics card. To reduce the memory consumption, we fuse the acquired depth maps and colors in a multi-scale octree representation of a signed distance function. To estimate the camera poses, we construct a pose graph and use dense image alignment to determine the relative pose between pairs of frames. We add edges between nodes when we detect loop-closures and optimize the pose graph to correct for long-term drift. Our implementation is highly parallelized on graphics hardware to achieve real-time performance. More specifically, we can reconstruct, store, and continuously update a colored 3D model of an entire corridor of nine rooms at high levels of detail in real-time on a single GPU with 2.5GB.</p><p>5 0.16844721 <a title="319-tfidf-5" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>Author: Bastien Jacquet, Christian Häne, Kevin Köser, Marc Pollefeys</p><p>Abstract: Although specular objects have gained interest in recent years, virtually no approaches exist for markerless reconstruction of reflective scenes in the wild. In this work, we present a practical approach to capturing normal maps in real-world scenes using video only. We focus on nearly planar surfaces such as windows, facades from glass or metal, or frames, screens and other indoor objects and show how normal maps of these can be obtained without the use of an artificial calibration object. Rather, we track the reflections of real-world straight lines, while moving with a hand-held or vehicle-mounted camera in front of the object. In contrast to error-prone local edge tracking, we obtain the reflections by a robust, global segmentation technique of an ortho-rectified 3D video cube that also naturally allows efficient user interaction. Then, at each point of the reflective surface, the resulting 2D-curve to 3D-line correspondence provides a novel quadratic constraint on the local surface normal. This allows to globally solve for the shape by integrability and smoothness constraints and easily supports the usage of multiple lines. We demonstrate the technique on several objects and facades.</p><p>6 0.16792007 <a title="319-tfidf-6" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>7 0.16593283 <a title="319-tfidf-7" href="./iccv-2013-Saliency_Detection_in_Large_Point_Sets.html">370 iccv-2013-Saliency Detection in Large Point Sets</a></p>
<p>8 0.15763541 <a title="319-tfidf-8" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>9 0.15438306 <a title="319-tfidf-9" href="./iccv-2013-Quadruplet-Wise_Image_Similarity_Learning.html">332 iccv-2013-Quadruplet-Wise Image Similarity Learning</a></p>
<p>10 0.15350308 <a title="319-tfidf-10" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>11 0.15342808 <a title="319-tfidf-11" href="./iccv-2013-Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization.html">284 iccv-2013-Multiview Photometric Stereo Using Planar Mesh Parameterization</a></p>
<p>12 0.15034516 <a title="319-tfidf-12" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>13 0.14923966 <a title="319-tfidf-13" href="./iccv-2013-Image_Guided_Depth_Upsampling_Using_Anisotropic_Total_Generalized_Variation.html">209 iccv-2013-Image Guided Depth Upsampling Using Anisotropic Total Generalized Variation</a></p>
<p>14 0.14892003 <a title="319-tfidf-14" href="./iccv-2013-High_Quality_Shape_from_a_Single_RGB-D_Image_under_Uncalibrated_Natural_Illumination.html">199 iccv-2013-High Quality Shape from a Single RGB-D Image under Uncalibrated Natural Illumination</a></p>
<p>15 0.1457454 <a title="319-tfidf-15" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>16 0.14507847 <a title="319-tfidf-16" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>17 0.13623431 <a title="319-tfidf-17" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>18 0.1351919 <a title="319-tfidf-18" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>19 0.12897092 <a title="319-tfidf-19" href="./iccv-2013-PM-Huber%3A_PatchMatch_with_Huber_Regularization_for_Stereo_Matching.html">304 iccv-2013-PM-Huber: PatchMatch with Huber Regularization for Stereo Matching</a></p>
<p>20 0.12866554 <a title="319-tfidf-20" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.243), (1, -0.245), (2, -0.044), (3, 0.02), (4, -0.028), (5, 0.054), (6, 0.006), (7, -0.173), (8, -0.074), (9, 0.002), (10, -0.049), (11, 0.082), (12, -0.05), (13, 0.059), (14, 0.04), (15, -0.101), (16, -0.024), (17, 0.005), (18, -0.057), (19, -0.04), (20, -0.023), (21, 0.07), (22, 0.044), (23, -0.023), (24, -0.038), (25, 0.011), (26, -0.028), (27, 0.089), (28, 0.013), (29, 0.059), (30, 0.018), (31, -0.021), (32, 0.017), (33, -0.048), (34, -0.007), (35, -0.07), (36, 0.076), (37, 0.065), (38, 0.061), (39, -0.079), (40, 0.018), (41, -0.098), (42, -0.039), (43, 0.03), (44, -0.049), (45, -0.008), (46, 0.008), (47, -0.048), (48, -0.001), (49, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97354621 <a title="319-lsi-1" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>Author: Benjamin Ummenhofer, Thomas Brox</p><p>Abstract: 3D reconstruction deals with the problem of finding the shape of an object from a set of images. Thin objects that have virtually no volumepose a special challengefor reconstruction with respect to shape representation and fusion of depth information. In this paper we present a dense pointbased reconstruction method that can deal with this special class of objects. We seek to jointly optimize a set of depth maps by treating each pixel as a point in space. Points are pulled towards a common surface by pairwise forces in an iterative scheme. The method also handles the problem of opposed surfaces by means of penalty forces. Efficient optimization is achieved by grouping points to superpixels and a spatial hashing approach for fast neighborhood queries. We show that the approach is on a par with state-of-the-art methods for standard multi view stereo settings and gives superior results for thin objects.</p><p>2 0.83981669 <a title="319-lsi-2" href="./iccv-2013-Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization.html">284 iccv-2013-Multiview Photometric Stereo Using Planar Mesh Parameterization</a></p>
<p>Author: Jaesik Park, Sudipta N. Sinha, Yasuyuki Matsushita, Yu-Wing Tai, In So Kweon</p><p>Abstract: We propose a method for accurate 3D shape reconstruction using uncalibrated multiview photometric stereo. A coarse mesh reconstructed using multiview stereo is first parameterized using a planar mesh parameterization technique. Subsequently, multiview photometric stereo is performed in the 2D parameter domain of the mesh, where all geometric and photometric cues from multiple images can be treated uniformly. Unlike traditional methods, there is no need for merging view-dependent surface normal maps. Our key contribution is a new photometric stereo based mesh refinement technique that can efficiently reconstruct meshes with extremely fine geometric details by directly estimating a displacement texture map in the 2D parameter domain. We demonstrate that intricate surface geometry can be reconstructed using several challenging datasets containing surfaces with specular reflections, multiple albedos and complex topologies.</p><p>3 0.82460332 <a title="319-lsi-3" href="./iccv-2013-Large-Scale_Multi-resolution_Surface_Reconstruction_from_RGB-D_Sequences.html">228 iccv-2013-Large-Scale Multi-resolution Surface Reconstruction from RGB-D Sequences</a></p>
<p>Author: Frank Steinbrücker, Christian Kerl, Daniel Cremers</p><p>Abstract: We propose a method to generate highly detailed, textured 3D models of large environments from RGB-D sequences. Our system runs in real-time on a standard desktop PC with a state-of-the-art graphics card. To reduce the memory consumption, we fuse the acquired depth maps and colors in a multi-scale octree representation of a signed distance function. To estimate the camera poses, we construct a pose graph and use dense image alignment to determine the relative pose between pairs of frames. We add edges between nodes when we detect loop-closures and optimize the pose graph to correct for long-term drift. Our implementation is highly parallelized on graphics hardware to achieve real-time performance. More specifically, we can reconstruct, store, and continuously update a colored 3D model of an entire corridor of nine rooms at high levels of detail in real-time on a single GPU with 2.5GB.</p><p>4 0.80113 <a title="319-lsi-4" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>Author: Qian-Yi Zhou, Stephen Miller, Vladlen Koltun</p><p>Abstract: We present an approach to reconstruction of detailed scene geometry from range video. Range data produced by commodity handheld cameras suffers from high-frequency errors and low-frequency distortion. Our approach deals with both sources of error by reconstructing locally smooth scene fragments and letting these fragments deform in order to align to each other. We develop a volumetric registration formulation that leverages the smoothness of the deformation to make optimization practical for large scenes. Experimental results demonstrate that our approach substantially increases the fidelity of complex scene geometry reconstructed with commodity handheld cameras.</p><p>5 0.76246971 <a title="319-lsi-5" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>Author: Diego Thomas, Akihiro Sugimoto</p><p>Abstract: Updating a global 3D model with live RGB-D measurements has proven to be successful for 3D reconstruction of indoor scenes. Recently, a Truncated Signed Distance Function (TSDF) volumetric model and a fusion algorithm have been introduced (KinectFusion), showing significant advantages such as computational speed and accuracy of the reconstructed scene. This algorithm, however, is expensive in memory when constructing and updating the global model. As a consequence, the method is not well scalable to large scenes. We propose a new flexible 3D scene representation using a set of planes that is cheap in memory use and, nevertheless, achieves accurate reconstruction of indoor scenes from RGB-D image sequences. Projecting the scene onto different planes reduces significantly the size of the scene representation and thus it allows us to generate a global textured 3D model with lower memory requirement while keeping accuracy and easiness to update with live RGB-D measurements. Experimental results demonstrate that our proposed flexible 3D scene representation achieves accurate reconstruction, while keeping the scalability for large indoor scenes.</p><p>6 0.759525 <a title="319-lsi-6" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>7 0.72447366 <a title="319-lsi-7" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>8 0.6982038 <a title="319-lsi-8" href="./iccv-2013-Shape_Anchors_for_Data-Driven_Multi-view_Reconstruction.html">387 iccv-2013-Shape Anchors for Data-Driven Multi-view Reconstruction</a></p>
<p>9 0.67156291 <a title="319-lsi-9" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>10 0.67038196 <a title="319-lsi-10" href="./iccv-2013-High_Quality_Shape_from_a_Single_RGB-D_Image_under_Uncalibrated_Natural_Illumination.html">199 iccv-2013-High Quality Shape from a Single RGB-D Image under Uncalibrated Natural Illumination</a></p>
<p>11 0.66708207 <a title="319-lsi-11" href="./iccv-2013-Live_Metric_3D_Reconstruction_on_Mobile_Phones.html">254 iccv-2013-Live Metric 3D Reconstruction on Mobile Phones</a></p>
<p>12 0.66238374 <a title="319-lsi-12" href="./iccv-2013-Data-Driven_3D_Primitives_for_Single_Image_Understanding.html">102 iccv-2013-Data-Driven 3D Primitives for Single Image Understanding</a></p>
<p>13 0.65674525 <a title="319-lsi-13" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>14 0.64656949 <a title="319-lsi-14" href="./iccv-2013-3D_Scene_Understanding_by_Voxel-CRF.html">2 iccv-2013-3D Scene Understanding by Voxel-CRF</a></p>
<p>15 0.6442287 <a title="319-lsi-15" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>16 0.62163752 <a title="319-lsi-16" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>17 0.62152737 <a title="319-lsi-17" href="./iccv-2013-A_Generic_Deformation_Model_for_Dense_Non-rigid_Surface_Registration%3A_A_Higher-Order_MRF-Based_Approach.html">16 iccv-2013-A Generic Deformation Model for Dense Non-rigid Surface Registration: A Higher-Order MRF-Based Approach</a></p>
<p>18 0.60577351 <a title="319-lsi-18" href="./iccv-2013-Image_Guided_Depth_Upsampling_Using_Anisotropic_Total_Generalized_Variation.html">209 iccv-2013-Image Guided Depth Upsampling Using Anisotropic Total Generalized Variation</a></p>
<p>19 0.59408492 <a title="319-lsi-19" href="./iccv-2013-Geometric_Registration_Based_on_Distortion_Estimation.html">183 iccv-2013-Geometric Registration Based on Distortion Estimation</a></p>
<p>20 0.58963275 <a title="319-lsi-20" href="./iccv-2013-Depth_from_Combining_Defocus_and_Correspondence_Using_Light-Field_Cameras.html">108 iccv-2013-Depth from Combining Defocus and Correspondence Using Light-Field Cameras</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.058), (6, 0.014), (7, 0.014), (26, 0.064), (31, 0.039), (40, 0.016), (42, 0.096), (48, 0.012), (64, 0.056), (73, 0.05), (78, 0.149), (89, 0.327), (95, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97982121 <a title="319-lda-1" href="./iccv-2013-Line_Assisted_Light_Field_Triangulation_and_Stereo_Matching.html">252 iccv-2013-Line Assisted Light Field Triangulation and Stereo Matching</a></p>
<p>Author: Zhan Yu, Xinqing Guo, Haibing Lin, Andrew Lumsdaine, Jingyi Yu</p><p>Abstract: Light fields are image-based representations that use densely sampled rays as a scene description. In this paper, we explore geometric structures of 3D lines in ray space for improving light field triangulation and stereo matching. The triangulation problem aims to fill in the ray space with continuous and non-overlapping simplices anchored at sampled points (rays). Such a triangulation provides a piecewise-linear interpolant useful for light field superresolution. We show that the light field space is largely bilinear due to 3D line segments in the scene, and direct triangulation of these bilinear subspaces leads to large errors. We instead present a simple but effective algorithm to first map bilinear subspaces to line constraints and then apply Constrained Delaunay Triangulation (CDT). Based on our analysis, we further develop a novel line-assisted graphcut (LAGC) algorithm that effectively encodes 3D line constraints into light field stereo matching. Experiments on synthetic and real data show that both our triangulation and LAGC algorithms outperform state-of-the-art solutions in accuracy and visual quality.</p><p>2 0.97138906 <a title="319-lda-2" href="./iccv-2013-Capturing_Global_Semantic_Relationships_for_Facial_Action_Unit_Recognition.html">69 iccv-2013-Capturing Global Semantic Relationships for Facial Action Unit Recognition</a></p>
<p>Author: Ziheng Wang, Yongqiang Li, Shangfei Wang, Qiang Ji</p><p>Abstract: In this paper we tackle the problem of facial action unit (AU) recognition by exploiting the complex semantic relationships among AUs, which carry crucial top-down information yet have not been thoroughly exploited. Towards this goal, we build a hierarchical model that combines the bottom-level image features and the top-level AU relationships to jointly recognize AUs in a principled manner. The proposed model has two major advantages over existing methods. 1) Unlike methods that can only capture local pair-wise AU dependencies, our model is developed upon the restricted Boltzmann machine and therefore can exploit the global relationships among AUs. 2) Although AU relationships are influenced by many related factors such as facial expressions, these factors are generally ignored by the current methods. Our model, however, can successfully capture them to more accurately characterize the AU relationships. Efficient learning and inference algorithms of the proposed model are also developed. Experimental results on benchmark databases demonstrate the effectiveness of the proposed approach in modelling complex AU relationships as well as its superior AU recognition performance over existing approaches.</p><p>3 0.95923877 <a title="319-lda-3" href="./iccv-2013-From_Actemes_to_Action%3A_A_Strongly-Supervised_Representation_for_Detailed_Action_Understanding.html">175 iccv-2013-From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding</a></p>
<p>Author: Weiyu Zhang, Menglong Zhu, Konstantinos G. Derpanis</p><p>Abstract: This paper presents a novel approach for analyzing human actions in non-scripted, unconstrained video settings based on volumetric, x-y-t, patch classifiers, termed actemes. Unlike previous action-related work, the discovery of patch classifiers is posed as a strongly-supervised process. Specifically, keypoint labels (e.g., position) across spacetime are used in a data-driven training process to discover patches that are highly clustered in the spacetime keypoint configuration space. To support this process, a new human action dataset consisting of challenging consumer videos is introduced, where notably the action label, the 2D position of a set of keypoints and their visibilities are provided for each video frame. On a novel input video, each acteme is used in a sliding volume scheme to yield a set of sparse, non-overlapping detections. These detections provide the intermediate substrate for segmenting out the action. For action classification, the proposed representation shows significant improvement over state-of-the-art low-level features, while providing spatiotemporal localiza- tion as additional output. This output sheds further light into detailed action understanding.</p><p>4 0.95635349 <a title="319-lda-4" href="./iccv-2013-Recognising_Human-Object_Interaction_via_Exemplar_Based_Modelling.html">344 iccv-2013-Recognising Human-Object Interaction via Exemplar Based Modelling</a></p>
<p>Author: Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai, Shaogang Gong, Tao Xiang</p><p>Abstract: Human action can be recognised from a single still image by modelling Human-object interaction (HOI), which infers the mutual spatial structure information between human and object as well as their appearance. Existing approaches rely heavily on accurate detection of human and object, and estimation of human pose. They are thus sensitive to large variations of human poses, occlusion and unsatisfactory detection of small size objects. To overcome this limitation, a novel exemplar based approach is proposed in this work. Our approach learns a set of spatial pose-object interaction exemplars, which are density functions describing how a person is interacting with a manipulated object for different activities spatially in a probabilistic way. A representation based on our HOI exemplar thus has great potential for being robust to the errors in human/object detection and pose estimation. A new framework consists of a proposed exemplar based HOI descriptor and an activity specific matching model that learns the parameters is formulated for robust human activity recog- nition. Experiments on two benchmark activity datasets demonstrate that the proposed approach obtains state-ofthe-art performance.</p><p>same-paper 5 0.95519269 <a title="319-lda-5" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>Author: Benjamin Ummenhofer, Thomas Brox</p><p>Abstract: 3D reconstruction deals with the problem of finding the shape of an object from a set of images. Thin objects that have virtually no volumepose a special challengefor reconstruction with respect to shape representation and fusion of depth information. In this paper we present a dense pointbased reconstruction method that can deal with this special class of objects. We seek to jointly optimize a set of depth maps by treating each pixel as a point in space. Points are pulled towards a common surface by pairwise forces in an iterative scheme. The method also handles the problem of opposed surfaces by means of penalty forces. Efficient optimization is achieved by grouping points to superpixels and a spatial hashing approach for fast neighborhood queries. We show that the approach is on a par with state-of-the-art methods for standard multi view stereo settings and gives superior results for thin objects.</p><p>6 0.94519264 <a title="319-lda-6" href="./iccv-2013-Facial_Action_Unit_Event_Detection_by_Cascade_of_Tasks.html">155 iccv-2013-Facial Action Unit Event Detection by Cascade of Tasks</a></p>
<p>7 0.93405133 <a title="319-lda-7" href="./iccv-2013-Modeling_4D_Human-Object_Interactions_for_Event_and_Object_Recognition.html">268 iccv-2013-Modeling 4D Human-Object Interactions for Event and Object Recognition</a></p>
<p>8 0.92829216 <a title="319-lda-8" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>9 0.92404521 <a title="319-lda-9" href="./iccv-2013-New_Graph_Structured_Sparsity_Model_for_Multi-label_Image_Annotations.html">290 iccv-2013-New Graph Structured Sparsity Model for Multi-label Image Annotations</a></p>
<p>10 0.92045617 <a title="319-lda-10" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>11 0.91980517 <a title="319-lda-11" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>12 0.91780609 <a title="319-lda-12" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<p>13 0.91760266 <a title="319-lda-13" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>14 0.91619527 <a title="319-lda-14" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>15 0.91432858 <a title="319-lda-15" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>16 0.91376764 <a title="319-lda-16" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>17 0.91358292 <a title="319-lda-17" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>18 0.91301775 <a title="319-lda-18" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>19 0.9124949 <a title="319-lda-19" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>20 0.91237748 <a title="319-lda-20" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
