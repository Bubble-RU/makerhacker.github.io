<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-291" href="#">iccv2013-291</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</h1>
<br/><p>Source: <a title="iccv-2013-291-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Yan_No_Matter_Where_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Yan Yan, Elisa Ricci, Ramanathan Subramanian, Oswald Lanz, Nicu Sebe</p><p>Abstract: We propose a novel Multi-Task Learning framework (FEGA-MTL) for classifying the head pose of a person who moves freely in an environment monitored by multiple, large field-of-view surveillance cameras. As the target (person) moves, distortions in facial appearance owing to camera perspective and scale severely impede performance of traditional head pose classification methods. FEGA-MTL operates on a dense uniform spatial grid and learns appearance relationships across partitions as well as partition-specific appearance variations for a given head pose to build region-specific classifiers. Guided by two graphs which a-priori model appearance similarity among (i) grid partitions based on camera geometry and (ii) head pose classes, the learner efficiently clusters appearancewise related grid partitions to derive the optimal partitioning. For pose classification, upon determining the target’s position using a person tracker, the appropriate regionspecific classifier is invoked. Experiments confirm that FEGA-MTL achieves state-of-the-art classification with few training data.</p><p>Reference: <a title="iccv-2013-291-reference" href="../iccv2013_reference/iccv-2013-No_Matter_Where_You_Are%3A_Flexible_Graph-Guided_Multi-task_Learning_for_Multi-view_Head_Pose_Classification_under_Target_Motion_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 s  Abstract We propose a novel Multi-Task Learning framework (FEGA-MTL) for classifying the head pose of a person who moves freely in an environment monitored by multiple, large field-of-view surveillance cameras. [sent-5, score-1.073]
</p><p>2 As the target (person) moves, distortions in facial appearance owing to camera perspective and scale severely impede performance of traditional head pose classification methods. [sent-6, score-1.463]
</p><p>3 FEGA-MTL operates on a dense uniform spatial grid and learns appearance relationships across partitions as well as partition-specific appearance variations for a given head pose to build region-specific classifiers. [sent-7, score-1.392]
</p><p>4 Guided by two graphs which a-priori model appearance similarity among (i) grid partitions based on camera geometry and (ii) head pose classes, the learner efficiently clusters appearancewise related grid partitions to derive the optimal partitioning. [sent-8, score-1.769]
</p><p>5 For pose classification, upon determining the target’s position using a person tracker, the appropriate regionspecific classifier is invoked. [sent-9, score-0.439]
</p><p>6 Introduction  ×  Head pose estimation and tracking is critical for surveillance and human-behavior understanding, and has been extensively studied for over a decade [15]. [sent-12, score-0.312]
</p><p>7 However, most existing approaches compute the head pose from high resolution images, where facial features are clearly visible. [sent-13, score-0.987]
</p><p>8 Estimating the head pose from large field-of-view surveillance cameras, where faces are typically captured at 50 50 or cloew cearm pixel r wehsoelruet ifoanc,e sh aasre ere tycpeiicveadlly importance only recently [5, 16, 19]. [sent-14, score-0.899]
</p><p>9 Computing the head pose under these conditions is difficult, as faces appear blurred and models employing detailed facial information are ineffective. [sent-15, score-1.054]
</p><p>10 (left)Facial p ear ncechangeundertargetmoti n:for  the same 3D head pose, automatically extracted face crops corresponding to camera C1-C4 are shown for target positions P1P3. [sent-22, score-0.896]
</p><p>11 Fewer still are head pose estimation methods that uti-  lize information from multiple surveillance cameras. [sent-24, score-0.899]
</p><p>12 Employing a single camera view is insufficient for studying people’s behavior in large environments and multi-view images have been exploited to achieve robust pose estimation [14, 22, 17, 20, 23]. [sent-25, score-0.348]
</p><p>13 However, methods such as [14, 20] estimate pose as a person rotates in place, but is not freely moving around in the environment. [sent-26, score-0.424]
</p><p>14 The broader goal of this work is to analyze behavior [13] from head pose cues in unstructured interactive settings (e. [sent-27, score-0.85]
</p><p>15 Therefore, in this paper we consider the problem of multi-view head pose classification under target motion. [sent-30, score-1.032]
</p><p>16 The facial appearance of a target with identical 3D head pose but at different positions varies considerably due to perspective and scale. [sent-33, score-1.191]
</p><p>17 As the target moves, the face can appear larger/smaller and face parts can become occluded/visible due to the target’s relative position with respect to the camera. [sent-34, score-0.236]
</p><p>18 We investigated the effect of appearance change on pose classification using the DPOSE dataset [17], which comprises synchronously recorded images of moving persons from four camera views, associated target positions and head pose annotations. [sent-35, score-1.592]
</p><p>19 The SVM was then tested with images from each of the quad-  rants and the task was to assign head pose to one of eight classes, each denoting a quantized 45o (360o/8) head-pan. [sent-37, score-0.892]
</p><p>20 Much lower accuracies were obtained when training and test images came from different quadrants, confirming the adverse impact of positioninduced appearance changes on head pose classification. [sent-40, score-1.02]
</p><p>21 To address this issue, we propose FEGA-MTL, a FlExible GrAph-guided Multi-Task Learning framework for multi-view head pose classification under target motion. [sent-41, score-1.032]
</p><p>22 Analogous with the MTL problem, one can expect some similarity in facial appearance for a given head pose across the regions, and region-specific differences owing to perspective and scale. [sent-45, score-1.197]
</p><p>23 FEGA-MTL seeks to simultaneously learn the relationship between facial appearance and head pose across all partitions of a dense uniform 2D spatial grid. [sent-46, score-1.259]
</p><p>24 Therefore, we devise a method where appearance-wise related grid clusters (which denote related tasks) are flexibly discovered, and the within-cluster appear-  ance similarity is modeled via the MTL parameters. [sent-48, score-0.269]
</p><p>25 During the classification stage, upon determining the position corresponding to a test instance using a person tracker, the corresponding region-specific classifier is invoked. [sent-50, score-0.262]
</p><p>26 Experiments confirm that FEGA-MTL outperforms competing head pose classification and MTL approaches. [sent-52, score-0.965]
</p><p>27 In [16], a Kullback-Leibler distance-based facial appearance descriptor is proposed for low resolution images. [sent-57, score-0.245]
</p><p>28 In [3, 5], head pose estimation with weak or no supervision is achieved employing motion-based cues and constraints imposed by joint modeling of head and body pose. [sent-59, score-1.504]
</p><p>29 However, all these works address single view head pose classification. [sent-60, score-0.85]
</p><p>30 Few works estimate head pose fusing information from multiple views [14, 17, 20, 23]. [sent-61, score-0.902]
</p><p>31 In [14], SVMs are employed to calculate a probability distribution for head pose in each view. [sent-64, score-0.85]
</p><p>32 Nevertheless, both these works attempt to determine head orientation as a person rotates in place and position-induced appearance variations are not considered. [sent-66, score-0.768]
</p><p>33 A weighted distance approach for classifying pose under target motion is proposed in [17]. [sent-67, score-0.394]
</p><p>34 Upon dividing the  space into four quadrants, max-margin distance learning is employed to learn a classifier per region– such a rigid space partitioning scheme will not optimally encode the pose-appearance relationship under motion, with arbitrary camera geometry. [sent-68, score-0.243]
</p><p>35 In [23], head pose under motion is determined by mapping the target’s face texture onto a spherical head model, and subsequently locating the face in the unfolded spherical head image. [sent-69, score-2.202]
</p><p>36 However, many camera views are required to produce an accurate texture map– 9 cameras are used in [23]. [sent-70, score-0.171]
</p><p>37 Overview  of the proposed head pose classification framework  assuming three camera views. [sent-78, score-1.021]
</p><p>38 The region graph and optimal  partitioning are as seen from a fourth (camera-less) view. [sent-79, score-0.166]
</p><p>39 in the form of a graph) defining task dependencies [6] or learn the task relationships simultaneously with task-specific parameters [11, 24, 10, 25, 9]. [sent-84, score-0.196]
</p><p>40 We also overcome the limitations of [6] as FEGA-MTL automatically discovers task relationships and refines the initial graph structure. [sent-87, score-0.188]
</p><p>41 For multi-view head pose estimation under motion, the graph structure is very useful as it reflects interregion facial appearance similarity as derived from the camera geometry. [sent-88, score-1.286]
</p><p>42 2 presents an overview of our multi-view head pose classification system which consists of three phases: (1) preprocessing and extraction of multi-view face appearance descriptors, (2) learning of head pose-appearance relationships under motion with FEGA-MTL and (3) classification. [sent-93, score-1.823]
</p><p>43 As we deal with freely moving targets, in the preprocessing stage, a color-based particle filter tracker incorporating multi-view geometry information is employed to reliably localize the target’s face and extract multi-view face crops. [sent-94, score-0.359]
</p><p>44 Also, the tracker allows for determining the target position corresponding to a test instance, so that the appropriate region-based pose classifier can be invoked. [sent-95, score-0.515]
</p><p>45 Features extracted from the multi-view face appearance images are fed to the FEGA-MTL module for learning region-specific classification parameters. [sent-96, score-0.249]
</p><p>46 Assuming a spherical model of the head, a contour likelihood is computed for each grid point by projecting a 3D sphere onto each view using camera calibration information. [sent-106, score-0.243]
</p><p>47 The grid point with the highest likelihood sum is determined as the head location. [sent-107, score-0.711]
</p><p>48 The 1The grid size accounts for the tracker’s variance and horizontal and vertical offsets of the head from the body centroid due to pan, tilt and roll. [sent-108, score-0.711]
</p><p>49 11 117799  tracking and head localization procedures are illustrated in Fig. [sent-109, score-0.587]
</p><p>50 The head is then cropped and resized to 20 20 pixels . [sent-111, score-0.587]
</p><p>51 Head crops from the different views are concatenated to generate the multi-view face crops as shown in Fig. [sent-114, score-0.253]
</p><p>52 2, and similar to previous works [3, 5], we employ HOG descriptors to effectively describe the face appearance for head pose classification. [sent-115, score-1.013]
</p><p>53 The multiview face appearance image is divided into non-overlapping 4 4 patches, and a 9-bin histogram is used as the HOG descriptor tfcohre esa,c ahn image patch. [sent-116, score-0.163]
</p><p>54 , C} are the head pose labels (C = 8 classes in our setting). [sent-132, score-0.89]
</p><p>55 OCn}e a roef the graphs guiding the learning process specifies the similarity in appearance for a given head pose across regions based on camera geometry. [sent-133, score-1.218]
</p><p>56 As mentioned earlier, we model the target’s head as a sphere. [sent-135, score-0.587]
</p><p>57 Let Zk denote the sphere placed at the target’s 3D head position pk, and whose multi-view camera projection yields training image Ik in Tm. [sent-136, score-0.702]
</p><p>58 The appearance distortion over U camera views due to dis-  placement v from pk to pl is defined as δ(Zk , pk → pl) = ? [sent-139, score-0.578]
</p><p>59 The appearance similarity between regions m and n is then computed based on a Gaussian model by considering distortion between all image-pairs associated to Tm, Tn as: e−NmNΩnσ2  ? [sent-146, score-0.228]
</p><p>60 3 depicts the appearance similarity maps for two different camera configurations when the head-sphere at pk is moved around in space (the projection of pk on the ground is denoted by the red ‘X’). [sent-154, score-0.483]
</p><p>61 When pk is close to the cameraless room corner in the 3-camera setup, a number of regions around pk share a high appearance similarity, implying that pose-appearance relationship can be learnt jointly in these regions. [sent-155, score-0.486]
</p><p>62 However, the similarity measure decreases sharply as the target moves from pk towards any of the three cameras, and tends to zero for the upper diagonal half of the room. [sent-156, score-0.315]
</p><p>63 Also, when a camera is introduced in the fourth room corner, appearance similarity holds only for a smaller portion of space around pk as compared to the 3-camera case. [sent-157, score-0.4]
</p><p>64 A second graph guiding the learning process models the fact that facial appearances should be more similar for neighboring pose classes as com-  pared to non-neighboring classes. [sent-159, score-0.565]
</p><p>65 2, the facial appearance of exemplars from class 1 should be most similar to exemplars from class 2 and 8. [sent-161, score-0.245]
</p><p>66 Exploiting this information, a pose graph E2 is defined with aEsxspolcoiiattiendg edge weights βij = p 1s eif g ria apnhd E j correspond to neighboring pose classes ci, cj, and βij = 0 otherwise. [sent-162, score-0.692]
</p><p>67 For each region t and pose class c, we propose to learn the region-specific weight vectors for pose classification wt,c = st,c + θt,c, wt,c, st,c, θt,c ∈ I RD. [sent-241, score-0.663]
</p><p>68 )∈E2 where γij ’s and βij ’s are the appearance similarity-based weights of region graph edges E1 and pose graph edges E2 respectively as odnes gcrraipbehd e ding eSse cE 3. [sent-333, score-0.581]
</p><p>69 While testing, upon determining the region t associated to a test sample xtest using the person tracker, the corresponding wt,c’s are used  to compute the head pose label as ac=rg1,m. [sent-431, score-1.017]
</p><p>70 Experimental Results In this section, we compare head pose classification results achieved with FEGA-MTL against (i) state-of-theart head pose estimation methods and (ii) other MTL approaches. [sent-437, score-1.786]
</p><p>71 To our knowledge, there are no other databases for benchmarking multi-view head pose classification performance under target motion. [sent-439, score-1.032]
</p><p>72 The CLEAR [18] and UcoHead [14] databases are recorded with targets rotating inplace, while the dataset proposed in [23] does not include ground-truth head pose measurements for moving targets. [sent-440, score-1.008]
</p><p>73 DPOSE comprises over 50000 4-view synchronized images recorded for 16 moving targets, with associated positional and head pose measurements (target positions are computed using the person tracker [12]). [sent-441, score-1.109]
</p><p>74 We consider an initial, uniformly spaced grid with R = 25 regions as shown in Fig. [sent-449, score-0.169]
</p><p>75 Table 1 presents results comparing FEGA-MTL with competing head pose classification methods. [sent-452, score-0.993]
</p><p>76 As shown in the table, both these methods perform poorly with respect to the proposed approach, as they are not designed to account for facial distortions due to scale/perspective changes. [sent-455, score-0.178]
</p><p>77 A better strategy in such cases is to compensate for position-induced appearance distortions in some way [17, 23]. [sent-456, score-0.149]
</p><p>78 We implemented a radial basis SVM to determine head pose from the warped 4-view images. [sent-459, score-0.85]
</p><p>79 It is pertinent to point out two differences between our approach and [17]– [17] proposes a pre-defined division of space (the room is divided into 4 quadrants) which is not necessarily optimal for describing the pose-appearance relationship under arbitrary camera geometry. [sent-461, score-0.17]
</p><p>80 Secondly, task relationships are not considered in [17], and an independent classifier is used for each quadrant. [sent-462, score-0.154]
</p><p>81 In contrast, FEGA-MTL discovers the optimal configuration of grid clusters that best describes the pose-appearance relationship given camera geometry. [sent-463, score-0.319]
</p><p>82 Considering task relationships enables FEGA-MTL to achieve higher classification accuracy than a single global classifier (Single SVM), Single SVM+Warping and separate region-specific classifiers that do not consider inter-region appearance relationships (Multiple Region-specific SVMs). [sent-464, score-0.43]
</p><p>83 We  also repeated the experiments employing only two of the four camera views for head pose classification, and while obtained accuracies are expectedly lower in this case, the accuracy trends are still consistent with the 4-view scenario. [sent-469, score-1.159]
</p><p>84 The advantage of employing MTL for head pose classification under target motion is obvious since all MTL approaches greatly outperform single SVM. [sent-471, score-1.099]
</p><p>85 Moreover, having a flexible learning algorithm which is able to infer appearance relationships among regions provides some advantages in terms of classification accuracy. [sent-472, score-0.369]
</p><p>86 FEGA-MTL, which independently considers features and employs graphs to explicitly model region and head pose-based appearance relationships, achieves the best 11 118822  Table 1. [sent-474, score-0.804]
</p><p>87 Using the region graph alone is beneficial as such, while employing the region and pose graphs in conjunction produces the best classification performance. [sent-497, score-0.669]
</p><p>88 3 shows the initial spatial grid and the optimal spatial partitioning learned for a three-camera system with 5 training images/class/region. [sent-501, score-0.175]
</p><p>89 Constrained by the appearance similarity graph weights, spatially adjacent regions tend to cluster together. [sent-505, score-0.259]
</p><p>90 While regions closer to the camera-less room corner tend to form large clusters, smaller clusters are observed as one moves closer to the cameras owing to larger facial appearance distortions caused by perspective and scale changes. [sent-506, score-0.585]
</p><p>91 Apart from the region and posebased appearance similarity graph weights, facial appearance features also influence the clustering ofrelated regions, ,c  and therefore, the computed optimal partitioning. [sent-507, score-0.51]
</p><p>92 4 presents the accuracies obtained with 4-view features against single-view features (mean of the accuracies obtained with each of the four views is considered here). [sent-519, score-0.204]
</p><p>93 With multiple targets, identical colors are used to denote the pose direction frustum and face crop rectangle for each target. [sent-524, score-0.318]
</p><p>94 This scenario is quite challenging, as six targets are interacting naturally and freely  moving around in the room. [sent-525, score-0.175]
</p><p>95 Conclusions We propose a novel graph-guided FEGA-MTL framework for classifying head pose of moving targets from multiple camera views. [sent-527, score-1.1]
</p><p>96 Starting from a dense 2D spatial grid, two graphs which respectively model appearance similarity among grid partitions and head pose classes guide the learner to output region-specific pose classifiers and the optimal space partitioning. [sent-528, score-1.649]
</p><p>97 (Bottom) Pose classification results for a party video involving multiple mobile targets (best viewed under zoom. [sent-533, score-0.173]
</p><p>98 We are not contortionists: coupled adaptive learning for head and body orientation estimation in surveillance video. [sent-565, score-0.636]
</p><p>99 An adaptation framework for head pose estimation in dynamic multi-view scenarios. [sent-651, score-0.85]
</p><p>100 A system for probabilistic joint 3d head tracking and pose estimation in low-resolution, multi-view environments. [sent-671, score-0.85]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('head', 0.587), ('mtl', 0.378), ('pose', 0.263), ('dpose', 0.145), ('facial', 0.137), ('pk', 0.124), ('grid', 0.124), ('partitions', 0.12), ('appearance', 0.108), ('fista', 0.107), ('zk', 0.096), ('target', 0.096), ('targets', 0.087), ('classification', 0.086), ('camera', 0.085), ('relationships', 0.082), ('subramanian', 0.075), ('arco', 0.073), ('crops', 0.073), ('employing', 0.067), ('ij', 0.067), ('yit', 0.066), ('tracker', 0.066), ('clusters', 0.066), ('graph', 0.064), ('ricci', 0.064), ('quadrants', 0.064), ('accuracies', 0.062), ('owing', 0.06), ('graphs', 0.058), ('face', 0.055), ('cholesky', 0.054), ('moves', 0.053), ('views', 0.052), ('pl', 0.052), ('region', 0.051), ('partitioning', 0.051), ('surveillance', 0.049), ('ta', 0.049), ('lanz', 0.048), ('poseappearance', 0.048), ('positional', 0.048), ('flexible', 0.048), ('italy', 0.047), ('upon', 0.045), ('freely', 0.045), ('regions', 0.045), ('learner', 0.044), ('relationship', 0.044), ('parties', 0.043), ('trento', 0.043), ('expectedly', 0.043), ('moving', 0.043), ('tn', 0.042), ('task', 0.042), ('similarity', 0.042), ('room', 0.041), ('distortions', 0.041), ('person', 0.041), ('classes', 0.04), ('loop', 0.04), ('shrinkagethresholding', 0.04), ('particle', 0.04), ('warping', 0.038), ('flexibly', 0.037), ('proximal', 0.035), ('classifying', 0.035), ('update', 0.035), ('spherical', 0.034), ('cameras', 0.034), ('sebe', 0.034), ('comprises', 0.033), ('distortion', 0.033), ('clustered', 0.033), ('evgeniou', 0.033), ('sk', 0.033), ('dividing', 0.033), ('yan', 0.032), ('rotates', 0.032), ('outer', 0.032), ('tm', 0.031), ('neighboring', 0.031), ('xit', 0.031), ('weights', 0.031), ('determining', 0.03), ('dependencies', 0.03), ('guiding', 0.03), ('position', 0.03), ('classifier', 0.03), ('sy', 0.029), ('beneficial', 0.029), ('competing', 0.029), ('factorization', 0.029), ('mn', 0.029), ('presents', 0.028), ('recorded', 0.028), ('geometry', 0.028), ('tasks', 0.028), ('sigkdd', 0.028), ('preprocessing', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="291-tfidf-1" href="./iccv-2013-No_Matter_Where_You_Are%3A_Flexible_Graph-Guided_Multi-task_Learning_for_Multi-view_Head_Pose_Classification_under_Target_Motion.html">291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</a></p>
<p>Author: Yan Yan, Elisa Ricci, Ramanathan Subramanian, Oswald Lanz, Nicu Sebe</p><p>Abstract: We propose a novel Multi-Task Learning framework (FEGA-MTL) for classifying the head pose of a person who moves freely in an environment monitored by multiple, large field-of-view surveillance cameras. As the target (person) moves, distortions in facial appearance owing to camera perspective and scale severely impede performance of traditional head pose classification methods. FEGA-MTL operates on a dense uniform spatial grid and learns appearance relationships across partitions as well as partition-specific appearance variations for a given head pose to build region-specific classifiers. Guided by two graphs which a-priori model appearance similarity among (i) grid partitions based on camera geometry and (ii) head pose classes, the learner efficiently clusters appearancewise related grid partitions to derive the optimal partitioning. For pose classification, upon determining the target’s position using a person tracker, the appropriate regionspecific classifier is invoked. Experiments confirm that FEGA-MTL achieves state-of-the-art classification with few training data.</p><p>2 0.23286074 <a title="291-tfidf-2" href="./iccv-2013-Strong_Appearance_and_Expressive_Spatial_Models_for_Human_Pose_Estimation.html">403 iccv-2013-Strong Appearance and Expressive Spatial Models for Human Pose Estimation</a></p>
<p>Author: Leonid Pishchulin, Mykhaylo Andriluka, Peter Gehler, Bernt Schiele</p><p>Abstract: Typical approaches to articulated pose estimation combine spatial modelling of the human body with appearance modelling of body parts. This paper aims to push the state-of-the-art in articulated pose estimation in two ways. First we explore various types of appearance representations aiming to substantially improve the bodypart hypotheses. And second, we draw on and combine several recently proposed powerful ideas such as more flexible spatial models as well as image-conditioned spatial models. In a series of experiments we draw several important conclusions: (1) we show that the proposed appearance representations are complementary; (2) we demonstrate that even a basic tree-structure spatial human body model achieves state-ofthe-art performance when augmented with the proper appearance representation; and (3) we show that the combination of the best performing appearance model with a flexible image-conditioned spatial model achieves the best result, significantly improving over the state of the art, on the “Leeds Sports Poses ” and “Parse ” benchmarks.</p><p>3 0.13966157 <a title="291-tfidf-3" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>Author: Taegyu Lim, Seunghoon Hong, Bohyung Han, Joon Hee Han</p><p>Abstract: We propose an on-line algorithm to extract a human by foreground/background segmentation and estimate pose of the human from the videos captured by moving cameras. We claim that a virtuous cycle can be created by appropriate interactions between the two modules to solve individual problems. This joint estimation problem is divided into two subproblems, , foreground/background segmentation and pose tracking, which alternate iteratively for optimization; segmentation step generates foreground mask for human pose tracking, and human pose tracking step provides foreground response map for segmentation. The final solution is obtained when the iterative procedure converges. We evaluate our algorithm quantitatively and qualitatively in real videos involving various challenges, and present its outstandingperformance compared to the state-of-the-art techniques for segmentation and pose estimation.</p><p>4 0.1361836 <a title="291-tfidf-4" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>Author: Ning Zhang, Ryan Farrell, Forrest Iandola, Trevor Darrell</p><p>Abstract: Recognizing objects in fine-grained domains can be extremely challenging due to the subtle differences between subcategories. Discriminative markings are often highly localized, leading traditional object recognition approaches to struggle with the large pose variation often present in these domains. Pose-normalization seeks to align training exemplars, either piecewise by part or globally for the whole object, effectively factoring out differences in pose and in viewing angle. Prior approaches relied on computationally-expensive filter ensembles for part localization and required extensive supervision. This paper proposes two pose-normalized descriptors based on computationally-efficient deformable part models. The first leverages the semantics inherent in strongly-supervised DPM parts. The second exploits weak semantic annotations to learn cross-component correspondences, computing pose-normalized descriptors from the latent parts of a weakly-supervised DPM. These representations enable pooling across pose and viewpoint, in turn facilitating tasks such as fine-grained recognition and attribute prediction. Experiments conducted on the Caltech-UCSD Birds 200 dataset and Berkeley Human Attribute dataset demonstrate significant improvements over state-of-art algorithms.</p><p>5 0.13117549 <a title="291-tfidf-5" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>Author: Yizhe Zhang, Ming Shao, Edward K. Wong, Yun Fu</p><p>Abstract: One of the most challenging task in face recognition is to identify people with varied poses. Namely, the test faces have significantly different poses compared with the registered faces. In this paper, we propose a high-level feature learning scheme to extract pose-invariant identity feature for face recognition. First, we build a single-hiddenlayer neural network with sparse constraint, to extractposeinvariant feature in a supervised fashion. Second, we further enhance the discriminative capability of the proposed feature by using multiple random faces as the target values for multiple encoders. By enforcing the target values to be uniquefor inputfaces over differentposes, the learned highlevel feature that is represented by the neurons in the hidden layer is pose free and only relevant to the identity information. Finally, we conduct face identification on CMU MultiPIE, and verification on Labeled Faces in the Wild (LFW) databases, where identification rank-1 accuracy and face verification accuracy with ROC curve are reported. These experiments demonstrate that our model is superior to oth- er state-of-the-art approaches on handling pose variations.</p><p>6 0.12693202 <a title="291-tfidf-6" href="./iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F.html">316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</a></p>
<p>7 0.12653214 <a title="291-tfidf-7" href="./iccv-2013-Latent_Multitask_Learning_for_View-Invariant_Action_Recognition.html">231 iccv-2013-Latent Multitask Learning for View-Invariant Action Recognition</a></p>
<p>8 0.12329808 <a title="291-tfidf-8" href="./iccv-2013-Monocular_Image_3D_Human_Pose_Estimation_under_Self-Occlusion.html">273 iccv-2013-Monocular Image 3D Human Pose Estimation under Self-Occlusion</a></p>
<p>9 0.12320571 <a title="291-tfidf-9" href="./iccv-2013-Accurate_and_Robust_3D_Facial_Capture_Using_a_Single_RGBD_Camera.html">36 iccv-2013-Accurate and Robust 3D Facial Capture Using a Single RGBD Camera</a></p>
<p>10 0.11974108 <a title="291-tfidf-10" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>11 0.11828941 <a title="291-tfidf-11" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>12 0.11817183 <a title="291-tfidf-12" href="./iccv-2013-Real-Time_Body_Tracking_with_One_Depth_Camera_and_Inertial_Sensors.html">341 iccv-2013-Real-Time Body Tracking with One Depth Camera and Inertial Sensors</a></p>
<p>13 0.11601212 <a title="291-tfidf-13" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>14 0.11478201 <a title="291-tfidf-14" href="./iccv-2013-Allocentric_Pose_Estimation.html">46 iccv-2013-Allocentric Pose Estimation</a></p>
<p>15 0.1142965 <a title="291-tfidf-15" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>16 0.11424338 <a title="291-tfidf-16" href="./iccv-2013-Interactive_Markerless_Articulated_Hand_Motion_Tracking_Using_RGB_and_Depth_Data.html">218 iccv-2013-Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth Data</a></p>
<p>17 0.10807318 <a title="291-tfidf-17" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>18 0.10542256 <a title="291-tfidf-18" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>19 0.10158141 <a title="291-tfidf-19" href="./iccv-2013-A_Non-parametric_Bayesian_Network_Prior_of_Human_Pose.html">24 iccv-2013-A Non-parametric Bayesian Network Prior of Human Pose</a></p>
<p>20 0.10150182 <a title="291-tfidf-20" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.245), (1, -0.025), (2, -0.035), (3, -0.007), (4, 0.043), (5, -0.122), (6, 0.09), (7, 0.054), (8, -0.023), (9, 0.103), (10, -0.009), (11, -0.019), (12, -0.117), (13, -0.056), (14, 0.004), (15, 0.111), (16, 0.017), (17, -0.04), (18, -0.001), (19, -0.02), (20, 0.093), (21, 0.019), (22, 0.043), (23, 0.01), (24, 0.035), (25, -0.01), (26, -0.0), (27, -0.047), (28, 0.044), (29, 0.042), (30, 0.077), (31, 0.034), (32, 0.036), (33, 0.002), (34, -0.004), (35, -0.006), (36, 0.017), (37, -0.053), (38, -0.027), (39, 0.055), (40, 0.093), (41, -0.085), (42, -0.063), (43, -0.032), (44, 0.045), (45, 0.032), (46, -0.101), (47, 0.036), (48, 0.057), (49, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96744102 <a title="291-lsi-1" href="./iccv-2013-No_Matter_Where_You_Are%3A_Flexible_Graph-Guided_Multi-task_Learning_for_Multi-view_Head_Pose_Classification_under_Target_Motion.html">291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</a></p>
<p>Author: Yan Yan, Elisa Ricci, Ramanathan Subramanian, Oswald Lanz, Nicu Sebe</p><p>Abstract: We propose a novel Multi-Task Learning framework (FEGA-MTL) for classifying the head pose of a person who moves freely in an environment monitored by multiple, large field-of-view surveillance cameras. As the target (person) moves, distortions in facial appearance owing to camera perspective and scale severely impede performance of traditional head pose classification methods. FEGA-MTL operates on a dense uniform spatial grid and learns appearance relationships across partitions as well as partition-specific appearance variations for a given head pose to build region-specific classifiers. Guided by two graphs which a-priori model appearance similarity among (i) grid partitions based on camera geometry and (ii) head pose classes, the learner efficiently clusters appearancewise related grid partitions to derive the optimal partitioning. For pose classification, upon determining the target’s position using a person tracker, the appropriate regionspecific classifier is invoked. Experiments confirm that FEGA-MTL achieves state-of-the-art classification with few training data.</p><p>2 0.77996856 <a title="291-lsi-2" href="./iccv-2013-Monocular_Image_3D_Human_Pose_Estimation_under_Self-Occlusion.html">273 iccv-2013-Monocular Image 3D Human Pose Estimation under Self-Occlusion</a></p>
<p>Author: Ibrahim Radwan, Abhinav Dhall, Roland Goecke</p><p>Abstract: In this paper, an automatic approach for 3D pose reconstruction from a single image is proposed. The presence of human body articulation, hallucinated parts and cluttered background leads to ambiguity during the pose inference, which makes the problem non-trivial. Researchers have explored various methods based on motion and shading in order to reduce the ambiguity and reconstruct the 3D pose. The key idea of our algorithm is to impose both kinematic and orientation constraints. The former is imposed by projecting a 3D model onto the input image and pruning the parts, which are incompatible with the anthropomorphism. The latter is applied by creating synthetic views via regressing the input view to multiple oriented views. After applying the constraints, the 3D model is projected onto the initial and synthetic views, which further reduces the ambiguity. Finally, we borrow the direction of the unambiguous parts from the synthetic views to the initial one, which results in the 3D pose. Quantitative experiments are performed on the HumanEva-I dataset and qualitatively on unconstrained images from the Image Parse dataset. The results show the robustness of the proposed approach to accurately reconstruct the 3D pose form a single image.</p><p>3 0.76510274 <a title="291-lsi-3" href="./iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F.html">316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</a></p>
<p>Author: Elisabeta Marinoiu, Dragos Papava, Cristian Sminchisescu</p><p>Abstract: Human motion analysis in images and video is a central computer vision problem. Yet, there are no studies that reveal how humans perceive other people in images and how accurate they are. In this paper we aim to unveil some of the processing–as well as the levels of accuracy–involved in the 3D perception of people from images by assessing the human performance. Our contributions are: (1) the construction of an experimental apparatus that relates perception and measurement, in particular the visual and kinematic performance with respect to 3D ground truth when the human subject is presented an image of a person in a given pose; (2) the creation of a dataset containing images, articulated 2D and 3D pose ground truth, as well as synchronized eye movement recordings of human subjects, shown a variety of human body configurations, both easy and difficult, as well as their ‘re-enacted’ 3D poses; (3) quantitative analysis revealing the human performance in 3D pose reenactment tasks, the degree of stability in the visual fixation patterns of human subjects, and the way it correlates with different poses. We also discuss the implications of our find- ings for the construction of visual human sensing systems.</p><p>4 0.75016981 <a title="291-lsi-4" href="./iccv-2013-A_Non-parametric_Bayesian_Network_Prior_of_Human_Pose.html">24 iccv-2013-A Non-parametric Bayesian Network Prior of Human Pose</a></p>
<p>Author: Andreas M. Lehrmann, Peter V. Gehler, Sebastian Nowozin</p><p>Abstract: Having a sensible prior of human pose is a vital ingredient for many computer vision applications, including tracking and pose estimation. While the application of global non-parametric approaches and parametric models has led to some success, finding the right balance in terms of flexibility and tractability, as well as estimating model parameters from data has turned out to be challenging. In this work, we introduce a sparse Bayesian network model of human pose that is non-parametric with respect to the estimation of both its graph structure and its local distributions. We describe an efficient sampling scheme for our model and show its tractability for the computation of exact log-likelihoods. We empirically validate our approach on the Human 3.6M dataset and demonstrate superior performance to global models and parametric networks. We further illustrate our model’s ability to represent and compose poses not present in the training set (compositionality) and describe a speed-accuracy trade-off that allows realtime scoring of poses.</p><p>5 0.72647244 <a title="291-lsi-5" href="./iccv-2013-Strong_Appearance_and_Expressive_Spatial_Models_for_Human_Pose_Estimation.html">403 iccv-2013-Strong Appearance and Expressive Spatial Models for Human Pose Estimation</a></p>
<p>Author: Leonid Pishchulin, Mykhaylo Andriluka, Peter Gehler, Bernt Schiele</p><p>Abstract: Typical approaches to articulated pose estimation combine spatial modelling of the human body with appearance modelling of body parts. This paper aims to push the state-of-the-art in articulated pose estimation in two ways. First we explore various types of appearance representations aiming to substantially improve the bodypart hypotheses. And second, we draw on and combine several recently proposed powerful ideas such as more flexible spatial models as well as image-conditioned spatial models. In a series of experiments we draw several important conclusions: (1) we show that the proposed appearance representations are complementary; (2) we demonstrate that even a basic tree-structure spatial human body model achieves state-ofthe-art performance when augmented with the proper appearance representation; and (3) we show that the combination of the best performing appearance model with a flexible image-conditioned spatial model achieves the best result, significantly improving over the state of the art, on the “Leeds Sports Poses ” and “Parse ” benchmarks.</p><p>6 0.69232768 <a title="291-lsi-6" href="./iccv-2013-Discovering_Object_Functionality.html">118 iccv-2013-Discovering Object Functionality</a></p>
<p>7 0.68710512 <a title="291-lsi-7" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>8 0.6674726 <a title="291-lsi-8" href="./iccv-2013-Allocentric_Pose_Estimation.html">46 iccv-2013-Allocentric Pose Estimation</a></p>
<p>9 0.65939647 <a title="291-lsi-9" href="./iccv-2013-Parsing_IKEA_Objects%3A_Fine_Pose_Estimation.html">308 iccv-2013-Parsing IKEA Objects: Fine Pose Estimation</a></p>
<p>10 0.65304512 <a title="291-lsi-10" href="./iccv-2013-Interactive_Markerless_Articulated_Hand_Motion_Tracking_Using_RGB_and_Depth_Data.html">218 iccv-2013-Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth Data</a></p>
<p>11 0.64408231 <a title="291-lsi-11" href="./iccv-2013-Real-Time_Body_Tracking_with_One_Depth_Camera_and_Inertial_Sensors.html">341 iccv-2013-Real-Time Body Tracking with One Depth Camera and Inertial Sensors</a></p>
<p>12 0.64167589 <a title="291-lsi-12" href="./iccv-2013-Recognising_Human-Object_Interaction_via_Exemplar_Based_Modelling.html">344 iccv-2013-Recognising Human-Object Interaction via Exemplar Based Modelling</a></p>
<p>13 0.63746011 <a title="291-lsi-13" href="./iccv-2013-Real-Time_Articulated_Hand_Pose_Estimation_Using_Semi-supervised_Transductive_Regression_Forests.html">340 iccv-2013-Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests</a></p>
<p>14 0.63009369 <a title="291-lsi-14" href="./iccv-2013-Human_Re-identification_by_Matching_Compositional_Template_with_Cluster_Sampling.html">205 iccv-2013-Human Re-identification by Matching Compositional Template with Cluster Sampling</a></p>
<p>15 0.62085754 <a title="291-lsi-15" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>16 0.61765844 <a title="291-lsi-16" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>17 0.58939683 <a title="291-lsi-17" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>18 0.58049721 <a title="291-lsi-18" href="./iccv-2013-Dynamic_Structured_Model_Selection.html">130 iccv-2013-Dynamic Structured Model Selection</a></p>
<p>19 0.55171764 <a title="291-lsi-19" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>20 0.54141819 <a title="291-lsi-20" href="./iccv-2013-Two-Point_Gait%3A_Decoupling_Gait_from_Body_Shape.html">430 iccv-2013-Two-Point Gait: Decoupling Gait from Body Shape</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.066), (4, 0.01), (7, 0.267), (26, 0.08), (31, 0.035), (35, 0.011), (42, 0.131), (48, 0.013), (64, 0.068), (73, 0.032), (89, 0.172), (95, 0.013), (98, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91387284 <a title="291-lda-1" href="./iccv-2013-From_Semi-supervised_to_Transfer_Counting_of_Crowds.html">178 iccv-2013-From Semi-supervised to Transfer Counting of Crowds</a></p>
<p>Author: Chen Change Loy, Shaogang Gong, Tao Xiang</p><p>Abstract: Regression-based techniques have shown promising results for people counting in crowded scenes. However, most existing techniques require expensive and laborious data annotation for model training. In this study, we propose to address this problem from three perspectives: (1) Instead of exhaustively annotating every single frame, the most informative frames are selected for annotation automatically and actively. (2) Rather than learning from only labelled data, the abundant unlabelled data are exploited. (3) Labelled data from other scenes are employed to further alleviate the burden for data annotation. All three ideas are implemented in a unified active and semi-supervised regression framework with ability to perform transfer learning, by exploiting the underlying geometric structure of crowd patterns via manifold analysis. Extensive experiments validate the effectiveness of our approach.</p><p>2 0.89032066 <a title="291-lda-2" href="./iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning.html">212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</a></p>
<p>Author: Jiwen Lu, Gang Wang, Pierre Moulin</p><p>Abstract: This paper presents a new approach for image set classification, where each training and testing example contains a set of image instances of an object captured from varying viewpoints or under varying illuminations. While a number of image set classification methods have been proposed in recent years, most of them model each image set as a single linear subspace or mixture of linear subspaces, which may lose some discriminative information for classification. To address this, we propose exploring multiple order statistics as features of image sets, and develop a localized multikernel metric learning (LMKML) algorithm to effectively combine different order statistics information for classification. Our method achieves the state-of-the-art performance on four widely used databases including the Honda/UCSD, CMU Mobo, and Youtube face datasets, and the ETH-80 object dataset.</p><p>3 0.87901622 <a title="291-lda-3" href="./iccv-2013-Text_Localization_in_Natural_Images_Using_Stroke_Feature_Transform_and_Text_Covariance_Descriptors.html">415 iccv-2013-Text Localization in Natural Images Using Stroke Feature Transform and Text Covariance Descriptors</a></p>
<p>Author: Weilin Huang, Zhe Lin, Jianchao Yang, Jue Wang</p><p>Abstract: In this paper, we present a new approach for text localization in natural images, by discriminating text and non-text regions at three levels: pixel, component and textline levels. Firstly, a powerful low-level filter called the Stroke Feature Transform (SFT) is proposed, which extends the widely-used Stroke Width Transform (SWT) by incorporating color cues of text pixels, leading to significantly enhanced performance on inter-component separation and intra-component connection. Secondly, based on the output of SFT, we apply two classifiers, a text component classifier and a text-line classifier, sequentially to extract text regions, eliminating the heuristic procedures that are commonly used in previous approaches. The two classifiers are built upon two novel Text Covariance Descriptors (TCDs) that encode both the heuristic properties and the statistical characteristics of text stokes. Finally, text regions are located by simply thresholding the text-line confident map. Our method was evaluated on two benchmark datasets: ICDAR 2005 and ICDAR 2011, and the corresponding F- , measure values are 0. 72 and 0. 73, respectively, surpassing previous methods in accuracy by a large margin.</p><p>4 0.8789196 <a title="291-lda-4" href="./iccv-2013-Pose_Estimation_with_Unknown_Focal_Length_Using_Points%2C_Directions_and_Lines.html">323 iccv-2013-Pose Estimation with Unknown Focal Length Using Points, Directions and Lines</a></p>
<p>Author: Yubin Kuang, Kalle Åström</p><p>Abstract: In this paper, we study the geometry problems of estimating camera pose with unknown focal length using combination of geometric primitives. We consider points, lines and also rich features such as quivers, i.e. points with one or more directions. We formulate the problems as polynomial systems where the constraints for different primitives are handled in a unified way. We develop efficient polynomial solvers for each of the derived cases with different combinations of primitives. The availability of these solvers enables robust pose estimation with unknown focal length for wider classes of features. Such rich features allow for fewer feature correspondences and generate larger inlier sets with higher probability. We demonstrate in synthetic experiments that our solvers are fast and numerically stable. For real images, we show that our solvers can be used in RANSAC loops to provide good initial solutions.</p><p>same-paper 5 0.85593224 <a title="291-lda-5" href="./iccv-2013-No_Matter_Where_You_Are%3A_Flexible_Graph-Guided_Multi-task_Learning_for_Multi-view_Head_Pose_Classification_under_Target_Motion.html">291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</a></p>
<p>Author: Yan Yan, Elisa Ricci, Ramanathan Subramanian, Oswald Lanz, Nicu Sebe</p><p>Abstract: We propose a novel Multi-Task Learning framework (FEGA-MTL) for classifying the head pose of a person who moves freely in an environment monitored by multiple, large field-of-view surveillance cameras. As the target (person) moves, distortions in facial appearance owing to camera perspective and scale severely impede performance of traditional head pose classification methods. FEGA-MTL operates on a dense uniform spatial grid and learns appearance relationships across partitions as well as partition-specific appearance variations for a given head pose to build region-specific classifiers. Guided by two graphs which a-priori model appearance similarity among (i) grid partitions based on camera geometry and (ii) head pose classes, the learner efficiently clusters appearancewise related grid partitions to derive the optimal partitioning. For pose classification, upon determining the target’s position using a person tracker, the appropriate regionspecific classifier is invoked. Experiments confirm that FEGA-MTL achieves state-of-the-art classification with few training data.</p><p>6 0.81335121 <a title="291-lda-6" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>7 0.77343583 <a title="291-lda-7" href="./iccv-2013-A_Robust_Analytical_Solution_to_Isometric_Shape-from-Template_with_Focal_Length_Calibration.html">27 iccv-2013-A Robust Analytical Solution to Isometric Shape-from-Template with Focal Length Calibration</a></p>
<p>8 0.77080208 <a title="291-lda-8" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>9 0.74941289 <a title="291-lda-9" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>10 0.74689543 <a title="291-lda-10" href="./iccv-2013-Complex_3D_General_Object_Reconstruction_from_Line_Drawings.html">84 iccv-2013-Complex 3D General Object Reconstruction from Line Drawings</a></p>
<p>11 0.74518985 <a title="291-lda-11" href="./iccv-2013-Revisiting_the_PnP_Problem%3A_A_Fast%2C_General_and_Optimal_Solution.html">353 iccv-2013-Revisiting the PnP Problem: A Fast, General and Optimal Solution</a></p>
<p>12 0.74477881 <a title="291-lda-12" href="./iccv-2013-Linear_Sequence_Discriminant_Analysis%3A_A_Model-Based_Dimensionality_Reduction_Method_for_Vector_Sequences.html">253 iccv-2013-Linear Sequence Discriminant Analysis: A Model-Based Dimensionality Reduction Method for Vector Sequences</a></p>
<p>13 0.74238908 <a title="291-lda-13" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>14 0.74176556 <a title="291-lda-14" href="./iccv-2013-Lifting_3D_Manhattan_Lines_from_a_Single_Image.html">250 iccv-2013-Lifting 3D Manhattan Lines from a Single Image</a></p>
<p>15 0.74047315 <a title="291-lda-15" href="./iccv-2013-Unsupervised_Intrinsic_Calibration_from_a_Single_Frame_Using_a_%22Plumb-Line%22_Approach.html">436 iccv-2013-Unsupervised Intrinsic Calibration from a Single Frame Using a "Plumb-Line" Approach</a></p>
<p>16 0.73817903 <a title="291-lda-16" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>17 0.730726 <a title="291-lda-17" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>18 0.72894323 <a title="291-lda-18" href="./iccv-2013-Shortest_Paths_with_Curvature_and_Torsion.html">389 iccv-2013-Shortest Paths with Curvature and Torsion</a></p>
<p>19 0.72770381 <a title="291-lda-19" href="./iccv-2013-Real-Time_Articulated_Hand_Pose_Estimation_Using_Semi-supervised_Transductive_Regression_Forests.html">340 iccv-2013-Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests</a></p>
<p>20 0.72693819 <a title="291-lda-20" href="./iccv-2013-On_the_Mean_Curvature_Flow_on_Graphs_with_Applications_in_Image_and_Manifold_Processing.html">296 iccv-2013-On the Mean Curvature Flow on Graphs with Applications in Image and Manifold Processing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
