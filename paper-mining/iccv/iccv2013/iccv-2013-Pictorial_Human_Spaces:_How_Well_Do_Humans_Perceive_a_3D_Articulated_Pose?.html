<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-316" href="#">iccv2013-316</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</h1>
<br/><p>Source: <a title="iccv-2013-316-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Marinoiu_Pictorial_Human_Spaces_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Elisabeta Marinoiu, Dragos Papava, Cristian Sminchisescu</p><p>Abstract: Human motion analysis in images and video is a central computer vision problem. Yet, there are no studies that reveal how humans perceive other people in images and how accurate they are. In this paper we aim to unveil some of the processing–as well as the levels of accuracy–involved in the 3D perception of people from images by assessing the human performance. Our contributions are: (1) the construction of an experimental apparatus that relates perception and measurement, in particular the visual and kinematic performance with respect to 3D ground truth when the human subject is presented an image of a person in a given pose; (2) the creation of a dataset containing images, articulated 2D and 3D pose ground truth, as well as synchronized eye movement recordings of human subjects, shown a variety of human body configurations, both easy and difficult, as well as their ‘re-enacted’ 3D poses; (3) quantitative analysis revealing the human performance in 3D pose reenactment tasks, the degree of stability in the visual fixation patterns of human subjects, and the way it correlates with different poses. We also discuss the implications of our find- ings for the construction of visual human sensing systems.</p><p>Reference: <a title="iccv-2013-316-reference" href="../iccv2013_reference/iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Introduction When shown a photograph of a person, humans have a vivid, immediate sense of 3D pose awareness, and a rapid understanding of the subtle body language, personal attributes, or intentionality of that person. [sent-15, score-0.496]
</p><p>2 1 Our approach to establish the observationperception link is to make humans re-enact the 3D pose of another person (for which ground truth is available), shown in a photograph, following a short exposure time of a few seconds. [sent-22, score-0.391]
</p><p>3 Simultaneously our apparatus allows the measure-  ment of human pose and eye movements during the ‘pose matching’ performance. [sent-23, score-0.622]
</p><p>4 As the poses are taken from everyday activities, their reproduction should not put subjects in difficulty as far as the ability to articulate a pose is concerned, however. [sent-25, score-0.746]
</p><p>5 We conclude with a discussion of the implications of our findings for the construction of 3D human pose analysis systems. [sent-29, score-0.455]
</p><p>6 Related Work: The problem of human pose estimation from static images has received significant attention in computer vision, both in the 2D [7, 15, 26] and the 3D case[5, 17, 21, 24, 1, 8, 3, 2]. [sent-30, score-0.353]
</p><p>7 3D human pose ambiguities from monocular images may not be unavoidable. [sent-37, score-0.431]
</p><p>8 While this work focuses on experimental human sensing in monocular images, the moving light display setup of Johansson[10] is worth mentioning as a milestone in emphasizing the sufficiency of dynamic minimalism with respect to human motion perception. [sent-42, score-0.349]
</p><p>9 We are not aware, however, of a study similar to ours, nor of an apparatus connecting real images of people, eye movement recordings and 3D perceptual pose data, with multiple subject ground truth. [sent-45, score-0.827]
</p><p>10 Our approach was to dress people in a motion capture suit, equip them with an eye tracker and show them images of other people in different poses, which were obtained using motion capture as well (fig. [sent-48, score-0.427]
</p><p>11 By asking the subjects to re-enact the poses shown, we can link perception and measurement. [sent-50, score-0.644]
</p><p>12 The mocap system tracks a large number of markers attached to the full body mocap suit worn by a person. [sent-52, score-0.34]
</p><p>13 Each subject was asked to look at images of people in different poses taken from the Human3. [sent-61, score-0.472]
</p><p>14 The eye tracker calibration was done by asking the subject to look at specific points, while the system was recording pupil positions at each point. [sent-68, score-0.436]
</p><p>15 Each subject was required to stand still and look at one image at a time until it disappeared, then re-enact the pose by taking as much time as necessary. [sent-70, score-0.411]
</p><p>16 We chose to display images for 5 seconds such that the subjects would have enough time to see all the necessary pose detail, while still being short enough not to run into free viewing. [sent-71, score-0.567]
</p><p>17 The duration was chosen by first recording two test subjects and showing them images for 3, 5 and 8s. [sent-72, score-0.319]
</p><p>18 From an eye tracking video we were interested in the 5s that captured the projected pose along with the gaze recordings over the image of the person in that pose. [sent-74, score-0.564]
</p><p>19 To increase the accuracy of recorded gazes, we mapped fixations that fell onto the image on the screen (captured by eye-tracker/viewer’s camera) back to the original high-resolution image (c. [sent-75, score-0.561]
</p><p>20 (c) heat map distribution of all fixations of one of our subjects for this particular pose. [sent-86, score-0.641]
</p><p>21 Moreover, we synchronized the mocap and eye tracker system by detecting the start and end frame of the displayed images in the eye tracker video as well as in the video recorded with the motion capture system (two digital cameras of the mocap system were pointed at the screen as well). [sent-89, score-0.78]
</p><p>22 During experiments, subjects were dressed in the mocap suit and had the mobile eye tracker head-mounted. [sent-90, score-0.677]
</p><p>23 For each pose projected on the screen, we captured both the eye-tracks and the 3D movement of the subject in the process of re-enacting the pose, once it had disappeared from the screen. [sent-91, score-0.521]
</p><p>24 Once the 5s exposure time passed, the subject no longer had the possibility to see the image of pose to re-enact, but had to adjust his position based on the memory of that pose. [sent-92, score-0.404]
</p><p>25 Moreover, it makes the process of translating fixations from the video coordinates of the eye tracker to the ones of the image on the screen robust, as there are no rapid head movements or frames where the pose is only partially seen (fig. [sent-94, score-0.994]
</p><p>26 In this way, the subjects could alternate between adjusting body parts and checking back in the image, without the constraint to memorize all the pose details. [sent-97, score-0.666]
</p><p>27 While this choice may appear more natural, it has the drawback of no longer being able to easily map the human fixations to the presented image, and makes it difficult to separate fixations that fall on the pose from those on one’s own body or the surroundings. [sent-98, score-1.202]
</p><p>28 Evaluation and Error Measures We use the same skeleton joints as in Human3. [sent-106, score-0.374]
</p><p>29 6M position error (MPJPE) between a recorded pose and the ground truth is computed by translating the root (pelvis)joint ofthe given pose to the one ofthe ground truth. [sent-109, score-0.555]
</p><p>30 The error is then computed for each joint as the norm of the difference between the recorded pose and ground truth. [sent-111, score-0.369]
</p><p>31 We normalize both the subject skeleton and the ground truth to a default skeleton of average weight and height, ensuring that all computed errors are comparable between poses and subjects. [sent-113, score-0.493]
</p><p>32 In this section we analyze how consistent the subjects are in terms of their fixated image locations. [sent-135, score-0.556]
</p><p>33 We are first concerned with evaluating static consistency which considers only the location of the fixations and then dynamic consistency which takes into account the order of fixations. [sent-136, score-0.382]
</p><p>34 To evaluate how well the subjects agree on fixated image locations, we predict each subject’s fixations in turn using the information from the other 9 subjects[6, 14]. [sent-137, score-0.907]
</p><p>35 Percentage of fixations falling on joints, for each of the 120 poses (easy and hard) shown to subjects. [sent-140, score-0.562]
</p><p>36 On average 54% of the fixations on easy poses and 30% of those for hard poses fell on joints. [sent-143, score-1.009]
</p><p>37 a probability distribution by assigning a value of 1 to each pixel fixated by at least one of the 9 subjects and 0 to others, then locally blurring with a Gaussian kernel. [sent-144, score-0.556]
</p><p>38 The probability at pixels where the subject’s fixations fall is taken as the prediction of the model obtained from 9 subjects. [sent-147, score-0.398]
</p><p>39 Fixations  from one subject are predicted using data from the other 9 subjects both on the same image (blue) and on a different image of a person, randomly selected from our 120 poses (green). [sent-153, score-0.63]
</p><p>40 To evaluate how consistent the subjects are in their order of fixating areas of interest (AOIs), we used the hidden Markov modeling recently developed by [14]. [sent-156, score-0.401]
</p><p>41 The states correspond to AOIs that were fixated by subjects and the transitions correspond to saccades. [sent-157, score-0.556]
</p><p>42 For each pose, we learn a dynamic model from the scanpaths of 9 subjects and compute the likelihood of the 10th subject’s scanpath under the trained model. [sent-158, score-0.389]
</p><p>43 Specifically, lfikore leihacoho pose we generate a random scanpath with the exception of the first fixation which was taken as the center of the image to account for central bias. [sent-163, score-0.429]
</p><p>44 Each random scanpath is evaluated against the model trained with subject fixations on that pose. [sent-164, score-0.546]
</p><p>45 In order to understand where our subjects look and what are the most important body cues in re-enacting each 3D pose, we project the skeletal joint positions onto the image shown. [sent-170, score-0.554]
</p><p>46 We analyze the fixations relative to the 17 joints in fig. [sent-171, score-0.679]
</p><p>47 For each pose, we take into account the 3D occlusions (based on mocap data and a 3D volumetric model) and consider, as possibly fixated, only those joints that are visible. [sent-173, score-0.41]
</p><p>48 Our first analysis aims to reveal to what extent subjects are fixating joints and how their particular choice of regions varies with the poses shown. [sent-177, score-0.978]
</p><p>49 2 shows what percent of fixations fell on joints for each pose, for each subject. [sent-179, score-0.771]
</p><p>50 On average 54% of fixations fell on various body joints for easy poses, but only 30% for hard poses. [sent-180, score-1.046]
</p><p>51 This is not surprising as more joints are usually occluded in the case of the complex poses shown. [sent-181, score-0.539]
</p><p>52 Since approximately half of  human fixations fell on joints, we want to know whether certain joints are always sought first, and to what extent the joints considered first are pose-dependent. [sent-183, score-1.207]
</p><p>53 The order 1292  in which joints are fixated can offer insight into the cognitive process involved in pose recognition. [sent-184, score-0.839]
</p><p>54 The first 3 fixations almost never fall on the lower body part which (typically) has less mobility, but mostly on the regions of the head and arms. [sent-187, score-0.579]
</p><p>55 Number of times a body joint was in the top-3 regions fixated, accumulated over the 120 poses shown. [sent-189, score-0.401]
</p><p>56 We study whether certain joints are fixated more than others and we want to know whether this would happen regardless of the pose shown, or  whether it varies with the pose. [sent-191, score-0.932]
</p><p>57 For this purpose, we consider the number of fixations that fall on a particular joint as well as the time spent on fixating each joint. [sent-192, score-0.627]
</p><p>58 6 shows the distribution of fixations on body joints, averaged over poses. [sent-194, score-0.482]
</p><p>59 Notice that although certain joints have been fixated more than others, this depends on the specific pose. [sent-195, score-0.594]
</p><p>60 The variation can also be observed in a detailed analysis of how frequently certain joints were fixated in two arbitrary poses, presented in fig. [sent-196, score-0.594]
</p><p>61 One explanation could be that the leg positions in the second image have greater deviation form a standard, rest pose than in the first image. [sent-202, score-0.343]
</p><p>62 7 we aggregate joint fixations for each subject, on all poses, and show the most frequently fixated joints, on average. [sent-204, score-0.676]
</p><p>63 The inter-subject variation is smaller than the one between poses, confirming a degree of subject consistency with respect to the joints more frequently fixated. [sent-205, score-0.457]
</p><p>64 10 we show the mean time and standard deviation spent on a pose, shown on each body joint, by aggregating over subjects. [sent-210, score-0.317]
</p><p>65 It can be further observed that joints at the extremity of the body (head, neck, wrists) are  Figure 6. [sent-214, score-0.459]
</p><p>66 The mean and standard deviation is computed among the 120 poses by aggregating over all 10 subjects, for each pose. [sent-216, score-0.338]
</p><p>67 The mean and standard deviation is computed for each of the 10 subjects by aggregating their fixations over all 120 poses. [sent-219, score-0.768]
</p><p>68 Number of fixations of each subject on the 17 body joints, when presented the pose-image shown on the right. [sent-221, score-0.611]
</p><p>69 Number of fixations made by each subject, on the 17 body joints, when presented the pose-image shown on the right. [sent-223, score-0.482]
</p><p>70 3D Pose Re-Enactment In this section we complement  eye-movement  studies  with an analysis of how well humans are able to reproduce the 3D poses of people shown in images. [sent-227, score-0.377]
</p><p>71 What is the joint angle distribution of the poses in our dataset? [sent-228, score-0.315]
</p><p>72 The mean and standard deviation is computed among the 120 poses by aggregating the duration of fixations for all 10 subjects, for each pose. [sent-232, score-0.718]
</p><p>73 The mean and standard deviation is computed among the 10 subjects by aggregating, for each, the fixation duration over all 120 poses. [sent-235, score-0.498]
</p><p>74 Easy poses contain mainly standing positions (very few angles over 30o in the lower body part), whereas the hard ones, often very different from standing, are spread across all joints. [sent-237, score-0.523]
</p><p>75 Distribution of joint angles in our dataset (under MPJAE) split over easy (left) and hard poses (right). [sent-239, score-0.452]
</p><p>76 Since subjects were asked to match the right and left components of a pose accordingly, we want to know whether there is a balanced distribution between the deviations of our selected poses from a resting pose, over the right and left sides. [sent-240, score-0.807]
</p><p>77 There is a similar degree of displacement required for both left and right body sides in replicating poses during experiments. [sent-245, score-0.342]
</p><p>78 While there is variance between subjects in the time taken to re-enact a pose, table 1 shows that all of them are consistent in taking  more time for hard poses compared to easy ones. [sent-247, score-0.645]
</p><p>79 Are easy poses really easy and hard poses really hard? [sent-248, score-0.644]
</p><p>80 The center pose seems to be perceived as slightly harder, with higher errors and longer completion times. [sent-274, score-0.432]
</p><p>81 The last image shows a hard pose as well as the errors and time taken to re-enact it. [sent-275, score-0.372]
</p><p>82 The errors are considerably higher than for the easy poses indicating that our selection of difficult poses indeed resulted in higher reenactment errors and longer completion times. [sent-276, score-0.816]
</p><p>83 The subjects decide when they consider completion: body configuration closest to the one shown. [sent-278, score-0.421]
</p><p>84 In table 2 we show that, on average, subject completion errors are worse (by 14 3% under MPJPE or 9 10% under MPJAE) than (thbyeir 1 4m±in3im%um un error PacJPhiEev oerd 9 during t uhned process oEf) pose re-enactment. [sent-280, score-0.54]
</p><p>85 The 20 poses we perceived (and selected) as hard to re-enact indeed have larger errors than easy poses by 73 20% (MPJPE) or 53 6% (MPJAE). [sent-281, score-0.707]
</p><p>86 ( In a second experiment, not covered here due to space constraints, but described in detail in [13] we allow subjects to adjust their pose while the image is available, thus ruling out short-term memory decay as a confounding factor. [sent-284, score-0.662]
</p><p>87 We first presented the image only for 5s, removed it, and asked the subjects to re-enact the pose (as in §2. [sent-285, score-0.565]
</p><p>88 Error variation,  over  time, for two easy  poses  (left and center) and a hard  one  (right). [sent-297, score-0.355]
</p><p>89 94203eanof the minimum  errors  attained by subjects during  re-enactment, as  well as the completion  errors  for the subjects. [sent-323, score-0.487]
</p><p>90 tion of a joint, thus exhibiting a large error in that particular articulation, there could be other joints that are wrongly positioned, perhaps to compensate. [sent-324, score-0.386]
</p><p>91 Hard poses selected in the construction of the dataset indeed lead to higher errors compared to easy poses. [sent-332, score-0.414]
</p><p>92 This indicates that people are not necessarily good at accurate 3D pose recovery, under conventional metrics, a finding consistent with earlier computational studies of 3D monocular human pose ambiguities[21, 22, 19]. [sent-333, score-0.712]
</p><p>93 In the process of reproducing the pose, subjects attend certain joints more than others and this depends on the pose, but the scanpaths are remarkably stable across subjects both spatially and sequentially. [sent-337, score-0.941]
</p><p>94 Extremities including the head or the wrists are fixated more than internal joints, perhaps because once ‘end-effector’ positions are known, more constrains are applicable to ‘fill-in’ intermediate joints on the kinematic chain. [sent-338, score-0.795]
</p><p>95 Familiar pose sub-configurations are often fixated less (or not at all) compared to unfamiliar ones indicating that a degree of familiar sub-component pose recognition occurs from low-resolution stimuli, not ruling out poselet approaches[4]. [sent-339, score-0.823]
</p><p>96 Conclusions The visual analysis of humans is an active computer vision research area–yet it faces open problems on what elements of meaning should we detect, what elements of the pose should we represent and how, and what are the acceptable levels of accuracy for different human sensing tasks. [sent-342, score-0.441]
</p><p>97 In this paper we have taken an experimental approach to such questions by investigating the human pictorial space, linking images of people with the process in which humans perceive and re-enact those 3D poses. [sent-343, score-0.347]
</p><p>98 the level of human performance, the accuracy in pose reenactment tasks, as well as the structure of eye movement patterns and the correlation with the pose difficulty. [sent-347, score-0.892]
</p><p>99 Pictorial human spaces: A study on the human perception of 3D articulated poses . [sent-432, score-0.53]
</p><p>100 Combined discriminative and generative articulated pose and non-rigid shape estimation. [sent-457, score-0.307]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fixations', 0.351), ('joints', 0.328), ('subjects', 0.29), ('fixated', 0.266), ('pose', 0.245), ('poses', 0.211), ('mpjae', 0.203), ('mpjpe', 0.18), ('eye', 0.163), ('body', 0.131), ('subject', 0.129), ('fixation', 0.118), ('fixating', 0.111), ('apparatus', 0.105), ('perception', 0.103), ('fell', 0.092), ('aois', 0.09), ('reenactment', 0.09), ('screen', 0.083), ('pictorial', 0.083), ('recordings', 0.083), ('mocap', 0.082), ('monocular', 0.08), ('easy', 0.078), ('human', 0.077), ('completion', 0.075), ('imar', 0.074), ('movement', 0.072), ('tracker', 0.07), ('humans', 0.068), ('edrror', 0.068), ('scanpath', 0.066), ('aggregating', 0.066), ('hard', 0.066), ('people', 0.065), ('articulated', 0.062), ('deviation', 0.061), ('errors', 0.061), ('papava', 0.06), ('spent', 0.059), ('joint', 0.059), ('sminchisescu', 0.058), ('implications', 0.057), ('perceive', 0.054), ('photograph', 0.052), ('perceived', 0.051), ('sensing', 0.051), ('head', 0.05), ('wrists', 0.048), ('fall', 0.047), ('skeleton', 0.046), ('mobility', 0.045), ('percepts', 0.045), ('sensorimotor', 0.045), ('veridical', 0.045), ('suit', 0.045), ('angle', 0.045), ('stimuli', 0.042), ('findings', 0.041), ('standing', 0.04), ('disappeared', 0.04), ('link', 0.04), ('reveal', 0.038), ('person', 0.038), ('angles', 0.038), ('kinematic', 0.038), ('ruling', 0.037), ('positions', 0.037), ('look', 0.037), ('projected', 0.035), ('recorded', 0.035), ('construction', 0.035), ('reproduce', 0.033), ('vivid', 0.033), ('scanpaths', 0.033), ('feedback', 0.032), ('confounding', 0.032), ('foreshortening', 0.032), ('lund', 0.032), ('motion', 0.032), ('movements', 0.032), ('display', 0.032), ('static', 0.031), ('whether', 0.031), ('perceptual', 0.03), ('adjust', 0.03), ('error', 0.03), ('relate', 0.03), ('asked', 0.03), ('familiar', 0.03), ('indeed', 0.029), ('ambiguities', 0.029), ('duration', 0.029), ('sigal', 0.028), ('decay', 0.028), ('perhaps', 0.028), ('mobile', 0.027), ('meters', 0.027), ('borders', 0.027), ('female', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="316-tfidf-1" href="./iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F.html">316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</a></p>
<p>Author: Elisabeta Marinoiu, Dragos Papava, Cristian Sminchisescu</p><p>Abstract: Human motion analysis in images and video is a central computer vision problem. Yet, there are no studies that reveal how humans perceive other people in images and how accurate they are. In this paper we aim to unveil some of the processing–as well as the levels of accuracy–involved in the 3D perception of people from images by assessing the human performance. Our contributions are: (1) the construction of an experimental apparatus that relates perception and measurement, in particular the visual and kinematic performance with respect to 3D ground truth when the human subject is presented an image of a person in a given pose; (2) the creation of a dataset containing images, articulated 2D and 3D pose ground truth, as well as synchronized eye movement recordings of human subjects, shown a variety of human body configurations, both easy and difficult, as well as their ‘re-enacted’ 3D poses; (3) quantitative analysis revealing the human performance in 3D pose reenactment tasks, the degree of stability in the visual fixation patterns of human subjects, and the way it correlates with different poses. We also discuss the implications of our find- ings for the construction of visual human sensing systems.</p><p>2 0.23551716 <a title="316-tfidf-2" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>Author: S. Karthikeyan, Vignesh Jagadeesh, Renuka Shenoy, Miguel Ecksteinz, B.S. Manjunath</p><p>Abstract: Eye movement studies have confirmed that overt attention is highly biased towards faces and text regions in images. In this paper we explore a novel problem of predicting face and text regions in images using eye tracking data from multiple subjects. The problem is challenging as we aim to predict the semantics (face/text/background) only from eye tracking data without utilizing any image information. The proposed algorithm spatially clusters eye tracking data obtained in an image into different coherent groups and subsequently models the likelihood of the clusters containing faces and text using afully connectedMarkov Random Field (MRF). Given the eye tracking datafrom a test image, itpredicts potential face/head (humans, dogs and cats) and text locations reliably. Furthermore, the approach can be used to select regions of interest for further analysis by object detectors for faces and text. The hybrid eye position/object detector approach achieves better detection performance and reduced computation time compared to using only the object detection algorithm. We also present a new eye tracking dataset on 300 images selected from ICDAR, Street-view, Flickr and Oxford-IIIT Pet Dataset from 15 subjects.</p><p>3 0.2184934 <a title="316-tfidf-3" href="./iccv-2013-Monocular_Image_3D_Human_Pose_Estimation_under_Self-Occlusion.html">273 iccv-2013-Monocular Image 3D Human Pose Estimation under Self-Occlusion</a></p>
<p>Author: Ibrahim Radwan, Abhinav Dhall, Roland Goecke</p><p>Abstract: In this paper, an automatic approach for 3D pose reconstruction from a single image is proposed. The presence of human body articulation, hallucinated parts and cluttered background leads to ambiguity during the pose inference, which makes the problem non-trivial. Researchers have explored various methods based on motion and shading in order to reduce the ambiguity and reconstruct the 3D pose. The key idea of our algorithm is to impose both kinematic and orientation constraints. The former is imposed by projecting a 3D model onto the input image and pruning the parts, which are incompatible with the anthropomorphism. The latter is applied by creating synthetic views via regressing the input view to multiple oriented views. After applying the constraints, the 3D model is projected onto the initial and synthetic views, which further reduces the ambiguity. Finally, we borrow the direction of the unambiguous parts from the synthetic views to the initial one, which results in the 3D pose. Quantitative experiments are performed on the HumanEva-I dataset and qualitatively on unconstrained images from the Image Parse dataset. The results show the robustness of the proposed approach to accurately reconstruct the 3D pose form a single image.</p><p>4 0.20583212 <a title="316-tfidf-4" href="./iccv-2013-Strong_Appearance_and_Expressive_Spatial_Models_for_Human_Pose_Estimation.html">403 iccv-2013-Strong Appearance and Expressive Spatial Models for Human Pose Estimation</a></p>
<p>Author: Leonid Pishchulin, Mykhaylo Andriluka, Peter Gehler, Bernt Schiele</p><p>Abstract: Typical approaches to articulated pose estimation combine spatial modelling of the human body with appearance modelling of body parts. This paper aims to push the state-of-the-art in articulated pose estimation in two ways. First we explore various types of appearance representations aiming to substantially improve the bodypart hypotheses. And second, we draw on and combine several recently proposed powerful ideas such as more flexible spatial models as well as image-conditioned spatial models. In a series of experiments we draw several important conclusions: (1) we show that the proposed appearance representations are complementary; (2) we demonstrate that even a basic tree-structure spatial human body model achieves state-ofthe-art performance when augmented with the proper appearance representation; and (3) we show that the combination of the best performing appearance model with a flexible image-conditioned spatial model achieves the best result, significantly improving over the state of the art, on the “Leeds Sports Poses ” and “Parse ” benchmarks.</p><p>5 0.2054867 <a title="316-tfidf-5" href="./iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction.html">50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</a></p>
<p>Author: Ali Borji, Hamed R. Tavakoli, Dicky N. Sihite, Laurent Itti</p><p>Abstract: Significant recent progress has been made in developing high-quality saliency models. However, less effort has been undertaken on fair assessment of these models, over large standardized datasets and correctly addressing confounding factors. In this study, we pursue a critical and quantitative look at challenges (e.g., center-bias, map smoothing) in saliency modeling and the way they affect model accuracy. We quantitatively compare 32 state-of-the-art models (using the shuffled AUC score to discount center-bias) on 4 benchmark eye movement datasets, for prediction of human fixation locations and scanpath sequence. We also account for the role of map smoothing. We find that, although model rankings vary, some (e.g., AWS, LG, AIM, and HouNIPS) consistently outperform other models over all datasets. Some models work well for prediction of both fixation locations and scanpath sequence (e.g., Judd, GBVS). Our results show low prediction accuracy for models over emotional stimuli from the NUSEF dataset. Our last benchmark, for the first time, gauges the ability of models to decode the stimulus category from statistics of fixations, saccades, and model saliency values at fixated locations. In this test, ITTI and AIM models win over other models. Our benchmark provides a comprehensive high-level picture of the strengths and weaknesses of many popular models, and suggests future research directions in saliency modeling.</p><p>6 0.20477462 <a title="316-tfidf-6" href="./iccv-2013-Real-Time_Body_Tracking_with_One_Depth_Camera_and_Inertial_Sensors.html">341 iccv-2013-Real-Time Body Tracking with One Depth Camera and Inertial Sensors</a></p>
<p>7 0.19725524 <a title="316-tfidf-7" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>8 0.17997633 <a title="316-tfidf-8" href="./iccv-2013-A_Non-parametric_Bayesian_Network_Prior_of_Human_Pose.html">24 iccv-2013-A Non-parametric Bayesian Network Prior of Human Pose</a></p>
<p>9 0.15713474 <a title="316-tfidf-9" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>10 0.15267567 <a title="316-tfidf-10" href="./iccv-2013-Saliency_and_Human_Fixations%3A_State-of-the-Art_and_Study_of_Comparison_Metrics.html">373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</a></p>
<p>11 0.14842536 <a title="316-tfidf-11" href="./iccv-2013-Interactive_Markerless_Articulated_Hand_Motion_Tracking_Using_RGB_and_Depth_Data.html">218 iccv-2013-Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth Data</a></p>
<p>12 0.14347368 <a title="316-tfidf-12" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>13 0.14095207 <a title="316-tfidf-13" href="./iccv-2013-Real-Time_Articulated_Hand_Pose_Estimation_Using_Semi-supervised_Transductive_Regression_Forests.html">340 iccv-2013-Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests</a></p>
<p>14 0.13972755 <a title="316-tfidf-14" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>15 0.13301441 <a title="316-tfidf-15" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>16 0.12878528 <a title="316-tfidf-16" href="./iccv-2013-Efficient_Hand_Pose_Estimation_from_a_Single_Depth_Image.html">133 iccv-2013-Efficient Hand Pose Estimation from a Single Depth Image</a></p>
<p>17 0.12693202 <a title="316-tfidf-17" href="./iccv-2013-No_Matter_Where_You_Are%3A_Flexible_Graph-Guided_Multi-task_Learning_for_Multi-view_Head_Pose_Classification_under_Target_Motion.html">291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</a></p>
<p>18 0.11520051 <a title="316-tfidf-18" href="./iccv-2013-Allocentric_Pose_Estimation.html">46 iccv-2013-Allocentric Pose Estimation</a></p>
<p>19 0.10945076 <a title="316-tfidf-19" href="./iccv-2013-Discovering_Object_Functionality.html">118 iccv-2013-Discovering Object Functionality</a></p>
<p>20 0.10510326 <a title="316-tfidf-20" href="./iccv-2013-Calibration-Free_Gaze_Estimation_Using_Human_Gaze_Patterns.html">67 iccv-2013-Calibration-Free Gaze Estimation Using Human Gaze Patterns</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.197), (1, -0.039), (2, 0.071), (3, 0.009), (4, 0.053), (5, -0.134), (6, 0.08), (7, 0.007), (8, -0.056), (9, 0.202), (10, 0.06), (11, -0.055), (12, -0.179), (13, -0.074), (14, -0.058), (15, 0.218), (16, -0.098), (17, -0.117), (18, 0.054), (19, 0.056), (20, 0.105), (21, -0.011), (22, 0.101), (23, -0.046), (24, 0.018), (25, -0.012), (26, 0.023), (27, 0.029), (28, 0.042), (29, -0.032), (30, 0.014), (31, -0.044), (32, 0.015), (33, -0.029), (34, 0.085), (35, -0.035), (36, 0.015), (37, -0.053), (38, -0.009), (39, 0.03), (40, 0.044), (41, 0.016), (42, -0.073), (43, 0.011), (44, 0.014), (45, -0.03), (46, -0.038), (47, 0.003), (48, -0.002), (49, 0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97458428 <a title="316-lsi-1" href="./iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F.html">316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</a></p>
<p>Author: Elisabeta Marinoiu, Dragos Papava, Cristian Sminchisescu</p><p>Abstract: Human motion analysis in images and video is a central computer vision problem. Yet, there are no studies that reveal how humans perceive other people in images and how accurate they are. In this paper we aim to unveil some of the processing–as well as the levels of accuracy–involved in the 3D perception of people from images by assessing the human performance. Our contributions are: (1) the construction of an experimental apparatus that relates perception and measurement, in particular the visual and kinematic performance with respect to 3D ground truth when the human subject is presented an image of a person in a given pose; (2) the creation of a dataset containing images, articulated 2D and 3D pose ground truth, as well as synchronized eye movement recordings of human subjects, shown a variety of human body configurations, both easy and difficult, as well as their ‘re-enacted’ 3D poses; (3) quantitative analysis revealing the human performance in 3D pose reenactment tasks, the degree of stability in the visual fixation patterns of human subjects, and the way it correlates with different poses. We also discuss the implications of our find- ings for the construction of visual human sensing systems.</p><p>2 0.80703527 <a title="316-lsi-2" href="./iccv-2013-Monocular_Image_3D_Human_Pose_Estimation_under_Self-Occlusion.html">273 iccv-2013-Monocular Image 3D Human Pose Estimation under Self-Occlusion</a></p>
<p>Author: Ibrahim Radwan, Abhinav Dhall, Roland Goecke</p><p>Abstract: In this paper, an automatic approach for 3D pose reconstruction from a single image is proposed. The presence of human body articulation, hallucinated parts and cluttered background leads to ambiguity during the pose inference, which makes the problem non-trivial. Researchers have explored various methods based on motion and shading in order to reduce the ambiguity and reconstruct the 3D pose. The key idea of our algorithm is to impose both kinematic and orientation constraints. The former is imposed by projecting a 3D model onto the input image and pruning the parts, which are incompatible with the anthropomorphism. The latter is applied by creating synthetic views via regressing the input view to multiple oriented views. After applying the constraints, the 3D model is projected onto the initial and synthetic views, which further reduces the ambiguity. Finally, we borrow the direction of the unambiguous parts from the synthetic views to the initial one, which results in the 3D pose. Quantitative experiments are performed on the HumanEva-I dataset and qualitatively on unconstrained images from the Image Parse dataset. The results show the robustness of the proposed approach to accurately reconstruct the 3D pose form a single image.</p><p>3 0.79321814 <a title="316-lsi-3" href="./iccv-2013-Interactive_Markerless_Articulated_Hand_Motion_Tracking_Using_RGB_and_Depth_Data.html">218 iccv-2013-Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth Data</a></p>
<p>Author: Srinath Sridhar, Antti Oulasvirta, Christian Theobalt</p><p>Abstract: Tracking the articulated 3D motion of the hand has important applications, for example, in human–computer interaction and teleoperation. We present a novel method that can capture a broad range of articulated hand motions at interactive rates. Our hybrid approach combines, in a voting scheme, a discriminative, part-based pose retrieval method with a generative pose estimation method based on local optimization. Color information from a multiview RGB camera setup along with a person-specific hand model are used by the generative method to find the pose that best explains the observed images. In parallel, our discriminative pose estimation method uses fingertips detected on depth data to estimate a complete or partial pose of the hand by adopting a part-based pose retrieval strategy. This part-based strategy helps reduce the search space drastically in comparison to a global pose retrieval strategy. Quantitative results show that our method achieves state-of-the-art accuracy on challenging sequences and a near-realtime performance of 10 fps on a desktop computer.</p><p>4 0.76713431 <a title="316-lsi-4" href="./iccv-2013-Discovering_Object_Functionality.html">118 iccv-2013-Discovering Object Functionality</a></p>
<p>Author: Bangpeng Yao, Jiayuan Ma, Li Fei-Fei</p><p>Abstract: Object functionality refers to the quality of an object that allows humans to perform some specific actions. It has been shown in psychology that functionality (affordance) is at least as essential as appearance in object recognition by humans. In computer vision, most previous work on functionality either assumes exactly one functionality for each object, or requires detailed annotation of human poses and objects. In this paper, we propose a weakly supervised approach to discover all possible object functionalities. Each object functionality is represented by a specific type of human-object interaction. Our method takes any possible human-object interaction into consideration, and evaluates image similarity in 3D rather than 2D in order to cluster human-object interactions more coherently. Experimental results on a dataset of people interacting with musical instruments show the effectiveness of our approach.</p><p>5 0.76712608 <a title="316-lsi-5" href="./iccv-2013-Strong_Appearance_and_Expressive_Spatial_Models_for_Human_Pose_Estimation.html">403 iccv-2013-Strong Appearance and Expressive Spatial Models for Human Pose Estimation</a></p>
<p>Author: Leonid Pishchulin, Mykhaylo Andriluka, Peter Gehler, Bernt Schiele</p><p>Abstract: Typical approaches to articulated pose estimation combine spatial modelling of the human body with appearance modelling of body parts. This paper aims to push the state-of-the-art in articulated pose estimation in two ways. First we explore various types of appearance representations aiming to substantially improve the bodypart hypotheses. And second, we draw on and combine several recently proposed powerful ideas such as more flexible spatial models as well as image-conditioned spatial models. In a series of experiments we draw several important conclusions: (1) we show that the proposed appearance representations are complementary; (2) we demonstrate that even a basic tree-structure spatial human body model achieves state-ofthe-art performance when augmented with the proper appearance representation; and (3) we show that the combination of the best performing appearance model with a flexible image-conditioned spatial model achieves the best result, significantly improving over the state of the art, on the “Leeds Sports Poses ” and “Parse ” benchmarks.</p><p>6 0.76160258 <a title="316-lsi-6" href="./iccv-2013-A_Non-parametric_Bayesian_Network_Prior_of_Human_Pose.html">24 iccv-2013-A Non-parametric Bayesian Network Prior of Human Pose</a></p>
<p>7 0.75492275 <a title="316-lsi-7" href="./iccv-2013-No_Matter_Where_You_Are%3A_Flexible_Graph-Guided_Multi-task_Learning_for_Multi-view_Head_Pose_Classification_under_Target_Motion.html">291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</a></p>
<p>8 0.74945652 <a title="316-lsi-8" href="./iccv-2013-Real-Time_Body_Tracking_with_One_Depth_Camera_and_Inertial_Sensors.html">341 iccv-2013-Real-Time Body Tracking with One Depth Camera and Inertial Sensors</a></p>
<p>9 0.72636449 <a title="316-lsi-9" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>10 0.72002596 <a title="316-lsi-10" href="./iccv-2013-Allocentric_Pose_Estimation.html">46 iccv-2013-Allocentric Pose Estimation</a></p>
<p>11 0.69360727 <a title="316-lsi-11" href="./iccv-2013-Real-Time_Articulated_Hand_Pose_Estimation_Using_Semi-supervised_Transductive_Regression_Forests.html">340 iccv-2013-Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests</a></p>
<p>12 0.66880602 <a title="316-lsi-12" href="./iccv-2013-Recognising_Human-Object_Interaction_via_Exemplar_Based_Modelling.html">344 iccv-2013-Recognising Human-Object Interaction via Exemplar Based Modelling</a></p>
<p>13 0.66038829 <a title="316-lsi-13" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>14 0.6486991 <a title="316-lsi-14" href="./iccv-2013-Parsing_IKEA_Objects%3A_Fine_Pose_Estimation.html">308 iccv-2013-Parsing IKEA Objects: Fine Pose Estimation</a></p>
<p>15 0.62943566 <a title="316-lsi-15" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>16 0.62535745 <a title="316-lsi-16" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>17 0.57778221 <a title="316-lsi-17" href="./iccv-2013-Dynamic_Structured_Model_Selection.html">130 iccv-2013-Dynamic Structured Model Selection</a></p>
<p>18 0.57573384 <a title="316-lsi-18" href="./iccv-2013-Efficient_Hand_Pose_Estimation_from_a_Single_Depth_Image.html">133 iccv-2013-Efficient Hand Pose Estimation from a Single Depth Image</a></p>
<p>19 0.5638597 <a title="316-lsi-19" href="./iccv-2013-Two-Point_Gait%3A_Decoupling_Gait_from_Body_Shape.html">430 iccv-2013-Two-Point Gait: Decoupling Gait from Body Shape</a></p>
<p>20 0.55579919 <a title="316-lsi-20" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.061), (7, 0.021), (12, 0.053), (13, 0.027), (23, 0.182), (26, 0.065), (31, 0.036), (34, 0.011), (35, 0.068), (42, 0.134), (64, 0.049), (73, 0.036), (84, 0.015), (89, 0.146), (97, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8354311 <a title="316-lda-1" href="./iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F.html">316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</a></p>
<p>Author: Elisabeta Marinoiu, Dragos Papava, Cristian Sminchisescu</p><p>Abstract: Human motion analysis in images and video is a central computer vision problem. Yet, there are no studies that reveal how humans perceive other people in images and how accurate they are. In this paper we aim to unveil some of the processing–as well as the levels of accuracy–involved in the 3D perception of people from images by assessing the human performance. Our contributions are: (1) the construction of an experimental apparatus that relates perception and measurement, in particular the visual and kinematic performance with respect to 3D ground truth when the human subject is presented an image of a person in a given pose; (2) the creation of a dataset containing images, articulated 2D and 3D pose ground truth, as well as synchronized eye movement recordings of human subjects, shown a variety of human body configurations, both easy and difficult, as well as their ‘re-enacted’ 3D poses; (3) quantitative analysis revealing the human performance in 3D pose reenactment tasks, the degree of stability in the visual fixation patterns of human subjects, and the way it correlates with different poses. We also discuss the implications of our find- ings for the construction of visual human sensing systems.</p><p>2 0.82539809 <a title="316-lda-2" href="./iccv-2013-SGTD%3A_Structure_Gradient_and_Texture_Decorrelating_Regularization_for_Image_Decomposition.html">364 iccv-2013-SGTD: Structure Gradient and Texture Decorrelating Regularization for Image Decomposition</a></p>
<p>Author: Qiegen Liu, Jianbo Liu, Pei Dong, Dong Liang</p><p>Abstract: This paper presents a novel structure gradient and texture decorrelating regularization (SGTD) for image decomposition. The motivation of the idea is under the assumption that the structure gradient and texture components should be properly decorrelated for a successful decomposition. The proposed model consists of the data fidelity term, total variation regularization and the SGTD regularization. An augmented Lagrangian method is proposed to address this optimization issue, by first transforming the unconstrained problem to an equivalent constrained problem and then applying an alternating direction method to iteratively solve the subproblems. Experimental results demonstrate that the proposed method presents better or comparable performance as state-of-the-art methods do.</p><p>3 0.78950965 <a title="316-lda-3" href="./iccv-2013-Network_Principles_for_SfM%3A_Disambiguating_Repeated_Structures_with_Local_Context.html">289 iccv-2013-Network Principles for SfM: Disambiguating Repeated Structures with Local Context</a></p>
<p>Author: Kyle Wilson, Noah Snavely</p><p>Abstract: Repeated features are common in urban scenes. Many objects, such as clock towers with nearly identical sides, or domes with strong radial symmetries, pose challenges for structure from motion. When similar but distinct features are mistakenly equated, the resulting 3D reconstructions can have errors ranging from phantom walls and superimposed structures to a complete failure to reconstruct. We present a new approach to solving such problems by considering the local visibility structure of such repeated features. Drawing upon network theory, we present a new way of scoring features using a measure of local clustering. Our model leads to a simple, fast, and highly scalable technique for disambiguating repeated features based on an analysis of an underlying visibility graph, without relying on explicit geometric reasoning. We demonstrate our method on several very large datasets drawn from Internet photo collections, and compare it to a more traditional geometry-based disambiguation technique.</p><p>4 0.76803195 <a title="316-lda-4" href="./iccv-2013-Discriminant_Tracking_Using_Tensor_Representation_with_Semi-supervised_Improvement.html">119 iccv-2013-Discriminant Tracking Using Tensor Representation with Semi-supervised Improvement</a></p>
<p>Author: Jin Gao, Junliang Xing, Weiming Hu, Steve Maybank</p><p>Abstract: Visual tracking has witnessed growing methods in object representation, which is crucial to robust tracking. The dominant mechanism in object representation is using image features encoded in a vector as observations to perform tracking, without considering that an image is intrinsically a matrix, or a 2nd-order tensor. Thus approaches following this mechanism inevitably lose a lot of useful information, and therefore cannot fully exploit the spatial correlations within the 2D image ensembles. In this paper, we address an image as a 2nd-order tensor in its original form, and find a discriminative linear embedding space approximation to the original nonlinear submanifold embedded in the tensor space based on the graph embedding framework. We specially design two graphs for characterizing the intrinsic local geometrical structure of the tensor space, so as to retain more discriminant information when reducing the dimension along certain tensor dimensions. However, spatial correlations within a tensor are not limited to the elements along these dimensions. This means that some part of the discriminant information may not be encoded in the embedding space. We introduce a novel technique called semi-supervised improvement to iteratively adjust the embedding space to compensate for the loss of discriminant information, hence improving the performance of our tracker. Experimental results on challenging videos demonstrate the effectiveness and robustness of the proposed tracker.</p><p>5 0.76774162 <a title="316-lda-5" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>Author: Heng Yang, Ioannis Patras</p><p>Abstract: In this paper we propose a method for the localization of multiple facial features on challenging face images. In the regression forests (RF) framework, observations (patches) that are extracted at several image locations cast votes for the localization of several facial features. In order to filter out votes that are not relevant, we pass them through two types of sieves, that are organised in a cascade, and which enforce geometric constraints. The first sieve filters out votes that are not consistent with a hypothesis for the location of the face center. Several sieves of the second type, one associated with each individual facial point, filter out distant votes. We propose a method that adjusts onthe-fly the proximity threshold of each second type sieve by applying a classifier which, based on middle-level features extracted from voting maps for the facial feature in question, makes a sequence of decisions on whether the threshold should be reduced or not. We validate our proposed method on two challenging datasets with images collected from the Internet in which we obtain state of the art results without resorting to explicit facial shape models. We also show the benefits of our method for proximity threshold adjustment especially on ’difficult’ face images.</p><p>6 0.76429296 <a title="316-lda-6" href="./iccv-2013-Low-Rank_Sparse_Coding_for_Image_Classification.html">258 iccv-2013-Low-Rank Sparse Coding for Image Classification</a></p>
<p>7 0.75546765 <a title="316-lda-7" href="./iccv-2013-Decomposing_Bag_of_Words_Histograms.html">104 iccv-2013-Decomposing Bag of Words Histograms</a></p>
<p>8 0.75316846 <a title="316-lda-8" href="./iccv-2013-Content-Aware_Rotation.html">90 iccv-2013-Content-Aware Rotation</a></p>
<p>9 0.75136435 <a title="316-lda-9" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>10 0.74553096 <a title="316-lda-10" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>11 0.74195999 <a title="316-lda-11" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>12 0.74105245 <a title="316-lda-12" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>13 0.7406323 <a title="316-lda-13" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<p>14 0.7404294 <a title="316-lda-14" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>15 0.73879868 <a title="316-lda-15" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>16 0.7380873 <a title="316-lda-16" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>17 0.73721087 <a title="316-lda-17" href="./iccv-2013-A_Method_of_Perceptual-Based_Shape_Decomposition.html">21 iccv-2013-A Method of Perceptual-Based Shape Decomposition</a></p>
<p>18 0.73717451 <a title="316-lda-18" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>19 0.73671663 <a title="316-lda-19" href="./iccv-2013-Bayesian_Joint_Topic_Modelling_for_Weakly_Supervised_Object_Localisation.html">59 iccv-2013-Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation</a></p>
<p>20 0.73666203 <a title="316-lda-20" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
