<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>251 iccv-2013-Like Father, Like Son: Facial Expression Dynamics for Kinship Verification</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-251" href="#">iccv2013-251</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>251 iccv-2013-Like Father, Like Son: Facial Expression Dynamics for Kinship Verification</h1>
<br/><p>Source: <a title="iccv-2013-251-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Dibeklioglu_Like_Father_Like_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Hamdi Dibeklioglu, Albert Ali Salah, Theo Gevers</p><p>Abstract: Kinship verification from facial appearance is a difficult problem. This paper explores the possibility of employing facial expression dynamics in this problem. By using features that describe facial dynamics and spatio-temporal appearance over smile expressions, we show that it is possible to improve the state ofthe art in thisproblem, and verify that it is indeed possible to recognize kinship by resemblance of facial expressions. The proposed method is tested on different kin relationships. On the average, 72.89% verification accuracy is achieved on spontaneous smiles.</p><p>Reference: <a title="iccv-2013-251-reference" href="../iccv2013_reference/iccv-2013-Like_Father%2C_Like_Son%3A_Facial_Expression_Dynamics_for_Kinship_Verification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 nl  Abstract Kinship verification from facial appearance is a difficult problem. [sent-7, score-0.379]
</p><p>2 This paper explores the possibility of employing facial expression dynamics in this problem. [sent-8, score-0.426]
</p><p>3 By using features that describe facial dynamics and spatio-temporal appearance over smile expressions, we show that it is possible to improve the state ofthe art in thisproblem, and verify that it is indeed possible to recognize kinship by resemblance of facial expressions. [sent-9, score-1.672]
</p><p>4 The proposed method is tested on different kin relationships. [sent-10, score-0.041]
</p><p>5 Introduction Automatic detection of kinship from facial appearance is a difficult problem with several applications, including social media analysis [20, 21], finding missing children and children adoptions [9], and coaching for imitation and personification. [sent-14, score-1.21]
</p><p>6 Kinship is a genetic relationship between two family members, including parent-child, siblingsibling, and grandparent-grandchild relations. [sent-15, score-0.057]
</p><p>7 Since a ge-  netic test may not always be available for checking kinship, an unobtrusive and rapid computer vision solution is potentially very useful. [sent-16, score-0.029]
</p><p>8 This paper proposes such a novel approach for kinship detection. [sent-17, score-0.845]
</p><p>9 Kinship may be verified between people that have different sex and different ages (e. [sent-18, score-0.048]
</p><p>10 Humans use an aggregate of different features to judge kinship from facial images [1]. [sent-21, score-1.167]
</p><p>11 Furthermore, depending on the age of the person assessed for kinship, humans use different sets of features consistent with the expected aging-related form changes in faces. [sent-22, score-0.041]
</p><p>12 For example, upper face cues are more prominently used for kids, as the lower face does not fully form until adulthood [13]. [sent-23, score-0.157]
</p><p>13 Automatic kinship detection methods also employ aggregate sets of features including color, geometry, and appearance. [sent-24, score-0.897]
</p><p>14 In Section 2 we summarize the recent related work in this area. [sent-25, score-0.02]
</p><p>15 All the methods proposed so far to verify kinship work with images. [sent-26, score-0.884]
</p><p>16 In contrast to all published material, in this paper, we propose a method using facial dynamics to verify kinship from videos. [sent-27, score-1.241]
</p><p>17 Our approach intuitively makes sense: we all know people who do not look like their parents, until they smile. [sent-28, score-0.025]
</p><p>18 Furthermore, findings of [14] show that the appearance of spontaneous facial expressions of born-blind people and their sighted relatives are similar. [sent-29, score-0.555]
</p><p>19 However, the resemblance between facial expressions depends not only on the appearance of the expression but also on its dynamics, as each expression is created by a combination of vol-  untary and involuntary muscle movements. [sent-30, score-0.57]
</p><p>20 In this paper, we verify this insight empirically, and show that dynamic features obtained during facial expressions have discriminatory power for the kinship verification. [sent-32, score-1.333]
</p><p>21 This is the first work that uses dynamic features for kinship detection. [sent-33, score-0.897]
</p><p>22 By combining dynamic and spatio-temporal features, we approach the problem of automatic kinship verification. [sent-34, score-0.877]
</p><p>23 We use the recently collected UvA-NEMO Smile Database [3] in our experiments, compare our method with three recent approaches from the literature [8, 9, 21], and report state-of-the-art results. [sent-35, score-0.019]
</p><p>24 Related work In one of the first works on kinship verification, Fang et al. [sent-37, score-0.845]
</p><p>25 used the skin, hair and eye color, facial geometry measures, as well as holistic texture features computed on texture gradients of the whole face [8]. [sent-38, score-0.367]
</p><p>26 Color based features performed better than the other features in general, since a good registration between individual face images was largely lacking in their approach. [sent-40, score-0.15]
</p><p>27 In the present study, we use their approach as a baseline under controlled registration conditions. [sent-41, score-0.025]
</p><p>28 Different feature descriptors are evaluated for the kinship verification problem in the literature. [sent-42, score-0.926]
</p><p>29 In [9], eyes, mouth and nose parts are matched via DAISY descriptors. [sent-43, score-0.033]
</p><p>30 Therefore, typically, the top few  11449977  matching features are used for verification. [sent-45, score-0.02]
</p><p>31 In [21], Gaborbased Gradient Orientation Pyramid (GGOP) descriptors are proposed and used to model facial appearance for kinship verification. [sent-46, score-1.115]
</p><p>32 In [11], the Self Similarity Representation of Weber face (SSRW) algorithm is proposed. [sent-50, score-0.065]
</p><p>33 Each face is represented by only its reflectance and difference of Gaussian filters are used to select keypoints to represent each face. [sent-51, score-0.065]
</p><p>34 While SVM seems to be the classifier of choice for kinship verification, in [12], a metric learning approach is adopted. [sent-53, score-0.845]
</p><p>35 Samples that have the kinship relation are pulled close, and other samples are pushed apart. [sent-54, score-0.892]
</p><p>36 In this space, the transformation is complemented by defining a margin for kinship. [sent-55, score-0.022]
</p><p>37 The evaluation protocols used for the kinship verification problem typically make use of pairs of photographs, where each pair is either a positive sample (i. [sent-56, score-0.946]
</p><p>38 In [9], 100 face pairs with kinship and 100 pairs without are selected from family photos. [sent-59, score-0.944]
</p><p>39 There was no decomposition of results into specific kinship categories. [sent-60, score-0.845]
</p><p>40 In [8], [21], and [20] photos of celebrities have been downloaded from the Internet. [sent-61, score-0.024]
</p><p>41 In these studies, as well as  in [12], four kinship relations (Father-Son, Father-Daughter, Mother-Son and Mother-Daughter) are analyzed separately. [sent-62, score-0.893]
</p><p>42 The largest database reported in the literature so far is the KinFaceW-II image database, with 250 pairs of kinship relations for each of these four categories. [sent-63, score-0.884]
</p><p>43 analyze the spontaneous facial expressions of born-blind people and their sighted relatives. [sent-65, score-0.506]
</p><p>44 They show that such expressions carry a unique family signature. [sent-66, score-0.128]
</p><p>45 Occurrences of a set of facial movements are used to classify families of blind subjects. [sent-67, score-0.3]
</p><p>46 Results show 64% correct classification on the average, with 60% in joy expressions. [sent-68, score-0.029]
</p><p>47 Although [14] has focused on the facial movements for the task, they did not analyze the dynamics of expressions in terms of duration, intensity, speed, and acceleration, which is an empirical contribution of this paper. [sent-70, score-0.482]
</p><p>48 Method In this paper, we propose to combine spatio-temporal facial features and facial expression dynamics for the kinship verification. [sent-72, score-1.539]
</p><p>49 To this end, videos of enjoyment smiles are used. [sent-73, score-0.077]
</p><p>50 Our system analyzes the entire duration of a smile, starting from a moderately frontal and neutral face, the unfolding of the smile, and the return to the neutral face. [sent-74, score-0.196]
</p><p>51 Unlike other approaches proposed in the literature, our method works with videos of faces, rather than images. [sent-75, score-0.021]
</p><p>52 This is the first approach using videos for kinship verification. [sent-76, score-0.866]
</p><p>53 (a) The facial feature points used in this study with their indices, (b) the 3D mesh model and visualization of the amplitude signals, which are defined as the mean of left/right amplitude signals on the face. [sent-78, score-0.411]
</p><p>54 For simplicity, visualizations are shown on a single side of the face We summarize the proposed method here. [sent-79, score-0.105]
</p><p>55 Our approach starts with face detection in the first frame and the localization of 17 facial landmarks, which are subsequently tracked during the rest of the video. [sent-80, score-0.347]
</p><p>56 Using the tracked landmarks, displacement signals of eyebrows, eyelids, cheeks, and lip corners are computed. [sent-81, score-0.293]
</p><p>57 Afterwards, the mean displacement signal of the lip corners is analyzed and the three main temporal phases (i. [sent-82, score-0.272]
</p><p>58 onset, apex, and offset, respectively) of the smile are estimated. [sent-84, score-0.098]
</p><p>59 Then, facial expression dynamics on eyebrows, eyelids, cheeks, and lip corners are extracted from each phase separately. [sent-85, score-0.626]
</p><p>60 To describe the change in ap-  pearance between the neutral and the expressive face (i. [sent-86, score-0.137]
</p><p>61 the apex of the expression), temporal Completed Local Binary Pattern (CLBP) descriptors are computed from the eye, cheek, and lip regions. [sent-88, score-0.197]
</p><p>62 After a feature selection step, the most informative dynamic features are identified and combined with temporal CLBP features. [sent-89, score-0.074]
</p><p>63 Landmark detection and tracking Both the correct detection and accurate tracking of facial landmarks are crucial for normalizing and aligning faces, and for extracting consistent dynamic features. [sent-94, score-0.38]
</p><p>64 In the first frame of the input video, 17 facial landmarks (i. [sent-95, score-0.348]
</p><p>65 centers of eyebrows, eyebrow corners, eye corners, centers of upper eyelids, cheek centers, nose tip, and lip corners) are detected using a recent landmarking approach [4] (see Fig. [sent-97, score-0.341]
</p><p>66 This method models Gabor wavelet features of a neighborhood of the landmarks using incremental mixtures of factor analyzers and enables a shape prior to ensure the integrity of the landmark constellation. [sent-99, score-0.175]
</p><p>67 It follows a coarse-to-fine strategy; landmarks are initially detected on a coarse level and then fine-tuned for higher resolution. [sent-100, score-0.12]
</p><p>68 Then, these points are tracked by a piecewise B ´ezier volume deformation (PBVD) tracker [18] during the rest of the video. [sent-101, score-0.06]
</p><p>69 11449988  Initially, the PBVD tracker warps a generic 3D mesh  model of the face (see Fig. [sent-102, score-0.135]
</p><p>70 1(b)) to fit the facial landmarks in the first frame of the image sequence. [sent-103, score-0.348]
</p><p>71 k=0  (1)  where the control points denoted with bi,j,k and mesh variables 0 < {u, v, w} < 1control the shape of the volume. [sent-115, score-0.044]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kinship', 0.845), ('facial', 0.248), ('lip', 0.121), ('dynamics', 0.109), ('landmarks', 0.1), ('eyelids', 0.099), ('ezier', 0.099), ('smile', 0.098), ('expressions', 0.094), ('verification', 0.081), ('corners', 0.079), ('spontaneous', 0.073), ('eyebrows', 0.07), ('expression', 0.069), ('amsterdam', 0.067), ('cheeks', 0.066), ('clbp', 0.066), ('delft', 0.066), ('pbvd', 0.066), ('sighted', 0.066), ('face', 0.065), ('apex', 0.054), ('neutral', 0.05), ('netherlands', 0.048), ('cheek', 0.048), ('mesh', 0.044), ('resemblance', 0.043), ('kin', 0.041), ('amplitude', 0.041), ('verify', 0.039), ('centers', 0.039), ('signals', 0.037), ('tracked', 0.034), ('eye', 0.034), ('family', 0.034), ('children', 0.033), ('nose', 0.033), ('dynamic', 0.032), ('aggregate', 0.032), ('movements', 0.031), ('unfolding', 0.029), ('uva', 0.029), ('imitation', 0.029), ('netic', 0.029), ('smiles', 0.029), ('joy', 0.029), ('boun', 0.029), ('discriminatory', 0.029), ('nl', 0.028), ('duration', 0.028), ('analyzed', 0.028), ('landmark', 0.028), ('kids', 0.027), ('integrity', 0.027), ('relatives', 0.027), ('contrasted', 0.027), ('prominently', 0.027), ('eyebrow', 0.027), ('enjoyment', 0.027), ('albert', 0.027), ('insight', 0.026), ('tracker', 0.026), ('peleg', 0.025), ('pulled', 0.025), ('muscle', 0.025), ('inherited', 0.025), ('people', 0.025), ('registration', 0.025), ('onset', 0.024), ('celebrities', 0.024), ('theo', 0.024), ('daisy', 0.023), ('genetic', 0.023), ('sex', 0.023), ('displacement', 0.022), ('judge', 0.022), ('justify', 0.022), ('complemented', 0.022), ('pushed', 0.022), ('weber', 0.022), ('pearance', 0.022), ('temporal', 0.022), ('appearance', 0.022), ('videos', 0.021), ('assessed', 0.021), ('families', 0.021), ('visualizations', 0.02), ('lacking', 0.02), ('son', 0.02), ('bioinformatics', 0.02), ('summarize', 0.02), ('features', 0.02), ('parents', 0.02), ('moderately', 0.02), ('relations', 0.02), ('initially', 0.02), ('protocols', 0.02), ('literature', 0.019), ('analyzes', 0.019), ('completed', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="251-tfidf-1" href="./iccv-2013-Like_Father%2C_Like_Son%3A_Facial_Expression_Dynamics_for_Kinship_Verification.html">251 iccv-2013-Like Father, Like Son: Facial Expression Dynamics for Kinship Verification</a></p>
<p>Author: Hamdi Dibeklioglu, Albert Ali Salah, Theo Gevers</p><p>Abstract: Kinship verification from facial appearance is a difficult problem. This paper explores the possibility of employing facial expression dynamics in this problem. By using features that describe facial dynamics and spatio-temporal appearance over smile expressions, we show that it is possible to improve the state ofthe art in thisproblem, and verify that it is indeed possible to recognize kinship by resemblance of facial expressions. The proposed method is tested on different kin relationships. On the average, 72.89% verification accuracy is achieved on spontaneous smiles.</p><p>2 0.21493784 <a title="251-tfidf-2" href="./iccv-2013-Accurate_and_Robust_3D_Facial_Capture_Using_a_Single_RGBD_Camera.html">36 iccv-2013-Accurate and Robust 3D Facial Capture Using a Single RGBD Camera</a></p>
<p>Author: Yen-Lin Chen, Hsiang-Tao Wu, Fuhao Shi, Xin Tong, Jinxiang Chai</p><p>Abstract: This paper presents an automatic and robust approach that accurately captures high-quality 3D facial performances using a single RGBD camera. The key of our approach is to combine the power of automatic facial feature detection and image-based 3D nonrigid registration techniques for 3D facial reconstruction. In particular, we develop a robust and accurate image-based nonrigid registration algorithm that incrementally deforms a 3D template mesh model to best match observed depth image data and important facial features detected from single RGBD images. The whole process is fully automatic and robust because it is based on single frame facial registration framework. The system is flexible because it does not require any strong 3D facial priors such as blendshape models. We demonstrate the power of our approach by capturing a wide range of 3D facial expressions using a single RGBD camera and achieve state-of-the-art accuracy by comparing against alternative methods.</p><p>3 0.1763901 <a title="251-tfidf-3" href="./iccv-2013-Cascaded_Shape_Space_Pruning_for_Robust_Facial_Landmark_Detection.html">70 iccv-2013-Cascaded Shape Space Pruning for Robust Facial Landmark Detection</a></p>
<p>Author: Xiaowei Zhao, Shiguang Shan, Xiujuan Chai, Xilin Chen</p><p>Abstract: In this paper, we propose a novel cascaded face shape space pruning algorithm for robust facial landmark detection. Through progressively excluding the incorrect candidate shapes, our algorithm can accurately and efficiently achieve the globally optimal shape configuration. Specifically, individual landmark detectors are firstly applied to eliminate wrong candidates for each landmark. Then, the candidate shape space is further pruned by jointly removing incorrect shape configurations. To achieve this purpose, a discriminative structure classifier is designed to assess the candidate shape configurations. Based on the learned discriminative structure classifier, an efficient shape space pruning strategy is proposed to quickly reject most incorrect candidate shapes while preserve the true shape. The proposed algorithm is carefully evaluated on a large set of real world face images. In addition, comparison results on the publicly available BioID and LFW face databases demonstrate that our algorithm outperforms some state-of-the-art algorithms.</p><p>4 0.12928088 <a title="251-tfidf-4" href="./iccv-2013-Pose-Free_Facial_Landmark_Fitting_via_Optimized_Part_Mixtures_and_Cascaded_Deformable_Shape_Model.html">321 iccv-2013-Pose-Free Facial Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Shape Model</a></p>
<p>Author: Xiang Yu, Junzhou Huang, Shaoting Zhang, Wang Yan, Dimitris N. Metaxas</p><p>Abstract: This paper addresses the problem of facial landmark localization and tracking from a single camera. We present a two-stage cascaded deformable shape model to effectively and efficiently localize facial landmarks with large head pose variations. For face detection, we propose a group sparse learning method to automatically select the most salient facial landmarks. By introducing 3D face shape model, we use procrustes analysis to achieve pose-free facial landmark initialization. For deformation, the first step uses mean-shift local search with constrained local model to rapidly approach the global optimum. The second step uses component-wise active contours to discriminatively refine the subtle shape variation. Our framework can simultaneously handle face detection, pose-free landmark localization and tracking in real time. Extensive experiments are conducted on both laboratory environmental face databases and face-in-the-wild databases. All results demonstrate that our approach has certain advantages over state-of-theart methods in handling pose variations1.</p><p>5 0.12257602 <a title="251-tfidf-5" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>Author: Ira Kemelmacher-Shlizerman</p><p>Abstract: In thispaper wepresent a new concept ofbuilding a morphable model directly from photos on the Internet. Morphable models have shown very impressive results more than a decade ago, and could potentially have a huge impact on all aspects of face modeling and recognition. One of the challenges, however, is to capture and register 3D laser scans of large number of people and facial expressions. Nowadays, there are enormous amounts of face photos on the Internet, large portion of which has semantic labels. We propose a framework to build a morphable model directly from photos, the framework includes dense registration of Internet photos, as well as, new single view shape reconstruction and modification algorithms.</p><p>6 0.10664729 <a title="251-tfidf-6" href="./iccv-2013-Capturing_Global_Semantic_Relationships_for_Facial_Action_Unit_Recognition.html">69 iccv-2013-Capturing Global Semantic Relationships for Facial Action Unit Recognition</a></p>
<p>7 0.10655949 <a title="251-tfidf-7" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>8 0.10549697 <a title="251-tfidf-8" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>9 0.099465914 <a title="251-tfidf-9" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>10 0.099418037 <a title="251-tfidf-10" href="./iccv-2013-Exemplar-Based_Graph_Matching_for_Robust_Facial_Landmark_Localization.html">149 iccv-2013-Exemplar-Based Graph Matching for Robust Facial Landmark Localization</a></p>
<p>11 0.090141498 <a title="251-tfidf-11" href="./iccv-2013-Facial_Action_Unit_Event_Detection_by_Cascade_of_Tasks.html">155 iccv-2013-Facial Action Unit Event Detection by Cascade of Tasks</a></p>
<p>12 0.06575349 <a title="251-tfidf-12" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>13 0.064240135 <a title="251-tfidf-13" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>14 0.058252055 <a title="251-tfidf-14" href="./iccv-2013-Rank_Minimization_across_Appearance_and_Shape_for_AAM_Ensemble_Fitting.html">339 iccv-2013-Rank Minimization across Appearance and Shape for AAM Ensemble Fitting</a></p>
<p>15 0.057627499 <a title="251-tfidf-15" href="./iccv-2013-Hidden_Factor_Analysis_for_Age_Invariant_Face_Recognition.html">195 iccv-2013-Hidden Factor Analysis for Age Invariant Face Recognition</a></p>
<p>16 0.053687282 <a title="251-tfidf-16" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>17 0.051430002 <a title="251-tfidf-17" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>18 0.051231463 <a title="251-tfidf-18" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>19 0.050784539 <a title="251-tfidf-19" href="./iccv-2013-No_Matter_Where_You_Are%3A_Flexible_Graph-Guided_Multi-task_Learning_for_Multi-view_Head_Pose_Classification_under_Target_Motion.html">291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</a></p>
<p>20 0.044866677 <a title="251-tfidf-20" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.091), (1, -0.005), (2, -0.045), (3, -0.04), (4, -0.009), (5, -0.092), (6, 0.2), (7, 0.048), (8, -0.009), (9, 0.012), (10, -0.052), (11, 0.063), (12, 0.067), (13, 0.043), (14, -0.03), (15, 0.01), (16, 0.028), (17, 0.024), (18, -0.041), (19, -0.093), (20, -0.001), (21, 0.063), (22, -0.012), (23, 0.078), (24, -0.027), (25, -0.055), (26, 0.043), (27, -0.092), (28, 0.001), (29, 0.022), (30, 0.0), (31, 0.053), (32, -0.056), (33, 0.021), (34, -0.017), (35, -0.005), (36, 0.016), (37, 0.015), (38, -0.065), (39, 0.065), (40, 0.016), (41, -0.013), (42, 0.072), (43, -0.072), (44, 0.038), (45, -0.005), (46, -0.009), (47, -0.018), (48, 0.074), (49, 0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94027418 <a title="251-lsi-1" href="./iccv-2013-Like_Father%2C_Like_Son%3A_Facial_Expression_Dynamics_for_Kinship_Verification.html">251 iccv-2013-Like Father, Like Son: Facial Expression Dynamics for Kinship Verification</a></p>
<p>Author: Hamdi Dibeklioglu, Albert Ali Salah, Theo Gevers</p><p>Abstract: Kinship verification from facial appearance is a difficult problem. This paper explores the possibility of employing facial expression dynamics in this problem. By using features that describe facial dynamics and spatio-temporal appearance over smile expressions, we show that it is possible to improve the state ofthe art in thisproblem, and verify that it is indeed possible to recognize kinship by resemblance of facial expressions. The proposed method is tested on different kin relationships. On the average, 72.89% verification accuracy is achieved on spontaneous smiles.</p><p>2 0.82154524 <a title="251-lsi-2" href="./iccv-2013-Accurate_and_Robust_3D_Facial_Capture_Using_a_Single_RGBD_Camera.html">36 iccv-2013-Accurate and Robust 3D Facial Capture Using a Single RGBD Camera</a></p>
<p>Author: Yen-Lin Chen, Hsiang-Tao Wu, Fuhao Shi, Xin Tong, Jinxiang Chai</p><p>Abstract: This paper presents an automatic and robust approach that accurately captures high-quality 3D facial performances using a single RGBD camera. The key of our approach is to combine the power of automatic facial feature detection and image-based 3D nonrigid registration techniques for 3D facial reconstruction. In particular, we develop a robust and accurate image-based nonrigid registration algorithm that incrementally deforms a 3D template mesh model to best match observed depth image data and important facial features detected from single RGBD images. The whole process is fully automatic and robust because it is based on single frame facial registration framework. The system is flexible because it does not require any strong 3D facial priors such as blendshape models. We demonstrate the power of our approach by capturing a wide range of 3D facial expressions using a single RGBD camera and achieve state-of-the-art accuracy by comparing against alternative methods.</p><p>3 0.79590482 <a title="251-lsi-3" href="./iccv-2013-Cascaded_Shape_Space_Pruning_for_Robust_Facial_Landmark_Detection.html">70 iccv-2013-Cascaded Shape Space Pruning for Robust Facial Landmark Detection</a></p>
<p>Author: Xiaowei Zhao, Shiguang Shan, Xiujuan Chai, Xilin Chen</p><p>Abstract: In this paper, we propose a novel cascaded face shape space pruning algorithm for robust facial landmark detection. Through progressively excluding the incorrect candidate shapes, our algorithm can accurately and efficiently achieve the globally optimal shape configuration. Specifically, individual landmark detectors are firstly applied to eliminate wrong candidates for each landmark. Then, the candidate shape space is further pruned by jointly removing incorrect shape configurations. To achieve this purpose, a discriminative structure classifier is designed to assess the candidate shape configurations. Based on the learned discriminative structure classifier, an efficient shape space pruning strategy is proposed to quickly reject most incorrect candidate shapes while preserve the true shape. The proposed algorithm is carefully evaluated on a large set of real world face images. In addition, comparison results on the publicly available BioID and LFW face databases demonstrate that our algorithm outperforms some state-of-the-art algorithms.</p><p>4 0.73506784 <a title="251-lsi-4" href="./iccv-2013-Exemplar-Based_Graph_Matching_for_Robust_Facial_Landmark_Localization.html">149 iccv-2013-Exemplar-Based Graph Matching for Robust Facial Landmark Localization</a></p>
<p>Author: Feng Zhou, Jonathan Brandt, Zhe Lin</p><p>Abstract: Localizing facial landmarks is a fundamental step in facial image analysis. However, the problem is still challenging due to the large variability in pose and appearance, and the existence ofocclusions in real-worldface images. In this paper, we present exemplar-based graph matching (EGM), a robust framework for facial landmark localization. Compared to conventional algorithms, EGM has three advantages: (1) an affine-invariant shape constraint is learned online from similar exemplars to better adapt to the test face; (2) the optimal landmark configuration can be directly obtained by solving a graph matching problem with the learned shape constraint; (3) the graph matching problem can be optimized efficiently by linear programming. To our best knowledge, this is the first attempt to apply a graph matching technique for facial landmark localization. Experiments on several challenging datasets demonstrate the advantages of EGM over state-of-the-art methods.</p><p>5 0.72834367 <a title="251-lsi-5" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>Author: Heng Yang, Ioannis Patras</p><p>Abstract: In this paper we propose a method for the localization of multiple facial features on challenging face images. In the regression forests (RF) framework, observations (patches) that are extracted at several image locations cast votes for the localization of several facial features. In order to filter out votes that are not relevant, we pass them through two types of sieves, that are organised in a cascade, and which enforce geometric constraints. The first sieve filters out votes that are not consistent with a hypothesis for the location of the face center. Several sieves of the second type, one associated with each individual facial point, filter out distant votes. We propose a method that adjusts onthe-fly the proximity threshold of each second type sieve by applying a classifier which, based on middle-level features extracted from voting maps for the facial feature in question, makes a sequence of decisions on whether the threshold should be reduced or not. We validate our proposed method on two challenging datasets with images collected from the Internet in which we obtain state of the art results without resorting to explicit facial shape models. We also show the benefits of our method for proximity threshold adjustment especially on ’difficult’ face images.</p><p>6 0.71996099 <a title="251-lsi-6" href="./iccv-2013-Pose-Free_Facial_Landmark_Fitting_via_Optimized_Part_Mixtures_and_Cascaded_Deformable_Shape_Model.html">321 iccv-2013-Pose-Free Facial Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Shape Model</a></p>
<p>7 0.68220299 <a title="251-lsi-7" href="./iccv-2013-Capturing_Global_Semantic_Relationships_for_Facial_Action_Unit_Recognition.html">69 iccv-2013-Capturing Global Semantic Relationships for Facial Action Unit Recognition</a></p>
<p>8 0.65328497 <a title="251-lsi-8" href="./iccv-2013-Facial_Action_Unit_Event_Detection_by_Cascade_of_Tasks.html">155 iccv-2013-Facial Action Unit Event Detection by Cascade of Tasks</a></p>
<p>9 0.61006439 <a title="251-lsi-9" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>10 0.5588811 <a title="251-lsi-10" href="./iccv-2013-Robust_Face_Landmark_Estimation_under_Occlusion.html">355 iccv-2013-Robust Face Landmark Estimation under Occlusion</a></p>
<p>11 0.53662425 <a title="251-lsi-11" href="./iccv-2013-Modifying_the_Memorability_of_Face_Photographs.html">272 iccv-2013-Modifying the Memorability of Face Photographs</a></p>
<p>12 0.51483238 <a title="251-lsi-12" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>13 0.4813534 <a title="251-lsi-13" href="./iccv-2013-Hidden_Factor_Analysis_for_Age_Invariant_Face_Recognition.html">195 iccv-2013-Hidden Factor Analysis for Age Invariant Face Recognition</a></p>
<p>14 0.45617494 <a title="251-lsi-14" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>15 0.41969189 <a title="251-lsi-15" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>16 0.40928322 <a title="251-lsi-16" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>17 0.40919283 <a title="251-lsi-17" href="./iccv-2013-Learning_Slow_Features_for_Behaviour_Analysis.html">243 iccv-2013-Learning Slow Features for Behaviour Analysis</a></p>
<p>18 0.34811687 <a title="251-lsi-18" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>19 0.34388572 <a title="251-lsi-19" href="./iccv-2013-Discovering_Details_and_Scene_Structure_with_Hierarchical_Iconoid_Shift.html">117 iccv-2013-Discovering Details and Scene Structure with Hierarchical Iconoid Shift</a></p>
<p>20 0.34298685 <a title="251-lsi-20" href="./iccv-2013-Geometric_Registration_Based_on_Distortion_Estimation.html">183 iccv-2013-Geometric Registration Based on Distortion Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.053), (7, 0.012), (12, 0.012), (26, 0.08), (31, 0.025), (35, 0.02), (42, 0.143), (64, 0.031), (73, 0.023), (78, 0.042), (82, 0.286), (89, 0.137), (98, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74120909 <a title="251-lda-1" href="./iccv-2013-Like_Father%2C_Like_Son%3A_Facial_Expression_Dynamics_for_Kinship_Verification.html">251 iccv-2013-Like Father, Like Son: Facial Expression Dynamics for Kinship Verification</a></p>
<p>Author: Hamdi Dibeklioglu, Albert Ali Salah, Theo Gevers</p><p>Abstract: Kinship verification from facial appearance is a difficult problem. This paper explores the possibility of employing facial expression dynamics in this problem. By using features that describe facial dynamics and spatio-temporal appearance over smile expressions, we show that it is possible to improve the state ofthe art in thisproblem, and verify that it is indeed possible to recognize kinship by resemblance of facial expressions. The proposed method is tested on different kin relationships. On the average, 72.89% verification accuracy is achieved on spontaneous smiles.</p><p>2 0.66612804 <a title="251-lda-2" href="./iccv-2013-Learning_to_Rank_Using_Privileged_Information.html">248 iccv-2013-Learning to Rank Using Privileged Information</a></p>
<p>Author: Viktoriia Sharmanska, Novi Quadrianto, Christoph H. Lampert</p><p>Abstract: Many computer visionproblems have an asymmetric distribution of information between training and test time. In this work, we study the case where we are given additional information about the training data, which however will not be available at test time. This situation is called learning using privileged information (LUPI). We introduce two maximum-margin techniques that are able to make use of this additional source of information, and we show that the framework is applicable to several scenarios that have been studied in computer vision before. Experiments with attributes, bounding boxes, image tags and rationales as additional information in object classification show promising results.</p><p>3 0.64465797 <a title="251-lda-3" href="./iccv-2013-Modeling_4D_Human-Object_Interactions_for_Event_and_Object_Recognition.html">268 iccv-2013-Modeling 4D Human-Object Interactions for Event and Object Recognition</a></p>
<p>Author: Ping Wei, Yibiao Zhao, Nanning Zheng, Song-Chun Zhu</p><p>Abstract: Recognizing the events and objects in the video sequence are two challenging tasks due to the complex temporal structures and the large appearance variations. In this paper, we propose a 4D human-object interaction model, where the two tasks jointly boost each other. Our human-object interaction is defined in 4D space: i) the cooccurrence and geometric constraints of human pose and object in 3D space; ii) the sub-events transition and objects coherence in 1D temporal dimension. We represent the structure of events, sub-events and objects in a hierarchical graph. For an input RGB-depth video, we design a dynamic programming beam search algorithm to: i) segment the video, ii) recognize the events, and iii) detect the objects simultaneously. For evaluation, we built a large-scale multiview 3D event dataset which contains 3815 video sequences and 383,036 RGBD frames captured by the Kinect cameras. The experiment results on this dataset show the effectiveness of our method.</p><p>4 0.60895705 <a title="251-lda-4" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>Author: Chenglong Bao, Jian-Feng Cai, Hui Ji</p><p>Abstract: In recent years, how to learn a dictionary from input images for sparse modelling has been one very active topic in image processing and recognition. Most existing dictionary learning methods consider an over-complete dictionary, e.g. the K-SVD method. Often they require solving some minimization problem that is very challenging in terms of computational feasibility and efficiency. However, if the correlations among dictionary atoms are not well constrained, the redundancy of the dictionary does not necessarily improve the performance of sparse coding. This paper proposed a fast orthogonal dictionary learning method for sparse image representation. With comparable performance on several image restoration tasks, the proposed method is much more computationally efficient than the over-complete dictionary based learning methods.</p><p>5 0.60891008 <a title="251-lda-5" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>Author: Chen-Kuo Chiang, Te-Feng Su, Chih Yen, Shang-Hong Lai</p><p>Abstract: We present a multi-attributed dictionary learning algorithm for sparse coding. Considering training samples with multiple attributes, a new distance matrix is proposed by jointly incorporating data and attribute similarities. Then, an objective function is presented to learn categorydependent dictionaries that are compact (closeness of dictionary atoms based on data distance and attribute similarity), reconstructive (low reconstruction error with correct dictionary) and label-consistent (encouraging the labels of dictionary atoms to be similar). We have demonstrated our algorithm on action classification and face recognition tasks on several publicly available datasets. Experimental results with improved performance over previous dictionary learning methods are shown to validate the effectiveness of the proposed algorithm.</p><p>6 0.60670948 <a title="251-lda-6" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>7 0.60652453 <a title="251-lda-7" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>8 0.60541928 <a title="251-lda-8" href="./iccv-2013-New_Graph_Structured_Sparsity_Model_for_Multi-label_Image_Annotations.html">290 iccv-2013-New Graph Structured Sparsity Model for Multi-label Image Annotations</a></p>
<p>9 0.60514247 <a title="251-lda-9" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>10 0.60468411 <a title="251-lda-10" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>11 0.6043936 <a title="251-lda-11" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>12 0.60364491 <a title="251-lda-12" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>13 0.60339385 <a title="251-lda-13" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>14 0.60246944 <a title="251-lda-14" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>15 0.60111952 <a title="251-lda-15" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>16 0.60011292 <a title="251-lda-16" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>17 0.60004437 <a title="251-lda-17" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>18 0.59924525 <a title="251-lda-18" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>19 0.59912103 <a title="251-lda-19" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>20 0.59870887 <a title="251-lda-20" href="./iccv-2013-Exemplar-Based_Graph_Matching_for_Robust_Facial_Landmark_Localization.html">149 iccv-2013-Exemplar-Based Graph Matching for Robust Facial Landmark Localization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
