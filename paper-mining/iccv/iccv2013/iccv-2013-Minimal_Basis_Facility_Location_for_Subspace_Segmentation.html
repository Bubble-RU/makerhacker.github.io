<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>264 iccv-2013-Minimal Basis Facility Location for Subspace Segmentation</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-264" href="#">iccv2013-264</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>264 iccv-2013-Minimal Basis Facility Location for Subspace Segmentation</h1>
<br/><p>Source: <a title="iccv-2013-264-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Lee_Minimal_Basis_Facility_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Choon-Meng Lee, Loong-Fah Cheong</p><p>Abstract: In contrast to the current motion segmentation paradigm that assumes independence between the motion subspaces, we approach the motion segmentation problem by seeking the parsimonious basis set that can represent the data. Our formulation explicitly looks for the overlap between subspaces in order to achieve a minimal basis representation. This parsimonious basis set is important for the performance of our model selection scheme because the sharing of basis results in savings of model complexity cost. We propose the use of affinity propagation based method to determine the number of motion. The key lies in the incorporation of a global cost model into the factor graph, serving the role of model complexity. The introduction of this global cost model requires additional message update in the factor graph. We derive an efficient update for the new messages associated with this global cost model. An important step in the use of affinity propagation is the subspace hypotheses generation. We use the row-sparse convex proxy solution as an initialization strategy. We further encourage the selection of subspace hypotheses with shared basis by integrat- ing a discount scheme that lowers the factor graph facility cost based on shared basis. We verified the model selection and classification performance of our proposed method on both the original Hopkins 155 dataset and the more balanced Hopkins 380 dataset.</p><p>Reference: <a title="iccv-2013-264-reference" href="../iccv2013_reference/iccv-2013-Minimal_Basis_Facility_Location_for_Subspace_Segmentation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 sg  Abstract In contrast to the current motion segmentation paradigm that assumes independence between the motion subspaces, we approach the motion segmentation problem by seeking the parsimonious basis set that can represent the data. [sent-3, score-0.602]
</p><p>2 This parsimonious basis set is important for the performance of our model selection scheme because the sharing of basis results in savings of model complexity cost. [sent-5, score-0.297]
</p><p>3 The introduction of this global cost model requires additional message update in the factor graph. [sent-8, score-0.461]
</p><p>4 An important step in the use of affinity propagation is the subspace hypotheses generation. [sent-10, score-0.512]
</p><p>5 We further encourage the selection of subspace hypotheses with shared basis by integrat-  ing a discount scheme that lowers the factor graph facility cost based on shared basis. [sent-12, score-1.638]
</p><p>6 Spectral clustering has proven to be an effective and robust clustering method in the motion segmentation literature. [sent-16, score-0.275]
</p><p>7 Sparse Subspace Clustering(SSC)[6], Low Rank Representation(LRR)[18] and Linear Subspace Spectral Clustering(LSSC)[13] use spectral clustering for motion segmentation to achieve excellent results. [sent-17, score-0.307]
</p><p>8 Instead of reducing it to local trajectory-trajectory affinity representation, we generate a set of subspace hypotheses and compute the distance between the trajectories and the subspace hypothesis. [sent-42, score-0.772]
</p><p>9 With this measure of affinity to subspace hypotheses, model selection is based on the affinity propagation(AP)[8] framework with a judiciously chosen global cost function. [sent-43, score-0.663]
</p><p>10 However, KO’s random subspace  hypotheses generation strategy is different from our work. [sent-46, score-0.406]
</p><p>11 The subsequent treatment of these subspace hypotheses is also different from our approach. [sent-47, score-0.374]
</p><p>12 KO merges these subspace hypotheses in a greedy manner, choosing the pair with the lowest kernel-target alignment at each step. [sent-48, score-0.374]
</p><p>13 In section 3, we demonstrate how a minimal basis subspace hypotheses set can be generated by requiring the representation matrix to be jointly row sparse. [sent-49, score-0.511]
</p><p>14 Due to the convex relaxation artefact, the number of subspace hypotheses is far greater than the true number of subspaces. [sent-50, score-0.441]
</p><p>15 Although the subspace hypotheses set contains many overlapping subspaces, we still need to ensure the selection of those overlapping subspaces by introducing the facility cost discount scheme. [sent-53, score-1.528]
</p><p>16 Our method is significantly different from the current motion segmentation paradigm that uses spectral clustering. [sent-58, score-0.272]
</p><p>17 Whereas every current algorithm assumes subspace independence, treating the overlap as noise, our proposed work properly accounts for subspace dependencies by offering facility cost discount for shared basis. [sent-61, score-1.516]
</p><p>18 The use of these shared basis subspace for representation has important application in areas such as articulated motion and non-rigid structure from motion. [sent-62, score-0.5]
</p><p>19 Lastly, we show how the introduction of a global facility cost function to the AP framework enables model selection with good performance while maintaining efficiency. [sent-63, score-0.89]
</p><p>20 The affinity propagation clustering method has been applied to image categorization[5] and extended to motion segmentation in FLoSS(Facility Location for Subspace Segmentation)[15] and UFLP(Uncapacitated Facility Location Problem)[14]). [sent-74, score-0.359]
</p><p>21 In FLoSS and UFLP, motion segmentation is formulated as an instance of the facility location(FL) problem. [sent-75, score-0.846]
</p><p>22 In FLoSS, inference is based on the max-product belief propagation(MPBP) algorithm that involves local message passing. [sent-78, score-0.274]
</p><p>23 In addition to MPBP, UFLP proposed a linear programming(LP) relaxation based message passing algorithm, known as max-product linear programming(MPLP). [sent-80, score-0.343]
</p><p>24 On a related note, [16] formulated two-view motion segmentation as a facility location problem and solve it as a LP problem by relaxing the original facility location problem. [sent-82, score-1.581]
</p><p>25 Each facility now has an upper bound on the number of customers it can be assigned to. [sent-84, score-0.765]
</p><p>26 [11] shows that tractability can be assured by sorting the messages and consider only the top messages related to the facility capacity. [sent-86, score-0.871]
</p><p>27 The additional message update due to the global cost function in our work are made tractable and efficient by using similar techniques. [sent-87, score-0.425]
</p><p>28 Our proposed work capitalizes on this inherent capability of AP for model selection with the use of a more elaborate facility cost model. [sent-91, score-0.87]
</p><p>29 Furthermore, our quest for a minimal basis representation drives a more specific subspace hypotheses generation strategy. [sent-92, score-0.543]
</p><p>30 In FLoSS/UFLP, the subspace hypotheses are generated by random sampling. [sent-93, score-0.374]
</p><p>31 The facility cost function we propose in section 4. [sent-96, score-0.787]
</p><p>32 Hypothesis  generation  with minimal basis  subspace representation 3. [sent-99, score-0.429]
</p><p>33 Formulation Our subspace hypotheses generation strategy is based on finding the minimal basis subspace representation for the data matrix. [sent-101, score-0.803]
</p><p>34 While the overall two subspace structure is discernible, over segmentation is revealed in the gaps in the rows and the resultant extra rows, making the true number of motion hard to tell. [sent-165, score-0.427]
</p><p>35 There are in fact 40 subspace hypotheses generated from this convex solution. [sent-166, score-0.408]
</p><p>36 1 function means that trajectories tfruodme d tehpee same subspace t? [sent-191, score-0.302]
</p><p>37 Each column of the coefficient matrix proposes a subspace hypothesis and carries with it a notion of AP responsibility message update to this subspace hypothesis. [sent-200, score-0.908]
</p><p>38 Row wise, the coefficient matrix indicates the importance of the subspace hypothesis, in terms of the number of trajectory that generates the subspace hypothesis. [sent-201, score-0.548]
</p><p>39 This is reminiscent of the AP availability message update from the facility. [sent-202, score-0.297]
</p><p>40 This close relationship lends the joint sparse representation matrix well suited for subspace hypothesis generation. [sent-204, score-0.334]
</p><p>41 These extensions are the facility cost model outlined in section 4. [sent-207, score-0.787]
</p><p>42 We thus follow the notations in [9] and [15] in deriving the new message update required by our modified facility cost model. [sent-212, score-1.084]
</p><p>43 FLoSS/UFLP formulates the facility location problem in terms of factor graph representation(fig. [sent-216, score-0.762]
</p><p>44 Similarly, the notation h:j refers to the subset of binary variables connecting all the customers from 1 to N to facility j. [sent-236, score-0.746]
</p><p>45 Sij describes the distance between customer i and facility j. [sent-237, score-0.747]
</p><p>46 fj describes the cost when facility j is turned on. [sent-238, score-0.86]
</p><p>47 Upon convergence of the message update, the binary variables {hij } are turned  on i fm tehses sum pofd athtee, messages arriving east {thhe v}ar aiareb tleusr are non-negative. [sent-239, score-0.427]
</p><p>48 1  Local facility cost  Due to the key role of facility cost, we describe the FLoSS facility cost model so as to provide a contrast to our proposed cost model. [sent-242, score-2.361]
</p><p>49 In FLoSS, the subspace hypotheses are generated as random subsets of two, three and four trajectories, thus taking into consideration degenerate subspaces. [sent-243, score-0.374]
</p><p>50 The cost of a facility is set to be the sum of all pairwise distances between the trajectories forming the subspace. [sent-244, score-0.829]
</p><p>51 This local cost primarily serves to balance the tendency towards the higher dimensional subspace hypotheses, since higher dimensional subspace hypotheses are able to fit the data better compared to the lower dimensional subspace hypotheses. [sent-245, score-1.002]
</p><p>52 Thus they can merge excess number of facilities opened or increase the number of facilities opened by iteratively scaling down the local cost across all facilities. [sent-248, score-0.53]
</p><p>53 MB-FLoSS facility cost To address the aforementioned shortcomings, the facility cost function we propose is a global function in the sense that it is a function of the cardinality of the number of facilities opened. [sent-251, score-1.758]
</p><p>54 Given an upper bound K on the number of motion, we propose a power law facility cost model  C =? [sent-252, score-0.859]
</p><p>55 akp  ioft hke frawciilsieties are opened, for k = 1 to K  (4)  where C is the facility cost function and a, p are constants. [sent-253, score-0.787]
</p><p>56 With the global facility cost function (4), the factor graph representation needs to be modified, as shown in figure 3. [sent-261, score-0.88]
</p><p>57 The facility cost potential function is now connected to the binary variables {ej }. [sent-262, score-0.813]
</p><p>58 tTurhnee facility icnodstic fautencdti boyn t hCe ei sn uthmebreefro oref a efu}n cntoiodens so sfe {ej } 1. [sent-265, score-0.679]
</p><p>59 T Thhies change cwoisllt now inoencCe s isista thtee message passing involving {ej h}a, refwleilclte ndo iwn figure 4it 4. [sent-267, score-0.31]
</p><p>60 Objective function The one customer-one facility constraint remains:  Ii(hi:) =? [sent-269, score-0.679]
</p><p>61 erjwhiisje= 1  (5)  The consistency constraint that ensures that if a customer chooses a facility, the facility gets turned on, also stays:  Ej(h:j,ej) =? [sent-271, score-0.802]
</p><p>62 When the message passing terminates, the estimated MAP settings for each binary variable is recovered by summing all of its incoming messages. [sent-286, score-0.333]
</p><p>63 The message passing not involving {ej } remains the same as iens sFagLoeS pSa. [sent-288, score-0.31]
</p><p>64 1 Message update for φ Recall that we only need to send the difference between the message values corresponding to the two different settings. [sent-293, score-0.297]
</p><p>65 ing on the insights offered by [11], we observe ing the max can be achieved by evaluating the  the mesLeveragthat findsorted set  ξˆ and the associated facility cost over the K upper bound number of facilities, where ξˆ is obtained by sorting {ξj = ξj (1) − ξj (0) ,j = 1, . [sent-317, score-0.832]
</p><p>66 2  Facility cost discount scheme  The motivation behind the facility cost discount scheme is to encourage the facilities to have shared basis; the more the number of shared basis, the greater the discount. [sent-332, score-1.495]
</p><p>67 This discount is applied to the cost (4) so that using this discounted C used in computing message update in (8) can influence fCac uilsietdie si nw citohm sphuatriendg b maseisss atog eb e u cphdoasteen i. [sent-333, score-0.627]
</p><p>68 n The degree of overlap is based on comparison with a reference subspace set Sref, which contains the set of opened facilities according to the current beliefs. [sent-334, score-0.471]
</p><p>69 This reference subspace is initialized as facility j whose node {ej } has the largest bee ilsie inf. [sent-335, score-0.957]
</p><p>70 The candidate set Scan is initialized to be the remaining members of the entire subspace hypothesis set S. [sent-337, score-0.316]
</p><p>71 The idea behind the discount scheme is to iteratively fill Sref with K subspaces with the largest beliefs, after taking into account the facility cost discount due to overlapping subspace basis. [sent-338, score-1.53]
</p><p>72 At the ith iteration, the discount is applied to cost Ci. [sent-339, score-0.289]
</p><p>73 The belief for each subspace in Scan is recomputed with this discounted cost. [sent-340, score-0.325]
</p><p>74 The subspace with the largest belief will then be removed from Scan and added to Sref. [sent-341, score-0.302]
</p><p>75 After filling Sref with K subspace hypotheses, the discounted φ values associated with Sref replace the corresponding φ message update computed using (13). [sent-342, score-0.598]
</p><p>76 This facility cost discount scheme is summarized below:  4. [sent-343, score-0.995]
</p><p>77 3 Message update for ξ The message ξj can be interpreted as the overall responsibility to the facility j. [sent-345, score-1.011]
</p><p>78 For each facility j,let k be the index of the largest element of the set {ρij , i = 1, . [sent-346, score-0.697]
</p><p>79 4 Message update for α The other message update that is affected by the global facility function is α. [sent-354, score-1.043]
</p><p>80 The message update for α can be shown to be  αij= min[0,i? [sent-355, score-0.297]
</p><p>81 Subspace hypothesis generation and selection We provide a different subspace hypothesis generation strategy from FLoSS/UFLP. [sent-359, score-0.519]
</p><p>82 We therefore retain only the top four largest absolute value coefficients in each column and form a subspace hypothesis using that column. [sent-363, score-0.352]
</p><p>83 The number of subspace hypothesis 11559900  M is therefore the number of unique subspace hypothesis proposed by all the trajectories. [sent-364, score-0.632]
</p><p>84 When the MB-FLoSS message update is completed, subspace hypothesis j is chosen as a representation subspace if the belief ξj + φj at facility j is non-negative. [sent-365, score-1.594]
</p><p>85 There are 120 two motion sequences and 35 three motion sequences. [sent-369, score-0.279]
</p><p>86 For our facility cost model, we th inere thfeore ra sngete t ohef upper . [sent-374, score-0.811]
</p><p>87 The discount factor η used in the facility cost discount scheme(algorithm 1) is set to 0. [sent-379, score-1.185]
</p><p>88 Since the number of motion is no longer known a priori, we need to generalize the misclassification rate to take into account the wrong number of motion group given by model selection. [sent-381, score-0.326]
</p><p>89 In particular, for the Hopkins 155 dataset, the model selection algorithms should be tested against not just two and three motion but one motion as well. [sent-392, score-0.321]
</p><p>90 three motion sequences distorts the model selection rate, since focusing solely on two motion sequences will lead to good model selection rate. [sent-394, score-0.486]
</p><p>91 In view of these considerations, we choose to augment the Hopkins 155 dataset with one motion sequences and additional three motion sequences. [sent-396, score-0.279]
</p><p>92 The one motion sequences are derived from the original two and three motion sequences by treating each motion as a one motion sequence. [sent-397, score-0.558]
</p><p>93 For example, from the three motion sequence 1R2RC, we derive three sequences of one motion  1R2RC g1, 1R2RC g2, 1R2RC g3. [sent-398, score-0.279]
</p><p>94 The additional three motion sequences are generated by concatenating the two motion traffic sequences with the foreground one motion sequences derived from the two motion traffic sequences. [sent-399, score-0.651]
</p><p>95 It is worthwhile noting that both LRR and KO show better performance for 2 motion at the expense of 3 motion whereas our proposed method handles both 2 and 3 motion more evenly. [sent-410, score-0.357]
</p><p>96 Conclusion We formulated and realized the minimal basis approach to subspace segmentation and demonstrated its model selection strength. [sent-437, score-0.51]
</p><p>97 The success hinges on the use of an enhanced FLoSS framework, employing a convex relaxation formulation for subspace hypothesis generation, and a power-law facility cost with a simple discount scheme that favors overlapping subspace. [sent-438, score-1.405]
</p><p>98 Despite the added complexity due to the modified facility cost, we show how the message passing can be made tractable and efficient. [sent-439, score-0.989]
</p><p>99 The ordered residual kernel for robust motion subspace clustering. [sent-462, score-0.379]
</p><p>100 Solving the uncapacitated facility location problem using message passing algorithms. [sent-519, score-1.046]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('facility', 0.679), ('subspace', 0.26), ('message', 0.25), ('hopkins', 0.186), ('discount', 0.181), ('ej', 0.17), ('floss', 0.162), ('facilities', 0.128), ('motion', 0.119), ('lrr', 0.117), ('hypotheses', 0.114), ('cost', 0.108), ('messages', 0.096), ('affinity', 0.096), ('spectral', 0.086), ('selection', 0.083), ('opened', 0.083), ('hij', 0.079), ('basis', 0.075), ('customer', 0.068), ('givoni', 0.068), ('sref', 0.066), ('uflp', 0.066), ('misclassification', 0.066), ('passing', 0.06), ('ko', 0.058), ('hypothesis', 0.056), ('turned', 0.055), ('clustering', 0.054), ('preference', 0.053), ('augmented', 0.051), ('artefact', 0.05), ('mpbp', 0.05), ('subspaces', 0.049), ('segmentation', 0.048), ('update', 0.047), ('minimal', 0.044), ('trajectories', 0.042), ('propagation', 0.042), ('singular', 0.042), ('ij', 0.041), ('sequences', 0.041), ('discounted', 0.041), ('customers', 0.041), ('sij', 0.038), ('ap', 0.037), ('parsimonious', 0.037), ('factor', 0.036), ('cardinality', 0.036), ('responsibility', 0.035), ('convex', 0.034), ('relaxation', 0.033), ('fl', 0.032), ('generation', 0.032), ('frey', 0.032), ('ssc', 0.032), ('dependence', 0.031), ('kmax', 0.029), ('lazic', 0.029), ('uncapacitated', 0.029), ('location', 0.028), ('proxy', 0.028), ('shared', 0.028), ('trajectory', 0.028), ('motions', 0.027), ('scheme', 0.027), ('mplp', 0.027), ('clusters', 0.027), ('law', 0.027), ('overlapping', 0.027), ('permutation', 0.026), ('traffic', 0.026), ('variables', 0.026), ('understood', 0.025), ('mcin', 0.024), ('hi', 0.024), ('belief', 0.024), ('upper', 0.024), ('incoming', 0.023), ('beliefs', 0.023), ('rate', 0.022), ('pages', 0.022), ('combinatorial', 0.022), ('scan', 0.021), ('checkerboard', 0.021), ('bound', 0.021), ('global', 0.02), ('chin', 0.02), ('concern', 0.02), ('elhamifar', 0.019), ('paradigm', 0.019), ('sparsity', 0.019), ('alm', 0.019), ('graph', 0.019), ('fj', 0.018), ('aistats', 0.018), ('largest', 0.018), ('representation', 0.018), ('independence', 0.018), ('coefficients', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="264-tfidf-1" href="./iccv-2013-Minimal_Basis_Facility_Location_for_Subspace_Segmentation.html">264 iccv-2013-Minimal Basis Facility Location for Subspace Segmentation</a></p>
<p>Author: Choon-Meng Lee, Loong-Fah Cheong</p><p>Abstract: In contrast to the current motion segmentation paradigm that assumes independence between the motion subspaces, we approach the motion segmentation problem by seeking the parsimonious basis set that can represent the data. Our formulation explicitly looks for the overlap between subspaces in order to achieve a minimal basis representation. This parsimonious basis set is important for the performance of our model selection scheme because the sharing of basis results in savings of model complexity cost. We propose the use of affinity propagation based method to determine the number of motion. The key lies in the incorporation of a global cost model into the factor graph, serving the role of model complexity. The introduction of this global cost model requires additional message update in the factor graph. We derive an efficient update for the new messages associated with this global cost model. An important step in the use of affinity propagation is the subspace hypotheses generation. We use the row-sparse convex proxy solution as an initialization strategy. We further encourage the selection of subspace hypotheses with shared basis by integrat- ing a discount scheme that lowers the factor graph facility cost based on shared basis. We verified the model selection and classification performance of our proposed method on both the original Hopkins 155 dataset and the more balanced Hopkins 380 dataset.</p><p>2 0.27579275 <a title="264-tfidf-2" href="./iccv-2013-Unsupervised_Intrinsic_Calibration_from_a_Single_Frame_Using_a_%22Plumb-Line%22_Approach.html">436 iccv-2013-Unsupervised Intrinsic Calibration from a Single Frame Using a "Plumb-Line" Approach</a></p>
<p>Author: R. Melo, M. Antunes, J.P. Barreto, G. Falcão, N. Gonçalves</p><p>Abstract: Estimating the amount and center ofdistortionfrom lines in the scene has been addressed in the literature by the socalled “plumb-line ” approach. In this paper we propose a new geometric method to estimate not only the distortion parameters but the entire camera calibration (up to an “angular” scale factor) using a minimum of 3 lines. We propose a new framework for the unsupervised simultaneous detection of natural image of lines and camera parameters estimation, enabling a robust calibration from a single image. Comparative experiments with existing automatic approaches for the distortion estimation and with ground truth data are presented.</p><p>3 0.22311342 <a title="264-tfidf-3" href="./iccv-2013-Robust_Subspace_Clustering_via_Half-Quadratic_Minimization.html">360 iccv-2013-Robust Subspace Clustering via Half-Quadratic Minimization</a></p>
<p>Author: Yingya Zhang, Zhenan Sun, Ran He, Tieniu Tan</p><p>Abstract: Subspace clustering has important and wide applications in computer vision and pattern recognition. It is a challenging task to learn low-dimensional subspace structures due to the possible errors (e.g., noise and corruptions) existing in high-dimensional data. Recent subspace clustering methods usually assume a sparse representation of corrupted errors and correct the errors iteratively. However large corruptions in real-world applications can not be well addressed by these methods. A novel optimization model for robust subspace clustering is proposed in this paper. The objective function of our model mainly includes two parts. The first part aims to achieve a sparse representation of each high-dimensional data point with other data points. The second part aims to maximize the correntropy between a given data point and its low-dimensional representation with other points. Correntropy is a robust measure so that the influence of large corruptions on subspace clustering can be greatly suppressed. An extension of our method with explicit introduction of representation error terms into the model is also proposed. Half-quadratic minimization is provided as an efficient solution to the proposed robust subspace clustering formulations. Experimental results on Hopkins 155 dataset and Extended Yale Database B demonstrate that our method outperforms state-of-the-art subspace clustering methods.</p><p>4 0.20636593 <a title="264-tfidf-4" href="./iccv-2013-Latent_Space_Sparse_Subspace_Clustering.html">232 iccv-2013-Latent Space Sparse Subspace Clustering</a></p>
<p>Author: Vishal M. Patel, Hien Van Nguyen, René Vidal</p><p>Abstract: We propose a novel algorithm called Latent Space Sparse Subspace Clustering for simultaneous dimensionality reduction and clustering of data lying in a union of subspaces. Specifically, we describe a method that learns the projection of data and finds the sparse coefficients in the low-dimensional latent space. Cluster labels are then assigned by applying spectral clustering to a similarity matrix built from these sparse coefficients. An efficient optimization method is proposed and its non-linear extensions based on the kernel methods are presented. One of the main advantages of our method is that it is computationally efficient as the sparse coefficients are found in the low-dimensional latent space. Various experiments show that the proposed method performs better than the competitive state-of-theart subspace clustering methods.</p><p>5 0.19733933 <a title="264-tfidf-5" href="./iccv-2013-Distributed_Low-Rank_Subspace_Segmentation.html">122 iccv-2013-Distributed Low-Rank Subspace Segmentation</a></p>
<p>Author: Ameet Talwalkar, Lester Mackey, Yadong Mu, Shih-Fu Chang, Michael I. Jordan</p><p>Abstract: Vision problems ranging from image clustering to motion segmentation to semi-supervised learning can naturally be framed as subspace segmentation problems, in which one aims to recover multiple low-dimensional subspaces from noisy and corrupted input data. Low-Rank Representation (LRR), a convex formulation of the subspace segmentation problem, is provably and empirically accurate on small problems but does not scale to the massive sizes of modern vision datasets. Moreover, past work aimed at scaling up low-rank matrix factorization is not applicable to LRR given its non-decomposable constraints. In this work, we propose a novel divide-and-conquer algorithm for large-scale subspace segmentation that can cope with LRR ’s non-decomposable constraints and maintains LRR ’s strong recovery guarantees. This has immediate implications for the scalability of subspace segmentation, which we demonstrate on a benchmark face recognition dataset and in simulations. We then introduce novel applications of LRR-based subspace segmentation to large-scale semisupervised learning for multimedia event detection, concept detection, and image tagging. In each case, we obtain stateof-the-art results and order-of-magnitude speed ups.</p><p>6 0.18308905 <a title="264-tfidf-6" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>7 0.16396289 <a title="264-tfidf-7" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>8 0.15339682 <a title="264-tfidf-8" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>9 0.13120991 <a title="264-tfidf-9" href="./iccv-2013-Correlation_Adaptive_Subspace_Segmentation_by_Trace_Lasso.html">93 iccv-2013-Correlation Adaptive Subspace Segmentation by Trace Lasso</a></p>
<p>10 0.11675752 <a title="264-tfidf-10" href="./iccv-2013-Correntropy_Induced_L2_Graph_for_Robust_Subspace_Clustering.html">94 iccv-2013-Correntropy Induced L2 Graph for Robust Subspace Clustering</a></p>
<p>11 0.11603938 <a title="264-tfidf-11" href="./iccv-2013-GOSUS%3A_Grassmannian_Online_Subspace_Updates_with_Structured-Sparsity.html">182 iccv-2013-GOSUS: Grassmannian Online Subspace Updates with Structured-Sparsity</a></p>
<p>12 0.097362727 <a title="264-tfidf-12" href="./iccv-2013-Efficient_Higher-Order_Clustering_on_the_Grassmann_Manifold.html">134 iccv-2013-Efficient Higher-Order Clustering on the Grassmann Manifold</a></p>
<p>13 0.094470739 <a title="264-tfidf-13" href="./iccv-2013-Higher_Order_Matching_for_Consistent_Multiple_Target_Tracking.html">200 iccv-2013-Higher Order Matching for Consistent Multiple Target Tracking</a></p>
<p>14 0.090968765 <a title="264-tfidf-14" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>15 0.089490749 <a title="264-tfidf-15" href="./iccv-2013-Unsupervised_Visual_Domain_Adaptation_Using_Subspace_Alignment.html">438 iccv-2013-Unsupervised Visual Domain Adaptation Using Subspace Alignment</a></p>
<p>16 0.081513166 <a title="264-tfidf-16" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>17 0.078701399 <a title="264-tfidf-17" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>18 0.078236386 <a title="264-tfidf-18" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>19 0.073062025 <a title="264-tfidf-19" href="./iccv-2013-Partial_Enumeration_and_Curvature_Regularization.html">309 iccv-2013-Partial Enumeration and Curvature Regularization</a></p>
<p>20 0.06965591 <a title="264-tfidf-20" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.153), (1, -0.024), (2, -0.036), (3, 0.037), (4, -0.104), (5, 0.128), (6, -0.009), (7, 0.166), (8, 0.185), (9, 0.027), (10, 0.076), (11, 0.011), (12, -0.133), (13, 0.011), (14, -0.084), (15, -0.025), (16, 0.022), (17, 0.003), (18, -0.007), (19, 0.008), (20, -0.056), (21, 0.093), (22, -0.0), (23, -0.043), (24, -0.013), (25, -0.077), (26, -0.053), (27, -0.016), (28, 0.006), (29, -0.055), (30, -0.055), (31, -0.009), (32, 0.043), (33, -0.021), (34, -0.035), (35, -0.043), (36, -0.051), (37, -0.028), (38, 0.057), (39, -0.015), (40, 0.059), (41, 0.009), (42, 0.071), (43, -0.01), (44, -0.018), (45, -0.076), (46, 0.05), (47, -0.013), (48, 0.013), (49, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9499805 <a title="264-lsi-1" href="./iccv-2013-Minimal_Basis_Facility_Location_for_Subspace_Segmentation.html">264 iccv-2013-Minimal Basis Facility Location for Subspace Segmentation</a></p>
<p>Author: Choon-Meng Lee, Loong-Fah Cheong</p><p>Abstract: In contrast to the current motion segmentation paradigm that assumes independence between the motion subspaces, we approach the motion segmentation problem by seeking the parsimonious basis set that can represent the data. Our formulation explicitly looks for the overlap between subspaces in order to achieve a minimal basis representation. This parsimonious basis set is important for the performance of our model selection scheme because the sharing of basis results in savings of model complexity cost. We propose the use of affinity propagation based method to determine the number of motion. The key lies in the incorporation of a global cost model into the factor graph, serving the role of model complexity. The introduction of this global cost model requires additional message update in the factor graph. We derive an efficient update for the new messages associated with this global cost model. An important step in the use of affinity propagation is the subspace hypotheses generation. We use the row-sparse convex proxy solution as an initialization strategy. We further encourage the selection of subspace hypotheses with shared basis by integrat- ing a discount scheme that lowers the factor graph facility cost based on shared basis. We verified the model selection and classification performance of our proposed method on both the original Hopkins 155 dataset and the more balanced Hopkins 380 dataset.</p><p>2 0.86521465 <a title="264-lsi-2" href="./iccv-2013-Robust_Subspace_Clustering_via_Half-Quadratic_Minimization.html">360 iccv-2013-Robust Subspace Clustering via Half-Quadratic Minimization</a></p>
<p>Author: Yingya Zhang, Zhenan Sun, Ran He, Tieniu Tan</p><p>Abstract: Subspace clustering has important and wide applications in computer vision and pattern recognition. It is a challenging task to learn low-dimensional subspace structures due to the possible errors (e.g., noise and corruptions) existing in high-dimensional data. Recent subspace clustering methods usually assume a sparse representation of corrupted errors and correct the errors iteratively. However large corruptions in real-world applications can not be well addressed by these methods. A novel optimization model for robust subspace clustering is proposed in this paper. The objective function of our model mainly includes two parts. The first part aims to achieve a sparse representation of each high-dimensional data point with other data points. The second part aims to maximize the correntropy between a given data point and its low-dimensional representation with other points. Correntropy is a robust measure so that the influence of large corruptions on subspace clustering can be greatly suppressed. An extension of our method with explicit introduction of representation error terms into the model is also proposed. Half-quadratic minimization is provided as an efficient solution to the proposed robust subspace clustering formulations. Experimental results on Hopkins 155 dataset and Extended Yale Database B demonstrate that our method outperforms state-of-the-art subspace clustering methods.</p><p>3 0.85690272 <a title="264-lsi-3" href="./iccv-2013-Latent_Space_Sparse_Subspace_Clustering.html">232 iccv-2013-Latent Space Sparse Subspace Clustering</a></p>
<p>Author: Vishal M. Patel, Hien Van Nguyen, René Vidal</p><p>Abstract: We propose a novel algorithm called Latent Space Sparse Subspace Clustering for simultaneous dimensionality reduction and clustering of data lying in a union of subspaces. Specifically, we describe a method that learns the projection of data and finds the sparse coefficients in the low-dimensional latent space. Cluster labels are then assigned by applying spectral clustering to a similarity matrix built from these sparse coefficients. An efficient optimization method is proposed and its non-linear extensions based on the kernel methods are presented. One of the main advantages of our method is that it is computationally efficient as the sparse coefficients are found in the low-dimensional latent space. Various experiments show that the proposed method performs better than the competitive state-of-theart subspace clustering methods.</p><p>4 0.84872782 <a title="264-lsi-4" href="./iccv-2013-Correlation_Adaptive_Subspace_Segmentation_by_Trace_Lasso.html">93 iccv-2013-Correlation Adaptive Subspace Segmentation by Trace Lasso</a></p>
<p>Author: Canyi Lu, Jiashi Feng, Zhouchen Lin, Shuicheng Yan</p><p>Abstract: This paper studies the subspace segmentation problem. Given a set of data points drawn from a union of subspaces, the goal is to partition them into their underlying subspaces they were drawn from. The spectral clustering method is used as the framework. It requires to find an affinity matrix which is close to block diagonal, with nonzero entries corresponding to the data point pairs from the same subspace. In this work, we argue that both sparsity and the grouping effect are important for subspace segmentation. A sparse affinity matrix tends to be block diagonal, with less connections between data points from different subspaces. The grouping effect ensures that the highly corrected data which are usually from the same subspace can be grouped together. Sparse Subspace Clustering (SSC), by using ?1-minimization, encourages sparsity for data selection, but it lacks of the grouping effect. On the contrary, Low-RankRepresentation (LRR), by rank minimization, and Least Squares Regression (LSR), by ?2-regularization, exhibit strong grouping effect, but they are short in subset selection. Thus the obtained affinity matrix is usually very sparse by SSC, yet very dense by LRR and LSR. In this work, we propose the Correlation Adaptive Subspace Segmentation (CASS) method by using trace Lasso. CASS is a data correlation dependent method which simultaneously performs automatic data selection and groups correlated data together. It can be regarded as a method which adaptively balances SSC and LSR. Both theoretical and experimental results show the effectiveness of CASS.</p><p>5 0.80488998 <a title="264-lsi-5" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>Author: Zhuwen Li, Jiaming Guo, Loong-Fah Cheong, Steven Zhiying Zhou</p><p>Abstract: This paper addresses real-world challenges in the motion segmentation problem, including perspective effects, missing data, and unknown number of motions. It first formulates the 3-D motion segmentation from two perspective views as a subspace clustering problem, utilizing the epipolar constraint of an image pair. It then combines the point correspondence information across multiple image frames via a collaborative clustering step, in which tight integration is achieved via a mixed norm optimization scheme. For model selection, wepropose an over-segment and merge approach, where the merging step is based on the property of the ?1-norm ofthe mutual sparse representation oftwo oversegmented groups. The resulting algorithm can deal with incomplete trajectories and perspective effects substantially better than state-of-the-art two-frame and multi-frame methods. Experiments on a 62-clip dataset show the significant superiority of the proposed idea in both segmentation accuracy and model selection.</p><p>6 0.80341309 <a title="264-lsi-6" href="./iccv-2013-Distributed_Low-Rank_Subspace_Segmentation.html">122 iccv-2013-Distributed Low-Rank Subspace Segmentation</a></p>
<p>7 0.74069148 <a title="264-lsi-7" href="./iccv-2013-Correntropy_Induced_L2_Graph_for_Robust_Subspace_Clustering.html">94 iccv-2013-Correntropy Induced L2 Graph for Robust Subspace Clustering</a></p>
<p>8 0.66991556 <a title="264-lsi-8" href="./iccv-2013-Efficient_Higher-Order_Clustering_on_the_Grassmann_Manifold.html">134 iccv-2013-Efficient Higher-Order Clustering on the Grassmann Manifold</a></p>
<p>9 0.64255255 <a title="264-lsi-9" href="./iccv-2013-GOSUS%3A_Grassmannian_Online_Subspace_Updates_with_Structured-Sparsity.html">182 iccv-2013-GOSUS: Grassmannian Online Subspace Updates with Structured-Sparsity</a></p>
<p>10 0.59685707 <a title="264-lsi-10" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>11 0.58644283 <a title="264-lsi-11" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>12 0.58059418 <a title="264-lsi-12" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>13 0.567702 <a title="264-lsi-13" href="./iccv-2013-Progressive_Multigrid_Eigensolvers_for_Multiscale_Spectral_Segmentation.html">329 iccv-2013-Progressive Multigrid Eigensolvers for Multiscale Spectral Segmentation</a></p>
<p>14 0.51832134 <a title="264-lsi-14" href="./iccv-2013-Optimal_Orthogonal_Basis_and_Image_Assimilation%3A_Motion_Modeling.html">301 iccv-2013-Optimal Orthogonal Basis and Image Assimilation: Motion Modeling</a></p>
<p>15 0.43487757 <a title="264-lsi-15" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>16 0.40303609 <a title="264-lsi-16" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>17 0.39862317 <a title="264-lsi-17" href="./iccv-2013-Finding_Causal_Interactions_in_Video_Sequences.html">167 iccv-2013-Finding Causal Interactions in Video Sequences</a></p>
<p>18 0.37928525 <a title="264-lsi-18" href="./iccv-2013-Unsupervised_Intrinsic_Calibration_from_a_Single_Frame_Using_a_%22Plumb-Line%22_Approach.html">436 iccv-2013-Unsupervised Intrinsic Calibration from a Single Frame Using a "Plumb-Line" Approach</a></p>
<p>19 0.36907375 <a title="264-lsi-19" href="./iccv-2013-Simultaneous_Clustering_and_Tracklet_Linking_for_Multi-face_Tracking_in_Videos.html">393 iccv-2013-Simultaneous Clustering and Tracklet Linking for Multi-face Tracking in Videos</a></p>
<p>20 0.36715955 <a title="264-lsi-20" href="./iccv-2013-Learning_Coupled_Feature_Spaces_for_Cross-Modal_Matching.html">235 iccv-2013-Learning Coupled Feature Spaces for Cross-Modal Matching</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.083), (7, 0.017), (26, 0.102), (27, 0.021), (31, 0.043), (42, 0.134), (48, 0.011), (55, 0.01), (64, 0.03), (71, 0.199), (73, 0.024), (74, 0.01), (89, 0.151), (95, 0.016), (98, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82734561 <a title="264-lda-1" href="./iccv-2013-Minimal_Basis_Facility_Location_for_Subspace_Segmentation.html">264 iccv-2013-Minimal Basis Facility Location for Subspace Segmentation</a></p>
<p>Author: Choon-Meng Lee, Loong-Fah Cheong</p><p>Abstract: In contrast to the current motion segmentation paradigm that assumes independence between the motion subspaces, we approach the motion segmentation problem by seeking the parsimonious basis set that can represent the data. Our formulation explicitly looks for the overlap between subspaces in order to achieve a minimal basis representation. This parsimonious basis set is important for the performance of our model selection scheme because the sharing of basis results in savings of model complexity cost. We propose the use of affinity propagation based method to determine the number of motion. The key lies in the incorporation of a global cost model into the factor graph, serving the role of model complexity. The introduction of this global cost model requires additional message update in the factor graph. We derive an efficient update for the new messages associated with this global cost model. An important step in the use of affinity propagation is the subspace hypotheses generation. We use the row-sparse convex proxy solution as an initialization strategy. We further encourage the selection of subspace hypotheses with shared basis by integrat- ing a discount scheme that lowers the factor graph facility cost based on shared basis. We verified the model selection and classification performance of our proposed method on both the original Hopkins 155 dataset and the more balanced Hopkins 380 dataset.</p><p>2 0.81554991 <a title="264-lda-2" href="./iccv-2013-Translating_Video_Content_to_Natural_Language_Descriptions.html">428 iccv-2013-Translating Video Content to Natural Language Descriptions</a></p>
<p>Author: Marcus Rohrbach, Wei Qiu, Ivan Titov, Stefan Thater, Manfred Pinkal, Bernt Schiele</p><p>Abstract: Humans use rich natural language to describe and communicate visual perceptions. In order to provide natural language descriptions for visual content, this paper combines two important ingredients. First, we generate a rich semantic representation of the visual content including e.g. object and activity labels. To predict the semantic representation we learn a CRF to model the relationships between different components of the visual input. And second, we propose to formulate the generation of natural language as a machine translation problem using the semantic representation as source language and the generated sentences as target language. For this we exploit the power of a parallel corpus of videos and textual descriptions and adapt statistical machine translation to translate between our two languages. We evaluate our video descriptions on the TACoS dataset [23], which contains video snippets aligned with sentence descriptions. Using automatic evaluation and human judgments we show significant improvements over several baseline approaches, motivated by prior work. Our translation approach also shows improvements over related work on an image description task.</p><p>3 0.80322695 <a title="264-lda-3" href="./iccv-2013-A_Robust_Analytical_Solution_to_Isometric_Shape-from-Template_with_Focal_Length_Calibration.html">27 iccv-2013-A Robust Analytical Solution to Isometric Shape-from-Template with Focal Length Calibration</a></p>
<p>Author: Adrien Bartoli, Daniel Pizarro, Toby Collins</p><p>Abstract: We study the uncalibrated isometric Shape-fromTemplate problem, that consists in estimating an isometric deformation from a template shape to an input image whose focal length is unknown. Our method is the first that combines the following features: solving for both the 3D deformation and the camera ’s focal length, involving only local analytical solutions (there is no numerical optimization), being robust to mismatches, handling general surfaces and running extremely fast. This was achieved through two key steps. First, an ‘uncalibrated’ 3D deformation is computed thanks to a novel piecewise weak-perspective projection model. Second, the camera’s focal length is estimated and enables upgrading the 3D deformation to metric. We use a variational framework, implemented using a smooth function basis and sampled local deformation models. The only degeneracy which we easily detect– for focal length estimation is a flat and fronto-parallel surface. Experimental results on simulated and real datasets show that our method achieves a 3D shape accuracy – slightly below state of the art methods using a precalibrated or the true focal length, and a focal length accuracy slightly below static calibration methods.</p><p>4 0.77650368 <a title="264-lda-4" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>Author: Sunil Bandla, Kristen Grauman</p><p>Abstract: Collecting and annotating videos of realistic human actions is tedious, yet critical for training action recognition systems. We propose a method to actively request the most useful video annotations among a large set of unlabeled videos. Predicting the utility of annotating unlabeled video is not trivial, since any given clip may contain multiple actions of interest, and it need not be trimmed to temporal regions of interest. To deal with this problem, we propose a detection-based active learner to train action category models. We develop a voting-based framework to localize likely intervals of interest in an unlabeled clip, and use them to estimate the total reduction in uncertainty that annotating that clip would yield. On three datasets, we show our approach can learn accurate action detectors more efficiently than alternative active learning strategies that fail to accommodate the “untrimmed” nature of real video data.</p><p>5 0.77004027 <a title="264-lda-5" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>Author: Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, Philip S. Yu</p><p>Abstract: Transfer learning is established as an effective technology in computer visionfor leveraging rich labeled data in the source domain to build an accurate classifier for the target domain. However, most prior methods have not simultaneously reduced the difference in both the marginal distribution and conditional distribution between domains. In this paper, we put forward a novel transfer learning approach, referred to as Joint Distribution Adaptation (JDA). Specifically, JDA aims to jointly adapt both the marginal distribution and conditional distribution in a principled dimensionality reduction procedure, and construct new feature representation that is effective and robustfor substantial distribution difference. Extensive experiments verify that JDA can significantly outperform several state-of-the-art methods on four types of cross-domain image classification problems.</p><p>6 0.76315737 <a title="264-lda-6" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>7 0.76191854 <a title="264-lda-7" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>8 0.76108885 <a title="264-lda-8" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>9 0.76098996 <a title="264-lda-9" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>10 0.76058924 <a title="264-lda-10" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>11 0.76020944 <a title="264-lda-11" href="./iccv-2013-Learning_a_Dictionary_of_Shape_Epitomes_with_Applications_to_Image_Labeling.html">245 iccv-2013-Learning a Dictionary of Shape Epitomes with Applications to Image Labeling</a></p>
<p>12 0.75976831 <a title="264-lda-12" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>13 0.75885355 <a title="264-lda-13" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>14 0.75781637 <a title="264-lda-14" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>15 0.75726825 <a title="264-lda-15" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>16 0.75682628 <a title="264-lda-16" href="./iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation.html">383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</a></p>
<p>17 0.75677061 <a title="264-lda-17" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>18 0.75665456 <a title="264-lda-18" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>19 0.75664389 <a title="264-lda-19" href="./iccv-2013-A_Convex_Optimization_Framework_for_Active_Learning.html">6 iccv-2013-A Convex Optimization Framework for Active Learning</a></p>
<p>20 0.75622588 <a title="264-lda-20" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
