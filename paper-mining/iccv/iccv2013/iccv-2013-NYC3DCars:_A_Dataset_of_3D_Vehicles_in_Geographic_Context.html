<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>286 iccv-2013-NYC3DCars: A Dataset of 3D Vehicles in Geographic Context</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-286" href="#">iccv2013-286</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>286 iccv-2013-NYC3DCars: A Dataset of 3D Vehicles in Geographic Context</h1>
<br/><p>Source: <a title="iccv-2013-286-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Matzen_NYC3DCars_A_Dataset_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Kevin Matzen, Noah Snavely</p><p>Abstract: Geometry and geography can play an important role in recognition tasks in computer vision. To aid in studying connections between geometry and recognition, we introduce NYC3DCars, a rich dataset for vehicle detection in urban scenes built from Internet photos drawn from the wild, focused on densely trafficked areas of New York City. Our dataset is augmented with detailed geometric and geographic information, including full camera poses derived from structure from motion, 3D vehicle annotations, and geographic information from open resources, including road segmentations and directions of travel. NYC3DCars can be used to study new questions about using geometric information in detection tasks, and to explore applications of Internet photos in understanding cities. To demonstrate the utility of our data, we evaluate the use of the geographic information in our dataset to enhance a parts-based detection method, and suggest other avenues for future exploration.</p><p>Reference: <a title="iccv-2013-286-reference" href="../iccv2013_reference/iccv-2013-NYC3DCars%3A_A_Dataset_of_3D_Vehicles_in_Geographic_Context_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('geograph', 0.568), ('vehic', 0.413), ('photo', 0.281), ('sfm', 0.214), ('sidewalk', 0.157), ('road', 0.153), ('georeg', 0.149), ('nyc', 0.149), ('roadb', 0.149), ('elev', 0.13), ('viewpoint', 0.116), ('kitt', 0.116), ('polygon', 0.114), ('car', 0.083), ('annot', 0.08), ('lsvm', 0.078), ('detect', 0.077), ('dpm', 0.073), ('internet', 0.07), ('cornel', 0.065)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="286-tfidf-1" href="./iccv-2013-NYC3DCars%3A_A_Dataset_of_3D_Vehicles_in_Geographic_Context.html">286 iccv-2013-NYC3DCars: A Dataset of 3D Vehicles in Geographic Context</a></p>
<p>Author: Kevin Matzen, Noah Snavely</p><p>Abstract: Geometry and geography can play an important role in recognition tasks in computer vision. To aid in studying connections between geometry and recognition, we introduce NYC3DCars, a rich dataset for vehicle detection in urban scenes built from Internet photos drawn from the wild, focused on densely trafficked areas of New York City. Our dataset is augmented with detailed geometric and geographic information, including full camera poses derived from structure from motion, 3D vehicle annotations, and geographic information from open resources, including road segmentations and directions of travel. NYC3DCars can be used to study new questions about using geometric information in detection tasks, and to explore applications of Internet photos in understanding cities. To demonstrate the utility of our data, we evaluate the use of the geographic information in our dataset to enhance a parts-based detection method, and suggest other avenues for future exploration.</p><p>2 0.23962484 <a title="286-tfidf-2" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>Author: Raúl Díaz, Sam Hallman, Charless C. Fowlkes</p><p>Abstract: The confluence of robust algorithms for structure from motion along with high-coverage mapping and imaging of the world around us suggests that it will soon be feasible to accurately estimate camera pose for a large class photographs taken in outdoor, urban environments. In this paper, we investigate how such information can be used to improve the detection of dynamic objects such as pedestrians and cars. First, we show that when rough camera location is known, we can utilize detectors that have been trained with a scene-specific background model in order to improve detection accuracy. Second, when precise camera pose is available, dense matching to a database of existing images using multi-view stereo provides a way to eliminate static backgrounds such as building facades, akin to background-subtraction often used in video analysis. We evaluate these ideas using a dataset of tourist photos with estimated camera pose. For template-based pedestrian detection, we achieve a 50 percent boost in average precision over baseline.</p><p>3 0.22656685 <a title="286-tfidf-3" href="./iccv-2013-Event_Recognition_in_Photo_Collections_with_a_Stopwatch_HMM.html">147 iccv-2013-Event Recognition in Photo Collections with a Stopwatch HMM</a></p>
<p>Author: Lukas Bossard, Matthieu Guillaumin, Luc Van_Gool</p><p>Abstract: The task of recognizing events in photo collections is central for automatically organizing images. It is also very challenging, because of the ambiguity of photos across different event classes and because many photos do not convey enough relevant information. Unfortunately, the field still lacks standard evaluation data sets to allow comparison of different approaches. In this paper, we introduce and release a novel data set of personal photo collections containing more than 61,000 images in 807 collections, annotated with 14 diverse social event classes. Casting collections as sequential data, we build upon recent and state-of-the-art work in event recognition in videos to propose a latent sub-event approach for event recognition in photo collections. However, photos in collections are sparsely sampled over time and come in bursts from which transpires the importance of specific moments for the photographers. Thus, we adapt a discriminative hidden Markov model to allow the transitions between states to be a function of the time gap between consecutive images, which we coin as Stopwatch Hidden Markov model (SHMM). In our experiments, we show that our proposed model outperforms approaches based only on feature pooling or a classical hidden Markov model. With an average accuracy of 56%, we also highlight the difficulty of the data set and the need for future advances in event recognition in photo collections.</p><p>4 0.19256832 <a title="286-tfidf-4" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>Author: Ira Kemelmacher-Shlizerman</p><p>Abstract: In thispaper wepresent a new concept ofbuilding a morphable model directly from photos on the Internet. Morphable models have shown very impressive results more than a decade ago, and could potentially have a huge impact on all aspects of face modeling and recognition. One of the challenges, however, is to capture and register 3D laser scans of large number of people and facial expressions. Nowadays, there are enormous amounts of face photos on the Internet, large portion of which has semantic labels. We propose a framework to build a morphable model directly from photos, the framework includes dense registration of Internet photos, as well as, new single view shape reconstruction and modification algorithms.</p><p>5 0.19225948 <a title="286-tfidf-5" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>Author: Bryan Klingner, David Martin, James Roseborough</p><p>Abstract: We describe a structure-from-motion framework that handles “generalized” cameras, such as moving rollingshutter cameras, and works at an unprecedented scale— billions of images covering millions of linear kilometers of roads—by exploiting a good relative pose prior along vehicle paths. We exhibit a planet-scale, appearanceaugmented point cloud constructed with our framework and demonstrate its practical use in correcting the pose of a street-level image collection.</p><p>6 0.16017368 <a title="286-tfidf-6" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>7 0.16012305 <a title="286-tfidf-7" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>8 0.13867734 <a title="286-tfidf-8" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>9 0.1323718 <a title="286-tfidf-9" href="./iccv-2013-Network_Principles_for_SfM%3A_Disambiguating_Repeated_Structures_with_Local_Context.html">289 iccv-2013-Network Principles for SfM: Disambiguating Repeated Structures with Local Context</a></p>
<p>10 0.12928173 <a title="286-tfidf-10" href="./iccv-2013-Modeling_Occlusion_by_Discriminative_AND-OR_Structures.html">269 iccv-2013-Modeling Occlusion by Discriminative AND-OR Structures</a></p>
<p>11 0.1246511 <a title="286-tfidf-11" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>12 0.12137811 <a title="286-tfidf-12" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<p>13 0.11321922 <a title="286-tfidf-13" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>14 0.1115387 <a title="286-tfidf-14" href="./iccv-2013-Sequential_Bayesian_Model_Update_under_Structured_Scene_Prior_for_Semantic_Road_Scenes_Labeling.html">386 iccv-2013-Sequential Bayesian Model Update under Structured Scene Prior for Semantic Road Scenes Labeling</a></p>
<p>15 0.11126551 <a title="286-tfidf-15" href="./iccv-2013-Allocentric_Pose_Estimation.html">46 iccv-2013-Allocentric Pose Estimation</a></p>
<p>16 0.10105029 <a title="286-tfidf-16" href="./iccv-2013-Joint_Noise_Level_Estimation_from_Personal_Photo_Collections.html">223 iccv-2013-Joint Noise Level Estimation from Personal Photo Collections</a></p>
<p>17 0.096933708 <a title="286-tfidf-17" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>18 0.095331125 <a title="286-tfidf-18" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>19 0.090849027 <a title="286-tfidf-19" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>20 0.089184858 <a title="286-tfidf-20" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.197), (1, -0.024), (2, 0.027), (3, 0.046), (4, -0.043), (5, 0.041), (6, -0.008), (7, -0.076), (8, 0.021), (9, -0.106), (10, -0.038), (11, 0.01), (12, 0.057), (13, 0.0), (14, -0.042), (15, 0.087), (16, 0.136), (17, -0.11), (18, 0.001), (19, -0.046), (20, 0.065), (21, 0.039), (22, 0.001), (23, -0.023), (24, 0.133), (25, -0.063), (26, 0.041), (27, -0.104), (28, -0.042), (29, -0.202), (30, -0.029), (31, 0.021), (32, 0.199), (33, -0.173), (34, 0.009), (35, -0.002), (36, -0.043), (37, -0.026), (38, -0.045), (39, 0.033), (40, 0.1), (41, -0.054), (42, -0.028), (43, 0.065), (44, -0.012), (45, -0.034), (46, 0.057), (47, 0.14), (48, -0.043), (49, -0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89905715 <a title="286-lsi-1" href="./iccv-2013-NYC3DCars%3A_A_Dataset_of_3D_Vehicles_in_Geographic_Context.html">286 iccv-2013-NYC3DCars: A Dataset of 3D Vehicles in Geographic Context</a></p>
<p>Author: Kevin Matzen, Noah Snavely</p><p>Abstract: Geometry and geography can play an important role in recognition tasks in computer vision. To aid in studying connections between geometry and recognition, we introduce NYC3DCars, a rich dataset for vehicle detection in urban scenes built from Internet photos drawn from the wild, focused on densely trafficked areas of New York City. Our dataset is augmented with detailed geometric and geographic information, including full camera poses derived from structure from motion, 3D vehicle annotations, and geographic information from open resources, including road segmentations and directions of travel. NYC3DCars can be used to study new questions about using geometric information in detection tasks, and to explore applications of Internet photos in understanding cities. To demonstrate the utility of our data, we evaluate the use of the geographic information in our dataset to enhance a parts-based detection method, and suggest other avenues for future exploration.</p><p>2 0.70254862 <a title="286-lsi-2" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>Author: Raúl Díaz, Sam Hallman, Charless C. Fowlkes</p><p>Abstract: The confluence of robust algorithms for structure from motion along with high-coverage mapping and imaging of the world around us suggests that it will soon be feasible to accurately estimate camera pose for a large class photographs taken in outdoor, urban environments. In this paper, we investigate how such information can be used to improve the detection of dynamic objects such as pedestrians and cars. First, we show that when rough camera location is known, we can utilize detectors that have been trained with a scene-specific background model in order to improve detection accuracy. Second, when precise camera pose is available, dense matching to a database of existing images using multi-view stereo provides a way to eliminate static backgrounds such as building facades, akin to background-subtraction often used in video analysis. We evaluate these ideas using a dataset of tourist photos with estimated camera pose. For template-based pedestrian detection, we achieve a 50 percent boost in average precision over baseline.</p><p>3 0.69442296 <a title="286-lsi-3" href="./iccv-2013-Event_Recognition_in_Photo_Collections_with_a_Stopwatch_HMM.html">147 iccv-2013-Event Recognition in Photo Collections with a Stopwatch HMM</a></p>
<p>Author: Lukas Bossard, Matthieu Guillaumin, Luc Van_Gool</p><p>Abstract: The task of recognizing events in photo collections is central for automatically organizing images. It is also very challenging, because of the ambiguity of photos across different event classes and because many photos do not convey enough relevant information. Unfortunately, the field still lacks standard evaluation data sets to allow comparison of different approaches. In this paper, we introduce and release a novel data set of personal photo collections containing more than 61,000 images in 807 collections, annotated with 14 diverse social event classes. Casting collections as sequential data, we build upon recent and state-of-the-art work in event recognition in videos to propose a latent sub-event approach for event recognition in photo collections. However, photos in collections are sparsely sampled over time and come in bursts from which transpires the importance of specific moments for the photographers. Thus, we adapt a discriminative hidden Markov model to allow the transitions between states to be a function of the time gap between consecutive images, which we coin as Stopwatch Hidden Markov model (SHMM). In our experiments, we show that our proposed model outperforms approaches based only on feature pooling or a classical hidden Markov model. With an average accuracy of 56%, we also highlight the difficulty of the data set and the need for future advances in event recognition in photo collections.</p><p>4 0.5349797 <a title="286-lsi-4" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<p>Author: Jiyan Pan, Takeo Kanade</p><p>Abstract: Objects in a real world image cannot have arbitrary appearance, sizes and locations due to geometric constraints in 3D space. Such a 3D geometric context plays an important role in resolving visual ambiguities and achieving coherent object detection. In this paper, we develop a RANSAC-CRF framework to detect objects that are geometrically coherent in the 3D world. Different from existing methods, we propose a novel generalized RANSAC algorithm to generate global 3D geometry hypothesesfrom local entities such that outlier suppression and noise reduction is achieved simultaneously. In addition, we evaluate those hypotheses using a CRF which considers both the compatibility of individual objects under global 3D geometric context and the compatibility between adjacent objects under local 3D geometric context. Experiment results show that our approach compares favorably with the state of the art.</p><p>5 0.52981192 <a title="286-lsi-5" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>Author: Jianxiong Xiao, Andrew Owens, Antonio Torralba</p><p>Abstract: Existing scene understanding datasets contain only a limited set of views of a place, and they lack representations of complete 3D spaces. In this paper, we introduce SUN3D, a large-scale RGB-D video database with camera pose and object labels, capturing the full 3D extent of many places. The tasks that go into constructing such a dataset are difficult in isolation hand-labeling videos is painstaking, and structure from motion (SfM) is unreliable for large spaces. But if we combine them together, we make the dataset construction task much easier. First, we introduce an intuitive labeling tool that uses a partial reconstruction to propagate labels from one frame to another. Then we use the object labels to fix errors in the reconstruction. For this, we introduce a generalization of bundle adjustment that incorporates object-to-object correspondences. This algorithm works by constraining points for the same object from different frames to lie inside a fixed-size bounding box, parameterized by its rotation and translation. The SUN3D database, the source code for the generalized bundle adjustment, and the web-based 3D annotation tool are all avail– able at http://sun3d.cs.princeton.edu.</p><p>6 0.49755615 <a title="286-lsi-6" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>7 0.49459702 <a title="286-lsi-7" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>8 0.4912321 <a title="286-lsi-8" href="./iccv-2013-Detecting_Avocados_to_Zucchinis%3A_What_Have_We_Done%2C_and_Where_Are_We_Going%3F.html">109 iccv-2013-Detecting Avocados to Zucchinis: What Have We Done, and Where Are We Going?</a></p>
<p>9 0.45914438 <a title="286-lsi-9" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>10 0.45584765 <a title="286-lsi-10" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>11 0.45165858 <a title="286-lsi-11" href="./iccv-2013-Discovering_Details_and_Scene_Structure_with_Hierarchical_Iconoid_Shift.html">117 iccv-2013-Discovering Details and Scene Structure with Hierarchical Iconoid Shift</a></p>
<p>12 0.44677159 <a title="286-lsi-12" href="./iccv-2013-Holistic_Scene_Understanding_for_3D_Object_Detection_with_RGBD_Cameras.html">201 iccv-2013-Holistic Scene Understanding for 3D Object Detection with RGBD Cameras</a></p>
<p>13 0.43531832 <a title="286-lsi-13" href="./iccv-2013-Shape_Anchors_for_Data-Driven_Multi-view_Reconstruction.html">387 iccv-2013-Shape Anchors for Data-Driven Multi-view Reconstruction</a></p>
<p>14 0.43053418 <a title="286-lsi-14" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>15 0.43004289 <a title="286-lsi-15" href="./iccv-2013-Parsing_IKEA_Objects%3A_Fine_Pose_Estimation.html">308 iccv-2013-Parsing IKEA Objects: Fine Pose Estimation</a></p>
<p>16 0.4268733 <a title="286-lsi-16" href="./iccv-2013-Allocentric_Pose_Estimation.html">46 iccv-2013-Allocentric Pose Estimation</a></p>
<p>17 0.42413059 <a title="286-lsi-17" href="./iccv-2013-An_Enhanced_Structure-from-Motion_Paradigm_Based_on_the_Absolute_Dual_Quadric_and_Images_of_Circular_Points.html">49 iccv-2013-An Enhanced Structure-from-Motion Paradigm Based on the Absolute Dual Quadric and Images of Circular Points</a></p>
<p>18 0.42249885 <a title="286-lsi-18" href="./iccv-2013-CoDeL%3A_A_Human_Co-detection_and_Labeling_Framework.html">75 iccv-2013-CoDeL: A Human Co-detection and Labeling Framework</a></p>
<p>19 0.41546065 <a title="286-lsi-19" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>20 0.41421932 <a title="286-lsi-20" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(12, 0.015), (13, 0.053), (20, 0.079), (25, 0.11), (29, 0.193), (42, 0.108), (48, 0.201), (60, 0.026), (77, 0.093), (89, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83340538 <a title="286-lda-1" href="./iccv-2013-NYC3DCars%3A_A_Dataset_of_3D_Vehicles_in_Geographic_Context.html">286 iccv-2013-NYC3DCars: A Dataset of 3D Vehicles in Geographic Context</a></p>
<p>Author: Kevin Matzen, Noah Snavely</p><p>Abstract: Geometry and geography can play an important role in recognition tasks in computer vision. To aid in studying connections between geometry and recognition, we introduce NYC3DCars, a rich dataset for vehicle detection in urban scenes built from Internet photos drawn from the wild, focused on densely trafficked areas of New York City. Our dataset is augmented with detailed geometric and geographic information, including full camera poses derived from structure from motion, 3D vehicle annotations, and geographic information from open resources, including road segmentations and directions of travel. NYC3DCars can be used to study new questions about using geometric information in detection tasks, and to explore applications of Internet photos in understanding cities. To demonstrate the utility of our data, we evaluate the use of the geographic information in our dataset to enhance a parts-based detection method, and suggest other avenues for future exploration.</p><p>2 0.81484056 <a title="286-lda-2" href="./iccv-2013-Elastic_Net_Constraints_for_Shape_Matching.html">140 iccv-2013-Elastic Net Constraints for Shape Matching</a></p>
<p>Author: Emanuele Rodolà, Andrea Torsello, Tatsuya Harada, Yasuo Kuniyoshi, Daniel Cremers</p><p>Abstract: We consider a parametrized relaxation of the widely adopted quadratic assignment problem (QAP) formulation for minimum distortion correspondence between deformable shapes. In order to control the accuracy/sparsity trade-off we introduce a weighting parameter on the combination of two existing relaxations, namely spectral and game-theoretic. This leads to the introduction of the elastic net penalty function into shape matching problems. In combination with an efficient algorithm to project onto the elastic net ball, we obtain an approach for deformable shape matching with controllable sparsity. Experiments on a standard benchmark confirm the effectiveness of the approach.</p><p>3 0.81419379 <a title="286-lda-3" href="./iccv-2013-Shape_Index_Descriptors_Applied_to_Texture-Based_Galaxy_Analysis.html">388 iccv-2013-Shape Index Descriptors Applied to Texture-Based Galaxy Analysis</a></p>
<p>Author: Kim Steenstrup Pedersen, Kristoffer Stensbo-Smidt, Andrew Zirm, Christian Igel</p><p>Abstract: A texture descriptor based on the shape index and the accompanying curvedness measure is proposed, and it is evaluated for the automated analysis of astronomical image data. A representative sample of images of low-redshift galaxies from the Sloan Digital Sky Survey (SDSS) serves as a testbed. The goal of applying texture descriptors to these data is to extract novel information about galaxies; information which is often lost in more traditional analysis. In this study, we build a regression model for predicting a spectroscopic quantity, the specific star-formation rate (sSFR). As texture features we consider multi-scale gradient orientation histograms as well as multi-scale shape index histograms, which lead to a new descriptor. Our results show that we can successfully predict spectroscopic quantities from the texture in optical multi-band images. We successfully recover the observed bi-modal distribution of galaxies into quiescent and star-forming. The state-ofthe-art for predicting the sSFR is a color-based physical model. We significantly improve its accuracy by augmenting the model with texture information. This study is thefirst step towards enabling the quantification of physical galaxy properties from imaging data alone.</p><p>4 0.81332803 <a title="286-lda-4" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>Author: Jianxiong Xiao, Andrew Owens, Antonio Torralba</p><p>Abstract: Existing scene understanding datasets contain only a limited set of views of a place, and they lack representations of complete 3D spaces. In this paper, we introduce SUN3D, a large-scale RGB-D video database with camera pose and object labels, capturing the full 3D extent of many places. The tasks that go into constructing such a dataset are difficult in isolation hand-labeling videos is painstaking, and structure from motion (SfM) is unreliable for large spaces. But if we combine them together, we make the dataset construction task much easier. First, we introduce an intuitive labeling tool that uses a partial reconstruction to propagate labels from one frame to another. Then we use the object labels to fix errors in the reconstruction. For this, we introduce a generalization of bundle adjustment that incorporates object-to-object correspondences. This algorithm works by constraining points for the same object from different frames to lie inside a fixed-size bounding box, parameterized by its rotation and translation. The SUN3D database, the source code for the generalized bundle adjustment, and the web-based 3D annotation tool are all avail– able at http://sun3d.cs.princeton.edu.</p><p>5 0.81274807 <a title="286-lda-5" href="./iccv-2013-Discovering_Object_Functionality.html">118 iccv-2013-Discovering Object Functionality</a></p>
<p>Author: Bangpeng Yao, Jiayuan Ma, Li Fei-Fei</p><p>Abstract: Object functionality refers to the quality of an object that allows humans to perform some specific actions. It has been shown in psychology that functionality (affordance) is at least as essential as appearance in object recognition by humans. In computer vision, most previous work on functionality either assumes exactly one functionality for each object, or requires detailed annotation of human poses and objects. In this paper, we propose a weakly supervised approach to discover all possible object functionalities. Each object functionality is represented by a specific type of human-object interaction. Our method takes any possible human-object interaction into consideration, and evaluates image similarity in 3D rather than 2D in order to cluster human-object interactions more coherently. Experimental results on a dataset of people interacting with musical instruments show the effectiveness of our approach.</p><p>6 0.79874498 <a title="286-lda-6" href="./iccv-2013-Correntropy_Induced_L2_Graph_for_Robust_Subspace_Clustering.html">94 iccv-2013-Correntropy Induced L2 Graph for Robust Subspace Clustering</a></p>
<p>7 0.79501092 <a title="286-lda-7" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>8 0.79227006 <a title="286-lda-8" href="./iccv-2013-Dictionary_Learning_and_Sparse_Coding_on_Grassmann_Manifolds%3A_An_Extrinsic_Solution.html">114 iccv-2013-Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution</a></p>
<p>9 0.79138464 <a title="286-lda-9" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>10 0.78892285 <a title="286-lda-10" href="./iccv-2013-Shortest_Paths_with_Curvature_and_Torsion.html">389 iccv-2013-Shortest Paths with Curvature and Torsion</a></p>
<p>11 0.78593683 <a title="286-lda-11" href="./iccv-2013-Rectangling_Stereographic_Projection_for_Wide-Angle_Image_Visualization.html">346 iccv-2013-Rectangling Stereographic Projection for Wide-Angle Image Visualization</a></p>
<p>12 0.78442991 <a title="286-lda-12" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>13 0.78353548 <a title="286-lda-13" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>14 0.78233081 <a title="286-lda-14" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>15 0.7822305 <a title="286-lda-15" href="./iccv-2013-Curvature-Aware_Regularization_on_Riemannian_Submanifolds.html">100 iccv-2013-Curvature-Aware Regularization on Riemannian Submanifolds</a></p>
<p>16 0.7820673 <a title="286-lda-16" href="./iccv-2013-A_Fully_Hierarchical_Approach_for_Finding_Correspondences_in_Non-rigid_Shapes.html">11 iccv-2013-A Fully Hierarchical Approach for Finding Correspondences in Non-rigid Shapes</a></p>
<p>17 0.7811442 <a title="286-lda-17" href="./iccv-2013-Shape_Anchors_for_Data-Driven_Multi-view_Reconstruction.html">387 iccv-2013-Shape Anchors for Data-Driven Multi-view Reconstruction</a></p>
<p>18 0.77886999 <a title="286-lda-18" href="./iccv-2013-Parallel_Transport_of_Deformations_in_Shape_Space_of_Elastic_Surfaces.html">307 iccv-2013-Parallel Transport of Deformations in Shape Space of Elastic Surfaces</a></p>
<p>19 0.77765065 <a title="286-lda-19" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>20 0.77739072 <a title="286-lda-20" href="./iccv-2013-Network_Principles_for_SfM%3A_Disambiguating_Repeated_Structures_with_Local_Context.html">289 iccv-2013-Network Principles for SfM: Disambiguating Repeated Structures with Local Context</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
