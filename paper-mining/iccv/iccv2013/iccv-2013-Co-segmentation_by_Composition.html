<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>74 iccv-2013-Co-segmentation by Composition</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-74" href="#">iccv2013-74</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>74 iccv-2013-Co-segmentation by Composition</h1>
<br/><p>Source: <a title="iccv-2013-74-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Faktor_Co-segmentation_by_Composition_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Alon Faktor, Michal Irani</p><p>Abstract: Given a set of images which share an object from the same semantic category, we would like to co-segment the shared object. We define ‘good’ co-segments to be ones which can be easily composed (like a puzzle) from large pieces of other co-segments, yet are difficult to compose from remaining image parts. These pieces must not only match well but also be statistically significant (hard to compose at random). This gives rise to co-segmentation of objects in very challenging scenarios with large variations in appearance, shape and large amounts of clutter. We further show how multiple images can collaborate and “score each others ’ co-segments to improve the overall fidelity and accuracy of the co-segmentation. Our co-segmentation can be applied both to large image collections, as well as to very few images (where there is too little data for unsupervised learning). At the extreme, it can be applied even to a single image, to extract its co-occurring objects. Our approach obtains state-of-the-art results on benchmark datasets. We further show very encouraging co-segmentation results on the challenging PASCAL-VOC dataset. ”</p><p>Reference: <a title="iccv-2013-74-reference" href="../iccv2013_reference/iccv-2013-Co-segmentation_by_Composition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 of Computer Science and Applied Math The Weizmann Institute of Science,ISRAEL  Abstract Given a set of images which share an object from the same semantic category, we would like to co-segment the shared object. [sent-2, score-0.227]
</p><p>2 We define ‘good’ co-segments to be ones which can be easily composed (like a puzzle) from large pieces of other co-segments, yet are difficult to compose from remaining image parts. [sent-3, score-0.266]
</p><p>3 These pieces must not only match well but also be statistically significant (hard to compose at random). [sent-4, score-0.31]
</p><p>4 We further show how multiple images can collaborate and “score each others ’ co-segments to improve the overall fidelity and accuracy of the co-segmentation. [sent-6, score-0.175]
</p><p>5 Existing work in this field has typically assumed a simple model common to the coobjects such as common color [16, 13] or common distribution of descriptors [19]. [sent-16, score-0.164]
</p><p>6 1 (bike riders, ballet dancers, cats), we can see that there seems to be no simple model common to the objects. [sent-23, score-0.215]
</p><p>7 Moreover, the objects may not be salient in their image and may be surrounded by large amounts of distracting clutter. [sent-25, score-0.194]
</p><p>8 Instead, our approach is based on the framework developed in [9, 5], which show that when non-trivial (rare) image parts re-occur in another image, they induce statistically meaningful affinities between these images. [sent-28, score-0.561]
</p><p>9 However, unlike [9] which employs this idea to induce affinities between entire images (for the purpose of image clustering), we employ their approach to induce 1297  Figure 2. [sent-29, score-0.52]
</p><p>10 (b) Co-occurring  regions induce affinities between image parts  across images. [sent-32, score-0.591]
</p><p>11 affinities between parts of images, thus initializing our cosegmentation process. [sent-35, score-0.58]
</p><p>12 That is, a co-segment should share large non-trivial (statistically significant) regions with other co-segments. [sent-37, score-0.195]
</p><p>13 Initialize the co-segmentation by inducing affinities between image parts - Large shared regions are detected across images, inducing affinities between those image  parts (see Fig. [sent-40, score-1.335]
</p><p>14 The larger and more rare those regions are, the higher their induced affinity. [sent-42, score-0.329]
</p><p>15 The shared regions provide a rough localization of the co-objcets in the images. [sent-43, score-0.315]
</p><p>16 The region detection is done efficiently using a randomized search and propagation algorithm, suggested by [9]. [sent-44, score-0.242]
</p><p>17 From co-occurring regions to co-segments - The detected shared regions are usually not good image segments on their own. [sent-48, score-0.724]
</p><p>18 However, they induce statistically significant affinities between parts of co-objects. [sent-50, score-0.521]
</p><p>19 We use these affinities to score multiple overlapping segment candidates (“soup of segments” see Fig. [sent-51, score-0.334]
</p><p>20 A segment which is highly overlapped by many shared regions gets a high score. [sent-53, score-0.394]
</p><p>21 The segments and their scores are then used to estimate co-segmentation likelihood maps (See Fig. [sent-54, score-0.358]
</p><p>22 Improving co-segmentation by “consensus scoring” We improve the the fidelity and accuracy of the cosegmentation by propagating the co-segmentation likelihood maps between the different images. [sent-60, score-0.457]
</p><p>23 This propagation is done using the mapping between the co-occurring (shared) regions across the different images. [sent-61, score-0.196]
</p><p>24 The cosegmentation score is determined using the consensus between each region and its co-occurring regions in other images. [sent-62, score-0.582]
</p><p>25 This leads to improved co-segmentation likelihood maps (see Fig. [sent-63, score-0.213]
</p><p>26 However, their regions are image segments which are extracted from each image separately ahead of time and then matched. [sent-78, score-0.394]
</p><p>27 In contrast, our shared regions are usually not good image segments that can be extracted ahead of time. [sent-79, score-0.54]
</p><p>28 What makes them “good” image regions is the fact that (i) they are rare (have low chance of occurring at random), yet (ii) they cooccur (are shared) by two images. [sent-80, score-0.323]
</p><p>29 When such a rare region co-occurs, it is unlikely to be accidental, thus inducing high meaningful affinity between those image parts. [sent-81, score-0.519]
</p><p>30 Recently, [17] suggested to combine visual saliency and dense pixel correspondences across images for the purpose of co-segmentation. [sent-82, score-0.163]
</p><p>31 However, we use the statistical significance of the shared regions to initialize the co-segmentation and not visual saliency like [17] does. [sent-84, score-0.384]
</p><p>32 [14] suggested to incorporate into co-segmentation generic knowledge transfer from datasets with humanannotated segmentations of objects. [sent-86, score-0.159]
</p><p>33 Inducing Affinities between Image Parts Our framework for inducing affinities between image parts is based on [5] and [9]. [sent-92, score-0.475]
</p><p>34 These regions must both match well, as well as be statistically significant (hard to compose at random). [sent-97, score-0.405]
</p><p>35 The co-occurring regions induce affinities between image parts across different images. [sent-98, score-0.591]
</p><p>36 If a region matches well, but is trivial, then its likelihood ratio will be low (inducing a low affinity). [sent-100, score-0.252]
</p><p>37 On the other hand, if a region is non-trivial, yet has a good match in another image, its likelihood ratio will be high (inducing a high affinity). [sent-101, score-0.378]
</p><p>38 e, the l2 distance between di and its corresponding descriptor in its region match in the other image I2). [sent-116, score-0.325]
</p><p>39 Approximate the random process H0 by generating a descriptor codebook (with a few hundred codewords). [sent-118, score-0.177]
</p><p>40 This codebook is generated by applying k-means clustering to all of the descriptors extracted from the image collection. [sent-119, score-0.189]
</p><p>41 Thus, the likelihood of each descriptor di in the region R ⊂ I1 to be generated at random (using Hi0n) i tsh eap rpergoixoinma Rted ⊂ ⊂by I: p(di|I1, I2) = exp  ? [sent-122, score-0.435]
</p><p>42 e, the l2 distance between di and its nearest neighbor descriptor in the codebook . [sent-127, score-0.269]
</p><p>43 ∈R  Namely, the affinity induced by a co-occurring region is equal to the difference between the total descriptor error with respect to a codebook and the total matching error between the matched regions in the two images. [sent-131, score-0.595]
</p><p>44 A high affinity will be obtained for image parts which are both rare (high codebook errors) and match well across images (low matching errors). [sent-132, score-0.46]
</p><p>45 These image parts tend to coincide with unique and informative parts of the co-occurring objects, yielding a good seed to the co-segments. [sent-133, score-0.157]
</p><p>46 However, Ballet dancer #1 can compose its arm gesture (red region) from Ballet dancer #2 and most of its leg gesture (yellow region) from Ballet dancer #3. [sent-136, score-0.472]
</p><p>47 Note that these regions are complex, thus have a low chance of appearing at random. [sent-137, score-0.161]
</p><p>48 Therefore, the fact that these regions found good matches in other images  can not be accidental, providing high evidence to the high affinity between those regions. [sent-138, score-0.343]
</p><p>49 Detecting Co-occurring Regions between Images Detecting large non-trivial co-occurring regions between images is in principle a very hard problem (already between a pair of images, let alone in a large image collection). [sent-141, score-0.2]
</p><p>50 Moreover, the regions may be of arbitrary size and shape. [sent-142, score-0.161]
</p><p>51 Therefore, [9] suggested a randomized search algorithm which guarantees with very highprobability the efficient detection of large shared regions. [sent-143, score-0.266]
</p><p>52 Each descriptor in each image, randomly samples several descriptors in another image and chooses the one with the best match. [sent-148, score-0.194]
</p><p>53 The neighboring descriptors will change their current match only if the new suggested match is better. [sent-150, score-0.252]
</p><p>54 Therefore, it is enough for one descriptor in a recurring region to find its correct matching descriptor in another image, and it can then propagate the correct matches to all the other descriptors in that cooccurring region. [sent-151, score-0.499]
</p><p>55 Very few words (k ∼ 100) suffice to represent well frequent descriptors (Vsmeroyo ftehw patches, kve ∼rtic 1a0l/0h)or siuzffoincteal t edges, eetnct. [sent-153, score-0.2]
</p><p>56 lDl furee qtou nthte ehesacvriypt-toarils distribution of natural image descriptors, adding more words would only refine the frequent descriptor representatives, and not add the rare ones [6]. [sent-155, score-0.306]
</p><p>57 For example, using 40 random samples per descriptor guarantees the detection of recurring regions of at least 10% of the image size, with very high probability above 98%. [sent-160, score-0.317]
</p><p>58 Therefore, large co-occurring regions between two images can be detected at linear time. [sent-161, score-0.27]
</p><p>59 If shared regions are searched between every pair of images, then the complexity will grow quadratically with the number of images, making it prohibitive for large image collections. [sent-162, score-0.315]
</p><p>60 This induces a guided random walk with high probability of finding the large shared regions between the images in the collection, at linear time. [sent-165, score-0.354]
</p><p>61 The region detection algorithm is applied to the entire multi-scale collec-  tion of images, allowing shared regions to be detected also between co-objects of different scales. [sent-173, score-0.48]
</p><p>62 From Co-occurring Regions to Co-segments The detected non-trivial co-occurring regions induce meaningful affinities between image parts across different images. [sent-175, score-0.701]
</p><p>63 We use the regions and their affinities to seed the co-segments and estimate for each pixel its ‘co-segment likelihood’ . [sent-178, score-0.416]
</p><p>64 Initializing the Co-segments Although the detected shared regions do not form ‘good’ segments on their own, they provide a rough estimation of the location of the co-objects within the image. [sent-181, score-0.53]
</p><p>65 Compute the co-segment score for each segment Sl by its “affinity density”, induced by the shared regions:  Score(Sl) =|S1l|? [sent-192, score-0.285]
</p><p>66 mAff(Rm|I,Iχ(m))  (5)  where {Rm} are shared regions detected between image I and other images, with high intersection with segment Sl (at least 75% intersection). [sent-193, score-0.464]
</p><p>67 Summing the contributions of all of these regions and normalizing by the segment size |Sl | results in the “affinity density” of the segsmegenmt. [sent-202, score-0.24]
</p><p>68 Normalize the co-segmentation likelihood map of the entire image to be in the range between 0 to 1. [sent-209, score-0.157]
</p><p>69 1, we have shown how to estimate cosegmentation likelihood maps, induced by detecting statistically significant co-occurring regions for each image in the collection, combined with information about segment boundaries extracted from a “soup of segments”. [sent-214, score-0.74]
</p><p>70 We next show how images can collaborate and share information with each other regrading their co-segmentation likelihood maps to improve the overall quality of the co-segmentation. [sent-215, score-0.378]
</p><p>71 The co-segmentation score is determined using the consensus between each region and all its detected co-  occurring regions in other images (according to their cosegmentation likelihood maps). [sent-217, score-0.848]
</p><p>72 be corresponding pixels to p in all other images induced by the detected shared regions. [sent-230, score-0.315]
</p><p>73 Then we update the co-segmentation likelihood of each pixel CSL(p) at iteration (t + 1) as follows:  logCSL(t+1)(p) = 21 ·M1 ·? [sent-231, score-0.157]
</p><p>74 =1  (7)  We initialize the co-segmentation likelihood of each image (at t = 0) using the estimation made in Sec. [sent-237, score-0.157]
</p><p>75 By performing several such scoring phases, we allow regions which are not directly connected to each other to also collaborate and ‘share’ information regarding the cosegmentation likelihood. [sent-240, score-0.536]
</p><p>76 Examples of the estimated co-segmentation likelihood before and after performing the re-scoring iterations can be found in Fig. [sent-241, score-0.157]
</p><p>77 Note that in the initial co-segmentation likelihood maps there may still remain clutter with high values. [sent-243, score-0.258]
</p><p>78 , binary co-segmentation maps), we use Grab-cut [15], where the unary terms (background/foreground likelihood) are initial2Recall that when shared regions are detected, each pixel in one region is mapped to a pixel in the other region. [sent-249, score-0.41]
</p><p>79 This includes linearity of our co-occurring region detection algorithm among all images (Sec. [sent-256, score-0.171]
</p><p>80 Handling large variability in appearance between the co-segments usually requires a large number of images, in order to “discover” shared properties of the co-objects (e. [sent-266, score-0.187]
</p><p>81 When a complex region recurs in the image, and is unlikely to recur at random (i. [sent-272, score-0.199]
</p><p>82 The co-occurring regions are detected by applying the randomized search and propagation algorithm internally on the image itself. [sent-276, score-0.323]
</p><p>83 To prevent a trivial composition of a region from itself, we restrict each descriptor to sample descriptors only outside the immediate neighborhood around the descriptor (typically of radius 116 of the image size). [sent-277, score-0.459]
</p><p>84 However, here we use “consensus” ofco-occurring regions within the same image and not across different images as before. [sent-280, score-0.2]
</p><p>85 Ineachbox,  we show co-segmentation results for a few images from a certain class (all the images in the class were used for the co-segmentation). [sent-283, score-0.158]
</p><p>86 with scale difference of the co-segments, we search for cooccurring regions across different scales of the same image. [sent-284, score-0.215]
</p><p>87 One co-object can be composed using regions extracted from several other cosegments (possibly at different image scales), thus gener-  ating a new configuration. [sent-290, score-0.27]
</p><p>88 In a way, this is very similar to the definition of [2], which defines a “good image segment” as one which is easy to compose (like a puzzle) from other regions of the segment, yet is hard to compose from the rest of the image outside the segment. [sent-291, score-0.419]
</p><p>89 Applying Grab-cut, using an initialization with a central window of size 25% of the image, fails to produce meaningful cosegmentations on such images (see Fig. [sent-295, score-0.181]
</p><p>90 Similarly, saliency based segmentation will not suffice either, since the co-occuring object is not necessarily salient in the image, and there can be other salient image parts (e. [sent-297, score-0.368]
</p><p>91 We, on the other hand, are able to produce good cosegmentations of these images by employing the reoccurrence of large non-trivial regions within each image. [sent-301, score-0.294]
</p><p>92 For the iCoseg dataset we added color descriptors in addition to the HOG descriptors (we used densely sampled descriptors, which are concatenation of HOG and LAB color histograms). [sent-353, score-0.206]
</p><p>93 We built a descriptor dictionary for each class separately and used it to compute the error of each descriptor with respect to the dictionary, which is required in the affinity calculation (Eq(4)). [sent-355, score-0.373]
</p><p>94 In the iCoseg dataset, in order to obtain the final binary co-segments from our continuous likelihood maps, we fol-  lowed the ‘Joint-Grab-Cut’ suggestion of [14]. [sent-359, score-0.157]
</p><p>95 We initialize the ‘Joint-GrabCut’ with our continuous co-segmentation likelihood maps. [sent-362, score-0.157]
</p><p>96 Moreover, initializing the co-segmentation using saliency maps  will also be problematic, since the co-segments are not necessarily salient in the image, as there are many other distracting objects in the image. [sent-372, score-0.344]
</p><p>97 We split the classes into two subsets - the first consists of animal and vehicle classes (total of 13 classes) and the second consists of the remaining classes such as person, table and potted plant (total of 7 classes). [sent-376, score-0.165]
</p><p>98 The reason for this large gap between the Precision and Jaccard index measures is that Precision gives equal contribution to foreground and background, whereas the Jaccard index considers only the foreground. [sent-388, score-0.211]
</p><p>99 Notice the large amount of clutter and distracting objects which exist in those images yet our algorithm yields very good results. [sent-391, score-0.253]
</p><p>100 We define ‘good’ co-segments to be ones which can be easily composed from large pieces of other co-segments, yet are difficult to compose from the remaining image parts. [sent-414, score-0.266]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('jaccard', 0.301), ('icoseg', 0.271), ('affinities', 0.255), ('cosegmentation', 0.2), ('ballet', 0.183), ('msrc', 0.176), ('regions', 0.161), ('inducing', 0.158), ('likelihood', 0.157), ('shared', 0.154), ('soup', 0.153), ('segments', 0.145), ('consensus', 0.126), ('dancer', 0.122), ('rare', 0.116), ('induce', 0.113), ('affinity', 0.11), ('compose', 0.106), ('pascal', 0.103), ('descriptors', 0.103), ('region', 0.095), ('di', 0.092), ('collaborate', 0.092), ('descriptor', 0.091), ('statistically', 0.091), ('distracting', 0.09), ('index', 0.086), ('codebook', 0.086), ('scoring', 0.083), ('aff', 0.081), ('puzzle', 0.081), ('composition', 0.079), ('segment', 0.079), ('csl', 0.075), ('detected', 0.07), ('saliency', 0.069), ('salient', 0.066), ('pieces', 0.066), ('sl', 0.065), ('recurring', 0.065), ('boiman', 0.065), ('initializing', 0.063), ('parts', 0.062), ('coobjects', 0.061), ('cosegmentations', 0.061), ('cosegments', 0.061), ('humanannotated', 0.061), ('logcsl', 0.061), ('nthte', 0.061), ('riders', 0.061), ('unsupervised', 0.06), ('suffice', 0.059), ('randomized', 0.057), ('fig', 0.057), ('maps', 0.056), ('classes', 0.055), ('suggested', 0.055), ('cooccurring', 0.054), ('faktor', 0.054), ('israeli', 0.054), ('recurs', 0.054), ('induced', 0.052), ('recur', 0.05), ('composed', 0.048), ('match', 0.047), ('ahead', 0.047), ('segmentation', 0.046), ('rother', 0.046), ('yet', 0.046), ('clutter', 0.045), ('accidental', 0.045), ('fidelity', 0.044), ('segmentations', 0.043), ('precision', 0.042), ('central', 0.041), ('external', 0.041), ('separately', 0.041), ('segmented', 0.04), ('class', 0.04), ('hog', 0.04), ('meaningful', 0.04), ('foreground', 0.039), ('codeword', 0.039), ('patchmatch', 0.039), ('images', 0.039), ('moreover', 0.039), ('rm', 0.038), ('frequent', 0.038), ('surrounded', 0.038), ('linearity', 0.037), ('joulin', 0.037), ('extreme', 0.036), ('ideas', 0.035), ('propagation', 0.035), ('collection', 0.034), ('share', 0.034), ('appearance', 0.033), ('obtains', 0.033), ('good', 0.033), ('seems', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="74-tfidf-1" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>Author: Alon Faktor, Michal Irani</p><p>Abstract: Given a set of images which share an object from the same semantic category, we would like to co-segment the shared object. We define ‘good’ co-segments to be ones which can be easily composed (like a puzzle) from large pieces of other co-segments, yet are difficult to compose from remaining image parts. These pieces must not only match well but also be statistically significant (hard to compose at random). This gives rise to co-segmentation of objects in very challenging scenarios with large variations in appearance, shape and large amounts of clutter. We further show how multiple images can collaborate and “score each others ’ co-segments to improve the overall fidelity and accuracy of the co-segmentation. Our co-segmentation can be applied both to large image collections, as well as to very few images (where there is too little data for unsupervised learning). At the extreme, it can be applied even to a single image, to extract its co-occurring objects. Our approach obtains state-of-the-art results on benchmark datasets. We further show very encouraging co-segmentation results on the challenging PASCAL-VOC dataset. ”</p><p>2 0.23034121 <a title="74-tfidf-2" href="./iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation.html">383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</a></p>
<p>Author: Zhengxiang Wang, Rujie Liu</p><p>Abstract: This paper introduces to use semi-supervised learning for large scale image cosegmentation. Different from traditional unsupervised cosegmentation that does not use any segmentation groundtruth, semi-supervised cosegmentation exploits the similarity from both the very limited training image foregrounds, as well as the common object shared between the large number of unsegmented images. This would be a much practical way to effectively cosegment a large number of related images simultaneously, where previous unsupervised cosegmentation work poorly due to the large variances in appearance between different images and the lack ofsegmentation groundtruthfor guidance in cosegmentation. For semi-supervised cosegmentation in large scale, we propose an effective method by minimizing an energy function, which consists of the inter-image distance, the intraimage distance and the balance term. We also propose an iterative updating algorithm to efficiently solve this energy function, which decomposes the original energy minimization problem into sub-problems, and updates each image alternatively to reduce the number of variables in each subproblem for computation efficiency. Experiment results on iCoseg and Pascal VOC datasets show that the proposed cosegmentation method can effectively cosegment hundreds of images in less than one minute. And our semi-supervised cosegmentation is able to outperform both unsupervised cosegmentation as well asfully supervised single image segmentation, especially when the training data is limited.</p><p>3 0.16831523 <a title="74-tfidf-3" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<p>Author: Fan Wang, Qixing Huang, Leonidas J. Guibas</p><p>Abstract: Joint segmentation of image sets has great importance for object recognition, image classification, and image retrieval. In this paper, we aim to jointly segment a set of images starting from a small number of labeled images or none at all. To allow the images to share segmentation information with each other, we build a network that contains segmented as well as unsegmented images, and extract functional maps between connected image pairs based on image appearance features. These functional maps act as general property transporters between the images and, in particular, are used to transfer segmentations. We define and operate in a reduced functional space optimized so that the functional maps approximately satisfy cycle-consistency under composition in the network. A joint optimization framework is proposed to simultaneously generate all segmentation functions over the images so that they both align with local segmentation cues in each particular image, and agree with each other under network transportation. This formulation allows us to extract segmentations even with no training data, but can also exploit such data when available. The collective effect of the joint processing using functional maps leads to accurate information sharing among images and yields superior segmentation results, as shown on the iCoseg, MSRC, and PASCAL data sets.</p><p>4 0.16662982 <a title="74-tfidf-4" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>Author: Jifeng Dai, Ying Nian Wu, Jie Zhou, Song-Chun Zhu</p><p>Abstract: Cosegmentation refers to theproblem ofsegmenting multiple images simultaneously by exploiting the similarities between the foreground and background regions in these images. The key issue in cosegmentation is to align common objects between these images. To address this issue, we propose an unsupervised learning framework for cosegmentation, by coupling cosegmentation with what we call “cosketch ”. The goal of cosketch is to automatically discover a codebook of deformable shape templates shared by the input images. These shape templates capture distinct image patterns and each template is matched to similar image patches in different images. Thus the cosketch of the images helps to align foreground objects, thereby providing crucial information for cosegmentation. We present a statistical model whose energy function couples cosketch and cosegmentation. We then present an unsupervised learning algorithm that performs cosketch and cosegmentation by energy minimization. Experiments show that our method outperforms state of the art methods for cosegmentation on the challenging MSRC and iCoseg datasets. We also illustrate our method on a new dataset called Coseg-Rep where cosegmentation can be performed within a single image with repetitive patterns.</p><p>5 0.14024043 <a title="74-tfidf-5" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>Author: Jian Sun, Jean Ponce</p><p>Abstract: In this paper, we address the problem of learning discriminative part detectors from image sets with category labels. We propose a novel latent SVM model regularized by group sparsity to learn these part detectors. Starting from a large set of initial parts, the group sparsity regularizer forces the model to jointly select and optimize a set of discriminative part detectors in a max-margin framework. We propose a stochastic version of a proximal algorithm to solve the corresponding optimization problem. We apply the proposed method to image classification and cosegmentation, and quantitative experiments with standard benchmarks show that it matches or improves upon the state of the art.</p><p>6 0.13024148 <a title="74-tfidf-6" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>7 0.12808712 <a title="74-tfidf-7" href="./iccv-2013-Saliency_Detection_via_Dense_and_Sparse_Reconstruction.html">372 iccv-2013-Saliency Detection via Dense and Sparse Reconstruction</a></p>
<p>8 0.12641621 <a title="74-tfidf-8" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>9 0.1205373 <a title="74-tfidf-9" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>10 0.11386945 <a title="74-tfidf-10" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>11 0.11217332 <a title="74-tfidf-11" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>12 0.10284156 <a title="74-tfidf-12" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>13 0.10280171 <a title="74-tfidf-13" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<p>14 0.10047986 <a title="74-tfidf-14" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>15 0.093061671 <a title="74-tfidf-15" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>16 0.092986889 <a title="74-tfidf-16" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>17 0.085445158 <a title="74-tfidf-17" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>18 0.082294829 <a title="74-tfidf-18" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>19 0.082105443 <a title="74-tfidf-19" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>20 0.081808709 <a title="74-tfidf-20" href="./iccv-2013-Quantize_and_Conquer%3A_A_Dimensionality-Recursive_Solution_to_Clustering%2C_Vector_Quantization%2C_and_Image_Retrieval.html">333 iccv-2013-Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.223), (1, 0.001), (2, 0.119), (3, -0.054), (4, 0.025), (5, 0.031), (6, -0.069), (7, 0.047), (8, -0.015), (9, -0.072), (10, 0.056), (11, 0.094), (12, 0.013), (13, -0.058), (14, -0.05), (15, -0.023), (16, 0.021), (17, -0.027), (18, 0.0), (19, -0.062), (20, 0.095), (21, -0.027), (22, -0.073), (23, 0.002), (24, 0.009), (25, 0.044), (26, -0.017), (27, 0.043), (28, -0.041), (29, 0.048), (30, 0.057), (31, 0.088), (32, 0.062), (33, 0.047), (34, -0.08), (35, 0.033), (36, -0.023), (37, -0.006), (38, 0.006), (39, -0.139), (40, 0.002), (41, 0.098), (42, 0.002), (43, -0.006), (44, -0.151), (45, -0.009), (46, -0.047), (47, -0.029), (48, -0.048), (49, 0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92774826 <a title="74-lsi-1" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>Author: Alon Faktor, Michal Irani</p><p>Abstract: Given a set of images which share an object from the same semantic category, we would like to co-segment the shared object. We define ‘good’ co-segments to be ones which can be easily composed (like a puzzle) from large pieces of other co-segments, yet are difficult to compose from remaining image parts. These pieces must not only match well but also be statistically significant (hard to compose at random). This gives rise to co-segmentation of objects in very challenging scenarios with large variations in appearance, shape and large amounts of clutter. We further show how multiple images can collaborate and “score each others ’ co-segments to improve the overall fidelity and accuracy of the co-segmentation. Our co-segmentation can be applied both to large image collections, as well as to very few images (where there is too little data for unsupervised learning). At the extreme, it can be applied even to a single image, to extract its co-occurring objects. Our approach obtains state-of-the-art results on benchmark datasets. We further show very encouraging co-segmentation results on the challenging PASCAL-VOC dataset. ”</p><p>2 0.75963801 <a title="74-lsi-2" href="./iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation.html">383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</a></p>
<p>Author: Zhengxiang Wang, Rujie Liu</p><p>Abstract: This paper introduces to use semi-supervised learning for large scale image cosegmentation. Different from traditional unsupervised cosegmentation that does not use any segmentation groundtruth, semi-supervised cosegmentation exploits the similarity from both the very limited training image foregrounds, as well as the common object shared between the large number of unsegmented images. This would be a much practical way to effectively cosegment a large number of related images simultaneously, where previous unsupervised cosegmentation work poorly due to the large variances in appearance between different images and the lack ofsegmentation groundtruthfor guidance in cosegmentation. For semi-supervised cosegmentation in large scale, we propose an effective method by minimizing an energy function, which consists of the inter-image distance, the intraimage distance and the balance term. We also propose an iterative updating algorithm to efficiently solve this energy function, which decomposes the original energy minimization problem into sub-problems, and updates each image alternatively to reduce the number of variables in each subproblem for computation efficiency. Experiment results on iCoseg and Pascal VOC datasets show that the proposed cosegmentation method can effectively cosegment hundreds of images in less than one minute. And our semi-supervised cosegmentation is able to outperform both unsupervised cosegmentation as well asfully supervised single image segmentation, especially when the training data is limited.</p><p>3 0.75323498 <a title="74-lsi-3" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>Author: Jifeng Dai, Ying Nian Wu, Jie Zhou, Song-Chun Zhu</p><p>Abstract: Cosegmentation refers to theproblem ofsegmenting multiple images simultaneously by exploiting the similarities between the foreground and background regions in these images. The key issue in cosegmentation is to align common objects between these images. To address this issue, we propose an unsupervised learning framework for cosegmentation, by coupling cosegmentation with what we call “cosketch ”. The goal of cosketch is to automatically discover a codebook of deformable shape templates shared by the input images. These shape templates capture distinct image patterns and each template is matched to similar image patches in different images. Thus the cosketch of the images helps to align foreground objects, thereby providing crucial information for cosegmentation. We present a statistical model whose energy function couples cosketch and cosegmentation. We then present an unsupervised learning algorithm that performs cosketch and cosegmentation by energy minimization. Experiments show that our method outperforms state of the art methods for cosegmentation on the challenging MSRC and iCoseg datasets. We also illustrate our method on a new dataset called Coseg-Rep where cosegmentation can be performed within a single image with repetitive patterns.</p><p>4 0.70942849 <a title="74-lsi-4" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<p>Author: Fan Wang, Qixing Huang, Leonidas J. Guibas</p><p>Abstract: Joint segmentation of image sets has great importance for object recognition, image classification, and image retrieval. In this paper, we aim to jointly segment a set of images starting from a small number of labeled images or none at all. To allow the images to share segmentation information with each other, we build a network that contains segmented as well as unsegmented images, and extract functional maps between connected image pairs based on image appearance features. These functional maps act as general property transporters between the images and, in particular, are used to transfer segmentations. We define and operate in a reduced functional space optimized so that the functional maps approximately satisfy cycle-consistency under composition in the network. A joint optimization framework is proposed to simultaneously generate all segmentation functions over the images so that they both align with local segmentation cues in each particular image, and agree with each other under network transportation. This formulation allows us to extract segmentations even with no training data, but can also exploit such data when available. The collective effect of the joint processing using functional maps leads to accurate information sharing among images and yields superior segmentation results, as shown on the iCoseg, MSRC, and PASCAL data sets.</p><p>5 0.6761564 <a title="74-lsi-5" href="./iccv-2013-Synergistic_Clustering_of_Image_and_Segment_Descriptors_for_Unsupervised_Scene_Understanding.html">412 iccv-2013-Synergistic Clustering of Image and Segment Descriptors for Unsupervised Scene Understanding</a></p>
<p>Author: Daniel M. Steinberg, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: With the advent of cheap, high fidelity, digital imaging systems, the quantity and rate of generation of visual data can dramatically outpace a humans ability to label or annotate it. In these situations there is scope for the use of unsupervised approaches that can model these datasets and automatically summarise their content. To this end, we present a totally unsupervised, and annotation-less, model for scene understanding. This model can simultaneously cluster whole-image and segment descriptors, therebyforming an unsupervised model of scenes and objects. We show that this model outperforms other unsupervised models that can only cluster one source of information (image or segment) at once. We are able to compare unsupervised and supervised techniques using standard measures derived from confusion matrices and contingency tables. This shows that our unsupervised model is competitive with current supervised and weakly-supervised models for scene understanding on standard datasets. We also demonstrate our model operating on a dataset with more than 100,000 images col- lected by an autonomous underwater vehicle.</p><p>6 0.62828171 <a title="74-lsi-6" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>7 0.61869818 <a title="74-lsi-7" href="./iccv-2013-Discriminatively_Trained_Templates_for_3D_Object_Detection%3A_A_Real_Time_Scalable_Approach.html">121 iccv-2013-Discriminatively Trained Templates for 3D Object Detection: A Real Time Scalable Approach</a></p>
<p>8 0.61852074 <a title="74-lsi-8" href="./iccv-2013-Detecting_Curved_Symmetric_Parts_Using_a_Deformable_Disc_Model.html">110 iccv-2013-Detecting Curved Symmetric Parts Using a Deformable Disc Model</a></p>
<p>9 0.6169979 <a title="74-lsi-9" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>10 0.60973376 <a title="74-lsi-10" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<p>11 0.60046744 <a title="74-lsi-11" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>12 0.5919143 <a title="74-lsi-12" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<p>13 0.58264703 <a title="74-lsi-13" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>14 0.57707363 <a title="74-lsi-14" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>15 0.57674903 <a title="74-lsi-15" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>16 0.56918383 <a title="74-lsi-16" href="./iccv-2013-Fine-Grained_Categorization_by_Alignments.html">169 iccv-2013-Fine-Grained Categorization by Alignments</a></p>
<p>17 0.56307095 <a title="74-lsi-17" href="./iccv-2013-Shape_Index_Descriptors_Applied_to_Texture-Based_Galaxy_Analysis.html">388 iccv-2013-Shape Index Descriptors Applied to Texture-Based Galaxy Analysis</a></p>
<p>18 0.5615536 <a title="74-lsi-18" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>19 0.55848455 <a title="74-lsi-19" href="./iccv-2013-SIFTpack%3A_A_Compact_Representation_for_Efficient_SIFT_Matching.html">365 iccv-2013-SIFTpack: A Compact Representation for Efficient SIFT Matching</a></p>
<p>20 0.55541539 <a title="74-lsi-20" href="./iccv-2013-A_New_Adaptive_Segmental_Matching_Measure_for_Human_Activity_Recognition.html">22 iccv-2013-A New Adaptive Segmental Matching Measure for Human Activity Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.081), (4, 0.028), (7, 0.011), (26, 0.104), (31, 0.053), (35, 0.018), (40, 0.011), (42, 0.098), (43, 0.206), (48, 0.016), (64, 0.061), (73, 0.02), (89, 0.184), (95, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83799529 <a title="74-lda-1" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>Author: Alon Faktor, Michal Irani</p><p>Abstract: Given a set of images which share an object from the same semantic category, we would like to co-segment the shared object. We define ‘good’ co-segments to be ones which can be easily composed (like a puzzle) from large pieces of other co-segments, yet are difficult to compose from remaining image parts. These pieces must not only match well but also be statistically significant (hard to compose at random). This gives rise to co-segmentation of objects in very challenging scenarios with large variations in appearance, shape and large amounts of clutter. We further show how multiple images can collaborate and “score each others ’ co-segments to improve the overall fidelity and accuracy of the co-segmentation. Our co-segmentation can be applied both to large image collections, as well as to very few images (where there is too little data for unsupervised learning). At the extreme, it can be applied even to a single image, to extract its co-occurring objects. Our approach obtains state-of-the-art results on benchmark datasets. We further show very encouraging co-segmentation results on the challenging PASCAL-VOC dataset. ”</p><p>2 0.8126936 <a title="74-lda-2" href="./iccv-2013-Robust_Face_Landmark_Estimation_under_Occlusion.html">355 iccv-2013-Robust Face Landmark Estimation under Occlusion</a></p>
<p>Author: Xavier P. Burgos-Artizzu, Pietro Perona, Piotr Dollár</p><p>Abstract: Human faces captured in real-world conditions present large variations in shape and occlusions due to differences in pose, expression, use of accessories such as sunglasses and hats and interactions with objects (e.g. food). Current face landmark estimation approaches struggle under such conditions since theyfail toprovide aprincipled way ofhandling outliers. We propose a novel method, called Robust Cascaded Pose Regression (RCPR) which reduces exposure to outliers by detecting occlusions explicitly and using robust shape-indexed features. We show that RCPR improves on previous landmark estimation methods on three popular face datasets (LFPW, LFW and HELEN). We further explore RCPR ’s performance by introducing a novel face dataset focused on occlusion, composed of 1,007 faces presenting a wide range of occlusion patterns. RCPR reduces failure cases by half on all four datasets, at the same time as it detects face occlusions with a 80/40% precision/recall.</p><p>3 0.80474806 <a title="74-lda-3" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>Author: Christoph Straehle, Ullrich Koethe, Fred A. Hamprecht</p><p>Abstract: We propose a scheme that allows to partition an image into a previously unknown number of segments, using only minimal supervision in terms of a few must-link and cannotlink annotations. We make no use of regional data terms, learning instead what constitutes a likely boundary between segments. Since boundaries are only implicitly specified through cannot-link constraints, this is a hard and nonconvex latent variable problem. We address this problem in a greedy fashion using a randomized decision tree on features associated with interpixel edges. We use a structured purity criterion during tree construction and also show how a backtracking strategy can be used to prevent the greedy search from ending up in poor local optima. The proposed strategy is compared with prior art on natural images.</p><p>4 0.79209405 <a title="74-lda-4" href="./iccv-2013-Interactive_Markerless_Articulated_Hand_Motion_Tracking_Using_RGB_and_Depth_Data.html">218 iccv-2013-Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth Data</a></p>
<p>Author: Srinath Sridhar, Antti Oulasvirta, Christian Theobalt</p><p>Abstract: Tracking the articulated 3D motion of the hand has important applications, for example, in human–computer interaction and teleoperation. We present a novel method that can capture a broad range of articulated hand motions at interactive rates. Our hybrid approach combines, in a voting scheme, a discriminative, part-based pose retrieval method with a generative pose estimation method based on local optimization. Color information from a multiview RGB camera setup along with a person-specific hand model are used by the generative method to find the pose that best explains the observed images. In parallel, our discriminative pose estimation method uses fingertips detected on depth data to estimate a complete or partial pose of the hand by adopting a part-based pose retrieval strategy. This part-based strategy helps reduce the search space drastically in comparison to a global pose retrieval strategy. Quantitative results show that our method achieves state-of-the-art accuracy on challenging sequences and a near-realtime performance of 10 fps on a desktop computer.</p><p>5 0.77318043 <a title="74-lda-5" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>Author: Jifeng Dai, Ying Nian Wu, Jie Zhou, Song-Chun Zhu</p><p>Abstract: Cosegmentation refers to theproblem ofsegmenting multiple images simultaneously by exploiting the similarities between the foreground and background regions in these images. The key issue in cosegmentation is to align common objects between these images. To address this issue, we propose an unsupervised learning framework for cosegmentation, by coupling cosegmentation with what we call “cosketch ”. The goal of cosketch is to automatically discover a codebook of deformable shape templates shared by the input images. These shape templates capture distinct image patterns and each template is matched to similar image patches in different images. Thus the cosketch of the images helps to align foreground objects, thereby providing crucial information for cosegmentation. We present a statistical model whose energy function couples cosketch and cosegmentation. We then present an unsupervised learning algorithm that performs cosketch and cosegmentation by energy minimization. Experiments show that our method outperforms state of the art methods for cosegmentation on the challenging MSRC and iCoseg datasets. We also illustrate our method on a new dataset called Coseg-Rep where cosegmentation can be performed within a single image with repetitive patterns.</p><p>6 0.76782441 <a title="74-lda-6" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>7 0.76370859 <a title="74-lda-7" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>8 0.76331681 <a title="74-lda-8" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>9 0.76312292 <a title="74-lda-9" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>10 0.76309097 <a title="74-lda-10" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>11 0.76156628 <a title="74-lda-11" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>12 0.76123834 <a title="74-lda-12" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>13 0.76122063 <a title="74-lda-13" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>14 0.76113987 <a title="74-lda-14" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>15 0.76038098 <a title="74-lda-15" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>16 0.75974381 <a title="74-lda-16" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>17 0.7595104 <a title="74-lda-17" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>18 0.75949359 <a title="74-lda-18" href="./iccv-2013-Real-Time_Articulated_Hand_Pose_Estimation_Using_Semi-supervised_Transductive_Regression_Forests.html">340 iccv-2013-Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests</a></p>
<p>19 0.75902492 <a title="74-lda-19" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>20 0.75879025 <a title="74-lda-20" href="./iccv-2013-Fast_High_Dimensional_Vector_Multiplication_Face_Recognition.html">158 iccv-2013-Fast High Dimensional Vector Multiplication Face Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
