<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>144 iccv-2013-Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-144" href="#">iccv2013-144</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>144 iccv-2013-Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors</h1>
<br/><p>Source: <a title="iccv-2013-144-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Zhang_Estimating_the_3D_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Jian Zhang, Chen Kan, Alexander G. Schwing, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly estimate the layout ofrooms as well as the clutterpresent in the scene using RGB-D data. Towards this goal, we propose an effective model that is able to exploit both depth and appearance features, which are complementary. Furthermore, our approach is efficient as we exploit the inherent decomposition of additive potentials. We demonstrate the effectiveness of our approach on the challenging NYU v2 dataset and show that employing depth reduces the layout error by 6% and the clutter estimation by 13%.</p><p>Reference: <a title="iccv-2013-144-reference" href="../iccv2013_reference/iccv-2013-Estimating_the_3D_Layout_of_Indoor_Scenes_and_Its_Clutter_from_Depth_Sensors_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com Abstract  In this paper we propose an approach to jointly estimate the layout ofrooms as well as the clutterpresent in the scene using RGB-D data. [sent-3, score-0.604]
</p><p>2 Towards this goal, we propose an effective model that is able to exploit both depth and appearance features, which are complementary. [sent-4, score-0.22]
</p><p>3 We demonstrate the effectiveness of our approach on the challenging NYU v2 dataset and show that employing depth reduces the layout error by 6% and the clutter estimation by 13%. [sent-6, score-0.97]
</p><p>4 While finding such structures from monocular imagery is extremely difficult, depth sensors can be employed to re-  duce the inherent ambiguities of still images. [sent-9, score-0.439]
</p><p>5 In the outdoor setting, high-end depth sensors such as the Velodyne laser scanner are a must for autonomous navigation. [sent-10, score-0.319]
</p><p>6 In the past few years, a wide variety of approaches have exploited cheap depth sensors (e. [sent-12, score-0.298]
</p><p>7 Additionally, the superiority of RGB-D sensors when compared to more traditional imagery has been demonstrated for the tasks of semantic segmentation [25, 26, 8], inferring support relations [26], 3D detection [14] or estimating physical properties of images [2]. [sent-16, score-0.273]
</p><p>8 Semantic parsing approaches that utilize RGB-D imagery try to estimate the basic components of a room (e. [sent-17, score-0.224]
</p><p>9 Infr(cai)nglyout,cl erabndseman(dtbi)chalbirntsaebl sca:bineTtch a  input image with layout prediction result (blue) and ground truth (red) overlayed is shown in (a), the depth map employed is shown in (b). [sent-25, score-0.768]
</p><p>10 (c) and (d) show our inferred labeling and segmentation. [sent-26, score-0.233]
</p><p>11 problem as a semantic segmentation task, failing to exploit the structure of the problem: rooms mostly satisfy the Manhattan world assumption, and the walls, floor and ceiling are typically aligned with three dominant orientations which are orthonormal. [sent-27, score-0.435]
</p><p>12 While these assumptions are widely used in the monocular setting in order to estimate the layout of rooms [29, 10, 11, 22, 23, 18], to our knowledge, they are not commonly exploited in the presence ofRGB-D imagery. [sent-28, score-0.802]
</p><p>13 In addition to estimation of the room layout, we should  be able to retrieve the objects that compose the scene in order to develop autonomous systems. [sent-29, score-0.185]
</p><p>14 In this paper we propose an approach to semantic parsing that estimates both the layout of rooms as well as the clutter present in the scene. [sent-30, score-0.999]
</p><p>15 We make use of both appearance and depth features, which, as we show in our experimental evaluation are complementary, and frame a joint optimization problem which exploits the dependencies between these two tasks. [sent-31, score-0.22]
</p><p>16 , integral geometry) can be employed to decompose our additive energies into Markov random fields (MRFs) with potentials containing at most two random variables. [sent-34, score-0.347]
</p><p>17 Layout parameterization and integral geometry: While the parameterization of the layout task is illustrated in (a), (b) depicts that the area of the left wall is decomposable into a difference of green and blue highlighted ranges both depending on two angles only. [sent-37, score-0.911]
</p><p>18 (c) Usage ofintegral geometry for computation ofcompatibility potentials where we count how much a superpixel overlaps with a hypothesized  wall. [sent-38, score-0.399]
</p><p>19 1, where RGB-D imagery is utilized in order to estimate the room layout, the clutter as well as semantic segmentation. [sent-41, score-0.355]
</p><p>20 We demonstrate the effectiveness of our approach us-  ing the challenging NYU v2 dataset [26] and show that by employing depth we boost performance of the layout estimation task by 6% while clutter estimation improves by 13%. [sent-42, score-1.042]
</p><p>21 This is to be expected since “clutter” occupies the foreground which is easily apparent in depth images. [sent-43, score-0.176]
</p><p>22 Additionally we show how a wide variety of semantic classes can be obtained by utilizing depth and appearance features. [sent-44, score-0.292]
</p><p>23 Related Work Early approaches to semantic scene understanding in the outdoor setting focused on producing qualitative 3D parses [19, 13, 6], ground plane estimates [30] or parsing facades [27, 17]. [sent-46, score-0.253]
</p><p>24 More recently, accurate estimations of the road topologies at intersections [4] as well as the 3D vehicles present in the scene [5] have been estimated from stereo and monocular video respectively. [sent-47, score-0.151]
</p><p>25 Indoor scene understanding approaches have taken advantage of the Manhattan world properties of rooms and frame the layout estimation task as the prediction of a 3D cuboid aligned with the three main dominant orientations [10, 11, 18, 22, 23, 16, 29]. [sent-51, score-0.9]
</p><p>26 [22] showed that the a priori high-order potentials, are decomposable into sums of pairwise potentials by extending the concept of integral images to accumulators oriented with the dominant orientations. [sent-58, score-0.473]
</p><p>27 In [23], a branch and bound approach was developed to retrieve a global optimum of the layout problem. [sent-60, score-0.56]
</p><p>28 Perhaps one of the reasons is the absence of a dataset with depth and layout labels. [sent-64, score-0.736]
</p><p>29 In this paper we investigate how cheap depth sensors can be used to help the layout problem and show that significant improvements are obtained. [sent-65, score-0.858]
</p><p>30 Towards this goal, we labeled a subset of the NYU-RBGD v2 dataset with layout labels. [sent-66, score-0.56]
</p><p>31 [29] reason jointly about the layout as well as the clutter present in the scene. [sent-68, score-0.72]
</p><p>32 They propose to make use of an iterated conditional modes (ICM) algorithm, to tractably deal with the complex potentials resulting from the interaction of the clutter and the layout. [sent-69, score-0.339]
</p><p>33 As a result, their layout estimation results are more than 5% lower than the state-of-the-art. [sent-71, score-0.591]
</p><p>34 In contrast, in this  paper we propose an effective approach to the joint layout and clutter estimation problem, which is able to exploit appearance, depth, as well as compatibility potentials linking the two estimation problems. [sent-72, score-1.08]
</p><p>35 We propose an effective inference scheme, which alternates between computing the layout and solving for the image labeling problem. [sent-73, score-0.859]
</p><p>36 As we take advantage of the inherent decomposition of the potentials, our approach is efficient and results in impressive performance improving 6% over the state-of-the-art in the layout task and 13% in estimating clutter. [sent-74, score-0.601]
</p><p>37 Joint Layout and Clutter Labeling In this section we describe our novel approach, which jointly estimates the layout of rooms as well as the clutter present in the scene. [sent-76, score-0.888]
</p><p>38 holistic approach that is able to exploit appearance as well as depth features. [sent-79, score-0.22]
</p><p>39 We then introduce our joint model which operates on superpixels and random variables representing the layout, and discuss our learning and inference procedures. [sent-81, score-0.178]
</p><p>40 Following [32], we extend the SLIC [1] algorithm to utilize both appearance and depth information. [sent-85, score-0.282]
</p><p>41 In particular, Eloc is a location energy encoding the fact that superpixels should have regular shape, Edepth encourages the depth of the superpixels to be piecewise planar and Eapp encodes the fact that we would like to have similar appearance for all pixels that are subsumed by a superpixel. [sent-87, score-0.575]
</p><p>42 More formally, let sp ∈ {1, · · · , K} be a random variable encoding the assignment {o1f, pixel p }to b a superpixel. [sent-88, score-0.201]
</p><p>43 We define the energy of a pixel p to be E(p, sp, μsp csp Dsp ) Eloc(p, sp) λaEapp(p, sp, csp ) +λdEdepth(p, sp, gsp), with two scalars λa and λd encoding the importance of the appearance and depth terms. [sent-89, score-0.58]
</p><p>44 We thus define the superpixel labeling problem as the following minimization ? [sent-91, score-0.396]
</p><p>45 =1E(p,sp,μsp,csp,gsp), with s = {s1, · · · , sN}, μ = {μ1 , · · · , μK}, c = {c1, · · · , cK}{ san,d· g ,=s {}g1, , ·μ · · ,gK{}μ th,e· s·e·t , oμf a}ll, superpixel assignments, mean positions, mean appearance panerddepth descriptors. [sent-93, score-0.207]
</p><p>46 Note that this is done very efficiently: The first step decomposes over pixels, while the latter decomposes over superpixels and even admits an update in closed form [1]. [sent-95, score-0.192]
</p><p>47 Joint model Now that we have partitioned the image into superpixels by incorporating the depth cue, we define our energy considering both layout and labeling variables. [sent-98, score-1.144]
</p><p>48 Our joint model is a conditional random field (CRF) over both the labeling and layout task. [sent-99, score-0.793]
</p><p>49 For each superpixel, the labeling task assigns one of the six labels, i. [sent-100, score-0.306]
</p><p>50 Fol∈lowing rre,lceefntt, rmiognhotc,uflraor approaches [l2oo9,r }22] =, we represent the layout task in terms of rays originating from two different vanishing points (VPs). [sent-103, score-0.712]
</p><p>51 In particular, we utilize the vanishing points of [26], which take advantage of the depth channel to produce better estimates. [sent-104, score-0.309]
</p><p>52 Given these  VPs, we represent the layout problem with four parameters y = {y1, y2 , y3 , y4} ∈ Y. [sent-105, score-0.56]
</p><p>53 We define the energy of the system to be the sum of three energies representing the layout and labeling tasks as well as a compatibility term which encodes the relationship between the two tasks. [sent-108, score-1.071]
</p><p>54 Layout Energy: Following monocular approaches [15, 22], we define the energy of the layout as a sum of energies over the faces α of the cuboid. [sent-112, score-0.789]
</p><p>55 Note that although a priori these potentials are high-order (i. [sent-125, score-0.211]
</p><p>56 , up to order 4 as specifying the front wall requires four variables), we utilize integral geometry to decompose the potentials into sums of terms of order at most two [22]. [sent-127, score-0.48]
</p><p>57 Labeling Energy: We define the labeling energy to be composed of unary and pairwise potentials as follows ? [sent-130, score-0.511]
</p><p>58 We utilize two depth features: The first one encodes the idea that the superpixel normals which do not belong to the clutter class should be in accordance with the main dominant directions. [sent-142, score-0.655]
</p><p>59 To capture this, we generate a six dimensional feature, where each entry is the cosine of the angle between the surface normal of the superpixel and each dominant orientation. [sent-143, score-0.3]
</p><p>60 The second feature, encodes our intuition that the superpixels labeled as bounding surfaces (i. [sent-144, score-0.149]
</p><p>61 We thus define a six dimensional feature, which computes the distance between the mean 3D position of the superpixel being referred to as its centroid and the centroid of the superpixel furthest away in each dominant direction. [sent-147, score-0.415]
</p><p>62 Compatibility Energy: This energy encodes the fact that the layout and the labeling problems should agree. [sent-153, score-0.893]
</p><p>63 β(y,α) with β(y, α), a function that returns the set of pixels p which compose the wall α defined by the layout y. [sent-166, score-0.633]
</p><p>64 For a specific choice of α and γ the feature measures the area that is predicted to be α by the layout task while being assigned  by the labeling prediction. [sent-167, score-0.834]
</p><p>65 Note that we can extend the concept of integral geometry introduced by [22] to this case. [sent-169, score-0.166]
</p><p>66 In a na¨ ıve way we could iterate over all superpixels, set their corresponding area to one while leaving everything else to be zero, thus performing integral geometry computations for each superpixel independently. [sent-170, score-0.363]
</p><p>67 Since we know which pixels belong to a specific superpixel, we derive a method that allows computation of those compatibility potentials with a single linear pass over the image, i. [sent-171, score-0.298]
</p><p>68 2(c) provides an illustration of both the potentials as well as integral geometry. [sent-178, score-0.288]
</p><p>69 Due to the high-order terms imposed by the compatibility potential, we propose to solve this in an iterative fashion, alternating between solving for the labeling x, and the layout y. [sent-182, score-0.912]
</p><p>70 As a consequence, the potentials are of order at most 2 for each minimization problem. [sent-183, score-0.179]
</p><p>71 Denote {(Ii, ˆx i, ˆ yi)} to be a set of N training examples fully anno{ta(tIed with a labeling x ta ondf a layout y feoxra an RleGsB fu-lDly image I. [sent-191, score-0.793]
</p><p>72 Let Xi = LK be the labeling product space of the K superpixels. [sent-197, score-0.233]
</p><p>73 For both losses we employ the pixel-wise loss as it decomposes into unary potentials for the labeling task and pairwise potentials for the layout task (via integral geometry). [sent-199, score-1.49]
</p><p>74 Speeding up computations One of the most expensive tasks in inferring the layout is computation of the accumulators required to perform integral geometry. [sent-204, score-0.799]
</p><p>75 When computing the accumulator A, we accumulate the value of every pixel into a 2D matrix indexed by two angles. [sent-213, score-0.169]
</p><p>76 3, where the pixel grid is given by the gray lines while the black rays illustrate a coarse discretization of the state space with the accumulator matrix entry A(3, 3) being highlighted in magenta color. [sent-215, score-0.257]
</p><p>77 In [22], the accumulator entry for every pixel is found independently. [sent-216, score-0.217]
</p><p>78 As shown in Table 2, in the layout task we outperform [22] by more than 5%. [sent-242, score-0.601]
</p><p>79 For the labeling task, the results are even better since usage of a depth cue improves GC by 13%. [sent-243, score-0.409]
</p><p>80 Importance of depth features: As shown in Table 2, by employing depth, our approach improves accuracy by 10% in labeling and by 6% in layout estimation. [sent-244, score-1.012]
</p><p>81 Depth features improve the labeling error on many of the images quite significantly. [sent-245, score-0.233]
</p><p>82 For samples above the red line, the approach utilizing only RGB features has an error larger than the method employing the depth cue. [sent-248, score-0.219]
</p><p>83 Unsupervised segmentation: We now compare the importance of using depth in our unsupervised segmentation algorithm. [sent-249, score-0.221]
</p><p>84 For all images, we utilize 200 superpixels per image. [sent-250, score-0.174]
</p><p>85 The first one is the mean error (ME), which is  define as the percentage of incorrectly labeled pixels if we had an oracle labeling each super-pixel with the semantic labels. [sent-253, score-0.336]
</p><p>86 sults as a function of the importance of the appearance and depth terms. [sent-289, score-0.22]
</p><p>87 Fast accumulator computation: We now compare our proposed computation of compatibility accumulators to the implementation proposed in [22]. [sent-297, score-0.34]
</p><p>88 4(b) (d) for the pixel-wise layout estimation error, the superpixel labeling error and the average intersection over union measure for the different walls as well as clutter. [sent-307, score-1.099]
</p><p>89 4 for the different walls and clutter class as well as the resulting average. [sent-309, score-0.234]
</p><p>90 In particular, we used six RGB-D kernel descriptors gradient, color, local binary pattern, depth gradient, spin/surface normal, and KPCA/self-similarity. [sent-311, score-0.208]
</p><p>91 In the fourth and fifth column we illustrate the ground truth labeling as well as our labeling estimation. [sent-317, score-0.529]
</p><p>92 The sixth column depicts the layout estimates in (blue) and the 1278  ? [sent-318, score-0.663]
</p><p>93 C  for layout, labeling and intersection  over  (d). [sent-357, score-0.233]
</p><p>94 The labeling task consist on 6 classes, the five walls and clutter. [sent-374, score-0.348]
</p><p>95 Note that by using depth the average IOU measure improves by more than 17%, a very significant result. [sent-375, score-0.176]
</p><p>96 Finally the last two columns provide the semantic labeling for the ground truth as well as our estimates. [sent-384, score-0.337]
</p><p>97 ut of rooms as well as the clutter present in the scene usi? [sent-395, score-0.339]
</p><p>98 In the fourth and fifth column we show the ground truth labeling as well as our labeling estimation. [sent-619, score-0.529]
</p><p>99 The sixth column depicted the layout estimates in (blue) and the ground truth in red. [sent-620, score-0.659]
</p><p>100 Finally the last two columns depicted the semantic labeling for the ground truth as well as our estimates. [sent-621, score-0.337]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('layout', 0.56), ('labeling', 0.233), ('potentials', 0.179), ('depth', 0.176), ('superpixel', 0.163), ('ecomp', 0.162), ('clutter', 0.16), ('rooms', 0.135), ('sp', 0.126), ('accumulator', 0.125), ('compatibility', 0.119), ('superpixels', 0.112), ('csp', 0.111), ('integral', 0.109), ('elabeling', 0.108), ('gsp', 0.108), ('monocular', 0.107), ('schwing', 0.107), ('accumulators', 0.096), ('indoor', 0.093), ('sensors', 0.091), ('eloc', 0.081), ('furniture', 0.076), ('walls', 0.074), ('wall', 0.073), ('elayout', 0.072), ('semantic', 0.072), ('vanishing', 0.071), ('inference', 0.066), ('imagery', 0.065), ('floor', 0.063), ('energy', 0.063), ('ceiling', 0.063), ('utilize', 0.062), ('slic', 0.061), ('gc', 0.06), ('energies', 0.059), ('room', 0.058), ('dominant', 0.057), ('geometry', 0.057), ('eapp', 0.054), ('tsingua', 0.054), ('lay', 0.054), ('hazan', 0.052), ('autonomous', 0.052), ('layouts', 0.049), ('hedau', 0.048), ('edepth', 0.048), ('gcs', 0.048), ('hil', 0.048), ('entry', 0.048), ('parameterization', 0.046), ('consequence', 0.046), ('nyu', 0.045), ('segmentation', 0.045), ('lil', 0.044), ('pollefeys', 0.044), ('appearance', 0.044), ('scene', 0.044), ('pixel', 0.044), ('employing', 0.043), ('vps', 0.042), ('hoiem', 0.041), ('task', 0.041), ('rays', 0.04), ('decomposes', 0.04), ('rli', 0.04), ('employ', 0.04), ('compromise', 0.04), ('rgbd', 0.039), ('parsing', 0.039), ('union', 0.038), ('encodes', 0.037), ('depicts', 0.036), ('om', 0.036), ('unary', 0.036), ('iou', 0.035), ('computations', 0.034), ('lab', 0.034), ('sixth', 0.034), ('qualitative', 0.033), ('vp', 0.033), ('estimates', 0.033), ('truth', 0.032), ('losses', 0.032), ('understanding', 0.032), ('priori', 0.032), ('gupta', 0.032), ('six', 0.032), ('silberman', 0.031), ('fifth', 0.031), ('cheap', 0.031), ('manhattan', 0.031), ('oracle', 0.031), ('efros', 0.031), ('summarizes', 0.031), ('encoding', 0.031), ('cuboids', 0.031), ('estimation', 0.031), ('reasoning', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000012 <a title="144-tfidf-1" href="./iccv-2013-Estimating_the_3D_Layout_of_Indoor_Scenes_and_Its_Clutter_from_Depth_Sensors.html">144 iccv-2013-Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors</a></p>
<p>Author: Jian Zhang, Chen Kan, Alexander G. Schwing, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly estimate the layout ofrooms as well as the clutterpresent in the scene using RGB-D data. Towards this goal, we propose an effective model that is able to exploit both depth and appearance features, which are complementary. Furthermore, our approach is efficient as we exploit the inherent decomposition of additive potentials. We demonstrate the effectiveness of our approach on the challenging NYU v2 dataset and show that employing depth reduces the layout error by 6% and the clutter estimation by 13%.</p><p>2 0.55934554 <a title="144-tfidf-2" href="./iccv-2013-Box_in_the_Box%3A_Joint_3D_Layout_and_Object_Reasoning_from_Single_Images.html">64 iccv-2013-Box in the Box: Joint 3D Layout and Object Reasoning from Single Images</a></p>
<p>Author: Alexander G. Schwing, Sanja Fidler, Marc Pollefeys, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly infer the room layout as well as the objects present in the scene. Towards this goal, we propose a branch and bound algorithm which is guaranteed to retrieve the global optimum of the joint problem. The main difficulty resides in taking into account occlusion in order to not over-count the evidence. We introduce a new decomposition method, which generalizes integral geometry to triangular shapes, and allows us to bound the different terms in constant time. We exploit both geometric cues and object detectors as image features and show large improvements in 2D and 3D object detection over state-of-the-art deformable part-based models.</p><p>3 0.26716447 <a title="144-tfidf-3" href="./iccv-2013-Characterizing_Layouts_of_Outdoor_Scenes_Using_Spatial_Topic_Processes.html">72 iccv-2013-Characterizing Layouts of Outdoor Scenes Using Spatial Topic Processes</a></p>
<p>Author: Dahua Lin, Jianxiong Xiao</p><p>Abstract: In this paper, we develop a generative model to describe the layouts of outdoor scenes the spatial configuration of regions. Specifically, the layout of an image is represented as a composite of regions, each associated with a semantic topic. At the heart of this model is a novel stochastic process called Spatial Topic Process, which generates a spatial map of topics from a set of coupled Gaussian processes, thus allowing the distributions of topics to vary continuously across the image plane. A key aspect that distinguishes this model from previous ones consists in its capability of capturing dependencies across both locations and topics while allowing substantial variations in the layouts. We demonstrate the practical utility of the proposed model by testing it on scene classification, semantic segmentation, and layout hallucination. –</p><p>4 0.2519536 <a title="144-tfidf-4" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>Author: Scott Satkin, Martial Hebert</p><p>Abstract: We present a new algorithm 3DNN (3D NearestNeighbor), which is capable of matching an image with 3D data, independently of the viewpoint from which the image was captured. By leveraging rich annotations associated with each image, our algorithm can automatically produce precise and detailed 3D models of a scene from a single image. Moreover, we can transfer information across images to accurately label and segment objects in a scene. The true benefit of 3DNN compared to a traditional 2D nearest-neighbor approach is that by generalizing across viewpoints, we free ourselves from the need to have training examples captured from all possible viewpoints. Thus, we are able to achieve comparable results using orders of magnitude less data, and recognize objects from never-beforeseen viewpoints. In this work, we describe the 3DNN algorithm and rigorously evaluate its performance for the tasks of geometry estimation and object detection/segmentation. By decoupling the viewpoint and the geometry of an image, we develop a scene matching approach which is truly 100% viewpoint invariant, yielding state-of-the-art performance on challenging data.</p><p>5 0.24360678 <a title="144-tfidf-5" href="./iccv-2013-Holistic_Scene_Understanding_for_3D_Object_Detection_with_RGBD_Cameras.html">201 iccv-2013-Holistic Scene Understanding for 3D Object Detection with RGBD Cameras</a></p>
<p>Author: Dahua Lin, Sanja Fidler, Raquel Urtasun</p><p>Abstract: In this paper, we tackle the problem of indoor scene understanding using RGBD data. Towards this goal, we propose a holistic approach that exploits 2D segmentation, 3D geometry, as well as contextual relations between scenes and objects. Specifically, we extend the CPMC [3] framework to 3D in order to generate candidate cuboids, and develop a conditional random field to integrate information from different sources to classify the cuboids. With this formulation, scene classification and 3D object recognition are coupled and can be jointly solved through probabilistic inference. We test the effectiveness of our approach on the challenging NYU v2 dataset. The experimental results demonstrate that through effective evidence integration and holistic reasoning, our approach achieves substantial improvement over the state-of-the-art.</p><p>6 0.21946824 <a title="144-tfidf-6" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>7 0.18726686 <a title="144-tfidf-7" href="./iccv-2013-Active_MAP_Inference_in_CRFs_for_Efficient_Semantic_Segmentation.html">42 iccv-2013-Active MAP Inference in CRFs for Efficient Semantic Segmentation</a></p>
<p>8 0.18154447 <a title="144-tfidf-8" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>9 0.17422092 <a title="144-tfidf-9" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>10 0.15106149 <a title="144-tfidf-10" href="./iccv-2013-3D_Scene_Understanding_by_Voxel-CRF.html">2 iccv-2013-3D Scene Understanding by Voxel-CRF</a></p>
<p>11 0.13174936 <a title="144-tfidf-11" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>12 0.13124637 <a title="144-tfidf-12" href="./iccv-2013-Lifting_3D_Manhattan_Lines_from_a_Single_Image.html">250 iccv-2013-Lifting 3D Manhattan Lines from a Single Image</a></p>
<p>13 0.1310672 <a title="144-tfidf-13" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>14 0.12968102 <a title="144-tfidf-14" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<p>15 0.12833156 <a title="144-tfidf-15" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>16 0.12612364 <a title="144-tfidf-16" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>17 0.12588385 <a title="144-tfidf-17" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>18 0.12392273 <a title="144-tfidf-18" href="./iccv-2013-Data-Driven_3D_Primitives_for_Single_Image_Understanding.html">102 iccv-2013-Data-Driven 3D Primitives for Single Image Understanding</a></p>
<p>19 0.11691289 <a title="144-tfidf-19" href="./iccv-2013-Efficient_Hand_Pose_Estimation_from_a_Single_Depth_Image.html">133 iccv-2013-Efficient Hand Pose Estimation from a Single Depth Image</a></p>
<p>20 0.10878503 <a title="144-tfidf-20" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.234), (1, -0.136), (2, 0.001), (3, 0.013), (4, 0.122), (5, -0.009), (6, -0.101), (7, -0.057), (8, -0.083), (9, -0.152), (10, 0.042), (11, 0.137), (12, -0.117), (13, 0.062), (14, -0.042), (15, -0.128), (16, -0.161), (17, -0.09), (18, -0.129), (19, -0.113), (20, -0.231), (21, -0.094), (22, 0.152), (23, -0.084), (24, 0.16), (25, -0.182), (26, 0.084), (27, 0.058), (28, -0.064), (29, 0.133), (30, -0.092), (31, 0.085), (32, -0.035), (33, 0.213), (34, 0.106), (35, -0.045), (36, -0.122), (37, -0.103), (38, -0.065), (39, 0.056), (40, -0.067), (41, -0.032), (42, 0.084), (43, 0.073), (44, -0.003), (45, 0.027), (46, -0.004), (47, 0.043), (48, 0.053), (49, -0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96699905 <a title="144-lsi-1" href="./iccv-2013-Estimating_the_3D_Layout_of_Indoor_Scenes_and_Its_Clutter_from_Depth_Sensors.html">144 iccv-2013-Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors</a></p>
<p>Author: Jian Zhang, Chen Kan, Alexander G. Schwing, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly estimate the layout ofrooms as well as the clutterpresent in the scene using RGB-D data. Towards this goal, we propose an effective model that is able to exploit both depth and appearance features, which are complementary. Furthermore, our approach is efficient as we exploit the inherent decomposition of additive potentials. We demonstrate the effectiveness of our approach on the challenging NYU v2 dataset and show that employing depth reduces the layout error by 6% and the clutter estimation by 13%.</p><p>2 0.87562335 <a title="144-lsi-2" href="./iccv-2013-Box_in_the_Box%3A_Joint_3D_Layout_and_Object_Reasoning_from_Single_Images.html">64 iccv-2013-Box in the Box: Joint 3D Layout and Object Reasoning from Single Images</a></p>
<p>Author: Alexander G. Schwing, Sanja Fidler, Marc Pollefeys, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly infer the room layout as well as the objects present in the scene. Towards this goal, we propose a branch and bound algorithm which is guaranteed to retrieve the global optimum of the joint problem. The main difficulty resides in taking into account occlusion in order to not over-count the evidence. We introduce a new decomposition method, which generalizes integral geometry to triangular shapes, and allows us to bound the different terms in constant time. We exploit both geometric cues and object detectors as image features and show large improvements in 2D and 3D object detection over state-of-the-art deformable part-based models.</p><p>3 0.74711412 <a title="144-lsi-3" href="./iccv-2013-Holistic_Scene_Understanding_for_3D_Object_Detection_with_RGBD_Cameras.html">201 iccv-2013-Holistic Scene Understanding for 3D Object Detection with RGBD Cameras</a></p>
<p>Author: Dahua Lin, Sanja Fidler, Raquel Urtasun</p><p>Abstract: In this paper, we tackle the problem of indoor scene understanding using RGBD data. Towards this goal, we propose a holistic approach that exploits 2D segmentation, 3D geometry, as well as contextual relations between scenes and objects. Specifically, we extend the CPMC [3] framework to 3D in order to generate candidate cuboids, and develop a conditional random field to integrate information from different sources to classify the cuboids. With this formulation, scene classification and 3D object recognition are coupled and can be jointly solved through probabilistic inference. We test the effectiveness of our approach on the challenging NYU v2 dataset. The experimental results demonstrate that through effective evidence integration and holistic reasoning, our approach achieves substantial improvement over the state-of-the-art.</p><p>4 0.68592983 <a title="144-lsi-4" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>Author: Scott Satkin, Martial Hebert</p><p>Abstract: We present a new algorithm 3DNN (3D NearestNeighbor), which is capable of matching an image with 3D data, independently of the viewpoint from which the image was captured. By leveraging rich annotations associated with each image, our algorithm can automatically produce precise and detailed 3D models of a scene from a single image. Moreover, we can transfer information across images to accurately label and segment objects in a scene. The true benefit of 3DNN compared to a traditional 2D nearest-neighbor approach is that by generalizing across viewpoints, we free ourselves from the need to have training examples captured from all possible viewpoints. Thus, we are able to achieve comparable results using orders of magnitude less data, and recognize objects from never-beforeseen viewpoints. In this work, we describe the 3DNN algorithm and rigorously evaluate its performance for the tasks of geometry estimation and object detection/segmentation. By decoupling the viewpoint and the geometry of an image, we develop a scene matching approach which is truly 100% viewpoint invariant, yielding state-of-the-art performance on challenging data.</p><p>5 0.66806 <a title="144-lsi-5" href="./iccv-2013-3D_Scene_Understanding_by_Voxel-CRF.html">2 iccv-2013-3D Scene Understanding by Voxel-CRF</a></p>
<p>Author: Byung-Soo Kim, Pushmeet Kohli, Silvio Savarese</p><p>Abstract: Scene understanding is an important yet very challenging problem in computer vision. In the past few years, researchers have taken advantage of the recent diffusion of depth-RGB (RGB-D) cameras to help simplify the problem of inferring scene semantics. However, while the added 3D geometry is certainly useful to segment out objects with different depth values, it also adds complications in that the 3D geometry is often incorrect because of noisy depth measurements and the actual 3D extent of the objects is usually unknown because of occlusions. In this paper we propose a new method that allows us to jointly refine the 3D reconstruction of the scene (raw depth values) while accurately segmenting out the objects or scene elements from the 3D reconstruction. This is achieved by introducing a new model which we called Voxel-CRF. The Voxel-CRF model is based on the idea of constructing a conditional random field over a 3D volume of interest which captures the semantic and 3D geometric relationships among different elements (voxels) of the scene. Such model allows to jointly estimate (1) a dense voxel-based 3D reconstruction and (2) the semantic labels associated with each voxel even in presence of par- tial occlusions using an approximate yet efficient inference strategy. We evaluated our method on the challenging NYU Depth dataset (Version 1and 2). Experimental results show that our method achieves competitive accuracy in inferring scene semantics and visually appealing results in improving the quality of the 3D reconstruction. We also demonstrate an interesting application of object removal and scene completion from RGB-D images.</p><p>6 0.66277754 <a title="144-lsi-6" href="./iccv-2013-Characterizing_Layouts_of_Outdoor_Scenes_Using_Spatial_Topic_Processes.html">72 iccv-2013-Characterizing Layouts of Outdoor Scenes Using Spatial Topic Processes</a></p>
<p>7 0.62964457 <a title="144-lsi-7" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>8 0.62848485 <a title="144-lsi-8" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>9 0.61928779 <a title="144-lsi-9" href="./iccv-2013-Active_MAP_Inference_in_CRFs_for_Efficient_Semantic_Segmentation.html">42 iccv-2013-Active MAP Inference in CRFs for Efficient Semantic Segmentation</a></p>
<p>10 0.57207584 <a title="144-lsi-10" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>11 0.55613959 <a title="144-lsi-11" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<p>12 0.50348282 <a title="144-lsi-12" href="./iccv-2013-Data-Driven_3D_Primitives_for_Single_Image_Understanding.html">102 iccv-2013-Data-Driven 3D Primitives for Single Image Understanding</a></p>
<p>13 0.43728754 <a title="144-lsi-13" href="./iccv-2013-Scene_Collaging%3A_Analysis_and_Synthesis_of_Natural_Images_with_Semantic_Layers.html">375 iccv-2013-Scene Collaging: Analysis and Synthesis of Natural Images with Semantic Layers</a></p>
<p>14 0.43286732 <a title="144-lsi-14" href="./iccv-2013-Lifting_3D_Manhattan_Lines_from_a_Single_Image.html">250 iccv-2013-Lifting 3D Manhattan Lines from a Single Image</a></p>
<p>15 0.43054038 <a title="144-lsi-15" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>16 0.42748034 <a title="144-lsi-16" href="./iccv-2013-Sequential_Bayesian_Model_Update_under_Structured_Scene_Prior_for_Semantic_Road_Scenes_Labeling.html">386 iccv-2013-Sequential Bayesian Model Update under Structured Scene Prior for Semantic Road Scenes Labeling</a></p>
<p>17 0.42664689 <a title="144-lsi-17" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>18 0.42225549 <a title="144-lsi-18" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>19 0.42222679 <a title="144-lsi-19" href="./iccv-2013-Efficient_Image_Dehazing_with_Boundary_Constraint_and_Contextual_Regularization.html">135 iccv-2013-Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</a></p>
<p>20 0.40116739 <a title="144-lsi-20" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.061), (7, 0.022), (12, 0.01), (26, 0.102), (31, 0.042), (34, 0.046), (40, 0.017), (42, 0.087), (48, 0.01), (55, 0.166), (64, 0.038), (73, 0.045), (89, 0.25), (95, 0.019), (98, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93073207 <a title="144-lda-1" href="./iccv-2013-Joint_Optimization_for_Consistent_Multiple_Graph_Matching.html">224 iccv-2013-Joint Optimization for Consistent Multiple Graph Matching</a></p>
<p>Author: Junchi Yan, Yu Tian, Hongyuan Zha, Xiaokang Yang, Ya Zhang, Stephen M. Chu</p><p>Abstract: The problem of graph matching in general is NP-hard and approaches have been proposed for its suboptimal solution, most focusing on finding the one-to-one node mapping between two graphs. A more general and challenging problem arises when one aims to find consistent mappings across a number of graphs more than two. Conventional graph pair matching methods often result in mapping inconsistency since the mapping between two graphs can either be determined by pair mapping or by an additional anchor graph. To address this issue, a novel formulation is derived which is maximized via alternating optimization. Our method enjoys several advantages: 1) the mappings are jointly optimized rather than sequentially performed by applying pair matching, allowing the global affinity information across graphs can be propagated and explored; 2) the number of concerned variables to optimize is in linear with the number of graphs, being superior to local pair matching resulting in O(n2) variables; 3) the mapping consistency constraints are analytically satisfied during optimization; and 4) off-the-shelf graph pair matching solvers can be reused under the proposed framework in an ‘out-of-thebox’ fashion. Competitive results on both the synthesized data and the real data are reported, by varying the level of deformation, outliers and edge densities. ∗Corresponding author. The work is supported by NSF IIS1116886, NSF IIS-1049694, NSFC 61129001/F010403 and the 111 Project (B07022). Yu Tian Shanghai Jiao Tong University Shanghai, China, 200240 yut ian @ s j tu . edu .cn Xiaokang Yang Shanghai Jiao Tong University Shanghai, China, 200240 xkyang@ s j tu .edu . cn Stephen M. Chu IBM T.J. Waston Research Center Yorktown Heights, NY USA, 10598 s chu @u s . ibm . com</p><p>2 0.92379791 <a title="144-lda-2" href="./iccv-2013-A_Unified_Rolling_Shutter_and_Motion_Blur_Model_for_3D_Visual_Registration.html">32 iccv-2013-A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration</a></p>
<p>Author: Maxime Meilland, Tom Drummond, Andrew I. Comport</p><p>Abstract: Motion blur and rolling shutter deformations both inhibit visual motion registration, whether it be due to a moving sensor or a moving target. Whilst both deformations exist simultaneously, no models have been proposed to handle them together. Furthermore, neither deformation has been consideredpreviously in the context of monocularfullimage 6 degrees of freedom registration or RGB-D structure and motion. As will be shown, rolling shutter deformation is observed when a camera moves faster than a single pixel in parallax between subsequent scan-lines. Blur is a function of the pixel exposure time and the motion vector. In this paper a complete dense 3D registration model will be derived to accountfor both motion blur and rolling shutter deformations simultaneously. Various approaches will be compared with respect to ground truth and live real-time performance will be demonstratedfor complex scenarios where both blur and shutter deformations are dominant.</p><p>3 0.91672063 <a title="144-lda-3" href="./iccv-2013-Geometric_Registration_Based_on_Distortion_Estimation.html">183 iccv-2013-Geometric Registration Based on Distortion Estimation</a></p>
<p>Author: Wei Zeng, Mayank Goswami, Feng Luo, Xianfeng Gu</p><p>Abstract: Surface registration plays a fundamental role in many applications in computer vision and aims at finding a oneto-one correspondence between surfaces. Conformal mapping based surface registration methods conformally map 2D/3D surfaces onto 2D canonical domains and perform the matching on the 2D plane. This registration framework reduces dimensionality, and the result is intrinsic to Riemannian metric and invariant under isometric deformation. However, conformal mapping will be affected by inconsistent boundaries and non-isometric deformations of surfaces. In this work, we quantify the effects of boundary variation and non-isometric deformation to conformal mappings, and give the theoretical upper bounds for the distortions of conformal mappings under these two factors. Besides giving the thorough theoretical proofs of the theorems, we verified them by concrete experiments using 3D human facial scans with dynamic expressions and varying boundaries. Furthermore, we used the distortion estimates for reducing search range in feature matching of surface registration applications. The experimental results are consistent with the theoreticalpredictions and also demonstrate the performance improvements in feature tracking.</p><p>4 0.91630018 <a title="144-lda-4" href="./iccv-2013-Rolling_Shutter_Stereo.html">363 iccv-2013-Rolling Shutter Stereo</a></p>
<p>Author: Olivier Saurer, Kevin Köser, Jean-Yves Bouguet, Marc Pollefeys</p><p>Abstract: A huge fraction of cameras used nowadays is based on CMOS sensors with a rolling shutter that exposes the image line by line. For dynamic scenes/cameras this introduces undesired effects like stretch, shear and wobble. It has been shown earlier that rotational shake induced rolling shutter effects in hand-held cell phone capture can be compensated based on an estimate of the camera rotation. In contrast, we analyse the case of significant camera motion, e.g. where a bypassing streetlevel capture vehicle uses a rolling shutter camera in a 3D reconstruction framework. The introduced error is depth dependent and cannot be compensated based on camera motion/rotation alone, invalidating also rectification for stereo camera systems. On top, significant lens distortion as often present in wide angle cameras intertwines with rolling shutter effects as it changes the time at which a certain 3D point is seen. We show that naive 3D reconstructions (assuming global shutter) will deliver biased geometry already for very mild assumptions on vehicle speed and resolution. We then develop rolling shutter dense multiview stereo algorithms that solve for time of exposure and depth at the same time, even in the presence of lens distortion and perform an evaluation on ground truth laser scan models as well as on real street-level data.</p><p>5 0.91169441 <a title="144-lda-5" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>Author: Feng Liu, Yuzhen Niu, Hailin Jin</p><p>Abstract: Shaky stereoscopic video is not only unpleasant to watch but may also cause 3D fatigue. Stabilizing the left and right view of a stereoscopic video separately using a monocular stabilization method tends to both introduce undesirable vertical disparities and damage horizontal disparities, which may destroy the stereoscopic viewing experience. In this paper, we present a joint subspace stabilization method for stereoscopic video. We prove that the low-rank subspace constraint for monocular video [10] also holds for stereoscopic video. Particularly, the feature trajectories from the left and right video share the same subspace. Based on this proof, we develop a stereo subspace stabilization method that jointly computes a common subspace from the left and right video and uses it to stabilize the two videos simultaneously. Our method meets the stereoscopic constraints without 3D reconstruction or explicit left-right correspondence. We test our method on a variety of stereoscopic videos with different scene content and camera motion. The experiments show that our method achieves high-quality stabilization for stereoscopic video in a robust and efficient way.</p><p>same-paper 6 0.90503675 <a title="144-lda-6" href="./iccv-2013-Estimating_the_3D_Layout_of_Indoor_Scenes_and_Its_Clutter_from_Depth_Sensors.html">144 iccv-2013-Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors</a></p>
<p>7 0.90149605 <a title="144-lda-7" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>8 0.90135109 <a title="144-lda-8" href="./iccv-2013-A_New_Image_Quality_Metric_for_Image_Auto-denoising.html">23 iccv-2013-A New Image Quality Metric for Image Auto-denoising</a></p>
<p>9 0.89061165 <a title="144-lda-9" href="./iccv-2013-Perceptual_Fidelity_Aware_Mean_Squared_Error.html">312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</a></p>
<p>10 0.86115181 <a title="144-lda-10" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>11 0.85990798 <a title="144-lda-11" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>12 0.85893118 <a title="144-lda-12" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>13 0.85808218 <a title="144-lda-13" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>14 0.85770702 <a title="144-lda-14" href="./iccv-2013-Fine-Grained_Categorization_by_Alignments.html">169 iccv-2013-Fine-Grained Categorization by Alignments</a></p>
<p>15 0.85587633 <a title="144-lda-15" href="./iccv-2013-Sequential_Bayesian_Model_Update_under_Structured_Scene_Prior_for_Semantic_Road_Scenes_Labeling.html">386 iccv-2013-Sequential Bayesian Model Update under Structured Scene Prior for Semantic Road Scenes Labeling</a></p>
<p>16 0.855156 <a title="144-lda-16" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>17 0.85373092 <a title="144-lda-17" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>18 0.85350704 <a title="144-lda-18" href="./iccv-2013-Image_Guided_Depth_Upsampling_Using_Anisotropic_Total_Generalized_Variation.html">209 iccv-2013-Image Guided Depth Upsampling Using Anisotropic Total Generalized Variation</a></p>
<p>19 0.85221702 <a title="144-lda-19" href="./iccv-2013-A_Joint_Intensity_and_Depth_Co-sparse_Analysis_Model_for_Depth_Map_Super-resolution.html">18 iccv-2013-A Joint Intensity and Depth Co-sparse Analysis Model for Depth Map Super-resolution</a></p>
<p>20 0.85221374 <a title="144-lda-20" href="./iccv-2013-Random_Forests_of_Local_Experts_for_Pedestrian_Detection.html">336 iccv-2013-Random Forests of Local Experts for Pedestrian Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
