<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>202 iccv-2013-How Do You Tell a Blackbird from a Crow?</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-202" href="#">iccv2013-202</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>202 iccv-2013-How Do You Tell a Blackbird from a Crow?</h1>
<br/><p>Source: <a title="iccv-2013-202-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Berg_How_Do_You_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Thomas Berg, Peter N. Belhumeur</p><p>Abstract: How do you tell a blackbirdfrom a crow? There has been great progress toward automatic methods for visual recognition, including fine-grained visual categorization in which the classes to be distinguished are very similar. In a task such as bird species recognition, automatic recognition systems can now exceed the performance of non-experts – most people are challenged to name a couple dozen bird species, let alone identify them. This leads us to the question, “Can a recognition system show humans what to look for when identifying classes (in this case birds)? ” In the context of fine-grained visual categorization, we show that we can automatically determine which classes are most visually similar, discover what visual features distinguish very similar classes, and illustrate the key features in a way meaningful to humans. Running these methods on a dataset of bird images, we can generate a visual field guide to birds which includes a tree of similarity that displays the similarity relations between all species, pages for each species showing the most similar other species, and pages for each pair of similar species illustrating their differences.</p><p>Reference: <a title="iccv-2013-202-reference" href="../iccv2013_reference/iccv-2013-How_Do_You_Tell_a_Blackbird_from_a_Crow%3F_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 There has been great progress toward automatic methods for visual recognition, including fine-grained visual categorization in which the classes to be distinguished are very similar. [sent-5, score-0.255]
</p><p>2 In a task such as bird species recognition, automatic recognition systems can now exceed the performance of non-experts – most people are challenged to name a couple dozen bird species, let alone identify them. [sent-6, score-1.034]
</p><p>3 This leads us to the question, “Can a recognition system show humans what to look for when identifying classes (in this case birds)? [sent-7, score-0.177]
</p><p>4 ” In the context of fine-grained visual categorization, we show that we can automatically determine which classes are most visually similar, discover what visual features distinguish very similar classes, and illustrate the key features in a way meaningful to humans. [sent-8, score-0.356]
</p><p>5 Introduction How do you tell a blackbird from a crow? [sent-11, score-0.229]
</p><p>6 From a computer vision standpoint, this is in the domain of fine-grained visual categorization, in which we must recognize a set of similar classes and distinguish them from each other. [sent-16, score-0.192]
</p><p>7 To contrast this with general object recognition, we must distinguish blackbirds from crows rather than birds from bicycles. [sent-17, score-0.394]
</p><p>8 There is good, recent progress on this problem, including work on bird species identification in particular (e. [sent-18, score-0.91]
</p><p>9 These methods learn classifiers which can (to some standard of accuracy) recognize bird species but do not explicitly tell us what to look for to recognize This work was supported by NSF award 111663 1, ONR award N0001408-1-0638, and Gordon and Betty Moore Foundation grant 2987. [sent-21, score-0.952]
</p><p>10 (a) For any bird species (here the red-winged blackbird, at center), we display the other species with most similar appearance. [sent-219, score-1.546]
</p><p>11 (b) For each similar species (here the American crow), we generate a “visual field guide” page highlighting differences between the species. [sent-221, score-0.768]
</p><p>12 visually similar  to  the red-winged blackbird (see Figure 1) are in blue, and those similar  bird species on our own. [sent-222, score-1.062]
</p><p>13 We do this by learning which classes appear similar, discovering features that discriminate between similar classes, and illustrating these features with a series of carefully chosen sample images annotated to indicate the relevant features. [sent-225, score-0.202]
</p><p>14 We can assemble these visualizations into an automatically-generated digital field guide to birds, showing which species are similar and what a birder should look for to tell them apart. [sent-226, score-0.98]
</p><p>15 Example figures from a page we have generated for such a guide are shown in Figure 1. [sent-227, score-0.168]
</p><p>16 In addition to the visualizations in these figures, we borrow a technique from phylogenetics, the study of the evolutionary relations between species, to generate a tree of visual similarity. [sent-228, score-0.408]
</p><p>17 Arranged in a wheel, as shown in Figure 2, this tree is suitable as a browsing interface for the field guide, allowing a user to quickly see each species and all the species similar to it. [sent-229, score-1.58]
</p><p>18 We compare our similarity-based tree with the phylogenetic “tree of life,” which describes the evolutionary relations between bird species. [sent-230, score-0.722]
</p><p>19 Places where the trees are not in agreement pairs of species that are close in the similarity tree but far in the evolutionary tree are of special interest, as these may be examples of convergent evolution [11], where similar traits arise independently in species that are not closely related. [sent-231, score-2.034]
</p><p>20 We base our similarity calculations on the “part-based one-vs-one features” (POOFs) of [1], for two reasons. [sent-232, score-0.114]
</p><p>21 First –  –  is the POOFs’ strong performance on fine-grained categorization; in particular they have done well on bird species recognition. [sent-233, score-0.86]
</p><p>22 Finegrained classification encourages part-based approaches because the classes under consideration have many of the same parts (for birds, every species has a beak, wings, legs, and eyes) so it is possible to distinguish classes by the appearance of corresponding parts. [sent-235, score-0.981]
</p><p>23 Experiments by Tversky and Hemenway [24] suggest that people also use properties of parts to distinguish similar classes, and bird guides often describe species in terms of their parts, as shown in Figure 3. [sent-236, score-1.008]
</p><p>24 puter vision techniques, in particular methods of finegrained visual categorization, to illustrate the differences between similar classes. [sent-242, score-0.096]
</p><p>25 We explore the relation between visual similarity and phylogeny in bird species. [sent-248, score-0.32]
</p><p>26 Species which are visually similar but not close in the evolutionary “tree of life” may be examples of convergent evolution. [sent-249, score-0.247]
</p><p>27 Related Work There is a good deal of recent work on fine-grained categorization, much of it dealing with species or breed recognition of e. [sent-251, score-0.711]
</p><p>28 , trees [13], flowers [16], butterflies [8, 27], dogs [15, 18, 19], and birds [1, 6, 8, 9, 29, 30]. [sent-253, score-0.195]
</p><p>29 Work on fine-grained visual categorization with “humans in the loop” [3, 25], proposes a model in which a  person answers questions to assist a recognition system that makes a final identification. [sent-256, score-0.098]
</p><p>30 This is similar to our method for finding regions to annotate in our illustrative images, but they work with a single image to find its distinctive regions, while we work with two classes of image to find the most discriminative regions. [sent-264, score-0.144]
</p><p>31 [6] found discriminative regions in bird by explicit human labeling in the guise of a game. [sent-267, score-0.174]
</p><p>32 This could be used to provide supplementary, nonpart-based text descriptions of species differences in our visual field guide. [sent-272, score-0.778]
</p><p>33 Visual Similarity Our goal is, in a set of visually similar classes, to determine which classes are most similar to each other, and among those most similar classes, to understand and visualize what it is that still distinguishes them. [sent-274, score-0.165]
</p><p>34 To make this concrete, we consider the problem of bird species recognition, using the Caltech-UCSD Birds 200 dataset (CUBS-200) [26]. [sent-275, score-0.86]
</p><p>35 We use the 2011 version of the dataset, which includes 11,788 images of 200 bird species, annotated with the locations of 15 parts (beak, forehead, crown, throat, breast, belly, nape, back, tail, and left and right eyes, wings, and legs). [sent-276, score-0.212]
</p><p>36 With many examples of species with similar appearance, and also many species with widely varying appearance, the dataset presents a difficult recognition task. [sent-278, score-1.372]
</p><p>37 A Vocabulary of Part-based One-vs-One Features (POOFs) The first step toward our goal is to construct a vocabulary of features suitable for differentiating among classes in our domain. [sent-281, score-0.157]
</p><p>38 For example, a POOF may discriminate between the red-winged blackbird and the rusty blackbird, based on color histograms at the wing after alignment by the wing and the eye. [sent-284, score-0.228]
</p><p>39 To build a POOF, all the training images of the two classes are rotated and scaled to put the two chosen parts in fixed locations. [sent-285, score-0.158]
</p><p>40 We extract the base feature from each tile, concatenate these features to get a feature vector for the image, and train a linear SVM 11 to distinguish the two classes. [sent-287, score-0.178]
</p><p>41 If we discover that two bird species are well-separated by a color histogrambased POOF aligned by the beak and the back, and the SVM weights are large at the grid cells around the beak, we can interpret this as “These two species are differentiated by the color of the beak. [sent-298, score-1.691]
</p><p>42 It would be difficult and not useful to describe the particular features that distinguish them; any feature you care to look at will suffice. [sent-303, score-0.153]
</p><p>43 The interesting problem is to find what details distinguish classes of similar appearance. [sent-304, score-0.159]
</p><p>44 To do this we must first determine which classes are similar to each other. [sent-305, score-0.12]
</p><p>45 An L1 or L2 distance-based similarity in this space is appealing for its simplicity, but considers all features to be equally important, which is unlikely to be a good idea. [sent-308, score-0.099]
</p><p>46 Some of these choices will be good, looking at two species that differ in a clear way at the parts being considered. [sent-310, score-0.724]
</p><p>47 By applying this image similarity measure to mean feature vectors over all the images in a class, we obtain a similarity measure between classes, with which we can determine the most similar class to any given class. [sent-315, score-0.193]
</p><p>48 The redwinged blackbird and its five most similar species are shown at the top of Figure 1. [sent-316, score-0.864]
</p><p>49 Choosing Discriminative Features Given a pair of very similar classes, we are now interested in discovering what features can be used to tell them apart. [sent-319, score-0.125]
</p><p>50 With the birds dataset, with twelve parts and two low-level features, there are 264 candidate features. [sent-321, score-0.233]
</p><p>51 We rank the features by their discriminativeness, defining the discriminativeness of feature f as −  df=(μ2σ−1σ μ21)2,  (1)  where μ1 and μ2 are the mean feature values for the two classes, and σ1 and σ2 are the corresponding standard deviations. [sent-322, score-0.122]
</p><p>52 Visualizing the Features Once we have determined which features are most useful to distinguish between a pair of classes, we would like to present this information in a format that will help a viewer understand what he should look for. [sent-327, score-0.176]
</p><p>53 We present each feature as a pair of illustrative images, one from each species, with the region of interest indicated in the two images. [sent-328, score-0.098]
</p><p>54 If the feature is beak color, where one class has a yellow beak and the other gray, then the images must have the beak clearly visible, with the beak distinctly yellow in one and gray in the other. [sent-332, score-0.473]
</p><p>55 If the yellow and gray-beaked species above can both be either brown or black, it is misleading to show one brown and one black, as this difference does not distinguish the classes. [sent-335, score-0.747]
</p><p>56 Taking c1 and c2 as the classes associated with positive and negative feature values respectively, let b1 be the 75th percentile of feature values on c1, and let b2 be the 25th percentile of feature values on c2. [sent-343, score-0.221]
</p><p>57 We take these exaggerated, but not extreme feature values as “best,” and attempt to minimize  +  +  F(I1, I2) = (1 |f(I1) − b1|) (1 |f(I2) − b2|) , (2) where I1 and I2 are the candidate illustrative images from classes c1 and c2, and f() is the feature to be illustrated. [sent-344, score-0.194]
</p><p>58 To achieve the second goal, we consider an additional set of features, based on POOFs trained on classes other than c1 and c2. [sent-345, score-0.098]
</p><p>59 We use the 5000 POOFs used to determine interclass similarity in Section 3. [sent-346, score-0.13]
</p><p>60 We resize the images so that in each, the mean squared distance between parts is 1, then find the best fit similarity transformation from the scaled locations  in image I1 to the scaled locations x2 in image I2. [sent-349, score-0.155]
</p><p>61 A Visual Field Guide to Birds As a direct application of the techniques in Section 3, we can construct a visual field guide to birds. [sent-360, score-0.19]
</p><p>62 While this guide will not have the notes on habitat and behavior of a traditional guide, it will have a couple advantages. [sent-361, score-0.122]
</p><p>63 While a tra-  ditional, hand-assembled guide will have an entry for each  Figure 6. [sent-364, score-0.157]
</p><p>64 In both, rows/columns are in order of a depthfirst traversal of the evolutionary tree, ensuring a clear structure in (b). [sent-368, score-0.175]
</p><p>65 The large dashed black box corresponds to the passerine birds (“perching birds,” mostly songbirds), while the small solid black box holds similarities between crows and ravens on the y-axis and blackbirds and cowbirds on the x-axis. [sent-369, score-0.397]
</p><p>66 We envision our field guide with a main entry for each species. [sent-372, score-0.192]
</p><p>67 The main entry shows the species in question, and the top k most similar other species (we use k = 5) as determined by the method of Section 3. [sent-374, score-1.407]
</p><p>68 Selecting one of the similar species will lead to a pair entry illustrating the differences between the two species as described in Sections 3. [sent-376, score-1.489]
</p><p>69 A Tree of Visual Similarity Visual similarity as estimated from the POOFs is the basis for our visual field guide. [sent-383, score-0.141]
</p><p>70 If we say a blackbird is more like a crow than like a raven, who can say we are wrong? [sent-385, score-0.283]
</p><p>71 Species close to each other in the tree of life are in a sense “more similar” than species that are not close, although this will not necessarily correspond to visual similarity. [sent-387, score-0.946]
</p><p>72 The scientific community has not reached consensus on the complete structure of the tree of life, or even the subtree containing just the birds in CUBS-200. [sent-388, score-0.34]
</p><p>73 [12] proposed the first complete tree of life for all 9993 extant bird 13  ? [sent-391, score-0.436]
</p><p>74 Visual field guide  pages  for the Kentucky warbler. [sent-1443, score-0.157]
</p><p>75 14  blackbird  are  in blue, and those similar  to  the Kentucky warbler  are  in red. [sent-1444, score-0.243]
</p><p>76 Although the American  crow  and  common raven are  visually  similar to blackbirds, they are not close in terms of evolution. [sent-1445, score-0.169]
</p><p>77 Rank Species Pair 1Gadwall vs Pacific Loon 2 6 11 16  Hooded Merganser vs Pigeon Guillemot Red-breasted Merganser vs Eared Grebe Least Auklet vs Whip poor Will Black billed Cuckoo vs Mockingbird  Table 1. [sent-1446, score-0.175]
</p><p>78 Species pairs with high visual and low phylogenetic similarity. [sent-1447, score-0.251]
</p><p>79 Pruning this tree to include only the species in CUBS-200 yields the tree shown in Figure 5 (produced  in part with code from [14]). [sent-1449, score-0.976]
</p><p>80 This tree shows the overall phylogenetic similarity relations between bird species. [sent-1450, score-0.62]
</p><p>81 This tree, however, is based on visual similarity rather than phylogenetic similarity. [sent-1452, score-0.304]
</p><p>82 Producing a tree from a similarity matrix is a basic operation in the study of phylogeny, for which standard methods exist (note the tree of life in Figure 5 is based on more advanced techniques that use additional data beyond a similarity matrix). [sent-1453, score-0.518]
</p><p>83 ity matrix of the bird species using the POOFs, then apply one of these standard methods, Saitou and Nei’s “neighborjoining” [20], to get a tree based not on evolutionary history but on visual similarity. [sent-1456, score-1.213]
</p><p>84 In an interactive form, it will allow a user to scroll through the birds in an order that respects similarity and shows a hierarchy of groups of similar birds. [sent-1458, score-0.268]
</p><p>85 We can compare the similarity-based tree in Figure 2 with the evolutionary tree in Figure 5. [sent-1459, score-0.465]
</p><p>86 They generally agree  as to which species are similar, but there are exceptions. [sent-1460, score-0.686]
</p><p>87 For example, crows are close to blackbirds in the similarity tree, but the evolutionary tree shows that they are not closely related. [sent-1461, score-0.552]
</p><p>88 Such cases may be examples of convergent evolution, in which two species independently develop similar traits. [sent-1462, score-0.734]
</p><p>89 15 We can find such species pairs, with high visual similarity and low phylogenetic similarity, in a systematic way. [sent-1463, score-0.99]
</p><p>90 The phylogenetic similarity between two species can be quantified as the length of shared evolutionary history, i. [sent-1464, score-1.132]
</p><p>91 , the path length, in years, from the root of the evolutionary tree to the species’ most recent common ancestor (techniques such as the neighbor-joining algorithm [20] also use this as a similarity measure). [sent-1466, score-0.393]
</p><p>92 Figure 6 (a) shows a similarity matrix calculated in this way for the 200 bird species, with the corresponding matrix based on visual similarity as Figure 6 (b). [sent-1467, score-0.353]
</p><p>93 species pairs by visual similarity (most similar first) and by phylogenetic difference (least similar first). [sent-1472, score-1.01]
</p><p>94 We then list all species pairs in order of the sum of these ranks. [sent-1473, score-0.706]
</p><p>95 Table 1shows the top five pairs, excluding pairs where one of the species has already appeared on the list to avoid excessive repetition (as the pacific loon scores highly when paired with the gadwall, it will also score highly with all near relatives of the gadwall). [sent-1474, score-0.793]
</p><p>96 The top ranked pair is a duck and a loon, two species the author had mistakenly as-  sumed Figure cludes images  were closely related based on their visual similarity. [sent-1475, score-0.767]
</p><p>97 Here we exploit a setting in which computers can do better than typical humans finegrained categorization in a specialized domain to show how progress in computer vision can be turned to helping humans understand the relations between the categories. [sent-1480, score-0.257]
</p><p>98 Leafsnap: A computer vision system for automatic plant species identification. [sent-1589, score-0.686]
</p><p>99 Interactive tree oflife (itol): An online tool for phylogenetic tree display and annotation. [sent-1595, score-0.488]
</p><p>100 The neighbor-joining method: A new method for reconstructing phylogenetic trees. [sent-1638, score-0.198]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('species', 0.686), ('poofs', 0.297), ('phylogenetic', 0.198), ('birds', 0.195), ('blackbird', 0.178), ('evolutionary', 0.175), ('bird', 0.174), ('tree', 0.145), ('poof', 0.139), ('guide', 0.122), ('beak', 0.112), ('crow', 0.105), ('classes', 0.098), ('life', 0.082), ('blackbirds', 0.079), ('similarity', 0.073), ('kentucky', 0.07), ('warbler', 0.065), ('categorization', 0.065), ('distinguish', 0.061), ('crows', 0.059), ('loon', 0.059), ('tell', 0.051), ('guides', 0.049), ('convergent', 0.048), ('illustrative', 0.046), ('discriminativeness', 0.046), ('look', 0.041), ('base', 0.041), ('gadwall', 0.04), ('jetz', 0.04), ('merganser', 0.04), ('phylogeny', 0.04), ('raven', 0.04), ('saitou', 0.04), ('sibley', 0.04), ('tversky', 0.04), ('yanai', 0.04), ('finegrained', 0.039), ('humans', 0.038), ('parts', 0.038), ('field', 0.035), ('evolution', 0.035), ('interclass', 0.035), ('extant', 0.035), ('entry', 0.035), ('vs', 0.035), ('vocabulary', 0.033), ('discover', 0.033), ('visual', 0.033), ('exemplify', 0.032), ('wings', 0.032), ('black', 0.032), ('branson', 0.031), ('wah', 0.031), ('illustrating', 0.031), ('exaggerated', 0.031), ('relations', 0.03), ('tiles', 0.029), ('kg', 0.029), ('doersch', 0.029), ('berg', 0.028), ('pacific', 0.028), ('cornell', 0.028), ('browsing', 0.028), ('pair', 0.027), ('lda', 0.027), ('kf', 0.027), ('perona', 0.026), ('farrell', 0.026), ('progress', 0.026), ('features', 0.026), ('kh', 0.025), ('wing', 0.025), ('breed', 0.025), ('feature', 0.025), ('visualizations', 0.025), ('differences', 0.024), ('biology', 0.024), ('thomas', 0.024), ('percentile', 0.024), ('visually', 0.024), ('identification', 0.024), ('shrivastava', 0.023), ('page', 0.023), ('figures', 0.023), ('parikh', 0.023), ('columbia', 0.022), ('attribute', 0.022), ('determine', 0.022), ('visualizing', 0.022), ('scaled', 0.022), ('discovering', 0.021), ('closely', 0.021), ('distinguishing', 0.021), ('understand', 0.021), ('collins', 0.021), ('welinder', 0.021), ('digital', 0.02), ('pairs', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="202-tfidf-1" href="./iccv-2013-How_Do_You_Tell_a_Blackbird_from_a_Crow%3F.html">202 iccv-2013-How Do You Tell a Blackbird from a Crow?</a></p>
<p>Author: Thomas Berg, Peter N. Belhumeur</p><p>Abstract: How do you tell a blackbirdfrom a crow? There has been great progress toward automatic methods for visual recognition, including fine-grained visual categorization in which the classes to be distinguished are very similar. In a task such as bird species recognition, automatic recognition systems can now exceed the performance of non-experts – most people are challenged to name a couple dozen bird species, let alone identify them. This leads us to the question, “Can a recognition system show humans what to look for when identifying classes (in this case birds)? ” In the context of fine-grained visual categorization, we show that we can automatically determine which classes are most visually similar, discover what visual features distinguish very similar classes, and illustrate the key features in a way meaningful to humans. Running these methods on a dataset of bird images, we can generate a visual field guide to birds which includes a tree of similarity that displays the similarity relations between all species, pages for each species showing the most similar other species, and pages for each pair of similar species illustrating their differences.</p><p>2 0.23497003 <a title="202-tfidf-2" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>Author: Jiongxin Liu, Peter N. Belhumeur</p><p>Abstract: In this paper, we propose a novel approach for bird part localization, targeting fine-grained categories with wide variations in appearance due to different poses (including aspect and orientation) and subcategories. As it is challenging to represent such variations across a large set of diverse samples with tractable parametric models, we turn to individual exemplars. Specifically, we extend the exemplarbased models in [4] by enforcing pose and subcategory consistency at the parts. During training, we build posespecific detectors scoring part poses across subcategories, and subcategory-specific detectors scoring part appearance across poses. At the testing stage, likely exemplars are matched to the image, suggesting part locations whose pose and subcategory consistency are well-supported by the image cues. From these hypotheses, part configuration can be predicted with very high accuracy. Experimental results demonstrate significantperformance gainsfrom our method on an extensive dataset: CUB-200-2011 [30], for both localization and classification tasks.</p><p>3 0.16605599 <a title="202-tfidf-3" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>Author: Lingxi Xie, Qi Tian, Richang Hong, Shuicheng Yan, Bo Zhang</p><p>Abstract: As a special topic in computer vision, , fine-grained visual categorization (FGVC) has been attracting growing attention these years. Different with traditional image classification tasks in which objects have large inter-class variation, the visual concepts in the fine-grained datasets, such as hundreds of bird species, often have very similar semantics. Due to the large inter-class similarity, it is very difficult to classify the objects without locating really discriminative features, therefore it becomes more important for the algorithm to make full use of the part information in order to train a robust model. In this paper, we propose a powerful flowchart named Hierarchical Part Matching (HPM) to cope with finegrained classification tasks. We extend the Bag-of-Features (BoF) model by introducing several novel modules to integrate into image representation, including foreground inference and segmentation, Hierarchical Structure Learn- ing (HSL), and Geometric Phrase Pooling (GPP). We verify in experiments that our algorithm achieves the state-ofthe-art classification accuracy in the Caltech-UCSD-Birds200-2011 dataset by making full use of the ground-truth part annotations.</p><p>4 0.14330173 <a title="202-tfidf-4" href="./iccv-2013-Fine-Grained_Categorization_by_Alignments.html">169 iccv-2013-Fine-Grained Categorization by Alignments</a></p>
<p>Author: E. Gavves, B. Fernando, C.G.M. Snoek, A.W.M. Smeulders, T. Tuytelaars</p><p>Abstract: The aim of this paper is fine-grained categorization without human interaction. Different from prior work, which relies on detectors for specific object parts, we propose to localize distinctive details by roughly aligning the objects using just the overall shape, since implicit to fine-grained categorization is the existence of a super-class shape shared among all classes. The alignments are then used to transfer part annotations from training images to test images (supervised alignment), or to blindly yet consistently segment the object in a number of regions (unsupervised alignment). We furthermore argue that in the distinction of finegrained sub-categories, classification-oriented encodings like Fisher vectors are better suited for describing localized information than popular matching oriented features like HOG. We evaluate the method on the CU-2011 Birds and Stanford Dogs fine-grained datasets, outperforming the state-of-the-art.</p><p>5 0.14221278 <a title="202-tfidf-5" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>Author: Ning Zhang, Ryan Farrell, Forrest Iandola, Trevor Darrell</p><p>Abstract: Recognizing objects in fine-grained domains can be extremely challenging due to the subtle differences between subcategories. Discriminative markings are often highly localized, leading traditional object recognition approaches to struggle with the large pose variation often present in these domains. Pose-normalization seeks to align training exemplars, either piecewise by part or globally for the whole object, effectively factoring out differences in pose and in viewing angle. Prior approaches relied on computationally-expensive filter ensembles for part localization and required extensive supervision. This paper proposes two pose-normalized descriptors based on computationally-efficient deformable part models. The first leverages the semantics inherent in strongly-supervised DPM parts. The second exploits weak semantic annotations to learn cross-component correspondences, computing pose-normalized descriptors from the latent parts of a weakly-supervised DPM. These representations enable pooling across pose and viewpoint, in turn facilitating tasks such as fine-grained recognition and attribute prediction. Experiments conducted on the Caltech-UCSD Birds 200 dataset and Berkeley Human Attribute dataset demonstrate significant improvements over state-of-art algorithms.</p><p>6 0.11771854 <a title="202-tfidf-6" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>7 0.10005351 <a title="202-tfidf-7" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>8 0.087146983 <a title="202-tfidf-8" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>9 0.073976249 <a title="202-tfidf-9" href="./iccv-2013-From_Large_Scale_Image_Categorization_to_Entry-Level_Categories.html">176 iccv-2013-From Large Scale Image Categorization to Entry-Level Categories</a></p>
<p>10 0.069313496 <a title="202-tfidf-10" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>11 0.067327142 <a title="202-tfidf-11" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>12 0.052265055 <a title="202-tfidf-12" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>13 0.050038185 <a title="202-tfidf-13" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>14 0.049010582 <a title="202-tfidf-14" href="./iccv-2013-Find_the_Best_Path%3A_An_Efficient_and_Accurate_Classifier_for_Image_Hierarchies.html">165 iccv-2013-Find the Best Path: An Efficient and Accurate Classifier for Image Hierarchies</a></p>
<p>15 0.047811288 <a title="202-tfidf-15" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>16 0.047185563 <a title="202-tfidf-16" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>17 0.045511827 <a title="202-tfidf-17" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>18 0.041349851 <a title="202-tfidf-18" href="./iccv-2013-HOGgles%3A_Visualizing_Object_Detection_Features.html">189 iccv-2013-HOGgles: Visualizing Object Detection Features</a></p>
<p>19 0.041174285 <a title="202-tfidf-19" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>20 0.039604001 <a title="202-tfidf-20" href="./iccv-2013-A_Non-parametric_Bayesian_Network_Prior_of_Human_Pose.html">24 iccv-2013-A Non-parametric Bayesian Network Prior of Human Pose</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.112), (1, 0.046), (2, -0.002), (3, -0.053), (4, 0.08), (5, -0.011), (6, -0.024), (7, -0.013), (8, 0.014), (9, -0.036), (10, 0.035), (11, 0.02), (12, -0.036), (13, -0.059), (14, -0.032), (15, 0.019), (16, 0.027), (17, 0.016), (18, 0.034), (19, -0.0), (20, 0.036), (21, 0.038), (22, 0.007), (23, 0.068), (24, -0.026), (25, 0.081), (26, 0.002), (27, -0.023), (28, 0.054), (29, 0.002), (30, 0.089), (31, -0.138), (32, -0.035), (33, -0.046), (34, -0.083), (35, 0.006), (36, 0.002), (37, -0.096), (38, 0.016), (39, -0.052), (40, -0.077), (41, -0.061), (42, 0.059), (43, 0.007), (44, -0.074), (45, 0.003), (46, -0.028), (47, 0.053), (48, 0.095), (49, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90906239 <a title="202-lsi-1" href="./iccv-2013-How_Do_You_Tell_a_Blackbird_from_a_Crow%3F.html">202 iccv-2013-How Do You Tell a Blackbird from a Crow?</a></p>
<p>Author: Thomas Berg, Peter N. Belhumeur</p><p>Abstract: How do you tell a blackbirdfrom a crow? There has been great progress toward automatic methods for visual recognition, including fine-grained visual categorization in which the classes to be distinguished are very similar. In a task such as bird species recognition, automatic recognition systems can now exceed the performance of non-experts – most people are challenged to name a couple dozen bird species, let alone identify them. This leads us to the question, “Can a recognition system show humans what to look for when identifying classes (in this case birds)? ” In the context of fine-grained visual categorization, we show that we can automatically determine which classes are most visually similar, discover what visual features distinguish very similar classes, and illustrate the key features in a way meaningful to humans. Running these methods on a dataset of bird images, we can generate a visual field guide to birds which includes a tree of similarity that displays the similarity relations between all species, pages for each species showing the most similar other species, and pages for each pair of similar species illustrating their differences.</p><p>2 0.74862087 <a title="202-lsi-2" href="./iccv-2013-Fine-Grained_Categorization_by_Alignments.html">169 iccv-2013-Fine-Grained Categorization by Alignments</a></p>
<p>Author: E. Gavves, B. Fernando, C.G.M. Snoek, A.W.M. Smeulders, T. Tuytelaars</p><p>Abstract: The aim of this paper is fine-grained categorization without human interaction. Different from prior work, which relies on detectors for specific object parts, we propose to localize distinctive details by roughly aligning the objects using just the overall shape, since implicit to fine-grained categorization is the existence of a super-class shape shared among all classes. The alignments are then used to transfer part annotations from training images to test images (supervised alignment), or to blindly yet consistently segment the object in a number of regions (unsupervised alignment). We furthermore argue that in the distinction of finegrained sub-categories, classification-oriented encodings like Fisher vectors are better suited for describing localized information than popular matching oriented features like HOG. We evaluate the method on the CU-2011 Birds and Stanford Dogs fine-grained datasets, outperforming the state-of-the-art.</p><p>3 0.71953446 <a title="202-lsi-3" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>Author: Lingxi Xie, Qi Tian, Richang Hong, Shuicheng Yan, Bo Zhang</p><p>Abstract: As a special topic in computer vision, , fine-grained visual categorization (FGVC) has been attracting growing attention these years. Different with traditional image classification tasks in which objects have large inter-class variation, the visual concepts in the fine-grained datasets, such as hundreds of bird species, often have very similar semantics. Due to the large inter-class similarity, it is very difficult to classify the objects without locating really discriminative features, therefore it becomes more important for the algorithm to make full use of the part information in order to train a robust model. In this paper, we propose a powerful flowchart named Hierarchical Part Matching (HPM) to cope with finegrained classification tasks. We extend the Bag-of-Features (BoF) model by introducing several novel modules to integrate into image representation, including foreground inference and segmentation, Hierarchical Structure Learn- ing (HSL), and Geometric Phrase Pooling (GPP). We verify in experiments that our algorithm achieves the state-ofthe-art classification accuracy in the Caltech-UCSD-Birds200-2011 dataset by making full use of the ground-truth part annotations.</p><p>4 0.65772635 <a title="202-lsi-4" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>Author: Ning Zhang, Ryan Farrell, Forrest Iandola, Trevor Darrell</p><p>Abstract: Recognizing objects in fine-grained domains can be extremely challenging due to the subtle differences between subcategories. Discriminative markings are often highly localized, leading traditional object recognition approaches to struggle with the large pose variation often present in these domains. Pose-normalization seeks to align training exemplars, either piecewise by part or globally for the whole object, effectively factoring out differences in pose and in viewing angle. Prior approaches relied on computationally-expensive filter ensembles for part localization and required extensive supervision. This paper proposes two pose-normalized descriptors based on computationally-efficient deformable part models. The first leverages the semantics inherent in strongly-supervised DPM parts. The second exploits weak semantic annotations to learn cross-component correspondences, computing pose-normalized descriptors from the latent parts of a weakly-supervised DPM. These representations enable pooling across pose and viewpoint, in turn facilitating tasks such as fine-grained recognition and attribute prediction. Experiments conducted on the Caltech-UCSD Birds 200 dataset and Berkeley Human Attribute dataset demonstrate significant improvements over state-of-art algorithms.</p><p>5 0.61188275 <a title="202-lsi-5" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>Author: Jiongxin Liu, Peter N. Belhumeur</p><p>Abstract: In this paper, we propose a novel approach for bird part localization, targeting fine-grained categories with wide variations in appearance due to different poses (including aspect and orientation) and subcategories. As it is challenging to represent such variations across a large set of diverse samples with tractable parametric models, we turn to individual exemplars. Specifically, we extend the exemplarbased models in [4] by enforcing pose and subcategory consistency at the parts. During training, we build posespecific detectors scoring part poses across subcategories, and subcategory-specific detectors scoring part appearance across poses. At the testing stage, likely exemplars are matched to the image, suggesting part locations whose pose and subcategory consistency are well-supported by the image cues. From these hypotheses, part configuration can be predicted with very high accuracy. Experimental results demonstrate significantperformance gainsfrom our method on an extensive dataset: CUB-200-2011 [30], for both localization and classification tasks.</p><p>6 0.55824023 <a title="202-lsi-6" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>7 0.55367118 <a title="202-lsi-7" href="./iccv-2013-From_Large_Scale_Image_Categorization_to_Entry-Level_Categories.html">176 iccv-2013-From Large Scale Image Categorization to Entry-Level Categories</a></p>
<p>8 0.53685802 <a title="202-lsi-8" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>9 0.51856852 <a title="202-lsi-9" href="./iccv-2013-Codemaps_-_Segment%2C_Classify_and_Search_Objects_Locally.html">77 iccv-2013-Codemaps - Segment, Classify and Search Objects Locally</a></p>
<p>10 0.51390427 <a title="202-lsi-10" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<p>11 0.4910349 <a title="202-lsi-11" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>12 0.47940412 <a title="202-lsi-12" href="./iccv-2013-Detecting_Avocados_to_Zucchinis%3A_What_Have_We_Done%2C_and_Where_Are_We_Going%3F.html">109 iccv-2013-Detecting Avocados to Zucchinis: What Have We Done, and Where Are We Going?</a></p>
<p>13 0.46446627 <a title="202-lsi-13" href="./iccv-2013-Shufflets%3A_Shared_Mid-level_Parts_for_Fast_Object_Detection.html">390 iccv-2013-Shufflets: Shared Mid-level Parts for Fast Object Detection</a></p>
<p>14 0.446978 <a title="202-lsi-14" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>15 0.44503233 <a title="202-lsi-15" href="./iccv-2013-Building_Part-Based_Object_Detectors_via_3D_Geometry.html">66 iccv-2013-Building Part-Based Object Detectors via 3D Geometry</a></p>
<p>16 0.43939358 <a title="202-lsi-16" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>17 0.43780822 <a title="202-lsi-17" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>18 0.42425749 <a title="202-lsi-18" href="./iccv-2013-A_Method_of_Perceptual-Based_Shape_Decomposition.html">21 iccv-2013-A Method of Perceptual-Based Shape Decomposition</a></p>
<p>19 0.40286312 <a title="202-lsi-19" href="./iccv-2013-Find_the_Best_Path%3A_An_Efficient_and_Accurate_Classifier_for_Image_Hierarchies.html">165 iccv-2013-Find the Best Path: An Efficient and Accurate Classifier for Image Hierarchies</a></p>
<p>20 0.39745277 <a title="202-lsi-20" href="./iccv-2013-YouTube2Text%3A_Recognizing_and_Describing_Arbitrary_Activities_Using_Semantic_Hierarchies_and_Zero-Shot_Recognition.html">452 iccv-2013-YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.069), (4, 0.011), (7, 0.012), (26, 0.085), (31, 0.022), (34, 0.391), (42, 0.069), (64, 0.04), (73, 0.036), (77, 0.012), (89, 0.117)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73687685 <a title="202-lda-1" href="./iccv-2013-How_Do_You_Tell_a_Blackbird_from_a_Crow%3F.html">202 iccv-2013-How Do You Tell a Blackbird from a Crow?</a></p>
<p>Author: Thomas Berg, Peter N. Belhumeur</p><p>Abstract: How do you tell a blackbirdfrom a crow? There has been great progress toward automatic methods for visual recognition, including fine-grained visual categorization in which the classes to be distinguished are very similar. In a task such as bird species recognition, automatic recognition systems can now exceed the performance of non-experts – most people are challenged to name a couple dozen bird species, let alone identify them. This leads us to the question, “Can a recognition system show humans what to look for when identifying classes (in this case birds)? ” In the context of fine-grained visual categorization, we show that we can automatically determine which classes are most visually similar, discover what visual features distinguish very similar classes, and illustrate the key features in a way meaningful to humans. Running these methods on a dataset of bird images, we can generate a visual field guide to birds which includes a tree of similarity that displays the similarity relations between all species, pages for each species showing the most similar other species, and pages for each pair of similar species illustrating their differences.</p><p>2 0.71694767 <a title="202-lda-2" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>Author: Naman Turakhia, Devi Parikh</p><p>Abstract: When we look at an image, some properties or attributes of the image stand out more than others. When describing an image, people are likely to describe these dominant attributes first. Attribute dominance is a result of a complex interplay between the various properties present or absent in the image. Which attributes in an image are more dominant than others reveals rich information about the content of the image. In this paper we tap into this information by modeling attribute dominance. We show that this helps improve the performance of vision systems on a variety of human-centric applications such as zero-shot learning, image search and generating textual descriptions of images.</p><p>3 0.64436555 <a title="202-lda-3" href="./iccv-2013-Multi-scale_Topological_Features_for_Hand_Posture_Representation_and_Analysis.html">278 iccv-2013-Multi-scale Topological Features for Hand Posture Representation and Analysis</a></p>
<p>Author: Kaoning Hu, Lijun Yin</p><p>Abstract: In this paper, we propose a multi-scale topological feature representation for automatic analysis of hand posture. Such topological features have the advantage of being posture-dependent while being preserved under certain variations of illumination, rotation, personal dependency, etc. Our method studies the topology of the holes between the hand region and its convex hull. Inspired by the principle of Persistent Homology, which is the theory of computational topology for topological feature analysis over multiple scales, we construct the multi-scale Betti Numbers matrix (MSBNM) for the topological feature representation. In our experiments, we used 12 different hand postures and compared our features with three popular features (HOG, MCT, and Shape Context) on different data sets. In addition to hand postures, we also extend the feature representations to arm postures. The results demonstrate the feasibility and reliability of the proposed method.</p><p>4 0.59814334 <a title="202-lda-4" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>Author: Aleksandr V. Segal, Ian Reid</p><p>Abstract: We propose a novel parametrization of the data association problem for multi-target tracking. In our formulation, the number of targets is implicitly inferred together with the data association, effectively solving data association and model selection as a single inference problem. The novel formulation allows us to interpret data association and tracking as a single Switching Linear Dynamical System (SLDS). We compute an approximate posterior solution to this problem using a dynamic programming/message passing technique. This inference-based approach allows us to incorporate richer probabilistic models into the tracking system. In particular, we incorporate inference over inliers/outliers and track termination times into the system. We evaluate our approach on publicly available datasets and demonstrate results competitive with, and in some cases exceeding the state of the art.</p><p>5 0.57240391 <a title="202-lda-5" href="./iccv-2013-Box_in_the_Box%3A_Joint_3D_Layout_and_Object_Reasoning_from_Single_Images.html">64 iccv-2013-Box in the Box: Joint 3D Layout and Object Reasoning from Single Images</a></p>
<p>Author: Alexander G. Schwing, Sanja Fidler, Marc Pollefeys, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly infer the room layout as well as the objects present in the scene. Towards this goal, we propose a branch and bound algorithm which is guaranteed to retrieve the global optimum of the joint problem. The main difficulty resides in taking into account occlusion in order to not over-count the evidence. We introduce a new decomposition method, which generalizes integral geometry to triangular shapes, and allows us to bound the different terms in constant time. We exploit both geometric cues and object detectors as image features and show large improvements in 2D and 3D object detection over state-of-the-art deformable part-based models.</p><p>6 0.56789976 <a title="202-lda-6" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>7 0.5614084 <a title="202-lda-7" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>8 0.55688822 <a title="202-lda-8" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>9 0.51947761 <a title="202-lda-9" href="./iccv-2013-Efficient_and_Robust_Large-Scale_Rotation_Averaging.html">138 iccv-2013-Efficient and Robust Large-Scale Rotation Averaging</a></p>
<p>10 0.50154454 <a title="202-lda-10" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>11 0.48105842 <a title="202-lda-11" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>12 0.47500777 <a title="202-lda-12" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>13 0.47201389 <a title="202-lda-13" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>14 0.46540606 <a title="202-lda-14" href="./iccv-2013-Fine-Grained_Categorization_by_Alignments.html">169 iccv-2013-Fine-Grained Categorization by Alignments</a></p>
<p>15 0.45086861 <a title="202-lda-15" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>16 0.44947413 <a title="202-lda-16" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>17 0.44434434 <a title="202-lda-17" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>18 0.44341293 <a title="202-lda-18" href="./iccv-2013-Semantically-Based_Human_Scanpath_Estimation_with_HMMs.html">381 iccv-2013-Semantically-Based Human Scanpath Estimation with HMMs</a></p>
<p>19 0.44076991 <a title="202-lda-19" href="./iccv-2013-Discovering_Details_and_Scene_Structure_with_Hierarchical_Iconoid_Shift.html">117 iccv-2013-Discovering Details and Scene Structure with Hierarchical Iconoid Shift</a></p>
<p>20 0.43941796 <a title="202-lda-20" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
