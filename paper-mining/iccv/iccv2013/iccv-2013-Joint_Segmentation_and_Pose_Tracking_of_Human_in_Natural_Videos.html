<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-225" href="#">iccv2013-225</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</h1>
<br/><p>Source: <a title="iccv-2013-225-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Lim_Joint_Segmentation_and_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Taegyu Lim, Seunghoon Hong, Bohyung Han, Joon Hee Han</p><p>Abstract: We propose an on-line algorithm to extract a human by foreground/background segmentation and estimate pose of the human from the videos captured by moving cameras. We claim that a virtuous cycle can be created by appropriate interactions between the two modules to solve individual problems. This joint estimation problem is divided into two subproblems, , foreground/background segmentation and pose tracking, which alternate iteratively for optimization; segmentation step generates foreground mask for human pose tracking, and human pose tracking step provides foreground response map for segmentation. The final solution is obtained when the iterative procedure converges. We evaluate our algorithm quantitatively and qualitatively in real videos involving various challenges, and present its outstandingperformance compared to the state-of-the-art techniques for segmentation and pose estimation.</p><p>Reference: <a title="iccv-2013-225-reference" href="../iccv2013_reference/iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 kr  Abstract We propose an on-line algorithm to extract a human by foreground/background segmentation and estimate pose of the human from the videos captured by moving cameras. [sent-5, score-0.853]
</p><p>2 We evaluate our algorithm quantitatively and qualitatively in real videos involving various challenges, and present its outstandingperformance compared to the state-of-the-art techniques for segmentation and pose estimation. [sent-9, score-0.656]
</p><p>3 Introduction Foreground/background segmentation and human pose  estimation have been studied intensively in recent years and significant performance improvement has been achieved so far. [sent-11, score-0.719]
</p><p>4 Although foreground/background segmentation and human pose estimation are potentially related and complementary, the majority of algorithms attempt to solve the two problems separately and the investigation of a joint estimation technique is not active yet. [sent-14, score-0.818]
</p><p>5 We introduce an algorithm to address foreground/background segmentation and human pose tracking simultaneously in a video captured by a moving camera. [sent-15, score-0.861]
</p><p>6 human body area in each frame, and the latter estimates temporally coherent human body configurations. [sent-22, score-0.65]
</p><p>7 Foreground/background segmentation problem in moving camera environment has been studied actively these days. [sent-24, score-0.381]
</p><p>8 [21] proposes an algorithm to construct foreground and background appearance models for pixelwise labeling based on a sparse set of motion trajectories. [sent-26, score-0.457]
</p><p>9 Similarly, a matrix factorization is employed in [6] to decompose a dense set of trajectories into foreground or background via low rank and group sparsity constraints. [sent-27, score-0.388]
</p><p>10 These labeled trajectories are used to compute pixel-wise motion and appearance models for  both foreground and background. [sent-30, score-0.391]
</p><p>11 In [14, 15], each frame is divided into regular grid blocks, and each block motion is estimated to propagate foreground and background models in a recursive manner. [sent-31, score-0.665]
</p><p>12 On the other hand, [17, 16] study techniques to extract human body areas from videos through the combination of various algorithms. [sent-32, score-0.376]
</p><p>13 883333  Several interesting approaches for pose estimation and tracking have been proposed. [sent-33, score-0.512]
</p><p>14 Pictorial structure [10] is a widely adopted model for human pose estimation, which computes the Maximum A Posteriori (MAP) estimate of body configuration efficiently by dynamic programming. [sent-34, score-0.696]
</p><p>15 A variation of the pictorial structure model is introduced in [19], where part-specific appearance models are learned and the pose of an articulated body is estimated by message passing in a Conditional Random Field (CRF). [sent-35, score-0.727]
</p><p>16 [1] present a discriminatively trained appearance model and a flexible kinematic tree prior on the configurations of body parts. [sent-38, score-0.416]
</p><p>17 Most of these approaches assume that foreground/background segmentation is given, or do not utilize the segmentation labels at all. [sent-40, score-0.564]
</p><p>18 There are several prior studies related to the joint formulation of foreground/background segmentation and pose estimation. [sent-41, score-0.668]
</p><p>19 ObjCut [13] tackles a joint problem of segmentation and pose estimation in an image, where shape model is obtained from pose and segmentation is estimated given the shape model, and a similar approach is proposed by [5]. [sent-42, score-1.424]
</p><p>20 [12] introduce PoseCut for simultaneous 3D pose tracking and segmentation. [sent-44, score-0.454]
</p><p>21 However, its results may be sensitive to initial pose estimation and background clutter due to weak foreground/background appearance models. [sent-45, score-0.517]
</p><p>22 [4] couple pose estimation and contour extraction problems in a multi-camera environment. [sent-47, score-0.371]
</p><p>23 In [22], the multi-level inference framework for pose estimation and segmentation from a single image is proposed. [sent-48, score-0.637]
</p><p>24 Recently, a technique for segmentation and pose estimation of human is presented in [8], where foreground area is first separated by grab-cut [20] given a bounding box, and human pose is estimated based on the foreground region. [sent-49, score-1.697]
</p><p>25 We propose a unified probabilistic framework for foreground/background segmentation and pose tracking in videos captured by moving cameras. [sent-51, score-0.86]
</p><p>26 Our algorithm is an iterative approach that combines foreground/background segmentation and pose tracking. [sent-52, score-0.626]
</p><p>27 In each iteration of our algorithm, segmentation module propagates foreground and background models, and provides pose tracking module with foreground mask using the estimated labels. [sent-53, score-1.539]
</p><p>28 In pose  tracking module, the configuration of each body part is estimated by multiple part detectors with label constraint, and gives shape prior represented by probabilistic foreground response map back to segmentation module. [sent-54, score-1.766]
</p><p>29 The refined segmentation result and the estimated pose configuration are utilized to update foreground/background motion and shape prior in the next iteration, respectively. [sent-55, score-0.856]
</p><p>30 Our joint human segmentation and pose tracking algorithm has the following contributions and characteristics: •  •  •  We formulate a probabilistic framework of a joint and iterative optimization procedure for foregroundbackground segmentation and pose tracking. [sent-57, score-1.54]
</p><p>31 We propose an online algorithm based on a recursive foreground/background appearance modeling and sequential Bayesian filtering for pose tracking. [sent-58, score-0.366]
</p><p>32 Our algorithm is applied to natural videos and improves both segmentation and pose tracking performance significantly. [sent-59, score-0.771]
</p><p>33 Foreground/background segmentation in a moving camera environment is discussed in Section 3, and our pose estimation technique is presented in Section 4. [sent-62, score-0.752]
</p><p>34 Objective and Main Formulation Our goal is to perform foreground/background segmentation and human pose tracking jointly and sequentially in a video captured by a moving camera. [sent-66, score-0.861]
</p><p>35 , xm,t} is composed of a set of pose parameters for individual body parts, where m is the number of parts1 . [sent-72, score-0.583]
</p><p>36 The pose of each body part, xi,t, is represented by location, orientation and scale information. [sent-73, score-0.556]
</p><p>37 883344  Figure 2: Overview of our algorithm, which is composed of two modules: foreground/background segmentation and pose tracking. [sent-85, score-0.579]
</p><p>38 Segmentation is refined with shape information given by pose tracking while pose tracking utilizes segmentation mask. [sent-86, score-1.238]
</p><p>39 mentation and pose tracking—and solve the following energy minimization problem:  Lt,mXint,Yt Eseg(Lt,Yt,It) + Epose(Xt,Lt,It),  (2)  where Eseg(Lt, Yt, It) and Epose(Xt, Lt ,It) denote energy functions for segmentation and pose tracking, respectively. [sent-88, score-0.892]
</p><p>40 Yt denotes a foreground response map for human body area generated from the pose variable Xt. [sent-89, score-1.102]
</p><p>41 To the end, we employ a slightly modified version of [15]; spatial model composition step is removed but pose tracking feedback is taken into ac-  count for the joint formulation. [sent-98, score-0.52]
</p><p>42 The segmentation labels, Lt∗, are obtained efficiently by graph-cut algorithm [2, 3] based on p(Lt |Yt, It), which depends on the two terms—observation likelihood and segmentation prior given pose. [sent-100, score-0.667]
</p><p>43 Estimation of Observation Likelihood We obtain the observation likelihood p(It |Lt) from the probabilistic models of foreground and background appearances. [sent-105, score-0.52]
</p><p>44 We divide a frame into N regular grid blocks2 and construct foreground and background models in each block by kernel density estimation. [sent-106, score-0.499]
</p><p>45 Suppose that foreground model ϕfk,t−1 and background model ϕbk,t−1 for the k-th block Btk−1 at time t − 1 are already given, where {y1ξ,t−1 , . [sent-107, score-0.458]
</p><p>46 The foreground and background likelihoods of an observed pixel zt−1 are respectively given by  ξ  p(zt−1  |ϕkf,t−1)  =  αU(zt−1) +1 −nf αi? [sent-111, score-0.428]
</p><p>47 To construct foreground and background models at time t based on the earlier ones, we compute foreground and background motion vectors in each block, and propagate models from the previous frame using the block motions. [sent-114, score-0.962]
</p><p>48 If the motion observation in a block is insufficient due to occlusion, background block motion is estimated by the average motion of adjacent blocks and foreground block motion is set to zero. [sent-117, score-1.059]
</p><p>49 Note that χξk,t depends on the segmentation labels and is updated in each iteration due to potential label changes of the pixels within the block. [sent-118, score-0.39]
</p><p>50 Through the iterative model propagation with respect to backward block motion Vξk,t, the likelihood of an observed 2The size of each block is 24  ξ  24 in our experiment. [sent-119, score-0.433]
</p><p>51 Prior of Segmentation Given Pose For the alternating procedure between segmentation and pose tracking, feedback from pose tracking needs to be incorporated for label estimation. [sent-139, score-1.083]
</p><p>52 In this work, pixel-wise foreground response map Yt plays this role, and the prior  of segmentation given human pose introduced in Eq. [sent-140, score-1.138]
</p><p>53 y  where spatial smoothness term defines the relationship between adjacent pixels and pose consistency term corresponds to the coherency between the labels from segmentation and pose tracking. [sent-169, score-0.956]
</p><p>54 On the other hand, pixel-wise foreground response map, Yt, is estimated based on the response maps of individual 3We used the standard four neighborhood  system. [sent-187, score-0.58]
</p><p>55 (b) Detector response of each body part (head, torso, upper and lower legs, upper and lower arms). [sent-190, score-0.473]
</p><p>56 (d) Foreground response map is generated by the sum of the marginalized response and the segmentation mask in the previous iteration. [sent-192, score-0.66]
</p><p>57 body parts obtained from pose tracking as well as the segmentation mask inferred in the previous iteration. [sent-193, score-1.061]
</p><p>58 To construct the foreground response map, we marginalize the responses of all body parts in a 2D image space, and then normalize the marginalized responses with the maximum value. [sent-194, score-0.822]
</p><p>59 Also, the label likelihood given the foreground response, p(? [sent-195, score-0.384]
</p><p>60 Figure 3 visualizes the construction of the foreground response map. [sent-201, score-0.393]
</p><p>61 Pose Tracking Pose tracking sequentially estimates p(Xt |Lt, I1:t), posterior distribution over the current human body configuration Xt at time t given segmentation result Lt and all image evidences I1:t. [sent-203, score-0.826]
</p><p>62 The posterior probability is factorized by Bayesian filtering as p(Xt |Lt, I1:t)  ∝  p(Lt, It |Xt)p(Xt |I1:t−1),  (15)  where p(Lt, It |Xt) is the likelihood of image evidence and segmentation given a particular body part configurations, and p(Xt |I1:t−1) is the prior of Xt. [sent-204, score-0.735]
</p><p>63 The configurations of body parts are estimated by tracking-by-detection paradigm, in which the responses of part detectors serve as observation for tracking. [sent-205, score-0.501]
</p><p>64 We restrict search space for each body part to the intersection of foreground area and predicted region corresponding to each part area. [sent-206, score-0.665]
</p><p>65 The restricted search region significantly reduces ambiguities and false positive detections caused by the features similar to human body part in background clutter. [sent-207, score-0.511]
</p><p>66 (a) Result of pose estimation at frame 125 in Skating sequence. [sent-210, score-0.412]
</p><p>67 The shape likelihood of the part xi,t is computed by the convolution of shape model filters and local foreground edge map with respect to xi,t, which is given by  p(It|xi,t,Lt)−( ∝1 e −xp ηi()−? [sent-227, score-0.578]
</p><p>68 (17) and (19), are used to construct foreground response map Yt. [sent-237, score-0.429]
</p><p>69 Pose Prediction The prediction p(Xt |I1:t−1) is composed of a spatial prior on the relative position between parts and a temporal prior of an individual body part as p(Xt|I1:t−1)  ∝  ? [sent-240, score-0.462]
</p><p>70 upper arms must be connected to torso), p(xi,t |xi,t−1) denotes the temporal prior for an individual part, and p(Xt−1 |I1:t−1) is the posterior of the pose parameters at the previous time step t − 1. [sent-245, score-0.537]
</p><p>71 The spatial prior on body part configurations is based on  a tree structure and represents the kinematic dependencies between body parts. [sent-246, score-0.688]
</p><p>72 To deal with various changes of pose and appearance in image, we adopt the spatial prior model by discrete binning [19], which is given by  (i? [sent-247, score-0.42]
</p><p>73 883377  Algorithm 1: Joint segmentation and pose tracking Input: It, ϕξk,t−1 , Xt−1 , ρis,t−1 Output: Lt, ϕξk,t, Xt, ρis,t iterate  11 12 13  Model Update: FG/BG model update (Eq. [sent-252, score-0.72]
</p><p>74 Model Updates After the iterative procedure at each frame, we obtain foreground/background labels and human body configuration. [sent-255, score-0.404]
</p><p>75 To propagate the labels and pose parameters accurately, foreground/background models and specific shape model should be updated in each frame based on the converged segmentation labels. [sent-256, score-0.809]
</p><p>76 The foreground and background models are recursively updated using the propagated models from the previous time step t − 1 and the observations in the current time step t, which are given by p(zt| ϕˆξk,t) =  τseg· p(zt| ϕ˜kξ,t) +1 −nξ τsegi? [sent-257, score-0.397]
</p><p>77 The specific shape model of each body part is also updated incrementally based on the local foreground edge map at the current time step t, which is given by ρˆsi,t  = τpose  · ˆρ is,t−1  + (1 − τpose)  · f(It; xi,t, Lt), (24)  where τpose is a forgetting factor for specific shape model. [sent-259, score-0.865]
</p><p>78 The pseudo code of overall joint segmentation and pose tracking algorithm is described in Algorithm 1. [sent-260, score-0.761]
</p><p>79 Our results are compared with existing methods for foreground/background segmentation and pose estimation such as [15, 8, 19]. [sent-263, score-0.637]
</p><p>80 Since the proposed technique combines segmentation and pose estimation, the two subproblems are evaluated separately. [sent-264, score-0.62]
</p><p>81 All the sequences involve various pose changes and substantial camera motions. [sent-268, score-0.374]
</p><p>82 On the other hand, pose estimation is evaluated by the Percentage of Correctly estimated body Parts (PCP) [7]. [sent-271, score-0.653]
</p><p>83 We compute PCP values for individual body parts, and the performance of entire human body is estimated based on the average of the PCP measures of all body parts. [sent-273, score-0.877]
</p><p>84 Results We present our foreground/background segmentation and human pose tracking results in Figure 5. [sent-277, score-0.802]
</p><p>85 The experimental results illustrate that our algorithm produces accurate and robust outputs in the presence of background clutter, significant pose variations, fast camera motions, occlusions, scale changes, and so on. [sent-278, score-0.432]
</p><p>86 To demonstrate the effectiveness of our joint estimation algorithm, we first compare our foreground/background segmentation algorithm with other methods such as pro-  gressive pruning [8], motion segmentation, and our algorithm with segmentation only. [sent-280, score-0.699]
</p><p>87 Then, our pose tracking algorithm is also compared with progressive pruning [8], iterative learning algorithm with our foreground/background segmentation [19], and our algorithm with pose tracking only. [sent-281, score-1.25]
</p><p>88 As illustrated in Figure 6, our joint estimation algorithm performs better than all other methods significantly and is robust to the background features similar to human body parts. [sent-282, score-0.515]
</p><p>89 The quantitative performance of foreground/background segmentation algorithm are summarized in Figure 7, where our algorithm is compared with a simple motion segmentation, our algorithm with segmentation only and an existing techniques [8]. [sent-283, score-0.628]
</p><p>90 The quantitative performance of human pose estimation is evaluated based on the PCP-curves, which are presented in Figure 8. [sent-285, score-0.481]
</p><p>91 Conclusion We presented a unified probabilistic framework to perform foreground/background segmentation and human pose tracking jointly in an on-line manner. [sent-304, score-0.832]
</p><p>92 The proposed algorithm presents outstanding segmentation and pose estimation performance by mutual interactions between the two complementary subsystems; they alternate each other and improve the quality of solution in each iteration. [sent-305, score-0.662]
</p><p>93 We  showed the robustness of our foreground/background segmentation and pose tracking algorithms to background clutter, pose changes, object scale changes, and illumination variations through qualitative and quantitative validation in real videos. [sent-306, score-1.152]
</p><p>94 PCP evaluation on Skating sequence  PCP − threshold  (a) Skating  PCP evaluation on Jumping sequence  PCP evaluation on Pitching sequence  PCP − threshold  (b) Pitching  PCP evaluation on Dunk sequence  PCP − threshold  PCP − threshold  (c) Jumping  (d) Dunk  %]PC[0 98. [sent-404, score-0.376]
</p><p>95 ]O[8naly 19] PCP evaluation on Javelin sequence  (e) Javelin  Figure 8: Quantitative  performance  evaluation  results of  pose estimation evaluation by PCP curves. [sent-411, score-0.439]
</p><p>96 Hybrid body representation for integrated pose recognition, localization and segmentation. [sent-442, score-0.556]
</p><p>97 2d articulated human pose estimation and retrieval in (almost) unconstrained still images. [sent-462, score-0.498]
</p><p>98 Simultaneous segmentation and pose estimation of humans using dynamic graph cuts. [sent-488, score-0.637]
</p><p>99 Modeling and segmentation of floating foreground and background in videos. [sent-510, score-0.629]
</p><p>100 Multi-level inference by relaxed dual decomposition for human pose segmentation. [sent-550, score-0.395]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lt', 0.33), ('pose', 0.313), ('foreground', 0.272), ('segmentation', 0.266), ('yt', 0.252), ('body', 0.243), ('pcp', 0.235), ('xt', 0.198), ('zt', 0.185), ('eseg', 0.159), ('skating', 0.143), ('tracking', 0.141), ('dunk', 0.132), ('epose', 0.132), ('response', 0.121), ('pitching', 0.117), ('javelin', 0.109), ('btk', 0.106), ('block', 0.095), ('background', 0.091), ('likelihood', 0.087), ('jumping', 0.083), ('human', 0.082), ('eichner', 0.072), ('sequence', 0.068), ('motion', 0.068), ('shape', 0.064), ('pictorial', 0.061), ('kinematic', 0.059), ('moving', 0.059), ('marginalized', 0.059), ('configuration', 0.058), ('estimation', 0.058), ('mask', 0.057), ('part', 0.055), ('riegion', 0.053), ('vti', 0.053), ('videos', 0.051), ('samsung', 0.051), ('arms', 0.051), ('prior', 0.048), ('iterative', 0.047), ('korea', 0.047), ('btj', 0.047), ('postech', 0.047), ('articulated', 0.045), ('module', 0.044), ('iti', 0.043), ('forgetting', 0.043), ('responses', 0.043), ('frame', 0.041), ('joint', 0.041), ('parts', 0.041), ('subproblems', 0.041), ('backward', 0.041), ('observation', 0.04), ('configurations', 0.04), ('region', 0.04), ('likelihoods', 0.039), ('estimated', 0.039), ('han', 0.039), ('torso', 0.039), ('electronics', 0.037), ('subtraction', 0.037), ('map', 0.036), ('seg', 0.036), ('lti', 0.036), ('posterior', 0.036), ('denotes', 0.035), ('updated', 0.034), ('changes', 0.033), ('modules', 0.032), ('labels', 0.032), ('adjacent', 0.032), ('propagate', 0.032), ('niebles', 0.031), ('mixture', 0.031), ('probabilistic', 0.03), ('clutter', 0.029), ('progressive', 0.029), ('environment', 0.028), ('quantitative', 0.028), ('blocks', 0.028), ('camera', 0.028), ('sheikh', 0.027), ('upper', 0.027), ('individual', 0.027), ('specific', 0.027), ('recursive', 0.027), ('andriluka', 0.027), ('pixel', 0.026), ('qualitatively', 0.026), ('appearance', 0.026), ('threshold', 0.026), ('legs', 0.025), ('trajectories', 0.025), ('feedback', 0.025), ('ig', 0.025), ('alternate', 0.025), ('label', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="225-tfidf-1" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>Author: Taegyu Lim, Seunghoon Hong, Bohyung Han, Joon Hee Han</p><p>Abstract: We propose an on-line algorithm to extract a human by foreground/background segmentation and estimate pose of the human from the videos captured by moving cameras. We claim that a virtuous cycle can be created by appropriate interactions between the two modules to solve individual problems. This joint estimation problem is divided into two subproblems, , foreground/background segmentation and pose tracking, which alternate iteratively for optimization; segmentation step generates foreground mask for human pose tracking, and human pose tracking step provides foreground response map for segmentation. The final solution is obtained when the iterative procedure converges. We evaluate our algorithm quantitatively and qualitatively in real videos involving various challenges, and present its outstandingperformance compared to the state-of-the-art techniques for segmentation and pose estimation.</p><p>2 0.26789236 <a title="225-tfidf-2" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>Author: Silvia Zuffi, Javier Romero, Cordelia Schmid, Michael J. Black</p><p>Abstract: We address the problem of upper-body human pose estimation in uncontrolled monocular video sequences, without manual initialization. Most current methods focus on isolated video frames and often fail to correctly localize arms and hands. Inferring pose over a video sequence is advantageous because poses of people in adjacent frames exhibit properties of smooth variation due to the nature of human and camera motion. To exploit this, previous methods have used prior knowledge about distinctive actions or generic temporal priors combined with static image likelihoods to track people in motion. Here we take a different approach based on a simple observation: Information about how a person moves from frame to frame is present in the optical flow field. We develop an approach for tracking articulated motions that “links” articulated shape models of peo- ple in adjacent frames through the dense optical flow. Key to this approach is a 2D shape model of the body that we use to compute how the body moves over time. The resulting “flowing puppets ” provide a way of integrating image evidence across frames to improve pose inference. We apply our method on a challenging dataset of TV video sequences and show state-of-the-art performance.</p><p>3 0.24317393 <a title="225-tfidf-3" href="./iccv-2013-Strong_Appearance_and_Expressive_Spatial_Models_for_Human_Pose_Estimation.html">403 iccv-2013-Strong Appearance and Expressive Spatial Models for Human Pose Estimation</a></p>
<p>Author: Leonid Pishchulin, Mykhaylo Andriluka, Peter Gehler, Bernt Schiele</p><p>Abstract: Typical approaches to articulated pose estimation combine spatial modelling of the human body with appearance modelling of body parts. This paper aims to push the state-of-the-art in articulated pose estimation in two ways. First we explore various types of appearance representations aiming to substantially improve the bodypart hypotheses. And second, we draw on and combine several recently proposed powerful ideas such as more flexible spatial models as well as image-conditioned spatial models. In a series of experiments we draw several important conclusions: (1) we show that the proposed appearance representations are complementary; (2) we demonstrate that even a basic tree-structure spatial human body model achieves state-ofthe-art performance when augmented with the proper appearance representation; and (3) we show that the combination of the best performing appearance model with a flexible image-conditioned spatial model achieves the best result, significantly improving over the state of the art, on the “Leeds Sports Poses ” and “Parse ” benchmarks.</p><p>4 0.22321771 <a title="225-tfidf-4" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>Author: Stefan Duffner, Christophe Garcia</p><p>Abstract: In this paper, we present a novel algorithm for fast tracking of generic objects in videos. The algorithm uses two components: a detector that makes use of the generalised Hough transform with pixel-based descriptors, and a probabilistic segmentation method based on global models for foreground and background. These components are used for tracking in a combined way, and they adapt each other in a co-training manner. Through effective model adaptation and segmentation, the algorithm is able to track objects that undergo rigid and non-rigid deformations and considerable shape and appearance variations. The proposed tracking method has been thoroughly evaluated on challenging standard videos, and outperforms state-of-theart tracking methods designed for the same task. Finally, the proposed models allow for an extremely efficient implementation, and thus tracking is very fast.</p><p>5 0.19640414 <a title="225-tfidf-5" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>Author: Karteek Alahari, Guillaume Seguin, Josef Sivic, Ivan Laptev</p><p>Abstract: We seek to obtain a pixel-wise segmentation and pose estimation of multiple people in a stereoscopic video. This involves challenges such as dealing with unconstrained stereoscopic video, non-stationary cameras, and complex indoor and outdoor dynamic scenes. The contributions of our work are two-fold: First, we develop a segmentation model incorporating person detection, pose estimation, as well as colour, motion, and disparity cues. Our new model explicitly represents depth ordering and occlusion. Second, we introduce a stereoscopic dataset with frames extracted from feature-length movies “StreetDance 3D ” and “Pina ”. The dataset contains 2727 realistic stereo pairs and includes annotation of human poses, person bounding boxes, and pixel-wise segmentations for hundreds of people. The dataset is composed of indoor and outdoor scenes depicting multiple people with frequent occlusions. We demonstrate results on our new challenging dataset, as well as on the H2view dataset from (Sheasby et al. ACCV 2012).</p><p>6 0.19057643 <a title="225-tfidf-6" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>7 0.18172282 <a title="225-tfidf-7" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>8 0.17930457 <a title="225-tfidf-8" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>9 0.17408872 <a title="225-tfidf-9" href="./iccv-2013-Monocular_Image_3D_Human_Pose_Estimation_under_Self-Occlusion.html">273 iccv-2013-Monocular Image 3D Human Pose Estimation under Self-Occlusion</a></p>
<p>10 0.16817485 <a title="225-tfidf-10" href="./iccv-2013-Real-Time_Body_Tracking_with_One_Depth_Camera_and_Inertial_Sensors.html">341 iccv-2013-Real-Time Body Tracking with One Depth Camera and Inertial Sensors</a></p>
<p>11 0.16542986 <a title="225-tfidf-11" href="./iccv-2013-Interactive_Markerless_Articulated_Hand_Motion_Tracking_Using_RGB_and_Depth_Data.html">218 iccv-2013-Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth Data</a></p>
<p>12 0.15996356 <a title="225-tfidf-12" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>13 0.15977986 <a title="225-tfidf-13" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>14 0.15713474 <a title="225-tfidf-14" href="./iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F.html">316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</a></p>
<p>15 0.15651159 <a title="225-tfidf-15" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>16 0.15123172 <a title="225-tfidf-16" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>17 0.15037893 <a title="225-tfidf-17" href="./iccv-2013-Tree_Shape_Priors_with_Connectivity_Constraints_Using_Convex_Relaxation_on_General_Graphs.html">429 iccv-2013-Tree Shape Priors with Connectivity Constraints Using Convex Relaxation on General Graphs</a></p>
<p>18 0.14935492 <a title="225-tfidf-18" href="./iccv-2013-A_Non-parametric_Bayesian_Network_Prior_of_Human_Pose.html">24 iccv-2013-A Non-parametric Bayesian Network Prior of Human Pose</a></p>
<p>19 0.14616328 <a title="225-tfidf-19" href="./iccv-2013-Sequential_Bayesian_Model_Update_under_Structured_Scene_Prior_for_Semantic_Road_Scenes_Labeling.html">386 iccv-2013-Sequential Bayesian Model Update under Structured Scene Prior for Semantic Road Scenes Labeling</a></p>
<p>20 0.14592196 <a title="225-tfidf-20" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.281), (1, -0.088), (2, 0.08), (3, 0.104), (4, 0.11), (5, -0.083), (6, -0.097), (7, 0.185), (8, -0.03), (9, 0.143), (10, 0.021), (11, 0.035), (12, -0.103), (13, -0.104), (14, -0.146), (15, 0.141), (16, -0.013), (17, -0.129), (18, -0.046), (19, -0.011), (20, 0.217), (21, -0.019), (22, 0.07), (23, -0.004), (24, -0.041), (25, 0.06), (26, 0.009), (27, 0.01), (28, 0.081), (29, 0.014), (30, -0.009), (31, 0.035), (32, 0.113), (33, -0.049), (34, 0.016), (35, 0.028), (36, -0.078), (37, 0.092), (38, 0.006), (39, 0.071), (40, -0.008), (41, 0.045), (42, -0.021), (43, 0.059), (44, 0.052), (45, 0.007), (46, -0.032), (47, -0.037), (48, 0.05), (49, -0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9881072 <a title="225-lsi-1" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>Author: Taegyu Lim, Seunghoon Hong, Bohyung Han, Joon Hee Han</p><p>Abstract: We propose an on-line algorithm to extract a human by foreground/background segmentation and estimate pose of the human from the videos captured by moving cameras. We claim that a virtuous cycle can be created by appropriate interactions between the two modules to solve individual problems. This joint estimation problem is divided into two subproblems, , foreground/background segmentation and pose tracking, which alternate iteratively for optimization; segmentation step generates foreground mask for human pose tracking, and human pose tracking step provides foreground response map for segmentation. The final solution is obtained when the iterative procedure converges. We evaluate our algorithm quantitatively and qualitatively in real videos involving various challenges, and present its outstandingperformance compared to the state-of-the-art techniques for segmentation and pose estimation.</p><p>2 0.78420961 <a title="225-lsi-2" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>Author: Silvia Zuffi, Javier Romero, Cordelia Schmid, Michael J. Black</p><p>Abstract: We address the problem of upper-body human pose estimation in uncontrolled monocular video sequences, without manual initialization. Most current methods focus on isolated video frames and often fail to correctly localize arms and hands. Inferring pose over a video sequence is advantageous because poses of people in adjacent frames exhibit properties of smooth variation due to the nature of human and camera motion. To exploit this, previous methods have used prior knowledge about distinctive actions or generic temporal priors combined with static image likelihoods to track people in motion. Here we take a different approach based on a simple observation: Information about how a person moves from frame to frame is present in the optical flow field. We develop an approach for tracking articulated motions that “links” articulated shape models of peo- ple in adjacent frames through the dense optical flow. Key to this approach is a 2D shape model of the body that we use to compute how the body moves over time. The resulting “flowing puppets ” provide a way of integrating image evidence across frames to improve pose inference. We apply our method on a challenging dataset of TV video sequences and show state-of-the-art performance.</p><p>3 0.72395438 <a title="225-lsi-3" href="./iccv-2013-Strong_Appearance_and_Expressive_Spatial_Models_for_Human_Pose_Estimation.html">403 iccv-2013-Strong Appearance and Expressive Spatial Models for Human Pose Estimation</a></p>
<p>Author: Leonid Pishchulin, Mykhaylo Andriluka, Peter Gehler, Bernt Schiele</p><p>Abstract: Typical approaches to articulated pose estimation combine spatial modelling of the human body with appearance modelling of body parts. This paper aims to push the state-of-the-art in articulated pose estimation in two ways. First we explore various types of appearance representations aiming to substantially improve the bodypart hypotheses. And second, we draw on and combine several recently proposed powerful ideas such as more flexible spatial models as well as image-conditioned spatial models. In a series of experiments we draw several important conclusions: (1) we show that the proposed appearance representations are complementary; (2) we demonstrate that even a basic tree-structure spatial human body model achieves state-ofthe-art performance when augmented with the proper appearance representation; and (3) we show that the combination of the best performing appearance model with a flexible image-conditioned spatial model achieves the best result, significantly improving over the state of the art, on the “Leeds Sports Poses ” and “Parse ” benchmarks.</p><p>4 0.70539534 <a title="225-lsi-4" href="./iccv-2013-Monocular_Image_3D_Human_Pose_Estimation_under_Self-Occlusion.html">273 iccv-2013-Monocular Image 3D Human Pose Estimation under Self-Occlusion</a></p>
<p>Author: Ibrahim Radwan, Abhinav Dhall, Roland Goecke</p><p>Abstract: In this paper, an automatic approach for 3D pose reconstruction from a single image is proposed. The presence of human body articulation, hallucinated parts and cluttered background leads to ambiguity during the pose inference, which makes the problem non-trivial. Researchers have explored various methods based on motion and shading in order to reduce the ambiguity and reconstruct the 3D pose. The key idea of our algorithm is to impose both kinematic and orientation constraints. The former is imposed by projecting a 3D model onto the input image and pruning the parts, which are incompatible with the anthropomorphism. The latter is applied by creating synthetic views via regressing the input view to multiple oriented views. After applying the constraints, the 3D model is projected onto the initial and synthetic views, which further reduces the ambiguity. Finally, we borrow the direction of the unambiguous parts from the synthetic views to the initial one, which results in the 3D pose. Quantitative experiments are performed on the HumanEva-I dataset and qualitatively on unconstrained images from the Image Parse dataset. The results show the robustness of the proposed approach to accurately reconstruct the 3D pose form a single image.</p><p>5 0.70182645 <a title="225-lsi-5" href="./iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F.html">316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</a></p>
<p>Author: Elisabeta Marinoiu, Dragos Papava, Cristian Sminchisescu</p><p>Abstract: Human motion analysis in images and video is a central computer vision problem. Yet, there are no studies that reveal how humans perceive other people in images and how accurate they are. In this paper we aim to unveil some of the processing–as well as the levels of accuracy–involved in the 3D perception of people from images by assessing the human performance. Our contributions are: (1) the construction of an experimental apparatus that relates perception and measurement, in particular the visual and kinematic performance with respect to 3D ground truth when the human subject is presented an image of a person in a given pose; (2) the creation of a dataset containing images, articulated 2D and 3D pose ground truth, as well as synchronized eye movement recordings of human subjects, shown a variety of human body configurations, both easy and difficult, as well as their ‘re-enacted’ 3D poses; (3) quantitative analysis revealing the human performance in 3D pose reenactment tasks, the degree of stability in the visual fixation patterns of human subjects, and the way it correlates with different poses. We also discuss the implications of our find- ings for the construction of visual human sensing systems.</p><p>6 0.68652904 <a title="225-lsi-6" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>7 0.67292166 <a title="225-lsi-7" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>8 0.66023648 <a title="225-lsi-8" href="./iccv-2013-Interactive_Markerless_Articulated_Hand_Motion_Tracking_Using_RGB_and_Depth_Data.html">218 iccv-2013-Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth Data</a></p>
<p>9 0.65991896 <a title="225-lsi-9" href="./iccv-2013-A_Deformable_Mixture_Parsing_Model_with_Parselets.html">8 iccv-2013-A Deformable Mixture Parsing Model with Parselets</a></p>
<p>10 0.6457845 <a title="225-lsi-10" href="./iccv-2013-Discovering_Object_Functionality.html">118 iccv-2013-Discovering Object Functionality</a></p>
<p>11 0.64490801 <a title="225-lsi-11" href="./iccv-2013-No_Matter_Where_You_Are%3A_Flexible_Graph-Guided_Multi-task_Learning_for_Multi-view_Head_Pose_Classification_under_Target_Motion.html">291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</a></p>
<p>12 0.6424346 <a title="225-lsi-12" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>13 0.64029509 <a title="225-lsi-13" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>14 0.6351462 <a title="225-lsi-14" href="./iccv-2013-A_Non-parametric_Bayesian_Network_Prior_of_Human_Pose.html">24 iccv-2013-A Non-parametric Bayesian Network Prior of Human Pose</a></p>
<p>15 0.6135025 <a title="225-lsi-15" href="./iccv-2013-Recognising_Human-Object_Interaction_via_Exemplar_Based_Modelling.html">344 iccv-2013-Recognising Human-Object Interaction via Exemplar Based Modelling</a></p>
<p>16 0.6129998 <a title="225-lsi-16" href="./iccv-2013-Real-Time_Body_Tracking_with_One_Depth_Camera_and_Inertial_Sensors.html">341 iccv-2013-Real-Time Body Tracking with One Depth Camera and Inertial Sensors</a></p>
<p>17 0.61158025 <a title="225-lsi-17" href="./iccv-2013-Allocentric_Pose_Estimation.html">46 iccv-2013-Allocentric Pose Estimation</a></p>
<p>18 0.60928935 <a title="225-lsi-18" href="./iccv-2013-Bounded_Labeling_Function_for_Global_Segmentation_of_Multi-part_Objects_with_Geometric_Constraints.html">63 iccv-2013-Bounded Labeling Function for Global Segmentation of Multi-part Objects with Geometric Constraints</a></p>
<p>19 0.59825093 <a title="225-lsi-19" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>20 0.59781873 <a title="225-lsi-20" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.055), (7, 0.02), (13, 0.01), (26, 0.126), (31, 0.034), (35, 0.026), (42, 0.092), (44, 0.237), (64, 0.079), (73, 0.041), (89, 0.182), (98, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83840597 <a title="225-lda-1" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>Author: Taegyu Lim, Seunghoon Hong, Bohyung Han, Joon Hee Han</p><p>Abstract: We propose an on-line algorithm to extract a human by foreground/background segmentation and estimate pose of the human from the videos captured by moving cameras. We claim that a virtuous cycle can be created by appropriate interactions between the two modules to solve individual problems. This joint estimation problem is divided into two subproblems, , foreground/background segmentation and pose tracking, which alternate iteratively for optimization; segmentation step generates foreground mask for human pose tracking, and human pose tracking step provides foreground response map for segmentation. The final solution is obtained when the iterative procedure converges. We evaluate our algorithm quantitatively and qualitatively in real videos involving various challenges, and present its outstandingperformance compared to the state-of-the-art techniques for segmentation and pose estimation.</p><p>2 0.83290231 <a title="225-lda-2" href="./iccv-2013-Orderless_Tracking_through_Model-Averaged_Posterior_Estimation.html">303 iccv-2013-Orderless Tracking through Model-Averaged Posterior Estimation</a></p>
<p>Author: Seunghoon Hong, Suha Kwak, Bohyung Han</p><p>Abstract: We propose a novel offline tracking algorithm based on model-averaged posterior estimation through patch matching across frames. Contrary to existing online and offline tracking methods, our algorithm is not based on temporallyordered estimates of target state but attempts to select easyto-track frames first out of the remaining ones without exploiting temporal coherency of target. The posterior of the selected frame is estimated by propagating densities from the already tracked frames in a recursive manner. The density propagation across frames is implemented by an efficient patch matching technique, which is useful for our algorithm since it does not require motion smoothness assumption. Also, we present a hierarchical approach, where a small set of key frames are tracked first and non-key frames are handled by local key frames. Our tracking algorithm is conceptually well-suited for the sequences with abrupt motion, shot changes, and occlusion. We compare our tracking algorithm with existing techniques in real videos with such challenges and illustrate its superior performance qualitatively and quantitatively.</p><p>3 0.8079769 <a title="225-lda-3" href="./iccv-2013-The_Interestingness_of_Images.html">416 iccv-2013-The Interestingness of Images</a></p>
<p>Author: Michael Gygli, Helmut Grabner, Hayko Riemenschneider, Fabian Nater, Luc Van_Gool</p><p>Abstract: We investigate human interest in photos. Based on our own and others ’psychological experiments, we identify various cues for “interestingness ”, namely aesthetics, unusualness and general preferences. For the ranking of retrieved images, interestingness is more appropriate than cues proposed earlier. Interestingness is, for example, correlated with what people believe they will remember. This is opposed to actual memorability, which is uncorrelated to both of them. We introduce a set of features computationally capturing the three main aspects of visual interestingness that we propose and build an interestingness predictor from them. Its performance is shown on three datasets with varying context, reflecting diverse levels of prior knowledge of the viewers.</p><p>4 0.80206859 <a title="225-lda-4" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>Author: Ping Wei, Nanning Zheng, Yibiao Zhao, Song-Chun Zhu</p><p>Abstract: Action recognition has often been posed as a classification problem, which assumes that a video sequence only have one action class label and different actions are independent. However, a single human body can perform multiple concurrent actions at the same time, and different actions interact with each other. This paper proposes a concurrent action detection model where the action detection is formulated as a structural prediction problem. In this model, an interval in a video sequence can be described by multiple action labels. An detected action interval is determined both by the unary local detector and the relations with other actions. We use a wavelet feature to represent the action sequence, and design a composite temporal logic descriptor to describe the action relations. The model parameters are trained by structural SVM learning. Given a long video sequence, a sequential decision window search algorithm is designed to detect the actions. Experiments on our new collected concurrent action dataset demonstrate the strength of our method.</p><p>5 0.75513983 <a title="225-lda-5" href="./iccv-2013-Volumetric_Semantic_Segmentation_Using_Pyramid_Context_Features.html">447 iccv-2013-Volumetric Semantic Segmentation Using Pyramid Context Features</a></p>
<p>Author: Jonathan T. Barron, Mark D. Biggin, Pablo Arbeláez, David W. Knowles, Soile V.E. Keranen, Jitendra Malik</p><p>Abstract: We present an algorithm for the per-voxel semantic segmentation of a three-dimensional volume. At the core of our algorithm is a novel “pyramid context” feature, a descriptive representation designed such that exact per-voxel linear classification can be made extremely efficient. This feature not only allows for efficient semantic segmentation but enables other aspects of our algorithm, such as novel learned features and a stacked architecture that can reason about self-consistency. We demonstrate our technique on 3Dfluorescence microscopy data ofDrosophila embryosfor which we are able to produce extremely accurate semantic segmentations in a matter of minutes, and for which other algorithms fail due to the size and high-dimensionality of the data, or due to the difficulty of the task.</p><p>6 0.74261487 <a title="225-lda-6" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>7 0.73784536 <a title="225-lda-7" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>8 0.7290473 <a title="225-lda-8" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>9 0.72333038 <a title="225-lda-9" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>10 0.72006494 <a title="225-lda-10" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>11 0.71966302 <a title="225-lda-11" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>12 0.71742165 <a title="225-lda-12" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>13 0.71738005 <a title="225-lda-13" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>14 0.71727222 <a title="225-lda-14" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>15 0.71686018 <a title="225-lda-15" href="./iccv-2013-A_Deformable_Mixture_Parsing_Model_with_Parselets.html">8 iccv-2013-A Deformable Mixture Parsing Model with Parselets</a></p>
<p>16 0.7165696 <a title="225-lda-16" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<p>17 0.71589363 <a title="225-lda-17" href="./iccv-2013-Data-Driven_3D_Primitives_for_Single_Image_Understanding.html">102 iccv-2013-Data-Driven 3D Primitives for Single Image Understanding</a></p>
<p>18 0.71554655 <a title="225-lda-18" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>19 0.71470523 <a title="225-lda-19" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>20 0.71359891 <a title="225-lda-20" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
