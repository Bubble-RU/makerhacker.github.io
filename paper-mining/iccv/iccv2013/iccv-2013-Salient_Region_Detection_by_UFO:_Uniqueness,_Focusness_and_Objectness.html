<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-374" href="#">iccv2013-374</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</h1>
<br/><p>Source: <a title="iccv-2013-374-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Jiang_Salient_Region_Detection_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Peng Jiang, Haibin Ling, Jingyi Yu, Jingliang Peng</p><p>Abstract: The goal of saliency detection is to locate important pixels or regions in an image which attract humans ’ visual attention the most. This is a fundamental task whose output may serve as the basis for further computer vision tasks like segmentation, resizing, tracking and so forth. In this paper we propose a novel salient region detection algorithm by integrating three important visual cues namely uniqueness, focusness and objectness (UFO). In particular, uniqueness captures the appearance-derived visual contrast; focusness reflects the fact that salient regions are often photographed in focus; and objectness helps keep completeness of detected salient regions. While uniqueness has been used for saliency detection for long, it is new to integrate focusness and objectness for this purpose. In fact, focusness and objectness both provide important saliency information complementary of uniqueness. In our experiments using public benchmark datasets, we show that, even with a simple pixel level combination of the three components, the proposed approach yields significant improve- ment compared with previously reported methods.</p><p>Reference: <a title="iccv-2013-374-reference" href="../iccv2013_reference/iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract The goal of saliency detection is to locate important pixels or regions in an image which attract humans ’ visual attention the most. [sent-5, score-0.628]
</p><p>2 This is a fundamental task whose output may serve as the basis for further computer vision tasks like segmentation, resizing, tracking and so forth. [sent-6, score-0.024]
</p><p>3 In this paper we propose a novel salient region detection algorithm by integrating three important visual cues namely uniqueness, focusness and objectness (UFO). [sent-7, score-1.199]
</p><p>4 In particular, uniqueness captures the appearance-derived visual contrast; focusness reflects the fact that salient regions are often photographed in focus; and objectness helps keep completeness of detected salient regions. [sent-8, score-1.554]
</p><p>5 While uniqueness has been used for saliency detection for long, it is new to integrate focusness and objectness for this purpose. [sent-9, score-1.652]
</p><p>6 In fact, focusness and objectness both provide important saliency information complementary of uniqueness. [sent-10, score-1.345]
</p><p>7 In our experiments using public benchmark datasets, we show that, even with a simple pixel level combination of the three components, the proposed approach yields significant improve-  ment compared with previously reported methods. [sent-11, score-0.032]
</p><p>8 Introduction Humans have the capability to quickly prioritize external visual stimuli and localize their most interest in a scene. [sent-13, score-0.106]
</p><p>9 As such, how to simulate such human capability with a computer, i. [sent-14, score-0.028]
</p><p>10 , how to identify the most salient pixels or regions in a digital image which attract humans’ first visual attention, has become an important task in computer vision. [sent-16, score-0.232]
</p><p>11 Further, results of saliency detection can be used to facilitate other computer vision tasks such as image resizing, thumbnailing, image segmentation and object detection. [sent-17, score-0.534]
</p><p>12 Due to its importance, saliency detection has received intensive research attention resulting in many recently proposed algorithms. [sent-18, score-0.512]
</p><p>13 based on low-level features of the image such as appearance uniqueness in pixel or superpixel level (See Sec. [sent-26, score-0.284]
</p><p>14 One basic idea is to derive the saliency value from the local contrast of various channels, such as in terms of uniqueness defined in [29]. [sent-28, score-0.71]
</p><p>15 While uniqueness often helps generate good saliency detection results, it sometimes produces high values for non-salient regions, especially for regions with complex structures. [sent-29, score-0.779]
</p><p>16 As a result, it is desired to integrate complementary cues to address the issue. [sent-30, score-0.106]
</p><p>17 Inspired by the above discussion, in this paper we propose integrating two additional cues, focusness and objectness to improve salient region detection. [sent-31, score-1.106]
</p><p>18 First, it is commonly observed that objects of interest in an image are often photographed in focus. [sent-32, score-0.051]
</p><p>19 We derive an  algorithm for focusness estimation by treating focusness as a reciprocal of blurriness, which is in turn estimated by the scale of edges using scale-space analysis. [sent-36, score-1.421]
</p><p>20 Second, intuitively, a salient region usually completes objects instead of cutting them into pieces. [sent-37, score-0.225]
</p><p>21 This suggests us to use object completeness as a cue to boost the salient region detection. [sent-38, score-0.236]
</p><p>22 The recently proposed objectness estimation method [3] serves well for this purpose by providing the likelihood that a region belongs to an object. [sent-39, score-0.283]
</p><p>23 11997766  Combining focusness and objectness with uniqueness, we propose a new salient region detection algorithm, named UFO saliency, which naturally addresses the aforementioned issues in salient region detection. [sent-40, score-1.333]
</p><p>24 Saliency Detection According to [26, 35], saliency can be computed either in a bottom-up fashion using low level features or in a topdown fashion driven by specific tasks. [sent-47, score-0.513]
</p><p>25 Many early works approach the problem of saliency de-  tection with bottom-up methods. [sent-48, score-0.457]
</p><p>26 [19] suggest that saliency is determined by center-surround contrast of low-level features. [sent-50, score-0.433]
</p><p>27 [14] define image saliency using a Difference of Gaussians approach. [sent-52, score-0.433]
</p><p>28 Motivated by this work, some approaches were proposed later which combine local, regional and global contrast-based features [1, 12, 22, 24]. [sent-53, score-0.025]
</p><p>29 Also some methods turn to the frequency domain to search for saliency cues [10, 13, 21]. [sent-54, score-0.509]
</p><p>30 The above methods strive to highlight the object boundaries without propagating saliency to the areas inside, limiting their applicability for some vision tasks like segmentation. [sent-55, score-0.485]
</p><p>31 Later on, many works were proposed which utilize various types of features in a global scope for saliency detection. [sent-56, score-0.433]
</p><p>32 Zhai and Shah [40] compute pixel-level saliency using the luminance information. [sent-57, score-0.455]
</p><p>33 [2] achieve globally consistent results by defining pixel’s color difference from the average image color. [sent-59, score-0.04]
</p><p>34 However, these two methods do not take full advantage of color information and therefore may not give good results for images (e. [sent-60, score-0.04]
</p><p>35 [7] study color contrast in the Lab color space and measure the contrast in the global scope. [sent-64, score-0.08]
</p><p>36 Depth cues are also introduced to saliency analysis by Niu et al. [sent-68, score-0.501]
</p><p>37 These methods heavily depend on color information and therefore may not work well for images with not much color variation, especially when foreground and  background objects have similar colors. [sent-71, score-0.08]
</p><p>38 High-level information from priors and/or special object detectors (e. [sent-73, score-0.03]
</p><p>39 [18] integrate high-level information, making their methods potentially suitable for specific tasks. [sent-84, score-0.034]
</p><p>40 Shen and Wu [34] unify the higher-level priors to a low rank matrix recovery framework. [sent-85, score-0.075]
</p><p>41 As a fast evolving topic, there are many other emerging saliency detection approaches worth notice. [sent-86, score-0.51]
</p><p>42 They find that combining evidences (features) from existing approaches may enhance the saliency detection accuracy. [sent-90, score-0.508]
</p><p>43 On the other hand, their experiment also shows that simple feature combination does not guarantee the improvement of saliency detection accuracy, suggesting  that the widely used features may not be complementary and some may even be mutually exclusive to each other. [sent-91, score-0.548]
</p><p>44 Uniqueness, Focusness and Objectness In the following we briefly summarize the work related to the three ingredients used in our approach. [sent-94, score-0.023]
</p><p>45 Uniqueness stands for the color rarity of a segmented region or pixel in a certain color space. [sent-95, score-0.193]
</p><p>46 It is worth noting that the two methods use different segmentation methods to get superpixels (regions) and the results turn out to be very different, suggesting the important role of segmentation algorithms in saliency region detection. [sent-99, score-0.656]
</p><p>47 We use the term focusness to indicate the degree of focus. [sent-100, score-0.71]
</p><p>48 Focusness of an object is usually inversely related to its degree of blur (blurriness) in the image. [sent-101, score-0.193]
</p><p>49 Focusness or blurriness has been used for many purposes such as depth recovery [41] and defocus magnification [3 1]. [sent-102, score-0.317]
</p><p>50 The blurriness is usually measured in edge regions and it is therefore a key step to propagate the blurriness information to the whole image. [sent-103, score-0.535]
</p><p>51 Bae and Durand [3 1] use colorization method to spread the edge blurriness which may work well only for regions with smooth interiors. [sent-104, score-0.373]
</p><p>52 [41] use image matting method to compute the blurriness of non-edge pixels. [sent-106, score-0.248]
</p><p>53 [30] also compute saliency by taking blur effects into account, but their method identifies blur by wavelet analysis while our solution by scale space analysis. [sent-108, score-0.703]
</p><p>54 [3], mea-  sures the likelihood of there being a complete object around a pixel or region. [sent-110, score-0.06]
</p><p>55 The measurement is calculated by fusing hybrid low level features such as multi-scale saliency, color contrast, edge density and superpixels straddling. [sent-111, score-0.11]
</p><p>56 The objectness is later used popularly in various vision tasks such as object detection [6] and image retargeting [32]. [sent-112, score-0.355]
</p><p>57 Problem Formulation and Method Overview We now formally define the problem of salient region detection studied in this paper. [sent-116, score-0.252]
</p><p>58 We denote an input color image as 퐼 : Λ → ℝ3, where Λ ⊂ ℝ2 is the set of pixels iomf a퐼g. [sent-117, score-0.07]
</p><p>59 Ta she 퐼 goal →is t oℝ compute a saliency map denoted as 풮 : Λ → ℝ, such that 풮(x) indicates the saliency value of pixel x. [sent-118, score-0.898]
</p><p>60 Given the input image 퐼, the proposed UFO saliency first calculates the three components separately, denoted as 풰 : cΛa →cul ℝte sfo trh uniqueness, oℱn : Λs →epa rℝat efolyr, fdoecnuostendess a,s a풰nd : 풪Λ : →Λ →ℝ oℝr f uonr objectness. [sent-119, score-0.456]
</p><p>61 T :h eΛ th →ree components are ,t ahendn c풪om :b Λin →ed ℝin ftoo rth oeb fjiencatln saliency 풮 th. [sent-120, score-0.52]
</p><p>62 A good segmentation for our task should reduce broken edges and generate regions with proper granularity. [sent-127, score-0.098]
</p><p>63 In the following subsections we give details on how to calculate each component and hwo to combine them for the final result. [sent-129, score-0.03]
</p><p>64 In general, sharp edges of an object may get spatially blurred when projected to the image plane. [sent-133, score-0.105]
</p><p>65 There are three main types of blur: penumbral blur at the edge of a shadow, focal blur due to finite depth of field and shading blur at the edge of a smooth object [8]. [sent-134, score-0.519]
</p><p>66 Focal blur occurs when a point is out of focus, as illustrated in Fig. [sent-135, score-0.135]
</p><p>67 When the point is placed at the focus distance, 푑푓, from the lens, all the rays from it converge to a single sensor point and the image will appear sharp. [sent-137, score-0.062]
</p><p>68 Otherwise, when 푑 푑푓, these rays will generate a blurred image ,in w thheen sensor area. [sent-138, score-0.136]
</p><p>69 The blur pattern generated this way is called the circle of confusion (CoC), whose size is determined by the diameter 푐. [sent-139, score-0.178]
</p><p>70 The focusness can be derived from the degree of blur. [sent-140, score-0.71]
</p><p>71 The effect of focus/defocus is often easier to be identified from edges than from object interiors. [sent-141, score-0.031]
</p><p>72 According to [8], the degree of blur can be measured by the distance between  =  each pair of minima and maxima of the second derivative responses of the blurred edge. [sent-142, score-0.241]
</p><p>73 In practice, however, second derivatives are often sensitive to noise and clutter edges. [sent-143, score-0.027]
</p><p>74 Therefore, it is often hard to accurately localize extrema of the second derivatives [3 1]. [sent-144, score-0.082]
</p><p>75 The defocus blur can be modeled as the convolution of a sharp image [28], denoted by 퐸(x), with a point spread function (PSF) approximated by a Gaussian kernel  √21휋휎exp(−2∣휎x∣22). [sent-147, score-0.237]
</p><p>76 Φ(x,휎) = The scale 휎 = 휆푐 is proportional to the CoC diameter 푐, and can be used to measure the degree of blur. [sent-148, score-0.101]
</p><p>77 Consequently, the estimation of focusness relies on the estimation of the scale of edges, i. [sent-149, score-0.652]
</p><p>78 Inspired by Lindeberg’s seminal work on scale estimation [20], we derive an approach for estimating 휎. [sent-152, score-0.025]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('focusness', 0.652), ('saliency', 0.433), ('uniqueness', 0.252), ('objectness', 0.229), ('blurriness', 0.225), ('ufo', 0.181), ('salient', 0.146), ('blur', 0.135), ('coc', 0.064), ('sdu', 0.064), ('degree', 0.058), ('temple', 0.056), ('peng', 0.055), ('region', 0.054), ('resizing', 0.053), ('detection', 0.052), ('photographed', 0.051), ('perazzi', 0.051), ('blurred', 0.048), ('attract', 0.044), ('cheng', 0.044), ('diameter', 0.043), ('defocus', 0.043), ('edge', 0.043), ('regions', 0.042), ('cues', 0.041), ('color', 0.04), ('completeness', 0.036), ('rays', 0.036), ('turn', 0.035), ('integrate', 0.034), ('spread', 0.033), ('suggesting', 0.032), ('ftoo', 0.032), ('haibin', 0.032), ('hbl', 0.032), ('thumbnailing', 0.032), ('pixel', 0.032), ('edges', 0.031), ('complementary', 0.031), ('priors', 0.03), ('subsections', 0.03), ('delaware', 0.03), ('jingyi', 0.03), ('newark', 0.03), ('iomf', 0.03), ('colorization', 0.03), ('zhuo', 0.03), ('lindeberg', 0.03), ('ahendn', 0.03), ('humans', 0.03), ('focal', 0.028), ('lang', 0.028), ('sures', 0.028), ('strive', 0.028), ('extrema', 0.028), ('prioritize', 0.028), ('capability', 0.028), ('attention', 0.027), ('superpixels', 0.027), ('derivatives', 0.027), ('et', 0.027), ('localize', 0.027), ('rarity', 0.027), ('magnification', 0.027), ('zhai', 0.027), ('fashion', 0.027), ('sensor', 0.026), ('sharp', 0.026), ('retargeting', 0.026), ('thheen', 0.026), ('niu', 0.026), ('reciprocal', 0.026), ('philadelphia', 0.026), ('topdown', 0.026), ('worth', 0.025), ('segmentation', 0.025), ('derive', 0.025), ('regional', 0.025), ('completes', 0.025), ('oeb', 0.025), ('integrating', 0.025), ('cn', 0.024), ('tection', 0.024), ('popularly', 0.024), ('tasks', 0.024), ('evidences', 0.023), ('ingredients', 0.023), ('goferman', 0.023), ('unify', 0.023), ('matting', 0.023), ('yis', 0.023), ('borji', 0.023), ('stimuli', 0.023), ('sfo', 0.023), ('recovery', 0.022), ('mai', 0.022), ('luminance', 0.022), ('bae', 0.022), ('judd', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="374-tfidf-1" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>Author: Peng Jiang, Haibin Ling, Jingyi Yu, Jingliang Peng</p><p>Abstract: The goal of saliency detection is to locate important pixels or regions in an image which attract humans ’ visual attention the most. This is a fundamental task whose output may serve as the basis for further computer vision tasks like segmentation, resizing, tracking and so forth. In this paper we propose a novel salient region detection algorithm by integrating three important visual cues namely uniqueness, focusness and objectness (UFO). In particular, uniqueness captures the appearance-derived visual contrast; focusness reflects the fact that salient regions are often photographed in focus; and objectness helps keep completeness of detected salient regions. While uniqueness has been used for saliency detection for long, it is new to integrate focusness and objectness for this purpose. In fact, focusness and objectness both provide important saliency information complementary of uniqueness. In our experiments using public benchmark datasets, we show that, even with a simple pixel level combination of the three components, the proposed approach yields significant improve- ment compared with previously reported methods.</p><p>2 0.46455175 <a title="374-tfidf-2" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>Author: Yangqing Jia, Mei Han</p><p>Abstract: It is known that purely low-level saliency cues such as frequency does not lead to a good salient object detection result, requiring high-level knowledge to be adopted for successful discovery of task-independent salient objects. In this paper, we propose an efficient way to combine such high-level saliency priors and low-level appearance models. We obtain the high-level saliency prior with the objectness algorithm to find potential object candidates without the need of category information, and then enforce the consistency among the salient regions using a Gaussian MRF with the weights scaled by diverse density that emphasizes the influence of potential foreground pixels. Our model obtains saliency maps that assign high scores for the whole salient object, and achieves state-of-the-art performance on benchmark datasets covering various foreground statistics.</p><p>3 0.37581056 <a title="374-tfidf-3" href="./iccv-2013-Saliency_Detection_via_Dense_and_Sparse_Reconstruction.html">372 iccv-2013-Saliency Detection via Dense and Sparse Reconstruction</a></p>
<p>Author: Xiaohui Li, Huchuan Lu, Lihe Zhang, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: In this paper, we propose a visual saliency detection algorithm from the perspective of reconstruction errors. The image boundaries are first extracted via superpixels as likely cues for background templates, from which dense and sparse appearance models are constructed. For each image region, we first compute dense and sparse reconstruction errors. Second, the reconstruction errors are propagated based on the contexts obtained from K-means clustering. Third, pixel-level saliency is computed by an integration of multi-scale reconstruction errors and refined by an object-biased Gaussian model. We apply the Bayes formula to integrate saliency measures based on dense and sparse reconstruction errors. Experimental results show that the proposed algorithm performs favorably against seventeen state-of-the-art methods in terms of precision and recall. In addition, the proposed algorithm is demonstrated to be more effective in highlighting salient objects uniformly and robust to background noise.</p><p>4 0.33756223 <a title="374-tfidf-4" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>Author: Xi Li, Yao Li, Chunhua Shen, Anthony Dick, Anton Van_Den_Hengel</p><p>Abstract: Salient object detection aims to locate objects that capture human attention within images. Previous approaches often pose this as a problem of image contrast analysis. In this work, we model an image as a hypergraph that utilizes a set of hyperedges to capture the contextual properties of image pixels or regions. As a result, the problem of salient object detection becomes one of finding salient vertices and hyperedges in the hypergraph. The main advantage of hypergraph modeling is that it takes into account each pixel’s (or region ’s) affinity with its neighborhood as well as its separation from image background. Furthermore, we propose an alternative approach based on centerversus-surround contextual contrast analysis, which performs salient object detection by optimizing a cost-sensitive support vector machine (SVM) objective function. Experimental results on four challenging datasets demonstrate the effectiveness of the proposed approaches against the stateof-the-art approaches to salient object detection.</p><p>5 0.28043875 <a title="374-tfidf-5" href="./iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction.html">50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</a></p>
<p>Author: Ali Borji, Hamed R. Tavakoli, Dicky N. Sihite, Laurent Itti</p><p>Abstract: Significant recent progress has been made in developing high-quality saliency models. However, less effort has been undertaken on fair assessment of these models, over large standardized datasets and correctly addressing confounding factors. In this study, we pursue a critical and quantitative look at challenges (e.g., center-bias, map smoothing) in saliency modeling and the way they affect model accuracy. We quantitatively compare 32 state-of-the-art models (using the shuffled AUC score to discount center-bias) on 4 benchmark eye movement datasets, for prediction of human fixation locations and scanpath sequence. We also account for the role of map smoothing. We find that, although model rankings vary, some (e.g., AWS, LG, AIM, and HouNIPS) consistently outperform other models over all datasets. Some models work well for prediction of both fixation locations and scanpath sequence (e.g., Judd, GBVS). Our results show low prediction accuracy for models over emotional stimuli from the NUSEF dataset. Our last benchmark, for the first time, gauges the ability of models to decode the stimulus category from statistics of fixations, saccades, and model saliency values at fixated locations. In this test, ITTI and AIM models win over other models. Our benchmark provides a comprehensive high-level picture of the strengths and weaknesses of many popular models, and suggests future research directions in saliency modeling.</p><p>6 0.26270971 <a title="374-tfidf-6" href="./iccv-2013-Saliency_and_Human_Fixations%3A_State-of-the-Art_and_Study_of_Comparison_Metrics.html">373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</a></p>
<p>7 0.25502333 <a title="374-tfidf-7" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>8 0.23842002 <a title="374-tfidf-8" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>9 0.21519157 <a title="374-tfidf-9" href="./iccv-2013-Saliency_Detection_via_Absorbing_Markov_Chain.html">371 iccv-2013-Saliency Detection via Absorbing Markov Chain</a></p>
<p>10 0.20814528 <a title="374-tfidf-10" href="./iccv-2013-Saliency_Detection_in_Large_Point_Sets.html">370 iccv-2013-Saliency Detection in Large Point Sets</a></p>
<p>11 0.19274282 <a title="374-tfidf-11" href="./iccv-2013-Saliency_Detection%3A_A_Boolean_Map_Approach.html">369 iccv-2013-Saliency Detection: A Boolean Map Approach</a></p>
<p>12 0.19089252 <a title="374-tfidf-12" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>13 0.14947973 <a title="374-tfidf-13" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>14 0.13604961 <a title="374-tfidf-14" href="./iccv-2013-Semantically-Based_Human_Scanpath_Estimation_with_HMMs.html">381 iccv-2013-Semantically-Based Human Scanpath Estimation with HMMs</a></p>
<p>15 0.097640082 <a title="374-tfidf-15" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>16 0.087995209 <a title="374-tfidf-16" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>17 0.084686741 <a title="374-tfidf-17" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>18 0.075533636 <a title="374-tfidf-18" href="./iccv-2013-Deblurring_by_Example_Using_Dense_Correspondence.html">103 iccv-2013-Deblurring by Example Using Dense Correspondence</a></p>
<p>19 0.074668773 <a title="374-tfidf-19" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>20 0.068689838 <a title="374-tfidf-20" href="./iccv-2013-Predicting_Primary_Gaze_Behavior_Using_Social_Saliency_Fields.html">325 iccv-2013-Predicting Primary Gaze Behavior Using Social Saliency Fields</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.132), (1, -0.08), (2, 0.439), (3, -0.205), (4, -0.142), (5, 0.002), (6, 0.033), (7, -0.067), (8, 0.03), (9, -0.02), (10, -0.035), (11, -0.013), (12, 0.049), (13, -0.073), (14, -0.025), (15, -0.048), (16, 0.096), (17, -0.051), (18, -0.068), (19, 0.062), (20, -0.003), (21, -0.011), (22, 0.004), (23, -0.013), (24, 0.006), (25, -0.05), (26, -0.001), (27, 0.005), (28, -0.007), (29, -0.03), (30, -0.016), (31, 0.037), (32, -0.026), (33, -0.004), (34, 0.038), (35, -0.044), (36, -0.002), (37, 0.022), (38, -0.005), (39, 0.017), (40, 0.006), (41, -0.014), (42, 0.012), (43, 0.029), (44, -0.016), (45, -0.005), (46, 0.065), (47, 0.013), (48, 0.026), (49, -0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95910549 <a title="374-lsi-1" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>Author: Peng Jiang, Haibin Ling, Jingyi Yu, Jingliang Peng</p><p>Abstract: The goal of saliency detection is to locate important pixels or regions in an image which attract humans ’ visual attention the most. This is a fundamental task whose output may serve as the basis for further computer vision tasks like segmentation, resizing, tracking and so forth. In this paper we propose a novel salient region detection algorithm by integrating three important visual cues namely uniqueness, focusness and objectness (UFO). In particular, uniqueness captures the appearance-derived visual contrast; focusness reflects the fact that salient regions are often photographed in focus; and objectness helps keep completeness of detected salient regions. While uniqueness has been used for saliency detection for long, it is new to integrate focusness and objectness for this purpose. In fact, focusness and objectness both provide important saliency information complementary of uniqueness. In our experiments using public benchmark datasets, we show that, even with a simple pixel level combination of the three components, the proposed approach yields significant improve- ment compared with previously reported methods.</p><p>2 0.93437475 <a title="374-lsi-2" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>Author: Xi Li, Yao Li, Chunhua Shen, Anthony Dick, Anton Van_Den_Hengel</p><p>Abstract: Salient object detection aims to locate objects that capture human attention within images. Previous approaches often pose this as a problem of image contrast analysis. In this work, we model an image as a hypergraph that utilizes a set of hyperedges to capture the contextual properties of image pixels or regions. As a result, the problem of salient object detection becomes one of finding salient vertices and hyperedges in the hypergraph. The main advantage of hypergraph modeling is that it takes into account each pixel’s (or region ’s) affinity with its neighborhood as well as its separation from image background. Furthermore, we propose an alternative approach based on centerversus-surround contextual contrast analysis, which performs salient object detection by optimizing a cost-sensitive support vector machine (SVM) objective function. Experimental results on four challenging datasets demonstrate the effectiveness of the proposed approaches against the stateof-the-art approaches to salient object detection.</p><p>3 0.91483903 <a title="374-lsi-3" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>Author: Yangqing Jia, Mei Han</p><p>Abstract: It is known that purely low-level saliency cues such as frequency does not lead to a good salient object detection result, requiring high-level knowledge to be adopted for successful discovery of task-independent salient objects. In this paper, we propose an efficient way to combine such high-level saliency priors and low-level appearance models. We obtain the high-level saliency prior with the objectness algorithm to find potential object candidates without the need of category information, and then enforce the consistency among the salient regions using a Gaussian MRF with the weights scaled by diverse density that emphasizes the influence of potential foreground pixels. Our model obtains saliency maps that assign high scores for the whole salient object, and achieves state-of-the-art performance on benchmark datasets covering various foreground statistics.</p><p>4 0.89626288 <a title="374-lsi-4" href="./iccv-2013-Saliency_Detection%3A_A_Boolean_Map_Approach.html">369 iccv-2013-Saliency Detection: A Boolean Map Approach</a></p>
<p>Author: Jianming Zhang, Stan Sclaroff</p><p>Abstract: A novel Boolean Map based Saliency (BMS) model is proposed. An image is characterized by a set of binary images, which are generated by randomly thresholding the image ’s color channels. Based on a Gestalt principle of figure-ground segregation, BMS computes saliency maps by analyzing the topological structure of Boolean maps. BMS is simple to implement and efficient to run. Despite its simplicity, BMS consistently achieves state-of-the-art performance compared with ten leading methods on five eye tracking datasets. Furthermore, BMS is also shown to be advantageous in salient object detection.</p><p>5 0.86657357 <a title="374-lsi-5" href="./iccv-2013-Saliency_Detection_via_Dense_and_Sparse_Reconstruction.html">372 iccv-2013-Saliency Detection via Dense and Sparse Reconstruction</a></p>
<p>Author: Xiaohui Li, Huchuan Lu, Lihe Zhang, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: In this paper, we propose a visual saliency detection algorithm from the perspective of reconstruction errors. The image boundaries are first extracted via superpixels as likely cues for background templates, from which dense and sparse appearance models are constructed. For each image region, we first compute dense and sparse reconstruction errors. Second, the reconstruction errors are propagated based on the contexts obtained from K-means clustering. Third, pixel-level saliency is computed by an integration of multi-scale reconstruction errors and refined by an object-biased Gaussian model. We apply the Bayes formula to integrate saliency measures based on dense and sparse reconstruction errors. Experimental results show that the proposed algorithm performs favorably against seventeen state-of-the-art methods in terms of precision and recall. In addition, the proposed algorithm is demonstrated to be more effective in highlighting salient objects uniformly and robust to background noise.</p><p>6 0.85391968 <a title="374-lsi-6" href="./iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction.html">50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</a></p>
<p>7 0.84738696 <a title="374-lsi-7" href="./iccv-2013-Saliency_and_Human_Fixations%3A_State-of-the-Art_and_Study_of_Comparison_Metrics.html">373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</a></p>
<p>8 0.83269823 <a title="374-lsi-8" href="./iccv-2013-Saliency_Detection_via_Absorbing_Markov_Chain.html">371 iccv-2013-Saliency Detection via Absorbing Markov Chain</a></p>
<p>9 0.8077243 <a title="374-lsi-9" href="./iccv-2013-Saliency_Detection_in_Large_Point_Sets.html">370 iccv-2013-Saliency Detection in Large Point Sets</a></p>
<p>10 0.8059538 <a title="374-lsi-10" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>11 0.71928418 <a title="374-lsi-11" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>12 0.59733009 <a title="374-lsi-12" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>13 0.42643896 <a title="374-lsi-13" href="./iccv-2013-Semantically-Based_Human_Scanpath_Estimation_with_HMMs.html">381 iccv-2013-Semantically-Based Human Scanpath Estimation with HMMs</a></p>
<p>14 0.30009094 <a title="374-lsi-14" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>15 0.28856254 <a title="374-lsi-15" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>16 0.28005096 <a title="374-lsi-16" href="./iccv-2013-Predicting_Primary_Gaze_Behavior_Using_Social_Saliency_Fields.html">325 iccv-2013-Predicting Primary Gaze Behavior Using Social Saliency Fields</a></p>
<p>17 0.26871148 <a title="374-lsi-17" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>18 0.26638892 <a title="374-lsi-18" href="./iccv-2013-Deblurring_by_Example_Using_Dense_Correspondence.html">103 iccv-2013-Deblurring by Example Using Dense Correspondence</a></p>
<p>19 0.25616875 <a title="374-lsi-19" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>20 0.24467021 <a title="374-lsi-20" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.478), (7, 0.019), (26, 0.082), (31, 0.034), (42, 0.066), (64, 0.018), (73, 0.026), (78, 0.012), (89, 0.154)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94323218 <a title="374-lda-1" href="./iccv-2013-A_General_Two-Step_Approach_to_Learning-Based_Hashing.html">13 iccv-2013-A General Two-Step Approach to Learning-Based Hashing</a></p>
<p>Author: Guosheng Lin, Chunhua Shen, David Suter, Anton van_den_Hengel</p><p>Abstract: Most existing approaches to hashing apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to respond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. This framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problemspecific hashing methods. Our framework decomposes the hashing learning problem into two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically be formulated as binary quadratic problems, and the second step can be accomplished by training standard binary classifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate that the proposed framework is effective, flexible and outperforms the state-of-the-art.</p><p>2 0.93202716 <a title="374-lda-2" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>Author: Jayaguru Panda, Michael S. Brown, C.V. Jawahar</p><p>Abstract: Existing mobile image instance retrieval applications assume a network-based usage where image features are sent to a server to query an online visual database. In this scenario, there are no restrictions on the size of the visual database. This paper, however, examines how to perform this same task offline, where the entire visual index must reside on the mobile device itself within a small memory footprint. Such solutions have applications on location recognition and product recognition. Mobile instance retrieval requires a significant reduction in the visual index size. To achieve this, we describe a set of strategies that can reduce the visual index up to 60-80 compared to a scatannd raerddu iens tthaen vceis rueatrli ienvdaelx xim upple tom 6en0t-8at0io ×n found on ddte osk atops or servers. While our proposed reduction steps affect the overall mean Average Precision (mAP), they are able to maintain a good Precision for the top K results (PK). We argue that for such offline application, maintaining a good PK is sufficient. The effectiveness of this approach is demonstrated on several standard databases. A working application designed for a remote historical site is also presented. This application is able to reduce an 50,000 image index structure to 25 MBs while providing a precision of 97% for P10 and 100% for P1.</p><p>3 0.89874369 <a title="374-lda-3" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<p>Author: Shi Qiu, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: This paper proposes modeling the complex web image collections with an automatically generated graph structure called visual semantic complex network (VSCN). The nodes on this complex network are clusters of images with both visual and semantic consistency, called semantic concepts. These nodes are connected based on the visual and semantic correlations. Our VSCN with 33, 240 concepts is generated from a collection of 10 million web images. 1 A great deal of valuable information on the structures of the web image collections can be revealed by exploring the VSCN, such as the small-world behavior, concept community, indegree distribution, hubs, and isolated concepts. It not only helps us better understand the web image collections at a macroscopic level, but also has many important practical applications. This paper presents two application examples: content-based image retrieval and image browsing. Experimental results show that the VSCN leads to significant improvement on both the precision of image retrieval (over 200%) and user experience for image browsing.</p><p>4 0.89591217 <a title="374-lda-4" href="./iccv-2013-Revisiting_Example_Dependent_Cost-Sensitive_Learning_with_Decision_Trees.html">352 iccv-2013-Revisiting Example Dependent Cost-Sensitive Learning with Decision Trees</a></p>
<p>Author: Oisin Mac Aodha, Gabriel J. Brostow</p><p>Abstract: Typical approaches to classification treat class labels as disjoint. For each training example, it is assumed that there is only one class label that correctly describes it, and that all other labels are equally bad. We know however, that good and bad labels are too simplistic in many scenarios, hurting accuracy. In the realm of example dependent costsensitive learning, each label is instead a vector representing a data point’s affinity for each of the classes. At test time, our goal is not to minimize the misclassification rate, but to maximize that affinity. We propose a novel example dependent cost-sensitive impurity measure for decision trees. Our experiments show that this new impurity measure improves test performance while still retaining the fast test times of standard classification trees. We compare our approach to classification trees and other cost-sensitive methods on three computer vision problems, tracking, descriptor matching, and optical flow, and show improvements in all three domains.</p><p>5 0.88573498 <a title="374-lda-5" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>Author: Arash Vahdat, Greg Mori</p><p>Abstract: Gathering accurate training data for recognizing a set of attributes or tags on images or videos is a challenge. Obtaining labels via manual effort or from weakly-supervised data typically results in noisy training labels. We develop the FlipSVM, a novel algorithm for handling these noisy, structured labels. The FlipSVM models label noise by “flipping ” labels on training examples. We show empirically that the FlipSVM is effective on images-and-attributes and video tagging datasets.</p><p>6 0.87096488 <a title="374-lda-6" href="./iccv-2013-Improving_Graph_Matching_via_Density_Maximization.html">214 iccv-2013-Improving Graph Matching via Density Maximization</a></p>
<p>7 0.86374241 <a title="374-lda-7" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>same-paper 8 0.84836954 <a title="374-lda-8" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>9 0.78930545 <a title="374-lda-9" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>10 0.77186692 <a title="374-lda-10" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>11 0.7643066 <a title="374-lda-11" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>12 0.76105779 <a title="374-lda-12" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>13 0.75851798 <a title="374-lda-13" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>14 0.74372864 <a title="374-lda-14" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>15 0.70456612 <a title="374-lda-15" href="./iccv-2013-Person_Re-identification_by_Salience_Matching.html">313 iccv-2013-Person Re-identification by Salience Matching</a></p>
<p>16 0.70314509 <a title="374-lda-16" href="./iccv-2013-Learning_to_Rank_Using_Privileged_Information.html">248 iccv-2013-Learning to Rank Using Privileged Information</a></p>
<p>17 0.70073366 <a title="374-lda-17" href="./iccv-2013-Class-Specific_Simplex-Latent_Dirichlet_Allocation_for_Image_Classification.html">73 iccv-2013-Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification</a></p>
<p>18 0.69631588 <a title="374-lda-18" href="./iccv-2013-SYM-FISH%3A_A_Symmetry-Aware_Flip_Invariant_Sketch_Histogram_Shape_Descriptor.html">368 iccv-2013-SYM-FISH: A Symmetry-Aware Flip Invariant Sketch Histogram Shape Descriptor</a></p>
<p>19 0.68779838 <a title="374-lda-19" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>20 0.68073952 <a title="374-lda-20" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
