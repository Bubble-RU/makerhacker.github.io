<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>64 iccv-2013-Box in the Box: Joint 3D Layout and Object Reasoning from Single Images</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-64" href="#">iccv2013-64</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>64 iccv-2013-Box in the Box: Joint 3D Layout and Object Reasoning from Single Images</h1>
<br/><p>Source: <a title="iccv-2013-64-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Schwing_Box_in_the_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Alexander G. Schwing, Sanja Fidler, Marc Pollefeys, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly infer the room layout as well as the objects present in the scene. Towards this goal, we propose a branch and bound algorithm which is guaranteed to retrieve the global optimum of the joint problem. The main difficulty resides in taking into account occlusion in order to not over-count the evidence. We introduce a new decomposition method, which generalizes integral geometry to triangular shapes, and allows us to bound the different terms in constant time. We exploit both geometric cues and object detectors as image features and show large improvements in 2D and 3D object detection over state-of-the-art deformable part-based models.</p><p>Reference: <a title="iccv-2013-64-reference" href="../iccv2013_reference/iccv-2013-Box_in_the_Box%3A_Joint_3D_Layout_and_Object_Reasoning_from_Single_Images_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu c  Abstract In this paper we propose an approach to jointly infer the room layout as well as the objects present in the scene. [sent-5, score-0.699]
</p><p>2 Towards this goal, we propose a branch and bound algorithm which is guaranteed to retrieve the global optimum of the joint problem. [sent-6, score-0.459]
</p><p>3 We introduce a new decomposition method, which generalizes integral geometry to triangular shapes, and allows us to bound the different terms in constant time. [sent-8, score-0.347]
</p><p>4 We exploit both geometric cues and object detectors as image features and show large improvements in 2D and 3D object detection over state-of-the-art deformable part-based models. [sent-9, score-0.266]
</p><p>5 In the past few years, a variety of approaches have been proposed in order to extract the 3D layout of rooms from single images [12, 13, 3 1, 19, 22, 25, 26]. [sent-13, score-0.635]
</p><p>6 Common to all these approaches is the use of the Manhattan world properties of indoor scenes, which assume that the room is aligned with the three dominant and orthogonal directions, defined by the vanishing points. [sent-14, score-0.29]
</p><p>7 As a consequence a simple parameterization exists, since, given the vanishing points, only 4 degrees of freedom are needed to represent the layout [12, 3 1]. [sent-15, score-0.908]
</p><p>8 By exploiting the inherent decomposition of additive energy functions, real-time inference was shown to be possible with this parameterization [25, 26]. [sent-16, score-0.268]
</p><p>9 The first attempts to incorporate object reasoning into semantic parsing of indoor scenes treated objects as clutter, and focused on removing them from the layout estimation [12, 3 1]. [sent-18, score-0.847]
</p><p>10 Similarly, if we could estimate the layout we should be able to better parse the objects. [sent-25, score-0.568]
</p><p>11 Alternatively, the layout has been employed to better detect objects. [sent-28, score-0.568]
</p><p>12 In contrast, in this paper we jointly reason about both the exponentially many layouts as well as the exponentially many object locations and sizes. [sent-33, score-0.239]
</p><p>13 Towards this goal, we propose an algorithm based on branch and bound, which is guaranteed to give a globally optimal solution of the joint problem. [sent-37, score-0.248]
</p><p>14 In order to compute the bounds in constant time and in order to be able to handle occlusion, we generalize the concept of integral geometry [25] to triangular shapes. [sent-38, score-0.416]
</p><p>15 Jointly inferring room layout and 3D object with occlusion reasoning:  The parameterization is indicated in (a) while the  object and the layout evidence are illustrated in (b) and (c) respectively. [sent-41, score-1.637]
</p><p>16 While outdoor scenarios remain fairly unexplored, estimating the 3D layout of indoor scenes has experienced increasing popularity. [sent-47, score-0.652]
</p><p>17 Most monocular approaches approximate the layout of rooms by 3D cuboids [12, 19, 3 1, 13, 25, 14, 26]. [sent-49, score-0.681]
</p><p>18 A notable exception is [20], which estimates the 3D layout of corridors by sweeping lines. [sent-50, score-0.568]
</p><p>19 Early approaches to 3D layout estimation [12, 19] reduce the complexity of the problem by utilizing a set of candidates. [sent-51, score-0.568]
</p><p>20 More recently, Schwing and Urtasun [26] showed that the global optimum of typical layout scoring  functions is obtained by employing a branch and bound approach. [sent-60, score-0.977]
</p><p>21 In this paper we make use of 2D and 3D deformable part-based models in order to estimate jointly the layout as well as the objects present in the scene. [sent-65, score-0.657]
</p><p>22 Objects and layout were combined in [3 1, 19, 13], and used in [11] to predict affordances as well as to investigate the interaction between humans and objects [3]. [sent-66, score-0.607]
</p><p>23 While [3 1] is more concerned about predicting ‘clutter’ rather than actual objects, [19] proposes to augment the space of layout candidates by a set of possible objects that are chosen to be either present or absent. [sent-67, score-0.641]
</p><p>24 , the product space of object and layout candidates) increases tremendously, a heuristic optimization with beam search is performed. [sent-70, score-0.654]
</p><p>25 In [13] the layout prediction is used to guide a 3D object detector. [sent-71, score-0.709]
</p><p>26 , object and layout prediction) by proposing a small set of candidates. [sent-74, score-0.654]
</p><p>27 In contrast, in this paper we propose  a provably exact solution to the joint problem, which reasons about the exponentially many layouts as well as the exponentially many object locations and sizes. [sent-76, score-0.342]
</p><p>28 Since the complexity is at least five orders of magnitude larger than a standard layout task the problem is much more difficult to solve. [sent-77, score-0.568]
</p><p>29 Approach We are interested in predicting the layout of the room as well as the objects present in the scene from monocular imagery. [sent-81, score-0.733]
</p><p>30 (a) Front face of an object is occluding the floor (blue color in (b)). [sent-84, score-0.301]
</p><p>31 Decomposition of the triangle in (c) into two positive parts (e) and (f) and a negative part (g) all depending on only two angles illustrating the generalization of integral geometry to triangular shapes, i. [sent-86, score-0.265]
</p><p>32 We advocate for a joint approach, as we would like to exploit the relationships that exist between the layout and object prediction tasks. [sent-90, score-0.776]
</p><p>33 Towards this goal, we propose a branch and bound approach, which is guaranteed to find the global optimum of the energy representing the joint problem. [sent-92, score-0.564]
</p><p>34 Joint layout-object problem More formally, given an image x, we are interested in predicting the layout y ∈ Y as well as the object z ∈ Z present ning tthhee scene. [sent-96, score-0.688]
</p><p>35 As geometric cues, we employ orientation maps (OM) [20] and geometric context (GC) [12], as they were shown to produce impressive results on the layout task [19, 25, 26]. [sent-98, score-0.656]
</p><p>36 We define the energy of a joint configuration as the sum of layout and object energies. [sent-105, score-0.826]
</p><p>37 These energies encode how well the layout and object estimates represent the image evidence. [sent-106, score-0.654]
</p><p>38 An additional term Epen (x, y, z) makes sure that objects cannot penetrate walls, and an occam razor term Eoccam (x, z) encodes the fact that we prefer simple explanations. [sent-107, score-0.249]
</p><p>39 Note that the energy of the layout depends on the 3D location and size of the object. [sent-110, score-0.673]
</p><p>40 This is due to the fact that the layout should only explain the image evidence that has not yet been explained by the object, as the object occludes the layout (see Fig. [sent-111, score-1.256]
</p><p>41 We thus first compute vanish-  ing points (VP), and perform joint inference over the remaining degrees of freedom. [sent-115, score-0.181]
</p><p>42 [3 1] showed that given the VPs only 4 degrees of freedom are necessary to represent the layout, consisting of four rays originating from two distinct vanishing points. [sent-118, score-0.244]
</p><p>43 In the case of an object, given the VPs, only 5 degrees of freedom are necessary, consisting of three rays originating from one VP and two rays from another. [sent-119, score-0.183]
</p><p>44 Object Energy: We define an additive energy which decomposes over the faces of the object, summing the evidence inside each face as illustrated in Fig. [sent-124, score-0.353]
</p><p>45 We define the features for each face to be weighted counts of image cues inside that face, as this will allow us to compute bounds in constant  time. [sent-141, score-0.36]
</p><p>46 Layout Energy:  The layout energy is defined as  Elay−occ(x, y, z)  = Elayout(x,  y) − Eocc(x, y, z),  where the last term discounts the image evidence which is already explained by the object, i. [sent-142, score-0.707]
</p><p>47 , the pixels for each layout face that are occluded by the object (see Fig. [sent-144, score-0.723]
</p><p>48 We define features for each face α of the layout and object occlusion as weighted counts ? [sent-146, score-0.868]
</p><p>49 3 provides the details for the case of α representing the floor and γ denoting the front face of the object. [sent-159, score-0.236]
</p><p>50 Penetration Energy: This energy makes sure that the object cannot penetrate the walls defined by the layout, i. [sent-162, score-0.298]
</p><p>51 , it equals 0 whenever the object is inside the layout and is  +∞ in the case of penetration. [sent-164, score-0.654]
</p><p>52 In practice we set the penalty to be 10% of the layout energy for the best configuration. [sent-167, score-0.673]
</p><p>53 We bound Elayout (x, y) and Eobject (x, z) by computing counts over minimal and maximal faces. [sent-169, score-0.222]
</p><p>54 Finding a global minimizer of the layout task, i. [sent-173, score-0.568]
</p><p>55 , Elayout (x, y), is possible using branch and bound [26]. [sent-175, score-0.288]
</p><p>56 In this paper we generalize this approach to solve the joint lay-  ×  out and object problem with an explicit occlusion reasoning. [sent-176, score-0.219]
</p><p>57 We now briefly describe the particular branch and bound algorithm we developed, which is inspired by the object detector of [18]. [sent-177, score-0.374]
</p><p>58 Then, it proceeds iteratively, where the most promising set on a priority queue is taken at each iteration. [sent-179, score-0.181]
</p><p>59 The algorithm terminates when the element on top of the priority queue consists of a single hypothesis. [sent-184, score-0.181]
</p><p>60 1 depicts the branch and bound algorithm more formally. [sent-186, score-0.288]
</p><p>61 In the worst case this algorithm investigates an exponential number of hypotheses, but if the bounds are tight, typically only a small fraction needs to be considered. [sent-187, score-0.212]
</p><p>62 In order to return a global optimum, the bounds have to be valid for all the elements in the sets, and the bounds have to be exact when a single hypothesis is evaluated. [sent-188, score-0.424]
</p><p>63 The bounds developed here satisfy these two properties, and thus we retrieve the global optimum of the joint problem. [sent-189, score-0.347]
</p><p>64 In order to utilize branch and bound, we need to parametrize sets of hypotheses, and derive bounds which are both efficient to compute and tight. [sent-190, score-0.405]
</p><p>65 We parameterize sets of hypotheses by intervals of the form  ××  [y1,min, y1,max] · · · [z5,min, z5,max], as such a parameterization simplifies our bounding functions. [sent-191, score-0.222]
</p><p>66 As the energy is a sum of terms, we bound each one separately and compute the final bound by summing the individual ones. [sent-197, score-0.425]
</p><p>67 We do not require to bound the penetration energy as we can simply carve the space to consider only objects which are contained within the layout. [sent-200, score-0.389]
</p><p>68 As far as the occam razor potential is concerned, we equivalently add to the priority queue the best layout  configuration found in the absence of any object with bound equal to its energy minus the penalty. [sent-201, score-1.245]
</p><p>69 Layout bounds: For the layout, we utilize the lower bounds of [26], which are obtained by dividing the layout scoring function into two parts, one containing positive weights and one containing negative weights:  Elayout(x,y) = wl+a? [sent-202, score-0.828]
</p><p>70 Lower bounds are then easily estimated by summing the smallest possible face for the positive features and the biggest possible face for the negative ones. [sent-205, score-0.384]
</p><p>71 Note that we have inverted the bounds with respect to [26] as they maximize a score (defined as the negative energy) while we minimize the energy. [sent-206, score-0.212]
</p><p>72 The bound for the right layout face is illustrated in Fig. [sent-207, score-0.825]
</p><p>73 Computing the content of maximal and minimal faces depends on the four intervals for the front face and on three intervals for all other walls, floor and ceiling. [sent-209, score-0.444]
</p><p>74 Using integral geometry [25] we decompose those functions into sums of accumulators that depend on at most two random variables. [sent-210, score-0.221]
</p><p>75 This allows computation of bounds in constant time while being memory  efficient as well. [sent-211, score-0.212]
</p><p>76 We split the energy into negative and positive components, and bound the counts using the minimally and  TopSideHullBB  loc[SO65u ]pr. [sent-213, score-0.365]
</p><p>77 All object faces na¨ ıvely depend on four intervals but, as for the layout bounds, we can utilize integral geometry [25], and, by decomposing the faces into sums of pairwise accumulators, we compute the bounds in constant time being memory efficient. [sent-223, score-1.266]
</p><p>78 Occlusion bounds: To effectively compute bounds for Eocc(x, y, z) we decompose the energy into sums over triangular faces. [sent-224, score-0.45]
</p><p>79 3 for the case of the front face of an object occluding the floor. [sent-226, score-0.266]
</p><p>80 While we have illustrated this decomposition with an example, all overlaps between object faces and layout walls are decomposed and computed in a similar manner. [sent-235, score-0.824]
</p><p>81 We then split the potentials into negative and positive and bound each with either its maximal or minimal face depending on the sign. [sent-242, score-0.254]
</p><p>82 This procedure again provides bounds computable in constant time. [sent-243, score-0.212]
</p><p>83 , the ray describing the top edge of the front face is required to be above the ray describing the bottom edge of that face. [sent-248, score-0.207]
</p><p>84 In a second  step we fix the previously obtained layout prediction ˆy and optimize Etotal w. [sent-257, score-0.623]
</p><p>85 Comparison of F1 scores and labeling error for the sparse and dense parameterization using oracle features. [sent-314, score-0.186]
</p><p>86 in performance, similar to the joint model when employing object detectors. [sent-315, score-0.206]
</p><p>87 Whereas the latter captures the performance on estimating orientation, irrespective of being part of the layout or the object, the 9-label metric takes into account this distinction, making it significantly harder. [sent-327, score-0.568]
</p><p>88 We report this measure to detect the top face (Top), all the side faces jointly (Side), the convex hull of the object as well as a 2D bounding box (BB). [sent-330, score-0.221]
</p><p>89 Illustration of prediction results (red, magenta) and best found ground truth state (blue, cyan) given vanishing points for joint  object and layout inference overlaying the image. [sent-348, score-0.961]
</p><p>90 Below each image we provide visible annotation floor plan (gray) and object on the left while corresponding prediction result on the right. [sent-349, score-0.242]
</p><p>91 3 shows that while the performance ofthe greedy approach is more or less identical when providing 9-label information, joint inference outperforms the greedy approach in the 5-label case. [sent-353, score-0.32]
</p><p>92 To illustrate the performance gain when increasing the discretization, we almost double the average number of states per layout variable from 18. [sent-356, score-0.607]
</p><p>93 4 shows the average inference time for both the greedy and joint approach when employing different types of features. [sent-365, score-0.282]
</p><p>94 The greedy approach is about two orders of magnitude faster for oracle features and three orders of magnitude faster for real features. [sent-367, score-0.185]
</p><p>95 o03v53er union of floor, object footprint and free-space for joint inference with indicated features. [sent-390, score-0.224]
</p><p>96 In general, our approach does a great job at estimating both the layout and object. [sent-396, score-0.568]
</p><p>97 Estimating multiple objects: We extend our approach to detect multiple objects in a greedy fashion, by fixing the layout and the previously detected object and solving for the next object. [sent-401, score-0.784]
</p><p>98 Conclusion  We have presented an approach to joint 3D room layout and object reasoning that predicts the optimal box within a box. [sent-405, score-0.883]
</p><p>99 To this end we carefully modeled the occlusions and phrased the problem as a structured prediction task that permits exact inference via a novel branch and bound algorithm. [sent-406, score-0.414]
</p><p>100 The main technical difficulty resides in the development of occlusion bounds which require the generalization of integral geometry to triangular shapes. [sent-407, score-0.522]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('layout', 0.568), ('gtours', 0.216), ('bounds', 0.212), ('elayout', 0.167), ('branch', 0.145), ('bound', 0.143), ('queue', 0.121), ('vanishing', 0.114), ('eobject', 0.108), ('energy', 0.105), ('floor', 0.101), ('eocc', 0.096), ('triangular', 0.094), ('oracle', 0.094), ('room', 0.092), ('parameterization', 0.092), ('greedy', 0.091), ('object', 0.086), ('indoor', 0.084), ('occam', 0.081), ('razor', 0.081), ('counts', 0.079), ('ay', 0.072), ('accumulators', 0.072), ('etotal', 0.072), ('schwing', 0.071), ('intervals', 0.071), ('inference', 0.071), ('reasoning', 0.07), ('face', 0.069), ('optimum', 0.068), ('joint', 0.067), ('rooms', 0.067), ('faces', 0.066), ('occ', 0.066), ('front', 0.066), ('occlusion', 0.066), ('hedau', 0.064), ('vps', 0.062), ('triangle', 0.061), ('priority', 0.06), ('integral', 0.059), ('walls', 0.059), ('magenta', 0.059), ('hypotheses', 0.059), ('consequence', 0.057), ('prediction', 0.055), ('boundi', 0.054), ('carve', 0.054), ('elay', 0.054), ('eoccam', 0.054), ('epen', 0.054), ('ovteorp', 0.054), ('employing', 0.053), ('rays', 0.053), ('iou', 0.052), ('exponentially', 0.052), ('geometry', 0.051), ('hoiem', 0.051), ('deformable', 0.05), ('vp', 0.049), ('fidler', 0.049), ('layouts', 0.049), ('utilize', 0.048), ('penetration', 0.048), ('gcs', 0.048), ('oms', 0.048), ('penetrate', 0.048), ('manhattan', 0.047), ('cuboids', 0.046), ('illustrated', 0.045), ('occluding', 0.045), ('gc', 0.045), ('geometric', 0.044), ('degrees', 0.043), ('potentials', 0.042), ('loc', 0.042), ('wl', 0.04), ('resides', 0.04), ('states', 0.039), ('objects', 0.039), ('sums', 0.039), ('discretization', 0.038), ('minimally', 0.038), ('bedroom', 0.038), ('wall', 0.036), ('om', 0.036), ('guaranteed', 0.036), ('put', 0.036), ('ray', 0.036), ('provably', 0.036), ('positively', 0.036), ('subtracted', 0.036), ('lay', 0.036), ('chicago', 0.035), ('summing', 0.034), ('freedom', 0.034), ('predicting', 0.034), ('evidence', 0.034), ('tti', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="64-tfidf-1" href="./iccv-2013-Box_in_the_Box%3A_Joint_3D_Layout_and_Object_Reasoning_from_Single_Images.html">64 iccv-2013-Box in the Box: Joint 3D Layout and Object Reasoning from Single Images</a></p>
<p>Author: Alexander G. Schwing, Sanja Fidler, Marc Pollefeys, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly infer the room layout as well as the objects present in the scene. Towards this goal, we propose a branch and bound algorithm which is guaranteed to retrieve the global optimum of the joint problem. The main difficulty resides in taking into account occlusion in order to not over-count the evidence. We introduce a new decomposition method, which generalizes integral geometry to triangular shapes, and allows us to bound the different terms in constant time. We exploit both geometric cues and object detectors as image features and show large improvements in 2D and 3D object detection over state-of-the-art deformable part-based models.</p><p>2 0.55934554 <a title="64-tfidf-2" href="./iccv-2013-Estimating_the_3D_Layout_of_Indoor_Scenes_and_Its_Clutter_from_Depth_Sensors.html">144 iccv-2013-Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors</a></p>
<p>Author: Jian Zhang, Chen Kan, Alexander G. Schwing, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly estimate the layout ofrooms as well as the clutterpresent in the scene using RGB-D data. Towards this goal, we propose an effective model that is able to exploit both depth and appearance features, which are complementary. Furthermore, our approach is efficient as we exploit the inherent decomposition of additive potentials. We demonstrate the effectiveness of our approach on the challenging NYU v2 dataset and show that employing depth reduces the layout error by 6% and the clutter estimation by 13%.</p><p>3 0.2688742 <a title="64-tfidf-3" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>Author: Scott Satkin, Martial Hebert</p><p>Abstract: We present a new algorithm 3DNN (3D NearestNeighbor), which is capable of matching an image with 3D data, independently of the viewpoint from which the image was captured. By leveraging rich annotations associated with each image, our algorithm can automatically produce precise and detailed 3D models of a scene from a single image. Moreover, we can transfer information across images to accurately label and segment objects in a scene. The true benefit of 3DNN compared to a traditional 2D nearest-neighbor approach is that by generalizing across viewpoints, we free ourselves from the need to have training examples captured from all possible viewpoints. Thus, we are able to achieve comparable results using orders of magnitude less data, and recognize objects from never-beforeseen viewpoints. In this work, we describe the 3DNN algorithm and rigorously evaluate its performance for the tasks of geometry estimation and object detection/segmentation. By decoupling the viewpoint and the geometry of an image, we develop a scene matching approach which is truly 100% viewpoint invariant, yielding state-of-the-art performance on challenging data.</p><p>4 0.23981112 <a title="64-tfidf-4" href="./iccv-2013-Characterizing_Layouts_of_Outdoor_Scenes_Using_Spatial_Topic_Processes.html">72 iccv-2013-Characterizing Layouts of Outdoor Scenes Using Spatial Topic Processes</a></p>
<p>Author: Dahua Lin, Jianxiong Xiao</p><p>Abstract: In this paper, we develop a generative model to describe the layouts of outdoor scenes the spatial configuration of regions. Specifically, the layout of an image is represented as a composite of regions, each associated with a semantic topic. At the heart of this model is a novel stochastic process called Spatial Topic Process, which generates a spatial map of topics from a set of coupled Gaussian processes, thus allowing the distributions of topics to vary continuously across the image plane. A key aspect that distinguishes this model from previous ones consists in its capability of capturing dependencies across both locations and topics while allowing substantial variations in the layouts. We demonstrate the practical utility of the proposed model by testing it on scene classification, semantic segmentation, and layout hallucination. –</p><p>5 0.18001601 <a title="64-tfidf-5" href="./iccv-2013-Holistic_Scene_Understanding_for_3D_Object_Detection_with_RGBD_Cameras.html">201 iccv-2013-Holistic Scene Understanding for 3D Object Detection with RGBD Cameras</a></p>
<p>Author: Dahua Lin, Sanja Fidler, Raquel Urtasun</p><p>Abstract: In this paper, we tackle the problem of indoor scene understanding using RGBD data. Towards this goal, we propose a holistic approach that exploits 2D segmentation, 3D geometry, as well as contextual relations between scenes and objects. Specifically, we extend the CPMC [3] framework to 3D in order to generate candidate cuboids, and develop a conditional random field to integrate information from different sources to classify the cuboids. With this formulation, scene classification and 3D object recognition are coupled and can be jointly solved through probabilistic inference. We test the effectiveness of our approach on the challenging NYU v2 dataset. The experimental results demonstrate that through effective evidence integration and holistic reasoning, our approach achieves substantial improvement over the state-of-the-art.</p><p>6 0.14300214 <a title="64-tfidf-6" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>7 0.13514645 <a title="64-tfidf-7" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>8 0.1308765 <a title="64-tfidf-8" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<p>9 0.12261664 <a title="64-tfidf-9" href="./iccv-2013-Find_the_Best_Path%3A_An_Efficient_and_Accurate_Classifier_for_Image_Hierarchies.html">165 iccv-2013-Find the Best Path: An Efficient and Accurate Classifier for Image Hierarchies</a></p>
<p>10 0.12146043 <a title="64-tfidf-10" href="./iccv-2013-Lifting_3D_Manhattan_Lines_from_a_Single_Image.html">250 iccv-2013-Lifting 3D Manhattan Lines from a Single Image</a></p>
<p>11 0.11412591 <a title="64-tfidf-11" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>12 0.10378783 <a title="64-tfidf-12" href="./iccv-2013-Shufflets%3A_Shared_Mid-level_Parts_for_Fast_Object_Detection.html">390 iccv-2013-Shufflets: Shared Mid-level Parts for Fast Object Detection</a></p>
<p>13 0.10243662 <a title="64-tfidf-13" href="./iccv-2013-Data-Driven_3D_Primitives_for_Single_Image_Understanding.html">102 iccv-2013-Data-Driven 3D Primitives for Single Image Understanding</a></p>
<p>14 0.093065277 <a title="64-tfidf-14" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>15 0.088266678 <a title="64-tfidf-15" href="./iccv-2013-3D_Scene_Understanding_by_Voxel-CRF.html">2 iccv-2013-3D Scene Understanding by Voxel-CRF</a></p>
<p>16 0.087819152 <a title="64-tfidf-16" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>17 0.086952142 <a title="64-tfidf-17" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>18 0.083528496 <a title="64-tfidf-18" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>19 0.083155707 <a title="64-tfidf-19" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>20 0.082295842 <a title="64-tfidf-20" href="./iccv-2013-Allocentric_Pose_Estimation.html">46 iccv-2013-Allocentric Pose Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.218), (1, -0.073), (2, -0.017), (3, -0.027), (4, 0.12), (5, -0.033), (6, -0.042), (7, -0.012), (8, -0.086), (9, -0.136), (10, 0.046), (11, 0.089), (12, -0.115), (13, 0.031), (14, -0.011), (15, -0.077), (16, -0.096), (17, 0.021), (18, -0.043), (19, -0.08), (20, -0.244), (21, -0.103), (22, 0.19), (23, -0.109), (24, 0.16), (25, -0.17), (26, 0.08), (27, 0.005), (28, -0.047), (29, 0.051), (30, -0.106), (31, 0.038), (32, -0.057), (33, 0.175), (34, 0.105), (35, -0.019), (36, -0.142), (37, -0.118), (38, -0.037), (39, 0.048), (40, -0.054), (41, 0.03), (42, 0.092), (43, 0.061), (44, 0.036), (45, -0.013), (46, -0.013), (47, 0.032), (48, 0.041), (49, -0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95079476 <a title="64-lsi-1" href="./iccv-2013-Box_in_the_Box%3A_Joint_3D_Layout_and_Object_Reasoning_from_Single_Images.html">64 iccv-2013-Box in the Box: Joint 3D Layout and Object Reasoning from Single Images</a></p>
<p>Author: Alexander G. Schwing, Sanja Fidler, Marc Pollefeys, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly infer the room layout as well as the objects present in the scene. Towards this goal, we propose a branch and bound algorithm which is guaranteed to retrieve the global optimum of the joint problem. The main difficulty resides in taking into account occlusion in order to not over-count the evidence. We introduce a new decomposition method, which generalizes integral geometry to triangular shapes, and allows us to bound the different terms in constant time. We exploit both geometric cues and object detectors as image features and show large improvements in 2D and 3D object detection over state-of-the-art deformable part-based models.</p><p>2 0.8878811 <a title="64-lsi-2" href="./iccv-2013-Estimating_the_3D_Layout_of_Indoor_Scenes_and_Its_Clutter_from_Depth_Sensors.html">144 iccv-2013-Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors</a></p>
<p>Author: Jian Zhang, Chen Kan, Alexander G. Schwing, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly estimate the layout ofrooms as well as the clutterpresent in the scene using RGB-D data. Towards this goal, we propose an effective model that is able to exploit both depth and appearance features, which are complementary. Furthermore, our approach is efficient as we exploit the inherent decomposition of additive potentials. We demonstrate the effectiveness of our approach on the challenging NYU v2 dataset and show that employing depth reduces the layout error by 6% and the clutter estimation by 13%.</p><p>3 0.76238751 <a title="64-lsi-3" href="./iccv-2013-Holistic_Scene_Understanding_for_3D_Object_Detection_with_RGBD_Cameras.html">201 iccv-2013-Holistic Scene Understanding for 3D Object Detection with RGBD Cameras</a></p>
<p>Author: Dahua Lin, Sanja Fidler, Raquel Urtasun</p><p>Abstract: In this paper, we tackle the problem of indoor scene understanding using RGBD data. Towards this goal, we propose a holistic approach that exploits 2D segmentation, 3D geometry, as well as contextual relations between scenes and objects. Specifically, we extend the CPMC [3] framework to 3D in order to generate candidate cuboids, and develop a conditional random field to integrate information from different sources to classify the cuboids. With this formulation, scene classification and 3D object recognition are coupled and can be jointly solved through probabilistic inference. We test the effectiveness of our approach on the challenging NYU v2 dataset. The experimental results demonstrate that through effective evidence integration and holistic reasoning, our approach achieves substantial improvement over the state-of-the-art.</p><p>4 0.72737443 <a title="64-lsi-4" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>Author: Scott Satkin, Martial Hebert</p><p>Abstract: We present a new algorithm 3DNN (3D NearestNeighbor), which is capable of matching an image with 3D data, independently of the viewpoint from which the image was captured. By leveraging rich annotations associated with each image, our algorithm can automatically produce precise and detailed 3D models of a scene from a single image. Moreover, we can transfer information across images to accurately label and segment objects in a scene. The true benefit of 3DNN compared to a traditional 2D nearest-neighbor approach is that by generalizing across viewpoints, we free ourselves from the need to have training examples captured from all possible viewpoints. Thus, we are able to achieve comparable results using orders of magnitude less data, and recognize objects from never-beforeseen viewpoints. In this work, we describe the 3DNN algorithm and rigorously evaluate its performance for the tasks of geometry estimation and object detection/segmentation. By decoupling the viewpoint and the geometry of an image, we develop a scene matching approach which is truly 100% viewpoint invariant, yielding state-of-the-art performance on challenging data.</p><p>5 0.67194444 <a title="64-lsi-5" href="./iccv-2013-Characterizing_Layouts_of_Outdoor_Scenes_Using_Spatial_Topic_Processes.html">72 iccv-2013-Characterizing Layouts of Outdoor Scenes Using Spatial Topic Processes</a></p>
<p>Author: Dahua Lin, Jianxiong Xiao</p><p>Abstract: In this paper, we develop a generative model to describe the layouts of outdoor scenes the spatial configuration of regions. Specifically, the layout of an image is represented as a composite of regions, each associated with a semantic topic. At the heart of this model is a novel stochastic process called Spatial Topic Process, which generates a spatial map of topics from a set of coupled Gaussian processes, thus allowing the distributions of topics to vary continuously across the image plane. A key aspect that distinguishes this model from previous ones consists in its capability of capturing dependencies across both locations and topics while allowing substantial variations in the layouts. We demonstrate the practical utility of the proposed model by testing it on scene classification, semantic segmentation, and layout hallucination. –</p><p>6 0.65188992 <a title="64-lsi-6" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<p>7 0.6372683 <a title="64-lsi-7" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>8 0.62679136 <a title="64-lsi-8" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>9 0.60737622 <a title="64-lsi-9" href="./iccv-2013-3D_Scene_Understanding_by_Voxel-CRF.html">2 iccv-2013-3D Scene Understanding by Voxel-CRF</a></p>
<p>10 0.60527861 <a title="64-lsi-10" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>11 0.56737179 <a title="64-lsi-11" href="./iccv-2013-Active_MAP_Inference_in_CRFs_for_Efficient_Semantic_Segmentation.html">42 iccv-2013-Active MAP Inference in CRFs for Efficient Semantic Segmentation</a></p>
<p>12 0.51037169 <a title="64-lsi-12" href="./iccv-2013-Data-Driven_3D_Primitives_for_Single_Image_Understanding.html">102 iccv-2013-Data-Driven 3D Primitives for Single Image Understanding</a></p>
<p>13 0.45865476 <a title="64-lsi-13" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>14 0.45633394 <a title="64-lsi-14" href="./iccv-2013-Allocentric_Pose_Estimation.html">46 iccv-2013-Allocentric Pose Estimation</a></p>
<p>15 0.44585636 <a title="64-lsi-15" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>16 0.4450545 <a title="64-lsi-16" href="./iccv-2013-Scene_Collaging%3A_Analysis_and_Synthesis_of_Natural_Images_with_Semantic_Layers.html">375 iccv-2013-Scene Collaging: Analysis and Synthesis of Natural Images with Semantic Layers</a></p>
<p>17 0.43269515 <a title="64-lsi-17" href="./iccv-2013-Lifting_3D_Manhattan_Lines_from_a_Single_Image.html">250 iccv-2013-Lifting 3D Manhattan Lines from a Single Image</a></p>
<p>18 0.43051717 <a title="64-lsi-18" href="./iccv-2013-Sequential_Bayesian_Model_Update_under_Structured_Scene_Prior_for_Semantic_Road_Scenes_Labeling.html">386 iccv-2013-Sequential Bayesian Model Update under Structured Scene Prior for Semantic Road Scenes Labeling</a></p>
<p>19 0.42869484 <a title="64-lsi-19" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<p>20 0.42207232 <a title="64-lsi-20" href="./iccv-2013-Detecting_Avocados_to_Zucchinis%3A_What_Have_We_Done%2C_and_Where_Are_We_Going%3F.html">109 iccv-2013-Detecting Avocados to Zucchinis: What Have We Done, and Where Are We Going?</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.046), (7, 0.013), (12, 0.015), (26, 0.076), (31, 0.046), (34, 0.252), (35, 0.01), (42, 0.112), (55, 0.025), (64, 0.048), (73, 0.036), (89, 0.211), (98, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91943753 <a title="64-lda-1" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>Author: Naman Turakhia, Devi Parikh</p><p>Abstract: When we look at an image, some properties or attributes of the image stand out more than others. When describing an image, people are likely to describe these dominant attributes first. Attribute dominance is a result of a complex interplay between the various properties present or absent in the image. Which attributes in an image are more dominant than others reveals rich information about the content of the image. In this paper we tap into this information by modeling attribute dominance. We show that this helps improve the performance of vision systems on a variety of human-centric applications such as zero-shot learning, image search and generating textual descriptions of images.</p><p>2 0.91034818 <a title="64-lda-2" href="./iccv-2013-How_Do_You_Tell_a_Blackbird_from_a_Crow%3F.html">202 iccv-2013-How Do You Tell a Blackbird from a Crow?</a></p>
<p>Author: Thomas Berg, Peter N. Belhumeur</p><p>Abstract: How do you tell a blackbirdfrom a crow? There has been great progress toward automatic methods for visual recognition, including fine-grained visual categorization in which the classes to be distinguished are very similar. In a task such as bird species recognition, automatic recognition systems can now exceed the performance of non-experts – most people are challenged to name a couple dozen bird species, let alone identify them. This leads us to the question, “Can a recognition system show humans what to look for when identifying classes (in this case birds)? ” In the context of fine-grained visual categorization, we show that we can automatically determine which classes are most visually similar, discover what visual features distinguish very similar classes, and illustrate the key features in a way meaningful to humans. Running these methods on a dataset of bird images, we can generate a visual field guide to birds which includes a tree of similarity that displays the similarity relations between all species, pages for each species showing the most similar other species, and pages for each pair of similar species illustrating their differences.</p><p>3 0.88535321 <a title="64-lda-3" href="./iccv-2013-Multi-scale_Topological_Features_for_Hand_Posture_Representation_and_Analysis.html">278 iccv-2013-Multi-scale Topological Features for Hand Posture Representation and Analysis</a></p>
<p>Author: Kaoning Hu, Lijun Yin</p><p>Abstract: In this paper, we propose a multi-scale topological feature representation for automatic analysis of hand posture. Such topological features have the advantage of being posture-dependent while being preserved under certain variations of illumination, rotation, personal dependency, etc. Our method studies the topology of the holes between the hand region and its convex hull. Inspired by the principle of Persistent Homology, which is the theory of computational topology for topological feature analysis over multiple scales, we construct the multi-scale Betti Numbers matrix (MSBNM) for the topological feature representation. In our experiments, we used 12 different hand postures and compared our features with three popular features (HOG, MCT, and Shape Context) on different data sets. In addition to hand postures, we also extend the feature representations to arm postures. The results demonstrate the feasibility and reliability of the proposed method.</p><p>4 0.8547864 <a title="64-lda-4" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>Author: Aleksandr V. Segal, Ian Reid</p><p>Abstract: We propose a novel parametrization of the data association problem for multi-target tracking. In our formulation, the number of targets is implicitly inferred together with the data association, effectively solving data association and model selection as a single inference problem. The novel formulation allows us to interpret data association and tracking as a single Switching Linear Dynamical System (SLDS). We compute an approximate posterior solution to this problem using a dynamic programming/message passing technique. This inference-based approach allows us to incorporate richer probabilistic models into the tracking system. In particular, we incorporate inference over inliers/outliers and track termination times into the system. We evaluate our approach on publicly available datasets and demonstrate results competitive with, and in some cases exceeding the state of the art.</p><p>same-paper 5 0.84567702 <a title="64-lda-5" href="./iccv-2013-Box_in_the_Box%3A_Joint_3D_Layout_and_Object_Reasoning_from_Single_Images.html">64 iccv-2013-Box in the Box: Joint 3D Layout and Object Reasoning from Single Images</a></p>
<p>Author: Alexander G. Schwing, Sanja Fidler, Marc Pollefeys, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly infer the room layout as well as the objects present in the scene. Towards this goal, we propose a branch and bound algorithm which is guaranteed to retrieve the global optimum of the joint problem. The main difficulty resides in taking into account occlusion in order to not over-count the evidence. We introduce a new decomposition method, which generalizes integral geometry to triangular shapes, and allows us to bound the different terms in constant time. We exploit both geometric cues and object detectors as image features and show large improvements in 2D and 3D object detection over state-of-the-art deformable part-based models.</p><p>6 0.82524431 <a title="64-lda-6" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>7 0.82380992 <a title="64-lda-7" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>8 0.81013781 <a title="64-lda-8" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>9 0.806898 <a title="64-lda-9" href="./iccv-2013-Efficient_and_Robust_Large-Scale_Rotation_Averaging.html">138 iccv-2013-Efficient and Robust Large-Scale Rotation Averaging</a></p>
<p>10 0.77415645 <a title="64-lda-10" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>11 0.76313752 <a title="64-lda-11" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>12 0.7604053 <a title="64-lda-12" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>13 0.75580192 <a title="64-lda-13" href="./iccv-2013-Fine-Grained_Categorization_by_Alignments.html">169 iccv-2013-Fine-Grained Categorization by Alignments</a></p>
<p>14 0.75519586 <a title="64-lda-14" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>15 0.74853027 <a title="64-lda-15" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>16 0.74250257 <a title="64-lda-16" href="./iccv-2013-Estimating_the_3D_Layout_of_Indoor_Scenes_and_Its_Clutter_from_Depth_Sensors.html">144 iccv-2013-Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors</a></p>
<p>17 0.74012566 <a title="64-lda-17" href="./iccv-2013-Event_Detection_in_Complex_Scenes_Using_Interval_Temporal_Constraints.html">146 iccv-2013-Event Detection in Complex Scenes Using Interval Temporal Constraints</a></p>
<p>18 0.73740786 <a title="64-lda-18" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>19 0.73655087 <a title="64-lda-19" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>20 0.73640215 <a title="64-lda-20" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
