<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>82 iccv-2013-Compensating for Motion during Direct-Global Separation</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-82" href="#">iccv2013-82</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>82 iccv-2013-Compensating for Motion during Direct-Global Separation</h1>
<br/><p>Source: <a title="iccv-2013-82-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Achar_Compensating_for_Motion_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Supreeth Achar, Stephen T. Nuske, Srinivasa G. Narasimhan</p><p>Abstract: Separating the direct and global components of radiance can aid shape recovery algorithms and can provide useful information about materials in a scene. Practical methods for finding the direct and global components use multiple images captured under varying illumination patterns and require the scene, light source and camera to remain stationary during the image acquisition process. In this paper, we develop a motion compensation method that relaxes this condition and allows direct-global separation to beperformed on video sequences of dynamic scenes captured by moving projector-camera systems. Key to our method is being able to register frames in a video sequence to each other in the presence of time varying, high frequency active illumination patterns. We compare our motion compensated method to alternatives such as single shot separation and frame interleaving as well as ground truth. We present results on challenging video sequences that include various types of motions and deformations in scenes that contain complex materials like fabric, skin, leaves and wax.</p><p>Reference: <a title="iccv-2013-82-reference" href="../iccv2013_reference/iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Narasimhan Robotics Institute, Carnegie Mellon University  Abstract Separating the direct and global components of radiance can aid shape recovery algorithms and can provide useful information about materials in a scene. [sent-3, score-0.552]
</p><p>2 Practical methods for finding the direct and global components use multiple images captured under varying illumination patterns and require the scene, light source and camera to remain stationary during the image acquisition process. [sent-4, score-0.99]
</p><p>3 In this paper, we develop a motion compensation method that relaxes this condition and allows direct-global separation to beperformed on video sequences of dynamic scenes captured by moving projector-camera systems. [sent-5, score-0.982]
</p><p>4 Key to our method is being able to register frames in a video sequence to each other in the presence of time varying, high frequency active illumination patterns. [sent-6, score-0.348]
</p><p>5 We compare our motion compensated method to alternatives such as single shot separation and frame interleaving as well as ground truth. [sent-7, score-0.887]
</p><p>6 Introduction  The radiance of a scene point illuminated by a light source is the sum of the direct and global components. [sent-10, score-0.725]
</p><p>7 The direct component is the light from the source that undergoes a single reflection in the scene before reaching the observer. [sent-11, score-0.594]
</p><p>8 The global component is due to indirect lighting from inter reflections, subsurface scattering, volumetric scattering and diffusion. [sent-12, score-0.448]
</p><p>9 Separating the direct and global components of illumination provides valuable insights into how light interacts with a scene. [sent-13, score-0.7]
</p><p>10 Being able to extract the direct component of illumination can improve the performance of classical photometry based algorithms like shape from shading as well as structured light reconstruction which typically do not account for global effects. [sent-14, score-0.76]
</p><p>11 An efficient method for finding the global and direct components was first proposed in [14]. [sent-16, score-0.396]
</p><p>12 The light source, camera and scene need to remain stationary during image acquisition. [sent-19, score-0.382]
</p><p>13 Single shot structured light methods [9] can be used on dynamic scenes but have low spatial resolution while multi-image methods [17] produce high quality depth estimates but require the scene to remain stationary. [sent-22, score-0.538]
</p><p>14 There has been work on developing motion compensation schemes to allow multi-image structured light algorithms to be applied to dynamic scenes [10, 20]. [sent-23, score-0.819]
</p><p>15 One approach is interleaving the projector patterns for structure estimation with uniformly lighting for motion tracking. [sent-24, score-0.905]
</p><p>16 Most structured light algorithms do not account for global illumination and those that do [3, 5] require many additional images. [sent-25, score-0.454]
</p><p>17 In this work, we address motion compensation in the context of direct-global separation. [sent-26, score-0.503]
</p><p>18 This allows separation to be performed on video sequences in which the projector-camera system and/or the scene are moving. [sent-28, score-0.367]
</p><p>19 We assume that the underlying global and direct components of a scene point vary only slightly over small motions. [sent-29, score-0.489]
</p><p>20 This means that if the frames in a temporal window can be aligned, the separation technique in [14] can be applied to the aligned frames. [sent-30, score-0.688]
</p><p>21 We use these relit images to aid alignment and then estimate the global and direct components from the aligned images. [sent-33, score-0.722]
</p><p>22 We use all the frames in a temporal window for estimating the global and direct components. [sent-35, score-0.668]
</p><p>23 No frames are used exclusively for tracking, so our method can handle faster motions than interleaving at a given frame rate. [sent-36, score-0.513]
</p><p>24 We show that our method compensates for motion effectively and generates separation results close to ground truth. [sent-38, score-0.438]
</p><p>25 We show that not compensating for motion introduces significant artifacts in the separation and compare our method to alternatives such as single shot separation and interleaving. [sent-39, score-0.847]
</p><p>26 Related Work The original work on direct-global separation [14] describes methods for separation using active illumination and source occluders. [sent-44, score-0.721]
</p><p>27 With active illumination, the separation can be performed using three sinusoid patterns, but the best results with practical projector-camera systems require around 20 high frequency pattern images. [sent-45, score-0.392]
</p><p>28 A method  that uses a single image was also presented, but it generates results at a fraction of the projector’s resolution which is undesirable since most projector-camera systems are projector resolution limited. [sent-46, score-0.418]
</p><p>29 In [15] an optical processing method that can be used to directly acquire the global component of illumination is presented. [sent-47, score-0.423]
</p><p>30 Global illumination and projector defocus were modeled jointly in [6] for depth recovery in scenes with significant global light transport effects. [sent-48, score-0.9]
</p><p>31 In [4], the separation technique was extended to scenes illuminated by multiple controllable light sources. [sent-49, score-0.595]
</p><p>32 Their goal was to extract the direct component for each light source to aid structure recovery techniques where global illumination is often a severe source of systematic error. [sent-50, score-0.851]
</p><p>33 The need for motion compensation also arises in structured light for 3D estimation. [sent-51, score-0.718]
</p><p>34 [19] developed structured light patterns that can be decoded both spatially and temporally which allows for motion adaptation. [sent-53, score-0.515]
</p><p>35 In [20] a motion compensation method for the phase shift structured light algorithm is presented. [sent-55, score-0.718]
</p><p>36 Motion estimation and compensation in image sequences with projected patterns is often done by interleaving the patterns with uniform lighting [21]. [sent-57, score-0.879]
</p><p>37 A similar ap-  proach is used in the structured light motion compensation scheme in [10] where patterns for structure estimation are interleaved with patterns optimized for estimating motion. [sent-58, score-0.969]
</p><p>38 An alternative optical flow formulation was derived in [18] that uses a direct search to compute optical flow and which can accommodate arbitrary data loss terms. [sent-65, score-0.578]
</p><p>39 Limitations We do not model changes in the underlying direct and global components at a scene point within a small temporal window. [sent-69, score-0.555]
</p><p>40 Image Formation Model The brightness It (x) of a pixel x at time t is a combination of the direct component Idt and global component Igt. [sent-73, score-0.633]
</p><p>41 When a binary pattern illuminates the scene, the direct component is modulated by the pattern. [sent-74, score-0.392]
</p><p>42 If the pattern has an equal number of bright and dark pixels and has high spatial frequency compared to Igt, the contribution of the global illumination to the brightness is 12Igt [14]. [sent-75, score-0.46]
</p><p>43 We colocate our projector and camera so the mapping between projector and camera pixels is fixed and independent of scene geometry. [sent-77, score-0.909]
</p><p>44 Even though the patterns are binary, the value of st at a pixel can be continuous because real projectors do not have ideal step responses and the projector and camera pixels need not be aligned. [sent-78, score-0.573]
</p><p>45 The specularities on the candles = appear in the direct image and most of the color is due to subsurface scattering in the wax and appears in the global image. [sent-83, score-0.61]
</p><p>46 We assume that the motion within a sliding window is small enough for these changes to be negligible. [sent-85, score-0.431]
</p><p>47 This allows us to relate the global and direct components at time instant t in the sliding window to time 0 Ig0(x) ≈ Igt(Wt(x)) Id0(x) ≈ Idt(Wt(x)) where, Wt is an (unknown) warping function that aligns the view at time 0 to the view at time t. [sent-86, score-0.663]
</p><p>48 Wt depends on the geometry of the scene and the motion of the scene and projector-camera system. [sent-88, score-0.35]
</p><p>49 Motion Estimation and Compensation We compute the direct-global separation at a frame in the video sequence using a small temporal sliding window centered at that frame. [sent-92, score-0.693]
</p><p>50 We seek to compensate for the motion that occurs inside a temporal sliding window so that the frames can be aligned to each other. [sent-93, score-0.73]
</p><p>51 With the help of the image formation model, we estimate how the scene would have appeared at each time instant under uniform lighting instead of the patterned illumination. [sent-94, score-0.336]
</p><p>52 Once the images are aligned we can compute the global and direct components robustly. [sent-96, score-0.458]
</p><p>53 The patterns violate the brightness and contrast constancy assumptions most optical flow methods rely on. [sent-100, score-0.382]
</p><p>54 To aid alignment, we compute an approximation of how the scene would have appeared (I˜ft) under uniform illumination from the frame It and the pattern st used to illuminate the scene. [sent-101, score-0.595]
</p><p>55 Under uniform illumination, the brightness at a pixel is the sum of two unknowns, the direct component and the global component Ift (x) = Igt (x) + Idt(x). [sent-103, score-0.689]
</p><p>56 To find an approximate solution to the problem, we introduce a regularizer that enforces piecewise spatial continuity of the estimated global and direct components and respectively). [sent-106, score-0.425]
</p><p>57 These artifacts are caused by projector blur and small errors in the colocation between the projector and camera. [sent-124, score-0.778]
</p><p>58 Registering Images To align a frame to the center frame, we could simply compute optical flow between the relit frames. [sent-131, score-0.544]
</p><p>59 x  where, α(x, Wt) is a weight that is high when a point is lit (s close to 1) in both the center frame I0 and the current frame It. [sent-140, score-0.324]
</p><p>60 Because we are seeking  2  to correct small errors in an existing optical flow estimate we search for an refined warp at each pixel using a small window centered around the original warp estimate. [sent-149, score-0.525]
</p><p>61 If the motion that occurs in a sliding window is large, optical flow may fail to correctly align some frames to the center frame. [sent-150, score-0.852]
</p><p>62 We detect poorly aligned frames by thresholding the correlation between the warped frame Wt ◦ I˜tf and center frame I˜f. [sent-151, score-0.422]
</p><p>63 Poorly aligned frames are discarded from the sliding window. [sent-152, score-0.325]
</p><p>64 Computing Direct-Global Separation Once the frames in a window have been warped to align with the center frame, we in effect have a set of images of the scene captured from the same viewpoint with different illumination patterns. [sent-155, score-0.62]
</p><p>65 Alternatively, since the projector pattern values (st) at each pixel are known, the global and direct components can be determined by fitting a line to the observed brightness values at a pixel using equation 1. [sent-157, score-0.983]
</p><p>66 For this line fit to make sense, each pixel needs to be observed under a range of projector pattern values. [sent-158, score-0.446]
</p><p>67 As a result, there will be pixels in the image where the global and direct components can not be estimated well because the projector brightness did not change sufficiently at the corresponding scene point. [sent-160, score-0.948]
</p><p>68 We search for piecewise continuous global and direct components that are a good fit to the observed aligned image data by minimizing L(Ig0, Id0) =  ? [sent-162, score-0.487]
</p><p>69 t∈T  + λgTV (Ig0) + λdTV (Id0)  (5)  where, T is the sliding window of frames selected about the center frame. [sent-168, score-0.455]
</p><p>70 For all experiments, the camera was radiometrically calibrated to have a linear response curve and the camera and projector were colocated using a plate beam splitter. [sent-179, score-0.535]
</p><p>71 To correct for projector vignetting, all images were normalized with respect to a reference image of the same planar surface while fully lit by the projector. [sent-183, score-0.461]
</p><p>72 Comparisons on Rigidly Moving Scenes The goal of these experiments is to compare the direct and global components generated by our algorithm on moving scenes to ground truth and to analyze the effect of temporal window size on separation accuracy. [sent-187, score-1.019]
</p><p>73 Ground truth was acquired by first capturing 25 frames of a scene while projecting checkerboard patterns at different offsets. [sent-188, score-0.333]
</p><p>74 The direct and global components calculated on these 25 frames are used as ground truth (RMS Error 0). [sent-190, score-0.537]
</p><p>75 We then captured a video sequence with the scene in motion while patterns were being projected. [sent-191, score-0.356]
</p><p>76 The regularization improves performance when the number offrames is small and many pixels have not seen enough different projector pattern values. [sent-201, score-0.408]
</p><p>77 For the video sequence corresponding to each trial, we tested our motion compensation method with different sliding window sizes using the first frame as the window cen-  formed using a camera and projector colocated with a plate beamsplitter. [sent-203, score-1.484]
</p><p>78 We evaluated the motion compensation with the warp refinement described in 3. [sent-206, score-0.584]
</p><p>79 We also tested an interleaved approach where the projector alternates between patterns and uniform illumination (’Interleaved’ in Fig. [sent-210, score-0.705]
</p><p>80 When the number of frames used is small, the regularized static method and the proposed motion compensated methods perform similarly. [sent-215, score-0.445]
</p><p>81 As the number of frames increases, the improvement in the motion compensated output reduces and then stops. [sent-216, score-0.383]
</p><p>82 When the window size is large, the frames near the edges of the sliding window can not be aligned to the center frame because the viewpoint changes are too large and the global and direct components of the scene points change appreciably. [sent-217, score-1.237]
</p><p>83 The motion compensation algorithm automatically discards these frames and they yield no improvement in the results. [sent-218, score-0.644]
</p><p>84 The temporal window available for performing separation on dynamic scenes is small. [sent-220, score-0.586]
</p><p>85 5  shows results from our motion compensation algorithm and interleaving with different temporal window sizes in an example scene. [sent-225, score-0.967]
</p><p>86 The blue ‘static’ curves are from direct-global separation on stationary scenes and represent the best possible performance a method could achieve for a given number of frames. [sent-233, score-0.383]
</p><p>87 The red ‘moving’ curves are from using our motion compensation algorithm on moving scenes. [sent-234, score-0.575]
</p><p>88 When the number of frames is small, the motion compensation method performs just as well on the moving sequences as normal separation on an equal number of static frames. [sent-235, score-1.052]
</p><p>89 When the window size increases, frames far away from the window center are discarded because alignment fails and so performance of the motion compensated algorithm levels off. [sent-236, score-0.758]
</p><p>90 Without motion compensation, the results are blurred and edges in the scene (around the fingers for example) are corrupted. [sent-240, score-0.323]
</p><p>91 Discussion Although we do not model the changes in global and direct components that occur within a small temporal window, our method is still able to handle broad specular lobes like shiny surfaces on wax and highlights on skin. [sent-244, score-0.607]
</p><p>92 Sharp specularities and specular inter reflections such as those from polished metal surfaces would cause both the image alignment and component separation steps to break down. [sent-245, score-0.637]
</p><p>93 The fast direct-global separation algorithm for static scenes can handle sharp specularities but not specular inter reflections. [sent-246, score-0.584]
</p><p>94 Using shorter exposure times and smaller apertures to avoid motion blur and defocus means that less light reaches the camera and image noise becomes more of a problem. [sent-251, score-0.424]
</p><p>95 We would need to consider how computational photography methods like coded aperture for motion deblurring [16] and light efficient photography [7] could be applied. [sent-252, score-0.447]
</p><p>96 Multiplexed illumination for scene recovery in the presence of global illumination. [sent-277, score-0.363]
</p><p>97 Our method makes more efficient use of images than interleaving because no frames are needed exclusively for tracking. [sent-301, score-0.394]
</p><p>98 Separations that resolve a given level of detail can be obtained with a smaller temporal sliding window than an interleaving approach. [sent-302, score-0.586]
</p><p>99 Fast separation of direct and global components of a scene using high frequency illumination. [sent-356, score-0.829]
</p><p>100 The two columns on the right show the component estimates on the same frames using our motion compensation method. [sent-405, score-0.767]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('projector', 0.356), ('compensation', 0.339), ('separation', 0.274), ('interleaving', 0.253), ('direct', 0.218), ('relit', 0.178), ('motion', 0.164), ('light', 0.163), ('window', 0.145), ('frames', 0.141), ('illumination', 0.141), ('sliding', 0.122), ('wt', 0.121), ('lit', 0.105), ('brightness', 0.103), ('patterns', 0.099), ('global', 0.098), ('optical', 0.096), ('scene', 0.093), ('scattering', 0.093), ('illuminated', 0.092), ('component', 0.088), ('frame', 0.086), ('flow', 0.084), ('subsurface', 0.083), ('warp', 0.081), ('components', 0.08), ('compensated', 0.078), ('specular', 0.078), ('colocated', 0.075), ('igt', 0.075), ('separations', 0.075), ('rms', 0.075), ('moving', 0.072), ('patterned', 0.067), ('wax', 0.067), ('artifacts', 0.066), ('warps', 0.066), ('scenes', 0.066), ('temporal', 0.066), ('frequency', 0.066), ('static', 0.062), ('aligned', 0.062), ('idt', 0.058), ('uniform', 0.056), ('reflections', 0.055), ('inter', 0.053), ('interleaved', 0.053), ('align', 0.053), ('camera', 0.052), ('structured', 0.052), ('pattern', 0.052), ('specularities', 0.051), ('gtv', 0.05), ('illuminate', 0.05), ('materials', 0.048), ('aid', 0.048), ('scanning', 0.047), ('center', 0.047), ('formation', 0.046), ('defocus', 0.045), ('diffuse', 0.045), ('stationary', 0.043), ('deblurring', 0.042), ('dtv', 0.041), ('fabric', 0.041), ('illuminating', 0.041), ('appeared', 0.041), ('gt', 0.04), ('photography', 0.039), ('pixel', 0.038), ('alignment', 0.038), ('skin', 0.037), ('compensating', 0.037), ('decoded', 0.037), ('smooths', 0.037), ('taguchi', 0.037), ('tf', 0.037), ('estimates', 0.035), ('dynamic', 0.035), ('modulated', 0.034), ('blurred', 0.033), ('lighting', 0.033), ('motions', 0.033), ('fingers', 0.033), ('raskar', 0.033), ('acquisition', 0.033), ('shot', 0.032), ('linearization', 0.032), ('relaxes', 0.032), ('narasimhan', 0.032), ('source', 0.032), ('recovery', 0.031), ('remain', 0.031), ('resolution', 0.031), ('compensate', 0.03), ('radiance', 0.029), ('piecewise', 0.029), ('tv', 0.028), ('st', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="82-tfidf-1" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>Author: Supreeth Achar, Stephen T. Nuske, Srinivasa G. Narasimhan</p><p>Abstract: Separating the direct and global components of radiance can aid shape recovery algorithms and can provide useful information about materials in a scene. Practical methods for finding the direct and global components use multiple images captured under varying illumination patterns and require the scene, light source and camera to remain stationary during the image acquisition process. In this paper, we develop a motion compensation method that relaxes this condition and allows direct-global separation to beperformed on video sequences of dynamic scenes captured by moving projector-camera systems. Key to our method is being able to register frames in a video sequence to each other in the presence of time varying, high frequency active illumination patterns. We compare our motion compensated method to alternatives such as single shot separation and frame interleaving as well as ground truth. We present results on challenging video sequences that include various types of motions and deformations in scenes that contain complex materials like fabric, skin, leaves and wax.</p><p>2 0.23679315 <a title="82-tfidf-2" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<p>Author: Nicolas Martin, Vincent Couture, Sébastien Roy</p><p>Abstract: We present a scanning method that recovers dense subpixel camera-projector correspondence without requiring any photometric calibration nor preliminary knowledge of their relative geometry. Subpixel accuracy is achieved by considering several zero-crossings defined by the difference between pairs of unstructured patterns. We use gray-level band-pass white noise patterns that increase robustness to indirect lighting and scene discontinuities. Simulated and experimental results show that our method recovers scene geometry with high subpixel precision, and that it can handle many challenges of active reconstruction systems. We compare our results to state of the art methods such as micro phase shifting and modulated phase shifting.</p><p>3 0.22365488 <a title="82-tfidf-3" href="./iccv-2013-Structured_Light_in_Sunlight.html">405 iccv-2013-Structured Light in Sunlight</a></p>
<p>Author: Mohit Gupta, Qi Yin, Shree K. Nayar</p><p>Abstract: Strong ambient illumination severely degrades the performance of structured light based techniques. This is especially true in outdoor scenarios, where the structured light sources have to compete with sunlight, whose power is often 2-5 orders of magnitude larger than the projected light. In this paper, we propose the concept of light-concentration to overcome strong ambient illumination. Our key observation is that given a fixed light (power) budget, it is always better to allocate it sequentially in several portions of the scene, as compared to spreading it over the entire scene at once. For a desired level of accuracy, we show that by distributing light appropriately, the proposed approach requires 1-2 orders lower acquisition time than existing approaches. Our approach is illumination-adaptive as the optimal light distribution is determined based on a measurement of the ambient illumination level. Since current light sources have a fixed light distribution, we have built a prototype light source that supports flexible light distribution by controlling the scanning speed of a laser scanner. We show several high quality 3D scanning results in a wide range of outdoor scenarios. The proposed approach will benefit 3D vision systems that need to operate outdoors under extreme ambient illumination levels on a limited time and power budget.</p><p>4 0.16768508 <a title="82-tfidf-4" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>Author: Christoph Vogel, Konrad Schindler, Stefan Roth</p><p>Abstract: Estimating dense 3D scene flow from stereo sequences remains a challenging task, despite much progress in both classical disparity and 2D optical flow estimation. To overcome the limitations of existing techniques, we introduce a novel model that represents the dynamic 3D scene by a collection of planar, rigidly moving, local segments. Scene flow estimation then amounts to jointly estimating the pixelto-segment assignment, and the 3D position, normal vector, and rigid motion parameters of a plane for each segment. The proposed energy combines an occlusion-sensitive data term with appropriate shape, motion, and segmentation regularizers. Optimization proceeds in two stages: Starting from an initial superpixelization, we estimate the shape and motion parameters of all segments by assigning a proposal from a set of moving planes. Then the pixel-to-segment assignment is updated, while holding the shape and motion parameters of the moving planes fixed. We demonstrate the benefits of our model on different real-world image sets, including the challenging KITTI benchmark. We achieve leading performance levels, exceeding competing 3D scene flow methods, and even yielding better 2D motion estimates than all tested dedicated optical flow techniques.</p><p>5 0.16245642 <a title="82-tfidf-5" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>Author: Jim Braux-Zin, Romain Dupont, Adrien Bartoli</p><p>Abstract: Dense motion field estimation (typically Romain Dupont1 romain . dupont @ cea . fr Adrien Bartoli2 adrien . bart o l @ gmai l com i . 2 ISIT, Universit e´ d’Auvergne/CNRS, France sions are explicitly modeled [32, 13]. Coarse-to-fine warping improves global convergence by making the assumption that optical flow, the motion of smaller structures is similar to the motion of stereo disparity and surface registration) is a key computer vision problem. Many solutions have been proposed to compute small or large displacements, narrow or wide baseline stereo disparity, but a unified methodology is still lacking. We here introduce a general framework that robustly combines direct and feature-based matching. The feature-based cost is built around a novel robust distance function that handles keypoints and “weak” features such as segments. It allows us to use putative feature matches which may contain mismatches to guide dense motion estimation out of local minima. Our framework uses a robust direct data term (AD-Census). It is implemented with a powerful second order Total Generalized Variation regularization with external and self-occlusion reasoning. Our framework achieves state of the art performance in several cases (standard optical flow benchmarks, wide-baseline stereo and non-rigid surface registration). Our framework has a modular design that customizes to specific application needs.</p><p>6 0.14480186 <a title="82-tfidf-6" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>7 0.1382083 <a title="82-tfidf-7" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>8 0.13388938 <a title="82-tfidf-8" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>9 0.1329602 <a title="82-tfidf-9" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>10 0.12605864 <a title="82-tfidf-10" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<p>11 0.12309498 <a title="82-tfidf-11" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>12 0.1221474 <a title="82-tfidf-12" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>13 0.12094256 <a title="82-tfidf-13" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<p>14 0.11250237 <a title="82-tfidf-14" href="./iccv-2013-Modeling_Self-Occlusions_in_Dynamic_Shape_and_Appearance_Tracking.html">270 iccv-2013-Modeling Self-Occlusions in Dynamic Shape and Appearance Tracking</a></p>
<p>15 0.10793687 <a title="82-tfidf-15" href="./iccv-2013-High_Quality_Shape_from_a_Single_RGB-D_Image_under_Uncalibrated_Natural_Illumination.html">199 iccv-2013-High Quality Shape from a Single RGB-D Image under Uncalibrated Natural Illumination</a></p>
<p>16 0.10702511 <a title="82-tfidf-16" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>17 0.10409191 <a title="82-tfidf-17" href="./iccv-2013-Exploiting_Reflection_Change_for_Automatic_Reflection_Removal.html">151 iccv-2013-Exploiting Reflection Change for Automatic Reflection Removal</a></p>
<p>18 0.10351512 <a title="82-tfidf-18" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>19 0.10277539 <a title="82-tfidf-19" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>20 0.10100853 <a title="82-tfidf-20" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.21), (1, -0.156), (2, -0.008), (3, 0.115), (4, -0.025), (5, 0.033), (6, 0.025), (7, -0.067), (8, 0.064), (9, 0.015), (10, -0.014), (11, -0.007), (12, 0.15), (13, -0.037), (14, -0.07), (15, -0.051), (16, -0.073), (17, 0.063), (18, 0.046), (19, 0.016), (20, 0.055), (21, -0.051), (22, 0.022), (23, -0.057), (24, -0.146), (25, 0.002), (26, 0.015), (27, 0.018), (28, 0.114), (29, -0.172), (30, 0.125), (31, 0.003), (32, 0.042), (33, 0.027), (34, 0.007), (35, 0.098), (36, -0.022), (37, -0.067), (38, -0.156), (39, 0.063), (40, 0.012), (41, 0.01), (42, 0.049), (43, 0.011), (44, -0.001), (45, -0.087), (46, 0.051), (47, 0.15), (48, 0.058), (49, -0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96166867 <a title="82-lsi-1" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>Author: Supreeth Achar, Stephen T. Nuske, Srinivasa G. Narasimhan</p><p>Abstract: Separating the direct and global components of radiance can aid shape recovery algorithms and can provide useful information about materials in a scene. Practical methods for finding the direct and global components use multiple images captured under varying illumination patterns and require the scene, light source and camera to remain stationary during the image acquisition process. In this paper, we develop a motion compensation method that relaxes this condition and allows direct-global separation to beperformed on video sequences of dynamic scenes captured by moving projector-camera systems. Key to our method is being able to register frames in a video sequence to each other in the presence of time varying, high frequency active illumination patterns. We compare our motion compensated method to alternatives such as single shot separation and frame interleaving as well as ground truth. We present results on challenging video sequences that include various types of motions and deformations in scenes that contain complex materials like fabric, skin, leaves and wax.</p><p>2 0.82434481 <a title="82-lsi-2" href="./iccv-2013-Structured_Light_in_Sunlight.html">405 iccv-2013-Structured Light in Sunlight</a></p>
<p>Author: Mohit Gupta, Qi Yin, Shree K. Nayar</p><p>Abstract: Strong ambient illumination severely degrades the performance of structured light based techniques. This is especially true in outdoor scenarios, where the structured light sources have to compete with sunlight, whose power is often 2-5 orders of magnitude larger than the projected light. In this paper, we propose the concept of light-concentration to overcome strong ambient illumination. Our key observation is that given a fixed light (power) budget, it is always better to allocate it sequentially in several portions of the scene, as compared to spreading it over the entire scene at once. For a desired level of accuracy, we show that by distributing light appropriately, the proposed approach requires 1-2 orders lower acquisition time than existing approaches. Our approach is illumination-adaptive as the optimal light distribution is determined based on a measurement of the ambient illumination level. Since current light sources have a fixed light distribution, we have built a prototype light source that supports flexible light distribution by controlling the scanning speed of a laser scanner. We show several high quality 3D scanning results in a wide range of outdoor scenarios. The proposed approach will benefit 3D vision systems that need to operate outdoors under extreme ambient illumination levels on a limited time and power budget.</p><p>3 0.80430186 <a title="82-lsi-3" href="./iccv-2013-Illuminant_Chromaticity_from_Image_Sequences.html">207 iccv-2013-Illuminant Chromaticity from Image Sequences</a></p>
<p>Author: Veronique Prinet, Dani Lischinski, Michael Werman</p><p>Abstract: We estimate illuminant chromaticity from temporal sequences, for scenes illuminated by either one or two dominant illuminants. While there are many methods for illuminant estimation from a single image, few works so far have focused on videos, and even fewer on multiple light sources. Our aim is to leverage information provided by the temporal acquisition, where either the objects or the camera or the light source are/is in motion in order to estimate illuminant color without the need for user interaction or using strong assumptions and heuristics. We introduce a simple physically-based formulation based on the assumption that the incident light chromaticity is constant over a short space-time domain. We show that a deterministic approach is not sufficient for accurate and robust estimation: however, a probabilistic formulation makes it possible to implicitly integrate away hidden factors that have been ignored by the physical model. Experimental results are reported on a dataset of natural video sequences and on the GrayBall benchmark, indicating that we compare favorably with the state-of-the-art.</p><p>4 0.75787312 <a title="82-lsi-4" href="./iccv-2013-Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illumination_in_the_Spectral_Domain.html">385 iccv-2013-Separating Reflective and Fluorescent Components Using High Frequency Illumination in the Spectral Domain</a></p>
<p>Author: Ying Fu, Antony Lam, Imari Sato, Takahiro Okabe, Yoichi Sato</p><p>Abstract: Hyperspectral imaging is beneficial to many applications but current methods do not consider fluorescent effects which are present in everyday items ranging from paper, to clothing, to even our food. Furthermore, everyday fluorescent items exhibit a mix of reflectance and fluorescence. So proper separation of these components is necessary for analyzing them. In this paper, we demonstrate efficient separation and recovery of reflective and fluorescent emission spectra through the use of high frequency illumination in the spectral domain. With the obtained fluorescent emission spectra from our high frequency illuminants, we then present to our knowledge, the first method for estimating the fluorescent absorption spectrum of a material given its emission spectrum. Conventional bispectral measurement of absorption and emission spectra needs to examine all combinations of incident and observed light wavelengths. In contrast, our method requires only two hyperspectral images. The effectiveness of our proposed methods are then evaluated through a combination of simulation and real experiments. We also demonstrate an application of our method to synthetic relighting of real scenes.</p><p>5 0.74118066 <a title="82-lsi-5" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<p>Author: Nicolas Martin, Vincent Couture, Sébastien Roy</p><p>Abstract: We present a scanning method that recovers dense subpixel camera-projector correspondence without requiring any photometric calibration nor preliminary knowledge of their relative geometry. Subpixel accuracy is achieved by considering several zero-crossings defined by the difference between pairs of unstructured patterns. We use gray-level band-pass white noise patterns that increase robustness to indirect lighting and scene discontinuities. Simulated and experimental results show that our method recovers scene geometry with high subpixel precision, and that it can handle many challenges of active reconstruction systems. We compare our results to state of the art methods such as micro phase shifting and modulated phase shifting.</p><p>6 0.69376415 <a title="82-lsi-6" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<p>7 0.68001831 <a title="82-lsi-7" href="./iccv-2013-Fibonacci_Exposure_Bracketing_for_High_Dynamic_Range_Imaging.html">164 iccv-2013-Fibonacci Exposure Bracketing for High Dynamic Range Imaging</a></p>
<p>8 0.65344614 <a title="82-lsi-8" href="./iccv-2013-Modeling_the_Calibration_Pipeline_of_the_Lytro_Camera_for_High_Quality_Light-Field_Image_Reconstruction.html">271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</a></p>
<p>9 0.63228035 <a title="82-lsi-9" href="./iccv-2013-Estimating_the_Material_Properties_of_Fabric_from_Video.html">145 iccv-2013-Estimating the Material Properties of Fabric from Video</a></p>
<p>10 0.61183983 <a title="82-lsi-10" href="./iccv-2013-Matching_Dry_to_Wet_Materials.html">262 iccv-2013-Matching Dry to Wet Materials</a></p>
<p>11 0.61037922 <a title="82-lsi-11" href="./iccv-2013-A_Simple_Model_for_Intrinsic_Image_Decomposition_with_Depth_Cues.html">30 iccv-2013-A Simple Model for Intrinsic Image Decomposition with Depth Cues</a></p>
<p>12 0.60019672 <a title="82-lsi-12" href="./iccv-2013-A_Color_Constancy_Model_with_Double-Opponency_Mechanisms.html">5 iccv-2013-A Color Constancy Model with Double-Opponency Mechanisms</a></p>
<p>13 0.59519202 <a title="82-lsi-13" href="./iccv-2013-Exploiting_Reflection_Change_for_Automatic_Reflection_Removal.html">151 iccv-2013-Exploiting Reflection Change for Automatic Reflection Removal</a></p>
<p>14 0.56929374 <a title="82-lsi-14" href="./iccv-2013-Motion-Aware_KNN_Laplacian_for_Video_Matting.html">275 iccv-2013-Motion-Aware KNN Laplacian for Video Matting</a></p>
<p>15 0.56824362 <a title="82-lsi-15" href="./iccv-2013-Optimal_Orthogonal_Basis_and_Image_Assimilation%3A_Motion_Modeling.html">301 iccv-2013-Optimal Orthogonal Basis and Image Assimilation: Motion Modeling</a></p>
<p>16 0.54141486 <a title="82-lsi-16" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>17 0.53521651 <a title="82-lsi-17" href="./iccv-2013-Dynamic_Probabilistic_Volumetric_Models.html">128 iccv-2013-Dynamic Probabilistic Volumetric Models</a></p>
<p>18 0.51832646 <a title="82-lsi-18" href="./iccv-2013-Fluttering_Pattern_Generation_Using_Modified_Legendre_Sequence_for_Coded_Exposure_Imaging.html">173 iccv-2013-Fluttering Pattern Generation Using Modified Legendre Sequence for Coded Exposure Imaging</a></p>
<p>19 0.51638728 <a title="82-lsi-19" href="./iccv-2013-Toward_Guaranteed_Illumination_Models_for_Non-convex_Objects.html">422 iccv-2013-Toward Guaranteed Illumination Models for Non-convex Objects</a></p>
<p>20 0.51453012 <a title="82-lsi-20" href="./iccv-2013-Modeling_Self-Occlusions_in_Dynamic_Shape_and_Appearance_Tracking.html">270 iccv-2013-Modeling Self-Occlusions in Dynamic Shape and Appearance Tracking</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.068), (7, 0.024), (13, 0.016), (26, 0.081), (31, 0.043), (40, 0.019), (42, 0.09), (48, 0.015), (64, 0.076), (73, 0.059), (74, 0.2), (89, 0.201), (98, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84815359 <a title="82-lda-1" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>Author: Supreeth Achar, Stephen T. Nuske, Srinivasa G. Narasimhan</p><p>Abstract: Separating the direct and global components of radiance can aid shape recovery algorithms and can provide useful information about materials in a scene. Practical methods for finding the direct and global components use multiple images captured under varying illumination patterns and require the scene, light source and camera to remain stationary during the image acquisition process. In this paper, we develop a motion compensation method that relaxes this condition and allows direct-global separation to beperformed on video sequences of dynamic scenes captured by moving projector-camera systems. Key to our method is being able to register frames in a video sequence to each other in the presence of time varying, high frequency active illumination patterns. We compare our motion compensated method to alternatives such as single shot separation and frame interleaving as well as ground truth. We present results on challenging video sequences that include various types of motions and deformations in scenes that contain complex materials like fabric, skin, leaves and wax.</p><p>2 0.84511083 <a title="82-lda-2" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>Author: Masakazu Iwamura, Tomokazu Sato, Koichi Kise</p><p>Abstract: Approximate nearest neighbor search (ANNS) is a basic and important technique used in many tasks such as object recognition. It involves two processes: selecting nearest neighbor candidates and performing a brute-force search of these candidates. Only the former though has scope for improvement. In most existing methods, it approximates the space by quantization. It then calculates all the distances between the query and all the quantized values (e.g., clusters or bit sequences), and selects a fixed number of candidates close to the query. The performance of the method is evaluated based on accuracy as a function of the number of candidates. This evaluation seems rational but poses a serious problem; it ignores the computational cost of the process of selection. In this paper, we propose a new ANNS method that takes into account costs in the selection process. Whereas existing methods employ computationally expensive techniques such as comparative sort and heap, the proposed method does not. This realizes a significantly more efficient search. We have succeeded in reducing computation times by one-third compared with the state-of-the- art on an experiment using 100 million SIFT features.</p><p>3 0.84339595 <a title="82-lda-3" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<p>Author: Tae Hyun Kim, Hee Seok Lee, Kyoung Mu Lee</p><p>Abstract: Many state-of-the-art optical flow estimation algorithms optimize the data and regularization terms to solve ill-posed problems. In this paper, in contrast to the conventional optical flow framework that uses a single or fixed data model, we study a novel framework that employs locally varying data term that adaptively combines different multiple types of data models. The locally adaptive data term greatly reduces the matching ambiguity due to the complementary nature of the multiple data models. The optimal number of complementary data models is learnt by minimizing the redundancy among them under the minimum description length constraint (MDL). From these chosen data models, a new optical flow estimation energy model is designed with the weighted sum of the multiple data models, and a convex optimization-based highly effective and practical solution thatfinds the opticalflow, as well as the weights isproposed. Comparative experimental results on the Middlebury optical flow benchmark show that the proposed method using the complementary data models outperforms the state-ofthe art methods.</p><p>4 0.82171595 <a title="82-lda-4" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>Author: Ross Girshick, Jitendra Malik</p><p>Abstract: In this paper, we show how to train a deformable part model (DPM) fast—typically in less than 20 minutes, or four times faster than the current fastest method—while maintaining high average precision on the PASCAL VOC datasets. At the core of our approach is “latent LDA,” a novel generalization of linear discriminant analysis for learning latent variable models. Unlike latent SVM, latent LDA uses efficient closed-form updates and does not require an expensive search for hard negative examples. Our approach also acts as a springboard for a detailed experimental study of DPM training. We isolate and quantify the impact of key training factors for the first time (e.g., How important are discriminative SVM filters? How important is joint parameter estimation? How many negative images are needed for training?). Our findings yield useful insights for researchers working with Markov random fields and partbased models, and have practical implications for speeding up tasks such as model selection.</p><p>5 0.82049704 <a title="82-lda-5" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>Author: Jun Wang, Wei Liu, Andy X. Sun, Yu-Gang Jiang</p><p>Abstract: Hashing techniques have been intensively investigated in the design of highly efficient search engines for largescale computer vision applications. Compared with prior approximate nearest neighbor search approaches like treebased indexing, hashing-based search schemes have prominent advantages in terms of both storage and computational efficiencies. Moreover, the procedure of devising hash functions can be easily incorporated into sophisticated machine learning tools, leading to data-dependent and task-specific compact hash codes. Therefore, a number of learning paradigms, ranging from unsupervised to supervised, have been applied to compose appropriate hash functions. How- ever, most of the existing hash function learning methods either treat hash function design as a classification problem or generate binary codes to satisfy pairwise supervision, and have not yet directly optimized the search accuracy. In this paper, we propose to leverage listwise supervision into a principled hash function learning framework. In particular, the ranking information is represented by a set of rank triplets that can be used to assess the quality of ranking. Simple linear projection-based hash functions are solved efficiently through maximizing the ranking quality over the training data. We carry out experiments on large image datasets with size up to one million and compare with the state-of-the-art hashing techniques. The extensive results corroborate that our learned hash codes via listwise supervision can provide superior search accuracy without incurring heavy computational overhead.</p><p>6 0.8204025 <a title="82-lda-6" href="./iccv-2013-Distributed_Low-Rank_Subspace_Segmentation.html">122 iccv-2013-Distributed Low-Rank Subspace Segmentation</a></p>
<p>7 0.7816242 <a title="82-lda-7" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>8 0.78049397 <a title="82-lda-8" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<p>9 0.78031492 <a title="82-lda-9" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>10 0.78003335 <a title="82-lda-10" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>11 0.7798326 <a title="82-lda-11" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>12 0.77974308 <a title="82-lda-12" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>13 0.77971315 <a title="82-lda-13" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>14 0.77970076 <a title="82-lda-14" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>15 0.77928078 <a title="82-lda-15" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>16 0.77837193 <a title="82-lda-16" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>17 0.77771372 <a title="82-lda-17" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<p>18 0.77755576 <a title="82-lda-18" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>19 0.77718639 <a title="82-lda-19" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>20 0.777098 <a title="82-lda-20" href="./iccv-2013-Real-Time_Articulated_Hand_Pose_Estimation_Using_Semi-supervised_Transductive_Regression_Forests.html">340 iccv-2013-Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
