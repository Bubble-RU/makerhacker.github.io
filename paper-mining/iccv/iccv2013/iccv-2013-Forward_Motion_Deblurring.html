<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>174 iccv-2013-Forward Motion Deblurring</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-174" href="#">iccv2013-174</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>174 iccv-2013-Forward Motion Deblurring</h1>
<br/><p>Source: <a title="iccv-2013-174-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Zheng_Forward_Motion_Deblurring_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Shicheng Zheng, Li Xu, Jiaya Jia</p><p>Abstract: We handle a special type of motion blur considering that cameras move primarily forward or backward. Solving this type of blur is of unique practical importance since nearly all car, traffic and bike-mounted cameras follow out-ofplane translational motion. We start with the study of geometric models and analyze the difficulty of existing methods to deal with them. We also propose a solution accounting for depth variation. Homographies associated with different 3D planes are considered and solved for in an optimization framework. Our method is verified on several natural image examples that cannot be satisfyingly dealt with by previous methods.</p><p>Reference: <a title="iccv-2013-174-reference" href="../iccv2013_reference/iccv-2013-Forward_Motion_Deblurring_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 hk/ leo j ia /pro j ect s / forwarddeblur  /  Abstract We handle a special type of motion blur considering that cameras move primarily forward or backward. [sent-5, score-0.838]
</p><p>2 Solving this type of blur is of unique practical importance since nearly all car, traffic and bike-mounted cameras follow out-ofplane translational motion. [sent-6, score-0.684]
</p><p>3 Introduction Motion blur is one ubiquitous problem in photo taking. [sent-12, score-0.461]
</p><p>4 Previous deblurring approaches model the degradation in different ways. [sent-13, score-0.411]
</p><p>5 For example, it is common to assume uniform blur with only in-plane translation or take into account camera rotation. [sent-14, score-0.683]
</p><p>6 While prior models are effective on images produced under their respectively defined conditions, there  are still a bunch of blurred images that find no solution in restoration using existing techniques. [sent-15, score-0.258]
</p><p>7 Motion blur caused by out-of-plane translation falls into this set. [sent-16, score-0.566]
</p><p>8 There are millions of, or even more, images that can be found easily on internet degenerated by forward or backward motion during image capture. [sent-17, score-0.281]
</p><p>9 It is because out-ofplane translation represents one dominating type of camera motion in many commonly seen scenarios. [sent-18, score-0.348]
</p><p>10 Most surveillance cameras placed on highway record moving vehicles, which could also produce this type ofmotion blur. [sent-20, score-0.247]
</p><p>11 In addition, out-of-plane translational motion is common from wearable sport cameras and other smart devices, such as Google glasses. [sent-21, score-0.229]
</p><p>12 In this paper, we focus on dealing with images blurred mainly by vehicle movement or alike, which makes it possible to safely ignore severe camera rotation for algorithmic tractability. [sent-29, score-0.399]
</p><p>13 Our objective is further narrowed down to deblurring 3D-planes with the understanding of general geometric models in image formation and its present limitation. [sent-30, score-0.459]
</p><p>14 We further explore parametric blur solution space and therefore handle photos taken from rapidly moving vehicles. [sent-37, score-0.457]
</p><p>15 Stateof-the-arts are roughly categorized to spatially-invariant and spatially-variant configurations, based on different assumptions of the underlying blur model. [sent-40, score-0.428]
</p><p>16 [2] proposed a blind deconvolution scheme applicable to natural images. [sent-44, score-0.246]
</p><p>17 It employs a variational Bayesian approach to estimate the blur kernel through marginal probability maximization. [sent-45, score-0.534]
</p><p>18 Another line of work is by extending the MAP framework to estimate latent images and blur kernels iteratively. [sent-47, score-0.467]
</p><p>19 [26] sought an unnatural representation for blur kernel estimation and proposed a fast solver for restoration. [sent-57, score-0.568]
</p><p>20 Non-Uniform Blind Deblurring The fact that blur caused by camera shake in images are usually non-uniformmotivates a series ofwork with method generalization to model spatially variant blur. [sent-60, score-0.677]
</p><p>21 In-plane translation and orthogonal rotation are used to model camera shake in another way. [sent-68, score-0.36]
</p><p>22 [6] assumed that blur is locally invariant and proposed a fast non-uniform framework based on efficient filter flow [7]. [sent-70, score-0.428]
</p><p>23 [8] developed a hardware solution to record camera shake and restore blurred images. [sent-72, score-0.433]
</p><p>24 Xu and Jia [25] used stereo images and incorporated depth into the deblurring framework. [sent-73, score-0.473]
</p><p>25 Other Work Optical aberration can be regarded as a special case of non-uniform blur caused by imperfection of lens. [sent-77, score-0.54]
</p><p>26 In [10, 16], optical blur was estimated for the  lens. [sent-80, score-0.472]
</p><p>27 [17] designed a set of bases to describe optical blur and proposed a blind deconvolution method to address optical aberration. [sent-82, score-0.762]
</p><p>28 Image blur typically stems from two sources during exposure, i. [sent-86, score-0.428]
</p><p>29 In modeling the geometric formation process of cameracaused motion blur, nearly all prior methods assume constant scene depth z under the condition that the scene is distant or front-parallel. [sent-90, score-0.265]
</p><p>30 Under the constant z assumption, blur image capture can be regarded as a sharp image l, which is formed in a very short time interval, moves in a longer duration. [sent-92, score-0.523]
</p><p>31 i  (1)  ×  where b denotes the blurred image and iindexes the transformed sharp image. [sent-95, score-0.328]
</p><p>32 (2) indicates that a view can be modeled as a homography warping of the latent sharp image l. [sent-108, score-0.358]
</p><p>33 (3)  ×  The problem of deblurring is actually to compute weight wi for each pose Hi, physically corresponding to duration of 11446666  each pose. [sent-110, score-0.517]
</p><p>34 There is a mapping between homography Hi and warping matrix Pi, where Pi is the N N matrix and each row is formed by the coefficiiesn tth oef N N bil ×ine Nar m minatterirxp oanladtion. [sent-111, score-0.253]
</p><p>35 Only the ratio of tx/z, ty/z, tz/z is required to parameterize homography H. [sent-118, score-0.251]
</p><p>36 Under constant depth z, the family of homography in the above blur process forms a 6D parameter space. [sent-119, score-0.712]
</p><p>37 H and Uniform Deblurring In the uniform blur model, θx, θy, θz and tz are all set to zeros. [sent-120, score-0.692]
</p><p>38 Effectiveness of these methods in approximating blur caused by typical hand-held camera shake was verified [11]. [sent-125, score-0.677]
</p><p>39 It is however unknown how to blindly estimate the blur kernel and latent image simultaneously, given the large solution space. [sent-127, score-0.573]
</p><p>40 Importance of Out-of-Plane Translation In nearly all prior deblurring methods, out-of-plane translation is ignored, assuming no movement orthogonal to the image plane. [sent-128, score-0.543]
</p><p>41 As noted in Section 1, this type of translation, however, is dominant in ubiquitous forward motion situations. [sent-129, score-0.348]
</p><p>42 In what follows, we discuss if motion caused by out-ofplane translation can be modeled by previous methods. [sent-130, score-0.256]
</p><p>43 This image is blurred with forward motion only, i. [sent-133, score-0.503]
</p><p>44 2(b), visually indicating that the blur is spatially variant in a radial shape. [sent-138, score-0.428]
</p><p>45 Our method actually handles a more general problem where blur occurs by translation with components existing along all axes and the motion is not necessarily perpendicular to the image plane. [sent-141, score-0.704]
</p><p>46 Almost  all practical single-image motion deblurring methods with  (a) Dot ed pat ern(b) Forward blur  Figure 2. [sent-144, score-0.957]
</p><p>47 implementation available online assume distant or frontparallel scene, which is actually not appropriate for general forward motion. [sent-146, score-0.231]
</p><p>48 It is because requiring objects completely undergo translational motion perpendicular to the camera sensor plane is overly restrictive. [sent-147, score-0.46]
</p><p>49 For example, traffic surveillance cameras are normally placed higher than vehicles or beside highway. [sent-148, score-0.276]
</p><p>50 This strategy balances system practicality and problem tractability, making the method a reasonable one for forward motion deblurring. [sent-151, score-0.281]
</p><p>51 Their projection on the blurred image is constrained, availing following optimization. [sent-154, score-0.222]
</p><p>52 For a 3D plane denoted as π = (n, d), where n is the normal vector and d is the offset to the camera center, any point X on the plane satisfies XTπ = 0. [sent-155, score-0.518]
</p><p>53 n kTewdo by a homography as  H = K(R +tndT)K−1,  (4)  where R refers to rotational motion and t denotes translation. [sent-159, score-0.34]
</p><p>54 Because we aim to deal with images primarily produced by car or traffic surveillance cameras, the rotation matrix is set to an identity I. [sent-163, score-0.282]
</p><p>55 Each blurred surface is a weighed sum of a few transformed  planes li defined in Eq. [sent-172, score-0.398]
</p><p>56 Given tx, ty, tz and n, we can uniquely determine a homography, which also corresponds to a N N warping matrix P described in Eq. [sent-176, score-0.236]
</p><p>57 P PIn d tehscisr regard, we transform originally very difficult whole-image deblurring to a plane-wise tractable problem, counting in non-frontal 3D planes. [sent-178, score-0.411]
</p><p>58 We thus propose constructing 3D homography space t = (tx, ty, tz)T and sample each tx, ty, and tz discretely to predefine a few camera poses. [sent-181, score-0.523]
</p><p>59 Put differently, our method uses a series of discrete Hin to  present the original continuous homography space, each homography or status is determined by a corresponding n and by a pose ti indexed by i. [sent-183, score-0.479]
</p><p>60 3 shows an example for demonstrating the specialty of forward motion blur. [sent-185, score-0.328]
</p><p>61 We use the dotted pattern to visualize point trajectories and homography basis, which make this kind of blur formation easy to comprehend. [sent-186, score-0.698]
</p><p>62 (a) is to show that forward motion blurred images, such as that in (b), can generally find a few planes. [sent-187, score-0.537]
</p><p>63 Previous methods, even for non-uniform deblurring, mostly consider the case n = (0, 0, 1), whereas our method handles all of them as well as planes with all non-zero elements in normal n. [sent-190, score-0.271]
</p><p>64 For planes with normals (1, 0, 0) and (0, 1, 0) (top two rows in (c)), a column and a row of pixels do not blur at all. [sent-202, score-0.571]
</p><p>65 It is because these points are infinitely distant or the plane passes the camera center. [sent-203, score-0.303]
</p><p>66 Color Mixing Issue The blur formation represented as b = ? [sent-204, score-0.476]
</p><p>67 color mixing arising in forward motion blur, which is cau? [sent-207, score-0.281]
</p><p>68 In this regard, one pixel in the blurred image is not a summation (or integration) of a few isolated unblurred pixels, but rather a combination of several patches, as illustrated in Fig. [sent-211, score-0.222]
</p><p>69 It is with tz = 0 and all other li are with tz < 0, corresponding to down-scaled versions of l. [sent-214, score-0.437]
</p><p>70 To practically model the resulting blurred image b and avoid aliasing, we regard b as the sum of latent images li blurred by Gaussian filter, whose standard deviation  is determined by tz corresponding to each li. [sent-215, score-0.756]
</p><p>71 The final blur model is finely expressed as b=  ? [sent-220, score-0.428]
</p><p>72 i  where Gi is a BTTB (block-Toeplitz with Toeplitz-block) matrix representing the Gaussian blur kernel in a matrix form. [sent-222, score-0.534]
</p><p>73 We describe in the next section our deblurring algorithm based on this model. [sent-224, score-0.411]
</p><p>74 Solving for w and lwith a fixed n corresponds to a non-uniform deblurring problem. [sent-228, score-0.411]
</p><p>75 w is known as blur kernel, since it records the duration of each camera pose, conceptually similar to 2D uniform blur PSFs. [sent-233, score-1.045]
</p><p>76 We use the local uniform assumption [6] for acceleration considering smoothly changing blur kernels under depth variation on 3D planes. [sent-270, score-0.549]
</p><p>77 Image Update In uniform deblurring, extra steps with shock filter [1] are generally employed to help kernel estimation. [sent-275, score-0.246]
</p><p>78 The scale-invariant property of L0-sparsity is vital to guide blur kernel estimation. [sent-282, score-0.534]
</p><p>79 A plane normal is parameterized in the spherical coordinate system as n = (n1, n2, n3)T = (cosαsinβ, sinα sinβ, cosβ)T, (10) where α and β are polar and azimuthal angles respectively. [sent-293, score-0.327]
</p><p>80 Given the input normal specified by angles α0 and β0, we update them within α0 15◦ and β0 15◦, with each interval 5◦. [sent-303, score-0.225]
</p><p>81 ±  11446699  ±  lel lines on a blurred plane are drawn to find two vanishing points. [sent-310, score-0.394]
</p><p>82 A plane with vanishing line v has it normal determined as n = KTv. [sent-325, score-0.334]
</p><p>83 But they are not reliable enough on blurred images. [sent-329, score-0.222]
</p><p>84 To develop a robust automatic plane detection method for forward motion blur will be our future work. [sent-330, score-0.839]
</p><p>85 Sampling Details We regularly sample tx, ty, and tz to get our homography basis Hin, accounting for out-of-plane and in-plane translation. [sent-331, score-0.464]
</p><p>86 To this end, we use the same blurred image illustrated in Fig. [sent-337, score-0.222]
</p><p>87 image a87nd) blur kernel, and updates this normal iteratively. [sent-345, score-0.59]
</p><p>88 Their respective blur kernel and normal estimates are visualized in (g)-(h). [sent-348, score-0.731]
</p><p>89 The deblurred image thus contains visual artifacts and leftover blur (see the close-ups). [sent-350, score-0.467]
</p><p>90 Our method produces a more compelling result from the single image input, shown in (f), with the corresponding 3D plane and blur kernel visualized in (g) and (h). [sent-363, score-0.699]
</p><p>91 Our result is shown in (f), with the associated plane and kernel visualized in (g) and (h). [sent-372, score-0.271]
</p><p>92 Conclusion In this paper, we focused on addressing a special and important type of motion deblurring problem, namely for11447700  (a) Input(b) Whyte et al. [sent-377, score-0.608]
</p><p>93 The initial and final deblurring results are shown in (c) and (d). [sent-384, score-0.411]
</p><p>94 ward/backward blur removal, which generally arises for traffic surveillance or vehicle cameras. [sent-392, score-0.618]
</p><p>95 Its specialty lies on modeling depth variation and pixel blending with high diThe corresponding kernel and normal results are shown in (g) and (h). [sent-393, score-0.377]
</p><p>96 We presented a method based on 3D plane models, which only needs rough plane normal initialization. [sent-401, score-0.422]
</p><p>97 Recording and playback of camera shake: benchmarking blind deconvolution with a real-world database. [sent-506, score-0.342]
</p><p>98 Rotational motion deblurring of a rigid object from a single image. [sent-565, score-0.529]
</p><p>99 Richardson-lucy deblurring for scenes under a projective motion path. [sent-571, score-0.569]
</p><p>100 Richardson-lucy deblurring for scenes under a projective motion path. [sent-579, score-0.569]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('blur', 0.428), ('deblurring', 0.411), ('blurred', 0.222), ('homography', 0.222), ('tz', 0.205), ('forward', 0.163), ('normal', 0.162), ('blind', 0.143), ('plane', 0.13), ('whyte', 0.122), ('motion', 0.118), ('hirsch', 0.116), ('shake', 0.115), ('traffic', 0.111), ('planes', 0.109), ('kernel', 0.106), ('wipingil', 0.105), ('deconvolution', 0.103), ('translation', 0.1), ('camera', 0.096), ('harmeling', 0.082), ('schuler', 0.082), ('joshi', 0.075), ('krishnan', 0.075), ('ty', 0.075), ('tx', 0.074), ('xu', 0.072), ('cho', 0.067), ('sharp', 0.066), ('depth', 0.062), ('pages', 0.062), ('translational', 0.061), ('uniform', 0.059), ('jia', 0.059), ('highway', 0.058), ('slanted', 0.054), ('anw', 0.053), ('bnl', 0.053), ('coli', 0.053), ('cameras', 0.05), ('rotation', 0.049), ('shan', 0.049), ('formation', 0.048), ('characters', 0.048), ('sch', 0.047), ('specialty', 0.047), ('shock', 0.047), ('pas', 0.047), ('special', 0.045), ('surveillance', 0.045), ('optical', 0.044), ('deblur', 0.043), ('vanishing', 0.042), ('car', 0.041), ('wi', 0.041), ('levin', 0.041), ('shaken', 0.041), ('contrarily', 0.041), ('license', 0.041), ('regard', 0.041), ('passes', 0.04), ('projective', 0.04), ('tai', 0.04), ('transformed', 0.04), ('latent', 0.039), ('deblurred', 0.039), ('vehicles', 0.039), ('caused', 0.038), ('regularly', 0.037), ('distant', 0.037), ('produced', 0.036), ('angles', 0.035), ('visualized', 0.035), ('restored', 0.035), ('status', 0.035), ('generally', 0.034), ('type', 0.034), ('normals', 0.034), ('duration', 0.034), ('unnatural', 0.034), ('ubiquitous', 0.033), ('sin', 0.032), ('movement', 0.032), ('actually', 0.031), ('placed', 0.031), ('warping', 0.031), ('energy', 0.031), ('iccp', 0.031), ('hin', 0.031), ('regarded', 0.029), ('moving', 0.029), ('saturated', 0.029), ('parameterize', 0.029), ('parallel', 0.029), ('nto', 0.028), ('sensor', 0.028), ('update', 0.028), ('zitnick', 0.028), ('li', 0.027), ('perpendicular', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="174-tfidf-1" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>Author: Shicheng Zheng, Li Xu, Jiaya Jia</p><p>Abstract: We handle a special type of motion blur considering that cameras move primarily forward or backward. Solving this type of blur is of unique practical importance since nearly all car, traffic and bike-mounted cameras follow out-ofplane translational motion. We start with the study of geometric models and analyze the difficulty of existing methods to deal with them. We also propose a solution accounting for depth variation. Homographies associated with different 3D planes are considered and solved for in an optimization framework. Our method is verified on several natural image examples that cannot be satisfyingly dealt with by previous methods.</p><p>2 0.61787713 <a title="174-tfidf-2" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>Author: Tae Hyun Kim, Byeongjoo Ahn, Kyoung Mu Lee</p><p>Abstract: Most conventional single image deblurring methods assume that the underlying scene is static and the blur is caused by only camera shake. In this paper, in contrast to this restrictive assumption, we address the deblurring problem of general dynamic scenes which contain multiple moving objects as well as camera shake. In case of dynamic scenes, moving objects and background have different blur motions, so the segmentation of the motion blur is required for deblurring each distinct blur motion accurately. Thus, we propose a novel energy model designed with the weighted sum of multiple blur data models, which estimates different motion blurs and their associated pixelwise weights, and resulting sharp image. In this framework, the local weights are determined adaptively and get high values when the corresponding data models have high data fidelity. And, the weight information is used for the segmentation of the motion blur. Non-local regularization of weights are also incorporated to produce more reliable segmentation results. A convex optimization-based method is used for the solution of the proposed energy model. Exper- imental results demonstrate that our method outperforms conventional approaches in deblurring both dynamic scenes and static scenes.</p><p>3 0.56487459 <a title="174-tfidf-3" href="./iccv-2013-Deblurring_by_Example_Using_Dense_Correspondence.html">103 iccv-2013-Deblurring by Example Using Dense Correspondence</a></p>
<p>Author: Yoav Hacohen, Eli Shechtman, Dani Lischinski</p><p>Abstract: This paper presents a new method for deblurring photos using a sharp reference example that contains some shared content with the blurry photo. Most previous deblurring methods that exploit information from other photos require an accurately registered photo of the same static scene. In contrast, our method aims to exploit reference images where the shared content may have undergone substantial photometric and non-rigid geometric transformations, as these are the kind of reference images most likely to be found in personal photo albums. Our approach builds upon a recent method for examplebased deblurring using non-rigid dense correspondence (NRDC) [11] and extends it in two ways. First, we suggest exploiting information from the reference image not only for blur kernel estimation, but also as a powerful local prior for the non-blind deconvolution step. Second, we introduce a simple yet robust technique for spatially varying blur estimation, rather than assuming spatially uniform blur. Unlike the aboveprevious method, which hasproven successful only with simple deblurring scenarios, we demonstrate that our method succeeds on a variety of real-world examples. We provide quantitative and qualitative evaluation of our method and show that it outperforms the state-of-the-art.</p><p>4 0.20592007 <a title="174-tfidf-4" href="./iccv-2013-Nonparametric_Blind_Super-resolution.html">293 iccv-2013-Nonparametric Blind Super-resolution</a></p>
<p>Author: Tomer Michaeli, Michal Irani</p><p>Abstract: Super resolution (SR) algorithms typically assume that the blur kernel is known (either the Point Spread Function ‘PSF’ of the camera, or some default low-pass filter, e.g. a Gaussian). However, the performance of SR methods significantly deteriorates when the assumed blur kernel deviates from the true one. We propose a general framework for “blind” super resolution. In particular, we show that: (i) Unlike the common belief, the PSF of the camera is the wrong blur kernel to use in SR algorithms. (ii) We show how the correct SR blur kernel can be recovered directly from the low-resolution image. This is done by exploiting the inherent recurrence property of small natural image patches (either internally within the same image, or externally in a collection of other natural images). In particular, we show that recurrence of small patches across scales of the low-res image (which forms the basis for single-image SR), can also be used for estimating the optimal blur kernel. This leads to significant improvement in SR results.</p><p>5 0.20400813 <a title="174-tfidf-5" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>Author: Netalee Efrat, Daniel Glasner, Alexander Apartsin, Boaz Nadler, Anat Levin</p><p>Abstract: Over the past decade, single image Super-Resolution (SR) research has focused on developing sophisticated image priors, leading to significant advances. Estimating and incorporating the blur model, that relates the high-res and low-res images, has received much less attention, however. In particular, the reconstruction constraint, namely that the blurred and downsampled high-res output should approximately equal the low-res input image, has been either ignored or applied with default fixed blur models. In this work, we examine the relative importance ofthe imageprior and the reconstruction constraint. First, we show that an accurate reconstruction constraint combined with a simple gradient regularization achieves SR results almost as good as those of state-of-the-art algorithms with sophisticated image priors. Second, we study both empirically and theoretically the sensitivity of SR algorithms to the blur model assumed in the reconstruction constraint. We find that an accurate blur model is more important than a sophisticated image prior. Finally, using real camera data, we demonstrate that the default blur models of various SR algorithms may differ from the camera blur, typically leading to over- smoothed results. Our findings highlight the importance of accurately estimating camera blur in reconstructing raw low- res images acquired by an actual camera.</p><p>6 0.18156853 <a title="174-tfidf-6" href="./iccv-2013-A_Unified_Rolling_Shutter_and_Motion_Blur_Model_for_3D_Visual_Registration.html">32 iccv-2013-A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration</a></p>
<p>7 0.1629843 <a title="174-tfidf-7" href="./iccv-2013-Fluttering_Pattern_Generation_Using_Modified_Legendre_Sequence_for_Coded_Exposure_Imaging.html">173 iccv-2013-Fluttering Pattern Generation Using Modified Legendre Sequence for Coded Exposure Imaging</a></p>
<p>8 0.15001281 <a title="174-tfidf-8" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>9 0.13087288 <a title="174-tfidf-9" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>10 0.126127 <a title="174-tfidf-10" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>11 0.12513947 <a title="174-tfidf-11" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>12 0.1195905 <a title="174-tfidf-12" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>13 0.1153357 <a title="174-tfidf-13" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>14 0.11036452 <a title="174-tfidf-14" href="./iccv-2013-Direct_Optimization_of_Frame-to-Frame_Rotation.html">115 iccv-2013-Direct Optimization of Frame-to-Frame Rotation</a></p>
<p>15 0.10277539 <a title="174-tfidf-15" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>16 0.09693002 <a title="174-tfidf-16" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>17 0.093795024 <a title="174-tfidf-17" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>18 0.09016294 <a title="174-tfidf-18" href="./iccv-2013-High_Quality_Shape_from_a_Single_RGB-D_Image_under_Uncalibrated_Natural_Illumination.html">199 iccv-2013-High Quality Shape from a Single RGB-D Image under Uncalibrated Natural Illumination</a></p>
<p>19 0.08722458 <a title="174-tfidf-19" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>20 0.085885994 <a title="174-tfidf-20" href="./iccv-2013-DCSH_-_Matching_Patches_in_RGBD_Images.html">101 iccv-2013-DCSH - Matching Patches in RGBD Images</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.206), (1, -0.186), (2, -0.038), (3, 0.079), (4, -0.071), (5, 0.065), (6, 0.037), (7, -0.174), (8, 0.101), (9, -0.105), (10, -0.047), (11, -0.29), (12, 0.202), (13, -0.356), (14, -0.181), (15, 0.123), (16, 0.077), (17, -0.045), (18, -0.088), (19, -0.169), (20, -0.128), (21, 0.003), (22, 0.051), (23, -0.093), (24, 0.01), (25, -0.097), (26, -0.075), (27, 0.077), (28, 0.071), (29, -0.055), (30, 0.001), (31, 0.139), (32, -0.099), (33, -0.12), (34, 0.043), (35, -0.086), (36, -0.029), (37, 0.036), (38, 0.014), (39, 0.047), (40, -0.08), (41, 0.045), (42, -0.05), (43, 0.01), (44, -0.037), (45, -0.021), (46, 0.042), (47, -0.007), (48, 0.048), (49, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96073818 <a title="174-lsi-1" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>Author: Shicheng Zheng, Li Xu, Jiaya Jia</p><p>Abstract: We handle a special type of motion blur considering that cameras move primarily forward or backward. Solving this type of blur is of unique practical importance since nearly all car, traffic and bike-mounted cameras follow out-ofplane translational motion. We start with the study of geometric models and analyze the difficulty of existing methods to deal with them. We also propose a solution accounting for depth variation. Homographies associated with different 3D planes are considered and solved for in an optimization framework. Our method is verified on several natural image examples that cannot be satisfyingly dealt with by previous methods.</p><p>2 0.95751399 <a title="174-lsi-2" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>Author: Tae Hyun Kim, Byeongjoo Ahn, Kyoung Mu Lee</p><p>Abstract: Most conventional single image deblurring methods assume that the underlying scene is static and the blur is caused by only camera shake. In this paper, in contrast to this restrictive assumption, we address the deblurring problem of general dynamic scenes which contain multiple moving objects as well as camera shake. In case of dynamic scenes, moving objects and background have different blur motions, so the segmentation of the motion blur is required for deblurring each distinct blur motion accurately. Thus, we propose a novel energy model designed with the weighted sum of multiple blur data models, which estimates different motion blurs and their associated pixelwise weights, and resulting sharp image. In this framework, the local weights are determined adaptively and get high values when the corresponding data models have high data fidelity. And, the weight information is used for the segmentation of the motion blur. Non-local regularization of weights are also incorporated to produce more reliable segmentation results. A convex optimization-based method is used for the solution of the proposed energy model. Exper- imental results demonstrate that our method outperforms conventional approaches in deblurring both dynamic scenes and static scenes.</p><p>3 0.94576877 <a title="174-lsi-3" href="./iccv-2013-Deblurring_by_Example_Using_Dense_Correspondence.html">103 iccv-2013-Deblurring by Example Using Dense Correspondence</a></p>
<p>Author: Yoav Hacohen, Eli Shechtman, Dani Lischinski</p><p>Abstract: This paper presents a new method for deblurring photos using a sharp reference example that contains some shared content with the blurry photo. Most previous deblurring methods that exploit information from other photos require an accurately registered photo of the same static scene. In contrast, our method aims to exploit reference images where the shared content may have undergone substantial photometric and non-rigid geometric transformations, as these are the kind of reference images most likely to be found in personal photo albums. Our approach builds upon a recent method for examplebased deblurring using non-rigid dense correspondence (NRDC) [11] and extends it in two ways. First, we suggest exploiting information from the reference image not only for blur kernel estimation, but also as a powerful local prior for the non-blind deconvolution step. Second, we introduce a simple yet robust technique for spatially varying blur estimation, rather than assuming spatially uniform blur. Unlike the aboveprevious method, which hasproven successful only with simple deblurring scenarios, we demonstrate that our method succeeds on a variety of real-world examples. We provide quantitative and qualitative evaluation of our method and show that it outperforms the state-of-the-art.</p><p>4 0.6660617 <a title="174-lsi-4" href="./iccv-2013-Nonparametric_Blind_Super-resolution.html">293 iccv-2013-Nonparametric Blind Super-resolution</a></p>
<p>Author: Tomer Michaeli, Michal Irani</p><p>Abstract: Super resolution (SR) algorithms typically assume that the blur kernel is known (either the Point Spread Function ‘PSF’ of the camera, or some default low-pass filter, e.g. a Gaussian). However, the performance of SR methods significantly deteriorates when the assumed blur kernel deviates from the true one. We propose a general framework for “blind” super resolution. In particular, we show that: (i) Unlike the common belief, the PSF of the camera is the wrong blur kernel to use in SR algorithms. (ii) We show how the correct SR blur kernel can be recovered directly from the low-resolution image. This is done by exploiting the inherent recurrence property of small natural image patches (either internally within the same image, or externally in a collection of other natural images). In particular, we show that recurrence of small patches across scales of the low-res image (which forms the basis for single-image SR), can also be used for estimating the optimal blur kernel. This leads to significant improvement in SR results.</p><p>5 0.65251476 <a title="174-lsi-5" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>Author: Netalee Efrat, Daniel Glasner, Alexander Apartsin, Boaz Nadler, Anat Levin</p><p>Abstract: Over the past decade, single image Super-Resolution (SR) research has focused on developing sophisticated image priors, leading to significant advances. Estimating and incorporating the blur model, that relates the high-res and low-res images, has received much less attention, however. In particular, the reconstruction constraint, namely that the blurred and downsampled high-res output should approximately equal the low-res input image, has been either ignored or applied with default fixed blur models. In this work, we examine the relative importance ofthe imageprior and the reconstruction constraint. First, we show that an accurate reconstruction constraint combined with a simple gradient regularization achieves SR results almost as good as those of state-of-the-art algorithms with sophisticated image priors. Second, we study both empirically and theoretically the sensitivity of SR algorithms to the blur model assumed in the reconstruction constraint. We find that an accurate blur model is more important than a sophisticated image prior. Finally, using real camera data, we demonstrate that the default blur models of various SR algorithms may differ from the camera blur, typically leading to over- smoothed results. Our findings highlight the importance of accurately estimating camera blur in reconstructing raw low- res images acquired by an actual camera.</p><p>6 0.65116388 <a title="174-lsi-6" href="./iccv-2013-Fluttering_Pattern_Generation_Using_Modified_Legendre_Sequence_for_Coded_Exposure_Imaging.html">173 iccv-2013-Fluttering Pattern Generation Using Modified Legendre Sequence for Coded Exposure Imaging</a></p>
<p>7 0.45824221 <a title="174-lsi-7" href="./iccv-2013-A_Unified_Rolling_Shutter_and_Motion_Blur_Model_for_3D_Visual_Registration.html">32 iccv-2013-A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration</a></p>
<p>8 0.42051804 <a title="174-lsi-8" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>9 0.39924634 <a title="174-lsi-9" href="./iccv-2013-Fibonacci_Exposure_Bracketing_for_High_Dynamic_Range_Imaging.html">164 iccv-2013-Fibonacci Exposure Bracketing for High Dynamic Range Imaging</a></p>
<p>10 0.390275 <a title="174-lsi-10" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<p>11 0.38696486 <a title="174-lsi-11" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>12 0.37677512 <a title="174-lsi-12" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>13 0.37202111 <a title="174-lsi-13" href="./iccv-2013-An_Adaptive_Descriptor_Design_for_Object_Recognition_in_the_Wild.html">48 iccv-2013-An Adaptive Descriptor Design for Object Recognition in the Wild</a></p>
<p>14 0.35553136 <a title="174-lsi-14" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>15 0.35108742 <a title="174-lsi-15" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>16 0.34717292 <a title="174-lsi-16" href="./iccv-2013-Space-Time_Tradeoffs_in_Photo_Sequencing.html">397 iccv-2013-Space-Time Tradeoffs in Photo Sequencing</a></p>
<p>17 0.34122476 <a title="174-lsi-17" href="./iccv-2013-Optimal_Orthogonal_Basis_and_Image_Assimilation%3A_Motion_Modeling.html">301 iccv-2013-Optimal Orthogonal Basis and Image Assimilation: Motion Modeling</a></p>
<p>18 0.33711711 <a title="174-lsi-18" href="./iccv-2013-Discovering_Details_and_Scene_Structure_with_Hierarchical_Iconoid_Shift.html">117 iccv-2013-Discovering Details and Scene Structure with Hierarchical Iconoid Shift</a></p>
<p>19 0.3322559 <a title="174-lsi-19" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>20 0.32840225 <a title="174-lsi-20" href="./iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding.html">14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.054), (7, 0.023), (12, 0.012), (26, 0.069), (31, 0.048), (34, 0.012), (40, 0.015), (42, 0.099), (64, 0.039), (73, 0.052), (78, 0.01), (81, 0.13), (89, 0.331), (98, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9551729 <a title="174-lda-1" href="./iccv-2013-Scene_Collaging%3A_Analysis_and_Synthesis_of_Natural_Images_with_Semantic_Layers.html">375 iccv-2013-Scene Collaging: Analysis and Synthesis of Natural Images with Semantic Layers</a></p>
<p>Author: Phillip Isola, Ce Liu</p><p>Abstract: To quickly synthesize complex scenes, digital artists often collage together visual elements from multiple sources: for example, mountainsfrom New Zealand behind a Scottish castle with wisps of Saharan sand in front. In this paper, we propose to use a similar process in order to parse a scene. We model a scene as a collage of warped, layered objects sampled from labeled, reference images. Each object is related to the rest by a set of support constraints. Scene parsing is achieved through analysis-by-synthesis. Starting with a dataset of labeled exemplar scenes, we retrieve a dictionary of candidate object segments thaOt miginaatl icmhag ea querEdyi eimd im-a age. We then combine elements of this set into a “scene collage ” that explains the query image. Beyond just assigning object labels to pixels, scene collaging produces a lot more information such as the number of each type of object in the scene, how they support one another, the ordinal depth of each object, and, to some degree, occluded content. We exploit this representation for several applications: image editing, random scene synthesis, and image-to-anaglyph.</p><p>same-paper 2 0.95498168 <a title="174-lda-2" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>Author: Shicheng Zheng, Li Xu, Jiaya Jia</p><p>Abstract: We handle a special type of motion blur considering that cameras move primarily forward or backward. Solving this type of blur is of unique practical importance since nearly all car, traffic and bike-mounted cameras follow out-ofplane translational motion. We start with the study of geometric models and analyze the difficulty of existing methods to deal with them. We also propose a solution accounting for depth variation. Homographies associated with different 3D planes are considered and solved for in an optimization framework. Our method is verified on several natural image examples that cannot be satisfyingly dealt with by previous methods.</p><p>3 0.94761401 <a title="174-lda-3" href="./iccv-2013-Parsing_IKEA_Objects%3A_Fine_Pose_Estimation.html">308 iccv-2013-Parsing IKEA Objects: Fine Pose Estimation</a></p>
<p>Author: Joseph J. Lim, Hamed Pirsiavash, Antonio Torralba</p><p>Abstract: We address the problem of localizing and estimating the fine-pose of objects in the image with exact 3D models. Our main focus is to unify contributions from the 1970s with recent advances in object detection: use local keypoint detectors to find candidate poses and score global alignment of each candidate pose to the image. Moreover, we also provide a new dataset containing fine-aligned objects with their exactly matched 3D models, and a set of models for widely used objects. We also evaluate our algorithm both on object detection and fine pose estimation, and show that our method outperforms state-of-the art algorithms.</p><p>4 0.93387812 <a title="174-lda-4" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>Author: Tae Hyun Kim, Byeongjoo Ahn, Kyoung Mu Lee</p><p>Abstract: Most conventional single image deblurring methods assume that the underlying scene is static and the blur is caused by only camera shake. In this paper, in contrast to this restrictive assumption, we address the deblurring problem of general dynamic scenes which contain multiple moving objects as well as camera shake. In case of dynamic scenes, moving objects and background have different blur motions, so the segmentation of the motion blur is required for deblurring each distinct blur motion accurately. Thus, we propose a novel energy model designed with the weighted sum of multiple blur data models, which estimates different motion blurs and their associated pixelwise weights, and resulting sharp image. In this framework, the local weights are determined adaptively and get high values when the corresponding data models have high data fidelity. And, the weight information is used for the segmentation of the motion blur. Non-local regularization of weights are also incorporated to produce more reliable segmentation results. A convex optimization-based method is used for the solution of the proposed energy model. Exper- imental results demonstrate that our method outperforms conventional approaches in deblurring both dynamic scenes and static scenes.</p><p>5 0.93269086 <a title="174-lda-5" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>Author: Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, Cordelia Schmid</p><p>Abstract: Optical flow computation is a key component in many computer vision systems designed for tasks such as action detection or activity recognition. However, despite several major advances over the last decade, handling large displacement in optical flow remains an open problem. Inspired by the large displacement optical flow of Brox & Malik [6], our approach, termed DeepFlow, blends a matching algorithm with a variational approach for optical flow. We propose a descriptor matching algorithm, tailored to the optical flow problem, that allows to boost performance on fast motions. The matching algorithm builds upon a multi-stage architecture with 6 layers, interleaving convolutions and max-pooling, a construction akin to deep convolutional nets. Using dense sampling, it allows to efficiently retrieve quasi-dense correspondences, and enjoys a built-in smoothing effect on descriptors matches, a valuable assetfor integration into an energy minimizationframework for optical flow estimation. DeepFlow efficiently handles large displacements occurring in realistic videos, and shows competitive performance on optical flow benchmarks. Furthermore, it sets a new state-of-the-art on the MPI-Sintel dataset [8].</p><p>6 0.9319573 <a title="174-lda-6" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>7 0.93189007 <a title="174-lda-7" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>8 0.93162435 <a title="174-lda-8" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>9 0.93113697 <a title="174-lda-9" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>10 0.93113095 <a title="174-lda-10" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<p>11 0.93091637 <a title="174-lda-11" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>12 0.92998618 <a title="174-lda-12" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>13 0.92992973 <a title="174-lda-13" href="./iccv-2013-EVSAC%3A_Accelerating_Hypotheses_Generation_by_Modeling_Matching_Scores_with_Extreme_Value_Theory.html">131 iccv-2013-EVSAC: Accelerating Hypotheses Generation by Modeling Matching Scores with Extreme Value Theory</a></p>
<p>14 0.92940289 <a title="174-lda-14" href="./iccv-2013-Estimating_the_3D_Layout_of_Indoor_Scenes_and_Its_Clutter_from_Depth_Sensors.html">144 iccv-2013-Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors</a></p>
<p>15 0.92744124 <a title="174-lda-15" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>16 0.92726302 <a title="174-lda-16" href="./iccv-2013-Robust_Non-parametric_Data_Fitting_for_Correspondence_Modeling.html">358 iccv-2013-Robust Non-parametric Data Fitting for Correspondence Modeling</a></p>
<p>17 0.92710572 <a title="174-lda-17" href="./iccv-2013-Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization.html">284 iccv-2013-Multiview Photometric Stereo Using Planar Mesh Parameterization</a></p>
<p>18 0.92658097 <a title="174-lda-18" href="./iccv-2013-Shape_Anchors_for_Data-Driven_Multi-view_Reconstruction.html">387 iccv-2013-Shape Anchors for Data-Driven Multi-view Reconstruction</a></p>
<p>19 0.92607117 <a title="174-lda-19" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>20 0.92593265 <a title="174-lda-20" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
