<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>339 iccv-2013-Rank Minimization across Appearance and Shape for AAM Ensemble Fitting</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-339" href="#">iccv2013-339</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>339 iccv-2013-Rank Minimization across Appearance and Shape for AAM Ensemble Fitting</h1>
<br/><p>Source: <a title="iccv-2013-339-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Cheng_Rank_Minimization_across_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Xin Cheng, Sridha Sridharan, Jason Saragih, Simon Lucey</p><p>Abstract: Active Appearance Models (AAMs) employ a paradigm of inverting a synthesis model of how an object can vary in terms of shape and appearance. As a result, the ability of AAMs to register an unseen object image is intrinsically linked to two factors. First, how well the synthesis model can reconstruct the object image. Second, the degrees of freedom in the model. Fewer degrees of freedom yield a higher likelihood of good fitting performance. In this paper we look at how these seemingly contrasting factors can complement one another for the problem of AAM fitting of an ensemble of images stemming from a constrained set (e.g. an ensemble of face images of the same person).</p><p>Reference: <a title="iccv-2013-339-reference" href="../iccv2013_reference/iccv-2013-Rank_Minimization_across_Appearance_and_Shape_for_AAM_Ensemble_Fitting_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 au  Abstract Active Appearance Models (AAMs) employ a paradigm of inverting a synthesis model of how an object can vary in terms of shape and appearance. [sent-12, score-0.109]
</p><p>2 Fewer degrees of freedom yield a higher likelihood of good fitting performance. [sent-16, score-0.223]
</p><p>3 In this paper we look at how these seemingly contrasting factors can complement one another for the problem of AAM fitting of an ensemble of images stemming from a constrained set (e. [sent-17, score-0.419]
</p><p>4 Introduction Active Appearance Models (AAMs) employ linear models of shape and appearance. [sent-21, score-0.109]
</p><p>5 [6] demonstrated this problem explicitly for the task of non-rigid face fitting. [sent-29, score-0.098]
</p><p>6 showed that: (i) person specific AAMs substantially outperform a generic AAM (i. [sent-31, score-0.115]
</p><p>7 models trained across many subjects), and (ii) this disparity in performance stems from the high degree of freedom of the generic AAM. [sent-33, score-0.16]
</p><p>8 The Problem: AAM fitting is typically applied to an ensemble of images stemming from a similar source. [sent-34, score-0.394]
</p><p>9 The proposed method simultaneously fits generic AAMs to images in an ensemble by constraining the shape and appearance variations with rank minimization. [sent-36, score-0.761]
</p><p>10 As a result the ensemblespecific AAM is determined with the ensemble’s specific shape and appearance variations. [sent-37, score-0.29]
</p><p>11 As a result the variation for the object can be modelled quite compactly in terms of a linear shape and appearance basis. [sent-44, score-0.348]
</p><p>12 It is obvious that if one has a priori knowledge of this ensemble’s specific shape and appearance basis, one could apply standard AAM fitting methods. [sent-46, score-0.384]
</p><p>13 Unfortunately, one rarely has such knowledge as an external agent would need to manually register the images in the ensemble to construct the AAM, thus defeating the purpose of the entire AAM fitting exercise. [sent-47, score-0.368]
</p><p>14 Instead one often resorts to generic AAM methods which result in sub-optimal performance. [sent-48, score-0.115]
</p><p>15 We de577  fine a generic AAM, as a model whose shape and appearance basis has been estimated to model all instances of the  object being modelled (e. [sent-49, score-0.434]
</p><p>16 We define an ensemble-specific AAM, as a model whose shape and appearance basis has been estimated to compactly model a specific instance of the object being modeled (e. [sent-52, score-0.321]
</p><p>17 Contributions: In this paper we explore the ambitious problem of automatically determining an ensemble-specific AAM directly from the ensemble in an unsupervised manner. [sent-55, score-0.206]
</p><p>18 We draw inspiration from recent works in unsupervised image ensemble alignment [4, 5, 11, 14, 17], specifically Robust Alignment by Spare and Low-rank (RASL) decomposition [14]. [sent-56, score-0.306]
</p><p>19 RASL attempts to align images in an ensemble by assuming that the aligned image ensemble is compact in terms of image variation. [sent-57, score-0.438]
</p><p>20 In this paper, we propose a RASL inspired generic AAM fitting algorithm for image ensembles. [sent-61, score-0.25]
</p><p>21 Specifically, we make three contributions in this paper: • We show how the ensemble-specific AAM can be dWeteer smhoinwed h by applying a l rea-nskp mciifnicim AizAaMtion c sntra bteegy to shape and appearance variations in conjunction with the standard AAM fitting objective function (Section 4). [sent-62, score-0.497]
</p><p>22 •  We empirically show that in the specific application oWfe efa ecme fitting, applying raatn kin c othnest srpaienctisf on shape taiondn appearance variations together yield notable better performance than constraining appearance variation alone  (Section 6). [sent-63, score-0.551]
</p><p>23 •  We show that the ensemble-specific AAM determined by et sheh proposed em eenthseomd bhlaes- slopewceirfi degrees odeft ferremedinoemd than the generic AAM. [sent-64, score-0.199]
</p><p>24 Further, the ensemble-specific AAM is capable of being applied to additional images of the same instance through canonical efficient AAM fitting methods (Section 6). [sent-65, score-0.135]
</p><p>25 Related Work: There are many methods proposed for nonrigid image ensemble alignment [2, 16, 19]. [sent-66, score-0.306]
</p><p>26 proposed a RASL inspired generic AAM fitting approach [19]. [sent-68, score-0.25]
</p><p>27 Their approach simultaneously fits generic AAMs to all images in the ensemble by constraining the compactness of the aligned appearances. [sent-69, score-0.417]
</p><p>28 Warp functions W(x; p) will be used throughiozuetd dt fhoirs paper Wtoa rdpe fnuontect a warping pof) a 2iDll b ceo uosreddin tahrtoe vec-  ×  tor x = [x, y]T by a warp parameter vector p ∈ RP, where P is the numbybe ar o wf warp parameters, btoarck p pto a f iRxed base coordinate system. [sent-81, score-0.318]
</p><p>29 he An an iumseage I warped by the warp parameter vector p, such is that I(p) = [I(W(x1; p)), . [sent-84, score-0.189]
</p><p>30 This D P matrix is formed by combining image gradients oTfh iIs( Dp)× wPith m tahtrei xJa isc foobrimane do fb yth ceo warp nfugn icmtiaogne W gra(xdi;e pnt)s, more pd)et waiiltsh on eth Jea cfoorbmiaanti oonf hofe th wiasr mp afutrnixct can W be( fxo;upn)d, in [13]. [sent-93, score-0.137]
</p><p>31 AAMs Active appearance models (AAMs) [3, 13] are usually constructed from a set of training images with the AAM mesh vertices hand-labelled on them. [sent-95, score-0.164]
</p><p>32 Then principal component analysis (PCA) is used to build a 2D linear model of shape variation. [sent-97, score-0.138]
</p><p>33 , xV, yV)T can be represented as a base shape s0 plus a linear combination of P shape vectors si, P  s = s0+? [sent-101, score-0.255]
</p><p>34 ,pP]T is the shape parameter vector and Φ = [s1, . [sent-105, score-0.109]
</p><p>35 , sP]T is the matrix of concatenated shape vectors. [sent-108, score-0.109]
</p><p>36 The AAM model of appearance variation is obtained by first warping all the training images onto the mean shape and then applying PCA on the shape normalized appearance images. [sent-109, score-0.539]
</p><p>37 The appearance of an AAM A(0) is an image vector defined over the pixels x ∈ s0 when p = 0. [sent-110, score-0.14]
</p><p>38 The appearance Aedλ o(v0e) can bp iex represented as a mean appearance A0 (0) plus a linear combination of K orthonormal appearance vectors Aj (0), K  Aλ(0)  = A0(0) +? [sent-111, score-0.42]
</p><p>39 , λK]T is the appearance parameter vector and A = [A1(0) , . [sent-115, score-0.14]
</p><p>40 , AK (0)] is the matrix of concatenated appearance vectors. [sent-118, score-0.14]
</p><p>41 578  To fit the predefined AAMs to the image, one can use the fitting algorithm based on the Lucas & Kanade (LK) algorithm [12]. [sent-119, score-0.135]
</p><p>42 In this approach one can pose AAM fitting as minimizing the following objective function,  argpm,λin? [sent-120, score-0.197]
</p><p>43 22  (3)  where I(p) represents the warped input image using the warp specified by the parameters p. [sent-122, score-0.189]
</p><p>44 The central task of this objective function is to find the shape p and appearance λ that minimizes the sum of squared distances (SSD) between the warped input image and the AAM. [sent-123, score-0.389]
</p><p>45 Since the relationship between the warp parameters p and the warped image I(p) is non-linear, a first order Taylor series linear approximation, I(p + Δp) ≈ I(p) + JΔp, is employed, waphperroex iJm satatinodns, f Io(rp pth +e image ≈Jac Io(bpi)a n+ m JaΔtrpix,. [sent-124, score-0.189]
</p><p>46 RASL Robust Alignment by Sparse and Low-rank (RASL) decomposition [14] method was built based on an assumption that the warped image ensemble matrix D(P) = [(I1(p1) , . [sent-126, score-0.284]
</p><p>47 , IF (pF)] is of low rank and the image errors are sparsely distributed [14], where P = [p1, . [sent-129, score-0.108]
</p><p>48 , pF] is the matrix of warp parameters for all F frames in the image ensemble. [sent-132, score-0.138]
</p><p>49 The central idea is to find the transformation between the original image and the warped image ensemble matrix by minimizing the rank of matrix L and the number of non-zero errors E,  arLg,Em,Pin s. [sent-134, score-0.392]
</p><p>50 The authors in [14] relaxed the objective convexity by replacing rank(·) and | | · | |0 with their convex approximations, namely trhaen nk(u·c)lae anrd norm | | · | |∗ and L1-norm | | · | | 1 respectively. [sent-137, score-0.127]
</p><p>51 i is the F 1 standard basis vector (all elements in this vecitsor t are zeros except idth b easleism veenctt oisr o (anell), e lΔemPe nist sth ien increment update of the warp parameter P. [sent-148, score-0.201]
</p><p>52 : In contrast to the conventional pair-wise image alignment methods such as LucasKanade inspired AAMs [3, 13], we proposed to fit an AAM to all images in an image ensemble simultaneously. [sent-152, score-0.356]
</p><p>53 In [19], the generic appearance model A employed i ,nλ λtheir implementation and experiment was formed by 98 appearance eigenvectors. [sent-156, score-0.395]
</p><p>54 showed that in the task of generic AAMs fitting, the shape component is the main cause of the reduced fitting robustness. [sent-158, score-0.359]
</p><p>55 We formulate the problem as,  argPm,Λin  rank(AΛ) + λ1rank(ΦP)  (7)  +λ2 | |D(P) − A0 − AΛ| |0, where AΛ represents the appearance variations of all im-  ages in the ensemble, and ΦP represents the shape variations, λ1 and λ2 are weights. [sent-160, score-0.3]
</p><p>56 The proposed method searches for the parameters P, Λ such that the appearance and shape variations are most compact. [sent-165, score-0.3]
</p><p>57 | | · | |0 is the number of non-zeros elements, this special norm t|e|rm is preferred instead of the conventional | | · | |22 since it is robust to image outliers [14]. [sent-166, score-0.115]
</p><p>58 ADMM Optimization Reformulation: It has been proven in [14] that the Alternating Direction Method of Multipliers (ADMM) [1] is extremely efficient to solve objective function which includes L1 norm | | · | | 1 or nuclear norm | | · | | . [sent-170, score-0.234]
</p><p>59 To solve our convex objective |f|u·n|c|tioonr using Ar nDoMrmM |,| we reformulate the objective of Equation 8 to, ∗  arg min  | |G| |∗ + λ1 | |X| |∗ + λ2| |E| |1  (9)  ΔP,Λ  s. [sent-171, score-0.216]
</p><p>60 iT− A0  − AΛ,  i= 1  where G, X and E are auxiliary variables to allow us to solve the objective using ADMM and the efficient softthreshold methods, G represents the appearance coefficients (same as Λ). [sent-176, score-0.252]
</p><p>61 E represents the errors between the current alignment and the estimated facial appearance. [sent-177, score-0.239]
</p><p>62 X represents the shape with the updated shape coefficients P + ΔP. [sent-178, score-0.268]
</p><p>63 Note in this formulation, we applied nuclear norm to the appearance coefficients directly instead of the appearance variations AΛ. [sent-179, score-0.488]
</p><p>64 This is because the linear appearance model A estimated by Principal Component Analysis is orthogonal, then we have | |AΛ| | = | |Λ| |∗ . [sent-180, score-0.14]
</p><p>65 ADMM Optimization: To solve the objective function of Equation 9, we rewrote the objective function in Augmented Lagrangian form, in which the equality constraints are appended into the objective function. [sent-181, score-0.186]
</p><p>66 The updates of G, E and X can be solved efficiently by the soft-threshold methods as described in [1, 14], the appearance coefficients Λ and the incremental shape coefficients ΔP are updated as,  [Λ,ΔP] = arΛg,ΔmPinμ21||G − Λ +μ11ξ1||22 +μ22||X − ΦP − ΦΔP +μ12ξ2||22+μ23||Γ||22  . [sent-195, score-0.349]
</p><p>67 The appearance coefficients Λ can then be determined as a  Figure2. [sent-203, score-0.231]
</p><p>68 MultiPIEtrain gsampleswithvaryingheadpose,fa580  cial expressions and illumination conditions with the facial landmark annotation. [sent-204, score-0.28]
</p><p>69 Implementation/Setup: The generic AAM applied in the experiments was trained using the MultiPIE database [7] and Cohn-Kanade database [9]. [sent-208, score-0.205]
</p><p>70 The MultiPIE training samples (as demonstrated in Figure 2) include identities from subject 21 to subject 346 with different head poses, facial expressions and illumination variations (subject 1 to subject 20 were reserved for testing). [sent-209, score-0.484]
</p><p>71 The obtained AAM includes 295 appearance basis vectors and 20 shape basis vectors (98% of the variations). [sent-210, score-0.333]
</p><p>72 In the implementation of the proposed method, the weight, λ1 was selec√ted using the same strategy as proposed in [14], λ1 = 1/√D, where D is the number of pixels in each appearance basis (30,000  ? [sent-211, score-0.182]
</p><p>73 A/ll|| Dthe(P PRM) S− registration errors in our experiments were determined in the reference shape system defined by AAM, in our AAM the size of face image is 116 113 pixels. [sent-215, score-0.415]
</p><p>74 Each subject includes discrete images taken from different illumination conditions, facial expressions and poses. [sent-221, score-0.26]
</p><p>75 Specifically, in this particular dataset, by constraining shape and appearance variation at the same time, the proposed method yield an average 31% accuracy improvement than Zhao’s method [19] which constrains appearance alone. [sent-227, score-0.5]
</p><p>76 To visualize the landmark registration performance, we have randomly selected test result from five test cases in Figure 3(b) 3(c). [sent-228, score-0.221]
</p><p>77 Both quantitative and qualitative results show that by constraining the shape and appearance in the ensemble, the proposed method produces more consistent landmark registrations for the discrete image ensemble. [sent-229, score-0.386]
</p><p>78 The residue frames were then registered by the standard AAM fitting algorithm using the ensemble-specific AAMs determined from the key frames. [sent-234, score-0.254]
</p><p>79 The registration performances of the proposed method with different sample sizes were compared with the conventional generic AAMs, Zhao’s method [19] and CLMs [15]. [sent-235, score-0.319]
</p><p>80 The Cumulative Distribution of the RMS registration errors of each sequence are presented in Figure 4. [sent-236, score-0.216]
</p><p>81 In this table, Da and Ds stand for the dimensionality of the ensemble-specific appearance and shape models determined by the proposed method. [sent-238, score-0.29]
</p><p>82 The original values of Da and Ds were defined in the generic AAM, which are 295 and 20. [sent-239, score-0.115]
</p><p>83 Since the frame numbers are much fewer than the appearance subspace dimension, then the original values of Da equals to the number of samples. [sent-240, score-0.167]
</p><p>84 The experimental results show that, (i) the proposed method outperforms the earlier work of generic AAMs, CLMs and [19] in terms of accuracy (on averages of 64. [sent-241, score-0.156]
</p><p>85 LFW Database: Labelled Faces in the Wild (LFW) [8] database is a collection of face photos taken under “real-  life” conditions. [sent-245, score-0.118]
</p><p>86 It includes multiple images of the same  subject with challenging nation conditions  poses, facial expressions,  and some partial occlusions. [sent-246, score-0.202]
</p><p>87 The Cumulative Distribution of the RMS registration errors evaluated on four subjects in IJAGS database. [sent-262, score-0.245]
</p><p>88 The alignment results in Figure 5 include four challenging cases, which are big facial expression variation, extreme lighting conditions, partial occlusion of face and image degration. [sent-267, score-0.327]
</p><p>89 The experimental results show that the proposed method produces impressive registration performance in these challenging test cases. [sent-268, score-0.154]
</p><p>90 The registration result of three clips are demonstrated with the registered facial landmarks in Figure 6. [sent-272, score-0.331]
</p><p>91 The qualitative result shows that the proposed method is able to produce consistent registration performance on “real-life” videos with varying image conditions. [sent-273, score-0.154]
</p><p>92 The registration performance of the existing method [19] (upper rows) and the proposed method (lower rows) tested on the images of the LFW database. [sent-275, score-0.154]
</p><p>93 The proposed method produces impressive registration performance on images with challenging conditions (big facial expression, large illumination variation, partial occlusion and image blurring) compared with the existing method. [sent-276, score-0.312]
</p><p>94 Conclusion  proposed method advances earlier methods in three ways: (i) applying appearance and shape consistency instead of  tfspciornauIcstegra,ehilntsg epomariptbnhejmlorbc,p-twsfohebrdpcytihmoefacpugoteAshnmoeadMrsitRceAfmarpSbolmLyesait. [sent-278, score-0.29]
</p><p>95 The proposed method produces consistent registration performance on video with complex background, bad resolution and big facial expressions. [sent-283, score-0.284]
</p><p>96 4% improvement in the fitting accuracy compares with the state-of-the-art method. [sent-286, score-0.135]
</p><p>97 Labeled faces in the wild: A database for studying face recognition in unconstrained environments. [sent-349, score-0.144]
</p><p>98 An iterative image registration technique with an application to stereo vision (darpa). [sent-376, score-0.154]
</p><p>99 Rasl:  Robust alignment by sparse and low-rank decomposition for linearly correlated images. [sent-389, score-0.1]
</p><p>100 Joint face alignment with a generic deformable face model. [sent-424, score-0.361]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('aam', 0.516), ('aams', 0.453), ('ensemble', 0.206), ('rasl', 0.177), ('multipie', 0.17), ('registration', 0.154), ('appearance', 0.14), ('fitting', 0.135), ('generic', 0.115), ('admm', 0.113), ('warp', 0.111), ('shape', 0.109), ('ijags', 0.107), ('facial', 0.101), ('alignment', 0.1), ('zhao', 0.093), ('clms', 0.088), ('argpm', 0.08), ('rms', 0.079), ('warped', 0.078), ('subject', 0.074), ('face', 0.073), ('sridharan', 0.071), ('constraining', 0.07), ('rank', 0.07), ('lfw', 0.069), ('gross', 0.069), ('landmark', 0.067), ('norm', 0.065), ('arg', 0.062), ('objective', 0.062), ('celebrities', 0.059), ('expressions', 0.055), ('congealing', 0.053), ('subjects', 0.053), ('stemming', 0.053), ('registered', 0.051), ('variations', 0.051), ('coefficients', 0.05), ('conventional', 0.05), ('lagrangian', 0.049), ('update', 0.048), ('lecture', 0.047), ('ji', 0.046), ('database', 0.045), ('freedom', 0.045), ('lucey', 0.045), ('degrees', 0.043), ('nuclear', 0.042), ('youtube', 0.042), ('basis', 0.042), ('matthews', 0.042), ('variation', 0.041), ('earlier', 0.041), ('determined', 0.041), ('notes', 0.041), ('conference', 0.041), ('errors', 0.038), ('base', 0.037), ('kanade', 0.037), ('active', 0.035), ('cox', 0.035), ('lucas', 0.033), ('optimisation', 0.033), ('span', 0.033), ('ar', 0.033), ('november', 0.032), ('saragih', 0.032), ('jacobian', 0.031), ('cohn', 0.03), ('scalars', 0.03), ('min', 0.03), ('illumination', 0.03), ('compactly', 0.03), ('international', 0.03), ('principal', 0.029), ('pf', 0.029), ('big', 0.029), ('modelled', 0.028), ('conditions', 0.027), ('multipliers', 0.027), ('register', 0.027), ('frames', 0.027), ('subspace', 0.027), ('io', 0.026), ('faces', 0.026), ('aligned', 0.026), ('square', 0.026), ('ceo', 0.026), ('rp', 0.026), ('da', 0.026), ('complement', 0.025), ('cumulative', 0.025), ('demonstrated', 0.025), ('darpa', 0.025), ('ganesh', 0.025), ('augmented', 0.024), ('sequence', 0.024), ('expression', 0.024), ('mesh', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="339-tfidf-1" href="./iccv-2013-Rank_Minimization_across_Appearance_and_Shape_for_AAM_Ensemble_Fitting.html">339 iccv-2013-Rank Minimization across Appearance and Shape for AAM Ensemble Fitting</a></p>
<p>Author: Xin Cheng, Sridha Sridharan, Jason Saragih, Simon Lucey</p><p>Abstract: Active Appearance Models (AAMs) employ a paradigm of inverting a synthesis model of how an object can vary in terms of shape and appearance. As a result, the ability of AAMs to register an unseen object image is intrinsically linked to two factors. First, how well the synthesis model can reconstruct the object image. Second, the degrees of freedom in the model. Fewer degrees of freedom yield a higher likelihood of good fitting performance. In this paper we look at how these seemingly contrasting factors can complement one another for the problem of AAM fitting of an ensemble of images stemming from a constrained set (e.g. an ensemble of face images of the same person).</p><p>2 0.58800268 <a title="339-tfidf-2" href="./iccv-2013-Optimization_Problems_for_Fast_AAM_Fitting_in-the-Wild.html">302 iccv-2013-Optimization Problems for Fast AAM Fitting in-the-Wild</a></p>
<p>Author: Georgios Tzimiropoulos, Maja Pantic</p><p>Abstract: We describe a very simple framework for deriving the most-well known optimization problems in Active Appearance Models (AAMs), and most importantly for providing efficient solutions. Our formulation results in two optimization problems for fast and exact AAM fitting, and one new algorithm which has the important advantage of being applicable to 3D. We show that the dominant cost for both forward and inverse algorithms is a few times mN which is the cost of projecting an image onto the appearance subspace. This makes both algorithms not only computationally realizable but also very attractive speed-wise for most current systems. Because exact AAM fitting is no longer computationally prohibitive, we trained AAMs in-the-wild with the goal of investigating whether AAMs benefit from such a training process. Our results show that although we did not use sophisticated shape priors, robust features or robust norms for improving performance, AAMs perform notably well and in some cases comparably with current state-ofthe-art methods. We provide Matlab source code for training, fitting and reproducing the results presented in this paper at ht tp ://ibug. . doc . i . a c . uk/resources. c</p><p>3 0.19439264 <a title="339-tfidf-3" href="./iccv-2013-Pose-Free_Facial_Landmark_Fitting_via_Optimized_Part_Mixtures_and_Cascaded_Deformable_Shape_Model.html">321 iccv-2013-Pose-Free Facial Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Shape Model</a></p>
<p>Author: Xiang Yu, Junzhou Huang, Shaoting Zhang, Wang Yan, Dimitris N. Metaxas</p><p>Abstract: This paper addresses the problem of facial landmark localization and tracking from a single camera. We present a two-stage cascaded deformable shape model to effectively and efficiently localize facial landmarks with large head pose variations. For face detection, we propose a group sparse learning method to automatically select the most salient facial landmarks. By introducing 3D face shape model, we use procrustes analysis to achieve pose-free facial landmark initialization. For deformation, the first step uses mean-shift local search with constrained local model to rapidly approach the global optimum. The second step uses component-wise active contours to discriminatively refine the subtle shape variation. Our framework can simultaneously handle face detection, pose-free landmark localization and tracking in real time. Extensive experiments are conducted on both laboratory environmental face databases and face-in-the-wild databases. All results demonstrate that our approach has certain advantages over state-of-theart methods in handling pose variations1.</p><p>4 0.19057526 <a title="339-tfidf-4" href="./iccv-2013-Cascaded_Shape_Space_Pruning_for_Robust_Facial_Landmark_Detection.html">70 iccv-2013-Cascaded Shape Space Pruning for Robust Facial Landmark Detection</a></p>
<p>Author: Xiaowei Zhao, Shiguang Shan, Xiujuan Chai, Xilin Chen</p><p>Abstract: In this paper, we propose a novel cascaded face shape space pruning algorithm for robust facial landmark detection. Through progressively excluding the incorrect candidate shapes, our algorithm can accurately and efficiently achieve the globally optimal shape configuration. Specifically, individual landmark detectors are firstly applied to eliminate wrong candidates for each landmark. Then, the candidate shape space is further pruned by jointly removing incorrect shape configurations. To achieve this purpose, a discriminative structure classifier is designed to assess the candidate shape configurations. Based on the learned discriminative structure classifier, an efficient shape space pruning strategy is proposed to quickly reject most incorrect candidate shapes while preserve the true shape. The proposed algorithm is carefully evaluated on a large set of real world face images. In addition, comparison results on the publicly available BioID and LFW face databases demonstrate that our algorithm outperforms some state-of-the-art algorithms.</p><p>5 0.14810805 <a title="339-tfidf-5" href="./iccv-2013-Accurate_and_Robust_3D_Facial_Capture_Using_a_Single_RGBD_Camera.html">36 iccv-2013-Accurate and Robust 3D Facial Capture Using a Single RGBD Camera</a></p>
<p>Author: Yen-Lin Chen, Hsiang-Tao Wu, Fuhao Shi, Xin Tong, Jinxiang Chai</p><p>Abstract: This paper presents an automatic and robust approach that accurately captures high-quality 3D facial performances using a single RGBD camera. The key of our approach is to combine the power of automatic facial feature detection and image-based 3D nonrigid registration techniques for 3D facial reconstruction. In particular, we develop a robust and accurate image-based nonrigid registration algorithm that incrementally deforms a 3D template mesh model to best match observed depth image data and important facial features detected from single RGBD images. The whole process is fully automatic and robust because it is based on single frame facial registration framework. The system is flexible because it does not require any strong 3D facial priors such as blendshape models. We demonstrate the power of our approach by capturing a wide range of 3D facial expressions using a single RGBD camera and achieve state-of-the-art accuracy by comparing against alternative methods.</p><p>6 0.14561464 <a title="339-tfidf-6" href="./iccv-2013-Coupling_Alignments_with_Recognition_for_Still-to-Video_Face_Recognition.html">97 iccv-2013-Coupling Alignments with Recognition for Still-to-Video Face Recognition</a></p>
<p>7 0.12588671 <a title="339-tfidf-7" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>8 0.11717951 <a title="339-tfidf-8" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>9 0.10718934 <a title="339-tfidf-9" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>10 0.10134273 <a title="339-tfidf-10" href="./iccv-2013-Exemplar-Based_Graph_Matching_for_Robust_Facial_Landmark_Localization.html">149 iccv-2013-Exemplar-Based Graph Matching for Robust Facial Landmark Localization</a></p>
<p>11 0.1010056 <a title="339-tfidf-11" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>12 0.10028015 <a title="339-tfidf-12" href="./iccv-2013-Robust_Non-parametric_Data_Fitting_for_Correspondence_Modeling.html">358 iccv-2013-Robust Non-parametric Data Fitting for Correspondence Modeling</a></p>
<p>13 0.096220307 <a title="339-tfidf-13" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>14 0.091682747 <a title="339-tfidf-14" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>15 0.090388246 <a title="339-tfidf-15" href="./iccv-2013-Uncertainty-Driven_Efficiently-Sampled_Sparse_Graphical_Models_for_Concurrent_Tumor_Segmentation_and_Atlas_Registration.html">432 iccv-2013-Uncertainty-Driven Efficiently-Sampled Sparse Graphical Models for Concurrent Tumor Segmentation and Atlas Registration</a></p>
<p>16 0.086581737 <a title="339-tfidf-16" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>17 0.086560339 <a title="339-tfidf-17" href="./iccv-2013-Multiple_Non-rigid_Surface_Detection_and_Registration.html">283 iccv-2013-Multiple Non-rigid Surface Detection and Registration</a></p>
<p>18 0.084332518 <a title="339-tfidf-18" href="./iccv-2013-Partial_Sum_Minimization_of_Singular_Values_in_RPCA_for_Low-Level_Vision.html">310 iccv-2013-Partial Sum Minimization of Singular Values in RPCA for Low-Level Vision</a></p>
<p>19 0.082927197 <a title="339-tfidf-19" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>20 0.082054585 <a title="339-tfidf-20" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.184), (1, -0.04), (2, -0.097), (3, -0.06), (4, -0.053), (5, -0.102), (6, 0.213), (7, 0.097), (8, 0.031), (9, 0.008), (10, -0.058), (11, 0.086), (12, 0.036), (13, 0.023), (14, -0.027), (15, 0.009), (16, 0.058), (17, 0.045), (18, -0.054), (19, -0.105), (20, 0.061), (21, 0.053), (22, -0.074), (23, 0.081), (24, 0.032), (25, -0.078), (26, 0.11), (27, -0.032), (28, 0.031), (29, -0.13), (30, 0.02), (31, -0.135), (32, -0.019), (33, 0.049), (34, 0.187), (35, 0.096), (36, -0.097), (37, -0.016), (38, 0.098), (39, -0.064), (40, -0.067), (41, -0.051), (42, -0.3), (43, 0.222), (44, -0.058), (45, -0.018), (46, 0.062), (47, 0.256), (48, -0.134), (49, -0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94347429 <a title="339-lsi-1" href="./iccv-2013-Optimization_Problems_for_Fast_AAM_Fitting_in-the-Wild.html">302 iccv-2013-Optimization Problems for Fast AAM Fitting in-the-Wild</a></p>
<p>Author: Georgios Tzimiropoulos, Maja Pantic</p><p>Abstract: We describe a very simple framework for deriving the most-well known optimization problems in Active Appearance Models (AAMs), and most importantly for providing efficient solutions. Our formulation results in two optimization problems for fast and exact AAM fitting, and one new algorithm which has the important advantage of being applicable to 3D. We show that the dominant cost for both forward and inverse algorithms is a few times mN which is the cost of projecting an image onto the appearance subspace. This makes both algorithms not only computationally realizable but also very attractive speed-wise for most current systems. Because exact AAM fitting is no longer computationally prohibitive, we trained AAMs in-the-wild with the goal of investigating whether AAMs benefit from such a training process. Our results show that although we did not use sophisticated shape priors, robust features or robust norms for improving performance, AAMs perform notably well and in some cases comparably with current state-ofthe-art methods. We provide Matlab source code for training, fitting and reproducing the results presented in this paper at ht tp ://ibug. . doc . i . a c . uk/resources. c</p><p>same-paper 2 0.94228184 <a title="339-lsi-2" href="./iccv-2013-Rank_Minimization_across_Appearance_and_Shape_for_AAM_Ensemble_Fitting.html">339 iccv-2013-Rank Minimization across Appearance and Shape for AAM Ensemble Fitting</a></p>
<p>Author: Xin Cheng, Sridha Sridharan, Jason Saragih, Simon Lucey</p><p>Abstract: Active Appearance Models (AAMs) employ a paradigm of inverting a synthesis model of how an object can vary in terms of shape and appearance. As a result, the ability of AAMs to register an unseen object image is intrinsically linked to two factors. First, how well the synthesis model can reconstruct the object image. Second, the degrees of freedom in the model. Fewer degrees of freedom yield a higher likelihood of good fitting performance. In this paper we look at how these seemingly contrasting factors can complement one another for the problem of AAM fitting of an ensemble of images stemming from a constrained set (e.g. an ensemble of face images of the same person).</p><p>3 0.61989444 <a title="339-lsi-3" href="./iccv-2013-Robust_Non-parametric_Data_Fitting_for_Correspondence_Modeling.html">358 iccv-2013-Robust Non-parametric Data Fitting for Correspondence Modeling</a></p>
<p>Author: Wen-Yan Lin, Ming-Ming Cheng, Shuai Zheng, Jiangbo Lu, Nigel Crook</p><p>Abstract: We propose a generic method for obtaining nonparametric image warps from noisy point correspondences. Our formulation integrates a huber function into a motion coherence framework. This makes our fitting function especially robust to piecewise correspondence noise (where an image section is consistently mismatched). By utilizing over parameterized curves, we can generate realistic nonparametric image warps from very noisy correspondence. We also demonstrate how our algorithm can be used to help stitch images taken from a panning camera by warping the images onto a virtual push-broom camera imaging plane.</p><p>4 0.58427733 <a title="339-lsi-4" href="./iccv-2013-Pose-Free_Facial_Landmark_Fitting_via_Optimized_Part_Mixtures_and_Cascaded_Deformable_Shape_Model.html">321 iccv-2013-Pose-Free Facial Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Shape Model</a></p>
<p>Author: Xiang Yu, Junzhou Huang, Shaoting Zhang, Wang Yan, Dimitris N. Metaxas</p><p>Abstract: This paper addresses the problem of facial landmark localization and tracking from a single camera. We present a two-stage cascaded deformable shape model to effectively and efficiently localize facial landmarks with large head pose variations. For face detection, we propose a group sparse learning method to automatically select the most salient facial landmarks. By introducing 3D face shape model, we use procrustes analysis to achieve pose-free facial landmark initialization. For deformation, the first step uses mean-shift local search with constrained local model to rapidly approach the global optimum. The second step uses component-wise active contours to discriminatively refine the subtle shape variation. Our framework can simultaneously handle face detection, pose-free landmark localization and tracking in real time. Extensive experiments are conducted on both laboratory environmental face databases and face-in-the-wild databases. All results demonstrate that our approach has certain advantages over state-of-theart methods in handling pose variations1.</p><p>5 0.56741571 <a title="339-lsi-5" href="./iccv-2013-Robust_Face_Landmark_Estimation_under_Occlusion.html">355 iccv-2013-Robust Face Landmark Estimation under Occlusion</a></p>
<p>Author: Xavier P. Burgos-Artizzu, Pietro Perona, Piotr Dollár</p><p>Abstract: Human faces captured in real-world conditions present large variations in shape and occlusions due to differences in pose, expression, use of accessories such as sunglasses and hats and interactions with objects (e.g. food). Current face landmark estimation approaches struggle under such conditions since theyfail toprovide aprincipled way ofhandling outliers. We propose a novel method, called Robust Cascaded Pose Regression (RCPR) which reduces exposure to outliers by detecting occlusions explicitly and using robust shape-indexed features. We show that RCPR improves on previous landmark estimation methods on three popular face datasets (LFPW, LFW and HELEN). We further explore RCPR ’s performance by introducing a novel face dataset focused on occlusion, composed of 1,007 faces presenting a wide range of occlusion patterns. RCPR reduces failure cases by half on all four datasets, at the same time as it detects face occlusions with a 80/40% precision/recall.</p><p>6 0.52303708 <a title="339-lsi-6" href="./iccv-2013-Cascaded_Shape_Space_Pruning_for_Robust_Facial_Landmark_Detection.html">70 iccv-2013-Cascaded Shape Space Pruning for Robust Facial Landmark Detection</a></p>
<p>7 0.48705956 <a title="339-lsi-7" href="./iccv-2013-Exemplar-Based_Graph_Matching_for_Robust_Facial_Landmark_Localization.html">149 iccv-2013-Exemplar-Based Graph Matching for Robust Facial Landmark Localization</a></p>
<p>8 0.43020114 <a title="339-lsi-8" href="./iccv-2013-Modifying_the_Memorability_of_Face_Photographs.html">272 iccv-2013-Modifying the Memorability of Face Photographs</a></p>
<p>9 0.39608225 <a title="339-lsi-9" href="./iccv-2013-Go-ICP%3A_Solving_3D_Registration_Efficiently_and_Globally_Optimally.html">185 iccv-2013-Go-ICP: Solving 3D Registration Efficiently and Globally Optimally</a></p>
<p>10 0.39550915 <a title="339-lsi-10" href="./iccv-2013-Modeling_Self-Occlusions_in_Dynamic_Shape_and_Appearance_Tracking.html">270 iccv-2013-Modeling Self-Occlusions in Dynamic Shape and Appearance Tracking</a></p>
<p>11 0.38400596 <a title="339-lsi-11" href="./iccv-2013-Coupling_Alignments_with_Recognition_for_Still-to-Video_Face_Recognition.html">97 iccv-2013-Coupling Alignments with Recognition for Still-to-Video Face Recognition</a></p>
<p>12 0.36482841 <a title="339-lsi-12" href="./iccv-2013-Abnormal_Event_Detection_at_150_FPS_in_MATLAB.html">34 iccv-2013-Abnormal Event Detection at 150 FPS in MATLAB</a></p>
<p>13 0.36153343 <a title="339-lsi-13" href="./iccv-2013-A_Method_of_Perceptual-Based_Shape_Decomposition.html">21 iccv-2013-A Method of Perceptual-Based Shape Decomposition</a></p>
<p>14 0.34203199 <a title="339-lsi-14" href="./iccv-2013-Partial_Sum_Minimization_of_Singular_Values_in_RPCA_for_Low-Level_Vision.html">310 iccv-2013-Partial Sum Minimization of Singular Values in RPCA for Low-Level Vision</a></p>
<p>15 0.34171349 <a title="339-lsi-15" href="./iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding.html">14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</a></p>
<p>16 0.33672163 <a title="339-lsi-16" href="./iccv-2013-SYM-FISH%3A_A_Symmetry-Aware_Flip_Invariant_Sketch_Histogram_Shape_Descriptor.html">368 iccv-2013-SYM-FISH: A Symmetry-Aware Flip Invariant Sketch Histogram Shape Descriptor</a></p>
<p>17 0.32852751 <a title="339-lsi-17" href="./iccv-2013-Automatic_Kronecker_Product_Model_Based_Detection_of_Repeated_Patterns_in_2D_Urban_Images.html">55 iccv-2013-Automatic Kronecker Product Model Based Detection of Repeated Patterns in 2D Urban Images</a></p>
<p>18 0.32399186 <a title="339-lsi-18" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>19 0.31501433 <a title="339-lsi-19" href="./iccv-2013-Detecting_Curved_Symmetric_Parts_Using_a_Deformable_Disc_Model.html">110 iccv-2013-Detecting Curved Symmetric Parts Using a Deformable Disc Model</a></p>
<p>20 0.31181407 <a title="339-lsi-20" href="./iccv-2013-Simultaneous_Clustering_and_Tracklet_Linking_for_Multi-face_Tracking_in_Videos.html">393 iccv-2013-Simultaneous Clustering and Tracklet Linking for Multi-face Tracking in Videos</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.04), (7, 0.021), (12, 0.026), (26, 0.066), (31, 0.041), (34, 0.023), (42, 0.152), (48, 0.012), (52, 0.131), (64, 0.036), (73, 0.054), (78, 0.014), (89, 0.269), (98, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94398385 <a title="339-lda-1" href="./iccv-2013-Rank_Minimization_across_Appearance_and_Shape_for_AAM_Ensemble_Fitting.html">339 iccv-2013-Rank Minimization across Appearance and Shape for AAM Ensemble Fitting</a></p>
<p>Author: Xin Cheng, Sridha Sridharan, Jason Saragih, Simon Lucey</p><p>Abstract: Active Appearance Models (AAMs) employ a paradigm of inverting a synthesis model of how an object can vary in terms of shape and appearance. As a result, the ability of AAMs to register an unseen object image is intrinsically linked to two factors. First, how well the synthesis model can reconstruct the object image. Second, the degrees of freedom in the model. Fewer degrees of freedom yield a higher likelihood of good fitting performance. In this paper we look at how these seemingly contrasting factors can complement one another for the problem of AAM fitting of an ensemble of images stemming from a constrained set (e.g. an ensemble of face images of the same person).</p><p>2 0.93246758 <a title="339-lda-2" href="./iccv-2013-Paper_Doll_Parsing%3A_Retrieving_Similar_Styles_to_Parse_Clothing_Items.html">306 iccv-2013-Paper Doll Parsing: Retrieving Similar Styles to Parse Clothing Items</a></p>
<p>Author: Kota Yamaguchi, M. Hadi Kiapour, Tamara L. Berg</p><p>Abstract: Clothing recognition is an extremely challenging problem due to wide variation in clothing item appearance, layering, and style. In this paper, we tackle the clothing parsing problem using a retrieval based approach. For a query image, we find similar styles from a large database of tagged fashion images and use these examples to parse the query. Our approach combines parsing from: pre-trained global clothing models, local clothing models learned on theflyfrom retrieved examples, and transferredparse masks (paper doll item transfer) from retrieved examples. Experimental evaluation shows that our approach significantly outperforms state of the art in parsing accuracy.</p><p>3 0.91666031 <a title="339-lda-3" href="./iccv-2013-Direct_Optimization_of_Frame-to-Frame_Rotation.html">115 iccv-2013-Direct Optimization of Frame-to-Frame Rotation</a></p>
<p>Author: Laurent Kneip, Simon Lynen</p><p>Abstract: This work makes use of a novel, recently proposed epipolar constraint for computing the relative pose between two calibrated images. By enforcing the coplanarity of epipolar plane normal vectors, it constrains the three degrees of freedom of the relative rotation between two camera views directly—independently of the translation. The present paper shows how the approach can be extended to n points, and translated into an efficient eigenvalue minimization over the three rotational degrees of freedom. Each iteration in the non-linear optimization has constant execution time, independently of the number of features. Two global optimization approaches are proposed. The first one consists of an efficient Levenberg-Marquardt scheme with randomized initial value, which already leads to stable and accurate results. The second scheme consists of a globally optimal branch-and-bound algorithm based on a bound on the eigenvalue variation derived from symmetric eigenvalue-perturbation theory. Analysis of the cost function reveals insights into the nature of a specific relative pose problem, and outlines the complexity under different conditions. The algorithm shows state-of-the-art performance w.r.t. essential-matrix based solutions, and a frameto-frame application to a video sequence immediately leads to an alternative, real-time visual odometry solution. Note: All algorithms in this paper are made available in the OpenGV library. Please visit http : / / l aurent kne ip .github . i / opengv o</p><p>4 0.9161694 <a title="339-lda-4" href="./iccv-2013-High_Quality_Shape_from_a_Single_RGB-D_Image_under_Uncalibrated_Natural_Illumination.html">199 iccv-2013-High Quality Shape from a Single RGB-D Image under Uncalibrated Natural Illumination</a></p>
<p>Author: Yudeog Han, Joon-Young Lee, In So Kweon</p><p>Abstract: We present a novel framework to estimate detailed shape of diffuse objects with uniform albedo from a single RGB-D image. To estimate accurate lighting in natural illumination environment, we introduce a general lighting model consisting oftwo components: global and local models. The global lighting model is estimated from the RGB-D input using the low-dimensional characteristic of a diffuse reflectance model. The local lighting model represents spatially varying illumination and it is estimated by using the smoothlyvarying characteristic of illumination. With both the global and local lighting model, we can estimate complex lighting variations in uncontrolled natural illumination conditions accurately. For high quality shape capture, a shapefrom-shading approach is applied with the estimated lighting model. Since the entire process is done with a single RGB-D input, our method is capable of capturing the high quality shape details of a dynamic object under natural illumination. Experimental results demonstrate the feasibility and effectiveness of our method that dramatically improves shape details of the rough depth input.</p><p>5 0.91518515 <a title="339-lda-5" href="./iccv-2013-HOGgles%3A_Visualizing_Object_Detection_Features.html">189 iccv-2013-HOGgles: Visualizing Object Detection Features</a></p>
<p>Author: Carl Vondrick, Aditya Khosla, Tomasz Malisiewicz, Antonio Torralba</p><p>Abstract: We introduce algorithms to visualize feature spaces used by object detectors. The tools in this paper allow a human to put on ‘HOG goggles ’ and perceive the visual world as a HOG based object detector sees it. We found that these visualizations allow us to analyze object detection systems in new ways and gain new insight into the detector’s failures. For example, when we visualize the features for high scoring false alarms, we discovered that, although they are clearly wrong in image space, they do look deceptively similar to true positives in feature space. This result suggests that many of these false alarms are caused by our choice of feature space, and indicates that creating a better learning algorithm or building bigger datasets is unlikely to correct these errors. By visualizing feature spaces, we can gain a more intuitive understanding of our detection systems.</p><p>6 0.91312873 <a title="339-lda-6" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<p>7 0.91309893 <a title="339-lda-7" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>8 0.91308647 <a title="339-lda-8" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>9 0.91282791 <a title="339-lda-9" href="./iccv-2013-Parsing_IKEA_Objects%3A_Fine_Pose_Estimation.html">308 iccv-2013-Parsing IKEA Objects: Fine Pose Estimation</a></p>
<p>10 0.91234642 <a title="339-lda-10" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>11 0.91218001 <a title="339-lda-11" href="./iccv-2013-Building_Part-Based_Object_Detectors_via_3D_Geometry.html">66 iccv-2013-Building Part-Based Object Detectors via 3D Geometry</a></p>
<p>12 0.91193247 <a title="339-lda-12" href="./iccv-2013-Efficient_and_Robust_Large-Scale_Rotation_Averaging.html">138 iccv-2013-Efficient and Robust Large-Scale Rotation Averaging</a></p>
<p>13 0.91170239 <a title="339-lda-13" href="./iccv-2013-Learning_to_Share_Latent_Tasks_for_Action_Recognition.html">249 iccv-2013-Learning to Share Latent Tasks for Action Recognition</a></p>
<p>14 0.91101134 <a title="339-lda-14" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>15 0.91058785 <a title="339-lda-15" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<p>16 0.91033661 <a title="339-lda-16" href="./iccv-2013-Accurate_and_Robust_3D_Facial_Capture_Using_a_Single_RGBD_Camera.html">36 iccv-2013-Accurate and Robust 3D Facial Capture Using a Single RGBD Camera</a></p>
<p>17 0.9098354 <a title="339-lda-17" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>18 0.90972877 <a title="339-lda-18" href="./iccv-2013-Joint_Noise_Level_Estimation_from_Personal_Photo_Collections.html">223 iccv-2013-Joint Noise Level Estimation from Personal Photo Collections</a></p>
<p>19 0.90952212 <a title="339-lda-19" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>20 0.90936083 <a title="339-lda-20" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
