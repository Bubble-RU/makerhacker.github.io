<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-188" href="#">iccv2013-188</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</h1>
<br/><p>Source: <a title="iccv-2013-188-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Luo_Group_Sparsity_and_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Jiajia Luo, Wei Wang, Hairong Qi</p><p>Abstract: Human action recognition based on the depth information provided by commodity depth sensors is an important yet challenging task. The noisy depth maps, different lengths of action sequences, and free styles in performing actions, may cause large intra-class variations. In this paper, a new framework based on sparse coding and temporal pyramid matching (TPM) is proposed for depthbased human action recognition. Especially, a discriminative class-specific dictionary learning algorithm isproposed for sparse coding. By adding the group sparsity and geometry constraints, features can be well reconstructed by the sub-dictionary belonging to the same class, and the geometry relationships among features are also kept in the calculated coefficients. The proposed approach is evaluated on two benchmark datasets captured by depth cameras. Experimental results show that the proposed algorithm repeatedly hqi } @ ut k . edu GB ImagesR epth ImagesD setkonlSy0 896.5170d4ept.3h021 .x02y 19.876504.dep3th02.1 x02. achieves superior performance to the state of the art algorithms. Moreover, the proposed dictionary learning method also outperforms classic dictionary learning approaches.</p><p>Reference: <a title="iccv-2013-188-reference" href="../iccv2013_reference/iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The noisy depth maps, different lengths of action sequences, and free styles in performing actions, may cause large intra-class variations. [sent-2, score-0.494]
</p><p>2 In this paper, a new framework based on sparse coding and temporal pyramid matching (TPM) is proposed for depthbased human action recognition. [sent-3, score-0.723]
</p><p>3 Especially, a discriminative class-specific dictionary learning algorithm isproposed for sparse coding. [sent-4, score-0.56]
</p><p>4 By adding the group sparsity and geometry constraints, features can be well reconstructed by the sub-dictionary belonging to the same class, and the geometry relationships among features are also kept in the calculated coefficients. [sent-5, score-0.679]
</p><p>5 Moreover, the proposed dictionary learning method also outperforms classic dictionary learning approaches. [sent-16, score-0.843]
</p><p>6 Introduction  Traditional human action recognition approaches focus on learning distinctive feature representations for actions from labelled videos and recognizing actions from unknown videos. [sent-18, score-0.687]
</p><p>7 Recently, the introduction of cost-effective depth cameras provides a new possibility to address difficult issues in traditional human action recognition. [sent-20, score-0.434]
</p><p>8 Compared to the monocular video sensors, depth cameras can provide 3D motion information so that the discrimination of actions can be enhanced and the influence of cluttered background and illumination variations can be mitigated. [sent-21, score-0.445]
</p><p>9 man motion capturing technology to accurately estimate the 3D skeleton joint positions from a single depth image, which are more compact and discriminative than RGB or depth sequences. [sent-26, score-0.6]
</p><p>10 As shown in Figure 1, the action “drink” from the MSR DailyActivity3D dataset [19], can be well reflected from the extracted 3D joints by comparing the joints “head” and “hand” in the two frames. [sent-27, score-0.443]
</p><p>11 Although with strong representation power, the esti-  mated 3D joints also bring challenges to perform depthdata based action recognition. [sent-29, score-0.351]
</p><p>12 For example, the estimated 3D joint positions are sometimes unstable due to the noisy depth maps. [sent-30, score-0.326]
</p><p>13 To extract robust features from estimated 3D joint positions, relative 3D joint features have been explored and 11880099  Class AClass B  (a) K-means  (c) SC + group sparsity  Similar features to be quantized (class A)  (b) Sparse Coding (SC)  (d) SC+group sparsity+geometry constraint  Figure 2. [sent-32, score-0.6]
</p><p>14 (d) Proposed method (sparse coding with group sparsity and geometry constraint). [sent-37, score-0.524]
</p><p>15 To represent depth sequences with different lengths, previous research  mainly focused on temporal alignment of sequences [11, 14, 21] or frequencies evolution of extracted features [19] within a given period. [sent-39, score-0.4]
</p><p>16 In this paper, a new framework is proposed for depthbased human action recognition. [sent-41, score-0.306]
</p><p>17 [23] proposed to add the Fisher discrimination criterion into the dictionary learning. [sent-51, score-0.404]
</p><p>18 In this paper, we propose a discriminative DL algorithm for depth-based action recognition. [sent-55, score-0.273]
</p><p>19 Instead of simultaneously learning one overcomplete dictionary for all classes, we learn class-specific sub-dictionaries to increase the discrimination. [sent-56, score-0.442]
</p><p>20 In addition, the l1,2-mixed norm and geometry constraint are added to the learning process to further increase the discriminative power. [sent-57, score-0.264]
</p><p>21 Existing class-specific dictionary learning methods [7, 15] are based on l1 norm which may result in randomly distributed coefficients [4]. [sent-58, score-0.558]
</p><p>22 In this paper, we add the group sparsity regularizer [26], which is a combination of l1- and l2- norms to ensure features are well reconstructed by atoms from the same class. [sent-59, score-0.748]
</p><p>23 Moreover, the geometry relationship among local features are incorporated during the process of dictionary learning, so that features from the same class with high similarity will be forced to have similar coefficients. [sent-60, score-0.68]
</p><p>24 The process that assigns each feature with coefficients according to a learned dictionary can be defined as “quantization”, following the similar definition in the field of image classification. [sent-61, score-0.545]
</p><p>25 In the sparse coding with l1 norm, features are assigned to atoms with lowest reconstruction error, but the distributions of selected atoms can be random and from different classes [4]. [sent-66, score-1.109]
</p><p>26 In the spare coding with group sparsity, features will choose atoms from the same group (class), but similar features may not choose the same atoms within the group. [sent-67, score-1.251]
</p><p>27 In our method, features from the same class will be forced to choose atoms within the same group, and the selections of atoms also relate to the similarity of features. [sent-68, score-0.929]
</p><p>28 First, a new discriminative dictionary learning algorithm is proposed to realize the quantization of depth features. [sent-70, score-0.751]
</p><p>29 Both the group sparsity and geometry constraints are incorporated to improve the discriminative power of the learned dictionary. [sent-71, score-0.459]
</p><p>30 Second, a new framework that based on sparse coding and temporal pyramid matching is proposed to solve the temporal alignment problem of depth features. [sent-72, score-0.689]
</p><p>31 Third, extensive experimental results have shown that both the proposed framework and the dictionary learning algorithm are effective for the task of action recognition based on depth maps. [sent-73, score-0.827]
</p><p>32 , yN] , sparse coding is a process to solve the optimization problem as:  mD,iXn? [sent-78, score-0.267]
</p><p>33 , dK] is the dictionary with K atoms and elements in matrix X = [x1, . [sent-86, score-0.737]
</p><p>34 Different from the K-means clustering that assigns every data with its nearest cluster center, sparse coding uses a linear combination of atoms in the dictionary D to reconstruct the data, and only a sparse number of atoms have nonzero coefficients. [sent-90, score-1.561]
</p><p>35 To increase the discriminative power of dictionary, classspecific dictionary learning methods have been proposed that learn a sub-dictionary for each class [7, 15]. [sent-91, score-0.54]
</p><p>36 , xiN⎭i] are the dataset and coefficients for class i, respectively. [sent-102, score-0.272]
</p><p>37 Since the sub-dictionaries are trained independently, it is possible that related atoms among those sub-dictionaries are generated. [sent-104, score-0.388]
</p><p>38 Proposed Method The proposed depth-based human action recognition framework consists of three components, feature extraction from the 3D joint positions, feature representation using the discriminative DL and temporal pyramid matching, and classification. [sent-118, score-0.681]
</p><p>39 Our discussion below focuses on the construction of the discriminative dictionary which is the main contributor to the success of the proposed framework. [sent-119, score-0.409]
</p><p>40 Feature Extraction Given a depth image, 20 joints of the human body can be tracked by the skeleton tracker [16]. [sent-122, score-0.35]
</p><p>41 For any depth sequence with T frames, there will be T joint features from y(1) to y(T). [sent-133, score-0.336]
</p><p>42 Group Sparsity and Geometry Constrained Dictionary Learning (DL-GSGC) The process that generates a vector representation for any depth sequence with a specific number of extracted 3D joint features is referred to as “feature representation”. [sent-137, score-0.407]
</p><p>43 Although the Bag-of-Words representation based on K-means clustering can serve the purpose, it discards all the temporal information and large vector quantization error can be introduced by assigning each 3D joint feature to its nearest “visual word”. [sent-138, score-0.346]
</p><p>44 [22] showed that classification accuracies benefit from generalizing vector quantization to sparse coding. [sent-140, score-0.265]
</p><p>45 However, discrimination of the representation can be compromised due to the possible randomly distributed coefficients solved by sparse coding [4]. [sent-141, score-0.564]
</p><p>46 In this paper, a class specific dictionary learning method based on  group sparsity and geometry constraint is proposed, referred to as DL-GSGC. [sent-142, score-0.929]
</p><p>47 Group sparsity encourages the sparse coefficients in the same group to be zero or nonzero simultaneously [2, 4, 26]. [sent-143, score-0.595]
</p><p>48 Adding the group sparsity constraint to the class-specific dictionary learning has three advantages. [sent-144, score-0.716]
</p><p>49 First, the intraclass variations among features can be compressed since features from the same class tend to select atoms within the same group (sub-dictionary). [sent-145, score-0.709]
</p><p>50 Second, influence of correlated atoms from different sub-dictionaries can be compromised since their coefficients will tend to be zero or nonzero simultaneously. [sent-146, score-0.631]
</p><p>51 Third, possible randomness in coefficients distribution can be removed since coefficients have groupclustered sparse characteristics. [sent-147, score-0.422]
</p><p>52 In this paper, the Elastic net regularizer [26] is added as the group sparsity constraint since it has automatic group effect. [sent-148, score-0.495]
</p><p>53 Given a learned dictionary that consists of all the subdictionaries and an input feature from class i, it is ideal to use atoms from the i-th class to reconstruct it. [sent-151, score-0.937]
</p><p>54 [5], we propose to add geometry constraint to the class-specific dictionary learning process. [sent-154, score-0.553]
</p><p>55 DL-G∈SG RC is designed to learn a discriminative dictionary D = [D1, . [sent-159, score-0.409]
</p><p>56 , XC] represents the coefficients matrix and coefficients vector for the j-th feature in class iis xji . [sent-174, score-0.639]
</p><p>57 reconstructed by atoms in the sub-dictionary Di but not by other atoms belonging to different classes. [sent-193, score-0.838]
</p><p>58 The group sparsity constraint is represented as λ1 |xij | 1+ λ2 ? [sent-194, score-0.318]
</p><p>59 Especially, coefficients αi for the template yi belonging to class m can be calculated by Eqs. [sent-209, score-0.449]
</p><p>60 After fixing the dictionary D, the coefficients vector xji can be calculated by solving the following convex problem (details are provided in the supplementary material):  mxijin? [sent-271, score-0.822]
</p><p>61 To remove the influence of shared features among classes, we use templates belonging to the same class as the input feature for similarity measure at this stage. [sent-298, score-0.276]
</p><p>62 6 and 7, we know that term L(xji) encourages the calculated coefficients to have zeros at atoms not from the same class as the input feature. [sent-300, score-0.785]
</p><p>63 2  Optimization Step - Dictionary  Fixing the coefficients, atoms in the dictionary can be updated. [sent-326, score-0.737]
</p><p>64 13, atoms in the sub-dictionary Di are updated one by one. [sent-336, score-0.388]
</p><p>65 When updating atom dik, all the rest atoms in D are fixed, and the first derivative of Eq. [sent-338, score-0.456]
</p><p>66 The updated atom dik can be calculated by setting Eq. [sent-346, score-0.299]
</p><p>67 Representation and Classification After constructing the discriminative dictionary D, the  coefficients for a given feature y can be calculated by solving the following optimization problem: ? [sent-352, score-0.685]
</p><p>68 To keep the temporal information during the feature representation, a temporal pyramid matching (TPM) based on a pooling function z = F(X) is used to yield the histogram representation =for every depth sequence. [sent-364, score-0.536]
</p><p>69 In addition, since the second dataset also contains the RGB video sequence, we further compare the performance between using the RGB sequence and the depth map sequence. [sent-380, score-0.245]
</p><p>70 Parameters Setting For the DL-GSGC dictionary learning, there are three parameters: λ1, λ2 and λ3 that corresponding to group spar-  ×  sity and geometry constraints, respectively. [sent-384, score-0.562]
</p><p>71 Therefore, the proposed dictionary learning method and framework is effective for the task of depth-based human action recognition. [sent-436, score-0.647]
</p><p>72 These methods include K-SVD [1], sparse coding used for image classification based on spatial pyramid matching (ScSPM) [22], and the dictionary learning with structured incoherence (DLSI) [15]. [sent-443, score-0.811]
</p><p>73 All the subsets(AS1, AS2 and AS3) are deliberately constructed such that similar movements are included within the group while A3 further contains complex actions with large and complicated body movements. [sent-450, score-0.329]
</p><p>74 It shows that the performance of DL-GSGC is superior to other sparse coding algorithms in terms of accuracies on all tests. [sent-460, score-0.348]
</p><p>75 In addition, class-specific dictionary learning methods, such as DL-GSGC and DLSI, perform better than methods learning a whole dictionary simultaneously for all classes (e. [sent-461, score-0.822]
</p><p>76 , sparse coding + TPM), is effective for action recognition, since accuracies when using different sparse coding methods outperform the literature work in both Tables 1 and 2. [sent-466, score-0.802]
</p><p>77 There are 10 subjects in this dataset, and each subject performs the same action twice, once in standing position, and once in sitting position. [sent-471, score-0.288]
</p><p>78 DInL La-dGdiStiGonC, class-specific dictionary learning bmye t4h%od ∼s, e. [sent-490, score-0.398]
</p><p>79 2  Comparison with RGB Features  Since both depth and RGB videos are available in this dataset, we also compare the performance of RGB features with that of depth features. [sent-499, score-0.41]
</p><p>80 For traditional human action recognition problem, spatio-temporal interest points based methods have been heavily explored. [sent-500, score-0.28]
</p><p>81 Compared with the performance of depth features, recognition rates on RGB sequences are lower. [sent-507, score-0.26]
</p><p>82 In this case, the 3D joint features containing depth information are more reliable than RGB features. [sent-511, score-0.306]
</p><p>83 In addition, the K-mean clustering method will cause larger quantization error than sparse coding algorithms. [sent-512, score-0.402]
</p><p>84 Therefore, depth information is important for the task of action recognition, and the sparse coding based representation is better for quantization. [sent-513, score-0.703]
</p><p>85 Conclusion This paper presented a new framework to perform human action recognition on depth sequences. [sent-515, score-0.465]
</p><p>86 To better represent the 3D joint features, a new discriminative dictionary learning algorithm (DL-GSGC) that incorporated both  group sparsity and geometry constraints was proposed. [sent-516, score-0.938]
</p><p>87 In addition, the temporal pyramid matching method was applied on each depth sequence to keep the temporal information in the representation. [sent-517, score-0.452]
</p><p>88 Moreover, the performance ofDL-GSGC is superior to classic sparse coding methods. [sent-519, score-0.34]
</p><p>89 Although the DL-GSGC is proposed for dictionary learning in the task of depth-based action recognition, it is applicable to other classification problems, such as image classification and face recognition. [sent-520, score-0.675]
</p><p>90 Graph-oriented learning via  [5]  [6]  [7]  [8]  automatic group sparsity for data analysis. [sent-547, score-0.31]
</p><p>91 Local features are not lonely - laplacian sparse coding for image classification. [sent-554, score-0.307]
</p><p>92 Learning a discriminative dictionary for sparse coding via label consistent k-svd. [sent-561, score-0.676]
</p><p>93 A dictionary learning approach for classification: separating the particularity and the commonality. [sent-566, score-0.398]
</p><p>94 Recognition and segmentation of 3-d human action using hmm and multi-class adaboost. [sent-592, score-0.249]
</p><p>95 Classification  [16]  [17]  [18]  [19]  [20]  [21]  [22]  [23]  [24]  and clustering via dictionary learning with structured incoherence and shared features. [sent-614, score-0.477]
</p><p>96 Real-time human pose recognition in parts from single depth cameras. [sent-625, score-0.252]
</p><p>97 Stop: space-time occupancy patterns for 3d action recognition from depth map sequences. [sent-635, score-0.462]
</p><p>98 Mining actionlet ensemble for action recognition with depth cameras. [sent-650, score-0.429]
</p><p>99 View invariant human action recognition using histograms of 3d joints. [sent-667, score-0.28]
</p><p>100 Linear spatial pyramid matching using sparse coding for image classification. [sent-674, score-0.33]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('atoms', 0.388), ('dictionary', 0.349), ('msr', 0.221), ('action', 0.213), ('xji', 0.201), ('tpm', 0.187), ('depth', 0.185), ('coding', 0.165), ('actions', 0.161), ('coefficients', 0.16), ('dik', 0.151), ('sparsity', 0.146), ('dl', 0.133), ('drink', 0.121), ('group', 0.115), ('dlsi', 0.114), ('rgb', 0.11), ('sparse', 0.102), ('joints', 0.1), ('geometry', 0.098), ('yi', 0.091), ('xij', 0.088), ('temporal', 0.087), ('di', 0.086), ('sctpm', 0.085), ('templates', 0.082), ('class', 0.082), ('joint', 0.081), ('calculated', 0.08), ('quantization', 0.076), ('subjects', 0.075), ('dxi', 0.07), ('atom', 0.068), ('ixi', 0.066), ('pyramid', 0.063), ('discriminative', 0.06), ('positions', 0.06), ('depthbased', 0.057), ('wmj', 0.057), ('constraint', 0.057), ('discrimination', 0.055), ('accuracies', 0.055), ('movements', 0.053), ('jn', 0.052), ('incoherence', 0.051), ('sji', 0.05), ('learning', 0.049), ('classic', 0.047), ('variations', 0.044), ('compromised', 0.044), ('sequences', 0.044), ('overcomplete', 0.044), ('elastic', 0.043), ('yji', 0.042), ('zeros', 0.042), ('features', 0.04), ('pooling', 0.04), ('incorporated', 0.04), ('nonzero', 0.039), ('representation', 0.038), ('draw', 0.038), ('hip', 0.038), ('derivations', 0.038), ('xit', 0.036), ('belonging', 0.036), ('feature', 0.036), ('human', 0.036), ('sc', 0.034), ('lengths', 0.034), ('activities', 0.034), ('frames', 0.034), ('yij', 0.034), ('referred', 0.033), ('occupancy', 0.033), ('encourages', 0.033), ('regularizer', 0.033), ('realize', 0.032), ('fixing', 0.032), ('classification', 0.032), ('write', 0.032), ('recognition', 0.031), ('addition', 0.031), ('styles', 0.031), ('cause', 0.031), ('forced', 0.031), ('dataset', 0.03), ('sequence', 0.03), ('wij', 0.029), ('skeleton', 0.029), ('net', 0.029), ('ki', 0.029), ('xi', 0.028), ('stand', 0.028), ('clustering', 0.028), ('od', 0.027), ('reconstructed', 0.026), ('superior', 0.026), ('yang', 0.026), ('classes', 0.026), ('rf', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="188-tfidf-1" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>Author: Jiajia Luo, Wei Wang, Hairong Qi</p><p>Abstract: Human action recognition based on the depth information provided by commodity depth sensors is an important yet challenging task. The noisy depth maps, different lengths of action sequences, and free styles in performing actions, may cause large intra-class variations. In this paper, a new framework based on sparse coding and temporal pyramid matching (TPM) is proposed for depthbased human action recognition. Especially, a discriminative class-specific dictionary learning algorithm isproposed for sparse coding. By adding the group sparsity and geometry constraints, features can be well reconstructed by the sub-dictionary belonging to the same class, and the geometry relationships among features are also kept in the calculated coefficients. The proposed approach is evaluated on two benchmark datasets captured by depth cameras. Experimental results show that the proposed algorithm repeatedly hqi } @ ut k . edu GB ImagesR epth ImagesD setkonlSy0 896.5170d4ept.3h021 .x02y 19.876504.dep3th02.1 x02. achieves superior performance to the state of the art algorithms. Moreover, the proposed dictionary learning method also outperforms classic dictionary learning approaches.</p><p>2 0.37647492 <a title="188-tfidf-2" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>Author: Chenglong Bao, Jian-Feng Cai, Hui Ji</p><p>Abstract: In recent years, how to learn a dictionary from input images for sparse modelling has been one very active topic in image processing and recognition. Most existing dictionary learning methods consider an over-complete dictionary, e.g. the K-SVD method. Often they require solving some minimization problem that is very challenging in terms of computational feasibility and efficiency. However, if the correlations among dictionary atoms are not well constrained, the redundancy of the dictionary does not necessarily improve the performance of sparse coding. This paper proposed a fast orthogonal dictionary learning method for sparse image representation. With comparable performance on several image restoration tasks, the proposed method is much more computationally efficient than the over-complete dictionary based learning methods.</p><p>3 0.33251715 <a title="188-tfidf-3" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>Author: Hua Wang, Feiping Nie, Weidong Cai, Heng Huang</p><p>Abstract: Representing the raw input of a data set by a set of relevant codes is crucial to many computer vision applications. Due to the intrinsic sparse property of real-world data, dictionary learning, in which the linear decomposition of a data point uses a set of learned dictionary bases, i.e., codes, has demonstrated state-of-the-art performance. However, traditional dictionary learning methods suffer from three weaknesses: sensitivity to noisy and outlier samples, difficulty to determine the optimal dictionary size, and incapability to incorporate supervision information. In this paper, we address these weaknesses by learning a Semi-Supervised Robust Dictionary (SSR-D). Specifically, we use the ℓ2,0+ norm as the loss function to improve the robustness against outliers, and develop a new structured sparse regularization com, , tom. . cai@sydney . edu . au , heng@uta .edu make the learning tasks easier to deal with and reduce the computational cost. For example, in image tagging, instead of using the raw pixel-wise features, semi-local or patch- based features, such as SIFT and geometric blur, are usually more desirable to achieve better performance. In practice, finding a set of compact features bases, also referred to as dictionary, with enhanced representative and discriminative power, plays a significant role in building a successful computer vision system. In this paper, we explore this important problem by proposing a novel formulation and its solution for learning Semi-Supervised Robust Dictionary (SSRD), where we examine the challenges in dictionary learning, and seek opportunities to overcome them and improve the dictionary qualities. 1.1. Challenges in Dictionary Learning to incorporate the supervision information in dictionary learning, without incurring additional parameters. Moreover, the optimal dictionary size is automatically learned from the input data. Minimizing the derived objective function is challenging because it involves many non-smooth ℓ2,0+ -norm terms. We present an efficient algorithm to solve the problem with a rigorous proof of the convergence of the algorithm. Extensive experiments are presented to show the superior performance of the proposed method.</p><p>4 0.32239234 <a title="188-tfidf-4" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>Author: Chen-Kuo Chiang, Te-Feng Su, Chih Yen, Shang-Hong Lai</p><p>Abstract: We present a multi-attributed dictionary learning algorithm for sparse coding. Considering training samples with multiple attributes, a new distance matrix is proposed by jointly incorporating data and attribute similarities. Then, an objective function is presented to learn categorydependent dictionaries that are compact (closeness of dictionary atoms based on data distance and attribute similarity), reconstructive (low reconstruction error with correct dictionary) and label-consistent (encouraging the labels of dictionary atoms to be similar). We have demonstrated our algorithm on action classification and face recognition tasks on several publicly available datasets. Experimental results with improved performance over previous dictionary learning methods are shown to validate the effectiveness of the proposed algorithm.</p><p>5 0.32038504 <a title="188-tfidf-5" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>Author: Jingjing Zheng, Zhuolin Jiang</p><p>Abstract: We present an approach to jointly learn a set of viewspecific dictionaries and a common dictionary for crossview action recognition. The set of view-specific dictionaries is learned for specific views while the common dictionary is shared across different views. Our approach represents videos in each view using both the corresponding view-specific dictionary and the common dictionary. More importantly, it encourages the set of videos taken from different views of the same action to have similar sparse representations. In this way, we can align view-specific features in the sparse feature spaces spanned by the viewspecific dictionary set and transfer the view-shared features in the sparse feature space spanned by the common dictionary. Meanwhile, the incoherence between the common dictionary and the view-specific dictionary set enables us to exploit the discrimination information encoded in viewspecific features and view-shared features separately. In addition, the learned common dictionary not only has the capability to represent actions from unseen views, but also , makes our approach effective in a semi-supervised setting where no correspondence videos exist and only a few labels exist in the target view. Extensive experiments using the multi-view IXMAS dataset demonstrate that our approach outperforms many recent approaches for cross-view action recognition.</p><p>6 0.27484488 <a title="188-tfidf-6" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>7 0.26213947 <a title="188-tfidf-7" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>8 0.25966352 <a title="188-tfidf-8" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>9 0.25708669 <a title="188-tfidf-9" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>10 0.24678719 <a title="188-tfidf-10" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>11 0.23465377 <a title="188-tfidf-11" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>12 0.23330744 <a title="188-tfidf-12" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>13 0.22582361 <a title="188-tfidf-13" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>14 0.207068 <a title="188-tfidf-14" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>15 0.19957282 <a title="188-tfidf-15" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>16 0.18817815 <a title="188-tfidf-16" href="./iccv-2013-Dictionary_Learning_and_Sparse_Coding_on_Grassmann_Manifolds%3A_An_Extrinsic_Solution.html">114 iccv-2013-Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution</a></p>
<p>17 0.17901424 <a title="188-tfidf-17" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>18 0.16377078 <a title="188-tfidf-18" href="./iccv-2013-Coupled_Dictionary_and_Feature_Space_Learning_with_Applications_to_Cross-Domain_Image_Synthesis_and_Recognition.html">96 iccv-2013-Coupled Dictionary and Feature Space Learning with Applications to Cross-Domain Image Synthesis and Recognition</a></p>
<p>19 0.15559588 <a title="188-tfidf-19" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>20 0.15035434 <a title="188-tfidf-20" href="./iccv-2013-Learning_to_Share_Latent_Tasks_for_Action_Recognition.html">249 iccv-2013-Learning to Share Latent Tasks for Action Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.298), (1, 0.239), (2, -0.038), (3, 0.188), (4, -0.343), (5, -0.19), (6, -0.11), (7, -0.158), (8, -0.136), (9, 0.029), (10, 0.056), (11, 0.119), (12, -0.025), (13, 0.021), (14, -0.001), (15, 0.02), (16, -0.02), (17, -0.078), (18, -0.011), (19, 0.012), (20, -0.031), (21, 0.024), (22, -0.022), (23, -0.018), (24, 0.002), (25, -0.046), (26, -0.012), (27, 0.049), (28, 0.039), (29, 0.033), (30, 0.022), (31, 0.012), (32, -0.068), (33, 0.031), (34, -0.017), (35, -0.007), (36, 0.035), (37, 0.019), (38, 0.004), (39, 0.02), (40, -0.026), (41, 0.003), (42, 0.02), (43, 0.033), (44, -0.098), (45, -0.017), (46, 0.042), (47, -0.028), (48, -0.013), (49, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96654361 <a title="188-lsi-1" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>Author: Jiajia Luo, Wei Wang, Hairong Qi</p><p>Abstract: Human action recognition based on the depth information provided by commodity depth sensors is an important yet challenging task. The noisy depth maps, different lengths of action sequences, and free styles in performing actions, may cause large intra-class variations. In this paper, a new framework based on sparse coding and temporal pyramid matching (TPM) is proposed for depthbased human action recognition. Especially, a discriminative class-specific dictionary learning algorithm isproposed for sparse coding. By adding the group sparsity and geometry constraints, features can be well reconstructed by the sub-dictionary belonging to the same class, and the geometry relationships among features are also kept in the calculated coefficients. The proposed approach is evaluated on two benchmark datasets captured by depth cameras. Experimental results show that the proposed algorithm repeatedly hqi } @ ut k . edu GB ImagesR epth ImagesD setkonlSy0 896.5170d4ept.3h021 .x02y 19.876504.dep3th02.1 x02. achieves superior performance to the state of the art algorithms. Moreover, the proposed dictionary learning method also outperforms classic dictionary learning approaches.</p><p>2 0.8416664 <a title="188-lsi-2" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>Author: Jingjing Zheng, Zhuolin Jiang</p><p>Abstract: We present an approach to jointly learn a set of viewspecific dictionaries and a common dictionary for crossview action recognition. The set of view-specific dictionaries is learned for specific views while the common dictionary is shared across different views. Our approach represents videos in each view using both the corresponding view-specific dictionary and the common dictionary. More importantly, it encourages the set of videos taken from different views of the same action to have similar sparse representations. In this way, we can align view-specific features in the sparse feature spaces spanned by the viewspecific dictionary set and transfer the view-shared features in the sparse feature space spanned by the common dictionary. Meanwhile, the incoherence between the common dictionary and the view-specific dictionary set enables us to exploit the discrimination information encoded in viewspecific features and view-shared features separately. In addition, the learned common dictionary not only has the capability to represent actions from unseen views, but also , makes our approach effective in a semi-supervised setting where no correspondence videos exist and only a few labels exist in the target view. Extensive experiments using the multi-view IXMAS dataset demonstrate that our approach outperforms many recent approaches for cross-view action recognition.</p><p>3 0.78064495 <a title="188-lsi-3" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>Author: Hua Wang, Feiping Nie, Weidong Cai, Heng Huang</p><p>Abstract: Representing the raw input of a data set by a set of relevant codes is crucial to many computer vision applications. Due to the intrinsic sparse property of real-world data, dictionary learning, in which the linear decomposition of a data point uses a set of learned dictionary bases, i.e., codes, has demonstrated state-of-the-art performance. However, traditional dictionary learning methods suffer from three weaknesses: sensitivity to noisy and outlier samples, difficulty to determine the optimal dictionary size, and incapability to incorporate supervision information. In this paper, we address these weaknesses by learning a Semi-Supervised Robust Dictionary (SSR-D). Specifically, we use the ℓ2,0+ norm as the loss function to improve the robustness against outliers, and develop a new structured sparse regularization com, , tom. . cai@sydney . edu . au , heng@uta .edu make the learning tasks easier to deal with and reduce the computational cost. For example, in image tagging, instead of using the raw pixel-wise features, semi-local or patch- based features, such as SIFT and geometric blur, are usually more desirable to achieve better performance. In practice, finding a set of compact features bases, also referred to as dictionary, with enhanced representative and discriminative power, plays a significant role in building a successful computer vision system. In this paper, we explore this important problem by proposing a novel formulation and its solution for learning Semi-Supervised Robust Dictionary (SSRD), where we examine the challenges in dictionary learning, and seek opportunities to overcome them and improve the dictionary qualities. 1.1. Challenges in Dictionary Learning to incorporate the supervision information in dictionary learning, without incurring additional parameters. Moreover, the optimal dictionary size is automatically learned from the input data. Minimizing the derived objective function is challenging because it involves many non-smooth ℓ2,0+ -norm terms. We present an efficient algorithm to solve the problem with a rigorous proof of the convergence of the algorithm. Extensive experiments are presented to show the superior performance of the proposed method.</p><p>4 0.77351558 <a title="188-lsi-4" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>Author: Chenglong Bao, Jian-Feng Cai, Hui Ji</p><p>Abstract: In recent years, how to learn a dictionary from input images for sparse modelling has been one very active topic in image processing and recognition. Most existing dictionary learning methods consider an over-complete dictionary, e.g. the K-SVD method. Often they require solving some minimization problem that is very challenging in terms of computational feasibility and efficiency. However, if the correlations among dictionary atoms are not well constrained, the redundancy of the dictionary does not necessarily improve the performance of sparse coding. This paper proposed a fast orthogonal dictionary learning method for sparse image representation. With comparable performance on several image restoration tasks, the proposed method is much more computationally efficient than the over-complete dictionary based learning methods.</p><p>5 0.76844776 <a title="188-lsi-5" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>Author: Zhaowen Wang, Jianchao Yang, Nasser Nasrabadi, Thomas Huang</p><p>Abstract: Sparse Representation-based Classification (SRC) is a powerful tool in distinguishing signal categories which lie on different subspaces. Despite its wide application to visual recognition tasks, current understanding of SRC is solely based on a reconstructive perspective, which neither offers any guarantee on its classification performance nor provides any insight on how to design a discriminative dictionary for SRC. In this paper, we present a novel perspective towards SRC and interpret it as a margin classifier. The decision boundary and margin of SRC are analyzed in local regions where the support of sparse code is stable. Based on the derived margin, we propose a hinge loss function as the gauge for the classification performance of SRC. A stochastic gradient descent algorithm is implemented to maximize the margin of SRC and obtain more discriminative dictionaries. Experiments validate the effectiveness of the proposed approach in predicting classification performance and improving dictionary quality over reconstructive ones. Classification results competitive with other state-ofthe-art sparse coding methods are reported on several data sets.</p><p>6 0.76322991 <a title="188-lsi-6" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>7 0.75345218 <a title="188-lsi-7" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>8 0.72934103 <a title="188-lsi-8" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>9 0.67664707 <a title="188-lsi-9" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>10 0.67567009 <a title="188-lsi-10" href="./iccv-2013-Coupled_Dictionary_and_Feature_Space_Learning_with_Applications_to_Cross-Domain_Image_Synthesis_and_Recognition.html">96 iccv-2013-Coupled Dictionary and Feature Space Learning with Applications to Cross-Domain Image Synthesis and Recognition</a></p>
<p>11 0.67512053 <a title="188-lsi-11" href="./iccv-2013-Dictionary_Learning_and_Sparse_Coding_on_Grassmann_Manifolds%3A_An_Extrinsic_Solution.html">114 iccv-2013-Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution</a></p>
<p>12 0.66568351 <a title="188-lsi-12" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>13 0.63489068 <a title="188-lsi-13" href="./iccv-2013-Action_Recognition_with_Actons.html">38 iccv-2013-Action Recognition with Actons</a></p>
<p>14 0.62605006 <a title="188-lsi-14" href="./iccv-2013-Manipulation_Pattern_Discovery%3A_A_Nonparametric_Bayesian_Approach.html">260 iccv-2013-Manipulation Pattern Discovery: A Nonparametric Bayesian Approach</a></p>
<p>15 0.6035971 <a title="188-lsi-15" href="./iccv-2013-Low-Rank_Sparse_Coding_for_Image_Classification.html">258 iccv-2013-Low-Rank Sparse Coding for Image Classification</a></p>
<p>16 0.58514494 <a title="188-lsi-16" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>17 0.58164841 <a title="188-lsi-17" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>18 0.57991463 <a title="188-lsi-18" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>19 0.56261122 <a title="188-lsi-19" href="./iccv-2013-Abnormal_Event_Detection_at_150_FPS_in_MATLAB.html">34 iccv-2013-Abnormal Event Detection at 150 FPS in MATLAB</a></p>
<p>20 0.56000096 <a title="188-lsi-20" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.082), (7, 0.02), (12, 0.024), (16, 0.169), (26, 0.091), (31, 0.053), (42, 0.138), (48, 0.021), (64, 0.085), (73, 0.033), (78, 0.019), (89, 0.154), (98, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90562248 <a title="188-lda-1" href="./iccv-2013-From_Point_to_Set%3A_Extend_the_Learning_of_Distance_Metrics.html">177 iccv-2013-From Point to Set: Extend the Learning of Distance Metrics</a></p>
<p>Author: Pengfei Zhu, Lei Zhang, Wangmeng Zuo, David Zhang</p><p>Abstract: Most of the current metric learning methods are proposed for point-to-point distance (PPD) based classification. In many computer vision tasks, however, we need to measure the point-to-set distance (PSD) and even set-to-set distance (SSD) for classification. In this paper, we extend the PPD based Mahalanobis distance metric learning to PSD and SSD based ones, namely point-to-set distance metric learning (PSDML) and set-to-set distance metric learning (SSDML), and solve them under a unified optimization framework. First, we generate positive and negative sample pairs by computing the PSD and SSD between training samples. Then, we characterize each sample pair by its covariance matrix, and propose a covariance kernel based discriminative function. Finally, we tackle the PSDML and SSDMLproblems by using standard support vector machine solvers, making the metric learning very efficient for multiclass visual classification tasks. Experiments on gender classification, digit recognition, object categorization and face recognition show that the proposed metric learning methods can effectively enhance the performance of PSD and SSD based classification.</p><p>same-paper 2 0.86559224 <a title="188-lda-2" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>Author: Jiajia Luo, Wei Wang, Hairong Qi</p><p>Abstract: Human action recognition based on the depth information provided by commodity depth sensors is an important yet challenging task. The noisy depth maps, different lengths of action sequences, and free styles in performing actions, may cause large intra-class variations. In this paper, a new framework based on sparse coding and temporal pyramid matching (TPM) is proposed for depthbased human action recognition. Especially, a discriminative class-specific dictionary learning algorithm isproposed for sparse coding. By adding the group sparsity and geometry constraints, features can be well reconstructed by the sub-dictionary belonging to the same class, and the geometry relationships among features are also kept in the calculated coefficients. The proposed approach is evaluated on two benchmark datasets captured by depth cameras. Experimental results show that the proposed algorithm repeatedly hqi } @ ut k . edu GB ImagesR epth ImagesD setkonlSy0 896.5170d4ept.3h021 .x02y 19.876504.dep3th02.1 x02. achieves superior performance to the state of the art algorithms. Moreover, the proposed dictionary learning method also outperforms classic dictionary learning approaches.</p><p>3 0.85451961 <a title="188-lda-3" href="./iccv-2013-PM-Huber%3A_PatchMatch_with_Huber_Regularization_for_Stereo_Matching.html">304 iccv-2013-PM-Huber: PatchMatch with Huber Regularization for Stereo Matching</a></p>
<p>Author: Philipp Heise, Sebastian Klose, Brian Jensen, Alois Knoll</p><p>Abstract: Most stereo correspondence algorithms match support windows at integer-valued disparities and assume a constant disparity value within the support window. The recently proposed PatchMatch stereo algorithm [7] overcomes this limitation of previous algorithms by directly estimating planes. This work presents a method that integrates the PatchMatch stereo algorithm into a variational smoothing formulation using quadratic relaxation. The resulting algorithm allows the explicit regularization of the disparity and normal gradients using the estimated plane parameters. Evaluation of our method in the Middlebury benchmark shows that our method outperforms the traditional integer-valued disparity strategy as well as the original algorithm and its variants in sub-pixel accurate disparity estimation.</p><p>4 0.83488154 <a title="188-lda-4" href="./iccv-2013-DCSH_-_Matching_Patches_in_RGBD_Images.html">101 iccv-2013-DCSH - Matching Patches in RGBD Images</a></p>
<p>Author: Yaron Eshet, Simon Korman, Eyal Ofek, Shai Avidan</p><p>Abstract: We extend patch based methods to work on patches in 3D space. We start with Coherency Sensitive Hashing [12] (CSH), which is an algorithm for matching patches between two RGB images, and extend it to work with RGBD images. This is done by warping all 3D patches to a common virtual plane in which CSH is performed. To avoid noise due to warping of patches of various normals and depths, we estimate a group of dominant planes and compute CSH on each plane separately, before merging the matching patches. The result is DCSH - an algorithm that matches world (3D) patches in order to guide the search for image plane matches. An independent contribution is an extension of CSH, which we term Social-CSH. It allows a major speedup of the k nearest neighbor (kNN) version of CSH - its runtime growing linearly, rather than quadratically, in k. Social-CSH is used as a subcomponent of DCSH when many NNs are required, as in the case of image denoising. We show the benefits ofusing depth information to image reconstruction and image denoising, demonstrated on several RGBD images.</p><p>5 0.82068771 <a title="188-lda-5" href="./iccv-2013-A_Global_Linear_Method_for_Camera_Pose_Registration.html">17 iccv-2013-A Global Linear Method for Camera Pose Registration</a></p>
<p>Author: Nianjuan Jiang, Zhaopeng Cui, Ping Tan</p><p>Abstract: We present a linear method for global camera pose registration from pairwise relative poses encoded in essential matrices. Our method minimizes an approximate geometric error to enforce the triangular relationship in camera triplets. This formulation does not suffer from the typical ‘unbalanced scale ’ problem in linear methods relying on pairwise translation direction constraints, i.e. an algebraic error; nor the system degeneracy from collinear motion. In the case of three cameras, our method provides a good linear approximation of the trifocal tensor. It can be directly scaled up to register multiple cameras. The results obtained are accurate for point triangulation and can serve as a good initialization for final bundle adjustment. We evaluate the algorithm performance with different types of data and demonstrate its effectiveness. Our system produces good accuracy, robustness, and outperforms some well-known systems on efficiency.</p><p>6 0.81838322 <a title="188-lda-6" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>7 0.81290966 <a title="188-lda-7" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>8 0.81101322 <a title="188-lda-8" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>9 0.8108052 <a title="188-lda-9" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>10 0.81072503 <a title="188-lda-10" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>11 0.81047863 <a title="188-lda-11" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>12 0.81018865 <a title="188-lda-12" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>13 0.81009626 <a title="188-lda-13" href="./iccv-2013-Bayesian_Joint_Topic_Modelling_for_Weakly_Supervised_Object_Localisation.html">59 iccv-2013-Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation</a></p>
<p>14 0.810094 <a title="188-lda-14" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>15 0.80861849 <a title="188-lda-15" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>16 0.8058539 <a title="188-lda-16" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>17 0.80526876 <a title="188-lda-17" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>18 0.80523473 <a title="188-lda-18" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>19 0.80507743 <a title="188-lda-19" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>20 0.8047483 <a title="188-lda-20" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
