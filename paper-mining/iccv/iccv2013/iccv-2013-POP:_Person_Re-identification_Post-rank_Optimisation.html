<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>305 iccv-2013-POP: Person Re-identification Post-rank Optimisation</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-305" href="#">iccv2013-305</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>305 iccv-2013-POP: Person Re-identification Post-rank Optimisation</h1>
<br/><p>Source: <a title="iccv-2013-305-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Liu_POP_Person_Re-identification_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Chunxiao Liu, Chen Change Loy, Shaogang Gong, Guijin Wang</p><p>Abstract: Owing to visual ambiguities and disparities, person reidentification methods inevitably produce suboptimal ranklist, which still requires exhaustive human eyeballing to identify the correct target from hundreds of different likelycandidates. Existing re-identification studies focus on improving the ranking performance, but rarely look into the critical problem of optimising the time-consuming and error-prone post-rank visual search at the user end. In this study, we present a novel one-shot Post-rank OPtimisation (POP) method, which allows a user to quickly refine their search by either “one-shot” or a couple of sparse negative selections during a re-identification process. We conduct systematic behavioural studies to understand user’s searching behaviour and show that the proposed method allows correct re-identification to converge 2.6 times faster than the conventional exhaustive search. Importantly, through extensive evaluations we demonstrate that the method is capable of achieving significant improvement over the stateof-the-art distance metric learning based ranking models, even with just “one shot” feedback optimisation, by as much as over 30% performance improvement for rank 1reidentification on the VIPeR and i-LIDS datasets.</p><p>Reference: <a title="iccv-2013-305-reference" href="../iccv2013_reference/iccv-2013-POP%3A_Person_Re-identification_Post-rank_Optimisation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn  Abstract Owing to visual ambiguities and disparities, person reidentification methods inevitably produce suboptimal ranklist, which still requires exhaustive human eyeballing to identify the correct target from hundreds of different likelycandidates. [sent-14, score-0.421]
</p><p>2 Existing re-identification studies focus on improving the ranking performance, but rarely look into the critical problem of optimising the time-consuming and error-prone post-rank visual search at the user end. [sent-15, score-0.471]
</p><p>3 In this study, we present a novel one-shot Post-rank OPtimisation (POP) method, which allows a user to quickly refine their search by either “one-shot” or a couple of sparse negative selections during a re-identification process. [sent-16, score-0.449]
</p><p>4 Introduction For person re-identification (re-id), a probe image serves as a query to be compared against a gallery that consists of images of different individuals captured at distributed locations at different time. [sent-21, score-0.801]
</p><p>5 There are two reasons for such considerations: Visual ambiguities and disparities - In the context of person re-identification, the visual samples are ambiguous, i. [sent-32, score-0.266]
</p><p>6 Off-line learning scalability - The performance of current distance learning based ranking approaches to person re-identification remain low [ 19, 26, 13, 17, 16, 25], e. [sent-38, score-0.271]
</p><p>7 s ≤et3 even wcoigthn person probe images manually aanr dV carefully cropped. [sent-42, score-0.417]
</p><p>8 Specifically, our method aims to minimise human-in-theloop effort by one-shot negative feedback selection. [sent-98, score-0.508]
</p><p>9 That is, a user only needs to select a single strong negative feedback, and optionally a few weak negatives, to trigger an automated refinement of the suboptimal rank list. [sent-99, score-0.631]
</p><p>10 A strong negative is a highly ranked, but confusing match in a machine generated suboptimal rank list with clear visual dissimilarity to the probe image, whilst a weak negative is a visually similar but wrong match in the same rank list (Fig. [sent-100, score-1.375]
</p><p>11 We formulate a new visual expansion model that not only synthesises pseudo-samples to complement the sparse negative selection, but also compute a generic mapping of visual change between different camera views. [sent-102, score-0.343]
</p><p>12 In addition, we introduce an incremental affinity graph construction for propagating sparse belief accumulated from human-in-theloop negative mining. [sent-103, score-0.368]
</p><p>13 In essence, the proposed model combines sparse human negative feedback on-the-fly to steer automatic selection of more relevant re-identification features. [sent-104, score-0.547]
</p><p>14 6 times of search efficiency compared to the typical exhaustive search strategy, but also brings about as much as over 30% performance improvement for rank 1re-identification over current distance metric learning and ranking models. [sent-107, score-0.43]
</p><p>15 This is based on “one shot” user negative selection only, and evaluated extensively using both the VIPeR and i-LIDS benchmark datasets. [sent-108, score-0.362]
</p><p>16 Related Work Post-rank optimisation for re-id is relatively unexplored in the person re-identification literature. [sent-110, score-0.286]
</p><p>17 One related study in [12] attempted to refine the rank list but their study does not model the process of enabling human-in-the-loop for optimising the suboptimal rank list with only sparse feedback, down to one-shot. [sent-111, score-0.454]
</p><p>18 Another related work [18] requires explicit relative feedback in image classifier training to diffuse the label to unlabelled images. [sent-115, score-0.4]
</p><p>19 face recognition in multimedia domain with feedback for query expansion in continuously tracked faces, a significantly more constrained problem when compared to person re-identification by a single image (see Fig. [sent-119, score-0.543]
</p><p>20 Our negative mining concept is related to human relevance feedback mining in generic image search and retrieval. [sent-123, score-0.668]
</p><p>21 They are: (1) top-ranked positive images are visually consistent to the probe (no visual ambiguities) [24, 10], (2) those positive images often form the largest cluster [28], or (3) sufficient positive samples can be gathered through text keyword expansion [21]. [sent-128, score-0.566]
</p><p>22 A true positive person re-id match does not necessarily forms a large cluster in the gallery set, in the contrary it is often sparse. [sent-130, score-0.641]
</p><p>23 Given a probe image to be matched against an unlabelled gallery set, a ranking function generates a suboptimal rank list of the gallery set according to each gallery image’s likelihood to be a true match of the probe image. [sent-193, score-2.218]
</p><p>24 All other samples in the gallery space are considered as negatives, which can be divided into two negative types (Fig. [sent-195, score-0.599]
</p><p>25 2): (1) Strong negatives - highly ranked gallery images that are visually clearly dissimilar to the probe image. [sent-196, score-0.992]
</p><p>26 (2) Weak negatives - albeit not the true match, these highly ranked negative gallery images  are visually similar to the probe image. [sent-198, score-1.214]
</p><p>27 They could be good candidates for disambiguating visual uncertainties and optimising the initial ranking function. [sent-199, score-0.311]
</p><p>28 We wish to formulate a model to best exploit human-in-the-loop feedback for postrank optimisation. [sent-200, score-0.395]
</p><p>29 Given a probe instance, xp, we assume an initial ranking function finit is available (e. [sent-202, score-0.465]
</p><p>30 true in the top N ranked candidates, we wish to learn a post-rank function fpr for rank re-ordering. [sent-216, score-0.259]
</p><p>31 3: (a) A user selects one (any) strong negative from the top N ranked instances, denoted as xs− 1 . [sent-218, score-0.477]
</p><p>32 than weak negatives (visually subtle) in an on-the-fly feedback process. [sent-238, score-0.631]
</p><p>33 We also show in comparative experiments in Section 6 that any performance advantage gained from additional multiple negative feedback over a single one-shot (b) For learning the post-rank function, we also require positive sample(s) in addition to the user selected negative sample. [sent-239, score-0.894]
</p><p>34 To that end, visual expansion is computed  to synthesise one or more instances of the probe image ( x˜p) in the gallery view (Sec. [sent-240, score-0.895]
</p><p>35 (c) An affinity graph weighted by an affinity matrix A¯ is constructed to capture the appearance similarities among all the images in the gallery view, including both the original gallery instances and the synthesised probe instances (Sec. [sent-243, score-1.702]
</p><p>36 (d) This sparse negative information obtained from the user is propagated to their nearby neighbours in the gallery view via the above weighted affinity graph (Sec. [sent-246, score-0.887]
</p><p>37 Cross-Camera View Visual Expansion Learning a post-rank function for rank re-ordering requires both labelled negative and positive data. [sent-254, score-0.336]
</p><p>38 Clearly, a single strong negative selected by user is insufficient for this purpose. [sent-255, score-0.439]
</p><p>39 Moreover, owing to potentially large feature inconsistency between different camera views, the probe image itself from the probe camera view  cannot be readily used as a positive sample in the gallery view. [sent-260, score-1.019]
</p><p>40 To resolve this problem, we specifically design a regression forest [4] based visual expansion method. [sent-261, score-0.28]
</p><p>41 Moreover, the nature of it being an ensemble oftrees allows efficient random permutation in the predictors to synthesise one or more samples that resemble the probe’s appearance as pseudo positive-labelled data in the gallery view. [sent-263, score-0.493]
</p><p>42 Specifically, the visual variations between a probe and a gallery camera view are accounted by the multi-output regression forest, with Tr trees, through learning an appearance mapping space  xp  xg  M: → ∈ Rd, (1) from a set of paired training instances extracted from crosscamera views (Fig. [sent-264, score-0.891]
</p><p>43 A synthesised probe instance can then be generated as follows negative feedback is insignificant  as a  result of post-rank optimisation. [sent-266, score-0.991]
</p><p>44 This} pro-  cess can ybe s repeated ntod generate more synthesised probe iron-stances if desired. [sent-278, score-0.483]
</p><p>45 To that end, we shall describe how to propagate the sparse labelled samples to the large quantity of unlabelled gallery set so to avoid the need for labelling exhaustively the gallery set. [sent-284, score-0.936]
</p><p>46 This process of transduction via an affinity graph is facilitated by first constructing an affinity graph of the unlabelled gallery set. [sent-285, score-0.824]
</p><p>47 (3)  We then collect the pairwise distances of all gallery instances to construct an affinity matrix At ∈ Rn×n of that tree, with each element Aitj given as Aitj =  exp−distt(xig,xjg)  . [sent-299, score-0.587]
</p><p>48 (4)  Intuitively, we assign affinity=1 (distance=0) to samples xig and xjg if they fall into the same leaf node, and affin2This fraction is typical in random forest bootstrap training [4]. [sent-300, score-0.312]
</p><p>49 now consider the case for including synthesised positives in the construction of the affinity graph. [sent-310, score-0.361]
</p><p>50 Recall that our method is designed to need only a single strong negative to re-order the rank. [sent-311, score-0.275]
</p><p>51 Nevertheless, a user has the  option to select more negatives in more than one round of feedback, if necessary and desired. [sent-312, score-0.429]
</p><p>52 To maintain a balance in positive-negative data for the post-rank function learning, the model needs to generate equal number of synthesised positive probe instances { x˜p} as pseudo positive-labelled dpaostai i vne eth per gallery avnicewe. [sent-313, score-1.013]
</p><p>53 s T{ ˜xhu}s, athse p nseuumdboer p oosfi txi ˜vpe can vary depending on the number of negatives selected by a user cumulatively. [sent-314, score-0.39]
</p><p>54 A more tractable approach is to first build a graph using the gallery data alone without the additional synthesised positives, and then expand it to accommodate the additional synthesised probe instances, as follows. [sent-316, score-1.141]
</p><p>55 First, we compute the affinity between { x˜p} and all the existing gallery ipnusttean thcees a {ffixngit}y. [sent-317, score-0.519]
</p><p>56 In particular, since the index of each gallery instances is stored in the leaf nod? [sent-320, score-0.483]
</p><p>57 Sparse Negative Propagation over Graph After constructing the affinity graph, we diffuse the sparse negative and synthesised positive information over the graph to all other gallery instances. [sent-354, score-0.986]
</p><p>58 First, we order the selected negatives and synthesised probe instances into the first llabelled samples L, followed by the remaining u gallery tiln s latabneclelesd as aumnplalebsel lLed, f samples b Uy, i t. [sent-355, score-1.244]
</p><p>59 Effects of negative accumulation: (a) three-dimensional embedding of gallery images obtained using multi-dimensional  scaling after the first round of negative selection, (b) the embedding after the second round. [sent-458, score-0.828]
</p><p>60 The gallery images are colour coded according to their new ranking score. [sent-459, score-0.561]
</p><p>61 The shrinking region of bright yellow colour indicates the effectiveness of negative mining in demoting initial false matches. [sent-460, score-0.358]
</p><p>62 2I, which enforces thec osimntirolalrs/d thisesi imntilrainr liacb erlesg uolfa nearby gallery instances with respect to the affinity graph to be close. [sent-476, score-0.632]
</p><p>63 Finally, the estimated relevance of an unlabelled gallery instance xjg to the probe is computed as  αl+u)T  sjpr = fpr(xjg)  =? [sent-495, score-0.86]
</p><p>64 The parameter β balances the influence between initial ranking and user feedback selections. [sent-502, score-0.637]
</p><p>65 Negative Accumulation After each round of negative mining, we add new negative selections to a cumulated strong negative sets collected from previous rounds (or also weak negative sets if weak negatives were selected). [sent-505, score-1.447]
</p><p>66 Figure 4 shows an example for the effect of feedback accumulation in two rounds of negative mining. [sent-506, score-0.636]
</p><p>67 As more negatives are accumulated, the classification boundary is refined, increasing the separation between the true match and other strong negatives. [sent-507, score-0.402]
</p><p>68 The above negative accumulation are repeated together with the negative mining steps (Sec. [sent-508, score-0.467]
</p><p>69 In the test set of each trial, we randomly chose one image from each person to set up the test gallery set and the remaining images were used as probe images. [sent-536, score-0.801]
</p><p>70 Note that for the i-LIDS dataset, 50 images in the gallery set were insufficient to construct the intrinsic regulariser ? [sent-537, score-0.417]
</p><p>71 They were asked to manually annotate the weak and strong negatives ranked by an off-line ranking model given a set of random probe images. [sent-548, score-0.91]
</p><p>72 It is evident from Table 1 that the proportion of weak and strong negatives are extremely imbalanced with the strong negatives outnumbers the weak  negatives significantly. [sent-549, score-1.022]
</p><p>73 Overall, these results suggest that the relatively more salient strong negatives are more likely to be selected by a user during a post-rank feedback selection process. [sent-557, score-0.836]
</p><p>74 This raises the question on how the POP model performs given a single strong negative feedback (i. [sent-558, score-0.595]
</p><p>75 oneshot) as compared to its performance given multiple weak negatives as feedback. [sent-560, score-0.311]
</p><p>76 1-norm, and were asked to perform one-shot strong negative selection from the top 15 ranked results. [sent-565, score-0.406]
</p><p>77 They were allocated a maximum of 3 rank feedback rounds with one strong negative selection each. [sent-566, score-0.799]
</p><p>78 If the true match cannot be promoted into the top 15 ranks by the model after the maximal 3 rounds of one-shot postrank optimisation, the users were asked to continue with an exhaustive visual search to find the true match. [sent-567, score-0.511]
</p><p>79 Figure 5 depicts several examples of actual user interactions during the post-rank optimisation process. [sent-570, score-0.287]
</p><p>80 5(b), when a user selected the first candidate as strong negative, both the first and second candidates who were wearing brown jackets were removed from the top ranks. [sent-573, score-0.28]
</p><p>81 5(c) shows a failure case where selecting one strong negative is insufficient to resolve the visual ambiguity, since the true match experiences large appearance variation due to viewpoint change. [sent-575, score-0.422]
</p><p>82 The probe and the true match are highlighted respectively with red and green bounding boxes. [sent-587, score-0.372]
</p><p>83 The selected strong negative is denoted by a red cross. [sent-589, score-0.304]
</p><p>84 1-norm, RankSVM, PRDC, MCC First we evaluate the benefits of POP on existing ranking based person re-identification methods using ? [sent-607, score-0.271]
</p><p>85 In each round, the negative selection was performed on the first N ranked images, N = 15 for the VIPeR dataset and N = 10 for the i-LIDS dataset due to its relatively smaller size. [sent-609, score-0.294]
</p><p>86 We treat the negative selections collected offline from the first behaviour study (Sec. [sent-610, score-0.305]
</p><p>87 Despite the negative selection was performed without a live user in the loop, the experiments were still using the real feedback from users. [sent-613, score-0.682]
</p><p>88 the number of feedback round on VIPeR and i-LIDS. [sent-633, score-0.388]
</p><p>89 The one-shot experiment depicted an extremely sparse feedback scenario, where only one strong negative within the top N ranked images was selected in a round. [sent-636, score-0.691]
</p><p>90 The maximum number of strong negatives was set to 5 assuming that the users do not bother to annotate more. [sent-638, score-0.313]
</p><p>91 With feedback increased to three rounds, the performance improves monotonically and converges. [sent-644, score-0.32]
</p><p>92 The one-shot negative selection in just one feedback round yields stable and competitive results with no obvious degradation in comparison to the multi-shot multi-rounds feedback, indicating the effectiveness of one-shot post-rank optimisation. [sent-649, score-0.615]
</p><p>93 It uses Euclidean distance to construct the affinity matrix and optimises a ranking function with least square regression. [sent-657, score-0.272]
</p><p>94 The yaxis shows the recognition rate at rank-5 along with the increment of feedback round. [sent-672, score-0.32]
</p><p>95 In addition, we implemented two baseline approaches: (1) a na¨ ıve feedback method which simply demotes the strong negatives to the bottom of the ranking list in each round; (2) a SVM approach using the strong negatives and synthesized positive examples for training. [sent-674, score-1.175]
</p><p>96 The y-axis shows the recognition rate at Rank-5 along with the increment of feedback round. [sent-691, score-0.32]
</p><p>97 NPRF, PRF and the na¨ ıve feedback are generally poor in boosting the recognition rate on VIPeR dataset, suggesting that the use of top-ranked images as positive feedback samples can lead to erroneous post-rank results in a re-identification task. [sent-693, score-0.701]
</p><p>98 The better performance of POP over EMR suggests the more effective propagation of negatives over the clustering-forest based affinity graph, rather than the Euclidean-based graph. [sent-700, score-0.361]
</p><p>99 To prepare the baseline without visual expansion, we randomly selected one weak negative image from the top N ranks (N = 15 for VIPeR, 10 for i-LIDS) to pair with the one-shot strong negative. [sent-704, score-0.447]
</p><p>100 Pseudo relevance feedback based on iterative probabilistic one-class SVMs in web image retrieval. [sent-777, score-0.32]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gallery', 0.384), ('feedback', 0.32), ('probe', 0.283), ('pop', 0.253), ('negatives', 0.226), ('viper', 0.221), ('synthesised', 0.2), ('negative', 0.188), ('optimisation', 0.152), ('ranking', 0.137), ('user', 0.135), ('affinity', 0.135), ('person', 0.134), ('emr', 0.117), ('xjg', 0.113), ('forest', 0.095), ('expansion', 0.089), ('strong', 0.087), ('reidentification', 0.087), ('weak', 0.085), ('rounds', 0.084), ('rank', 0.081), ('exhaustive', 0.08), ('unlabelled', 0.08), ('fpr', 0.077), ('mcc', 0.075), ('nprf', 0.075), ('postrank', 0.075), ('prf', 0.075), ('instances', 0.068), ('round', 0.068), ('optimising', 0.067), ('distt', 0.067), ('prdc', 0.067), ('ranked', 0.067), ('search', 0.066), ('selections', 0.06), ('ranksvm', 0.058), ('list', 0.058), ('rroanukn', 0.056), ('sinit', 0.056), ('suboptimal', 0.055), ('match', 0.055), ('xg', 0.052), ('systematic', 0.049), ('mining', 0.047), ('xig', 0.046), ('initial', 0.045), ('graph', 0.045), ('accumulation', 0.044), ('pseudo', 0.044), ('xw', 0.043), ('xs', 0.042), ('colour', 0.04), ('disparities', 0.04), ('selection', 0.039), ('regression', 0.038), ('aitj', 0.038), ('demoting', 0.038), ('ekenel', 0.038), ('irniantiakl', 0.038), ('spr', 0.038), ('synthesise', 0.038), ('whilst', 0.036), ('loy', 0.035), ('owing', 0.035), ('positive', 0.034), ('true', 0.034), ('invited', 0.033), ('behavioural', 0.033), ('regulariser', 0.033), ('visual', 0.033), ('jp', 0.033), ('studies', 0.033), ('xp', 0.033), ('labelled', 0.033), ('ambiguities', 0.032), ('visually', 0.032), ('leaf', 0.031), ('judgement', 0.031), ('fischer', 0.031), ('behaviour', 0.03), ('candidates', 0.029), ('inghua', 0.029), ('ali', 0.029), ('accommodate', 0.029), ('selected', 0.029), ('shall', 0.028), ('hirzer', 0.028), ('belkin', 0.028), ('interactive', 0.027), ('samples', 0.027), ('study', 0.027), ('positives', 0.026), ('mm', 0.026), ('ranks', 0.025), ('asked', 0.025), ('tre', 0.025), ('resolve', 0.025), ('shot', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="305-tfidf-1" href="./iccv-2013-POP%3A_Person_Re-identification_Post-rank_Optimisation.html">305 iccv-2013-POP: Person Re-identification Post-rank Optimisation</a></p>
<p>Author: Chunxiao Liu, Chen Change Loy, Shaogang Gong, Guijin Wang</p><p>Abstract: Owing to visual ambiguities and disparities, person reidentification methods inevitably produce suboptimal ranklist, which still requires exhaustive human eyeballing to identify the correct target from hundreds of different likelycandidates. Existing re-identification studies focus on improving the ranking performance, but rarely look into the critical problem of optimising the time-consuming and error-prone post-rank visual search at the user end. In this study, we present a novel one-shot Post-rank OPtimisation (POP) method, which allows a user to quickly refine their search by either “one-shot” or a couple of sparse negative selections during a re-identification process. We conduct systematic behavioural studies to understand user’s searching behaviour and show that the proposed method allows correct re-identification to converge 2.6 times faster than the conventional exhaustive search. Importantly, through extensive evaluations we demonstrate that the method is capable of achieving significant improvement over the stateof-the-art distance metric learning based ranking models, even with just “one shot” feedback optimisation, by as much as over 30% performance improvement for rank 1reidentification on the VIPeR and i-LIDS datasets.</p><p>2 0.32754719 <a title="305-tfidf-2" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>Author: Renliang Weng, Jiwen Lu, Junlin Hu, Gao Yang, Yap-Peng Tan</p><p>Abstract: Over the past two decades, a number of face recognition methods have been proposed in the literature. Most of them use holistic face images to recognize people. However, human faces are easily occluded by other objects in many real-world scenarios and we have to recognize the person of interest from his/her partial faces. In this paper, we propose a new partial face recognition approach by using feature set matching, which is able to align partial face patches to holistic gallery faces automatically and is robust to occlusions and illumination changes. Given each gallery image and probe face patch, we first detect keypoints and extract their local features. Then, we propose a Metric Learned ExtendedRobust PointMatching (MLERPM) method to discriminatively match local feature sets of a pair of gallery and probe samples. Lastly, the similarity of two faces is converted as the distance between two feature sets. Experimental results on three public face databases are presented to show the effectiveness of the proposed approach.</p><p>3 0.31920165 <a title="305-tfidf-3" href="./iccv-2013-Implied_Feedback%3A_Learning_Nuances_of_User_Behavior_in_Image_Search.html">213 iccv-2013-Implied Feedback: Learning Nuances of User Behavior in Image Search</a></p>
<p>Author: Devi Parikh, Kristen Grauman</p><p>Abstract: User feedback helps an image search system refine its relevance predictions, tailoring the search towards the user’s preferences. Existing methods simply take feedback at face value: clicking on an image means the user wants things like it; commenting that an image lacks a specific attribute means the user wants things that have it. However, we expect there is actually more information behind the user’s literal feedback. In particular, a user’s (possibly subconscious) search strategy leads him to comment on certain images rather than others, based on how any of the visible candidate images compare to the desired content. For example, he may be more likely to give negative feedback on an irrelevant image that is relatively close to his target, as opposed to bothering with one that is altogether different. We introduce novel features to capitalize on such implied feedback cues, and learn a ranking function that uses them to improve the system’s relevance estimates. We validate the approach with real users searching for shoes, faces, or scenes using two different modes of feedback: binary relevance feedback and relative attributes-based feedback. The results show that retrieval improves significantly when the system accounts for the learned behaviors. We show that the nuances learned are domain-invariant, and useful for both generic user-independent search as well as personalized user-specific search.</p><p>4 0.25201881 <a title="305-tfidf-4" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>Author: Adriana Kovashka, Kristen Grauman</p><p>Abstract: In interactive image search, a user iteratively refines his results by giving feedback on exemplar images. Active selection methods aim to elicit useful feedback, but traditional approaches suffer from expensive selection criteria and cannot predict informativeness reliably due to the imprecision of relevance feedback. To address these drawbacks, we propose to actively select “pivot” exemplars for which feedback in the form of a visual comparison will most reduce the system’s uncertainty. For example, the system might ask, “Is your target image more or less crowded than this image? ” Our approach relies on a series of binary search trees in relative attribute space, together with a selection function that predicts the information gain were the user to compare his envisioned target to the next node deeper in a given attribute ’s tree. It makes interactive search more efficient than existing strategies—both in terms of the system ’s selection time as well as the user’s feedback effort.</p><p>5 0.20150399 <a title="305-tfidf-5" href="./iccv-2013-Model_Recommendation_with_Virtual_Probes_for_Egocentric_Hand_Detection.html">267 iccv-2013-Model Recommendation with Virtual Probes for Egocentric Hand Detection</a></p>
<p>Author: Cheng Li, Kris M. Kitani</p><p>Abstract: Egocentric cameras can be used to benefit such tasks as analyzing fine motor skills, recognizing gestures and learning about hand-object manipulation. To enable such technology, we believe that the hands must detected on thepixellevel to gain important information about the shape of the hands and fingers. We show that the problem of pixel-wise hand detection can be effectively solved, by posing the problem as a model recommendation task. As such, the goal of a recommendation system is to recommend the n-best hand detectors based on the probe set a small amount of labeled data from the test distribution. This requirement of a probe set is a serious limitation in many applications, such as ego-centric hand detection, where the test distribution may be continually changing. To address this limitation, we propose the use of virtual probes which can be automatically extracted from the test distribution. The key idea is – that many features, such as the color distribution or relative performance between two detectors, can be used as a proxy to the probe set. In our experiments we show that the recommendation paradigm is well-equipped to handle complex changes in the appearance of the hands in firstperson vision. In particular, we show how our system is able to generalize to new scenarios by testing our model across multiple users.</p><p>6 0.20022395 <a title="305-tfidf-6" href="./iccv-2013-Coupling_Alignments_with_Recognition_for_Still-to-Video_Face_Recognition.html">97 iccv-2013-Coupling Alignments with Recognition for Still-to-Video Face Recognition</a></p>
<p>7 0.16935372 <a title="305-tfidf-7" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>8 0.15620223 <a title="305-tfidf-8" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>9 0.15270856 <a title="305-tfidf-9" href="./iccv-2013-Markov_Network-Based_Unified_Classifier_for_Face_Identification.html">261 iccv-2013-Markov Network-Based Unified Classifier for Face Identification</a></p>
<p>10 0.12902546 <a title="305-tfidf-10" href="./iccv-2013-Person_Re-identification_by_Salience_Matching.html">313 iccv-2013-Person Re-identification by Salience Matching</a></p>
<p>11 0.11448504 <a title="305-tfidf-11" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>12 0.10313972 <a title="305-tfidf-12" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>13 0.098623775 <a title="305-tfidf-13" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<p>14 0.089833632 <a title="305-tfidf-14" href="./iccv-2013-Human_Re-identification_by_Matching_Compositional_Template_with_Cluster_Sampling.html">205 iccv-2013-Human Re-identification by Matching Compositional Template with Cluster Sampling</a></p>
<p>15 0.083245724 <a title="305-tfidf-15" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>16 0.080731593 <a title="305-tfidf-16" href="./iccv-2013-Unsupervised_Random_Forest_Manifold_Alignment_for_Lipreading.html">437 iccv-2013-Unsupervised Random Forest Manifold Alignment for Lipreading</a></p>
<p>17 0.076291822 <a title="305-tfidf-17" href="./iccv-2013-Beyond_Hard_Negative_Mining%3A_Efficient_Detector_Learning_via_Block-Circulant_Decomposition.html">61 iccv-2013-Beyond Hard Negative Mining: Efficient Detector Learning via Block-Circulant Decomposition</a></p>
<p>18 0.075285651 <a title="305-tfidf-18" href="./iccv-2013-Random_Forests_of_Local_Experts_for_Pedestrian_Detection.html">336 iccv-2013-Random Forests of Local Experts for Pedestrian Detection</a></p>
<p>19 0.074328333 <a title="305-tfidf-19" href="./iccv-2013-From_Semi-supervised_to_Transfer_Counting_of_Crowds.html">178 iccv-2013-From Semi-supervised to Transfer Counting of Crowds</a></p>
<p>20 0.073573858 <a title="305-tfidf-20" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.176), (1, 0.053), (2, -0.064), (3, -0.112), (4, 0.042), (5, -0.008), (6, 0.05), (7, 0.029), (8, 0.063), (9, 0.081), (10, -0.027), (11, 0.01), (12, 0.01), (13, -0.011), (14, 0.012), (15, -0.022), (16, -0.033), (17, -0.043), (18, 0.006), (19, 0.04), (20, -0.062), (21, -0.177), (22, -0.118), (23, -0.076), (24, 0.151), (25, 0.2), (26, -0.18), (27, 0.244), (28, -0.028), (29, -0.226), (30, -0.015), (31, -0.143), (32, -0.116), (33, 0.101), (34, 0.134), (35, 0.087), (36, -0.115), (37, 0.093), (38, 0.079), (39, 0.025), (40, 0.011), (41, -0.036), (42, 0.044), (43, -0.195), (44, -0.054), (45, -0.073), (46, -0.029), (47, -0.024), (48, 0.069), (49, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94700718 <a title="305-lsi-1" href="./iccv-2013-POP%3A_Person_Re-identification_Post-rank_Optimisation.html">305 iccv-2013-POP: Person Re-identification Post-rank Optimisation</a></p>
<p>Author: Chunxiao Liu, Chen Change Loy, Shaogang Gong, Guijin Wang</p><p>Abstract: Owing to visual ambiguities and disparities, person reidentification methods inevitably produce suboptimal ranklist, which still requires exhaustive human eyeballing to identify the correct target from hundreds of different likelycandidates. Existing re-identification studies focus on improving the ranking performance, but rarely look into the critical problem of optimising the time-consuming and error-prone post-rank visual search at the user end. In this study, we present a novel one-shot Post-rank OPtimisation (POP) method, which allows a user to quickly refine their search by either “one-shot” or a couple of sparse negative selections during a re-identification process. We conduct systematic behavioural studies to understand user’s searching behaviour and show that the proposed method allows correct re-identification to converge 2.6 times faster than the conventional exhaustive search. Importantly, through extensive evaluations we demonstrate that the method is capable of achieving significant improvement over the stateof-the-art distance metric learning based ranking models, even with just “one shot” feedback optimisation, by as much as over 30% performance improvement for rank 1reidentification on the VIPeR and i-LIDS datasets.</p><p>2 0.69806874 <a title="305-lsi-2" href="./iccv-2013-Implied_Feedback%3A_Learning_Nuances_of_User_Behavior_in_Image_Search.html">213 iccv-2013-Implied Feedback: Learning Nuances of User Behavior in Image Search</a></p>
<p>Author: Devi Parikh, Kristen Grauman</p><p>Abstract: User feedback helps an image search system refine its relevance predictions, tailoring the search towards the user’s preferences. Existing methods simply take feedback at face value: clicking on an image means the user wants things like it; commenting that an image lacks a specific attribute means the user wants things that have it. However, we expect there is actually more information behind the user’s literal feedback. In particular, a user’s (possibly subconscious) search strategy leads him to comment on certain images rather than others, based on how any of the visible candidate images compare to the desired content. For example, he may be more likely to give negative feedback on an irrelevant image that is relatively close to his target, as opposed to bothering with one that is altogether different. We introduce novel features to capitalize on such implied feedback cues, and learn a ranking function that uses them to improve the system’s relevance estimates. We validate the approach with real users searching for shoes, faces, or scenes using two different modes of feedback: binary relevance feedback and relative attributes-based feedback. The results show that retrieval improves significantly when the system accounts for the learned behaviors. We show that the nuances learned are domain-invariant, and useful for both generic user-independent search as well as personalized user-specific search.</p><p>3 0.64604914 <a title="305-lsi-3" href="./iccv-2013-Model_Recommendation_with_Virtual_Probes_for_Egocentric_Hand_Detection.html">267 iccv-2013-Model Recommendation with Virtual Probes for Egocentric Hand Detection</a></p>
<p>Author: Cheng Li, Kris M. Kitani</p><p>Abstract: Egocentric cameras can be used to benefit such tasks as analyzing fine motor skills, recognizing gestures and learning about hand-object manipulation. To enable such technology, we believe that the hands must detected on thepixellevel to gain important information about the shape of the hands and fingers. We show that the problem of pixel-wise hand detection can be effectively solved, by posing the problem as a model recommendation task. As such, the goal of a recommendation system is to recommend the n-best hand detectors based on the probe set a small amount of labeled data from the test distribution. This requirement of a probe set is a serious limitation in many applications, such as ego-centric hand detection, where the test distribution may be continually changing. To address this limitation, we propose the use of virtual probes which can be automatically extracted from the test distribution. The key idea is – that many features, such as the color distribution or relative performance between two detectors, can be used as a proxy to the probe set. In our experiments we show that the recommendation paradigm is well-equipped to handle complex changes in the appearance of the hands in firstperson vision. In particular, we show how our system is able to generalize to new scenarios by testing our model across multiple users.</p><p>4 0.55007029 <a title="305-lsi-4" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>Author: Adriana Kovashka, Kristen Grauman</p><p>Abstract: In interactive image search, a user iteratively refines his results by giving feedback on exemplar images. Active selection methods aim to elicit useful feedback, but traditional approaches suffer from expensive selection criteria and cannot predict informativeness reliably due to the imprecision of relevance feedback. To address these drawbacks, we propose to actively select “pivot” exemplars for which feedback in the form of a visual comparison will most reduce the system’s uncertainty. For example, the system might ask, “Is your target image more or less crowded than this image? ” Our approach relies on a series of binary search trees in relative attribute space, together with a selection function that predicts the information gain were the user to compare his envisioned target to the next node deeper in a given attribute ’s tree. It makes interactive search more efficient than existing strategies—both in terms of the system ’s selection time as well as the user’s feedback effort.</p><p>5 0.53904492 <a title="305-lsi-5" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>Author: Renliang Weng, Jiwen Lu, Junlin Hu, Gao Yang, Yap-Peng Tan</p><p>Abstract: Over the past two decades, a number of face recognition methods have been proposed in the literature. Most of them use holistic face images to recognize people. However, human faces are easily occluded by other objects in many real-world scenarios and we have to recognize the person of interest from his/her partial faces. In this paper, we propose a new partial face recognition approach by using feature set matching, which is able to align partial face patches to holistic gallery faces automatically and is robust to occlusions and illumination changes. Given each gallery image and probe face patch, we first detect keypoints and extract their local features. Then, we propose a Metric Learned ExtendedRobust PointMatching (MLERPM) method to discriminatively match local feature sets of a pair of gallery and probe samples. Lastly, the similarity of two faces is converted as the distance between two feature sets. Experimental results on three public face databases are presented to show the effectiveness of the proposed approach.</p><p>6 0.49386752 <a title="305-lsi-6" href="./iccv-2013-Coupling_Alignments_with_Recognition_for_Still-to-Video_Face_Recognition.html">97 iccv-2013-Coupling Alignments with Recognition for Still-to-Video Face Recognition</a></p>
<p>7 0.47836024 <a title="305-lsi-7" href="./iccv-2013-Person_Re-identification_by_Salience_Matching.html">313 iccv-2013-Person Re-identification by Salience Matching</a></p>
<p>8 0.46462631 <a title="305-lsi-8" href="./iccv-2013-Markov_Network-Based_Unified_Classifier_for_Face_Identification.html">261 iccv-2013-Markov Network-Based Unified Classifier for Face Identification</a></p>
<p>9 0.40539694 <a title="305-lsi-9" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>10 0.36054602 <a title="305-lsi-10" href="./iccv-2013-From_Semi-supervised_to_Transfer_Counting_of_Crowds.html">178 iccv-2013-From Semi-supervised to Transfer Counting of Crowds</a></p>
<p>11 0.32805046 <a title="305-lsi-11" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>12 0.32655498 <a title="305-lsi-12" href="./iccv-2013-Unsupervised_Random_Forest_Manifold_Alignment_for_Lipreading.html">437 iccv-2013-Unsupervised Random Forest Manifold Alignment for Lipreading</a></p>
<p>13 0.32458103 <a title="305-lsi-13" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>14 0.32038173 <a title="305-lsi-14" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<p>15 0.31478092 <a title="305-lsi-15" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>16 0.30694425 <a title="305-lsi-16" href="./iccv-2013-Face_Recognition_via_Archetype_Hull_Ranking.html">154 iccv-2013-Face Recognition via Archetype Hull Ranking</a></p>
<p>17 0.29256278 <a title="305-lsi-17" href="./iccv-2013-Recognising_Human-Object_Interaction_via_Exemplar_Based_Modelling.html">344 iccv-2013-Recognising Human-Object Interaction via Exemplar Based Modelling</a></p>
<p>18 0.29161769 <a title="305-lsi-18" href="./iccv-2013-Beyond_Hard_Negative_Mining%3A_Efficient_Detector_Learning_via_Block-Circulant_Decomposition.html">61 iccv-2013-Beyond Hard Negative Mining: Efficient Detector Learning via Block-Circulant Decomposition</a></p>
<p>19 0.28982013 <a title="305-lsi-19" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>20 0.28934348 <a title="305-lsi-20" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.093), (7, 0.031), (12, 0.345), (26, 0.072), (31, 0.029), (40, 0.014), (42, 0.118), (48, 0.012), (64, 0.035), (73, 0.021), (89, 0.11), (98, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85288203 <a title="305-lda-1" href="./iccv-2013-Target-Driven_Moire_Pattern_Synthesis_by_Phase_Modulation.html">413 iccv-2013-Target-Driven Moire Pattern Synthesis by Phase Modulation</a></p>
<p>Author: Pei-Hen Tsai, Yung-Yu Chuang</p><p>Abstract: This paper investigates an approach for generating two grating images so that the moir e´ pattern of their superposition resembles the target image. Our method is grounded on the fundamental moir e´ theorem. By focusing on the visually most dominant (1, −1)-moir e´ component, we obtain the phase smto ddoumlaintiaonnt c (o1n,s−tr1a)in-mt on the phase shifts bee otwbteaeinn the two grating images. For improving visual appearance of the grating images and hiding capability the embedded image, a smoothness term is added to spread information between the two grating images and an appearance phase function is used to add irregular structures into grating images. The grating images can be printed on transparencies and the hidden image decoding can be performed optically by overlaying them together. The proposed method enables the creation of moir e´ art and allows visual decoding without computers.</p><p>same-paper 2 0.82309395 <a title="305-lda-2" href="./iccv-2013-POP%3A_Person_Re-identification_Post-rank_Optimisation.html">305 iccv-2013-POP: Person Re-identification Post-rank Optimisation</a></p>
<p>Author: Chunxiao Liu, Chen Change Loy, Shaogang Gong, Guijin Wang</p><p>Abstract: Owing to visual ambiguities and disparities, person reidentification methods inevitably produce suboptimal ranklist, which still requires exhaustive human eyeballing to identify the correct target from hundreds of different likelycandidates. Existing re-identification studies focus on improving the ranking performance, but rarely look into the critical problem of optimising the time-consuming and error-prone post-rank visual search at the user end. In this study, we present a novel one-shot Post-rank OPtimisation (POP) method, which allows a user to quickly refine their search by either “one-shot” or a couple of sparse negative selections during a re-identification process. We conduct systematic behavioural studies to understand user’s searching behaviour and show that the proposed method allows correct re-identification to converge 2.6 times faster than the conventional exhaustive search. Importantly, through extensive evaluations we demonstrate that the method is capable of achieving significant improvement over the stateof-the-art distance metric learning based ranking models, even with just “one shot” feedback optimisation, by as much as over 30% performance improvement for rank 1reidentification on the VIPeR and i-LIDS datasets.</p><p>3 0.7339775 <a title="305-lda-3" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>Author: Mohamed Elhoseiny, Babak Saleh, Ahmed Elgammal</p><p>Abstract: The main question we address in this paper is how to use purely textual description of categories with no training images to learn visual classifiers for these categories. We propose an approach for zero-shot learning of object categories where the description of unseen categories comes in the form of typical text such as an encyclopedia entry, without the need to explicitly defined attributes. We propose and investigate two baseline formulations, based on regression and domain adaptation. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the classifier parameters for new classes. We applied the proposed approach on two fine-grained categorization datasets, and the results indicate successful classifier prediction.</p><p>4 0.64810705 <a title="305-lda-4" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>Author: Michael Van_Den_Bergh, Gemma Roig, Xavier Boix, Santiago Manen, Luc Van_Gool</p><p>Abstract: Superpixel and objectness algorithms are broadly used as a pre-processing step to generate support regions and to speed-up further computations. Recently, many algorithms have been extended to video in order to exploit the temporal consistency between frames. However, most methods are computationally too expensive for real-time applications. We introduce an online, real-time video superpixel algorithm based on the recently proposed SEEDS superpixels. A new capability is incorporated which delivers multiple diverse samples (hypotheses) of superpixels in the same image or video sequence. The multiple samples are shown to provide a strong cue to efficiently measure the objectness of image windows, and we introduce the novel concept of objectness in temporal windows. Experiments show that the video superpixels achieve comparable performance to state-of-the-art offline methods while running at 30 fps on a single 2.8 GHz i7 CPU. State-of-the-art performance on objectness is also demonstrated, yet orders of magnitude faster and extended to temporal windows in video.</p><p>5 0.64547426 <a title="305-lda-5" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>Author: Mihai Zanfir, Marius Leordeanu, Cristian Sminchisescu</p><p>Abstract: Human action recognition under low observational latency is receiving a growing interest in computer vision due to rapidly developing technologies in human-robot interaction, computer gaming and surveillance. In this paper we propose a fast, simple, yet powerful non-parametric Moving Pose (MP)frameworkfor low-latency human action and activity recognition. Central to our methodology is a moving pose descriptor that considers both pose information as well as differential quantities (speed and acceleration) of the human body joints within a short time window around the current frame. The proposed descriptor is used in conjunction with a modified kNN classifier that considers both the temporal location of a particular frame within the action sequence as well as the discrimination power of its moving pose descriptor compared to other frames in the training set. The resulting method is non-parametric and enables low-latency recognition, one-shot learning, and action detection in difficult unsegmented sequences. Moreover, the framework is real-time, scalable, and outperforms more sophisticated approaches on challenging benchmarks like MSR-Action3D or MSR-DailyActivities3D.</p><p>6 0.62301654 <a title="305-lda-6" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>7 0.56891859 <a title="305-lda-7" href="./iccv-2013-Efficient_Pedestrian_Detection_by_Directly_Optimizing_the_Partial_Area_under_the_ROC_Curve.html">136 iccv-2013-Efficient Pedestrian Detection by Directly Optimizing the Partial Area under the ROC Curve</a></p>
<p>8 0.56855828 <a title="305-lda-8" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>9 0.5664196 <a title="305-lda-9" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>10 0.55282551 <a title="305-lda-10" href="./iccv-2013-Translating_Video_Content_to_Natural_Language_Descriptions.html">428 iccv-2013-Translating Video Content to Natural Language Descriptions</a></p>
<p>11 0.55198723 <a title="305-lda-11" href="./iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F.html">316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</a></p>
<p>12 0.55090177 <a title="305-lda-12" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>13 0.55007654 <a title="305-lda-13" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>14 0.54440963 <a title="305-lda-14" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>15 0.54283834 <a title="305-lda-15" href="./iccv-2013-YouTube2Text%3A_Recognizing_and_Describing_Arbitrary_Activities_Using_Semantic_Hierarchies_and_Zero-Shot_Recognition.html">452 iccv-2013-YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition</a></p>
<p>16 0.54150546 <a title="305-lda-16" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>17 0.54079318 <a title="305-lda-17" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>18 0.53950673 <a title="305-lda-18" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<p>19 0.53211367 <a title="305-lda-19" href="./iccv-2013-Beyond_Hard_Negative_Mining%3A_Efficient_Detector_Learning_via_Block-Circulant_Decomposition.html">61 iccv-2013-Beyond Hard Negative Mining: Efficient Detector Learning via Block-Circulant Decomposition</a></p>
<p>20 0.52979344 <a title="305-lda-20" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
