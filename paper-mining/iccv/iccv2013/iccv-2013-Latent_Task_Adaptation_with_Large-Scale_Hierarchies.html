<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-233" href="#">iccv2013-233</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</h1>
<br/><p>Source: <a title="iccv-2013-233-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Jia_Latent_Task_Adaptation_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Yangqing Jia, Trevor Darrell</p><p>Abstract: Recent years have witnessed the success of large-scale image classification systems that are able to identify objects among thousands of possible labels. However, it is yet unclear how general classifiers such as ones trained on ImageNet can be optimally adapted to specific tasks, each of which only covers a semantically related subset of all the objects in the world. It is inefficient and suboptimal to retrain classifiers whenever a new task is given, and is inapplicable when tasks are not given explicitly, but implicitly specified as a set of image queries. In this paper we propose a novel probabilistic model that jointly identifies the underlying task and performs prediction with a lineartime probabilistic inference algorithm, given a set of query images from a latent task. We present efficient ways to estimate parameters for the model, and an open-source toolbox to train classifiers distributedly at a large scale. Empirical results based on the ImageNet data showed significant performance increase over several baseline algorithms.</p><p>Reference: <a title="iccv-2013-233-reference" href="../iccv2013_reference/iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 However, it is yet unclear how general classifiers such as ones trained on ImageNet can be optimally adapted to specific tasks, each of which only covers a semantically related subset of all the objects in the world. [sent-4, score-0.24]
</p><p>2 It is inefficient and suboptimal to retrain classifiers whenever a new task is given, and is inapplicable when tasks are not given explicitly, but implicitly specified as a set of image queries. [sent-5, score-0.561]
</p><p>3 In this paper we propose a novel probabilistic model that jointly identifies the underlying task and performs prediction with a lineartime probabilistic inference algorithm, given a set of query images from a latent task. [sent-6, score-1.048]
</p><p>4 Introduction  Recent years have witnessed a growing interest in object classification tasks involving specific sets of object categories, such as fine-grained object classification [6, 12] and home object recognition in visual robotics. [sent-10, score-0.506]
</p><p>5 A dog breed classifier is trained and tested on dogs and a cat breed classifier done on cats, without the use of out-of-task images. [sent-14, score-0.479]
</p><p>6 First, it’s known that using images of related tasks is often beneficial to build a better model for the general visual world [18], which serves as a better regularization for the specific task as well. [sent-16, score-0.549]
</p><p>7 Second, object categories in the real world are often organized in, or at least well modeled by, a nested taxonomical hierarchy (e. [sent-17, score-0.3]
</p><p>8 Bottom: Adapting the ImageNet classifier allows us to perform accurate prediction (bold), while the original classifier prediction (in parentheses) suffers from a higher confusion. [sent-22, score-0.38]
</p><p>9 While it is reasonable to train separate classifiers for specific tasks, this quickly becomes infeasible as there are a huge number of possible tasks - any subtree in the hierarchy may be a latent task requiring one to distinguish object categories under the subtree. [sent-24, score-1.057]
</p><p>10 Thus, it would be beneficial to have a system which learns a large number of object categories in the world, and which is able to quickly adapt to specific incoming classification tasks (subsets of all the object categories) once deployed. [sent-25, score-0.439]
</p><p>11 We are particularly interested in the scenario where tasks are not explicitly given, but implicitly specified with  a set of query images, or a stream of query images in an online fashion. [sent-26, score-0.787]
</p><p>12 This is a new challenge beyond simple classification - one needs to discover the latent task using the context given by the queries, a problem that has not been tackled in previous classification problems. [sent-28, score-0.742]
</p><p>13 To this end, we propose a novel probabilistic framework that generatively models a latent classification task and test time image queries, built on top of the success of classical, large-scale one-vs-all classifiers. [sent-29, score-0.691]
</p><p>14 The framework allows efficient inference to be carried out to both identify the latent task from query images and adapt the classifier for the specific task. [sent-30, score-1.09]
</p><p>15 We instantiate an experimental testbed with the benchmark ImageNet large scale visual recognition challenge (ILSVRC) data using a series of latent fine-grained tasks sampled from the taxonomy, and show promising performance over conventional classification methods. [sent-31, score-0.575]
</p><p>16 We show that with a large-scale image source where object labels are organized in a taxonomical structure, it is almost always beneficial to learn the classifier on the whole dataset even for tasks involving only subtrees of the overall taxonomy. [sent-33, score-0.688]
</p><p>17 More importantly, we examine a novel task adaptation paradigm  that is beyond recognizing individual images, and propose an algorithm to easily adapt a general classifier to unknown latent tasks during testing time, yielding a significant performance boost. [sent-34, score-1.101]
</p><p>18 Finally, our pipeline will be made open-source, including a toolbox for distributed classifier learning with quasiNewton stochastic algorithms [5], which allows one to train large-scale classifiers (such as ILSVRC) without the need of huge clusters or sophisticated infrastructure support. [sent-35, score-0.4]
</p><p>19 Related Work The problem of task adaptation is analogous to, but essentially distinctive from domain adaptation [19, 14]. [sent-37, score-0.471]
</p><p>20 While domain adaptation aims to model the perceptual difference of the training and testing images from the same labels, task adaptation focuses on modeling the conceptual difference: different label spaces during training and testing. [sent-38, score-0.529]
</p><p>21 Additionally, as one is often able to use large amounts of data during training, we assume that the testing tasks involve subsets of labels encountered during training time. [sent-39, score-0.296]
</p><p>22 There are several algorithms in image classification that use label hierarchy or structured regularizations to learn bet-  corresponding query images. [sent-43, score-0.539]
</p><p>23 Right: the prior probabilities of the latent tasks from psychological study, along the path leading to the synsets oriental poppy and can opener respectively, with darker color indicating higher probability. [sent-44, score-0.706]
</p><p>24 ter classifiers [20, 10, 8], or to leverage the accuracy and information gain from classifiers [4]. [sent-45, score-0.287]
</p><p>25 The ultimate goal thus remains to be better accuracy on classifying individual images, not to adapt to different tasks during testing time by utilizing contextual information. [sent-47, score-0.338]
</p><p>26 Better classifiers presented in these papers could, of course, be incorporated in our model to improve the end-to-end performance of task adaptation. [sent-48, score-0.345]
</p><p>27 In this paper we utilize a novel type of context - task context - that is implied by a semantically related group of images. [sent-50, score-0.298]
</p><p>28 A Generative Model for Task Adaptation Formally, we define a classification task to be a subset of all the possible object labels that are semantically related (such as all breeds of dogs in ImageNet). [sent-52, score-0.515]
</p><p>29 During testing time, a number of query images are randomly sampled from the labels belonging to a task, and the learning algorithm needs to give predictions on these images. [sent-53, score-0.399]
</p><p>30 In this section we propose a probabilistic framework that models the generation of latent tasks and the test time query images. [sent-54, score-0.807]
</p><p>31 As stated in the previous section, we are interested in the scenario when the task is latent, i. [sent-55, score-0.239]
</p><p>32 We introduce two key components for modeling the generative process of query images: a latent task space that defines possible tasks and their probability, and a procedure to sample query images given a specific latent task. [sent-58, score-1.597]
</p><p>33 Specifically, we propose the graphical model in Figure 2 which generates a set of N query images when given T possible tasks and K object categories: 1. [sent-59, score-0.463]
</p><p>34 Sample a latent task h from the task priors P(h) with hyperparameter α; 2081  2. [sent-60, score-0.81]
</p><p>35 For the N query images: (a) Sample an object category yi from the conditional probability P(yi |h; βh) ; (b) Sample a query image f;rβom category yi with P(xi |yi ; θyi ). [sent-61, score-0.853]
</p><p>36 To this end, we take advantage of the existing research in cognitive science to construct the latent task space and the prior distribution. [sent-66, score-0.571]
</p><p>37 For the structure of the latent task space, we adopt the WordNet hierarchy [7], which models the semantic relations in a psychologically justified tree structure [17]. [sent-67, score-0.674]
</p><p>38 The use of WordNet in cognitive science has shown promising results in identifying latent concepts (semantically related sets from the universe of objects) for human concept learning [1, 24]. [sent-68, score-0.332]
</p><p>39 In our work, we follow the existing classification protocols [2] by considering the set of leaf nodes in the tree as the object labels that we need to classify images into. [sent-69, score-0.292]
</p><p>40 It could be observed that basic level tasks have higher probability than overly general tasks such as “entity”, which means that our bias is for the computer to assist us in more specific tasks, e. [sent-78, score-0.492]
</p><p>41 For each query image, we first sample the object class label from the set of possible  labels that belong to the task. [sent-85, score-0.341]
</p><p>42 01,/|h|, iofth taesrkw ihse c,ontains label yi  (2) where |h| is the size of the task - the number of leaf node cwlhaessrees hin| t ihse t thaesk s. [sent-87, score-0.558]
</p><p>43 i zTeh oef s tihzee principle plays a rcri otfic laela rfo nleo dine inferring the latent task, as larger tasks will generate lower probabilities for each individual object class. [sent-88, score-0.467]
</p><p>44 Thus, when we observe a Dalmatian, a corgi and a Shih-Tzu, the latent task “dog” is more probable than task “animal” since the former yields higher conditional probability for the detailed dog breeds. [sent-89, score-0.92]
</p><p>45 Thus, we use a mixed approach by having a classifier trained on all the leaf node objects, and obtain the classifier prediction f(xi) =  argmaxj  θj? [sent-91, score-0.446]
</p><p>46 The condlinitieoanra cll probability his p tahreanm deetfeirne {dθ as P(xi|yi)  = Cyif(xi),  (4)  where C is the confusion matrix of the classifier, and Cij is the probability that an image of object class iis classified as class j. [sent-93, score-0.315]
</p><p>47 (5)  We will discuss in the next section how the various parameters, especially the parameters θ for the classifiers and the confusion matrix C, can be estimated from training data, and how to carry out efficient inference to find the solution to Eqn. [sent-95, score-0.38]
</p><p>48 In this section, we present a novel approach to estimate the confusion matrix for the classifier, and a linear-time inference algorithm that jointly identifies the latent task and predictions for individual images. [sent-99, score-0.8]
</p><p>49 Confusion Matrix Estimation with One-step Unlearning Given a classifier, evaluating its behavior (including ac-  curacy and confusion matrix) is often tackled with two approaches: using cross-validation or using a held-out validation dataset. [sent-102, score-0.263]
</p><p>50 A held-out validation dataset usually estimates the accuracy well, but not for the confusion matrix C due to insufficient number of validation images. [sent-105, score-0.419]
</p><p>51 For example, the ILSVRC challenge has only 50K validation images versus 1million confusion matrix entries, leading to a large number of incorrect zero entries in the estimated confusion matrix (see supplementary material). [sent-106, score-0.517]
</p><p>52 We use the new parameter θ\xi to perform prediction on xi as if xi has been left out during training, and accumulate the approximated LOO results to obtain the confusion matrix. [sent-133, score-0.44]
</p><p>53 Linear Time MAP Inference A conventional way to do probabilistic inference with nested latent variables is to use variational inference or 2In practice we used the accumulated matrix H obtained from Adagrad [5] as a good approximation of the Hessian matrix. [sent-137, score-0.558]
</p><p>54 We show that when the latent task space is organized in a DAG structure, the exact MAP solution (Eqn. [sent-142, score-0.561]
</p><p>55 defining the latent task space gives us βhyi = |h1|I(yi ∈ h), the equation above could further be written as logαh− N log|h|  +? [sent-148, score-0.571]
</p><p>56 Finally, the latent task could be estimated as  hˆ = arghmax? [sent-156, score-0.571]
</p><p>57 In practice, simply finding the MAP solution (using γ = 1) often involves a task that is smaller than the ground truth, as there are two ways to explain the predicted labels: assuming correct prediction and a task of larger size, or assuming wrong prediction and a task of smaller size. [sent-161, score-0.905]
</p><p>58 We found it beneficial to explicitly add a weight term that favors the classifier outputs using γ > 1learned on validation data. [sent-163, score-0.254]
</p><p>59 In general, our dynamic programming method runs in O(TNb) time where T is the number oftasks, N is the number of query images, and b is the branching factor of the tree (usually a small constant factor). [sent-164, score-0.283]
</p><p>60 This complexity is linear to the number of testing images and to the number of latent tasks, and is usually negligible compared to the basic classification algorithm, which runs O(KND) time where K is the number of classes and D is the feature dimension (usually very large). [sent-165, score-0.453]
</p><p>61 2083  Finally, one may prefer an online algorithm that could take new images as a stream, performing classification sequentially while discovering the latent task on the fly. [sent-166, score-0.72]
</p><p>62 Specifically, qi (h) serves as the sufficient statistics for the task discovery, and we only need to keep record of the ac-  cumulated auxiliary function values seen so far as  q:n(h) =? [sent-168, score-0.351]
</p><p>63 Distributed Implementation Details Recent image classification tasks often involve large amounts of images, making the training of classifiers increasingly difficult. [sent-172, score-0.394]
</p><p>64 We note that more comprehensive features and better classification pipelines may lead to better 1-vs-all accuracy  on ImageNet, but it is not the main goal of the paper, as we focus on the adaptation on top of the base classifiers. [sent-195, score-0.26]
</p><p>65 Estimating the Confusion Matrix As stated in Section 4, an good estimation of the confusion matrix C is crucial for the probabilistic inference. [sent-199, score-0.274]
</p><p>66 We evaluate the quality of different estimations using the test data: for each testing pair (y, ˆy ), where ˆy is the classifier output, its probability is given by the confusion matrix entry Cyˆ y. [sent-200, score-0.446]
</p><p>67 The perplexity measure [11] then evaluates how “surprising” the confusion matrix sees the testing data results (a smaller value indicates a better fit): perp =  Power? [sent-201, score-0.336]
</p><p>68 27 using our unlearning algorithm, while the validation data gave a value of 68. [sent-212, score-0.267]
</p><p>69 To this end, we specify 5 subtrees from the ILSVRC hierarchy: bui lding, dogs, fe l ine (the superset of cats), home appl iance, and vehi cle, the subcategories of which are often of interest. [sent-220, score-0.287]
</p><p>70 Figure 1 visualizes the corresponding subtrees for dog, feline and vehicles respectively. [sent-221, score-0.251]
</p><p>71 We explicitly trained classifiers on these three subtrees only, and compared the retrained accuracy against our adapted classifier with the given task. [sent-222, score-0.491]
</p><p>72 We also test the naive baseline that uses the raw 1000 class predictions, and the forced choice baseline (FC) which simply selects the class under the task that has the largest output from the original classifiers. [sent-223, score-0.326]
</p><p>73 It is worth pointing out that retraining the classifiers for the specific tasks does not help improve the classification 2084  bufieTdloasdigknie g43N5 75a. [sent-225, score-0.54]
</p><p>74 r876a03cy  Table 2: The average task overlap score and the average accuracy for the algorithms, under query sizes 5 and 100 respectively. [sent-244, score-0.598]
</p><p>75 The last row provides the oracle performance in which the ground truth task is given. [sent-246, score-0.275]
</p><p>76 20864125s0eti2z (log5s0cale1)02ahpn5dieraso0tipvgoet Figure 3: Classification accuracy (left) and the task overlap score (right) with different query set sizes for our method and the baselines. [sent-249, score-0.598]
</p><p>77 Our method further benefits from the statistics from all the classifiers (for in-task and out-of-task classes) in the proposed probabilistic framework to achieve the best adapted accuracy in most cases (only slightly worse than the FC baseline on vehi cle). [sent-253, score-0.297]
</p><p>78 Joint Task Discovery and Classification We next analyze the performance when we have the classifier trained on the whole ILSVRC data, and adapt it to an unknown task that is defined by a set of query images. [sent-256, score-0.743]
</p><p>79 The forced choice option is not available in this case as we do not know the latent task beforehand, and one has to use the semantic relationships between the query images to infer the latent task. [sent-257, score-1.145]
</p><p>80 To sample the latent tasks, we used the Erlang prior defined in Section 3 from the ImageNet Tree excluding leaf nodes (as leaf nodes would contain only 1 label). [sent-258, score-0.539]
</p><p>81 We then randomly sampled N query images from the subtree of the sampled task. [sent-259, score-0.342]
</p><p>82 All query images were randomly selected from the test images of ILSVRC and had not been seen by the classifier training. [sent-260, score-0.405]
</p><p>83 For each query image size N, we created 10,000 independent tasks  ×  and reported the average performance here. [sent-262, score-0.463]
</p><p>84 To the best of our knowledge there is no published classification algorithm that is able to identify the latent task, i. [sent-266, score-0.395]
</p><p>85 the intermediate node in the taxonomy hierarchy, given a set of query images. [sent-268, score-0.424]
</p><p>86 • Hedging approach: we extend the hedging idea [4] to hHaenddglein sge atsp pofr query images. [sent-273, score-0.351]
</p><p>87 The corresponding task is then chosen as the predicted latent task. [sent-276, score-0.578]
</p><p>88 Each row shows 5 images from a latent task, and on the right we give the predicted task by different algorithms, ordered and colored as naive, proto, hist, hedge, and adapt. [sent-280, score-0.578]
</p><p>89 Table 2 summarizes the performance of the methods above with a small query set size (5 images) and a relatively large size (100 image). [sent-285, score-0.283]
</p><p>90 It could be observed that when we have a reasonable amount of testing queries, identifying the latent task leads to a significant performance gain than the baseline method that does classification against all possible labels, with an increase of near 30% percent. [sent-287, score-0.776]
</p><p>91 Even with a small query size (such as 5), the performance gain is already noticably high, indicating the ability of the algorithm to perform task adaptation with very few images from the latent task. [sent-288, score-0.964]
</p><p>92 Online Evaluation Our final evaluation tests the performance of the proposed method in an online fashion - when images of an unknown task come as a streaming sequence. [sent-291, score-0.315]
</p><p>93 Intuitively, our algorithm obtains better information about the unknown task as new images arrive, which would in turn increase the classification accuracy. [sent-292, score-0.382]
</p><p>94 We test such conjecture by evaluat-  ing the averaged accuracy of the n-th image, over multiple independent test query sequences that are generated in the same way as described in the previous subsection. [sent-293, score-0.359]
</p><p>95 Figure 5 shows the average accuracy of the n-th query image, as well as the overlap between the identified task so far and the ground truth task. [sent-294, score-0.598]
</p><p>96 This has particular practical interest, as one may want the computer to quickly adapt to a new task 2086  cyau rc0 0. [sent-296, score-0.303]
</p><p>97 40862134Que5rynId6ex78hpnairdeo9siadtvopegt10 Figure 5: Classification accuracy (left) and task overlap score (right) of our online algorithm against baselines. [sent-299, score-0.356]
</p><p>98 It is worth pointing out that with heuristic task estimation methods (see the baselines in Figure 5 left), one may incorrectly assert the latent task, which then hurts classification performance for the first few query images. [sent-303, score-0.917]
</p><p>99 Conclusion We addressed a novel challenge when the classification problem involves latent tasks corresponding to semantically related subsets of all the objects in the world. [sent-305, score-0.634]
</p><p>100 We proposed a novel framework that is able to adapt to latent tasks to achieve a significant performance gain given a relatively small set of query images. [sent-306, score-0.853]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('latent', 0.287), ('query', 0.283), ('ilsvrc', 0.245), ('task', 0.239), ('subtrees', 0.19), ('unlearning', 0.184), ('tasks', 0.18), ('confusion', 0.18), ('hierarchy', 0.148), ('imagenet', 0.143), ('classifier', 0.122), ('adaptation', 0.116), ('retraining', 0.108), ('classification', 0.108), ('classifiers', 0.106), ('loo', 0.101), ('yi', 0.099), ('xi', 0.096), ('minibatch', 0.092), ('leaf', 0.09), ('toolbox', 0.087), ('validation', 0.083), ('adagrad', 0.082), ('psychological', 0.07), ('qi', 0.069), ('prediction', 0.068), ('hedging', 0.068), ('dog', 0.066), ('adapt', 0.064), ('nested', 0.063), ('erlang', 0.061), ('feline', 0.061), ('grocery', 0.061), ('logcyif', 0.061), ('logi', 0.061), ('opener', 0.061), ('oriental', 0.061), ('perplexity', 0.061), ('vehi', 0.061), ('semantically', 0.059), ('subtree', 0.059), ('breed', 0.059), ('labels', 0.058), ('testing', 0.058), ('inference', 0.057), ('probabilistic', 0.057), ('tenenbaum', 0.056), ('taxonomical', 0.054), ('predicted', 0.052), ('taxonomy', 0.051), ('dogs', 0.051), ('griffiths', 0.05), ('hyi', 0.05), ('probability', 0.049), ('forced', 0.049), ('beneficial', 0.049), ('queries', 0.048), ('synsets', 0.047), ('hessian', 0.047), ('intermediate', 0.046), ('trip', 0.045), ('hyperparameter', 0.045), ('cognitive', 0.045), ('could', 0.045), ('stochastic', 0.044), ('node', 0.044), ('log', 0.044), ('serves', 0.043), ('cle', 0.043), ('ihse', 0.043), ('behavioral', 0.043), ('cha', 0.043), ('ui', 0.043), ('distributed', 0.041), ('adapting', 0.041), ('online', 0.041), ('overlap', 0.04), ('averaged', 0.04), ('conditional', 0.04), ('encounter', 0.039), ('rke', 0.039), ('wordnet', 0.039), ('gain', 0.039), ('entity', 0.038), ('naive', 0.038), ('specific', 0.038), ('ey', 0.037), ('matrix', 0.037), ('adapted', 0.037), ('retrain', 0.036), ('witnessed', 0.036), ('bayesian', 0.036), ('nodes', 0.036), ('oracle', 0.036), ('darrell', 0.036), ('home', 0.036), ('accuracy', 0.036), ('generalization', 0.035), ('unknown', 0.035), ('organized', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="233-tfidf-1" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>Author: Yangqing Jia, Trevor Darrell</p><p>Abstract: Recent years have witnessed the success of large-scale image classification systems that are able to identify objects among thousands of possible labels. However, it is yet unclear how general classifiers such as ones trained on ImageNet can be optimally adapted to specific tasks, each of which only covers a semantically related subset of all the objects in the world. It is inefficient and suboptimal to retrain classifiers whenever a new task is given, and is inapplicable when tasks are not given explicitly, but implicitly specified as a set of image queries. In this paper we propose a novel probabilistic model that jointly identifies the underlying task and performs prediction with a lineartime probabilistic inference algorithm, given a set of query images from a latent task. We present efficient ways to estimate parameters for the model, and an open-source toolbox to train classifiers distributedly at a large scale. Empirical results based on the ImageNet data showed significant performance increase over several baseline algorithms.</p><p>2 0.23873638 <a title="233-tfidf-2" href="./iccv-2013-Learning_to_Share_Latent_Tasks_for_Action_Recognition.html">249 iccv-2013-Learning to Share Latent Tasks for Action Recognition</a></p>
<p>Author: Qiang Zhou, Gang Wang, Kui Jia, Qi Zhao</p><p>Abstract: Sharing knowledge for multiple related machine learning tasks is an effective strategy to improve the generalization performance. In this paper, we investigate knowledge sharing across categories for action recognition in videos. The motivation is that many action categories are related, where common motion pattern are shared among them (e.g. diving and high jump share the jump motion). We propose a new multi-task learning method to learn latent tasks shared across categories, and reconstruct a classifier for each category from these latent tasks. Compared to previous methods, our approach has two advantages: (1) The learned latent tasks correspond to basic motionpatterns instead offull actions, thus enhancing discrimination power of the classifiers. (2) Categories are selected to share information with a sparsity regularizer, avoidingfalselyforcing all categories to share knowledge. Experimental results on multiplepublic data sets show that the proposed approach can effectively transfer knowledge between different action categories to improve the performance of conventional single task learning methods.</p><p>3 0.21299453 <a title="233-tfidf-3" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>Author: Basura Fernando, Tinne Tuytelaars</p><p>Abstract: In this paper we present a new method for object retrieval starting from multiple query images. The use of multiple queries allows for a more expressive formulation of the query object including, e.g., different viewpoints and/or viewing conditions. This, in turn, leads to more diverse and more accurate retrieval results. When no query images are available to the user, they can easily be retrieved from the internet using a standard image search engine. In particular, we propose a new method based on pattern mining. Using the minimal description length principle, we derive the most suitable set of patterns to describe the query object, with patterns corresponding to local feature configurations. This results in apowerful object-specific mid-level image representation. The archive can then be searched efficiently for similar images based on this representation, using a combination of two inverted file systems. Since the patterns already encode local spatial information, good results on several standard image retrieval datasets are obtained even without costly re-ranking based on geometric verification.</p><p>4 0.20399064 <a title="233-tfidf-4" href="./iccv-2013-Group_Norm_for_Learning_Structured_SVMs_with_Unstructured_Latent_Variables.html">187 iccv-2013-Group Norm for Learning Structured SVMs with Unstructured Latent Variables</a></p>
<p>Author: Daozheng Chen, Dhruv Batra, William T. Freeman</p><p>Abstract: Latent variables models have been applied to a number of computer vision problems. However, the complexity of the latent space is typically left as a free design choice. A larger latent space results in a more expressive model, but such models are prone to overfitting and are slower to perform inference with. The goal of this paper is to regularize the complexity of the latent space and learn which hidden states are really relevant for prediction. Specifically, we propose using group-sparsity-inducing regularizers such as ?1-?2 to estimate the parameters of Structured SVMs with unstructured latent variables. Our experiments on digit recognition and object detection show that our approach is indeed able to control the complexity of latent space without any significant loss in accuracy of the learnt model.</p><p>5 0.19997491 <a title="233-tfidf-5" href="./iccv-2013-Find_the_Best_Path%3A_An_Efficient_and_Accurate_Classifier_for_Image_Hierarchies.html">165 iccv-2013-Find the Best Path: An Efficient and Accurate Classifier for Image Hierarchies</a></p>
<p>Author: Min Sun, Wan Huang, Silvio Savarese</p><p>Abstract: Many methods have been proposed to solve the image classification problem for a large number of categories. Among them, methods based on tree-based representations achieve good trade-off between accuracy and test time efficiency. While focusing on learning a tree-shaped hierarchy and the corresponding set of classifiers, most of them [11, 2, 14] use a greedy prediction algorithm for test time efficiency. We argue that the dramatic decrease in accuracy at high efficiency is caused by the specific design choice of the learning and greedy prediction algorithms. In this work, we propose a classifier which achieves a better trade-off between efficiency and accuracy with a given tree-shaped hierarchy. First, we convert the classification problem as finding the best path in the hierarchy, and a novel branchand-bound-like algorithm is introduced to efficiently search for the best path. Second, we jointly train the classifiers using a novel Structured SVM (SSVM) formulation with additional bound constraints. As a result, our method achieves a significant 4.65%, 5.43%, and 4.07% (relative 24.82%, 41.64%, and 109.79%) improvement in accuracy at high efficiency compared to state-of-the-art greedy “tree-based” methods [14] on Caltech-256 [15], SUN [32] and ImageNet 1K [9] dataset, respectively. Finally, we show that our branch-and-bound-like algorithm naturally ranks the paths in the hierarchy (Fig. 8) so that users can further process them.</p><p>6 0.17952867 <a title="233-tfidf-6" href="./iccv-2013-From_Large_Scale_Image_Categorization_to_Entry-Level_Categories.html">176 iccv-2013-From Large Scale Image Categorization to Entry-Level Categories</a></p>
<p>7 0.1781207 <a title="233-tfidf-7" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>8 0.17002976 <a title="233-tfidf-8" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>9 0.16898054 <a title="233-tfidf-9" href="./iccv-2013-Detecting_Avocados_to_Zucchinis%3A_What_Have_We_Done%2C_and_Where_Are_We_Going%3F.html">109 iccv-2013-Detecting Avocados to Zucchinis: What Have We Done, and Where Are We Going?</a></p>
<p>10 0.14709765 <a title="233-tfidf-10" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>11 0.14483285 <a title="233-tfidf-11" href="./iccv-2013-Query-Adaptive_Asymmetrical_Dissimilarities_for_Visual_Object_Retrieval.html">334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</a></p>
<p>12 0.12245706 <a title="233-tfidf-12" href="./iccv-2013-Handling_Occlusions_with_Franken-Classifiers.html">190 iccv-2013-Handling Occlusions with Franken-Classifiers</a></p>
<p>13 0.11996345 <a title="233-tfidf-13" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>14 0.11192366 <a title="233-tfidf-14" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>15 0.10692219 <a title="233-tfidf-15" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>16 0.1068155 <a title="233-tfidf-16" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>17 0.10670304 <a title="233-tfidf-17" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>18 0.10657974 <a title="233-tfidf-18" href="./iccv-2013-Unsupervised_Visual_Domain_Adaptation_Using_Subspace_Alignment.html">438 iccv-2013-Unsupervised Visual Domain Adaptation Using Subspace Alignment</a></p>
<p>19 0.10397247 <a title="233-tfidf-19" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<p>20 0.10109732 <a title="233-tfidf-20" href="./iccv-2013-Unbiased_Metric_Learning%3A_On_the_Utilization_of_Multiple_Datasets_and_Web_Images_for_Softening_Bias.html">431 iccv-2013-Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.259), (1, 0.146), (2, -0.049), (3, -0.11), (4, 0.078), (5, 0.114), (6, -0.019), (7, 0.012), (8, -0.07), (9, -0.067), (10, 0.061), (11, -0.1), (12, -0.032), (13, -0.0), (14, 0.06), (15, -0.051), (16, 0.014), (17, -0.118), (18, 0.123), (19, -0.019), (20, -0.071), (21, -0.093), (22, -0.066), (23, 0.033), (24, -0.073), (25, -0.101), (26, 0.092), (27, -0.117), (28, 0.117), (29, -0.011), (30, 0.047), (31, -0.002), (32, -0.111), (33, -0.062), (34, -0.022), (35, -0.09), (36, -0.033), (37, 0.144), (38, 0.022), (39, -0.086), (40, 0.064), (41, -0.017), (42, 0.056), (43, -0.085), (44, 0.051), (45, 0.049), (46, 0.006), (47, 0.151), (48, 0.058), (49, -0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96516186 <a title="233-lsi-1" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>Author: Yangqing Jia, Trevor Darrell</p><p>Abstract: Recent years have witnessed the success of large-scale image classification systems that are able to identify objects among thousands of possible labels. However, it is yet unclear how general classifiers such as ones trained on ImageNet can be optimally adapted to specific tasks, each of which only covers a semantically related subset of all the objects in the world. It is inefficient and suboptimal to retrain classifiers whenever a new task is given, and is inapplicable when tasks are not given explicitly, but implicitly specified as a set of image queries. In this paper we propose a novel probabilistic model that jointly identifies the underlying task and performs prediction with a lineartime probabilistic inference algorithm, given a set of query images from a latent task. We present efficient ways to estimate parameters for the model, and an open-source toolbox to train classifiers distributedly at a large scale. Empirical results based on the ImageNet data showed significant performance increase over several baseline algorithms.</p><p>2 0.69763875 <a title="233-lsi-2" href="./iccv-2013-From_Large_Scale_Image_Categorization_to_Entry-Level_Categories.html">176 iccv-2013-From Large Scale Image Categorization to Entry-Level Categories</a></p>
<p>Author: Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C. Berg, Tamara L. Berg</p><p>Abstract: Entry level categories the labels people will use to name an object were originally defined and studied by psychologists in the 1980s. In this paper we study entrylevel categories at a large scale and learn the first models for predicting entry-level categories for images. Our models combine visual recognition predictions with proxies for word “naturalness ” mined from the enormous amounts of text on the web. We demonstrate the usefulness of our models for predicting nouns (entry-level words) associated with images by people. We also learn mappings between concepts predicted by existing visual recognition systems and entry-level concepts that could be useful for improving human-focused applications such as natural language image description or retrieval. – –</p><p>3 0.6807999 <a title="233-lsi-3" href="./iccv-2013-Group_Norm_for_Learning_Structured_SVMs_with_Unstructured_Latent_Variables.html">187 iccv-2013-Group Norm for Learning Structured SVMs with Unstructured Latent Variables</a></p>
<p>Author: Daozheng Chen, Dhruv Batra, William T. Freeman</p><p>Abstract: Latent variables models have been applied to a number of computer vision problems. However, the complexity of the latent space is typically left as a free design choice. A larger latent space results in a more expressive model, but such models are prone to overfitting and are slower to perform inference with. The goal of this paper is to regularize the complexity of the latent space and learn which hidden states are really relevant for prediction. Specifically, we propose using group-sparsity-inducing regularizers such as ?1-?2 to estimate the parameters of Structured SVMs with unstructured latent variables. Our experiments on digit recognition and object detection show that our approach is indeed able to control the complexity of latent space without any significant loss in accuracy of the learnt model.</p><p>4 0.62980133 <a title="233-lsi-4" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>Author: Basura Fernando, Tinne Tuytelaars</p><p>Abstract: In this paper we present a new method for object retrieval starting from multiple query images. The use of multiple queries allows for a more expressive formulation of the query object including, e.g., different viewpoints and/or viewing conditions. This, in turn, leads to more diverse and more accurate retrieval results. When no query images are available to the user, they can easily be retrieved from the internet using a standard image search engine. In particular, we propose a new method based on pattern mining. Using the minimal description length principle, we derive the most suitable set of patterns to describe the query object, with patterns corresponding to local feature configurations. This results in apowerful object-specific mid-level image representation. The archive can then be searched efficiently for similar images based on this representation, using a combination of two inverted file systems. Since the patterns already encode local spatial information, good results on several standard image retrieval datasets are obtained even without costly re-ranking based on geometric verification.</p><p>5 0.62930328 <a title="233-lsi-5" href="./iccv-2013-Learning_to_Share_Latent_Tasks_for_Action_Recognition.html">249 iccv-2013-Learning to Share Latent Tasks for Action Recognition</a></p>
<p>Author: Qiang Zhou, Gang Wang, Kui Jia, Qi Zhao</p><p>Abstract: Sharing knowledge for multiple related machine learning tasks is an effective strategy to improve the generalization performance. In this paper, we investigate knowledge sharing across categories for action recognition in videos. The motivation is that many action categories are related, where common motion pattern are shared among them (e.g. diving and high jump share the jump motion). We propose a new multi-task learning method to learn latent tasks shared across categories, and reconstruct a classifier for each category from these latent tasks. Compared to previous methods, our approach has two advantages: (1) The learned latent tasks correspond to basic motionpatterns instead offull actions, thus enhancing discrimination power of the classifiers. (2) Categories are selected to share information with a sparsity regularizer, avoidingfalselyforcing all categories to share knowledge. Experimental results on multiplepublic data sets show that the proposed approach can effectively transfer knowledge between different action categories to improve the performance of conventional single task learning methods.</p><p>6 0.62875426 <a title="233-lsi-6" href="./iccv-2013-Query-Adaptive_Asymmetrical_Dissimilarities_for_Visual_Object_Retrieval.html">334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</a></p>
<p>7 0.62626237 <a title="233-lsi-7" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<p>8 0.62250662 <a title="233-lsi-8" href="./iccv-2013-Find_the_Best_Path%3A_An_Efficient_and_Accurate_Classifier_for_Image_Hierarchies.html">165 iccv-2013-Find the Best Path: An Efficient and Accurate Classifier for Image Hierarchies</a></p>
<p>9 0.58742446 <a title="233-lsi-9" href="./iccv-2013-Unbiased_Metric_Learning%3A_On_the_Utilization_of_Multiple_Datasets_and_Web_Images_for_Softening_Bias.html">431 iccv-2013-Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias</a></p>
<p>10 0.58269423 <a title="233-lsi-10" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>11 0.56627703 <a title="233-lsi-11" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>12 0.56488532 <a title="233-lsi-12" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>13 0.56038886 <a title="233-lsi-13" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>14 0.55657166 <a title="233-lsi-14" href="./iccv-2013-Image_Segmentation_with_Cascaded_Hierarchical_Models_and_Logistic_Disjunctive_Normal_Networks.html">211 iccv-2013-Image Segmentation with Cascaded Hierarchical Models and Logistic Disjunctive Normal Networks</a></p>
<p>15 0.54853082 <a title="233-lsi-15" href="./iccv-2013-NEIL%3A_Extracting_Visual_Knowledge_from_Web_Data.html">285 iccv-2013-NEIL: Extracting Visual Knowledge from Web Data</a></p>
<p>16 0.54545027 <a title="233-lsi-16" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>17 0.53849846 <a title="233-lsi-17" href="./iccv-2013-Learning_Slow_Features_for_Behaviour_Analysis.html">243 iccv-2013-Learning Slow Features for Behaviour Analysis</a></p>
<p>18 0.53176987 <a title="233-lsi-18" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<p>19 0.52873725 <a title="233-lsi-19" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>20 0.52646452 <a title="233-lsi-20" href="./iccv-2013-Learning_to_Rank_Using_Privileged_Information.html">248 iccv-2013-Learning to Rank Using Privileged Information</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.081), (4, 0.012), (7, 0.017), (17, 0.118), (26, 0.081), (31, 0.054), (34, 0.012), (42, 0.135), (55, 0.011), (64, 0.035), (65, 0.014), (73, 0.083), (77, 0.021), (89, 0.162), (93, 0.028), (95, 0.018), (98, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89515287 <a title="233-lda-1" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>Author: Yangqing Jia, Trevor Darrell</p><p>Abstract: Recent years have witnessed the success of large-scale image classification systems that are able to identify objects among thousands of possible labels. However, it is yet unclear how general classifiers such as ones trained on ImageNet can be optimally adapted to specific tasks, each of which only covers a semantically related subset of all the objects in the world. It is inefficient and suboptimal to retrain classifiers whenever a new task is given, and is inapplicable when tasks are not given explicitly, but implicitly specified as a set of image queries. In this paper we propose a novel probabilistic model that jointly identifies the underlying task and performs prediction with a lineartime probabilistic inference algorithm, given a set of query images from a latent task. We present efficient ways to estimate parameters for the model, and an open-source toolbox to train classifiers distributedly at a large scale. Empirical results based on the ImageNet data showed significant performance increase over several baseline algorithms.</p><p>2 0.88842314 <a title="233-lda-2" href="./iccv-2013-Flattening_Supervoxel_Hierarchies_by_the_Uniform_Entropy_Slice.html">172 iccv-2013-Flattening Supervoxel Hierarchies by the Uniform Entropy Slice</a></p>
<p>Author: Chenliang Xu, Spencer Whitt, Jason J. Corso</p><p>Abstract: Supervoxel hierarchies provide a rich multiscale decomposition of a given video suitable for subsequent processing in video analysis. The hierarchies are typically computed by an unsupervised process that is susceptible to undersegmentation at coarse levels and over-segmentation at fine levels, which make it a challenge to adopt the hierarchies for later use. In this paper, we propose the first method to overcome this limitation and flatten the hierarchy into a single segmentation. Our method, called the uniform entropy slice, seeks a selection of supervoxels that balances the relative level of information in the selected supervoxels based on some post hoc feature criterion such as objectness. For example, with this criterion, in regions nearby objects, our method prefers finer supervoxels to capture the local details, but in regions away from any objects we prefer coarser supervoxels. We formulate the uniform entropy slice as a binary quadratic program and implement four different feature criteria, both unsupervised and supervised, to drive the flattening. Although we apply it only to supervoxel hierarchies in this paper, our method is generally applicable to segmentation tree hierarchies. Our experiments demonstrate both strong qualitative performance and superior quantitative performance to state of the art baselines on benchmark internet videos.</p><p>3 0.87560773 <a title="233-lda-3" href="./iccv-2013-A_New_Image_Quality_Metric_for_Image_Auto-denoising.html">23 iccv-2013-A New Image Quality Metric for Image Auto-denoising</a></p>
<p>Author: Xiangfei Kong, Kuan Li, Qingxiong Yang, Liu Wenyin, Ming-Hsuan Yang</p><p>Abstract: This paper proposes a new non-reference image quality metric that can be adopted by the state-of-the-art image/video denoising algorithms for auto-denoising. The proposed metric is extremely simple and can be implemented in four lines of Matlab code1. The basic assumption employed by the proposed metric is that the noise should be independent of the original image. A direct measurement of this dependence is, however, impractical due to the relatively low accuracy of existing denoising method. The proposed metric thus aims at maximizing the structure similarity between the input noisy image and the estimated image noise around homogeneous regions and the structure similarity between the input noisy image and the denoised image around highly-structured regions, and is computed as the linear correlation coefficient of the two corresponding structure similarity maps. Numerous experimental results demonstrate that the proposed metric not only outperforms the current state-of-the-art non-reference quality metric quantitatively and qualitatively, but also better maintains temporal coherence when used for video denoising. ˜</p><p>4 0.8685751 <a title="233-lda-4" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>Author: Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, Philip S. Yu</p><p>Abstract: Transfer learning is established as an effective technology in computer visionfor leveraging rich labeled data in the source domain to build an accurate classifier for the target domain. However, most prior methods have not simultaneously reduced the difference in both the marginal distribution and conditional distribution between domains. In this paper, we put forward a novel transfer learning approach, referred to as Joint Distribution Adaptation (JDA). Specifically, JDA aims to jointly adapt both the marginal distribution and conditional distribution in a principled dimensionality reduction procedure, and construct new feature representation that is effective and robustfor substantial distribution difference. Extensive experiments verify that JDA can significantly outperform several state-of-the-art methods on four types of cross-domain image classification problems.</p><p>5 0.86766756 <a title="233-lda-5" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>Author: S. Karthikeyan, Vignesh Jagadeesh, Renuka Shenoy, Miguel Ecksteinz, B.S. Manjunath</p><p>Abstract: Eye movement studies have confirmed that overt attention is highly biased towards faces and text regions in images. In this paper we explore a novel problem of predicting face and text regions in images using eye tracking data from multiple subjects. The problem is challenging as we aim to predict the semantics (face/text/background) only from eye tracking data without utilizing any image information. The proposed algorithm spatially clusters eye tracking data obtained in an image into different coherent groups and subsequently models the likelihood of the clusters containing faces and text using afully connectedMarkov Random Field (MRF). Given the eye tracking datafrom a test image, itpredicts potential face/head (humans, dogs and cats) and text locations reliably. Furthermore, the approach can be used to select regions of interest for further analysis by object detectors for faces and text. The hybrid eye position/object detector approach achieves better detection performance and reduced computation time compared to using only the object detection algorithm. We also present a new eye tracking dataset on 300 images selected from ICDAR, Street-view, Flickr and Oxford-IIIT Pet Dataset from 15 subjects.</p><p>6 0.86707246 <a title="233-lda-6" href="./iccv-2013-Joint_Noise_Level_Estimation_from_Personal_Photo_Collections.html">223 iccv-2013-Joint Noise Level Estimation from Personal Photo Collections</a></p>
<p>7 0.86650825 <a title="233-lda-7" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>8 0.86643773 <a title="233-lda-8" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>9 0.86591983 <a title="233-lda-9" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<p>10 0.86472261 <a title="233-lda-10" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>11 0.86338061 <a title="233-lda-11" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>12 0.86334562 <a title="233-lda-12" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>13 0.86164981 <a title="233-lda-13" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>14 0.86078143 <a title="233-lda-14" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>15 0.86066633 <a title="233-lda-15" href="./iccv-2013-Detecting_Avocados_to_Zucchinis%3A_What_Have_We_Done%2C_and_Where_Are_We_Going%3F.html">109 iccv-2013-Detecting Avocados to Zucchinis: What Have We Done, and Where Are We Going?</a></p>
<p>16 0.86053532 <a title="233-lda-16" href="./iccv-2013-Distributed_Low-Rank_Subspace_Segmentation.html">122 iccv-2013-Distributed Low-Rank Subspace Segmentation</a></p>
<p>17 0.86050349 <a title="233-lda-17" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>18 0.85966825 <a title="233-lda-18" href="./iccv-2013-Active_Visual_Recognition_with_Expertise_Estimation_in_Crowdsourcing.html">43 iccv-2013-Active Visual Recognition with Expertise Estimation in Crowdsourcing</a></p>
<p>19 0.85834432 <a title="233-lda-19" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>20 0.85815668 <a title="233-lda-20" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
