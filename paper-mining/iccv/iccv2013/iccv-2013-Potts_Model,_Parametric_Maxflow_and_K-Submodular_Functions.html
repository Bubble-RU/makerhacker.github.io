<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>324 iccv-2013-Potts Model, Parametric Maxflow and K-Submodular Functions</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-324" href="#">iccv2013-324</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>324 iccv-2013-Potts Model, Parametric Maxflow and K-Submodular Functions</h1>
<br/><p>Source: <a title="iccv-2013-324-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Gridchyn_Potts_Model_Parametric_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Igor Gridchyn, Vladimir Kolmogorov</p><p>Abstract: The problem of minimizing the Potts energy function frequently occurs in computer vision applications. One way to tackle this NP-hard problem was proposed by Kovtun [20, 21]. It identifies a part of an optimal solution by running k maxflow computations, where k is the number of labels. The number of “labeled” pixels can be significant in some applications, e.g. 50-93% in our tests for stereo. We show how to reduce the runtime to O(log k) maxflow computations (or one parametric maxflow computation). Furthermore, the output of our algorithm allows to speed-up the subsequent alpha expansion for the unlabeled part, or can be used as it is for time-critical applications. To derive our technique, we generalize the algorithm of Felzenszwalb et al. [7] for Tree Metrics. We also show a connection to k-submodular functions from combinatorial optimization, and discuss k-submodular relaxations for general energy functions.</p><p>Reference: <a title="iccv-2013-324-reference" href="../iccv2013_reference/iccv-2013-Potts_Model%2C_Parametric_Maxflow_and_K-Submodular_Functions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Potts model, parametric maxflow and k-submodular functions  Igor Gridchyn IST Austria igor . [sent-1, score-0.419]
</p><p>2 It identifies a part of an optimal solution by running k maxflow computations, where k is the number of labels. [sent-6, score-0.297]
</p><p>3 We show how to reduce the runtime to O(log k) maxflow computations (or one parametric maxflow computation). [sent-10, score-0.799]
</p><p>4 Furthermore, the output of our algorithm allows to speed-up the subsequent alpha expansion for the unlabeled part, or can be used as it is for time-critical applications. [sent-11, score-0.172]
</p><p>5 We also show a connection to k-submodular functions from combinatorial optimization, and discuss k-submodular relaxations for general energy functions. [sent-14, score-0.162]
</p><p>6 [4] who proposed an efficient approximation algorithm for this NP-hard problem called alpha expansion. [sent-18, score-0.119]
</p><p>7 The algorithm of [4] is based on the maxflow algorithm, also known as graph cuts. [sent-19, score-0.34]
</p><p>8 Each iteration involves k maxflow computations, where k is the number of labels. [sent-20, score-0.297]
</p><p>9 The most relevant to us is the method of Kovtun [20, 21] which computes a part of an optimal solution via k maxflow computations. [sent-22, score-0.297]
</p><p>10 We can then fix “labeled” nodes and run the alpha expansion algorithm for the remaining nodes. [sent-23, score-0.244]
</p><p>11 Our main contribution is to improve the efficiency of Kovtun’s method from k maxflow computations to ? [sent-26, score-0.45]
</p><p>12 We may get an improvement even when there are few  labeled nodes: it is reported in [1] that using flow from Kovtun’s computations always speeds up the alpha expansion algorithm for unlabeled nodes. [sent-33, score-0.455]
</p><p>13 Similarly, Kovtun’s approach does not improve on the standard Schlesinger’s LP relaxation of the energy [25]. [sent-40, score-0.139]
</p><p>14 The default version of FastPD for the Potts energy produces the same answer as the alpha expansion algorithm but faster, since it maintains not only primal variables (current solution) but also dual variables (“messages”). [sent-43, score-0.228]
</p><p>15 Intuitively, it allows to reuse flow between different maxflow computations. [sent-44, score-0.361]
</p><p>16 Preliminaries The Potts energy for labeling x ∈ LV is given by  =  f(x) ? [sent-48, score-0.168]
</p><p>17 Consequently, there exists minimizer x∗ ∈ argx m∈iLnVf(x)  ing xyis defined via  such that xi∗ = a for all nodes i∈ V with  yi  = a. [sent-69, score-0.302]
</p><p>18 A naive way to do this is to use k maxflow computations on a graph awyi ttoh |dVo |t hniosd ises t oa undse |E k |m edges, w choemrepku =ta |iLo|n . [sent-71, score-0.493]
</p><p>19 yi =y) a iyf xi a=, a }and} yi r= e ac ho tahe ∈rw Lis ien. [sent-76, score-0.297]
</p><p>20 (3)  where d(·, ·) is a tree metric with respect to a certain tree T: Definition 2. [sent-83, score-0.146]
</p><p>21 ( A3)l are gseets as efo alslsoiwgnse: gi e(nog) h= 1 . [sent-92, score-0.299]
</p><p>22 seen cthtiaotn minimizing )f ias is equivalent to minimizing g(y) over y ∈ {a, o}V. [sent-97, score-0.114]
</p><p>23 For any i ∈ V and a, b ∈ L with a tPhreorep ohsoiltdiosn gi 3(. [sent-100, score-0.299]
</p><p>24 More generally, we say that function gi : D → R is Tconvex if for any pair of edges {a, b}, {b, c} ∈: D DE w→ith R a s= T c tchoenrvee xho ilfd fos d(a, c)gi (b) ≤ d(b, c)gi (a)  + d(a, b)gi (c)  (4)  Clearly, terms gi contructed above are T-convex. [sent-106, score-0.628]
</p><p>25 We will prove the following result for an arbitrary tree T and function g with T-convex unary terms gi. [sent-107, score-0.203]
</p><p>26 For labeling x h∈e oDreVm define binary l}ab beelin agn nx e[adbg] e∈ i n{a E, . [sent-111, score-0.112]
</p><p>27 Note, part (a) and a repeated application of Theorem 1 give that any minimizer x ∈ DV of g is a partially optimal labeling f aonry yf m, i . [sent-116, score-0.232]
</p><p>28 m fiz hear sx a ∈m Dinimizer x∗ ∈ LV such that xi∗ = xi for all iwith xi o. [sent-118, score-0.154]
</p><p>29 sider the case of an arbitrary tree T, and present an efficient algorithm for minimizing function g with T-convex unary terms. [sent-120, score-0.218]
</p><p>30 They considered the case when unary functions gi (·) are given by gi (xi) = λid(xi , ci) where λi ≥ 0 and ci is( a constant node in D. [sent-127, score-0.802]
</p><p>31 m [a1x5i]m suhmow wfleodw t computations on graphs wmiinthi mOiz(|eVd |) v ano |dDes| manadx iOmu(|Em|) fl edges. [sent-129, score-0.153]
</p><p>32 UOs(ilnogg | aD d|)iv midaex-falnodw-  computations (plus 7O] (i m|Dp| loovge d|D t|h)i st tiome O f(olorg bookkeeping). [sent-131, score-0.153]
</p><p>33 {i,j} λij |xj −xi | with convex  +  k}V  terms gi over x? [sent-137, score-0.299]
</p><p>34 with T-convex unary terms, and present a self-contained proof of  correctness. [sent-158, score-0.157]
</p><p>35 1  The main step of the algorithm is computing a minimizer y ∈ arg min{g(y) | y ∈ {a, b}V} for some edge {mai,z eb}r ∈y E∈ (t ahrigs can {bge( dyo)n |e yvia ∈ a {ma,abxf}low} algorithm). [sent-159, score-0.245]
</p><p>36 Algorithm 1 SPLIT(g) Input: function g : DV → R specified by graph (V, E), tIrnepe Tt: f=u (cDti,o En, gd) :, unary te Rrm ssp gi : Dd →y g rRap ahn (dV edge weights λij Output: labeling x ∈ arg min{g(x) | x ∈ DV} 1:if D = {a} return (a,. [sent-164, score-0.682]
</p><p>37 a ignec(dx) fr =m gg( xb¯y) wfixhienrge x¯i = no xi fso irn ni V∈ −Vc Vand x¯i = c for i∈ V − Vc let xc := SPLITf(orgc i) end for merge labelings xb into labeling x, return x  xa,  gc  Note that function in line 7 is defined on the subgraph of (V, E) induced by Vc. [sent-169, score-0.388]
</p><p>38 In our view, the new proof shows more clearly why the extension to T-convex unary terms is possible. [sent-172, score-0.157]
</p><p>39 e G Tiv eisn nm nooddiefi ead ∈ as Dfol alnodws th: (i) aartdidnew noof idtse nbe ∈/ Dhb;o (ii) Nad ∪d new edge {a, b}; (iii) keep nodes c ∈ N as neighbors ∈o fD a, ibi)ut a dmda knee wno eddegse ec ? [sent-180, score-0.13]
</p><p>40 The following theorem implies that the algorithm is correct; its proof is given in section 3. [sent-185, score-0.167]
</p><p>41 If in line 9 is a minimizer of over DVcc for each c ∈ {a, b} then labeling x in line 10 is a movienirm Dizer foofr g e over cD ∈V {. [sent-188, score-0.232]
</p><p>42 To deal with this issue, [7] proposed to expand tree T (and modify the input function accordingly) so that the new tree T? [sent-194, score-0.146]
</p><p>43 (5)  where we assume that gi (b) = gi (a), and ui ∈ R is chosen in such a way that function gi? [sent-219, score-0.733]
</p><p>44 To summarize, we showed that the SPLI T algorithm remains correct if we replace line 2 with the tree modification step described above, and in line 3 compute y ∈ arg omnin s{tegp? [sent-260, score-0.14]
</p><p>45 n dA ilnso, l nine l3 in eco m10p we n yee ∈d ator gcomnivne{rtg labeling x∈b {toa ,(bx}b)b? [sent-262, score-0.112]
</p><p>46 a Ablesfoor,e i merging w witeh Selecting ui It remains to show that value ui for node i∈ V can be set in such a way that function (5) is Tf? [sent-264, score-0.339]
</p><p>47 i)(a)  There holds uimin ≤ uimax, and for any ui ∈ [uimin, uimax] and ? [sent-272, score-0.231]
</p><p>48 Proof of theorems 4 and 5 The proof is based on the theorem below. [sent-279, score-0.133]
</p><p>49 In part (b) we exploit the fact that unary functions gi are T-convex, and make use of a well-known result about the parametric maxflow problem [8]. [sent-282, score-0.783]
</p><p>50 Part (a) It is straightforward to check that the following holds for nodes i∈ V and edges {i, j} ∈ E respectively:  gi(xi) λijd(xi,xj)  = ? [sent-302, score-0.14]
</p><p>51 This is a well-known fact abou∨t t hye parametric maxflow problem ([8], Lemma 2. [sent-353, score-0.349]
</p><p>52 = (0, 1)  We say that a family of binary labelings y = (yab ∈ {a, b}V | {a, b} ∈ E}) is consistent if there exists labeling x ,∈b }DV| s{ua,chb }th ∈at Ex}[)ab] i = co ynasibs feonrt a ifll {haer,e eb }e i∈s Es . [sent-387, score-0.25]
</p><p>53 Family y is consistent iff for every for any pair of edges {a, b}, {b, c} ∈ E with a c and any node pi a∈i rV o fth eedrgee sho {ldas, b(}y,ia{b,b ,yicbc}) E(a w, cit)h. [sent-393, score-0.165]
</p><p>54 Let us fix a node i ∈ V , and denote yi = (yaib | {a, b} ∈ E}). [sent-395, score-0.179]
</p><p>55 Clearly, ∈there is one-to-one corrre-  spondence }be∈t we Een} possible labelings yi aen-tdo -oornieent caotriorrnesof tree T. [sent-396, score-0.288]
</p><p>56 Namely, to each yi we associate a directed graph G[yi] = (D, with ={(a, b) | {a, b} ∈E, yiab=b}. [sent-397, score-0.153]
</p><p>57 t,hbe}re∈ eEx,isyts xi ∈ D with yaib = for all {a, b} ∈ E) iff graph G[yi] has exactly one sink, i. [sent-400, score-0.191]
</p><p>58 This is equivalent to the condition that each node a ∈ D has at imso esqtu one outgoing edge iitni oGn[ tyhia] . [sent-403, score-0.157]
</p><p>59 Consider the following algorithm for constructing a family of binary labelings y. [sent-409, score-0.138]
</p><p>60 Initially, we set y = (yab) where yab = y is the labeling chosen in Theorem 4(b). [sent-410, score-0.208]
</p><p>61 By Theorem 7(b), the constructed family of binary labelings y satisfies the following: ∈ arg min{g(y) | y ∈ {a? [sent-446, score-0.205]
</p><p>62 Tyh ieso creomns i7st(aen) implies ∈tha Dt x is a minimizer of g. [sent-452, score-0.154]
</p><p>63 Implementation details In this section we sketch implementation details of Algorithm 1 applied to the function constructed in section 2 (so T is a star graph with nodes D = L ∪ {o}). [sent-456, score-0.115]
</p><p>64 Thus, computations can ebne Dde=s cri {bae,do i}n oterr msosm oef a a binary htruese, whose nodes correspond to subsets oflabels A ⊆ L (Fig. [sent-460, score-0.225]
</p><p>65 Therefore, these maxflow computations can be treated as a single maxflow on the graph of the original size. [sent-467, score-0.79]
</p><p>66 m For each A ∈ Ω we set up a graph with the set of nodes VAF ∪o r{ es,a ct}h aAnd ∈ th Ωe wcuet sfuetn uctpio an g fA(S∪{s},T∪{t})=  ? [sent-472, score-0.115]
</p><p>67 gi(a),−gi(o)  +a m∈Ainrgi(a)]  (9)  where we use the current value of gi (o) (it is zero initially and then gets decreased). [sent-489, score-0.299]
</p><p>68 For a leaf A = {a} we use a adinfdfer theennt intepretation: s corresponds fto A Alab =el a a}nd w te corresponds to label o, therefore uiA = gi (o) − gi (a). [sent-490, score-0.63]
</p><p>69 We perform all maxflow computations on a single graph. [sent-491, score-0.45]
</p><p>70 We maintain values ui for nodes i∈ V that give the current cut functions encoded by tnhoed reess i d ∈ua Vl graph. [sent-493, score-0.239]
</p><p>71 Avef ttehre computing tm fuanxcfltoiown sa te a noodne-dle bayf node A the residual graph is modified as follows. [sent-494, score-0.112]
</p><p>72 Set ui := ui − λij and uj := uj λij ; this simulates pushing flow λij along the path t → j → i→ s. [sent-496, score-0.334]
</p><p>73 Now we need to set unary costs for maxflow computations at the children A? [sent-501, score-0.601]
</p><p>74 Suppose that node i ∈ V ended up at a leaf {a} ∈ Ω. [sent-509, score-0.129]
</p><p>75 2  uubtiArac  aciA}  caAfi {  ciA  2It can be shown that we only need to know a∗ ∈ arg mina∈L gi (a), gi (a∗ ) and gi (o) for that. [sent-517, score-0.964]
</p><p>76 22332244  Such monotonicity implies that computations at non-leaf nodes fall into the framework of parametric maxflow of Gallo et al. [sent-520, score-0.608]
</p><p>77 As shown in [8], all computations can be done with the same worst-case complexity as a single maxflow computation. [sent-522, score-0.45]
</p><p>78 An important question is how to obtain an optimal flow correspoding to this computation; as reported in [1], using this flow speeds up the alpha expansion algorithm. [sent-528, score-0.337]
</p><p>79 It suffices to specify the flow ξij for each arc (i → j) with {i, j} ∈ E (the flow from the source cahn da tco tihe → →sin jk) can hth {ei,n b}e easily computed). [sent-529, score-0.165]
</p><p>80 For each node i∈ V we also store leaf Ai ∈ Ω at which node i eenacdhed n up. [sent-535, score-0.17]
</p><p>81 It was shown that if f is a quadratic pseudo-Boolean function then the tightest bisubmodular relaxation is equivalent to the roof duality relaxation [10]. [sent-580, score-0.395]
</p><p>82 It was also proved that bisubmodular relaxations possess the persistency, or partial optimality property. [sent-581, score-0.23]
</p><p>83 Let g be a k-submodular relaxation of f and y∗ ∈ DV be a minimizer of g. [sent-584, score-0.203]
</p><p>84 Function f has a minimizer x∈∗ D∈ LV such that xi∗ = yi∗ for all i ∈ V with yi∗ ∈ L. [sent-585, score-0.12]
</p><p>85 Labeling x L∗, aisn a minimizer of f since f(x∗)  = g((x ? [sent-603, score-0.12]
</p><p>86 y∗) ≤ g(x)  = f(x)  Thus, k-submodular relaxations can be viewed as a generalization of the roof duality relaxation to the case of multiple labels. [sent-606, score-0.26]
</p><p>87 (This was proved by showing the tightness of the Basic LP relaxation (BLP); when g is a sum of unary and pairwise terms, BLP is equivalent to the standard Schlesinger’s LP [27]. [sent-608, score-0.216]
</p><p>88 j}∈E  (11)  where g˜i is a k-submodular relaxation of fi and d is the tree metric used in section 2. [sent-616, score-0.243]
</p><p>89 22332255  The proposition below shows that minimizing ˜g yields the same or fewer number of labeled nodes compared to the Kovtun’s approach. [sent-618, score-0.23]
</p><p>90 Although a k-submodular relaxation of the Potts energy turns out to be worse than Kovtun’s approach, there are clear similarities between the two (e. [sent-624, score-0.139]
</p><p>91 As a by-product, k-sub Kovtun produces a labeling which we call a Kovtun labeling: pixel iis assigned the label a where it ended up, as described in Sec. [sent-637, score-0.14]
</p><p>92 Matching costs The number of labeled pixels strongly depends on the method for computing matching costs fi(·) and on the regularization parameter λ (which is the same for all edges). [sent-640, score-0.125]
</p><p>93 It is reported in [1] that the flow from Kovtun’s computations speeds the alpha-expansion algorithm. [sent-661, score-0.254]
</p><p>94 However, we observed that initializing FastPD with the Kovtun’s labeling speeds it up compared to the “? [sent-663, score-0.149]
</p><p>95 Quality ofthe Kovtun’s labeling We found that in the majority of cases Kovtun’s labeling actually has a lower error rate compared to the alpha-expansion solution (even though the energy of the latter is better) - see Fig. [sent-667, score-0.28]
</p><p>96 Not surprisingly, Kovtun’s labeling is more reliable in the labeled part, i. [sent-671, score-0.141]
</p><p>97 If the number of persistent pixels is low for a given application then one could use the cost aggregation trick to get more discriminative unary functions; as we saw for stereo, this only improves the accuracy. [sent-680, score-0.131]
</p><p>98 For time-critical applications one could potentially skip the second phase and use the Kovtun’s labeling as the final output. [sent-681, score-0.145]
</p><p>99 On the theoretical side, we introduced several concepts (such as k-submodular relaxations) that may turn out to be 5In this method we set xi ∈ arg mina fi(a). [sent-682, score-0.144]
</p><p>100 Approximate labeling via graph cuts based on linear programming. [sent-805, score-0.155]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kovtun', 0.538), ('gi', 0.299), ('maxflow', 0.297), ('computations', 0.153), ('ui', 0.135), ('potts', 0.132), ('minimizer', 0.12), ('alpha', 0.119), ('dv', 0.118), ('labeling', 0.112), ('yi', 0.11), ('labelings', 0.105), ('unary', 0.103), ('ijd', 0.102), ('ij', 0.101), ('bisubmodular', 0.096), ('yab', 0.096), ('proposition', 0.087), ('fi', 0.087), ('relaxation', 0.083), ('theorem', 0.079), ('fastpd', 0.077), ('xi', 0.077), ('const', 0.074), ('relaxations', 0.074), ('tree', 0.073), ('nodes', 0.072), ('node', 0.069), ('arg', 0.067), ('flow', 0.064), ('uia', 0.063), ('lv', 0.062), ('ssd', 0.062), ('edge', 0.058), ('iab', 0.058), ('uimax', 0.058), ('uimin', 0.058), ('ab', 0.057), ('energy', 0.056), ('proof', 0.054), ('expansion', 0.053), ('komodakis', 0.052), ('roof', 0.052), ('parametric', 0.052), ('ivn', 0.051), ('duality', 0.051), ('vc', 0.049), ('costs', 0.048), ('stereo', 0.046), ('alahari', 0.045), ('db', 0.044), ('graph', 0.043), ('minimizing', 0.042), ('blp', 0.038), ('coarea', 0.038), ('cwi', 0.038), ('gridchyn', 0.038), ('igor', 0.038), ('persistency', 0.038), ('schlesinger', 0.038), ('spli', 0.038), ('thapper', 0.038), ('toa', 0.038), ('vui', 0.038), ('yaib', 0.038), ('ybc', 0.038), ('holds', 0.038), ('speeds', 0.037), ('da', 0.037), ('dg', 0.035), ('implies', 0.034), ('uisn', 0.034), ('abr', 0.034), ('sfeotr', 0.034), ('shekhovtsov', 0.034), ('deg', 0.034), ('family', 0.033), ('phase', 0.033), ('fth', 0.033), ('iff', 0.033), ('leaf', 0.032), ('xb', 0.032), ('gc', 0.032), ('functions', 0.032), ('optimality', 0.032), ('bth', 0.032), ('gallo', 0.032), ('yj', 0.031), ('min', 0.031), ('let', 0.03), ('edges', 0.03), ('xa', 0.03), ('equivalent', 0.03), ('cia', 0.03), ('minimizers', 0.03), ('labeled', 0.029), ('ended', 0.028), ('persistent', 0.028), ('partial', 0.028), ('prove', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000017 <a title="324-tfidf-1" href="./iccv-2013-Potts_Model%2C_Parametric_Maxflow_and_K-Submodular_Functions.html">324 iccv-2013-Potts Model, Parametric Maxflow and K-Submodular Functions</a></p>
<p>Author: Igor Gridchyn, Vladimir Kolmogorov</p><p>Abstract: The problem of minimizing the Potts energy function frequently occurs in computer vision applications. One way to tackle this NP-hard problem was proposed by Kovtun [20, 21]. It identifies a part of an optimal solution by running k maxflow computations, where k is the number of labels. The number of “labeled” pixels can be significant in some applications, e.g. 50-93% in our tests for stereo. We show how to reduce the runtime to O(log k) maxflow computations (or one parametric maxflow computation). Furthermore, the output of our algorithm allows to speed-up the subsequent alpha expansion for the unlabeled part, or can be used as it is for time-critical applications. To derive our technique, we generalize the algorithm of Felzenszwalb et al. [7] for Tree Metrics. We also show a connection to k-submodular functions from combinatorial optimization, and discuss k-submodular relaxations for general energy functions.</p><p>2 0.10994345 <a title="324-tfidf-2" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>Author: Minsu Cho, Karteek Alahari, Jean Ponce</p><p>Abstract: Many tasks in computer vision are formulated as graph matching problems. Despite the NP-hard nature of the problem, fast and accurate approximations have led to significant progress in a wide range of applications. Learning graph models from observed data, however, still remains a challenging issue. This paper presents an effective scheme to parameterize a graph model, and learn its structural attributes for visual object matching. For this, we propose a graph representation with histogram-based attributes, and optimize them to increase the matching accuracy. Experimental evaluations on synthetic and real image datasets demonstrate the effectiveness of our approach, and show significant improvement in matching accuracy over graphs with pre-defined structures.</p><p>3 0.10746416 <a title="324-tfidf-3" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>Author: Christoph Straehle, Ullrich Koethe, Fred A. Hamprecht</p><p>Abstract: We propose a scheme that allows to partition an image into a previously unknown number of segments, using only minimal supervision in terms of a few must-link and cannotlink annotations. We make no use of regional data terms, learning instead what constitutes a likely boundary between segments. Since boundaries are only implicitly specified through cannot-link constraints, this is a hard and nonconvex latent variable problem. We address this problem in a greedy fashion using a randomized decision tree on features associated with interpixel edges. We use a structured purity criterion during tree construction and also show how a backtracking strategy can be used to prevent the greedy search from ending up in poor local optima. The proposed strategy is compared with prior art on natural images.</p><p>4 0.10459989 <a title="324-tfidf-4" href="./iccv-2013-Partial_Enumeration_and_Curvature_Regularization.html">309 iccv-2013-Partial Enumeration and Curvature Regularization</a></p>
<p>Author: Carl Olsson, Johannes Ulén, Yuri Boykov, Vladimir Kolmogorov</p><p>Abstract: Energies with high-order non-submodular interactions have been shown to be very useful in vision due to their high modeling power. Optimization of such energies, however, is generally NP-hard. A naive approach that works for small problem instances is exhaustive search, that is, enumeration of all possible labelings of the underlying graph. We propose a general minimization approach for large graphs based on enumeration of labelings of certain small patches. This partial enumeration technique reduces complex highorder energy formulations to pairwise Constraint Satisfaction Problems with unary costs (uCSP), which can be efficiently solved using standard methods like TRW-S. Our approach outperforms a number of existing state-of-the-art algorithms on well known difficult problems (e.g. curvature regularization, stereo, deconvolution); it gives near global minimum and better speed. Our main application of interest is curvature regularization. In the context of segmentation, our partial enumeration technique allows to evaluate curvature directly on small patches using a novel integral geometry approach. 1</p><p>5 0.096419878 <a title="324-tfidf-5" href="./iccv-2013-Total_Variation_Regularization_for_Functions_with_Values_in_a_Manifold.html">421 iccv-2013-Total Variation Regularization for Functions with Values in a Manifold</a></p>
<p>Author: Jan Lellmann, Evgeny Strekalovskiy, Sabrina Koetter, Daniel Cremers</p><p>Abstract: While total variation is among the most popular regularizers for variational problems, its extension to functions with values in a manifold is an open problem. In this paper, we propose the first algorithm to solve such problems which applies to arbitrary Riemannian manifolds. The key idea is to reformulate the variational problem as a multilabel optimization problem with an infinite number of labels. This leads to a hard optimization problem which can be approximately solved using convex relaxation techniques. The framework can be easily adapted to different manifolds including spheres and three-dimensional rotations, and allows to obtain accurate solutions even with a relatively coarse discretization. With numerous examples we demonstrate that the proposed framework can be applied to variational models that incorporate chromaticity values, normal fields, or camera trajectories.</p><p>6 0.093915962 <a title="324-tfidf-6" href="./iccv-2013-Fix_Structured_Learning_of_2013_ICCV_paper_k2opt.pdf.html">171 iccv-2013-Fix Structured Learning of 2013 ICCV paper k2opt.pdf</a></p>
<p>7 0.088947728 <a title="324-tfidf-7" href="./iccv-2013-New_Graph_Structured_Sparsity_Model_for_Multi-label_Image_Annotations.html">290 iccv-2013-New Graph Structured Sparsity Model for Multi-label Image Annotations</a></p>
<p>8 0.086099952 <a title="324-tfidf-8" href="./iccv-2013-A_Learning-Based_Approach_to_Reduce_JPEG_Artifacts_in_Image_Matting.html">19 iccv-2013-A Learning-Based Approach to Reduce JPEG Artifacts in Image Matting</a></p>
<p>9 0.086080857 <a title="324-tfidf-9" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>10 0.084252544 <a title="324-tfidf-10" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>11 0.082111664 <a title="324-tfidf-11" href="./iccv-2013-Learning_Graph_Matching%3A_Oriented_to_Category_Modeling_from_Cluttered_Scenes.html">237 iccv-2013-Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes</a></p>
<p>12 0.081914112 <a title="324-tfidf-12" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>13 0.080347344 <a title="324-tfidf-13" href="./iccv-2013-Group_Norm_for_Learning_Structured_SVMs_with_Unstructured_Latent_Variables.html">187 iccv-2013-Group Norm for Learning Structured SVMs with Unstructured Latent Variables</a></p>
<p>14 0.079025023 <a title="324-tfidf-14" href="./iccv-2013-Tree_Shape_Priors_with_Connectivity_Constraints_Using_Convex_Relaxation_on_General_Graphs.html">429 iccv-2013-Tree Shape Priors with Connectivity Constraints Using Convex Relaxation on General Graphs</a></p>
<p>15 0.078772813 <a title="324-tfidf-15" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>16 0.077766143 <a title="324-tfidf-16" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>17 0.077501856 <a title="324-tfidf-17" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>18 0.077416733 <a title="324-tfidf-18" href="./iccv-2013-Active_MAP_Inference_in_CRFs_for_Efficient_Semantic_Segmentation.html">42 iccv-2013-Active MAP Inference in CRFs for Efficient Semantic Segmentation</a></p>
<p>19 0.073613115 <a title="324-tfidf-19" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>20 0.072004445 <a title="324-tfidf-20" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.163), (1, -0.015), (2, -0.034), (3, -0.017), (4, 0.002), (5, 0.057), (6, -0.061), (7, 0.027), (8, 0.045), (9, -0.109), (10, -0.073), (11, 0.015), (12, 0.014), (13, 0.036), (14, 0.089), (15, 0.095), (16, -0.044), (17, -0.061), (18, -0.022), (19, 0.039), (20, 0.012), (21, -0.01), (22, -0.037), (23, -0.014), (24, 0.079), (25, 0.013), (26, 0.031), (27, -0.047), (28, 0.057), (29, 0.017), (30, -0.019), (31, 0.005), (32, 0.035), (33, 0.019), (34, 0.05), (35, 0.009), (36, -0.064), (37, -0.003), (38, 0.012), (39, -0.019), (40, -0.008), (41, -0.064), (42, 0.023), (43, -0.066), (44, 0.033), (45, 0.052), (46, 0.065), (47, -0.024), (48, 0.01), (49, -0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95454884 <a title="324-lsi-1" href="./iccv-2013-Potts_Model%2C_Parametric_Maxflow_and_K-Submodular_Functions.html">324 iccv-2013-Potts Model, Parametric Maxflow and K-Submodular Functions</a></p>
<p>Author: Igor Gridchyn, Vladimir Kolmogorov</p><p>Abstract: The problem of minimizing the Potts energy function frequently occurs in computer vision applications. One way to tackle this NP-hard problem was proposed by Kovtun [20, 21]. It identifies a part of an optimal solution by running k maxflow computations, where k is the number of labels. The number of “labeled” pixels can be significant in some applications, e.g. 50-93% in our tests for stereo. We show how to reduce the runtime to O(log k) maxflow computations (or one parametric maxflow computation). Furthermore, the output of our algorithm allows to speed-up the subsequent alpha expansion for the unlabeled part, or can be used as it is for time-critical applications. To derive our technique, we generalize the algorithm of Felzenszwalb et al. [7] for Tree Metrics. We also show a connection to k-submodular functions from combinatorial optimization, and discuss k-submodular relaxations for general energy functions.</p><p>2 0.77228791 <a title="324-lsi-2" href="./iccv-2013-Fix_Structured_Learning_of_2013_ICCV_paper_k2opt.pdf.html">171 iccv-2013-Fix Structured Learning of 2013 ICCV paper k2opt.pdf</a></p>
<p>Author: empty-author</p><p>Abstract: Submodular functions can be exactly minimized in polynomial time, and the special case that graph cuts solve with max flow [19] has had significant impact in computer vision [5, 21, 28]. In this paper we address the important class of sum-of-submodular (SoS) functions [2, 18], which can be efficiently minimized via a variant of max flow called submodular flow [6]. SoS functions can naturally express higher order priors involving, e.g., local image patches; however, it is difficult to fully exploit their expressive power because they have so many parameters. Rather than trying to formulate existing higher order priors as an SoS function, we take a discriminative learning approach, effectively searching the space of SoS functions for a higher order prior that performs well on our training set. We adopt a structural SVM approach [15, 34] and formulate the training problem in terms of quadratic programming; as a result we can efficiently search the space of SoS priors via an extended cutting-plane algorithm. We also show how the state-of-the-art max flow method for vision problems [11] can be modified to efficiently solve the submodular flow problem. Experimental comparisons are made against the OpenCVimplementation ofthe GrabCut interactive seg- mentation technique [28], which uses hand-tuned parameters instead of machine learning. On a standard dataset [12] our method learns higher order priors with hundreds of parameter values, and produces significantly better segmentations. While our focus is on binary labeling problems, we show that our techniques can be naturally generalized to handle more than two labels.</p><p>3 0.6794315 <a title="324-lsi-3" href="./iccv-2013-Tree_Shape_Priors_with_Connectivity_Constraints_Using_Convex_Relaxation_on_General_Graphs.html">429 iccv-2013-Tree Shape Priors with Connectivity Constraints Using Convex Relaxation on General Graphs</a></p>
<p>Author: Jan Stühmer, Peter Schröder, Daniel Cremers</p><p>Abstract: We propose a novel method to include a connectivity prior into image segmentation that is based on a binary labeling of a directed graph, in this case a geodesic shortest path tree. Specifically we make two contributions: First, we construct a geodesic shortest path tree with a distance measure that is related to the image data and the bending energy of each path in the tree. Second, we include a connectivity prior in our segmentation model, that allows to segment not only a single elongated structure, but instead a whole connected branching tree. Because both our segmentation model and the connectivity constraint are convex, a global optimal solution can be found. To this end, we generalize a recent primal-dual algorithm for continuous convex optimization to an arbitrary graph structure. To validate our method we present results on data from medical imaging in angiography and retinal blood vessel segmentation.</p><p>4 0.63551706 <a title="324-lsi-4" href="./iccv-2013-Active_MAP_Inference_in_CRFs_for_Efficient_Semantic_Segmentation.html">42 iccv-2013-Active MAP Inference in CRFs for Efficient Semantic Segmentation</a></p>
<p>Author: Gemma Roig, Xavier Boix, Roderick De_Nijs, Sebastian Ramos, Koljia Kuhnlenz, Luc Van_Gool</p><p>Abstract: Most MAP inference algorithms for CRFs optimize an energy function knowing all the potentials. In this paper, we focus on CRFs where the computational cost of instantiating the potentials is orders of magnitude higher than MAP inference. This is often the case in semantic image segmentation, where most potentials are instantiated by slow classifiers fed with costly features. We introduce Active MAP inference 1) to on-the-fly select a subset of potentials to be instantiated in the energy function, leaving the rest of the parameters of the potentials unknown, and 2) to estimate the MAP labeling from such incomplete energy function. Results for semantic segmentation benchmarks, namely PASCAL VOC 2010 [5] and MSRC-21 [19], show that Active MAP inference achieves similar levels of accuracy but with major efficiency gains.</p><p>5 0.62136108 <a title="324-lsi-5" href="./iccv-2013-New_Graph_Structured_Sparsity_Model_for_Multi-label_Image_Annotations.html">290 iccv-2013-New Graph Structured Sparsity Model for Multi-label Image Annotations</a></p>
<p>Author: Xiao Cai, Feiping Nie, Weidong Cai, Heng Huang</p><p>Abstract: In multi-label image annotations, because each image is associated to multiple categories, the semantic terms (label classes) are not mutually exclusive. Previous research showed that such label correlations can largely boost the annotation accuracy. However, all existing methods only directly apply the label correlation matrix to enhance the label inference and assignment without further learning the structural information among classes. In this paper, we model the label correlations using the relational graph, and propose a novel graph structured sparse learning model to incorporate the topological constraints of relation graph in multi-label classifications. As a result, our new method will capture and utilize the hidden class structures in relational graph to improve the annotation results. In proposed objective, a large number of structured sparsity-inducing norms are utilized, thus the optimization becomes difficult. To solve this problem, we derive an efficient optimization algorithm with proved convergence. We perform extensive experiments on six multi-label image annotation benchmark data sets. In all empirical results, our new method shows better annotation results than the state-of-the-art approaches.</p><p>6 0.61637115 <a title="324-lsi-6" href="./iccv-2013-Learning_CRFs_for_Image_Parsing_with_Adaptive_Subgradient_Descent.html">234 iccv-2013-Learning CRFs for Image Parsing with Adaptive Subgradient Descent</a></p>
<p>7 0.60729671 <a title="324-lsi-7" href="./iccv-2013-Total_Variation_Regularization_for_Functions_with_Values_in_a_Manifold.html">421 iccv-2013-Total Variation Regularization for Functions with Values in a Manifold</a></p>
<p>8 0.60126895 <a title="324-lsi-8" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>9 0.59354204 <a title="324-lsi-9" href="./iccv-2013-Higher_Order_Matching_for_Consistent_Multiple_Target_Tracking.html">200 iccv-2013-Higher Order Matching for Consistent Multiple Target Tracking</a></p>
<p>10 0.58707511 <a title="324-lsi-10" href="./iccv-2013-Discriminative_Label_Propagation_for_Multi-object_Tracking_with_Sporadic_Appearance_Features.html">120 iccv-2013-Discriminative Label Propagation for Multi-object Tracking with Sporadic Appearance Features</a></p>
<p>11 0.57876658 <a title="324-lsi-11" href="./iccv-2013-SGTD%3A_Structure_Gradient_and_Texture_Decorrelating_Regularization_for_Image_Decomposition.html">364 iccv-2013-SGTD: Structure Gradient and Texture Decorrelating Regularization for Image Decomposition</a></p>
<p>12 0.5769363 <a title="324-lsi-12" href="./iccv-2013-Partial_Enumeration_and_Curvature_Regularization.html">309 iccv-2013-Partial Enumeration and Curvature Regularization</a></p>
<p>13 0.57542765 <a title="324-lsi-13" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>14 0.55584818 <a title="324-lsi-14" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>15 0.54725039 <a title="324-lsi-15" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<p>16 0.54706573 <a title="324-lsi-16" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>17 0.54686552 <a title="324-lsi-17" href="./iccv-2013-Joint_Optimization_for_Consistent_Multiple_Graph_Matching.html">224 iccv-2013-Joint Optimization for Consistent Multiple Graph Matching</a></p>
<p>18 0.54518574 <a title="324-lsi-18" href="./iccv-2013-Finding_Causal_Interactions_in_Video_Sequences.html">167 iccv-2013-Finding Causal Interactions in Video Sequences</a></p>
<p>19 0.54286838 <a title="324-lsi-19" href="./iccv-2013-Example-Based_Facade_Texture_Synthesis.html">148 iccv-2013-Example-Based Facade Texture Synthesis</a></p>
<p>20 0.52599043 <a title="324-lsi-20" href="./iccv-2013-Elastic_Net_Constraints_for_Shape_Matching.html">140 iccv-2013-Elastic Net Constraints for Shape Matching</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.067), (7, 0.024), (13, 0.014), (16, 0.018), (26, 0.078), (27, 0.029), (31, 0.058), (40, 0.021), (42, 0.111), (48, 0.023), (64, 0.032), (73, 0.029), (85, 0.212), (89, 0.154), (95, 0.011), (98, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81881976 <a title="324-lda-1" href="./iccv-2013-Potts_Model%2C_Parametric_Maxflow_and_K-Submodular_Functions.html">324 iccv-2013-Potts Model, Parametric Maxflow and K-Submodular Functions</a></p>
<p>Author: Igor Gridchyn, Vladimir Kolmogorov</p><p>Abstract: The problem of minimizing the Potts energy function frequently occurs in computer vision applications. One way to tackle this NP-hard problem was proposed by Kovtun [20, 21]. It identifies a part of an optimal solution by running k maxflow computations, where k is the number of labels. The number of “labeled” pixels can be significant in some applications, e.g. 50-93% in our tests for stereo. We show how to reduce the runtime to O(log k) maxflow computations (or one parametric maxflow computation). Furthermore, the output of our algorithm allows to speed-up the subsequent alpha expansion for the unlabeled part, or can be used as it is for time-critical applications. To derive our technique, we generalize the algorithm of Felzenszwalb et al. [7] for Tree Metrics. We also show a connection to k-submodular functions from combinatorial optimization, and discuss k-submodular relaxations for general energy functions.</p><p>2 0.77042019 <a title="324-lda-2" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>Author: Lingqiao Liu, Lei Wang</p><p>Abstract: To achieve a good trade-off between recognition accuracy and computational efficiency, it is often needed to reduce high-dimensional visual data to medium-dimensional ones. For this task, even applying a simple full-matrixbased linear projection causes significant computation and memory use. When the number of visual data is large, how to efficiently learn such a projection could even become a problem. The recent feature merging approach offers an efficient way to reduce the dimensionality, which only requires a single scan of features to perform reduction. However, existing merging algorithms do not scale well with highdimensional data, especially in the unsupervised case. To address this problem, we formulate unsupervised feature merging as a PCA problem imposed with a special structure constraint. By exploiting its connection with kmeans, we transform this constrained PCA problem into a feature clustering problem. Moreover, we employ the hashing technique to improve its scalability. These produce a scalable feature merging algorithm for our dimensional- ity reduction task. In addition, we develop an extension of this method by leveraging the neighborhood structure in the data to further improve dimensionality reduction performance. In further, we explore the incorporation of bipolar merging a variant of merging function which allows the subtraction operation into our algorithms. Through three applications in visual recognition, we demonstrate that our methods can not only achieve good dimensionality reduction performance with little computational cost but also help to create more powerful representation at both image level and local feature level. – –</p><p>3 0.75152701 <a title="324-lda-3" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>Author: Vidit Jain, Sachin Sudhakar Farfade</p><p>Abstract: Classification cascades have been very effective for object detection. Such a cascade fails to perform well in data domains with variations in appearances that may not be captured in the training examples. This limited generalization severely restricts the domains for which they can be used effectively. A common approach to address this limitation is to train a new cascade of classifiers from scratch for each of the new domains. Building separate detectors for each of the different domains requires huge annotation and computational effort, making it not scalable to a large number of data domains. Here we present an algorithm for quickly adapting a pre-trained cascade of classifiers using a small number oflabeledpositive instancesfrom a different yet similar data domain. In our experiments with images of human babies and human-like characters from movies, we demonstrate that the adapted cascade significantly outperforms both of the original cascade and the one trained from scratch using the given training examples. –</p><p>4 0.7215426 <a title="324-lda-4" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>Author: Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, Philip S. Yu</p><p>Abstract: Transfer learning is established as an effective technology in computer visionfor leveraging rich labeled data in the source domain to build an accurate classifier for the target domain. However, most prior methods have not simultaneously reduced the difference in both the marginal distribution and conditional distribution between domains. In this paper, we put forward a novel transfer learning approach, referred to as Joint Distribution Adaptation (JDA). Specifically, JDA aims to jointly adapt both the marginal distribution and conditional distribution in a principled dimensionality reduction procedure, and construct new feature representation that is effective and robustfor substantial distribution difference. Extensive experiments verify that JDA can significantly outperform several state-of-the-art methods on four types of cross-domain image classification problems.</p><p>5 0.71419507 <a title="324-lda-5" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>Author: Bo Wang, Zhuowen Tu, John K. Tsotsos</p><p>Abstract: In graph-based semi-supervised learning approaches, the classification rate is highly dependent on the size of the availabel labeled data, as well as the accuracy of the similarity measures. Here, we propose a semi-supervised multi-class/multi-label classification scheme, dynamic label propagation (DLP), which performs transductive learning through propagation in a dynamic process. Existing semi-supervised classification methods often have difficulty in dealing with multi-class/multi-label problems due to the lack in consideration of label correlation; our algorithm instead emphasizes dynamic metric fusion with label information. Significant improvement over the state-of-the-art methods is observed on benchmark datasets for both multiclass and multi-label tasks.</p><p>6 0.71352851 <a title="324-lda-6" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>7 0.71344572 <a title="324-lda-7" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>8 0.71245998 <a title="324-lda-8" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>9 0.71154529 <a title="324-lda-9" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>10 0.71146882 <a title="324-lda-10" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>11 0.71135283 <a title="324-lda-11" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>12 0.71135056 <a title="324-lda-12" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>13 0.71026242 <a title="324-lda-13" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>14 0.71019274 <a title="324-lda-14" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>15 0.70969212 <a title="324-lda-15" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>16 0.70948553 <a title="324-lda-16" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<p>17 0.70883572 <a title="324-lda-17" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>18 0.70874286 <a title="324-lda-18" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>19 0.70826417 <a title="324-lda-19" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>20 0.70817077 <a title="324-lda-20" href="./iccv-2013-Hybrid_Deep_Learning_for_Face_Verification.html">206 iccv-2013-Hybrid Deep Learning for Face Verification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
