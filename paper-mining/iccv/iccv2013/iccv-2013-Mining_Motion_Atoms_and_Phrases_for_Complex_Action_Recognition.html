<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-265" href="#">iccv2013-265</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</h1>
<br/><p>Source: <a title="iccv-2013-265-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Wang_Mining_Motion_Atoms_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Limin Wang, Yu Qiao, Xiaoou Tang</p><p>Abstract: This paper proposes motion atom and phrase as a midlevel temporal “part” for representing and classifying complex action. Motion atom is defined as an atomic part of action, and captures the motion information of action video in a short temporal scale. Motion phrase is a temporal composite of multiple motion atoms with an AND/OR structure, which further enhances the discriminative ability of motion atoms by incorporating temporal constraints in a longer scale. Specifically, given a set of weakly labeled action videos, we firstly design a discriminative clustering method to automatically discovera set ofrepresentative motion atoms. Then, based on these motion atoms, we mine effective motion phrases with high discriminative and representativepower. We introduce a bottom-upphrase construction algorithm and a greedy selection method for this mining task. We examine the classification performance of the motion atom and phrase based representation on two complex action datasets: Olympic Sports and UCF50. Experimental results show that our method achieves superior performance over recent published methods on both datasets.</p><p>Reference: <a title="iccv-2013-265-reference" href="../iccv2013_reference/iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract This paper proposes motion atom and phrase as a midlevel temporal “part” for representing and classifying complex action. [sent-12, score-1.36]
</p><p>2 Motion atom is defined as an atomic part of action, and captures the motion information of action video in a short temporal scale. [sent-13, score-1.179]
</p><p>3 Motion phrase is a temporal composite of multiple motion atoms with an AND/OR structure, which further enhances the discriminative ability of motion atoms by incorporating temporal constraints in a longer scale. [sent-14, score-2.265]
</p><p>4 Specifically, given a set of weakly labeled action videos, we firstly design a discriminative clustering method to automatically discovera set ofrepresentative motion atoms. [sent-15, score-0.629]
</p><p>5 Then, based on these motion atoms, we mine effective motion phrases with high discriminative and representativepower. [sent-16, score-1.151]
</p><p>6 We examine the classification performance of the motion atom and phrase based representation on two complex action datasets: Olympic Sports and UCF50. [sent-18, score-1.449]
</p><p>7 For each atom, it describes a short temporal scale motion information, which can be shared by different complex action classes. [sent-33, score-0.79]
</p><p>8 For each complex action, there exist temporal structures of multiple atoms in a long temporal scale. [sent-34, score-0.718]
</p><p>9 Firstly, due to background clutter, viewpoint changes, and motion speed variation, there exist always large intra-class appearance and motion variations within the same class of action. [sent-36, score-0.663]
</p><p>10 Recently, researches show that the temporal structures of complex action yield effective cues for action classification [8, 15, 23, 26]. [sent-38, score-0.723]
</p><p>11 From a short temporal scale, each atomic motion corresponds to a simple pattern and these atomic motions may be shared by different complex action classes. [sent-42, score-0.992]
</p><p>12 These observations offer us insights to complex action recognition: •  Unsupervised  discovery of motion atoms. [sent-44, score-0.64]
</p><p>13 We need to design an unsupervised method to discover a set of motion atoms automatically from video dataset. [sent-50, score-0.734]
</p><p>14 The discriminative power of motion atom is limited by its temporal duration. [sent-54, score-0.853]
</p><p>15 sequential composition of motion atoms), captures motion information in a longer scale and provides important cue to discriminate different action classes. [sent-57, score-0.866]
</p><p>16 Based on the above insights, this paper proposes motion atom and phrase, a mid-level representation of action video, which jointly encodes the motion, appearance, and temporal structure of multiple atomic actions. [sent-60, score-1.142]
</p><p>17 Firstly, we discover a set of motion atoms from training samples in an unsupervised manner. [sent-61, score-0.707]
</p><p>18 Then, we construct motion phrase as a temporal composite of multiple atoms. [sent-67, score-1.042]
</p><p>19 It not only captures short-scale motion information of each atom, but also models the temporal structure of multiple atoms in a longer temporal scale. [sent-68, score-0.993]
</p><p>20 We propose a bottom-up mining algorithm and greedy selection method to obtain a set of motion phrases with high discriminative and representative power. [sent-70, score-0.93]
</p><p>21 Finally, we represent each video by the activation vector of motion atoms and phrases by max pooling the response score of each atom and phrase. [sent-71, score-1.521]
</p><p>22 motion atoms and phrase, to represent video of complex action, and our representation is flexible with the classifier used for recognition. [sent-87, score-0.774]
</p><p>23 Besides, previous studies usually train a single model for each action class, but our method can discover a set of motion atoms and phrases. [sent-88, score-0.902]
</p><p>24 Their cuboids are limited in temporal duration and not suitable for complex action recognition Our motion atom has the similar role as these motion attributes and parts in essence. [sent-99, score-1.451]
</p><p>25 However, our motion atoms are obtained through an unsupervised manner from training data, and we model temporal structure of multiple motion atoms to enhance their descriptive power. [sent-100, score-1.517]
</p><p>26 Unsupervised Discovery of Motion Atoms To construct effective representations for complex actions, we first discover a set ofmotion atoms that capture the  motion patterns in a short temporal scale. [sent-106, score-1.002]
</p><p>27 These atoms act 2681  corresponds to running and opening arms for complex action gym-vault; right: motion atom corresponds to rolling in circles for complex  action hammer throw. [sent-107, score-1.618]
</p><p>28 as basic units for constructing more discriminative motion phrase in a longer scale. [sent-108, score-0.968]
</p><p>29 Given a set of training videos, our objective is to automatically discover a set of common motion patterns as motion atoms. [sent-109, score-0.682]
</p><p>30 Our main goal is to determine a large set of simple motion patterns, which are shared by many complex actions and can be used as basic units to represent complex actions. [sent-116, score-0.641]
</p><p>31 We need to make sure that the obtained atom set  can cover different motion patterns occurring in various actions. [sent-117, score-0.69]
</p><p>32 Given two segments Algorithm  1:  Discovery  of  motion  Algorithm 1: Discovery of motion atoms. [sent-127, score-0.678]
</p><p>33 One atom usually corresponds to a simple motion pattern within a short temporal scale, and may occur in different classes of complex actions. [sent-167, score-0.884]
</p><p>34 These facts limit the discriminative ability of motion atoms in classifying complex actions. [sent-168, score-0.816]
</p><p>35 To circumvent this problem, we make use of these atoms as basic units to construct motion phrase with a longer scale. [sent-169, score-1.273]
</p><p>36 For action classification task, motion phrases are expected to have the following properties: •  •  Descriptive property: Each phrase should be a temporDale composite oopfe highly rcehla ptehdra mseo sthioonu dato bmes a. [sent-170, score-1.552]
</p><p>37 Meanwhile, to deal with motion speed variations, motion phrase needs to allow temporal displacement among its composite atoms. [sent-172, score-1.362]
</p><p>38 Illustration for motion phrase: motion phrase is an AND/OR structure over a set of atom units, which are indicated by ellipsoids. [sent-174, score-1.454]
</p><p>39 It is desirable that a motion phrase is highly related to a certain class of action. [sent-176, score-0.846]
</p><p>40 Representative property: Due to large variations among complex apcrotipoenr vtyid:eos, each motion phrase can only cover part of the action videos. [sent-178, score-1.149]
</p><p>41 Thus, we need to take account of the correlations between different phrases, and we wish to determine a set of motion phrases which convey enough motion patterns to handle the variations of complex actions. [sent-179, score-1.173]
</p><p>42 Motion Phrase Definition: Based on the analysis above, we define motion phrase as an AND/OR structure on a set of motion atom units as shown in Figure 3. [sent-180, score-1.542]
</p><p>43 •  Each atom unit, denoted as Π = (A, t, σ), refer to a motion atom A detected in the neighborhood of temporal anchor point t. [sent-183, score-1.102]
</p><p>44 Based on these atom units, we construct motion phrases by AND/OR structure. [sent-193, score-1.09]
</p><p>45 We first apply OR operation over several atom units that have the same atom label and are located nearby (e. [sent-194, score-0.749]
</p><p>46 Then, we conduct AND operation over the selected atom units and choose the smallest response as motion phrase response. [sent-200, score-1.279]
</p><p>47 Thus the response value r of an motion phrase P with respect to a given video V :  r(V,P) = O mRiin∈PΠjm∈aOxRiv(V,Πj),  (4)  2683  where ORi denote the OR operations in motion phrase P. [sent-201, score-1.703]
</p><p>48 The size of motion phrase is defined as the number of OR  operations it includes (e. [sent-202, score-0.808]
</p><p>49 In essence, motion phrase representation is the temporal composite of multiple atomic motion units. [sent-205, score-1.448]
</p><p>50 The OR operation allows us to search for the best location for current motion atom, and makes it flexible to deal with the temporal displacement caused by motion speed variations. [sent-206, score-0.809]
</p><p>51 Above all, motion phrase not only delivers motion information of each atom, but also encodes temporal structure among them. [sent-208, score-1.285]
</p><p>52 Evaluation of Discriminative Ability: A motion phrase P is discriminative for c-th class of complex action if it is highly related with this class, but appears sparely in other action classes. [sent-210, score-1.423]
</p><p>53 Due to the large variance among action videos, a single motion phrase could obtain strong value only on part of the videos of certain class. [sent-213, score-1.099]
</p><p>54 Mining Motion Phrase: Given a training video set V = {Vi}iN=1 with class label Y = {yi}iN=1 and a set of Vmo =tion { Vato}ms Aw =h {laAsis} liMa=b1e,l our goal i}s to find a set of mmoottiioonn phrases AP = == { A{P}i}iK=1for complex action classes. [sent-215, score-0.851]
</p><p>55 mGiovteinon nth peh rcalsaesss c, f=or {ePach} individual motion phrase, we want each motion phrase to have high discriminative and representative ability with current class c. [sent-216, score-1.283]
</p><p>56 Thus, the set of motion phrases is able to cover the complexity of action videos. [sent-223, score-1.003]
</p><p>57 The main challenge comes from the fact that the possible combination atom units that form a motion phrase is huge. [sent-224, score-1.218]
</p><p>58 Assuming a video with k segments and the size of motion atoms is M, there are M k possible atom units. [sent-225, score-1.073]
</p><p>59 If a phrase of size s has a high representative ability for action class c (Equation (6)), then any (s 1)-atom phrase by eliminating one motion tahteomn asnhyou (lsd −als 1o) -haatvoem a high representative ability as owtieolnl. [sent-230, score-1.753]
</p><p>60 f Finally, we heleimmiinnainteg some motion phrase of low discriminative ability with a threshold −  τ. [sent-233, score-0.889]
</p><p>61 In each iteration, we determine a motion phrase with high individual representative power, that meanwhile increases the set representative power the most. [sent-238, score-0.96]
</p><p>62 Data: motion phrases candidates P = {Pi}iL=1 , class: c, mnuomtiboner: p hKrac. [sent-243, score-0.754]
</p><p>63 -  Return  motion  phrases:  P  the mining process, for each motion phrase, we consider top 40 videos with highest response value (i. [sent-252, score-0.769]
</p><p>64 Recognition with Motion Atoms and Phrases Motion atoms and phrases can be regarded as mid-level units for representing complex action. [sent-259, score-0.949]
</p><p>65 Specifically, for each motion atom A, we define a special motion phrase, in which there is only one atom unit (A, 0, +∞). [sent-261, score-1.274]
</p><p>66 We call this special motion phrase as 0-motion phrase. [sent-262, score-0.808]
</p><p>67 n, W wei ctha a stheti sm sopteicoina phrase Pn p =hr a{sPei} asiK= 0-1 mwohtioosne spihzreass range nfr,o wmit h0 atsoe MtmAoXtion, we represent ePac}h video V by an activation vector f = [r1, · · · , rK], where ri is the  response value of motion phrase ·P·i· ,writh respect to video V . [sent-264, score-1.464]
</p><p>68 Experiments We evaluate the effectiveness of motion atom and phrase on two complex action datasets: Olympic Sports dataset [15] and UCF50 dataset [18]. [sent-268, score-1.435]
</p><p>69 Left: performance of motion phrase for different sizes on the Olympic Sports dataset. [sent-270, score-0.808]
</p><p>70 Right: performance trend of varying maximum size for motion phrase on the Olympic Sports dataset. [sent-271, score-0.808]
</p><p>71 Size of Motion Phrases: We examine the performance of motion phrases with different sizes on the Olympic Sports dataset and the results are shown in Figure 4. [sent-274, score-0.754]
</p><p>72 1-motion phrases are mined for high discriminative and representative power, and thus their performance is better than 0-motion phrases (motion atoms), whose discriminative power is relatively low. [sent-276, score-1.082]
</p><p>73 Secondly, we notice that the mAPs of 2-motion phrases and 3-motion phrases are lower than the mAPs of 1-motion phrases and 0-motion phrases. [sent-277, score-1.347]
</p><p>74 This may be due to the large variations of video data, and the number of mined 2-motion phrases and 3-motion phrases is much smaller than the other two. [sent-278, score-0.972]
</p><p>75 Although motion phrases of large size are more discriminative than others, they only cover a small part of the video data. [sent-279, score-0.862]
</p><p>76 Besides, the information conveyed by large motion phrases has been partly contained in the motion phrases of smaller size. [sent-281, score-1.525]
</p><p>77 We combine the representation of motion phrases with different sizes and the performance is shown in the right of Figure 4. [sent-282, score-0.768]
</p><p>78 We see that the performance increases apparently in using motion phrases of size from 0 to 2. [sent-283, score-0.754]
</p><p>79 But there is only slight improvement when including motion phrases of size 3. [sent-284, score-0.754]
</p><p>80 Therefore, in the remaining discussions, we fix the maximum size of motion phrases as 2. [sent-286, score-0.754]
</p><p>81 Motion phrase  can  automatically locate temporal composites of multiple motion  atoms  (indicated by red boxes) in complex actions. [sent-304, score-1.396]
</p><p>82 Combine all indicates the combination of low-level features with motion atoms and phrases, with which we obtain state-  TabNlei2. [sent-309, score-0.64]
</p><p>83 Note that for motion atoms and phrases, we only use linear SVM. [sent-318, score-0.64]
</p><p>84 We can find that the motion atom based mid-level representations achieve better performance than low-level  features on both datasets. [sent-320, score-0.644]
</p><p>85 However, motion atoms can achieve good results just with linear SVM. [sent-322, score-0.64]
</p><p>86 The combination of motion atoms and phrases can further improve the recognition results. [sent-323, score-1.089]
</p><p>87 Finally, we combine motion atoms and phrases with lowlevel features, and obtain the state-of-the-art performances on both datasets. [sent-325, score-1.105]
</p><p>88 Comparison with Other Methods: We compare motion atoms and phrases with other methods on both datasets, and the results are shown in Table 2 and Table 3. [sent-327, score-1.089]
</p><p>89 Our mid-level representation aims to find multiple motion atoms and phrases, and each representation covers a subset of videos. [sent-330, score-0.668]
</p><p>90 In [14], the authors use attribute representation, where the attributes are specified in advance, and we find motion atoms and phrases learned from training data are more flexible and effective. [sent-332, score-1.142]
</p><p>91 From the results of Table 3, we see that our motion atoms and phrases outperform these low-level features on UCF50 dataset. [sent-334, score-1.089]
</p><p>92 Unlike action bank, our motion atom and phrase correspond to middle-level “parts” of the action, similar to the mid-level motionlet [25]. [sent-337, score-1.387]
</p><p>93 Compared with the latest paper [24], our motion atom and phrase use less descriptor and smaller codebook size. [sent-340, score-1.147]
</p><p>94 This indicates that motion atom and phrase is effective for action classification, especially for complex action classes with longer temporal scale. [sent-343, score-1.862]
</p><p>95 Visualization: We show some examples of motion atoms and phrases in Figure 2 and Figure 5 respectively. [sent-344, score-1.089]
</p><p>96 Motion phrase consists of a sequence of motion atoms. [sent-347, score-0.808]
</p><p>97 As shown in the examples of Figure 5, motion phrase can discover waiting and diving for diving-platform, running and layup for basketball-layup, running and jumping for triple jumping, and running and landing for vault. [sent-348, score-0.978]
</p><p>98 Conclusion We propose motion atom and phrase for representing and recognizing complex actions. [sent-350, score-1.226]
</p><p>99 Motion atom describes simple motion pattern in a short temporal scale, and motion phrase encodes temporal structure of multiple atoms in a longer scale. [sent-351, score-2.15]
</p><p>100 From the experimental results, we see that motion atoms and phrases are effective representations and outperform several recently published low-level features and complex models. [sent-354, score-1.215]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('phrase', 0.503), ('phrases', 0.449), ('atoms', 0.335), ('atom', 0.322), ('motion', 0.305), ('action', 0.228), ('temporal', 0.153), ('olympic', 0.121), ('sports', 0.104), ('atomic', 0.101), ('actions', 0.094), ('units', 0.088), ('complex', 0.077), ('segments', 0.068), ('composite', 0.067), ('mining', 0.066), ('representative', 0.051), ('videos', 0.049), ('rep', 0.047), ('vi', 0.046), ('discriminative', 0.044), ('qiao', 0.044), ('response', 0.044), ('video', 0.043), ('cooking', 0.04), ('class', 0.038), ('ability', 0.037), ('jumping', 0.036), ('discover', 0.034), ('descriptive', 0.032), ('cluster', 0.032), ('firstly', 0.031), ('discovery', 0.03), ('mine', 0.03), ('power', 0.029), ('hjm', 0.029), ('motionlet', 0.029), ('longer', 0.028), ('short', 0.027), ('running', 0.027), ('return', 0.027), ('resort', 0.026), ('motionlets', 0.025), ('svm', 0.025), ('duration', 0.024), ('pi', 0.024), ('activity', 0.024), ('composites', 0.023), ('activation', 0.023), ('patterns', 0.022), ('ik', 0.022), ('bovw', 0.021), ('interchange', 0.021), ('grouplet', 0.021), ('shenzhen', 0.021), ('attributes', 0.021), ('clustering', 0.021), ('bank', 0.021), ('cover', 0.021), ('meanwhile', 0.021), ('sure', 0.02), ('actom', 0.02), ('amer', 0.02), ('rohrbach', 0.02), ('unit', 0.02), ('rolling', 0.019), ('diving', 0.019), ('recognizing', 0.019), ('structure', 0.019), ('researches', 0.019), ('script', 0.019), ('facts', 0.018), ('latent', 0.018), ('gaidon', 0.018), ('effective', 0.018), ('representations', 0.017), ('codebook', 0.017), ('partly', 0.017), ('niebles', 0.017), ('unsupervised', 0.017), ('operation', 0.017), ('cuboids', 0.016), ('attribute', 0.016), ('training', 0.016), ('lowlevel', 0.016), ('ap', 0.016), ('mined', 0.016), ('preference', 0.015), ('decomposed', 0.015), ('libsvm', 0.015), ('marszalek', 0.015), ('variations', 0.015), ('displacement', 0.015), ('greedy', 0.015), ('representation', 0.014), ('peng', 0.014), ('construct', 0.014), ('deal', 0.014), ('published', 0.014), ('variance', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="265-tfidf-1" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>Author: Limin Wang, Yu Qiao, Xiaoou Tang</p><p>Abstract: This paper proposes motion atom and phrase as a midlevel temporal “part” for representing and classifying complex action. Motion atom is defined as an atomic part of action, and captures the motion information of action video in a short temporal scale. Motion phrase is a temporal composite of multiple motion atoms with an AND/OR structure, which further enhances the discriminative ability of motion atoms by incorporating temporal constraints in a longer scale. Specifically, given a set of weakly labeled action videos, we firstly design a discriminative clustering method to automatically discovera set ofrepresentative motion atoms. Then, based on these motion atoms, we mine effective motion phrases with high discriminative and representativepower. We introduce a bottom-upphrase construction algorithm and a greedy selection method for this mining task. We examine the classification performance of the motion atom and phrase based representation on two complex action datasets: Olympic Sports and UCF50. Experimental results show that our method achieves superior performance over recent published methods on both datasets.</p><p>2 0.26213947 <a title="265-tfidf-2" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>Author: Jiajia Luo, Wei Wang, Hairong Qi</p><p>Abstract: Human action recognition based on the depth information provided by commodity depth sensors is an important yet challenging task. The noisy depth maps, different lengths of action sequences, and free styles in performing actions, may cause large intra-class variations. In this paper, a new framework based on sparse coding and temporal pyramid matching (TPM) is proposed for depthbased human action recognition. Especially, a discriminative class-specific dictionary learning algorithm isproposed for sparse coding. By adding the group sparsity and geometry constraints, features can be well reconstructed by the sub-dictionary belonging to the same class, and the geometry relationships among features are also kept in the calculated coefficients. The proposed approach is evaluated on two benchmark datasets captured by depth cameras. Experimental results show that the proposed algorithm repeatedly hqi } @ ut k . edu GB ImagesR epth ImagesD setkonlSy0 896.5170d4ept.3h021 .x02y 19.876504.dep3th02.1 x02. achieves superior performance to the state of the art algorithms. Moreover, the proposed dictionary learning method also outperforms classic dictionary learning approaches.</p><p>3 0.23985794 <a title="265-tfidf-3" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<p>Author: Tian Lan, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: The appearance of an object changes profoundly with pose, camera view and interactions of the object with other objects in the scene. This makes it challenging to learn detectors based on an object-level label (e.g., “car”). We postulate that having a richer set oflabelings (at different levels of granularity) for an object, including finer-grained subcategories, consistent in appearance and view, and higherorder composites – contextual groupings of objects consistent in their spatial layout and appearance, can significantly alleviate these problems. However, obtaining such a rich set of annotations, including annotation of an exponentially growing set of object groupings, is simply not feasible. We propose a weakly-supervised framework for object detection where we discover subcategories and the composites automatically with only traditional object-level category labels as input. To this end, we first propose an exemplar-SVM-based clustering approach, with latent SVM refinement, that discovers a variable length set of discriminative subcategories for each object class. We then develop a structured model for object detection that captures interactions among object subcategories and automatically discovers semantically meaningful and discriminatively relevant visual composites. We show that this model produces state-of-the-art performance on UIUC phrase object detection benchmark.</p><p>4 0.20892553 <a title="265-tfidf-4" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>Author: Jiang Wang, Ying Wu</p><p>Abstract: Temporal misalignment and duration variation in video actions largely influence the performance of action recognition, but it is very difficult to specify effective temporal alignment on action sequences. To address this challenge, this paper proposes a novel discriminative learning-based temporal alignment method, called maximum margin temporal warping (MMTW), to align two action sequences and measure their matching score. Based on the latent structure SVM formulation, the proposed MMTW method is able to learn a phantom action template to represent an action class for maximum discrimination against other classes. The recognition of this action class is based on the associated learned alignment of the input action. Extensive experiments on five benchmark datasets have demonstrated that this MMTW model is able to significantly promote the accuracy and robustness of action recognition under temporal misalignment and variations.</p><p>5 0.20486647 <a title="265-tfidf-5" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>Author: Ping Wei, Nanning Zheng, Yibiao Zhao, Song-Chun Zhu</p><p>Abstract: Action recognition has often been posed as a classification problem, which assumes that a video sequence only have one action class label and different actions are independent. However, a single human body can perform multiple concurrent actions at the same time, and different actions interact with each other. This paper proposes a concurrent action detection model where the action detection is formulated as a structural prediction problem. In this model, an interval in a video sequence can be described by multiple action labels. An detected action interval is determined both by the unary local detector and the relations with other actions. We use a wavelet feature to represent the action sequence, and design a composite temporal logic descriptor to describe the action relations. The model parameters are trained by structural SVM learning. Given a long video sequence, a sequential decision window search algorithm is designed to detect the actions. Experiments on our new collected concurrent action dataset demonstrate the strength of our method.</p><p>6 0.20404172 <a title="265-tfidf-6" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>7 0.19728681 <a title="265-tfidf-7" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>8 0.19598435 <a title="265-tfidf-8" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>9 0.19382015 <a title="265-tfidf-9" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>10 0.19082312 <a title="265-tfidf-10" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>11 0.17881331 <a title="265-tfidf-11" href="./iccv-2013-Learning_to_Share_Latent_Tasks_for_Action_Recognition.html">249 iccv-2013-Learning to Share Latent Tasks for Action Recognition</a></p>
<p>12 0.15616766 <a title="265-tfidf-12" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>13 0.15527229 <a title="265-tfidf-13" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>14 0.1552114 <a title="265-tfidf-14" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>15 0.15219402 <a title="265-tfidf-15" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>16 0.14799054 <a title="265-tfidf-16" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>17 0.1328941 <a title="265-tfidf-17" href="./iccv-2013-From_Actemes_to_Action%3A_A_Strongly-Supervised_Representation_for_Detailed_Action_Understanding.html">175 iccv-2013-From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding</a></p>
<p>18 0.12906554 <a title="265-tfidf-18" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>19 0.12577882 <a title="265-tfidf-19" href="./iccv-2013-Manipulation_Pattern_Discovery%3A_A_Nonparametric_Bayesian_Approach.html">260 iccv-2013-Manipulation Pattern Discovery: A Nonparametric Bayesian Approach</a></p>
<p>20 0.1183892 <a title="265-tfidf-20" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.19), (1, 0.199), (2, 0.079), (3, 0.255), (4, -0.046), (5, 0.003), (6, 0.041), (7, -0.072), (8, 0.02), (9, 0.029), (10, 0.045), (11, 0.066), (12, 0.047), (13, -0.04), (14, 0.006), (15, 0.003), (16, 0.033), (17, -0.012), (18, 0.03), (19, -0.031), (20, -0.02), (21, 0.033), (22, 0.013), (23, -0.014), (24, -0.046), (25, -0.018), (26, 0.001), (27, 0.006), (28, 0.007), (29, 0.001), (30, 0.007), (31, -0.096), (32, -0.097), (33, 0.058), (34, -0.034), (35, 0.052), (36, -0.034), (37, -0.002), (38, -0.051), (39, 0.015), (40, -0.027), (41, 0.003), (42, -0.003), (43, -0.03), (44, -0.074), (45, 0.042), (46, -0.02), (47, 0.082), (48, 0.155), (49, 0.132)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94182444 <a title="265-lsi-1" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>Author: Limin Wang, Yu Qiao, Xiaoou Tang</p><p>Abstract: This paper proposes motion atom and phrase as a midlevel temporal “part” for representing and classifying complex action. Motion atom is defined as an atomic part of action, and captures the motion information of action video in a short temporal scale. Motion phrase is a temporal composite of multiple motion atoms with an AND/OR structure, which further enhances the discriminative ability of motion atoms by incorporating temporal constraints in a longer scale. Specifically, given a set of weakly labeled action videos, we firstly design a discriminative clustering method to automatically discovera set ofrepresentative motion atoms. Then, based on these motion atoms, we mine effective motion phrases with high discriminative and representativepower. We introduce a bottom-upphrase construction algorithm and a greedy selection method for this mining task. We examine the classification performance of the motion atom and phrase based representation on two complex action datasets: Olympic Sports and UCF50. Experimental results show that our method achieves superior performance over recent published methods on both datasets.</p><p>2 0.72058964 <a title="265-lsi-2" href="./iccv-2013-Manipulation_Pattern_Discovery%3A_A_Nonparametric_Bayesian_Approach.html">260 iccv-2013-Manipulation Pattern Discovery: A Nonparametric Bayesian Approach</a></p>
<p>Author: Bingbing Ni, Pierre Moulin</p><p>Abstract: We aim to unsupervisedly discover human’s action (motion) patterns of manipulating various objects in scenarios such as assisted living. We are motivated by two key observations. First, large variation exists in motion patterns associated with various types of objects being manipulated, thus manually defining motion primitives is infeasible. Second, some motion patterns are shared among different objects being manipulated while others are object specific. We therefore propose a nonparametric Bayesian method that adopts a hierarchical Dirichlet process prior to learn representative manipulation (motion) patterns in an unsupervised manner. Taking easy-to-obtain object detection score maps and dense motion trajectories as inputs, the proposed probabilistic model can discover motion pattern groups associated with different types of objects being manipulated with a shared manipulation pattern dictionary. The size of the learned dictionary is automatically inferred. Com- prehensive experiments on two assisted living benchmarks and a cooking motion dataset demonstrate superiority of our learned manipulation pattern dictionary in representing manipulation actions for recognition.</p><p>3 0.6891219 <a title="265-lsi-3" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>Author: Jiang Wang, Ying Wu</p><p>Abstract: Temporal misalignment and duration variation in video actions largely influence the performance of action recognition, but it is very difficult to specify effective temporal alignment on action sequences. To address this challenge, this paper proposes a novel discriminative learning-based temporal alignment method, called maximum margin temporal warping (MMTW), to align two action sequences and measure their matching score. Based on the latent structure SVM formulation, the proposed MMTW method is able to learn a phantom action template to represent an action class for maximum discrimination against other classes. The recognition of this action class is based on the associated learned alignment of the input action. Extensive experiments on five benchmark datasets have demonstrated that this MMTW model is able to significantly promote the accuracy and robustness of action recognition under temporal misalignment and variations.</p><p>4 0.68646622 <a title="265-lsi-4" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>Author: Ping Wei, Nanning Zheng, Yibiao Zhao, Song-Chun Zhu</p><p>Abstract: Action recognition has often been posed as a classification problem, which assumes that a video sequence only have one action class label and different actions are independent. However, a single human body can perform multiple concurrent actions at the same time, and different actions interact with each other. This paper proposes a concurrent action detection model where the action detection is formulated as a structural prediction problem. In this model, an interval in a video sequence can be described by multiple action labels. An detected action interval is determined both by the unary local detector and the relations with other actions. We use a wavelet feature to represent the action sequence, and design a composite temporal logic descriptor to describe the action relations. The model parameters are trained by structural SVM learning. Given a long video sequence, a sequential decision window search algorithm is designed to detect the actions. Experiments on our new collected concurrent action dataset demonstrate the strength of our method.</p><p>5 0.68451118 <a title="265-lsi-5" href="./iccv-2013-From_Actemes_to_Action%3A_A_Strongly-Supervised_Representation_for_Detailed_Action_Understanding.html">175 iccv-2013-From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding</a></p>
<p>Author: Weiyu Zhang, Menglong Zhu, Konstantinos G. Derpanis</p><p>Abstract: This paper presents a novel approach for analyzing human actions in non-scripted, unconstrained video settings based on volumetric, x-y-t, patch classifiers, termed actemes. Unlike previous action-related work, the discovery of patch classifiers is posed as a strongly-supervised process. Specifically, keypoint labels (e.g., position) across spacetime are used in a data-driven training process to discover patches that are highly clustered in the spacetime keypoint configuration space. To support this process, a new human action dataset consisting of challenging consumer videos is introduced, where notably the action label, the 2D position of a set of keypoints and their visibilities are provided for each video frame. On a novel input video, each acteme is used in a sliding volume scheme to yield a set of sparse, non-overlapping detections. These detections provide the intermediate substrate for segmenting out the action. For action classification, the proposed representation shows significant improvement over state-of-the-art low-level features, while providing spatiotemporal localiza- tion as additional output. This output sheds further light into detailed action understanding.</p><p>6 0.66484576 <a title="265-lsi-6" href="./iccv-2013-Action_Recognition_with_Actons.html">38 iccv-2013-Action Recognition with Actons</a></p>
<p>7 0.6568014 <a title="265-lsi-7" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>8 0.65640241 <a title="265-lsi-8" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>9 0.64844763 <a title="265-lsi-9" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>10 0.63699597 <a title="265-lsi-10" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>11 0.60634476 <a title="265-lsi-11" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>12 0.59518272 <a title="265-lsi-12" href="./iccv-2013-Latent_Multitask_Learning_for_View-Invariant_Action_Recognition.html">231 iccv-2013-Latent Multitask Learning for View-Invariant Action Recognition</a></p>
<p>13 0.5795908 <a title="265-lsi-13" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>14 0.57549417 <a title="265-lsi-14" href="./iccv-2013-Learning_to_Share_Latent_Tasks_for_Action_Recognition.html">249 iccv-2013-Learning to Share Latent Tasks for Action Recognition</a></p>
<p>15 0.55187201 <a title="265-lsi-15" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>16 0.551561 <a title="265-lsi-16" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>17 0.54635108 <a title="265-lsi-17" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>18 0.54373896 <a title="265-lsi-18" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>19 0.53575563 <a title="265-lsi-19" href="./iccv-2013-Capturing_Global_Semantic_Relationships_for_Facial_Action_Unit_Recognition.html">69 iccv-2013-Capturing Global Semantic Relationships for Facial Action Unit Recognition</a></p>
<p>20 0.52658045 <a title="265-lsi-20" href="./iccv-2013-Estimating_the_Material_Properties_of_Fabric_from_Video.html">145 iccv-2013-Estimating the Material Properties of Fabric from Video</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.107), (4, 0.012), (7, 0.019), (12, 0.018), (26, 0.144), (31, 0.029), (42, 0.065), (64, 0.111), (73, 0.024), (78, 0.041), (83, 0.088), (89, 0.198)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91084719 <a title="265-lda-1" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>Author: Matthias Reso, Jörn Jachalsky, Bodo Rosenhahn, Jörn Ostermann</p><p>Abstract: Superpixel algorithms represent a very useful and increasingly popular preprocessing step for a wide range of computer vision applications, as they offer the potential to boost efficiency and effectiveness. In this regards, this paper presents a highly competitive approach for temporally consistent superpixelsfor video content. The approach is based on energy-minimizing clustering utilizing a novel hybrid clustering strategy for a multi-dimensional feature space working in a global color subspace and local spatial subspaces. Moreover, a new contour evolution based strategy is introduced to ensure spatial coherency of the generated superpixels. For a thorough evaluation the proposed approach is compared to state of the art supervoxel algorithms using established benchmarks and shows a superior performance.</p><p>same-paper 2 0.90374112 <a title="265-lda-2" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>Author: Limin Wang, Yu Qiao, Xiaoou Tang</p><p>Abstract: This paper proposes motion atom and phrase as a midlevel temporal “part” for representing and classifying complex action. Motion atom is defined as an atomic part of action, and captures the motion information of action video in a short temporal scale. Motion phrase is a temporal composite of multiple motion atoms with an AND/OR structure, which further enhances the discriminative ability of motion atoms by incorporating temporal constraints in a longer scale. Specifically, given a set of weakly labeled action videos, we firstly design a discriminative clustering method to automatically discovera set ofrepresentative motion atoms. Then, based on these motion atoms, we mine effective motion phrases with high discriminative and representativepower. We introduce a bottom-upphrase construction algorithm and a greedy selection method for this mining task. We examine the classification performance of the motion atom and phrase based representation on two complex action datasets: Olympic Sports and UCF50. Experimental results show that our method achieves superior performance over recent published methods on both datasets.</p><p>3 0.90311074 <a title="265-lda-3" href="./iccv-2013-From_Actemes_to_Action%3A_A_Strongly-Supervised_Representation_for_Detailed_Action_Understanding.html">175 iccv-2013-From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding</a></p>
<p>Author: Weiyu Zhang, Menglong Zhu, Konstantinos G. Derpanis</p><p>Abstract: This paper presents a novel approach for analyzing human actions in non-scripted, unconstrained video settings based on volumetric, x-y-t, patch classifiers, termed actemes. Unlike previous action-related work, the discovery of patch classifiers is posed as a strongly-supervised process. Specifically, keypoint labels (e.g., position) across spacetime are used in a data-driven training process to discover patches that are highly clustered in the spacetime keypoint configuration space. To support this process, a new human action dataset consisting of challenging consumer videos is introduced, where notably the action label, the 2D position of a set of keypoints and their visibilities are provided for each video frame. On a novel input video, each acteme is used in a sliding volume scheme to yield a set of sparse, non-overlapping detections. These detections provide the intermediate substrate for segmenting out the action. For action classification, the proposed representation shows significant improvement over state-of-the-art low-level features, while providing spatiotemporal localiza- tion as additional output. This output sheds further light into detailed action understanding.</p><p>4 0.90004981 <a title="265-lda-4" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>Author: Yuning Chai, Victor Lempitsky, Andrew Zisserman</p><p>Abstract: We propose a new method for the task of fine-grained visual categorization. The method builds a model of the baselevel category that can be fitted to images, producing highquality foreground segmentation and mid-level part localizations. The model can be learnt from the typical datasets available for fine-grained categorization, where the only annotation provided is a loose bounding box around the instance (e.g. bird) in each image. Both segmentation and part localizations are then used to encode the image content into a highly-discriminative visual signature. The model is symbiotic in that part discovery/localization is helped by segmentation and, conversely, the segmentation is helped by the detection (e.g. part layout). Our model builds on top of the part-based object category detector of Felzenszwalb et al., and also on the powerful GrabCut segmentation algorithm of Rother et al., and adds a simple spatial saliency coupling between them. In our evaluation, the model improves the categorization accuracy over the state-of-the-art. It also improves over what can be achieved with an analogous system that runs segmentation and part-localization independently.</p><p>5 0.89832842 <a title="265-lda-5" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>Author: Ping Wei, Nanning Zheng, Yibiao Zhao, Song-Chun Zhu</p><p>Abstract: Action recognition has often been posed as a classification problem, which assumes that a video sequence only have one action class label and different actions are independent. However, a single human body can perform multiple concurrent actions at the same time, and different actions interact with each other. This paper proposes a concurrent action detection model where the action detection is formulated as a structural prediction problem. In this model, an interval in a video sequence can be described by multiple action labels. An detected action interval is determined both by the unary local detector and the relations with other actions. We use a wavelet feature to represent the action sequence, and design a composite temporal logic descriptor to describe the action relations. The model parameters are trained by structural SVM learning. Given a long video sequence, a sequential decision window search algorithm is designed to detect the actions. Experiments on our new collected concurrent action dataset demonstrate the strength of our method.</p><p>6 0.89820957 <a title="265-lda-6" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>7 0.89681089 <a title="265-lda-7" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>8 0.89527595 <a title="265-lda-8" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>9 0.89462978 <a title="265-lda-9" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>10 0.89419818 <a title="265-lda-10" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>11 0.89351416 <a title="265-lda-11" href="./iccv-2013-A_Deformable_Mixture_Parsing_Model_with_Parselets.html">8 iccv-2013-A Deformable Mixture Parsing Model with Parselets</a></p>
<p>12 0.89199102 <a title="265-lda-12" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>13 0.88873816 <a title="265-lda-13" href="./iccv-2013-Data-Driven_3D_Primitives_for_Single_Image_Understanding.html">102 iccv-2013-Data-Driven 3D Primitives for Single Image Understanding</a></p>
<p>14 0.88850015 <a title="265-lda-14" href="./iccv-2013-A_New_Adaptive_Segmental_Matching_Measure_for_Human_Activity_Recognition.html">22 iccv-2013-A New Adaptive Segmental Matching Measure for Human Activity Recognition</a></p>
<p>15 0.88794583 <a title="265-lda-15" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>16 0.88757098 <a title="265-lda-16" href="./iccv-2013-ACTIVE%3A_Activity_Concept_Transitions_in_Video_Event_Classification.html">4 iccv-2013-ACTIVE: Activity Concept Transitions in Video Event Classification</a></p>
<p>17 0.88734186 <a title="265-lda-17" href="./iccv-2013-Facial_Action_Unit_Event_Detection_by_Cascade_of_Tasks.html">155 iccv-2013-Facial Action Unit Event Detection by Cascade of Tasks</a></p>
<p>18 0.88712704 <a title="265-lda-18" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>19 0.88676 <a title="265-lda-19" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>20 0.88515794 <a title="265-lda-20" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
