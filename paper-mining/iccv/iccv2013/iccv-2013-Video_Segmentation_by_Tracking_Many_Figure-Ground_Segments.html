<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-442" href="#">iccv2013-442</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</h1>
<br/><p>Source: <a title="iccv-2013-442-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Li_Video_Segmentation_by_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, James M. Rehg</p><p>Abstract: We propose an unsupervised video segmentation approach by simultaneously tracking multiple holistic figureground segments. Segment tracks are initialized from a pool of segment proposals generated from a figure-ground segmentation algorithm. Then, online non-local appearance models are trained incrementally for each track using a multi-output regularized least squares formulation. By using the same set of training examples for all segment tracks, a computational trick allows us to track hundreds of segment tracks efficiently, as well as perform optimal online updates in closed-form. Besides, a new composite statistical inference approach is proposed for refining the obtained segment tracks, which breaks down the initial segment proposals and recombines for better ones by utilizing highorder statistic estimates from the appearance model and enforcing temporal consistency. For evaluating the algorithm, a dataset, SegTrack v2, is collected with about 1,000 frames with pixel-level annotations. The proposed framework outperforms state-of-the-art approaches in the dataset, show- ing its efficiency and robustness to challenges in different video sequences.</p><p>Reference: <a title="iccv-2013-442-reference" href="../iccv2013_reference/iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu , Abstract We propose an unsupervised video segmentation approach by simultaneously tracking multiple holistic figureground segments. [sent-4, score-0.434]
</p><p>2 Segment tracks are initialized from a pool of segment proposals generated from a figure-ground segmentation algorithm. [sent-5, score-1.095]
</p><p>3 By using the same set of training examples for all segment tracks, a computational trick allows us to track hundreds of segment tracks efficiently, as well as perform optimal online updates in closed-form. [sent-7, score-1.5]
</p><p>4 Besides, a new composite statistical inference approach is proposed for refining the obtained segment tracks, which breaks down the initial segment proposals and recombines for better ones by utilizing highorder statistic estimates from the appearance model and enforcing temporal consistency. [sent-8, score-1.498]
</p><p>5 Recent advances show that one can create a pool of several hundred overlapping figure-ground segment proposals so that for most objects in the scene, at least one segment in the pool covers 70 − 80% of it. [sent-13, score-1.234]
</p><p>6 A video sequence offers rich motion and depth cues, from where we could hope to automatically suppress temporally inconsistent segments and obtain good object proposals with fewer segments in the pool. [sent-19, score-0.785]
</p><p>7 We propose to solve unsupervised video segmentation by simultaneously tracking all segments from segment pools generated by figure-ground segmentation at each frame. [sent-25, score-1.294]
</p><p>8 Initially, one segment track is initialized from each unsupervised segment generated in the first frame. [sent-26, score-1.225]
</p><p>9 Then for each segment track, a persistent global appearance model is incrementally trained: at each frame, predictions from the models are used to find the segment that best matches each track, and those matching segments are then used to update the track models. [sent-27, score-1.627]
</p><p>10 We propose to train the appearance model for each track using all segments in the pools from all previous frames. [sent-33, score-0.582]
</p><p>11 At each frame, the target output is the overlap between a segment and the matching segment of the track at that frame. [sent-34, score-1.302]
</p><p>12 Different part segments have different –  –  target outputs based on their spatial overlap with the matching segment. [sent-35, score-0.44]
</p><p>13 Segments that do not overlap the matching segment would have a target of 0. [sent-36, score-0.639]
</p><p>14 We assume that all tracks start from the first frame, so that the training examples are the same for all the segment tracks. [sent-39, score-0.818]
</p><p>15 Then it turns out most ofthe costly operations for solving least squares need to be done only once, regardless of the number of segment tracks. [sent-40, score-0.549]
</p><p>16 This formulation allows us to track hundreds of segments with fairly low complexities in both time and space. [sent-41, score-0.513]
</p><p>17 Our matching framework assumes that at least one good segment is present in most frames for each object. [sent-42, score-0.591]
</p><p>18 Moreover, we aggressively prune the segment tracks: at each frame, if multiple segment tracks match to the same segment, then only the track with the highest score is retained. [sent-43, score-1.451]
</p><p>19 Remarkably, our long-term appearance models are robust enough, so that under such strong assumptions and aggressive pruning, we are still able to cover most objects in the testing videos, while reducing the average number of tracks to 60 from about 1, 200 initial segments per frame. [sent-44, score-0.697]
</p><p>20 Given the fully learnt appearance models, we adopt a recent composite statistical inference (CSI) approach [22] to  refine the segments in the previous frames. [sent-45, score-0.596]
</p><p>21 CSI breaks segment proposals into superpixels and recombines the superpixels by optimizing the likelihood of predictions on the segment proposals given by the appearance model. [sent-47, score-1.525]
</p><p>22 This framework reflects our attempt to test the validity of using and tracking holistic figure-ground segment proposals for video segmentation. [sent-50, score-0.816]
</p><p>23 To extend it into a practical tracking algorithm, we would need to lift the assumption that all segment tracks start from the first frame. [sent-51, score-0.942]
</p><p>24 One can use the proposed framework for multiple intervals of several seconds and regard the generated segment tracks as tracklets. [sent-53, score-0.788]
</p><p>25 The local tracking methods usually track non-overlapping feature points/superpixels and hence are different from our approach that tracks overlapping holistic segments. [sent-59, score-0.629]
</p><p>26 [25] proposes a maximal weighted clique framework to optimally link segments in each frame, their mutual exclusion constraint allows only one segment to be selected in each frame, thus segments that partially match the segment tracks are not utilized. [sent-65, score-1.863]
</p><p>27 Our segment tracking scheme however uses segment proposals which are better boundary-aligned than bounding boxes. [sent-69, score-1.207]
</p><p>28 Besides the video segmentation work that utilizes segment tracking [29, 34, 6], a great deal of research have been on segment tracking with active contours [13, 28, 5], which require a user-drawn region in the first frame. [sent-77, score-1.413]
</p><p>29 Our segment tracking does not have a requirement for user initialization. [sent-78, score-0.615]
</p><p>30 • Compute appearance features for each segment in all fCroammepsu. [sent-83, score-0.568]
</p><p>31 • Initialize a segment track for each segment in the first fIrnaimtiael. [sent-84, score-1.154]
</p><p>32 i • Simultaneously learn the appearance models for all segment etroaucsklsy by multi-output regression (eSlsec f. [sent-85, score-0.599]
</p><p>33 l 2193  Generate segment pools using multiple figure-ground segmentation  Incremental regression, at each frame, match one  Composite Statistical Inference  H W415. [sent-88, score-0.651]
</p><p>34 Each segment from the first frame spawns a segment track, and the appearance models of all the tracks are learnt incrementally and simultaneously. [sent-95, score-1.531]
</p><p>35 At each frame, a segment that best matches each appearance model is found, and then all the segments are added to the training, with the target outputs decided by the overlap with the matching segments (middle). [sent-96, score-1.3]
</p><p>36 Finally, in order to refine the segments, the learned models are tested on all segments across all frames, then relevant regions for each segment track are broken into superpixels and an optimal configuration of the superpixels is found through a composite statistical inference (right). [sent-97, score-1.303]
</p><p>37 • Match segments in the next frame to existing segment tMraactkcsh wseigtmh a greedy algorithm. [sent-98, score-0.941]
</p><p>38 • For long enough segment tracks, perform composite sFtoartis ltoincagl einnfoeurgehnc see g(mSeecn. [sent-104, score-0.594]
</p><p>39 Our ability to simultaneously track hundreds of segments comes from the adoption of the regression-to-overlap framework and casting the problem as multi-output regularized least squares regression. [sent-106, score-0.602]
</p><p>40 Importantly, with the overlap as targets, different segment tracks can now train on the same set of training examples. [sent-110, score-0.867]
</p><p>41 In consequence, adding more segment tracks adds very little to the training/testing time, unless the number of tracks exceed the feature dimension. [sent-113, score-1.085]
</p><p>42 By storing and updating only these two matrices, we can learn the optimal appearance models of all the segment tracks simultaneously. [sent-114, score-0.865]
</p><p>43 Figure-Ground Segmentation with SpatialTemporal Boundaries A pool of figure-ground segments is generated for each frame by a parametric min-cut [17] figure-ground segmentation algorithm such as [11, 14]. [sent-117, score-0.575]
</p><p>44 The segments are invariant to internal edges since their sizes are controlled by λ and pairwise losses are only counted at the boundaries (when xi xj). [sent-125, score-0.403]
</p><p>45 Compounded with a grid-based enumeration of foreground seed pixels, such a figure-ground segmentation approach can generate several hundreds of segments per image that covers full objects and parts within a consistent framework. [sent-126, score-0.517]
</p><p>46 In order to create more diversity, the resulting boundaries are fed to the segmentation algorithm as E(xi, xj) in 3 different ways: image boundaries only, flow boundaries only, and a 50%-50% linear combination between image boundaries and flow boundaries (Fig. [sent-128, score-0.502]
</p><p>47 The resulting segment pool contains all the segments generated from the three boundary types. [sent-130, score-0.867]
</p><p>48 However sometimes motion boundaries are unreliable, therefore we enumerate segments generated from different types of boundaries (image, flow, and image+flow) in the pool. [sent-133, score-0.442]
</p><p>49 Suppose one segment overlaps 50% with the other segment, then they likely share about  ×  50% similar SIFT feature points. [sent-146, score-0.53]
</p><p>50 For segment Ati, denote its feature vector after d-dimensional RF mapping as Xti, and denote Xt = [X? [sent-157, score-0.491]
</p><p>51 Suppose tuthreere m are n segment tracks, each is represented at frame t by a matching segment Atj ,j = 1, . [sent-164, score-1.131]
</p><p>52 An nt n overlap matrix Vt is computed between all segments i nn t ohev frrlaampe m aantrdi xth Ve matching segments. [sent-168, score-0.463]
</p><p>53 Wt is now the learned model for all the segment tracks. [sent-187, score-0.491]
</p><p>54 Given a new segment At+1,i in frame t + 1, Xt+1,iWt predicts its overlap with hypothetical ground truth segments corresponding to the objects represented by all the segment tracks. [sent-188, score-1.488]
</p><p>55 Greedy Matching To match segments and eliminate redundant segment tracks rapidly, a greedy matching algorithm is proposed to extend segment tracks to new frames. [sent-195, score-1.967]
</p><p>56 Suppose we have segment tracks represented by the weight matrix Wt and need to find matching segments for all tracks in frame It+1. [sent-196, score-1.526]
</p><p>57 , where  Vˆt+1  Vˆt+1  [Vˆ1,  Vˆn]  Vˆj is the prediction vector for segment track Tj on all the segments in frame t + 1. [sent-201, score-1.059]
</p><p>58 For each segment track Tj, we first threshold with crude motion cues (e. [sent-202, score-0.699]
</p><p>59 Among all segments that satisfy the motion threshold, we find k = arg max so that Atk is the segment with the best predicted overlap sjk = for the track Tj. [sent-205, score-1.106]
</p><p>60 If the same segment Aj is matched to multiple tracks, then only the track with the highest score arg maxj sjk is retained (Fig. [sent-206, score-0.699]
</p><p>61 This simple greedy procedure serves as a nonmaximum suppression (NMS) process to reduce the number of segment tracks. [sent-208, score-0.545]
</p><p>62 Importantly, greedy matching retains an order of magnitude fewer segment tracks than Hungarian because of the NMS effect. [sent-211, score-0.887]
</p><p>63 At each frame, NMS is performed among the tracks that match to the same segment and the low-scoring tracks (red) are stopped. [sent-225, score-1.085]
</p><p>64 After matching, each surviving segment track is updated with a new segment at time t 1. [sent-226, score-1.18]
</p><p>65 Then, all the segments at time t + 1 are added to the training set, with the target output computed as the overlaps between the segment and the matching segment of each segment track. [sent-227, score-1.873]
</p><p>66 We start a new track for each segment that has not been matched to any track. [sent-229, score-0.693]
</p><p>67 In the experiments we assume all the objects are present from frame 1, therefore we only start segment tracks in the first 5 frames to strike a balance between speed and robustness to missing segmentations in the first few frames. [sent-230, score-1.038]
</p><p>68 Refinement ference  using Composite  Statistical In-  It would be too optimistic to assume that a perfect segment is always present in the initial segment pool. [sent-232, score-0.982]
</p><p>69 Therefore, in this section we propose an approach to refine the segments in each frame given the learned segment tracks. [sent-234, score-0.887]
</p><p>70 Composite statistical inference (CSI) [22] is a recent inference approach designed to perform inference using predictions on segment statistics. [sent-236, score-0.693]
</p><p>71 In CSI, superpixels are obtained from multiple intersections on the candidate segments, defined as the crudest superpixel partition of the image, so that each superpixel either completely belongs to a segment or stays completely outside. [sent-239, score-0.8]
</p><p>72 Then, real-valued superpixel statistics are defined so that the segment statistics are computable from them. [sent-240, score-0.604]
</p><p>73 This means that there exist a formula to compute the segment statistics given the superpixel statistics. [sent-241, score-0.604]
</p><p>74 With these links, one can maximize the composite likelihood of the noisy predictions on segment statistics to recover the unknown superpixel statistics. [sent-242, score-0.74]
</p><p>75 While [22] deals with semantic segmentation, this paper extends the CSI approach to segment tracking. [sent-244, score-0.491]
</p><p>76 Second, we introduce temporal consistency terms to connect superpixels in adjacent frames, which leads to segment tracks that deform more smoothly over time. [sent-249, score-0.978]
</p><p>77 , T, with Ft being the ground truth segment at frame t. [sent-253, score-0.595]
</p><p>78 Now  suppose the final appearance model for one segment track is WT, we use it to predict = XtiWT for all segments Ati in all frames t = 1, . [sent-267, score-1.136]
</p><p>79 j  Putting everything together, we solve the joint optimization problem on the entire segment track:  mθin  ? [sent-310, score-0.491]
</p><p>80 The segment with the best predicted overlap in each frame is used as a natural initialization for θ. [sent-348, score-0.674]
</p><p>81 After obtaining θ, we adopt the following procedure in [22] in order to output the optimal segment for each frame  given θ: • Sort all θ in descending order. [sent-349, score-0.595]
</p><p>82 • From the start of the sorted list, include superpixel into tFhreo mfin tahle segment one-by-one ta,n ind compute tehrpei overlap V of the current segment using formula (5) from θ. [sent-351, score-1.204]
</p><p>83 •  Stop when V > superpixels 1to j  1−θjθj, −  and output the segment with 1θ in the sorted list. [sent-352, score-0.574]
</p><p>84 The CPMC algorithm [11] is used to compute the segment proposals. [sent-380, score-0.491]
</p><p>85 In the result tables, SPT refers to the online segment tracking algorithm presented in Section 3 without refinement. [sent-390, score-0.615]
</p><p>86 SPT+CSI refers to the results obtained by CSI refinement of the SPT segment tracks. [sent-391, score-0.529]
</p><p>87 Among all segment tracks returned by a video segmenta2197  tion algorithm, we report the performance on the best track w. [sent-394, score-1.024]
</p><p>88 The main competitors are the key segments approach by Lee and Grauman [20] which uses multiple segment proposals followed by an spatial-temporal graph-cut, and Grundmann et al. [sent-400, score-0.884]
</p><p>89 creates a hierarchy of segment tracks, and we report the score of the best segment track among all levels. [sent-404, score-1.154]
</p><p>90 In addition, we adapt a recent tracking-by-detection approach [26] to our segment tracking problem (represented as Pairwise ([26]) in Table 2), in order to make a comparison between our long-term appearance models and tracking based on pairwise appearance similarities. [sent-405, score-0.893]
</p><p>91 In our adoption we put 0 as the unary term (since we do not have detectors) and the similarity computed by the exponential χ2 kernel on our feature descriptors as the pairwise terms connecting segments in adjacent frames. [sent-407, score-0.413]
</p><p>92 CPMC Best represents the average score for the best CPMC segment in each frame, respectively. [sent-412, score-0.491]
</p><p>93 SPT is able to reduce the over 1, 000 segments in each frame from CPMC down to about 60 segment tracks while still capturing most of the objects. [sent-416, score-1.184]
</p><p>94 Overlap of the best segment from each algorithm on SegTrack v2. [sent-419, score-0.491]
</p><p>95 Conclusion In this paper we present a new unsupervised video segmentation approach by tracking a pool of holistic, figureground segments on each frame, generated by a multiple figure-ground segmentation algorithm. [sent-428, score-0.869]
</p><p>96 Long-term appearance models are learnt using a regression-to-overlap framework on many segment tracks initialized from all the segment proposals in the pool. [sent-429, score-1.529]
</p><p>97 By using the same training examples for many segment tracks, we are able to track 2198  frame in the Dri ft ing Car video. [sent-430, score-0.799]
</p><p>98 The segment generation and feature computation steps are still very slow at the moment, which we aim to improve in future work. [sent-431, score-0.491]
</p><p>99 Note in frames other than the first one, training and testing time scales linearly with the number of frames a segment can start on, but overlap computation takes shorter after pruning the targets. [sent-433, score-0.71]
</p><p>100 Besides, an algorithm based on composite statistical inference is proposed to refine the segment tracks using the learnt appearance models as high-order potentials, and shown to be efficient while able to improve the appearance and temporal consistency in many sequences. [sent-436, score-1.222]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('segment', 0.491), ('stk', 0.322), ('tracks', 0.297), ('segments', 0.292), ('csi', 0.249), ('segtrack', 0.185), ('spt', 0.181), ('track', 0.172), ('tracking', 0.124), ('segmentation', 0.119), ('superpixel', 0.113), ('frame', 0.104), ('composite', 0.103), ('proposals', 0.101), ('cpmc', 0.099), ('superpixels', 0.083), ('overlap', 0.079), ('appearance', 0.077), ('penguin', 0.074), ('video', 0.064), ('frog', 0.062), ('bmx', 0.06), ('paradi', 0.06), ('pool', 0.06), ('ati', 0.058), ('boundaries', 0.057), ('frames', 0.055), ('xi', 0.054), ('greedy', 0.054), ('rf', 0.054), ('worm', 0.053), ('temporal', 0.053), ('suppose', 0.049), ('hundreds', 0.049), ('flow', 0.049), ('carreira', 0.049), ('figureground', 0.047), ('nt', 0.047), ('sequences', 0.046), ('xt', 0.046), ('matching', 0.045), ('inference', 0.045), ('learnt', 0.045), ('ht', 0.045), ('unsupervised', 0.044), ('wt', 0.042), ('pools', 0.041), ('dri', 0.041), ('cheet', 0.04), ('hummingbi', 0.04), ('recombines', 0.04), ('wkbjst', 0.04), ('wkfjst', 0.04), ('xj', 0.04), ('overlaps', 0.039), ('refinement', 0.038), ('tk', 0.038), ('exponential', 0.038), ('statistic', 0.038), ('nms', 0.037), ('motion', 0.036), ('monkeydog', 0.036), ('rift', 0.036), ('sjk', 0.036), ('holistic', 0.036), ('statistical', 0.034), ('grundmann', 0.034), ('predictions', 0.033), ('lebanon', 0.033), ('ft', 0.032), ('car', 0.032), ('ct', 0.031), ('regression', 0.031), ('squares', 0.031), ('regularized', 0.031), ('drift', 0.031), ('objects', 0.031), ('adjacent', 0.03), ('saliency', 0.03), ('start', 0.03), ('cholesky', 0.03), ('hungarian', 0.03), ('strike', 0.03), ('besides', 0.029), ('optical', 0.029), ('bi', 0.028), ('adoption', 0.027), ('initialized', 0.027), ('costly', 0.027), ('links', 0.027), ('incrementally', 0.026), ('kernel', 0.026), ('foreground', 0.026), ('updated', 0.026), ('breaks', 0.025), ('deform', 0.024), ('target', 0.024), ('boundary', 0.024), ('imperfect', 0.024), ('metric', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="442-tfidf-1" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>Author: Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, James M. Rehg</p><p>Abstract: We propose an unsupervised video segmentation approach by simultaneously tracking multiple holistic figureground segments. Segment tracks are initialized from a pool of segment proposals generated from a figure-ground segmentation algorithm. Then, online non-local appearance models are trained incrementally for each track using a multi-output regularized least squares formulation. By using the same set of training examples for all segment tracks, a computational trick allows us to track hundreds of segment tracks efficiently, as well as perform optimal online updates in closed-form. Besides, a new composite statistical inference approach is proposed for refining the obtained segment tracks, which breaks down the initial segment proposals and recombines for better ones by utilizing highorder statistic estimates from the appearance model and enforcing temporal consistency. For evaluating the algorithm, a dataset, SegTrack v2, is collected with about 1,000 frames with pixel-level annotations. The proposed framework outperforms state-of-the-art approaches in the dataset, show- ing its efficiency and robustness to challenges in different video sequences.</p><p>2 0.34027028 <a title="442-tfidf-2" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>Author: Shugao Ma, Jianming Zhang, Nazli Ikizler-Cinbis, Stan Sclaroff</p><p>Abstract: We propose Hierarchical Space-Time Segments as a new representation for action recognition and localization. This representation has a two-level hierarchy. The first level comprises the root space-time segments that may contain a human body. The second level comprises multi-grained space-time segments that contain parts of the root. We present an unsupervised method to generate this representation from video, which extracts both static and non-static relevant space-time segments, and also preserves their hierarchical and temporal relationships. Using simple linear SVM on the resultant bag of hierarchical space-time segments representation, we attain better than, or comparable to, state-of-the-art action recognition performance on two challenging benchmark datasets and at the same time produce good action localization results.</p><p>3 0.3008337 <a title="442-tfidf-3" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>Author: Anestis Papazoglou, Vittorio Ferrari</p><p>Abstract: We present a technique for separating foreground objects from the background in a video. Our method isfast, , fully automatic, and makes minimal assumptions about the video. This enables handling essentially unconstrained settings, including rapidly moving background, arbitrary object motion and appearance, and non-rigid deformations and articulations. In experiments on two datasets containing over 1400 video shots, our method outperforms a state-of-theart background subtraction technique [4] as well as methods based on clustering point tracks [6, 18, 19]. Moreover, it performs comparably to recent video object segmentation methods based on objectproposals [14, 16, 27], while being orders of magnitude faster.</p><p>4 0.226467 <a title="442-tfidf-4" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>Author: Christoph Vogel, Konrad Schindler, Stefan Roth</p><p>Abstract: Estimating dense 3D scene flow from stereo sequences remains a challenging task, despite much progress in both classical disparity and 2D optical flow estimation. To overcome the limitations of existing techniques, we introduce a novel model that represents the dynamic 3D scene by a collection of planar, rigidly moving, local segments. Scene flow estimation then amounts to jointly estimating the pixelto-segment assignment, and the 3D position, normal vector, and rigid motion parameters of a plane for each segment. The proposed energy combines an occlusion-sensitive data term with appropriate shape, motion, and segmentation regularizers. Optimization proceeds in two stages: Starting from an initial superpixelization, we estimate the shape and motion parameters of all segments by assigning a proposal from a set of moving planes. Then the pixel-to-segment assignment is updated, while holding the shape and motion parameters of the moving planes fixed. We demonstrate the benefits of our model on different real-world image sets, including the challenging KITTI benchmark. We achieve leading performance levels, exceeding competing 3D scene flow methods, and even yielding better 2D motion estimates than all tested dedicated optical flow techniques.</p><p>5 0.19808699 <a title="442-tfidf-5" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>Author: Wei Xia, Csaba Domokos, Jian Dong, Loong-Fah Cheong, Shuicheng Yan</p><p>Abstract: Numerous existing object segmentation frameworks commonly utilize the object bounding box as a prior. In this paper, we address semantic segmentation assuming that object bounding boxes are provided by object detectors, but no training data with annotated segments are available. Based on a set of segment hypotheses, we introduce a simple voting scheme to estimate shape guidance for each bounding box. The derived shape guidance is used in the subsequent graph-cut-based figure-ground segmentation. The final segmentation result is obtained by merging the segmentation results in the bounding boxes. We conduct an extensive analysis of the effect of object bounding box accuracy. Comprehensive experiments on both the challenging PASCAL VOC object segmentation dataset and GrabCut50 image segmentation dataset show that the proposed approach achieves competitive results compared to previous detection or bounding box prior based methods, as well as other state-of-the-art semantic segmentation methods.</p><p>6 0.18404968 <a title="442-tfidf-6" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<p>7 0.18239543 <a title="442-tfidf-7" href="./iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects.html">320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</a></p>
<p>8 0.17076124 <a title="442-tfidf-8" href="./iccv-2013-Network_Principles_for_SfM%3A_Disambiguating_Repeated_Structures_with_Local_Context.html">289 iccv-2013-Network Principles for SfM: Disambiguating Repeated Structures with Local Context</a></p>
<p>9 0.16332778 <a title="442-tfidf-9" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>10 0.16327663 <a title="442-tfidf-10" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>11 0.15982096 <a title="442-tfidf-11" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>12 0.15736738 <a title="442-tfidf-12" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>13 0.15342925 <a title="442-tfidf-13" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>14 0.15038384 <a title="442-tfidf-14" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>15 0.14596026 <a title="442-tfidf-15" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>16 0.14135443 <a title="442-tfidf-16" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>17 0.12872888 <a title="442-tfidf-17" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>18 0.12682535 <a title="442-tfidf-18" href="./iccv-2013-A_New_Adaptive_Segmental_Matching_Measure_for_Human_Activity_Recognition.html">22 iccv-2013-A New Adaptive Segmental Matching Measure for Human Activity Recognition</a></p>
<p>19 0.12641621 <a title="442-tfidf-19" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>20 0.12514891 <a title="442-tfidf-20" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.261), (1, -0.062), (2, 0.133), (3, 0.127), (4, 0.085), (5, 0.028), (6, -0.12), (7, 0.157), (8, 0.006), (9, -0.011), (10, -0.043), (11, 0.079), (12, 0.132), (13, 0.058), (14, -0.079), (15, -0.035), (16, -0.043), (17, -0.02), (18, -0.083), (19, -0.098), (20, 0.087), (21, -0.101), (22, -0.037), (23, -0.092), (24, -0.013), (25, -0.044), (26, -0.029), (27, -0.022), (28, -0.148), (29, 0.014), (30, -0.067), (31, 0.003), (32, -0.128), (33, -0.078), (34, -0.158), (35, 0.22), (36, 0.074), (37, -0.008), (38, 0.085), (39, -0.113), (40, 0.088), (41, 0.154), (42, -0.01), (43, -0.038), (44, -0.058), (45, 0.119), (46, 0.008), (47, 0.05), (48, -0.077), (49, 0.115)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97704244 <a title="442-lsi-1" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>Author: Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, James M. Rehg</p><p>Abstract: We propose an unsupervised video segmentation approach by simultaneously tracking multiple holistic figureground segments. Segment tracks are initialized from a pool of segment proposals generated from a figure-ground segmentation algorithm. Then, online non-local appearance models are trained incrementally for each track using a multi-output regularized least squares formulation. By using the same set of training examples for all segment tracks, a computational trick allows us to track hundreds of segment tracks efficiently, as well as perform optimal online updates in closed-form. Besides, a new composite statistical inference approach is proposed for refining the obtained segment tracks, which breaks down the initial segment proposals and recombines for better ones by utilizing highorder statistic estimates from the appearance model and enforcing temporal consistency. For evaluating the algorithm, a dataset, SegTrack v2, is collected with about 1,000 frames with pixel-level annotations. The proposed framework outperforms state-of-the-art approaches in the dataset, show- ing its efficiency and robustness to challenges in different video sequences.</p><p>2 0.73601514 <a title="442-lsi-2" href="./iccv-2013-A_New_Adaptive_Segmental_Matching_Measure_for_Human_Activity_Recognition.html">22 iccv-2013-A New Adaptive Segmental Matching Measure for Human Activity Recognition</a></p>
<p>Author: Shahriar Shariat, Vladimir Pavlovic</p><p>Abstract: The problem of human activity recognition is a central problem in many real-world applications. In this paper we propose a fast and effective segmental alignmentbased method that is able to classify activities and interactions in complex environments. We empirically show that such model is able to recover the alignment that leads to improved similarity measures within sequence classes and hence, raises the classification performance. We also apply a bounding technique on the histogram distances to reduce the computation of the otherwise exhaustive search.</p><p>3 0.71610039 <a title="442-lsi-3" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>Author: Shugao Ma, Jianming Zhang, Nazli Ikizler-Cinbis, Stan Sclaroff</p><p>Abstract: We propose Hierarchical Space-Time Segments as a new representation for action recognition and localization. This representation has a two-level hierarchy. The first level comprises the root space-time segments that may contain a human body. The second level comprises multi-grained space-time segments that contain parts of the root. We present an unsupervised method to generate this representation from video, which extracts both static and non-static relevant space-time segments, and also preserves their hierarchical and temporal relationships. Using simple linear SVM on the resultant bag of hierarchical space-time segments representation, we attain better than, or comparable to, state-of-the-art action recognition performance on two challenging benchmark datasets and at the same time produce good action localization results.</p><p>4 0.70804518 <a title="442-lsi-4" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>Author: Anestis Papazoglou, Vittorio Ferrari</p><p>Abstract: We present a technique for separating foreground objects from the background in a video. Our method isfast, , fully automatic, and makes minimal assumptions about the video. This enables handling essentially unconstrained settings, including rapidly moving background, arbitrary object motion and appearance, and non-rigid deformations and articulations. In experiments on two datasets containing over 1400 video shots, our method outperforms a state-of-theart background subtraction technique [4] as well as methods based on clustering point tracks [6, 18, 19]. Moreover, it performs comparably to recent video object segmentation methods based on objectproposals [14, 16, 27], while being orders of magnitude faster.</p><p>5 0.67231011 <a title="442-lsi-5" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<p>Author: Federico Tombari, Alessandro Franchi, Luigi Di_Stefano</p><p>Abstract: Object detection in images withstanding significant clutter and occlusion is still a challenging task whenever the object surface is characterized by poor informative content. We propose to tackle this problem by a compact and distinctive representation of groups of neighboring line segments aggregated over limited spatial supports and invariant to rotation, translation and scale changes. Peculiarly, our proposal allows for leveraging on the inherent strengths of descriptor-based approaches, i.e. robustness to occlusion and clutter and scalability with respect to the size of the model library, also when dealing with scarcely textured objects.</p><p>6 0.6585331 <a title="442-lsi-6" href="./iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects.html">320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</a></p>
<p>7 0.6391533 <a title="442-lsi-7" href="./iccv-2013-Synergistic_Clustering_of_Image_and_Segment_Descriptors_for_Unsupervised_Scene_Understanding.html">412 iccv-2013-Synergistic Clustering of Image and Segment Descriptors for Unsupervised Scene Understanding</a></p>
<p>8 0.57942462 <a title="442-lsi-8" href="./iccv-2013-Scene_Collaging%3A_Analysis_and_Synthesis_of_Natural_Images_with_Semantic_Layers.html">375 iccv-2013-Scene Collaging: Analysis and Synthesis of Natural Images with Semantic Layers</a></p>
<p>9 0.56870872 <a title="442-lsi-9" href="./iccv-2013-A_Unified_Video_Segmentation_Benchmark%3A_Annotation%2C_Metrics_and_Analysis.html">33 iccv-2013-A Unified Video Segmentation Benchmark: Annotation, Metrics and Analysis</a></p>
<p>10 0.56129688 <a title="442-lsi-10" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>11 0.551278 <a title="442-lsi-11" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>12 0.54238361 <a title="442-lsi-12" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>13 0.53493726 <a title="442-lsi-13" href="./iccv-2013-Fingerspelling_Recognition_with_Semi-Markov_Conditional_Random_Fields.html">170 iccv-2013-Fingerspelling Recognition with Semi-Markov Conditional Random Fields</a></p>
<p>14 0.52741998 <a title="442-lsi-14" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>15 0.52552253 <a title="442-lsi-15" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>16 0.51089716 <a title="442-lsi-16" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>17 0.49964148 <a title="442-lsi-17" href="./iccv-2013-Orderless_Tracking_through_Model-Averaged_Posterior_Estimation.html">303 iccv-2013-Orderless Tracking through Model-Averaged Posterior Estimation</a></p>
<p>18 0.49914408 <a title="442-lsi-18" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<p>19 0.4894973 <a title="442-lsi-19" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>20 0.48783746 <a title="442-lsi-20" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.056), (7, 0.016), (12, 0.013), (26, 0.106), (31, 0.045), (34, 0.024), (40, 0.03), (42, 0.079), (48, 0.011), (57, 0.129), (64, 0.167), (73, 0.033), (89, 0.171), (98, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89771086 <a title="442-lda-1" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>Author: Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, James M. Rehg</p><p>Abstract: We propose an unsupervised video segmentation approach by simultaneously tracking multiple holistic figureground segments. Segment tracks are initialized from a pool of segment proposals generated from a figure-ground segmentation algorithm. Then, online non-local appearance models are trained incrementally for each track using a multi-output regularized least squares formulation. By using the same set of training examples for all segment tracks, a computational trick allows us to track hundreds of segment tracks efficiently, as well as perform optimal online updates in closed-form. Besides, a new composite statistical inference approach is proposed for refining the obtained segment tracks, which breaks down the initial segment proposals and recombines for better ones by utilizing highorder statistic estimates from the appearance model and enforcing temporal consistency. For evaluating the algorithm, a dataset, SegTrack v2, is collected with about 1,000 frames with pixel-level annotations. The proposed framework outperforms state-of-the-art approaches in the dataset, show- ing its efficiency and robustness to challenges in different video sequences.</p><p>2 0.87716877 <a title="442-lda-2" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>Author: Shugao Ma, Jianming Zhang, Nazli Ikizler-Cinbis, Stan Sclaroff</p><p>Abstract: We propose Hierarchical Space-Time Segments as a new representation for action recognition and localization. This representation has a two-level hierarchy. The first level comprises the root space-time segments that may contain a human body. The second level comprises multi-grained space-time segments that contain parts of the root. We present an unsupervised method to generate this representation from video, which extracts both static and non-static relevant space-time segments, and also preserves their hierarchical and temporal relationships. Using simple linear SVM on the resultant bag of hierarchical space-time segments representation, we attain better than, or comparable to, state-of-the-art action recognition performance on two challenging benchmark datasets and at the same time produce good action localization results.</p><p>3 0.87684751 <a title="442-lda-3" href="./iccv-2013-Incorporating_Cloud_Distribution_in_Sky_Representation.html">215 iccv-2013-Incorporating Cloud Distribution in Sky Representation</a></p>
<p>Author: Kuan-Chuan Peng, Tsuhan Chen</p><p>Abstract: Most sky models only describe the cloudiness ofthe overall sky by a single category or parameter such as sky index, which does not account for the distribution of the clouds across the sky. To capture variable cloudiness, we extend the concept of sky index to a random field indicating the level of cloudiness of each sky pixel in our proposed sky representation based on the Igawa sky model. We formulate the problem of solving the sky index of every sky pixel as a labeling problem, where an approximate solution can be efficiently found. Experimental results show that our proposed sky model has better expressiveness, stability with respect to variation in camera parameters, and geo-location estimation in outdoor images compared to the uniform sky index model. Potential applications of our proposed sky model include sky image rendering, where sky images can be generated with an arbitrary cloud distribution at any time and any location, previously impossible with traditional sky models.</p><p>4 0.87350476 <a title="442-lda-4" href="./iccv-2013-Constant_Time_Weighted_Median_Filtering_for_Stereo_Matching_and_Beyond.html">88 iccv-2013-Constant Time Weighted Median Filtering for Stereo Matching and Beyond</a></p>
<p>Author: Ziyang Ma, Kaiming He, Yichen Wei, Jian Sun, Enhua Wu</p><p>Abstract: Despite the continuous advances in local stereo matching for years, most efforts are on developing robust cost computation and aggregation methods. Little attention has been seriously paid to the disparity refinement. In this work, we study weighted median filtering for disparity refinement. We discover that with this refinement, even the simple box filter aggregation achieves comparable accuracy with various sophisticated aggregation methods (with the same refinement). This is due to the nice weighted median filtering properties of removing outlier error while respecting edges/structures. This reveals that the previously overlooked refinement can be at least as crucial as aggregation. We also develop the first constant time algorithmfor the previously time-consuming weighted median filter. This makes the simple combination “box aggregation + weighted median ” an attractive solution in practice for both speed and accuracy. As a byproduct, the fast weighted median filtering unleashes its potential in other applications that were hampered by high complexities. We show its superiority in various applications such as depth upsampling, clip-art JPEG artifact removal, and image stylization.</p><p>5 0.87029409 <a title="442-lda-5" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>Author: Siyu Tang, Mykhaylo Andriluka, Anton Milan, Konrad Schindler, Stefan Roth, Bernt Schiele</p><p>Abstract: People tracking in crowded real-world scenes is challenging due to frequent and long-term occlusions. Recent tracking methods obtain the image evidence from object (people) detectors, but typically use off-the-shelf detectors and treat them as black box components. In this paper we argue that for best performance one should explicitly train people detectors on failure cases of the overall tracker instead. To that end, we first propose a novel joint people detector that combines a state-of-the-art single person detector with a detector for pairs of people, which explicitly exploits common patterns of person-person occlusions across multiple viewpoints that are a frequent failure case for tracking in crowded scenes. To explicitly address remaining failure modes of the tracker we explore two methods. First, we analyze typical failures of trackers and train a detector explicitly on these cases. And second, we train the detector with the people tracker in the loop, focusing on the most common tracker failures. We show that our joint multi-person detector significantly improves both de- tection accuracy as well as tracker performance, improving the state-of-the-art on standard benchmarks.</p><p>6 0.86941117 <a title="442-lda-6" href="./iccv-2013-Online_Robust_Non-negative_Dictionary_Learning_for_Visual_Tracking.html">298 iccv-2013-Online Robust Non-negative Dictionary Learning for Visual Tracking</a></p>
<p>7 0.86851698 <a title="442-lda-7" href="./iccv-2013-Finding_Actors_and_Actions_in_Movies.html">166 iccv-2013-Finding Actors and Actions in Movies</a></p>
<p>8 0.8675943 <a title="442-lda-8" href="./iccv-2013-Video_Motion_for_Every_Visible_Point.html">441 iccv-2013-Video Motion for Every Visible Point</a></p>
<p>9 0.85568821 <a title="442-lda-9" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>10 0.85314041 <a title="442-lda-10" href="./iccv-2013-Orderless_Tracking_through_Model-Averaged_Posterior_Estimation.html">303 iccv-2013-Orderless Tracking through Model-Averaged Posterior Estimation</a></p>
<p>11 0.84949911 <a title="442-lda-11" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>12 0.84911299 <a title="442-lda-12" href="./iccv-2013-Cross-View_Action_Recognition_over_Heterogeneous_Feature_Spaces.html">99 iccv-2013-Cross-View Action Recognition over Heterogeneous Feature Spaces</a></p>
<p>13 0.84469932 <a title="442-lda-13" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>14 0.83984077 <a title="442-lda-14" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>15 0.83768845 <a title="442-lda-15" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>16 0.83673441 <a title="442-lda-16" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>17 0.82589549 <a title="442-lda-17" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>18 0.82587206 <a title="442-lda-18" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>19 0.82271671 <a title="442-lda-19" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>20 0.82033288 <a title="442-lda-20" href="./iccv-2013-A_New_Adaptive_Segmental_Matching_Measure_for_Human_Activity_Recognition.html">22 iccv-2013-A New Adaptive Segmental Matching Measure for Human Activity Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
