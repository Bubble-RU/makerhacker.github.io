<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-212" href="#">iccv2013-212</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</h1>
<br/><p>Source: <a title="iccv-2013-212-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Lu_Image_Set_Classification_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Jiwen Lu, Gang Wang, Pierre Moulin</p><p>Abstract: This paper presents a new approach for image set classification, where each training and testing example contains a set of image instances of an object captured from varying viewpoints or under varying illuminations. While a number of image set classification methods have been proposed in recent years, most of them model each image set as a single linear subspace or mixture of linear subspaces, which may lose some discriminative information for classification. To address this, we propose exploring multiple order statistics as features of image sets, and develop a localized multikernel metric learning (LMKML) algorithm to effectively combine different order statistics information for classification. Our method achieves the state-of-the-art performance on four widely used databases including the Honda/UCSD, CMU Mobo, and Youtube face datasets, and the ETH-80 object dataset.</p><p>Reference: <a title="iccv-2013-212-reference" href="../iccv2013_reference/iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract This paper presents a new approach for image set classification, where each training and testing example contains a set of image instances of an object captured from varying viewpoints or under varying illuminations. [sent-8, score-0.035]
</p><p>2 While a number of image set classification methods have been proposed in recent years, most of them model each image set as a single linear subspace or mixture of linear subspaces, which may lose some discriminative information for classification. [sent-9, score-0.205]
</p><p>3 To address this, we propose exploring multiple order statistics as features of image sets, and develop a localized multikernel metric learning (LMKML) algorithm to effectively combine different order statistics information for classification. [sent-10, score-1.434]
</p><p>4 Introduction  Image set classification has attracted increasing interest in computer vision and pattern recognition in recent years [1, 4, 6, 7, 10, 15, 16, 17, 20, 25, 28, 30, 34, 37, 40] due to its wide potential applications such as visual surveillance and multi-view image analysis. [sent-13, score-0.124]
</p><p>5 One representative application of image set classification is the video-based face recognition problem, where each gallery and probe face video can be considered as an image set and the characteristics of the image set are used for person identification. [sent-14, score-0.148]
</p><p>6 Different from the conventional image classification problem where each training and testing example is a single image, for image set classification, each training and testing example contains a set of image instances. [sent-15, score-0.152]
</p><p>7 However, it is also challenging to exploit discriminative information of image sets as there  Figure1. [sent-17, score-0.115]
</p><p>8 Foreachimagest,we  first compute its multiple order statistics as feature representation. [sent-19, score-0.511]
</p><p>9 For each order statistic, we compute a kernel matrix to measure the pairwise similarity of two image sets. [sent-20, score-0.251]
</p><p>10 Then, we learn a dis-  tance metric by using the localized multi-kernel metric learning (LMKML) method to combine the different order statistics. [sent-21, score-0.746]
</p><p>11 Lastly, the nearest neighbor classifier is used for classification. [sent-22, score-0.085]
</p><p>12 There has been a number of work on image set classification over the past two decades [1, 4, 10, 16, 21, 23, 30, 34, 35, 38]. [sent-24, score-0.082]
</p><p>13 However, to our best knowledge, most existing image set classification methods usually make some prior assumptions such as singe Gaussian, Gaussian mixture models, subspace or manifold models to represent image sets. [sent-25, score-0.129]
</p><p>14 In many practical applications, these assumptions may not be held, especially when there are large and complex data variations within a set. [sent-26, score-0.087]
</p><p>15 Moreover, the models learned based on these assumptions may also lose some discriminative information for classification. [sent-27, score-0.17]
</p><p>16 Given an image set, we compute its holistic multiple order statistics as features for set representation. [sent-29, score-0.575]
</p><p>17 Compared with most existing image set modeling methods [4, 16, 35], our multiple order statistics features can more robustly capture the distribution of image instances within a set in a holistic way because no parameter estimation is required. [sent-30, score-0.575]
</p><p>18 Moreover, they are also less sensitive to noise because noisy samples can be largely filtered out in the extracted statistic features. [sent-31, score-0.422]
</p><p>19 Related work Image Set Classification: There has been a growing interest in developing new algorithms for image set classification in recent years [1, 4, 5, 9, 10, 14, 15, 16, 21, 23, 30, 33, 35], and they can be mainly classified into two categories: parametric and nonparametric. [sent-37, score-0.124]
</p><p>20 Compared to these works, the contribution of our work is two-fold: 1) extracting multiple order statistics features to reliably represent an image set; 2) a localized multi-kernel metric learning algorithm. [sent-38, score-0.954]
</p><p>21 While [34] explored the second-order statistics of image sets for feature representation, our approach can extract more discriminative information because it considers and utilizes multiple different order statistics of image sets. [sent-39, score-0.987]
</p><p>22 We also achieve state-of-the-art performance on the image set classification problem with existing publicly available datasets. [sent-40, score-0.082]
</p><p>23 Multiple Kernel Learning: There have been extensive work on multiple kernel learning in the literature [2, 8, 11, 13, 18, 22, 26, 29, 32, 36, 39, 41]. [sent-41, score-0.225]
</p><p>24 While many efforts have been made including classification [2, 11, 29, 36],  clustering [39], transfer learning [8], and dimensionality reduction [26], little progress has been made in metric learning with multiple kernel learning. [sent-42, score-0.52]
</p><p>25 [32] proposed a metric learning method with multiple kernels by learning a universal weight vector over the whole space. [sent-44, score-0.356]
</p><p>26 Differently, our proposed LMKML algorithm learns an adaptive weight to each local region in the kernel space when learning the distance metric. [sent-45, score-0.221]
</p><p>27 Hence, our approach is complementary to existing multiple kernel learning methods. [sent-46, score-0.275]
</p><p>28 For each image set, we first extract its multiple order statistics for set modeling. [sent-49, score-0.475]
</p><p>29 For each order statistic, we compute a kernel matrix to measure the pairwise similarity of two image sets. [sent-50, score-0.251]
</p><p>30 Then, we propose a LMKML method to learn a discriminative, localized distance metric to combine statistics information at different orders. [sent-51, score-0.815]
</p><p>31 Lastly, the nearest neighbor classifier is employed for classification. [sent-52, score-0.085]
</p><p>32 Given each image set, we extract the following different order statistics information as features to represent  ×  the set. [sent-58, score-0.474]
</p><p>33 These multiple order statistics can reliably describe the distribution of image samples of a set, and hence can be used as image set features. [sent-59, score-0.623]
</p><p>34 H iser ae 푑 “×⊗”푑 dmeantoritxes, atnhed 풯Kr oisne ac 푑k ×er product nosfo trw, ore mspaetcriticveesly. [sent-61, score-0.224]
</p><p>35 N Hoteer eth “a⊗t more higherorder statistics can also be computed for each image set. [sent-63, score-0.399]
</p><p>36 However, we only consider these three in our approach because it is very expensive to compute higher-order statistics features. [sent-64, score-0.361]
</p><p>37 Compared with previous image set representation methods, there are several advantages to model image sets with multiple order statistics information: 1. [sent-65, score-0.524]
</p><p>38 No assumption on the data distribution is required and the statistics features can be computed from an image set containing any number of samples. [sent-66, score-0.399]
</p><p>39 Different order statistics information can characterize the image set from different perspectives. [sent-68, score-0.436]
</p><p>40 For example, the mean vector roughly reflects the position of the object in the high-dimensional space, and the covariance matrix represents the variance of each individual feature in the diagonal elements and measures the correlations of different features in the non-diagonal elements. [sent-69, score-0.169]
</p><p>41 Hence, these statistics features can provide complementary information to represent the image set. [sent-70, score-0.449]
</p><p>42 Illustration of the importance of different order statistics in image set classification. [sent-72, score-0.436]
</p><p>43 In this figure, the squares and triangles demote two different image sets. [sent-73, score-0.032]
</p><p>44 The first-order statistics are the same and the second-order statistics are different in (a), where the first-order statistics are different and the second-order statistics are the same in (b). [sent-74, score-1.444]
</p><p>45 Hence, we can see that different order statistics contribute different discriminative and complementary information for image set classification. [sent-75, score-0.552]
</p><p>46 Figure 2 shows a toy example to illustrate that different order statistics contain different discriminative information for image set classification. [sent-76, score-0.502]
</p><p>47 These statistic features are more robust to outlines since they are statistics of all the samples in the image set and the effect of the noisy samples can be largely alleviated, especially compared to the previous nearest sample pair based image classification method-  s [4, 16]. [sent-78, score-1.04]
</p><p>48 Localized Multi-Kernel Metric Learning Having extracted multiple order statistics features, we perform classification by using the nearest neighbor classifier, which involves calculating the similarity between two image sets. [sent-81, score-0.685]
</p><p>49 We compare two statistics features in the kernel space, given the great success of kernel learning [26, 39]. [sent-82, score-0.717]
</p><p>50 This is equivalent to mapping the original statistic features to a new space, and calculating the dot product in the new space. [sent-83, score-0.451]
</p><p>51 We write the new feature for the 푝th statistic feature as 휙푝, and the mapping function as 푅푑푝 → ℱ, where 푅푑푝 is the original feature space and ℱ is the mapped highdimeinss tihoen aolr space. [sent-84, score-0.584]
</p><p>52 Though 휙ac푝e i sa usually implicit, we fiigrhstconsider it as an explicit feature vector for simplicity. [sent-85, score-0.036]
</p><p>53 Later, we will show any manipulation based on 휙푝 can be represented based on kernel values by using the kernel trick. [sent-86, score-0.264]
</p><p>54 Similar to [2, 11], we assume different order statistic features can be mapped to a common high-dimensional feature space. [sent-87, score-0.543]
</p><p>55 And we aim to learn a distance metric to enforce objects from the same category to be close, and objects from different categories to be far away, in the learned metric space. [sent-88, score-0.389]
</p><p>56 Different from [2, 11] which assume the weights of different types of features (which are the different order statistic features here) are the same for all objects, we argue that weights should be data-adaptive. [sent-89, score-0.48]
</p><p>57 We  formulate our learning problem based on this intuition as below, and call it Localized Multi-Kernel Metric Learning (LMKML). [sent-91, score-0.054]
</p><p>58 Write 푆 = [푆1 , 푆2, , 푆푁] as the training set of 푁 different image sets, wh,e⋅⋅re⋅ ,푆푆푖 = [푠푖1 , 푠푖2 , , 푠푖푛푖] denotes the 푖th image set, 1 ≤ 푖 ≤ 푁, and 푛푖 is t,h⋅e⋅ ⋅nu ,m푠ber of samples tinh itmhiasg image 1s e≤t. [sent-92, score-0.132]
</p><p>59 푖F ≤or 푁 푁ea,c ahn image set 푆푖, we compute its first-, second-, and third-order statistics 푚푖, 퐶푖 and 풯푖, respectively. [sent-93, score-0.361]
</p><p>60 Let 푋푝 = [푥푝1, 푥2푝, ⋅ ⋅ ⋅ , 푥푝푁] be the 푝th satnadtis 풯tic feature set of all training samples, ,an푥d 푥푖푝 ∈ 푅푑푝 denotes the 푝th statistic feature extracted from the 푖th∈ image set 푆푖, where 1 ≤ 푝 ≤ 푃. [sent-94, score-0.436]
</p><p>61 i Icsn f tehaistu wreosr kfo, r푃 image sse wt representation. [sent-96, score-0.032]
</p><p>62 푀 is the distance metric to be learned in the high-dimensional space ℱ. [sent-98, score-0.194]
</p><p>63 The distance between two image sets 푆푖 and 푆푗 supnadceer ℱ푀. [sent-99, score-0.084]
</p><p>64 i sT: ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ∑푃  푑(푆푖, 푆푗) =  ∑휂푝(휙푖푝)(휙푖푝  − 휙푗푝)푇푀(휙푖푝  − 휙푗푝)휂푝(휙푗푝)  (4)  푝∑= ∑1  where 휂푝(휙푖푝) is a gating function to generate different positive weighting numbers for different 휙푖푝, which will be detailed later. [sent-100, score-0.312]
</p><p>65 It is obvious that previous global kernel weighting algorithms [2, 11] can be considered as a special case of our method, where 휂푝(휙푖푝) is the same for any 휙푖푝. [sent-102, score-0.132]
</p><p>66 To learn a distance metric 푀, we maximize inter-class variations and minimize intra-class variations, simultane-  ously. [sent-103, score-0.27]
</p><p>67 The objective function is formulated as:  푖∑,푁푗=1 푑(푁푆푖퐶,푆−푗)− 푖∑,푁푗=1 푑(푁푆푖퐶,푆+푗)  m푀ax퐽 =  (푆푖 ,푆∑푗 ∑) ∈퐶−(푆푖 ,푆∑푗 ∑) ∈퐶+  ×  (5)  where 퐶−and 퐶+ denote the inter-class and intra-class sample pairs in the training set, and 푁퐶−and 푁퐶+ denote the number of pairs in these two sets, respectively. [sent-104, score-0.035]
</p><p>68 We can seek a nonsquare matrix 푊 (푊 = [푤1 , 푤2 , ⋅ ⋅ ⋅ , 푤푑]) of size 푑ℱ 푑, where 푑ℱ is the dimensionality o,f⋅ t⋅h⋅e , high-dimensional feature space, and 푑 is the number of basis in 푊, such that 푀 =  푊푊푇  (6)  Combining Eqs. [sent-106, score-0.08]
</p><p>69 aHteunrec se,p we use etchea kseer ntheel ftroircmk method [3] by expressing the basis 푤푘 as a linear combination of all the training samples in the mapped space, i. [sent-108, score-0.189]
</p><p>70 푖푝 i ss ath 푁e 푖 ×th 1c coloulummnn o vfe tchteo r 푝 athnd k itersn 푖etlh m enattryrix i s퐾 푢푝. [sent-113, score-0.104]
</p><p>71 Here 퐾푝 is an 푁 푁 kernel matrix, calculated from the 푝th statistiics f aenatu 푁re × using tehren eRlB mFa ktreirxn,e cl a blectuwlaeteedn e fracomh pair of image set. [sent-114, score-0.184]
</p><p>72 푝푗)푇휂푝(휙푗푝)  (14)  Now we discuss how to choose the gating function 휂푝(휙푖푝). [sent-124, score-0.312]
</p><p>73 There are a number of possible functions which could be used as the gating function. [sent-125, score-0.312]
</p><p>74 In this work, the gating function is selected as follow [11]: 휂푝(휙푖푝)  =  푃exp(ℎ푝푇휙푖푝+ 푏푝) 푝∑=1exp(ℎ푝푇휙푖푝+ 푏푝)  (15)  where ℎ푝 and 푏푝 are the parameters of this grating function. [sent-126, score-0.364]
</p><p>75 There are two reasons to select this gating function: 1) this function is monotonically increasing with the importance of 휙푖푝; 2) this function can guarantees nonnegative weights and it is easy to obtain the derivatives with respect to ℎ푝 and 푏푝. [sent-127, score-0.312]
</p><p>76 Since 휙푖푝 is implicit and its dimension is unknown, we express ℎ푝푇휙푖푝 as follow similar to Eq. [sent-128, score-0.042]
</p><p>77 푝푖  (16)  Then, the gating function can be written as: 휂푝(휙푖푝)  =  푃exp(푎푝푇퐾. [sent-130, score-0.312]
</p><p>78 (12) because we aim to learn but have to infer 푎푝 and 푏푝 simultaneously. [sent-134, score-0.036]
</p><p>79 Hence, We solve this problem in an iterative manner inspired by some recent EM-like multiple kernel learning algorithms [26, 32]. [sent-135, score-0.225]
</p><p>80 The basic idea is to fix 푎푝 and 푏푝, update and fix update 푎푝 and 푏푝, iteratively. [sent-136, score-0.17]
</p><p>81 aWine 푈ad bdy a ocolvnisntgra itnhte 푈 m푇in푈im =iz 퐼ti oton restrict the scale of such that the optimization problem in Eq. [sent-139, score-0.086]
</p><p>82 Then, can be obtained by solving the following eigenvalue problem  푈  푈, ≤ ≤  푈  푈  (퐵1 − 퐵2)푢 = 휆푢. [sent-141, score-0.073]
</p><p>83 An 푁 푑 transformation matrix  푈  ×  [푢 휆1 , 푢2 , ⋅ ⋅ ⋅ , ≥푢푑] 휆 can Aben 푁obta ×in 푑ed t. [sent-144, score-0.044]
</p><p>84 r Having ob,⋅ta⋅i⋅ne ,d푢 푈, we use the gradient descent method to update 푎푝 and 푏푝 as follows: =≥  푎푡푝+1= 푎푡푝− 훼∂∂푎퐽푝  (19)  푏푡푝+1= 푏푡푝− 훼∂∂푏퐽푝  (20)  where 훼 is the learning rate and set as 0. [sent-145, score-0.106]
</p><p>85 Then, we update 푈 by re-solving the eigenvalue equation in Eq. [sent-150, score-0.125]
</p><p>86 332  Algorithm  1:  Algorithm 1: LMKML Input:  Training  set:  푃  푁  ×  푁  LMKML  kernels  computed  from  Input: Training set: 푃 푁 푁 kernels computed from T푁r image sets, 푃ite 푁ra ×tio 푁n n kuemrnbeelrs c푇o, mfepaututered dimension 푑, convergence error 휖. [sent-154, score-0.1]
</p><p>87 Output: Transformation matrix 푈 and parameters 푎푝  Step 1 (Inaitnidali 푏z푝a. [sent-155, score-0.044]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lmkml', 0.528), ('statistics', 0.361), ('statistic', 0.329), ('gating', 0.312), ('localized', 0.188), ('metric', 0.159), ('kernel', 0.132), ('classification', 0.082), ('order', 0.075), ('eigenvalue', 0.073), ('discriminative', 0.066), ('mapped', 0.065), ('holistic', 0.062), ('lastly', 0.057), ('lose', 0.057), ('samples', 0.056), ('learning', 0.054), ('singapore', 0.053), ('hence', 0.052), ('update', 0.052), ('oisne', 0.052), ('athnd', 0.052), ('inno', 0.052), ('vfe', 0.052), ('tehren', 0.052), ('iser', 0.052), ('obta', 0.052), ('aben', 0.052), ('emra', 0.052), ('grating', 0.052), ('iesr', 0.052), ('jiwen', 0.052), ('mfa', 0.052), ('mobo', 0.052), ('moulin', 0.052), ('twhen', 0.052), ('wanggang', 0.052), ('tensor', 0.051), ('covariance', 0.051), ('complementary', 0.05), ('kernels', 0.05), ('sets', 0.049), ('kfo', 0.048), ('bdy', 0.048), ('tati', 0.048), ('multikernel', 0.048), ('nanyang', 0.048), ('sist', 0.048), ('tthriex', 0.048), ('assumptions', 0.047), ('pierre', 0.045), ('athne', 0.045), ('snet', 0.045), ('nearest', 0.044), ('matrix', 0.044), ('write', 0.043), ('atnhed', 0.043), ('calculating', 0.043), ('implicit', 0.042), ('years', 0.042), ('product', 0.041), ('neighbor', 0.041), ('oofd', 0.041), ('gang', 0.041), ('goef', 0.041), ('tinh', 0.041), ('email', 0.041), ('th', 0.041), ('reliably', 0.04), ('variations', 0.04), ('illinois', 0.04), ('wheh', 0.04), ('multiple', 0.039), ('tance', 0.039), ('tihoen', 0.039), ('technological', 0.039), ('features', 0.038), ('higherorder', 0.038), ('oton', 0.038), ('largely', 0.037), ('iz', 0.037), ('outlines', 0.037), ('feature', 0.036), ('tic', 0.036), ('ite', 0.036), ('ore', 0.036), ('ris', 0.036), ('combine', 0.036), ('learn', 0.036), ('training', 0.035), ('alleviated', 0.035), ('distance', 0.035), ('trans', 0.034), ('expressing', 0.033), ('gallery', 0.033), ('der', 0.033), ('probe', 0.033), ('fix', 0.033), ('triangles', 0.032), ('sse', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="212-tfidf-1" href="./iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning.html">212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</a></p>
<p>Author: Jiwen Lu, Gang Wang, Pierre Moulin</p><p>Abstract: This paper presents a new approach for image set classification, where each training and testing example contains a set of image instances of an object captured from varying viewpoints or under varying illuminations. While a number of image set classification methods have been proposed in recent years, most of them model each image set as a single linear subspace or mixture of linear subspaces, which may lose some discriminative information for classification. To address this, we propose exploring multiple order statistics as features of image sets, and develop a localized multikernel metric learning (LMKML) algorithm to effectively combine different order statistics information for classification. Our method achieves the state-of-the-art performance on four widely used databases including the Honda/UCSD, CMU Mobo, and Youtube face datasets, and the ETH-80 object dataset.</p><p>2 0.11618028 <a title="212-tfidf-2" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>Author: Qiong Cao, Yiming Ying, Peng Li</p><p>Abstract: Recently, there is a considerable amount of efforts devoted to the problem of unconstrained face verification, where the task is to predict whether pairs of images are from the same person or not. This problem is challenging and difficult due to the large variations in face images. In this paper, we develop a novel regularization framework to learn similarity metrics for unconstrained face verification. We formulate its objective function by incorporating the robustness to the large intra-personal variations and the discriminative power of novel similarity metrics. In addition, our formulation is a convex optimization problem which guarantees the existence of its global solution. Experiments show that our proposed method achieves the state-of-the-art results on the challenging Labeled Faces in the Wild (LFW) database [10].</p><p>3 0.10409589 <a title="212-tfidf-3" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>Author: Stefanos Zafeiriou, Irene Kotsia</p><p>Abstract: Kernels have been a common tool of machine learning and computer vision applications for modeling nonlinearities and/or the design of robust1 similarity measures between objects. Arguably, the class of positive semidefinite (psd) kernels, widely known as Mercer’s Kernels, constitutes one of the most well-studied cases. For every psd kernel there exists an associated feature map to an arbitrary dimensional Hilbert space H, the so-called feature space. Tdihme mnsaiionn reason ebreth sipnadc ep s Hd ,ke threne slos’-c c aplolpedul aferiattyu rise the fact that classification/regression techniques (such as Support Vector Machines (SVMs)) and component analysis algorithms (such as Kernel Principal Component Analysis (KPCA)) can be devised in H, without an explicit defisnisiti (oKnP of t)h)e c feature map, only by using athne xkperlniceitl (dtehfeso-called kernel trick). Recently, due to the development of very efficient solutions for large scale linear SVMs and for incremental linear component analysis, the research to- wards finding feature map approximations for classes of kernels has attracted significant interest. In this paper, we attempt the derivation of explicit feature maps of a recently proposed class of kernels, the so-called one-shot similarity kernels. We show that for this class of kernels either there exists an explicit representation in feature space or the kernel can be expressed in such a form that allows for exact incremental learning. We theoretically explore the properties of these kernels and show how these kernels can be used for the development of robust visual tracking, recognition and deformable fitting algorithms. 1Robustness may refer to either the presence of outliers and noise the robustness to a class of transformations (e.g., translation). or to ∗ Irene Kotsia ,†,? ∗Electronics Laboratory, Department of Physics, University of Patras, Greece ?School of Science and Technology, Middlesex University, London i .kot s i @mdx . ac .uk a</p><p>4 0.09897805 <a title="212-tfidf-4" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>Author: Sadeep Jayasumana, Mathieu Salzmann, Hongdong Li, Mehrtash Harandi</p><p>Abstract: We propose a framework for 2D shape analysis using positive definite kernels defined on Kendall’s shape manifold. Different representations of 2D shapes are known to generate different nonlinear spaces. Due to the nonlinearity of these spaces, most existing shape classification algorithms resort to nearest neighbor methods and to learning distances on shape spaces. Here, we propose to map shapes on Kendall’s shape manifold to a high dimensional Hilbert space where Euclidean geometry applies. To this end, we introduce a kernel on this manifold that permits such a mapping, and prove its positive definiteness. This kernel lets us extend kernel-based algorithms developed for Euclidean spaces, such as SVM, MKL and kernel PCA, to the shape manifold. We demonstrate the benefits of our approach over the state-of-the-art methods on shape classification, clustering and retrieval.</p><p>5 0.084746353 <a title="212-tfidf-5" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>Author: Renliang Weng, Jiwen Lu, Junlin Hu, Gao Yang, Yap-Peng Tan</p><p>Abstract: Over the past two decades, a number of face recognition methods have been proposed in the literature. Most of them use holistic face images to recognize people. However, human faces are easily occluded by other objects in many real-world scenarios and we have to recognize the person of interest from his/her partial faces. In this paper, we propose a new partial face recognition approach by using feature set matching, which is able to align partial face patches to holistic gallery faces automatically and is robust to occlusions and illumination changes. Given each gallery image and probe face patch, we first detect keypoints and extract their local features. Then, we propose a Metric Learned ExtendedRobust PointMatching (MLERPM) method to discriminatively match local feature sets of a pair of gallery and probe samples. Lastly, the similarity of two faces is converted as the distance between two feature sets. Experimental results on three public face databases are presented to show the effectiveness of the proposed approach.</p><p>6 0.077301167 <a title="212-tfidf-6" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>7 0.072131701 <a title="212-tfidf-7" href="./iccv-2013-Discriminant_Tracking_Using_Tensor_Representation_with_Semi-supervised_Improvement.html">119 iccv-2013-Discriminant Tracking Using Tensor Representation with Semi-supervised Improvement</a></p>
<p>8 0.071469426 <a title="212-tfidf-8" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>9 0.070797876 <a title="212-tfidf-9" href="./iccv-2013-From_Point_to_Set%3A_Extend_the_Learning_of_Distance_Metrics.html">177 iccv-2013-From Point to Set: Extend the Learning of Distance Metrics</a></p>
<p>10 0.068713404 <a title="212-tfidf-10" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>11 0.06524618 <a title="212-tfidf-11" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>12 0.064599827 <a title="212-tfidf-12" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>13 0.062179428 <a title="212-tfidf-13" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>14 0.062069006 <a title="212-tfidf-14" href="./iccv-2013-Sequential_Bayesian_Model_Update_under_Structured_Scene_Prior_for_Semantic_Road_Scenes_Labeling.html">386 iccv-2013-Sequential Bayesian Model Update under Structured Scene Prior for Semantic Road Scenes Labeling</a></p>
<p>15 0.060830709 <a title="212-tfidf-15" href="./iccv-2013-A_New_Image_Quality_Metric_for_Image_Auto-denoising.html">23 iccv-2013-A New Image Quality Metric for Image Auto-denoising</a></p>
<p>16 0.060481399 <a title="212-tfidf-16" href="./iccv-2013-Joint_Learning_of_Discriminative_Prototypes_and_Large_Margin_Nearest_Neighbor_Classifiers.html">222 iccv-2013-Joint Learning of Discriminative Prototypes and Large Margin Nearest Neighbor Classifiers</a></p>
<p>17 0.059213687 <a title="212-tfidf-17" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>18 0.05803163 <a title="212-tfidf-18" href="./iccv-2013-Nonparametric_Blind_Super-resolution.html">293 iccv-2013-Nonparametric Blind Super-resolution</a></p>
<p>19 0.058026694 <a title="212-tfidf-19" href="./iccv-2013-Latent_Space_Sparse_Subspace_Clustering.html">232 iccv-2013-Latent Space Sparse Subspace Clustering</a></p>
<p>20 0.057873085 <a title="212-tfidf-20" href="./iccv-2013-Unbiased_Metric_Learning%3A_On_the_Utilization_of_Multiple_Datasets_and_Web_Images_for_Softening_Bias.html">431 iccv-2013-Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.151), (1, 0.038), (2, -0.041), (3, -0.045), (4, -0.046), (5, 0.028), (6, 0.032), (7, 0.046), (8, 0.021), (9, -0.052), (10, -0.021), (11, -0.064), (12, -0.006), (13, -0.062), (14, 0.017), (15, -0.021), (16, 0.002), (17, -0.03), (18, 0.014), (19, -0.03), (20, -0.007), (21, -0.0), (22, -0.002), (23, -0.026), (24, 0.026), (25, 0.054), (26, 0.024), (27, 0.086), (28, -0.028), (29, 0.08), (30, 0.013), (31, -0.034), (32, -0.062), (33, -0.048), (34, 0.048), (35, 0.042), (36, 0.014), (37, -0.075), (38, -0.017), (39, -0.007), (40, 0.018), (41, 0.037), (42, 0.032), (43, 0.048), (44, 0.091), (45, -0.038), (46, 0.004), (47, 0.037), (48, 0.055), (49, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94141304 <a title="212-lsi-1" href="./iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning.html">212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</a></p>
<p>Author: Jiwen Lu, Gang Wang, Pierre Moulin</p><p>Abstract: This paper presents a new approach for image set classification, where each training and testing example contains a set of image instances of an object captured from varying viewpoints or under varying illuminations. While a number of image set classification methods have been proposed in recent years, most of them model each image set as a single linear subspace or mixture of linear subspaces, which may lose some discriminative information for classification. To address this, we propose exploring multiple order statistics as features of image sets, and develop a localized multikernel metric learning (LMKML) algorithm to effectively combine different order statistics information for classification. Our method achieves the state-of-the-art performance on four widely used databases including the Honda/UCSD, CMU Mobo, and Youtube face datasets, and the ETH-80 object dataset.</p><p>2 0.8343603 <a title="212-lsi-2" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>Author: Zheyun Feng, Rong Jin, Anil Jain</p><p>Abstract: One of the key challenges in search-based image annotation models is to define an appropriate similarity measure between images. Many kernel distance metric learning (KML) algorithms have been developed in order to capture the nonlinear relationships between visual features and semantics ofthe images. Onefundamental limitation in applying KML to image annotation is that it requires converting image annotations into binary constraints, leading to a significant information loss. In addition, most KML algorithms suffer from high computational cost due to the requirement that the learned matrix has to be positive semi-definitive (PSD). In this paper, we propose a robust kernel metric learning (RKML) algorithm based on the regression technique that is able to directly utilize image annotations. The proposed method is also computationally more efficient because PSD property is automatically ensured by regression. We provide the theoretical guarantee for the proposed algorithm, and verify its efficiency and effectiveness for image annotation by comparing it to state-of-the-art approaches for both distance metric learning and image annotation. ,</p><p>3 0.81606799 <a title="212-lsi-3" href="./iccv-2013-From_Point_to_Set%3A_Extend_the_Learning_of_Distance_Metrics.html">177 iccv-2013-From Point to Set: Extend the Learning of Distance Metrics</a></p>
<p>Author: Pengfei Zhu, Lei Zhang, Wangmeng Zuo, David Zhang</p><p>Abstract: Most of the current metric learning methods are proposed for point-to-point distance (PPD) based classification. In many computer vision tasks, however, we need to measure the point-to-set distance (PSD) and even set-to-set distance (SSD) for classification. In this paper, we extend the PPD based Mahalanobis distance metric learning to PSD and SSD based ones, namely point-to-set distance metric learning (PSDML) and set-to-set distance metric learning (SSDML), and solve them under a unified optimization framework. First, we generate positive and negative sample pairs by computing the PSD and SSD between training samples. Then, we characterize each sample pair by its covariance matrix, and propose a covariance kernel based discriminative function. Finally, we tackle the PSDML and SSDMLproblems by using standard support vector machine solvers, making the metric learning very efficient for multiclass visual classification tasks. Experiments on gender classification, digit recognition, object categorization and face recognition show that the proposed metric learning methods can effectively enhance the performance of PSD and SSD based classification.</p><p>4 0.79534543 <a title="212-lsi-4" href="./iccv-2013-Joint_Learning_of_Discriminative_Prototypes_and_Large_Margin_Nearest_Neighbor_Classifiers.html">222 iccv-2013-Joint Learning of Discriminative Prototypes and Large Margin Nearest Neighbor Classifiers</a></p>
<p>Author: Martin Köstinger, Paul Wohlhart, Peter M. Roth, Horst Bischof</p><p>Abstract: In this paper, we raise important issues concerning the evaluation complexity of existing Mahalanobis metric learning methods. The complexity scales linearly with the size of the dataset. This is especially cumbersome on large scale or for real-time applications with limited time budget. To alleviate this problem we propose to represent the dataset by a fixed number of discriminative prototypes. In particular, we introduce a new method that jointly chooses the positioning of prototypes and also optimizes the Mahalanobis distance metric with respect to these. We show that choosing the positioning of the prototypes and learning the metric in parallel leads to a drastically reduced evaluation effort while maintaining the discriminative essence of the original dataset. Moreover, for most problems our method performing k-nearest prototype (k-NP) classification on the condensed dataset leads to even better generalization compared to k-NN classification using all data. Results on a variety of challenging benchmarks demonstrate the power of our method. These include standard machine learning datasets as well as the challenging Public Fig- ures Face Database. On the competitive machine learning benchmarks we are comparable to the state-of-the-art while being more efficient. On the face benchmark we clearly outperform the state-of-the-art in Mahalanobis metric learning with drastically reduced evaluation effort.</p><p>5 0.78271931 <a title="212-lsi-5" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>Author: Stefanos Zafeiriou, Irene Kotsia</p><p>Abstract: Kernels have been a common tool of machine learning and computer vision applications for modeling nonlinearities and/or the design of robust1 similarity measures between objects. Arguably, the class of positive semidefinite (psd) kernels, widely known as Mercer’s Kernels, constitutes one of the most well-studied cases. For every psd kernel there exists an associated feature map to an arbitrary dimensional Hilbert space H, the so-called feature space. Tdihme mnsaiionn reason ebreth sipnadc ep s Hd ,ke threne slos’-c c aplolpedul aferiattyu rise the fact that classification/regression techniques (such as Support Vector Machines (SVMs)) and component analysis algorithms (such as Kernel Principal Component Analysis (KPCA)) can be devised in H, without an explicit defisnisiti (oKnP of t)h)e c feature map, only by using athne xkperlniceitl (dtehfeso-called kernel trick). Recently, due to the development of very efficient solutions for large scale linear SVMs and for incremental linear component analysis, the research to- wards finding feature map approximations for classes of kernels has attracted significant interest. In this paper, we attempt the derivation of explicit feature maps of a recently proposed class of kernels, the so-called one-shot similarity kernels. We show that for this class of kernels either there exists an explicit representation in feature space or the kernel can be expressed in such a form that allows for exact incremental learning. We theoretically explore the properties of these kernels and show how these kernels can be used for the development of robust visual tracking, recognition and deformable fitting algorithms. 1Robustness may refer to either the presence of outliers and noise the robustness to a class of transformations (e.g., translation). or to ∗ Irene Kotsia ,†,? ∗Electronics Laboratory, Department of Physics, University of Patras, Greece ?School of Science and Technology, Middlesex University, London i .kot s i @mdx . ac .uk a</p><p>6 0.77440178 <a title="212-lsi-6" href="./iccv-2013-A_Novel_Earth_Mover%27s_Distance_Methodology_for_Image_Matching_with_Gaussian_Mixture_Models.html">25 iccv-2013-A Novel Earth Mover's Distance Methodology for Image Matching with Gaussian Mixture Models</a></p>
<p>7 0.77287591 <a title="212-lsi-7" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>8 0.69056863 <a title="212-lsi-8" href="./iccv-2013-Quadruplet-Wise_Image_Similarity_Learning.html">332 iccv-2013-Quadruplet-Wise Image Similarity Learning</a></p>
<p>9 0.68942964 <a title="212-lsi-9" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>10 0.6883176 <a title="212-lsi-10" href="./iccv-2013-Fast_High_Dimensional_Vector_Multiplication_Face_Recognition.html">158 iccv-2013-Fast High Dimensional Vector Multiplication Face Recognition</a></p>
<p>11 0.66826928 <a title="212-lsi-11" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>12 0.6594457 <a title="212-lsi-12" href="./iccv-2013-Unbiased_Metric_Learning%3A_On_the_Utilization_of_Multiple_Datasets_and_Web_Images_for_Softening_Bias.html">431 iccv-2013-Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias</a></p>
<p>13 0.6541701 <a title="212-lsi-13" href="./iccv-2013-Log-Euclidean_Kernels_for_Sparse_Representation_and_Dictionary_Learning.html">257 iccv-2013-Log-Euclidean Kernels for Sparse Representation and Dictionary Learning</a></p>
<p>14 0.65216881 <a title="212-lsi-14" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>15 0.65012586 <a title="212-lsi-15" href="./iccv-2013-Ensemble_Projection_for_Semi-supervised_Image_Classification.html">142 iccv-2013-Ensemble Projection for Semi-supervised Image Classification</a></p>
<p>16 0.64424974 <a title="212-lsi-16" href="./iccv-2013-An_Adaptive_Descriptor_Design_for_Object_Recognition_in_the_Wild.html">48 iccv-2013-An Adaptive Descriptor Design for Object Recognition in the Wild</a></p>
<p>17 0.63598138 <a title="212-lsi-17" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>18 0.60524195 <a title="212-lsi-18" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>19 0.58162856 <a title="212-lsi-19" href="./iccv-2013-Discriminant_Tracking_Using_Tensor_Representation_with_Semi-supervised_Improvement.html">119 iccv-2013-Discriminant Tracking Using Tensor Representation with Semi-supervised Improvement</a></p>
<p>20 0.56970161 <a title="212-lsi-20" href="./iccv-2013-Codemaps_-_Segment%2C_Classify_and_Search_Objects_Locally.html">77 iccv-2013-Codemaps - Segment, Classify and Search Objects Locally</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.061), (7, 0.441), (12, 0.011), (26, 0.061), (31, 0.035), (42, 0.094), (64, 0.037), (73, 0.016), (89, 0.167)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78198606 <a title="212-lda-1" href="./iccv-2013-From_Semi-supervised_to_Transfer_Counting_of_Crowds.html">178 iccv-2013-From Semi-supervised to Transfer Counting of Crowds</a></p>
<p>Author: Chen Change Loy, Shaogang Gong, Tao Xiang</p><p>Abstract: Regression-based techniques have shown promising results for people counting in crowded scenes. However, most existing techniques require expensive and laborious data annotation for model training. In this study, we propose to address this problem from three perspectives: (1) Instead of exhaustively annotating every single frame, the most informative frames are selected for annotation automatically and actively. (2) Rather than learning from only labelled data, the abundant unlabelled data are exploited. (3) Labelled data from other scenes are employed to further alleviate the burden for data annotation. All three ideas are implemented in a unified active and semi-supervised regression framework with ability to perform transfer learning, by exploiting the underlying geometric structure of crowd patterns via manifold analysis. Extensive experiments validate the effectiveness of our approach.</p><p>2 0.74578154 <a title="212-lda-2" href="./iccv-2013-Pose_Estimation_with_Unknown_Focal_Length_Using_Points%2C_Directions_and_Lines.html">323 iccv-2013-Pose Estimation with Unknown Focal Length Using Points, Directions and Lines</a></p>
<p>Author: Yubin Kuang, Kalle Åström</p><p>Abstract: In this paper, we study the geometry problems of estimating camera pose with unknown focal length using combination of geometric primitives. We consider points, lines and also rich features such as quivers, i.e. points with one or more directions. We formulate the problems as polynomial systems where the constraints for different primitives are handled in a unified way. We develop efficient polynomial solvers for each of the derived cases with different combinations of primitives. The availability of these solvers enables robust pose estimation with unknown focal length for wider classes of features. Such rich features allow for fewer feature correspondences and generate larger inlier sets with higher probability. We demonstrate in synthetic experiments that our solvers are fast and numerically stable. For real images, we show that our solvers can be used in RANSAC loops to provide good initial solutions.</p><p>same-paper 3 0.74247563 <a title="212-lda-3" href="./iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning.html">212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</a></p>
<p>Author: Jiwen Lu, Gang Wang, Pierre Moulin</p><p>Abstract: This paper presents a new approach for image set classification, where each training and testing example contains a set of image instances of an object captured from varying viewpoints or under varying illuminations. While a number of image set classification methods have been proposed in recent years, most of them model each image set as a single linear subspace or mixture of linear subspaces, which may lose some discriminative information for classification. To address this, we propose exploring multiple order statistics as features of image sets, and develop a localized multikernel metric learning (LMKML) algorithm to effectively combine different order statistics information for classification. Our method achieves the state-of-the-art performance on four widely used databases including the Honda/UCSD, CMU Mobo, and Youtube face datasets, and the ETH-80 object dataset.</p><p>4 0.73535711 <a title="212-lda-4" href="./iccv-2013-Text_Localization_in_Natural_Images_Using_Stroke_Feature_Transform_and_Text_Covariance_Descriptors.html">415 iccv-2013-Text Localization in Natural Images Using Stroke Feature Transform and Text Covariance Descriptors</a></p>
<p>Author: Weilin Huang, Zhe Lin, Jianchao Yang, Jue Wang</p><p>Abstract: In this paper, we present a new approach for text localization in natural images, by discriminating text and non-text regions at three levels: pixel, component and textline levels. Firstly, a powerful low-level filter called the Stroke Feature Transform (SFT) is proposed, which extends the widely-used Stroke Width Transform (SWT) by incorporating color cues of text pixels, leading to significantly enhanced performance on inter-component separation and intra-component connection. Secondly, based on the output of SFT, we apply two classifiers, a text component classifier and a text-line classifier, sequentially to extract text regions, eliminating the heuristic procedures that are commonly used in previous approaches. The two classifiers are built upon two novel Text Covariance Descriptors (TCDs) that encode both the heuristic properties and the statistical characteristics of text stokes. Finally, text regions are located by simply thresholding the text-line confident map. Our method was evaluated on two benchmark datasets: ICDAR 2005 and ICDAR 2011, and the corresponding F- , measure values are 0. 72 and 0. 73, respectively, surpassing previous methods in accuracy by a large margin.</p><p>5 0.67793936 <a title="212-lda-5" href="./iccv-2013-No_Matter_Where_You_Are%3A_Flexible_Graph-Guided_Multi-task_Learning_for_Multi-view_Head_Pose_Classification_under_Target_Motion.html">291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</a></p>
<p>Author: Yan Yan, Elisa Ricci, Ramanathan Subramanian, Oswald Lanz, Nicu Sebe</p><p>Abstract: We propose a novel Multi-Task Learning framework (FEGA-MTL) for classifying the head pose of a person who moves freely in an environment monitored by multiple, large field-of-view surveillance cameras. As the target (person) moves, distortions in facial appearance owing to camera perspective and scale severely impede performance of traditional head pose classification methods. FEGA-MTL operates on a dense uniform spatial grid and learns appearance relationships across partitions as well as partition-specific appearance variations for a given head pose to build region-specific classifiers. Guided by two graphs which a-priori model appearance similarity among (i) grid partitions based on camera geometry and (ii) head pose classes, the learner efficiently clusters appearancewise related grid partitions to derive the optimal partitioning. For pose classification, upon determining the target’s position using a person tracker, the appropriate regionspecific classifier is invoked. Experiments confirm that FEGA-MTL achieves state-of-the-art classification with few training data.</p><p>6 0.65981269 <a title="212-lda-6" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>7 0.58838344 <a title="212-lda-7" href="./iccv-2013-A_Robust_Analytical_Solution_to_Isometric_Shape-from-Template_with_Focal_Length_Calibration.html">27 iccv-2013-A Robust Analytical Solution to Isometric Shape-from-Template with Focal Length Calibration</a></p>
<p>8 0.57459527 <a title="212-lda-8" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>9 0.56364244 <a title="212-lda-9" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>10 0.55452049 <a title="212-lda-10" href="./iccv-2013-Lifting_3D_Manhattan_Lines_from_a_Single_Image.html">250 iccv-2013-Lifting 3D Manhattan Lines from a Single Image</a></p>
<p>11 0.55160832 <a title="212-lda-11" href="./iccv-2013-Complex_3D_General_Object_Reconstruction_from_Line_Drawings.html">84 iccv-2013-Complex 3D General Object Reconstruction from Line Drawings</a></p>
<p>12 0.55055004 <a title="212-lda-12" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>13 0.54139501 <a title="212-lda-13" href="./iccv-2013-Unsupervised_Intrinsic_Calibration_from_a_Single_Frame_Using_a_%22Plumb-Line%22_Approach.html">436 iccv-2013-Unsupervised Intrinsic Calibration from a Single Frame Using a "Plumb-Line" Approach</a></p>
<p>14 0.53935468 <a title="212-lda-14" href="./iccv-2013-Revisiting_the_PnP_Problem%3A_A_Fast%2C_General_and_Optimal_Solution.html">353 iccv-2013-Revisiting the PnP Problem: A Fast, General and Optimal Solution</a></p>
<p>15 0.53249013 <a title="212-lda-15" href="./iccv-2013-Linear_Sequence_Discriminant_Analysis%3A_A_Model-Based_Dimensionality_Reduction_Method_for_Vector_Sequences.html">253 iccv-2013-Linear Sequence Discriminant Analysis: A Model-Based Dimensionality Reduction Method for Vector Sequences</a></p>
<p>16 0.52857715 <a title="212-lda-16" href="./iccv-2013-Real-Time_Solution_to_the_Absolute_Pose_Problem_with_Unknown_Radial_Distortion_and_Focal_Length.html">342 iccv-2013-Real-Time Solution to the Absolute Pose Problem with Unknown Radial Distortion and Focal Length</a></p>
<p>17 0.52481484 <a title="212-lda-17" href="./iccv-2013-On_the_Mean_Curvature_Flow_on_Graphs_with_Applications_in_Image_and_Manifold_Processing.html">296 iccv-2013-On the Mean Curvature Flow on Graphs with Applications in Image and Manifold Processing</a></p>
<p>18 0.52239847 <a title="212-lda-18" href="./iccv-2013-Shortest_Paths_with_Curvature_and_Torsion.html">389 iccv-2013-Shortest Paths with Curvature and Torsion</a></p>
<p>19 0.52014208 <a title="212-lda-19" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>20 0.51637459 <a title="212-lda-20" href="./iccv-2013-Rectangling_Stereographic_Projection_for_Wide-Angle_Image_Visualization.html">346 iccv-2013-Rectangling Stereographic Projection for Wide-Angle Image Visualization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
