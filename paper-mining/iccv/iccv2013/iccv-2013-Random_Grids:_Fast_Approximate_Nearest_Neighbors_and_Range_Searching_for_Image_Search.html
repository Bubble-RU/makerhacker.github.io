<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-337" href="#">iccv2013-337</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</h1>
<br/><p>Source: <a title="iccv-2013-337-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Aiger_Random_Grids_Fast_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Dror Aiger, Efi Kokiopoulou, Ehud Rivlin</p><p>Abstract: We propose two solutions for both nearest neighbors and range search problems. For the nearest neighbors problem, we propose a c-approximate solutionfor the restricted version ofthe decisionproblem with bounded radius which is then reduced to the nearest neighbors by a known reduction. For range searching we propose a scheme that learns the parameters in a learning stage adopting them to the case of a set of points with low intrinsic dimension that are embedded in high dimensional space (common scenario for image point descriptors). We compare our algorithms to the best known methods for these problems, i.e. LSH, ANN and FLANN. We show analytically and experimentally that we can do better for moderate approximation factor. Our algorithms are trivial to parallelize. In the experiments conducted, running on couple of million im- ages, our algorithms show meaningful speed-ups when compared with the above mentioned methods.</p><p>Reference: <a title="iccv-2013-337-reference" href="../iccv2013_reference/iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We propose two solutions for both nearest neighbors and range search problems. [sent-5, score-0.439]
</p><p>2 For the nearest neighbors problem, we propose a c-approximate solutionfor the restricted version ofthe decisionproblem with bounded radius which is then reduced to the nearest neighbors by a known reduction. [sent-6, score-0.792]
</p><p>3 For range searching we propose a scheme that learns the parameters in a learning stage adopting them to the case of a set of points with low intrinsic dimension that are embedded in high dimensional space (common scenario for image point descriptors). [sent-7, score-0.502]
</p><p>4 Image structures are commonly described by points in space and one is searching for similar structures by applying proximity searching in this space. [sent-16, score-0.543]
</p><p>5 A considerably larger effort has been invested in preprocessing-and-query problems, where the goal is to construct some data structure on the input point set which supports proximity queries of a certain kind. [sent-17, score-0.22]
</p><p>6 The most common of this kind of problems is nearest neighbor searching, where we are given a set P of n points in a high-dimensional space Rd, and wish to construct a data structure that, given a query point q, finds the point(s) in P closest to q. [sent-18, score-0.983]
</p><p>7 Many of the known exact and approximate near-  est neighbor searching data structures can be modified to report all (or most) points ofP that are within a certain given distance r from a query point q. [sent-21, score-1.171]
</p><p>8 We give a brief review of the state-of-the-art approximate nearest neighbor data structures. [sent-22, score-0.295]
</p><p>9 These approximate nearest neighbor data structures return, for any query point q, a point p whose distance from q is at most (1 + ε) times the distance between q and its nearest neighbor. [sent-24, score-1.122]
</p><p>10 This structure takes O(n) space, can be constructed in O(n log n) time, and answers a query in O(ε1d log n) time. [sent-27, score-0.602]
</p><p>11 To date, many trade-offs between query time and space have been achieved, and in all of the more efficient ones the product of the term depending on ε in the storage and the square of the term depending on ε in the query time is roughly ε1d [5]. [sent-28, score-0.956]
</p><p>12 The first component in this method is a reduction from  the problem of approximate nearest neighbor search to the problem of finding a neighbor at distance ≤ (1 + ε)r if there exists a neighbor at distance r, f≤or ( some pre-specified r >s a 0 n. [sent-30, score-0.648]
</p><p>13 iTghhebno trh aet l adittsetra problem is solved using a family of hash functions that tend to map close points to the same bin (an LSH family in short). [sent-31, score-0.307]
</p><p>14 The solution of Indyk and Motwani answers a query in ) time and takes ) preprocessing time and storage. [sent-32, score-0.561]
</p><p>15 This was later improved, using more complex hash func-  O(n1+1ε  O(n1+11+ε  33446714  O(n(1+1ε)2)  O(n1+(1+1ε)2)  tions, to query time and preprocessing and storage [3, 6]. [sent-33, score-0.621]
</p><p>16 In this paper we consider the problems of nearest neighbors and range searching for the application of image matching. [sent-42, score-0.642]
</p><p>17 We focus on the decision problem called the (c, r)-NN where we only interested in the neighbors if they are close enough to the query. [sent-43, score-0.26]
</p><p>18 While there is a known reduction (increasing the query time by a factor log n) from the general nearest neihgbors to this problem, it seems that for practical applications, one can usually use this when r is specified in the input. [sent-44, score-0.697]
</p><p>19 We also present an algorithm for the range searching problem where we are interested in reporting all points at distance at most r from the query. [sent-45, score-0.559]
</p><p>20 Here we allow randomization and we obtain a randomized algorithm that is guaranteed to report points having pre-specified high probability. [sent-46, score-0.296]
</p><p>21 In many applications the input set P has a restricted structure, in the sense that its points have much fewer “degrees of freedom” than the ambient  dimension d. [sent-47, score-0.214]
</p><p>22 Low doubling dimension was exploited before for fast algorithms for approximate nearest neighbor searching [8, 11]. [sent-49, score-0.588]
</p><p>23 Theory An important result from [1] states that if p and q are two points at distance at most 1 in ddimensional Euclidean space and we impose a randomly rotated and shifted grid of cell size w on this space, then the probability of capturing both p and for sufficiently large w. [sent-67, score-0.687]
</p><p>24 q in the same cell is at least e−√d/w  We impose a grid of cell size w = √cd on P. [sent-69, score-0.387]
</p><p>25 Based on the result above, a random unit vector in Rd will be captured in one cell with probability at least = create copies of P, randomly rotated and shifted, and store them in the grid cells where each non empty cell contains a list of the points contained in it for every rotated/shifted copy. [sent-70, score-0.776]
</p><p>26 We  33446725  ecd  Figure 1: Randomized Grids indexing process: Feature descriptors are extracted from the key points of the dataset of images and represented as a set of points in Rd, where a series of random grids is applied. [sent-72, score-0.577]
</p><p>27 For each random grid, the points falling in the same bin hash to the same value. [sent-73, score-0.378]
</p><p>28 a hash table for indexing, where points falling in the same bin hash to the same value (see Fig. [sent-74, score-0.449]
</p><p>29 For a given query point q, we rotate and shift q, ecd times by the corresponding transformations and report the first point found in the corresponding grid cells. [sent-76, score-0.847]
</p><p>30 With positive constant probability, if there is a point at distance at most 1from q, it will be found in one of the grids cells. [sent-77, score-0.244]
</p><p>31 The space is O(decd n) and the preprocessing time is n) (in computational model with floor function) and the query time is ). [sent-79, score-0.515]
</p><p>32 The  O(d2ecd  O(d2ecd  probability of success can be amplified to 1 δ by repeating tyh oef process (increasing pthliefi eddat tao structure, space and query time) ln(1/δ) times. [sent-80, score-0.561]
</p><p>33 rT ghere probability oεf high adilsletor trthioann 1is− upper bounded by f = , using standard bound on distortion under random projections [10, 3, 9]. [sent-91, score-0.228]
</p><p>34 We have to tune the grid size accordingly to still have c-approximation with high probability when we first project P to dimension t. [sent-93, score-0.315]
</p><p>35 Then the query time becomes The projection time can be improved to O(d log d) instead of O(td) by the FJLT  O(tdet(1+t−1/4)/c). [sent-95, score-0.572]
</p><p>36 This is a plot of the query time bound we obtained and the p-stable LSH that has query time O(n1/c). [sent-97, score-0.997]
</p><p>37 Even better, one can set t to  the value that minimizes the query expression. [sent-100, score-0.478]
</p><p>38 The goal of LSH is to remove the exponential dependency in the dimension, while still allowing sub-linear query time and good dependency on the approximation factor, c. [sent-103, score-0.649]
</p><p>39 The multiple random projection to a line is conceptually similar to the regular rotated and shifted grids and like [3] we use a grid in some smaller dimension t to which we project the dataset. [sent-108, score-0.524]
</p><p>40 In practice, we learn the best t for random projection  from a sample of the data as we do for the other two parameters, the cell size and the number of random grids. [sent-111, score-0.239]
</p><p>41 For example, for 1For c = 2 the near optimal LSH is far slower and was omitted for better visualization 33446736  log2/3n and c  t= = 2 below is numerical comparison of the number of hash access for our proposal and the p-stable LSH where n = |P| goes up to 1an06d. [sent-114, score-0.216]
</p><p>42 hTehe p r-setaalb query Hrun wtihmeere e( fnor = bot |hP range ssue aprc toh and 1-NN) of course depends also on the number of points in the dataset near the query at distance at most 1. [sent-115, score-1.277]
</p><p>43 For this reason, the query time cannot be bounded theoretically. [sent-116, score-0.514]
</p><p>44 Randomized-NN The goal is to build a data structure that for a given query point q, if there is a point in P at distance at most 1from q, returns one such point with pre specified probability ρ. [sent-131, score-0.793]
</p><p>45 Note that the data structure does not necessarily return the nearest neighbour, it rather returns some point in the given range. [sent-132, score-0.19]
</p><p>46 We impose a grid of cell size  w and use m randomly shifted and rotated grids to store the points in P. [sent-133, score-0.689]
</p><p>47 For a query point q, we compute the m grid cells it falls into (for each of these m grids). [sent-135, score-0.711]
</p><p>48 Then we pick from the set of points in every such cell, a random set of k points where k is a parameter. [sent-136, score-0.311]
</p><p>49 We report the first point in this set that has distance at most 1from q and stop when such point has been found. [sent-137, score-0.22]
</p><p>50 For a given pre-specified probability ρ we optimize the runtime, provided that for overall sampled query points we report at least ρN near neighbors where N is the true exact near neighbors (computed using a standard kd-tree). [sent-139, score-1.389]
</p><p>51 For given query point q, we rotate and shift q, m times by the corresponding transformations and report all points found in the corresponding grid cells that have distance at most 1to q . [sent-143, score-0.979]
</p><p>52 We have to explicitly compute the distance from q for all points found in the cells and filter only the ones we need. [sent-144, score-0.246]
</p><p>53 If w, m are set correctly, with sufficiently large probability, every point at distance at most 1from q will be found in one of the grid cells. [sent-145, score-0.223]
</p><p>54 Here, the runtime is determined by the number of ”redundant” points we have to examine in all cells. [sent-146, score-0.278]
</p><p>55 For learning, we have two sets of points P and Q where P is considered to be the index and Q is a set of queries (we can simply set Q = P). [sent-150, score-0.307]
</p><p>56 For a given probability ρ we seek w, m such that the overall number of reported points (when we query all points in Q? [sent-154, score-0.906]
</p><p>57 For a query image, we compute the same features set using the same method we used for the index. [sent-167, score-0.478]
</p><p>58 Let Q be the set of features that we extract from the query image. [sent-168, score-0.478]
</p><p>59 Then we query for each point in Q, all points in P that are closer than r to it. [sent-169, score-0.677]
</p><p>60 For these images that have sufficient number of reporting points, we apply geometric verification by a robust (RANSAC) feature matching under perspective transformation or by finding all points consistent with the same fundamental matrix. [sent-171, score-0.236]
</p><p>61 We assume that each map worker has access to the whole query set, which is typically much smaller than the training set. [sent-180, score-0.518]
</p><p>62 e E partition po wf tohrek edra tdao assigned dteox xitand then queries each point from the query set using its (local) index set. [sent-185, score-0.708]
</p><p>63 The matched pairs are output to the reducer using the query point id as the key. [sent-186, score-0.649]
</p><p>64 In the reduce phase all matches corresponding Iton tthhee same query point waitcllh bees  collected in the same reduce worker. [sent-188, score-0.581]
</p><p>65 The job of the reducer is to collect together all matches of the query coming from the whole training set. [sent-189, score-0.585]
</p><p>66 Range Searching  The first experiment is for the randomized range searching problem. [sent-199, score-0.336]
</p><p>67 We measure the accuracy of the methods by comparing the total number of reported neighbors (across all query points) for each method with the exact number of neighbors (obtained using an exact method). [sent-203, score-1.091]
</p><p>68 Imposing that index set and query set are of the same size, we randomly sample from the original data set and perform several measurements with different data sizes. [sent-209, score-0.616]
</p><p>69 The index data set consists of 100K points and the query set contains 1K points. [sent-214, score-0.721]
</p><p>70 2For small size, the need to project any query vector from dimension d to dimension t requires O(dt) time with naive implementation. [sent-217, score-0.63]
</p><p>71 33446758  Runtime Comparison  sio(temrnu)cd21350 FLAN40(aRurtynod,M6m0eiduzn)t(,MGNure8ijad0s,nL(yoauwhrse1,02 9 380)12 #points  (a)  cursot(ind)mes 13274650 LS5H0(auRtondem,i1zA0dnGor0id,sIn(ouyrk,1520 36)20  E2LSH runtime comparison - increasing index and query  #index points  (c)  FLANN took 0. [sent-220, score-0.918]
</p><p>72 7 precision which resulted in 1102 reported neighbors with 0. [sent-224, score-0.282]
</p><p>73 The exact number of points in the neighbourhood (for all queries) in this case was 1113. [sent-227, score-0.197]
</p><p>74 The parameters for E2LSH were learned automatically from a sample of 4000 index points and 4000 for query points. [sent-231, score-0.721]
</p><p>75 Taking the total number of returned points from all queries, we learned the best parameters for our algorithm, requesting the same number of returned points. [sent-234, score-0.216]
</p><p>76 5 201LSH(auRtonud mei,z A5dn0 Gor0nid,sIn(oduyrks,20 163) e+06 #index points  (d)  set of query points of size 1000. [sent-238, score-0.754]
</p><p>77 The purpose of the second experiment is to evaluate the rate of increasing running time as a function of the index in this application. [sent-239, score-0.196]
</p><p>78 We looked for a practical value of r such that each query gets approximately one reported neighbor. [sent-245, score-0.598]
</p><p>79 Once r is fixed, we need to tune the parameters of the methods (for a fair comparison) such that the average total number of neighbors reported by each method is approximately ρN, where N is the exact number of neighbors (in the range r). [sent-248, score-0.653]
</p><p>80 In order to achieve that, we optimized our algorithm by selecting the parameters w (bin size), m (number of random translations) and k (number of randomly selected points from each bin) such that in the learning stage the probability of reporting a near neigh-  bor is at least 0. [sent-251, score-0.465]
</p><p>81 The number of returned neighbors in both algorithms in comparison to the exact number is shown in Figure (f). [sent-258, score-0.357]
</p><p>82 More specifically, for a given maximum number of images we randomly split half of the images in the training set and half of them in the query set. [sent-263, score-0.511]
</p><p>83 We index all the descriptors of the training images and then we query each descriptor of the query images using range searching. [sent-264, score-1.159]
</p><p>84 A query image is said to match a training image if the number of descriptor matches (filtered after geometric verification) is higher than 20. [sent-265, score-0.52]
</p><p>85 , using the rule that comparable number of neighbors are reported by all methods). [sent-271, score-0.282]
</p><p>86 The query set consists of 3357 images (randomly) sub-sampled from the original data set. [sent-274, score-0.478]
</p><p>87 The algorithm found 2056 matches of the 3357 query images across 14,128,635 training im-  searpod#ti1058493762 0 0 ExAa2cN0tnu(mRbryead4,o0Mfizupendot,rGN6i0aesny(rouhig,128b90or3s) Number of reported near neighbors  #points  (f)  ages. [sent-275, score-0.88]
</p><p>88 Querying all descriptors from the query set takes about 100 sec for a single partition (or 0. [sent-276, score-0.52]
</p><p>89 Discussion and Conclusion We considered the problems ofnearest neighbors and range searching for the application of image matching. [sent-281, score-0.513]
</p><p>90 We focused on the (c, r)-NN problem where we were only interested in neighbors that are close enough to the query. [sent-282, score-0.26]
</p><p>91 For the range searching problem we were interested in reporting all points of distance at most r from the query. [sent-283, score-0.559]
</p><p>92 We proposed a randomized algorithm that is guaranteed to report points having pre-specified high probability. [sent-284, score-0.296]
</p><p>93 These two solutions for the nearest neighbors and range search problems belong to the LSH family. [sent-285, score-0.477]
</p><p>94 We compared our algorithms to LSH, ANN and FLANN and showed analytically and experimentally that we can do better for moderate approximation factor. [sent-286, score-0.224]
</p><p>95 For range searching we proposed a  scheme that learns the parameters adopting them to the case of a set of points with low intrinsic dimension that are embedded in high dimensional space. [sent-288, score-0.441]
</p><p>96 We learned the best t for random projection from a sample of the data as we do for the other two parameters, the cell size and the number of random grids. [sent-289, score-0.239]
</p><p>97 Chazelle, Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform, STOC 06: Proceedings of the thirtyeighth annual ACM symposium on Theory of computing, pages 557-563, New York, NY, USA, 2006. [sent-310, score-0.342]
</p><p>98 Indyk, Near-optimal hashing algorithms for approximate nearest neighbor in  high dimensions, in Proc. [sent-314, score-0.417]
</p><p>99 Wu, An optimal algorithm for approximate nearest neighbor searching in fixed dimensions, J. [sent-329, score-0.466]
</p><p>100 Mount, Space-time tradeoffs for approximate nearest neighbor searching, J. [sent-335, score-0.295]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('query', 0.478), ('lsh', 0.294), ('flann', 0.252), ('neighbors', 0.213), ('searching', 0.171), ('runtime', 0.14), ('points', 0.138), ('grids', 0.134), ('nearest', 0.129), ('arya', 0.125), ('indyk', 0.124), ('cell', 0.114), ('grid', 0.113), ('randomized', 0.109), ('neighbor', 0.107), ('hash', 0.106), ('index', 0.105), ('ann', 0.099), ('reporting', 0.098), ('worst', 0.085), ('probability', 0.083), ('mount', 0.079), ('near', 0.078), ('dimension', 0.076), ('hashing', 0.076), ('motwani', 0.075), ('radius', 0.072), ('reported', 0.069), ('dependency', 0.067), ('ehud', 0.065), ('figs', 0.065), ('fjlt', 0.065), ('lon', 0.065), ('reducer', 0.065), ('stewenius', 0.065), ('queries', 0.064), ('proximity', 0.063), ('bin', 0.063), ('shifted', 0.062), ('point', 0.061), ('exact', 0.059), ('approximate', 0.059), ('cells', 0.059), ('loga', 0.057), ('aiger', 0.057), ('increasing', 0.057), ('range', 0.056), ('projection', 0.055), ('ecd', 0.053), ('eps', 0.053), ('analytically', 0.053), ('practical', 0.051), ('rotated', 0.049), ('distance', 0.049), ('report', 0.049), ('trip', 0.048), ('tourist', 0.048), ('secs', 0.048), ('europe', 0.048), ('surf', 0.047), ('interested', 0.047), ('experimentally', 0.046), ('impose', 0.046), ('algorithms', 0.046), ('parallelize', 0.046), ('answers', 0.046), ('andoni', 0.046), ('nets', 0.046), ('matched', 0.045), ('nister', 0.043), ('timings', 0.043), ('tune', 0.043), ('matches', 0.042), ('descriptors', 0.042), ('moderate', 0.042), ('parallelization', 0.041), ('search', 0.041), ('bound', 0.041), ('worker', 0.04), ('google', 0.04), ('log', 0.039), ('returned', 0.039), ('problems', 0.038), ('copies', 0.038), ('approximation', 0.037), ('indexing', 0.037), ('supporting', 0.037), ('preprocessing', 0.037), ('falling', 0.036), ('bounded', 0.036), ('random', 0.035), ('application', 0.035), ('theory', 0.034), ('running', 0.034), ('randomly', 0.033), ('distortion', 0.033), ('rotate', 0.032), ('library', 0.032), ('construct', 0.032), ('numerical', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999917 <a title="337-tfidf-1" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>Author: Dror Aiger, Efi Kokiopoulou, Ehud Rivlin</p><p>Abstract: We propose two solutions for both nearest neighbors and range search problems. For the nearest neighbors problem, we propose a c-approximate solutionfor the restricted version ofthe decisionproblem with bounded radius which is then reduced to the nearest neighbors by a known reduction. For range searching we propose a scheme that learns the parameters in a learning stage adopting them to the case of a set of points with low intrinsic dimension that are embedded in high dimensional space (common scenario for image point descriptors). We compare our algorithms to the best known methods for these problems, i.e. LSH, ANN and FLANN. We show analytically and experimentally that we can do better for moderate approximation factor. Our algorithms are trivial to parallelize. In the experiments conducted, running on couple of million im- ages, our algorithms show meaningful speed-ups when compared with the above mentioned methods.</p><p>2 0.35702196 <a title="337-tfidf-2" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>Author: Xu Wang, Stefan Atev, John Wright, Gilad Lerman</p><p>Abstract: The problem of efficiently deciding which of a database of models is most similar to a given input query arises throughout modern computer vision. Motivated by applications in recognition, image retrieval and optimization, there has been significant recent interest in the variant of this problem in which the database models are linear subspaces and the input is either a point or a subspace. Current approaches to this problem have poor scaling in high dimensions, and may not guarantee sublinear query complexity. We present a new approach to approximate nearest subspace search, based on a simple, new locality sensitive hash for subspaces. Our approach allows point-tosubspace query for a database of subspaces of arbitrary dimension d, in a time that depends sublinearly on the number of subspaces in the database. The query complexity of our algorithm is linear in the ambient dimension D, allow- ing it to be directly applied to high-dimensional imagery data. Numerical experiments on model problems in image repatching and automatic face recognition confirm the advantages of our algorithm in terms of both speed and accuracy.</p><p>3 0.35145667 <a title="337-tfidf-3" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>Author: Basura Fernando, Tinne Tuytelaars</p><p>Abstract: In this paper we present a new method for object retrieval starting from multiple query images. The use of multiple queries allows for a more expressive formulation of the query object including, e.g., different viewpoints and/or viewing conditions. This, in turn, leads to more diverse and more accurate retrieval results. When no query images are available to the user, they can easily be retrieved from the internet using a standard image search engine. In particular, we propose a new method based on pattern mining. Using the minimal description length principle, we derive the most suitable set of patterns to describe the query object, with patterns corresponding to local feature configurations. This results in apowerful object-specific mid-level image representation. The archive can then be searched efficiently for similar images based on this representation, using a combination of two inverted file systems. Since the patterns already encode local spatial information, good results on several standard image retrieval datasets are obtained even without costly re-ranking based on geometric verification.</p><p>4 0.20820954 <a title="337-tfidf-4" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>Author: Masakazu Iwamura, Tomokazu Sato, Koichi Kise</p><p>Abstract: Approximate nearest neighbor search (ANNS) is a basic and important technique used in many tasks such as object recognition. It involves two processes: selecting nearest neighbor candidates and performing a brute-force search of these candidates. Only the former though has scope for improvement. In most existing methods, it approximates the space by quantization. It then calculates all the distances between the query and all the quantized values (e.g., clusters or bit sequences), and selects a fixed number of candidates close to the query. The performance of the method is evaluated based on accuracy as a function of the number of candidates. This evaluation seems rational but poses a serious problem; it ignores the computational cost of the process of selection. In this paper, we propose a new ANNS method that takes into account costs in the selection process. Whereas existing methods employ computationally expensive techniques such as comparative sort and heap, the proposed method does not. This realizes a significantly more efficient search. We have succeeded in reducing computation times by one-third compared with the state-of-the- art on an experiment using 100 million SIFT features.</p><p>5 0.19800323 <a title="337-tfidf-5" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>Author: Matthijs Douze, Jérôme Revaud, Cordelia Schmid, Hervé Jégou</p><p>Abstract: This paper makes two complementary contributions to event retrieval in large collections of videos. First, we propose hyper-pooling strategies that encode the frame descriptors into a representation of the video sequence in a stable manner. Our best choices compare favorably with regular pooling techniques based on k-means quantization. Second, we introduce a technique to improve the ranking. It can be interpreted either as a query expansion method or as a similarity adaptation based on the local context of the query video descriptor. Experiments on public benchmarks show that our methods are complementary and improve event retrieval results, without sacrificing efficiency.</p><p>6 0.19775257 <a title="337-tfidf-6" href="./iccv-2013-Quantize_and_Conquer%3A_A_Dimensionality-Recursive_Solution_to_Clustering%2C_Vector_Quantization%2C_and_Image_Retrieval.html">333 iccv-2013-Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval</a></p>
<p>7 0.19508418 <a title="337-tfidf-7" href="./iccv-2013-Query-Adaptive_Asymmetrical_Dissimilarities_for_Visual_Object_Retrieval.html">334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</a></p>
<p>8 0.19082141 <a title="337-tfidf-8" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>9 0.18058835 <a title="337-tfidf-9" href="./iccv-2013-Fast_Neighborhood_Graph_Search_Using_Cartesian_Concatenation.html">159 iccv-2013-Fast Neighborhood Graph Search Using Cartesian Concatenation</a></p>
<p>10 0.17858618 <a title="337-tfidf-10" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>11 0.1781207 <a title="337-tfidf-11" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>12 0.17281197 <a title="337-tfidf-12" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>13 0.16003498 <a title="337-tfidf-13" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>14 0.1589454 <a title="337-tfidf-14" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<p>15 0.15670878 <a title="337-tfidf-15" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>16 0.14259945 <a title="337-tfidf-16" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>17 0.14123505 <a title="337-tfidf-17" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>18 0.13155113 <a title="337-tfidf-18" href="./iccv-2013-A_General_Two-Step_Approach_to_Learning-Based_Hashing.html">13 iccv-2013-A General Two-Step Approach to Learning-Based Hashing</a></p>
<p>19 0.11794998 <a title="337-tfidf-19" href="./iccv-2013-EVSAC%3A_Accelerating_Hypotheses_Generation_by_Modeling_Matching_Scores_with_Extreme_Value_Theory.html">131 iccv-2013-EVSAC: Accelerating Hypotheses Generation by Modeling Matching Scores with Extreme Value Theory</a></p>
<p>20 0.11753099 <a title="337-tfidf-20" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.196), (1, 0.032), (2, -0.1), (3, -0.144), (4, -0.02), (5, 0.35), (6, 0.042), (7, -0.049), (8, -0.223), (9, 0.13), (10, 0.072), (11, 0.042), (12, 0.068), (13, 0.067), (14, 0.045), (15, 0.019), (16, 0.129), (17, -0.141), (18, 0.156), (19, -0.09), (20, -0.026), (21, -0.082), (22, -0.009), (23, -0.048), (24, -0.053), (25, -0.002), (26, 0.023), (27, -0.062), (28, 0.018), (29, -0.038), (30, -0.047), (31, 0.035), (32, 0.021), (33, -0.01), (34, 0.032), (35, -0.032), (36, -0.02), (37, -0.004), (38, 0.011), (39, -0.006), (40, 0.059), (41, 0.062), (42, 0.025), (43, 0.014), (44, -0.014), (45, 0.041), (46, -0.021), (47, 0.014), (48, -0.01), (49, -0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96888757 <a title="337-lsi-1" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>Author: Dror Aiger, Efi Kokiopoulou, Ehud Rivlin</p><p>Abstract: We propose two solutions for both nearest neighbors and range search problems. For the nearest neighbors problem, we propose a c-approximate solutionfor the restricted version ofthe decisionproblem with bounded radius which is then reduced to the nearest neighbors by a known reduction. For range searching we propose a scheme that learns the parameters in a learning stage adopting them to the case of a set of points with low intrinsic dimension that are embedded in high dimensional space (common scenario for image point descriptors). We compare our algorithms to the best known methods for these problems, i.e. LSH, ANN and FLANN. We show analytically and experimentally that we can do better for moderate approximation factor. Our algorithms are trivial to parallelize. In the experiments conducted, running on couple of million im- ages, our algorithms show meaningful speed-ups when compared with the above mentioned methods.</p><p>2 0.91003817 <a title="337-lsi-2" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>Author: Basura Fernando, Tinne Tuytelaars</p><p>Abstract: In this paper we present a new method for object retrieval starting from multiple query images. The use of multiple queries allows for a more expressive formulation of the query object including, e.g., different viewpoints and/or viewing conditions. This, in turn, leads to more diverse and more accurate retrieval results. When no query images are available to the user, they can easily be retrieved from the internet using a standard image search engine. In particular, we propose a new method based on pattern mining. Using the minimal description length principle, we derive the most suitable set of patterns to describe the query object, with patterns corresponding to local feature configurations. This results in apowerful object-specific mid-level image representation. The archive can then be searched efficiently for similar images based on this representation, using a combination of two inverted file systems. Since the patterns already encode local spatial information, good results on several standard image retrieval datasets are obtained even without costly re-ranking based on geometric verification.</p><p>3 0.89312953 <a title="337-lsi-3" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>Author: Xu Wang, Stefan Atev, John Wright, Gilad Lerman</p><p>Abstract: The problem of efficiently deciding which of a database of models is most similar to a given input query arises throughout modern computer vision. Motivated by applications in recognition, image retrieval and optimization, there has been significant recent interest in the variant of this problem in which the database models are linear subspaces and the input is either a point or a subspace. Current approaches to this problem have poor scaling in high dimensions, and may not guarantee sublinear query complexity. We present a new approach to approximate nearest subspace search, based on a simple, new locality sensitive hash for subspaces. Our approach allows point-tosubspace query for a database of subspaces of arbitrary dimension d, in a time that depends sublinearly on the number of subspaces in the database. The query complexity of our algorithm is linear in the ambient dimension D, allow- ing it to be directly applied to high-dimensional imagery data. Numerical experiments on model problems in image repatching and automatic face recognition confirm the advantages of our algorithm in terms of both speed and accuracy.</p><p>4 0.8879388 <a title="337-lsi-4" href="./iccv-2013-Query-Adaptive_Asymmetrical_Dissimilarities_for_Visual_Object_Retrieval.html">334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</a></p>
<p>Author: Cai-Zhi Zhu, Hervé Jégou, Shin'Ichi Satoh</p><p>Abstract: Visual object retrieval aims at retrieving, from a collection of images, all those in which a given query object appears. It is inherently asymmetric: the query object is mostly included in the database image, while the converse is not necessarily true. However, existing approaches mostly compare the images with symmetrical measures, without considering the different roles of query and database. This paper first measure the extent of asymmetry on large-scale public datasets reflecting this task. Considering the standard bag-of-words representation, we then propose new asymmetrical dissimilarities accounting for the different inlier ratios associated with query and database images. These asymmetrical measures depend on the query, yet they are compatible with an inverted file structure, without noticeably impacting search efficiency. Our experiments show the benefit of our approach, and show that the visual object retrieval task is better treated asymmetrically, in the spirit of state-of-the-art text retrieval.</p><p>5 0.84953481 <a title="337-lsi-5" href="./iccv-2013-Joint_Inverted_Indexing.html">221 iccv-2013-Joint Inverted Indexing</a></p>
<p>Author: Yan Xia, Kaiming He, Fang Wen, Jian Sun</p><p>Abstract: Inverted indexing is a popular non-exhaustive solution to large scale search. An inverted file is built by a quantizer such as k-means or a tree structure. It has been found that multiple inverted files, obtained by multiple independent random quantizers, are able to achieve practically good recall and speed. Instead of computing the multiple quantizers independently, we present a method that creates them jointly. Our method jointly optimizes all codewords in all quantizers. Then it assigns these codewords to the quantizers. In experiments this method shows significant improvement over various existing methods that use multiple independent quantizers. On the one-billion set of SIFT vectors, our method is faster and more accurate than a recent state-of-the-art inverted indexing method.</p><p>6 0.81672323 <a title="337-lsi-6" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>7 0.81295121 <a title="337-lsi-7" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>8 0.80864906 <a title="337-lsi-8" href="./iccv-2013-Fast_Neighborhood_Graph_Search_Using_Cartesian_Concatenation.html">159 iccv-2013-Fast Neighborhood Graph Search Using Cartesian Concatenation</a></p>
<p>9 0.78042871 <a title="337-lsi-9" href="./iccv-2013-Quantize_and_Conquer%3A_A_Dimensionality-Recursive_Solution_to_Clustering%2C_Vector_Quantization%2C_and_Image_Retrieval.html">333 iccv-2013-Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval</a></p>
<p>10 0.74795991 <a title="337-lsi-10" href="./iccv-2013-To_Aggregate_or_Not_to_aggregate%3A_Selective_Match_Kernels_for_Image_Search.html">419 iccv-2013-To Aggregate or Not to aggregate: Selective Match Kernels for Image Search</a></p>
<p>11 0.73367357 <a title="337-lsi-11" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>12 0.71484637 <a title="337-lsi-12" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>13 0.69252032 <a title="337-lsi-13" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<p>14 0.64244312 <a title="337-lsi-14" href="./iccv-2013-3D_Sub-query_Expansion_for_Improving_Sketch-Based_Multi-view_Image_Retrieval.html">3 iccv-2013-3D Sub-query Expansion for Improving Sketch-Based Multi-view Image Retrieval</a></p>
<p>15 0.55164105 <a title="337-lsi-15" href="./iccv-2013-EVSAC%3A_Accelerating_Hypotheses_Generation_by_Modeling_Matching_Scores_with_Extreme_Value_Theory.html">131 iccv-2013-EVSAC: Accelerating Hypotheses Generation by Modeling Matching Scores with Extreme Value Theory</a></p>
<p>16 0.50897294 <a title="337-lsi-16" href="./iccv-2013-SYM-FISH%3A_A_Symmetry-Aware_Flip_Invariant_Sketch_Histogram_Shape_Descriptor.html">368 iccv-2013-SYM-FISH: A Symmetry-Aware Flip Invariant Sketch Histogram Shape Descriptor</a></p>
<p>17 0.50574386 <a title="337-lsi-17" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>18 0.49661487 <a title="337-lsi-18" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<p>19 0.49097028 <a title="337-lsi-19" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>20 0.48801228 <a title="337-lsi-20" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.072), (7, 0.01), (26, 0.032), (31, 0.018), (42, 0.094), (64, 0.019), (73, 0.013), (89, 0.672)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99905151 <a title="337-lda-1" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>Author: Kevin Tang, Bangpeng Yao, Li Fei-Fei, Daphne Koller</p><p>Abstract: In this paper, we tackle the problem of combining features extracted from video for complex event recognition. Feature combination is an especially relevant task in video data, as there are many features we can extract, ranging from image features computed from individual frames to video features that take temporal information into account. To combine features effectively, we propose a method that is able to be selective of different subsets of features, as some features or feature combinations may be uninformative for certain classes. We introduce a hierarchical method for combining features based on the AND/OR graph structure, where nodes in the graph represent combinations of different sets of features. Our method automatically learns the structure of the AND/OR graph using score-based structure learning, and we introduce an inference procedure that is able to efficiently compute structure scores. We present promising results and analysis on the difficult and large-scale 2011 TRECVID Multimedia Event Detection dataset [17].</p><p>2 0.99762642 <a title="337-lda-2" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>Author: Qian-Yi Zhou, Stephen Miller, Vladlen Koltun</p><p>Abstract: We present an approach to reconstruction of detailed scene geometry from range video. Range data produced by commodity handheld cameras suffers from high-frequency errors and low-frequency distortion. Our approach deals with both sources of error by reconstructing locally smooth scene fragments and letting these fragments deform in order to align to each other. We develop a volumetric registration formulation that leverages the smoothness of the deformation to make optimization practical for large scenes. Experimental results demonstrate that our approach substantially increases the fidelity of complex scene geometry reconstructed with commodity handheld cameras.</p><p>3 0.99626338 <a title="337-lda-3" href="./iccv-2013-Deblurring_by_Example_Using_Dense_Correspondence.html">103 iccv-2013-Deblurring by Example Using Dense Correspondence</a></p>
<p>Author: Yoav Hacohen, Eli Shechtman, Dani Lischinski</p><p>Abstract: This paper presents a new method for deblurring photos using a sharp reference example that contains some shared content with the blurry photo. Most previous deblurring methods that exploit information from other photos require an accurately registered photo of the same static scene. In contrast, our method aims to exploit reference images where the shared content may have undergone substantial photometric and non-rigid geometric transformations, as these are the kind of reference images most likely to be found in personal photo albums. Our approach builds upon a recent method for examplebased deblurring using non-rigid dense correspondence (NRDC) [11] and extends it in two ways. First, we suggest exploiting information from the reference image not only for blur kernel estimation, but also as a powerful local prior for the non-blind deconvolution step. Second, we introduce a simple yet robust technique for spatially varying blur estimation, rather than assuming spatially uniform blur. Unlike the aboveprevious method, which hasproven successful only with simple deblurring scenarios, we demonstrate that our method succeeds on a variety of real-world examples. We provide quantitative and qualitative evaluation of our method and show that it outperforms the state-of-the-art.</p><p>4 0.99611151 <a title="337-lda-4" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>Author: Heng Wang, Cordelia Schmid</p><p>Abstract: Recently dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets. This paper improves their performance by taking into account camera motion to correct them. To estimate camera motion, we match feature points between frames using SURF descriptors and dense optical flow, which are shown to be complementary. These matches are, then, used to robustly estimate a homography with RANSAC. Human motion is in general different from camera motion and generates inconsistent matches. To improve the estimation, a human detector is employed to remove these matches. Given the estimated camera motion, we remove trajectories consistent with it. We also use this estimation to cancel out camera motion from the optical flow. This significantly improves motion-based descriptors, such as HOF and MBH. Experimental results onfour challenging action datasets (i.e., Hollywood2, HMDB51, Olympic Sports and UCF50) significantly outperform the current state of the art.</p><p>5 0.99562848 <a title="337-lda-5" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>Author: Dan Xie, Sinisa Todorovic, Song-Chun Zhu</p><p>Abstract: This paper presents an approach to localizing functional objects in surveillance videos without domain knowledge about semantic object classes that may appear in the scene. Functional objects do not have discriminative appearance and shape, but they affect behavior of people in the scene. For example, they “attract” people to approach them for satisfying certain needs (e.g., vending machines could quench thirst), or “repel” people to avoid them (e.g., grass lawns). Therefore, functional objects can be viewed as “dark matter”, emanating “dark energy ” that affects people ’s trajectories in the video. To detect “dark matter” and infer their “dark energy ” field, we extend the Lagrangian mechanics. People are treated as particle-agents with latent intents to approach “dark matter” and thus satisfy their needs, where their motions are subject to a composite “dark energy ” field of all functional objects in the scene. We make the assumption that people take globally optimal paths toward the intended “dark matter” while avoiding latent obstacles. A Bayesian framework is used to probabilistically model: people ’s trajectories and intents, constraint map of the scene, and locations of functional objects. A data-driven Markov Chain Monte Carlo (MCMC) process is used for inference. Our evaluation on videos of public squares and courtyards demonstrates our effectiveness in localizing functional objects and predicting people ’s trajectories in unobserved parts of the video footage.</p><p>same-paper 6 0.99535304 <a title="337-lda-6" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>7 0.99368483 <a title="337-lda-7" href="./iccv-2013-Optimization_Problems_for_Fast_AAM_Fitting_in-the-Wild.html">302 iccv-2013-Optimization Problems for Fast AAM Fitting in-the-Wild</a></p>
<p>8 0.99278235 <a title="337-lda-8" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>9 0.98770297 <a title="337-lda-9" href="./iccv-2013-3D_Scene_Understanding_by_Voxel-CRF.html">2 iccv-2013-3D Scene Understanding by Voxel-CRF</a></p>
<p>10 0.98009551 <a title="337-lda-10" href="./iccv-2013-Shufflets%3A_Shared_Mid-level_Parts_for_Fast_Object_Detection.html">390 iccv-2013-Shufflets: Shared Mid-level Parts for Fast Object Detection</a></p>
<p>11 0.97562408 <a title="337-lda-11" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>12 0.96474946 <a title="337-lda-12" href="./iccv-2013-Large-Scale_Multi-resolution_Surface_Reconstruction_from_RGB-D_Sequences.html">228 iccv-2013-Large-Scale Multi-resolution Surface Reconstruction from RGB-D Sequences</a></p>
<p>13 0.96192908 <a title="337-lda-13" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>14 0.96068931 <a title="337-lda-14" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>15 0.9581697 <a title="337-lda-15" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>16 0.9573679 <a title="337-lda-16" href="./iccv-2013-Saliency_Detection_in_Large_Point_Sets.html">370 iccv-2013-Saliency Detection in Large Point Sets</a></p>
<p>17 0.95683795 <a title="337-lda-17" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>18 0.95502412 <a title="337-lda-18" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<p>19 0.95491523 <a title="337-lda-19" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>20 0.953228 <a title="337-lda-20" href="./iccv-2013-Active_MAP_Inference_in_CRFs_for_Efficient_Semantic_Segmentation.html">42 iccv-2013-Active MAP Inference in CRFs for Efficient Semantic Segmentation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
