<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>25 iccv-2013-A Novel Earth Mover's Distance Methodology for Image Matching with Gaussian Mixture Models</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-25" href="#">iccv2013-25</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>25 iccv-2013-A Novel Earth Mover's Distance Methodology for Image Matching with Gaussian Mixture Models</h1>
<br/><p>Source: <a title="iccv-2013-25-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Li_A_Novel_Earth_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Peihua Li, Qilong Wang, Lei Zhang</p><p>Abstract: The similarity or distance measure between Gaussian mixture models (GMMs) plays a crucial role in contentbased image matching. Though the Earth Mover’s Distance (EMD) has shown its advantages in matching histogram features, its potentials in matching GMMs remain unclear and are not fully explored. To address this problem, we propose a novel EMD methodology for GMM matching. We ?rst present a sparse representation based EMD called SR-EMD by exploiting the sparse property of the underlying problem. SR-EMD is more ef?cient and robust than the conventional EMD. Second, we present two novel ground distances between component Gaussians based on the information geometry. The perspective from the Riemannian geometry distinguishes the proposed ground distances from the classical entropy- or divergence-based ones. Furthermore, motivated by the success of distance metric learning of vector data, we make the ?rst attempt to learn the EMD distance metrics between GMMs by using a simple yet effective supervised pair-wise based method. It can adapt the distance metrics between GMMs to speci?c classi?ca- tion tasks. The proposed method is evaluated on both simulated data and benchmark real databases and achieves very promising performance.</p><p>Reference: <a title="iccv-2013-25-reference" href="../iccv2013_reference/iccv-2013-A_Novel_Earth_Mover%27s_Distance_Methodology_for_Image_Matching_with_Gaussian_Mixture_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 hk  Abstract The similarity or distance measure between Gaussian mixture models (GMMs) plays a crucial role in contentbased image matching. [sent-8, score-0.091]
</p><p>2 rst present a sparse representation based EMD called SR-EMD by exploiting the sparse property of the underlying problem. [sent-12, score-0.213]
</p><p>3 Second, we present two novel ground distances between component Gaussians based on the information geometry. [sent-15, score-0.121]
</p><p>4 The perspective from the Riemannian geometry distinguishes the proposed ground distances from the classical entropy- or divergence-based ones. [sent-16, score-0.095]
</p><p>5 Furthermore, motivated by the success of distance metric learning of vector data, we make the ? [sent-17, score-0.186]
</p><p>6 rst attempt to learn the EMD distance metrics between GMMs by using a simple yet effective supervised pair-wise based method. [sent-18, score-0.289]
</p><p>7 It can adapt the distance metrics between GMMs to speci? [sent-19, score-0.119]
</p><p>8 The Earth Mover’s Distance (EMD) [24] can compare cross bins of histograms (or signatures), which has proven its advantages over the conventional bin-to-bin based measures. [sent-29, score-0.063]
</p><p>9 ciency and accuracy of EMD for histogram matching [16, 25, 22, 21, 30]. [sent-31, score-0.084]
</p><p>10 [7] also focused on the approximation of K-L divergence and studied two strategies, which are based on the analytical-form solution between component Gaussians and on unscented transform, respectively. [sent-38, score-0.076]
</p><p>11 [3] proposed the Gaussian Quadratic Form Distance (GQFD) as similarity measure, which has a closed form for GMMs of diagonal covariance matrices and is well suited for high-dimensional image descriptors. [sent-41, score-0.138]
</p><p>12 Compared with the conventional EMD, SR-EMD is more robust to image noise and is more ef? [sent-54, score-0.086]
</p><p>13 (2) An appropriate ground distance plays a fundamental role for the effectiveness of EMD. [sent-56, score-0.114]
</p><p>14 Two novel ground distances are presented based on theory of information geometry to boost the effectiveness of EMD. [sent-61, score-0.095]
</p><p>15 rst attempt to learn the distance metrics between GMMs, aiming to adapt the metrics to speci? [sent-63, score-0.313]
</p><p>16 a Lie group or regarding it as the product of Lie groups, we present two novel ground distances between component Gaussians. [sent-66, score-0.183]
</p><p>17 Unlike the traditional ones, the new ground distances can characterize the intrinsic distance of the underlying Riemannian manifold of the space of Gaussians. [sent-67, score-0.162]
</p><p>18 (3) We propose a simple yet effective supervised EMD learning method for GMMs, in order to adapt the distance metrics between GMMs to speci? [sent-68, score-0.172]
</p><p>19 Though metric learning methods for vector data have been extensively studied, little work has been done on the metric learning for GMMs. [sent-70, score-0.238]
</p><p>20 bbreviated as gi below) is a componwehnetr eG aNus(sfi|μan with prior probability wi, mean vector μi and covariance matrix Σi. [sent-81, score-0.117]
</p><p>21 It is modeled as a classical transportation problem, which can be written in the standard matrix form EMD(Gp,Gq) s. [sent-90, score-0.065]
</p><p>22 0,  (2)  where cpq is an npnq-dimensional vector which is the vectorization of ground distance matrix {dipjq}np , Apq is a (np+nq) n× o (npnq) mda dtirsitxa nwcieth m m0a or 1x e {ndtrie}s, and the weight vector ypq (=n [wp1, . [sent-93, score-0.424]
</p><p>23 Therefore zpq is sparse, for example, there are only less than 20% non-zero entries if np = nq = 10. [sent-110, score-0.291]
</p><p>24 Let us consider a more general case where the data is contaminated by noise, that is, Apqzpq = ypq + vpq, where vpq is Gaussian noise of zero mean. [sent-111, score-0.209]
</p><p>25 Recalling the sparse property of (2), we let Cpqzpq = xpq, where Cpq is a diagonal matrix whose diagonal entries are the elements of cpq. [sent-112, score-0.11]
</p><p>26 Here the distance is the sum of two  terms: the ? [sent-132, score-0.067]
</p><p>27 rst term measures the error of the system of linear constraint equations while the second one measures the transportation cost. [sent-133, score-0.199]
</p><p>28 rst term is actually very small and negligible compared with that of the second. [sent-135, score-0.167]
</p><p>29 Formulating the conventional EMD (2) as a sparse representation problem in (3) brings two bene? [sent-138, score-0.086]
</p><p>30 It is known that, from the Bayesian viewpoint, the objective function (3) leads to the optimal solution if vpq is Gaussian distributed and the prior distribution of xpq is Laplacian. [sent-141, score-0.337]
</p><p>31 As the algorithm converges in about no more than np + nq iterations, and in each iteration the complexity is comparable to that of the least squares [9, Sec. [sent-147, score-0.24]
</p><p>32 In the conventional EMD, as each iteration involves operation of Gaussian elimination of the constraint system, the computational complexity is O(nl (np + nq)2npnq), where nl denotes the number of  iterations involved. [sent-150, score-0.092]
</p><p>33 ciency of SR-EMD is significantly higher than the conventional EMD, particularly for large size problems. [sent-155, score-0.147]
</p><p>34 Ground Distances Between Component Gaussians We propose two novel ground distances between component Gaussians based on information geometry [13], which conerns the study of the probability and statistics from the Riemannian geometrical point of view. [sent-158, score-0.121]
</p><p>35 By embedding the space ofGaussians into a Lie group or regarding it as a product of Lie groups, we can measure the intrinsic distance between Gaussians in the underlying Riemannian manifold. [sent-159, score-0.129]
</p><p>36 The proposed ground distances make the metric learning for GMMs possible. [sent-160, score-0.214]
</p><p>37 Ground distance based on Lie Group The space of multivariate Gaussians is a Riemannian manifold and can be embedded into the space of SPD matrices [19]. [sent-161, score-0.094]
</p><p>38 Lithe tth Ne mean vector 0 and covariance matrix I (identity matrix). [sent-163, score-0.117]
</p><p>39 The space of SPD matrices is a Lie group that forms a Riemannian manifold, and in this paper we use the LogEuclidean metric [2] to measure the intrinsic distance in this space. [sent-183, score-0.222]
</p><p>40 If M is the identity matrix, (5) reduces to the geodesic distance between Pi and Pj . [sent-206, score-0.094]
</p><p>41 As d(Pi, Pj) is a metric, dgi (M) is also a metric (matrix mahalanobis norm). [sent-207, score-0.166]
</p><p>42 F ,  (6)  we see that the ground distance (5) can be seen as a linear transformation ofthe matrix in the logarithmic domain. [sent-211, score-0.183]
</p><p>43 ne the ground distance (5) or (6) is that in the Log-Euclidean framework, SPDn+ is endoor(w6e)d i sw thithat a nlti nheea Lr space cstlriduecatnurfer. [sent-213, score-0.138]
</p><p>44 It can be interpreted as that while retaining the geodesic distance, we attempt to seek a linear transformation so that the distance is more discriminative. [sent-215, score-0.094]
</p><p>45 ×  Ground distance based on product of Lie groups An ndimensional Gaussian is determinedby its mean vector, μ ∈ dRimn, annsido tnhael Gcoavuasrsiiaanncies dmetaterrimx,i nΣed ∈b ySitPsDmen+a. [sent-217, score-0.095]
</p><p>46 n vIte cist o krn,oμw ∈n that, R annd dis t a eL cieo group cuen mdeart vriexc,to Σr a ∈dd SitiPonD and SPDn+ is a Lie group uan Ldieer g logarithmic multiplication ? [sent-218, score-0.127]
</p><p>47 Σ2)  (7)  It can be shown that this product group is also a Lie group and its Lie algebra is Rn Sn. [sent-227, score-0.096]
</p><p>48 It is noteworthy that the distance in the Lie group Rn between two mean vectors should be appropriately weighted by the associative covariance matrices, and it is also reasonable to balance their distance in Rn and that in SPDn+ . [sent-229, score-0.252]
</p><p>49 dgi (θ) is a metric satisfying all the metric axioms, which can be easily shown by noting that agi and bgi are both metrics. [sent-243, score-0.267]
</p><p>50 Computational complexity of the ground distances In the proposed ground distances two component Gaussians are “decoupled” in the sense that the logarithm of the ebmedding SPD matrix (5), or the inverse and logarithm of the covariance matrix (8) of each component Gaussian can be computed of? [sent-245, score-0.502]
</p><p>51 rst studied in [30], which, however, is not applicable to GMMs. [sent-253, score-0.167]
</p><p>52 Here we propose a simple supervised pair-wise based metric learning method for GMMs. [sent-256, score-0.147]
</p><p>53 rst consider metric learning for the ground distance based on Lie group. [sent-264, score-0.4]
</p><p>54 9) H ienrveo |lv ·e |s d tehneo t joesin tth eop ctiamrdiiznaatiloityn of ground distance matrix Cpq (M) and vector xpq, which, similar to the problem of dictionary learning [1], is nonlinear and non-convex. [sent-286, score-0.172]
</p><p>55 xed, seeking xpq reduces to a typical sparse representation problem; when xpq is ? [sent-290, score-0.595]
</p><p>56 vector r = ypq − ApqCp−q1 (M)xpq, Apq (i, j) denotes the entry at row i, column j of the matrix Apq, and  Qp(jq)  =  11669922  (log(Rpk) − log(Rlq))(log(Rkp) − log(Rlq))T, where k = (j + 1) m)o −d (np +R nq), l = j/(np −+ l nq) R+ 1. [sent-309, score-0.168]
</p><p>57 Let us then consider the metric learning for the ground distance based on product of Lie groups. [sent-313, score-0.261]
</p><p>58 This metric learning problem (is− simpler and more ef? [sent-340, score-0.119]
</p><p>59 rst perform experiments on noisy images to verify the robustness and ef? [sent-345, score-0.167]
</p><p>60 Then, we evaluate, on benchmark image retrieval databases and texture databases, respectively, the SR-EMD with Lie group-based ground distance (SR-EMD-M) and SR-EMD with product of Lie groups-based one (SR-EMD-θ), as well as SR-EMD-M and SR-EMD-θ with metric learning. [sent-347, score-0.35]
</p><p>61 In the default cases without metric learning, we set θ = 0. [sent-358, score-0.094]
</p><p>62 rst compare SR-EMD with the conventional EMD on “similar” GMMs. [sent-372, score-0.23]
</p><p>63 We compute the distance of two GMMs estimated from a clean image and its polluted counterpart using SR-EMD and the conventional EMD (the ground truth is 0). [sent-373, score-0.222]
</p><p>64 It can be seen that with the increase of noise level, the errors of both methods get larger; but apparently, the errors of SR-EMD are much smaller that  those of the conventional EMD. [sent-380, score-0.086]
</p><p>65 For each noise level, we compute the distance between a clean image and a polluted image from different classes; the relative errors (the ground truth can be calculated from the two clean images involved) are computed and averaged over all such pairs at that level. [sent-382, score-0.204]
</p><p>66 2(b), the SR-EMD again demonstrates better robustness to noise than the conventional EMD. [sent-384, score-0.086]
</p><p>67 The robustness of SR-EMD and conventional EMD to noise. [sent-401, score-0.063]
</p><p>68 ciency of SR-EMD, we build GMM Gp with varying number (np=10∼150) ofcomponent Gaussians, and let Gq = Gp. [sent-403, score-0.084]
</p><p>69 We= compare tohfec running time of SR-EMD and EMD in computing one distance (averaged over 1000 times). [sent-404, score-0.067]
</p><p>70 Note that the computation of ground distances is the same for SR-EMD and EMD and this part is not counted since we aim to compare the optimization algorithms. [sent-405, score-0.095]
</p><p>71 When np< 10, the two algorithms are comparable; however, as np increases, the speedups of SR-EMD over EMD becomes more and more signi? [sent-408, score-0.103]
</p><p>72 Running time comparison between SR-EMD and conventional EMD (unit: s). [sent-415, score-0.063]
</p><p>73 , [R, G, B, |Ix |, |Iy | , |Ixx |, |Iyy |], which are subject to matrix logarithm aInd|, |tIhe|n, Ivec|t,o|rIize|d] t wo 2ic8h-d airmee snusbiojencatl t vectors; and (3) the 45-dimensional RGB histogram via the code released in [27]. [sent-440, score-0.088]
</p><p>74 In GQFD the diagonal covariance matrices are used and all features involved are free of dimensionality reduction; all the other methods use full covariance matrices in estimating GMMs. [sent-442, score-0.249]
</p><p>75 Note that the results of Match-KL obtained in [3] were not satisfactory and we conjecture that this metric may not be suitable for high-dimensional GMMs of diagonal covariance matrices. [sent-444, score-0.205]
</p><p>76 In this paper we implement it by ourselves for comparison of GMMs with full covariance matrices. [sent-445, score-0.084]
</p><p>77 s0t875  On the Corel Wang database, SR-EMD-M and SREMD-θ (without metric learning) are both better than the other methods while SR-EMD-θ performs the best. [sent-457, score-0.094]
</p><p>78 With metric learning, the MAP values of SR-EMD-M and SR-EMD-θ increase about 1. [sent-462, score-0.094]
</p><p>79 eA psr they are mcleotsherelatives, we attribute the performance gains to the novel sparse representation-based formulation and the ground distances. [sent-466, score-0.07]
</p><p>80 The improvement of MAP values of SR-EMD-M and SREMD-θ with metric learning is about 2. [sent-469, score-0.119]
</p><p>81 In EMD-like methods, while the computations of the ground distances are implemented with Matlab, the algorithm for solving EMD is written with C++. [sent-476, score-0.095]
</p><p>82 Table 3 presents the the average time for one distance computation in image retrieval averaged over 1000 trials. [sent-477, score-0.114]
</p><p>83 However, please note that in EMDlike algorithms, ground distance computation takes a large portion of the total running time. [sent-481, score-0.114]
</p><p>84 cation  We further use three benchmark texture databases to evaluate the performance of the proposed method: KTH-TIPS [10], CUReT [5], and UMD http://www. [sent-492, score-0.211]
</p><p>85 Summary of benchmark texture databases  UKDCUMTatRHaDbe-aT sIPeSny Reo s t. [sent-514, score-0.089]
</p><p>86 Between SR-EMD-M and SR-EMD-θ, the former has clear advantage and its performance is even comparable to SR-EMD-θ with metric learning. [sent-523, score-0.094]
</p><p>87 Both SREMD-θ and SR-EMD-M’s performance can be improved by 2%∼3% with metric learning. [sent-524, score-0.094]
</p><p>88 After metric learning, both have performance increase of 2%∼3%, while SRiEnMg,D bo-θt his h a vliet tplee rfboerttmera tnhcaeni nScRre-aEsMeD of- 2M%. [sent-529, score-0.094]
</p><p>89 ∼On3 %C,U wRheiTle, SSRR-EMD-θ and SR-EMD-M have very similar performance before and after metric learning, and they have clear superiority to the other two methods. [sent-530, score-0.094]
</p><p>90 Finally, we compare SR-EMD-θ and SR-EMD-M (with metric learning) with six state-of-the-art methods on texture classi? [sent-531, score-0.143]
</p><p>91 Our contributions lie in the sparse representation formulation of EMD, and the novel ground distances between component Gaussians based on information geometry. [sent-555, score-0.214]
</p><p>92 In addition, we presented a simple yet effective supervised metric learning method to adapt the distance metrics between GMMs to the given data. [sent-556, score-0.266]
</p><p>93 Compared with the conventional EMD, it is more ef? [sent-557, score-0.063]
</p><p>94 Though metric learning has been extensively studied for vector data, to the best of our knowledge, the problem of metric learning for GMMs is not explored yet and we made the ? [sent-560, score-0.238]
</p><p>95 It would be interesting to study more advanced metric learning methods for GMMs, which may improve further the ef? [sent-562, score-0.119]
</p><p>96 cient image similarity measure based on approximations of KLdivergence between two gaussian mixtures. [sent-635, score-0.095]
</p><p>97 Approximating the kullback leibler divergence between gaussian mixture models. [sent-662, score-0.104]
</p><p>98 A linear time histogram metric for improved SIFT matching. [sent-726, score-0.094]
</p><p>99 The Earth Mover’s Distance as a metric for image retrieval. [sent-744, score-0.094]
</p><p>100 Supervised earth mover’s distance learning and its computer vision applications. [sent-779, score-0.174]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('emd', 0.488), ('gmms', 0.418), ('xpq', 0.286), ('classi', 0.247), ('rst', 0.167), ('nq', 0.137), ('gqfd', 0.135), ('ypq', 0.135), ('gq', 0.124), ('cation', 0.122), ('gmm', 0.122), ('cpq', 0.118), ('gp', 0.107), ('np', 0.103), ('mover', 0.103), ('spdn', 0.101), ('ef', 0.1), ('gaussians', 0.096), ('metric', 0.094), ('spd', 0.091), ('apqcp', 0.084), ('ciency', 0.084), ('covariance', 0.084), ('earth', 0.082), ('riemannian', 0.081), ('lie', 0.07), ('rgbhist', 0.067), ('slk', 0.067), ('distance', 0.067), ('cient', 0.065), ('log', 0.064), ('conventional', 0.063), ('curet', 0.06), ('umd', 0.06), ('logarithm', 0.055), ('corel', 0.052), ('pq', 0.051), ('signi', 0.051), ('vpq', 0.051), ('zpq', 0.051), ('divergence', 0.05), ('texture', 0.049), ('distances', 0.048), ('ground', 0.047), ('apq', 0.045), ('dgi', 0.045), ('polluted', 0.045), ('speci', 0.045), ('qm', 0.044), ('pj', 0.044), ('nite', 0.041), ('databases', 0.04), ('logarithmic', 0.036), ('methodology', 0.035), ('group', 0.034), ('affk', 0.034), ('apqzpq', 0.034), ('beecks', 0.034), ('bgi', 0.034), ('npnq', 0.034), ('rlq', 0.034), ('spdk', 0.034), ('sremd', 0.034), ('matrix', 0.033), ('transportation', 0.032), ('gaussian', 0.03), ('pm', 0.03), ('goldberger', 0.03), ('pennec', 0.03), ('nl', 0.029), ('supervised', 0.028), ('product', 0.028), ('fillard', 0.028), ('iyy', 0.028), ('pele', 0.028), ('mahalanobis', 0.027), ('diagonal', 0.027), ('matrices', 0.027), ('geodesic', 0.027), ('metrics', 0.027), ('eg', 0.026), ('ixx', 0.026), ('component', 0.026), ('learning', 0.025), ('retrieval', 0.025), ('accomplished', 0.025), ('adapt', 0.025), ('bde', 0.025), ('cq', 0.025), ('music', 0.025), ('xed', 0.025), ('ne', 0.024), ('pi', 0.024), ('mda', 0.024), ('mixture', 0.024), ('multiplication', 0.023), ('sparse', 0.023), ('noise', 0.023), ('averaged', 0.022), ('ned', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="25-tfidf-1" href="./iccv-2013-A_Novel_Earth_Mover%27s_Distance_Methodology_for_Image_Matching_with_Gaussian_Mixture_Models.html">25 iccv-2013-A Novel Earth Mover's Distance Methodology for Image Matching with Gaussian Mixture Models</a></p>
<p>Author: Peihua Li, Qilong Wang, Lei Zhang</p><p>Abstract: The similarity or distance measure between Gaussian mixture models (GMMs) plays a crucial role in contentbased image matching. Though the Earth Mover’s Distance (EMD) has shown its advantages in matching histogram features, its potentials in matching GMMs remain unclear and are not fully explored. To address this problem, we propose a novel EMD methodology for GMM matching. We ?rst present a sparse representation based EMD called SR-EMD by exploiting the sparse property of the underlying problem. SR-EMD is more ef?cient and robust than the conventional EMD. Second, we present two novel ground distances between component Gaussians based on the information geometry. The perspective from the Riemannian geometry distinguishes the proposed ground distances from the classical entropy- or divergence-based ones. Furthermore, motivated by the success of distance metric learning of vector data, we make the ?rst attempt to learn the EMD distance metrics between GMMs by using a simple yet effective supervised pair-wise based method. It can adapt the distance metrics between GMMs to speci?c classi?ca- tion tasks. The proposed method is evaluated on both simulated data and benchmark real databases and achieves very promising performance.</p><p>2 0.28181154 <a title="25-tfidf-2" href="./iccv-2013-Log-Euclidean_Kernels_for_Sparse_Representation_and_Dictionary_Learning.html">257 iccv-2013-Log-Euclidean Kernels for Sparse Representation and Dictionary Learning</a></p>
<p>Author: Peihua Li, Qilong Wang, Wangmeng Zuo, Lei Zhang</p><p>Abstract: The symmetric positive de?nite (SPD) matrices have been widely used in image and vision problems. Recently there are growing interests in studying sparse representation (SR) of SPD matrices, motivated by the great success of SR for vector data. Though the space of SPD matrices is well-known to form a Lie group that is a Riemannian manifold, existing work fails to take full advantage of its geometric structure. This paper attempts to tackle this problem by proposing a kernel based method for SR and dictionary learning (DL) of SPD matrices. We disclose that the space of SPD matrices, with the operations of logarithmic multiplication and scalar logarithmic multiplication de?ned in the Log-Euclidean framework, is a complete inner product space. We can thus develop a broad family of kernels that satis?es Mercer’s condition. These kernels characterize the geodesic distance and can be computed ef?ciently. We also consider the geometric structure in the DL process by updating atom matrices in the Riemannian space instead of in the Euclidean space. The proposed method is evaluated with various vision problems and shows notable per- formance gains over state-of-the-arts.</p><p>3 0.16054064 <a title="25-tfidf-3" href="./iccv-2013-Joint_Deep_Learning_for_Pedestrian_Detection.html">220 iccv-2013-Joint Deep Learning for Pedestrian Detection</a></p>
<p>Author: Wanli Ouyang, Xiaogang Wang</p><p>Abstract: Feature extraction, deformation handling, occlusion handling, and classi?cation are four important components in pedestrian detection. Existing methods learn or design these components either individually or sequentially. The interaction among these components is not yet well explored. This paper proposes that they should be jointly learned in order to maximize their strengths through cooperation. We formulate these four components into a joint deep learning framework and propose a new deep network architecture1. By establishing automatic, mutual interaction among components, the deep model achieves a 9% reduction in the average miss rate compared with the current best-performing pedestrian detection approaches on the largest Caltech benchmark dataset.</p><p>4 0.10339738 <a title="25-tfidf-4" href="./iccv-2013-Saliency_and_Human_Fixations%3A_State-of-the-Art_and_Study_of_Comparison_Metrics.html">373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</a></p>
<p>Author: Nicolas Riche, Matthieu Duvinage, Matei Mancas, Bernard Gosselin, Thierry Dutoit</p><p>Abstract: Visual saliency has been an increasingly active research area in the last ten years with dozens of saliency models recently published. Nowadays, one of the big challenges in the field is to find a way to fairly evaluate all of these models. In this paper, on human eye fixations ,we compare the ranking of 12 state-of-the art saliency models using 12 similarity metrics. The comparison is done on Jian Li ’s database containing several hundreds of natural images. Based on Kendall concordance coefficient, it is shown that some of the metrics are strongly correlated leading to a redundancy in the performance metrics reported in the available benchmarks. On the other hand, other metrics provide a more diverse picture of models ’ overall performance. As a recommendation, three similarity metrics should be used to obtain a complete point of view of saliency model performance.</p><p>5 0.080681659 <a title="25-tfidf-5" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>Author: Qiong Cao, Yiming Ying, Peng Li</p><p>Abstract: Recently, there is a considerable amount of efforts devoted to the problem of unconstrained face verification, where the task is to predict whether pairs of images are from the same person or not. This problem is challenging and difficult due to the large variations in face images. In this paper, we develop a novel regularization framework to learn similarity metrics for unconstrained face verification. We formulate its objective function by incorporating the robustness to the large intra-personal variations and the discriminative power of novel similarity metrics. In addition, our formulation is a convex optimization problem which guarantees the existence of its global solution. Experiments show that our proposed method achieves the state-of-the-art results on the challenging Labeled Faces in the Wild (LFW) database [10].</p><p>6 0.074682236 <a title="25-tfidf-6" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>7 0.073534898 <a title="25-tfidf-7" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>8 0.063484803 <a title="25-tfidf-8" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>9 0.060023613 <a title="25-tfidf-9" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>10 0.051680688 <a title="25-tfidf-10" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<p>11 0.047231384 <a title="25-tfidf-11" href="./iccv-2013-Quadruplet-Wise_Image_Similarity_Learning.html">332 iccv-2013-Quadruplet-Wise Image Similarity Learning</a></p>
<p>12 0.046285942 <a title="25-tfidf-12" href="./iccv-2013-From_Point_to_Set%3A_Extend_the_Learning_of_Distance_Metrics.html">177 iccv-2013-From Point to Set: Extend the Learning of Distance Metrics</a></p>
<p>13 0.045396674 <a title="25-tfidf-13" href="./iccv-2013-CoDeL%3A_A_Human_Co-detection_and_Labeling_Framework.html">75 iccv-2013-CoDeL: A Human Co-detection and Labeling Framework</a></p>
<p>14 0.044778999 <a title="25-tfidf-14" href="./iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning.html">212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</a></p>
<p>15 0.043032568 <a title="25-tfidf-15" href="./iccv-2013-Group_Norm_for_Learning_Structured_SVMs_with_Unstructured_Latent_Variables.html">187 iccv-2013-Group Norm for Learning Structured SVMs with Unstructured Latent Variables</a></p>
<p>16 0.042286202 <a title="25-tfidf-16" href="./iccv-2013-Total_Variation_Regularization_for_Functions_with_Values_in_a_Manifold.html">421 iccv-2013-Total Variation Regularization for Functions with Values in a Manifold</a></p>
<p>17 0.0419943 <a title="25-tfidf-17" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>18 0.041516531 <a title="25-tfidf-18" href="./iccv-2013-Matching_Dry_to_Wet_Materials.html">262 iccv-2013-Matching Dry to Wet Materials</a></p>
<p>19 0.041321762 <a title="25-tfidf-19" href="./iccv-2013-Dictionary_Learning_and_Sparse_Coding_on_Grassmann_Manifolds%3A_An_Extrinsic_Solution.html">114 iccv-2013-Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution</a></p>
<p>20 0.041146223 <a title="25-tfidf-20" href="./iccv-2013-A_New_Image_Quality_Metric_for_Image_Auto-denoising.html">23 iccv-2013-A New Image Quality Metric for Image Auto-denoising</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.109), (1, 0.003), (2, -0.015), (3, -0.046), (4, -0.052), (5, 0.023), (6, 0.001), (7, 0.025), (8, 0.006), (9, -0.04), (10, -0.01), (11, -0.022), (12, -0.004), (13, -0.009), (14, 0.051), (15, 0.016), (16, 0.003), (17, 0.0), (18, 0.039), (19, 0.045), (20, 0.012), (21, 0.014), (22, 0.044), (23, -0.026), (24, -0.05), (25, 0.071), (26, 0.061), (27, 0.087), (28, -0.083), (29, 0.131), (30, -0.049), (31, -0.07), (32, -0.007), (33, 0.009), (34, 0.126), (35, 0.129), (36, 0.107), (37, 0.062), (38, -0.095), (39, 0.03), (40, 0.059), (41, 0.067), (42, 0.047), (43, -0.001), (44, 0.023), (45, -0.044), (46, -0.042), (47, -0.044), (48, 0.052), (49, 0.095)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88107204 <a title="25-lsi-1" href="./iccv-2013-A_Novel_Earth_Mover%27s_Distance_Methodology_for_Image_Matching_with_Gaussian_Mixture_Models.html">25 iccv-2013-A Novel Earth Mover's Distance Methodology for Image Matching with Gaussian Mixture Models</a></p>
<p>Author: Peihua Li, Qilong Wang, Lei Zhang</p><p>Abstract: The similarity or distance measure between Gaussian mixture models (GMMs) plays a crucial role in contentbased image matching. Though the Earth Mover’s Distance (EMD) has shown its advantages in matching histogram features, its potentials in matching GMMs remain unclear and are not fully explored. To address this problem, we propose a novel EMD methodology for GMM matching. We ?rst present a sparse representation based EMD called SR-EMD by exploiting the sparse property of the underlying problem. SR-EMD is more ef?cient and robust than the conventional EMD. Second, we present two novel ground distances between component Gaussians based on the information geometry. The perspective from the Riemannian geometry distinguishes the proposed ground distances from the classical entropy- or divergence-based ones. Furthermore, motivated by the success of distance metric learning of vector data, we make the ?rst attempt to learn the EMD distance metrics between GMMs by using a simple yet effective supervised pair-wise based method. It can adapt the distance metrics between GMMs to speci?c classi?ca- tion tasks. The proposed method is evaluated on both simulated data and benchmark real databases and achieves very promising performance.</p><p>2 0.8469696 <a title="25-lsi-2" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>Author: Hesamoddin Salehian, Guang Cheng, Baba C. Vemuri, Jeffrey Ho</p><p>Abstract: Symmetric positive-definite (SPD) matrices are ubiquitous in Computer Vision, Machine Learning and Medical Image Analysis. Finding the center/average of a population of such matrices is a common theme in many algorithms such as clustering, segmentation, principal geodesic analysis, etc. The center of a population of such matrices can be defined using a variety of distance/divergence measures as the minimizer of the sum of squared distances/divergences from the unknown center to the members of the population. It is well known that the computation of the Karcher mean for the space of SPD matrices which is a negativelycurved Riemannian manifold is computationally expensive. Recently, the LogDet divergence-based center was shown to be a computationally attractive alternative. However, the LogDet-based mean of more than two matrices can not be computed in closed form, which makes it computationally less attractive for large populations. In this paper we present a novel recursive estimator for center based on the Stein distance which is the square root of the LogDet di– vergence that is significantly faster than the batch mode computation of this center. The key theoretical contribution is a closed-form solution for the weighted Stein center of two SPD matrices, which is used in the recursive computation of the Stein center for a population of SPD matrices. Additionally, we show experimental evidence of the convergence of our recursive Stein center estimator to the batch mode Stein center. We present applications of our recursive estimator to K-means clustering and image indexing depicting significant time gains over corresponding algorithms that use the batch mode computations. For the latter application, we develop novel hashing functions using the Stein distance and apply it to publicly available data sets, and experimental results have shown favorable com– ∗This research was funded in part by the NIH grant NS066340 to BCV. †Corresponding author parisons to other competing methods.</p><p>3 0.82911086 <a title="25-lsi-3" href="./iccv-2013-Log-Euclidean_Kernels_for_Sparse_Representation_and_Dictionary_Learning.html">257 iccv-2013-Log-Euclidean Kernels for Sparse Representation and Dictionary Learning</a></p>
<p>Author: Peihua Li, Qilong Wang, Wangmeng Zuo, Lei Zhang</p><p>Abstract: The symmetric positive de?nite (SPD) matrices have been widely used in image and vision problems. Recently there are growing interests in studying sparse representation (SR) of SPD matrices, motivated by the great success of SR for vector data. Though the space of SPD matrices is well-known to form a Lie group that is a Riemannian manifold, existing work fails to take full advantage of its geometric structure. This paper attempts to tackle this problem by proposing a kernel based method for SR and dictionary learning (DL) of SPD matrices. We disclose that the space of SPD matrices, with the operations of logarithmic multiplication and scalar logarithmic multiplication de?ned in the Log-Euclidean framework, is a complete inner product space. We can thus develop a broad family of kernels that satis?es Mercer’s condition. These kernels characterize the geodesic distance and can be computed ef?ciently. We also consider the geometric structure in the DL process by updating atom matrices in the Riemannian space instead of in the Euclidean space. The proposed method is evaluated with various vision problems and shows notable per- formance gains over state-of-the-arts.</p><p>4 0.54132128 <a title="25-lsi-4" href="./iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning.html">212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</a></p>
<p>Author: Jiwen Lu, Gang Wang, Pierre Moulin</p><p>Abstract: This paper presents a new approach for image set classification, where each training and testing example contains a set of image instances of an object captured from varying viewpoints or under varying illuminations. While a number of image set classification methods have been proposed in recent years, most of them model each image set as a single linear subspace or mixture of linear subspaces, which may lose some discriminative information for classification. To address this, we propose exploring multiple order statistics as features of image sets, and develop a localized multikernel metric learning (LMKML) algorithm to effectively combine different order statistics information for classification. Our method achieves the state-of-the-art performance on four widely used databases including the Honda/UCSD, CMU Mobo, and Youtube face datasets, and the ETH-80 object dataset.</p><p>5 0.53904337 <a title="25-lsi-5" href="./iccv-2013-From_Point_to_Set%3A_Extend_the_Learning_of_Distance_Metrics.html">177 iccv-2013-From Point to Set: Extend the Learning of Distance Metrics</a></p>
<p>Author: Pengfei Zhu, Lei Zhang, Wangmeng Zuo, David Zhang</p><p>Abstract: Most of the current metric learning methods are proposed for point-to-point distance (PPD) based classification. In many computer vision tasks, however, we need to measure the point-to-set distance (PSD) and even set-to-set distance (SSD) for classification. In this paper, we extend the PPD based Mahalanobis distance metric learning to PSD and SSD based ones, namely point-to-set distance metric learning (PSDML) and set-to-set distance metric learning (SSDML), and solve them under a unified optimization framework. First, we generate positive and negative sample pairs by computing the PSD and SSD between training samples. Then, we characterize each sample pair by its covariance matrix, and propose a covariance kernel based discriminative function. Finally, we tackle the PSDML and SSDMLproblems by using standard support vector machine solvers, making the metric learning very efficient for multiclass visual classification tasks. Experiments on gender classification, digit recognition, object categorization and face recognition show that the proposed metric learning methods can effectively enhance the performance of PSD and SSD based classification.</p><p>6 0.51236886 <a title="25-lsi-6" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>7 0.50273418 <a title="25-lsi-7" href="./iccv-2013-Discriminant_Tracking_Using_Tensor_Representation_with_Semi-supervised_Improvement.html">119 iccv-2013-Discriminant Tracking Using Tensor Representation with Semi-supervised Improvement</a></p>
<p>8 0.50104499 <a title="25-lsi-8" href="./iccv-2013-Joint_Learning_of_Discriminative_Prototypes_and_Large_Margin_Nearest_Neighbor_Classifiers.html">222 iccv-2013-Joint Learning of Discriminative Prototypes and Large Margin Nearest Neighbor Classifiers</a></p>
<p>9 0.4752 <a title="25-lsi-9" href="./iccv-2013-Joint_Deep_Learning_for_Pedestrian_Detection.html">220 iccv-2013-Joint Deep Learning for Pedestrian Detection</a></p>
<p>10 0.45244515 <a title="25-lsi-10" href="./iccv-2013-Robust_Tucker_Tensor_Decomposition_for_Effective_Image_Representation.html">362 iccv-2013-Robust Tucker Tensor Decomposition for Effective Image Representation</a></p>
<p>11 0.44956499 <a title="25-lsi-11" href="./iccv-2013-Parallel_Transport_of_Deformations_in_Shape_Space_of_Elastic_Surfaces.html">307 iccv-2013-Parallel Transport of Deformations in Shape Space of Elastic Surfaces</a></p>
<p>12 0.44225097 <a title="25-lsi-12" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>13 0.44194835 <a title="25-lsi-13" href="./iccv-2013-Perceptual_Fidelity_Aware_Mean_Squared_Error.html">312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</a></p>
<p>14 0.44040829 <a title="25-lsi-14" href="./iccv-2013-Automatic_Kronecker_Product_Model_Based_Detection_of_Repeated_Patterns_in_2D_Urban_Images.html">55 iccv-2013-Automatic Kronecker Product Model Based Detection of Repeated Patterns in 2D Urban Images</a></p>
<p>15 0.43958825 <a title="25-lsi-15" href="./iccv-2013-Quadruplet-Wise_Image_Similarity_Learning.html">332 iccv-2013-Quadruplet-Wise Image Similarity Learning</a></p>
<p>16 0.43207929 <a title="25-lsi-16" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<p>17 0.40553844 <a title="25-lsi-17" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>18 0.40295061 <a title="25-lsi-18" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>19 0.38844907 <a title="25-lsi-19" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>20 0.38523197 <a title="25-lsi-20" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.056), (7, 0.036), (13, 0.013), (26, 0.058), (31, 0.045), (41, 0.044), (42, 0.082), (48, 0.012), (64, 0.028), (73, 0.025), (89, 0.106), (95, 0.364), (97, 0.012), (98, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89828414 <a title="25-lda-1" href="./iccv-2013-Corrected-Moment_Illuminant_Estimation.html">92 iccv-2013-Corrected-Moment Illuminant Estimation</a></p>
<p>Author: Graham D. Finlayson</p><p>Abstract: unkown-abstract</p><p>same-paper 2 0.7242679 <a title="25-lda-2" href="./iccv-2013-A_Novel_Earth_Mover%27s_Distance_Methodology_for_Image_Matching_with_Gaussian_Mixture_Models.html">25 iccv-2013-A Novel Earth Mover's Distance Methodology for Image Matching with Gaussian Mixture Models</a></p>
<p>Author: Peihua Li, Qilong Wang, Lei Zhang</p><p>Abstract: The similarity or distance measure between Gaussian mixture models (GMMs) plays a crucial role in contentbased image matching. Though the Earth Mover’s Distance (EMD) has shown its advantages in matching histogram features, its potentials in matching GMMs remain unclear and are not fully explored. To address this problem, we propose a novel EMD methodology for GMM matching. We ?rst present a sparse representation based EMD called SR-EMD by exploiting the sparse property of the underlying problem. SR-EMD is more ef?cient and robust than the conventional EMD. Second, we present two novel ground distances between component Gaussians based on the information geometry. The perspective from the Riemannian geometry distinguishes the proposed ground distances from the classical entropy- or divergence-based ones. Furthermore, motivated by the success of distance metric learning of vector data, we make the ?rst attempt to learn the EMD distance metrics between GMMs by using a simple yet effective supervised pair-wise based method. It can adapt the distance metrics between GMMs to speci?c classi?ca- tion tasks. The proposed method is evaluated on both simulated data and benchmark real databases and achieves very promising performance.</p><p>3 0.64903021 <a title="25-lda-3" href="./iccv-2013-Learning_Graph_Matching%3A_Oriented_to_Category_Modeling_from_Cluttered_Scenes.html">237 iccv-2013-Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes</a></p>
<p>Author: Quanshi Zhang, Xuan Song, Xiaowei Shao, Huijing Zhao, Ryosuke Shibasaki</p><p>Abstract: Although graph matching is a fundamental problem in pattern recognition, and has drawn broad interest from many fields, the problem of learning graph matching has not received much attention. In this paper, we redefine the learning ofgraph matching as a model learningproblem. In addition to conventional training of matching parameters, our approach modifies the graph structure and attributes to generate a graphical model. In this way, the model learning is oriented toward both matching and recognition performance, and can proceed in an unsupervised1 fashion. Experiments demonstrate that our approach outperforms conventional methods for learning graph matching.</p><p>4 0.58861381 <a title="25-lda-4" href="./iccv-2013-A_Generic_Deformation_Model_for_Dense_Non-rigid_Surface_Registration%3A_A_Higher-Order_MRF-Based_Approach.html">16 iccv-2013-A Generic Deformation Model for Dense Non-rigid Surface Registration: A Higher-Order MRF-Based Approach</a></p>
<p>Author: Yun Zeng, Chaohui Wang, Xianfeng Gu, Dimitris Samaras, Nikos Paragios</p><p>Abstract: We propose a novel approach for dense non-rigid 3D surface registration, which brings together Riemannian geometry and graphical models. To this end, we first introduce a generic deformation model, called Canonical Distortion Coefficients (CDCs), by characterizing the deformation of every point on a surface using the distortions along its two principle directions. This model subsumes the deformation groups commonly used in surface registration such as isometry and conformality, and is able to handle more complex deformations. We also derive its discrete counterpart which can be computed very efficiently in a closed form. Based on these, we introduce a higher-order Markov Random Field (MRF) model which seamlessly integrates our deformation model and a geometry/texture similarity metric. Then we jointly establish the optimal correspondences for all the points via maximum a posteriori (MAP) inference. Moreover, we develop a parallel optimization algorithm to efficiently perform the inference for the proposed higher-order MRF model. The resulting registration algorithm outperforms state-of-the-art methods in both dense non-rigid 3D surface registration and tracking.</p><p>5 0.57417959 <a title="25-lda-5" href="./iccv-2013-Measuring_Flow_Complexity_in_Videos.html">263 iccv-2013-Measuring Flow Complexity in Videos</a></p>
<p>Author: Saad Ali</p><p>Abstract: In this paper a notion of flow complexity that measures the amount of interaction among objects is introduced and an approach to compute it directly from a video sequence is proposed. The approach employs particle trajectories as the input representation of motion and maps it into a ‘braid’ based representation. The mapping is based on the observation that 2D trajectories of particles take the form of a braid in space-time due to the intermingling among particles over time. As a result of this mapping, the problem of estimating the flow complexity from particle trajectories becomes the problem of estimating braid complexity, which in turn can be computed by measuring the topological entropy of a braid. For this purpose recently developed mathematical tools from braid theory are employed which allow rapid computation of topological entropy of braids. The approach is evaluated on a dataset consisting of open source videos depicting variations in terms of types of moving objects, scene layout, camera view angle, motion patterns, and object densities. The results show that the proposed approach is able to quantify the complexity of the flow, and at the same time provides useful insights about the sources of the complexity.</p><p>6 0.57167953 <a title="25-lda-6" href="./iccv-2013-GOSUS%3A_Grassmannian_Online_Subspace_Updates_with_Structured-Sparsity.html">182 iccv-2013-GOSUS: Grassmannian Online Subspace Updates with Structured-Sparsity</a></p>
<p>7 0.52546084 <a title="25-lda-7" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>8 0.48941016 <a title="25-lda-8" href="./iccv-2013-Log-Euclidean_Kernels_for_Sparse_Representation_and_Dictionary_Learning.html">257 iccv-2013-Log-Euclidean Kernels for Sparse Representation and Dictionary Learning</a></p>
<p>9 0.46079701 <a title="25-lda-9" href="./iccv-2013-Dictionary_Learning_and_Sparse_Coding_on_Grassmann_Manifolds%3A_An_Extrinsic_Solution.html">114 iccv-2013-Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution</a></p>
<p>10 0.45087421 <a title="25-lda-10" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<p>11 0.44503587 <a title="25-lda-11" href="./iccv-2013-Joint_Optimization_for_Consistent_Multiple_Graph_Matching.html">224 iccv-2013-Joint Optimization for Consistent Multiple Graph Matching</a></p>
<p>12 0.43997833 <a title="25-lda-12" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>13 0.43674168 <a title="25-lda-13" href="./iccv-2013-Dynamic_Structured_Model_Selection.html">130 iccv-2013-Dynamic Structured Model Selection</a></p>
<p>14 0.42710212 <a title="25-lda-14" href="./iccv-2013-Curvature-Aware_Regularization_on_Riemannian_Submanifolds.html">100 iccv-2013-Curvature-Aware Regularization on Riemannian Submanifolds</a></p>
<p>15 0.42628708 <a title="25-lda-15" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>16 0.42327565 <a title="25-lda-16" href="./iccv-2013-Flattening_Supervoxel_Hierarchies_by_the_Uniform_Entropy_Slice.html">172 iccv-2013-Flattening Supervoxel Hierarchies by the Uniform Entropy Slice</a></p>
<p>17 0.4224509 <a title="25-lda-17" href="./iccv-2013-Modifying_the_Memorability_of_Face_Photographs.html">272 iccv-2013-Modifying the Memorability of Face Photographs</a></p>
<p>18 0.42206919 <a title="25-lda-18" href="./iccv-2013-Geometric_Registration_Based_on_Distortion_Estimation.html">183 iccv-2013-Geometric Registration Based on Distortion Estimation</a></p>
<p>19 0.42082894 <a title="25-lda-19" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>20 0.41908425 <a title="25-lda-20" href="./iccv-2013-Enhanced_Continuous_Tabu_Search_for_Parameter_Estimation_in_Multiview_Geometry.html">141 iccv-2013-Enhanced Continuous Tabu Search for Parameter Estimation in Multiview Geometry</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
