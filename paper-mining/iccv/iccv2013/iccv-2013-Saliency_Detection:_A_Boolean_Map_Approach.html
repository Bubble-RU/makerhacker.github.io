<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>369 iccv-2013-Saliency Detection: A Boolean Map Approach</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-369" href="#">iccv2013-369</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>369 iccv-2013-Saliency Detection: A Boolean Map Approach</h1>
<br/><p>Source: <a title="iccv-2013-369-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Zhang_Saliency_Detection_A_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Jianming Zhang, Stan Sclaroff</p><p>Abstract: A novel Boolean Map based Saliency (BMS) model is proposed. An image is characterized by a set of binary images, which are generated by randomly thresholding the image ’s color channels. Based on a Gestalt principle of figure-ground segregation, BMS computes saliency maps by analyzing the topological structure of Boolean maps. BMS is simple to implement and efficient to run. Despite its simplicity, BMS consistently achieves state-of-the-art performance compared with ten leading methods on five eye tracking datasets. Furthermore, BMS is also shown to be advantageous in salient object detection.</p><p>Reference: <a title="iccv-2013-369-reference" href="../iccv2013_reference/iccv-2013-Saliency_Detection%3A_A_Boolean_Map_Approach_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Based on a Gestalt principle of figure-ground segregation, BMS computes saliency maps by analyzing the topological structure of Boolean maps. [sent-4, score-0.468]
</p><p>2 Despite its simplicity, BMS consistently achieves state-of-the-art performance compared with ten leading methods on five eye tracking datasets. [sent-6, score-0.181]
</p><p>3 Furthermore, BMS is also shown to be advantageous in salient object detection. [sent-7, score-0.142]
</p><p>4 Introduction In this paper, we focus on the bottom-up saliency detection problem. [sent-9, score-0.351]
</p><p>5 The main goal is to compute a saliency map that topographically represents the level of saliency for visual attention. [sent-10, score-0.738]
</p><p>6 Computing such saliency maps has recently raised a great amount of research interest (see [4] for a review) and has been shown to be beneficial in many applications, e. [sent-11, score-0.416]
</p><p>7 Many previous works have exploited the contrast and the rarity properties of local image patches for saliency detection [19, 6, 3]. [sent-14, score-0.405]
</p><p>8 As Gestalt psychological studies suggest, figures are more likely to be attended to than background elements [3 1, 29] and the figure-ground assignment can occur without focal attention [22]. [sent-17, score-0.202]
</p><p>9 1 shows an example that global cues for figureground segregation can help in saliency detection. [sent-20, score-0.428]
</p><p>10 A natural image along with eye tracking data is displayed in Fig. [sent-21, score-0.14]
</p><p>11 AIM and LG measure an image patch’s saliency based on its rarity. [sent-24, score-0.331]
</p><p>12 The eye fixations are concentrated on the bird, corresponding well to this figureground assignment. [sent-27, score-0.187]
</p><p>13 However, without the awareness of this global structure, rarity based models [6, 3] falsely assign high saliency values to the edge area between the trees and the sky in the background, because of the rarity of high contrast regions in this image (Fig. [sent-28, score-0.489]
</p><p>14 In this paper, we explore the surroundedness cue for saliency detection. [sent-34, score-0.454]
</p><p>15 The essence of surroundedness is the  enclosure topological relationship between the figure and the ground, which is well defined and invariant to various transformations. [sent-35, score-0.155]
</p><p>16 Then saliency is modeled as the expected attention level given the set of randomly sampled Boolean maps. [sent-39, score-0.491]
</p><p>17 the mean attention map, is a full-resolution preliminary saliency map that can be further processed for a specific task such as eye fixation prediction or salient object detection [5]. [sent-42, score-0.972]
</p><p>18 2 shows two types of saliency maps of BMS for eye fixation prediction and salient object detection. [sent-44, score-0.819]
</p><p>19 We evaluate BMS against ten state-of-the-art saliency models on five benchmark eye tracking datasets. [sent-45, score-0.512]
</p><p>20 We also show with both qualitative and quantitative results that the outputs of BMS are useful in salient object detection. [sent-48, score-0.142]
</p><p>21 Related Works A majority of the previous saliency models use centersurround filters or image statistics to identify salient patches that are complex (local complexity/contrast) or rare in their appearance (rarity/improbability). [sent-50, score-0.473]
</p><p>22 The negative logarithm of the probability, known as Shannon’s selfinformation, is used to measure the improbability of a local patch as a bottom-up saliency cue in [6] and [39]. [sent-52, score-0.381]
</p><p>23 Recently, [10] uses a hierarchically whitened feature space, where the square of the vector norms serves as a saliency metric to measure how far a pixel feature vector deviates from the center of the data. [sent-54, score-0.331]
</p><p>24 Unlike models based on properties like contrast, rarity and symmetry, another family of saliency models are based on spectral domain analysis [15, 14, 33, 27]. [sent-56, score-0.408]
</p><p>25 However, [27]  shows that some previous spectral analysis based methods are in some sense equivalent to a local gradient operator plus Gaussian blurring on natural images, and thus cannot detect large salient regions very well. [sent-57, score-0.216]
</p><p>26 [21] learn a kernel support vector machine (SVM) for image patches based on eye tracking data. [sent-61, score-0.164]
</p><p>27 [20] train a SVM using a combination of low, middle and high level features, and the saliency classification is done in a pixel-by-pixel manner. [sent-63, score-0.331]
</p><p>28 Instead, it makes use of topological structural information, which is scale-invariant and known to have a strong influence on visual attention [37, 8]. [sent-65, score-0.257]
</p><p>29 Only a few attempts have been made to leverage the topological structure of a scene for saliency detection. [sent-67, score-0.383]
</p><p>30 In [36], a local patch’s saliency is measured on a graphical model, by its shortest distance to the image borders. [sent-69, score-0.331]
</p><p>31 The salient region detection method of [16] also employs a feature channel thresholding step. [sent-72, score-0.235]
</p><p>32 In contrast, BMS computes saliency entirely based on the set of randomly thresholded Boolean maps. [sent-74, score-0.331]
</p><p>33 Boolean Map based Saliency To derive a bottom-up saliency model, we borrow the Boolean Map concept that was put forward in the Boolean Map Theory of visual attention [17], where an observer’s momentary conscious awareness of a scene can be represented by a Boolean Map. [sent-76, score-0.53]
</p><p>34 We assume that Boolean maps in BMS are generated from randomly selected feature channels, and the influence of a Boolean map B on visual attention can be represented by an Attention Map A(B), which highlights regions on B that attract visual attention. [sent-77, score-0.395]
</p><p>35 Then the saliency is modeled by the mean attention map A¯ over randomly generated Boolean maps:  A¯ =? [sent-78, score-0.549]
</p><p>36 A¯ can be further post-processed is to form a final saliency map S for some specific task. [sent-80, score-0.389]
</p><p>37 Beta soefd B on a Gn mestaaplts principle of figure-ground segregation, an attention map Ai is computed for each Boolean map Bi. [sent-88, score-0.276]
</p><p>38 Then a mean attention map A¯ is obtained through a linear combination of the resulting attention maps. [sent-89, score-0.378]
</p><p>39 Finally, some post-processing is applied on the mean attention map to output a saliency map S. [sent-90, score-0.607]
</p><p>40 Generation of Boolean Maps BMS generates a set of Boolean maps by randomly thresholding the input image’s feature maps, according to the prior distributions over the feature channels and the threshold: Bi = THRESH(φ(I), θ), φ ∼ pφ, θ ∼ pθ . [sent-94, score-0.145]
</p><p>41 T mhaeprpeifnogre, f given an image, the distribution of generated Boolean maps is solely determined by the choice of color space and the prior distribution for color channel selection. [sent-103, score-0.16]
</p><p>42 Boolean maps should be generated in such a way that more salient regions have higher chances to be separated from the surrounding background. [sent-104, score-0.256]
</p><p>43 An opening operation with kernel ωo is then applied to each Boolean map for noise removal. [sent-111, score-0.183]
</p><p>44 Attention Map Computation Given a Boolean map B, BMS computes an attention map A(B) based on a Gestalt principle for figure-ground segregation: surrounded regions are more likely to be perceived as figures [30]. [sent-114, score-0.358]
</p><p>45 To compute the attention map, BMS assigns 1 to the union of surrounded regions, and 0 to the rest of the map. [sent-117, score-0.188]
</p><p>46 The resultant attention maps need to be normalized before the linear combination step, so that attention maps with small concentrated active areas will receive more emphasis. [sent-119, score-0.527]
</p><p>47 For eye fixation prediction, BMS uses simple L2-normalization, i. [sent-121, score-0.237]
</p><p>48 dividing a vectorized map by its L2-norm, to emphasize  attention maps with small active areas. [sent-123, score-0.303]
</p><p>49 Compared with L1-normalization, L2-normalization is less sensitive to attention maps with extremely small active areas, which will otherwise dominate the fusion process. [sent-124, score-0.245]
</p><p>50 To further penalize attention maps with small scattered active areas, we dilate the attention map with kernel width ωd1 before normalization. [sent-125, score-0.521]
</p><p>51 All the attention maps are linearly combined into a fullresolution mean attention map The mean attention maps can be further processed for a specific task. [sent-126, score-0.708]
</p><p>52 Eye Fixation Prediction In this section, we evaluate the performance of BMS in eye fixation prediction. [sent-147, score-0.237]
</p><p>53 The sampling step size δ is set to 8 and the dilation kernel width ωd1 is fixed at 7. [sent-150, score-0.156]
</p><p>54 We post-process A¯ to produce the saliency map S by Gaussian blurring with standard deviation (STD) σ. [sent-151, score-0.426]
</p><p>55 However, strong Gaussian blur will remove small peaks on the mean attention map, which is sometimes undesirable. [sent-152, score-0.195]
</p><p>56 To control for this factor, we use a dilation operation with kernel width ωd2 before Gaussian blur. [sent-153, score-0.203]
</p><p>57 We do not find this dilation operation improves the performance of other compared methods. [sent-154, score-0.16]
</p><p>58 Experimental Setup We have quantitatively evaluated our algorithm in comparison with ten state-of-the-art saliency methods shown in Table 2. [sent-160, score-0.347]
</p><p>59 The methods are evaluated on five benchmark eye tracking data sets: MIT [20] (MIT data set), Toronto [6], Kootstra [24], Cerf [7] (FIFA data set) and ImgSal [27]. [sent-168, score-0.165]
</p><p>60 One of the most widely used metrics for saliency method evaluation is the ROC Area Under the Curve (AUC) metric. [sent-172, score-0.331]
</p><p>61 0, while any static saliency map will give a score of approximately 0. [sent-178, score-0.389]
</p><p>62 Results AUC scores are sensitive to the level of blurring applied on the saliency maps. [sent-184, score-0.387]
</p><p>63 As in [14, 3], we smooth the saliency maps of each method by varying the Gaussian blur standard deviation (STD), and show in Fig. [sent-185, score-0.466]
</p><p>64 The  input images are roughly arranged in ascending order of the size of their salient regions. [sent-231, score-0.176]
</p><p>65 Moreover, they tend to favor the boundaries rather than the interior regions of large salient objects, like the car and the STOP sign in the last two examples, even with the help of multi-scale processing [33, 3, 10, 11, 13, 19]. [sent-233, score-0.171]
</p><p>66 Five parameters are involved in the implementation of BMS: sample step δ, kernel widths of opening operation ωo, kernel widths of two dilation operations ωd1 and ωd2 , and the Gaussian blur STD σ. [sent-242, score-0.324]
</p><p>67 Overall, BMS is not very sensitive to these parameters except the dilation kernel width ωd2 in the post-precessing step. [sent-247, score-0.156]
</p><p>68 Having a  slight dilation before the final smoothing improves the AUC scores on all the datasets, while setting ωd2 to greater than 20 only improves the average AUC scores on the Toronto and Kootstra dataset. [sent-249, score-0.211]
</p><p>69 Applying an opening operation over Boolean maps does not significantly change the average AUC scores on most of the datasets, but the score on the ImgSal dataset improves by more than 0. [sent-252, score-0.235]
</p><p>70 The fixation heat maps are computed by applying Gaussian blur on the raw eye fixation maps. [sent-254, score-0.495]
</p><p>71 The rest columns show the saliency maps from BMS and the compared methods. [sent-255, score-0.416]
</p><p>72 Images are roughly arranged in  ascending order of the size of their salient regions. [sent-256, score-0.176]
</p><p>73 Applying a dilation operation over the attention maps improves the AUC scores on average, but the improvement drops when ωd1 is greater than 7. [sent-259, score-0.454]
</p><p>74 Salient Object Detection  In this section, we show that BMS is also useful in salient object detection. [sent-276, score-0.142]
</p><p>75 Salient object detection aims at segmenting salient objects from the background. [sent-277, score-0.162]
</p><p>76 Models for salient object detection have different emphasis compared with models for eye fixation prediction. [sent-278, score-0.399]
</p><p>77 Because eye fixations are sparsely distributed and possess some level of uncertainty, the corresponding saliency maps are usually highly blurred and very selective. [sent-279, score-0.561]
</p><p>78 However, salient object detection requires object level segmentation, which means the corresponding saliency map should be highresolution with uniformly highlighted salient regions and 3Note that some compared methods implicitly down-sampled input images before processing. [sent-280, score-0.722]
</p><p>79 We also turn off the dilation operation in the attention maps computation (i. [sent-284, score-0.39]
</p><p>80 ωd1 = 1) to enhance the accuracy of attention maps. [sent-286, score-0.16]
</p><p>81 Then we post-process the mean attention maps ofBMS using an opening-by-reconstruction operation followed by a closing-by-reconstruction operation [35] with kernel radius 15, in order to smooth the saliency maps but keep the boundary details. [sent-289, score-0.779]
</p><p>82 BMS is compared with six state-of-the-art salient object detection methods (HSal [38], GSSP, GSGD [36], RC, HC [9] and FT [1]), as well as some leading models for eye fixation prediction. [sent-291, score-0.399]
</p><p>83 Similar to previous works [1, 36], we binarize the saliency maps at a fixed threshold and compute the average precision and recall (PR) for each method. [sent-292, score-0.445]
</p><p>84 Leading models for eye fixation prediction perform significantly worse than salient object detection methods. [sent-309, score-0.439]
</p><p>85 The ImgSal dataset [27] used in the previous section also has ground-truth salient regions labeled by 19 subjects. [sent-312, score-0.171]
</p><p>86 The labeled salient regions of this dataset are not very precise, and thus unsuitable for quantitative evaluation using the PR metric. [sent-315, score-0.171]
</p><p>87 The model borrows the concept of Boolean map from the Boolean Map Theory of visual attention [17], and characterizes an image by a set of Boolean maps. [sent-318, score-0.251]
</p><p>88 This representation leads to an efficient algorithm for saliency detection. [sent-319, score-0.331]
</p><p>89 BMS is the only model that consistently achieves state-of-the-art performance on five benchmark eye tracking datasets, and it is also shown to be useful in salient object detection. [sent-320, score-0.307]
</p><p>90 Another interesting direction for future work is to improve the attention map computation by incorporating more saliency cues like convexity, symmetry and familiarity. [sent-323, score-0.577]
</p><p>91 This may help to redeem the limitation that salient regions that touch the image borders cannot be well detected using the surroundedness cue alone. [sent-324, score-0.312]
</p><p>92 Exploiting local and global patch rarities for saliency detection. [sent-344, score-0.331]
</p><p>93 Predicting human gaze using low-level saliency combined with face detection. [sent-368, score-0.331]
</p><p>94 Wecan otshow  saliency maps of GSSP [36] because its code is not publicly  available. [sent-383, score-0.416]
</p><p>95 Unsupervised extraction of visual attention objects in color images. [sent-402, score-0.199]
</p><p>96 Salient region detection using weighted feature maps based on the human visual attention model. [sent-430, score-0.301]
</p><p>97 A model of saliencybased visual attention for rapid scene analysis. [sent-446, score-0.178]
</p><p>98 Visual saliency based on scale-space analysis in the frequency domain. [sent-497, score-0.331]
</p><p>99 What attributes guide the deployment ofvisual attention and how do they do it? [sent-551, score-0.179]
</p><p>100 Sun: A bayesian framework for saliency using natural statistics. [sent-566, score-0.331]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bms', 0.724), ('boolean', 0.341), ('saliency', 0.331), ('attention', 0.16), ('salient', 0.142), ('fixation', 0.12), ('auc', 0.118), ('eye', 0.117), ('surroundedness', 0.103), ('dilation', 0.098), ('kootstra', 0.092), ('imgsal', 0.092), ('maps', 0.085), ('segregation', 0.074), ('gssp', 0.059), ('map', 0.058), ('std', 0.054), ('rarity', 0.054), ('asd', 0.054), ('opening', 0.054), ('gestalt', 0.052), ('topological', 0.052), ('toronto', 0.049), ('cerf', 0.048), ('operation', 0.047), ('judd', 0.045), ('hsal', 0.044), ('thresh', 0.044), ('psychological', 0.042), ('ak', 0.04), ('qdct', 0.039), ('channels', 0.038), ('gbvs', 0.036), ('blur', 0.035), ('width', 0.034), ('aws', 0.034), ('scores', 0.034), ('itti', 0.033), ('channel', 0.033), ('mit', 0.031), ('gsgd', 0.03), ('improbability', 0.03), ('shuffling', 0.03), ('sigsal', 0.03), ('harel', 0.029), ('lg', 0.029), ('regions', 0.029), ('symmetry', 0.028), ('lab', 0.028), ('borji', 0.028), ('fixations', 0.028), ('surrounded', 0.028), ('influence', 0.027), ('neuroscience', 0.026), ('perceived', 0.025), ('five', 0.025), ('prediction', 0.024), ('kernel', 0.024), ('spectral', 0.023), ('tracking', 0.023), ('figureground', 0.023), ('koch', 0.023), ('blurring', 0.022), ('thresholding', 0.022), ('widths', 0.021), ('awareness', 0.021), ('color', 0.021), ('pr', 0.02), ('ivc', 0.02), ('detection', 0.02), ('cue', 0.02), ('perceptual', 0.02), ('gaussian', 0.02), ('deployment', 0.019), ('concentrated', 0.019), ('runtime', 0.019), ('areas', 0.018), ('heat', 0.018), ('ascending', 0.018), ('region', 0.018), ('visual', 0.018), ('borders', 0.018), ('hc', 0.018), ('surprise', 0.018), ('perception', 0.017), ('critically', 0.017), ('arranged', 0.016), ('ten', 0.016), ('worse', 0.016), ('datasets', 0.016), ('highlighting', 0.016), ('inverted', 0.015), ('characterizes', 0.015), ('hou', 0.015), ('improves', 0.015), ('greater', 0.015), ('bk', 0.015), ('deviation', 0.015), ('recall', 0.015), ('threshold', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="369-tfidf-1" href="./iccv-2013-Saliency_Detection%3A_A_Boolean_Map_Approach.html">369 iccv-2013-Saliency Detection: A Boolean Map Approach</a></p>
<p>Author: Jianming Zhang, Stan Sclaroff</p><p>Abstract: A novel Boolean Map based Saliency (BMS) model is proposed. An image is characterized by a set of binary images, which are generated by randomly thresholding the image ’s color channels. Based on a Gestalt principle of figure-ground segregation, BMS computes saliency maps by analyzing the topological structure of Boolean maps. BMS is simple to implement and efficient to run. Despite its simplicity, BMS consistently achieves state-of-the-art performance compared with ten leading methods on five eye tracking datasets. Furthermore, BMS is also shown to be advantageous in salient object detection.</p><p>2 0.34594271 <a title="369-tfidf-2" href="./iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction.html">50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</a></p>
<p>Author: Ali Borji, Hamed R. Tavakoli, Dicky N. Sihite, Laurent Itti</p><p>Abstract: Significant recent progress has been made in developing high-quality saliency models. However, less effort has been undertaken on fair assessment of these models, over large standardized datasets and correctly addressing confounding factors. In this study, we pursue a critical and quantitative look at challenges (e.g., center-bias, map smoothing) in saliency modeling and the way they affect model accuracy. We quantitatively compare 32 state-of-the-art models (using the shuffled AUC score to discount center-bias) on 4 benchmark eye movement datasets, for prediction of human fixation locations and scanpath sequence. We also account for the role of map smoothing. We find that, although model rankings vary, some (e.g., AWS, LG, AIM, and HouNIPS) consistently outperform other models over all datasets. Some models work well for prediction of both fixation locations and scanpath sequence (e.g., Judd, GBVS). Our results show low prediction accuracy for models over emotional stimuli from the NUSEF dataset. Our last benchmark, for the first time, gauges the ability of models to decode the stimulus category from statistics of fixations, saccades, and model saliency values at fixated locations. In this test, ITTI and AIM models win over other models. Our benchmark provides a comprehensive high-level picture of the strengths and weaknesses of many popular models, and suggests future research directions in saliency modeling.</p><p>3 0.31693408 <a title="369-tfidf-3" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>Author: Yangqing Jia, Mei Han</p><p>Abstract: It is known that purely low-level saliency cues such as frequency does not lead to a good salient object detection result, requiring high-level knowledge to be adopted for successful discovery of task-independent salient objects. In this paper, we propose an efficient way to combine such high-level saliency priors and low-level appearance models. We obtain the high-level saliency prior with the objectness algorithm to find potential object candidates without the need of category information, and then enforce the consistency among the salient regions using a Gaussian MRF with the weights scaled by diverse density that emphasizes the influence of potential foreground pixels. Our model obtains saliency maps that assign high scores for the whole salient object, and achieves state-of-the-art performance on benchmark datasets covering various foreground statistics.</p><p>4 0.30644807 <a title="369-tfidf-4" href="./iccv-2013-Saliency_Detection_via_Dense_and_Sparse_Reconstruction.html">372 iccv-2013-Saliency Detection via Dense and Sparse Reconstruction</a></p>
<p>Author: Xiaohui Li, Huchuan Lu, Lihe Zhang, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: In this paper, we propose a visual saliency detection algorithm from the perspective of reconstruction errors. The image boundaries are first extracted via superpixels as likely cues for background templates, from which dense and sparse appearance models are constructed. For each image region, we first compute dense and sparse reconstruction errors. Second, the reconstruction errors are propagated based on the contexts obtained from K-means clustering. Third, pixel-level saliency is computed by an integration of multi-scale reconstruction errors and refined by an object-biased Gaussian model. We apply the Bayes formula to integrate saliency measures based on dense and sparse reconstruction errors. Experimental results show that the proposed algorithm performs favorably against seventeen state-of-the-art methods in terms of precision and recall. In addition, the proposed algorithm is demonstrated to be more effective in highlighting salient objects uniformly and robust to background noise.</p><p>5 0.29498473 <a title="369-tfidf-5" href="./iccv-2013-Saliency_and_Human_Fixations%3A_State-of-the-Art_and_Study_of_Comparison_Metrics.html">373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</a></p>
<p>Author: Nicolas Riche, Matthieu Duvinage, Matei Mancas, Bernard Gosselin, Thierry Dutoit</p><p>Abstract: Visual saliency has been an increasingly active research area in the last ten years with dozens of saliency models recently published. Nowadays, one of the big challenges in the field is to find a way to fairly evaluate all of these models. In this paper, on human eye fixations ,we compare the ranking of 12 state-of-the art saliency models using 12 similarity metrics. The comparison is done on Jian Li ’s database containing several hundreds of natural images. Based on Kendall concordance coefficient, it is shown that some of the metrics are strongly correlated leading to a redundancy in the performance metrics reported in the available benchmarks. On the other hand, other metrics provide a more diverse picture of models ’ overall performance. As a recommendation, three similarity metrics should be used to obtain a complete point of view of saliency model performance.</p><p>6 0.26728457 <a title="369-tfidf-6" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>7 0.19890678 <a title="369-tfidf-7" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>8 0.19361787 <a title="369-tfidf-8" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>9 0.19274282 <a title="369-tfidf-9" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>10 0.18096289 <a title="369-tfidf-10" href="./iccv-2013-Saliency_Detection_via_Absorbing_Markov_Chain.html">371 iccv-2013-Saliency Detection via Absorbing Markov Chain</a></p>
<p>11 0.17489593 <a title="369-tfidf-11" href="./iccv-2013-Saliency_Detection_in_Large_Point_Sets.html">370 iccv-2013-Saliency Detection in Large Point Sets</a></p>
<p>12 0.15578747 <a title="369-tfidf-12" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>13 0.14498135 <a title="369-tfidf-13" href="./iccv-2013-Semantically-Based_Human_Scanpath_Estimation_with_HMMs.html">381 iccv-2013-Semantically-Based Human Scanpath Estimation with HMMs</a></p>
<p>14 0.11916434 <a title="369-tfidf-14" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>15 0.071911938 <a title="369-tfidf-15" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>16 0.064140245 <a title="369-tfidf-16" href="./iccv-2013-Predicting_Primary_Gaze_Behavior_Using_Social_Saliency_Fields.html">325 iccv-2013-Predicting Primary Gaze Behavior Using Social Saliency Fields</a></p>
<p>17 0.06319458 <a title="369-tfidf-17" href="./iccv-2013-Learning_to_Predict_Gaze_in_Egocentric_Video.html">247 iccv-2013-Learning to Predict Gaze in Egocentric Video</a></p>
<p>18 0.062097061 <a title="369-tfidf-18" href="./iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F.html">316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</a></p>
<p>19 0.058605496 <a title="369-tfidf-19" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>20 0.055468041 <a title="369-tfidf-20" href="./iccv-2013-Calibration-Free_Gaze_Estimation_Using_Human_Gaze_Patterns.html">67 iccv-2013-Calibration-Free Gaze Estimation Using Human Gaze Patterns</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.119), (1, -0.047), (2, 0.378), (3, -0.202), (4, -0.11), (5, -0.006), (6, 0.052), (7, -0.042), (8, 0.01), (9, 0.017), (10, -0.007), (11, -0.035), (12, 0.011), (13, -0.027), (14, 0.009), (15, -0.002), (16, 0.031), (17, -0.014), (18, -0.007), (19, 0.06), (20, 0.003), (21, 0.023), (22, 0.027), (23, -0.002), (24, 0.029), (25, -0.018), (26, 0.03), (27, -0.017), (28, 0.014), (29, 0.0), (30, -0.029), (31, -0.001), (32, -0.016), (33, -0.022), (34, 0.024), (35, -0.011), (36, -0.031), (37, 0.015), (38, -0.002), (39, -0.022), (40, 0.01), (41, -0.006), (42, 0.018), (43, -0.024), (44, -0.008), (45, -0.002), (46, -0.0), (47, 0.005), (48, -0.07), (49, -0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95535016 <a title="369-lsi-1" href="./iccv-2013-Saliency_Detection%3A_A_Boolean_Map_Approach.html">369 iccv-2013-Saliency Detection: A Boolean Map Approach</a></p>
<p>Author: Jianming Zhang, Stan Sclaroff</p><p>Abstract: A novel Boolean Map based Saliency (BMS) model is proposed. An image is characterized by a set of binary images, which are generated by randomly thresholding the image ’s color channels. Based on a Gestalt principle of figure-ground segregation, BMS computes saliency maps by analyzing the topological structure of Boolean maps. BMS is simple to implement and efficient to run. Despite its simplicity, BMS consistently achieves state-of-the-art performance compared with ten leading methods on five eye tracking datasets. Furthermore, BMS is also shown to be advantageous in salient object detection.</p><p>2 0.93972582 <a title="369-lsi-2" href="./iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction.html">50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</a></p>
<p>Author: Ali Borji, Hamed R. Tavakoli, Dicky N. Sihite, Laurent Itti</p><p>Abstract: Significant recent progress has been made in developing high-quality saliency models. However, less effort has been undertaken on fair assessment of these models, over large standardized datasets and correctly addressing confounding factors. In this study, we pursue a critical and quantitative look at challenges (e.g., center-bias, map smoothing) in saliency modeling and the way they affect model accuracy. We quantitatively compare 32 state-of-the-art models (using the shuffled AUC score to discount center-bias) on 4 benchmark eye movement datasets, for prediction of human fixation locations and scanpath sequence. We also account for the role of map smoothing. We find that, although model rankings vary, some (e.g., AWS, LG, AIM, and HouNIPS) consistently outperform other models over all datasets. Some models work well for prediction of both fixation locations and scanpath sequence (e.g., Judd, GBVS). Our results show low prediction accuracy for models over emotional stimuli from the NUSEF dataset. Our last benchmark, for the first time, gauges the ability of models to decode the stimulus category from statistics of fixations, saccades, and model saliency values at fixated locations. In this test, ITTI and AIM models win over other models. Our benchmark provides a comprehensive high-level picture of the strengths and weaknesses of many popular models, and suggests future research directions in saliency modeling.</p><p>3 0.93097389 <a title="369-lsi-3" href="./iccv-2013-Saliency_and_Human_Fixations%3A_State-of-the-Art_and_Study_of_Comparison_Metrics.html">373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</a></p>
<p>Author: Nicolas Riche, Matthieu Duvinage, Matei Mancas, Bernard Gosselin, Thierry Dutoit</p><p>Abstract: Visual saliency has been an increasingly active research area in the last ten years with dozens of saliency models recently published. Nowadays, one of the big challenges in the field is to find a way to fairly evaluate all of these models. In this paper, on human eye fixations ,we compare the ranking of 12 state-of-the art saliency models using 12 similarity metrics. The comparison is done on Jian Li ’s database containing several hundreds of natural images. Based on Kendall concordance coefficient, it is shown that some of the metrics are strongly correlated leading to a redundancy in the performance metrics reported in the available benchmarks. On the other hand, other metrics provide a more diverse picture of models ’ overall performance. As a recommendation, three similarity metrics should be used to obtain a complete point of view of saliency model performance.</p><p>4 0.92525983 <a title="369-lsi-4" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>Author: Xi Li, Yao Li, Chunhua Shen, Anthony Dick, Anton Van_Den_Hengel</p><p>Abstract: Salient object detection aims to locate objects that capture human attention within images. Previous approaches often pose this as a problem of image contrast analysis. In this work, we model an image as a hypergraph that utilizes a set of hyperedges to capture the contextual properties of image pixels or regions. As a result, the problem of salient object detection becomes one of finding salient vertices and hyperedges in the hypergraph. The main advantage of hypergraph modeling is that it takes into account each pixel’s (or region ’s) affinity with its neighborhood as well as its separation from image background. Furthermore, we propose an alternative approach based on centerversus-surround contextual contrast analysis, which performs salient object detection by optimizing a cost-sensitive support vector machine (SVM) objective function. Experimental results on four challenging datasets demonstrate the effectiveness of the proposed approaches against the stateof-the-art approaches to salient object detection.</p><p>5 0.89997512 <a title="369-lsi-5" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>Author: Peng Jiang, Haibin Ling, Jingyi Yu, Jingliang Peng</p><p>Abstract: The goal of saliency detection is to locate important pixels or regions in an image which attract humans ’ visual attention the most. This is a fundamental task whose output may serve as the basis for further computer vision tasks like segmentation, resizing, tracking and so forth. In this paper we propose a novel salient region detection algorithm by integrating three important visual cues namely uniqueness, focusness and objectness (UFO). In particular, uniqueness captures the appearance-derived visual contrast; focusness reflects the fact that salient regions are often photographed in focus; and objectness helps keep completeness of detected salient regions. While uniqueness has been used for saliency detection for long, it is new to integrate focusness and objectness for this purpose. In fact, focusness and objectness both provide important saliency information complementary of uniqueness. In our experiments using public benchmark datasets, we show that, even with a simple pixel level combination of the three components, the proposed approach yields significant improve- ment compared with previously reported methods.</p><p>6 0.88336635 <a title="369-lsi-6" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>7 0.86896902 <a title="369-lsi-7" href="./iccv-2013-Saliency_Detection_via_Dense_and_Sparse_Reconstruction.html">372 iccv-2013-Saliency Detection via Dense and Sparse Reconstruction</a></p>
<p>8 0.83321297 <a title="369-lsi-8" href="./iccv-2013-Saliency_Detection_via_Absorbing_Markov_Chain.html">371 iccv-2013-Saliency Detection via Absorbing Markov Chain</a></p>
<p>9 0.83182496 <a title="369-lsi-9" href="./iccv-2013-Saliency_Detection_in_Large_Point_Sets.html">370 iccv-2013-Saliency Detection in Large Point Sets</a></p>
<p>10 0.81763184 <a title="369-lsi-10" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>11 0.70987797 <a title="369-lsi-11" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>12 0.59807301 <a title="369-lsi-12" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>13 0.56164581 <a title="369-lsi-13" href="./iccv-2013-Semantically-Based_Human_Scanpath_Estimation_with_HMMs.html">381 iccv-2013-Semantically-Based Human Scanpath Estimation with HMMs</a></p>
<p>14 0.42434177 <a title="369-lsi-14" href="./iccv-2013-Predicting_Primary_Gaze_Behavior_Using_Social_Saliency_Fields.html">325 iccv-2013-Predicting Primary Gaze Behavior Using Social Saliency Fields</a></p>
<p>15 0.31394714 <a title="369-lsi-15" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>16 0.29869017 <a title="369-lsi-16" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>17 0.27506626 <a title="369-lsi-17" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>18 0.25502458 <a title="369-lsi-18" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>19 0.22317298 <a title="369-lsi-19" href="./iccv-2013-Detecting_Curved_Symmetric_Parts_Using_a_Deformable_Disc_Model.html">110 iccv-2013-Detecting Curved Symmetric Parts Using a Deformable Disc Model</a></p>
<p>20 0.21848823 <a title="369-lsi-20" href="./iccv-2013-Learning_to_Predict_Gaze_in_Egocentric_Video.html">247 iccv-2013-Learning to Predict Gaze in Egocentric Video</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.057), (4, 0.014), (7, 0.017), (12, 0.018), (21, 0.013), (26, 0.057), (31, 0.039), (34, 0.012), (42, 0.081), (48, 0.016), (63, 0.23), (64, 0.033), (73, 0.035), (89, 0.131), (97, 0.124)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73367596 <a title="369-lda-1" href="./iccv-2013-Saliency_Detection%3A_A_Boolean_Map_Approach.html">369 iccv-2013-Saliency Detection: A Boolean Map Approach</a></p>
<p>Author: Jianming Zhang, Stan Sclaroff</p><p>Abstract: A novel Boolean Map based Saliency (BMS) model is proposed. An image is characterized by a set of binary images, which are generated by randomly thresholding the image ’s color channels. Based on a Gestalt principle of figure-ground segregation, BMS computes saliency maps by analyzing the topological structure of Boolean maps. BMS is simple to implement and efficient to run. Despite its simplicity, BMS consistently achieves state-of-the-art performance compared with ten leading methods on five eye tracking datasets. Furthermore, BMS is also shown to be advantageous in salient object detection.</p><p>2 0.68068403 <a title="369-lda-2" href="./iccv-2013-Linear_Sequence_Discriminant_Analysis%3A_A_Model-Based_Dimensionality_Reduction_Method_for_Vector_Sequences.html">253 iccv-2013-Linear Sequence Discriminant Analysis: A Model-Based Dimensionality Reduction Method for Vector Sequences</a></p>
<p>Author: Bing Su, Xiaoqing Ding</p><p>Abstract: Dimensionality reduction for vectors in sequences is challenging since labels are attached to sequences as a whole. This paper presents a model-based dimensionality reduction method for vector sequences, namely linear sequence discriminant analysis (LSDA), which attempts to find a subspace in which sequences of the same class are projected together while those of different classes are projected as far as possible. For each sequence class, an HMM is built from states of which statistics are extracted. Means of these states are linked in order to form a mean sequence, and the variance of the sequence class is defined as the sum of all variances of component states. LSDA then learns a transformation by maximizing the separability between sequence classes and at the same time minimizing the within-sequence class scatter. DTW distance between mean sequences is used to measure the separability between sequence classes. We show that the optimization problem can be approximately transformed into an eigen decomposition problem. LDA can be seen as a special case of LSDA by considering non-sequential vectors as sequences of length one. The effectiveness of the proposed LSDA is demonstrated on two individual sequence datasets from UCI machine learning repository as well as two concatenate sequence datasets: APTI Arabic printed text database and IFN/ENIT Arabic handwriting database.</p><p>3 0.67611128 <a title="369-lda-3" href="./iccv-2013-Saliency_and_Human_Fixations%3A_State-of-the-Art_and_Study_of_Comparison_Metrics.html">373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</a></p>
<p>Author: Nicolas Riche, Matthieu Duvinage, Matei Mancas, Bernard Gosselin, Thierry Dutoit</p><p>Abstract: Visual saliency has been an increasingly active research area in the last ten years with dozens of saliency models recently published. Nowadays, one of the big challenges in the field is to find a way to fairly evaluate all of these models. In this paper, on human eye fixations ,we compare the ranking of 12 state-of-the art saliency models using 12 similarity metrics. The comparison is done on Jian Li ’s database containing several hundreds of natural images. Based on Kendall concordance coefficient, it is shown that some of the metrics are strongly correlated leading to a redundancy in the performance metrics reported in the available benchmarks. On the other hand, other metrics provide a more diverse picture of models ’ overall performance. As a recommendation, three similarity metrics should be used to obtain a complete point of view of saliency model performance.</p><p>4 0.65426022 <a title="369-lda-4" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>Author: Hesamoddin Salehian, Guang Cheng, Baba C. Vemuri, Jeffrey Ho</p><p>Abstract: Symmetric positive-definite (SPD) matrices are ubiquitous in Computer Vision, Machine Learning and Medical Image Analysis. Finding the center/average of a population of such matrices is a common theme in many algorithms such as clustering, segmentation, principal geodesic analysis, etc. The center of a population of such matrices can be defined using a variety of distance/divergence measures as the minimizer of the sum of squared distances/divergences from the unknown center to the members of the population. It is well known that the computation of the Karcher mean for the space of SPD matrices which is a negativelycurved Riemannian manifold is computationally expensive. Recently, the LogDet divergence-based center was shown to be a computationally attractive alternative. However, the LogDet-based mean of more than two matrices can not be computed in closed form, which makes it computationally less attractive for large populations. In this paper we present a novel recursive estimator for center based on the Stein distance which is the square root of the LogDet di– vergence that is significantly faster than the batch mode computation of this center. The key theoretical contribution is a closed-form solution for the weighted Stein center of two SPD matrices, which is used in the recursive computation of the Stein center for a population of SPD matrices. Additionally, we show experimental evidence of the convergence of our recursive Stein center estimator to the batch mode Stein center. We present applications of our recursive estimator to K-means clustering and image indexing depicting significant time gains over corresponding algorithms that use the batch mode computations. For the latter application, we develop novel hashing functions using the Stein distance and apply it to publicly available data sets, and experimental results have shown favorable com– ∗This research was funded in part by the NIH grant NS066340 to BCV. †Corresponding author parisons to other competing methods.</p><p>5 0.64050055 <a title="369-lda-5" href="./iccv-2013-Synergistic_Clustering_of_Image_and_Segment_Descriptors_for_Unsupervised_Scene_Understanding.html">412 iccv-2013-Synergistic Clustering of Image and Segment Descriptors for Unsupervised Scene Understanding</a></p>
<p>Author: Daniel M. Steinberg, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: With the advent of cheap, high fidelity, digital imaging systems, the quantity and rate of generation of visual data can dramatically outpace a humans ability to label or annotate it. In these situations there is scope for the use of unsupervised approaches that can model these datasets and automatically summarise their content. To this end, we present a totally unsupervised, and annotation-less, model for scene understanding. This model can simultaneously cluster whole-image and segment descriptors, therebyforming an unsupervised model of scenes and objects. We show that this model outperforms other unsupervised models that can only cluster one source of information (image or segment) at once. We are able to compare unsupervised and supervised techniques using standard measures derived from confusion matrices and contingency tables. This shows that our unsupervised model is competitive with current supervised and weakly-supervised models for scene understanding on standard datasets. We also demonstrate our model operating on a dataset with more than 100,000 images col- lected by an autonomous underwater vehicle.</p><p>6 0.6309306 <a title="369-lda-6" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>7 0.6271888 <a title="369-lda-7" href="./iccv-2013-Saliency_Detection_via_Dense_and_Sparse_Reconstruction.html">372 iccv-2013-Saliency Detection via Dense and Sparse Reconstruction</a></p>
<p>8 0.62557554 <a title="369-lda-8" href="./iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction.html">50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</a></p>
<p>9 0.62553805 <a title="369-lda-9" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>10 0.61513615 <a title="369-lda-10" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>11 0.60316843 <a title="369-lda-11" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>12 0.60072923 <a title="369-lda-12" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>13 0.59499735 <a title="369-lda-13" href="./iccv-2013-Saliency_Detection_via_Absorbing_Markov_Chain.html">371 iccv-2013-Saliency Detection via Absorbing Markov Chain</a></p>
<p>14 0.5824948 <a title="369-lda-14" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>15 0.58213258 <a title="369-lda-15" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>16 0.58093143 <a title="369-lda-16" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>17 0.57941872 <a title="369-lda-17" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>18 0.57378674 <a title="369-lda-18" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>19 0.56889814 <a title="369-lda-19" href="./iccv-2013-Semantically-Based_Human_Scanpath_Estimation_with_HMMs.html">381 iccv-2013-Semantically-Based Human Scanpath Estimation with HMMs</a></p>
<p>20 0.56846309 <a title="369-lda-20" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
