<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-318" href="#">iccv2013-318</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</h1>
<br/><p>Source: <a title="iccv-2013-318-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Duffner_PixelTrack_A_Fast_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Stefan Duffner, Christophe Garcia</p><p>Abstract: In this paper, we present a novel algorithm for fast tracking of generic objects in videos. The algorithm uses two components: a detector that makes use of the generalised Hough transform with pixel-based descriptors, and a probabilistic segmentation method based on global models for foreground and background. These components are used for tracking in a combined way, and they adapt each other in a co-training manner. Through effective model adaptation and segmentation, the algorithm is able to track objects that undergo rigid and non-rigid deformations and considerable shape and appearance variations. The proposed tracking method has been thoroughly evaluated on challenging standard videos, and outperforms state-of-theart tracking methods designed for the same task. Finally, the proposed models allow for an extremely efficient implementation, and thus tracking is very fast.</p><p>Reference: <a title="iccv-2013-318-reference" href="../iccv2013_reference/iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 PixelTrack: a fast adaptive algorithm for tracking non-rigid objects Stefan Duffner and Christophe Garcia Universit e´ de Lyon, CNRS INSA-Lyon, LIRIS, UMR5205, F-69621, France  st e fan . [sent-1, score-0.467]
</p><p>2 fr , i Abstract In this paper, we present a novel algorithm for fast tracking of generic objects in videos. [sent-4, score-0.421]
</p><p>3 The algorithm uses two components: a detector that makes use of the generalised Hough transform with pixel-based descriptors, and a probabilistic segmentation method based on global models for foreground and background. [sent-5, score-0.574]
</p><p>4 These components are used for tracking in a combined way, and they adapt each other in a co-training manner. [sent-6, score-0.38]
</p><p>5 Through effective model adaptation and segmentation, the algorithm is able to track objects that undergo rigid and non-rigid deformations and considerable shape and appearance variations. [sent-7, score-0.314]
</p><p>6 The proposed tracking method has been thoroughly evaluated on challenging standard videos, and outperforms state-of-theart tracking methods designed for the same task. [sent-8, score-0.76]
</p><p>7 Finally, the proposed models allow for an extremely efficient implementation, and thus tracking is very fast. [sent-9, score-0.38]
</p><p>8 Introduction  Given a video stream, tracking arbitrary objects that are non-rigid, moving or static, rotating and deforming, partially occluded, under changing illumination and without any prior knowledge is a challenging task. [sent-11, score-0.456]
</p><p>9 This unconstrained tracking problem where the object model is initialised from a bounding box in the first video frame and continuously adapted has been increasingly addressed in the literature in the past years. [sent-12, score-0.858]
</p><p>10 the gradual inclusion of background appearance, which can ultimately lead to tracking failure. [sent-16, score-0.448]
</p><p>11 Our method addresses these issues with an adaptive approach combining a detector based on pixel-based descriptors and a probabilistic segmentation framework. [sent-17, score-0.393]
</p><p>12 Related Work Earlier works [21, 11, 32, 30, 41, 18] on visual object tracking mostly consider a bounding box (or some other simple geometric model) representation of the object to track, and often a global appearance model is used. [sent-23, score-0.686]
</p><p>13 However, for tracking non-rigid objects that undergo a large amount of deformation and appearance variation, e. [sent-25, score-0.545]
</p><p>14 Although some algorithms effectively cope with object deformations by tracking their contour (e. [sent-28, score-0.494]
</p><p>15 [1] use a patch-based appearance model with integral histograms of colour and intensity. [sent-37, score-0.267]
</p><p>16 Be-  sides boosting algorithms, Online Random Forests have been proposed for adaptive visual object tracking [36, 37], where randomised trees are incrementally grown to clas22448800  sify an image region as object or background. [sent-44, score-0.632]
</p><p>17 [22] also use randomised forests which they combine effectively with a Lucas-Kanade tracker in a framework called Tracking-Learning-Detection (TLD) where the tracker updates the detector using spatial and temporal constraints and the detector re-initialises the tracker in case of drift. [sent-46, score-0.375]
</p><p>18 [28] introduced the l1 tracker that is based on a sparse set of appearance templates that are collected during tracking and used in the observation model of a particle filter. [sent-48, score-0.534]
</p><p>19 [3], for example, pro-  posed an ensemble tracking method that label each pixel as foreground or background with an Adaboost algorithm that is updated online. [sent-58, score-0.727]
</p><p>20 However, all of these methods still operate more or less on image regions described by bounding boxed and inherently have difficulties to track objects undergoing large deformations. [sent-59, score-0.282]
</p><p>21 To overcome this problem, recent approaches integrate some form of segmentation into the tracking process. [sent-60, score-0.581]
</p><p>22 [24] handle deforming objects by tracking configurations of a dynamic set of image patches, and they use Basin Hopping Monte Carlo (BHMC) sampling to reduce the computational complexity. [sent-64, score-0.457]
</p><p>23 [8] propose an adaptive probabilistic framework separating the tracking of non-rigid objects into registration and level-set segmentation, where posterior probabilities are computed at the pixel level. [sent-67, score-0.627]
</p><p>24 [2] also combined tracking and segmentation in a Bayesian framework, where pixel-wise likelihood distributions of several objects and the background are modelled by Gaussian functions the parameters of which are learnt online. [sent-69, score-0.69]
</p><p>25 In a different application context, pixel-based descriptors have also been used for 3D articulated human-body detection and tracking by Shotton et al. [sent-70, score-0.418]
</p><p>26 [7],  a graph-cut segmentation is applied separately to the image patches provided by a particle filter. [sent-73, score-0.295]
</p><p>27 By back-projecting the patches that voted for the object centre, the authors initialise a graph-cut algorithm to segment foreground from background. [sent-77, score-0.258]
</p><p>28 The resulting segmentation is then used to update the patches’ foreground and background probabilities in the Hough forest. [sent-78, score-0.51]
</p><p>29 This method achieves state-of-the-art tracking results on many challenging benchmark videos. [sent-79, score-0.38]
</p><p>30 Also, the segmentation is discrete and binary, which can increase the risk of drift due to wrongly segmented image regions. [sent-81, score-0.247]
</p><p>31 Motivation The algorithm presented in this paper is inspired by these recent works on combined tracking and segmentation, which is beneficial for tracking non-rigid objects. [sent-84, score-0.76]
</p><p>32 The method tightly integrates with a probabilistic segmentation of foreground and background that is used to incrementally  update the local pixel-based descriptors and vice versa. [sent-87, score-0.598]
</p><p>33 The local Hough-voting model and the global colour model operate both at the pixel level and thus allow for very efficient model representation and inference. [sent-88, score-0.317]
</p><p>34 Note that the main goal of the proposed approach is to provide an accurate position estimate of an object to track in a video. [sent-90, score-0.246]
</p><p>35 Here, we are not so much interested in a perfect segmentation of the object from the background. [sent-91, score-0.26]
</p><p>36 Then we will detail each of the com22448811  Each pixel inside the search window (blue dotted rectangle) in the input image casts a vote (1) according to the current Hough transform model (darker: high vote sum, lighter: low vote sum). [sent-95, score-0.582]
</p><p>37 In parallel, the current segmentation model is used to segment the image region inside the search window (3) (binarised segmentation shown in red). [sent-99, score-0.549]
</p><p>38 Finally, after updating the objects current position (4), the segmentation model is adapted (5) using the backprojected image pixels, and the Hough transform model is updated (6) with foreground pixels from the segmentation and background pixels from a region around the object (blue frame). [sent-100, score-1.175]
</p><p>39 The algorithm receives as input the current video frame as well as the bounding box and segmentation from the tracking result of the previous frame. [sent-106, score-0.824]
</p><p>40 The pixelbased Hough transform is applied on each pixel inside the search window, the enlarged bounding box, i. [sent-107, score-0.317]
</p><p>41 each pixel votes for the centre position of the object according to the learnt model. [sent-109, score-0.442]
</p><p>42 Votes are cumulated in a common reference frame, the voting map, and the position with the highest sum of votes determines the most likely position of the object’s centre (see Section 3 for a more detailed explanation). [sent-110, score-0.553]
</p><p>43 In parallel, the image region corresponding to the search window is segmented using the current segmentation model. [sent-113, score-0.309]
</p><p>44 The position of the tracked object is updated using the maximum vote position and the centre of mass of the segmentation output (see Section 5). [sent-115, score-0.806]
</p><p>45 That means, the segmentation model is updated using the backprojection, and the pixel-based Hough model is adapted according to the segmentation output (see Section 6 for more details). [sent-117, score-0.509]
</p><p>46 Also, in this tracking framework, this risk is considerably reduced by combining the detector with the segmentation output. [sent-132, score-0.639]
</p><p>47 In the training image, the pixels inside a given initial bounding box are quantised according to the vector composed of its HSV colour values (with separate V quantisation) and its x and y gradient orientation (with a separate quantisation for low gradient magnitudes) (see Fig. [sent-138, score-0.605]
</p><p>48 Left: the model D is constructed by storing for each quantised pixel value in the given bounding box all the displacement vectors to the object’s centre position (here only colour is used for illustration). [sent-149, score-0.779]
</p><p>49 Right: the object is detected in a search window by accumulating the displacement votes of each pixel in a voting map (bright pixels: many votes, dark pixels: few votes). [sent-150, score-0.535]
</p><p>50 Thus, training consists in constructing D, where each pixel I(x) in the given bounding box produces a displacement vector dz (arrows in Fig. [sent-153, score-0.328]
</p><p>51 2) corresponding to its quantised value zx and pointing to the centre of the bounding box. [sent-154, score-0.299]
</p><p>52 Detection  ×  In a new video frame, the object can be detected by letting each pixel I(x) inside the search window vote according to Dz corresponding to its quantised value zx. [sent-157, score-0.527]
</p><p>53 Each vote is a list of displacements dzm that are weighted by wzm and accumulated in a voting map. [sent-160, score-0.407]
</p><p>54 Note that, as illustrated in figure 2, the position estimation is “diffused” by two factors: the deformation of the object (one of the green pixels in the figure), and pixels of the same colour (green and blue pixels). [sent-162, score-0.5]
</p><p>55 But the maximum value in the voting map is still distinctive and corresponds well to the centre position of the object. [sent-163, score-0.362]
</p><p>56 Nevertheless, to be robust to very small deformations we group the votes in small voting cells of 3 3 pixels (as [15]). [sent-165, score-0.371]
</p><p>57 Backprojection  With the position of the maximum in the voting map xmax, we can determine which pixels in the search window contributed to it during the detection. [sent-168, score-0.482]
</p><p>58 ed for xmax,(1)  The backprojected pixels are used for adapting the segmentation model (see Section 6 for more details). [sent-174, score-0.345]
</p><p>59 The idea behind this is that, intuitively, pixels that contributed to xmax are most likely corresponding to the object. [sent-175, score-0.265]
</p><p>60 A probabilistic soft segmentation approach is adopted here (similar to [2]). [sent-178, score-0.251]
</p><p>61 Let ct,x ∈ {0, 1} be the class of the pixel at position x at time t: 0∈ f {or0 background, aasnsd o 1f fthoer foreground, and let y1:t,x be the pixel’s colour observations from time 1to t. [sent-179, score-0.372]
</p><p>62 H T anhed foreground histogram is initialised from the image region defined by the bounding box around the object in the first frame. [sent-186, score-0.465]
</p><p>63 The background histogram is initialised from the image region surrounding this rectangle (with some margin between). [sent-187, score-0.277]
</p><p>64 The transition probabilities for foreground and background are set to: p(ct = 0 | ct−1 ) = 0. [sent-188, score-0.261]
</p><p>65 Note that the tracking algorithm is not very sensitive to these parameters. [sent-191, score-0.38]
</p><p>66 2, we are not so much interested here in a perfectly “clean” segmentation but rather in fast and robust tracking of the position of an object. [sent-196, score-0.674]
</p><p>67 Tracking In a new video frame, pixel-based detection and segmentation are performed inside a search window Ω, which is set here to twice the size of the object’s bounding box. [sent-198, score-0.455]
</p><p>68 the maximum position in the voting map, but also the segmentation output. [sent-202, score-0.442]
</p><p>69 Clearly, this makes the tracking algorithm more robust to non-rigid deformations. [sent-203, score-0.38]
</p><p>70 More precisely, we calculate the centre of mass of the soft segmentation produced by Eq. [sent-204, score-0.357]
</p><p>71 p(cx= 1|y) x,  (4)  where S is the sum of all foreground probabilities p(cx = 1|y) in the search window Ω. [sent-206, score-0.301]
</p><p>72 n T hofe voting map maximum xmax and xs : Xt =  αxs  + (1 − α)xmax . [sent-208, score-0.32]
</p><p>73 It is computed dynamically at each frame by a simple reliability measure that is defined as the proportion of pixels in the search window that change from foreground to background or vice versa, i. [sent-210, score-0.479]
</p><p>74 Model adaptation  Both pixel-based Hough model and segmentation model are updated at each frame in a co-training manner, i. [sent-217, score-0.334]
</p><p>75 To update the Hough model, only foreground pixels are used, that is pixels for which p(cx = 1|y) > 0. [sent-220, score-0.342]
</p><p>76 For each of these pixels x the displacement =d t 1o| yth)e new object’s caecnhtr oef fi tsh hceasleculated, and its weight w is set according to its foreground probability:  w ←? [sent-222, score-0.275]
</p><p>77 The foreground and background distributions of the segmentation model are adapted using the backprojection bx. [sent-227, score-0.592]
</p><p>78 5) of the backprojected pixels oilso cuarl dciusltartiebudt, aonnd p (uys|ebd >to 0 linearly update trhoecurrent foreground colour distribution: p(yt |ct = 1) = δ p(y|b > 0. [sent-229, score-0.554]
</p><p>79 The background colour distribution is updated in the same way but using the colour distribution from a rectangular frame surrounding the object borders (as for initialisation). [sent-232, score-0.711]
</p><p>80 The tracking accuracy and speed on these datasets has been measured and compared to two state-ofthe-art tracking methods. [sent-237, score-0.76]
</p><p>81 That means, tracking is only performed by Hough voting. [sent-244, score-0.38]
</p><p>82 Although, these and other previous works have reported tracking accuracy results on some of the videos from our datasets, we evaluated these methods  again using our performance measure in order to have a consistent comparison. [sent-250, score-0.457]
</p><p>83 For our method, the initial rectangle is smaller as it should contain as few background pixels as possible in order to obtain a good initial segmentation model. [sent-252, score-0.39]
</p><p>84 To measure the performance of the different tracking algorithms, we determine, for each video, the percentage of frames in which the object is correctly tracked. [sent-253, score-0.439]
</p><p>85 In each video frame, the tracking is considered correct if the PAS-  CthAreLsh oVlOd,C wChe [r1e3 ]R oTviesrl tahpe m re catasnugreleR fRrToTm∪∩RR tGGhT e trisac akbinogve al a-  gorithm, and RGT is the ground truth rectangle surrounding the object. [sent-254, score-0.501]
</p><p>86 value would discriminate our method too much because the bounding box that is output currently does not change its size and aspect ratio during the tracking, and it is rarely initialised to surround the complete object. [sent-330, score-0.252]
</p><p>87 3 shows some tracking results from the “Tiger 2” sequence. [sent-337, score-0.38]
</p><p>88 Also the average of correct tracking is almost 7 percentage points  HT  TLD  PixelTrack  2. [sent-342, score-0.38]
</p><p>89 4 illustrates some tracking results of the three compared methods on one of the sequences: “Diving”. [sent-351, score-0.38]
</p><p>90 This is due to pixel-based Hough voting that allows for an extremely efficient implementation with lookup-tables as well as a fast segmentation algorithm. [sent-363, score-0.349]
</p><p>91 Conclusions We presented an algorithm for tracking generic objects in videos without any prior knowledge. [sent-365, score-0.498]
</p><p>92 It is an effective combination of a local pixel-based detector based on a Hough voting scheme and a global probabilistic segmentation method that operate jointly and update each other in a co-training manner. [sent-366, score-0.543]
</p><p>93 Experimental results show that the method outperforms state-of-theart tracking algorithms on challenging videos from standard benchmarks. [sent-370, score-0.457]
</p><p>94 Visual tracking using a pixelwise spatiotemporal oriented energy representation. [sent-436, score-0.38]
</p><p>95 Adaptive fragments-based tracking of non-rigid objects using level sets. [sent-443, score-0.421]
</p><p>96 Combined object categorization and segmentation with an implicit shape model. [sent-556, score-0.26]
</p><p>97 Robust tracking using local sparse appearance model and k-selection. [sent-572, score-0.439]
</p><p>98 Robust visual tracking and vehicle classification via sparse representation. [sent-578, score-0.38]
</p><p>99 Geodesic active contours and level sets for the detection and tracking of moving objects. [sent-602, score-0.38]
</p><p>100 Multi-camera multi-person 3D space tracking with MCMC in surveillance scenarios. [sent-686, score-0.38]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tracking', 0.38), ('hough', 0.317), ('colour', 0.208), ('segmentation', 0.201), ('tld', 0.191), ('pixeltrack', 0.179), ('foreground', 0.154), ('voting', 0.148), ('xmax', 0.132), ('initialised', 0.123), ('centre', 0.121), ('houghtrack', 0.12), ('backprojection', 0.116), ('ct', 0.112), ('vote', 0.109), ('quantised', 0.106), ('votes', 0.098), ('track', 0.094), ('ht', 0.093), ('position', 0.093), ('motocross', 0.09), ('wzm', 0.09), ('tiger', 0.084), ('grabner', 0.082), ('frame', 0.079), ('godec', 0.079), ('videos', 0.077), ('dz', 0.077), ('backprojected', 0.074), ('generalised', 0.074), ('saffari', 0.073), ('bounding', 0.072), ('forests', 0.072), ('pixel', 0.071), ('pixels', 0.07), ('window', 0.07), ('cx', 0.069), ('background', 0.068), ('undergo', 0.065), ('kwon', 0.065), ('babenko', 0.064), ('contributed', 0.063), ('online', 0.062), ('diving', 0.061), ('dzm', 0.06), ('nejhum', 0.06), ('pixelbased', 0.06), ('object', 0.059), ('initialisation', 0.059), ('appearance', 0.059), ('detector', 0.058), ('kalal', 0.057), ('box', 0.057), ('deformations', 0.055), ('updated', 0.054), ('adapted', 0.053), ('skiing', 0.053), ('belagiannis', 0.053), ('aeschliman', 0.053), ('gymnastics', 0.053), ('bibby', 0.053), ('quantisation', 0.053), ('yt', 0.052), ('displacement', 0.051), ('rectangle', 0.051), ('probabilistic', 0.05), ('hopping', 0.049), ('randomised', 0.049), ('particle', 0.049), ('update', 0.048), ('tracker', 0.046), ('adaptive', 0.046), ('drift', 0.046), ('mei', 0.045), ('patches', 0.045), ('cnrs', 0.044), ('transactions', 0.043), ('hare', 0.042), ('objects', 0.041), ('tracked', 0.041), ('xs', 0.04), ('graphcut', 0.039), ('inside', 0.039), ('incrementally', 0.039), ('pages', 0.039), ('occlusions', 0.039), ('probabilities', 0.039), ('descriptors', 0.038), ('operate', 0.038), ('basin', 0.038), ('search', 0.038), ('difficulties', 0.037), ('transform', 0.037), ('deforming', 0.036), ('girl', 0.036), ('ris', 0.036), ('mass', 0.035), ('hsv', 0.035), ('video', 0.035), ('surrounding', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="318-tfidf-1" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>Author: Stefan Duffner, Christophe Garcia</p><p>Abstract: In this paper, we present a novel algorithm for fast tracking of generic objects in videos. The algorithm uses two components: a detector that makes use of the generalised Hough transform with pixel-based descriptors, and a probabilistic segmentation method based on global models for foreground and background. These components are used for tracking in a combined way, and they adapt each other in a co-training manner. Through effective model adaptation and segmentation, the algorithm is able to track objects that undergo rigid and non-rigid deformations and considerable shape and appearance variations. The proposed tracking method has been thoroughly evaluated on challenging standard videos, and outperforms state-of-theart tracking methods designed for the same task. Finally, the proposed models allow for an extremely efficient implementation, and thus tracking is very fast.</p><p>2 0.2391564 <a title="318-tfidf-2" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>Author: Junliang Xing, Jin Gao, Bing Li, Weiming Hu, Shuicheng Yan</p><p>Abstract: Recently, sparse representation has been introduced for robust object tracking. By representing the object sparsely, i.e., using only a few templates via ?1-norm minimization, these so-called ?1-trackers exhibit promising tracking results. In this work, we address the object template building and updating problem in these ?1-tracking approaches, which has not been fully studied. We propose to perform template updating, in a new perspective, as an online incremental dictionary learning problem, which is efficiently solved through an online optimization procedure. To guarantee the robustness and adaptability of the tracking algorithm, we also propose to build a multi-lifespan dictionary model. By building target dictionaries of different lifespans, effective object observations can be obtained to deal with the well-known drifting problem in tracking and thus improve the tracking accuracy. We derive effective observa- tion models both generatively and discriminatively based on the online multi-lifespan dictionary learning model and deploy them to the Bayesian sequential estimation framework to perform tracking. The proposed approach has been extensively evaluated on ten challenging video sequences. Experimental results demonstrate the effectiveness of the online learned templates, as well as the state-of-the-art tracking performance of the proposed approach.</p><p>3 0.22957781 <a title="318-tfidf-3" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>Author: Shuran Song, Jianxiong Xiao</p><p>Abstract: Despite significant progress, tracking is still considered to be a very challenging task. Recently, the increasing popularity of depth sensors has made it possible to obtain reliable depth easily. This may be a game changer for tracking, since depth can be used to prevent model drift and handle occlusion. We also observe that current tracking algorithms are mostly evaluated on a very small number of videos collectedandannotated by different groups. The lack of a reasonable size and consistently constructed benchmark has prevented a persuasive comparison among different algorithms. In this paper, we construct a unified benchmark dataset of 100 RGBD videos with high diversity, propose different kinds of RGBD tracking algorithms using 2D or 3D model, and present a quantitative comparison of various algorithms with RGB or RGBD input. We aim to lay the foundation for further research in both RGB and RGBD tracking, and our benchmark is available at http://tracking.cs.princeton.edu.</p><p>4 0.22321771 <a title="318-tfidf-4" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>Author: Taegyu Lim, Seunghoon Hong, Bohyung Han, Joon Hee Han</p><p>Abstract: We propose an on-line algorithm to extract a human by foreground/background segmentation and estimate pose of the human from the videos captured by moving cameras. We claim that a virtuous cycle can be created by appropriate interactions between the two modules to solve individual problems. This joint estimation problem is divided into two subproblems, , foreground/background segmentation and pose tracking, which alternate iteratively for optimization; segmentation step generates foreground mask for human pose tracking, and human pose tracking step provides foreground response map for segmentation. The final solution is obtained when the iterative procedure converges. We evaluate our algorithm quantitatively and qualitatively in real videos involving various challenges, and present its outstandingperformance compared to the state-of-the-art techniques for segmentation and pose estimation.</p><p>5 0.2192537 <a title="318-tfidf-5" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>Author: Kwang Moo Yi, Hawook Jeong, Byeongho Heo, Hyung Jin Chang, Jin Young Choi</p><p>Abstract: In this paper we propose an object tracking method in case of inaccurate initializations. To track objects accurately in such situation, the proposed method uses “motion saliency ” and “descriptor saliency ” of local features and performs tracking based on generalized Hough transform (GHT). The proposed motion saliency of a local feature emphasizes features having distinctive motions, compared to the motions which are not from the target object. The descriptor saliency emphasizes features which are likely to be of the object in terms of its feature descriptors. Through these saliencies, the proposed method tries to “learn and find” the target object rather than looking for what was given at initialization, giving robust results even with inaccurate initializations. Also, our tracking result is obtained by combining the results of each local feature of the target and the surroundings with GHT voting, thus is robust against severe occlusions as well. The proposed method is compared against nine other methods, with nine image sequences, and hundred random initializations. The experimental results show that our method outperforms all other compared methods.</p><p>6 0.19498636 <a title="318-tfidf-6" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>7 0.19484445 <a title="318-tfidf-7" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>8 0.19190608 <a title="318-tfidf-8" href="./iccv-2013-Online_Robust_Non-negative_Dictionary_Learning_for_Visual_Tracking.html">298 iccv-2013-Online Robust Non-negative Dictionary Learning for Visual Tracking</a></p>
<p>9 0.1863369 <a title="318-tfidf-9" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>10 0.17785183 <a title="318-tfidf-10" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>11 0.17030871 <a title="318-tfidf-11" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>12 0.16856328 <a title="318-tfidf-12" href="./iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects.html">320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</a></p>
<p>13 0.16741237 <a title="318-tfidf-13" href="./iccv-2013-Finding_the_Best_from_the_Second_Bests_-_Inhibiting_Subjective_Bias_in_Evaluation_of_Visual_Tracking_Algorithms.html">168 iccv-2013-Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms</a></p>
<p>14 0.16503623 <a title="318-tfidf-14" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>15 0.16349936 <a title="318-tfidf-15" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>16 0.16327663 <a title="318-tfidf-16" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>17 0.16233917 <a title="318-tfidf-17" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>18 0.14901215 <a title="318-tfidf-18" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>19 0.14847548 <a title="318-tfidf-19" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>20 0.13995872 <a title="318-tfidf-20" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.277), (1, -0.089), (2, 0.076), (3, 0.065), (4, 0.066), (5, -0.095), (6, -0.156), (7, 0.237), (8, -0.095), (9, 0.155), (10, -0.043), (11, -0.066), (12, 0.07), (13, 0.053), (14, -0.013), (15, -0.042), (16, 0.057), (17, -0.018), (18, -0.145), (19, -0.093), (20, 0.003), (21, 0.018), (22, -0.086), (23, -0.009), (24, -0.08), (25, 0.071), (26, -0.016), (27, 0.024), (28, 0.014), (29, -0.011), (30, -0.04), (31, -0.017), (32, -0.043), (33, -0.02), (34, -0.051), (35, 0.025), (36, -0.017), (37, 0.013), (38, -0.009), (39, 0.045), (40, -0.004), (41, 0.084), (42, -0.063), (43, -0.01), (44, 0.034), (45, 0.002), (46, 0.011), (47, -0.095), (48, 0.09), (49, -0.135)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97030395 <a title="318-lsi-1" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>Author: Stefan Duffner, Christophe Garcia</p><p>Abstract: In this paper, we present a novel algorithm for fast tracking of generic objects in videos. The algorithm uses two components: a detector that makes use of the generalised Hough transform with pixel-based descriptors, and a probabilistic segmentation method based on global models for foreground and background. These components are used for tracking in a combined way, and they adapt each other in a co-training manner. Through effective model adaptation and segmentation, the algorithm is able to track objects that undergo rigid and non-rigid deformations and considerable shape and appearance variations. The proposed tracking method has been thoroughly evaluated on challenging standard videos, and outperforms state-of-theart tracking methods designed for the same task. Finally, the proposed models allow for an extremely efficient implementation, and thus tracking is very fast.</p><p>2 0.76870072 <a title="318-lsi-2" href="./iccv-2013-Orderless_Tracking_through_Model-Averaged_Posterior_Estimation.html">303 iccv-2013-Orderless Tracking through Model-Averaged Posterior Estimation</a></p>
<p>Author: Seunghoon Hong, Suha Kwak, Bohyung Han</p><p>Abstract: We propose a novel offline tracking algorithm based on model-averaged posterior estimation through patch matching across frames. Contrary to existing online and offline tracking methods, our algorithm is not based on temporallyordered estimates of target state but attempts to select easyto-track frames first out of the remaining ones without exploiting temporal coherency of target. The posterior of the selected frame is estimated by propagating densities from the already tracked frames in a recursive manner. The density propagation across frames is implemented by an efficient patch matching technique, which is useful for our algorithm since it does not require motion smoothness assumption. Also, we present a hierarchical approach, where a small set of key frames are tracked first and non-key frames are handled by local key frames. Our tracking algorithm is conceptually well-suited for the sequences with abrupt motion, shot changes, and occlusion. We compare our tracking algorithm with existing techniques in real videos with such challenges and illustrate its superior performance qualitatively and quantitatively.</p><p>3 0.75719458 <a title="318-lsi-3" href="./iccv-2013-Finding_the_Best_from_the_Second_Bests_-_Inhibiting_Subjective_Bias_in_Evaluation_of_Visual_Tracking_Algorithms.html">168 iccv-2013-Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms</a></p>
<p>Author: Yu Pang, Haibin Ling</p><p>Abstract: Evaluating visual tracking algorithms, or “trackers ” for short, is of great importance in computer vision. However, it is hard to “fairly” compare trackers due to many parameters need to be tuned in the experimental configurations. On the other hand, when introducing a new tracker, a recent trend is to validate it by comparing it with several existing ones. Such an evaluation may have subjective biases towards the new tracker which typically performs the best. This is mainly due to the difficulty to optimally tune all its competitors and sometimes the selected testing sequences. By contrast, little subjective bias exists towards the “second best” ones1 in the contest. This observation inspires us with a novel perspective towards inhibiting subjective bias in evaluating trackers by analyzing the results between the second bests. In particular, we first collect all tracking papers published in major computer vision venues in recent years. From these papers, after filtering out potential biases in various aspects, we create a dataset containing many records of comparison results between various visual trackers. Using these records, we derive performance rank- ings of the involved trackers by four different methods. The first two methods model the dataset as a graph and then derive the rankings over the graph, one by a rank aggregation algorithm and the other by a PageRank-like solution. The other two methods take the records as generated from sports contests and adopt widely used Elo’s and Glicko ’s rating systems to derive the rankings. The experimental results are presented and may serve as a reference for related research.</p><p>4 0.71237618 <a title="318-lsi-4" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>Author: Shuran Song, Jianxiong Xiao</p><p>Abstract: Despite significant progress, tracking is still considered to be a very challenging task. Recently, the increasing popularity of depth sensors has made it possible to obtain reliable depth easily. This may be a game changer for tracking, since depth can be used to prevent model drift and handle occlusion. We also observe that current tracking algorithms are mostly evaluated on a very small number of videos collectedandannotated by different groups. The lack of a reasonable size and consistently constructed benchmark has prevented a persuasive comparison among different algorithms. In this paper, we construct a unified benchmark dataset of 100 RGBD videos with high diversity, propose different kinds of RGBD tracking algorithms using 2D or 3D model, and present a quantitative comparison of various algorithms with RGB or RGBD input. We aim to lay the foundation for further research in both RGB and RGBD tracking, and our benchmark is available at http://tracking.cs.princeton.edu.</p><p>5 0.70552558 <a title="318-lsi-5" href="./iccv-2013-Conservation_Tracking.html">87 iccv-2013-Conservation Tracking</a></p>
<p>Author: Martin Schiegg, Philipp Hanslovsky, Bernhard X. Kausler, Lars Hufnagel, Fred A. Hamprecht</p><p>Abstract: The quality of any tracking-by-assignment hinges on the accuracy of the foregoing target detection / segmentation step. In many kinds of images, errors in this first stage are unavoidable. These errors then propagate to, and corrupt, the tracking result. Our main contribution is the first probabilistic graphical model that can explicitly account for over- and undersegmentation errors even when the number of tracking targets is unknown and when they may divide, as in cell cultures. The tracking model we present implements global consistency constraints for the number of targets comprised by each detection and is solved to global optimality on reasonably large 2D+t and 3D+t datasets. In addition, we empirically demonstrate the effectiveness of a postprocessing that allows to establish target identity even across occlusion / undersegmentation. The usefulness and efficiency of this new tracking method is demonstrated on three different and challenging 2D+t and 3D+t datasets from developmental biology.</p><p>6 0.69299263 <a title="318-lsi-6" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>7 0.69056666 <a title="318-lsi-7" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>8 0.68721652 <a title="318-lsi-8" href="./iccv-2013-Online_Robust_Non-negative_Dictionary_Learning_for_Visual_Tracking.html">298 iccv-2013-Online Robust Non-negative Dictionary Learning for Visual Tracking</a></p>
<p>9 0.67082715 <a title="318-lsi-9" href="./iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects.html">320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</a></p>
<p>10 0.66863501 <a title="318-lsi-10" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>11 0.66578728 <a title="318-lsi-11" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>12 0.66034842 <a title="318-lsi-12" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>13 0.65374905 <a title="318-lsi-13" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>14 0.64845592 <a title="318-lsi-14" href="./iccv-2013-Slice_Sampling_Particle_Belief_Propagation.html">395 iccv-2013-Slice Sampling Particle Belief Propagation</a></p>
<p>15 0.64013427 <a title="318-lsi-15" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>16 0.63110983 <a title="318-lsi-16" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>17 0.63067758 <a title="318-lsi-17" href="./iccv-2013-Dynamic_Probabilistic_Volumetric_Models.html">128 iccv-2013-Dynamic Probabilistic Volumetric Models</a></p>
<p>18 0.59407634 <a title="318-lsi-18" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>19 0.58487797 <a title="318-lsi-19" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>20 0.58151048 <a title="318-lsi-20" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.052), (13, 0.018), (26, 0.094), (31, 0.041), (35, 0.01), (40, 0.348), (42, 0.066), (48, 0.01), (64, 0.093), (73, 0.034), (89, 0.135), (98, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88630635 <a title="318-lda-1" href="./iccv-2013-Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illumination_in_the_Spectral_Domain.html">385 iccv-2013-Separating Reflective and Fluorescent Components Using High Frequency Illumination in the Spectral Domain</a></p>
<p>Author: Ying Fu, Antony Lam, Imari Sato, Takahiro Okabe, Yoichi Sato</p><p>Abstract: Hyperspectral imaging is beneficial to many applications but current methods do not consider fluorescent effects which are present in everyday items ranging from paper, to clothing, to even our food. Furthermore, everyday fluorescent items exhibit a mix of reflectance and fluorescence. So proper separation of these components is necessary for analyzing them. In this paper, we demonstrate efficient separation and recovery of reflective and fluorescent emission spectra through the use of high frequency illumination in the spectral domain. With the obtained fluorescent emission spectra from our high frequency illuminants, we then present to our knowledge, the first method for estimating the fluorescent absorption spectrum of a material given its emission spectrum. Conventional bispectral measurement of absorption and emission spectra needs to examine all combinations of incident and observed light wavelengths. In contrast, our method requires only two hyperspectral images. The effectiveness of our proposed methods are then evaluated through a combination of simulation and real experiments. We also demonstrate an application of our method to synthetic relighting of real scenes.</p><p>same-paper 2 0.75035059 <a title="318-lda-2" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>Author: Stefan Duffner, Christophe Garcia</p><p>Abstract: In this paper, we present a novel algorithm for fast tracking of generic objects in videos. The algorithm uses two components: a detector that makes use of the generalised Hough transform with pixel-based descriptors, and a probabilistic segmentation method based on global models for foreground and background. These components are used for tracking in a combined way, and they adapt each other in a co-training manner. Through effective model adaptation and segmentation, the algorithm is able to track objects that undergo rigid and non-rigid deformations and considerable shape and appearance variations. The proposed tracking method has been thoroughly evaluated on challenging standard videos, and outperforms state-of-theart tracking methods designed for the same task. Finally, the proposed models allow for an extremely efficient implementation, and thus tracking is very fast.</p><p>3 0.72601658 <a title="318-lda-3" href="./iccv-2013-Total_Variation_Regularization_for_Functions_with_Values_in_a_Manifold.html">421 iccv-2013-Total Variation Regularization for Functions with Values in a Manifold</a></p>
<p>Author: Jan Lellmann, Evgeny Strekalovskiy, Sabrina Koetter, Daniel Cremers</p><p>Abstract: While total variation is among the most popular regularizers for variational problems, its extension to functions with values in a manifold is an open problem. In this paper, we propose the first algorithm to solve such problems which applies to arbitrary Riemannian manifolds. The key idea is to reformulate the variational problem as a multilabel optimization problem with an infinite number of labels. This leads to a hard optimization problem which can be approximately solved using convex relaxation techniques. The framework can be easily adapted to different manifolds including spheres and three-dimensional rotations, and allows to obtain accurate solutions even with a relatively coarse discretization. With numerous examples we demonstrate that the proposed framework can be applied to variational models that incorporate chromaticity values, normal fields, or camera trajectories.</p><p>4 0.60587776 <a title="318-lda-4" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>Author: Anestis Papazoglou, Vittorio Ferrari</p><p>Abstract: We present a technique for separating foreground objects from the background in a video. Our method isfast, , fully automatic, and makes minimal assumptions about the video. This enables handling essentially unconstrained settings, including rapidly moving background, arbitrary object motion and appearance, and non-rigid deformations and articulations. In experiments on two datasets containing over 1400 video shots, our method outperforms a state-of-theart background subtraction technique [4] as well as methods based on clustering point tracks [6, 18, 19]. Moreover, it performs comparably to recent video object segmentation methods based on objectproposals [14, 16, 27], while being orders of magnitude faster.</p><p>5 0.59739548 <a title="318-lda-5" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>Author: Olaf Kähler, Ian Reid</p><p>Abstract: We address the problem of 3D scene labeling in a structured learning framework. Unlike previous work which uses structured Support VectorMachines, we employ the recently described Decision Tree Field and Regression Tree Field frameworks, which learn the unary and binary terms of a Conditional Random Field from training data. We show this has significant advantages in terms of inference speed, while maintaining similar accuracy. We also demonstrate empirically the importance for overall labeling accuracy of features that make use of prior knowledge about the coarse scene layout such as the location of the ground plane. We show how this coarse layout can be estimated by our framework automatically, and that this information can be used to bootstrap improved accuracy in the detailed labeling.</p><p>6 0.58800554 <a title="318-lda-6" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>7 0.56592673 <a title="318-lda-7" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>8 0.54017758 <a title="318-lda-8" href="./iccv-2013-Alternating_Regression_Forests_for_Object_Detection_and_Pose_Estimation.html">47 iccv-2013-Alternating Regression Forests for Object Detection and Pose Estimation</a></p>
<p>9 0.53413504 <a title="318-lda-9" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>10 0.53362805 <a title="318-lda-10" href="./iccv-2013-Real-Time_Articulated_Hand_Pose_Estimation_Using_Semi-supervised_Transductive_Regression_Forests.html">340 iccv-2013-Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests</a></p>
<p>11 0.52717507 <a title="318-lda-11" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>12 0.52656323 <a title="318-lda-12" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>13 0.52458346 <a title="318-lda-13" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>14 0.52236152 <a title="318-lda-14" href="./iccv-2013-Volumetric_Semantic_Segmentation_Using_Pyramid_Context_Features.html">447 iccv-2013-Volumetric Semantic Segmentation Using Pyramid Context Features</a></p>
<p>15 0.52221113 <a title="318-lda-15" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>16 0.51879179 <a title="318-lda-16" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>17 0.5156498 <a title="318-lda-17" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>18 0.51559794 <a title="318-lda-18" href="./iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects.html">320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</a></p>
<p>19 0.51432246 <a title="318-lda-19" href="./iccv-2013-Progressive_Multigrid_Eigensolvers_for_Multiscale_Spectral_Segmentation.html">329 iccv-2013-Progressive Multigrid Eigensolvers for Multiscale Spectral Segmentation</a></p>
<p>20 0.51227492 <a title="318-lda-20" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
