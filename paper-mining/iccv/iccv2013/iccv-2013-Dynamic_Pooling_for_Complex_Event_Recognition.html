<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>127 iccv-2013-Dynamic Pooling for Complex Event Recognition</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-127" href="#">iccv2013-127</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>127 iccv-2013-Dynamic Pooling for Complex Event Recognition</h1>
<br/><p>Source: <a title="iccv-2013-127-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Li_Dynamic_Pooling_for_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Weixin Li, Qian Yu, Ajay Divakaran, Nuno Vasconcelos</p><p>Abstract: The problem of adaptively selecting pooling regions for the classification of complex video events is considered. Complex events are defined as events composed of several characteristic behaviors, whose temporal configuration can change from sequence to sequence. A dynamic pooling operator is defined so as to enable a unified solution to the problems of event specific video segmentation, temporal structure modeling, and event detection. Video is decomposed into segments, and the segments most informative for detecting a given event are identified, so as to dynamically determine the pooling operator most suited for each sequence. This dynamic pooling is implemented by treating the locations of characteristic segments as hidden information, which is inferred, on a sequence-by-sequence basis, via a large-margin classification rule with latent variables. Although the feasible set of segment selections is combinatorial, it is shown that a globally optimal solution to the inference problem can be obtained efficiently, through the solution of a series of linear programs. Besides the coarselevel location of segments, a finer model of video struc- ture is implemented by jointly pooling features of segmenttuples. Experimental evaluation demonstrates that the re- sulting event detector has state-of-the-art performance on challenging video datasets.</p><p>Reference: <a title="iccv-2013-127-reference" href="../iccv2013_reference/iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu 0 Abstract The problem of adaptively selecting pooling regions for the classification of complex video events is considered. [sent-2, score-0.69]
</p><p>2 Complex events are defined as events composed of several characteristic behaviors, whose temporal configuration can change from sequence to sequence. [sent-3, score-0.569]
</p><p>3 A dynamic pooling operator is defined so as to enable a unified solution to the problems of event specific video segmentation, temporal structure modeling, and event detection. [sent-4, score-1.48]
</p><p>4 Video is decomposed into segments, and the segments most informative for detecting a given event are identified, so as to dynamically determine the pooling operator most suited for each sequence. [sent-5, score-1.012]
</p><p>5 This dynamic pooling is implemented by treating the locations of characteristic segments as hidden information, which is inferred, on a sequence-by-sequence basis, via a large-margin classification rule with latent variables. [sent-6, score-0.951]
</p><p>6 Besides the coarselevel location of segments,  a finer model of video struc-  ture is implemented by jointly pooling features of segmenttuples. [sent-8, score-0.558]
</p><p>7 Experimental evaluation demonstrates  that the re-  sulting event detector has state-of-the-art performance  on  challenging video datasets. [sent-9, score-0.469]
</p><p>8 Figure 1: Challenges of event recognition in open source video (best viewed in color). [sent-83, score-0.432]
</p><p>9 , “wedding”, can differ substantially in the atomic actions that compose them and corresponding durations (indicated by color bars). [sent-89, score-0.251]
</p><p>10 For example, the upper “wedding” video includes the atomic actions “walking the bride” (red), “dancing” (light grey), “flower throwing” (orange), “cake cutting” (yellow) and “bride and groom traveling” (green). [sent-90, score-0.426]
</p><p>11 In the “feeding an animal” examples, only a small portion (red box) of the video actually depicts the action of handing food to an animal. [sent-93, score-0.265]
</p><p>12 an event can  recognition of primitive or atomic actions, such as “walking”, “’running”, from carefully assembled video, complex events depict human behaviors in unconstraint scenes, performing more sophisticated activities, which involve more complex interactions with the environment, e. [sent-95, score-1.008]
</p><p>13 Due to all these, the detection of complex events presents two major challenges beyond those commonly addressed in the action recognition literature. [sent-103, score-0.25]
</p><p>14 The first is that the video is usually not precisely segmented to include only the behaviors of interest. [sent-104, score-0.342]
</p><p>15 For example, as shown in Figure 1, while the event “feeding an animal” is mostly about the behavior of handing the animal food, a typical YouTube video in this class depicts a caretaker approaching the animal, playing with it, checking its health, etc. [sent-105, score-0.571]
</p><p>16 The second challenge is that the behaviors of interest can have a complex temporal structure. [sent-106, score-0.496]
</p><p>17 In general, a complex event can have multiple  such behaviors and these can appear with great variability of temporal configurations. [sent-107, score-0.824]
</p><p>18 For example, the “birthday party” and “wedding” events of Figure 1, have significant variation in the continuity, order, and duration of characteristic behaviors such as “walking the bride,” “dancing,” “flower throwing,” or “cake cutting”. [sent-108, score-0.523]
</p><p>19 One operation critical for its success is the pooling of visual features into a holistic video representation. [sent-110, score-0.521]
</p><p>20 However, while fixed pooling strategies, such as average pooling or temporal pyramid matching, are suitable for carefully manicured video, they have two strong limitations for complex event recognition. [sent-111, score-1.459]
</p><p>21 First, by integrating information in a pre-defined manner, they cannot adapt to the temporal structure of the behaviors of interest. [sent-112, score-0.47]
</p><p>22 Second, by pooling features from video regions that do not depict characteristic behaviors, they produce noisy histograms, where the feature counts due to characteristic behavior can be easily overwhelmed by those due to uninformative content. [sent-114, score-0.841]
</p><p>23 In this work, we address both limitations by proposing a pooling scheme adaptive to the temporal structure of the particular video to recognize. [sent-115, score-0.753]
</p><p>24 The video sequence is decom-  posed into segments, and the most informative segments for detection of a given event are identified, so as to dynamically determine the pooling operator most suited for that particular video sequence. [sent-116, score-1.27]
</p><p>25 This dynamic pooling is implemented by treating the locations of the characteristic segments as hidden information, which is inferred, on a sequence-by-sequence basis, via a large-margin classification rule with latent variables. [sent-117, score-0.951]
</p><p>26 In this way, only the portions of the video informative about the event of interest are used for its representation. [sent-119, score-0.469]
</p><p>27 The proposed pooling scheme can be seen either as 1) a discriminant form of segmentation and grouping, which eliminates histogram noise due to uninformative content, or 2) a discriminant approach to modeling video structure, which automatically identifies the locations of behaviors of interest. [sent-120, score-0.873]
</p><p>28 Besides the coarse-level location of segments, finer modeling of structure can be achieved by jointly pooling histograms of segment-tuples. [sent-122, score-0.489]
</p><p>29 This is akin to recent attempts at modeling the short-term temporal layout of simple actions [9], but relies on adaptively rather than manually specified video segments. [sent-123, score-0.41]
</p><p>30 Related Work  There has, so far, been limited work on pooling mechanisms for complex event detection. [sent-126, score-0.811]
</p><p>31 extend spatial pyramid matching to the video domain and propose a BoF temporal pyramid (BoF-TP) matching for atomic action recognition in movie clips [13]. [sent-128, score-0.664]
</p><p>32 use unsupervised clustering of image features to guide feature pooling at the image level [5]. [sent-130, score-0.417]
</p><p>33 Since these pooling schemes cannot 1) select informative video segments, or 2) model the temporal structure of the underlying activities, they have limited applicability to complex event modeling. [sent-131, score-1.184]
</p><p>34 While [10] assumes that the optimal spatial regions (receptive fields) for pooling descriptors of a given category are fixed, our work addresses content-driven pooling regions, dynamically or adaptively discovered on a sequence-by-sequence basis. [sent-133, score-0.895]
</p><p>35 Several works have addressed the modeling of temporal structure of human activities. [sent-134, score-0.264]
</p><p>36 In [21], Schindler and Gool show that simple actions can be recognized almost instantaneously, with a signature video segment less than 1 second long. [sent-139, score-0.334]
</p><p>37 , 1) ignoring the temporal structure within subsequences, 2) limiting the hypothesis space of video cropping to continuous subsequences (which precludes temporally disconnected subsequences that are potentially more discriminant for complex event recognition), and 3) limited 2729  ? [sent-143, score-1.039]
</p><p>38 ionsf“jumpinguwithboard” and “landing”,  which are mined out to represent the event either by segment or segment-pair pooling. [sent-193, score-0.42]
</p><p>39 We address this problem by proposing an efficient procedure to dynamically determine the most discriminant segments for video classification. [sent-196, score-0.375]
</p><p>40 The second class aims to factorize activities into sequences of atomic behaviors, and characterize their temporal dependencies [17, 8, 4, 23, 24, 14, 16]. [sent-197, score-0.521]
</p><p>41 [8] raise the semantics of the representation, explicitly characterizing activities as sequences of atomic actions (e. [sent-203, score-0.411]
</p><p>42 Li and Vasconcelos extend this idea by characterizing the dynamics of action attributes, using a binary dynamic system (BDS) to model trajectories of human activity in attribute space [14], and then to bag of words for attribute dynamics (BoWAD) [16]. [sent-206, score-0.304]
</p><p>43 Some drawbacks of these approaches include the need for manual 1) segmentation of activities into predefined atomic actions, or 2) annotation of training sets for learning attributes or atomic actions. [sent-207, score-0.498]
</p><p>44 Some automated methods have, however, been proposed for discovery of latent temporal structure. [sent-208, score-0.288]
</p><p>45 Most methods in this group assume that 1) the entire video sequence is well described by the associated label, and 2) video sequences are precisely cropped and aligned with activities of interest. [sent-211, score-0.418]
</p><p>46 Event Detection via Dynamic Pooling In this section we introduce a detector of complex events using dynamic pooling. [sent-214, score-0.277]
</p><p>47 Complex Events A complex event is defined as an event composed of sev-  eral local behaviors. [sent-217, score-0.722]
</p><p>48 A video sequence v is first divided into a series of short-term temporal segments S = {si}iτ=1, winthoic ah are edse onfot sehdo rat-totemrmic segments. [sent-218, score-0.515]
</p><p>49 By determining the composition of the subset S¯, h controls the temporal pooling of visual word counts. [sent-244, score-0.647]
</p><p>50 AS f,ix hed co hn implements a rstaalti pco pooling 2730  mechanism, e. [sent-245, score-0.417]
</p><p>51 In this work, we introduce a dynamic pooling operator, by making h a latent variable, adapted to each sequence so as to maximize classification accuracy. [sent-248, score-0.634]
</p><p>52 Prediction Rule A detector for event class c is implemented as d(v) = sign[fw (v)], where fw (v) is a linear predictor that quantifies the confidence with which v belongs to c. [sent-252, score-0.507]
</p><p>53 In this case, a, b are fixed hyperparameters, encoding prior knowledge on event structure. [sent-262, score-0.328]
</p><p>54 Hypothesis Space for Pooled Features In this section we discuss several possibilities for the hypothesis space of the proposed complex event detector. [sent-332, score-0.479]
</p><p>55 The fourth is an unconstrained selector h, which is a special temporally localized selector with window (1, τ). [sent-358, score-0.481]
</p><p>56 Structure of Pooled Features So far, we have assumed that the features xi of (1) are histogram of visual word counts of video segments si. [sent-363, score-0.351]
</p><p>57 The first consists of the sequence of atomic behaviors “car accelerates” and “car crashes”, corresponding to regular traffic accidents. [sent-367, score-0.457]
</p><p>58 In the absence of an explicit encoding of the temporal sequence of the atomic behaviors, the two events cannot be disambiguated. [sent-369, score-0.514]
</p><p>59 Another possibility is to extend the proposed pooling scheme to tuples of pooling regions. [sent-372, score-0.871]
</p><p>60 For example, dynamic pooling can be applied to segment pairs, by simply replacing the segment set S with S2 = {(si , sj ) |L1 ? [sent-373, score-0.672]
</p><p>61 For example, when L1 = L2 = 1, the pair pooling strategy is similar to the localized version of the t2 temporal pyramid matching scheme of [13], albeit with dynamically selected pooling windows. [sent-393, score-1.185]
</p><p>62 t hSe representation of [8], where activities are manually decomposed into three atomic actions. [sent-395, score-0.329]
</p><p>63 In this second learning stage, the hidden variable selector h of (1) is restricted to a continuous pooling window (CPW), producing a latent SVM of parameter wCPW. [sent-407, score-0.786]
</p><p>64 This parameter is next used to initialize the CCCP algorithm for learning a latent SVM of temporally localized window for single segment pooling (SSP), i. [sent-408, score-0.812]
</p><p>65 3%  pole-vault gymnastics-vault shot-put snatch clean-jerk javelin throw hammer throw discus throw diving-platform diving-springboard basketball-layup bowling tennis-serve  60. [sent-429, score-0.275]
</p><p>66 Finally, wSSP is used to initialize CCCP for learning a latent SVM of temporally localized pooling window with segment pair selection (SSP), i. [sent-517, score-0.812]
</p><p>67 Experiments Several experiments were conducted to evaluate the performance of the proposed event detector, using three datasets and a number of benchmark methods for activity or event recognition. [sent-521, score-0.742]
</p><p>68 Unless otherwise specified, all experiments relied on the popular spatio-temporal interest point (STIP) descriptor of [13], and parameters of dynamic  pooling were selected by cross-validation in the training set. [sent-523, score-0.488]
</p><p>69 While not really an open-source video collection (many of the sequences are extracted from sports broadcasts and depict a single well defined activity), this dataset is challenging for two main reasons: 1) some activities (e. [sent-527, score-0.356]
</p><p>70 , “tennis serve”, or “basketball layup”) have a variety of signature behaviors ofvariable location or duration, due to intra-class variability and poor segmentation/alignment; and 2) it contains pairs of confusing activities (e. [sent-529, score-0.454]
</p><p>71 , sub-types of a common category, such as the weight lifting activities of “snatch” and “clean-andjerk”), whose discrimination requires fine-grained models of temporal structure. [sent-531, score-0.352]
</p><p>72 Low-level features were extracted from video segments of 30-frames (with an overlap of 15frames) and quantized with a 4000-word codebook. [sent-532, score-0.273]
</p><p>73 Pooling Strategy We first evaluated the benefits of the various pooling structures of Section 4. [sent-534, score-0.417]
</p><p>74 The top of Figure 3 shows results for 4 structures: average pooling on the whole sequence (BoF), or on a continuous window (CW) (t, δ), temporally localized (TL) selector, and unconstrained (U) selector. [sent-535, score-0.709]
</p><p>75 The latter two were repeated for two feature configurations - single segments (SSP) and segment pairs (SPP) - for a total of 6 configurations. [sent-536, score-0.261]
</p><p>76 All dynamic pooling mech-  anisms outperformed BoF, with gains as high as 10%. [sent-537, score-0.52]
</p><p>77 The only exception was the U selector which, while beating BoF and CW, underperformed its temporally localized counterpart (TL). [sent-541, score-0.325]
</p><p>78 This suggests that it is important to rely on a flexible selector h, but it helps to localize the region from which segments are selected. [sent-542, score-0.306]
</p><p>79 With respect to features, pooling of segment pairs (SPP) substantially outperformed single segment pooling (SSP). [sent-543, score-1.05]
</p><p>80 This is intuitive, since the SPP representation accounts for long-term temporal video structure, which is important for the discrimination of similar activities (see discussion below). [sent-544, score-0.456]
</p><p>81 Given these observations, we adopted the TL pooling strategy in all remaining experiments. [sent-545, score-0.417]
</p><p>82 Modeling Temporal Structure We next compared the proposed detector to prior methods for modeling the temporal structure of complex activities. [sent-546, score-0.367]
</p><p>83 Keyframes ofthe characteristic segments are shown with their anchor points in the timeline. [sent-556, score-0.29]
</p><p>84 This suggests that there are two important components of activity representation: 1) the selection of signature segments depicting characteristic behaviors; and 2) the temporal structure of these behaviors. [sent-569, score-0.664]
</p><p>85 Note, in fact, that the prior models underperform even TL-SSP on categories with characteristic behaviors widely scattered across the video, e. [sent-573, score-0.394]
</p><p>86 This is illustrated in Figure 4, which shows the segments selected by TL-SSP for the activities “tennis-serve”, “basketball layup” and “bowling”. [sent-576, score-0.329]
</p><p>87 Note that, despite the large variability of location of the characteristic behaviors in the video of these cate-  gories, e. [sent-577, score-0.463]
</p><p>88 This ability is also quantified in Figure 3 by a small experiment, where we 1) manually annotated the characteristic behaviors of “bowling” and “tennis-serve”, and 2) compared this ground-truth to the video portion selected by TL-SSP. [sent-580, score-0.463]
</p><p>89 TRECVID-MED11 The second and third sets of experiments were conducted on the 2011 TRECVID multimedia event detection (MED) dataset [ 19]. [sent-589, score-0.328]
</p><p>90 It contains over 45, 000 videos of 15 high-level event classes (denoted “E001” to “E015”) collected from a variety of Internet resources. [sent-590, score-0.328]
</p><p>91 The training set (denoted “EC”), contains 100 to 200 ground-truth instances of each event class, totaling over 2000 videos. [sent-591, score-0.328]
</p><p>92 The large variation in temporal duration, scenes, illumination, cutting, resolution, etc in these video clips, together with the size of the negative class, make the detection task extremely difficult. [sent-595, score-0.332]
</p><p>93 To improve discriminative power, we implemented the feature mapping of [26] for dynamic pooling and the baseline BoF-TP of [13]. [sent-597, score-0.525]
</p><p>94 This is too much for approaches modeling holistic temporal structure like DMS [ 17], VD-HMM [23] and BDS [ 14], which significantly underperform the baseline BoF-TP. [sent-600, score-0.299]
</p><p>95 both the identification of characteristic segments and the  modeling of their temporal structure are important. [sent-674, score-0.554]
</p><p>96 Conclusion We proposed a joint framework for extracting characteristic behaviors, modeling temporal structure, and recognizing activity on video of complex events. [sent-678, score-0.601]
</p><p>97 It was shown that, under this formulation, efficient and exact inference for selection of signature video portion is possible over the combinatorial space of possible segment selections. [sent-679, score-0.252]
</p><p>98 An experimental comparison to various benchmarks for event detection, on challenging datasets, justified the effectiveness of the proposed approach. [sent-680, score-0.328]
</p><p>99 [6]  [7]  [8] [9] [10] [11]  [12] [13] [14] [15]  [16] [17]  [18] [19]  Scene aligned pooling for complex video recognition. [sent-712, score-0.587]
</p><p>100 Modeling temporal structure of decomposable motion segments for activity classification. [sent-796, score-0.487]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pooling', 0.417), ('event', 0.328), ('behaviors', 0.238), ('temporal', 0.192), ('segments', 0.169), ('atomic', 0.169), ('activities', 0.16), ('ssp', 0.158), ('bof', 0.152), ('selector', 0.137), ('characteristic', 0.121), ('olympic', 0.118), ('cccp', 0.11), ('spp', 0.105), ('video', 0.104), ('events', 0.103), ('latent', 0.096), ('segment', 0.092), ('wedding', 0.091), ('animal', 0.091), ('temporally', 0.087), ('activity', 0.086), ('actions', 0.082), ('action', 0.081), ('throwing', 0.078), ('party', 0.076), ('bowling', 0.074), ('birthday', 0.072), ('bds', 0.071), ('bride', 0.071), ('groom', 0.071), ('wdttxhh', 0.071), ('dynamic', 0.071), ('cutting', 0.068), ('complex', 0.066), ('trecvid', 0.065), ('devt', 0.063), ('dancing', 0.063), ('duration', 0.061), ('window', 0.061), ('dynamically', 0.061), ('localized', 0.059), ('dms', 0.059), ('fw', 0.057), ('signature', 0.056), ('cake', 0.055), ('sports', 0.054), ('throw', 0.053), ('subsequences', 0.05), ('sequence', 0.05), ('cw', 0.049), ('predictor', 0.048), ('crashes', 0.048), ('handing', 0.048), ('ilfp', 0.048), ('lfp', 0.048), ('subsequence', 0.047), ('hypothesis', 0.046), ('gaidon', 0.044), ('tl', 0.044), ('beating', 0.042), ('snatch', 0.042), ('yifw', 0.042), ('si', 0.042), ('receptive', 0.041), ('discriminant', 0.041), ('feeding', 0.041), ('programming', 0.04), ('clips', 0.04), ('structure', 0.04), ('counts', 0.04), ('flower', 0.04), ('hidden', 0.04), ('pyramid', 0.039), ('dtf', 0.039), ('devo', 0.039), ('ihi', 0.039), ('layup', 0.039), ('possibilities', 0.039), ('depict', 0.038), ('word', 0.038), ('youtube', 0.038), ('implemented', 0.037), ('detector', 0.037), ('informative', 0.037), ('united', 0.037), ('tuples', 0.037), ('etc', 0.036), ('parade', 0.035), ('vasconcelos', 0.035), ('underperform', 0.035), ('pooled', 0.035), ('continuous', 0.035), ('convex', 0.034), ('divakaran', 0.034), ('accelerates', 0.034), ('attribute', 0.033), ('modeling', 0.032), ('outperformed', 0.032), ('food', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000014 <a title="127-tfidf-1" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>Author: Weixin Li, Qian Yu, Ajay Divakaran, Nuno Vasconcelos</p><p>Abstract: The problem of adaptively selecting pooling regions for the classification of complex video events is considered. Complex events are defined as events composed of several characteristic behaviors, whose temporal configuration can change from sequence to sequence. A dynamic pooling operator is defined so as to enable a unified solution to the problems of event specific video segmentation, temporal structure modeling, and event detection. Video is decomposed into segments, and the segments most informative for detecting a given event are identified, so as to dynamically determine the pooling operator most suited for each sequence. This dynamic pooling is implemented by treating the locations of characteristic segments as hidden information, which is inferred, on a sequence-by-sequence basis, via a large-margin classification rule with latent variables. Although the feasible set of segment selections is combinatorial, it is shown that a globally optimal solution to the inference problem can be obtained efficiently, through the solution of a series of linear programs. Besides the coarselevel location of segments, a finer model of video struc- ture is implemented by jointly pooling features of segmenttuples. Experimental evaluation demonstrates that the re- sulting event detector has state-of-the-art performance on challenging video datasets.</p><p>2 0.34851846 <a title="127-tfidf-2" href="./iccv-2013-Modeling_4D_Human-Object_Interactions_for_Event_and_Object_Recognition.html">268 iccv-2013-Modeling 4D Human-Object Interactions for Event and Object Recognition</a></p>
<p>Author: Ping Wei, Yibiao Zhao, Nanning Zheng, Song-Chun Zhu</p><p>Abstract: Recognizing the events and objects in the video sequence are two challenging tasks due to the complex temporal structures and the large appearance variations. In this paper, we propose a 4D human-object interaction model, where the two tasks jointly boost each other. Our human-object interaction is defined in 4D space: i) the cooccurrence and geometric constraints of human pose and object in 3D space; ii) the sub-events transition and objects coherence in 1D temporal dimension. We represent the structure of events, sub-events and objects in a hierarchical graph. For an input RGB-depth video, we design a dynamic programming beam search algorithm to: i) segment the video, ii) recognize the events, and iii) detect the objects simultaneously. For evaluation, we built a large-scale multiview 3D event dataset which contains 3815 video sequences and 383,036 RGBD frames captured by the Kinect cameras. The experiment results on this dataset show the effectiveness of our method.</p><p>3 0.33531204 <a title="127-tfidf-3" href="./iccv-2013-Event_Detection_in_Complex_Scenes_Using_Interval_Temporal_Constraints.html">146 iccv-2013-Event Detection in Complex Scenes Using Interval Temporal Constraints</a></p>
<p>Author: Yifan Zhang, Qiang Ji, Hanqing Lu</p><p>Abstract: In complex scenes with multiple atomic events happening sequentially or in parallel, detecting each individual event separately may not always obtain robust and reliable result. It is essential to detect them in a holistic way which incorporates the causality and temporal dependency among them to compensate the limitation of current computer vision techniques. In this paper, we propose an interval temporal constrained dynamic Bayesian network to extendAllen ’s interval algebra network (IAN) [2]from a deterministic static model to a probabilistic dynamic system, which can not only capture the complex interval temporal relationships, but also model the evolution dynamics and handle the uncertainty from the noisy visual observation. In the model, the topology of the IAN on each time slice and the interlinks between the time slices are discovered by an advanced structure learning method. The duration of the event and the unsynchronized time lags between two correlated event intervals are captured by a duration model, so that we can better determine the temporal boundary of the event. Empirical results on two real world datasets show the power of the proposed interval temporal constrained model.</p><p>4 0.30466959 <a title="127-tfidf-4" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>Author: Arash Vahdat, Kevin Cannons, Greg Mori, Sangmin Oh, Ilseo Kim</p><p>Abstract: We present a compositional model for video event detection. A video is modeled using a collection of both global and segment-level features and kernel functions are employed for similarity comparisons. The locations of salient, discriminative video segments are treated as a latent variable, allowing the model to explicitly ignore portions of the video that are unimportant for classification. A novel, multiple kernel learning (MKL) latent support vector machine (SVM) is defined, that is used to combine and re-weight multiple feature types in a principled fashion while simultaneously operating within the latent variable framework. The compositional nature of the proposed model allows it to respond directly to the challenges of temporal clutter and intra-class variation, which are prevalent in unconstrained internet videos. Experimental results on the TRECVID Multimedia Event Detection 2011 (MED11) dataset demonstrate the efficacy of the method.</p><p>5 0.26736039 <a title="127-tfidf-5" href="./iccv-2013-Event_Recognition_in_Photo_Collections_with_a_Stopwatch_HMM.html">147 iccv-2013-Event Recognition in Photo Collections with a Stopwatch HMM</a></p>
<p>Author: Lukas Bossard, Matthieu Guillaumin, Luc Van_Gool</p><p>Abstract: The task of recognizing events in photo collections is central for automatically organizing images. It is also very challenging, because of the ambiguity of photos across different event classes and because many photos do not convey enough relevant information. Unfortunately, the field still lacks standard evaluation data sets to allow comparison of different approaches. In this paper, we introduce and release a novel data set of personal photo collections containing more than 61,000 images in 807 collections, annotated with 14 diverse social event classes. Casting collections as sequential data, we build upon recent and state-of-the-art work in event recognition in videos to propose a latent sub-event approach for event recognition in photo collections. However, photos in collections are sparsely sampled over time and come in bursts from which transpires the importance of specific moments for the photographers. Thus, we adapt a discriminative hidden Markov model to allow the transitions between states to be a function of the time gap between consecutive images, which we coin as Stopwatch Hidden Markov model (SHMM). In our experiments, we show that our proposed model outperforms approaches based only on feature pooling or a classical hidden Markov model. With an average accuracy of 56%, we also highlight the difficulty of the data set and the need for future advances in event recognition in photo collections.</p><p>6 0.24777967 <a title="127-tfidf-6" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>7 0.23767366 <a title="127-tfidf-7" href="./iccv-2013-ACTIVE%3A_Activity_Concept_Transitions_in_Video_Event_Classification.html">4 iccv-2013-ACTIVE: Activity Concept Transitions in Video Event Classification</a></p>
<p>8 0.23212855 <a title="127-tfidf-8" href="./iccv-2013-How_Related_Exemplars_Help_Complex_Event_Detection_in_Web_Videos%3F.html">203 iccv-2013-How Related Exemplars Help Complex Event Detection in Web Videos?</a></p>
<p>9 0.21346177 <a title="127-tfidf-9" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>10 0.20864011 <a title="127-tfidf-10" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>11 0.19787207 <a title="127-tfidf-11" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>12 0.17588505 <a title="127-tfidf-12" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>13 0.16998014 <a title="127-tfidf-13" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>14 0.16871215 <a title="127-tfidf-14" href="./iccv-2013-Feature_Weighting_via_Optimal_Thresholding_for_Video_Analysis.html">163 iccv-2013-Feature Weighting via Optimal Thresholding for Video Analysis</a></p>
<p>15 0.16727665 <a title="127-tfidf-15" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>16 0.1575563 <a title="127-tfidf-16" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>17 0.15645039 <a title="127-tfidf-17" href="./iccv-2013-Learning_to_Share_Latent_Tasks_for_Action_Recognition.html">249 iccv-2013-Learning to Share Latent Tasks for Action Recognition</a></p>
<p>18 0.1552114 <a title="127-tfidf-18" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>19 0.15505025 <a title="127-tfidf-19" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>20 0.15220237 <a title="127-tfidf-20" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.258), (1, 0.243), (2, 0.127), (3, 0.217), (4, 0.085), (5, 0.07), (6, 0.099), (7, -0.072), (8, -0.042), (9, -0.134), (10, -0.131), (11, -0.099), (12, -0.034), (13, 0.174), (14, -0.236), (15, -0.109), (16, 0.055), (17, 0.071), (18, 0.058), (19, 0.053), (20, 0.107), (21, -0.007), (22, -0.014), (23, -0.027), (24, -0.021), (25, 0.012), (26, -0.008), (27, 0.05), (28, 0.015), (29, 0.004), (30, -0.041), (31, -0.013), (32, -0.099), (33, 0.023), (34, -0.026), (35, 0.011), (36, -0.015), (37, -0.07), (38, 0.079), (39, -0.003), (40, 0.006), (41, 0.066), (42, -0.004), (43, 0.014), (44, -0.023), (45, 0.067), (46, -0.01), (47, 0.045), (48, -0.008), (49, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96970582 <a title="127-lsi-1" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>Author: Weixin Li, Qian Yu, Ajay Divakaran, Nuno Vasconcelos</p><p>Abstract: The problem of adaptively selecting pooling regions for the classification of complex video events is considered. Complex events are defined as events composed of several characteristic behaviors, whose temporal configuration can change from sequence to sequence. A dynamic pooling operator is defined so as to enable a unified solution to the problems of event specific video segmentation, temporal structure modeling, and event detection. Video is decomposed into segments, and the segments most informative for detecting a given event are identified, so as to dynamically determine the pooling operator most suited for each sequence. This dynamic pooling is implemented by treating the locations of characteristic segments as hidden information, which is inferred, on a sequence-by-sequence basis, via a large-margin classification rule with latent variables. Although the feasible set of segment selections is combinatorial, it is shown that a globally optimal solution to the inference problem can be obtained efficiently, through the solution of a series of linear programs. Besides the coarselevel location of segments, a finer model of video struc- ture is implemented by jointly pooling features of segmenttuples. Experimental evaluation demonstrates that the re- sulting event detector has state-of-the-art performance on challenging video datasets.</p><p>2 0.86685169 <a title="127-lsi-2" href="./iccv-2013-Modeling_4D_Human-Object_Interactions_for_Event_and_Object_Recognition.html">268 iccv-2013-Modeling 4D Human-Object Interactions for Event and Object Recognition</a></p>
<p>Author: Ping Wei, Yibiao Zhao, Nanning Zheng, Song-Chun Zhu</p><p>Abstract: Recognizing the events and objects in the video sequence are two challenging tasks due to the complex temporal structures and the large appearance variations. In this paper, we propose a 4D human-object interaction model, where the two tasks jointly boost each other. Our human-object interaction is defined in 4D space: i) the cooccurrence and geometric constraints of human pose and object in 3D space; ii) the sub-events transition and objects coherence in 1D temporal dimension. We represent the structure of events, sub-events and objects in a hierarchical graph. For an input RGB-depth video, we design a dynamic programming beam search algorithm to: i) segment the video, ii) recognize the events, and iii) detect the objects simultaneously. For evaluation, we built a large-scale multiview 3D event dataset which contains 3815 video sequences and 383,036 RGBD frames captured by the Kinect cameras. The experiment results on this dataset show the effectiveness of our method.</p><p>3 0.86066931 <a title="127-lsi-3" href="./iccv-2013-Event_Detection_in_Complex_Scenes_Using_Interval_Temporal_Constraints.html">146 iccv-2013-Event Detection in Complex Scenes Using Interval Temporal Constraints</a></p>
<p>Author: Yifan Zhang, Qiang Ji, Hanqing Lu</p><p>Abstract: In complex scenes with multiple atomic events happening sequentially or in parallel, detecting each individual event separately may not always obtain robust and reliable result. It is essential to detect them in a holistic way which incorporates the causality and temporal dependency among them to compensate the limitation of current computer vision techniques. In this paper, we propose an interval temporal constrained dynamic Bayesian network to extendAllen ’s interval algebra network (IAN) [2]from a deterministic static model to a probabilistic dynamic system, which can not only capture the complex interval temporal relationships, but also model the evolution dynamics and handle the uncertainty from the noisy visual observation. In the model, the topology of the IAN on each time slice and the interlinks between the time slices are discovered by an advanced structure learning method. The duration of the event and the unsynchronized time lags between two correlated event intervals are captured by a duration model, so that we can better determine the temporal boundary of the event. Empirical results on two real world datasets show the power of the proposed interval temporal constrained model.</p><p>4 0.82529765 <a title="127-lsi-4" href="./iccv-2013-ACTIVE%3A_Activity_Concept_Transitions_in_Video_Event_Classification.html">4 iccv-2013-ACTIVE: Activity Concept Transitions in Video Event Classification</a></p>
<p>Author: Chen Sun, Ram Nevatia</p><p>Abstract: The goal of high level event classification from videos is to assign a single, high level event label to each query video. Traditional approaches represent each video as a set of low level features and encode it into a fixed length feature vector (e.g. Bag-of-Words), which leave a big gap between low level visual features and high level events. Our paper tries to address this problem by exploiting activity concept transitions in video events (ACTIVE). A video is treated as a sequence of short clips, all of which are observations corresponding to latent activity concept variables in a Hidden Markov Model (HMM). We propose to apply Fisher Kernel techniques so that the concept transitions over time can be encoded into a compact and fixed length feature vector very efficiently. Our approach can utilize concept annotations from independent datasets, and works well even with a very small number of training samples. Experiments on the challenging NIST TRECVID Multimedia Event Detection (MED) dataset shows our approach performs favorably over the state-of-the-art.</p><p>5 0.82296002 <a title="127-lsi-5" href="./iccv-2013-How_Related_Exemplars_Help_Complex_Event_Detection_in_Web_Videos%3F.html">203 iccv-2013-How Related Exemplars Help Complex Event Detection in Web Videos?</a></p>
<p>Author: Yi Yang, Zhigang Ma, Zhongwen Xu, Shuicheng Yan, Alexander G. Hauptmann</p><p>Abstract: Compared to visual concepts such as actions, scenes and objects, complex event is a higher level abstraction of longer video sequences. For example, a “marriage proposal” event is described by multiple objects (e.g., ring, faces), scenes (e.g., in a restaurant, outdoor) and actions (e.g., kneeling down). The positive exemplars which exactly convey the precise semantic of an event are hard to obtain. It would be beneficial to utilize the related exemplars for complex event detection. However, the semantic correlations between related exemplars and the target event vary substantially as relatedness assessment is subjective. Two related exemplars can be about completely different events, e.g., in the TRECVID MED dataset, both bicycle riding and equestrianism are labeled as related to “attempting a bike trick” event. To tackle the subjectiveness of human assessment, our algorithm automatically evaluates how positive the related exemplars are for the detection of an event and uses them on an exemplar-specific basis. Experiments demonstrate that our algorithm is able to utilize related exemplars adaptively, and the algorithm gains good perform- z. ance for complex event detection.</p><p>6 0.78165126 <a title="127-lsi-6" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>7 0.76177764 <a title="127-lsi-7" href="./iccv-2013-Event_Recognition_in_Photo_Collections_with_a_Stopwatch_HMM.html">147 iccv-2013-Event Recognition in Photo Collections with a Stopwatch HMM</a></p>
<p>8 0.75429708 <a title="127-lsi-8" href="./iccv-2013-Feature_Weighting_via_Optimal_Thresholding_for_Video_Analysis.html">163 iccv-2013-Feature Weighting via Optimal Thresholding for Video Analysis</a></p>
<p>9 0.66203445 <a title="127-lsi-9" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>10 0.6125049 <a title="127-lsi-10" href="./iccv-2013-Facial_Action_Unit_Event_Detection_by_Cascade_of_Tasks.html">155 iccv-2013-Facial Action Unit Event Detection by Cascade of Tasks</a></p>
<p>11 0.59521097 <a title="127-lsi-11" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>12 0.53322572 <a title="127-lsi-12" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>13 0.52751541 <a title="127-lsi-13" href="./iccv-2013-A_New_Adaptive_Segmental_Matching_Measure_for_Human_Activity_Recognition.html">22 iccv-2013-A New Adaptive Segmental Matching Measure for Human Activity Recognition</a></p>
<p>14 0.52645797 <a title="127-lsi-14" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<p>15 0.52560425 <a title="127-lsi-15" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>16 0.51328439 <a title="127-lsi-16" href="./iccv-2013-Learning_Slow_Features_for_Behaviour_Analysis.html">243 iccv-2013-Learning Slow Features for Behaviour Analysis</a></p>
<p>17 0.50845194 <a title="127-lsi-17" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>18 0.50756949 <a title="127-lsi-18" href="./iccv-2013-Abnormal_Event_Detection_at_150_FPS_in_MATLAB.html">34 iccv-2013-Abnormal Event Detection at 150 FPS in MATLAB</a></p>
<p>19 0.50354999 <a title="127-lsi-19" href="./iccv-2013-Action_Recognition_with_Actons.html">38 iccv-2013-Action Recognition with Actons</a></p>
<p>20 0.49237102 <a title="127-lsi-20" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.057), (4, 0.016), (7, 0.013), (12, 0.036), (26, 0.092), (31, 0.058), (34, 0.012), (35, 0.012), (40, 0.016), (42, 0.093), (64, 0.089), (68, 0.014), (73, 0.026), (77, 0.023), (78, 0.022), (80, 0.145), (89, 0.183), (95, 0.011), (98, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88999379 <a title="127-lda-1" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>Author: Weixin Li, Qian Yu, Ajay Divakaran, Nuno Vasconcelos</p><p>Abstract: The problem of adaptively selecting pooling regions for the classification of complex video events is considered. Complex events are defined as events composed of several characteristic behaviors, whose temporal configuration can change from sequence to sequence. A dynamic pooling operator is defined so as to enable a unified solution to the problems of event specific video segmentation, temporal structure modeling, and event detection. Video is decomposed into segments, and the segments most informative for detecting a given event are identified, so as to dynamically determine the pooling operator most suited for each sequence. This dynamic pooling is implemented by treating the locations of characteristic segments as hidden information, which is inferred, on a sequence-by-sequence basis, via a large-margin classification rule with latent variables. Although the feasible set of segment selections is combinatorial, it is shown that a globally optimal solution to the inference problem can be obtained efficiently, through the solution of a series of linear programs. Besides the coarselevel location of segments, a finer model of video struc- ture is implemented by jointly pooling features of segmenttuples. Experimental evaluation demonstrates that the re- sulting event detector has state-of-the-art performance on challenging video datasets.</p><p>2 0.87587011 <a title="127-lda-2" href="./iccv-2013-Fix_Structured_Learning_of_2013_ICCV_paper_k2opt.pdf.html">171 iccv-2013-Fix Structured Learning of 2013 ICCV paper k2opt.pdf</a></p>
<p>Author: empty-author</p><p>Abstract: Submodular functions can be exactly minimized in polynomial time, and the special case that graph cuts solve with max flow [19] has had significant impact in computer vision [5, 21, 28]. In this paper we address the important class of sum-of-submodular (SoS) functions [2, 18], which can be efficiently minimized via a variant of max flow called submodular flow [6]. SoS functions can naturally express higher order priors involving, e.g., local image patches; however, it is difficult to fully exploit their expressive power because they have so many parameters. Rather than trying to formulate existing higher order priors as an SoS function, we take a discriminative learning approach, effectively searching the space of SoS functions for a higher order prior that performs well on our training set. We adopt a structural SVM approach [15, 34] and formulate the training problem in terms of quadratic programming; as a result we can efficiently search the space of SoS priors via an extended cutting-plane algorithm. We also show how the state-of-the-art max flow method for vision problems [11] can be modified to efficiently solve the submodular flow problem. Experimental comparisons are made against the OpenCVimplementation ofthe GrabCut interactive seg- mentation technique [28], which uses hand-tuned parameters instead of machine learning. On a standard dataset [12] our method learns higher order priors with hundreds of parameter values, and produces significantly better segmentations. While our focus is on binary labeling problems, we show that our techniques can be naturally generalized to handle more than two labels.</p><p>3 0.87413526 <a title="127-lda-3" href="./iccv-2013-Face_Recognition_via_Archetype_Hull_Ranking.html">154 iccv-2013-Face Recognition via Archetype Hull Ranking</a></p>
<p>Author: Yuanjun Xiong, Wei Liu, Deli Zhao, Xiaoou Tang</p><p>Abstract: The archetype hull model is playing an important role in large-scale data analytics and mining, but rarely applied to vision problems. In this paper, we migrate such a geometric model to address face recognition and verification together through proposing a unified archetype hull ranking framework. Upon a scalable graph characterized by a compact set of archetype exemplars whose convex hull encompasses most of the training images, the proposed framework explicitly captures the relevance between any query and the stored archetypes, yielding a rank vector over the archetype hull. The archetype hull ranking is then executed on every block of face images to generate a blockwise similarity measure that is achieved by comparing two different rank vectors with respect to the same archetype hull. After integrating blockwise similarity measurements with learned importance weights, we accomplish a sensible face similarity measure which can support robust and effective face recognition and verification. We evaluate the face similarity measure in terms of experiments performed on three benchmark face databases Multi-PIE, Pubfig83, and LFW, demonstrat- ing its performance superior to the state-of-the-arts.</p><p>4 0.85375255 <a title="127-lda-4" href="./iccv-2013-Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization.html">284 iccv-2013-Multiview Photometric Stereo Using Planar Mesh Parameterization</a></p>
<p>Author: Jaesik Park, Sudipta N. Sinha, Yasuyuki Matsushita, Yu-Wing Tai, In So Kweon</p><p>Abstract: We propose a method for accurate 3D shape reconstruction using uncalibrated multiview photometric stereo. A coarse mesh reconstructed using multiview stereo is first parameterized using a planar mesh parameterization technique. Subsequently, multiview photometric stereo is performed in the 2D parameter domain of the mesh, where all geometric and photometric cues from multiple images can be treated uniformly. Unlike traditional methods, there is no need for merging view-dependent surface normal maps. Our key contribution is a new photometric stereo based mesh refinement technique that can efficiently reconstruct meshes with extremely fine geometric details by directly estimating a displacement texture map in the 2D parameter domain. We demonstrate that intricate surface geometry can be reconstructed using several challenging datasets containing surfaces with specular reflections, multiple albedos and complex topologies.</p><p>5 0.84319198 <a title="127-lda-5" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>Author: Qinxun Bai, Zheng Wu, Stan Sclaroff, Margrit Betke, Camille Monnier</p><p>Abstract: We propose a randomized ensemble algorithm to model the time-varying appearance of an object for visual tracking. In contrast with previous online methods for updating classifier ensembles in tracking-by-detection, the weight vector that combines weak classifiers is treated as a random variable and the posterior distribution for the weight vector is estimated in a Bayesian manner. In essence, the weight vector is treated as a distribution that reflects the confidence among the weak classifiers used to construct and adapt the classifier ensemble. The resulting formulation models the time-varying discriminative ability among weak classifiers so that the ensembled strong classifier can adapt to the varying appearance, backgrounds, and occlusions. The formulation is tested in a tracking-by-detection implementation. Experiments on 28 challenging benchmark videos demonstrate that the proposed method can achieve results comparable to and often better than those of stateof-the-art approaches.</p><p>6 0.84091389 <a title="127-lda-6" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>7 0.84065175 <a title="127-lda-7" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>8 0.83776152 <a title="127-lda-8" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>9 0.83664209 <a title="127-lda-9" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>10 0.83635974 <a title="127-lda-10" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<p>11 0.83621138 <a title="127-lda-11" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>12 0.83559424 <a title="127-lda-12" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>13 0.83533633 <a title="127-lda-13" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>14 0.83516788 <a title="127-lda-14" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>15 0.83425009 <a title="127-lda-15" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>16 0.83372962 <a title="127-lda-16" href="./iccv-2013-Event_Detection_in_Complex_Scenes_Using_Interval_Temporal_Constraints.html">146 iccv-2013-Event Detection in Complex Scenes Using Interval Temporal Constraints</a></p>
<p>17 0.8337152 <a title="127-lda-17" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>18 0.83307374 <a title="127-lda-18" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>19 0.8329531 <a title="127-lda-19" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>20 0.83253956 <a title="127-lda-20" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
