<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-89" href="#">iccv2013-89</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</h1>
<br/><p>Source: <a title="iccv-2013-89-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Chen_Constructing_Adaptive_Complex_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Dapeng Chen, Zejian Yuan, Yang Wu, Geng Zhang, Nanning Zheng</p><p>Abstract: Representation is a fundamental problem in object tracking. Conventional methods track the target by describing its local or global appearance. In this paper we present that, besides the two paradigms, the composition of local region histograms can also provide diverse and important object cues. We use cells to extract local appearance, and construct complex cells to integrate the information from cells. With different spatial arrangements of cells, complex cells can explore various contextual information at multiple scales, which is important to improve the tracking performance. We also develop a novel template-matching algorithm for object tracking, where the template is composed of temporal varying cells and has two layers to capture the target and background appearance respectively. An adaptive weight is associated with each complex cell to cope with occlusion as well as appearance variation. A fusion weight is associated with each complex cell type to preserve the global distinctiveness. Our algorithm is evaluated on 25 challenging sequences, and the results not only confirm the contribution of each component in our tracking system, but also outperform other competing trackers.</p><p>Reference: <a title="iccv-2013-89-reference" href="../iccv2013_reference/iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We use cells to extract local appearance, and construct complex cells to integrate the information from cells. [sent-4, score-1.179]
</p><p>2 With different spatial arrangements of cells, complex cells can explore various contextual information at multiple scales, which is important to improve the tracking performance. [sent-5, score-0.872]
</p><p>3 We also develop a novel template-matching algorithm for object tracking, where the template is composed of temporal varying cells and has two layers to capture the target and background appearance respectively. [sent-6, score-0.92]
</p><p>4 An adaptive weight is associated with each complex cell to cope with occlusion as well as appearance variation. [sent-7, score-0.825]
</p><p>5 A fusion weight is associated with each complex cell type to preserve the global distinctiveness. [sent-8, score-0.735]
</p><p>6 To overcome these challenges, a representation should be robust enough to identify the object under motion deformation, while at the same time, the representation should also be distinctive enough to differentiate the target from background clutters. [sent-13, score-0.24]
</p><p>7 These cells spread over regular grids covering both object region and neighbouring background. [sent-36, score-0.629]
</p><p>8 To obtain the global distinctiveness, we integrate specific cells to construct complex cells, which can explore multiple contextual information. [sent-37, score-0.705]
</p><p>9 According to different spatial arrangements of cells, the complex cells are categorized into four types that encode the object dependencies from local region, block neighbourhood, inter-region relations and surrounding background respectively. [sent-38, score-0.856]
</p><p>10 For object tracking, we develop a novel template representation and an efficient matching algorithm. [sent-41, score-0.234]
</p><p>11 The template is composed of temporal varying cells and has two layers that store the appearance of the target and background respectively. [sent-42, score-0.888]
</p><p>12 In greater detail, the cells are modeled as Gaussian distribution according to their temporal variation and the two-layer template is convenient for context exploitation as well as occlusion inference. [sent-43, score-0.817]
</p><p>13 We track the object by matching the complex cells from candidates with those  from the template. [sent-44, score-0.757]
</p><p>14 11 11 1133  One weight is associated with each complex cell to cope with occlusion as well as appearance variation, and the other weight is associated with complex cell type to preserve global distinctiveness. [sent-46, score-1.402]
</p><p>15 As the combination of complex cells form a score field with desirable heuristic cues, we utilize a coarse-to-fine search strategy, leading to a more accurate and efficient object localization. [sent-47, score-0.739]
</p><p>16 we develop a novel two-layer template for object tracking, wevheilcohp naonto only wmoo-ldayelesr ttheem temporal varying appearance of both target and background but also encodes spatial-temporal cues for occlusion inference and stability analysis. [sent-49, score-0.619]
</p><p>17 we evaluate the effectiveness ofindividual components owfe tehvea proposed tfrfeaccktievre on s25o challenging sequences, and demonstrate that the complex cells are the major force to boost the performance. [sent-50, score-0.679]
</p><p>18 Recently, psychophysical studies indicate that generic object tracking might be implemented in a low level neural mechanism [19], and then we propose a template-based tracking method without a complicated high level object model. [sent-55, score-0.332]
</p><p>19 In addition, Haar-like features [27] and binary test based descriptors [21] are also employed by many competing trackers [7, 30, 2, 14, 5], as they are computational efficient and can capture large object structures. [sent-60, score-0.26]
</p><p>20 Inspired by the merits of aforementioned methods, our complex cells integrate local histograms through several simple operations. [sent-61, score-0.708]
</p><p>21 Bs aase thdr on a bounding l b voexc,t we cndonicsatrtiuncgt a hierarchical representation architecture, where cells are the bases and complex cells are constructed upon the cells. [sent-73, score-1.237]
</p><p>22 Among them, the cells inside the target are called inner cells while the others are called outer cells. [sent-80, score-1.253]
</p><p>23 of inner cell and oWuete rd ecneollt respectively, dw Lith Lall = Lin ? [sent-82, score-0.542]
</p><p>24 The descriptor for cell lis a 16 dimensional vector obtained by concatenating the two histograms, denoted as hl (xt). [sent-89, score-0.504]
</p><p>25 We use histogram to describe each cell because it can characterize local structures well and is robust to local motion deformation. [sent-90, score-0.431]
</p><p>26 Complex Cells A complex cell is composed of a group of cells. [sent-94, score-0.61]
</p><p>27 We introduce two basic operators to describe the complex cells, where merge maintains the histogram sum of participating cells, while contrast calculates the histogram difference for a selected cell pair. [sent-95, score-0.638]
</p><p>28 The cells composite to form complex cells, and  different complex cells  corporate to  form the final  score. [sent-103, score-1.358]
</p><p>29 cell compositions and different operators, we propose four kinds of complex cells, also displayed in Fig. [sent-104, score-0.61]
</p><p>30 Local Complex Cell (LCC) is constructed by a single inner cell directly, and its descriptor is just the L2-norm normalized cell descriptor. [sent-106, score-1.014]
</p><p>31 Block Complex Cell (BCC) takes neighbouring 2 2 cells tBol represent larger region oCf) )th taek object, ahbndo uitrisn descriptor liss the merge of the cells. [sent-109, score-0.638]
</p><p>32 Non-local Complex Cell (NCC) is composed of a randomly selected inner cell pair, and its descriptor is the contrast of the cell pair. [sent-114, score-1.014]
</p><p>33 Background-Contrast Complex Cell (CCC) is composed of a neighbouring inner-outer cell pair, and its descriptor is the contrast of the two cells. [sent-120, score-0.569]
</p><p>34 It delivers two-fold benefits: (1) It highlights target contours, which are salient cues of the target; (2) It exploits the spatial correlations between a target and its neighbouring background , which in turn serves for localization. [sent-122, score-0.37]
</p><p>35 Template A two-layer template is proposed to represent the target and background information separately. [sent-125, score-0.332]
</p><p>36 The target template Tta is corresponding to inner cells, while the background template Tbg is corresponding to both inner and outer cells as the inner cells may be occupied by background. [sent-126, score-1.893]
</p><p>37 Specifically, each bin of a cell descriptor is modeled as a single Gaussian, then the cell descriptor is a 16 dimensional Gaussian with mean μ and variance D, where μ describes the local appearance, and D reflects its temporal variance. [sent-128, score-0.97]
</p><p>38 For simplicity, the bins of the cell descriptor are assumed to be independently distributed, therefore D is a diagonal matrix. [sent-129, score-0.472]
</p><p>39 We use  μ  Tbg = {μblg, Dblg|l ∈ Lall} (1) as the cell descriptors for tem-  plate, and? [sent-132, score-0.47]
</p><p>40 take the inner cells from target template and the outer cells from background template to construct the complex cells. [sent-133, score-1.85]
</p><p>41 The complex cell descriptors are generated according to Sec. [sent-134, score-0.649]
</p><p>42 Adaptive Complex Cell based Tracker We develop a novel template-based tracking algorithm to exhibit the superiorities of proposed complex cells. [sent-138, score-0.313]
</p><p>43 αm is the fusion weight associated with each complex cell type, while wj is the adaptive weight associated with each complex cell. [sent-148, score-0.98]
</p><p>44 M = {L, B, N, C} are the indexes for complex cell types, and J{Lm, are t,hCe complex icnedlle xinedse fxoers c ofomr a specific type m. [sent-149, score-0.822]
</p><p>45 C(xt) and CT are complex cell descriptors for xt and template T respectively. [sent-150, score-0.942]
</p><p>46 Suppose f and g are the corresponding complex cell descriptors, function k integrates the two channel features by a linear combination:  11 11 1155  k(f, g) = ? [sent-160, score-0.64]
</p><p>47 (3)  The results of function k have different ranges depending on the complex cell type. [sent-164, score-0.61]
</p><p>48 The appearance variation reflects the inner changes from the object itself, while occlusion is related to the surrounding background. [sent-169, score-0.288]
</p><p>49 To reduce the influence of the two factors, we focus more on stable complex cells and exclude occluded complex cells. [sent-171, score-0.92]
</p><p>50 Jm  where sj , oj are the stability factor and occlusion factor associated with complex cell j. [sent-175, score-0.891]
</p><p>51 Lj  (5)  Lj indexes the subcells of complex cell j. [sent-181, score-0.61]
</p><p>52 Note that diffLerent complex cells share the same weighting factors from cells so that wj can be efficiently computed. [sent-182, score-1.254]
</p><p>53 Stability Spatial stable parts within or around a target are important for tracking because they provide more reliable  evidence to predict the target state. [sent-183, score-0.347]
</p><p>54 The stability of cell l can be directly reflected by the template variance Dl . [sent-184, score-0.691]
</p><p>55 In general, a smaller Tr(Dl) corresponds to a more stable cell l(Tr is the trace of a matrix), therefore the stability factors for inner and outer cells can be calculated as:  sl=? [sent-185, score-1.253]
</p><p>56 o necessary, because it can alleviate the template deterioration and can use valid complex cells for accurate tracking. [sent-191, score-0.855]
</p><p>57 Assuming background is consistent in neighbouring cells, we determine if an inner cell is covered by the background through evaluating its affinity to neighbouring background cells. [sent-193, score-0.934]
</p><p>58 Let ol be a binary indicator associated with cell l, if a cell is occupied by the background, ol = 0, otherwise ol = 1. [sent-194, score-1.12]
</p><p>59 Suppose cell j is aeddj abcaesnetd t oon nc tehlle el ,c uwrree onntl yop pctihmanalge s ttahtee o ? [sent-197, score-0.431]
</p><p>60 The timevarying curve of the fusing weights α for the four types of complex cells, where the fusion weights automatically adjust to different challenges. [sent-226, score-0.443]
</p><p>61 its affinities with the neighbouring background template and the affinity with the its target template. [sent-228, score-0.429]
</p><p>62 We occlude the cellwhen it is more similar to the neighbouring background cell, and de-occlude the cell when it is similar to its target template again (θocc = 1. [sent-229, score-0.86]
</p><p>63 Once the cell l is changed to be occluded, we initialize its background template with a Gaussian (hl ( x? [sent-232, score-0.673]
</p><p>64 ) If more than 60% of the inner cells are occluded, we de-occlude all the cells. [sent-236, score-0.611]
</p><p>65 (2) If an inner cell is occluded for more than 15 frames, we de-occluded the cell. [sent-237, score-0.571]
</p><p>66 Fusion Weights The fusion weights  α  balance between different com-  plex cell types to preserve global distinctiveness. [sent-240, score-0.663]
</p><p>67 j∈Jm wjlj (C(xt), CT) is the score for mtype complex ce? [sent-243, score-0.247]
</p><p>68 With fusion weight αm, we can weight more on distinctive complex cell types, which are less prone to be confounded by the background and improve the global distinctiveness of object model. [sent-247, score-0.836]
</p><p>69 The four types of complex cells are complementary for both representation and optimal state estimation, see Fig. [sent-248, score-0.789]
</p><p>70 Since complex cells of different types are responsible for different structures, when a certain challenge happens, some types will degenerate their discriminate abilities, while other types are still be distinctive. [sent-250, score-0.817]
</p><p>71 Besides, combining the complex cells with different receptive field forms a score distribution with “high peak” and “heavy tail”, which is desirable for a heuristical search strategy . [sent-252, score-0.753]
</p><p>72 Updating with Occlusion As cell descriptors in Tta and Tbg are modeled as Gaussian distribution, we incrementally update the parameters (μtla, Dtla) and (μblg, Dblg) by current cell descriptor (xt) , which is also modeled as a Gaussian distribution , D0). [sent-268, score-0.942]
</p><p>73 The most computationally expensive procedures are the extraction of the cell descriptors and the computation of score values. [sent-312, score-0.498]
</p><p>74 The configuration of the cells depends on the shape of the initial bounding box, where the number of inner cells is around 25 and the outer cells are generated around the bounding box. [sent-313, score-1.727]
</p><p>75 The success plot and the precision plot for trackers with different complex cells and different adaptive weights. [sent-372, score-0.961]
</p><p>76 Center error plots of typical samples to explicit the properties of each type of complex cells. [sent-549, score-0.271]
</p><p>77 The average VORs and CLEs of constructed trackers with different complex cells. [sent-551, score-0.368]
</p><p>78 71  and CLEs of trackers with different adaptive weights. [sent-571, score-0.255]
</p><p>79 The average VORs and CLEs of the trackers with and without fusion weights. [sent-581, score-0.281]
</p><p>80 Analysis of our Method Performance of complex cells We investigate the properties of complex cells by building the trackers L-T, B-T, N-T, C-T based on the four different types of complex cell independently. [sent-588, score-2.203]
</p><p>81 We also verify the necessity of each complex cell by constructing the L∗-T, B∗-T, N∗-T, C∗-T which cast the corresponding complex cells away from CCT. [sent-589, score-1.289]
</p><p>82 The Success plots and Precision plots of the trackers over these frames are reported in Fig. [sent-593, score-0.307]
</p><p>83 We found that the tracking performance is significantly improved by the combination of different complex cells. [sent-598, score-0.313]
</p><p>84 The more types of complex cells the tracking system integrates, the better performance it achieves, see Fig. [sent-599, score-0.859]
</p><p>85 If we discard either complex cell from CCT, the overall performance will decrease. [sent-606, score-0.61]
</p><p>86 Here, we investigate the performance of the each type of complex cells one by one. [sent-607, score-0.712]
</p><p>87 The success plots and the precision plots for investigating the effect of fusion weights and for the comparison of different algorithms respectively. [sent-659, score-0.3]
</p><p>88 Performance of adaptive weights To verify the effectiveness of adaptive weights w, we also construct three trackers S∗-T, O∗-T, OS∗-T that drop the stability weights s, occlusion weights o, and the two weights from CCT, re-  spectively. [sent-665, score-0.808]
</p><p>89 3(c)-(d), where the results demonstrate that weighting the complex cells with occlusion and stability factors can cooperatively improve the tracking performance. [sent-668, score-1.027]
</p><p>90 Occlusion weights force CCT only use un-occluded cell to track the object, and they protect the occluded content from updating the background. [sent-670, score-0.569]
</p><p>91 We display stability weights and occlusion masks for some representative frames in Fig. [sent-671, score-0.235]
</p><p>92 Performance of fusion weights To justify the effectiveness of adaptive weights α, we construct a tracker α∗-T ignoring the fusion weights α, which combines the score of four types of complex cells equally. [sent-673, score-1.262]
</p><p>93 Although their contribution is not as significant as other components, they provide a reasonable way to balance between difference complex cell types. [sent-677, score-0.61]
</p><p>94 Empirical comparison of other trackers We compare CCT with eight competing trackers named Semi [6], OAB [5], MIL [2], TLD [14], CT [30], LSHT [8], ASLA [13] and Struck [7]. [sent-680, score-0.378]
</p><p>95 Different from other trackers that may severely fail on certain types of videos, CCT tracks well on almost all the listed data. [sent-688, score-0.235]
</p><p>96 Furthermore, if we only use a single type of complex cells (compare Fig. [sent-690, score-0.712]
</p><p>97 7 (c)-(d) ), the performance may be similar to or even worse than other existing methods, which again confirms the importance of complex cell combination. [sent-692, score-0.61]
</p><p>98 We constructed complex cells from local descriptors to represent multiple scale and multiple contextual object information. [sent-695, score-0.776]
</p><p>99 Equipped with a two-layer template, the complex cells were further weighted by adaptive weights and fusion weights to cope 2 For ASLA, we evaluate them using a fixed motion model as [29]. [sent-696, score-0.994]
</p><p>100 Experiments over 25 sequences confirmed the complementarity between different complex cells and showed that the combination of them would significantly improve the tracking performance. [sent-701, score-0.847]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cells', 0.5), ('cell', 0.431), ('trackers', 0.189), ('bcc', 0.18), ('complex', 0.179), ('template', 0.176), ('cct', 0.164), ('ncc', 0.141), ('vors', 0.14), ('tracking', 0.134), ('ccc', 0.12), ('lcc', 0.118), ('xt', 0.117), ('inner', 0.111), ('cles', 0.098), ('neighbouring', 0.097), ('fusion', 0.092), ('target', 0.09), ('occlusion', 0.088), ('ol', 0.086), ('stability', 0.084), ('tta', 0.082), ('ave', 0.079), ('vor', 0.077), ('tbg', 0.071), ('tracker', 0.07), ('background', 0.066), ('adaptive', 0.066), ('oj', 0.065), ('weights', 0.063), ('tla', 0.062), ('bg', 0.061), ('blg', 0.06), ('dblg', 0.06), ('dlta', 0.06), ('dtla', 0.06), ('plots', 0.059), ('lta', 0.053), ('outer', 0.052), ('receptive', 0.046), ('track', 0.046), ('types', 0.046), ('prost', 0.044), ('sj', 0.044), ('cle', 0.042), ('factors', 0.042), ('descriptor', 0.041), ('ta', 0.04), ('asla', 0.04), ('lall', 0.04), ('lout', 0.04), ('semictoabmillshtaslastrucktldcct', 0.04), ('wjlj', 0.04), ('lj', 0.039), ('descriptors', 0.039), ('state', 0.038), ('distinctiveness', 0.036), ('cliffbar', 0.035), ('surfer', 0.035), ('xrt', 0.035), ('jm', 0.035), ('sequences', 0.034), ('mil', 0.034), ('tr', 0.034), ('bt', 0.033), ('type', 0.033), ('arrangements', 0.033), ('lemming', 0.033), ('panda', 0.033), ('stable', 0.033), ('wj', 0.033), ('grabner', 0.033), ('bounding', 0.032), ('hl', 0.032), ('object', 0.032), ('cope', 0.031), ('plex', 0.031), ('qn', 0.031), ('appearance', 0.03), ('channel', 0.03), ('occluded', 0.029), ('surf', 0.029), ('histograms', 0.029), ('score', 0.028), ('sm', 0.028), ('shaking', 0.028), ('operators', 0.028), ('dl', 0.028), ('variation', 0.027), ('heavy', 0.027), ('cues', 0.027), ('handling', 0.027), ('success', 0.027), ('cortex', 0.026), ('struck', 0.026), ('contextual', 0.026), ('temporal', 0.026), ('median', 0.026), ('representation', 0.026), ('tld', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000017 <a title="89-tfidf-1" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>Author: Dapeng Chen, Zejian Yuan, Yang Wu, Geng Zhang, Nanning Zheng</p><p>Abstract: Representation is a fundamental problem in object tracking. Conventional methods track the target by describing its local or global appearance. In this paper we present that, besides the two paradigms, the composition of local region histograms can also provide diverse and important object cues. We use cells to extract local appearance, and construct complex cells to integrate the information from cells. With different spatial arrangements of cells, complex cells can explore various contextual information at multiple scales, which is important to improve the tracking performance. We also develop a novel template-matching algorithm for object tracking, where the template is composed of temporal varying cells and has two layers to capture the target and background appearance respectively. An adaptive weight is associated with each complex cell to cope with occlusion as well as appearance variation. A fusion weight is associated with each complex cell type to preserve the global distinctiveness. Our algorithm is evaluated on 25 challenging sequences, and the results not only confirm the contribution of each component in our tracking system, but also outperform other competing trackers.</p><p>2 0.27197167 <a title="89-tfidf-2" href="./iccv-2013-A_Color_Constancy_Model_with_Double-Opponency_Mechanisms.html">5 iccv-2013-A Color Constancy Model with Double-Opponency Mechanisms</a></p>
<p>Author: Shaobing Gao, Kaifu Yang, Chaoyi Li, Yongjie Li</p><p>Abstract: The double-opponent color-sensitive cells in the primary visual cortex (V1) of the human visual system (HVS) have long been recognized as the physiological basis of color constancy. We introduce a new color constancy model by imitating the functional properties of the HVS from the retina to the double-opponent cells in V1. The idea behind the model originates from the observation that the color distribution of the responses of double-opponent cells to the input color-biased images coincides well with the light source direction. Then the true illuminant color of a scene is easily estimated by searching for the maxima of the separate RGB channels of the responses of double-opponent cells in the RGB space. Our systematical experimental evaluations on two commonly used image datasets show that the proposed model can produce competitive results in comparison to the complex state-of-the-art approaches, but with a simple implementation and without the need for training.</p><p>3 0.20870884 <a title="89-tfidf-3" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>Author: Shuran Song, Jianxiong Xiao</p><p>Abstract: Despite significant progress, tracking is still considered to be a very challenging task. Recently, the increasing popularity of depth sensors has made it possible to obtain reliable depth easily. This may be a game changer for tracking, since depth can be used to prevent model drift and handle occlusion. We also observe that current tracking algorithms are mostly evaluated on a very small number of videos collectedandannotated by different groups. The lack of a reasonable size and consistently constructed benchmark has prevented a persuasive comparison among different algorithms. In this paper, we construct a unified benchmark dataset of 100 RGBD videos with high diversity, propose different kinds of RGBD tracking algorithms using 2D or 3D model, and present a quantitative comparison of various algorithms with RGB or RGBD input. We aim to lay the foundation for further research in both RGB and RGBD tracking, and our benchmark is available at http://tracking.cs.princeton.edu.</p><p>4 0.1898606 <a title="89-tfidf-4" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>Author: Junliang Xing, Jin Gao, Bing Li, Weiming Hu, Shuicheng Yan</p><p>Abstract: Recently, sparse representation has been introduced for robust object tracking. By representing the object sparsely, i.e., using only a few templates via ?1-norm minimization, these so-called ?1-trackers exhibit promising tracking results. In this work, we address the object template building and updating problem in these ?1-tracking approaches, which has not been fully studied. We propose to perform template updating, in a new perspective, as an online incremental dictionary learning problem, which is efficiently solved through an online optimization procedure. To guarantee the robustness and adaptability of the tracking algorithm, we also propose to build a multi-lifespan dictionary model. By building target dictionaries of different lifespans, effective object observations can be obtained to deal with the well-known drifting problem in tracking and thus improve the tracking accuracy. We derive effective observa- tion models both generatively and discriminatively based on the online multi-lifespan dictionary learning model and deploy them to the Bayesian sequential estimation framework to perform tracking. The proposed approach has been extensively evaluated on ten challenging video sequences. Experimental results demonstrate the effectiveness of the online learned templates, as well as the state-of-the-art tracking performance of the proposed approach.</p><p>5 0.18424124 <a title="89-tfidf-5" href="./iccv-2013-Decomposing_Bag_of_Words_Histograms.html">104 iccv-2013-Decomposing Bag of Words Histograms</a></p>
<p>Author: Ankit Gandhi, Karteek Alahari, C.V. Jawahar</p><p>Abstract: We aim to decompose a global histogram representation of an image into histograms of its associated objects and regions. This task is formulated as an optimization problem, given a set of linear classifiers, which can effectively discriminate the object categories present in the image. Our decomposition bypasses harder problems associated with accurately localizing and segmenting objects. We evaluate our method on a wide variety of composite histograms, and also compare it with MRF-based solutions. In addition to merely measuring the accuracy of decomposition, we also show the utility of the estimated object and background histograms for the task of image classification on the PASCAL VOC 2007 dataset.</p><p>6 0.17287022 <a title="89-tfidf-6" href="./iccv-2013-Finding_the_Best_from_the_Second_Bests_-_Inhibiting_Subjective_Bias_in_Evaluation_of_Visual_Tracking_Algorithms.html">168 iccv-2013-Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms</a></p>
<p>7 0.1685371 <a title="89-tfidf-7" href="./iccv-2013-Conservation_Tracking.html">87 iccv-2013-Conservation Tracking</a></p>
<p>8 0.16500925 <a title="89-tfidf-8" href="./iccv-2013-Online_Robust_Non-negative_Dictionary_Learning_for_Visual_Tracking.html">298 iccv-2013-Online Robust Non-negative Dictionary Learning for Visual Tracking</a></p>
<p>9 0.16422206 <a title="89-tfidf-9" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>10 0.13583642 <a title="89-tfidf-10" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>11 0.13572989 <a title="89-tfidf-11" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>12 0.12644409 <a title="89-tfidf-12" href="./iccv-2013-Dynamic_Probabilistic_Volumetric_Models.html">128 iccv-2013-Dynamic Probabilistic Volumetric Models</a></p>
<p>13 0.11418691 <a title="89-tfidf-13" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>14 0.1050792 <a title="89-tfidf-14" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>15 0.10325079 <a title="89-tfidf-15" href="./iccv-2013-Discriminatively_Trained_Templates_for_3D_Object_Detection%3A_A_Real_Time_Scalable_Approach.html">121 iccv-2013-Discriminatively Trained Templates for 3D Object Detection: A Real Time Scalable Approach</a></p>
<p>16 0.1022884 <a title="89-tfidf-16" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>17 0.099722534 <a title="89-tfidf-17" href="./iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects.html">320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</a></p>
<p>18 0.097446725 <a title="89-tfidf-18" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>19 0.086761706 <a title="89-tfidf-19" href="./iccv-2013-Multiple_Non-rigid_Surface_Detection_and_Registration.html">283 iccv-2013-Multiple Non-rigid Surface Detection and Registration</a></p>
<p>20 0.085734047 <a title="89-tfidf-20" href="./iccv-2013-Human_Re-identification_by_Matching_Compositional_Template_with_Cluster_Sampling.html">205 iccv-2013-Human Re-identification by Matching Compositional Template with Cluster Sampling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.198), (1, -0.037), (2, 0.034), (3, 0.014), (4, 0.017), (5, -0.046), (6, -0.105), (7, 0.134), (8, -0.103), (9, 0.111), (10, -0.061), (11, -0.126), (12, 0.079), (13, 0.078), (14, 0.06), (15, -0.072), (16, 0.125), (17, 0.064), (18, 0.046), (19, -0.086), (20, -0.003), (21, 0.051), (22, -0.036), (23, -0.079), (24, -0.076), (25, 0.04), (26, -0.007), (27, 0.039), (28, -0.01), (29, 0.013), (30, 0.043), (31, -0.008), (32, 0.07), (33, 0.121), (34, -0.018), (35, -0.026), (36, -0.086), (37, -0.164), (38, -0.027), (39, 0.099), (40, -0.057), (41, 0.017), (42, 0.081), (43, 0.041), (44, 0.035), (45, -0.033), (46, 0.032), (47, -0.077), (48, -0.096), (49, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96869147 <a title="89-lsi-1" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>Author: Dapeng Chen, Zejian Yuan, Yang Wu, Geng Zhang, Nanning Zheng</p><p>Abstract: Representation is a fundamental problem in object tracking. Conventional methods track the target by describing its local or global appearance. In this paper we present that, besides the two paradigms, the composition of local region histograms can also provide diverse and important object cues. We use cells to extract local appearance, and construct complex cells to integrate the information from cells. With different spatial arrangements of cells, complex cells can explore various contextual information at multiple scales, which is important to improve the tracking performance. We also develop a novel template-matching algorithm for object tracking, where the template is composed of temporal varying cells and has two layers to capture the target and background appearance respectively. An adaptive weight is associated with each complex cell to cope with occlusion as well as appearance variation. A fusion weight is associated with each complex cell type to preserve the global distinctiveness. Our algorithm is evaluated on 25 challenging sequences, and the results not only confirm the contribution of each component in our tracking system, but also outperform other competing trackers.</p><p>2 0.72838378 <a title="89-lsi-2" href="./iccv-2013-Finding_the_Best_from_the_Second_Bests_-_Inhibiting_Subjective_Bias_in_Evaluation_of_Visual_Tracking_Algorithms.html">168 iccv-2013-Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms</a></p>
<p>Author: Yu Pang, Haibin Ling</p><p>Abstract: Evaluating visual tracking algorithms, or “trackers ” for short, is of great importance in computer vision. However, it is hard to “fairly” compare trackers due to many parameters need to be tuned in the experimental configurations. On the other hand, when introducing a new tracker, a recent trend is to validate it by comparing it with several existing ones. Such an evaluation may have subjective biases towards the new tracker which typically performs the best. This is mainly due to the difficulty to optimally tune all its competitors and sometimes the selected testing sequences. By contrast, little subjective bias exists towards the “second best” ones1 in the contest. This observation inspires us with a novel perspective towards inhibiting subjective bias in evaluating trackers by analyzing the results between the second bests. In particular, we first collect all tracking papers published in major computer vision venues in recent years. From these papers, after filtering out potential biases in various aspects, we create a dataset containing many records of comparison results between various visual trackers. Using these records, we derive performance rank- ings of the involved trackers by four different methods. The first two methods model the dataset as a graph and then derive the rankings over the graph, one by a rank aggregation algorithm and the other by a PageRank-like solution. The other two methods take the records as generated from sports contests and adopt widely used Elo’s and Glicko ’s rating systems to derive the rankings. The experimental results are presented and may serve as a reference for related research.</p><p>3 0.70498198 <a title="89-lsi-3" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>Author: Zhibin Hong, Xue Mei, Danil Prokhorov, Dacheng Tao</p><p>Abstract: Combining multiple observation views has proven beneficial for tracking. In this paper, we cast tracking as a novel multi-task multi-view sparse learning problem and exploit the cues from multiple views including various types of visual features, such as intensity, color, and edge, where each feature observation can be sparsely represented by a linear combination of atoms from an adaptive feature dictionary. The proposed method is integrated in a particle filter framework where every view in each particle is regarded as an individual task. We jointly consider the underlying relationship between tasks across different views and different particles, and tackle it in a unified robust multi-task formulation. In addition, to capture the frequently emerging outlier tasks, we decompose the representation matrix to two collaborative components which enable a more robust and accurate approximation. We show that theproposedformulation can be efficiently solved using the Accelerated Proximal Gradient method with a small number of closed-form updates. The presented tracker is implemented using four types of features and is tested on numerous benchmark video sequences. Both the qualitative and quantitative results demonstrate the superior performance of the proposed approach compared to several stateof-the-art trackers.</p><p>4 0.68517923 <a title="89-lsi-4" href="./iccv-2013-Online_Robust_Non-negative_Dictionary_Learning_for_Visual_Tracking.html">298 iccv-2013-Online Robust Non-negative Dictionary Learning for Visual Tracking</a></p>
<p>Author: Naiyan Wang, Jingdong Wang, Dit-Yan Yeung</p><p>Abstract: This paper studies the visual tracking problem in video sequences and presents a novel robust sparse tracker under the particle filter framework. In particular, we propose an online robust non-negative dictionary learning algorithm for updating the object templates so that each learned template can capture a distinctive aspect of the tracked object. Another appealing property of this approach is that it can automatically detect and reject the occlusion and cluttered background in a principled way. In addition, we propose a new particle representation formulation using the Huber loss function. The advantage is that it can yield robust estimation without using trivial templates adopted by previous sparse trackers, leading to faster computation. We also reveal the equivalence between this new formulation and the previous one which uses trivial templates. The proposed tracker is empirically compared with state-of-the-art trackers on some challenging video sequences. Both quantitative and qualitative comparisons show that our proposed tracker is superior and more stable.</p><p>5 0.66565996 <a title="89-lsi-5" href="./iccv-2013-Slice_Sampling_Particle_Belief_Propagation.html">395 iccv-2013-Slice Sampling Particle Belief Propagation</a></p>
<p>Author: Oliver Müller, Michael Ying Yang, Bodo Rosenhahn</p><p>Abstract: Inference in continuous label Markov random fields is a challenging task. We use particle belief propagation (PBP) for solving the inference problem in continuous label space. Sampling particles from the belief distribution is typically done by using Metropolis-Hastings (MH) Markov chain Monte Carlo (MCMC) methods which involves sampling from a proposal distribution. This proposal distribution has to be carefully designed depending on the particular model and input data to achieve fast convergence. We propose to avoid dependence on a proposal distribution by introducing a slice sampling based PBP algorithm. The proposed approach shows superior convergence performance on an image denoising toy example. Our findings are validated on a challenging relational 2D feature tracking application.</p><p>6 0.6631428 <a title="89-lsi-6" href="./iccv-2013-A_Color_Constancy_Model_with_Double-Opponency_Mechanisms.html">5 iccv-2013-A Color Constancy Model with Double-Opponency Mechanisms</a></p>
<p>7 0.64112473 <a title="89-lsi-7" href="./iccv-2013-Conservation_Tracking.html">87 iccv-2013-Conservation Tracking</a></p>
<p>8 0.62172246 <a title="89-lsi-8" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>9 0.61581004 <a title="89-lsi-9" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>10 0.5968349 <a title="89-lsi-10" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>11 0.57644564 <a title="89-lsi-11" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>12 0.56475651 <a title="89-lsi-12" href="./iccv-2013-Orderless_Tracking_through_Model-Averaged_Posterior_Estimation.html">303 iccv-2013-Orderless Tracking through Model-Averaged Posterior Estimation</a></p>
<p>13 0.53044397 <a title="89-lsi-13" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>14 0.52713609 <a title="89-lsi-14" href="./iccv-2013-Modeling_Self-Occlusions_in_Dynamic_Shape_and_Appearance_Tracking.html">270 iccv-2013-Modeling Self-Occlusions in Dynamic Shape and Appearance Tracking</a></p>
<p>15 0.52377045 <a title="89-lsi-15" href="./iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects.html">320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</a></p>
<p>16 0.502496 <a title="89-lsi-16" href="./iccv-2013-Discriminatively_Trained_Templates_for_3D_Object_Detection%3A_A_Real_Time_Scalable_Approach.html">121 iccv-2013-Discriminatively Trained Templates for 3D Object Detection: A Real Time Scalable Approach</a></p>
<p>17 0.48312405 <a title="89-lsi-17" href="./iccv-2013-Decomposing_Bag_of_Words_Histograms.html">104 iccv-2013-Decomposing Bag of Words Histograms</a></p>
<p>18 0.47464016 <a title="89-lsi-18" href="./iccv-2013-Shape_Index_Descriptors_Applied_to_Texture-Based_Galaxy_Analysis.html">388 iccv-2013-Shape Index Descriptors Applied to Texture-Based Galaxy Analysis</a></p>
<p>19 0.47420481 <a title="89-lsi-19" href="./iccv-2013-Illuminant_Chromaticity_from_Image_Sequences.html">207 iccv-2013-Illuminant Chromaticity from Image Sequences</a></p>
<p>20 0.47172657 <a title="89-lsi-20" href="./iccv-2013-Dynamic_Probabilistic_Volumetric_Models.html">128 iccv-2013-Dynamic Probabilistic Volumetric Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.059), (7, 0.026), (9, 0.167), (21, 0.023), (26, 0.071), (31, 0.047), (35, 0.015), (40, 0.025), (42, 0.068), (48, 0.02), (64, 0.086), (73, 0.062), (89, 0.214), (97, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88801193 <a title="89-lda-1" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>Author: Dapeng Chen, Zejian Yuan, Yang Wu, Geng Zhang, Nanning Zheng</p><p>Abstract: Representation is a fundamental problem in object tracking. Conventional methods track the target by describing its local or global appearance. In this paper we present that, besides the two paradigms, the composition of local region histograms can also provide diverse and important object cues. We use cells to extract local appearance, and construct complex cells to integrate the information from cells. With different spatial arrangements of cells, complex cells can explore various contextual information at multiple scales, which is important to improve the tracking performance. We also develop a novel template-matching algorithm for object tracking, where the template is composed of temporal varying cells and has two layers to capture the target and background appearance respectively. An adaptive weight is associated with each complex cell to cope with occlusion as well as appearance variation. A fusion weight is associated with each complex cell type to preserve the global distinctiveness. Our algorithm is evaluated on 25 challenging sequences, and the results not only confirm the contribution of each component in our tracking system, but also outperform other competing trackers.</p><p>2 0.84518659 <a title="89-lda-2" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>Author: Xiaoyu Wang, Ming Yang, Shenghuo Zhu, Yuanqing Lin</p><p>Abstract: Generic object detection is confronted by dealing with different degrees of variations in distinct object classes with tractable computations, which demands for descriptive and flexible object representations that are also efficient to evaluate for many locations. In view of this, we propose to model an object class by a cascaded boosting classifier which integrates various types of features from competing local regions, named as regionlets. A regionlet is a base feature extraction region defined proportionally to a detection window at an arbitrary resolution (i.e. size and aspect ratio). These regionlets are organized in small groups with stable relative positions to delineate fine-grained spatial layouts inside objects. Their features are aggregated to a one-dimensional feature within one group so as to tolerate deformations. Then we evaluate the object bounding box proposal in selective search from segmentation cues, limiting the evaluation locations to thousands. Our approach significantly outperforms the state-of-the-art on popular multi-class detection benchmark datasets with a single method, without any contexts. It achieves the detec- tion mean average precision of 41. 7% on the PASCAL VOC 2007 dataset and 39. 7% on the VOC 2010 for 20 object categories. It achieves 14. 7% mean average precision on the ImageNet dataset for 200 object categories, outperforming the latest deformable part-based model (DPM) by 4. 7%.</p><p>3 0.83071697 <a title="89-lda-3" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>Author: Wei Xia, Csaba Domokos, Jian Dong, Loong-Fah Cheong, Shuicheng Yan</p><p>Abstract: Numerous existing object segmentation frameworks commonly utilize the object bounding box as a prior. In this paper, we address semantic segmentation assuming that object bounding boxes are provided by object detectors, but no training data with annotated segments are available. Based on a set of segment hypotheses, we introduce a simple voting scheme to estimate shape guidance for each bounding box. The derived shape guidance is used in the subsequent graph-cut-based figure-ground segmentation. The final segmentation result is obtained by merging the segmentation results in the bounding boxes. We conduct an extensive analysis of the effect of object bounding box accuracy. Comprehensive experiments on both the challenging PASCAL VOC object segmentation dataset and GrabCut50 image segmentation dataset show that the proposed approach achieves competitive results compared to previous detection or bounding box prior based methods, as well as other state-of-the-art semantic segmentation methods.</p><p>4 0.82810897 <a title="89-lda-4" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>Author: Zhibin Hong, Xue Mei, Danil Prokhorov, Dacheng Tao</p><p>Abstract: Combining multiple observation views has proven beneficial for tracking. In this paper, we cast tracking as a novel multi-task multi-view sparse learning problem and exploit the cues from multiple views including various types of visual features, such as intensity, color, and edge, where each feature observation can be sparsely represented by a linear combination of atoms from an adaptive feature dictionary. The proposed method is integrated in a particle filter framework where every view in each particle is regarded as an individual task. We jointly consider the underlying relationship between tasks across different views and different particles, and tackle it in a unified robust multi-task formulation. In addition, to capture the frequently emerging outlier tasks, we decompose the representation matrix to two collaborative components which enable a more robust and accurate approximation. We show that theproposedformulation can be efficiently solved using the Accelerated Proximal Gradient method with a small number of closed-form updates. The presented tracker is implemented using four types of features and is tested on numerous benchmark video sequences. Both the qualitative and quantitative results demonstrate the superior performance of the proposed approach compared to several stateof-the-art trackers.</p><p>5 0.82398272 <a title="89-lda-5" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>Author: Shuran Song, Jianxiong Xiao</p><p>Abstract: Despite significant progress, tracking is still considered to be a very challenging task. Recently, the increasing popularity of depth sensors has made it possible to obtain reliable depth easily. This may be a game changer for tracking, since depth can be used to prevent model drift and handle occlusion. We also observe that current tracking algorithms are mostly evaluated on a very small number of videos collectedandannotated by different groups. The lack of a reasonable size and consistently constructed benchmark has prevented a persuasive comparison among different algorithms. In this paper, we construct a unified benchmark dataset of 100 RGBD videos with high diversity, propose different kinds of RGBD tracking algorithms using 2D or 3D model, and present a quantitative comparison of various algorithms with RGB or RGBD input. We aim to lay the foundation for further research in both RGB and RGBD tracking, and our benchmark is available at http://tracking.cs.princeton.edu.</p><p>6 0.82179224 <a title="89-lda-6" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>7 0.81895572 <a title="89-lda-7" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>8 0.81809306 <a title="89-lda-8" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<p>9 0.81794858 <a title="89-lda-9" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>10 0.81740081 <a title="89-lda-10" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>11 0.81731969 <a title="89-lda-11" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>12 0.81637424 <a title="89-lda-12" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>13 0.81628829 <a title="89-lda-13" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>14 0.8146199 <a title="89-lda-14" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>15 0.81426406 <a title="89-lda-15" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>16 0.81423807 <a title="89-lda-16" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>17 0.81400943 <a title="89-lda-17" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>18 0.81388134 <a title="89-lda-18" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>19 0.81308126 <a title="89-lda-19" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>20 0.81286287 <a title="89-lda-20" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
