<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>148 iccv-2013-Example-Based Facade Texture Synthesis</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-148" href="#">iccv2013-148</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>148 iccv-2013-Example-Based Facade Texture Synthesis</h1>
<br/><p>Source: <a title="iccv-2013-148-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Dai_Example-Based_Facade_Texture_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Dengxin Dai, Hayko Riemenschneider, Gerhard Schmitt, Luc Van_Gool</p><p>Abstract: There is an increased interest in the efficient creation of city models, be it virtual or as-built. We present a method for synthesizing complex, photo-realistic facade images, from a single example. After parsing the example image into its semantic components, a tiling for it is generated. Novel tilings can then be created, yielding facade textures with different dimensions or with occluded parts inpainted. A genetic algorithm guides the novel facades as well as inpainted parts to be consistent with the example, both in terms of their overall structure and their detailed textures. Promising results for multiple standard datasets in particular for the different building styles they contain demonstrate the potential of the method. – –</p><p>Reference: <a title="iccv-2013-148-reference" href="../iccv2013_reference/iccv-2013-Example-Based_Facade_Texture_Synthesis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We present a method for synthesizing complex, photo-realistic facade images, from a single example. [sent-4, score-0.621]
</p><p>2 After parsing the example image into its semantic components, a tiling for it is generated. [sent-5, score-0.21]
</p><p>3 Novel tilings can then be created, yielding facade textures with different dimensions or with occluded parts inpainted. [sent-6, score-0.741]
</p><p>4 A genetic algorithm guides the novel facades as well as inpainted parts to be consistent with the example, both in terms of their overall structure and their detailed textures. [sent-7, score-0.3]
</p><p>5 During the last decade, texture synthesis has undergone important changes. [sent-14, score-0.302]
</p><p>6 If a facade pattern is considered as a texture, it will not follow the local and stationarity assumption that comes with these methods, however. [sent-16, score-0.599]
</p><p>7 ch  (a) Facade texture  (b) Stone texture  Figure 1. [sent-26, score-0.251]
</p><p>8 Illustration of textures’ properties: (a) a facade texture with its two local patches, and (b) a stone texture with its two local patches. [sent-27, score-0.858]
</p><p>9 It shows that facade textures do not own the local and stationary properties as normal textures do. [sent-28, score-0.759]
</p><p>10 Similar to [24] we decompose facades into tiles that are defined through a series of horizontal and vertical split lines. [sent-32, score-0.66]
</p><p>11 Each resulting tile is given an individual label and represents a node of a regular grid. [sent-34, score-0.328]
</p><p>12 A facade texture is then created by extending the grid (to its new dimensions for a novel facade or across the occlusion for inpainting), see Fig. [sent-38, score-1.372]
</p><p>13 The texture synthesis then amounts to assigning one of the labels in (b) to each of the tiles in (c). [sent-40, score-0.652]
</p><p>14 We impose two constraints: 1) neighboring tiles should be photo-consistent, and 2) have to follow the large-scale structures in the example. [sent-41, score-0.339]
</p><p>15 Our contributions are: (1) an automatic method for the  tessellation of an example facade into tiles lying on a regular, rectangular grid (§3. [sent-45, score-1.077]
</p><p>16 1); (2) formulating facade textuulraer, synthesis as a rcidons (§tr3a. [sent-46, score-0.785]
</p><p>17 The pipeline of our method: From a parsed example facade (a), to its grid representation (b), to a larger, synthesized grid with inferred label configuration (c), and to the synthesized facade (d). [sent-49, score-1.42]
</p><p>18 Each node in (b) has a unique label indicating its own tile and it is highlighted with a specific color. [sent-50, score-0.328]
</p><p>19 The facade example is from Paris201 1 [26]  netic algorithm (§3. [sent-51, score-0.599]
</p><p>20 Techniques of example-based texture synthesis can be broadly categorized into model-based methods and model-free methods. [sent-58, score-0.302]
</p><p>21 The former group learn the essence of exemplar textures with parametric models, from which they sample new textures. [sent-59, score-0.198]
</p><p>22 Model-free methods generate textures by copying pixels or patches from the exemplar inputs. [sent-63, score-0.283]
</p><p>23 In a seminal paper, Efros and Leung [11] synthesized high-quality textures by copying pixels. [sent-64, score-0.196]
</p><p>24 Our method is most akin to the model-free strand, but works on semantic tiles rather than arbitrary patches. [sent-67, score-0.368]
</p><p>25 Tiles have already been used for texture synthesis [6, 19], but the alignment of tiles to texture elements are either ignored [6] or handled interactively [19]. [sent-68, score-0.731]
</p><p>26 Traditional inpainting fills in small holes through color or texture extrapolation (e. [sent-70, score-0.264]
</p><p>27 Here, large parts of facades need to be filled in, including diverse and complex patterns. [sent-73, score-0.211]
</p><p>28 Just as with the retargeting of facade textures (see previous point), the key is to detect and exploit the regularities that are present in facades. [sent-74, score-0.76]
</p><p>29 A similar work is [13], where the occlusion of facades are inpainted by grid structure propagation. [sent-75, score-0.303]
</p><p>30 Our method operates at tile level, allowing it to exploit larger-scale semantic and geometric structures. [sent-88, score-0.383]
</p><p>31 Yet, that method needs artists to design the tile sets and a few exemplars of interesting patterns, reintroducing a need for interactivity. [sent-93, score-0.347]
</p><p>32 Our method lifts the limitations by creating the tiles automatically from an exemplar facade image. [sent-94, score-1.03]
</p><p>33 Approach This section presents our approach, which consists of three components: facade tessellation, the synthesis model, and the optimization. [sent-96, score-0.785]
</p><p>34 Exemplar’s Irregular Rectangular Lattice Our synthesis assembles a new facade tiling, as a puzzle with tiles from the exemplar facade image as its pieces. [sent-99, score-1.849]
</p><p>35 Obtaining a high-quality tiling of the exemplar therefore is paramount. [sent-100, score-0.238]
</p><p>36 This segmentation should (1) yield tile bound-  aries that conform with the boundaries of semantic facade components as not to break them up, (2) naturally reflect the organization of the facade in terms of floors, window columns, etc. [sent-101, score-1.653]
</p><p>37 , and (3) yield tiles big enough to enable a sufficiently efficient creation of new tilings. [sent-102, score-0.375]
</p><p>38 ditions, we opted for an irregular rectangular lattice (IRL), as already shown in Fig. [sent-107, score-0.261]
</p><p>39 An IRL splits the facade rectilinearly into differently sized rows and columns of tiles, defined by a set of horizontal and vertical split lines (SLs). [sent-109, score-0.756]
</p><p>40 In order to arrive at a tiling coinciding with the boundaries of the facade’s semantic components, we first need to  ×  label the facade. [sent-110, score-0.205]
</p><p>41 It is noteworthy that a better labeling could be obtained by more sophisticated methods [8, 26], but they require training with facades of the same building styles. [sent-121, score-0.319]
</p><p>42 This term helps to avoid overly big tiles, which lead to a verbatim copying of large portions of the exemplar facade. [sent-125, score-0.181]
</p><p>43 The facade image is referred to as X, with resolution H W. [sent-128, score-0.599]
</p><p>44 Starting from a single-tile lattice (the exemplar image), the method each time adds either a horizontal or a vertical SL with the then highest value of ΛH (h) = Λ1 (h) · Λ2 (h)  (3)  or the similarly defined measure ΛW (w) for vertical case. [sent-144, score-0.392]
</p><p>45 Synthesis of the retargeted image In this section, we will build new tilings from the exemplar tiling. [sent-153, score-0.316]
</p><p>46 The latter has provided us with a lattice of M N tiles T = {T1, . [sent-154, score-0.424]
</p><p>47 The synthesis first considers the corresponding, retargeted grid and assigns one of the original tile identifiers j to each new node. [sent-171, score-0.731]
</p><p>48 1 First, we describe how we turn the retargeted tiling G? [sent-211, score-0.279]
</p><p>49 The assigned tiles in the same row/column of the retargeted tiling may have different heights/widths. [sent-214, score-0.592]
</p><p>50 In order to avoid strong distortions, the average height of all tiles in the same row is taken as the new, common height. [sent-216, score-0.313]
</p><p>51 Similarly, all tiles in a column get the average width. [sent-217, score-0.313]
</p><p>52 The selection of the optimal tile labels is guided by two constraints: photo consistency and structural consistency. [sent-223, score-0.46]
</p><p>53 Photo consistency should avoid visual artifacts at the tile boundaries in X? [sent-230, score-0.415]
</p><p>54 We exemplify the computation of photo consistency across the vertical boundary for tile Ti? [sent-234, score-0.455]
</p><p>55 We look for a window bi of the same size anywhere in X (so not only at tile boundaries) whose appearance is most similar to bi? [sent-243, score-0.368]
</p><p>56 Structural consistency should ensure that structures that extend beyond individual tiles are also similar to those in the exemplar, e. [sent-249, score-0.374]
</p><p>57 Since most facade structures stretch out either horizontally or vertically, we define a horizontal and a vertical matching template for contiguous tiles. [sent-252, score-0.762]
</p><p>58 Photo consistency is measured within the thin, black rectangular regions, and structural consistency is evaluated by the 4-order horizontal and vertical tile templates. [sent-261, score-0.597]
</p><p>59 The blue tiles indicate the tile position ifor which the constraints are applied. [sent-262, score-0.641]
</p><p>60 too high one causes some neighborhoods in the retargeted image to be dissimilar to all neighborhoods in the exemplar image. [sent-263, score-0.335]
</p><p>61 0 be the histogram of semantic class labels for all pixels contained in tile Ti? [sent-266, score-0.42]
</p><p>62 1 be the histogram for the neighboring tile to the right of i, and fi? [sent-268, score-0.328]
</p><p>63 It describes the horizontal semantic structures at site i. [sent-273, score-0.196]
</p><p>64 Let fi be the concatenated histogram in X (computed over tile templates of the 4th order in X) that is most similar to fi? [sent-274, score-0.367]
</p><p>65 4, a complete assignment of exemplar tile labels to the tiles of the retargeted grid is determined. [sent-281, score-1.013]
</p><p>66 Optimization of tile assignment Assigning the optimal tile labels to all nodes of the retargeted grid is a very hard problem, given the high number of possible tile labels and the non-trivial nature of the constraints. [sent-285, score-1.275]
</p><p>67 For the breeding we randomly select 2 individuals from the current generation, and then pick the fittest (2-tournament). [sent-312, score-0.17]
</p><p>68 The crossover and mutations are actually performed on grid G? [sent-323, score-0.213]
</p><p>69 For the sake ofefficiency, we modify multiple tile labels at each step (blocked tweak). [sent-326, score-0.391]
</p><p>70 Each blocked tweak changes the labels of a set of nodes relative to an anchor site i. [sent-327, score-0.218]
</p><p>71 sTethe { crossover exchanges all labels in the block at a randomly chosen site ibetween the two chosen parents. [sent-334, score-0.211]
</p><p>72 The mutation modifies all tile labels in the block at a randomly chosen site iby copying from a similar block at another randomly chosen  site j of the exemplar grid G. [sent-335, score-0.792]
</p><p>73 The blocked crossover provides a way of combining these local optima to move towards the global one. [sent-341, score-0.206]
</p><p>74 The blocked mutations directly transfer locally optimal configurations from the exemplar facade, such that local ‘garbage’ configurations can be refined quickly. [sent-342, score-0.308]
</p><p>75 Experiments In this section, we evaluate our method on facade examples from three datasets: Paris201 1 [26], the Barcelona and Timisoara image collection (BT5 1) by O. [sent-350, score-0.599]
</p><p>76 Paris201 1 consists of 104 facades of Hausmanian style. [sent-361, score-0.211]
</p><p>77 BT51 contains 34 facades taken in Barcelona and 17 images taken in Timisoara. [sent-362, score-0.211]
</p><p>78 FaSyn13 is our new facade collection, comprising 200 facades of varying building styles, including Classicism, Renaissance, Modern, etc. [sent-363, score-0.851]
</p><p>79 For the facade labeling, 10 trees of depth 25 (searched from 10 11006699  ×  ? [sent-369, score-0.599]
</p><p>80 An illustration of how the synthesis result evolves with the number of iterations, resulting in decreasing energy (cf. [sent-389, score-0.213]
</p><p>81 Facade Synthesis We compare our method with the texture synthesis method [10] and the image retargeting method [4]. [sent-403, score-0.357]
</p><p>82 The figure shows that image retargeting methods cannot serve our purpose creating style-preserving, novel facades from an exemplar. [sent-406, score-0.266]
</p><p>83 7 also shows that algorithms designed for normal texture cannot be expected to synthesize facade tex–  tures well. [sent-409, score-0.758]
</p><p>84 From the figures, we can see our method can synthesize structured facades of a wide variety of styles. [sent-418, score-0.254]
</p><p>85 ear lattice parsing is quite brutal it is not uncommon that its SLs are not exactly aligned with the boundaries of building components. [sent-426, score-0.217]
</p><p>86 This can be alleviated by allowing the positions of tiles to shift slightly or their shapes to change in order to fit local data. [sent-430, score-0.313]
</p><p>87 The method is quite efficient, as we operate on the tile level. [sent-438, score-0.328]
</p><p>88 We found this setting to be satisfactory for all facades used in the experiments. [sent-440, score-0.211]
</p><p>89 It is interesting to see that the result gets more realistic as the number of iterations increases, and finally converges to a facade of high quality. [sent-443, score-0.599]
</p><p>90 The whole synthesis of each facade takes 6 10 minutes. [sent-444, score-0.785]
</p><p>91 Facade Inpainting We also evaluated our method on the task of facade inpainting. [sent-448, score-0.599]
</p><p>92 In inpainting, tiles and the constraints are obtained and learned from the non-occluded area. [sent-449, score-0.313]
</p><p>93 From left to right, a facade image with occlusion, inpainting result by the Content Aware Fill ofAdobe Photoshop CS5, and inpainting result by our method. [sent-457, score-0.851]
</p><p>94 From left to right, a facade image with occlusion, ground truth, and our result. [sent-460, score-0.599]
</p><p>95 Conclusion This paper has tackled the problem of synthesizing complex facades from given examples. [sent-466, score-0.233]
</p><p>96 In order to not stretch and break up building assets, we proposed to operate on top of the irregular rectangular lattice (IRL), and designed a method to obtain the exemplar IRL. [sent-467, score-0.466]
</p><p>97 We then solved the synthesis problem as a graph labeling problem, with an adapted genetic algorithm. [sent-469, score-0.272]
</p><p>98 We evaluated our method at different levels (tasks): the IRL representation, facade synthesis, and facade inpainting, and obtained promising results for all of those. [sent-470, score-1.198]
</p><p>99 In this paper, we have restricted the texture synthesis to patches that are characterized by the colors of their pixels. [sent-471, score-0.324]
</p><p>100 Graphcut textures: image and video synthesis using graph cuts. [sent-580, score-0.186]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('facade', 0.599), ('tile', 0.328), ('tiles', 0.313), ('facades', 0.211), ('sls', 0.189), ('synthesis', 0.186), ('retargeted', 0.159), ('inpainting', 0.126), ('tiling', 0.12), ('exemplar', 0.118), ('texture', 0.116), ('lattice', 0.111), ('crossover', 0.097), ('blocked', 0.086), ('irregular', 0.082), ('textures', 0.08), ('individuals', 0.078), ('irl', 0.077), ('rectangular', 0.068), ('siggraph', 0.064), ('copying', 0.063), ('population', 0.059), ('horizontal', 0.059), ('grid', 0.058), ('fittest', 0.058), ('mutations', 0.058), ('site', 0.056), ('semantic', 0.055), ('retargeting', 0.055), ('genetic', 0.055), ('synthesized', 0.053), ('vertical', 0.052), ('procedural', 0.048), ('teboul', 0.048), ('synthesize', 0.043), ('building', 0.041), ('architectural', 0.041), ('bi', 0.04), ('creation', 0.04), ('photo', 0.04), ('fi', 0.039), ('lefebvre', 0.039), ('mutate', 0.039), ('tessellation', 0.039), ('tilings', 0.039), ('tweak', 0.039), ('labels', 0.037), ('noteworthy', 0.036), ('consistency', 0.035), ('parsing', 0.035), ('puzzle', 0.034), ('tiled', 0.034), ('breeding', 0.034), ('consi', 0.034), ('inpainted', 0.034), ('mutation', 0.034), ('wonka', 0.032), ('tency', 0.032), ('styles', 0.032), ('labeling', 0.031), ('boundaries', 0.03), ('photoshop', 0.03), ('essa', 0.03), ('turbopixels', 0.03), ('einn', 0.03), ('neighborhoods', 0.029), ('yeh', 0.028), ('fill', 0.027), ('barcelona', 0.027), ('evolves', 0.027), ('stone', 0.027), ('sake', 0.026), ('fitness', 0.026), ('stretch', 0.026), ('regularities', 0.026), ('structures', 0.026), ('split', 0.025), ('sl', 0.025), ('copy', 0.025), ('optima', 0.023), ('configurations', 0.023), ('ethz', 0.023), ('occluded', 0.023), ('holes', 0.022), ('synthesizing', 0.022), ('efros', 0.022), ('yield', 0.022), ('grammar', 0.022), ('artifacts', 0.022), ('patches', 0.022), ('block', 0.021), ('generation', 0.021), ('differently', 0.021), ('virtual', 0.021), ('kwatra', 0.021), ('structural', 0.02), ('break', 0.02), ('exemplars', 0.019), ('ch', 0.019), ('keeping', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="148-tfidf-1" href="./iccv-2013-Example-Based_Facade_Texture_Synthesis.html">148 iccv-2013-Example-Based Facade Texture Synthesis</a></p>
<p>Author: Dengxin Dai, Hayko Riemenschneider, Gerhard Schmitt, Luc Van_Gool</p><p>Abstract: There is an increased interest in the efficient creation of city models, be it virtual or as-built. We present a method for synthesizing complex, photo-realistic facade images, from a single example. After parsing the example image into its semantic components, a tiling for it is generated. Novel tilings can then be created, yielding facade textures with different dimensions or with occluded parts inpainted. A genetic algorithm guides the novel facades as well as inpainted parts to be consistent with the example, both in terms of their overall structure and their detailed textures. Promising results for multiple standard datasets in particular for the different building styles they contain demonstrate the potential of the method. – –</p><p>2 0.17096993 <a title="148-tfidf-2" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>Author: Bastien Jacquet, Christian Häne, Kevin Köser, Marc Pollefeys</p><p>Abstract: Although specular objects have gained interest in recent years, virtually no approaches exist for markerless reconstruction of reflective scenes in the wild. In this work, we present a practical approach to capturing normal maps in real-world scenes using video only. We focus on nearly planar surfaces such as windows, facades from glass or metal, or frames, screens and other indoor objects and show how normal maps of these can be obtained without the use of an artificial calibration object. Rather, we track the reflections of real-world straight lines, while moving with a hand-held or vehicle-mounted camera in front of the object. In contrast to error-prone local edge tracking, we obtain the reflections by a robust, global segmentation technique of an ortho-rectified 3D video cube that also naturally allows efficient user interaction. Then, at each point of the reflective surface, the resulting 2D-curve to 3D-line correspondence provides a novel quadratic constraint on the local surface normal. This allows to globally solve for the shape by integrability and smoothness constraints and easily supports the usage of multiple lines. We demonstrate the technique on several objects and facades.</p><p>3 0.088715762 <a title="148-tfidf-3" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>Author: Hongteng Xu, Hongyuan Zha</p><p>Abstract: Data sparsity has been a thorny issuefor manifold-based image synthesis, and in this paper we address this critical problem by leveraging ideas from transfer learning. Specifically, we propose methods based on generating auxiliary data in the form of synthetic samples using transformations of the original sparse samples. To incorporate the auxiliary data, we propose a weighted data synthesis method, which adaptively selects from the generated samples for inclusion during the manifold learning process via a weighted iterative algorithm. To demonstrate the feasibility of the proposed method, we apply it to the problem of face image synthesis from sparse samples. Compared with existing methods, the proposed method shows encouraging results with good performance improvements.</p><p>4 0.084375441 <a title="148-tfidf-4" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>Author: Jimei Yang, Yi-Hsuan Tsai, Ming-Hsuan Yang</p><p>Abstract: We present a hybrid parametric and nonparametric algorithm, exemplar cut, for generating class-specific object segmentation hypotheses. For the parametric part, we train a pylon model on a hierarchical region tree as the energy function for segmentation. For the nonparametric part, we match the input image with each exemplar by using regions to obtain a score which augments the energy function from the pylon model. Our method thus generates a set of highly plausible segmentation hypotheses by solving a series of exemplar augmented graph cuts. Experimental results on the Graz and PASCAL datasets show that the proposed algorithm achievesfavorable segmentationperformance against the state-of-the-art methods in terms of visual quality and accuracy.</p><p>5 0.068776585 <a title="148-tfidf-5" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>Author: Raúl Díaz, Sam Hallman, Charless C. Fowlkes</p><p>Abstract: The confluence of robust algorithms for structure from motion along with high-coverage mapping and imaging of the world around us suggests that it will soon be feasible to accurately estimate camera pose for a large class photographs taken in outdoor, urban environments. In this paper, we investigate how such information can be used to improve the detection of dynamic objects such as pedestrians and cars. First, we show that when rough camera location is known, we can utilize detectors that have been trained with a scene-specific background model in order to improve detection accuracy. Second, when precise camera pose is available, dense matching to a database of existing images using multi-view stereo provides a way to eliminate static backgrounds such as building facades, akin to background-subtraction often used in video analysis. We evaluate these ideas using a dataset of tourist photos with estimated camera pose. For template-based pedestrian detection, we achieve a 50 percent boost in average precision over baseline.</p><p>6 0.059283223 <a title="148-tfidf-6" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>7 0.058488682 <a title="148-tfidf-7" href="./iccv-2013-Recognising_Human-Object_Interaction_via_Exemplar_Based_Modelling.html">344 iccv-2013-Recognising Human-Object Interaction via Exemplar Based Modelling</a></p>
<p>8 0.057138115 <a title="148-tfidf-8" href="./iccv-2013-Coupled_Dictionary_and_Feature_Space_Learning_with_Applications_to_Cross-Domain_Image_Synthesis_and_Recognition.html">96 iccv-2013-Coupled Dictionary and Feature Space Learning with Applications to Cross-Domain Image Synthesis and Recognition</a></p>
<p>9 0.051381249 <a title="148-tfidf-9" href="./iccv-2013-Scene_Collaging%3A_Analysis_and_Synthesis_of_Natural_Images_with_Semantic_Layers.html">375 iccv-2013-Scene Collaging: Analysis and Synthesis of Natural Images with Semantic Layers</a></p>
<p>10 0.048478417 <a title="148-tfidf-10" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>11 0.047700677 <a title="148-tfidf-11" href="./iccv-2013-Content-Aware_Rotation.html">90 iccv-2013-Content-Aware Rotation</a></p>
<p>12 0.047315568 <a title="148-tfidf-12" href="./iccv-2013-Estimating_the_3D_Layout_of_Indoor_Scenes_and_Its_Clutter_from_Depth_Sensors.html">144 iccv-2013-Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors</a></p>
<p>13 0.046576418 <a title="148-tfidf-13" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>14 0.043576822 <a title="148-tfidf-14" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<p>15 0.04170401 <a title="148-tfidf-15" href="./iccv-2013-3D_Scene_Understanding_by_Voxel-CRF.html">2 iccv-2013-3D Scene Understanding by Voxel-CRF</a></p>
<p>16 0.041650638 <a title="148-tfidf-16" href="./iccv-2013-SGTD%3A_Structure_Gradient_and_Texture_Decorrelating_Regularization_for_Image_Decomposition.html">364 iccv-2013-SGTD: Structure Gradient and Texture Decorrelating Regularization for Image Decomposition</a></p>
<p>17 0.039865006 <a title="148-tfidf-17" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>18 0.039357092 <a title="148-tfidf-18" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>19 0.038512003 <a title="148-tfidf-19" href="./iccv-2013-Automatic_Kronecker_Product_Model_Based_Detection_of_Repeated_Patterns_in_2D_Urban_Images.html">55 iccv-2013-Automatic Kronecker Product Model Based Detection of Repeated Patterns in 2D Urban Images</a></p>
<p>20 0.035549801 <a title="148-tfidf-20" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.108), (1, -0.024), (2, -0.009), (3, -0.019), (4, 0.02), (5, 0.001), (6, -0.015), (7, -0.035), (8, -0.016), (9, -0.059), (10, 0.008), (11, 0.003), (12, -0.001), (13, 0.027), (14, -0.003), (15, 0.003), (16, 0.007), (17, 0.009), (18, -0.014), (19, -0.011), (20, -0.024), (21, 0.003), (22, 0.031), (23, 0.014), (24, -0.01), (25, 0.01), (26, -0.009), (27, 0.0), (28, -0.017), (29, -0.061), (30, -0.021), (31, 0.038), (32, 0.05), (33, -0.001), (34, 0.021), (35, 0.031), (36, -0.043), (37, -0.002), (38, -0.038), (39, -0.052), (40, 0.041), (41, -0.081), (42, -0.067), (43, -0.016), (44, 0.06), (45, 0.057), (46, -0.063), (47, -0.021), (48, -0.034), (49, -0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89536405 <a title="148-lsi-1" href="./iccv-2013-Example-Based_Facade_Texture_Synthesis.html">148 iccv-2013-Example-Based Facade Texture Synthesis</a></p>
<p>Author: Dengxin Dai, Hayko Riemenschneider, Gerhard Schmitt, Luc Van_Gool</p><p>Abstract: There is an increased interest in the efficient creation of city models, be it virtual or as-built. We present a method for synthesizing complex, photo-realistic facade images, from a single example. After parsing the example image into its semantic components, a tiling for it is generated. Novel tilings can then be created, yielding facade textures with different dimensions or with occluded parts inpainted. A genetic algorithm guides the novel facades as well as inpainted parts to be consistent with the example, both in terms of their overall structure and their detailed textures. Promising results for multiple standard datasets in particular for the different building styles they contain demonstrate the potential of the method. – –</p><p>2 0.58137232 <a title="148-lsi-2" href="./iccv-2013-Rectangling_Stereographic_Projection_for_Wide-Angle_Image_Visualization.html">346 iccv-2013-Rectangling Stereographic Projection for Wide-Angle Image Visualization</a></p>
<p>Author: Che-Han Chang, Min-Chun Hu, Wen-Huang Cheng, Yung-Yu Chuang</p><p>Abstract: This paper proposes a new projection model for mapping a hemisphere to a plane. Such a model can be useful for viewing wide-angle images. Our model consists of two steps. In the first step, the hemisphere is projected onto a swung surface constructed by a circular profile and a rounded rectangular trajectory. The second step maps the projected image on the swung surface onto the image plane through the perspective projection. We also propose a method for automatically determining proper parameters for the projection model based on image content. The proposed model has several advantages. It is simple, efficient and easy to control. Most importantly, it makes a better compromise between distortion minimization and line preserving than popular projection models, such as stereographic and Pannini projections. Experiments and analysis demonstrate the effectiveness of our model.</p><p>3 0.57499635 <a title="148-lsi-3" href="./iccv-2013-Detecting_Irregular_Curvilinear_Structures_in_Gray_Scale_and_Color_Imagery_Using_Multi-directional_Oriented_Flux.html">112 iccv-2013-Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-directional Oriented Flux</a></p>
<p>Author: Engin Türetken, Carlos Becker, Przemyslaw Glowacki, Fethallah Benmansour, Pascal Fua</p><p>Abstract: We propose a new approach to detecting irregular curvilinear structures in noisy image stacks. In contrast to earlier approaches that rely on circular models of the crosssections, ours allows for the arbitrarily-shaped ones that are prevalent in biological imagery. This is achieved by maximizing the image gradient flux along multiple directions and radii, instead of only two with a unique radius as is usually done. This yields a more complex optimization problem for which we propose a computationally efficient solution. We demonstrate the effectiveness of our approach on a wide range ofchallenging gray scale and color datasets and show that it outperforms existing techniques, especially on very irregular structures.</p><p>4 0.56215626 <a title="148-lsi-4" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>Author: Bastien Jacquet, Christian Häne, Kevin Köser, Marc Pollefeys</p><p>Abstract: Although specular objects have gained interest in recent years, virtually no approaches exist for markerless reconstruction of reflective scenes in the wild. In this work, we present a practical approach to capturing normal maps in real-world scenes using video only. We focus on nearly planar surfaces such as windows, facades from glass or metal, or frames, screens and other indoor objects and show how normal maps of these can be obtained without the use of an artificial calibration object. Rather, we track the reflections of real-world straight lines, while moving with a hand-held or vehicle-mounted camera in front of the object. In contrast to error-prone local edge tracking, we obtain the reflections by a robust, global segmentation technique of an ortho-rectified 3D video cube that also naturally allows efficient user interaction. Then, at each point of the reflective surface, the resulting 2D-curve to 3D-line correspondence provides a novel quadratic constraint on the local surface normal. This allows to globally solve for the shape by integrability and smoothness constraints and easily supports the usage of multiple lines. We demonstrate the technique on several objects and facades.</p><p>5 0.53623515 <a title="148-lsi-5" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>Author: Ruiqi Guo, Derek Hoiem</p><p>Abstract: In this paper, we present an approach to predict the extent and height of supporting surfaces such as tables, chairs, and cabinet tops from a single RGBD image. We define support surfaces to be horizontal, planar surfaces that can physically support objects and humans. Given a RGBD image, our goal is to localize the height and full extent of such surfaces in 3D space. To achieve this, we created a labeling tool and annotated 1449 images with rich, complete 3D scene models in NYU dataset. We extract ground truth from the annotated dataset and developed a pipeline for predicting floor space, walls, the height and full extent of support surfaces. Finally we match the predicted extent with annotated scenes in training scenes and transfer the the support surface configuration from training scenes. We evaluate the proposed approach in our dataset and demonstrate its effectiveness in understanding scenes in 3D space.</p><p>6 0.53377503 <a title="148-lsi-6" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>7 0.52789211 <a title="148-lsi-7" href="./iccv-2013-Scene_Collaging%3A_Analysis_and_Synthesis_of_Natural_Images_with_Semantic_Layers.html">375 iccv-2013-Scene Collaging: Analysis and Synthesis of Natural Images with Semantic Layers</a></p>
<p>8 0.51703966 <a title="148-lsi-8" href="./iccv-2013-Lifting_3D_Manhattan_Lines_from_a_Single_Image.html">250 iccv-2013-Lifting 3D Manhattan Lines from a Single Image</a></p>
<p>9 0.51584131 <a title="148-lsi-9" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>10 0.51552927 <a title="148-lsi-10" href="./iccv-2013-Paper_Doll_Parsing%3A_Retrieving_Similar_Styles_to_Parse_Clothing_Items.html">306 iccv-2013-Paper Doll Parsing: Retrieving Similar Styles to Parse Clothing Items</a></p>
<p>11 0.51394308 <a title="148-lsi-11" href="./iccv-2013-Exploiting_Reflection_Change_for_Automatic_Reflection_Removal.html">151 iccv-2013-Exploiting Reflection Change for Automatic Reflection Removal</a></p>
<p>12 0.5095492 <a title="148-lsi-12" href="./iccv-2013-Data-Driven_3D_Primitives_for_Single_Image_Understanding.html">102 iccv-2013-Data-Driven 3D Primitives for Single Image Understanding</a></p>
<p>13 0.49990121 <a title="148-lsi-13" href="./iccv-2013-3D_Scene_Understanding_by_Voxel-CRF.html">2 iccv-2013-3D Scene Understanding by Voxel-CRF</a></p>
<p>14 0.49402216 <a title="148-lsi-14" href="./iccv-2013-Complex_3D_General_Object_Reconstruction_from_Line_Drawings.html">84 iccv-2013-Complex 3D General Object Reconstruction from Line Drawings</a></p>
<p>15 0.48238322 <a title="148-lsi-15" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>16 0.47752419 <a title="148-lsi-16" href="./iccv-2013-A_Deformable_Mixture_Parsing_Model_with_Parselets.html">8 iccv-2013-A Deformable Mixture Parsing Model with Parselets</a></p>
<p>17 0.47089842 <a title="148-lsi-17" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>18 0.46461296 <a title="148-lsi-18" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>19 0.45755649 <a title="148-lsi-19" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>20 0.45521557 <a title="148-lsi-20" href="./iccv-2013-3D_Sub-query_Expansion_for_Improving_Sketch-Based_Multi-view_Image_Retrieval.html">3 iccv-2013-3D Sub-query Expansion for Improving Sketch-Based Multi-view Image Retrieval</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.058), (12, 0.019), (26, 0.058), (31, 0.05), (35, 0.021), (40, 0.019), (42, 0.077), (48, 0.021), (50, 0.269), (64, 0.052), (73, 0.025), (78, 0.02), (89, 0.182), (98, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79465711 <a title="148-lda-1" href="./iccv-2013-Example-Based_Facade_Texture_Synthesis.html">148 iccv-2013-Example-Based Facade Texture Synthesis</a></p>
<p>Author: Dengxin Dai, Hayko Riemenschneider, Gerhard Schmitt, Luc Van_Gool</p><p>Abstract: There is an increased interest in the efficient creation of city models, be it virtual or as-built. We present a method for synthesizing complex, photo-realistic facade images, from a single example. After parsing the example image into its semantic components, a tiling for it is generated. Novel tilings can then be created, yielding facade textures with different dimensions or with occluded parts inpainted. A genetic algorithm guides the novel facades as well as inpainted parts to be consistent with the example, both in terms of their overall structure and their detailed textures. Promising results for multiple standard datasets in particular for the different building styles they contain demonstrate the potential of the method. – –</p><p>2 0.75344104 <a title="148-lda-2" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<p>Author: Salil Tambe, Ashok Veeraraghavan, Amit Agrawal</p><p>Abstract: Current Light Field (LF) cameras offer fixed resolution in space, time and angle which is decided a-priori and is independent of the scene. These cameras either trade-off spatial resolution to capture single-shot LF [20, 27, 12] or tradeoff temporal resolution by assuming a static scene to capture high spatial resolution LF [18, 3]. Thus, capturing high spatial resolution LF video for dynamic scenes remains an open and challenging problem. We present the concept, design and implementation of a LF video camera that allows capturing high resolution LF video. The spatial, angular and temporal resolution are not fixed a-priori and we exploit the scene-specific redundancy in space, time and angle. Our reconstruction is motion-aware and offers a continuum of resolution tradeoff with increasing motion in the scene. The key idea is (a) to design efficient multiplexing matrices that allow resolution tradeoffs, (b) use dictionary learning and sparse repre- sentations for robust reconstruction, and (c) perform local motion-aware adaptive reconstruction. We perform extensive analysis and characterize the performance of our motion-aware reconstruction algorithm. We show realistic simulations using a graphics simulator as well as real results using a LCoS based programmable camera. We demonstrate novel results such as high resolution digital refocusing for dynamic moving objects.</p><p>3 0.7383638 <a title="148-lda-3" href="./iccv-2013-Facial_Action_Unit_Event_Detection_by_Cascade_of_Tasks.html">155 iccv-2013-Facial Action Unit Event Detection by Cascade of Tasks</a></p>
<p>Author: Xiaoyu Ding, Wen-Sheng Chu, Fernando De_La_Torre, Jeffery F. Cohn, Qiao Wang</p><p>Abstract: Automatic facial Action Unit (AU) detection from video is a long-standing problem in facial expression analysis. AU detection is typically posed as a classification problem between frames or segments of positive examples and negative ones, where existing work emphasizes the use of different features or classifiers. In this paper, we propose a method called Cascade of Tasks (CoT) that combines the use ofdifferent tasks (i.e., , frame, segment and transition)for AU event detection. We train CoT in a sequential manner embracing diversity, which ensures robustness and generalization to unseen data. In addition to conventional framebased metrics that evaluate frames independently, we propose a new event-based metric to evaluate detection performance at event-level. We show how the CoT method consistently outperforms state-of-the-art approaches in both frame-based and event-based metrics, across three public datasets that differ in complexity: CK+, FERA and RUFACS.</p><p>4 0.71477711 <a title="148-lda-4" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>Author: Feng Shi, Zhong Zhou, Jiangjian Xiao, Wei Wu</p><p>Abstract: Due to occlusions and objects ’ non-rigid deformation in the scene, the obtained motion trajectories from common trackers may contain a number of missing or mis-associated entries. To cluster such corrupted point based trajectories into multiple motions is still a hard problem. In this paper, we present an approach that exploits temporal and spatial characteristics from tracked points to facilitate segmentation of incomplete and corrupted trajectories, thereby obtain highly robust results against severe data missing and noises. Our method first uses the Discrete Cosine Transform (DCT) bases as a temporal smoothness constraint on trajectory projection to ensure the validity of resulting components to repair pathological trajectories. Then, based on an observation that the trajectories of foreground and background in a scene may have different spatial distributions, we propose a two-stage clustering strategy that first performs foreground-background separation then segments remaining foreground trajectories. We show that, with this new clustering strategy, sequences with complex motions can be accurately segmented by even using a simple trans- lational model. Finally, a series of experiments on Hopkins 155 dataset andBerkeley motion segmentation dataset show the advantage of our method over other state-of-the-art motion segmentation algorithms in terms of both effectiveness and robustness.</p><p>5 0.66795546 <a title="148-lda-5" href="./iccv-2013-Find_the_Best_Path%3A_An_Efficient_and_Accurate_Classifier_for_Image_Hierarchies.html">165 iccv-2013-Find the Best Path: An Efficient and Accurate Classifier for Image Hierarchies</a></p>
<p>Author: Min Sun, Wan Huang, Silvio Savarese</p><p>Abstract: Many methods have been proposed to solve the image classification problem for a large number of categories. Among them, methods based on tree-based representations achieve good trade-off between accuracy and test time efficiency. While focusing on learning a tree-shaped hierarchy and the corresponding set of classifiers, most of them [11, 2, 14] use a greedy prediction algorithm for test time efficiency. We argue that the dramatic decrease in accuracy at high efficiency is caused by the specific design choice of the learning and greedy prediction algorithms. In this work, we propose a classifier which achieves a better trade-off between efficiency and accuracy with a given tree-shaped hierarchy. First, we convert the classification problem as finding the best path in the hierarchy, and a novel branchand-bound-like algorithm is introduced to efficiently search for the best path. Second, we jointly train the classifiers using a novel Structured SVM (SSVM) formulation with additional bound constraints. As a result, our method achieves a significant 4.65%, 5.43%, and 4.07% (relative 24.82%, 41.64%, and 109.79%) improvement in accuracy at high efficiency compared to state-of-the-art greedy “tree-based” methods [14] on Caltech-256 [15], SUN [32] and ImageNet 1K [9] dataset, respectively. Finally, we show that our branch-and-bound-like algorithm naturally ranks the paths in the hierarchy (Fig. 8) so that users can further process them.</p><p>6 0.65958196 <a title="148-lda-6" href="./iccv-2013-Exemplar-Based_Graph_Matching_for_Robust_Facial_Landmark_Localization.html">149 iccv-2013-Exemplar-Based Graph Matching for Robust Facial Landmark Localization</a></p>
<p>7 0.63667834 <a title="148-lda-7" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>8 0.6346193 <a title="148-lda-8" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>9 0.63449222 <a title="148-lda-9" href="./iccv-2013-Scene_Collaging%3A_Analysis_and_Synthesis_of_Natural_Images_with_Semantic_Layers.html">375 iccv-2013-Scene Collaging: Analysis and Synthesis of Natural Images with Semantic Layers</a></p>
<p>10 0.63443315 <a title="148-lda-10" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>11 0.63366616 <a title="148-lda-11" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>12 0.63322347 <a title="148-lda-12" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>13 0.63318777 <a title="148-lda-13" href="./iccv-2013-Higher_Order_Matching_for_Consistent_Multiple_Target_Tracking.html">200 iccv-2013-Higher Order Matching for Consistent Multiple Target Tracking</a></p>
<p>14 0.63205117 <a title="148-lda-14" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>15 0.6318965 <a title="148-lda-15" href="./iccv-2013-Modeling_4D_Human-Object_Interactions_for_Event_and_Object_Recognition.html">268 iccv-2013-Modeling 4D Human-Object Interactions for Event and Object Recognition</a></p>
<p>16 0.63185143 <a title="148-lda-16" href="./iccv-2013-Dynamic_Probabilistic_Volumetric_Models.html">128 iccv-2013-Dynamic Probabilistic Volumetric Models</a></p>
<p>17 0.63184226 <a title="148-lda-17" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>18 0.63172293 <a title="148-lda-18" href="./iccv-2013-A_Non-parametric_Bayesian_Network_Prior_of_Human_Pose.html">24 iccv-2013-A Non-parametric Bayesian Network Prior of Human Pose</a></p>
<p>19 0.63163704 <a title="148-lda-19" href="./iccv-2013-Volumetric_Semantic_Segmentation_Using_Pyramid_Context_Features.html">447 iccv-2013-Volumetric Semantic Segmentation Using Pyramid Context Features</a></p>
<p>20 0.6315226 <a title="148-lda-20" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
