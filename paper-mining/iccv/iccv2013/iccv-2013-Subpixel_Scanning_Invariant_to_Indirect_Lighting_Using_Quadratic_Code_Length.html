<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-407" href="#">iccv2013-407</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</h1>
<br/><p>Source: <a title="iccv-2013-407-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Martin_Subpixel_Scanning_Invariant_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Nicolas Martin, Vincent Couture, Sébastien Roy</p><p>Abstract: We present a scanning method that recovers dense subpixel camera-projector correspondence without requiring any photometric calibration nor preliminary knowledge of their relative geometry. Subpixel accuracy is achieved by considering several zero-crossings defined by the difference between pairs of unstructured patterns. We use gray-level band-pass white noise patterns that increase robustness to indirect lighting and scene discontinuities. Simulated and experimental results show that our method recovers scene geometry with high subpixel precision, and that it can handle many challenges of active reconstruction systems. We compare our results to state of the art methods such as micro phase shifting and modulated phase shifting.</p><p>Reference: <a title="iccv-2013-407-reference" href="../iccv2013_reference/iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('subpixel', 0.694), ('microp', 0.282), ('indirect', 0.177), ('bit', 0.171), ('phas', 0.164), ('pattern', 0.16), ('unwrap', 0.159), ('uq', 0.151), ('reconstruct', 0.129), ('pj', 0.112), ('unstruct', 0.112), ('quadr', 0.107), ('frequ', 0.103), ('modp', 0.103), ('gamm', 0.099), ('pi', 0.095), ('shift', 0.09), ('micro', 0.087), ('scan', 0.079), ('umont', 0.077)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="407-tfidf-1" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<p>Author: Nicolas Martin, Vincent Couture, Sébastien Roy</p><p>Abstract: We present a scanning method that recovers dense subpixel camera-projector correspondence without requiring any photometric calibration nor preliminary knowledge of their relative geometry. Subpixel accuracy is achieved by considering several zero-crossings defined by the difference between pairs of unstructured patterns. We use gray-level band-pass white noise patterns that increase robustness to indirect lighting and scene discontinuities. Simulated and experimental results show that our method recovers scene geometry with high subpixel precision, and that it can handle many challenges of active reconstruction systems. We compare our results to state of the art methods such as micro phase shifting and modulated phase shifting.</p><p>2 0.13136828 <a title="407-tfidf-2" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>Author: Michael Weinmann, Aljosa Osep, Roland Ruiters, Reinhard Klein</p><p>Abstract: In this paper, we present a novel, robust multi-view normal field integration technique for reconstructing the full 3D shape of mirroring objects. We employ a turntablebased setup with several cameras and displays. These are used to display illumination patterns which are reflected by the object surface. The pattern information observed in the cameras enables the calculation of individual volumetric normal fields for each combination of camera, display and turntable angle. As the pattern information might be blurred depending on the surface curvature or due to nonperfect mirroring surface characteristics, we locally adapt the decoding to the finest still resolvable pattern resolution. In complex real-world scenarios, the normal fields contain regions without observations due to occlusions and outliers due to interreflections and noise. Therefore, a robust reconstruction using only normal information is challenging. Via a non-parametric clustering of normal hypotheses derived for each point in the scene, we obtain both the most likely local surface normal and a local surface consistency estimate. This information is utilized in an iterative mincut based variational approach to reconstruct the surface geometry.</p><p>3 0.11975898 <a title="407-tfidf-3" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>Author: Supreeth Achar, Stephen T. Nuske, Srinivasa G. Narasimhan</p><p>Abstract: Separating the direct and global components of radiance can aid shape recovery algorithms and can provide useful information about materials in a scene. Practical methods for finding the direct and global components use multiple images captured under varying illumination patterns and require the scene, light source and camera to remain stationary during the image acquisition process. In this paper, we develop a motion compensation method that relaxes this condition and allows direct-global separation to beperformed on video sequences of dynamic scenes captured by moving projector-camera systems. Key to our method is being able to register frames in a video sequence to each other in the presence of time varying, high frequency active illumination patterns. We compare our motion compensated method to alternatives such as single shot separation and frame interleaving as well as ground truth. We present results on challenging video sequences that include various types of motions and deformations in scenes that contain complex materials like fabric, skin, leaves and wax.</p><p>4 0.11903664 <a title="407-tfidf-4" href="./iccv-2013-High_Quality_Shape_from_a_Single_RGB-D_Image_under_Uncalibrated_Natural_Illumination.html">199 iccv-2013-High Quality Shape from a Single RGB-D Image under Uncalibrated Natural Illumination</a></p>
<p>Author: Yudeog Han, Joon-Young Lee, In So Kweon</p><p>Abstract: We present a novel framework to estimate detailed shape of diffuse objects with uniform albedo from a single RGB-D image. To estimate accurate lighting in natural illumination environment, we introduce a general lighting model consisting oftwo components: global and local models. The global lighting model is estimated from the RGB-D input using the low-dimensional characteristic of a diffuse reflectance model. The local lighting model represents spatially varying illumination and it is estimated by using the smoothlyvarying characteristic of illumination. With both the global and local lighting model, we can estimate complex lighting variations in uncontrolled natural illumination conditions accurately. For high quality shape capture, a shapefrom-shading approach is applied with the estimated lighting model. Since the entire process is done with a single RGB-D input, our method is capable of capturing the high quality shape details of a dynamic object under natural illumination. Experimental results demonstrate the feasibility and effectiveness of our method that dramatically improves shape details of the rough depth input.</p><p>5 0.11796515 <a title="407-tfidf-5" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>Author: Benjamin Ummenhofer, Thomas Brox</p><p>Abstract: 3D reconstruction deals with the problem of finding the shape of an object from a set of images. Thin objects that have virtually no volumepose a special challengefor reconstruction with respect to shape representation and fusion of depth information. In this paper we present a dense pointbased reconstruction method that can deal with this special class of objects. We seek to jointly optimize a set of depth maps by treating each pixel as a point in space. Points are pulled towards a common surface by pairwise forces in an iterative scheme. The method also handles the problem of opposed surfaces by means of penalty forces. Efficient optimization is achieved by grouping points to superpixels and a spatial hashing approach for fast neighborhood queries. We show that the approach is on a par with state-of-the-art methods for standard multi view stereo settings and gives superior results for thin objects.</p><p>6 0.11137806 <a title="407-tfidf-6" href="./iccv-2013-Modeling_the_Calibration_Pipeline_of_the_Lytro_Camera_for_High_Quality_Light-Field_Image_Reconstruction.html">271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</a></p>
<p>7 0.10693671 <a title="407-tfidf-7" href="./iccv-2013-Local_Signal_Equalization_for_Correspondence_Matching.html">255 iccv-2013-Local Signal Equalization for Correspondence Matching</a></p>
<p>8 0.088824138 <a title="407-tfidf-8" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>9 0.087012693 <a title="407-tfidf-9" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>10 0.084163316 <a title="407-tfidf-10" href="./iccv-2013-Structured_Light_in_Sunlight.html">405 iccv-2013-Structured Light in Sunlight</a></p>
<p>11 0.083464243 <a title="407-tfidf-11" href="./iccv-2013-Space-Time_Tradeoffs_in_Photo_Sequencing.html">397 iccv-2013-Space-Time Tradeoffs in Photo Sequencing</a></p>
<p>12 0.08322712 <a title="407-tfidf-12" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>13 0.083199464 <a title="407-tfidf-13" href="./iccv-2013-A_Global_Linear_Method_for_Camera_Pose_Registration.html">17 iccv-2013-A Global Linear Method for Camera Pose Registration</a></p>
<p>14 0.081226565 <a title="407-tfidf-14" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>15 0.075245515 <a title="407-tfidf-15" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>16 0.074816838 <a title="407-tfidf-16" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>17 0.074743859 <a title="407-tfidf-17" href="./iccv-2013-Shape_Anchors_for_Data-Driven_Multi-view_Reconstruction.html">387 iccv-2013-Shape Anchors for Data-Driven Multi-view Reconstruction</a></p>
<p>18 0.072157897 <a title="407-tfidf-18" href="./iccv-2013-A_Simple_Model_for_Intrinsic_Image_Decomposition_with_Depth_Cues.html">30 iccv-2013-A Simple Model for Intrinsic Image Decomposition with Depth Cues</a></p>
<p>19 0.069939956 <a title="407-tfidf-19" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>20 0.069858275 <a title="407-tfidf-20" href="./iccv-2013-Multi-view_3D_Reconstruction_from_Uncalibrated_Radially-Symmetric_Cameras.html">280 iccv-2013-Multi-view 3D Reconstruction from Uncalibrated Radially-Symmetric Cameras</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.143), (1, -0.002), (2, 0.071), (3, 0.136), (4, 0.021), (5, 0.093), (6, -0.022), (7, 0.017), (8, -0.027), (9, 0.011), (10, 0.025), (11, -0.006), (12, -0.002), (13, 0.014), (14, 0.001), (15, 0.021), (16, -0.006), (17, -0.074), (18, 0.008), (19, -0.063), (20, 0.023), (21, -0.045), (22, 0.026), (23, 0.005), (24, -0.04), (25, -0.004), (26, 0.023), (27, 0.003), (28, -0.001), (29, -0.023), (30, -0.017), (31, -0.013), (32, 0.029), (33, 0.015), (34, 0.02), (35, 0.021), (36, -0.007), (37, -0.022), (38, 0.091), (39, -0.018), (40, -0.032), (41, -0.008), (42, 0.081), (43, -0.01), (44, -0.006), (45, 0.058), (46, -0.008), (47, -0.003), (48, 0.027), (49, -0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93558341 <a title="407-lsi-1" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<p>Author: Nicolas Martin, Vincent Couture, Sébastien Roy</p><p>Abstract: We present a scanning method that recovers dense subpixel camera-projector correspondence without requiring any photometric calibration nor preliminary knowledge of their relative geometry. Subpixel accuracy is achieved by considering several zero-crossings defined by the difference between pairs of unstructured patterns. We use gray-level band-pass white noise patterns that increase robustness to indirect lighting and scene discontinuities. Simulated and experimental results show that our method recovers scene geometry with high subpixel precision, and that it can handle many challenges of active reconstruction systems. We compare our results to state of the art methods such as micro phase shifting and modulated phase shifting.</p><p>2 0.82356232 <a title="407-lsi-2" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>Author: Michael Weinmann, Aljosa Osep, Roland Ruiters, Reinhard Klein</p><p>Abstract: In this paper, we present a novel, robust multi-view normal field integration technique for reconstructing the full 3D shape of mirroring objects. We employ a turntablebased setup with several cameras and displays. These are used to display illumination patterns which are reflected by the object surface. The pattern information observed in the cameras enables the calculation of individual volumetric normal fields for each combination of camera, display and turntable angle. As the pattern information might be blurred depending on the surface curvature or due to nonperfect mirroring surface characteristics, we locally adapt the decoding to the finest still resolvable pattern resolution. In complex real-world scenarios, the normal fields contain regions without observations due to occlusions and outliers due to interreflections and noise. Therefore, a robust reconstruction using only normal information is challenging. Via a non-parametric clustering of normal hypotheses derived for each point in the scene, we obtain both the most likely local surface normal and a local surface consistency estimate. This information is utilized in an iterative mincut based variational approach to reconstruct the surface geometry.</p><p>3 0.80864817 <a title="407-lsi-3" href="./iccv-2013-Modeling_the_Calibration_Pipeline_of_the_Lytro_Camera_for_High_Quality_Light-Field_Image_Reconstruction.html">271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</a></p>
<p>Author: Donghyeon Cho, Minhaeng Lee, Sunyeong Kim, Yu-Wing Tai</p><p>Abstract: Light-field imaging systems have got much attention recently as the next generation camera model. A light-field imaging system consists of three parts: data acquisition, manipulation, and application. Given an acquisition system, it is important to understand how a light-field camera converts from its raw image to its resulting refocused image. In this paper, using the Lytro camera as an example, we describe step-by-step procedures to calibrate a raw light-field image. In particular, we are interested in knowing the spatial and angular coordinates of the micro lens array and the resampling process for image reconstruction. Since Lytro uses a hexagonal arrangement of a micro lens image, additional treatments in calibration are required. After calibration, we analyze and compare the performances of several resampling methods for image reconstruction with and without calibration. Finally, a learning based interpolation method is proposed which demonstrates a higher quality image reconstruction than previous interpolation methods including a method used in Lytro software.</p><p>4 0.72907317 <a title="407-lsi-4" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<p>Author: Salil Tambe, Ashok Veeraraghavan, Amit Agrawal</p><p>Abstract: Current Light Field (LF) cameras offer fixed resolution in space, time and angle which is decided a-priori and is independent of the scene. These cameras either trade-off spatial resolution to capture single-shot LF [20, 27, 12] or tradeoff temporal resolution by assuming a static scene to capture high spatial resolution LF [18, 3]. Thus, capturing high spatial resolution LF video for dynamic scenes remains an open and challenging problem. We present the concept, design and implementation of a LF video camera that allows capturing high resolution LF video. The spatial, angular and temporal resolution are not fixed a-priori and we exploit the scene-specific redundancy in space, time and angle. Our reconstruction is motion-aware and offers a continuum of resolution tradeoff with increasing motion in the scene. The key idea is (a) to design efficient multiplexing matrices that allow resolution tradeoffs, (b) use dictionary learning and sparse repre- sentations for robust reconstruction, and (c) perform local motion-aware adaptive reconstruction. We perform extensive analysis and characterize the performance of our motion-aware reconstruction algorithm. We show realistic simulations using a graphics simulator as well as real results using a LCoS based programmable camera. We demonstrate novel results such as high resolution digital refocusing for dynamic moving objects.</p><p>5 0.70206559 <a title="407-lsi-5" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>Author: Bastien Jacquet, Christian Häne, Kevin Köser, Marc Pollefeys</p><p>Abstract: Although specular objects have gained interest in recent years, virtually no approaches exist for markerless reconstruction of reflective scenes in the wild. In this work, we present a practical approach to capturing normal maps in real-world scenes using video only. We focus on nearly planar surfaces such as windows, facades from glass or metal, or frames, screens and other indoor objects and show how normal maps of these can be obtained without the use of an artificial calibration object. Rather, we track the reflections of real-world straight lines, while moving with a hand-held or vehicle-mounted camera in front of the object. In contrast to error-prone local edge tracking, we obtain the reflections by a robust, global segmentation technique of an ortho-rectified 3D video cube that also naturally allows efficient user interaction. Then, at each point of the reflective surface, the resulting 2D-curve to 3D-line correspondence provides a novel quadratic constraint on the local surface normal. This allows to globally solve for the shape by integrability and smoothness constraints and easily supports the usage of multiple lines. We demonstrate the technique on several objects and facades.</p><p>6 0.67546427 <a title="407-lsi-6" href="./iccv-2013-Multi-view_3D_Reconstruction_from_Uncalibrated_Radially-Symmetric_Cameras.html">280 iccv-2013-Multi-view 3D Reconstruction from Uncalibrated Radially-Symmetric Cameras</a></p>
<p>7 0.66669136 <a title="407-lsi-7" href="./iccv-2013-Matching_Dry_to_Wet_Materials.html">262 iccv-2013-Matching Dry to Wet Materials</a></p>
<p>8 0.6643303 <a title="407-lsi-8" href="./iccv-2013-High_Quality_Shape_from_a_Single_RGB-D_Image_under_Uncalibrated_Natural_Illumination.html">199 iccv-2013-High Quality Shape from a Single RGB-D Image under Uncalibrated Natural Illumination</a></p>
<p>9 0.66288209 <a title="407-lsi-9" href="./iccv-2013-Structured_Light_in_Sunlight.html">405 iccv-2013-Structured Light in Sunlight</a></p>
<p>10 0.66249979 <a title="407-lsi-10" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>11 0.66015452 <a title="407-lsi-11" href="./iccv-2013-Incorporating_Cloud_Distribution_in_Sky_Representation.html">215 iccv-2013-Incorporating Cloud Distribution in Sky Representation</a></p>
<p>12 0.65873456 <a title="407-lsi-12" href="./iccv-2013-Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization.html">284 iccv-2013-Multiview Photometric Stereo Using Planar Mesh Parameterization</a></p>
<p>13 0.6478039 <a title="407-lsi-13" href="./iccv-2013-Extrinsic_Camera_Calibration_without_a_Direct_View_Using_Spherical_Mirror.html">152 iccv-2013-Extrinsic Camera Calibration without a Direct View Using Spherical Mirror</a></p>
<p>14 0.63996279 <a title="407-lsi-14" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>15 0.63470739 <a title="407-lsi-15" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>16 0.63029367 <a title="407-lsi-16" href="./iccv-2013-Refractive_Structure-from-Motion_on_Underwater_Images.html">348 iccv-2013-Refractive Structure-from-Motion on Underwater Images</a></p>
<p>17 0.62101579 <a title="407-lsi-17" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>18 0.6102953 <a title="407-lsi-18" href="./iccv-2013-Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illumination_in_the_Spectral_Domain.html">385 iccv-2013-Separating Reflective and Fluorescent Components Using High Frequency Illumination in the Spectral Domain</a></p>
<p>19 0.60834157 <a title="407-lsi-19" href="./iccv-2013-Large-Scale_Multi-resolution_Surface_Reconstruction_from_RGB-D_Sequences.html">228 iccv-2013-Large-Scale Multi-resolution Surface Reconstruction from RGB-D Sequences</a></p>
<p>20 0.60570353 <a title="407-lsi-20" href="./iccv-2013-A_Simple_Model_for_Intrinsic_Image_Decomposition_with_Depth_Cues.html">30 iccv-2013-A Simple Model for Intrinsic Image Decomposition with Depth Cues</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.247), (13, 0.075), (18, 0.013), (20, 0.084), (25, 0.05), (42, 0.148), (48, 0.128), (50, 0.01), (55, 0.022), (60, 0.025), (77, 0.071), (89, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.7551375 <a title="407-lda-1" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>Author: Qian-Yi Zhou, Stephen Miller, Vladlen Koltun</p><p>Abstract: We present an approach to reconstruction of detailed scene geometry from range video. Range data produced by commodity handheld cameras suffers from high-frequency errors and low-frequency distortion. Our approach deals with both sources of error by reconstructing locally smooth scene fragments and letting these fragments deform in order to align to each other. We develop a volumetric registration formulation that leverages the smoothness of the deformation to make optimization practical for large scenes. Experimental results demonstrate that our approach substantially increases the fidelity of complex scene geometry reconstructed with commodity handheld cameras.</p><p>same-paper 2 0.74592352 <a title="407-lda-2" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<p>Author: Nicolas Martin, Vincent Couture, Sébastien Roy</p><p>Abstract: We present a scanning method that recovers dense subpixel camera-projector correspondence without requiring any photometric calibration nor preliminary knowledge of their relative geometry. Subpixel accuracy is achieved by considering several zero-crossings defined by the difference between pairs of unstructured patterns. We use gray-level band-pass white noise patterns that increase robustness to indirect lighting and scene discontinuities. Simulated and experimental results show that our method recovers scene geometry with high subpixel precision, and that it can handle many challenges of active reconstruction systems. We compare our results to state of the art methods such as micro phase shifting and modulated phase shifting.</p><p>3 0.68640828 <a title="407-lda-3" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>Author: Vidit Jain, Sachin Sudhakar Farfade</p><p>Abstract: Classification cascades have been very effective for object detection. Such a cascade fails to perform well in data domains with variations in appearances that may not be captured in the training examples. This limited generalization severely restricts the domains for which they can be used effectively. A common approach to address this limitation is to train a new cascade of classifiers from scratch for each of the new domains. Building separate detectors for each of the different domains requires huge annotation and computational effort, making it not scalable to a large number of data domains. Here we present an algorithm for quickly adapting a pre-trained cascade of classifiers using a small number oflabeledpositive instancesfrom a different yet similar data domain. In our experiments with images of human babies and human-like characters from movies, we demonstrate that the adapted cascade significantly outperforms both of the original cascade and the one trained from scratch using the given training examples. –</p><p>4 0.68393171 <a title="407-lda-4" href="./iccv-2013-Video_Motion_for_Every_Visible_Point.html">441 iccv-2013-Video Motion for Every Visible Point</a></p>
<p>Author: Susanna Ricco, Carlo Tomasi</p><p>Abstract: Dense motion of image points over many video frames can provide important information about the world. However, occlusions and drift make it impossible to compute long motionpaths by merely concatenating opticalflow vectors between consecutive frames. Instead, we solve for entire paths directly, and flag the frames in which each is visible. As in previous work, we anchor each path to a unique pixel which guarantees an even spatial distribution of paths. Unlike earlier methods, we allow paths to be anchored in any frame. By explicitly requiring that at least one visible path passes within a small neighborhood of every pixel, we guarantee complete coverage of all visible points in all frames. We achieve state-of-the-art results on real sequences including both rigid and non-rigid motions with significant occlusions.</p><p>5 0.6813814 <a title="407-lda-5" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>Author: Yuning Chai, Victor Lempitsky, Andrew Zisserman</p><p>Abstract: We propose a new method for the task of fine-grained visual categorization. The method builds a model of the baselevel category that can be fitted to images, producing highquality foreground segmentation and mid-level part localizations. The model can be learnt from the typical datasets available for fine-grained categorization, where the only annotation provided is a loose bounding box around the instance (e.g. bird) in each image. Both segmentation and part localizations are then used to encode the image content into a highly-discriminative visual signature. The model is symbiotic in that part discovery/localization is helped by segmentation and, conversely, the segmentation is helped by the detection (e.g. part layout). Our model builds on top of the part-based object category detector of Felzenszwalb et al., and also on the powerful GrabCut segmentation algorithm of Rother et al., and adds a simple spatial saliency coupling between them. In our evaluation, the model improves the categorization accuracy over the state-of-the-art. It also improves over what can be achieved with an analogous system that runs segmentation and part-localization independently.</p><p>6 0.67942524 <a title="407-lda-6" href="./iccv-2013-Measuring_Flow_Complexity_in_Videos.html">263 iccv-2013-Measuring Flow Complexity in Videos</a></p>
<p>7 0.67674327 <a title="407-lda-7" href="./iccv-2013-Content-Aware_Rotation.html">90 iccv-2013-Content-Aware Rotation</a></p>
<p>8 0.67651707 <a title="407-lda-8" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>9 0.67630428 <a title="407-lda-9" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>10 0.67493445 <a title="407-lda-10" href="./iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F.html">316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</a></p>
<p>11 0.67388189 <a title="407-lda-11" href="./iccv-2013-Live_Metric_3D_Reconstruction_on_Mobile_Phones.html">254 iccv-2013-Live Metric 3D Reconstruction on Mobile Phones</a></p>
<p>12 0.67289495 <a title="407-lda-12" href="./iccv-2013-Global_Fusion_of_Relative_Motions_for_Robust%2C_Accurate_and_Scalable_Structure_from_Motion.html">184 iccv-2013-Global Fusion of Relative Motions for Robust, Accurate and Scalable Structure from Motion</a></p>
<p>13 0.67015302 <a title="407-lda-13" href="./iccv-2013-Go-ICP%3A_Solving_3D_Registration_Efficiently_and_Globally_Optimally.html">185 iccv-2013-Go-ICP: Solving 3D Registration Efficiently and Globally Optimally</a></p>
<p>14 0.67015082 <a title="407-lda-14" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>15 0.66993415 <a title="407-lda-15" href="./iccv-2013-A_Unified_Rolling_Shutter_and_Motion_Blur_Model_for_3D_Visual_Registration.html">32 iccv-2013-A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration</a></p>
<p>16 0.66989416 <a title="407-lda-16" href="./iccv-2013-Efficient_and_Robust_Large-Scale_Rotation_Averaging.html">138 iccv-2013-Efficient and Robust Large-Scale Rotation Averaging</a></p>
<p>17 0.66922486 <a title="407-lda-17" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>18 0.66890854 <a title="407-lda-18" href="./iccv-2013-Correntropy_Induced_L2_Graph_for_Robust_Subspace_Clustering.html">94 iccv-2013-Correntropy Induced L2 Graph for Robust Subspace Clustering</a></p>
<p>19 0.6688264 <a title="407-lda-19" href="./iccv-2013-Direct_Optimization_of_Frame-to-Frame_Rotation.html">115 iccv-2013-Direct Optimization of Frame-to-Frame Rotation</a></p>
<p>20 0.66878456 <a title="407-lda-20" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
