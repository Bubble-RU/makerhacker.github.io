<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>49 iccv-2013-An Enhanced Structure-from-Motion Paradigm Based on the Absolute Dual Quadric and Images of Circular Points</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-49" href="#">iccv2013-49</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>49 iccv-2013-An Enhanced Structure-from-Motion Paradigm Based on the Absolute Dual Quadric and Images of Circular Points</h1>
<br/><p>Source: <a title="iccv-2013-49-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Calvet_An_Enhanced_Structure-from-Motion_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Lilian Calvet, Pierre Gurdjos</p><p>Abstract: This work aims at introducing a new unified Structurefrom-Motion (SfM) paradigm in which images of circular point-pairs can be combined with images of natural points. An imaged circular point-pair encodes the 2D Euclidean structure of a world plane and can easily be derived from the image of a planar shape, especially those including circles. A classical SfM method generally runs two steps: first a projective factorization of all matched image points (into projective cameras and points) and second a camera selfcalibration that updates the obtained world from projective to Euclidean. This work shows how to introduce images of circular points in these two SfM steps while its key contribution is to provide the theoretical foundations for combining “classical” linear self-calibration constraints with additional ones derived from such images. We show that the two proposed SfM steps clearly contribute to better results than the classical approach. We validate our contributions on synthetic and real images.</p><p>Reference: <a title="iccv-2013-49-reference" href="../iccv2013_reference/iccv-2013-An_Enhanced_Structure-from-Motion_Paradigm_Based_on_the_Absolute_Dual_Quadric_and_Images_of_Circular_Points_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 fr Abstract This work aims at introducing a new unified Structurefrom-Motion (SfM) paradigm in which images of circular point-pairs can be combined with images of natural points. [sent-3, score-0.512]
</p><p>2 An imaged circular point-pair encodes the 2D Euclidean structure of a world plane and can easily be derived from the image of a planar shape, especially those including circles. [sent-4, score-0.65]
</p><p>3 A classical SfM method generally runs two steps: first a projective factorization of all matched image points (into projective cameras and points) and second a camera selfcalibration that updates the obtained world from projective to Euclidean. [sent-5, score-1.721]
</p><p>4 This work shows how to introduce images of circular points in these two SfM steps while its key contribution is to provide the theoretical foundations for combining “classical” linear self-calibration constraints with additional ones derived from such images. [sent-6, score-0.615]
</p><p>5 Introduction A now classical Structure-from-Motion (SfM) paradigm basically consists of two key steps: (i) compute a projective reconstruction of cameras and points; (ii) update the obtained projective representation to Euclidean by locating the absolute conic on the plane at infinity [3]. [sent-10, score-1.514]
</p><p>6 They rely on the factorization of a measurement matrix consisting of matched points scaled to their projective depths, into two lower-rank matrices, one representing the camera motion and one representing the 3D points of the scene structure, both w. [sent-15, score-0.875]
</p><p>7 A method that solves (ii) is socalled self-calibration and is always achieved under some assumptions on the camera parameters [10] as the absolute Pierre Gurdjos IRIT-ENSEEIHT, Toulouse, France pgurdj o s @ en see iht . [sent-19, score-0.224]
</p><p>8 fr conic refers to the (unique) imaginary circle at infinity in projective 3-space which is fixed under general motions of the camera. [sent-20, score-0.641]
</p><p>9 Once the absolute conic is recovered, a projective transfomation can be applied to the world space to  ×  update it to Euclidean. [sent-21, score-0.721]
</p><p>10 We focus here on direct approaches which rely on the fact that the absolute conic may be represented by a single equation in dual projective space (and hence by a single matrix) as the assemblage of its tangent planes i. [sent-22, score-1.028]
</p><p>11 The absolute dual quadric and its use in computer vision has been introduced by Triggs in 1997 [13] but surely popularised in 1999 by the linear least-squares algebraic solution of Pollefeys et al. [sent-25, score-0.278]
</p><p>12 In this work, it is assumed to have at one’s disposal a set of views where some “natural” points of interest are matched and the 2D Euclidean structure of a world plane is available (e. [sent-32, score-0.412]
</p><p>13 On the one hand, omitting the available 2D Euclidean structure, if we run the classical SfM approach [5, 6, 10] with these sets as input, it fails because either there are too few views or matched points or the camera motion is critical [2]. [sent-37, score-0.391]
</p><p>14 In our examples, using the new paradigm, we were able to obtain a satisfactory final dense reconstruction, which was provided by PMVS 998855  (a) (b)  (c) (d) Figure 1: (a) Three views of a dragon where 54 points can be matched and a very small checkerboard-style grid is visible except in view 2. [sent-42, score-0.383]
</p><p>15 (b) Five views of a human face where 11points can be matched and the 2D Euclidean structure is given by the imaged circular points which are two (complex) intersection points of the two coplanar circles formed by the eye’s iris outlines. [sent-44, score-0.902]
</p><p>16 (c)+(d) Only the proposed method, starting from the same projective reconstruction than other methods, by adding circular points in self-calibration (see Table 4), was able to provide PMVS [1] —even if the planar shape was very small— accurate Euclidean cameras. [sent-46, score-0.978]
</p><p>17 In this work, we treat the circular point-pair (CP-pair) of  a plane as a feature. [sent-50, score-0.518]
</p><p>18 The CP-pair consists of two complex conjugate points at infinity [3] whose images, easily derived from the geometry of any planar shape like [15, 16, 11], define the well-known plane-based calibration constraints [16]. [sent-51, score-0.385]
</p><p>19 Our key idea is that incorporating the images of CPpairs into the SfM algorithm sequence, namely (i) in projective factorization scheme mixing images of natural points and images of CP as well and (ii) in the Euclidean update of the obtained projective reconstruction. [sent-52, score-1.028]
</p><p>20 The contribution is double: the first one is to show how to determine the complex projective depths of the images of CP and how to we deal with the subproblem of missing data i. [sent-53, score-0.473]
</p><p>21 In fact, we give the theoretical foundation proving that both Pollefey’s constraints and the proposed ones define 3D lines through the camera centre that cut the absolute conic i. [sent-57, score-0.452]
</p><p>22 At the end of this section, some background on projective geometry in 3-space is reminded. [sent-65, score-0.396]
</p><p>23 In section 2, we show how to introduce images of CP in the projective factorization algorithm. [sent-66, score-0.507]
</p><p>24 Any projective plane intersects the absolute conic Ω∞ at a (complex) circular point-pair. [sent-69, score-1.2]
</p><p>25 Planes that are parallel share the same circular point-pair which is also the common point-pair of all circles lying on such parallel planes [3, p. [sent-70, score-0.555]
</p><p>26 The image of a circular point only have 4 dof as its (complex) homogeneous 3-vector is defined up to a complex factor. [sent-72, score-0.479]
</p><p>27 When it is not necessary to distinguish betwen I+ and I−, the image of a circular point will simply be referred to as I. [sent-74, score-0.441]
</p><p>28 Projective factorization with circular points The statement of the projective factorization problem assumes that M world points are viewed by N different cameras and that the MN image points are gathered within a measurement 3N M-matrix. [sent-76, score-1.434]
</p><p>29 In 998866  our case, we assume the image points to be projections of “natural” points and circular points. [sent-82, score-0.592]
</p><p>30 While the proposed method can deal with more than one circular point, we will focus on the case of a single circular point which is the only case that theoretically requires images of natural points in addition. [sent-84, score-0.978]
</p><p>31 Rescaling the image Ij of a circular point is not straightforward as it must be multiplied by a complex factor μj . [sent-97, score-0.479]
</p><p>32 We propose an iterative algorithm (see table 1) that rescales the available images of circular points in such a way that, at the current step processing view jc, the images have already been rescaled in previous view jp. [sent-98, score-0.685]
</p><p>33 We first write μj = aj + ibj, with aj , bj ∈ R, such that  μjIj= Cj  ? [sent-102, score-0.199]
</p><p>34 − μjpHjpjcCjp= 03×2  ×  (1)  that is a rewriting of μjcIjc−μjpHjpjcIjc =0 which transfers the image of a circular point μfrom previous view jp to current view jc, via the homography Hjpjc , induced by some world plane that contains the circular point-pair. [sent-110, score-1.324]
</p><p>35 Given the matched rescaled images ( u˜jp, ˜ ujc) of a natural world point, the second one is  ? [sent-111, score-0.21]
</p><p>36 ging the jp-th camera centre and u˜j is the rescaled image point in view j. [sent-119, score-0.232]
</p><p>37 Hjpjc mappings a point in view jp to its epipolar lin? [sent-127, score-0.232]
</p><p>38 stimated in previous step jp, clearly (1) is an homogeneous equation that linearly constrains the unknown scalars ajc, bjc (which rescaled plus the unknowns in Hjpjc . [sent-132, score-0.204]
</p><p>39 This is ×  I˜jc)  due to the fact that an absolute point-pair is common to a 1D set of parallel planes: if we represent their (common) vanishing lines in view jp by the 3-vectors vjp , then the set induces a family of inter-view homographies of the form H(z) = Hjpjc− zejpjcv? [sent-135, score-0.339]
</p><p>40 As a counting argument, it can be seen that the configuration of the image of one circular point and the (scaled) matched images of P natural points introduces 10 unknowns while providing 2P +6 equations. [sent-140, score-0.725]
</p><p>41 An overdetermined set of equations can be easily solved in a linear least squares manner. [sent-142, score-0.172]
</p><p>42 F2 7) Extract  μjc  8) Set ˜Ijc  ←  =  ajc  + ibjc from the solution Xˆ  μjcI˜jc  9) end 10) Set jp ← jc, select a new view jc and go to 2). [sent-146, score-0.547]
</p><p>43 Table 1: Algorithm scaling the images of circular points  ×  Missing data. [sent-147, score-0.502]
</p><p>44 Suppose that we ran the algorithm of Table 2 only for the visible images of circular points. [sent-148, score-0.44]
</p><p>45 Assume that the circular point is not seen in view jc but is viewed in V other views, of which view jp is taken. [sent-151, score-1.001]
</p><p>46 A linear solution exists based on two equations involving the matched rescaled images ( u˜jp , ˜u jc ), one of which being a direct rewriting of (1) Djc − μjpHjpjcCjp  = 03×2  (3)  and the other one being (2). [sent-152, score-0.539]
</p><p>47 Aens a counting argument, it can be seen that a data set, consisting of the (rescaled) images in V views of the circular point and P natural points, introduces 8V + 6 unknowns while providing 6V + 2PV equations. [sent-154, score-0.666]
</p><p>48 Again, an overdetermined set of equations can be easily solved by linear 1Albeit at least 7 points will be always required quired fundamental matrix. [sent-157, score-0.233]
</p><p>49 Notice that another way to compute scaled images of circular points in missing views may also be achieved by first computing them, up to a complex  ×× ×  scale factor, based on their visible images (at least two) and on fundamental matrices and next, rescaling them as described above. [sent-159, score-0.8]
</p><p>50 Linear self-calibration constraints on Q∗∞ [5, 10] We consider a set of n projective cameras, represented by n projection 3 4-matrices, mapping 3D points in some projective representation ttroi cthese,ir m images a 3nDd wporiintttesn i as  Pj=⎝⎛abcjj ? [sent-164, score-0.941]
</p><p>51 (4)  The absolute co⎝nic Ω∞ projects in dual form in view j as ω∗j  ∼  PjQ∗Pj? [sent-171, score-0.26]
</p><p>52 3 3th see projective representation a thnde dωua∗jl the 3 3-matrix of its image. [sent-175, score-0.396]
</p><p>53 ), the image of the absolute conic in dual fdoerrm h ywporittehse:s ω∗j = diag ? [sent-181, score-0.35]
</p><p>54 Adding linear circular-point constraints on Q∗∞ In this section, the issue is that ofwriting the independent self-calibration constraints brought by the image I± of a circular point-pair. [sent-197, score-0.585]
</p><p>55 All these issues can be solved by introducing the 3D linepair L(I±) which back-projects I± and cuts the absolute conic (see fig. [sent-204, score-0.286]
</p><p>56 Our key contribution is to set up the correct theoretical background in order to answer the question “what are the additional independent equations on Q∗∞ brought by the absolute line-pair L(I±) ? [sent-206, score-0.327]
</p><p>57 ”,  Figure 2: The line through the camera centre C and the image I a circular point touches the absolute conic Ω∞. [sent-208, score-0.844]
</p><p>58 The following proposition reveals the linear constraints on Q∗∞ brought by line L(I). [sent-210, score-0.208]
</p><p>59 ×  Proposition 1 A line L cuts the absolute conic Ω∞ iff through L it passes a plane tangent to Ω∞, i. [sent-213, score-0.528]
</p><p>60 Q∗Π = 0,  (10)  and a (complex or real) plane q conjugate to Π w. [sent-215, score-0.161]
</p><p>61 Any 3D line can be defined as the intersection of two planes so let L be defined by planes r and s. [sent-221, score-0.209]
</p><p>62 Through L it also passes two planes that are tangent to Ω∞, i. [sent-222, score-0.185]
</p><p>63 (⇒) If L touches Ω∞, the two tangent planes c+oi unsci)de = w 0h. [sent-226, score-0.185]
</p><p>64 Hence, ifthe tangent plane w∞here −L (trouch∞es Q∗∞ is ∞either r or s , then the other one necessarily is a plane t∞hat is conjugate to it w. [sent-233, score-0.366]
</p><p>65 Hence, the Π and q share a line through this circular  ×  point so line L cuts the absolute conic. [sent-245, score-0.651]
</p><p>66 Proposition 1teaches us the fact that, by cutting the absolute conic, L(I) brings the two complex constraints (10,1 1) on Q∗∞ i. [sent-246, score-0.233]
</p><p>67 Thus, Π is the plane of Q∗∞ through L(I), obtained by back-projecting the line T tangent to ω at I. [sent-252, score-0.242]
</p><p>68 e O OTn tangent t hoa tnhde, image (oHf the absolute conic at I  × ×  ×  is given by p∞ = L∞ T, where L∞ = (0, 0, 1)? [sent-260, score-0.385]
</p><p>69 Proposition 3 A (real) plane through L(I), that is conjugate to plane Π = P? [sent-281, score-0.267]
</p><p>70 (13)  This plane through L(I) includes the circular point-pair and can be obtained by back-projecting the vanishing line. [sent-287, score-0.518]
</p><p>71 Note that, in the general case, the four equations brought by constraints (10,1 1) are linearly independent w. [sent-288, score-0.223]
</p><p>72 Unifying the linear self-calibration constraints What is the link between the circular-point constraints (10,1 1) and the “original” linear self-calibration constraints (7,8)? [sent-294, score-0.177]
</p><p>73 respectively represent the line tangent to ω at Io and the image plane’s line at infinity. [sent-316, score-0.173]
</p><p>74 The problem was naturally formulated in the projective 5-space –i. [sent-320, score-0.396]
</p><p>75 linearise (10,1 1) as D1X = 04 with D1 ∈ R4×10 add the block-row D1 to the data matrix∈ ∈D  if a plane’s circular point-pair is imaged in the view then /* proposed equations */ estimate I+ = (ψ1 ,ψ2 ,ψ3)? [sent-327, score-0.651]
</p><p>76 2 = 1 Table 2: Unified self-calibration algorithm based  on  circular points  4. [sent-334, score-0.502]
</p><p>77 Experiments We conducted a large number of experiments with synthetic and real images to quantify the performance of the proposed SfM paradigm, using a small amount of natural 998899  points as input and depending on the presence (or not) of one circular point-pair (CP in the sequel). [sent-335, score-0.537]
</p><p>78 Basically, four use cases were considered: (i) the incorporation (or not) of a CP into the projective factorization algorithm and (ii) the incorporation (or not) of the constraints provided by a CP in the self-calibration algorithm. [sent-336, score-0.566]
</p><p>79 We put special emphasis on the interest of projective factorization based on circular points as it offers a twofold advantage. [sent-337, score-1.009]
</p><p>80 This important point were confirmed by our experiments: more accurate projective cameras were obtained and the reprojections of the obtained projective points yielded “corrected” —and  more accurate— image points. [sent-338, score-1.011]
</p><p>81 When n2 n1 > 0, the images of CP used in self-calibration were ≥the reprojections of projective CP computed in the factorization. [sent-349, score-0.434]
</p><p>82 c oTnhseis tCinPg were computed as tahtecommon projective point-pair of two concentric circles (see [15]). [sent-352, score-0.482]
</p><p>83 3(c), 6 views were considered and we let vary the number of points from 6 to 20. [sent-380, score-0.194]
</p><p>84 3(d), we also considered 6 views but let vary the number of views in which the CP is visible. [sent-382, score-0.208]
</p><p>85 Of course, for a very small number of natural points, adding images of CP in the projective factorization significantly reduced the 3D RMS error and, in that case, the overall improvement for PF[1]+SC[1] was mostly due to the factorization. [sent-385, score-0.542]
</p><p>86 All these observations may be justified by considering the number of constraints provided by the images of CP with regard to the size of equation systems involved in the projective factorization and in self-calibration respectively. [sent-386, score-0.597]
</p><p>87 3 shows median errors on the focal length for the proposed method while varying the number ofviews, considering 20 natural points corrupted by a noise σ = 1. [sent-397, score-0.169]
</p><p>88 The focal length fk of each projective camera Pk was c? [sent-399, score-0.523]
</p><p>89 adjustment and points refers  to  points which are not circular points. [sent-429, score-0.592]
</p><p>90 In the first experiment, we matched 12 points of a toy figurine seen in 6 views (see Fig. [sent-432, score-0.296]
</p><p>91 In this experiment, we matched 7 points of a car seen in 8 views (see Fig. [sent-438, score-0.296]
</p><p>92 Images of CP which are the (complex) intersection points of the two coplanar circles formed by the car’s wheels are computed as described in [15]. [sent-440, score-0.191]
</p><p>93 Dense reconstructions based on Euclidean cameras computed with PF[0]+SC[0] and PF[1]+SC[0] were inconsistent even if the one from PF[1]+SC[0] involving images of CP in the projective factorization is slightly better than the one from PF[0]+SC[0] based on only natural points. [sent-441, score-0.604]
</p><p>94 In this experiment, we matched 11points of a human face seen in 5 views (see Fig. [sent-447, score-0.206]
</p><p>95 In the last experiment, we matched 54 points of a dragon seen in 3 views (see Fig. [sent-452, score-0.324]
</p><p>96 Only the proposed method, starting from the same projective reconstruction, reached to provide good results by adding circular points in self-calibration (see Fig. [sent-455, score-0.898]
</p><p>97 Conclusion The purpose of this work was first to show how to introduce images of circular points in a projective factorization algorithm. [sent-466, score-1.009]
</p><p>98 In a second part, we showed that it is possible to combine “classical” linear self-calibration constraints on the absolute dual quadric, under the assumption that shape pixels and principal points are known, with additional ones derived from the images of the circular points. [sent-468, score-0.788]
</p><p>99 We described  the additional independent equations brought by the absolute line-pair, back-projections of the image of some circular point-pair and how to compute them. [sent-469, score-0.712]
</p><p>100 camera tracking, to better control the self-calibration quality and ensuring the presence of a reliable subset of key-frames to upgrade the projective SfM to Euclidean. [sent-487, score-0.45]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('circular', 0.412), ('projective', 0.396), ('jc', 0.268), ('pf', 0.264), ('sc', 0.212), ('cp', 0.209), ('sfm', 0.18), ('hjpjc', 0.17), ('conic', 0.15), ('jp', 0.143), ('absolute', 0.136), ('factorization', 0.111), ('equations', 0.109), ('plane', 0.106), ('views', 0.104), ('tangent', 0.099), ('ejpjc', 0.095), ('points', 0.09), ('planes', 0.086), ('euclidean', 0.084), ('quadric', 0.078), ('ajc', 0.076), ('aj', 0.075), ('matched', 0.073), ('pollefeys', 0.072), ('classical', 0.07), ('paradigm', 0.065), ('dual', 0.064), ('rescaled', 0.063), ('cameras', 0.062), ('rescaling', 0.061), ('view', 0.06), ('constraints', 0.059), ('circles', 0.057), ('proposition', 0.057), ('vald', 0.057), ('brought', 0.055), ('conjugate', 0.055), ('camera', 0.054), ('bjc', 0.05), ('imaginary', 0.05), ('bj', 0.049), ('reconstruction', 0.046), ('infinity', 0.045), ('rms', 0.045), ('focal', 0.044), ('coplanar', 0.044), ('basically', 0.042), ('world', 0.039), ('missing', 0.039), ('assemblage', 0.038), ('djc', 0.038), ('fjpjc', 0.038), ('jphjpjccjp', 0.038), ('linearise', 0.038), ('pollefey', 0.038), ('reprojections', 0.038), ('toulouse', 0.038), ('ujc', 0.038), ('ujp', 0.038), ('complex', 0.038), ('line', 0.037), ('homography', 0.037), ('calibration', 0.037), ('natural', 0.035), ('planar', 0.034), ('cj', 0.034), ('gurdjos', 0.034), ('iht', 0.034), ('ofviews', 0.034), ('overdetermined', 0.034), ('selfcalibration', 0.034), ('measurement', 0.033), ('imaged', 0.032), ('jmiv', 0.031), ('fj', 0.031), ('equation', 0.031), ('unknown', 0.03), ('unknowns', 0.03), ('autocalibration', 0.029), ('concentric', 0.029), ('worthy', 0.029), ('seen', 0.029), ('squares', 0.029), ('fk', 0.029), ('point', 0.029), ('visible', 0.028), ('hence', 0.028), ('dragon', 0.028), ('pmvs', 0.028), ('scaled', 0.028), ('theoretical', 0.027), ('ib', 0.027), ('derived', 0.027), ('providing', 0.027), ('ponce', 0.027), ('hypothesis', 0.026), ('rewriting', 0.026), ('ivc', 0.026), ('centre', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000015 <a title="49-tfidf-1" href="./iccv-2013-An_Enhanced_Structure-from-Motion_Paradigm_Based_on_the_Absolute_Dual_Quadric_and_Images_of_Circular_Points.html">49 iccv-2013-An Enhanced Structure-from-Motion Paradigm Based on the Absolute Dual Quadric and Images of Circular Points</a></p>
<p>Author: Lilian Calvet, Pierre Gurdjos</p><p>Abstract: This work aims at introducing a new unified Structurefrom-Motion (SfM) paradigm in which images of circular point-pairs can be combined with images of natural points. An imaged circular point-pair encodes the 2D Euclidean structure of a world plane and can easily be derived from the image of a planar shape, especially those including circles. A classical SfM method generally runs two steps: first a projective factorization of all matched image points (into projective cameras and points) and second a camera selfcalibration that updates the obtained world from projective to Euclidean. This work shows how to introduce images of circular points in these two SfM steps while its key contribution is to provide the theoretical foundations for combining “classical” linear self-calibration constraints with additional ones derived from such images. We show that the two proposed SfM steps clearly contribute to better results than the classical approach. We validate our contributions on synthetic and real images.</p><p>2 0.15567826 <a title="49-tfidf-2" href="./iccv-2013-Multi-view_3D_Reconstruction_from_Uncalibrated_Radially-Symmetric_Cameras.html">280 iccv-2013-Multi-view 3D Reconstruction from Uncalibrated Radially-Symmetric Cameras</a></p>
<p>Author: Jae-Hak Kim, Yuchao Dai, Hongdong Li, Xin Du, Jonghyuk Kim</p><p>Abstract: We present a new multi-view 3D Euclidean reconstruction method for arbitrary uncalibrated radially-symmetric cameras, which needs no calibration or any camera model parameters other than radial symmetry. It is built on the radial 1D camera model [25], a unified mathematical abstraction to different types of radially-symmetric cameras. We formulate the problem of multi-view reconstruction for radial 1D cameras as a matrix rank minimization problem. Efficient implementation based on alternating direction continuation is proposed to handle scalability issue for real-world applications. Our method applies to a wide range of omnidirectional cameras including both dioptric and catadioptric (central and non-central) cameras. Additionally, our method deals with complete and incomplete measurements under a unified framework elegantly. Experiments on both synthetic and real images from various types of cameras validate the superior performance of our new method, in terms of numerical accuracy and robustness.</p><p>3 0.12463311 <a title="49-tfidf-3" href="./iccv-2013-Unsupervised_Intrinsic_Calibration_from_a_Single_Frame_Using_a_%22Plumb-Line%22_Approach.html">436 iccv-2013-Unsupervised Intrinsic Calibration from a Single Frame Using a "Plumb-Line" Approach</a></p>
<p>Author: R. Melo, M. Antunes, J.P. Barreto, G. Falcão, N. Gonçalves</p><p>Abstract: Estimating the amount and center ofdistortionfrom lines in the scene has been addressed in the literature by the socalled “plumb-line ” approach. In this paper we propose a new geometric method to estimate not only the distortion parameters but the entire camera calibration (up to an “angular” scale factor) using a minimum of 3 lines. We propose a new framework for the unsupervised simultaneous detection of natural image of lines and camera parameters estimation, enabling a robust calibration from a single image. Comparative experiments with existing automatic approaches for the distortion estimation and with ground truth data are presented.</p><p>4 0.10923178 <a title="49-tfidf-4" href="./iccv-2013-A_Global_Linear_Method_for_Camera_Pose_Registration.html">17 iccv-2013-A Global Linear Method for Camera Pose Registration</a></p>
<p>Author: Nianjuan Jiang, Zhaopeng Cui, Ping Tan</p><p>Abstract: We present a linear method for global camera pose registration from pairwise relative poses encoded in essential matrices. Our method minimizes an approximate geometric error to enforce the triangular relationship in camera triplets. This formulation does not suffer from the typical ‘unbalanced scale ’ problem in linear methods relying on pairwise translation direction constraints, i.e. an algebraic error; nor the system degeneracy from collinear motion. In the case of three cameras, our method provides a good linear approximation of the trifocal tensor. It can be directly scaled up to register multiple cameras. The results obtained are accurate for point triangulation and can serve as a good initialization for final bundle adjustment. We evaluate the algorithm performance with different types of data and demonstrate its effectiveness. Our system produces good accuracy, robustness, and outperforms some well-known systems on efficiency.</p><p>5 0.10387892 <a title="49-tfidf-5" href="./iccv-2013-Pose_Estimation_with_Unknown_Focal_Length_Using_Points%2C_Directions_and_Lines.html">323 iccv-2013-Pose Estimation with Unknown Focal Length Using Points, Directions and Lines</a></p>
<p>Author: Yubin Kuang, Kalle Åström</p><p>Abstract: In this paper, we study the geometry problems of estimating camera pose with unknown focal length using combination of geometric primitives. We consider points, lines and also rich features such as quivers, i.e. points with one or more directions. We formulate the problems as polynomial systems where the constraints for different primitives are handled in a unified way. We develop efficient polynomial solvers for each of the derived cases with different combinations of primitives. The availability of these solvers enables robust pose estimation with unknown focal length for wider classes of features. Such rich features allow for fewer feature correspondences and generate larger inlier sets with higher probability. We demonstrate in synthetic experiments that our solvers are fast and numerically stable. For real images, we show that our solvers can be used in RANSAC loops to provide good initial solutions.</p><p>6 0.10143421 <a title="49-tfidf-6" href="./iccv-2013-Real-Time_Solution_to_the_Absolute_Pose_Problem_with_Unknown_Radial_Distortion_and_Focal_Length.html">342 iccv-2013-Real-Time Solution to the Absolute Pose Problem with Unknown Radial Distortion and Focal Length</a></p>
<p>7 0.09434025 <a title="49-tfidf-7" href="./iccv-2013-Unifying_Nuclear_Norm_and_Bilinear_Factorization_Approaches_for_Low-Rank_Matrix_Decomposition.html">434 iccv-2013-Unifying Nuclear Norm and Bilinear Factorization Approaches for Low-Rank Matrix Decomposition</a></p>
<p>8 0.09304893 <a title="49-tfidf-8" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>9 0.090210274 <a title="49-tfidf-9" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>10 0.084935367 <a title="49-tfidf-10" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>11 0.084612638 <a title="49-tfidf-11" href="./iccv-2013-Space-Time_Tradeoffs_in_Photo_Sequencing.html">397 iccv-2013-Space-Time Tradeoffs in Photo Sequencing</a></p>
<p>12 0.083345219 <a title="49-tfidf-12" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>13 0.07913781 <a title="49-tfidf-13" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>14 0.078560658 <a title="49-tfidf-14" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>15 0.077511318 <a title="49-tfidf-15" href="./iccv-2013-Shape_Anchors_for_Data-Driven_Multi-view_Reconstruction.html">387 iccv-2013-Shape Anchors for Data-Driven Multi-view Reconstruction</a></p>
<p>16 0.07312689 <a title="49-tfidf-16" href="./iccv-2013-A_Robust_Analytical_Solution_to_Isometric_Shape-from-Template_with_Focal_Length_Calibration.html">27 iccv-2013-A Robust Analytical Solution to Isometric Shape-from-Template with Focal Length Calibration</a></p>
<p>17 0.071572848 <a title="49-tfidf-17" href="./iccv-2013-Global_Fusion_of_Relative_Motions_for_Robust%2C_Accurate_and_Scalable_Structure_from_Motion.html">184 iccv-2013-Global Fusion of Relative Motions for Robust, Accurate and Scalable Structure from Motion</a></p>
<p>18 0.071271703 <a title="49-tfidf-18" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>19 0.069767833 <a title="49-tfidf-19" href="./iccv-2013-NYC3DCars%3A_A_Dataset_of_3D_Vehicles_in_Geographic_Context.html">286 iccv-2013-NYC3DCars: A Dataset of 3D Vehicles in Geographic Context</a></p>
<p>20 0.069381244 <a title="49-tfidf-20" href="./iccv-2013-Monocular_Image_3D_Human_Pose_Estimation_under_Self-Occlusion.html">273 iccv-2013-Monocular Image 3D Human Pose Estimation under Self-Occlusion</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.135), (1, -0.107), (2, -0.044), (3, 0.021), (4, -0.044), (5, 0.035), (6, 0.031), (7, -0.079), (8, 0.047), (9, 0.009), (10, 0.025), (11, -0.027), (12, -0.119), (13, 0.002), (14, 0.019), (15, 0.051), (16, 0.061), (17, 0.129), (18, -0.014), (19, 0.012), (20, 0.007), (21, -0.127), (22, -0.049), (23, 0.011), (24, -0.003), (25, 0.029), (26, 0.002), (27, -0.04), (28, -0.065), (29, -0.009), (30, -0.044), (31, 0.043), (32, -0.025), (33, -0.021), (34, -0.05), (35, 0.045), (36, 0.03), (37, 0.003), (38, 0.04), (39, -0.002), (40, -0.001), (41, -0.066), (42, -0.015), (43, 0.065), (44, -0.029), (45, -0.007), (46, -0.033), (47, -0.08), (48, 0.041), (49, -0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95960349 <a title="49-lsi-1" href="./iccv-2013-An_Enhanced_Structure-from-Motion_Paradigm_Based_on_the_Absolute_Dual_Quadric_and_Images_of_Circular_Points.html">49 iccv-2013-An Enhanced Structure-from-Motion Paradigm Based on the Absolute Dual Quadric and Images of Circular Points</a></p>
<p>Author: Lilian Calvet, Pierre Gurdjos</p><p>Abstract: This work aims at introducing a new unified Structurefrom-Motion (SfM) paradigm in which images of circular point-pairs can be combined with images of natural points. An imaged circular point-pair encodes the 2D Euclidean structure of a world plane and can easily be derived from the image of a planar shape, especially those including circles. A classical SfM method generally runs two steps: first a projective factorization of all matched image points (into projective cameras and points) and second a camera selfcalibration that updates the obtained world from projective to Euclidean. This work shows how to introduce images of circular points in these two SfM steps while its key contribution is to provide the theoretical foundations for combining “classical” linear self-calibration constraints with additional ones derived from such images. We show that the two proposed SfM steps clearly contribute to better results than the classical approach. We validate our contributions on synthetic and real images.</p><p>2 0.90453309 <a title="49-lsi-2" href="./iccv-2013-Multi-view_3D_Reconstruction_from_Uncalibrated_Radially-Symmetric_Cameras.html">280 iccv-2013-Multi-view 3D Reconstruction from Uncalibrated Radially-Symmetric Cameras</a></p>
<p>Author: Jae-Hak Kim, Yuchao Dai, Hongdong Li, Xin Du, Jonghyuk Kim</p><p>Abstract: We present a new multi-view 3D Euclidean reconstruction method for arbitrary uncalibrated radially-symmetric cameras, which needs no calibration or any camera model parameters other than radial symmetry. It is built on the radial 1D camera model [25], a unified mathematical abstraction to different types of radially-symmetric cameras. We formulate the problem of multi-view reconstruction for radial 1D cameras as a matrix rank minimization problem. Efficient implementation based on alternating direction continuation is proposed to handle scalability issue for real-world applications. Our method applies to a wide range of omnidirectional cameras including both dioptric and catadioptric (central and non-central) cameras. Additionally, our method deals with complete and incomplete measurements under a unified framework elegantly. Experiments on both synthetic and real images from various types of cameras validate the superior performance of our new method, in terms of numerical accuracy and robustness.</p><p>3 0.84069872 <a title="49-lsi-3" href="./iccv-2013-Unsupervised_Intrinsic_Calibration_from_a_Single_Frame_Using_a_%22Plumb-Line%22_Approach.html">436 iccv-2013-Unsupervised Intrinsic Calibration from a Single Frame Using a "Plumb-Line" Approach</a></p>
<p>Author: R. Melo, M. Antunes, J.P. Barreto, G. Falcão, N. Gonçalves</p><p>Abstract: Estimating the amount and center ofdistortionfrom lines in the scene has been addressed in the literature by the socalled “plumb-line ” approach. In this paper we propose a new geometric method to estimate not only the distortion parameters but the entire camera calibration (up to an “angular” scale factor) using a minimum of 3 lines. We propose a new framework for the unsupervised simultaneous detection of natural image of lines and camera parameters estimation, enabling a robust calibration from a single image. Comparative experiments with existing automatic approaches for the distortion estimation and with ground truth data are presented.</p><p>4 0.82193708 <a title="49-lsi-4" href="./iccv-2013-Refractive_Structure-from-Motion_on_Underwater_Images.html">348 iccv-2013-Refractive Structure-from-Motion on Underwater Images</a></p>
<p>Author: Anne Jordt-Sedlazeck, Reinhard Koch</p><p>Abstract: In underwater environments, cameras need to be confined in an underwater housing, viewing the scene through a piece of glass. In case of flat port underwater housings, light rays entering the camera housing are refracted twice, due to different medium densities of water, glass, and air. This causes the usually linear rays of light to bend and the commonly used pinhole camera model to be invalid. When using the pinhole camera model without explicitly modeling refraction in Structure-from-Motion (SfM) methods, a systematic model error occurs. Therefore, in this paper, we propose a system for computing camera path and 3D points with explicit incorporation of refraction using new methods for pose estimation. Additionally, a new error function is introduced for non-linear optimization, especially bundle adjustment. The proposed method allows to increase reconstruction accuracy and is evaluated in a set of experiments, where the proposed method’s performance is compared to SfM with the perspective camera model.</p><p>5 0.79279602 <a title="49-lsi-5" href="./iccv-2013-Real-Time_Solution_to_the_Absolute_Pose_Problem_with_Unknown_Radial_Distortion_and_Focal_Length.html">342 iccv-2013-Real-Time Solution to the Absolute Pose Problem with Unknown Radial Distortion and Focal Length</a></p>
<p>Author: Zuzana Kukelova, Martin Bujnak, Tomas Pajdla</p><p>Abstract: Theproblem ofdetermining the absoluteposition andorientation of a camera from a set of 2D-to-3D point correspondences is one of the most important problems in computer vision with a broad range of applications. In this paper we present a new solution to the absolute pose problem for camera with unknown radial distortion and unknown focal length from five 2D-to-3D point correspondences. Our new solver is numerically more stable, more accurate, and significantly faster than the existing state-of-the-art minimal fourpoint absolutepose solvers for this problem. Moreover, our solver results in less solutions and can handle larger radial distortions. The new solver is straightforward and uses only simple concepts from linear algebra. Therefore it is simpler than the state-of-the-art Gr¨ obner basis solvers. We compare our new solver with the existing state-of-theart solvers and show its usefulness on synthetic and real datasets. 1</p><p>6 0.77669317 <a title="49-lsi-6" href="./iccv-2013-Extrinsic_Camera_Calibration_without_a_Direct_View_Using_Spherical_Mirror.html">152 iccv-2013-Extrinsic Camera Calibration without a Direct View Using Spherical Mirror</a></p>
<p>7 0.76754671 <a title="49-lsi-7" href="./iccv-2013-Pose_Estimation_with_Unknown_Focal_Length_Using_Points%2C_Directions_and_Lines.html">323 iccv-2013-Pose Estimation with Unknown Focal Length Using Points, Directions and Lines</a></p>
<p>8 0.69754153 <a title="49-lsi-8" href="./iccv-2013-Revisiting_the_PnP_Problem%3A_A_Fast%2C_General_and_Optimal_Solution.html">353 iccv-2013-Revisiting the PnP Problem: A Fast, General and Optimal Solution</a></p>
<p>9 0.69460219 <a title="49-lsi-9" href="./iccv-2013-A_Global_Linear_Method_for_Camera_Pose_Registration.html">17 iccv-2013-A Global Linear Method for Camera Pose Registration</a></p>
<p>10 0.69394785 <a title="49-lsi-10" href="./iccv-2013-Rectangling_Stereographic_Projection_for_Wide-Angle_Image_Visualization.html">346 iccv-2013-Rectangling Stereographic Projection for Wide-Angle Image Visualization</a></p>
<p>11 0.68699235 <a title="49-lsi-11" href="./iccv-2013-Lifting_3D_Manhattan_Lines_from_a_Single_Image.html">250 iccv-2013-Lifting 3D Manhattan Lines from a Single Image</a></p>
<p>12 0.68025029 <a title="49-lsi-12" href="./iccv-2013-A_Robust_Analytical_Solution_to_Isometric_Shape-from-Template_with_Focal_Length_Calibration.html">27 iccv-2013-A Robust Analytical Solution to Isometric Shape-from-Template with Focal Length Calibration</a></p>
<p>13 0.66022193 <a title="49-lsi-13" href="./iccv-2013-Enhanced_Continuous_Tabu_Search_for_Parameter_Estimation_in_Multiview_Geometry.html">141 iccv-2013-Enhanced Continuous Tabu Search for Parameter Estimation in Multiview Geometry</a></p>
<p>14 0.62041914 <a title="49-lsi-14" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>15 0.61355275 <a title="49-lsi-15" href="./iccv-2013-Direct_Optimization_of_Frame-to-Frame_Rotation.html">115 iccv-2013-Direct Optimization of Frame-to-Frame Rotation</a></p>
<p>16 0.60899436 <a title="49-lsi-16" href="./iccv-2013-Space-Time_Tradeoffs_in_Photo_Sequencing.html">397 iccv-2013-Space-Time Tradeoffs in Photo Sequencing</a></p>
<p>17 0.5775218 <a title="49-lsi-17" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>18 0.55817622 <a title="49-lsi-18" href="./iccv-2013-Non-convex_P-Norm_Projection_for_Robust_Sparsity.html">292 iccv-2013-Non-convex P-Norm Projection for Robust Sparsity</a></p>
<p>19 0.55774283 <a title="49-lsi-19" href="./iccv-2013-Global_Fusion_of_Relative_Motions_for_Robust%2C_Accurate_and_Scalable_Structure_from_Motion.html">184 iccv-2013-Global Fusion of Relative Motions for Robust, Accurate and Scalable Structure from Motion</a></p>
<p>20 0.54106212 <a title="49-lsi-20" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.046), (7, 0.03), (22, 0.231), (26, 0.06), (27, 0.022), (31, 0.043), (40, 0.019), (42, 0.129), (64, 0.05), (73, 0.055), (89, 0.168), (98, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84450054 <a title="49-lda-1" href="./iccv-2013-Fingerspelling_Recognition_with_Semi-Markov_Conditional_Random_Fields.html">170 iccv-2013-Fingerspelling Recognition with Semi-Markov Conditional Random Fields</a></p>
<p>Author: Taehwan Kim, Greg Shakhnarovich, Karen Livescu</p><p>Abstract: Recognition of gesture sequences is in general a very difficult problem, but in certain domains the difficulty may be mitigated by exploiting the domain ’s “grammar”. One such grammatically constrained gesture sequence domain is sign language. In this paper we investigate the case of fingerspelling recognition, which can be very challenging due to the quick, small motions of the fingers. Most prior work on this task has assumed a closed vocabulary of fingerspelled words; here we study the more natural open-vocabulary case, where the only domain knowledge is the possible fingerspelled letters and statistics of their sequences. We develop a semi-Markov conditional model approach, where feature functions are defined over segments of video and their corresponding letter labels. We use classifiers of letters and linguistic handshape features, along with expected motion profiles, to define segmental feature functions. This approach improves letter error rate (Levenshtein distance between hypothesized and correct letter sequences) from 16.3% using a hidden Markov model baseline to 11.6% us- ing the proposed semi-Markov model.</p><p>same-paper 2 0.82927537 <a title="49-lda-2" href="./iccv-2013-An_Enhanced_Structure-from-Motion_Paradigm_Based_on_the_Absolute_Dual_Quadric_and_Images_of_Circular_Points.html">49 iccv-2013-An Enhanced Structure-from-Motion Paradigm Based on the Absolute Dual Quadric and Images of Circular Points</a></p>
<p>Author: Lilian Calvet, Pierre Gurdjos</p><p>Abstract: This work aims at introducing a new unified Structurefrom-Motion (SfM) paradigm in which images of circular point-pairs can be combined with images of natural points. An imaged circular point-pair encodes the 2D Euclidean structure of a world plane and can easily be derived from the image of a planar shape, especially those including circles. A classical SfM method generally runs two steps: first a projective factorization of all matched image points (into projective cameras and points) and second a camera selfcalibration that updates the obtained world from projective to Euclidean. This work shows how to introduce images of circular points in these two SfM steps while its key contribution is to provide the theoretical foundations for combining “classical” linear self-calibration constraints with additional ones derived from such images. We show that the two proposed SfM steps clearly contribute to better results than the classical approach. We validate our contributions on synthetic and real images.</p><p>3 0.79942763 <a title="49-lda-3" href="./iccv-2013-Real-Time_Articulated_Hand_Pose_Estimation_Using_Semi-supervised_Transductive_Regression_Forests.html">340 iccv-2013-Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests</a></p>
<p>Author: Danhang Tang, Tsz-Ho Yu, Tae-Kyun Kim</p><p>Abstract: This paper presents the first semi-supervised transductive algorithm for real-time articulated hand pose estimation. Noisy data and occlusions are the major challenges of articulated hand pose estimation. In addition, the discrepancies among realistic and synthetic pose data undermine the performances of existing approaches that use synthetic data extensively in training. We therefore propose the Semi-supervised Transductive Regression (STR) forest which learns the relationship between a small, sparsely labelled realistic dataset and a large synthetic dataset. We also design a novel data-driven, pseudo-kinematic technique to refine noisy or occluded joints. Our contributions include: (i) capturing the benefits of both realistic and synthetic data via transductive learning; (ii) showing accuracies can be improved by considering unlabelled data; and (iii) introducing a pseudo-kinematic technique to refine articulations efficiently. Experimental results show not only the promising performance of our method with respect to noise and occlusions, but also its superiority over state-of- the-arts in accuracy, robustness and speed.</p><p>4 0.73727536 <a title="49-lda-4" href="./iccv-2013-CoDeL%3A_A_Human_Co-detection_and_Labeling_Framework.html">75 iccv-2013-CoDeL: A Human Co-detection and Labeling Framework</a></p>
<p>Author: Jianping Shi, Renjie Liao, Jiaya Jia</p><p>Abstract: We propose a co-detection and labeling (CoDeL) framework to identify persons that contain self-consistent appearance in multiple images. Our CoDeL model builds upon the deformable part-based model to detect human hypotheses and exploits cross-image correspondence via a matching classifier. Relying on a Gaussian process, this matching classifier models the similarity of two hypotheses and efficiently captures the relative importance contributed by various visual features, reducing the adverse effect of scattered occlusion. Further, the detector and matching classifier together make our modelfit into a semi-supervised co-training framework, which can get enhanced results with a small amount of labeled training data. Our CoDeL model achieves decent performance on existing and new benchmark datasets.</p><p>5 0.72391677 <a title="49-lda-5" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>Author: Mohamed R. Amer, Sinisa Todorovic, Alan Fern, Song-Chun Zhu</p><p>Abstract: This paper presents an efficient approach to video parsing. Our videos show a number of co-occurring individual and group activities. To address challenges of the domain, we use an expressive spatiotemporal AND-OR graph (ST-AOG) that jointly models activity parts, their spatiotemporal relations, and context, as well as enables multitarget tracking. The standard ST-AOG inference is prohibitively expensive in our setting, since it would require running a multitude of detectors, and tracking their detections in a long video footage. This problem is addressed by formulating a cost-sensitive inference of ST-AOG as Monte Carlo Tree Search (MCTS). For querying an activity in the video, MCTS optimally schedules a sequence of detectors and trackers to be run, and where they should be applied in the space-time volume. Evaluation on the benchmark datasets demonstrates that MCTS enables two-magnitude speed-ups without compromising accuracy relative to the standard cost-insensitive inference.</p><p>6 0.71711284 <a title="49-lda-6" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>7 0.70857573 <a title="49-lda-7" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>8 0.70786297 <a title="49-lda-8" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>9 0.7064141 <a title="49-lda-9" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>10 0.70498222 <a title="49-lda-10" href="./iccv-2013-Joint_Noise_Level_Estimation_from_Personal_Photo_Collections.html">223 iccv-2013-Joint Noise Level Estimation from Personal Photo Collections</a></p>
<p>11 0.70397562 <a title="49-lda-11" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>12 0.7034716 <a title="49-lda-12" href="./iccv-2013-Robust_Tucker_Tensor_Decomposition_for_Effective_Image_Representation.html">362 iccv-2013-Robust Tucker Tensor Decomposition for Effective Image Representation</a></p>
<p>13 0.70316362 <a title="49-lda-13" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<p>14 0.70279908 <a title="49-lda-14" href="./iccv-2013-Unsupervised_Visual_Domain_Adaptation_Using_Subspace_Alignment.html">438 iccv-2013-Unsupervised Visual Domain Adaptation Using Subspace Alignment</a></p>
<p>15 0.70219791 <a title="49-lda-15" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>16 0.70206904 <a title="49-lda-16" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>17 0.70187068 <a title="49-lda-17" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>18 0.70165908 <a title="49-lda-18" href="./iccv-2013-Distributed_Low-Rank_Subspace_Segmentation.html">122 iccv-2013-Distributed Low-Rank Subspace Segmentation</a></p>
<p>19 0.70161897 <a title="49-lda-19" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>20 0.70120758 <a title="49-lda-20" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
