<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>168 iccv-2013-Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-168" href="#">iccv2013-168</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>168 iccv-2013-Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms</h1>
<br/><p>Source: <a title="iccv-2013-168-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Pang_Finding_the_Best_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Yu Pang, Haibin Ling</p><p>Abstract: Evaluating visual tracking algorithms, or “trackers ” for short, is of great importance in computer vision. However, it is hard to “fairly” compare trackers due to many parameters need to be tuned in the experimental configurations. On the other hand, when introducing a new tracker, a recent trend is to validate it by comparing it with several existing ones. Such an evaluation may have subjective biases towards the new tracker which typically performs the best. This is mainly due to the difficulty to optimally tune all its competitors and sometimes the selected testing sequences. By contrast, little subjective bias exists towards the “second best” ones1 in the contest. This observation inspires us with a novel perspective towards inhibiting subjective bias in evaluating trackers by analyzing the results between the second bests. In particular, we first collect all tracking papers published in major computer vision venues in recent years. From these papers, after filtering out potential biases in various aspects, we create a dataset containing many records of comparison results between various visual trackers. Using these records, we derive performance rank- ings of the involved trackers by four different methods. The first two methods model the dataset as a graph and then derive the rankings over the graph, one by a rank aggregation algorithm and the other by a PageRank-like solution. The other two methods take the records as generated from sports contests and adopt widely used Elo’s and Glicko ’s rating systems to derive the rankings. The experimental results are presented and may serve as a reference for related research.</p><p>Reference: <a title="iccv-2013-168-reference" href="../iccv2013_reference/iccv-2013-Finding_the_Best_from_the_Second_Bests_-_Inhibiting_Subjective_Bias_in_Evaluation_of_Visual_Tracking_Algorithms_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Evaluating visual tracking algorithms, or “trackers ” for short, is of great importance in computer vision. [sent-3, score-0.282]
</p><p>2 However, it is hard to “fairly” compare trackers due to many parameters need to be tuned in the experimental configurations. [sent-4, score-0.503]
</p><p>3 Such an evaluation may have subjective biases towards the new tracker which typically performs the best. [sent-6, score-0.462]
</p><p>4 This observation inspires us with a novel perspective towards inhibiting subjective bias in evaluating trackers by analyzing the results between the second bests. [sent-9, score-0.727]
</p><p>5 In particular, we first collect all tracking papers published in major computer vision venues in recent years. [sent-10, score-0.494]
</p><p>6 From these papers, after filtering out potential biases in various aspects, we create a dataset containing many records of comparison results between various visual trackers. [sent-11, score-0.543]
</p><p>7 Using these records, we derive performance rank-  ings of the involved trackers by four different methods. [sent-12, score-0.535]
</p><p>8 The first two methods model the dataset as a graph and then derive the rankings over the graph, one by a rank aggregation algorithm and the other by a PageRank-like solution. [sent-13, score-0.322]
</p><p>9 The other two methods take the records as generated from sports contests and adopt widely used Elo’s and Glicko ’s rating systems to derive the rankings. [sent-14, score-0.878]
</p><p>10 Introduction As an important topic in computer vision, visual tracking has been a widely explored area attracting a great amount of research efforts. [sent-17, score-0.314]
</p><p>11 Over the decades, dozens ofvisual tracking 1We treat all trackers other than the “best” as second best ones. [sent-18, score-0.755]
</p><p>12 Tracker E is newly proposed in the mock paper; C is from the authors’ previous work; other trackers (A, B and D) were proposed in papers sharing no co-author with the mock paper. [sent-27, score-0.869]
</p><p>13 algorithms, or trackers in short, have been developed and a  great packs of public datasets are available alongside [73]. [sent-28, score-0.503]
</p><p>14 These biases arise from many sources such as tracker parameters (e. [sent-30, score-0.324]
</p><p>15 It is therefore hard to tune many different trackers for a fair comparison. [sent-33, score-0.529]
</p><p>16 The basic idea is to test the trackers on a lot of public datasets and evaluate the results using uniformed metrics. [sent-35, score-0.503]
</p><p>17 The trackers are submitted by their authors and thus by assumption they are tuned optimally to win. [sent-36, score-0.529]
</p><p>18 However, many state-of-the-art trackers have not been tested this way. [sent-38, score-0.503]
</p><p>19 A byproduct of many such papers is the numerous evaluations for various tracking algorithms. [sent-40, score-0.423]
</p><p>20 However, such papers often have subjective biases towards their proposed new trackers which typically perform the best in the evaluation. [sent-41, score-0.919]
</p><p>21 On one hand, new trackers usually have some advantages that the authors aim to highlight. [sent-43, score-0.529]
</p><p>22 On the other hand, it is non-trivial for the paper authors to optimize all other trackers involved  2784  in the contest. [sent-44, score-0.529]
</p><p>23 For example, Table 1 simulates results in a typical tracking paper, where the newly proposed tracker E performs the best as expected. [sent-46, score-0.5]
</p><p>24 This observation inspires us with a novel perspective towards unbiased evaluation of visual trackers to explore the unbiased comparison information among the second best trackers reported by previous tracking papers. [sent-48, score-1.538]
</p><p>25 With this idea, we first collect all tracking papers published in major computer vision venues in recent years. [sent-49, score-0.494]
</p><p>26 From these papers, after filtering out potential biases in various aspects, we create a dataset containing many records of comparison results between various visual trackers. [sent-50, score-0.543]
</p><p>27 Using these records, we derive performance rankings of the involved trackers by four different methods. [sent-51, score-0.664]
</p><p>28 The first two methods model the dataset as a graph and then derive the rankings over the graph, one by a rank aggregation algorithm and the other by a PageRank-like solution. [sent-52, score-0.322]
</p><p>29 The other two methods take the records as generated from sports contests and adopt widely used Elo’s and Glicko’s rating systems to derive the rankings. [sent-53, score-0.878]
</p><p>30 –  Our contributions are twofold: First, we propose a subjective bias-resisting tracking evaluation method which has never been explored to the best of our knowledge. [sent-55, score-0.357]
</p><p>31 Related work The VS Workshop and the PETS Workshop are among the earliest ones to put efforts on comparing different trackers on public datasets. [sent-63, score-0.503]
</p><p>32 They provide datasets on different aspects of tracking scenarios and researchers apply their trackers on the same datasets, so that people can use certain metrics or evaluation methods to compare the results. [sent-64, score-0.811]
</p><p>33 At the early stage, the workshops focused more on bringing up new evaluation methods and tracking algorithms and there is no explicit comparison summary in each workshop. [sent-65, score-0.31]
</p><p>34 In recent years, PETS has covered many aspects of tracking scenarios and now its focus shifts towards multi-target tracking. [sent-66, score-0.311]
</p><p>35 Despite the efforts, it is still hard to run various trackers without biases due to the reason mention in previous sections. [sent-71, score-0.64]
</p><p>36 [68] built a large benchmark on visual tracking and evaluated thoroughly the performances of over 29 trackers. [sent-78, score-0.31]
</p><p>37 Hereafter, we call a visual tracking algorithm being evaluated as a tracker, a paper containing comparison results as a contest paper, or contest for short and a pairwise comparison between two trackers as a record. [sent-82, score-1.111]
</p><p>38 So we restrict contests to papers that have the same focus. [sent-87, score-0.352]
</p><p>39 Now the topic is determined, we collect contest papers from major computer vision journals including PAMI and IJCV, from 2000 to up-to-date issue. [sent-89, score-0.401]
</p><p>40 Extracting Records The key to extract records from contest papers is again to inhibit potential biases. [sent-112, score-0.705]
</p><p>41 For each contest, we remove the results which contain the trackers proposed by the authors ofthis very contest, including both the newly proposed one and possibly the trackers in the authors’ previous study. [sent-113, score-1.067]
</p><p>42 After the above filtering, what left in a contest paper is the evaluation of several trackers on several sequences. [sent-115, score-0.696]
</p><p>43 For example, only trackers A, B and D remains from Table 1. [sent-116, score-0.503]
</p><p>44 The ranking representation contains the rankings from each sequence. [sent-118, score-0.348]
</p><p>45 In particular, for every pair of trackers in the partial ranking, say A and B, we will generate a record as follows: If A performs better than or as good as B, we generate a record as (A, B, label), such that label = ? [sent-122, score-0.688]
</p><p>46 Figure 1(a) lists all records extracted from the mock paper in Table 1. [sent-128, score-0.394]
</p><p>47 Following the above procedure, we obtain a set of records involving 48 trackers. [sent-129, score-0.314]
</p><p>48 To further reduce the chance of biases, we remove any trackers who appear in less than 10 records or in only one contest. [sent-132, score-0.817]
</p><p>49 After all the cleaning, we have 15 trackers, 664 partial rankings, and 6280 records among which there are 151 records of ? [sent-133, score-0.653]
</p><p>50 The 15 trackers are Meanshift [15], ColorPF [55], IVT [57], Ensemble [4], OFS [13], FragT [1], OBT [25], SemiBoost [26], MIL [5], L1T [5 1], BOBT [60], TLD [34], VTD [37], Struck [27] and MTT [76]. [sent-168, score-0.503]
</p><p>51 For a directed edge from A to B, the color and thickness are proportional to the number of records that agree on “A is better than B”. [sent-170, score-0.341]
</p><p>52 It is worth noting there are some factors ignored in the above data preparation, such as the degree of challenges of different sequences or the extent to which one tracker outperforms another one, all of which could potentially affect the ranking results. [sent-171, score-0.406]
</p><p>53 It is also worth noting that there are many important tracking papers that do not follow the above evaluation paradigms for either trackers or contests. [sent-176, score-0.956]
</p><p>54 Rank Aggregation Algorithm Rank aggregation has been widely used in webpage ranking and other fields [2, 3, 18]. [sent-179, score-0.355]
</p><p>55 >  weighted graph G = (V, A), where V = T is the same universe set and any wij ∈ A is the fraction of inputs ranking ibefore j. [sent-194, score-0.341]
</p><p>56 Our ranking representation can be naturally fit into the rank aggregation model. [sent-195, score-0.346]
</p><p>57 If we view the collected records P1 as competition results between sevtehreal c otrallcekceterds, ethcoenrd our problem naturally simulates a sport game and each tracker naturally an athlete. [sent-246, score-0.527]
</p><p>58 Elo’s Rating One of the most successful ranking methods is the Elo’s rating [20]. [sent-251, score-0.465]
</p><p>59 The core idea is that the ranking score is a scaling rating, so that the score difference between two nodes determines an estimation of expected outcomes. [sent-252, score-0.321]
</p><p>60 Given the ranking scores Ri and Rj for two trackers iand j, the expectation is estimated as: Eij  =  1 + 10(R1j−Ri)/400  . [sent-253, score-0.746]
</p><p>61 5, idfr iaw lwosineot vejr 2787  (5)  Then, the difference between the expected outcome and the actual outcome will be used to update the ranking score: RiNew = RiOld + K(Sij − Eij)  (6)  where K is the updating rate. [sent-258, score-0.341]
</p><p>62 One of the interesting and desired properties of this model is that the ranking score does not necessarily increase when a winning is observed. [sent-261, score-0.309]
</p><p>63 The above method depends on the order of the input records since the ranking scores are updated in a sequential manner: at each step the result will be updated according to the actual outcome of Sij and the expected outcome of Eij . [sent-272, score-0.655]
</p><p>64 Glicko’s rating Glicko’s rating [24] is a generalized version of Elo’s rating. [sent-276, score-0.492]
</p><p>65 It uses two parameters to model the rating: Ri is the expected rating score for node iand Di measures its confidence. [sent-277, score-0.297]
</p><p>66 More precisely, we are 95% confident that the true rating of the i-th tracker ranges between Ri − 2Di to Ri + 2Di. [sent-278, score-0.459]
</p><p>67 Another difference is that the final rating score will be the lower 95% confidence score which is Ri − 2Di. [sent-283, score-0.348]
</p><p>68 We used the total 6280 records as the original pool and uniformly generated a random sequence of 200,000 a sufficiently large amount of records to run the algorithm. [sent-299, score-0.628]
</p><p>69 We measured the means and standard deviations of the ranking positions for each tracker over 100 different runs as shown in Table 2(c). [sent-301, score-0.406]
</p><p>70 Similar to Elo’s rating, we used a random sequence of 200,000 records for Glicko’s rating. [sent-303, score-0.314]
</p><p>71 But this time we uniformly generated 10,000 records for one round, run the algorithm for 20 rounds and report the results. [sent-304, score-0.314]
</p><p>72 That is, the top few trackers are always top and the bottom trackers are often bottom. [sent-313, score-1.006]
</p><p>73 Also in sports ranking algorithms, we could see they form cliques in terms of their score distances. [sent-314, score-0.377]
</p><p>74 This suggests that in different runs, trackers will have different ranking positions within the clique, but the relative ranking positions of different cliques are mostly preserved. [sent-317, score-0.975]
</p><p>75 For example OFS, Ensemble and Meanshift have compared to only 4 other trackers within the set, compared to an average number of 8. [sent-320, score-0.503]
</p><p>76 In addition to these 3 trackers, ColorPF and BOBT also have quite low number of records compared with the average. [sent-322, score-0.314]
</p><p>77 We have selected 15 out of 48 trackers to be compared according to our criteria. [sent-327, score-0.503]
</p><p>78 Some of the early trackers have been integrated into other trackers, such as the particle filter [32], [36]. [sent-329, score-0.536]
</p><p>79 Some other early trackers have been considered as the baseline tracker, such as the Meanshift tracker [15]. [sent-332, score-0.69]
</p><p>80 Such trackers are mostly used to compare with the author’s methods in their papers before 2009. [sent-333, score-0.674]
</p><p>81 However, since most of such papers compared only two trackers and one is their owns, we can not extract anything from these papers based on our criteria described in Section 2. [sent-334, score-0.845]
</p><p>82 Many other trackers including most recently published ones have no open-source codes. [sent-336, score-0.503]
</p><p>83 Although some of the trackers are compared in their own authors’ new work, according to our criteria, they are not included as well. [sent-338, score-0.503]
</p><p>84 Another potential problem is some trackers are sensitive to initialization or parameter tuning, but we have not taken them into consideration. [sent-349, score-0.532]
</p><p>85 It is possible that different trackers have different specializations, for example, some trackers may be good at dealing with occlusion, illumination variation, re-identify the missing target and so on. [sent-357, score-1.006]
</p><p>86 In the future if we can categorize the dataset into different scenarios when  more records are available, we may be able to provide more specialized rankings. [sent-359, score-0.314]
</p><p>87 We would also like to point out that the records we extracted from contests are possibly biased themselves, because some sequences are more popular than others and compared more often. [sent-360, score-0.495]
</p><p>88 So the argument is similar to [62], the whole datasets the tracking community shares may not be a good representation of the real world, thus the ranking results we have may only partially reflect their performances in the real world. [sent-364, score-0.499]
</p><p>89 In summary, it is unrealistic to perform a rigorous unbiased evaluation for tracking algorithms. [sent-365, score-0.383]
</p><p>90 Conclusion In this paper, we have proposed a novel method to compare trackers performances and rank them using four different algorithms. [sent-369, score-0.579]
</p><p>91 Following the trend in tracking papers, we are able to collect a dataset of comparisons of the “second best” ones. [sent-370, score-0.308]
</p><p>92 After filtering out potential biases in various aspects, we construct a dataset containing 15 trackers and 6280 records. [sent-372, score-0.702]
</p><p>93 Rank aggregation is to use the partial rankings find a full ranking that optimize some objective function. [sent-374, score-0.452]
</p><p>94 The latter two take the records as generated from sports contests and adopt widely used Elo’s and Glicko’s rating systems to derive the rankings. [sent-376, score-0.878]
</p><p>95 Robust visual tracking using an adaptive coupled-layer visual model. [sent-447, score-0.312]
</p><p>96 Robust object tracking via online dynamic spatial bias appearance models. [sent-452, score-0.341]
</p><p>97 Mcmc-based particle filtering for tracking a variable number of interacting targets. [sent-608, score-0.318]
</p><p>98 Treat samples differently: Object tracking with semi-supervised online covboost. [sent-637, score-0.298]
</p><p>99 Non-sparse linear representations for visual tracking with online reservoir metric learning. [sent-667, score-0.328]
</p><p>100 Visual tracking via adaptive tracker selection with multiple features. [sent-866, score-0.439]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('trackers', 0.503), ('records', 0.314), ('tracking', 0.252), ('rating', 0.246), ('ranking', 0.219), ('glicko', 0.201), ('tracker', 0.187), ('contests', 0.181), ('papers', 0.171), ('contest', 0.163), ('elo', 0.15), ('biases', 0.137), ('rankings', 0.129), ('eij', 0.083), ('mock', 0.08), ('record', 0.08), ('aggregation', 0.079), ('unbiased', 0.077), ('subjective', 0.075), ('sports', 0.073), ('sij', 0.07), ('win', 0.066), ('pami', 0.063), ('outcome', 0.061), ('kwon', 0.055), ('dick', 0.053), ('wij', 0.052), ('score', 0.051), ('den', 0.051), ('pets', 0.05), ('ri', 0.048), ('rank', 0.048), ('shen', 0.047), ('pagerank', 0.047), ('online', 0.046), ('grabner', 0.044), ('bias', 0.043), ('mt', 0.041), ('bobt', 0.04), ('colorpf', 0.04), ('fragt', 0.04), ('inhibiting', 0.04), ('lpkwiksorth', 0.04), ('ofs', 0.04), ('rinew', 0.04), ('riold', 0.04), ('venues', 0.04), ('meanshift', 0.04), ('winning', 0.039), ('chess', 0.036), ('journals', 0.036), ('obt', 0.036), ('universe', 0.036), ('literatures', 0.035), ('newly', 0.035), ('cliques', 0.034), ('van', 0.034), ('graph', 0.034), ('yao', 0.034), ('ling', 0.034), ('filtering', 0.033), ('draw', 0.033), ('towards', 0.033), ('inspires', 0.033), ('particle', 0.033), ('widely', 0.032), ('derive', 0.032), ('tji', 0.031), ('collect', 0.031), ('visual', 0.03), ('evaluation', 0.03), ('vivid', 0.03), ('pang', 0.03), ('potential', 0.029), ('inhibit', 0.028), ('bringing', 0.028), ('di', 0.028), ('performances', 0.028), ('eigenvector', 0.028), ('dj', 0.027), ('wu', 0.027), ('agree', 0.027), ('tij', 0.026), ('simulates', 0.026), ('struck', 0.026), ('vtd', 0.026), ('tune', 0.026), ('authors', 0.026), ('aspects', 0.026), ('confident', 0.026), ('tld', 0.026), ('zhong', 0.026), ('lu', 0.025), ('webpage', 0.025), ('trend', 0.025), ('partial', 0.025), ('saffari', 0.024), ('expectation', 0.024), ('robust', 0.024), ('unrealistic', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="168-tfidf-1" href="./iccv-2013-Finding_the_Best_from_the_Second_Bests_-_Inhibiting_Subjective_Bias_in_Evaluation_of_Visual_Tracking_Algorithms.html">168 iccv-2013-Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms</a></p>
<p>Author: Yu Pang, Haibin Ling</p><p>Abstract: Evaluating visual tracking algorithms, or “trackers ” for short, is of great importance in computer vision. However, it is hard to “fairly” compare trackers due to many parameters need to be tuned in the experimental configurations. On the other hand, when introducing a new tracker, a recent trend is to validate it by comparing it with several existing ones. Such an evaluation may have subjective biases towards the new tracker which typically performs the best. This is mainly due to the difficulty to optimally tune all its competitors and sometimes the selected testing sequences. By contrast, little subjective bias exists towards the “second best” ones1 in the contest. This observation inspires us with a novel perspective towards inhibiting subjective bias in evaluating trackers by analyzing the results between the second bests. In particular, we first collect all tracking papers published in major computer vision venues in recent years. From these papers, after filtering out potential biases in various aspects, we create a dataset containing many records of comparison results between various visual trackers. Using these records, we derive performance rank- ings of the involved trackers by four different methods. The first two methods model the dataset as a graph and then derive the rankings over the graph, one by a rank aggregation algorithm and the other by a PageRank-like solution. The other two methods take the records as generated from sports contests and adopt widely used Elo’s and Glicko ’s rating systems to derive the rankings. The experimental results are presented and may serve as a reference for related research.</p><p>2 0.30447516 <a title="168-tfidf-2" href="./iccv-2013-Online_Robust_Non-negative_Dictionary_Learning_for_Visual_Tracking.html">298 iccv-2013-Online Robust Non-negative Dictionary Learning for Visual Tracking</a></p>
<p>Author: Naiyan Wang, Jingdong Wang, Dit-Yan Yeung</p><p>Abstract: This paper studies the visual tracking problem in video sequences and presents a novel robust sparse tracker under the particle filter framework. In particular, we propose an online robust non-negative dictionary learning algorithm for updating the object templates so that each learned template can capture a distinctive aspect of the tracked object. Another appealing property of this approach is that it can automatically detect and reject the occlusion and cluttered background in a principled way. In addition, we propose a new particle representation formulation using the Huber loss function. The advantage is that it can yield robust estimation without using trivial templates adopted by previous sparse trackers, leading to faster computation. We also reveal the equivalence between this new formulation and the previous one which uses trivial templates. The proposed tracker is empirically compared with state-of-the-art trackers on some challenging video sequences. Both quantitative and qualitative comparisons show that our proposed tracker is superior and more stable.</p><p>3 0.24729504 <a title="168-tfidf-3" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>Author: Zhibin Hong, Xue Mei, Danil Prokhorov, Dacheng Tao</p><p>Abstract: Combining multiple observation views has proven beneficial for tracking. In this paper, we cast tracking as a novel multi-task multi-view sparse learning problem and exploit the cues from multiple views including various types of visual features, such as intensity, color, and edge, where each feature observation can be sparsely represented by a linear combination of atoms from an adaptive feature dictionary. The proposed method is integrated in a particle filter framework where every view in each particle is regarded as an individual task. We jointly consider the underlying relationship between tasks across different views and different particles, and tackle it in a unified robust multi-task formulation. In addition, to capture the frequently emerging outlier tasks, we decompose the representation matrix to two collaborative components which enable a more robust and accurate approximation. We show that theproposedformulation can be efficiently solved using the Accelerated Proximal Gradient method with a small number of closed-form updates. The presented tracker is implemented using four types of features and is tested on numerous benchmark video sequences. Both the qualitative and quantitative results demonstrate the superior performance of the proposed approach compared to several stateof-the-art trackers.</p><p>4 0.23384178 <a title="168-tfidf-4" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>Author: Shuran Song, Jianxiong Xiao</p><p>Abstract: Despite significant progress, tracking is still considered to be a very challenging task. Recently, the increasing popularity of depth sensors has made it possible to obtain reliable depth easily. This may be a game changer for tracking, since depth can be used to prevent model drift and handle occlusion. We also observe that current tracking algorithms are mostly evaluated on a very small number of videos collectedandannotated by different groups. The lack of a reasonable size and consistently constructed benchmark has prevented a persuasive comparison among different algorithms. In this paper, we construct a unified benchmark dataset of 100 RGBD videos with high diversity, propose different kinds of RGBD tracking algorithms using 2D or 3D model, and present a quantitative comparison of various algorithms with RGB or RGBD input. We aim to lay the foundation for further research in both RGB and RGBD tracking, and our benchmark is available at http://tracking.cs.princeton.edu.</p><p>5 0.19890858 <a title="168-tfidf-5" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>Author: Kwang Moo Yi, Hawook Jeong, Byeongho Heo, Hyung Jin Chang, Jin Young Choi</p><p>Abstract: In this paper we propose an object tracking method in case of inaccurate initializations. To track objects accurately in such situation, the proposed method uses “motion saliency ” and “descriptor saliency ” of local features and performs tracking based on generalized Hough transform (GHT). The proposed motion saliency of a local feature emphasizes features having distinctive motions, compared to the motions which are not from the target object. The descriptor saliency emphasizes features which are likely to be of the object in terms of its feature descriptors. Through these saliencies, the proposed method tries to “learn and find” the target object rather than looking for what was given at initialization, giving robust results even with inaccurate initializations. Also, our tracking result is obtained by combining the results of each local feature of the target and the surroundings with GHT voting, thus is robust against severe occlusions as well. The proposed method is compared against nine other methods, with nine image sequences, and hundred random initializations. The experimental results show that our method outperforms all other compared methods.</p><p>6 0.17287022 <a title="168-tfidf-6" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>7 0.16741237 <a title="168-tfidf-7" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>8 0.16616403 <a title="168-tfidf-8" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>9 0.15577406 <a title="168-tfidf-9" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>10 0.15210696 <a title="168-tfidf-10" href="./iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects.html">320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</a></p>
<p>11 0.14356619 <a title="168-tfidf-11" href="./iccv-2013-Real-Time_Body_Tracking_with_One_Depth_Camera_and_Inertial_Sensors.html">341 iccv-2013-Real-Time Body Tracking with One Depth Camera and Inertial Sensors</a></p>
<p>12 0.13484973 <a title="168-tfidf-12" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>13 0.1181486 <a title="168-tfidf-13" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>14 0.1034047 <a title="168-tfidf-14" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>15 0.095421173 <a title="168-tfidf-15" href="./iccv-2013-Discriminative_Label_Propagation_for_Multi-object_Tracking_with_Sporadic_Appearance_Features.html">120 iccv-2013-Discriminative Label Propagation for Multi-object Tracking with Sporadic Appearance Features</a></p>
<p>16 0.087762795 <a title="168-tfidf-16" href="./iccv-2013-Orderless_Tracking_through_Model-Averaged_Posterior_Estimation.html">303 iccv-2013-Orderless Tracking through Model-Averaged Posterior Estimation</a></p>
<p>17 0.08573284 <a title="168-tfidf-17" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>18 0.085348487 <a title="168-tfidf-18" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>19 0.07998728 <a title="168-tfidf-19" href="./iccv-2013-Slice_Sampling_Particle_Belief_Propagation.html">395 iccv-2013-Slice Sampling Particle Belief Propagation</a></p>
<p>20 0.078458056 <a title="168-tfidf-20" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.169), (1, -0.033), (2, 0.03), (3, 0.003), (4, -0.004), (5, -0.07), (6, -0.113), (7, 0.16), (8, -0.09), (9, 0.213), (10, -0.09), (11, -0.195), (12, 0.059), (13, 0.089), (14, 0.067), (15, -0.04), (16, 0.102), (17, 0.044), (18, -0.079), (19, -0.069), (20, -0.062), (21, 0.03), (22, -0.095), (23, -0.093), (24, 0.016), (25, 0.049), (26, 0.015), (27, 0.047), (28, 0.008), (29, 0.024), (30, 0.028), (31, -0.055), (32, -0.055), (33, 0.04), (34, -0.005), (35, -0.043), (36, -0.038), (37, -0.036), (38, -0.03), (39, 0.02), (40, -0.011), (41, -0.058), (42, 0.004), (43, -0.064), (44, 0.084), (45, 0.032), (46, -0.04), (47, -0.03), (48, 0.031), (49, 0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96794564 <a title="168-lsi-1" href="./iccv-2013-Finding_the_Best_from_the_Second_Bests_-_Inhibiting_Subjective_Bias_in_Evaluation_of_Visual_Tracking_Algorithms.html">168 iccv-2013-Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms</a></p>
<p>Author: Yu Pang, Haibin Ling</p><p>Abstract: Evaluating visual tracking algorithms, or “trackers ” for short, is of great importance in computer vision. However, it is hard to “fairly” compare trackers due to many parameters need to be tuned in the experimental configurations. On the other hand, when introducing a new tracker, a recent trend is to validate it by comparing it with several existing ones. Such an evaluation may have subjective biases towards the new tracker which typically performs the best. This is mainly due to the difficulty to optimally tune all its competitors and sometimes the selected testing sequences. By contrast, little subjective bias exists towards the “second best” ones1 in the contest. This observation inspires us with a novel perspective towards inhibiting subjective bias in evaluating trackers by analyzing the results between the second bests. In particular, we first collect all tracking papers published in major computer vision venues in recent years. From these papers, after filtering out potential biases in various aspects, we create a dataset containing many records of comparison results between various visual trackers. Using these records, we derive performance rank- ings of the involved trackers by four different methods. The first two methods model the dataset as a graph and then derive the rankings over the graph, one by a rank aggregation algorithm and the other by a PageRank-like solution. The other two methods take the records as generated from sports contests and adopt widely used Elo’s and Glicko ’s rating systems to derive the rankings. The experimental results are presented and may serve as a reference for related research.</p><p>2 0.85957944 <a title="168-lsi-2" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>Author: Zhibin Hong, Xue Mei, Danil Prokhorov, Dacheng Tao</p><p>Abstract: Combining multiple observation views has proven beneficial for tracking. In this paper, we cast tracking as a novel multi-task multi-view sparse learning problem and exploit the cues from multiple views including various types of visual features, such as intensity, color, and edge, where each feature observation can be sparsely represented by a linear combination of atoms from an adaptive feature dictionary. The proposed method is integrated in a particle filter framework where every view in each particle is regarded as an individual task. We jointly consider the underlying relationship between tasks across different views and different particles, and tackle it in a unified robust multi-task formulation. In addition, to capture the frequently emerging outlier tasks, we decompose the representation matrix to two collaborative components which enable a more robust and accurate approximation. We show that theproposedformulation can be efficiently solved using the Accelerated Proximal Gradient method with a small number of closed-form updates. The presented tracker is implemented using four types of features and is tested on numerous benchmark video sequences. Both the qualitative and quantitative results demonstrate the superior performance of the proposed approach compared to several stateof-the-art trackers.</p><p>3 0.85790461 <a title="168-lsi-3" href="./iccv-2013-Online_Robust_Non-negative_Dictionary_Learning_for_Visual_Tracking.html">298 iccv-2013-Online Robust Non-negative Dictionary Learning for Visual Tracking</a></p>
<p>Author: Naiyan Wang, Jingdong Wang, Dit-Yan Yeung</p><p>Abstract: This paper studies the visual tracking problem in video sequences and presents a novel robust sparse tracker under the particle filter framework. In particular, we propose an online robust non-negative dictionary learning algorithm for updating the object templates so that each learned template can capture a distinctive aspect of the tracked object. Another appealing property of this approach is that it can automatically detect and reject the occlusion and cluttered background in a principled way. In addition, we propose a new particle representation formulation using the Huber loss function. The advantage is that it can yield robust estimation without using trivial templates adopted by previous sparse trackers, leading to faster computation. We also reveal the equivalence between this new formulation and the previous one which uses trivial templates. The proposed tracker is empirically compared with state-of-the-art trackers on some challenging video sequences. Both quantitative and qualitative comparisons show that our proposed tracker is superior and more stable.</p><p>4 0.75292337 <a title="168-lsi-4" href="./iccv-2013-Slice_Sampling_Particle_Belief_Propagation.html">395 iccv-2013-Slice Sampling Particle Belief Propagation</a></p>
<p>Author: Oliver Müller, Michael Ying Yang, Bodo Rosenhahn</p><p>Abstract: Inference in continuous label Markov random fields is a challenging task. We use particle belief propagation (PBP) for solving the inference problem in continuous label space. Sampling particles from the belief distribution is typically done by using Metropolis-Hastings (MH) Markov chain Monte Carlo (MCMC) methods which involves sampling from a proposal distribution. This proposal distribution has to be carefully designed depending on the particular model and input data to achieve fast convergence. We propose to avoid dependence on a proposal distribution by introducing a slice sampling based PBP algorithm. The proposed approach shows superior convergence performance on an image denoising toy example. Our findings are validated on a challenging relational 2D feature tracking application.</p><p>5 0.73816931 <a title="168-lsi-5" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>Author: Dapeng Chen, Zejian Yuan, Yang Wu, Geng Zhang, Nanning Zheng</p><p>Abstract: Representation is a fundamental problem in object tracking. Conventional methods track the target by describing its local or global appearance. In this paper we present that, besides the two paradigms, the composition of local region histograms can also provide diverse and important object cues. We use cells to extract local appearance, and construct complex cells to integrate the information from cells. With different spatial arrangements of cells, complex cells can explore various contextual information at multiple scales, which is important to improve the tracking performance. We also develop a novel template-matching algorithm for object tracking, where the template is composed of temporal varying cells and has two layers to capture the target and background appearance respectively. An adaptive weight is associated with each complex cell to cope with occlusion as well as appearance variation. A fusion weight is associated with each complex cell type to preserve the global distinctiveness. Our algorithm is evaluated on 25 challenging sequences, and the results not only confirm the contribution of each component in our tracking system, but also outperform other competing trackers.</p><p>6 0.70804143 <a title="168-lsi-6" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>7 0.70600063 <a title="168-lsi-7" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>8 0.70214087 <a title="168-lsi-8" href="./iccv-2013-Orderless_Tracking_through_Model-Averaged_Posterior_Estimation.html">303 iccv-2013-Orderless Tracking through Model-Averaged Posterior Estimation</a></p>
<p>9 0.68222058 <a title="168-lsi-9" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>10 0.63669729 <a title="168-lsi-10" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>11 0.63563156 <a title="168-lsi-11" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>12 0.58624375 <a title="168-lsi-12" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>13 0.57365286 <a title="168-lsi-13" href="./iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects.html">320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</a></p>
<p>14 0.53342122 <a title="168-lsi-14" href="./iccv-2013-Conservation_Tracking.html">87 iccv-2013-Conservation Tracking</a></p>
<p>15 0.49957243 <a title="168-lsi-15" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>16 0.47825921 <a title="168-lsi-16" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>17 0.46273685 <a title="168-lsi-17" href="./iccv-2013-Discriminative_Label_Propagation_for_Multi-object_Tracking_with_Sporadic_Appearance_Features.html">120 iccv-2013-Discriminative Label Propagation for Multi-object Tracking with Sporadic Appearance Features</a></p>
<p>18 0.45444936 <a title="168-lsi-18" href="./iccv-2013-Dynamic_Probabilistic_Volumetric_Models.html">128 iccv-2013-Dynamic Probabilistic Volumetric Models</a></p>
<p>19 0.44664872 <a title="168-lsi-19" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>20 0.42369628 <a title="168-lsi-20" href="./iccv-2013-The_Way_They_Move%3A_Tracking_Multiple_Targets_with_Similar_Appearance.html">418 iccv-2013-The Way They Move: Tracking Multiple Targets with Similar Appearance</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.072), (6, 0.015), (7, 0.032), (12, 0.031), (26, 0.048), (31, 0.032), (40, 0.04), (42, 0.089), (48, 0.018), (64, 0.12), (73, 0.025), (84, 0.169), (89, 0.149), (95, 0.014), (97, 0.015), (98, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87208194 <a title="168-lda-1" href="./iccv-2013-Stacked_Predictive_Sparse_Coding_for_Classification_of_Distinct_Regions_in_Tumor_Histopathology.html">401 iccv-2013-Stacked Predictive Sparse Coding for Classification of Distinct Regions in Tumor Histopathology</a></p>
<p>Author: Hang Chang, Yin Zhou, Paul Spellman, Bahram Parvin</p><p>Abstract: Image-based classification ofhistology sections, in terms of distinct components (e.g., tumor, stroma, normal), provides a series of indices for tumor composition. Furthermore, aggregation of these indices, from each whole slide image (WSI) in a large cohort, can provide predictive models of the clinical outcome. However, performance of the existing techniques is hindered as a result of large technical variations and biological heterogeneities that are always present in a large cohort. We propose a system that automatically learns a series of basis functions for representing the underlying spatial distribution using stacked predictive sparse decomposition (PSD). The learned representation is then fed into the spatial pyramid matching framework (SPM) with a linear SVM classifier. The system has been evaluated for classification of (a) distinct histological components for two cohorts of tumor types, and (b) colony organization of normal and malignant cell lines in 3D cell culture models. Throughput has been increased through the utility of graphical processing unit (GPU), and evalu- ation indicates a superior performance results, compared with previous research.</p><p>same-paper 2 0.83433241 <a title="168-lda-2" href="./iccv-2013-Finding_the_Best_from_the_Second_Bests_-_Inhibiting_Subjective_Bias_in_Evaluation_of_Visual_Tracking_Algorithms.html">168 iccv-2013-Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms</a></p>
<p>Author: Yu Pang, Haibin Ling</p><p>Abstract: Evaluating visual tracking algorithms, or “trackers ” for short, is of great importance in computer vision. However, it is hard to “fairly” compare trackers due to many parameters need to be tuned in the experimental configurations. On the other hand, when introducing a new tracker, a recent trend is to validate it by comparing it with several existing ones. Such an evaluation may have subjective biases towards the new tracker which typically performs the best. This is mainly due to the difficulty to optimally tune all its competitors and sometimes the selected testing sequences. By contrast, little subjective bias exists towards the “second best” ones1 in the contest. This observation inspires us with a novel perspective towards inhibiting subjective bias in evaluating trackers by analyzing the results between the second bests. In particular, we first collect all tracking papers published in major computer vision venues in recent years. From these papers, after filtering out potential biases in various aspects, we create a dataset containing many records of comparison results between various visual trackers. Using these records, we derive performance rank- ings of the involved trackers by four different methods. The first two methods model the dataset as a graph and then derive the rankings over the graph, one by a rank aggregation algorithm and the other by a PageRank-like solution. The other two methods take the records as generated from sports contests and adopt widely used Elo’s and Glicko ’s rating systems to derive the rankings. The experimental results are presented and may serve as a reference for related research.</p><p>3 0.82137722 <a title="168-lda-3" href="./iccv-2013-Semantically-Based_Human_Scanpath_Estimation_with_HMMs.html">381 iccv-2013-Semantically-Based Human Scanpath Estimation with HMMs</a></p>
<p>Author: Huiying Liu, Dong Xu, Qingming Huang, Wen Li, Min Xu, Stephen Lin</p><p>Abstract: We present a method for estimating human scanpaths, which are sequences of gaze shifts that follow visual attention over an image. In this work, scanpaths are modeled based on three principal factors that influence human attention, namely low-levelfeature saliency, spatialposition, and semantic content. Low-level feature saliency is formulated as transition probabilities between different image regions based on feature differences. The effect of spatial position on gaze shifts is modeled as a Levy flight with the shifts following a 2D Cauchy distribution. To account for semantic content, we propose to use a Hidden Markov Model (HMM) with a Bag-of-Visual-Words descriptor of image regions. An HMM is well-suited for this purpose in that 1) the hidden states, obtained by unsupervised learning, can represent latent semantic concepts, 2) the prior distribution of the hidden states describes visual attraction to the semantic concepts, and 3) the transition probabilities represent human gaze shift patterns. The proposed method is applied to task-driven viewing processes. Experiments and analysis performed on human eye gaze data verify the effectiveness of this method.</p><p>4 0.8127687 <a title="168-lda-4" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<p>Author: Tianfu Wu, Song-Chun Zhu</p><p>Abstract: Many object detectors, such as AdaBoost, SVM and deformable part-based models (DPM), compute additive scoring functions at a large number of windows scanned over image pyramid, thus computational efficiency is an important consideration beside accuracy performance. In this paper, we present a framework of learning cost-sensitive decision policy which is a sequence of two-sided thresholds to execute early rejection or early acceptance based on the accumulative scores at each step. A decision policy is said to be optimal if it minimizes an empirical global risk function that sums over the loss of false negatives (FN) and false positives (FP), and the cost of computation. While the risk function is very complex due to high-order connections among the two-sided thresholds, we find its upper bound can be optimized by dynamic programming (DP) efficiently and thus say the learned policy is near-optimal. Given the loss of FN and FP and the cost in three numbers, our method can produce a policy on-the-fly for Adaboost, SVM and DPM. In experiments, we show that our decision policy outperforms state-of-the-art cascade methods significantly in terms of speed with similar accuracy performance.</p><p>5 0.78236187 <a title="168-lda-5" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>Author: Sukrit Shankar, Joan Lasenby, Roberto Cipolla</p><p>Abstract: Relative (comparative) attributes are promising for thematic ranking of visual entities, which also aids in recognition tasks [19, 23]. However, attribute rank learning often requires a substantial amount of relational supervision, which is highly tedious, and apparently impracticalfor realworld applications. In this paper, we introduce the Semantic Transform, which under minimal supervision, adaptively finds a semantic feature space along with a class ordering that is related in the best possible way. Such a semantic space is found for every attribute category. To relate the classes under weak supervision, the class ordering needs to be refined according to a cost function in an iterative procedure. This problem is ideally NP-hard, and we thus propose a constrained search tree formulation for the same. Driven by the adaptive semantic feature space representation, our model achieves the best results to date for all of the tasks of relative, absolute and zero-shot classification on two popular datasets.</p><p>6 0.78215569 <a title="168-lda-6" href="./iccv-2013-Incorporating_Cloud_Distribution_in_Sky_Representation.html">215 iccv-2013-Incorporating Cloud Distribution in Sky Representation</a></p>
<p>7 0.78035724 <a title="168-lda-7" href="./iccv-2013-Bayesian_Robust_Matrix_Factorization_for_Image_and_Video_Processing.html">60 iccv-2013-Bayesian Robust Matrix Factorization for Image and Video Processing</a></p>
<p>8 0.77672809 <a title="168-lda-8" href="./iccv-2013-Finding_Actors_and_Actions_in_Movies.html">166 iccv-2013-Finding Actors and Actions in Movies</a></p>
<p>9 0.77381021 <a title="168-lda-9" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>10 0.77301741 <a title="168-lda-10" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>11 0.77229863 <a title="168-lda-11" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>12 0.77205467 <a title="168-lda-12" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>13 0.77171695 <a title="168-lda-13" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>14 0.77082682 <a title="168-lda-14" href="./iccv-2013-Efficient_Hand_Pose_Estimation_from_a_Single_Depth_Image.html">133 iccv-2013-Efficient Hand Pose Estimation from a Single Depth Image</a></p>
<p>15 0.76985949 <a title="168-lda-15" href="./iccv-2013-Linear_Sequence_Discriminant_Analysis%3A_A_Model-Based_Dimensionality_Reduction_Method_for_Vector_Sequences.html">253 iccv-2013-Linear Sequence Discriminant Analysis: A Model-Based Dimensionality Reduction Method for Vector Sequences</a></p>
<p>16 0.7698344 <a title="168-lda-16" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>17 0.7690025 <a title="168-lda-17" href="./iccv-2013-Orderless_Tracking_through_Model-Averaged_Posterior_Estimation.html">303 iccv-2013-Orderless Tracking through Model-Averaged Posterior Estimation</a></p>
<p>18 0.76630944 <a title="168-lda-18" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>19 0.76564902 <a title="168-lda-19" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>20 0.76421297 <a title="168-lda-20" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
