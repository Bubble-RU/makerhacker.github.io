<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-31" href="#">iccv2013-31</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</h1>
<br/><p>Source: <a title="iccv-2013-31-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Wang_A_Unified_Probabilistic_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Xiaoyang Wang, Qiang Ji</p><p>Abstract: This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. As a list of semantically meaningful properties of objects, attributes generally relate to each other statistically. In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and objectindependent attribute relationships. The model utilizes the captured relationships to benefit both attribute prediction and object recognition. Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases.</p><p>Reference: <a title="iccv-2013-31-reference" href="../iccv2013_reference/iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. [sent-2, score-1.759]
</p><p>2 In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and objectindependent attribute relationships. [sent-4, score-1.102]
</p><p>3 The model utilizes the captured relationships to benefit both attribute prediction and object recognition. [sent-5, score-1.132]
</p><p>4 Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases. [sent-6, score-1.919]
</p><p>5 [6, 17, 24]) generally utilizes attributes as an intermediate layer descriptive representation, and has applied attributes to several interesting applications like zero-shot learning [17], description of objects [6], and object recognition [34]. [sent-11, score-0.924]
</p><p>6 Standard attribute approaches generally learn a group of attribute classifiers, one for each attribute. [sent-12, score-1.482]
</p><p>7 During testing, attributes are predicted individually through the learned attribute classifiers. [sent-13, score-1.172]
</p><p>8 And, a low quality attribute prediction can adversely affect the subsequent object recognition. [sent-17, score-0.888]
</p><p>9 This motivates us to exploit the statistical relationships between attributes and objects, and utilize these relationships to help predict certain attributes (or ob-  ? [sent-18, score-1.076]
</p><p>10 In this way, both the object-dependent and object-independent attribute relationships are automatically discovered and captured. [sent-75, score-0.894]
</p><p>11 During testing, the unified model can infer both the attribute node states and the object node state given the attribute measurements predicted individually by the prelearned attribute classifiers. [sent-76, score-2.879]
</p><p>12 Experiments on four benchmark datasets show that the unified model can effectively improve the attribute prediction accuracy by utilizing the captured relationships. [sent-77, score-1.092]
</p><p>13 describing task which utilizes attribute predictions as a detailed object description, besides the class label; 2. [sent-84, score-0.904]
</p><p>14 To improve the performance on either of these two attribute related tasks, approaches can be divided into either defining/discovering better attributes, or building better models for attribute prediction and object recognition using existing attributes. [sent-86, score-1.642]
</p><p>15 Among these approaches, the latent SVM attribute model [34] and CRF [2] also utilize attribute relationships. [sent-91, score-1.516]
</p><p>16 In the latent SVM based attribute approaches, attributes are treated as latent variables for both model training and testing. [sent-94, score-1.215]
</p><p>17 Also, the attribute relations [34] in the latent SVM attribute approach are either manually specified, or pre-learned through a network purely consisting of attribute nodes and then interpolated into latent SVM. [sent-95, score-2.427]
</p><p>18 By contrast, in our unified model, the semantic attributes are not latent during training of the unified model, and thus our model explicitly captures the semantic meanings of the attributes which can generalize to other classes for the zeroshot learning. [sent-96, score-1.449]
</p><p>19 Moreover, our unified model automatically discovers and captures the attribute relationships in a systematic manner, considering both the object-dependent and object-independent attribute relations. [sent-97, score-1.897]
</p><p>20 [2] apply a CRF model to improve attribute predictions for the clothing appearance describing tasks. [sent-99, score-0.87]
</p><p>21 This CRF model consists of only attribute nodes with corresponding attribute observations. [sent-100, score-1.591]
</p><p>22 It incorporates the attribute relations by specifying a fully connected network connecting all its attribute nodes. [sent-101, score-1.548]
</p><p>23 Our model can then improve the tasks of both attribute classification and object recognition either in standard or zero-shot learning scenarios. [sent-104, score-0.851]
</p><p>24 Moreover, instead of specifying a fully connected graphical model, we try to discover necessary statistical relationships between the attributes nodes and object class nodes using the incorporated structure and parameter learning. [sent-105, score-0.899]
</p><p>25 However, these two approaches do NOT incorporate attribute relationships in their models. [sent-109, score-0.894]
</p><p>26 Another probabilistic attribute approach is the direct attribute prediction (DAP) and indirect attribute prediction (IAP) models proposed by Lampert et al. [sent-112, score-2.448]
</p><p>27 The DAP and IAP models assume the attribute vector can be induced deterministically given the class label, but our model assumes that given the class label, the related attributes still obey certain probabilistic distributions. [sent-115, score-1.165]
</p><p>28 Also, our unified model discovers and captures the object-dependent and object-independent attribute relationships, but DAP and IAP models do not capture such relationships. [sent-116, score-1.023]
</p><p>29 Approach In Section 3, we describe in details about the unified model we propose to capture the relationships between attributes and objects for attribute prediction and object recognition. [sent-118, score-1.72]
</p><p>30 , AM as the M ground truth attributes, and X as the raw feature shared by attribute classifiers. [sent-122, score-0.754]
</p><p>31 Modeling Relationships The object-dependent attribute relationships are the relationships resulted from specific properties of an object. [sent-125, score-1.057]
</p><p>32 Comparatively, the object-independent attribute relationships capture intrinsic properties among all or many objects. [sent-126, score-0.914]
</p><p>33 We believe attribute relationships should consist of both types. [sent-127, score-0.894]
</p><p>34 To automatically differentiate and capture these relationships, we construct a Bayesian network (BN) based unified model consisting of both object label node Y and attribute nodes A1, A2 , . [sent-128, score-1.34]
</p><p>35 In our unified model as shown in Figure 2, attribute nodes connecting to each other without connecting to object node capture object-independent relationships, while attribute nodes connecting to each other via object node capture object-dependent relationships. [sent-153, score-2.306]
</p><p>36 This advantage of BN enables us to discover the attribute relationships systematically in the model learning phase. [sent-156, score-0.981]
</p><p>37 To avoid a complex BN with too many parameters, we limit the number of parental nodes for each attribute, based on the assumption that each attribute is only closely related to a few (N) attributes. [sent-174, score-0.836]
</p><p>38 the object label state representing “sheep” with the attribute “black” to be true), and the MLE learning of such parameter would be affected by the limited training samples for the certain state parentchild combination. [sent-182, score-0.907]
</p><p>39 The independent attribute classifiers pre-trained on the training images are used first to obtain the attribute measurements OA1, OA2 , . [sent-200, score-1.611]
</p><p>40 And then, these attribute measurements are used by the unified model for inferring the attributes and object classes. [sent-204, score-1.486]
</p><p>41 To incorporate these attribute measurements into the unified model for testing, we further associate the attribute nodes A1, A2 , . [sent-205, score-1.9]
</p><p>42 Figure 3 shows an example of the unified model incorporated with attribute measurements. [sent-209, score-1.004]
</p><p>43 In Figure 3, the object label node Y and attribute nodes A1, A2 , . [sent-210, score-1.0]
</p><p>44 , AM are indicated with white circles, and the attribute measurement  nodes OA1, OA2, . [sent-213, score-0.919]
</p><p>45 The links between the attribute nodes and the attribute measurement nodes model the measurement uncertainty of the independent attribute classifiers. [sent-217, score-2.537]
</p><p>46 From the BN model point of view, the measurement nodes are regarded as observed nodes, which provide evidence in the inference procedure, and the object label and attribute nodes are ground truth nodes, whose states need to be inferred from the BN model given the evidence. [sent-218, score-1.129]
</p><p>47 The factorized form of the joint probability for the unified model incorporated with attribute measurements is: P(Y, A1, . [sent-219, score-1.086]
</p><p>48 =  (5)  1  where pa(Y ) stands for the parent node(s) of object label node Y , and pa(Am) stands for the parent node(s) of attribute node Am. [sent-231, score-1.092]
</p><p>49 n Tdietrimonsa Pl dependencies among object label node and attribute nodes, which capture the objectdependent and object-independent attribute relationships, and the term P(OAm |Am) represents the attribute measurement uncertainty te|Arms. [sent-233, score-2.446]
</p><p>50 In practice, we use discrete  for attribute predictions and object recognition. [sent-234, score-0.84]
</p><p>51 The object label node Y and attribute nodes A1, A2 , . [sent-235, score-1.0]
</p><p>52 With the unified model incorporated with attribute measurements shown in Figure 3, we can infer the probabilities of different attributes and object classes given the attribute measurements obtained from independent attribute classifiers. [sent-244, score-3.159]
</p><p>53 For object recognition, given a testing image X with its attribute measurements OA1, OA2 , . [sent-245, score-0.903]
</p><p>54 , OAM, we infer the marginal probability of Y given the attribute measurements. [sent-248, score-0.751]
</p><p>55 )  (6)  Also, for attribute prediction, we infer the marginal probability of Am with m = 1, 2, . [sent-253, score-0.751]
</p><p>56 The attribute prediction am is: am = argmraxP(Am = r|OA1, . [sent-257, score-0.841]
</p><p>57 In addition, the attribute states and the object state can also be inferred jointly with the most probable explanation (MPE) [21] of the evidences. [sent-262, score-0.833]
</p><p>58 However, with our unified model, we still want to discover and capture the statistical relationships between attributes and new objects even without raw images during training. [sent-268, score-0.868]
</p><p>59 On the other hand, the independent attribute classifiers are learned with raw images from observed classes. [sent-271, score-0.817]
</p><p>60 During testing, the independent attribute classifiers are applied to raw images of new classes to obtain attribute measurements, which are further applied to unified model for inference. [sent-272, score-1.865]
</p><p>61 We still use the inferences discussed in Equation 7 and 6 for attribute predictions in new class examples and the classification of new classes respectively. [sent-273, score-0.902]
</p><p>62 A support vector machine (SVM) based attribute classifier trained on the training set of a-Pascal is provided in [6] to predict the 64 attributes with the given features. [sent-281, score-1.131]
</p><p>63 The pre-calculated image features and attribute classifiers are provided in [27]. [sent-288, score-0.773]
</p><p>64 This dataset also provides pre-calculated feature vector and the baseline attribute classifiers. [sent-293, score-0.753]
</p><p>65 Here, we first test our unified models for attribute prediction both with observed classes on a-Pascal dataset [6], and with new classes on a-Yahoo dataset [6] in Section 4. [sent-310, score-1.242]
</p><p>66 1  Attribute Prediction on a-Pascal and a-Yahoo  In this experiment, we use the proposed unified model for attribute prediction of observed class images on a-Pascal [6] and new class images on a-Yahoo respectively with the inference discussed in Equation 7. [sent-318, score-1.092]
</p><p>67 We use the provided attribute classifiers [6] trained on the a-Pascal training set as “Baseline” for attribute prediction, and these baseline attribute classifiers also provide attribute measurements for our unified model to predict attributes. [sent-319, score-3.388]
</p><p>68 Figure 5 and 6 give the per-attribute G-Mean comparison between the Baseline and the proposed unified model for attribute prediction of observed classes on a-Pascal and new classes on a-Yahoo respectively. [sent-322, score-1.266]
</p><p>69 From Figure 5, we can see the proposed unified model  can improve the attribute prediction over the attribute measurements for most of the attributes on a-Pascal dataset. [sent-323, score-2.28]
</p><p>70 Also, in Figure 6, the improvements for attribute prediction in images of new classes on a-Yahoo dataset are even greater than that in a-Pascal. [sent-328, score-0.928]
</p><p>71 We believe the captured object-dependent and object-independent attribute relationships in a-Yahoo classes significantly benefit the attribute prediction for these new class images, even though the baseline attribute classifiers perform worse on a-Yahoo due to generalization issues. [sent-329, score-2.639]
</p><p>72 We further compare the baseline independent attribute classifier and the proposed unified model on the average Gmean over all 64 attributes in Table 1. [sent-330, score-1.379]
</p><p>73 To verify the effectiveness of the distinction between object-(in)dependent attribute relations, we compare with a BN model consisting of attribute nodes but omitting the object node (BN-Att model). [sent-331, score-1.756]
</p><p>74 Also, the CRF in [2] using a fully connected network of only attribute nodes to relate attributes is also compared here. [sent-333, score-1.283]
</p><p>75 From this comparison, we can see that incorporating the traditional attribute relationships alone by the BN-Att or CRF model can already improve the baseline performance. [sent-334, score-0.94]
</p><p>76 Moreover, with our proposed model that differentiates the object-dependent and object-independent attribute relationships, the overall attribute prediction accuracy can be significantly improved. [sent-335, score-1.621]
</p><p>77 2  Attribute Prediction on SUN Attribute Dataset  To study model performance for complex cases that involve large attribute set, we also perform attribute prediction on the SUN Attribute dataset. [sent-351, score-1.596]
</p><p>78 release the latest attribute prediction results in [27] with the new training and testing splits. [sent-355, score-0.909]
</p><p>79 In [27], the average precision (AP) number is used for evaluating attribute prediction accuracy. [sent-356, score-0.841]
</p><p>80 For attribute prediction, our model improves the overall result from 50. [sent-360, score-0.78]
</p><p>81 Both the attribute classifiers and the unified model are learned on the a-Pascal training set. [sent-380, score-1.07]
</p><p>82 Here, we further compare with the per-object BN network where a BN model consisting ofonly attribute nodes is learned for each object class. [sent-406, score-0.997]
</p><p>83 The attribute classifiers are trained on a-Pascal training set and provide attribute measurements on a-Yahoo images as input for unified model. [sent-417, score-1.838]
</p><p>84 During testing, SVM uses interpreted attributes predicted both from the attribute classifier (i. [sent-419, score-1.126]
</p><p>85 original attribute predictions), and from the unified model (i. [sent-421, score-0.982]
</p><p>86 improved attribute predictions by our unified model in Section 4. [sent-423, score-1.044]
</p><p>87 0315%%  SVMa: testing on original attribute predictions; SVMb: testing proved attribute predictions by our unified model. [sent-436, score-1.837]
</p><p>88 on  im-  Table 3 gives the overall evaluations for SVM testing on original attribute predictions as well as testing on attribute predictions by our unified model. [sent-437, score-1.924]
</p><p>89 We can see the improved attribute predictions can also significantly benefit the object recognition on new classes. [sent-438, score-0.885]
</p><p>90 [17] propose two attribute based zero-shot learning models DAP and IAP. [sent-445, score-0.757]
</p><p>91 We test on the same 10 animal classes as defined in [17], and use the provided baseline attribute classifiers trained on the images of the rest 40 animals to obtain attribute measurement input. [sent-447, score-1.715]
</p><p>92 In this comparison, our unified model can outperform both IAP and DAP models for attribute based zero-shot learning. [sent-449, score-0.982]
</p><p>93 Conclusion In this paper, we propose a unified probabilistic model to capture the relationships between attributes and objects for attribute prediction and object recognition. [sent-468, score-1.755]
</p><p>94 The paper proposes a unified probabilistic model to automatically discover and capture both the object-dependent and object-independent attribute relationships. [sent-470, score-1.074]
</p><p>95 During testing, the unified model utilizes these captured relationships to benefit both attribute prediction and object recognition with probabilistic inference given the attribute measurements predicted individually by the pre-learned attribute classifiers. [sent-471, score-3.006]
</p><p>96 We experiment on four benchmark attribute datasets including a-Pascal, aYahoo, SUN Attribute and AWA for attribute prediction and object recognition tasks. [sent-472, score-1.642]
</p><p>97 The experiment results with the proposed unified model show significant improvements for attribute prediction as well as object recognition, especially in cases of new objects. [sent-473, score-1.139]
</p><p>98 Learning to detect unseen object  [18]  [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30]  [3 1] [32] [33] [34] [35] [36]  classes by between-class attribute transfer. [sent-587, score-0.865]
</p><p>99 A joint learning framework for attribute models and object descriptions. [sent-605, score-0.804]
</p><p>100 Sun attribute database: Discovering, annotating, and recognizing scene attributes. [sent-657, score-0.759]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('attribute', 0.731), ('attributes', 0.375), ('unified', 0.227), ('relationships', 0.163), ('bn', 0.151), ('iap', 0.125), ('oam', 0.124), ('prediction', 0.11), ('nodes', 0.105), ('dap', 0.103), ('awa', 0.096), ('node', 0.093), ('classes', 0.087), ('measurements', 0.082), ('gs', 0.08), ('pa', 0.065), ('predictions', 0.062), ('patterson', 0.051), ('farhadi', 0.048), ('exhaust', 0.048), ('object', 0.047), ('meaonv', 0.047), ('sail', 0.047), ('crf', 0.047), ('network', 0.044), ('testing', 0.043), ('classifiers', 0.042), ('measurement', 0.041), ('discover', 0.037), ('comparatively', 0.036), ('probabilistic', 0.035), ('utilizes', 0.035), ('discovering', 0.034), ('bicycle', 0.033), ('animals', 0.031), ('nijk', 0.031), ('xi', 0.031), ('wheel', 0.031), ('stripe', 0.031), ('parent', 0.031), ('animal', 0.03), ('latent', 0.03), ('semantic', 0.029), ('describing', 0.029), ('ijk', 0.029), ('sun', 0.029), ('states', 0.028), ('recognizing', 0.028), ('links', 0.028), ('relate', 0.028), ('ayahoo', 0.028), ('objectdependent', 0.028), ('objectindependent', 0.028), ('rki', 0.028), ('svm', 0.027), ('base', 0.027), ('state', 0.027), ('learning', 0.026), ('shaded', 0.026), ('axp', 0.026), ('individually', 0.025), ('overall', 0.025), ('consisting', 0.025), ('training', 0.025), ('exclusive', 0.025), ('parikh', 0.024), ('structure', 0.024), ('clothing', 0.024), ('model', 0.024), ('label', 0.024), ('recognition', 0.023), ('mle', 0.023), ('objects', 0.023), ('raw', 0.023), ('benefit', 0.022), ('inferences', 0.022), ('dag', 0.022), ('lampert', 0.022), ('incorporated', 0.022), ('baseline', 0.022), ('indicated', 0.021), ('white', 0.021), ('capturing', 0.021), ('stands', 0.021), ('scheirer', 0.021), ('zebra', 0.021), ('learned', 0.021), ('specifying', 0.021), ('connecting', 0.021), ('saddle', 0.021), ('captures', 0.021), ('generally', 0.02), ('capture', 0.02), ('mutually', 0.02), ('ject', 0.02), ('wing', 0.02), ('predicted', 0.02), ('car', 0.02), ('infer', 0.02), ('equation', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="31-tfidf-1" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>Author: Xiaoyang Wang, Qiang Ji</p><p>Abstract: This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. As a list of semantically meaningful properties of objects, attributes generally relate to each other statistically. In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and objectindependent attribute relationships. The model utilizes the captured relationships to benefit both attribute prediction and object recognition. Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases.</p><p>2 0.50604153 <a title="31-tfidf-2" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>Author: Adriana Kovashka, Kristen Grauman</p><p>Abstract: Current methods learn monolithic attribute predictors, with the assumption that a single model is sufficient to reflect human understanding of a visual attribute. However, in reality, humans vary in how they perceive the association between a named property and image content. For example, two people may have slightly different internal models for what makes a shoe look “formal”, or they may disagree on which of two scenes looks “more cluttered”. Rather than discount these differences as noise, we propose to learn user-specific attribute models. We adapt a generic model trained with annotations from multiple users, tailoring it to satisfy user-specific labels. Furthermore, we propose novel techniques to infer user-specific labels based on transitivity and contradictions in the user’s search history. We demonstrate that adapted attributes improve accuracy over both existing monolithic models as well as models that learn from scratch with user-specific data alone. In addition, we show how adapted attributes are useful to personalize image search, whether with binary or relative attributes.</p><p>3 0.43619028 <a title="31-tfidf-3" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Devi Parikh, Tsuhan Chen</p><p>Abstract: In recent years, there has been a great deal of progress in describing objects with attributes. Attributes have proven useful for object recognition, image search, face verification, image description, and zero-shot learning. Typically, attributes are either binary or relative: they describe either the presence or absence of a descriptive characteristic, or the relative magnitude of the characteristic when comparing two exemplars. However, prior work fails to model the actual way in which humans use these attributes in descriptive statements of images. Specifically, it does not address the important interactions between the binary and relative aspects of an attribute. In this work we propose a spoken attribute classifier which models a more natural way of using an attribute in a description. For each attribute we train a classifier which captures the specific way this attribute should be used. We show that as a result of using this model, we produce descriptions about images of people that are more natural and specific than past systems.</p><p>4 0.42045537 <a title="31-tfidf-4" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>Author: Jungseock Joo, Shuo Wang, Song-Chun Zhu</p><p>Abstract: We present a part-based approach to the problem of human attribute recognition from a single image of a human body. To recognize the attributes of human from the body parts, it is important to reliably detect the parts. This is a challenging task due to the geometric variation such as articulation and view-point changes as well as the appearance variation of the parts arisen from versatile clothing types. The prior works have primarily focused on handling . edu . cn ???????????? geometric variation by relying on pre-trained part detectors or pose estimators, which require manual part annotation, but the appearance variation has been relatively neglected in these works. This paper explores the importance of the appearance variation, which is directly related to the main task, attribute recognition. To this end, we propose to learn a rich appearance part dictionary of human with significantly less supervision by decomposing image lattice into overlapping windows at multiscale and iteratively refining local appearance templates. We also present quantitative results in which our proposed method outperforms the existing approaches.</p><p>5 0.39834744 <a title="31-tfidf-5" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>Author: Naman Turakhia, Devi Parikh</p><p>Abstract: When we look at an image, some properties or attributes of the image stand out more than others. When describing an image, people are likely to describe these dominant attributes first. Attribute dominance is a result of a complex interplay between the various properties present or absent in the image. Which attributes in an image are more dominant than others reveals rich information about the content of the image. In this paper we tap into this information by modeling attribute dominance. We show that this helps improve the performance of vision systems on a variety of human-centric applications such as zero-shot learning, image search and generating textual descriptions of images.</p><p>6 0.37272897 <a title="31-tfidf-6" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>7 0.35674411 <a title="31-tfidf-7" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>8 0.29634741 <a title="31-tfidf-8" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>9 0.26379973 <a title="31-tfidf-9" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>10 0.21702504 <a title="31-tfidf-10" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>11 0.20682156 <a title="31-tfidf-11" href="./iccv-2013-Learning_Graph_Matching%3A_Oriented_to_Category_Modeling_from_Cluttered_Scenes.html">237 iccv-2013-Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes</a></p>
<p>12 0.20651241 <a title="31-tfidf-12" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>13 0.20004779 <a title="31-tfidf-13" href="./iccv-2013-NEIL%3A_Extracting_Visual_Knowledge_from_Web_Data.html">285 iccv-2013-NEIL: Extracting Visual Knowledge from Web Data</a></p>
<p>14 0.19617456 <a title="31-tfidf-14" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>15 0.18779673 <a title="31-tfidf-15" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>16 0.17677322 <a title="31-tfidf-16" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<p>17 0.16906191 <a title="31-tfidf-17" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>18 0.15227161 <a title="31-tfidf-18" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>19 0.13543576 <a title="31-tfidf-19" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>20 0.11933647 <a title="31-tfidf-20" href="./iccv-2013-Implied_Feedback%3A_Learning_Nuances_of_User_Behavior_in_Image_Search.html">213 iccv-2013-Implied Feedback: Learning Nuances of User Behavior in Image Search</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.191), (1, 0.259), (2, -0.082), (3, -0.233), (4, 0.252), (5, -0.09), (6, -0.168), (7, -0.274), (8, 0.38), (9, 0.234), (10, -0.125), (11, 0.068), (12, 0.042), (13, 0.034), (14, -0.076), (15, 0.02), (16, 0.02), (17, 0.047), (18, -0.052), (19, 0.016), (20, -0.028), (21, 0.112), (22, 0.039), (23, -0.037), (24, -0.078), (25, -0.06), (26, 0.014), (27, -0.025), (28, -0.013), (29, 0.076), (30, -0.024), (31, 0.057), (32, -0.037), (33, 0.008), (34, -0.017), (35, -0.021), (36, 0.022), (37, -0.038), (38, 0.012), (39, -0.033), (40, 0.033), (41, 0.021), (42, -0.003), (43, 0.027), (44, -0.018), (45, 0.066), (46, 0.039), (47, 0.057), (48, 0.019), (49, -0.101)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98306376 <a title="31-lsi-1" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>Author: Xiaoyang Wang, Qiang Ji</p><p>Abstract: This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. As a list of semantically meaningful properties of objects, attributes generally relate to each other statistically. In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and objectindependent attribute relationships. The model utilizes the captured relationships to benefit both attribute prediction and object recognition. Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases.</p><p>2 0.95666784 <a title="31-lsi-2" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>Author: Naman Turakhia, Devi Parikh</p><p>Abstract: When we look at an image, some properties or attributes of the image stand out more than others. When describing an image, people are likely to describe these dominant attributes first. Attribute dominance is a result of a complex interplay between the various properties present or absent in the image. Which attributes in an image are more dominant than others reveals rich information about the content of the image. In this paper we tap into this information by modeling attribute dominance. We show that this helps improve the performance of vision systems on a variety of human-centric applications such as zero-shot learning, image search and generating textual descriptions of images.</p><p>3 0.95576501 <a title="31-lsi-3" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Devi Parikh, Tsuhan Chen</p><p>Abstract: In recent years, there has been a great deal of progress in describing objects with attributes. Attributes have proven useful for object recognition, image search, face verification, image description, and zero-shot learning. Typically, attributes are either binary or relative: they describe either the presence or absence of a descriptive characteristic, or the relative magnitude of the characteristic when comparing two exemplars. However, prior work fails to model the actual way in which humans use these attributes in descriptive statements of images. Specifically, it does not address the important interactions between the binary and relative aspects of an attribute. In this work we propose a spoken attribute classifier which models a more natural way of using an attribute in a description. For each attribute we train a classifier which captures the specific way this attribute should be used. We show that as a result of using this model, we produce descriptions about images of people that are more natural and specific than past systems.</p><p>4 0.87774265 <a title="31-lsi-4" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>Author: Adriana Kovashka, Kristen Grauman</p><p>Abstract: Current methods learn monolithic attribute predictors, with the assumption that a single model is sufficient to reflect human understanding of a visual attribute. However, in reality, humans vary in how they perceive the association between a named property and image content. For example, two people may have slightly different internal models for what makes a shoe look “formal”, or they may disagree on which of two scenes looks “more cluttered”. Rather than discount these differences as noise, we propose to learn user-specific attribute models. We adapt a generic model trained with annotations from multiple users, tailoring it to satisfy user-specific labels. Furthermore, we propose novel techniques to infer user-specific labels based on transitivity and contradictions in the user’s search history. We demonstrate that adapted attributes improve accuracy over both existing monolithic models as well as models that learn from scratch with user-specific data alone. In addition, we show how adapted attributes are useful to personalize image search, whether with binary or relative attributes.</p><p>5 0.8316561 <a title="31-lsi-5" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>Author: Sukrit Shankar, Joan Lasenby, Roberto Cipolla</p><p>Abstract: Relative (comparative) attributes are promising for thematic ranking of visual entities, which also aids in recognition tasks [19, 23]. However, attribute rank learning often requires a substantial amount of relational supervision, which is highly tedious, and apparently impracticalfor realworld applications. In this paper, we introduce the Semantic Transform, which under minimal supervision, adaptively finds a semantic feature space along with a class ordering that is related in the best possible way. Such a semantic space is found for every attribute category. To relate the classes under weak supervision, the class ordering needs to be refined according to a cost function in an iterative procedure. This problem is ideally NP-hard, and we thus propose a constrained search tree formulation for the same. Driven by the adaptive semantic feature space representation, our model achieves the best results to date for all of the tasks of relative, absolute and zero-shot classification on two popular datasets.</p><p>6 0.80335087 <a title="31-lsi-6" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>7 0.73980558 <a title="31-lsi-7" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>8 0.68362439 <a title="31-lsi-8" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>9 0.63536358 <a title="31-lsi-9" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>10 0.58828944 <a title="31-lsi-10" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<p>11 0.5816952 <a title="31-lsi-11" href="./iccv-2013-Relative_Attributes_for_Large-Scale_Abandoned_Object_Detection.html">350 iccv-2013-Relative Attributes for Large-Scale Abandoned Object Detection</a></p>
<p>12 0.53274089 <a title="31-lsi-12" href="./iccv-2013-NEIL%3A_Extracting_Visual_Knowledge_from_Web_Data.html">285 iccv-2013-NEIL: Extracting Visual Knowledge from Web Data</a></p>
<p>13 0.52542132 <a title="31-lsi-13" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>14 0.4524965 <a title="31-lsi-14" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>15 0.44524878 <a title="31-lsi-15" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>16 0.42960921 <a title="31-lsi-16" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>17 0.42059296 <a title="31-lsi-17" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>18 0.41795924 <a title="31-lsi-18" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>19 0.40966636 <a title="31-lsi-19" href="./iccv-2013-Implied_Feedback%3A_Learning_Nuances_of_User_Behavior_in_Image_Search.html">213 iccv-2013-Implied Feedback: Learning Nuances of User Behavior in Image Search</a></p>
<p>20 0.3875114 <a title="31-lsi-20" href="./iccv-2013-Learning_Graph_Matching%3A_Oriented_to_Category_Modeling_from_Cluttered_Scenes.html">237 iccv-2013-Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.087), (7, 0.016), (12, 0.021), (17, 0.097), (26, 0.086), (31, 0.048), (34, 0.122), (42, 0.092), (48, 0.02), (55, 0.014), (64, 0.075), (73, 0.022), (77, 0.012), (78, 0.031), (89, 0.142)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8891871 <a title="31-lda-1" href="./iccv-2013-How_Do_You_Tell_a_Blackbird_from_a_Crow%3F.html">202 iccv-2013-How Do You Tell a Blackbird from a Crow?</a></p>
<p>Author: Thomas Berg, Peter N. Belhumeur</p><p>Abstract: How do you tell a blackbirdfrom a crow? There has been great progress toward automatic methods for visual recognition, including fine-grained visual categorization in which the classes to be distinguished are very similar. In a task such as bird species recognition, automatic recognition systems can now exceed the performance of non-experts – most people are challenged to name a couple dozen bird species, let alone identify them. This leads us to the question, “Can a recognition system show humans what to look for when identifying classes (in this case birds)? ” In the context of fine-grained visual categorization, we show that we can automatically determine which classes are most visually similar, discover what visual features distinguish very similar classes, and illustrate the key features in a way meaningful to humans. Running these methods on a dataset of bird images, we can generate a visual field guide to birds which includes a tree of similarity that displays the similarity relations between all species, pages for each species showing the most similar other species, and pages for each pair of similar species illustrating their differences.</p><p>same-paper 2 0.86520278 <a title="31-lda-2" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>Author: Xiaoyang Wang, Qiang Ji</p><p>Abstract: This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. As a list of semantically meaningful properties of objects, attributes generally relate to each other statistically. In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and objectindependent attribute relationships. The model utilizes the captured relationships to benefit both attribute prediction and object recognition. Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases.</p><p>3 0.85722828 <a title="31-lda-3" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>Author: Naman Turakhia, Devi Parikh</p><p>Abstract: When we look at an image, some properties or attributes of the image stand out more than others. When describing an image, people are likely to describe these dominant attributes first. Attribute dominance is a result of a complex interplay between the various properties present or absent in the image. Which attributes in an image are more dominant than others reveals rich information about the content of the image. In this paper we tap into this information by modeling attribute dominance. We show that this helps improve the performance of vision systems on a variety of human-centric applications such as zero-shot learning, image search and generating textual descriptions of images.</p><p>4 0.84910035 <a title="31-lda-4" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>Author: Aleksandr V. Segal, Ian Reid</p><p>Abstract: We propose a novel parametrization of the data association problem for multi-target tracking. In our formulation, the number of targets is implicitly inferred together with the data association, effectively solving data association and model selection as a single inference problem. The novel formulation allows us to interpret data association and tracking as a single Switching Linear Dynamical System (SLDS). We compute an approximate posterior solution to this problem using a dynamic programming/message passing technique. This inference-based approach allows us to incorporate richer probabilistic models into the tracking system. In particular, we incorporate inference over inliers/outliers and track termination times into the system. We evaluate our approach on publicly available datasets and demonstrate results competitive with, and in some cases exceeding the state of the art.</p><p>5 0.83591259 <a title="31-lda-5" href="./iccv-2013-Multi-scale_Topological_Features_for_Hand_Posture_Representation_and_Analysis.html">278 iccv-2013-Multi-scale Topological Features for Hand Posture Representation and Analysis</a></p>
<p>Author: Kaoning Hu, Lijun Yin</p><p>Abstract: In this paper, we propose a multi-scale topological feature representation for automatic analysis of hand posture. Such topological features have the advantage of being posture-dependent while being preserved under certain variations of illumination, rotation, personal dependency, etc. Our method studies the topology of the holes between the hand region and its convex hull. Inspired by the principle of Persistent Homology, which is the theory of computational topology for topological feature analysis over multiple scales, we construct the multi-scale Betti Numbers matrix (MSBNM) for the topological feature representation. In our experiments, we used 12 different hand postures and compared our features with three popular features (HOG, MCT, and Shape Context) on different data sets. In addition to hand postures, we also extend the feature representations to arm postures. The results demonstrate the feasibility and reliability of the proposed method.</p><p>6 0.8183918 <a title="31-lda-6" href="./iccv-2013-Box_in_the_Box%3A_Joint_3D_Layout_and_Object_Reasoning_from_Single_Images.html">64 iccv-2013-Box in the Box: Joint 3D Layout and Object Reasoning from Single Images</a></p>
<p>7 0.8160302 <a title="31-lda-7" href="./iccv-2013-Flattening_Supervoxel_Hierarchies_by_the_Uniform_Entropy_Slice.html">172 iccv-2013-Flattening Supervoxel Hierarchies by the Uniform Entropy Slice</a></p>
<p>8 0.81407535 <a title="31-lda-8" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>9 0.81245422 <a title="31-lda-9" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>10 0.81153297 <a title="31-lda-10" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>11 0.81152821 <a title="31-lda-11" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>12 0.80641747 <a title="31-lda-12" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>13 0.8025924 <a title="31-lda-13" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>14 0.8010692 <a title="31-lda-14" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>15 0.80032003 <a title="31-lda-15" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>16 0.79866612 <a title="31-lda-16" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>17 0.79783052 <a title="31-lda-17" href="./iccv-2013-Linear_Sequence_Discriminant_Analysis%3A_A_Model-Based_Dimensionality_Reduction_Method_for_Vector_Sequences.html">253 iccv-2013-Linear Sequence Discriminant Analysis: A Model-Based Dimensionality Reduction Method for Vector Sequences</a></p>
<p>18 0.79332089 <a title="31-lda-18" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>19 0.79030252 <a title="31-lda-19" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>20 0.79017931 <a title="31-lda-20" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
