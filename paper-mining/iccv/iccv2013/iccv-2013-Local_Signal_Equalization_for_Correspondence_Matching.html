<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>255 iccv-2013-Local Signal Equalization for Correspondence Matching</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-255" href="#">iccv2013-255</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>255 iccv-2013-Local Signal Equalization for Correspondence Matching</h1>
<br/><p>Source: <a title="iccv-2013-255-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Bradley_Local_Signal_Equalization_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Derek Bradley, Thabo Beeler</p><p>Abstract: Correspondence matching is one of the most common problems in computer vision, and it is often solved using photo-consistency of local regions. These approaches typically assume that the frequency content in the local region is consistent in the image pair, such that matching is performed on similar signals. However, in many practical situations this is not the case, for example with low depth of field cameras a scene point may be out of focus in one view and in-focus in the other, causing a mismatch of frequency signals. Furthermore, this mismatch can vary spatially over the entire image. In this paper we propose a local signal equalization approach for correspondence matching. Using a measure of local image frequency, we equalize local signals using an efficient scale-space image representation such that their frequency contents are optimally suited for matching. Our approach allows better correspondence matching, which we demonstrate with a number of stereo reconstruction examples on synthetic and real datasets.</p><p>Reference: <a title="iccv-2013-255-reference" href="../iccv2013_reference/iccv-2013-Local_Signal_Equalization_for_Correspondence_Matching_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 These approaches typically assume that the frequency content in the local region is consistent in the image pair, such that matching is performed on similar signals. [sent-2, score-0.649]
</p><p>2 However, in many practical situations this is not the case, for example with low depth of field cameras a scene point may be out of focus in one view and in-focus in the other, causing a mismatch of frequency signals. [sent-3, score-0.618]
</p><p>3 In this paper we propose a local signal equalization approach for correspondence matching. [sent-5, score-0.968]
</p><p>4 Using a measure of local image frequency, we equalize local signals using an efficient scale-space image representation such that their frequency contents are optimally suited for matching. [sent-6, score-0.998]
</p><p>5 Our approach allows better correspondence matching, which we demonstrate with a number of stereo reconstruction examples on synthetic and real datasets. [sent-7, score-0.524]
</p><p>6 Correspondence matching is key to many applications including stereo and multi-view reconstruction, optical flow, template matching, image registration and video stabilization (to name just a few). [sent-10, score-0.43]
</p><p>7 In this work we focus on patch-based correspondence matching, often used for dense stereo reconstruction. [sent-11, score-0.444]
</p><p>8 The drawback of NCC is that it assumes the input images contain the same signal content, either all images are true representations of the original scene signal or all are equally distorted through defocus, motion blur, under-sampling, etc. [sent-14, score-0.578]
</p><p>9 However, in many practical situations we have to compute correspondence of mixed-signals, caused by different amounts of defocus blur for example. [sent-15, score-0.472]
</p><p>10 Furthermore, the signal degradation will vary spatially in most practical situations causing different mixed-signals for every pixel in an image. [sent-16, score-0.509]
</p><p>11 Thabo Beeler Disney Research Zurich In this paper, we take a pre-processing approach to solve the signal mismatch problem. [sent-17, score-0.355]
</p><p>12 Rather than defining new measures and devising new algorithms for matching mixed-  signals, we believe it is more advantageous to modify the input signals in an optimal way to enable the wide range of existing algorithms which assume the signals are already equalized. [sent-18, score-0.601]
</p><p>13 To this end, we propose to locally equalize the image signals before computing correspondence matches. [sent-19, score-0.721]
</p><p>14 Given two images patches, our technique involves a frequency equalization step in order to generate patches with similar spatial signals, and an optimal frequency scaling step that ensures the patches can be matched reliably. [sent-20, score-1.471]
</p><p>15 This is achieved using a notion of local image frequency, which quantifies the highest spatial frequency in a local region, and an efficient implementation of scale-space that builds multi-resolution image patches on demand. [sent-21, score-0.598]
</p><p>16 We demonstrate the power of the proposed signal equalization technique by improving the performance of a na¨ ıve window matching algorithm by an order of magnitude on several examples of dense stereo reconstruction, using a synthetic data set with ground truth, and additional realworld examples. [sent-23, score-1.325]
</p><p>17 Related Work Correspondence matching is very common in several domains, including stereo reconstruction [20], multi-view reconstruction [21], optical flow [1], and many more. [sent-27, score-0.536]
</p><p>18 In this work we address the general issue of inconsistent spatial frequency signals between multiple images when performing correspondence estimation. [sent-28, score-0.846]
</p><p>19 While there exist many patch-based correlation measures [10] for stereo, they typically assume that frequency signals are consistent in the images before matching. [sent-29, score-0.655]
</p><p>20 We present a technique for establishing consistent frequency signals automatically. [sent-30, score-0.671]
</p><p>21 While some approaches have been developed for recovering depth from defocus information [6, 23, 12], we have a different goal, which is to improve correspondence matching between multiple images containing different signals. [sent-32, score-0.475]
</p><p>22 However, these approaches do not solve the ”mixed” signal problem, and in contrast we aim to explicitly locally equalize the signal of both images before computing correspondences. [sent-36, score-0.883]
</p><p>23 Signal analysis in stereo reconstruction is not an entirely new concept. [sent-37, score-0.309]
</p><p>24 [14] analyze signal content in patch-based stereo systems, showing that the amplitude of high-frequency details is diminished in reconstructions, and they derive a modulation transfer function in frequency space that can be inverted to restore details. [sent-39, score-1.103]
</p><p>25 Following this theory, they show how Gaussian weighting of stereo patch correspondences links reconstruction to a scale-space representation of the underlying surface [15]. [sent-40, score-0.351]
</p><p>26 Our method is complementary to this work, since our signal equalization approach can be applied before any type of descriptor matching or learning techniques. [sent-44, score-0.886]
</p><p>27 We aim to equalize two signals G1(ω) and G2 (ω), which are degraded versions of the same signal Gˆ(ω), such that their frequency contents are both compatible and optimally suited for matching with respect to a given matching function Ψ. [sent-47, score-1.625]
</p><p>28 We first discuss the equalization of two different signals, and then a theory of frequency scaling to produce optimal conditions for correspondence matching. [sent-53, score-1.09]
</p><p>29 Signal Equalization - Two degraded versions (G1, G2) of the same signal can be equalized. [sent-55, score-0.4]
</p><p>30 Left: A threshold α is applied in frequency domain to find the highest frequencies ω˜1 and ω˜2 . [sent-56, score-0.597]
</p><p>31 Center: The lower of the two is used as cutoff frequency ˜ω for a lowpass filter Π ω˜. [sent-57, score-0.598]
</p><p>32 Inter-Signal Equalization In the frequency domain, the degradation is modeled as G(ω) = D(ω)Gˆ(ω) + μ(ω), (2) where D(ω) is the degradation function and μ(ω) is the noise model. [sent-61, score-0.602]
</p><p>33 Since the degradation we are primarily interested in is spatial blur due to de-focus, which corresponds to low-pass filtering Gˆ, we model D as box filter Π ω˜, where ω˜ is the cutoff frequency. [sent-62, score-0.335]
</p><p>34 The cutoff frequency is set to be the highest frequency present in the spectrum. [sent-63, score-0.984]
</p><p>35 To be robust with respect to noise, we employ a threshold α to find the highest frequencies ω˜1 and ω˜2 of the two signals G1 and G2, respectively. [sent-64, score-0.406]
</p><p>36 The lower of the two is then used as cutoff frequency for the lowpass filter ω˜ = min( ω˜1 , ˜ω 2) (Fig. [sent-65, score-0.598]
</p><p>37 Multiplying both signals in frequency domain by Π ω˜ will render them compatible for matching (Fig. [sent-67, score-0.841]
</p><p>38 However, the frequency content is not guaranteed to be optimal for the matching function ΨNCC, which we address in the next section. [sent-69, score-0.616]
</p><p>39 Frequency Scaling We are interested in finding the frequency for which spatial localization is best. [sent-72, score-0.43]
</p><p>40 Considering only a single frequency ω this can be formalized as  ? [sent-74, score-0.398]
</p><p>41 spatial frequency ω the better the signal can be localized. [sent-96, score-0.719]
</p><p>42 Due to the discrete nature of an image, the shortest wavelength possible is λ = 2 pixels and thus the highest frequency is ωmax = 2π1λ = π. [sent-97, score-0.453]
</p><p>43 2 the fully equalized signal is given as  G˜(ω) = Π ω˜G? [sent-104, score-0.356]
</p><p>44 Equalization Implementation In this section we will describe our implementation of local signal equalization for correspondence matching. [sent-108, score-0.968]
</p><p>45 Following the theoretical considerations introduced in Section 3, the algorithm requires to identify the joint cutoff frequency ˜ω to equalize the signals and scale the signal frequencies. [sent-109, score-1.329]
</p><p>46 We then address frequency scaling using a scale-space approach, which is implemented using a locally centered pyramid structure. [sent-111, score-0.577]
</p><p>47 Finally, these tools are used to perform local signal equalization before correspondence matching, for example in stereo reconstruction. [sent-112, score-1.207]
</p><p>48 Local Frequency Maps As we discussed in Section 3, an image patch that contains high spatial frequencies matches more robustly than one with only low frequency information, and the best matching frequency is the highest possible. [sent-117, score-1.175]
</p><p>49 Motivated by this theory, we aim to compute local frequency information at each point in an image and analyze the highest spatial  frequency within the local region. [sent-118, score-0.949]
</p><p>50 The user-defined correspondence matching window of size k is also used as the region of interest for the DFT, ensuring that we compute exactly the frequencies that will be used when matching. [sent-120, score-0.533]
</p><p>51 2 illustrates the local frequency at two different points in an image. [sent-122, score-0.431]
</p><p>52 In order to determine the highest reliable frequency in each local patch we need to account for noise, by thresholding on the frequency amplitudes. [sent-128, score-0.926]
</p><p>53 We then determine the highest frequency ω˜p as ω˜p = maix(Ωp |aip > α) ,  (5)  where α characterizes the amplitude of the image noise in the frequency domain. [sent-130, score-0.882]
</p><p>54 Computing ω˜p for every pixel results in a local frequency map, which we illustrate for two images in Fig. [sent-134, score-0.459]
</p><p>55 Notice that the local frequency map is highly correlated with the image focus. [sent-137, score-0.431]
</p><p>56 This measure of local highest frequency will guide us to equalize local signals for image correspondence matching. [sent-138, score-1.185]
</p><p>57 Efficient Scale-Space Representation In order to perform frequency scaling, the local signal at an image pixel can be altered by traversing the scalespace ofthe image. [sent-141, score-0.81]
</p><p>58 Specifically, changing levels in the scale space corresponds to frequency scaling of the original signal. [sent-142, score-0.467]
</p><p>59 However, this technique only works if the signal content is consistent between images at each level. [sent-148, score-0.413]
</p><p>60 If the signal  is inconsistent on any level, then matching can become erroneous (see our experiments in Section 5). [sent-149, score-0.422]
</p><p>61 We build on the concept of centered pyramids [4] and construct a set of locally centered image pyramids that span the scale-space of the image (Fig. [sent-151, score-0.393]
</p><p>62 First, since we are only ever interested in a region-of-interest around each pixel (defined by the matching window), we do not need to compute or store  ×  the full image at each level, only a k k centered window as itlhleufs utrlalte imd aigne Fig. [sent-161, score-0.393]
</p><p>63 Using these slices of locally centered pyramids, we will next describe our signal equalization algorithm for correspondence matching. [sent-164, score-1.103]
</p><p>64 EkxqLualiztonALgcrMaliS sFit-arghm entaqcmtluh e n-cyWeontiualyxkRtravesu10n3p2  the scale-space of two images centered on xL and xR, respectively, until the local frequency signals are equalized and optimal  for correspondence matching. [sent-171, score-0.992]
</p><p>65 We will describe the algorithm for the application of stereo correspondence of rectified images. [sent-172, score-0.421]
</p><p>66 Then for each potential correspondence (xL , xR) we determine if the highest local frequencies ω˜L and ω˜R are sufficiently high for matching. [sent-174, score-0.387]
</p><p>67 If either local patch does not meet the required frequency content, then we traverse up both respective pyramids one level and repeat the process until both patches contain sufficiently high frequencies. [sent-175, score-0.635]
</p><p>68 5), the patches can be used directly for correspondence computation since they are locally centered on xL and xR respectively, they contain the optimal frequency content for matching, and they contain consistent signals, allowing the best possible correspondence match. [sent-178, score-1.027]
</p><p>69 Note that since the frequency information is computed only locally, we use an iterative technique to proceed through the scale-space sequentially in practice, rather than immediately computing the final equalized signal as proposed in Eqn. [sent-179, score-0.793]
</p><p>70 Synthetic Plane To quantitatively evaluate the effect of the proposed signal equalization we generated a synthetic plane with wavelet noise texture and rendered it using the Mitsuba physically-based renderer [11] with simulated shallow depth of field. [sent-190, score-0.862]
</p><p>71 The second algorithm extends  the na¨ ıve approach using the proposed signal equalization 11888844  sizes. [sent-195, score-0.753]
</p><p>72 As a third algorithm we employ PMVS [7], a state-of-the-art multi-view stereo matching algorithm. [sent-207, score-0.372]
</p><p>73 Signal equalization helps the na¨ ıve approach to overcome this limitation. [sent-218, score-0.464]
</p><p>74 Even for small window sizes, with signal equalization the algorithm successfully reconstructs almost the complete plane (96%) while increasing the accuracy by more than an order of magnitude. [sent-219, score-0.93]
</p><p>75 This indicates that signal equalization is likely to also improve the performance of sophisticated state-of-theart matching algorithms. [sent-222, score-0.886]
</p><p>76 With smaller window sizes matching is spread out over many layers in the scale-space  because the frequencies present in such a limited neighborhood vary substantially across patches. [sent-226, score-0.405]
</p><p>77 For smaller window sizes the frequency content varies more between patches, leading to a wider spread in matching levels. [sent-230, score-0.771]
</p><p>78 dow sizes the variation of frequency content is lower and thus the matching levels are spatially more correlated, revealing the expected behavior that the matching level is a function of signal dissimilarity due to degradation. [sent-231, score-1.116]
</p><p>79 Real-world Examples  As a first practical example, we demonstrate the improvement of signal equalization for the reconstruction of a human face from stereo views. [sent-234, score-1.087]
</p><p>80 9 (left) shows the two images acquired by the stereo rig as well as closeups of various corresponding areas with different amounts of defocus blur. [sent-238, score-0.474]
</p><p>81 9 visualizes the difference in local frequency content which is directly related to camera focus and shows the problem, where for camera C2 the neck is in focus (purple) while camera C1 focuses more on the cheek (yellow). [sent-240, score-0.586]
</p><p>82 Signal equalization greatly extends the reconstructable area and improves reconstruction quality as depicted in Fig. [sent-242, score-0.558]
</p><p>83 A second example is stereo reconstruction from macro photography. [sent-244, score-0.364]
</p><p>84 Inevitably, a stereo rig of macro images will contain different defocus regions. [sent-246, score-0.45]
</p><p>85 Again, our signal equalization approach greatly improves reconstruction quality as shown in Fig. [sent-249, score-0.847]
</p><p>86 Conclusion In this paper we aim to alleviate correspondence errors when matching two images that differ in signal content due to degradation. [sent-252, score-0.689]
</p><p>87 Our approach is to locally equalize and optimize image signals before computing correspondence matches. [sent-254, score-0.721]
</p><p>88 This is made possible by a local frequency estimation technique and an efficient scalespace image representation. [sent-255, score-0.532]
</p><p>89 In our current implementation, we model signal degradation as isotropic lowpass filtering. [sent-256, score-0.458]
</p><p>90 Another interesting venue for future research would be to use the differently degraded signals after matching to recover the original signal as well as possible. [sent-259, score-0.741]
</p><p>91 We have described and evaluated our technique in the context of improving dense stereo reconstruction given images that are degraded by different amounts of defocus blur due to low depth of field. [sent-261, score-0.702]
</p><p>92 However, correspondence matching is a fundamental tool in computer vision, and our algorithm can be applied in a variety of other contexts. [sent-262, score-0.315]
</p><p>93 An example would be to equalize frame-to-frame signals for  optical flow estimation or video stabilization that have been degraded by time varying blur, such as motion blur or focus change. [sent-263, score-0.718]
</p><p>94 Another example is to equalize local signals for template matching or image registration. [sent-264, score-0.65]
</p><p>95 The na¨ ıve algorithm successfully reconstructs areas where the signals are similar but quickly degrades with increasing signal mismatch. [sent-314, score-0.602]
</p><p>96 Signal equalization greatly improves accuracy and robustness in areas of signal mismatch. [sent-315, score-0.815]
</p><p>97 A stereo matching algorithm with an adaptive window: Theory and experiment. [sent-357, score-0.399]
</p><p>98 An area-based stereo matching using adaptive search range and window size. [sent-377, score-0.5]
</p><p>99 A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. [sent-400, score-0.421]
</p><p>100 Fast dense stereo matching using adaptive window in hierarchical framework. [sent-430, score-0.5]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('equalization', 0.464), ('frequency', 0.398), ('signal', 0.289), ('equalize', 0.25), ('stereo', 0.239), ('signals', 0.234), ('correspondence', 0.182), ('cutoff', 0.133), ('matching', 0.133), ('defocus', 0.119), ('frequencies', 0.117), ('ncc', 0.106), ('degradation', 0.102), ('window', 0.101), ('pmvs', 0.092), ('pyramids', 0.091), ('ve', 0.088), ('degraded', 0.085), ('content', 0.085), ('na', 0.082), ('lncs', 0.078), ('centered', 0.078), ('klowsky', 0.075), ('reconstruction', 0.07), ('blur', 0.068), ('lowpass', 0.067), ('equalized', 0.067), ('mismatch', 0.066), ('scalespace', 0.062), ('macro', 0.055), ('locally', 0.055), ('highest', 0.055), ('dft', 0.053), ('kuijper', 0.05), ('mitsuba', 0.05), ('patches', 0.047), ('scaling', 0.046), ('beeler', 0.044), ('splat', 0.044), ('disney', 0.044), ('patch', 0.042), ('amounts', 0.041), ('amplitudes', 0.041), ('reconstructs', 0.041), ('depth', 0.041), ('xr', 0.04), ('technique', 0.039), ('focal', 0.039), ('aip', 0.039), ('modulation', 0.039), ('areas', 0.038), ('situations', 0.037), ('rig', 0.037), ('scharstein', 0.036), ('slices', 0.035), ('plane', 0.035), ('stabilization', 0.034), ('goesele', 0.034), ('xl', 0.034), ('local', 0.033), ('bronstein', 0.033), ('synthetic', 0.033), ('spatial', 0.032), ('bradley', 0.032), ('amplitude', 0.031), ('sizes', 0.031), ('store', 0.031), ('strecha', 0.03), ('zurich', 0.03), ('pixel', 0.028), ('causing', 0.028), ('contents', 0.028), ('adaptive', 0.027), ('realworld', 0.027), ('compatible', 0.027), ('domain', 0.027), ('darker', 0.026), ('versions', 0.026), ('curless', 0.025), ('considerations', 0.025), ('practical', 0.025), ('level', 0.024), ('greatly', 0.024), ('optical', 0.024), ('visualizes', 0.024), ('coverage', 0.023), ('correlation', 0.023), ('focus', 0.023), ('spread', 0.023), ('right', 0.023), ('levels', 0.023), ('render', 0.022), ('optimally', 0.022), ('diminished', 0.022), ('hoppe', 0.022), ('subbarao', 0.022), ('koo', 0.022), ('beardsley', 0.022), ('aigne', 0.022), ('incredibly', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="255-tfidf-1" href="./iccv-2013-Local_Signal_Equalization_for_Correspondence_Matching.html">255 iccv-2013-Local Signal Equalization for Correspondence Matching</a></p>
<p>Author: Derek Bradley, Thabo Beeler</p><p>Abstract: Correspondence matching is one of the most common problems in computer vision, and it is often solved using photo-consistency of local regions. These approaches typically assume that the frequency content in the local region is consistent in the image pair, such that matching is performed on similar signals. However, in many practical situations this is not the case, for example with low depth of field cameras a scene point may be out of focus in one view and in-focus in the other, causing a mismatch of frequency signals. Furthermore, this mismatch can vary spatially over the entire image. In this paper we propose a local signal equalization approach for correspondence matching. Using a measure of local image frequency, we equalize local signals using an efficient scale-space image representation such that their frequency contents are optimally suited for matching. Our approach allows better correspondence matching, which we demonstrate with a number of stereo reconstruction examples on synthetic and real datasets.</p><p>2 0.19241121 <a title="255-tfidf-2" href="./iccv-2013-Depth_from_Combining_Defocus_and_Correspondence_Using_Light-Field_Cameras.html">108 iccv-2013-Depth from Combining Defocus and Correspondence Using Light-Field Cameras</a></p>
<p>Author: Michael W. Tao, Sunil Hadap, Jitendra Malik, Ravi Ramamoorthi</p><p>Abstract: Light-field cameras have recently become available to the consumer market. An array of micro-lenses captures enough information that one can refocus images after acquisition, as well as shift one ’s viewpoint within the subapertures of the main lens, effectively obtaining multiple views. Thus, depth cues from both defocus and correspondence are available simultaneously in a single capture. Previously, defocus could be achieved only through multiple image exposures focused at different depths, while correspondence cues needed multiple exposures at different viewpoints or multiple cameras; moreover, both cues could not easily be obtained together. In this paper, we present a novel simple and principled algorithm that computes dense depth estimation by combining both defocus and correspondence depth cues. We analyze the x-u 2D epipolar image (EPI), where by convention we assume the spatial x coordinate is horizontal and the angular u coordinate is vertical (our final algorithm uses the full 4D EPI). We show that defocus depth cues are obtained by computing the horizontal (spatial) variance after vertical (angular) integration, and correspondence depth cues by computing the vertical (angular) variance. We then show how to combine the two cues into a high quality depth map, suitable for computer vision applications such as matting, full control of depth-of-field, and surface reconstruction.</p><p>3 0.1348725 <a title="255-tfidf-3" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>Author: Hamed Kiani Galoogahi, Terence Sim, Simon Lucey</p><p>Abstract: Modern descriptors like HOG and SIFT are now commonly used in vision for pattern detection within image and video. From a signal processing perspective, this detection process can be efficiently posed as a correlation/convolution between a multi-channel image and a multi-channel detector/filter which results in a singlechannel response map indicating where the pattern (e.g. object) has occurred. In this paper, we propose a novel framework for learning a multi-channel detector/filter efficiently in the frequency domain, both in terms of training time and memory footprint, which we refer to as a multichannel correlation filter. To demonstrate the effectiveness of our strategy, we evaluate it across a number of visual detection/localization tasks where we: (i) exhibit superiorperformance to current state of the art correlation filters, and (ii) superior computational and memory efficiencies compared to state of the art spatial detectors.</p><p>4 0.13034721 <a title="255-tfidf-4" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>Author: Christoph Vogel, Konrad Schindler, Stefan Roth</p><p>Abstract: Estimating dense 3D scene flow from stereo sequences remains a challenging task, despite much progress in both classical disparity and 2D optical flow estimation. To overcome the limitations of existing techniques, we introduce a novel model that represents the dynamic 3D scene by a collection of planar, rigidly moving, local segments. Scene flow estimation then amounts to jointly estimating the pixelto-segment assignment, and the 3D position, normal vector, and rigid motion parameters of a plane for each segment. The proposed energy combines an occlusion-sensitive data term with appropriate shape, motion, and segmentation regularizers. Optimization proceeds in two stages: Starting from an initial superpixelization, we estimate the shape and motion parameters of all segments by assigning a proposal from a set of moving planes. Then the pixel-to-segment assignment is updated, while holding the shape and motion parameters of the moving planes fixed. We demonstrate the benefits of our model on different real-world image sets, including the challenging KITTI benchmark. We achieve leading performance levels, exceeding competing 3D scene flow methods, and even yielding better 2D motion estimates than all tested dedicated optical flow techniques.</p><p>5 0.1259639 <a title="255-tfidf-5" href="./iccv-2013-Shape_Anchors_for_Data-Driven_Multi-view_Reconstruction.html">387 iccv-2013-Shape Anchors for Data-Driven Multi-view Reconstruction</a></p>
<p>Author: Andrew Owens, Jianxiong Xiao, Antonio Torralba, William Freeman</p><p>Abstract: We present a data-driven method for building dense 3D reconstructions using a combination of recognition and multi-view cues. Our approach is based on the idea that there are image patches that are so distinctive that we can accurately estimate their latent 3D shapes solely using recognition. We call these patches shape anchors, and we use them as the basis of a multi-view reconstruction system that transfers dense, complex geometry between scenes. We “anchor” our 3D interpretation from these patches, using them to predict geometry for parts of the scene that are relatively ambiguous. The resulting algorithm produces dense reconstructions from stereo point clouds that are sparse and noisy, and we demonstrate it on a challenging dataset of real-world, indoor scenes.</p><p>6 0.11371169 <a title="255-tfidf-6" href="./iccv-2013-PM-Huber%3A_PatchMatch_with_Huber_Regularization_for_Stereo_Matching.html">304 iccv-2013-PM-Huber: PatchMatch with Huber Regularization for Stereo Matching</a></p>
<p>7 0.11351761 <a title="255-tfidf-7" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>8 0.10720383 <a title="255-tfidf-8" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>9 0.099801995 <a title="255-tfidf-9" href="./iccv-2013-A_Joint_Intensity_and_Depth_Co-sparse_Analysis_Model_for_Depth_Map_Super-resolution.html">18 iccv-2013-A Joint Intensity and Depth Co-sparse Analysis Model for Depth Map Super-resolution</a></p>
<p>10 0.097147696 <a title="255-tfidf-10" href="./iccv-2013-A_Rotational_Stereo_Model_Based_on_XSlit_Imaging.html">28 iccv-2013-A Rotational Stereo Model Based on XSlit Imaging</a></p>
<p>11 0.093790054 <a title="255-tfidf-11" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<p>12 0.089902259 <a title="255-tfidf-12" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>13 0.088140607 <a title="255-tfidf-13" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>14 0.086949758 <a title="255-tfidf-14" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>15 0.084841669 <a title="255-tfidf-15" href="./iccv-2013-Elastic_Net_Constraints_for_Shape_Matching.html">140 iccv-2013-Elastic Net Constraints for Shape Matching</a></p>
<p>16 0.08195176 <a title="255-tfidf-16" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>17 0.078779452 <a title="255-tfidf-17" href="./iccv-2013-Constant_Time_Weighted_Median_Filtering_for_Stereo_Matching_and_Beyond.html">88 iccv-2013-Constant Time Weighted Median Filtering for Stereo Matching and Beyond</a></p>
<p>18 0.078427672 <a title="255-tfidf-18" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>19 0.078082845 <a title="255-tfidf-19" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>20 0.07685709 <a title="255-tfidf-20" href="./iccv-2013-Robust_Non-parametric_Data_Fitting_for_Correspondence_Modeling.html">358 iccv-2013-Robust Non-parametric Data Fitting for Correspondence Modeling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.157), (1, -0.134), (2, -0.038), (3, 0.011), (4, -0.038), (5, 0.005), (6, 0.011), (7, -0.121), (8, 0.013), (9, -0.039), (10, -0.007), (11, -0.013), (12, 0.077), (13, -0.031), (14, 0.009), (15, -0.045), (16, -0.038), (17, -0.024), (18, 0.056), (19, 0.046), (20, 0.021), (21, 0.07), (22, -0.103), (23, 0.045), (24, 0.067), (25, 0.022), (26, -0.059), (27, -0.053), (28, -0.026), (29, 0.017), (30, 0.072), (31, -0.003), (32, 0.067), (33, -0.01), (34, 0.032), (35, -0.04), (36, 0.029), (37, -0.077), (38, 0.017), (39, 0.003), (40, -0.029), (41, -0.001), (42, -0.016), (43, -0.037), (44, -0.017), (45, 0.005), (46, -0.004), (47, -0.003), (48, 0.014), (49, -0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95973164 <a title="255-lsi-1" href="./iccv-2013-Local_Signal_Equalization_for_Correspondence_Matching.html">255 iccv-2013-Local Signal Equalization for Correspondence Matching</a></p>
<p>Author: Derek Bradley, Thabo Beeler</p><p>Abstract: Correspondence matching is one of the most common problems in computer vision, and it is often solved using photo-consistency of local regions. These approaches typically assume that the frequency content in the local region is consistent in the image pair, such that matching is performed on similar signals. However, in many practical situations this is not the case, for example with low depth of field cameras a scene point may be out of focus in one view and in-focus in the other, causing a mismatch of frequency signals. Furthermore, this mismatch can vary spatially over the entire image. In this paper we propose a local signal equalization approach for correspondence matching. Using a measure of local image frequency, we equalize local signals using an efficient scale-space image representation such that their frequency contents are optimally suited for matching. Our approach allows better correspondence matching, which we demonstrate with a number of stereo reconstruction examples on synthetic and real datasets.</p><p>2 0.77139628 <a title="255-lsi-2" href="./iccv-2013-Constant_Time_Weighted_Median_Filtering_for_Stereo_Matching_and_Beyond.html">88 iccv-2013-Constant Time Weighted Median Filtering for Stereo Matching and Beyond</a></p>
<p>Author: Ziyang Ma, Kaiming He, Yichen Wei, Jian Sun, Enhua Wu</p><p>Abstract: Despite the continuous advances in local stereo matching for years, most efforts are on developing robust cost computation and aggregation methods. Little attention has been seriously paid to the disparity refinement. In this work, we study weighted median filtering for disparity refinement. We discover that with this refinement, even the simple box filter aggregation achieves comparable accuracy with various sophisticated aggregation methods (with the same refinement). This is due to the nice weighted median filtering properties of removing outlier error while respecting edges/structures. This reveals that the previously overlooked refinement can be at least as crucial as aggregation. We also develop the first constant time algorithmfor the previously time-consuming weighted median filter. This makes the simple combination “box aggregation + weighted median ” an attractive solution in practice for both speed and accuracy. As a byproduct, the fast weighted median filtering unleashes its potential in other applications that were hampered by high complexities. We show its superiority in various applications such as depth upsampling, clip-art JPEG artifact removal, and image stylization.</p><p>3 0.76031566 <a title="255-lsi-3" href="./iccv-2013-PM-Huber%3A_PatchMatch_with_Huber_Regularization_for_Stereo_Matching.html">304 iccv-2013-PM-Huber: PatchMatch with Huber Regularization for Stereo Matching</a></p>
<p>Author: Philipp Heise, Sebastian Klose, Brian Jensen, Alois Knoll</p><p>Abstract: Most stereo correspondence algorithms match support windows at integer-valued disparities and assume a constant disparity value within the support window. The recently proposed PatchMatch stereo algorithm [7] overcomes this limitation of previous algorithms by directly estimating planes. This work presents a method that integrates the PatchMatch stereo algorithm into a variational smoothing formulation using quadratic relaxation. The resulting algorithm allows the explicit regularization of the disparity and normal gradients using the estimated plane parameters. Evaluation of our method in the Middlebury benchmark shows that our method outperforms the traditional integer-valued disparity strategy as well as the original algorithm and its variants in sub-pixel accurate disparity estimation.</p><p>4 0.75763154 <a title="255-lsi-4" href="./iccv-2013-A_Rotational_Stereo_Model_Based_on_XSlit_Imaging.html">28 iccv-2013-A Rotational Stereo Model Based on XSlit Imaging</a></p>
<p>Author: Jinwei Ye, Yu Ji, Jingyi Yu</p><p>Abstract: Traditional stereo matching assumes perspective viewing cameras under a translational motion: the second camera is translated away from the first one to create parallax. In this paper, we investigate a different, rotational stereo model on a special multi-perspective camera, the XSlit camera [9, 24]. We show that rotational XSlit (R-XSlit) stereo can be effectively created by fixing the sensor and slit locations but switching the two slits’ directions. We first derive the epipolar geometry of R-XSlit in the 4D light field ray space. Our derivation leads to a simple but effective scheme for locating corresponding epipolar “curves ”. To conduct stereo matching, we further derive a new disparity term in our model and develop a patch-based graph-cut solution. To validate our theory, we assemble an XSlit lens by using a pair of cylindrical lenses coupled with slit-shaped apertures. The XSlit lens can be mounted on commodity cameras where the slit directions are adjustable to form desirable R-XSlit pairs. We show through experiments that R-XSlitprovides apotentially advantageous imaging system for conducting fixed-location, dynamic baseline stereo.</p><p>5 0.71926224 <a title="255-lsi-5" href="./iccv-2013-Depth_from_Combining_Defocus_and_Correspondence_Using_Light-Field_Cameras.html">108 iccv-2013-Depth from Combining Defocus and Correspondence Using Light-Field Cameras</a></p>
<p>Author: Michael W. Tao, Sunil Hadap, Jitendra Malik, Ravi Ramamoorthi</p><p>Abstract: Light-field cameras have recently become available to the consumer market. An array of micro-lenses captures enough information that one can refocus images after acquisition, as well as shift one ’s viewpoint within the subapertures of the main lens, effectively obtaining multiple views. Thus, depth cues from both defocus and correspondence are available simultaneously in a single capture. Previously, defocus could be achieved only through multiple image exposures focused at different depths, while correspondence cues needed multiple exposures at different viewpoints or multiple cameras; moreover, both cues could not easily be obtained together. In this paper, we present a novel simple and principled algorithm that computes dense depth estimation by combining both defocus and correspondence depth cues. We analyze the x-u 2D epipolar image (EPI), where by convention we assume the spatial x coordinate is horizontal and the angular u coordinate is vertical (our final algorithm uses the full 4D EPI). We show that defocus depth cues are obtained by computing the horizontal (spatial) variance after vertical (angular) integration, and correspondence depth cues by computing the vertical (angular) variance. We then show how to combine the two cues into a high quality depth map, suitable for computer vision applications such as matting, full control of depth-of-field, and surface reconstruction.</p><p>6 0.66838622 <a title="255-lsi-6" href="./iccv-2013-DCSH_-_Matching_Patches_in_RGBD_Images.html">101 iccv-2013-DCSH - Matching Patches in RGBD Images</a></p>
<p>7 0.6456877 <a title="255-lsi-7" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>8 0.64246553 <a title="255-lsi-8" href="./iccv-2013-Line_Assisted_Light_Field_Triangulation_and_Stereo_Matching.html">252 iccv-2013-Line Assisted Light Field Triangulation and Stereo Matching</a></p>
<p>9 0.6217435 <a title="255-lsi-9" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<p>10 0.61836731 <a title="255-lsi-10" href="./iccv-2013-Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization.html">284 iccv-2013-Multiview Photometric Stereo Using Planar Mesh Parameterization</a></p>
<p>11 0.59590441 <a title="255-lsi-11" href="./iccv-2013-Shape_Anchors_for_Data-Driven_Multi-view_Reconstruction.html">387 iccv-2013-Shape Anchors for Data-Driven Multi-view Reconstruction</a></p>
<p>12 0.56727904 <a title="255-lsi-12" href="./iccv-2013-Efficient_Image_Dehazing_with_Boundary_Constraint_and_Contextual_Regularization.html">135 iccv-2013-Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</a></p>
<p>13 0.56713045 <a title="255-lsi-13" href="./iccv-2013-Modeling_the_Calibration_Pipeline_of_the_Lytro_Camera_for_High_Quality_Light-Field_Image_Reconstruction.html">271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</a></p>
<p>14 0.55501503 <a title="255-lsi-14" href="./iccv-2013-Person_Re-identification_by_Salience_Matching.html">313 iccv-2013-Person Re-identification by Salience Matching</a></p>
<p>15 0.55326116 <a title="255-lsi-15" href="./iccv-2013-Live_Metric_3D_Reconstruction_on_Mobile_Phones.html">254 iccv-2013-Live Metric 3D Reconstruction on Mobile Phones</a></p>
<p>16 0.54997689 <a title="255-lsi-16" href="./iccv-2013-Image_Guided_Depth_Upsampling_Using_Anisotropic_Total_Generalized_Variation.html">209 iccv-2013-Image Guided Depth Upsampling Using Anisotropic Total Generalized Variation</a></p>
<p>17 0.5456928 <a title="255-lsi-17" href="./iccv-2013-High_Quality_Shape_from_a_Single_RGB-D_Image_under_Uncalibrated_Natural_Illumination.html">199 iccv-2013-High Quality Shape from a Single RGB-D Image under Uncalibrated Natural Illumination</a></p>
<p>18 0.52854776 <a title="255-lsi-18" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<p>19 0.52657586 <a title="255-lsi-19" href="./iccv-2013-A_Joint_Intensity_and_Depth_Co-sparse_Analysis_Model_for_Depth_Map_Super-resolution.html">18 iccv-2013-A Joint Intensity and Depth Co-sparse Analysis Model for Depth Map Super-resolution</a></p>
<p>20 0.52619815 <a title="255-lsi-20" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.093), (7, 0.011), (12, 0.015), (13, 0.019), (26, 0.089), (31, 0.037), (32, 0.239), (42, 0.097), (64, 0.035), (73, 0.031), (89, 0.204), (95, 0.024), (97, 0.014), (98, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82444859 <a title="255-lda-1" href="./iccv-2013-Local_Signal_Equalization_for_Correspondence_Matching.html">255 iccv-2013-Local Signal Equalization for Correspondence Matching</a></p>
<p>Author: Derek Bradley, Thabo Beeler</p><p>Abstract: Correspondence matching is one of the most common problems in computer vision, and it is often solved using photo-consistency of local regions. These approaches typically assume that the frequency content in the local region is consistent in the image pair, such that matching is performed on similar signals. However, in many practical situations this is not the case, for example with low depth of field cameras a scene point may be out of focus in one view and in-focus in the other, causing a mismatch of frequency signals. Furthermore, this mismatch can vary spatially over the entire image. In this paper we propose a local signal equalization approach for correspondence matching. Using a measure of local image frequency, we equalize local signals using an efficient scale-space image representation such that their frequency contents are optimally suited for matching. Our approach allows better correspondence matching, which we demonstrate with a number of stereo reconstruction examples on synthetic and real datasets.</p><p>2 0.82324338 <a title="255-lda-2" href="./iccv-2013-Codemaps_-_Segment%2C_Classify_and_Search_Objects_Locally.html">77 iccv-2013-Codemaps - Segment, Classify and Search Objects Locally</a></p>
<p>Author: Zhenyang Li, Efstratios Gavves, Koen E.A. van_de_Sande, Cees G.M. Snoek, Arnold W.M. Smeulders</p><p>Abstract: In this paper we aim for segmentation and classification of objects. We propose codemaps that are a joint formulation of the classification score and the local neighborhood it belongs to in the image. We obtain the codemap by reordering the encoding, pooling and classification steps over lattice elements. Other than existing linear decompositions who emphasize only the efficiency benefits for localized search, we make three novel contributions. As a preliminary, we provide a theoretical generalization of the sufficient mathematical conditions under which image encodings and classification becomes locally decomposable. As first novelty we introduce ℓ2 normalization for arbitrarily shaped image regions, which is fast enough for semantic segmentation using our Fisher codemaps. Second, using the same lattice across images, we propose kernel pooling which embeds nonlinearities into codemaps for object classification by explicit or approximate feature mappings. Results demonstrate that ℓ2 normalized Fisher codemaps improve the state-of-the-art in semantic segmentation for PAS- CAL VOC. For object classification the addition of nonlinearities brings us on par with the state-of-the-art, but is 3x faster. Because of the codemaps ’ inherent efficiency, we can reach significant speed-ups for localized search as well. We exploit the efficiency gain for our third novelty: object segment retrieval using a single query image only.</p><p>3 0.7695393 <a title="255-lda-3" href="./iccv-2013-Coupling_Alignments_with_Recognition_for_Still-to-Video_Face_Recognition.html">97 iccv-2013-Coupling Alignments with Recognition for Still-to-Video Face Recognition</a></p>
<p>Author: Zhiwu Huang, Xiaowei Zhao, Shiguang Shan, Ruiping Wang, Xilin Chen</p><p>Abstract: The Still-to-Video (S2V) face recognition systems typically need to match faces in low-quality videos captured under unconstrained conditions against high quality still face images, which is very challenging because of noise, image blur, lowface resolutions, varying headpose, complex lighting, and alignment difficulty. To address the problem, one solution is to select the frames of ‘best quality ’ from videos (hereinafter called quality alignment in this paper). Meanwhile, the faces in the selected frames should also be geometrically aligned to the still faces offline well-aligned in the gallery. In this paper, we discover that the interactions among the three tasks–quality alignment, geometric alignment and face recognition–can benefit from each other, thus should be performed jointly. With this in mind, we propose a Coupling Alignments with Recognition (CAR) method to tightly couple these tasks via low-rank regularized sparse representation in a unified framework. Our method makes the three tasks promote mutually by a joint optimization in an Augmented Lagrange Multiplier routine. Extensive , experiments on two challenging S2V datasets demonstrate that our method outperforms the state-of-the-art methods impressively.</p><p>4 0.7608453 <a title="255-lda-4" href="./iccv-2013-Joint_Noise_Level_Estimation_from_Personal_Photo_Collections.html">223 iccv-2013-Joint Noise Level Estimation from Personal Photo Collections</a></p>
<p>Author: Yichang Shih, Vivek Kwatra, Troy Chinen, Hui Fang, Sergey Ioffe</p><p>Abstract: Personal photo albums are heavily biased towards faces of people, but most state-of-the-art algorithms for image denoising and noise estimation do not exploit facial information. We propose a novel technique for jointly estimating noise levels of all face images in a photo collection. Photos in a personal album are likely to contain several faces of the same people. While some of these photos would be clean and high quality, others may be corrupted by noise. Our key idea is to estimate noise levels by comparing multiple images of the same content that differ predominantly in their noise content. Specifically, we compare geometrically and photometrically aligned face images of the same person. Our estimation algorithm is based on a probabilistic formulation that seeks to maximize the joint probability of estimated noise levels across all images. We propose an approximate solution that decomposes this joint maximization into a two-stage optimization. The first stage determines the relative noise between pairs of images by pooling estimates from corresponding patch pairs in a probabilistic fashion. The second stage then jointly optimizes for all absolute noise parameters by conditioning them upon relative noise levels, which allows for a pairwise factorization of the probability distribution. We evaluate our noise estimation method using quantitative experiments to measure accuracy on synthetic data. Additionally, we employ the estimated noise levels for automatic denoising using “BM3D”, and evaluate the quality of denoising on real-world photos through a user study.</p><p>5 0.73693782 <a title="255-lda-5" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>Author: Raúl Díaz, Sam Hallman, Charless C. Fowlkes</p><p>Abstract: The confluence of robust algorithms for structure from motion along with high-coverage mapping and imaging of the world around us suggests that it will soon be feasible to accurately estimate camera pose for a large class photographs taken in outdoor, urban environments. In this paper, we investigate how such information can be used to improve the detection of dynamic objects such as pedestrians and cars. First, we show that when rough camera location is known, we can utilize detectors that have been trained with a scene-specific background model in order to improve detection accuracy. Second, when precise camera pose is available, dense matching to a database of existing images using multi-view stereo provides a way to eliminate static backgrounds such as building facades, akin to background-subtraction often used in video analysis. We evaluate these ideas using a dataset of tourist photos with estimated camera pose. For template-based pedestrian detection, we achieve a 50 percent boost in average precision over baseline.</p><p>6 0.7360006 <a title="255-lda-6" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>7 0.73507965 <a title="255-lda-7" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>8 0.73492628 <a title="255-lda-8" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>9 0.73467273 <a title="255-lda-9" href="./iccv-2013-A_Convex_Optimization_Framework_for_Active_Learning.html">6 iccv-2013-A Convex Optimization Framework for Active Learning</a></p>
<p>10 0.73441017 <a title="255-lda-10" href="./iccv-2013-Low-Rank_Sparse_Coding_for_Image_Classification.html">258 iccv-2013-Low-Rank Sparse Coding for Image Classification</a></p>
<p>11 0.73417258 <a title="255-lda-11" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>12 0.73372769 <a title="255-lda-12" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>13 0.73365772 <a title="255-lda-13" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>14 0.73354483 <a title="255-lda-14" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>15 0.73329496 <a title="255-lda-15" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>16 0.73264623 <a title="255-lda-16" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>17 0.7321595 <a title="255-lda-17" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>18 0.73177862 <a title="255-lda-18" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>19 0.73159188 <a title="255-lda-19" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>20 0.73139977 <a title="255-lda-20" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
