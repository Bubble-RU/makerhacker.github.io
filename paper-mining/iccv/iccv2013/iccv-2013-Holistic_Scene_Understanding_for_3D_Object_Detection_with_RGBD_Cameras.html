<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>201 iccv-2013-Holistic Scene Understanding for 3D Object Detection with RGBD Cameras</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-201" href="#">iccv2013-201</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>201 iccv-2013-Holistic Scene Understanding for 3D Object Detection with RGBD Cameras</h1>
<br/><p>Source: <a title="iccv-2013-201-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Lin_Holistic_Scene_Understanding_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Dahua Lin, Sanja Fidler, Raquel Urtasun</p><p>Abstract: In this paper, we tackle the problem of indoor scene understanding using RGBD data. Towards this goal, we propose a holistic approach that exploits 2D segmentation, 3D geometry, as well as contextual relations between scenes and objects. Specifically, we extend the CPMC [3] framework to 3D in order to generate candidate cuboids, and develop a conditional random field to integrate information from different sources to classify the cuboids. With this formulation, scene classification and 3D object recognition are coupled and can be jointly solved through probabilistic inference. We test the effectiveness of our approach on the challenging NYU v2 dataset. The experimental results demonstrate that through effective evidence integration and holistic reasoning, our approach achieves substantial improvement over the state-of-the-art.</p><p>Reference: <a title="iccv-2013-201-reference" href="../iccv2013_reference/iccv-2013-Holistic_Scene_Understanding_for_3D_Object_Detection_with_RGBD_Cameras_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu c  Abstract In this paper, we tackle the problem of indoor scene understanding using RGBD data. [sent-2, score-0.263]
</p><p>2 Towards this goal, we propose a holistic approach that exploits 2D segmentation, 3D geometry, as well as contextual relations between scenes and objects. [sent-3, score-0.303]
</p><p>3 Specifically, we extend the CPMC [3] framework to 3D in order to generate candidate cuboids, and develop a conditional random field to integrate information from different sources to classify the cuboids. [sent-4, score-0.175]
</p><p>4 With this formulation, scene classification and 3D object recognition are coupled and can be jointly solved through probabilistic inference. [sent-5, score-0.173]
</p><p>5 Introduction One of the fundamental problems in indoor robotics is to be able to reliably detect objects in 3D. [sent-9, score-0.243]
</p><p>6 Over the last decade a variety of approaches have been developed in order to infer 3D objects from monocular imagery [12, 6, 19]. [sent-13, score-0.244]
</p><p>7 The most successful approaches extend the popular (2D) deformable part-based model [5] to perform category-level 3D object detection [12, 6, 19]. [sent-14, score-0.157]
</p><p>8 While 3D detection is extremely difficult when employing still images, the use of additional information such as video or depth sensors is key in order to solve the inherent ambiguities of the monocular setting. [sent-15, score-0.209]
</p><p>9 Numerous approaches have been developed that model both appearance and depth information to score object detectors in 3D [16, 10, 24, 20, 8] , showing improved performance over the monocular setting. [sent-16, score-0.309]
</p><p>10 Objects, however, are not randomly placed in space, but respect certain physical and statistical properties of the 3D world. [sent-17, score-0.144]
</p><p>11 We leverage this information as well as contextual relations to detect and recognize objects in indoor scenes. [sent-20, score-0.502]
</p><p>12 In particular, we first generate candidate cuboids through an extension to CPMC and then use a CRF to assign semantic labels to them. [sent-21, score-0.629]
</p><p>13 Exploiting physical and contextual relationships between objects and the environment is key to achieve a high precision in semantic tasks such as segmentation [15, 22, 29] and detection [9, 8]. [sent-23, score-0.566]
</p><p>14 In this paper we are interested in exploiting RGB-D imagery to perform category level 3D object detection. [sent-24, score-0.153]
</p><p>15 We represent the objects in the world in terms of 3D cuboids and model the physical and statistical interactions between the objects and the environment (the scene) as well as interactions between objects. [sent-25, score-0.817]
</p><p>16 Towards this goal, we develop an approach that extends the CPMC framework [3] to generate  cuboid hypotheses in point clouds by placing them tightly around bottom-up 3D region candidates. [sent-26, score-0.487]
</p><p>17 Our region candidates are ranked according to “objectness” in appearance and are encouraged to respect occlusion boundaries in 3D. [sent-27, score-0.176]
</p><p>18 We formulate the joint detection problem in a conditional random field to model the contextual relationships between objects in 3D. [sent-28, score-0.341]
</p><p>19 These models mainly focus on segmentation by enforcing statistical physical constraints. [sent-38, score-0.168]
</p><p>20 [23] reason about support surfaces and structural object classes (such as “ground”, “furniture” and “props”). [sent-40, score-0.155]
</p><p>21 [22, 28, 15] do segmentation based on a variety of carefully designed 2D appearance and 3D geometry features. [sent-41, score-0.173]
</p><p>22 [22] reasons about spatial transitions between superpixels based on RGB and depth information, while [15] enforces statistical coocurrences of 3D spatial relations such as near or on top of. [sent-42, score-0.252]
</p><p>23 Most 3D object detectors make use of both appearance and depth information, but do not exploit the inter-object relationships. [sent-45, score-0.281]
</p><p>24 Scene-object geometry is exploited for indoor scenarios in [8], adopting a sliding window approach that uses appearance as well as 3D features such as normals, height and distance to the ground. [sent-52, score-0.297]
</p><p>25 In [13], object candidates are found by fitting 3D cuboids in point clouds, but do not reason about the class of the object. [sent-53, score-0.638]
</p><p>26 Related work on contextual object detection in 3D has  been sparser. [sent-54, score-0.258]
</p><p>27 Most approaches use monocular imagery to infer the objects [19], and use the context of the problem to parametrize the model. [sent-55, score-0.244]
</p><p>28 In [12, 6, 17], objects in indoor scenarios are represented as cuboids aligned with the major axes of the room. [sent-56, score-0.619]
</p><p>29 These approaches use estimated room layouts to rescore object detections in 3D, which softly imposes the constraints that the objects do not penetrate the room, and rest on the floor. [sent-57, score-0.276]
</p><p>30 [9] represent the objects in the world as cuboids and model physical constraints among them encoding that objects need to be supported in 3D, and cannot penetrate each other. [sent-59, score-0.714]
</p><p>31 [7] exploit the strong priors in outdoor scenarios to reason jointly about objects and road intersections. [sent-62, score-0.161]
</p><p>32 3D detection with RGBD Imagery We generate a set of cuboids via candidate 3D “objectness” regions that are encouraged to respect intensity as well as occlusion boundaries in 3D. [sent-64, score-0.652]
</p><p>33 We propose a simple extension to generate candidate class-independent object regions by exploiting both depth as well as appearance cues. [sent-66, score-0.38]
</p><p>34 Generating bottom-up 3D region candidates CPMC [3] uses parametric min-cut to generate a wide variety of foreground candidates from equally spaced seeds. [sent-69, score-0.204]
</p><p>35 , V the set of all pixels, E the edges between neighboring pixels, a snedt oCfλ athlle p unary potentials:  Cλ(xu)=⎪⎨⎪ ⎪ ⎧⎪f∞ 0(,xu)+λ,i f x xu u= = 10 , u u / ∈ ∈/ V V bf (2) Here λ is an of⎩⎪fset and is used to generate different solutions of the objective function. [sent-79, score-0.204]
</p><p>36 This simple combina+tio(n1 −hasα b)e·genP sbhown to improve boundary detection in RGBD imagery by roughly 2% [28]. [sent-96, score-0.153]
</p><p>37 Fitting the Cuboids We generate cuboids from the candidate regions. [sent-99, score-0.551]
</p><p>38 Specifically, we select top K candidate regions ranked by the objectness scores [3], after performing non-maxima suppression (we use 0. [sent-100, score-0.269]
</p><p>39 5 as max overlap), and then generate candidate cuboids by fitting a 3D cube around each candidate region. [sent-101, score-0.852]
</p><p>40 A natural idea to accomplish this is to map the pixels in a given region into 3D coordinates, and find the minimal bounding cube around them. [sent-102, score-0.14]
</p><p>41 To improve the robustness, we consider a variant that instead of finding a minimal bounding cube to all the points, returns the minimal cube that contains 95% of the 3D points. [sent-104, score-0.28]
</p><p>42 Indoor Scene Model Assigning class labels to candidate cuboids is a challenging task. [sent-109, score-0.588]
</p><p>43 , sceneobject relations and spatial configurations) to improve the recognition accuracy. [sent-113, score-0.151]
</p><p>44 Contextual model for 3D Object Detection We employ candidate cuboids obtained by the method described in the previous section as input to our holistic model. [sent-116, score-0.55]
</p><p>45 We characterize each cuboid by both a 2D bounding box and a 3D bounding cube. [sent-117, score-0.384]
</p><p>46 In this paper, we are interested in simultaneously classifying the scene and assigning semantic labels to candidate cuboids. [sent-119, score-0.318]
</p><p>47 ,dS C} are respectively twhiet hnyu mb∈er { 0of, scene aCn}d, object classes. [sent-128, score-0.173]
</p><p>48 Note that the value of yi can be 0, which  indicates that the cuboid is a false positive. [sent-129, score-0.456]
</p><p>49 We define a CRF in terms of these variables, which exploits appearance features, geometric properties as well as semantic relations between objects and the scene (see Fig. [sent-130, score-0.506]
</p><p>50 Each cube is associated with unary potentials that characterize both the appearance and geometric properties. [sent-133, score-0.592]
</p><p>51 In addition, the pairwise potentials between the scene and the objects, as well as between the objects themselves encourage certain configurations. [sent-134, score-0.473]
</p><p>52 Scene appearance: In order to incorporate global information about the scene without making hard decisions, we define a unary potential over the scene label s as ψs (s = u) = σ(tu) , where tu denotes the classifier score for scene class u and σ is the logistic function. [sent-147, score-0.572]
</p><p>53 Since our cuboid hypotheses were generated via bottom-up region proposals, each cuboid thus has a 3D segment associated with it. [sent-152, score-0.768]
</p><p>54 We use the output score as a unary potential in our model: φrank(yi = l) = f(l),  where f(l) represents the predicted overlap of object candidate yi with ground-truth. [sent-159, score-0.449]
</p><p>55 Segmentation potential: Since the number of training examples of each class is limited, classifiers trained to recognize full objects in our experience did not work particularly well. [sent-161, score-0.174]
</p><p>56 This motivates the use of segmentation potentials as unaries for our cuboid hypotheses. [sent-162, score-0.721]
</p><p>57 In order to derive the potential for a cuboid, we project the cuboid onto the image plane and obtain a convex hull. [sent-169, score-0.475]
</p><p>58 Our model also employs geometric potentials such as absolute size and aspect ratio in 3D, which ensure that the joint model recovers physically valid hypotheses. [sent-172, score-0.299]
</p><p>59 Object geometry: Geometric properties of an object are an important source of discriminative information which is complementary to appearance and depth features. [sent-173, score-0.242]
</p><p>60 Note that these properties capture not only the intrinsic attributes of an object, but also its position relative to the scene layout. [sent-177, score-0.148]
</p><p>61 This has been proven very useful in the indoor setting [23]. [sent-179, score-0.152]
</p><p>62 To use geometric properties in our model, we train an SVM with RBF kernel on the geometric features, one for each class, and use the resultant scores rl as unary potentials for the candidate cubes, i. [sent-180, score-0.691]
</p><p>63 , the room that an object resides in or other objects in the same scene) often provides useful information for object recognition. [sent-185, score-0.282]
</p><p>64 m=j1(sj= k,yi(j)= l), yi(j)  where is the i-th cuboid on the j-th training example, mj is the number of objects in the j-th scene, Ntr is the number of training images, and (·) the indicator function, which equals 1when the enclosed( ·c)on thdeit i onndi hcaotlodrs. [sent-192, score-0.475]
</p><p>65 Geometric context: We introduce two potentials to exploit the spatial relations between cuboids in 3D: (1) closeto relation (e. [sent-204, score-0.777]
</p><p>66 ), is defined to be the normalized frequency that an object of class l is close to that of class l? [sent-215, score-0.168]
</p><p>67 In particular, we define a 0-1 loss over the scene type, and a 0-1 loss over the cuboid detections. [sent-228, score-0.495]
</p><p>68 This worked very well in practice, not imposing any restrictions in the types and order of potentials nor on the structure of the graph. [sent-234, score-0.233]
</p><p>69 the fraction of objects which are covered by at least one candidate cube (with overlap > 0. [sent-246, score-0.36]
</p><p>70 These classes are “special” as they define the scene layout. [sent-250, score-0.168]
</p><p>71 For each object instance that belongs to one of these 21 classes, we generate a ground-truth cuboid using the fitting algorithm described in Sec. [sent-252, score-0.524]
</p><p>72 We also identified cubes that were poorly fitted due to imperfect segmentation (less than 5%) and ignored them in both training and performance evaluation. [sent-255, score-0.185]
</p><p>73 In what follows, we will first compare the performance of two major components (cuboid detection and scene & object classification) to state-of-the-art methods, and then examine the overall performance. [sent-259, score-0.235]
</p><p>74 Performance of cuboid detection: The primary goal of the cuboid detection stage is to generate a reasonable amount of candidate cuboids, such that a significant portion of the true objects is contained in the candidate set. [sent-260, score-1.225]
</p><p>75 The performance of a cuboid detector is measured in terms of base recall, given a fixed Kc – the maximum number of candidates for each scene. [sent-261, score-0.555]
</p><p>76 Specifically, an object is said to be recalled if there is a candidate cube which overlaps with it more than 0. [sent-262, score-0.506]
</p><p>77 The base recall is defined to be the fraction of ground-truth objects that are recalled by the candidate set. [sent-264, score-0.42]
</p><p>78 By varying K, we obtain curves that show how the number of candidates per image influ1421  truth cuboids in terms of the percentage of correct labels. [sent-267, score-0.455]
</p><p>79 Here, “scene-obj“, “obj-obj“, and “obj-spa“ respectively refers to sceneobject co-occurances, object-object co-occurances, and spatial relations between objects. [sent-268, score-0.151]
</p><p>80 “ refers to appearancebased potentials, including segmentation and scene appearance, and “unaries“ refers to all unary potentials. [sent-270, score-0.267]
</p><p>81 One of the reason that non-maximal suppression helps is that it actively removes redundant cuboids, and consequently the detector is able to cover more objects using fewer candidates. [sent-282, score-0.221]
</p><p>82 Particularly, we test settings using only feature-based potentials to set up the baselines, which are equivalent to feature-based classification. [sent-287, score-0.233]
</p><p>83 (3) The accuracy increases as more potentials are added to the framework. [sent-301, score-0.233]
</p><p>84 Overall Performance: To test the integrated performance, we considered different combination of cuboid detectors and CRF configurations. [sent-309, score-0.426]
</p><p>85 Here, an object is said to be recalled if there is a cuboid with the same class label that overlaps with it by more than 50%. [sent-312, score-0.674]
</p><p>86 This is different from the base recall in detector evaluation, where the cuboids have not been labeled. [sent-313, score-0.518]
</p><p>87 The poor performance of DPM indicates that 2D object detectors that focuses on objects with regular structures encounter significant difficulties in a general indoor setting. [sent-315, score-0.347]
</p><p>88 Moreover, we can observe that the use of geometric feature, scene-object relations, and other potentials further improves the overall performance. [sent-318, score-0.299]
</p><p>89 In an indoor environment, objects of some classes (e. [sent-320, score-0.3]
</p><p>90 We can see that the use of object geometry and contextual information leads to notable improvements, especially over frequent classes. [sent-327, score-0.29]
</p><p>91 Computational Complexity: With all the cuboids ready, both learning and inference are very efficient. [sent-330, score-0.411]
</p><p>92 With K = 15, the learning of the full CRF model takes about 2 minutes on a workstation with Intel i7 quad-core CPU (using 1We applied DPM detectors for different classes respectively, and sorted the detected objects by their decreasing scores ofall classes together. [sent-331, score-0.28]
</p><p>93 We kept only the top K detected objects in each scene for performance evaluation. [sent-332, score-0.202]
</p><p>94 In particular, the first row shows the results obtained using the MILP detector [13], while the other four rows  corresponds  to the setting where a tunable detector is used to generate K  = 8, 15, 30, 50 candidates  per images. [sent-355, score-0.199]
</p><p>95 Empirically, the time needed to learn the model parameters or to infer the labels scales up linearly as the number of potentials increases. [sent-363, score-0.263]
</p><p>96 Conclusion We have developed an integrated framework to detect and recognize 3D cuboids in indoor scenes, an provided a detailed evaluation on the challenging NYU v2 dataset. [sent-365, score-0.558]
</p><p>97 Our experiments demonstrate that our approach consistently outperforms state-of-the-art detectors by effectively combining segmentation features, geometric properties, as well as contextual relations between objects. [sent-366, score-0.398]
</p><p>98 The 3d hough transform for plane detection in point clouds - a review and a new accumulator design. [sent-377, score-0.163]
</p><p>99 Semantic labeling of 3d point clouds for indoor scenes. [sent-472, score-0.209]
</p><p>100 Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation. [sent-571, score-0.332]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cuboid', 0.384), ('cuboids', 0.376), ('cpmc', 0.312), ('potentials', 0.233), ('milp', 0.158), ('indoor', 0.152), ('cube', 0.14), ('crf', 0.135), ('contextual', 0.134), ('candidate', 0.129), ('scene', 0.111), ('rgbd', 0.106), ('relations', 0.095), ('unary', 0.095), ('recalled', 0.095), ('cubes', 0.093), ('potential', 0.091), ('objects', 0.091), ('imagery', 0.091), ('depth', 0.085), ('vuv', 0.084), ('dpm', 0.081), ('candidates', 0.079), ('yi', 0.072), ('physical', 0.071), ('room', 0.067), ('geometric', 0.066), ('gpbdepth', 0.063), ('gpbrgb', 0.063), ('xu', 0.063), ('monocular', 0.062), ('object', 0.062), ('detection', 0.062), ('segmentation', 0.061), ('bed', 0.061), ('floor', 0.059), ('appearance', 0.058), ('carreira', 0.057), ('fidler', 0.057), ('classes', 0.057), ('clouds', 0.057), ('suppression', 0.057), ('penetrate', 0.056), ('sceneobject', 0.056), ('silberman', 0.055), ('base', 0.055), ('geometry', 0.054), ('relationships', 0.054), ('class', 0.053), ('objectness', 0.05), ('recall', 0.05), ('semantic', 0.048), ('desk', 0.046), ('generate', 0.046), ('overlaps', 0.045), ('environment', 0.045), ('holistic', 0.045), ('gpb', 0.045), ('hough', 0.044), ('unaries', 0.043), ('detectors', 0.042), ('chair', 0.041), ('hazan', 0.04), ('frequent', 0.04), ('encouraged', 0.039), ('interactions', 0.039), ('relation', 0.039), ('nms', 0.038), ('encourage', 0.038), ('properties', 0.037), ('detector', 0.037), ('lai', 0.037), ('messages', 0.037), ('reason', 0.036), ('superpixels', 0.036), ('statistical', 0.036), ('exp', 0.035), ('inference', 0.035), ('ren', 0.035), ('nyu', 0.035), ('geiger', 0.035), ('said', 0.035), ('pf', 0.034), ('exploit', 0.034), ('scores', 0.033), ('deformable', 0.033), ('land', 0.033), ('height', 0.033), ('rl', 0.032), ('fitting', 0.032), ('fitted', 0.031), ('primal', 0.031), ('recognize', 0.03), ('labels', 0.03), ('sun', 0.029), ('scenes', 0.029), ('world', 0.029), ('specifically', 0.029), ('disparity', 0.029), ('configurations', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="201-tfidf-1" href="./iccv-2013-Holistic_Scene_Understanding_for_3D_Object_Detection_with_RGBD_Cameras.html">201 iccv-2013-Holistic Scene Understanding for 3D Object Detection with RGBD Cameras</a></p>
<p>Author: Dahua Lin, Sanja Fidler, Raquel Urtasun</p><p>Abstract: In this paper, we tackle the problem of indoor scene understanding using RGBD data. Towards this goal, we propose a holistic approach that exploits 2D segmentation, 3D geometry, as well as contextual relations between scenes and objects. Specifically, we extend the CPMC [3] framework to 3D in order to generate candidate cuboids, and develop a conditional random field to integrate information from different sources to classify the cuboids. With this formulation, scene classification and 3D object recognition are coupled and can be jointly solved through probabilistic inference. We test the effectiveness of our approach on the challenging NYU v2 dataset. The experimental results demonstrate that through effective evidence integration and holistic reasoning, our approach achieves substantial improvement over the state-of-the-art.</p><p>2 0.24360678 <a title="201-tfidf-2" href="./iccv-2013-Estimating_the_3D_Layout_of_Indoor_Scenes_and_Its_Clutter_from_Depth_Sensors.html">144 iccv-2013-Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors</a></p>
<p>Author: Jian Zhang, Chen Kan, Alexander G. Schwing, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly estimate the layout ofrooms as well as the clutterpresent in the scene using RGB-D data. Towards this goal, we propose an effective model that is able to exploit both depth and appearance features, which are complementary. Furthermore, our approach is efficient as we exploit the inherent decomposition of additive potentials. We demonstrate the effectiveness of our approach on the challenging NYU v2 dataset and show that employing depth reduces the layout error by 6% and the clutter estimation by 13%.</p><p>3 0.1978907 <a title="201-tfidf-3" href="./iccv-2013-Active_MAP_Inference_in_CRFs_for_Efficient_Semantic_Segmentation.html">42 iccv-2013-Active MAP Inference in CRFs for Efficient Semantic Segmentation</a></p>
<p>Author: Gemma Roig, Xavier Boix, Roderick De_Nijs, Sebastian Ramos, Koljia Kuhnlenz, Luc Van_Gool</p><p>Abstract: Most MAP inference algorithms for CRFs optimize an energy function knowing all the potentials. In this paper, we focus on CRFs where the computational cost of instantiating the potentials is orders of magnitude higher than MAP inference. This is often the case in semantic image segmentation, where most potentials are instantiated by slow classifiers fed with costly features. We introduce Active MAP inference 1) to on-the-fly select a subset of potentials to be instantiated in the energy function, leaving the rest of the parameters of the potentials unknown, and 2) to estimate the MAP labeling from such incomplete energy function. Results for semantic segmentation benchmarks, namely PASCAL VOC 2010 [5] and MSRC-21 [19], show that Active MAP inference achieves similar levels of accuracy but with major efficiency gains.</p><p>4 0.18830425 <a title="201-tfidf-4" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>Author: Scott Satkin, Martial Hebert</p><p>Abstract: We present a new algorithm 3DNN (3D NearestNeighbor), which is capable of matching an image with 3D data, independently of the viewpoint from which the image was captured. By leveraging rich annotations associated with each image, our algorithm can automatically produce precise and detailed 3D models of a scene from a single image. Moreover, we can transfer information across images to accurately label and segment objects in a scene. The true benefit of 3DNN compared to a traditional 2D nearest-neighbor approach is that by generalizing across viewpoints, we free ourselves from the need to have training examples captured from all possible viewpoints. Thus, we are able to achieve comparable results using orders of magnitude less data, and recognize objects from never-beforeseen viewpoints. In this work, we describe the 3DNN algorithm and rigorously evaluate its performance for the tasks of geometry estimation and object detection/segmentation. By decoupling the viewpoint and the geometry of an image, we develop a scene matching approach which is truly 100% viewpoint invariant, yielding state-of-the-art performance on challenging data.</p><p>5 0.18001601 <a title="201-tfidf-5" href="./iccv-2013-Box_in_the_Box%3A_Joint_3D_Layout_and_Object_Reasoning_from_Single_Images.html">64 iccv-2013-Box in the Box: Joint 3D Layout and Object Reasoning from Single Images</a></p>
<p>Author: Alexander G. Schwing, Sanja Fidler, Marc Pollefeys, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly infer the room layout as well as the objects present in the scene. Towards this goal, we propose a branch and bound algorithm which is guaranteed to retrieve the global optimum of the joint problem. The main difficulty resides in taking into account occlusion in order to not over-count the evidence. We introduce a new decomposition method, which generalizes integral geometry to triangular shapes, and allows us to bound the different terms in constant time. We exploit both geometric cues and object detectors as image features and show large improvements in 2D and 3D object detection over state-of-the-art deformable part-based models.</p><p>6 0.16120425 <a title="201-tfidf-6" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>7 0.15251671 <a title="201-tfidf-7" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>8 0.15050389 <a title="201-tfidf-8" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<p>9 0.13551496 <a title="201-tfidf-9" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>10 0.13258211 <a title="201-tfidf-10" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>11 0.13181169 <a title="201-tfidf-11" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>12 0.12772198 <a title="201-tfidf-12" href="./iccv-2013-3D_Scene_Understanding_by_Voxel-CRF.html">2 iccv-2013-3D Scene Understanding by Voxel-CRF</a></p>
<p>13 0.12616676 <a title="201-tfidf-13" href="./iccv-2013-Allocentric_Pose_Estimation.html">46 iccv-2013-Allocentric Pose Estimation</a></p>
<p>14 0.12299445 <a title="201-tfidf-14" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>15 0.11814662 <a title="201-tfidf-15" href="./iccv-2013-Building_Part-Based_Object_Detectors_via_3D_Geometry.html">66 iccv-2013-Building Part-Based Object Detectors via 3D Geometry</a></p>
<p>16 0.11694828 <a title="201-tfidf-16" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>17 0.11583627 <a title="201-tfidf-17" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>18 0.11376177 <a title="201-tfidf-18" href="./iccv-2013-Parsing_IKEA_Objects%3A_Fine_Pose_Estimation.html">308 iccv-2013-Parsing IKEA Objects: Fine Pose Estimation</a></p>
<p>19 0.11290347 <a title="201-tfidf-19" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>20 0.10748506 <a title="201-tfidf-20" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.258), (1, -0.059), (2, 0.016), (3, -0.016), (4, 0.141), (5, -0.031), (6, -0.086), (7, -0.013), (8, -0.085), (9, -0.118), (10, 0.049), (11, 0.071), (12, -0.079), (13, 0.003), (14, -0.026), (15, -0.086), (16, -0.081), (17, 0.001), (18, -0.079), (19, -0.085), (20, -0.16), (21, 0.01), (22, 0.083), (23, -0.015), (24, 0.104), (25, -0.102), (26, 0.022), (27, 0.008), (28, -0.066), (29, 0.025), (30, -0.076), (31, 0.011), (32, -0.004), (33, 0.04), (34, 0.07), (35, 0.013), (36, -0.053), (37, -0.034), (38, -0.033), (39, 0.084), (40, -0.06), (41, 0.036), (42, 0.034), (43, 0.066), (44, 0.059), (45, 0.019), (46, 0.032), (47, 0.014), (48, -0.012), (49, -0.002)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94900012 <a title="201-lsi-1" href="./iccv-2013-Holistic_Scene_Understanding_for_3D_Object_Detection_with_RGBD_Cameras.html">201 iccv-2013-Holistic Scene Understanding for 3D Object Detection with RGBD Cameras</a></p>
<p>Author: Dahua Lin, Sanja Fidler, Raquel Urtasun</p><p>Abstract: In this paper, we tackle the problem of indoor scene understanding using RGBD data. Towards this goal, we propose a holistic approach that exploits 2D segmentation, 3D geometry, as well as contextual relations between scenes and objects. Specifically, we extend the CPMC [3] framework to 3D in order to generate candidate cuboids, and develop a conditional random field to integrate information from different sources to classify the cuboids. With this formulation, scene classification and 3D object recognition are coupled and can be jointly solved through probabilistic inference. We test the effectiveness of our approach on the challenging NYU v2 dataset. The experimental results demonstrate that through effective evidence integration and holistic reasoning, our approach achieves substantial improvement over the state-of-the-art.</p><p>2 0.91866595 <a title="201-lsi-2" href="./iccv-2013-Box_in_the_Box%3A_Joint_3D_Layout_and_Object_Reasoning_from_Single_Images.html">64 iccv-2013-Box in the Box: Joint 3D Layout and Object Reasoning from Single Images</a></p>
<p>Author: Alexander G. Schwing, Sanja Fidler, Marc Pollefeys, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly infer the room layout as well as the objects present in the scene. Towards this goal, we propose a branch and bound algorithm which is guaranteed to retrieve the global optimum of the joint problem. The main difficulty resides in taking into account occlusion in order to not over-count the evidence. We introduce a new decomposition method, which generalizes integral geometry to triangular shapes, and allows us to bound the different terms in constant time. We exploit both geometric cues and object detectors as image features and show large improvements in 2D and 3D object detection over state-of-the-art deformable part-based models.</p><p>3 0.89316815 <a title="201-lsi-3" href="./iccv-2013-Estimating_the_3D_Layout_of_Indoor_Scenes_and_Its_Clutter_from_Depth_Sensors.html">144 iccv-2013-Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors</a></p>
<p>Author: Jian Zhang, Chen Kan, Alexander G. Schwing, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly estimate the layout ofrooms as well as the clutterpresent in the scene using RGB-D data. Towards this goal, we propose an effective model that is able to exploit both depth and appearance features, which are complementary. Furthermore, our approach is efficient as we exploit the inherent decomposition of additive potentials. We demonstrate the effectiveness of our approach on the challenging NYU v2 dataset and show that employing depth reduces the layout error by 6% and the clutter estimation by 13%.</p><p>4 0.80482125 <a title="201-lsi-4" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>Author: Scott Satkin, Martial Hebert</p><p>Abstract: We present a new algorithm 3DNN (3D NearestNeighbor), which is capable of matching an image with 3D data, independently of the viewpoint from which the image was captured. By leveraging rich annotations associated with each image, our algorithm can automatically produce precise and detailed 3D models of a scene from a single image. Moreover, we can transfer information across images to accurately label and segment objects in a scene. The true benefit of 3DNN compared to a traditional 2D nearest-neighbor approach is that by generalizing across viewpoints, we free ourselves from the need to have training examples captured from all possible viewpoints. Thus, we are able to achieve comparable results using orders of magnitude less data, and recognize objects from never-beforeseen viewpoints. In this work, we describe the 3DNN algorithm and rigorously evaluate its performance for the tasks of geometry estimation and object detection/segmentation. By decoupling the viewpoint and the geometry of an image, we develop a scene matching approach which is truly 100% viewpoint invariant, yielding state-of-the-art performance on challenging data.</p><p>5 0.80013114 <a title="201-lsi-5" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<p>Author: Jiyan Pan, Takeo Kanade</p><p>Abstract: Objects in a real world image cannot have arbitrary appearance, sizes and locations due to geometric constraints in 3D space. Such a 3D geometric context plays an important role in resolving visual ambiguities and achieving coherent object detection. In this paper, we develop a RANSAC-CRF framework to detect objects that are geometrically coherent in the 3D world. Different from existing methods, we propose a novel generalized RANSAC algorithm to generate global 3D geometry hypothesesfrom local entities such that outlier suppression and noise reduction is achieved simultaneously. In addition, we evaluate those hypotheses using a CRF which considers both the compatibility of individual objects under global 3D geometric context and the compatibility between adjacent objects under local 3D geometric context. Experiment results show that our approach compares favorably with the state of the art.</p><p>6 0.78799713 <a title="201-lsi-6" href="./iccv-2013-3D_Scene_Understanding_by_Voxel-CRF.html">2 iccv-2013-3D Scene Understanding by Voxel-CRF</a></p>
<p>7 0.77949107 <a title="201-lsi-7" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>8 0.72094309 <a title="201-lsi-8" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>9 0.69875413 <a title="201-lsi-9" href="./iccv-2013-Active_MAP_Inference_in_CRFs_for_Efficient_Semantic_Segmentation.html">42 iccv-2013-Active MAP Inference in CRFs for Efficient Semantic Segmentation</a></p>
<p>10 0.66259468 <a title="201-lsi-10" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>11 0.64597028 <a title="201-lsi-11" href="./iccv-2013-Characterizing_Layouts_of_Outdoor_Scenes_Using_Spatial_Topic_Processes.html">72 iccv-2013-Characterizing Layouts of Outdoor Scenes Using Spatial Topic Processes</a></p>
<p>12 0.63392353 <a title="201-lsi-12" href="./iccv-2013-Data-Driven_3D_Primitives_for_Single_Image_Understanding.html">102 iccv-2013-Data-Driven 3D Primitives for Single Image Understanding</a></p>
<p>13 0.63309795 <a title="201-lsi-13" href="./iccv-2013-Detecting_Avocados_to_Zucchinis%3A_What_Have_We_Done%2C_and_Where_Are_We_Going%3F.html">109 iccv-2013-Detecting Avocados to Zucchinis: What Have We Done, and Where Are We Going?</a></p>
<p>14 0.62436849 <a title="201-lsi-14" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>15 0.62106931 <a title="201-lsi-15" href="./iccv-2013-Scene_Collaging%3A_Analysis_and_Synthesis_of_Natural_Images_with_Semantic_Layers.html">375 iccv-2013-Scene Collaging: Analysis and Synthesis of Natural Images with Semantic Layers</a></p>
<p>16 0.5926708 <a title="201-lsi-16" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>17 0.59146911 <a title="201-lsi-17" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<p>18 0.585585 <a title="201-lsi-18" href="./iccv-2013-Sequential_Bayesian_Model_Update_under_Structured_Scene_Prior_for_Semantic_Road_Scenes_Labeling.html">386 iccv-2013-Sequential Bayesian Model Update under Structured Scene Prior for Semantic Road Scenes Labeling</a></p>
<p>19 0.58533007 <a title="201-lsi-19" href="./iccv-2013-Decomposing_Bag_of_Words_Histograms.html">104 iccv-2013-Decomposing Bag of Words Histograms</a></p>
<p>20 0.57862121 <a title="201-lsi-20" href="./iccv-2013-Allocentric_Pose_Estimation.html">46 iccv-2013-Allocentric Pose Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.06), (7, 0.013), (12, 0.014), (26, 0.085), (31, 0.074), (34, 0.024), (40, 0.023), (42, 0.12), (55, 0.017), (64, 0.049), (73, 0.031), (88, 0.159), (89, 0.231), (98, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92943972 <a title="201-lda-1" href="./iccv-2013-Simultaneous_Clustering_and_Tracklet_Linking_for_Multi-face_Tracking_in_Videos.html">393 iccv-2013-Simultaneous Clustering and Tracklet Linking for Multi-face Tracking in Videos</a></p>
<p>Author: Baoyuan Wu, Siwei Lyu, Bao-Gang Hu, Qiang Ji</p><p>Abstract: We describe a novel method that simultaneously clusters and associates short sequences of detected faces (termed as face tracklets) in videos. The rationale of our method is that face tracklet clustering and linking are related problems that can benefit from the solutions of each other. Our method is based on a hidden Markov random field model that represents the joint dependencies of cluster labels and tracklet linking associations . We provide an efficient algorithm based on constrained clustering and optimal matching for the simultaneous inference of cluster labels and tracklet associations. We demonstrate significant improvements on the state-of-the-art results in face tracking and clustering performances on several video datasets.</p><p>2 0.9051401 <a title="201-lda-2" href="./iccv-2013-A_Non-parametric_Bayesian_Network_Prior_of_Human_Pose.html">24 iccv-2013-A Non-parametric Bayesian Network Prior of Human Pose</a></p>
<p>Author: Andreas M. Lehrmann, Peter V. Gehler, Sebastian Nowozin</p><p>Abstract: Having a sensible prior of human pose is a vital ingredient for many computer vision applications, including tracking and pose estimation. While the application of global non-parametric approaches and parametric models has led to some success, finding the right balance in terms of flexibility and tractability, as well as estimating model parameters from data has turned out to be challenging. In this work, we introduce a sparse Bayesian network model of human pose that is non-parametric with respect to the estimation of both its graph structure and its local distributions. We describe an efficient sampling scheme for our model and show its tractability for the computation of exact log-likelihoods. We empirically validate our approach on the Human 3.6M dataset and demonstrate superior performance to global models and parametric networks. We further illustrate our model’s ability to represent and compose poses not present in the training set (compositionality) and describe a speed-accuracy trade-off that allows realtime scoring of poses.</p><p>same-paper 3 0.90325087 <a title="201-lda-3" href="./iccv-2013-Holistic_Scene_Understanding_for_3D_Object_Detection_with_RGBD_Cameras.html">201 iccv-2013-Holistic Scene Understanding for 3D Object Detection with RGBD Cameras</a></p>
<p>Author: Dahua Lin, Sanja Fidler, Raquel Urtasun</p><p>Abstract: In this paper, we tackle the problem of indoor scene understanding using RGBD data. Towards this goal, we propose a holistic approach that exploits 2D segmentation, 3D geometry, as well as contextual relations between scenes and objects. Specifically, we extend the CPMC [3] framework to 3D in order to generate candidate cuboids, and develop a conditional random field to integrate information from different sources to classify the cuboids. With this formulation, scene classification and 3D object recognition are coupled and can be jointly solved through probabilistic inference. We test the effectiveness of our approach on the challenging NYU v2 dataset. The experimental results demonstrate that through effective evidence integration and holistic reasoning, our approach achieves substantial improvement over the state-of-the-art.</p><p>4 0.87297392 <a title="201-lda-4" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>Author: Hongyi Zhang, Andreas Geiger, Raquel Urtasun</p><p>Abstract: In this paper, we are interested in understanding the semantics of outdoor scenes in the context of autonomous driving. Towards this goal, we propose a generative model of 3D urban scenes which is able to reason not only about the geometry and objects present in the scene, but also about the high-level semantics in the form of traffic patterns. We found that a small number of patterns is sufficient to model the vast majority of traffic scenes and show how these patterns can be learned. As evidenced by our experiments, this high-level reasoning significantly improves the overall scene estimation as well as the vehicle-to-lane association when compared to state-of-the-art approaches [10].</p><p>5 0.86653614 <a title="201-lda-5" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>Author: Xiaoyu Wang, Ming Yang, Shenghuo Zhu, Yuanqing Lin</p><p>Abstract: Generic object detection is confronted by dealing with different degrees of variations in distinct object classes with tractable computations, which demands for descriptive and flexible object representations that are also efficient to evaluate for many locations. In view of this, we propose to model an object class by a cascaded boosting classifier which integrates various types of features from competing local regions, named as regionlets. A regionlet is a base feature extraction region defined proportionally to a detection window at an arbitrary resolution (i.e. size and aspect ratio). These regionlets are organized in small groups with stable relative positions to delineate fine-grained spatial layouts inside objects. Their features are aggregated to a one-dimensional feature within one group so as to tolerate deformations. Then we evaluate the object bounding box proposal in selective search from segmentation cues, limiting the evaluation locations to thousands. Our approach significantly outperforms the state-of-the-art on popular multi-class detection benchmark datasets with a single method, without any contexts. It achieves the detec- tion mean average precision of 41. 7% on the PASCAL VOC 2007 dataset and 39. 7% on the VOC 2010 for 20 object categories. It achieves 14. 7% mean average precision on the ImageNet dataset for 200 object categories, outperforming the latest deformable part-based model (DPM) by 4. 7%.</p><p>6 0.86567456 <a title="201-lda-6" href="./iccv-2013-Volumetric_Semantic_Segmentation_Using_Pyramid_Context_Features.html">447 iccv-2013-Volumetric Semantic Segmentation Using Pyramid Context Features</a></p>
<p>7 0.8655473 <a title="201-lda-7" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>8 0.86488605 <a title="201-lda-8" href="./iccv-2013-Building_Part-Based_Object_Detectors_via_3D_Geometry.html">66 iccv-2013-Building Part-Based Object Detectors via 3D Geometry</a></p>
<p>9 0.8628 <a title="201-lda-9" href="./iccv-2013-Group_Norm_for_Learning_Structured_SVMs_with_Unstructured_Latent_Variables.html">187 iccv-2013-Group Norm for Learning Structured SVMs with Unstructured Latent Variables</a></p>
<p>10 0.86267716 <a title="201-lda-10" href="./iccv-2013-Dynamic_Probabilistic_Volumetric_Models.html">128 iccv-2013-Dynamic Probabilistic Volumetric Models</a></p>
<p>11 0.86245579 <a title="201-lda-11" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>12 0.862221 <a title="201-lda-12" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>13 0.86216551 <a title="201-lda-13" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>14 0.86143541 <a title="201-lda-14" href="./iccv-2013-PhotoOCR%3A_Reading_Text_in_Uncontrolled_Conditions.html">315 iccv-2013-PhotoOCR: Reading Text in Uncontrolled Conditions</a></p>
<p>15 0.86137766 <a title="201-lda-15" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>16 0.86134303 <a title="201-lda-16" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>17 0.86119664 <a title="201-lda-17" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>18 0.86091805 <a title="201-lda-18" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>19 0.86075962 <a title="201-lda-19" href="./iccv-2013-Scene_Collaging%3A_Analysis_and_Synthesis_of_Natural_Images_with_Semantic_Layers.html">375 iccv-2013-Scene Collaging: Analysis and Synthesis of Natural Images with Semantic Layers</a></p>
<p>20 0.86038017 <a title="201-lda-20" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
