<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-116" href="#">iccv2013-116</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</h1>
<br/><p>Source: <a title="iccv-2013-116-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Wang_Directed_Acyclic_Graph_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Ling Wang, Hichem Sahbi</p><p>Abstract: One of the trends of action recognition consists in extracting and comparing mid-level features which encode visual and motion aspects of objects into scenes. However, when scenes contain high-level semantic actions with many interacting parts, these mid-level features are not sufficient to capture high level structures as well as high order causal relationships between moving objects resulting into a clear drop in performances. In this paper, we address this issue and we propose an alternative action recognition method based on a novel graph kernel. In the main contributions of this work, we first describe actions in videos using directed acyclic graphs (DAGs), that naturally encode pairwise interactions between moving object parts, and then we compare these DAGs by analyzing the spectrum of their sub-patterns that capture complex higher order interactions. This extraction and comparison process is computationally tractable, re- sulting from the acyclic property of DAGs, and it also defines a positive semi-definite kernel. When plugging the latter into support vector machines, we obtain an action recognition algorithm that overtakes related work, including graph-based methods, on a standard evaluation dataset.</p><p>Reference: <a title="iccv-2013-116-reference" href="../iccv2013_reference/iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('walk', 0.387), ('dag', 0.354), ('grwk', 0.295), ('kernel', 0.269), ('traject', 0.267), ('acyc', 0.261), ('nod', 0.208), ('graph', 0.178), ('ke', 0.13), ('srwk', 0.118), ('act', 0.109), ('compon', 0.1), ('undirect', 0.098), ('famy', 0.088), ('video', 0.084), ('tens', 0.076), ('kick', 0.073), ('barrault', 0.059), ('hichem', 0.059), ('kjw', 0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="116-tfidf-1" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>Author: Ling Wang, Hichem Sahbi</p><p>Abstract: One of the trends of action recognition consists in extracting and comparing mid-level features which encode visual and motion aspects of objects into scenes. However, when scenes contain high-level semantic actions with many interacting parts, these mid-level features are not sufficient to capture high level structures as well as high order causal relationships between moving objects resulting into a clear drop in performances. In this paper, we address this issue and we propose an alternative action recognition method based on a novel graph kernel. In the main contributions of this work, we first describe actions in videos using directed acyclic graphs (DAGs), that naturally encode pairwise interactions between moving object parts, and then we compare these DAGs by analyzing the spectrum of their sub-patterns that capture complex higher order interactions. This extraction and comparison process is computationally tractable, re- sulting from the acyclic property of DAGs, and it also defines a positive semi-definite kernel. When plugging the latter into support vector machines, we obtain an action recognition algorithm that overtakes related work, including graph-based methods, on a standard evaluation dataset.</p><p>2 0.25399661 <a title="116-tfidf-2" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>Author: Jiaming Guo, Zhuwen Li, Loong-Fah Cheong, Steven Zhiying Zhou</p><p>Abstract: Given a pair of videos having a common action, our goal is to simultaneously segment this pair of videos to extract this common action. As a preprocessing step, we first remove background trajectories by a motion-based figureground segmentation. To remove the remaining background and those extraneous actions, we propose the trajectory cosaliency measure, which captures the notion that trajectories recurring in all the videos should have their mutual saliency boosted. This requires a trajectory matching process which can compare trajectories with different lengths and not necessarily spatiotemporally aligned, and yet be discriminative enough despite significant intra-class variation in the common action. We further leverage the graph matching to enforce geometric coherence between regions so as to reduce feature ambiguity and matching errors. Finally, to classify the trajectories into common action and action outliers, we formulate the problem as a binary labeling of a Markov Random Field, in which the data term is measured by the trajectory co-saliency and the smooth- ness term is measured by the spatiotemporal consistency between trajectories. To evaluate the performance of our framework, we introduce a dataset containing clips that have animal actions as well as human actions. Experimental results show that the proposed method performs well in common action extraction.</p><p>3 0.2413339 <a title="116-tfidf-3" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>Author: Kevin Tang, Bangpeng Yao, Li Fei-Fei, Daphne Koller</p><p>Abstract: In this paper, we tackle the problem of combining features extracted from video for complex event recognition. Feature combination is an especially relevant task in video data, as there are many features we can extract, ranging from image features computed from individual frames to video features that take temporal information into account. To combine features effectively, we propose a method that is able to be selective of different subsets of features, as some features or feature combinations may be uninformative for certain classes. We introduce a hierarchical method for combining features based on the AND/OR graph structure, where nodes in the graph represent combinations of different sets of features. Our method automatically learns the structure of the AND/OR graph using score-based structure learning, and we introduce an inference procedure that is able to efficiently compute structure scores. We present promising results and analysis on the difficult and large-scale 2011 TRECVID Multimedia Event Detection dataset [17].</p><p>4 0.20951304 <a title="116-tfidf-4" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>Author: Heng Wang, Cordelia Schmid</p><p>Abstract: Recently dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets. This paper improves their performance by taking into account camera motion to correct them. To estimate camera motion, we match feature points between frames using SURF descriptors and dense optical flow, which are shown to be complementary. These matches are, then, used to robustly estimate a homography with RANSAC. Human motion is in general different from camera motion and generates inconsistent matches. To improve the estimation, a human detector is employed to remove these matches. Given the estimated camera motion, we remove trajectories consistent with it. We also use this estimation to cancel out camera motion from the optical flow. This significantly improves motion-based descriptors, such as HOF and MBH. Experimental results onfour challenging action datasets (i.e., Hollywood2, HMDB51, Olympic Sports and UCF50) significantly outperform the current state of the art.</p><p>5 0.20583367 <a title="116-tfidf-5" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>Author: Ali Elqursh, Ahmed Elgammal</p><p>Abstract: The vast majority of work on motion segmentation adopts the affine camera model due to its simplicity. Under the affine model, the motion segmentation problem becomes that of subspace separation. Due to this assumption, such methods are mainly offline and exhibit poor performance when the assumption is not satisfied. This is made evident in state-of-the-art methods that relax this assumption by using piecewise affine spaces and spectral clustering techniques to achieve better results. In this paper, we formulate the problem of motion segmentation as that of manifold separation. We then show how label propagation can be used in an online framework to achieve manifold separation. The performance of our framework is evaluated on a benchmark dataset and achieves competitive performance while being online.</p><p>6 0.1980194 <a title="116-tfidf-6" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>7 0.18851811 <a title="116-tfidf-7" href="./iccv-2013-Camera_Alignment_Using_Trajectory_Intersections_in_Unsynchronized_Videos.html">68 iccv-2013-Camera Alignment Using Trajectory Intersections in Unsynchronized Videos</a></p>
<p>8 0.17389689 <a title="116-tfidf-8" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>9 0.17385043 <a title="116-tfidf-9" href="./iccv-2013-Improving_Graph_Matching_via_Density_Maximization.html">214 iccv-2013-Improving Graph Matching via Density Maximization</a></p>
<p>10 0.16527028 <a title="116-tfidf-10" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>11 0.16302992 <a title="116-tfidf-11" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>12 0.15812203 <a title="116-tfidf-12" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>13 0.15366676 <a title="116-tfidf-13" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>14 0.13999446 <a title="116-tfidf-14" href="./iccv-2013-Discriminative_Label_Propagation_for_Multi-object_Tracking_with_Sporadic_Appearance_Features.html">120 iccv-2013-Discriminative Label Propagation for Multi-object Tracking with Sporadic Appearance Features</a></p>
<p>15 0.13244185 <a title="116-tfidf-15" href="./iccv-2013-Learning_Graph_Matching%3A_Oriented_to_Category_Modeling_from_Cluttered_Scenes.html">237 iccv-2013-Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes</a></p>
<p>16 0.13241848 <a title="116-tfidf-16" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>17 0.12998541 <a title="116-tfidf-17" href="./iccv-2013-Modeling_Occlusion_by_Discriminative_AND-OR_Structures.html">269 iccv-2013-Modeling Occlusion by Discriminative AND-OR Structures</a></p>
<p>18 0.11615791 <a title="116-tfidf-18" href="./iccv-2013-Event_Detection_in_Complex_Scenes_Using_Interval_Temporal_Constraints.html">146 iccv-2013-Event Detection in Complex Scenes Using Interval Temporal Constraints</a></p>
<p>19 0.11551882 <a title="116-tfidf-19" href="./iccv-2013-Measuring_Flow_Complexity_in_Videos.html">263 iccv-2013-Measuring Flow Complexity in Videos</a></p>
<p>20 0.11354317 <a title="116-tfidf-20" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.231), (1, -0.029), (2, 0.007), (3, -0.144), (4, 0.085), (5, 0.015), (6, 0.147), (7, 0.225), (8, -0.075), (9, 0.021), (10, -0.075), (11, -0.038), (12, -0.003), (13, -0.157), (14, -0.112), (15, 0.121), (16, -0.077), (17, 0.077), (18, -0.072), (19, -0.001), (20, 0.052), (21, -0.021), (22, 0.087), (23, 0.018), (24, 0.075), (25, 0.009), (26, -0.024), (27, -0.002), (28, 0.026), (29, -0.02), (30, -0.094), (31, -0.053), (32, 0.049), (33, 0.101), (34, -0.123), (35, 0.067), (36, -0.041), (37, 0.035), (38, 0.013), (39, -0.022), (40, 0.02), (41, -0.131), (42, 0.013), (43, -0.047), (44, -0.088), (45, 0.049), (46, -0.01), (47, -0.012), (48, -0.021), (49, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94066375 <a title="116-lsi-1" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>Author: Ling Wang, Hichem Sahbi</p><p>Abstract: One of the trends of action recognition consists in extracting and comparing mid-level features which encode visual and motion aspects of objects into scenes. However, when scenes contain high-level semantic actions with many interacting parts, these mid-level features are not sufficient to capture high level structures as well as high order causal relationships between moving objects resulting into a clear drop in performances. In this paper, we address this issue and we propose an alternative action recognition method based on a novel graph kernel. In the main contributions of this work, we first describe actions in videos using directed acyclic graphs (DAGs), that naturally encode pairwise interactions between moving object parts, and then we compare these DAGs by analyzing the spectrum of their sub-patterns that capture complex higher order interactions. This extraction and comparison process is computationally tractable, re- sulting from the acyclic property of DAGs, and it also defines a positive semi-definite kernel. When plugging the latter into support vector machines, we obtain an action recognition algorithm that overtakes related work, including graph-based methods, on a standard evaluation dataset.</p><p>2 0.78634644 <a title="116-lsi-2" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>Author: Kevin Tang, Bangpeng Yao, Li Fei-Fei, Daphne Koller</p><p>Abstract: In this paper, we tackle the problem of combining features extracted from video for complex event recognition. Feature combination is an especially relevant task in video data, as there are many features we can extract, ranging from image features computed from individual frames to video features that take temporal information into account. To combine features effectively, we propose a method that is able to be selective of different subsets of features, as some features or feature combinations may be uninformative for certain classes. We introduce a hierarchical method for combining features based on the AND/OR graph structure, where nodes in the graph represent combinations of different sets of features. Our method automatically learns the structure of the AND/OR graph using score-based structure learning, and we introduce an inference procedure that is able to efficiently compute structure scores. We present promising results and analysis on the difficult and large-scale 2011 TRECVID Multimedia Event Detection dataset [17].</p><p>3 0.67669117 <a title="116-lsi-3" href="./iccv-2013-Camera_Alignment_Using_Trajectory_Intersections_in_Unsynchronized_Videos.html">68 iccv-2013-Camera Alignment Using Trajectory Intersections in Unsynchronized Videos</a></p>
<p>Author: Thomas Kuo, Santhoshkumar Sunderrajan, B.S. Manjunath</p><p>Abstract: This paper addresses the novel and challenging problem of aligning camera views that are unsynchronized by low and/or variable frame rates using object trajectories. Unlike existing trajectory-based alignment methods, our method does not require frame-to-frame synchronization. Instead, we propose using the intersections of corresponding object trajectories to match views. To find these intersections, we introduce a novel trajectory matching algorithm based on matching Spatio-Temporal Context Graphs (STCGs). These graphs represent the distances between trajectories in time and space within a view, and are matched to an STCG from another view to find the corresponding trajectories. To the best of our knowledge, this is one of the first attempts to align views that are unsynchronized with variable frame rates. The results on simulated and real-world datasets show trajectory intersections area viablefeatureforcamera alignment, and that the trajectory matching method performs well in real-world scenarios.</p><p>4 0.66198814 <a title="116-lsi-4" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>Author: Jiaming Guo, Zhuwen Li, Loong-Fah Cheong, Steven Zhiying Zhou</p><p>Abstract: Given a pair of videos having a common action, our goal is to simultaneously segment this pair of videos to extract this common action. As a preprocessing step, we first remove background trajectories by a motion-based figureground segmentation. To remove the remaining background and those extraneous actions, we propose the trajectory cosaliency measure, which captures the notion that trajectories recurring in all the videos should have their mutual saliency boosted. This requires a trajectory matching process which can compare trajectories with different lengths and not necessarily spatiotemporally aligned, and yet be discriminative enough despite significant intra-class variation in the common action. We further leverage the graph matching to enforce geometric coherence between regions so as to reduce feature ambiguity and matching errors. Finally, to classify the trajectories into common action and action outliers, we formulate the problem as a binary labeling of a Markov Random Field, in which the data term is measured by the trajectory co-saliency and the smooth- ness term is measured by the spatiotemporal consistency between trajectories. To evaluate the performance of our framework, we introduce a dataset containing clips that have animal actions as well as human actions. Experimental results show that the proposed method performs well in common action extraction.</p><p>5 0.59679723 <a title="116-lsi-5" href="./iccv-2013-Improving_Graph_Matching_via_Density_Maximization.html">214 iccv-2013-Improving Graph Matching via Density Maximization</a></p>
<p>Author: Chao Wang, Lei Wang, Lingqiao Liu</p><p>Abstract: Graph matching has been widely used in various applications in computer vision due to its powerful performance. However, it poses three challenges to image sparse feature matching: (1) The combinatorial nature limits the size of the possible matches; (2) It is sensitive to outliers because the objective function prefers more matches; (3) It works poorly when handling many-to-many object correspondences, due to its assumption of one single cluster for each graph. In this paper, we address these problems with a unified framework—Density Maximization. We propose a graph density local estimator (퐷퐿퐸) to measure the quality of matches. Density Maximization aims to maximize the 퐷퐿퐸 values both locally and globally. The local maximization of 퐷퐿퐸 finds the clusters of nodes as well as eliminates the outliers. The global maximization of 퐷퐿퐸 efficiently refines the matches by exploring a much larger matching space. Our Density Maximization is orthogonal to specific graph matching algorithms. Experimental evaluation demonstrates that it significantly boosts the true matches and enables graph matching to handle both outliers and many-to-many object correspondences.</p><p>6 0.5783444 <a title="116-lsi-6" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>7 0.57608485 <a title="116-lsi-7" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>8 0.57557982 <a title="116-lsi-8" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>9 0.57449347 <a title="116-lsi-9" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>10 0.56989592 <a title="116-lsi-10" href="./iccv-2013-Joint_Optimization_for_Consistent_Multiple_Graph_Matching.html">224 iccv-2013-Joint Optimization for Consistent Multiple Graph Matching</a></p>
<p>11 0.56654763 <a title="116-lsi-11" href="./iccv-2013-Discriminative_Label_Propagation_for_Multi-object_Tracking_with_Sporadic_Appearance_Features.html">120 iccv-2013-Discriminative Label Propagation for Multi-object Tracking with Sporadic Appearance Features</a></p>
<p>12 0.55473644 <a title="116-lsi-12" href="./iccv-2013-A_Fully_Hierarchical_Approach_for_Finding_Correspondences_in_Non-rigid_Shapes.html">11 iccv-2013-A Fully Hierarchical Approach for Finding Correspondences in Non-rigid Shapes</a></p>
<p>13 0.53891045 <a title="116-lsi-13" href="./iccv-2013-Feature_Weighting_via_Optimal_Thresholding_for_Video_Analysis.html">163 iccv-2013-Feature Weighting via Optimal Thresholding for Video Analysis</a></p>
<p>14 0.53529459 <a title="116-lsi-14" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>15 0.53446245 <a title="116-lsi-15" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>16 0.53052431 <a title="116-lsi-16" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>17 0.52317882 <a title="116-lsi-17" href="./iccv-2013-Learning_Graph_Matching%3A_Oriented_to_Category_Modeling_from_Cluttered_Scenes.html">237 iccv-2013-Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes</a></p>
<p>18 0.5186938 <a title="116-lsi-18" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>19 0.51724881 <a title="116-lsi-19" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>20 0.50466216 <a title="116-lsi-20" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.116), (20, 0.059), (25, 0.038), (42, 0.107), (48, 0.145), (50, 0.024), (55, 0.022), (60, 0.029), (77, 0.069), (87, 0.296), (89, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.70618856 <a title="116-lda-1" href="./iccv-2013-Perceptual_Fidelity_Aware_Mean_Squared_Error.html">312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</a></p>
<p>Author: Wufeng Xue, Xuanqin Mou, Lei Zhang, Xiangchu Feng</p><p>Abstract: How to measure the perceptual quality of natural images is an important problem in low level vision. It is known that the Mean Squared Error (MSE) is not an effective index to describe the perceptual fidelity of images. Numerous perceptual fidelity indices have been developed, while the representatives include the Structural SIMilarity (SSIM) index and its variants. However, most of those perceptual measures are nonlinear, and they cannot be easily adopted as an objective function to minimize in various low level vision tasks. Can MSE be perceptual fidelity aware after some minor adaptation ? In this paper we propose a simple framework to enhance the perceptual fidelity awareness of MSE by introducing an l2-norm structural error term to it. Such a Structural MSE (SMSE) can lead to very competitive image quality assessment (IQA) results. More surprisingly, we show that by using certain structure extractors, SMSE can befurther turned into a Gaussian smoothed MSE (i.e., the Euclidean distance between the original and distorted images after Gaussian , smooth filtering), which is much simpler to calculate but achieves rather better IQA performance than SSIM. The socalled Perceptual-fidelity Aware MSE (PAMSE) can have great potentials in applications such as perceptual image coding and perceptual image restoration.</p><p>same-paper 2 0.69049585 <a title="116-lda-2" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>Author: Ling Wang, Hichem Sahbi</p><p>Abstract: One of the trends of action recognition consists in extracting and comparing mid-level features which encode visual and motion aspects of objects into scenes. However, when scenes contain high-level semantic actions with many interacting parts, these mid-level features are not sufficient to capture high level structures as well as high order causal relationships between moving objects resulting into a clear drop in performances. In this paper, we address this issue and we propose an alternative action recognition method based on a novel graph kernel. In the main contributions of this work, we first describe actions in videos using directed acyclic graphs (DAGs), that naturally encode pairwise interactions between moving object parts, and then we compare these DAGs by analyzing the spectrum of their sub-patterns that capture complex higher order interactions. This extraction and comparison process is computationally tractable, re- sulting from the acyclic property of DAGs, and it also defines a positive semi-definite kernel. When plugging the latter into support vector machines, we obtain an action recognition algorithm that overtakes related work, including graph-based methods, on a standard evaluation dataset.</p><p>3 0.6140523 <a title="116-lda-3" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>Author: Aleksandr V. Segal, Ian Reid</p><p>Abstract: We propose a novel parametrization of the data association problem for multi-target tracking. In our formulation, the number of targets is implicitly inferred together with the data association, effectively solving data association and model selection as a single inference problem. The novel formulation allows us to interpret data association and tracking as a single Switching Linear Dynamical System (SLDS). We compute an approximate posterior solution to this problem using a dynamic programming/message passing technique. This inference-based approach allows us to incorporate richer probabilistic models into the tracking system. In particular, we incorporate inference over inliers/outliers and track termination times into the system. We evaluate our approach on publicly available datasets and demonstrate results competitive with, and in some cases exceeding the state of the art.</p><p>4 0.60570836 <a title="116-lda-4" href="./iccv-2013-Alternating_Regression_Forests_for_Object_Detection_and_Pose_Estimation.html">47 iccv-2013-Alternating Regression Forests for Object Detection and Pose Estimation</a></p>
<p>Author: Samuel Schulter, Christian Leistner, Paul Wohlhart, Peter M. Roth, Horst Bischof</p><p>Abstract: We present Alternating Regression Forests (ARFs), a novel regression algorithm that learns a Random Forest by optimizing a global loss function over all trees. This interrelates the information of single trees during the training phase and results in more accurate predictions. ARFs can minimize any differentiable regression loss without sacrificing the appealing properties of Random Forests, like low computational complexity during both, training and testing. Inspired by recent developments for classification [19], we derive a new algorithm capable of dealing with different regression loss functions, discuss its properties and investigate the relations to other methods like Boosted Trees. We evaluate ARFs on standard machine learning benchmarks, where we observe better generalization power compared to both standard Random Forests and Boosted Trees. Moreover, we apply the proposed regressor to two computer vision applications: object detection and head pose estimation from depth images. ARFs outperform the Random Forest baselines in both tasks, illustrating the importance of optimizing a common loss function for all trees.</p><p>5 0.60533023 <a title="116-lda-5" href="./iccv-2013-Online_Robust_Non-negative_Dictionary_Learning_for_Visual_Tracking.html">298 iccv-2013-Online Robust Non-negative Dictionary Learning for Visual Tracking</a></p>
<p>Author: Naiyan Wang, Jingdong Wang, Dit-Yan Yeung</p><p>Abstract: This paper studies the visual tracking problem in video sequences and presents a novel robust sparse tracker under the particle filter framework. In particular, we propose an online robust non-negative dictionary learning algorithm for updating the object templates so that each learned template can capture a distinctive aspect of the tracked object. Another appealing property of this approach is that it can automatically detect and reject the occlusion and cluttered background in a principled way. In addition, we propose a new particle representation formulation using the Huber loss function. The advantage is that it can yield robust estimation without using trivial templates adopted by previous sparse trackers, leading to faster computation. We also reveal the equivalence between this new formulation and the previous one which uses trivial templates. The proposed tracker is empirically compared with state-of-the-art trackers on some challenging video sequences. Both quantitative and qualitative comparisons show that our proposed tracker is superior and more stable.</p><p>6 0.60516548 <a title="116-lda-6" href="./iccv-2013-Video_Motion_for_Every_Visible_Point.html">441 iccv-2013-Video Motion for Every Visible Point</a></p>
<p>7 0.60512334 <a title="116-lda-7" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>8 0.60299635 <a title="116-lda-8" href="./iccv-2013-Uncertainty-Driven_Efficiently-Sampled_Sparse_Graphical_Models_for_Concurrent_Tumor_Segmentation_and_Atlas_Registration.html">432 iccv-2013-Uncertainty-Driven Efficiently-Sampled Sparse Graphical Models for Concurrent Tumor Segmentation and Atlas Registration</a></p>
<p>9 0.60081702 <a title="116-lda-9" href="./iccv-2013-Slice_Sampling_Particle_Belief_Propagation.html">395 iccv-2013-Slice Sampling Particle Belief Propagation</a></p>
<p>10 0.60078114 <a title="116-lda-10" href="./iccv-2013-How_Related_Exemplars_Help_Complex_Event_Detection_in_Web_Videos%3F.html">203 iccv-2013-How Related Exemplars Help Complex Event Detection in Web Videos?</a></p>
<p>11 0.60032517 <a title="116-lda-11" href="./iccv-2013-Measuring_Flow_Complexity_in_Videos.html">263 iccv-2013-Measuring Flow Complexity in Videos</a></p>
<p>12 0.60032111 <a title="116-lda-12" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>13 0.59931821 <a title="116-lda-13" href="./iccv-2013-PhotoOCR%3A_Reading_Text_in_Uncontrolled_Conditions.html">315 iccv-2013-PhotoOCR: Reading Text in Uncontrolled Conditions</a></p>
<p>14 0.59880257 <a title="116-lda-14" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>15 0.59823477 <a title="116-lda-15" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>16 0.59787494 <a title="116-lda-16" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>17 0.5977245 <a title="116-lda-17" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>18 0.59703666 <a title="116-lda-18" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>19 0.59642059 <a title="116-lda-19" href="./iccv-2013-Constant_Time_Weighted_Median_Filtering_for_Stereo_Matching_and_Beyond.html">88 iccv-2013-Constant Time Weighted Median Filtering for Stereo Matching and Beyond</a></p>
<p>20 0.59626037 <a title="116-lda-20" href="./iccv-2013-Shortest_Paths_with_Curvature_and_Torsion.html">389 iccv-2013-Shortest Paths with Curvature and Torsion</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
