<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-50" href="#">iccv2013-50</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</h1>
<br/><p>Source: <a title="iccv-2013-50-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Borji_Analysis_of_Scores_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Ali Borji, Hamed R. Tavakoli, Dicky N. Sihite, Laurent Itti</p><p>Abstract: Significant recent progress has been made in developing high-quality saliency models. However, less effort has been undertaken on fair assessment of these models, over large standardized datasets and correctly addressing confounding factors. In this study, we pursue a critical and quantitative look at challenges (e.g., center-bias, map smoothing) in saliency modeling and the way they affect model accuracy. We quantitatively compare 32 state-of-the-art models (using the shuffled AUC score to discount center-bias) on 4 benchmark eye movement datasets, for prediction of human fixation locations and scanpath sequence. We also account for the role of map smoothing. We find that, although model rankings vary, some (e.g., AWS, LG, AIM, and HouNIPS) consistently outperform other models over all datasets. Some models work well for prediction of both fixation locations and scanpath sequence (e.g., Judd, GBVS). Our results show low prediction accuracy for models over emotional stimuli from the NUSEF dataset. Our last benchmark, for the first time, gauges the ability of models to decode the stimulus category from statistics of fixations, saccades, and model saliency values at fixated locations. In this test, ITTI and AIM models win over other models. Our benchmark provides a comprehensive high-level picture of the strengths and weaknesses of many popular models, and suggests future research directions in saliency modeling.</p><p>Reference: <a title="iccv-2013-50-reference" href="../iccv2013_reference/iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Analysis of scores, datasets, and models in visual saliency prediction Ali Borji† Hamed R. [sent-1, score-0.68]
</p><p>2 Sihite† Laurent Itti† † Department of Computer Science, University of Southern California, Los Angeles Center for Machine Vision Research, University of Oulu, Finland  +  Abstract Significant recent progress has been made in developing high-quality saliency models. [sent-3, score-0.577]
</p><p>3 , center-bias, map smoothing) in saliency modeling and the way they affect model accuracy. [sent-7, score-0.622]
</p><p>4 We quantitatively compare 32 state-of-the-art models (using the shuffled AUC score to discount center-bias) on 4 benchmark eye movement datasets, for prediction of human fixation locations and scanpath sequence. [sent-8, score-0.846]
</p><p>5 Some models work well for prediction of both fixation locations and scanpath sequence (e. [sent-13, score-0.59]
</p><p>6 Our results show low prediction accuracy for models over emotional stimuli from the NUSEF dataset. [sent-16, score-0.351]
</p><p>7 Our last benchmark, for the first time, gauges the ability of models to decode the stimulus category from statistics of fixations, saccades, and model saliency values at fixated locations. [sent-17, score-0.928]
</p><p>8 Our  benchmark provides a comprehensive high-level picture of the strengths and weaknesses of many popular models, and suggests future research directions in saliency modeling. [sent-19, score-0.577]
</p><p>9 Modeling visual saliency broadens our understanding of a highly complex cognitive behavior, which may lead to subsequent findings in other areas (object and scene recognition, visual search, etc. [sent-29, score-0.657]
</p><p>10 We offer 3 main contributions: (1) discussing current challenges and directions in saliency modeling (evaluation metrics, dataset bias, model parameters,  etc. [sent-35, score-0.577]
</p><p>11 ) and proposing solutions, (2) comparing 32 models and their pros and cons in a unified quantitative framework over 4 widely-used datasets for fixation prediction (on classic and emotional stimuli) as well as scanpath prediction, and (3) stimuli/task decoding using saliency and fixation statistics. [sent-36, score-1.534]
</p><p>12 Hopefully, our study will open new directions and conversations and help better organize the saliency literature. [sent-37, score-0.577]
</p><p>13 Few attempts has been made for saliency model comparison, but their shortcomings have driven us to conduct a new benchmark considering the latest progress. [sent-39, score-0.577]
</p><p>14 We provide the latest update on saliency modeling, with the most comprehensive set of models, challenges/parameters, datasets, and measures. [sent-49, score-0.577]
</p><p>15 Saliency is a property of the perceived visual stimulus (bottom-up (BU)) or, at most, of the features that the visual system extracts from the stimulus (which can be manipulated by top-down (TD) cues). [sent-53, score-0.402]
</p><p>16 Attention is a much more general concept that depends on many cognitive factors of very high-level such as strategy for image search and interactions between saliency and search strategy, as well as subjective factors such as age and experience. [sent-54, score-0.605]
</p><p>17 Bottom-up attention is reflexive, fast, likely feed-forward, and mainly deployed by stimulus saliency (e. [sent-57, score-0.83]
</p><p>18 Visual stimuli used in neurophysiological and modeling works include: static (synthetic search arrays involving pop-out and conjunction search arrays, cartoons, or photographs) and over spatio-temporal dynamic stimuli (movies and interactive video games). [sent-62, score-0.346]
</p><p>19 These stimuli have been exploited for studying visual attention over three types of tasks: (1) free viewing, (2) visual search, and (3) interactive tasks (games or real-world tasks [54]). [sent-63, score-0.289]
</p><p>20 Similar observations indicate that visual attention is essentially guided by recognized objects, with low-level saliency contributing only indirectly [50]. [sent-68, score-0.681]
</p><p>21 A closely related field to saliency modeling is salient region detection. [sent-71, score-0.621]
</p><p>22 Evaluation is often done by measuring precision-recall of saliency maps of a model against ground truth data (explicit saliency judgments of subjects by annotating salient objects or clicking on locations). [sent-73, score-1.285]
</p><p>23 Available eye movement datasets vary on several parameters, for instance: number of images, number of viewers, viewing time per image, subject’s distance from the screen, and stimulus variety [55]. [sent-87, score-0.382]
</p><p>24 1 shows larger fixation datasets with many images and  eye-tracking subjects are needed. [sent-94, score-0.33]
</p><p>25 Further, it has been shown that fixation density maps from different laboratories differ significantly due to inter-laboratory differences and experimental conditions [56]. [sent-97, score-0.283]
</p><p>26 A difficult challenge in fixation datasets which has affected fair model comparison is “Center-Bias (CB)”, whereby humans often appear to preferentially look near an image’s center [28]. [sent-98, score-0.407]
</p><p>27 Annoyingly, due to CB in data, a trivial saliency model that just consists of a Gaussian blob at the center of the image, often scores higher than almost all saliency models [2]. [sent-100, score-1.332]
</p><p>28 This can be verified from the average eye fixation maps of 3 popular datasets (See supplement). [sent-101, score-0.413]
</p><p>29 We observed higher central fixation densities for images with objects at the center compared with those with objects off the center. [sent-102, score-0.324]
</p><p>30 975  placing a variable Gaussian (σ1) at fixated locations, 2nd column: scores of the central Gaussian blob (σ2), and the 3rd column: scores of the image with variable border size. [sent-133, score-0.311]
</p><p>31 Traditionally, saliency models have been evaluated against eye movement datasets. [sent-141, score-0.752]
</p><p>32 The saliency map is then treated as a binary classifier to separate the positive samples from negatives. [sent-144, score-0.622]
</p><p>33 Some studies have evaluated the sequence of fixations in scanpath [32, 3 1]. [sent-149, score-0.442]
</p><p>34 2 shows analysis of how the above scores are affected by smoothness of the saliency map and possible center bias in the reference data. [sent-151, score-0.747]
</p><p>35 We generated some random eye fixations (sampled from a Gaussian distribution) and made a saliency map by convolving it with a Gaussian filter with variable sigma σ1 . [sent-152, score-0.99]
</p><p>36 Shown in the 2nd column, we placed a  Gaussian at the center of the image and calculated the score again by varying the σ2 of the central Gaussian as well as σ1 of the Gaussian convolved with fixations (only for CC since for NSS and AUC, fixation positions are used). [sent-155, score-0.568]
</p><p>37 These analyses show that smoothing the saliency maps and the size of the central Gaussian affect scores and should be accounted for fair model comparison. [sent-160, score-0.839]
</p><p>38 All scores are invariant to saliency map shifting and scaling. [sent-169, score-0.692]
</p><p>39 A proper score for tackling CB is shuffled AUC (sAUC) [25] with the only difference to AUC being that instead of selecting negative points randomly, all fixations over other images are used as the negative set. [sent-171, score-0.332]
</p><p>40 , saliency from other images but at fixations of the current image), this type of AUC leads to the exact value of 0. [sent-179, score-0.789]
</p><p>41 Traditionally, intensity, orientation, and color (in LAB and RGB spaces) have been used for saliency derivation over static images. [sent-183, score-0.577]
</p><p>42 Furthermore, several other low-level features have been used to estimate saliency (size, depth, optical flow, etc. [sent-185, score-0.577]
</p><p>43 , causality, action-influence) which have been suggested to be important in guiding attention (location and fixation duration) [4]. [sent-190, score-0.325]
</p><p>44 A sample saliency map smoothed by convolving with a variable-size Gaussian kernel (for the AWS model over an image of the Toronto dataset). [sent-203, score-0.685]
</p><p>45 For a given stimulus, the human model outputs a map built by integrating fixations from other sub-  jects than the one under test while they watched that same stimulus. [sent-219, score-0.287]
</p><p>46 We resized saliency maps to the size ofthe original images onto which eye movements have been recorded. [sent-222, score-0.737]
</p><p>47 Predicting fixation locations: Model scores and average ranks using sAUC over four datasets are shown in Table 1. [sent-235, score-0.349]
</p><p>48 We smoothed saliency map of each model by convolving it with a Gaussian kernel (Fig. [sent-236, score-0.685]
</p><p>49 Therefore, we do not recommend using them for saliency model comparison. [sent-254, score-0.577]
</p><p>50 There is no significant difference over dif-  ferent categories of stimuli averaged over all models (Inset; See also supplement) although it seems that models perform better over face stimuli and the worst over portrait and nude (this pattern is more clear considering only  NUSEF- 4 12  06. [sent-258, score-0.535]
</p><p>51 Abbreviations are: M: Matlab, C: C/C++, E: Executables, S: Sent saliency maps. [sent-325, score-0.577]
</p><p>52 Numbers are maximum shuffled AUC scores of models by optimizing the saliency map smoothness (Fig. [sent-328, score-0.817]
</p><p>53 In ITTI98, each feature map’s contribution to the saliency map is weighted by the squared difference between the globally most active location and the average activity of all other local maxima in the feature map [3]. [sent-341, score-0.667]
</p><p>54 This gives rise to smooth saliency maps, which tend to correlate better with noisy human eye movement data. [sent-342, score-0.745]
</p><p>55 In the ITTI model [33], the spatial competition for saliency is much stronger, and is implemented in each feature map as 10 rounds of convolution by a large difference-of-Gaussians followed by half-wave rectification. [sent-343, score-0.622]
</p><p>56 This gives rise to much sparser saliency maps, which are more useful than the ITTI98 maps when trying to decide on the single next location to look at (e. [sent-344, score-0.643]
</p><p>57 Faces are often located at the center while nude and event stimuli are mostly off-center. [sent-367, score-0.273]
</p><p>58 A separate analysis over the Kootstra dataset showed that models have difficulty in saliency detection over nature stimuli where there are less distinctive and salient objects (See supplement). [sent-369, score-0.817]
</p><p>59 This means that much progress remains to be done in saliency detection over stimuli containing conceptual stimuli (e. [sent-370, score-0.895]
</p><p>60 Sample emotional images with positive, negative, and neutral emotional valence from NUSEF dataset along with saliency maps of the AWS model. [sent-382, score-0.843]
</p><p>61 [45] showed that initial fixations were more likely to be on emotional objects than more visually salient neutral ones. [sent-386, score-0.371]
</p><p>62 Our results (using shuffled AUC and with smoothing similar to Table 1; see supplement) suggest that only a fraction of fixations landed on emotional image regions, possibly due to bottom-up saliency (interaction between saliency and emotion; AWS on emotional = 0. [sent-391, score-1.728]
</p><p>63 Predicting scanpath: Not only humans are correlated in  of the locations they fixate, but they also agree somewhat in the order of their fixations [3 1, 32]. [sent-398, score-0.283]
</p><p>64 In the context of saliency modeling, few models have aimed to predict scanpath sequence, partly due to difficulty in measuring and quantizing scanpaths. [sent-399, score-0.846]
</p><p>65 Here, we first define a measure to quantify scanpath and then compare models in terms of their ability to generate saccades similar to human scanpath. [sent-400, score-0.335]
</p><p>66 Shortly, first for an image some clusters are derived from its human fixation map and then scanpath of each subject is coded into a string using these clusters. [sent-409, score-0.565]
</p><p>67 Then for a saliency map, an inhibition of return (IOR) mechanism is used to generate a sequence. [sent-410, score-0.577]
</p><p>68 Using NeedlemanWunsch [53] string matching algorithm, a model’s scanpath is compared against a subject’s scanpath and then the average score over all subjects is calculated (Supplement). [sent-411, score-0.524]
</p><p>69 Results of scanpath prediction over Toronto and MIT datasets are shown in Fig. [sent-412, score-0.27]
</p><p>70 To investigate the relationship between fixation location and sequence prediction, we plotted these two scores versus each other (Fig. [sent-416, score-0.349]
</p><p>71 An application of saliency modeling is to decode task (illustrated by the classic study of Yarbus [63]), stimulus category, or different populations of human subjects (e. [sent-438, score-0.893]
</p><p>72 Here, as an example we intend to decode  ××  the category of the stimulus from features augmented from statistics of fixations, saccades, and saliency at fixations. [sent-444, score-0.839]
</p><p>73 a shows decoding results using saliency maps of four models and the MEP map. [sent-474, score-0.691]
</p><p>74 Using all features (and ITTI98 saliency maps), the best decoding accuracy is 0. [sent-475, score-0.618]
</p><p>75 Fixation histogram, saliency (here MEP), and (a) Stimuuls category decodnig  (b) Performance ofd fierentf eatures  +FCfosxmaSlibc. [sent-486, score-0.604]
</p><p>76 Smoothing saliency maps affects performance and should be taken into account for fair model comparison. [sent-520, score-0.647]
</p><p>77 Finally, we showed that it is feasible to decode the stimulus category from a feature vector combined from saliency, saccade, and fixation statistics. [sent-524, score-0.509]
</p><p>78 This could be because of two reasons: similar saliency patterns across scenes of a category and/or systematic/semantic biases of fixations in each category. [sent-525, score-0.816]
</p><p>79 In this regard, it will also be interesting to test the feasibility of predicting whether a scene is natural or man-made from saliency and fixations. [sent-526, score-0.621]
</p><p>80 Some saliency models implicitly emphasize the central parts of objects (e. [sent-540, score-0.662]
</p><p>81 Explicit central object-bias may lead to even higher fixation predic2Our online challenge: https://sites. [sent-542, score-0.295]
</p><p>82 , regions with the same global saliency could have different local saliences). [sent-548, score-0.577]
</p><p>83 We showed that, from statistics of fixations, saccades, and saliency at fixations, it is possible to decode the stimulus category. [sent-549, score-0.812]
</p><p>84 Future extensions would be designing stan-  dard and more challenging decoding scenarios and considering other features such as saccade sequence (scanpath) information for better discriminating tasks, populations of patients, or stimulus category. [sent-550, score-0.369]
</p><p>85 Another promising research direction is designing better saliency evaluation scores which: (1) are able to better distinguish fixated vs. [sent-552, score-0.699]
</p><p>86 An eye fixation database for saliency detection in images. [sent-581, score-0.922]
</p><p>87 A novel multiresolution spatiotemporal saliency detection model and Its applications in image and video compression. [sent-641, score-0.577]
</p><p>88 Modeling saliency to predict gaze direction for short videos. [sent-695, score-0.669]
</p><p>89 Cottrell, SUN: A Bayesian framework for saliency using natural statistics. [sent-710, score-0.577]
</p><p>90 Probabilistic multi-task learning for visual saliency estimation in video. [sent-717, score-0.603]
</p><p>91 The central fixation bias in scene viewing: selecting an optimal viewing position independently of motor bases and image feature distributions. [sent-729, score-0.358]
</p><p>92 Methods for comparing scanpaths and saliency maps: strengths and weaknesses. [sent-735, score-0.577]
</p><p>93 Fast and efficient saliency detection using sparse sampling and kernel density estimation. [sent-765, score-0.577]
</p><p>94 Predicting human gaze using low-level saliency combined with face detection. [sent-772, score-0.665]
</p><p>95 Learning a saliency map using fixated locations in natural scenes. [sent-777, score-0.71]
</p><p>96 A unified method for comparison of algorithms of saliency extraction, SPIE, 2012. [sent-808, score-0.577]
</p><p>97 Salience of the lambs: A test of the saliency map hypothesis with pictures of emotive objects, J. [sent-823, score-0.622]
</p><p>98 Comparative evaluation of visual saliency models for quality assessment task, 6th Int. [sent-870, score-0.64]
</p><p>99 Stiefelhagen, Quaternion-based spectral saliency detection for eye fixation prediction, ECCV, 2012. [sent-908, score-0.922]
</p><p>100 Erdem, Visual saliency estimation by nonlinearly integrating features using region covariances, Journal of Vision, 13:4, 1-20, 2013. [sent-911, score-0.577]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('saliency', 0.577), ('fixation', 0.247), ('fixations', 0.212), ('auc', 0.201), ('scanpath', 0.198), ('nusef', 0.176), ('stimulus', 0.175), ('nss', 0.163), ('stimuli', 0.159), ('sauc', 0.156), ('aws', 0.142), ('saccade', 0.121), ('emotional', 0.115), ('itti', 0.11), ('supplement', 0.103), ('kootstra', 0.1), ('hounips', 0.099), ('eye', 0.098), ('judd', 0.095), ('shuffled', 0.088), ('nude', 0.085), ('toronto', 0.084), ('gauss', 0.084), ('borji', 0.079), ('attention', 0.078), ('cc', 0.076), ('scores', 0.07), ('saccades', 0.07), ('affective', 0.06), ('decode', 0.06), ('portrait', 0.058), ('gaze', 0.058), ('mep', 0.057), ('tavakoli', 0.057), ('fixated', 0.052), ('subjects', 0.051), ('gaussian', 0.049), ('central', 0.048), ('lg', 0.047), ('gbvs', 0.046), ('string', 0.045), ('map', 0.045), ('predicting', 0.044), ('smoothing', 0.044), ('salient', 0.044), ('mit', 0.043), ('yarbus', 0.043), ('blob', 0.042), ('decoding', 0.041), ('prediction', 0.04), ('behavioral', 0.04), ('chance', 0.04), ('movement', 0.04), ('ior', 0.038), ('stb', 0.038), ('bins', 0.037), ('sihite', 0.037), ('viewing', 0.037), ('models', 0.037), ('maps', 0.036), ('locations', 0.036), ('einh', 0.035), ('humans', 0.035), ('observers', 0.034), ('predict', 0.034), ('cb', 0.034), ('fair', 0.034), ('convolving', 0.033), ('io', 0.033), ('score', 0.032), ('sequence', 0.032), ('datasets', 0.032), ('analyses', 0.03), ('look', 0.03), ('smoothed', 0.03), ('human', 0.03), ('border', 0.029), ('center', 0.029), ('attentional', 0.028), ('houcvpr', 0.028), ('humphrey', 0.028), ('iaps', 0.028), ('marat', 0.028), ('neurophysiological', 0.028), ('saliences', 0.028), ('sdsr', 0.028), ('vocus', 0.028), ('harel', 0.028), ('aim', 0.028), ('cognitive', 0.028), ('rankings', 0.027), ('category', 0.027), ('movements', 0.026), ('agreement', 0.026), ('bias', 0.026), ('visual', 0.026), ('callet', 0.025), ('erdem', 0.025), ('meur', 0.025), ('sigma', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="50-tfidf-1" href="./iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction.html">50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</a></p>
<p>Author: Ali Borji, Hamed R. Tavakoli, Dicky N. Sihite, Laurent Itti</p><p>Abstract: Significant recent progress has been made in developing high-quality saliency models. However, less effort has been undertaken on fair assessment of these models, over large standardized datasets and correctly addressing confounding factors. In this study, we pursue a critical and quantitative look at challenges (e.g., center-bias, map smoothing) in saliency modeling and the way they affect model accuracy. We quantitatively compare 32 state-of-the-art models (using the shuffled AUC score to discount center-bias) on 4 benchmark eye movement datasets, for prediction of human fixation locations and scanpath sequence. We also account for the role of map smoothing. We find that, although model rankings vary, some (e.g., AWS, LG, AIM, and HouNIPS) consistently outperform other models over all datasets. Some models work well for prediction of both fixation locations and scanpath sequence (e.g., Judd, GBVS). Our results show low prediction accuracy for models over emotional stimuli from the NUSEF dataset. Our last benchmark, for the first time, gauges the ability of models to decode the stimulus category from statistics of fixations, saccades, and model saliency values at fixated locations. In this test, ITTI and AIM models win over other models. Our benchmark provides a comprehensive high-level picture of the strengths and weaknesses of many popular models, and suggests future research directions in saliency modeling.</p><p>2 0.58768719 <a title="50-tfidf-2" href="./iccv-2013-Saliency_and_Human_Fixations%3A_State-of-the-Art_and_Study_of_Comparison_Metrics.html">373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</a></p>
<p>Author: Nicolas Riche, Matthieu Duvinage, Matei Mancas, Bernard Gosselin, Thierry Dutoit</p><p>Abstract: Visual saliency has been an increasingly active research area in the last ten years with dozens of saliency models recently published. Nowadays, one of the big challenges in the field is to find a way to fairly evaluate all of these models. In this paper, on human eye fixations ,we compare the ranking of 12 state-of-the art saliency models using 12 similarity metrics. The comparison is done on Jian Li ’s database containing several hundreds of natural images. Based on Kendall concordance coefficient, it is shown that some of the metrics are strongly correlated leading to a redundancy in the performance metrics reported in the available benchmarks. On the other hand, other metrics provide a more diverse picture of models ’ overall performance. As a recommendation, three similarity metrics should be used to obtain a complete point of view of saliency model performance.</p><p>3 0.4699735 <a title="50-tfidf-3" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>Author: Yangqing Jia, Mei Han</p><p>Abstract: It is known that purely low-level saliency cues such as frequency does not lead to a good salient object detection result, requiring high-level knowledge to be adopted for successful discovery of task-independent salient objects. In this paper, we propose an efficient way to combine such high-level saliency priors and low-level appearance models. We obtain the high-level saliency prior with the objectness algorithm to find potential object candidates without the need of category information, and then enforce the consistency among the salient regions using a Gaussian MRF with the weights scaled by diverse density that emphasizes the influence of potential foreground pixels. Our model obtains saliency maps that assign high scores for the whole salient object, and achieves state-of-the-art performance on benchmark datasets covering various foreground statistics.</p><p>4 0.46327072 <a title="50-tfidf-4" href="./iccv-2013-Saliency_Detection_via_Dense_and_Sparse_Reconstruction.html">372 iccv-2013-Saliency Detection via Dense and Sparse Reconstruction</a></p>
<p>Author: Xiaohui Li, Huchuan Lu, Lihe Zhang, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: In this paper, we propose a visual saliency detection algorithm from the perspective of reconstruction errors. The image boundaries are first extracted via superpixels as likely cues for background templates, from which dense and sparse appearance models are constructed. For each image region, we first compute dense and sparse reconstruction errors. Second, the reconstruction errors are propagated based on the contexts obtained from K-means clustering. Third, pixel-level saliency is computed by an integration of multi-scale reconstruction errors and refined by an object-biased Gaussian model. We apply the Bayes formula to integrate saliency measures based on dense and sparse reconstruction errors. Experimental results show that the proposed algorithm performs favorably against seventeen state-of-the-art methods in terms of precision and recall. In addition, the proposed algorithm is demonstrated to be more effective in highlighting salient objects uniformly and robust to background noise.</p><p>5 0.38792107 <a title="50-tfidf-5" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>Author: Xi Li, Yao Li, Chunhua Shen, Anthony Dick, Anton Van_Den_Hengel</p><p>Abstract: Salient object detection aims to locate objects that capture human attention within images. Previous approaches often pose this as a problem of image contrast analysis. In this work, we model an image as a hypergraph that utilizes a set of hyperedges to capture the contextual properties of image pixels or regions. As a result, the problem of salient object detection becomes one of finding salient vertices and hyperedges in the hypergraph. The main advantage of hypergraph modeling is that it takes into account each pixel’s (or region ’s) affinity with its neighborhood as well as its separation from image background. Furthermore, we propose an alternative approach based on centerversus-surround contextual contrast analysis, which performs salient object detection by optimizing a cost-sensitive support vector machine (SVM) objective function. Experimental results on four challenging datasets demonstrate the effectiveness of the proposed approaches against the stateof-the-art approaches to salient object detection.</p><p>6 0.34594271 <a title="50-tfidf-6" href="./iccv-2013-Saliency_Detection%3A_A_Boolean_Map_Approach.html">369 iccv-2013-Saliency Detection: A Boolean Map Approach</a></p>
<p>7 0.34375906 <a title="50-tfidf-7" href="./iccv-2013-Semantically-Based_Human_Scanpath_Estimation_with_HMMs.html">381 iccv-2013-Semantically-Based Human Scanpath Estimation with HMMs</a></p>
<p>8 0.33146128 <a title="50-tfidf-8" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>9 0.28043875 <a title="50-tfidf-9" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>10 0.26265422 <a title="50-tfidf-10" href="./iccv-2013-Saliency_Detection_in_Large_Point_Sets.html">370 iccv-2013-Saliency Detection in Large Point Sets</a></p>
<p>11 0.25776908 <a title="50-tfidf-11" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>12 0.25613838 <a title="50-tfidf-12" href="./iccv-2013-Saliency_Detection_via_Absorbing_Markov_Chain.html">371 iccv-2013-Saliency Detection via Absorbing Markov Chain</a></p>
<p>13 0.25294197 <a title="50-tfidf-13" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>14 0.23628114 <a title="50-tfidf-14" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>15 0.2054867 <a title="50-tfidf-15" href="./iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F.html">316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</a></p>
<p>16 0.14719209 <a title="50-tfidf-16" href="./iccv-2013-Learning_to_Predict_Gaze_in_Egocentric_Video.html">247 iccv-2013-Learning to Predict Gaze in Egocentric Video</a></p>
<p>17 0.14692958 <a title="50-tfidf-17" href="./iccv-2013-Calibration-Free_Gaze_Estimation_Using_Human_Gaze_Patterns.html">67 iccv-2013-Calibration-Free Gaze Estimation Using Human Gaze Patterns</a></p>
<p>18 0.13861077 <a title="50-tfidf-18" href="./iccv-2013-Predicting_Primary_Gaze_Behavior_Using_Social_Saliency_Fields.html">325 iccv-2013-Predicting Primary Gaze Behavior Using Social Saliency Fields</a></p>
<p>19 0.11318893 <a title="50-tfidf-19" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>20 0.068813473 <a title="50-tfidf-20" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.174), (1, -0.036), (2, 0.575), (3, -0.294), (4, -0.162), (5, -0.026), (6, 0.109), (7, -0.067), (8, 0.014), (9, 0.064), (10, 0.003), (11, -0.062), (12, -0.032), (13, 0.014), (14, 0.006), (15, 0.025), (16, -0.048), (17, 0.009), (18, 0.039), (19, 0.05), (20, -0.011), (21, 0.004), (22, 0.035), (23, 0.012), (24, 0.037), (25, -0.024), (26, 0.027), (27, -0.006), (28, 0.026), (29, -0.014), (30, -0.018), (31, -0.035), (32, -0.014), (33, -0.021), (34, 0.027), (35, -0.013), (36, -0.017), (37, -0.009), (38, -0.019), (39, -0.003), (40, 0.017), (41, -0.001), (42, 0.019), (43, -0.008), (44, -0.006), (45, -0.02), (46, -0.038), (47, 0.017), (48, -0.024), (49, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96505481 <a title="50-lsi-1" href="./iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction.html">50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</a></p>
<p>Author: Ali Borji, Hamed R. Tavakoli, Dicky N. Sihite, Laurent Itti</p><p>Abstract: Significant recent progress has been made in developing high-quality saliency models. However, less effort has been undertaken on fair assessment of these models, over large standardized datasets and correctly addressing confounding factors. In this study, we pursue a critical and quantitative look at challenges (e.g., center-bias, map smoothing) in saliency modeling and the way they affect model accuracy. We quantitatively compare 32 state-of-the-art models (using the shuffled AUC score to discount center-bias) on 4 benchmark eye movement datasets, for prediction of human fixation locations and scanpath sequence. We also account for the role of map smoothing. We find that, although model rankings vary, some (e.g., AWS, LG, AIM, and HouNIPS) consistently outperform other models over all datasets. Some models work well for prediction of both fixation locations and scanpath sequence (e.g., Judd, GBVS). Our results show low prediction accuracy for models over emotional stimuli from the NUSEF dataset. Our last benchmark, for the first time, gauges the ability of models to decode the stimulus category from statistics of fixations, saccades, and model saliency values at fixated locations. In this test, ITTI and AIM models win over other models. Our benchmark provides a comprehensive high-level picture of the strengths and weaknesses of many popular models, and suggests future research directions in saliency modeling.</p><p>2 0.95446587 <a title="50-lsi-2" href="./iccv-2013-Saliency_and_Human_Fixations%3A_State-of-the-Art_and_Study_of_Comparison_Metrics.html">373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</a></p>
<p>Author: Nicolas Riche, Matthieu Duvinage, Matei Mancas, Bernard Gosselin, Thierry Dutoit</p><p>Abstract: Visual saliency has been an increasingly active research area in the last ten years with dozens of saliency models recently published. Nowadays, one of the big challenges in the field is to find a way to fairly evaluate all of these models. In this paper, on human eye fixations ,we compare the ranking of 12 state-of-the art saliency models using 12 similarity metrics. The comparison is done on Jian Li ’s database containing several hundreds of natural images. Based on Kendall concordance coefficient, it is shown that some of the metrics are strongly correlated leading to a redundancy in the performance metrics reported in the available benchmarks. On the other hand, other metrics provide a more diverse picture of models ’ overall performance. As a recommendation, three similarity metrics should be used to obtain a complete point of view of saliency model performance.</p><p>3 0.92337179 <a title="50-lsi-3" href="./iccv-2013-Saliency_Detection%3A_A_Boolean_Map_Approach.html">369 iccv-2013-Saliency Detection: A Boolean Map Approach</a></p>
<p>Author: Jianming Zhang, Stan Sclaroff</p><p>Abstract: A novel Boolean Map based Saliency (BMS) model is proposed. An image is characterized by a set of binary images, which are generated by randomly thresholding the image ’s color channels. Based on a Gestalt principle of figure-ground segregation, BMS computes saliency maps by analyzing the topological structure of Boolean maps. BMS is simple to implement and efficient to run. Despite its simplicity, BMS consistently achieves state-of-the-art performance compared with ten leading methods on five eye tracking datasets. Furthermore, BMS is also shown to be advantageous in salient object detection.</p><p>4 0.89343721 <a title="50-lsi-4" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>Author: Xi Li, Yao Li, Chunhua Shen, Anthony Dick, Anton Van_Den_Hengel</p><p>Abstract: Salient object detection aims to locate objects that capture human attention within images. Previous approaches often pose this as a problem of image contrast analysis. In this work, we model an image as a hypergraph that utilizes a set of hyperedges to capture the contextual properties of image pixels or regions. As a result, the problem of salient object detection becomes one of finding salient vertices and hyperedges in the hypergraph. The main advantage of hypergraph modeling is that it takes into account each pixel’s (or region ’s) affinity with its neighborhood as well as its separation from image background. Furthermore, we propose an alternative approach based on centerversus-surround contextual contrast analysis, which performs salient object detection by optimizing a cost-sensitive support vector machine (SVM) objective function. Experimental results on four challenging datasets demonstrate the effectiveness of the proposed approaches against the stateof-the-art approaches to salient object detection.</p><p>5 0.85290724 <a title="50-lsi-5" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>Author: Yangqing Jia, Mei Han</p><p>Abstract: It is known that purely low-level saliency cues such as frequency does not lead to a good salient object detection result, requiring high-level knowledge to be adopted for successful discovery of task-independent salient objects. In this paper, we propose an efficient way to combine such high-level saliency priors and low-level appearance models. We obtain the high-level saliency prior with the objectness algorithm to find potential object candidates without the need of category information, and then enforce the consistency among the salient regions using a Gaussian MRF with the weights scaled by diverse density that emphasizes the influence of potential foreground pixels. Our model obtains saliency maps that assign high scores for the whole salient object, and achieves state-of-the-art performance on benchmark datasets covering various foreground statistics.</p><p>6 0.84644002 <a title="50-lsi-6" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>7 0.83374506 <a title="50-lsi-7" href="./iccv-2013-Saliency_Detection_via_Dense_and_Sparse_Reconstruction.html">372 iccv-2013-Saliency Detection via Dense and Sparse Reconstruction</a></p>
<p>8 0.81897432 <a title="50-lsi-8" href="./iccv-2013-Saliency_Detection_in_Large_Point_Sets.html">370 iccv-2013-Saliency Detection in Large Point Sets</a></p>
<p>9 0.80308491 <a title="50-lsi-9" href="./iccv-2013-Saliency_Detection_via_Absorbing_Markov_Chain.html">371 iccv-2013-Saliency Detection via Absorbing Markov Chain</a></p>
<p>10 0.78602749 <a title="50-lsi-10" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>11 0.70204568 <a title="50-lsi-11" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>12 0.68946236 <a title="50-lsi-12" href="./iccv-2013-Semantically-Based_Human_Scanpath_Estimation_with_HMMs.html">381 iccv-2013-Semantically-Based Human Scanpath Estimation with HMMs</a></p>
<p>13 0.58946383 <a title="50-lsi-13" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>14 0.5832274 <a title="50-lsi-14" href="./iccv-2013-Predicting_Primary_Gaze_Behavior_Using_Social_Saliency_Fields.html">325 iccv-2013-Predicting Primary Gaze Behavior Using Social Saliency Fields</a></p>
<p>15 0.39591384 <a title="50-lsi-15" href="./iccv-2013-Learning_to_Predict_Gaze_in_Egocentric_Video.html">247 iccv-2013-Learning to Predict Gaze in Egocentric Video</a></p>
<p>16 0.36900392 <a title="50-lsi-16" href="./iccv-2013-Calibration-Free_Gaze_Estimation_Using_Human_Gaze_Patterns.html">67 iccv-2013-Calibration-Free Gaze Estimation Using Human Gaze Patterns</a></p>
<p>17 0.35808447 <a title="50-lsi-17" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>18 0.26536 <a title="50-lsi-18" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>19 0.24795784 <a title="50-lsi-19" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>20 0.24649125 <a title="50-lsi-20" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.054), (4, 0.01), (7, 0.016), (12, 0.051), (21, 0.147), (26, 0.058), (31, 0.037), (40, 0.012), (42, 0.088), (63, 0.014), (64, 0.028), (73, 0.029), (77, 0.026), (84, 0.036), (89, 0.132), (97, 0.144), (98, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80648869 <a title="50-lda-1" href="./iccv-2013-Saliency_and_Human_Fixations%3A_State-of-the-Art_and_Study_of_Comparison_Metrics.html">373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</a></p>
<p>Author: Nicolas Riche, Matthieu Duvinage, Matei Mancas, Bernard Gosselin, Thierry Dutoit</p><p>Abstract: Visual saliency has been an increasingly active research area in the last ten years with dozens of saliency models recently published. Nowadays, one of the big challenges in the field is to find a way to fairly evaluate all of these models. In this paper, on human eye fixations ,we compare the ranking of 12 state-of-the art saliency models using 12 similarity metrics. The comparison is done on Jian Li ’s database containing several hundreds of natural images. Based on Kendall concordance coefficient, it is shown that some of the metrics are strongly correlated leading to a redundancy in the performance metrics reported in the available benchmarks. On the other hand, other metrics provide a more diverse picture of models ’ overall performance. As a recommendation, three similarity metrics should be used to obtain a complete point of view of saliency model performance.</p><p>same-paper 2 0.79209954 <a title="50-lda-2" href="./iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction.html">50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</a></p>
<p>Author: Ali Borji, Hamed R. Tavakoli, Dicky N. Sihite, Laurent Itti</p><p>Abstract: Significant recent progress has been made in developing high-quality saliency models. However, less effort has been undertaken on fair assessment of these models, over large standardized datasets and correctly addressing confounding factors. In this study, we pursue a critical and quantitative look at challenges (e.g., center-bias, map smoothing) in saliency modeling and the way they affect model accuracy. We quantitatively compare 32 state-of-the-art models (using the shuffled AUC score to discount center-bias) on 4 benchmark eye movement datasets, for prediction of human fixation locations and scanpath sequence. We also account for the role of map smoothing. We find that, although model rankings vary, some (e.g., AWS, LG, AIM, and HouNIPS) consistently outperform other models over all datasets. Some models work well for prediction of both fixation locations and scanpath sequence (e.g., Judd, GBVS). Our results show low prediction accuracy for models over emotional stimuli from the NUSEF dataset. Our last benchmark, for the first time, gauges the ability of models to decode the stimulus category from statistics of fixations, saccades, and model saliency values at fixated locations. In this test, ITTI and AIM models win over other models. Our benchmark provides a comprehensive high-level picture of the strengths and weaknesses of many popular models, and suggests future research directions in saliency modeling.</p><p>3 0.76447469 <a title="50-lda-3" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>Author: Hesamoddin Salehian, Guang Cheng, Baba C. Vemuri, Jeffrey Ho</p><p>Abstract: Symmetric positive-definite (SPD) matrices are ubiquitous in Computer Vision, Machine Learning and Medical Image Analysis. Finding the center/average of a population of such matrices is a common theme in many algorithms such as clustering, segmentation, principal geodesic analysis, etc. The center of a population of such matrices can be defined using a variety of distance/divergence measures as the minimizer of the sum of squared distances/divergences from the unknown center to the members of the population. It is well known that the computation of the Karcher mean for the space of SPD matrices which is a negativelycurved Riemannian manifold is computationally expensive. Recently, the LogDet divergence-based center was shown to be a computationally attractive alternative. However, the LogDet-based mean of more than two matrices can not be computed in closed form, which makes it computationally less attractive for large populations. In this paper we present a novel recursive estimator for center based on the Stein distance which is the square root of the LogDet di– vergence that is significantly faster than the batch mode computation of this center. The key theoretical contribution is a closed-form solution for the weighted Stein center of two SPD matrices, which is used in the recursive computation of the Stein center for a population of SPD matrices. Additionally, we show experimental evidence of the convergence of our recursive Stein center estimator to the batch mode Stein center. We present applications of our recursive estimator to K-means clustering and image indexing depicting significant time gains over corresponding algorithms that use the batch mode computations. For the latter application, we develop novel hashing functions using the Stein distance and apply it to publicly available data sets, and experimental results have shown favorable com– ∗This research was funded in part by the NIH grant NS066340 to BCV. †Corresponding author parisons to other competing methods.</p><p>4 0.75407314 <a title="50-lda-4" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>Author: Xi Li, Yao Li, Chunhua Shen, Anthony Dick, Anton Van_Den_Hengel</p><p>Abstract: Salient object detection aims to locate objects that capture human attention within images. Previous approaches often pose this as a problem of image contrast analysis. In this work, we model an image as a hypergraph that utilizes a set of hyperedges to capture the contextual properties of image pixels or regions. As a result, the problem of salient object detection becomes one of finding salient vertices and hyperedges in the hypergraph. The main advantage of hypergraph modeling is that it takes into account each pixel’s (or region ’s) affinity with its neighborhood as well as its separation from image background. Furthermore, we propose an alternative approach based on centerversus-surround contextual contrast analysis, which performs salient object detection by optimizing a cost-sensitive support vector machine (SVM) objective function. Experimental results on four challenging datasets demonstrate the effectiveness of the proposed approaches against the stateof-the-art approaches to salient object detection.</p><p>5 0.7433275 <a title="50-lda-5" href="./iccv-2013-Synergistic_Clustering_of_Image_and_Segment_Descriptors_for_Unsupervised_Scene_Understanding.html">412 iccv-2013-Synergistic Clustering of Image and Segment Descriptors for Unsupervised Scene Understanding</a></p>
<p>Author: Daniel M. Steinberg, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: With the advent of cheap, high fidelity, digital imaging systems, the quantity and rate of generation of visual data can dramatically outpace a humans ability to label or annotate it. In these situations there is scope for the use of unsupervised approaches that can model these datasets and automatically summarise their content. To this end, we present a totally unsupervised, and annotation-less, model for scene understanding. This model can simultaneously cluster whole-image and segment descriptors, therebyforming an unsupervised model of scenes and objects. We show that this model outperforms other unsupervised models that can only cluster one source of information (image or segment) at once. We are able to compare unsupervised and supervised techniques using standard measures derived from confusion matrices and contingency tables. This shows that our unsupervised model is competitive with current supervised and weakly-supervised models for scene understanding on standard datasets. We also demonstrate our model operating on a dataset with more than 100,000 images col- lected by an autonomous underwater vehicle.</p><p>6 0.73323673 <a title="50-lda-6" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>7 0.7236346 <a title="50-lda-7" href="./iccv-2013-A_Color_Constancy_Model_with_Double-Opponency_Mechanisms.html">5 iccv-2013-A Color Constancy Model with Double-Opponency Mechanisms</a></p>
<p>8 0.72301137 <a title="50-lda-8" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>9 0.72264469 <a title="50-lda-9" href="./iccv-2013-Saliency_Detection_via_Dense_and_Sparse_Reconstruction.html">372 iccv-2013-Saliency Detection via Dense and Sparse Reconstruction</a></p>
<p>10 0.71946257 <a title="50-lda-10" href="./iccv-2013-Person_Re-identification_by_Salience_Matching.html">313 iccv-2013-Person Re-identification by Salience Matching</a></p>
<p>11 0.70932961 <a title="50-lda-11" href="./iccv-2013-Saliency_Detection%3A_A_Boolean_Map_Approach.html">369 iccv-2013-Saliency Detection: A Boolean Map Approach</a></p>
<p>12 0.70616442 <a title="50-lda-12" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>13 0.69849044 <a title="50-lda-13" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>14 0.68517524 <a title="50-lda-14" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>15 0.6838361 <a title="50-lda-15" href="./iccv-2013-Saliency_Detection_via_Absorbing_Markov_Chain.html">371 iccv-2013-Saliency Detection via Absorbing Markov Chain</a></p>
<p>16 0.67760104 <a title="50-lda-16" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>17 0.6719631 <a title="50-lda-17" href="./iccv-2013-Semantically-Based_Human_Scanpath_Estimation_with_HMMs.html">381 iccv-2013-Semantically-Based Human Scanpath Estimation with HMMs</a></p>
<p>18 0.66376817 <a title="50-lda-18" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>19 0.66364223 <a title="50-lda-19" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<p>20 0.66164315 <a title="50-lda-20" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
