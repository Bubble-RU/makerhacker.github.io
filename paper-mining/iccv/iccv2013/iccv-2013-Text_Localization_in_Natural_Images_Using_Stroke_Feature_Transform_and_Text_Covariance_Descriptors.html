<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>415 iccv-2013-Text Localization in Natural Images Using Stroke Feature Transform and Text Covariance Descriptors</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-415" href="#">iccv2013-415</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>415 iccv-2013-Text Localization in Natural Images Using Stroke Feature Transform and Text Covariance Descriptors</h1>
<br/><p>Source: <a title="iccv-2013-415-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Huang_Text_Localization_in_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Weilin Huang, Zhe Lin, Jianchao Yang, Jue Wang</p><p>Abstract: In this paper, we present a new approach for text localization in natural images, by discriminating text and non-text regions at three levels: pixel, component and textline levels. Firstly, a powerful low-level filter called the Stroke Feature Transform (SFT) is proposed, which extends the widely-used Stroke Width Transform (SWT) by incorporating color cues of text pixels, leading to significantly enhanced performance on inter-component separation and intra-component connection. Secondly, based on the output of SFT, we apply two classifiers, a text component classifier and a text-line classifier, sequentially to extract text regions, eliminating the heuristic procedures that are commonly used in previous approaches. The two classifiers are built upon two novel Text Covariance Descriptors (TCDs) that encode both the heuristic properties and the statistical characteristics of text stokes. Finally, text regions are located by simply thresholding the text-line confident map. Our method was evaluated on two benchmark datasets: ICDAR 2005 and ICDAR 2011, and the corresponding F- , measure values are 0. 72 and 0. 73, respectively, surpassing previous methods in accuracy by a large margin.</p><p>Reference: <a title="iccv-2013-415-reference" href="../iccv2013_reference/iccv-2013-Text_Localization_in_Natural_Images_Using_Stroke_Feature_Transform_and_Text_Covariance_Descriptors_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn ;{z l at in Abstract In this paper, we present a new approach for text localization in natural images, by discriminating text and non-text regions at three levels: pixel, component and textline levels. [sent-7, score-0.826]
</p><p>2 Secondly, based on the output of SFT, we apply two classifiers, a text component classifier and a text-line classifier, sequentially to extract text regions, eliminating the heuristic procedures that are commonly used in previous approaches. [sent-9, score-0.814]
</p><p>3 The two classifiers are built upon two novel Text Covariance Descriptors (TCDs) that encode both the heuristic properties and the statistical characteristics of text stokes. [sent-10, score-0.441]
</p><p>4 Finally, text regions are located by simply thresholding the text-line confident map. [sent-11, score-0.347]
</p><p>5 Introduction Text detection and localization in natural images serves as a crucial component for content-based information retrieval, as textual information often provides important clues for understanding the high-level semantics of multimedia content. [sent-16, score-0.238]
</p><p>6 Despite the tremendous effort devoted to solving this problem, text localization remains to be challenging. [sent-18, score-0.342]
</p><p>7 The difficulties mainly lie in the diversity of text patterns and the complexity of j iayang , j uewang} @ adobe . [sent-19, score-0.315]
</p><p>8 Furthermore, text-like background objects, such as bricks, windows and leaves, often lead to many false alarms in text detection. [sent-22, score-0.321]
</p><p>9 Previous text localization methods can be roughly divided into two categories: texture-based and componentbased approaches. [sent-23, score-0.371]
</p><p>10 Texture-based methods scan the image at different scales using sliding windows, and classify text and non-text regions based on extracted window descriptors [10, 4, 9, 25]. [sent-24, score-0.318]
</p><p>11 Component-based methods first discard the majority of background pixels using low-level filters, and then construct component candidates from remaining pixels using a set of heuristic properties, e. [sent-33, score-0.345]
</p><p>12 consistency of stroke width [6, 26] and color homogeneity [27, 16, 3, 15]. [sent-35, score-0.936]
</p><p>13 Another advantage is that the detected components provide a direct segmentation of the text letters, which benefits future applications such as recognition. [sent-38, score-0.374]
</p><p>14 We first apply the SFT filter on the input image  to generate a stroke width map (b) and a stroke color map (c). [sent-43, score-1.786]
</p><p>15 We then apply the component classifier to generate a component confidence map (d), which is then fed into the text-line classifier to generate a text-line confidence map (e). [sent-44, score-0.454]
</p><p>16 Secondly, using heuristic rules to filter out outliers at the component and tex-line levels involve a set of manually tuned parameters, which may not generalize well to different datasets. [sent-47, score-0.271]
</p><p>17 Our goal is to develop a text localization system that combines the advantages of both classification-based and component-based methods, while overcoming their inherent limitations. [sent-50, score-0.342]
</p><p>18 1(b,c)), the SFT filter effectively mitigates inter-component connections while enhancing intracomponent connections, leading to a significantly better component candidate detection than SWT. [sent-56, score-0.222]
</p><p>19 Using the two TCDs, we build two classifiers instead of the commonly-used heuristic filtering methods for robust component and text-line classification, as shown in Fig. [sent-60, score-0.277]
</p><p>20 Based on the color uniformity of characters in a text string, Yi and Tian [27] proposed a color-based partition scheme, which applies weighted kmeans clustering in the RGB space for separating text and background pixels. [sent-67, score-0.68]
</p><p>21 Neumann and Matas assumed each component as an independent extremal region (ER), and exploited maximally stable extremal regions (MSERs) to extract possible components in multiple channels [16, 15, 14]. [sent-68, score-0.246]
</p><p>22 A unique feature of a textual region is that the text  strokes inside it usually have consistent, uniform width. [sent-71, score-0.424]
</p><p>23 [6] proposed the SWT to detect stroke pixels by measuring the orientation difference between pairs of edge pixels, and grouping stroke pixels with similar widths as connected components. [sent-73, score-1.71]
</p><p>24 On the other hand, a number of histogram-statisticsbased text descriptors have been proposed in texture-based methods. [sent-76, score-0.318]
</p><p>25 In [25], a simple HOG descriptor was applied to Random 1242  Ferns [1] classifiers for both text detection and recognition. [sent-79, score-0.387]
</p><p>26 [17] computed text confidence values of sub-windows by using the HOG feature and a boosted cascade classifier: WaldBoost [21]. [sent-81, score-0.368]
</p><p>27 Proposed Approach The proposed text localization system contains four main parts: (1) component1 detection, (2) component filtering, (3) text-line construction and (4) text-line filtering. [sent-90, score-0.444]
</p><p>28 Firstly, we use a new stroke filtering method called Stroke Feature Transform (SFT) to identify text stroke pixels. [sent-95, score-1.781]
</p><p>29 Secondly, we use the two maps generated from SFT, a stroke width map and a stroke color map, to perform robust text pixel grouping (component generation). [sent-96, score-2.07]
</p><p>30 1  The Stroke Feature Transform  The recently introduced Stroke Width Transform (SWT) [6]  has been shown to be effective for text detection in the wild. [sent-99, score-0.332]
</p><p>31 It detects stroke pixels by shooting a pixel ray from an edge pixel (px) to its opposite edge pixel (py) along the gradient direction dx. [sent-100, score-1.193]
</p><p>32 The ray is considered as valid only if the gradient orientations of the pair of edge pixels are roughly opposite to each other. [sent-101, score-0.331]
</p><p>33 All pixels covered by a valid ray are labeled by the same stroke width, which is the distance between the pair of edge pixels. [sent-103, score-0.991]
</p><p>34 In this way, SWT filters out the background pixels and assigns text pixels with stroke widths. [sent-104, score-1.156]
</p><p>35 However, only gradient orientation and edge information are used for ray tracking, and each ray is handled independently. [sent-105, score-0.324]
</p><p>36 In real cases, there often exist a large number of edge pixels that have irregular gradient orientations, which are not perpendicular to the correct stroke edge directions. [sent-106, score-0.947]
</p><p>37 As shown in 1Component  means text  character in this  paper. [sent-107, score-0.327]
</p><p>38 To remedy these problems, we extend SWT by leveraging two additional cues during ray tracking: color uniformity and local relationships  of edge pixels, and generate two maps, a stroke width map and a stroke color map jointly. [sent-111, score-2.016]
</p><p>39 Firstly, Canny edge detector is applied to detect edge pixels from the input image. [sent-114, score-0.207]
</p><p>40 Secondly, for each edge pixel px on the canny edge map, we shoot a ray along its gradient direction dx and check the pixels it encounters along the way. [sent-115, score-0.432]
</p><p>41 We end this ray at the current pixel pcur and set it as a valid ray if pcur satisfies either of the following two constraints: 1. [sent-116, score-0.426]
</p><p>42 Stroke width constraint: pcur is an edge pixel and its gradient orientation dcur is roughly opposite to dx as: | |dcur dx | π | < 2π . [sent-117, score-0.485]
</p><p>43 Stroke color constraint: the distance between the current pixel’s color pcur (denoted as Ccur) and the median ray color C r¯ (computed as median R, G, B of pixels on the ray) satisfies ? [sent-119, score-0.433]
</p><p>44 If this color discontinuity is detected, we reconsider the current pixel as an edge pixel and check it’s orientation as in the Step 1 using a more strict threshold, | |dcur − dx | − π | < 6π . [sent-124, score-0.243]
</p><p>45 If neither constraints is met for a certain number of checked pixels on the ray, we discard the current ray, and continue  to the next edge pixel and repeat the above process. [sent-125, score-0.198]
</p><p>46 Once all the edge pixels are considered on the canny edge map, we further filter out invalid rays whose median colors are significantly different from its local neighbors on the canny edge map. [sent-126, score-0.479]
</p><p>47 Finally, we assign a stroke width value and the median RGB color value to all pixels in a valid ray to construct the stroke width map and the stroke color map. [sent-128, score-2.855]
</p><p>48 Due to the stroke color constraint, SFT is very effective at discarding rays shooting towards the outside of the strokes, because color often changes dramatically in the background region. [sent-130, score-0.867]
</p><p>49 Furthermore, it can help us recover some missing 1243  (a) Input(b) Can y edge map (c) Stroke color map(d) Stroke width map(e) SWT stroke width map Figure 2. [sent-131, score-1.269]
</p><p>50 The neighborhood coherency constraint is used to discard occasional errors in text strokes, as  well as a large amount of incorrect, scattered connections in the background. [sent-137, score-0.377]
</p><p>51 Another important advantage of SFT is that it produces a stroke color map as a byproduct of the transform, in which the stroke pixels have better uniformity and stronger distinction from background pixels than the stroke width map. [sent-138, score-2.57]
</p><p>52 Hence, by applying the stroke width map and the stroke color map jointly for grouping, our filter can effectively identify incorrectly-connected stroke components and other outliers in the background, as shown in Fig. [sent-139, score-2.582]
</p><p>53 2  Component Generation  We apply region growing [8] for grouping the stroke pixels into different components by using both the stroke width and color maps. [sent-143, score-1.834]
</p><p>54 The values in the stroke width map are normalized to [0 255]. [sent-144, score-0.973]
</p><p>55 Then region growing is performed in a 4-dimensional space by representing each stroke pixel using a width value and R, G, B color values. [sent-145, score-0.998]
</p><p>56 To this end, we have successfully incorporated both stroke width and color information for low-level filtering and grouping, which outperforms the original SWT approach significantly on letter candidate detection, as shown in Fig. [sent-147, score-1.02]
</p><p>57 Text Covariance Descriptors for Filtering As discussed earlier, many previous approaches  use  heuristic rules for filtering out false components, and grouping true text components into text-lines. [sent-151, score-0.547]
</p><p>58 For a given region U = {ui}in=1 ⊂ F, and ui ∈ Rd is dF-odrim ae gnisvioenna rle gfieoatnuUr e v =ec t{our o}f the⊂ ⊂ele Fm,e anntsd i unsi∈de RU, covariance descriptor of region U can be computed as:  CU=n −1 1? [sent-161, score-0.215]
</p><p>59 2  TCD for Components (TCD-C)  The most straight-forward method for component filtering is to perform heuristic filtering with multiple features [6, 26, 27]. [sent-166, score-0.313]
</p><p>60 The diagonal entries of the covariance matrix are the variances of each feature, while the nondiagonal entries capture the correlation between features, which are also informative for discriminating text strokes from background clutter. [sent-170, score-0.474]
</p><p>61 Furthermore, by jointly computing coordinates of stroke pixels with other features, the proposed TCD-C naturally encodes local spatial information into the descriptor. [sent-171, score-0.8]
</p><p>62 The basic elements in each text component are the stroke pixels. [sent-172, score-1.111]
</p><p>63 Although the number of stroke pixels vary significantly among different components, an important merit of the covariance descriptor is that, it is invariant to the num-  ber of elements within the regions. [sent-173, score-0.912]
</p><p>64 Based on heuristic and geometric characteristics of text strokes, we adopt nine different basic features for computing the TCD-C. [sent-175, score-0.419]
</p><p>65 Stroke width values in the stroke width map Sswm for stroke width consistency. [sent-212, score-2.007]
</p><p>66 The values are normalized by the maximum stroke width in the region. [sent-213, score-0.929]
</p><p>67 Stroke distance values in a stroke distance normalized to [0, 1], which compensate width map for stroke width consistency. [sent-215, score-1.859]
</p><p>68 distance map is computed from the stroke  map Sdist, the stroke The stroke width map  ×  using the the Euclidean distance transform, the details are described in [3]. [sent-216, score-2.446]
</p><p>69 ratio between the height and width of the component); (2) the occupation percentage, computed as the ratio of total number of pixels to the number of stoke pixels in the component; and (3) the ratio of the component scale (i. [sent-225, score-0.444]
</p><p>70 the larger value of the width and height of the component region) to its mean stroke width map value. [sent-227, score-1.226]
</p><p>71 We train a random forests classifier [2] and use it to generate a confident score for each text component, as shown in Fig 3. [sent-229, score-0.369]
</p><p>72 Firstly, two components having similar characteristics are paired together by using a set of heuristic conditions, such as similar mean stroke width, color, height and distance between them. [sent-232, score-0.916]
</p><p>73 3  TCD for Text-lines (TCD-T)  Most text-like components have similar local structures to true text components, thus are difficult to be distinguished solely by component-level filtering. [sent-238, score-0.351]
</p><p>74 In TCD-T we use two covariance matrices to compute two different types of the component features independently. [sent-243, score-0.206]
</p><p>75 Seven heuristic features used in TCD-C, including mean values of intensities, colors, stroke widths and distances (mean[I? [sent-245, score-0.851]
</p><p>76 The last one is the occupation percentage of the stroke pixels in each component. [sent-251, score-0.801]
</p><p>77 Given the input image shown in (a), TCD-C generates a component confidence map in (b), which is fed into TCD-T to generate a text-line confidence map in (c). [sent-256, score-0.319]
</p><p>78 Horizontal distance from the current component to the next one, measured by the normalized horizontal coordinates (Cx) of two component centers. [sent-270, score-0.253]
</p><p>79 In total there are 12 component features adopted for the first covariance matrix, which in turn generates a 78dimensional vector for text-line representation. [sent-272, score-0.227]
</p><p>80 r xW ies c goennceartaetnedat,e tehseu fletiantgur ae vectors extracted from the two covariance matrices, along with two additional features: (1) the number of components in the text-line, normalized by dividing the maximum number of components in a textline, e. [sent-276, score-0.24]
</p><p>81 Note that according to our feature design, TCD-T allows a single component to be treated as a word or text-line (only happens when using the text separation method in text-line aggregation). [sent-280, score-0.397]
</p><p>82 The text-line classifier generates a confidence value for each tex-line candidate, and the final textline detection result is produced by simply thresholding this confidence map. [sent-283, score-0.282]
</p><p>83 Both datasets have been widely used as the standard benchmarks for text detection in natural images. [sent-290, score-0.354]
</p><p>84 The proposed SFT operator was first applied on the training images to obtain positive and negative component samples, classified by matching to the ground truth text regions. [sent-307, score-0.397]
</p><p>85 In our experiments, we kept all components (with confidence values larger than zero) after running them through the component classifier, and discarded non-paired components and pairs with confidence values less than 0. [sent-311, score-0.36]
</p><p>86 We assume each test image includes at least one text-line, and select the pair or component of top confident value as the detected text if no text-line is detected in an image. [sent-317, score-0.471]
</p><p>87 They suggest that our system is robust against large variations in text font, color, size, and geometric distortion. [sent-342, score-0.295]
</p><p>88 In addition to detected text lines, our system also generates text pixel segmentation results shown at the bottom of each example, where white pixels include all pixels in the remaining valid text components. [sent-343, score-1.117]
</p><p>89 The segmentation can be potentially used in other applications such as text content or font recognition. [sent-344, score-0.332]
</p><p>90 Our system fails when the text strokes are too subtle and do not have strong edges (left), or the text region is partially occluded by other structures (right). [sent-347, score-0.689]
</p><p>91 In both cases the low level SFT filter failed to detect the right stroke pixels. [sent-348, score-0.762]
</p><p>92 tributions include a novel Stroke Feature Transform, a lowlevel filter that is extended from the original SWT approach by jointly considering stroke width and color information for robust filtering. [sent-366, score-0.984]
</p><p>93 Robust text detection in natural images with edgeenhanced maximally stable extremal regions, 2012. [sent-387, score-0.386]
</p><p>94 Detecting text in natural scenes with stroke width transform, 2010. [sent-403, score-1.203]
</p><p>95 Texture-based approach for text detection in images using support vector machines and continuously adaptive mean shift algorithm. [sent-426, score-0.332]
</p><p>96 A method for text localization and recognition in real-world images, 2010. [sent-451, score-0.342]
</p><p>97 Icdar 2011robust reading competition challenge 2: Reading text in scene images, 2011. [sent-476, score-0.376]
</p><p>98 Icdar 2011robust reading competition challenge 2: Reading text in scene images, 2011. [sent-482, score-0.376]
</p><p>99 A laplacian approach to multi-oriented text detection in video. [sent-488, score-0.332]
</p><p>100 Character energy and link energybased text extraction in scene images, 2010. [sent-541, score-0.295]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stroke', 0.714), ('sft', 0.301), ('text', 0.295), ('swt', 0.2), ('width', 0.172), ('icdar', 0.171), ('ray', 0.115), ('covariance', 0.104), ('component', 0.102), ('tcds', 0.097), ('heuristic', 0.095), ('strokes', 0.075), ('edge', 0.073), ('pcur', 0.065), ('textline', 0.065), ('pixels', 0.061), ('filtering', 0.058), ('components', 0.056), ('confidence', 0.054), ('reading', 0.054), ('color', 0.05), ('transform', 0.05), ('canny', 0.049), ('classication', 0.048), ('dcur', 0.048), ('sdist', 0.048), ('sswm', 0.048), ('filter', 0.048), ('localization', 0.047), ('map', 0.044), ('grouping', 0.043), ('neumann', 0.043), ('texts', 0.04), ('tcd', 0.04), ('uniformity', 0.04), ('pixel', 0.038), ('font', 0.037), ('detection', 0.037), ('connections', 0.035), ('descriptor', 0.033), ('epshtein', 0.033), ('character', 0.032), ('ccur', 0.032), ('ferns', 0.032), ('methodyearprf', 0.032), ('extremal', 0.032), ('rays', 0.032), ('orientations', 0.032), ('secondly', 0.031), ('textual', 0.03), ('ui', 0.03), ('firstly', 0.029), ('characteristics', 0.029), ('tuzel', 0.029), ('waldboost', 0.029), ('componentbased', 0.029), ('hanif', 0.029), ('valid', 0.028), ('confident', 0.028), ('classifier', 0.027), ('competition', 0.027), ('shivakumara', 0.026), ('shahab', 0.026), ('occupation', 0.026), ('outliers', 0.026), ('windows', 0.026), ('discard', 0.026), ('irregular', 0.026), ('letter', 0.026), ('filters', 0.025), ('shafait', 0.025), ('adaboost', 0.025), ('coordinates', 0.025), ('region', 0.024), ('normalized', 0.024), ('thresholding', 0.024), ('shenzhen', 0.024), ('descriptors', 0.023), ('dx', 0.023), ('widths', 0.023), ('detected', 0.023), ('generalizes', 0.022), ('height', 0.022), ('opposite', 0.022), ('classifiers', 0.022), ('natural', 0.022), ('shooting', 0.021), ('median', 0.021), ('generates', 0.021), ('intensities', 0.021), ('coherency', 0.021), ('string', 0.021), ('orientation', 0.021), ('adobe', 0.02), ('thirdly', 0.02), ('values', 0.019), ('forests', 0.019), ('porikli', 0.019), ('recall', 0.019), ('cx', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="415-tfidf-1" href="./iccv-2013-Text_Localization_in_Natural_Images_Using_Stroke_Feature_Transform_and_Text_Covariance_Descriptors.html">415 iccv-2013-Text Localization in Natural Images Using Stroke Feature Transform and Text Covariance Descriptors</a></p>
<p>Author: Weilin Huang, Zhe Lin, Jianchao Yang, Jue Wang</p><p>Abstract: In this paper, we present a new approach for text localization in natural images, by discriminating text and non-text regions at three levels: pixel, component and textline levels. Firstly, a powerful low-level filter called the Stroke Feature Transform (SFT) is proposed, which extends the widely-used Stroke Width Transform (SWT) by incorporating color cues of text pixels, leading to significantly enhanced performance on inter-component separation and intra-component connection. Secondly, based on the output of SFT, we apply two classifiers, a text component classifier and a text-line classifier, sequentially to extract text regions, eliminating the heuristic procedures that are commonly used in previous approaches. The two classifiers are built upon two novel Text Covariance Descriptors (TCDs) that encode both the heuristic properties and the statistical characteristics of text stokes. Finally, text regions are located by simply thresholding the text-line confident map. Our method was evaluated on two benchmark datasets: ICDAR 2005 and ICDAR 2011, and the corresponding F- , measure values are 0. 72 and 0. 73, respectively, surpassing previous methods in accuracy by a large margin.</p><p>2 0.57862502 <a title="415-tfidf-2" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>Author: Lukáš Neumann, Jiri Matas</p><p>Abstract: An unconstrained end-to-end text localization and recognition method is presented. The method introduces a novel approach for character detection and recognition which combines the advantages of sliding-window and connected component methods. Characters are detected and recognized as image regions which contain strokes of specific orientations in a specific relative position, where the strokes are efficiently detected by convolving the image gradient field with a set of oriented bar filters. Additionally, a novel character representation efficiently calculated from the values obtained in the stroke detection phase is introduced. The representation is robust to shift at the stroke level, which makes it less sensitive to intra-class variations and the noise induced by normalizing character size and positioning. The effectiveness of the representation is demonstrated by the results achieved in the classification of real-world characters using an euclidian nearestneighbor classifier trained on synthetic data in a plain form. The method was evaluated on a standard dataset, where it achieves state-of-the-art results in both text localization and recognition.</p><p>3 0.25023872 <a title="415-tfidf-3" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>Author: S. Karthikeyan, Vignesh Jagadeesh, Renuka Shenoy, Miguel Ecksteinz, B.S. Manjunath</p><p>Abstract: Eye movement studies have confirmed that overt attention is highly biased towards faces and text regions in images. In this paper we explore a novel problem of predicting face and text regions in images using eye tracking data from multiple subjects. The problem is challenging as we aim to predict the semantics (face/text/background) only from eye tracking data without utilizing any image information. The proposed algorithm spatially clusters eye tracking data obtained in an image into different coherent groups and subsequently models the likelihood of the clusters containing faces and text using afully connectedMarkov Random Field (MRF). Given the eye tracking datafrom a test image, itpredicts potential face/head (humans, dogs and cats) and text locations reliably. Furthermore, the approach can be used to select regions of interest for further analysis by object detectors for faces and text. The hybrid eye position/object detector approach achieves better detection performance and reduced computation time compared to using only the object detection algorithm. We also present a new eye tracking dataset on 300 images selected from ICDAR, Street-view, Flickr and Oxford-IIIT Pet Dataset from 15 subjects.</p><p>4 0.21411064 <a title="415-tfidf-4" href="./iccv-2013-PhotoOCR%3A_Reading_Text_in_Uncontrolled_Conditions.html">315 iccv-2013-PhotoOCR: Reading Text in Uncontrolled Conditions</a></p>
<p>Author: Alessandro Bissacco, Mark Cummins, Yuval Netzer, Hartmut Neven</p><p>Abstract: We describe PhotoOCR, a system for text extraction from images. Our particular focus is reliable text extraction from smartphone imagery, with the goal of text recognition as a user input modality similar to speech recognition. Commercially available OCR performs poorly on this task. Recent progress in machine learning has substantially improved isolated character classification; we build on this progress by demonstrating a complete OCR system using these techniques. We also incorporate modern datacenter-scale distributed language modelling. Our approach is capable of recognizing text in a variety of challenging imaging conditions where traditional OCR systems fail, notably in the presence of substantial blur, low resolution, low contrast, high image noise and other distortions. It also operates with low latency; mean processing time is 600 ms per image. We evaluate our system on public benchmark datasets for text extraction and outperform all previously reported results, more than halving the error rate on multiple benchmarks. The system is currently in use in many applications at Google, and is available as a user input modality in Google Translate for Android.</p><p>5 0.20090429 <a title="415-tfidf-5" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<p>Author: Anand Mishra, Karteek Alahari, C.V. Jawahar</p><p>Abstract: We present an approach for the text-to-image retrieval problem based on textual content present in images. Given the recent developments in understanding text in images, an appealing approach to address this problem is to localize and recognize the text, and then query the database, as in a text retrieval problem. We show that such an approach, despite being based on state-of-the-artmethods, is insufficient, and propose a method, where we do not rely on an exact localization and recognition pipeline. We take a query-driven search approach, where we find approximate locations of characters in the text query, and then impose spatial constraints to generate a ranked list of images in the database. The retrieval performance is evaluated on public scene text datasets as well as three large datasets, namely IIIT scene text retrieval, Sports-10K and TV series-1M, we introduce.</p><p>6 0.12761192 <a title="415-tfidf-6" href="./iccv-2013-Recognizing_Text_with_Perspective_Distortion_in_Natural_Scenes.html">345 iccv-2013-Recognizing Text with Perspective Distortion in Natural Scenes</a></p>
<p>7 0.090051807 <a title="415-tfidf-7" href="./iccv-2013-A_Robust_Analytical_Solution_to_Isometric_Shape-from-Template_with_Focal_Length_Calibration.html">27 iccv-2013-A Robust Analytical Solution to Isometric Shape-from-Template with Focal Length Calibration</a></p>
<p>8 0.069344021 <a title="415-tfidf-8" href="./iccv-2013-Learning_Coupled_Feature_Spaces_for_Cross-Modal_Matching.html">235 iccv-2013-Learning Coupled Feature Spaces for Cross-Modal Matching</a></p>
<p>9 0.063841075 <a title="415-tfidf-9" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>10 0.057555623 <a title="415-tfidf-10" href="./iccv-2013-Constant_Time_Weighted_Median_Filtering_for_Stereo_Matching_and_Beyond.html">88 iccv-2013-Constant Time Weighted Median Filtering for Stereo Matching and Beyond</a></p>
<p>11 0.054122157 <a title="415-tfidf-11" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>12 0.051087961 <a title="415-tfidf-12" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>13 0.047610231 <a title="415-tfidf-13" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>14 0.044680282 <a title="415-tfidf-14" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>15 0.043702364 <a title="415-tfidf-15" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>16 0.043057788 <a title="415-tfidf-16" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>17 0.043056615 <a title="415-tfidf-17" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>18 0.041256137 <a title="415-tfidf-18" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>19 0.041022111 <a title="415-tfidf-19" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<p>20 0.040398132 <a title="415-tfidf-20" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.131), (1, -0.004), (2, 0.003), (3, -0.058), (4, 0.044), (5, 0.027), (6, 0.015), (7, -0.006), (8, -0.062), (9, -0.04), (10, 0.323), (11, -0.121), (12, 0.142), (13, 0.088), (14, 0.04), (15, 0.095), (16, -0.084), (17, 0.135), (18, -0.224), (19, 0.129), (20, 0.112), (21, 0.133), (22, 0.033), (23, -0.007), (24, 0.015), (25, -0.005), (26, -0.019), (27, 0.034), (28, 0.025), (29, 0.001), (30, 0.0), (31, 0.033), (32, -0.011), (33, 0.036), (34, 0.055), (35, 0.025), (36, 0.039), (37, 0.005), (38, 0.023), (39, -0.007), (40, -0.047), (41, -0.026), (42, 0.089), (43, -0.028), (44, -0.019), (45, 0.016), (46, -0.036), (47, 0.029), (48, 0.022), (49, -0.0)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96367264 <a title="415-lsi-1" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>Author: Lukáš Neumann, Jiri Matas</p><p>Abstract: An unconstrained end-to-end text localization and recognition method is presented. The method introduces a novel approach for character detection and recognition which combines the advantages of sliding-window and connected component methods. Characters are detected and recognized as image regions which contain strokes of specific orientations in a specific relative position, where the strokes are efficiently detected by convolving the image gradient field with a set of oriented bar filters. Additionally, a novel character representation efficiently calculated from the values obtained in the stroke detection phase is introduced. The representation is robust to shift at the stroke level, which makes it less sensitive to intra-class variations and the noise induced by normalizing character size and positioning. The effectiveness of the representation is demonstrated by the results achieved in the classification of real-world characters using an euclidian nearestneighbor classifier trained on synthetic data in a plain form. The method was evaluated on a standard dataset, where it achieves state-of-the-art results in both text localization and recognition.</p><p>same-paper 2 0.94508356 <a title="415-lsi-2" href="./iccv-2013-Text_Localization_in_Natural_Images_Using_Stroke_Feature_Transform_and_Text_Covariance_Descriptors.html">415 iccv-2013-Text Localization in Natural Images Using Stroke Feature Transform and Text Covariance Descriptors</a></p>
<p>Author: Weilin Huang, Zhe Lin, Jianchao Yang, Jue Wang</p><p>Abstract: In this paper, we present a new approach for text localization in natural images, by discriminating text and non-text regions at three levels: pixel, component and textline levels. Firstly, a powerful low-level filter called the Stroke Feature Transform (SFT) is proposed, which extends the widely-used Stroke Width Transform (SWT) by incorporating color cues of text pixels, leading to significantly enhanced performance on inter-component separation and intra-component connection. Secondly, based on the output of SFT, we apply two classifiers, a text component classifier and a text-line classifier, sequentially to extract text regions, eliminating the heuristic procedures that are commonly used in previous approaches. The two classifiers are built upon two novel Text Covariance Descriptors (TCDs) that encode both the heuristic properties and the statistical characteristics of text stokes. Finally, text regions are located by simply thresholding the text-line confident map. Our method was evaluated on two benchmark datasets: ICDAR 2005 and ICDAR 2011, and the corresponding F- , measure values are 0. 72 and 0. 73, respectively, surpassing previous methods in accuracy by a large margin.</p><p>3 0.89688367 <a title="415-lsi-3" href="./iccv-2013-Recognizing_Text_with_Perspective_Distortion_in_Natural_Scenes.html">345 iccv-2013-Recognizing Text with Perspective Distortion in Natural Scenes</a></p>
<p>Author: Trung Quy Phan, Palaiahnakote Shivakumara, Shangxuan Tian, Chew Lim Tan</p><p>Abstract: This paper presents an approach to text recognition in natural scene images. Unlike most existing works which assume that texts are horizontal and frontal parallel to the image plane, our method is able to recognize perspective texts of arbitrary orientations. For individual character recognition, we adopt a bag-of-keypoints approach, in which Scale Invariant Feature Transform (SIFT) descriptors are extracted densely and quantized using a pre-trained vocabulary. Following [1, 2], the context information is utilized through lexicons. We formulate word recognition as finding the optimal alignment between the set of characters and the list of lexicon words. Furthermore, we introduce a new dataset called StreetViewText-Perspective, which contains texts in street images with a great variety of viewpoints. Experimental results on public datasets and the proposed dataset show that our method significantly outperforms the state-of-the-art on perspective texts of arbitrary orientations.</p><p>4 0.88045883 <a title="415-lsi-4" href="./iccv-2013-PhotoOCR%3A_Reading_Text_in_Uncontrolled_Conditions.html">315 iccv-2013-PhotoOCR: Reading Text in Uncontrolled Conditions</a></p>
<p>Author: Alessandro Bissacco, Mark Cummins, Yuval Netzer, Hartmut Neven</p><p>Abstract: We describe PhotoOCR, a system for text extraction from images. Our particular focus is reliable text extraction from smartphone imagery, with the goal of text recognition as a user input modality similar to speech recognition. Commercially available OCR performs poorly on this task. Recent progress in machine learning has substantially improved isolated character classification; we build on this progress by demonstrating a complete OCR system using these techniques. We also incorporate modern datacenter-scale distributed language modelling. Our approach is capable of recognizing text in a variety of challenging imaging conditions where traditional OCR systems fail, notably in the presence of substantial blur, low resolution, low contrast, high image noise and other distortions. It also operates with low latency; mean processing time is 600 ms per image. We evaluate our system on public benchmark datasets for text extraction and outperform all previously reported results, more than halving the error rate on multiple benchmarks. The system is currently in use in many applications at Google, and is available as a user input modality in Google Translate for Android.</p><p>5 0.77103806 <a title="415-lsi-5" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<p>Author: Anand Mishra, Karteek Alahari, C.V. Jawahar</p><p>Abstract: We present an approach for the text-to-image retrieval problem based on textual content present in images. Given the recent developments in understanding text in images, an appealing approach to address this problem is to localize and recognize the text, and then query the database, as in a text retrieval problem. We show that such an approach, despite being based on state-of-the-artmethods, is insufficient, and propose a method, where we do not rely on an exact localization and recognition pipeline. We take a query-driven search approach, where we find approximate locations of characters in the text query, and then impose spatial constraints to generate a ranked list of images in the database. The retrieval performance is evaluated on public scene text datasets as well as three large datasets, namely IIIT scene text retrieval, Sports-10K and TV series-1M, we introduce.</p><p>6 0.70663118 <a title="415-lsi-6" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>7 0.48593074 <a title="415-lsi-7" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<p>8 0.47675022 <a title="415-lsi-8" href="./iccv-2013-Linear_Sequence_Discriminant_Analysis%3A_A_Model-Based_Dimensionality_Reduction_Method_for_Vector_Sequences.html">253 iccv-2013-Linear Sequence Discriminant Analysis: A Model-Based Dimensionality Reduction Method for Vector Sequences</a></p>
<p>9 0.37143943 <a title="415-lsi-9" href="./iccv-2013-Learning_Coupled_Feature_Spaces_for_Cross-Modal_Matching.html">235 iccv-2013-Learning Coupled Feature Spaces for Cross-Modal Matching</a></p>
<p>10 0.32740507 <a title="415-lsi-10" href="./iccv-2013-Fingerspelling_Recognition_with_Semi-Markov_Conditional_Random_Fields.html">170 iccv-2013-Fingerspelling Recognition with Semi-Markov Conditional Random Fields</a></p>
<p>11 0.26164347 <a title="415-lsi-11" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>12 0.24612987 <a title="415-lsi-12" href="./iccv-2013-Detecting_Irregular_Curvilinear_Structures_in_Gray_Scale_and_Color_Imagery_Using_Multi-directional_Oriented_Flux.html">112 iccv-2013-Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-directional Oriented Flux</a></p>
<p>13 0.24264954 <a title="415-lsi-13" href="./iccv-2013-Shape_Index_Descriptors_Applied_to_Texture-Based_Galaxy_Analysis.html">388 iccv-2013-Shape Index Descriptors Applied to Texture-Based Galaxy Analysis</a></p>
<p>14 0.24122152 <a title="415-lsi-14" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>15 0.24025406 <a title="415-lsi-15" href="./iccv-2013-Efficient_Image_Dehazing_with_Boundary_Constraint_and_Contextual_Regularization.html">135 iccv-2013-Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</a></p>
<p>16 0.23865475 <a title="415-lsi-16" href="./iccv-2013-Automatic_Kronecker_Product_Model_Based_Detection_of_Repeated_Patterns_in_2D_Urban_Images.html">55 iccv-2013-Automatic Kronecker Product Model Based Detection of Repeated Patterns in 2D Urban Images</a></p>
<p>17 0.2324068 <a title="415-lsi-17" href="./iccv-2013-SGTD%3A_Structure_Gradient_and_Texture_Decorrelating_Regularization_for_Image_Decomposition.html">364 iccv-2013-SGTD: Structure Gradient and Texture Decorrelating Regularization for Image Decomposition</a></p>
<p>18 0.22652622 <a title="415-lsi-18" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>19 0.22648478 <a title="415-lsi-19" href="./iccv-2013-Finding_Actors_and_Actions_in_Movies.html">166 iccv-2013-Finding Actors and Actions in Movies</a></p>
<p>20 0.21791288 <a title="415-lsi-20" href="./iccv-2013-Bayesian_Robust_Matrix_Factorization_for_Image_and_Video_Processing.html">60 iccv-2013-Bayesian Robust Matrix Factorization for Image and Video Processing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.054), (7, 0.327), (26, 0.066), (31, 0.094), (34, 0.012), (40, 0.013), (42, 0.096), (48, 0.01), (64, 0.049), (73, 0.039), (89, 0.122)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.79939604 <a title="415-lda-1" href="./iccv-2013-From_Semi-supervised_to_Transfer_Counting_of_Crowds.html">178 iccv-2013-From Semi-supervised to Transfer Counting of Crowds</a></p>
<p>Author: Chen Change Loy, Shaogang Gong, Tao Xiang</p><p>Abstract: Regression-based techniques have shown promising results for people counting in crowded scenes. However, most existing techniques require expensive and laborious data annotation for model training. In this study, we propose to address this problem from three perspectives: (1) Instead of exhaustively annotating every single frame, the most informative frames are selected for annotation automatically and actively. (2) Rather than learning from only labelled data, the abundant unlabelled data are exploited. (3) Labelled data from other scenes are employed to further alleviate the burden for data annotation. All three ideas are implemented in a unified active and semi-supervised regression framework with ability to perform transfer learning, by exploiting the underlying geometric structure of crowd patterns via manifold analysis. Extensive experiments validate the effectiveness of our approach.</p><p>same-paper 2 0.78301167 <a title="415-lda-2" href="./iccv-2013-Text_Localization_in_Natural_Images_Using_Stroke_Feature_Transform_and_Text_Covariance_Descriptors.html">415 iccv-2013-Text Localization in Natural Images Using Stroke Feature Transform and Text Covariance Descriptors</a></p>
<p>Author: Weilin Huang, Zhe Lin, Jianchao Yang, Jue Wang</p><p>Abstract: In this paper, we present a new approach for text localization in natural images, by discriminating text and non-text regions at three levels: pixel, component and textline levels. Firstly, a powerful low-level filter called the Stroke Feature Transform (SFT) is proposed, which extends the widely-used Stroke Width Transform (SWT) by incorporating color cues of text pixels, leading to significantly enhanced performance on inter-component separation and intra-component connection. Secondly, based on the output of SFT, we apply two classifiers, a text component classifier and a text-line classifier, sequentially to extract text regions, eliminating the heuristic procedures that are commonly used in previous approaches. The two classifiers are built upon two novel Text Covariance Descriptors (TCDs) that encode both the heuristic properties and the statistical characteristics of text stokes. Finally, text regions are located by simply thresholding the text-line confident map. Our method was evaluated on two benchmark datasets: ICDAR 2005 and ICDAR 2011, and the corresponding F- , measure values are 0. 72 and 0. 73, respectively, surpassing previous methods in accuracy by a large margin.</p><p>3 0.76235968 <a title="415-lda-3" href="./iccv-2013-Pose_Estimation_with_Unknown_Focal_Length_Using_Points%2C_Directions_and_Lines.html">323 iccv-2013-Pose Estimation with Unknown Focal Length Using Points, Directions and Lines</a></p>
<p>Author: Yubin Kuang, Kalle Åström</p><p>Abstract: In this paper, we study the geometry problems of estimating camera pose with unknown focal length using combination of geometric primitives. We consider points, lines and also rich features such as quivers, i.e. points with one or more directions. We formulate the problems as polynomial systems where the constraints for different primitives are handled in a unified way. We develop efficient polynomial solvers for each of the derived cases with different combinations of primitives. The availability of these solvers enables robust pose estimation with unknown focal length for wider classes of features. Such rich features allow for fewer feature correspondences and generate larger inlier sets with higher probability. We demonstrate in synthetic experiments that our solvers are fast and numerically stable. For real images, we show that our solvers can be used in RANSAC loops to provide good initial solutions.</p><p>4 0.761976 <a title="415-lda-4" href="./iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning.html">212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</a></p>
<p>Author: Jiwen Lu, Gang Wang, Pierre Moulin</p><p>Abstract: This paper presents a new approach for image set classification, where each training and testing example contains a set of image instances of an object captured from varying viewpoints or under varying illuminations. While a number of image set classification methods have been proposed in recent years, most of them model each image set as a single linear subspace or mixture of linear subspaces, which may lose some discriminative information for classification. To address this, we propose exploring multiple order statistics as features of image sets, and develop a localized multikernel metric learning (LMKML) algorithm to effectively combine different order statistics information for classification. Our method achieves the state-of-the-art performance on four widely used databases including the Honda/UCSD, CMU Mobo, and Youtube face datasets, and the ETH-80 object dataset.</p><p>5 0.70943666 <a title="415-lda-5" href="./iccv-2013-No_Matter_Where_You_Are%3A_Flexible_Graph-Guided_Multi-task_Learning_for_Multi-view_Head_Pose_Classification_under_Target_Motion.html">291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</a></p>
<p>Author: Yan Yan, Elisa Ricci, Ramanathan Subramanian, Oswald Lanz, Nicu Sebe</p><p>Abstract: We propose a novel Multi-Task Learning framework (FEGA-MTL) for classifying the head pose of a person who moves freely in an environment monitored by multiple, large field-of-view surveillance cameras. As the target (person) moves, distortions in facial appearance owing to camera perspective and scale severely impede performance of traditional head pose classification methods. FEGA-MTL operates on a dense uniform spatial grid and learns appearance relationships across partitions as well as partition-specific appearance variations for a given head pose to build region-specific classifiers. Guided by two graphs which a-priori model appearance similarity among (i) grid partitions based on camera geometry and (ii) head pose classes, the learner efficiently clusters appearancewise related grid partitions to derive the optimal partitioning. For pose classification, upon determining the target’s position using a person tracker, the appropriate regionspecific classifier is invoked. Experiments confirm that FEGA-MTL achieves state-of-the-art classification with few training data.</p><p>6 0.67911744 <a title="415-lda-6" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>7 0.64862442 <a title="415-lda-7" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>8 0.62351942 <a title="415-lda-8" href="./iccv-2013-A_Robust_Analytical_Solution_to_Isometric_Shape-from-Template_with_Focal_Length_Calibration.html">27 iccv-2013-A Robust Analytical Solution to Isometric Shape-from-Template with Focal Length Calibration</a></p>
<p>9 0.59403944 <a title="415-lda-9" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>10 0.58942533 <a title="415-lda-10" href="./iccv-2013-Linear_Sequence_Discriminant_Analysis%3A_A_Model-Based_Dimensionality_Reduction_Method_for_Vector_Sequences.html">253 iccv-2013-Linear Sequence Discriminant Analysis: A Model-Based Dimensionality Reduction Method for Vector Sequences</a></p>
<p>11 0.58913749 <a title="415-lda-11" href="./iccv-2013-Complex_3D_General_Object_Reconstruction_from_Line_Drawings.html">84 iccv-2013-Complex 3D General Object Reconstruction from Line Drawings</a></p>
<p>12 0.58765501 <a title="415-lda-12" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>13 0.58448958 <a title="415-lda-13" href="./iccv-2013-Revisiting_the_PnP_Problem%3A_A_Fast%2C_General_and_Optimal_Solution.html">353 iccv-2013-Revisiting the PnP Problem: A Fast, General and Optimal Solution</a></p>
<p>14 0.58325046 <a title="415-lda-14" href="./iccv-2013-Lifting_3D_Manhattan_Lines_from_a_Single_Image.html">250 iccv-2013-Lifting 3D Manhattan Lines from a Single Image</a></p>
<p>15 0.57805347 <a title="415-lda-15" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>16 0.57692838 <a title="415-lda-16" href="./iccv-2013-Unsupervised_Intrinsic_Calibration_from_a_Single_Frame_Using_a_%22Plumb-Line%22_Approach.html">436 iccv-2013-Unsupervised Intrinsic Calibration from a Single Frame Using a "Plumb-Line" Approach</a></p>
<p>17 0.57668912 <a title="415-lda-17" href="./iccv-2013-Shortest_Paths_with_Curvature_and_Torsion.html">389 iccv-2013-Shortest Paths with Curvature and Torsion</a></p>
<p>18 0.57560682 <a title="415-lda-18" href="./iccv-2013-On_the_Mean_Curvature_Flow_on_Graphs_with_Applications_in_Image_and_Manifold_Processing.html">296 iccv-2013-On the Mean Curvature Flow on Graphs with Applications in Image and Manifold Processing</a></p>
<p>19 0.57325643 <a title="415-lda-19" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>20 0.57153875 <a title="415-lda-20" href="./iccv-2013-PhotoOCR%3A_Reading_Text_in_Uncontrolled_Conditions.html">315 iccv-2013-PhotoOCR: Reading Text in Uncontrolled Conditions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
