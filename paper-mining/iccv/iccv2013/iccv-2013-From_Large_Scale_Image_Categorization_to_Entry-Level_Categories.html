<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>176 iccv-2013-From Large Scale Image Categorization to Entry-Level Categories</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-176" href="#">iccv2013-176</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>176 iccv-2013-From Large Scale Image Categorization to Entry-Level Categories</h1>
<br/><p>Source: <a title="iccv-2013-176-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Ordonez_From_Large_Scale_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C. Berg, Tamara L. Berg</p><p>Abstract: Entry level categories the labels people will use to name an object were originally defined and studied by psychologists in the 1980s. In this paper we study entrylevel categories at a large scale and learn the first models for predicting entry-level categories for images. Our models combine visual recognition predictions with proxies for word “naturalness ” mined from the enormous amounts of text on the web. We demonstrate the usefulness of our models for predicting nouns (entry-level words) associated with images by people. We also learn mappings between concepts predicted by existing visual recognition systems and entry-level concepts that could be useful for improving human-focused applications such as natural language image description or retrieval. – –</p><p>Reference: <a title="iccv-2013-176-reference" href="../iccv2013_reference/iccv-2013-From_Large_Scale_Image_Categorization_to_Entry-Level_Categories_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Entry level categories the labels people will use to name an object were originally defined and studied by psychologists in the 1980s. [sent-8, score-0.263]
</p><p>2 In this paper we study entrylevel categories at a large scale and learn the first models for predicting entry-level categories for images. [sent-9, score-0.489]
</p><p>3 We demonstrate the usefulness of our models for predicting nouns (entry-level words) associated with images by people. [sent-11, score-0.31]
</p><p>4 We also learn mappings between concepts predicted by existing visual recognition systems and entry-level concepts that could be useful for improving human-focused applications such as natural language image description or retrieval. [sent-12, score-0.315]
</p><p>5 Although far from solved, algorithms have now advanced to the point where they can recognize or localize thousands of object categories with reasonable accuracy [17, 4, 3, 12]. [sent-15, score-0.141]
</p><p>6 For instance, if a classifier were lucky enough to get the example in Figure 1 correct, it might output grampus griseus, while most people are more likely to simply say dolphin. [sent-18, score-0.129]
</p><p>7 This is closely related to ideas of basic and entry level categories formulated by psychologists such as Eleanor Rosch [18] and Stephan Kosslyn [11]. [sent-19, score-0.189]
</p><p>8 Mr Ed is a palomino, but also a horse, an equine, an odd-toed ungulate, a placental mammal, a mammal, and so on most people looking at Mr Ed would tend to call him a “horse”, his entry level category (unless they are fans of the show). [sent-22, score-0.152]
</p><p>9 In this paper we consider two related problems 1) learn–  –  grampus griseus  dolphin  Figure 1. [sent-24, score-0.181]
</p><p>10 Example translation between a WordNet based object category prediction and what people might call the depicted object. [sent-25, score-0.304]
</p><p>11 , leaf nodes in WordNet [9] to what people are likely to call them and 2) learning to map from outputs of thousands of noisy computer vision classifiers/detectors evaluated on an image to what a person is likely to call the image. [sent-28, score-0.346]
</p><p>12 Taken together, we are able to study patterns for choice of basic level categories at a much larger scale than previous psychology experiments. [sent-31, score-0.141]
</p><p>13 One key difference is that our approach allows a reward function over the WordNet hierarchy that is not monotonic along paths from the root to leaves. [sent-34, score-0.223]
</p><p>14 This allows reward based on factors including frequency of word use that are not monotonic along such paths in WordNet. [sent-35, score-0.199]
</p><p>15 This also al–  –  lows mappings to be learned from a WordNet leaf node, l, to natural word choices that are not along a path from lto the root, “entity”. [sent-36, score-0.337]
</p><p>16 However, for any specific end user application, the categories of objects, scenes, and attributes labeled in a particular dataset may not be the most useful predictions. [sent-41, score-0.141]
</p><p>17 One benefit of our work can be seen as exploring the problem of translating the outputs of a vision system trained with one vocabulary of labels (WordNet leaf nodes) to labels in a new vocabulary (commonly used visually descriptive nouns). [sent-42, score-0.533]
</p><p>18 Furthermore, we show that using noisy vision estimates for image content, our system can output words that are significantly closer to human annotations than either the raw noisy vision estimates or the results of using the state of the art hedging  system from Deng et al. [sent-44, score-0.183]
</p><p>19 Insights into Entry-Level Categories At first glance, the task of finding the entry-level categories may seem like a linguistic problem of finding a hypernym of any given word. [sent-48, score-0.201]
</p><p>20 Although there is a considerable conceptual connection between entry-level categories and hypernyms, there are two notable differences: 1. [sent-49, score-0.141]
</p><p>21 Entry-level categories are not confined by (inherited) hypernyms, in part because encyclopedic knowledge is different from common sense knowledge. [sent-53, score-0.254]
</p><p>22 In section 3 we learn translations between leaf node concepts and entry-level concepts. [sent-64, score-0.613]
</p><p>23 Obtaining Natural Categories from Humans We use Amazon Mechanical Turk to crowd source translations of ImageNet synsets into entry-level categories D = {xi , yi | xi is a leaf node, yi is a word}. [sent-68, score-0.641]
</p><p>24 Results are obtained for 500 ImageNet synsets and aggregated across 8 users per task. [sent-70, score-0.212]
</p><p>25 We found agreement (measured as at least 3 of 8 users in agreement) among users for 447 of the 500 concepts, indicating that even though there are many potential labels for each synset (e. [sent-71, score-0.37]
</p><p>26 Cheap and easy online crowdsourcing enables us to gather these labels for a much larger set of (500) concepts than previous experiments. [sent-75, score-0.196]
</p><p>27 Furthermore, we use the results of our experiments to automatically learn generalizations to a substantially larger set of ImageNet synsets in section 3. [sent-76, score-0.157]
</p><p>28 Translating Encyclopedic Concepts to Entry-Level Concepts Our objective in this section is to discover mappings between encyclopedic concepts (ImageNet leaf categories, e. [sent-78, score-0.52]
</p><p>29 Chlorophyllum molybdites) to output concepts that are more natural (e. [sent-80, score-0.135]
</p><p>30 1we present an approach that relies on the wordnet hierarchy and frequency of words in a web scale corpus. [sent-84, score-0.35]
</p><p>31 Specifically, for a synset w, we quantify “naturalness” as the maximum log count φ(w) of all of the terms in the synset. [sent-92, score-0.157]
</p><p>32 More specifically, we define ψ(w, v) as a function that measures the distance between leaf node v and node w in the hypernym structure. [sent-94, score-0.557]
</p><p>33 Then the translation function τ(v, λ) : V → W maps a leaf node v to a target node w by maximizing a t Wrade m-oapfsf abe ltewaefe nno dnea tvu traoln ae tassr aentd n osedema wnt bicy proximity. [sent-95, score-0.662]
</p><p>34 Left: shows the relationship between parameter λ and the target vocabulary size. [sent-108, score-0.135]
</p><p>35 Right: shows the relationship between parameter λ and agreement accuracy with human labeled synsets evaluated against the most agreed human label (red) and any human label (cyan). [sent-109, score-0.251]
</p><p>36 At a high level, increasing λ serves to encourage mappings to be close to the input node in the WordNet hierarchy, thereby increasing the vocabulary size and limiting the generalization of concepts. [sent-113, score-0.272]
</p><p>37 Conversely, “naturalness”, Φ(D, λ), increases initially and then decreases as too much specificity or generalization hurts the naturalness of the outputs. [sent-114, score-0.393]
</p><p>38 In Figure 2 the red line shows accuracy for predicting the most agreed upon  word for a synset, while the cyan line shows the accuracy for predicting any word collected from any user. [sent-116, score-0.324]
</p><p>39 Input Concept  Ngram-  2  cactus wren  4  buzzard, Buteo buteo whinchat, Saxicola rubetra  6  Weimaraner  7  Gordon setter  3  8 9 10 11 12 13  numbat, banded anteater, anteater rhea, Rhea americana Africanized bee, killer bee, Apis mellifera conger, conger eel merino, merino sheep Europ. [sent-117, score-0.309]
</p><p>40 Translations from ImageNet leaf node synset categories to entry level categories using our automatic approaches from sections 3. [sent-120, score-0.849]
</p><p>41 Visually-Informed Translation In this approach, for a given leaf synset v we sample a set of n = 100 images s = {I1, I2 , . [sent-125, score-0.384]
</p><p>42 , In} and pealech a image fi sn automatically easnn osta =ted wIith nouns N}i a =d {ni1, ni2 , . [sent-128, score-0.239]
</p><p>43 ∪e dN inn as keyword annotations for synset v and rank∪ ∪th Nem. [sent-136, score-0.157]
</p><p>44 We pick the most relevant keyword for each node v as the entry-level categorical translation. [sent-139, score-0.135]
</p><p>45 The models we propose are: 1) a method that combines “naturalness” measures computed from the web with direct estimates of visual content computed at leaf nodes and inferred for internal nodes (section 4. [sent-142, score-0.446]
</p><p>46 Prediction using Propagated Visual Estimates As our first method for predicting entry level categories for an image, we present a variation on the hedging approach [6]. [sent-148, score-0.443]
</p><p>47 In the hedging work, the output is the node with the maximum expected reward, where the reward is monotonic in the hierarchy and has been smoothed by adding a carefully chosen constant to the reward for all nodes. [sent-149, score-0.633]
</p><p>48 In our modification, we construct a non-monotonic reward γ based 2770  on naturalness and a smoothing offset that is scaled by the position in the hierarchy. [sent-150, score-0.433]
</p><p>49 These models predict presence or absence of 7404 leaf node concepts in the ImageNet hierarchy. [sent-152, score-0.541]
</p><p>50 Following the approach of [6], we compute estimates of visual content for internal nodes by hierarchically accumulating all predictions below a node:1  f(v,I) =⎨⎧v? [sent-153, score-0.221]
</p><p>51 ,I,I),), i f v v i s a an le ianfte nrnoadel node (3) Where Z(v) ⎩is the set of all leaf nodes under node v and I) is the output of a Platt-scaled decision value from a linear SVM trained for the category corresponding to input leaf node v. [sent-156, score-0.96]
</p><p>52 For entry-level category prediction on images, we would like to maximize both “naturalness” and content estimates. [sent-161, score-0.182]
</p><p>53 For example, text based “naturalness” will tell us that both cat and dog are good entry level categories, but a confident visual prediction for German shepherd for an image tells us that dog is a much better entry-level prediction than cat for that image. [sent-162, score-0.476]
</p><p>54 Therefore, for an input image, we want to output a set of concepts that have a large prediction for both “naturalness” and content estimate score. [sent-163, score-0.274]
</p><p>55 For our experiments we output the top 5 Wordnet synsets according to: fnat(v, I,  λˆ)  λˆ)  λˆ) −λˆψ˜(v)]  = f(v, I)γ(v,  (5)  fnat(v, I, = f(v, I)[φ(v) (6) As we change we expect similar behavior to our web based concept translations (section 3. [sent-164, score-0.318]
</p><p>56 We compare our framework to hedging [6] for different settings of λˆ. [sent-167, score-0.183]
</p><p>57 For a side by side comparison we modify hedging to output the top 5 synsets based on their scoring function. [sent-168, score-0.34]
</p><p>58 Here, the working vocabulary is the unique set of predicted labels output for  λˆ  λˆ  1This function might bias decisions toward internal nodes. [sent-169, score-0.195]
</p><p>59 Other alternatives could be explored to estimate internal node scores. [sent-170, score-0.177]
</p><p>60 Relationship between average precision agreement and working vocabulary size (on a set of 1000 images) for the hedging method [6] (blue) and our direct translation method (red). [sent-172, score-0.439]
</p><p>61 Results demonstrate (Figure 4) that under different parameter settings we consistently obtain much higher levels of precision for predicting entrylevel categories than hedging [6]. [sent-174, score-0.531]
</p><p>62 Prediction using Supervised Learning In the previous section we rely on wordnet structure to compute estimates of image content, especially for internal nodes. [sent-177, score-0.303]
</p><p>63 However, this is not always a good measure of content because: 1) The wordnet hierarchy doesn’t encode knowledge about some semantic relationships between objects (i. [sent-178, score-0.411]
</p><p>64 functional or contextual relationships), 2) Even with the vast coverage of 7404 ImageNet leaf nodes we are missing models for many potentially important entry-level categories that are not at the leaf level. [sent-180, score-0.653]
</p><p>65 As an alternative, we directly train models for entry-level categories from data where people have provided entrylevel labels in the form of nouns present in visually descriptive image captions. [sent-181, score-0.638]
</p><p>66 We postulate that these nouns represent examples of entry-level labels because they have been naturally annotated by people to describe what is present in an image. [sent-182, score-0.361]
</p><p>67 For estimating the presence of objects from our set of 7404 ImageNet leaf node categories we use the same models as the previous section with one additional consideration. [sent-185, score-0.503]
</p><p>68 r Winien tgh iemn aggregate thhee w wreinsudlotws across multiple bounding boxes by max pooling of visual concepts scores. [sent-187, score-0.135]
</p><p>69 Entry-level categories with their corresponding top weighted leaf node features after training an SVM on our noisy data and a visualization of weights grouped by an arbitrary categorization of leaf nodes. [sent-189, score-0.73]
</p><p>70 of the visual concept represented by the leaf node vj and bounding box bk on image . [sent-191, score-0.452]
</p><p>71 For training our D target categories, we obtain labels  I(i)  Y from the million captions by running a POS-tagger [1] and defining Yi = [yj | image ihas noun j] . [sent-192, score-0.244]
</p><p>72 This provides us with a target vocabulary that is both likely to contain entry-level categories (because we expect entrylevel category nouns to commonly occur in our visual descriptions) and to contain sufficient images for training effective recognition models. [sent-196, score-0.694]
</p><p>73 Since we are using human labels from real-world data, the frequency of words in our target vocabulary follows a power-law distribution. [sent-198, score-0.196]
</p><p>74 One of the drawbacks of using the ImageNet hierarchy to aggregate estimates of visual concepts (section 3) is that it ignores more complex relationships between concepts. [sent-213, score-0.224]
</p><p>75 For instance a concept like tree has a co-occurrence relationship with bird that may be useful for prediction. [sent-215, score-0.26]
</p><p>76 Given this large dataset of images with noisy visual predictions and text labels, we manage to learn quite good predictors of high-level content, even for categories with relatively high intra-class variation (e. [sent-222, score-0.201]
</p><p>77 Note that image labels come from caption nouns, so some images marked as correct predictions might not depict the target concept whereas some images marked as wrong predictions might actually depict the target category. [sent-236, score-0.312]
</p><p>78 Performance at predicting the union of labels provided by 3 Turkers on dataset A (random images) and Dataset B (images with high confidence scores). [sent-280, score-0.132]
</p><p>79 Performance at predicting the labels agreed upon by 2 (of 3) Turkers on dataset A (random images) and Dataset B (images with high confidence scores). [sent-283, score-0.184]
</p><p>80 ne ss and hierarchical aggregation fnat and b) supervised learning fsvm. [sent-434, score-0.136]
</p><p>81 target nouns D to the best matching synset concept. [sent-435, score-0.439]
</p><p>82 For each synset, v, we also associate its direct translation score, fnat (v, I, (section 4. [sent-436, score-0.258]
</p><p>83 This means that for all WordNet synsets we have a direct translation score, and for some synsets we have a mapped SVM score fsvm (v, I,θv) (for nodes not appearing in D we set this score to be zero). [sent-438, score-0.614]
</p><p>84 Likewise the SVM scoring function introduces some new concepts not present in the WordNet hierarchy that have a value of zero for fnat (v, I, ˆλ). [sent-439, score-0.36]
</p><p>85 Experimental Evaluation We evaluate learning general translations from encyclopedic to entry-level concepts (section 5. [sent-464, score-0.364]
</p><p>86 1) and predicting entry-level concepts for images (section 5. [sent-465, score-0.206]
</p><p>87 Evaluating Translations  We show sample results from each of our methods to learn concept translations in Figure 3 (more are included in the supplemental material). [sent-469, score-0.161]
</p><p>88 Even for categories like “marmot” most people named it “squirrel”. [sent-474, score-0.202]
</p><p>89 Overall, ngram translation agrees 37% of the time with human supplied translations and the SVM translation agrees 21% of the time, indicating that translation learning is non-trivial. [sent-475, score-0.482]
</p><p>90 Evaluating Image Entry-Level Predictions We measure the accuracy of our proposed entry-level category prediction methods by evaluating how well we can predict nouns freely associated with images by users on MTurk. [sent-478, score-0.459]
</p><p>91 3rd col shows predicted nouns using a standard multiclass flat-classifier. [sent-486, score-0.288]
</p><p>92 4th col shows nouns predicted by the method of [6]. [sent-487, score-0.288]
</p><p>93 6th col shows our SVM mapping predictions and finally the 7th column shows the labels predicted by our joint model. [sent-489, score-0.17]
</p><p>94 For each image, we instruct 3 users on MTurk to write down any nouns that are relevant to the image content. [sent-495, score-0.294]
</p><p>95 Because these annotations are free associations we observe a large and varied set of associated nouns  –  3610 distinct nouns total in our evaluation sets. [sent-496, score-0.478]
</p><p>96 We evaluate prediction of all nouns associated with an image by Turkers (Table 1) and prediction of nouns assigned by at least 2 of 3 Turkers (Table 2). [sent-498, score-0.634]
</p><p>97 Results show precision and recall for prediction on each of our Datasets, comparing: leaf node classification performance (flat classifier), the outputs of hedging [6], and our proposed entry-level category predictors (ngram-biased mapping, SVM mapping, and a joint model). [sent-503, score-0.666]
</p><p>98 In addition, we greatly outperform both leaf node classification and the hedging technique [6] (approximately doubling their performance on this task). [sent-510, score-0.545]
</p><p>99 Conclusion  Results indicate that our inferred concept translations are meaningful and that our models are able to predict entrylevel categories – the words people use to describe image content – for images. [sent-512, score-0.604]
</p><p>100 What does classifying more than 10,000 image categories tell us? [sent-530, score-0.141]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('naturalness', 0.341), ('wordnet', 0.261), ('nouns', 0.239), ('leaf', 0.227), ('hedging', 0.183), ('bird', 0.161), ('synset', 0.157), ('synsets', 0.157), ('imagenet', 0.153), ('categories', 0.141), ('entrylevel', 0.136), ('fnat', 0.136), ('node', 0.135), ('concepts', 0.135), ('translation', 0.122), ('fsvm', 0.12), ('translations', 0.116), ('encyclopedic', 0.113), ('rhea', 0.113), ('house', 0.104), ('reward', 0.092), ('vocabulary', 0.092), ('anteater', 0.09), ('ostrich', 0.09), ('hierarchy', 0.089), ('dog', 0.088), ('deng', 0.084), ('duck', 0.083), ('prediction', 0.078), ('turkers', 0.074), ('predicting', 0.071), ('grampus', 0.068), ('griseus', 0.068), ('hypernyms', 0.068), ('marmot', 0.068), ('word', 0.065), ('plant', 0.063), ('people', 0.061), ('labels', 0.061), ('content', 0.061), ('hypernym', 0.06), ('predictions', 0.06), ('nodes', 0.058), ('water', 0.058), ('swan', 0.056), ('users', 0.055), ('tree', 0.054), ('agreed', 0.052), ('specificity', 0.052), ('chat', 0.052), ('bee', 0.052), ('million', 0.051), ('col', 0.049), ('cat', 0.048), ('entry', 0.048), ('beach', 0.048), ('svm', 0.046), ('captioned', 0.046), ('captions', 0.046), ('acalln', 0.045), ('americana', 0.045), ('bcalln', 0.045), ('buteo', 0.045), ('camper', 0.045), ('conger', 0.045), ('disoem', 0.045), ('dolphin', 0.045), ('fjoint', 0.045), ('hawk', 0.045), ('mammal', 0.045), ('merino', 0.045), ('methodprecisiondataserte', 0.045), ('porch', 0.045), ('precisiondataserte', 0.045), ('rdmap', 0.045), ('snorkel', 0.045), ('snorkeling', 0.045), ('snvgmram', 0.045), ('squirrel', 0.045), ('stoop', 0.045), ('trailer', 0.045), ('vascular', 0.045), ('whinchat', 0.045), ('woody', 0.045), ('mappings', 0.045), ('bk', 0.045), ('concept', 0.045), ('predict', 0.044), ('noun', 0.043), ('category', 0.043), ('target', 0.043), ('monotonic', 0.042), ('internal', 0.042), ('agreement', 0.042), ('girl', 0.041), ('insect', 0.04), ('vhe', 0.04), ('sheep', 0.039), ('home', 0.039), ('berg', 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="176-tfidf-1" href="./iccv-2013-From_Large_Scale_Image_Categorization_to_Entry-Level_Categories.html">176 iccv-2013-From Large Scale Image Categorization to Entry-Level Categories</a></p>
<p>Author: Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C. Berg, Tamara L. Berg</p><p>Abstract: Entry level categories the labels people will use to name an object were originally defined and studied by psychologists in the 1980s. In this paper we study entrylevel categories at a large scale and learn the first models for predicting entry-level categories for images. Our models combine visual recognition predictions with proxies for word “naturalness ” mined from the enormous amounts of text on the web. We demonstrate the usefulness of our models for predicting nouns (entry-level words) associated with images by people. We also learn mappings between concepts predicted by existing visual recognition systems and entry-level concepts that could be useful for improving human-focused applications such as natural language image description or retrieval. – –</p><p>2 0.19482461 <a title="176-tfidf-2" href="./iccv-2013-Find_the_Best_Path%3A_An_Efficient_and_Accurate_Classifier_for_Image_Hierarchies.html">165 iccv-2013-Find the Best Path: An Efficient and Accurate Classifier for Image Hierarchies</a></p>
<p>Author: Min Sun, Wan Huang, Silvio Savarese</p><p>Abstract: Many methods have been proposed to solve the image classification problem for a large number of categories. Among them, methods based on tree-based representations achieve good trade-off between accuracy and test time efficiency. While focusing on learning a tree-shaped hierarchy and the corresponding set of classifiers, most of them [11, 2, 14] use a greedy prediction algorithm for test time efficiency. We argue that the dramatic decrease in accuracy at high efficiency is caused by the specific design choice of the learning and greedy prediction algorithms. In this work, we propose a classifier which achieves a better trade-off between efficiency and accuracy with a given tree-shaped hierarchy. First, we convert the classification problem as finding the best path in the hierarchy, and a novel branchand-bound-like algorithm is introduced to efficiently search for the best path. Second, we jointly train the classifiers using a novel Structured SVM (SSVM) formulation with additional bound constraints. As a result, our method achieves a significant 4.65%, 5.43%, and 4.07% (relative 24.82%, 41.64%, and 109.79%) improvement in accuracy at high efficiency compared to state-of-the-art greedy “tree-based” methods [14] on Caltech-256 [15], SUN [32] and ImageNet 1K [9] dataset, respectively. Finally, we show that our branch-and-bound-like algorithm naturally ranks the paths in the hierarchy (Fig. 8) so that users can further process them.</p><p>3 0.17952867 <a title="176-tfidf-3" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>Author: Yangqing Jia, Trevor Darrell</p><p>Abstract: Recent years have witnessed the success of large-scale image classification systems that are able to identify objects among thousands of possible labels. However, it is yet unclear how general classifiers such as ones trained on ImageNet can be optimally adapted to specific tasks, each of which only covers a semantically related subset of all the objects in the world. It is inefficient and suboptimal to retrain classifiers whenever a new task is given, and is inapplicable when tasks are not given explicitly, but implicitly specified as a set of image queries. In this paper we propose a novel probabilistic model that jointly identifies the underlying task and performs prediction with a lineartime probabilistic inference algorithm, given a set of query images from a latent task. We present efficient ways to estimate parameters for the model, and an open-source toolbox to train classifiers distributedly at a large scale. Empirical results based on the ImageNet data showed significant performance increase over several baseline algorithms.</p><p>4 0.16069153 <a title="176-tfidf-4" href="./iccv-2013-YouTube2Text%3A_Recognizing_and_Describing_Arbitrary_Activities_Using_Semantic_Hierarchies_and_Zero-Shot_Recognition.html">452 iccv-2013-YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition</a></p>
<p>Author: Sergio Guadarrama, Niveda Krishnamoorthy, Girish Malkarnenkar, Subhashini Venugopalan, Raymond Mooney, Trevor Darrell, Kate Saenko</p><p>Abstract: Despite a recent push towards large-scale object recognition, activity recognition remains limited to narrow domains and small vocabularies of actions. In this paper, we tackle the challenge of recognizing and describing activities “in-the-wild”. We present a solution that takes a short video clip and outputs a brief sentence that sums up the main activity in the video, such as the actor, the action and its object. Unlike previous work, our approach works on out-of-domain actions: it does not require training videos of the exact activity. If it cannot find an accurate prediction for a pre-trained model, it finds a less specific answer that is also plausible from a pragmatic standpoint. We use semantic hierarchies learned from the data to help to choose an appropriate level of generalization, and priors learned from web-scale natural language corpora to penalize unlikely combinations of actors/actions/objects; we also use a web-scale language model to “fill in ” novel verbs, i.e. when the verb does not appear in the training set. We evaluate our method on a large YouTube corpus and demonstrate it is able to generate short sentence descriptions of video clips better than baseline approaches.</p><p>5 0.11932766 <a title="176-tfidf-5" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>Author: Mohamed Elhoseiny, Babak Saleh, Ahmed Elgammal</p><p>Abstract: The main question we address in this paper is how to use purely textual description of categories with no training images to learn visual classifiers for these categories. We propose an approach for zero-shot learning of object categories where the description of unseen categories comes in the form of typical text such as an encyclopedia entry, without the need to explicitly defined attributes. We propose and investigate two baseline formulations, based on regression and domain adaptation. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the classifier parameters for new classes. We applied the proposed approach on two fine-grained categorization datasets, and the results indicate successful classifier prediction.</p><p>6 0.11500888 <a title="176-tfidf-6" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>7 0.11370052 <a title="176-tfidf-7" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>8 0.11206681 <a title="176-tfidf-8" href="./iccv-2013-Translating_Video_Content_to_Natural_Language_Descriptions.html">428 iccv-2013-Translating Video Content to Natural Language Descriptions</a></p>
<p>9 0.11180268 <a title="176-tfidf-9" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>10 0.091671988 <a title="176-tfidf-10" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<p>11 0.084825329 <a title="176-tfidf-11" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>12 0.082387626 <a title="176-tfidf-12" href="./iccv-2013-NEIL%3A_Extracting_Visual_Knowledge_from_Web_Data.html">285 iccv-2013-NEIL: Extracting Visual Knowledge from Web Data</a></p>
<p>13 0.082226619 <a title="176-tfidf-13" href="./iccv-2013-ACTIVE%3A_Activity_Concept_Transitions_in_Video_Event_Classification.html">4 iccv-2013-ACTIVE: Activity Concept Transitions in Video Event Classification</a></p>
<p>14 0.079812974 <a title="176-tfidf-14" href="./iccv-2013-Global_Fusion_of_Relative_Motions_for_Robust%2C_Accurate_and_Scalable_Structure_from_Motion.html">184 iccv-2013-Global Fusion of Relative Motions for Robust, Accurate and Scalable Structure from Motion</a></p>
<p>15 0.078680456 <a title="176-tfidf-15" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>16 0.075986005 <a title="176-tfidf-16" href="./iccv-2013-Random_Forests_of_Local_Experts_for_Pedestrian_Detection.html">336 iccv-2013-Random Forests of Local Experts for Pedestrian Detection</a></p>
<p>17 0.07423386 <a title="176-tfidf-17" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>18 0.073976249 <a title="176-tfidf-18" href="./iccv-2013-How_Do_You_Tell_a_Blackbird_from_a_Crow%3F.html">202 iccv-2013-How Do You Tell a Blackbird from a Crow?</a></p>
<p>19 0.07289432 <a title="176-tfidf-19" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>20 0.071949102 <a title="176-tfidf-20" href="./iccv-2013-Detecting_Avocados_to_Zucchinis%3A_What_Have_We_Done%2C_and_Where_Are_We_Going%3F.html">109 iccv-2013-Detecting Avocados to Zucchinis: What Have We Done, and Where Are We Going?</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.169), (1, 0.083), (2, -0.006), (3, -0.066), (4, 0.109), (5, 0.035), (6, -0.041), (7, -0.021), (8, 0.013), (9, -0.078), (10, 0.017), (11, -0.066), (12, -0.011), (13, 0.031), (14, 0.049), (15, 0.041), (16, -0.024), (17, -0.046), (18, -0.016), (19, 0.064), (20, -0.063), (21, -0.025), (22, -0.026), (23, 0.115), (24, -0.043), (25, -0.036), (26, 0.034), (27, -0.07), (28, 0.002), (29, -0.042), (30, 0.155), (31, -0.16), (32, -0.055), (33, -0.092), (34, -0.067), (35, -0.027), (36, -0.082), (37, 0.059), (38, -0.081), (39, -0.064), (40, -0.065), (41, -0.013), (42, 0.013), (43, -0.033), (44, 0.022), (45, -0.005), (46, 0.051), (47, 0.034), (48, 0.054), (49, 0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93068188 <a title="176-lsi-1" href="./iccv-2013-From_Large_Scale_Image_Categorization_to_Entry-Level_Categories.html">176 iccv-2013-From Large Scale Image Categorization to Entry-Level Categories</a></p>
<p>Author: Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C. Berg, Tamara L. Berg</p><p>Abstract: Entry level categories the labels people will use to name an object were originally defined and studied by psychologists in the 1980s. In this paper we study entrylevel categories at a large scale and learn the first models for predicting entry-level categories for images. Our models combine visual recognition predictions with proxies for word “naturalness ” mined from the enormous amounts of text on the web. We demonstrate the usefulness of our models for predicting nouns (entry-level words) associated with images by people. We also learn mappings between concepts predicted by existing visual recognition systems and entry-level concepts that could be useful for improving human-focused applications such as natural language image description or retrieval. – –</p><p>2 0.81484747 <a title="176-lsi-2" href="./iccv-2013-YouTube2Text%3A_Recognizing_and_Describing_Arbitrary_Activities_Using_Semantic_Hierarchies_and_Zero-Shot_Recognition.html">452 iccv-2013-YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition</a></p>
<p>Author: Sergio Guadarrama, Niveda Krishnamoorthy, Girish Malkarnenkar, Subhashini Venugopalan, Raymond Mooney, Trevor Darrell, Kate Saenko</p><p>Abstract: Despite a recent push towards large-scale object recognition, activity recognition remains limited to narrow domains and small vocabularies of actions. In this paper, we tackle the challenge of recognizing and describing activities “in-the-wild”. We present a solution that takes a short video clip and outputs a brief sentence that sums up the main activity in the video, such as the actor, the action and its object. Unlike previous work, our approach works on out-of-domain actions: it does not require training videos of the exact activity. If it cannot find an accurate prediction for a pre-trained model, it finds a less specific answer that is also plausible from a pragmatic standpoint. We use semantic hierarchies learned from the data to help to choose an appropriate level of generalization, and priors learned from web-scale natural language corpora to penalize unlikely combinations of actors/actions/objects; we also use a web-scale language model to “fill in ” novel verbs, i.e. when the verb does not appear in the training set. We evaluate our method on a large YouTube corpus and demonstrate it is able to generate short sentence descriptions of video clips better than baseline approaches.</p><p>3 0.71755379 <a title="176-lsi-3" href="./iccv-2013-Find_the_Best_Path%3A_An_Efficient_and_Accurate_Classifier_for_Image_Hierarchies.html">165 iccv-2013-Find the Best Path: An Efficient and Accurate Classifier for Image Hierarchies</a></p>
<p>Author: Min Sun, Wan Huang, Silvio Savarese</p><p>Abstract: Many methods have been proposed to solve the image classification problem for a large number of categories. Among them, methods based on tree-based representations achieve good trade-off between accuracy and test time efficiency. While focusing on learning a tree-shaped hierarchy and the corresponding set of classifiers, most of them [11, 2, 14] use a greedy prediction algorithm for test time efficiency. We argue that the dramatic decrease in accuracy at high efficiency is caused by the specific design choice of the learning and greedy prediction algorithms. In this work, we propose a classifier which achieves a better trade-off between efficiency and accuracy with a given tree-shaped hierarchy. First, we convert the classification problem as finding the best path in the hierarchy, and a novel branchand-bound-like algorithm is introduced to efficiently search for the best path. Second, we jointly train the classifiers using a novel Structured SVM (SSVM) formulation with additional bound constraints. As a result, our method achieves a significant 4.65%, 5.43%, and 4.07% (relative 24.82%, 41.64%, and 109.79%) improvement in accuracy at high efficiency compared to state-of-the-art greedy “tree-based” methods [14] on Caltech-256 [15], SUN [32] and ImageNet 1K [9] dataset, respectively. Finally, we show that our branch-and-bound-like algorithm naturally ranks the paths in the hierarchy (Fig. 8) so that users can further process them.</p><p>4 0.68316782 <a title="176-lsi-4" href="./iccv-2013-Translating_Video_Content_to_Natural_Language_Descriptions.html">428 iccv-2013-Translating Video Content to Natural Language Descriptions</a></p>
<p>Author: Marcus Rohrbach, Wei Qiu, Ivan Titov, Stefan Thater, Manfred Pinkal, Bernt Schiele</p><p>Abstract: Humans use rich natural language to describe and communicate visual perceptions. In order to provide natural language descriptions for visual content, this paper combines two important ingredients. First, we generate a rich semantic representation of the visual content including e.g. object and activity labels. To predict the semantic representation we learn a CRF to model the relationships between different components of the visual input. And second, we propose to formulate the generation of natural language as a machine translation problem using the semantic representation as source language and the generated sentences as target language. For this we exploit the power of a parallel corpus of videos and textual descriptions and adapt statistical machine translation to translate between our two languages. We evaluate our video descriptions on the TACoS dataset [23], which contains video snippets aligned with sentence descriptions. Using automatic evaluation and human judgments we show significant improvements over several baseline approaches, motivated by prior work. Our translation approach also shows improvements over related work on an image description task.</p><p>5 0.67606449 <a title="176-lsi-5" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>Author: Mohamed Elhoseiny, Babak Saleh, Ahmed Elgammal</p><p>Abstract: The main question we address in this paper is how to use purely textual description of categories with no training images to learn visual classifiers for these categories. We propose an approach for zero-shot learning of object categories where the description of unseen categories comes in the form of typical text such as an encyclopedia entry, without the need to explicitly defined attributes. We propose and investigate two baseline formulations, based on regression and domain adaptation. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the classifier parameters for new classes. We applied the proposed approach on two fine-grained categorization datasets, and the results indicate successful classifier prediction.</p><p>6 0.61935776 <a title="176-lsi-6" href="./iccv-2013-How_Do_You_Tell_a_Blackbird_from_a_Crow%3F.html">202 iccv-2013-How Do You Tell a Blackbird from a Crow?</a></p>
<p>7 0.6166538 <a title="176-lsi-7" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>8 0.60318875 <a title="176-lsi-8" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>9 0.57729757 <a title="176-lsi-9" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>10 0.54587668 <a title="176-lsi-10" href="./iccv-2013-Revisiting_Example_Dependent_Cost-Sensitive_Learning_with_Decision_Trees.html">352 iccv-2013-Revisiting Example Dependent Cost-Sensitive Learning with Decision Trees</a></p>
<p>11 0.54448503 <a title="176-lsi-11" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<p>12 0.5415321 <a title="176-lsi-12" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>13 0.53520644 <a title="176-lsi-13" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<p>14 0.52099156 <a title="176-lsi-14" href="./iccv-2013-Image_Segmentation_with_Cascaded_Hierarchical_Models_and_Logistic_Disjunctive_Normal_Networks.html">211 iccv-2013-Image Segmentation with Cascaded Hierarchical Models and Logistic Disjunctive Normal Networks</a></p>
<p>15 0.51411748 <a title="176-lsi-15" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>16 0.50579798 <a title="176-lsi-16" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>17 0.50485915 <a title="176-lsi-17" href="./iccv-2013-Learning_to_Rank_Using_Privileged_Information.html">248 iccv-2013-Learning to Rank Using Privileged Information</a></p>
<p>18 0.50318694 <a title="176-lsi-18" href="./iccv-2013-Fingerspelling_Recognition_with_Semi-Markov_Conditional_Random_Fields.html">170 iccv-2013-Fingerspelling Recognition with Semi-Markov Conditional Random Fields</a></p>
<p>19 0.49636459 <a title="176-lsi-19" href="./iccv-2013-Detecting_Avocados_to_Zucchinis%3A_What_Have_We_Done%2C_and_Where_Are_We_Going%3F.html">109 iccv-2013-Detecting Avocados to Zucchinis: What Have We Done, and Where Are We Going?</a></p>
<p>20 0.4841018 <a title="176-lsi-20" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.08), (12, 0.032), (13, 0.011), (26, 0.073), (31, 0.032), (34, 0.024), (42, 0.085), (48, 0.011), (64, 0.039), (65, 0.295), (73, 0.035), (77, 0.021), (89, 0.148), (98, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7441172 <a title="176-lda-1" href="./iccv-2013-From_Large_Scale_Image_Categorization_to_Entry-Level_Categories.html">176 iccv-2013-From Large Scale Image Categorization to Entry-Level Categories</a></p>
<p>Author: Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C. Berg, Tamara L. Berg</p><p>Abstract: Entry level categories the labels people will use to name an object were originally defined and studied by psychologists in the 1980s. In this paper we study entrylevel categories at a large scale and learn the first models for predicting entry-level categories for images. Our models combine visual recognition predictions with proxies for word “naturalness ” mined from the enormous amounts of text on the web. We demonstrate the usefulness of our models for predicting nouns (entry-level words) associated with images by people. We also learn mappings between concepts predicted by existing visual recognition systems and entry-level concepts that could be useful for improving human-focused applications such as natural language image description or retrieval. – –</p><p>2 0.67728543 <a title="176-lda-2" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<p>Author: Xiatian Zhu, Chen Change Loy, Shaogang Gong</p><p>Abstract: Generating coherent synopsis for surveillance video stream remains a formidable challenge due to the ambiguity and uncertainty inherent to visual observations. In contrast to existing video synopsis approaches that rely on visual cues alone, we propose a novel multi-source synopsis framework capable of correlating visual data and independent non-visual auxiliary information to better describe and summarise subtlephysical events in complex scenes. Specifically, our unsupervised framework is capable of seamlessly uncovering latent correlations among heterogeneous types of data sources, despite the non-trivial heteroscedasticity and dimensionality discrepancy problems. Additionally, the proposed model is robust to partial or missing non-visual information. We demonstrate the effectiveness of our framework on two crowded public surveillance datasets.</p><p>3 0.57453477 <a title="176-lda-3" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>Author: S. Karthikeyan, Vignesh Jagadeesh, Renuka Shenoy, Miguel Ecksteinz, B.S. Manjunath</p><p>Abstract: Eye movement studies have confirmed that overt attention is highly biased towards faces and text regions in images. In this paper we explore a novel problem of predicting face and text regions in images using eye tracking data from multiple subjects. The problem is challenging as we aim to predict the semantics (face/text/background) only from eye tracking data without utilizing any image information. The proposed algorithm spatially clusters eye tracking data obtained in an image into different coherent groups and subsequently models the likelihood of the clusters containing faces and text using afully connectedMarkov Random Field (MRF). Given the eye tracking datafrom a test image, itpredicts potential face/head (humans, dogs and cats) and text locations reliably. Furthermore, the approach can be used to select regions of interest for further analysis by object detectors for faces and text. The hybrid eye position/object detector approach achieves better detection performance and reduced computation time compared to using only the object detection algorithm. We also present a new eye tracking dataset on 300 images selected from ICDAR, Street-view, Flickr and Oxford-IIIT Pet Dataset from 15 subjects.</p><p>4 0.56963784 <a title="176-lda-4" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>Author: Suyog Dutt Jain, Kristen Grauman</p><p>Abstract: The mode of manual annotation used in an interactive segmentation algorithm affects both its accuracy and easeof-use. For example, bounding boxes are fast to supply, yet may be too coarse to get good results on difficult images; freehand outlines are slower to supply and more specific, yet they may be overkill for simple images. Whereas existing methods assume a fixed form of input no matter the image, we propose to predict the tradeoff between accuracy and effort. Our approach learns whether a graph cuts segmentation will succeed if initialized with a given annotation mode, based on the image ’s visual separability and foreground uncertainty. Using these predictions, we optimize the mode of input requested on new images a user wants segmented. Whether given a single image that should be segmented as quickly as possible, or a batch of images that must be segmented within a specified time budget, we show how to select the easiest modality that will be sufficiently strong to yield high quality segmentations. Extensive results with real users and three datasets demonstrate the impact.</p><p>5 0.56870472 <a title="176-lda-5" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>Author: Jimei Yang, Yi-Hsuan Tsai, Ming-Hsuan Yang</p><p>Abstract: We present a hybrid parametric and nonparametric algorithm, exemplar cut, for generating class-specific object segmentation hypotheses. For the parametric part, we train a pylon model on a hierarchical region tree as the energy function for segmentation. For the nonparametric part, we match the input image with each exemplar by using regions to obtain a score which augments the energy function from the pylon model. Our method thus generates a set of highly plausible segmentation hypotheses by solving a series of exemplar augmented graph cuts. Experimental results on the Graz and PASCAL datasets show that the proposed algorithm achievesfavorable segmentationperformance against the state-of-the-art methods in terms of visual quality and accuracy.</p><p>6 0.56866729 <a title="176-lda-6" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>7 0.56737411 <a title="176-lda-7" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>8 0.56737208 <a title="176-lda-8" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<p>9 0.56735909 <a title="176-lda-9" href="./iccv-2013-Beyond_Hard_Negative_Mining%3A_Efficient_Detector_Learning_via_Block-Circulant_Decomposition.html">61 iccv-2013-Beyond Hard Negative Mining: Efficient Detector Learning via Block-Circulant Decomposition</a></p>
<p>10 0.56728667 <a title="176-lda-10" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>11 0.56710565 <a title="176-lda-11" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>12 0.56710345 <a title="176-lda-12" href="./iccv-2013-NEIL%3A_Extracting_Visual_Knowledge_from_Web_Data.html">285 iccv-2013-NEIL: Extracting Visual Knowledge from Web Data</a></p>
<p>13 0.56699049 <a title="176-lda-13" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>14 0.5668723 <a title="176-lda-14" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>15 0.56627953 <a title="176-lda-15" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>16 0.56615269 <a title="176-lda-16" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>17 0.56613618 <a title="176-lda-17" href="./iccv-2013-Ensemble_Projection_for_Semi-supervised_Image_Classification.html">142 iccv-2013-Ensemble Projection for Semi-supervised Image Classification</a></p>
<p>18 0.56576204 <a title="176-lda-18" href="./iccv-2013-A_Convex_Optimization_Framework_for_Active_Learning.html">6 iccv-2013-A Convex Optimization Framework for Active Learning</a></p>
<p>19 0.56566077 <a title="176-lda-19" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>20 0.56529999 <a title="176-lda-20" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
