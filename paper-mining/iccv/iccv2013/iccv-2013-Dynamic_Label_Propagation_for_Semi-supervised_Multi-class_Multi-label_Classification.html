<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-126" href="#">iccv2013-126</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</h1>
<br/><p>Source: <a title="iccv-2013-126-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Wang_Dynamic_Label_Propagation_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Bo Wang, Zhuowen Tu, John K. Tsotsos</p><p>Abstract: In graph-based semi-supervised learning approaches, the classification rate is highly dependent on the size of the availabel labeled data, as well as the accuracy of the similarity measures. Here, we propose a semi-supervised multi-class/multi-label classification scheme, dynamic label propagation (DLP), which performs transductive learning through propagation in a dynamic process. Existing semi-supervised classification methods often have difficulty in dealing with multi-class/multi-label problems due to the lack in consideration of label correlation; our algorithm instead emphasizes dynamic metric fusion with label information. Significant improvement over the state-of-the-art methods is observed on benchmark datasets for both multiclass and multi-label tasks.</p><p>Reference: <a title="iccv-2013-126-reference" href="../iccv2013_reference/iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ca  Abstract In graph-based semi-supervised learning approaches, the classification rate is highly dependent on the size of the availabel labeled data, as well as the accuracy of the similarity measures. [sent-7, score-0.357]
</p><p>2 Here, we propose a semi-supervised multi-class/multi-label classification scheme, dynamic label propagation (DLP), which performs transductive learning through propagation in a dynamic process. [sent-8, score-1.027]
</p><p>3 Existing semi-supervised classification methods often have difficulty in dealing with multi-class/multi-label problems due to the lack in consideration of label correlation; our algorithm instead emphasizes dynamic metric fusion with label information. [sent-9, score-0.813]
</p><p>4 Moreover, nice properties enjoyed by graph-based (built on the distance metric) two-class semi-supervised classification  [37] become less obvious in the multi-class classification situations [11], due to the correlations of the multiple labels. [sent-13, score-0.239]
</p><p>5 Supervised metric learning methods often learn a Mahalanobis distance by encouraging small distances among points of the same label while maintaining large distances for points of different labels [30, 29]. [sent-14, score-0.414]
</p><p>6 Graph-based semisupervised learning frameworks on the other hand utilize a limited amount of labeled data to explore information on a large volume of unlabeled data. [sent-15, score-0.383]
</p><p>7 Label propagation (LP) [37] specifically assumes that nodes connected by edges of large similarity tend to have the same label through information propagated within the graph. [sent-16, score-0.551]
</p><p>8 A wide range of applications such as classification, ranking, and retrieval [38] have adopted the label propagation strategy. [sent-17, score-0.455]
</p><p>9 was at  York Uni-  view features to help each other by pulling out unlabeled data to re-train and enhance the classifiers. [sent-19, score-0.167]
</p><p>10 For the multi-class/multilabel case, the label propagation algorithm [37] becomes more problematic, therefore some special care needs to be taken. [sent-21, score-0.455]
</p><p>11 all approaches is, however, that the  correlations among different classes are not fully utilized. [sent-25, score-0.175]
</p><p>12 In this paper, we propose a new method, dynamic label propagation (DLP), to simultaneously deal with the multiclass and multi-label problem. [sent-27, score-0.618]
</p><p>13 Our method incorporates the label correlations and instance similarities into a new way of performing label propagation. [sent-28, score-0.668]
</p><p>14 Our intuition in DLP is to update the similarity measures dynamically by fusing multi-label/multi-class information, which can be understood in a probabilistic framework. [sent-29, score-0.128]
</p><p>15 The K nearest neighbor (KNN) matrix is used to preserve the intrinsic structure of the input data. [sent-30, score-0.117]
</p><p>16 Transduction by Laplacian graph [4, 10] is also shown to be able to solve multi-class semi-supervised problems; although these algorithms make use of the relationship between unlabeled and labeled data, their computational complexity is demanding, e. [sent-41, score-0.238]
</p><p>17 However, there are much fewer attempts to tackle semisupervised multi-label problem, despite there being a rich body of literature about supervised multi-label learning. [sent-44, score-0.11]
</p><p>18 One popular method is label ranking [8], which learns a ranking function of category labels from the labeled in-  stances and classifying each unlabeled instance by thresholding the scores ofthe learned ranking functions. [sent-45, score-0.652]
</p><p>19 Although being easy to scale up, label ranking fails to exploit the correlations among data categories. [sent-46, score-0.449]
</p><p>20 Recently, category correlations are given more attention in multi-label learning. [sent-47, score-0.133]
</p><p>21 A maximum entropy method is employed to model the correlations among categories in [36]. [sent-48, score-0.134]
</p><p>22 [19] studies a hierarchical structure to handle the correlation information. [sent-49, score-0.114]
</p><p>23 In [13], a correlated label propagation framework is developed for multi-label learning that explicitly fuses the information of different classes. [sent-50, score-0.567]
</p><p>24 However, these methods are only for supervised learning, and how to make use of label correlation among unlabeled instances is still unclear. [sent-51, score-0.592]
</p><p>25 [17] uses constrained non-negative matrix factorization to propagate the label information by enforcing the examples with similar input patterns to share similar sets of class labels. [sent-52, score-0.338]
</p><p>26 Another semi-supervised multi-label learning technique [7] develops a regularization with two energy terms about smoothness of input instances and label information by solving a Sylvester Equation. [sent-53, score-0.384]
</p><p>27 Different from these semi-supervised multi-label methods, the proposed method explicitly merges the input data and label correlations. [sent-55, score-0.305]
</p><p>28 Moreover, by doing projection on the fused manifolds, DLP further takes advantage of the correlations among labeling information of unlabeled data. [sent-56, score-0.347]
</p><p>29 Our work also differs significantly from a very recent algorithm  [14], which emphasizes the learning of fusion parameters for unlabeled data; the focus here is however the dynamic update of the similarity functions from both data and label information. [sent-57, score-0.716]
</p><p>30 First, the multi-label problem considers the label correlations, but it may lead to a loss in the discrimination power of the multi-class classifiers. [sent-61, score-0.247]
</p><p>31 The proposed dynamic label propagation method (DLP) aims to solve semi-supervised multiclass and multi-label problem simultaneously by combining the discriminative graph similarities and the label correlations in a dynamic way, while preserving the intrinsic structure of input data. [sent-63, score-1.201]
</p><p>32 Label Propagation First, a brief introduction of the well-known label propagation algorithm is provided in this section. [sent-66, score-0.455]
</p><p>33 If ρ is a distance metric defined on the graph, then the similarities matrix can be constructed as follows:  W(i,j) = h(ρ(xμiσ,x2j)2),  (1)  for some function h with exponential decay at infinity. [sent-74, score-0.173]
</p><p>34 A natural transition matrix on V can be defined by normalizing the weight matrix as:  P(i,j) =? [sent-79, score-0.232]
</p><p>35 , the labels of labeled data must be reset after each iteration. [sent-92, score-0.143]
</p><p>36 The= =ma Pin algorithm of label propagation is summarized in Fig. [sent-97, score-0.455]
</p><p>37 Essentially we make the assumption that local similarities (high values) are more reliable than far-away ones; and accordingly local similarities can be propagated to non-local points through a diffusion process on the graph. [sent-110, score-0.419]
</p><p>38 e full pair-wise similarity information among the data whereas P only encodes the similarity ttioo nearby gda tthae points. [sent-116, score-0.16]
</p><p>39 hFeo rro cblausr-t ity, we call P the status matrix and P the corresponding KityN,N w me catarlilx. [sent-119, score-0.114]
</p><p>40 Label Fusion on Diffusion Space One disadvantage of label propagation is that it does not work well on multi-class/multi-label classification problem due to a lack of interplay among labels within different classes. [sent-122, score-0.66]
</p><p>41 In this paper, we propose a dynamic version of label propagation that aims to improve the effectiveness on multi-class/multi-label classification. [sent-123, score-0.552]
</p><p>42 Our main idea is to have an improved transition matrix by fusing information of both data features and data labels in each iteration. [sent-124, score-0.325]
</p><p>43 Given the kernel Pt, where t denotes the number of it-  erations, we can define the diffusion distance [15] at time t as: Dt(i, j) =? [sent-125, score-0.321]
</p><p>44 (5) The diffusion process maps the data space into an ndimensional space Rtn in which each data point is represented by its transition probability to the other data points. [sent-128, score-0.435]
</p><p>45 It is reasonable to assume that for each data xt ∈ Rtn, we have p(xt) = N(xt |μt, Pt), where μt is unkno∈wn R. [sent-129, score-0.151]
</p><p>46 Note that the lab)el = =m Natr(ixx Y|μt contains information about class labels, and the correlation of these labels KY = YtYtT can be viewed as the similarity between data points in the label space Qtn, and data points in this label space Qtn have the probability p(yt) = N(yt |0, Kt). [sent-130, score-0.767]
</p><p>47 The first part of dynamic label propagation is the fusion of the status matrix Pt and the label kernel KY = YtYtT. [sent-132, score-1.073]
</p><p>48 (6) This operation corresponds to an addition operator in the diffusion spaces: zt  =  xt  + √αyt. [sent-135, score-0.477]
</p><p>49 (8)  This simple fusion technique considers the correlation among the instance label vectors. [sent-137, score-0.469]
</p><p>50 The underlying assumption is that two instances with high correlated label vectors tend to have high similarity in the input data space. [sent-138, score-0.458]
</p><p>51 The correlation between label vectors can represent the label dependency among instances, especially for the multi-label/multiclass problem. [sent-139, score-0.635]
</p><p>52 The advantage of fusing transition kernel and the label correlation is two-fold: On one hand, two instances with high correlated label vectors are likely to have high similarity in input data space, this fusion process therefore enhances the fitness of the kernel matrix for the input manifold. [sent-140, score-1.308]
</p><p>53 On the other hand, the resulting kernel matrix leads to better label information through next round of label propagation. [sent-141, score-0.657]
</p><p>54 In this way, we build up a dynamic interaction process between the feature space and label space. [sent-142, score-0.344]
</p><p>55 However, since the label information is dynamically updated during the propagation process, the resulting label information after the initial several rounds no longer improves the transition matrix, sometimes even makes it worse. [sent-143, score-0.842]
</p><p>56 Assume P0 is the initial status matrix of the input data calculated using (1) and (2), and P = KNN(P0) by (3) acnaldc (u4la);t Wde u employ th anisd l (in2e)a,r a operator P K tNo Ndo( Pthe projec-  tainodn = Pzt + λtε, (9) where ε is white noise, i. [sent-146, score-0.172]
</p><p>57 (11)  The above equation implies that, the essence of dynamic label propagation is to do linear operations on diffusion space iteratively. [sent-158, score-0.794]
</p><p>58 Note that xt+1 is a point in the diffusion space. [sent-159, score-0.242]
</p><p>59 Instead of performing linear projection in the original data space, we do projection in the diffusion space. [sent-160, score-0.341]
</p><p>60 The advantages of projection onto the diffusion space are two-fold: 1) we avoid the need to perform computational expensive sampling procedures in the input space; 2) The resulting variance matrix again is a good diffusion kernel for label propagation. [sent-161, score-0.937]
</p><p>61 The intuition behind this projection lies in the fact that  simple fusion of label correlation in Eqn. [sent-162, score-0.478]
</p><p>62 (6) would result in a degeneration at the first round when the learned label information of unlabeled data is not accurate enough to infer the similarities in the input space. [sent-163, score-0.536]
</p><p>63 From (13), we can see that, the diffusion process propagates the similarities through the KNN matrix. [sent-165, score-0.309]
</p><p>64 In this way, we can adjust the fused kernel matrix to maintain part of the information of the initial structure. [sent-166, score-0.176]
</p><p>65 The direct reflection of this projection on diffusion space is that , at each iteration, we construct the transition matrix for next iteration to be: Pt+1 = P(Pt  + αYtYtT)PT + λtI. [sent-167, score-0.479]
</p><p>66 (13), we see that only information between dominant neighbours are propagated into the transition matrix of next iteration. [sent-175, score-0.28]
</p><p>67 An important observation is that if data iand j have common dominant neighbours in both similarity metrics, it is highly possible that they belong to the same class. [sent-176, score-0.145]
</p><p>68 We summarize the details of dynamic label propagation in Fig. [sent-177, score-0.552]
</p><p>69 The learned labels can improve the similarity between input instances quickly. [sent-190, score-0.2]
</p><p>70 A loose theoretical proof of convergence can be constructed based on the spectral analysis of the diffusion projection P. [sent-191, score-0.278]
</p><p>71 We have −  Yt  ∝  Y(∞)+[(P)t(P0+αY0Y0T)(PT)t]P0Y0+o(t)  (14)  where o(t) is an infinitesimal as t approaches infinity, and Y(∞) ∈ Rn×C is a constant label matrix. [sent-193, score-0.247]
</p><p>72 An easy way to speed up the diffusion process is, first we keep a record of the KNN matrix and then every time we perform the diffusion process, we extract the fixed local structure from the KNN structure and only perform multiplication K times for each pair of data points. [sent-203, score-0.571]
</p><p>73 Therefore we can update the transition kernel in (12) in time Kn2 +Kn. [sent-204, score-0.191]
</p><p>74 α is the weight of label correlation, while λ represents the importance of regularization. [sent-216, score-0.247]
</p><p>75 A Toy Data We first test our dynamic label propagation on a toy data set. [sent-224, score-0.632]
</p><p>76 We test the effect of the two steps in the dynamic label propagation. [sent-230, score-0.344]
</p><p>77 First, we omit the first step that fuses the label correlation with the kernel matrix. [sent-232, score-0.5]
</p><p>78 Second, we do the first step to fuse label correlations but omit the second step of kernel diffusion. [sent-237, score-0.459]
</p><p>79 Without the label correlation, DLP fails to capture the dependence between different classes; without the kernel diffusion process, the DLP goes wild because the label correlation in the beginning provides a poor guidance for the kernel matrix. [sent-242, score-1.008]
</p><p>80 Semi-supervised Multi-class Learning We compare our DLP with several popular semisupervised learning methods: 1) Label Propagation (LP) ; 2) A variant of LP on KNN structure(LP+KNN) [23]; 3) Local and Global Consistency (LGC) [35]; 5) Transductive SVM (TSVM) 6) LapRLS [3]. [sent-249, score-0.152]
</p><p>81 All the datasets have 12 splits each of which has 100 labeled and 1,400 unlabeled instances. [sent-255, score-0.204]
</p><p>82 To show the effect of fusing label correlation, we especially set α = 0 in our method and denote this special method  ×  as DLP0. [sent-256, score-0.294]
</p><p>83 We can see that, our method is still capable of performing binary classification but it is especially suitable for the multi-class classification problem, such as in the dataset COIL. [sent-259, score-0.132]
</p><p>84 05 for DLP, it does not indicate that the label correlation is of little importance. [sent-261, score-0.361]
</p><p>85 The only reason for small value of α lies in difference of the numerical scale of label correlation and transition probability. [sent-262, score-0.473]
</p><p>86 (A) is the toy data with only  (C)  (D)  (E)  one labeled data (the colored dots) for each class. [sent-305, score-0.171]
</p><p>87 (B) is the classification result without using label correlations. [sent-306, score-0.313]
</p><p>88 (C)  is the classification result without using diffusion process. [sent-307, score-0.308]
</p><p>89 These classes are chosen due to the relatively large number of availabel images within the category. [sent-319, score-0.123]
</p><p>90 We also show the dynamics of label propagation and the proposed methods. [sent-340, score-0.455]
</p><p>91 However, our method does not suffer from this problem because DLP iteratively update the transition matrix based on local similarity and label information. [sent-347, score-0.472]
</p><p>92 The second one is based on Constrained Non-negative Matrix Factorization (CNMF) [17], which assumes that two instances tend to have large overlap in their assigned labels if they share high similarity in their input patterns. [sent-377, score-0.2]
</p><p>93 The third one is Multi-label Informed Latent Semantic Indexing (MISL) [33], which maps the input features into a new feature space which captures the structure of both input data −  and label dependency, and then uses SVM on the projected space. [sent-378, score-0.336]
</p><p>94 , a transductive multi-label classification algorithm via label set propagation [14], which estimates the label sets of the unlabeled instances by utilizing the information from both unlabeled instances and unlabeled data. [sent-381, score-1.378]
</p><p>95 We can see that our dynamic label propagation can properly capture the inner structure of label correlation and improve the classification accuracy. [sent-387, score-0.979]
</p><p>96 Number of rtaninig nisatnces = 500  Number of rtaninig nisatnces = 2000  F1oicMr0. [sent-389, score-0.216]
</p><p>97 Conclusion In this paper, we have proposed a novel classification method named dynamic label propagation (DLP), which improves the discriminative power in multi-class/multilabel problems in the framework of semi-supervised learning. [sent-398, score-0.618]
</p><p>98 Our method explores the effect of labeled information and local structure in improving the transition matrix in semi-supervised learning. [sent-399, score-0.236]
</p><p>99 The rendezvous algorithm: multiclass semisupervised lenaring with markov random walks. [sent-413, score-0.176]
</p><p>100 Distance metric learning for large margin nearest neighbor classification. [sent-608, score-0.114]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dlp', 0.536), ('label', 0.247), ('diffusion', 0.242), ('propagation', 0.208), ('lp', 0.198), ('knn', 0.194), ('unlabeled', 0.14), ('pt', 0.134), ('xt', 0.124), ('correlation', 0.114), ('transition', 0.112), ('zt', 0.111), ('semisupervised', 0.11), ('tsvm', 0.109), ('ytytt', 0.109), ('correlations', 0.107), ('dynamic', 0.097), ('yt', 0.094), ('availabel', 0.082), ('laprls', 0.082), ('pzt', 0.082), ('fusion', 0.081), ('kernel', 0.079), ('similarities', 0.067), ('multiclass', 0.066), ('classification', 0.066), ('neighbours', 0.065), ('instances', 0.064), ('labeled', 0.064), ('digit', 0.063), ('sylvester', 0.063), ('transductive', 0.062), ('pages', 0.06), ('matrix', 0.06), ('disadvantage', 0.06), ('cnmf', 0.054), ('knndlp', 0.054), ('nisatnces', 0.054), ('qtn', 0.054), ('rtaninig', 0.054), ('rtn', 0.054), ('status', 0.054), ('similarity', 0.053), ('toy', 0.053), ('labels', 0.052), ('rate', 0.05), ('lgc', 0.048), ('smse', 0.048), ('transduction', 0.048), ('fusing', 0.047), ('metric', 0.046), ('sigir', 0.046), ('tram', 0.045), ('propagated', 0.043), ('jin', 0.042), ('learning', 0.042), ('hta', 0.042), ('ranking', 0.041), ('classes', 0.041), ('tab', 0.04), ('caltech', 0.039), ('fused', 0.037), ('multilabel', 0.037), ('xl', 0.037), ('projection', 0.036), ('correlated', 0.036), ('ky', 0.035), ('graph', 0.034), ('coding', 0.034), ('informed', 0.034), ('fuses', 0.034), ('versus', 0.034), ('nn', 0.033), ('canada', 0.032), ('graphbased', 0.032), ('laplacian', 0.031), ('input', 0.031), ('la', 0.031), ('aaai', 0.029), ('emphasizes', 0.029), ('iteration', 0.029), ('error', 0.029), ('dynamically', 0.028), ('among', 0.027), ('xu', 0.027), ('data', 0.027), ('gr', 0.027), ('topics', 0.027), ('unifying', 0.027), ('category', 0.026), ('omit', 0.026), ('neighbor', 0.026), ('mit', 0.025), ('sch', 0.024), ('round', 0.024), ('zhuowen', 0.024), ('afor', 0.024), ('ailnu', 0.024), ('ajt', 0.024), ('ansd', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="126-tfidf-1" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>Author: Bo Wang, Zhuowen Tu, John K. Tsotsos</p><p>Abstract: In graph-based semi-supervised learning approaches, the classification rate is highly dependent on the size of the availabel labeled data, as well as the accuracy of the similarity measures. Here, we propose a semi-supervised multi-class/multi-label classification scheme, dynamic label propagation (DLP), which performs transductive learning through propagation in a dynamic process. Existing semi-supervised classification methods often have difficulty in dealing with multi-class/multi-label problems due to the lack in consideration of label correlation; our algorithm instead emphasizes dynamic metric fusion with label information. Significant improvement over the state-of-the-art methods is observed on benchmark datasets for both multiclass and multi-label tasks.</p><p>2 0.1339846 <a title="126-tfidf-2" href="./iccv-2013-New_Graph_Structured_Sparsity_Model_for_Multi-label_Image_Annotations.html">290 iccv-2013-New Graph Structured Sparsity Model for Multi-label Image Annotations</a></p>
<p>Author: Xiao Cai, Feiping Nie, Weidong Cai, Heng Huang</p><p>Abstract: In multi-label image annotations, because each image is associated to multiple categories, the semantic terms (label classes) are not mutually exclusive. Previous research showed that such label correlations can largely boost the annotation accuracy. However, all existing methods only directly apply the label correlation matrix to enhance the label inference and assignment without further learning the structural information among classes. In this paper, we model the label correlations using the relational graph, and propose a novel graph structured sparse learning model to incorporate the topological constraints of relation graph in multi-label classifications. As a result, our new method will capture and utilize the hidden class structures in relational graph to improve the annotation results. In proposed objective, a large number of structured sparsity-inducing norms are utilized, thus the optimization becomes difficult. To solve this problem, we derive an efficient optimization algorithm with proved convergence. We perform extensive experiments on six multi-label image annotation benchmark data sets. In all empirical results, our new method shows better annotation results than the state-of-the-art approaches.</p><p>3 0.12696803 <a title="126-tfidf-3" href="./iccv-2013-Decomposing_Bag_of_Words_Histograms.html">104 iccv-2013-Decomposing Bag of Words Histograms</a></p>
<p>Author: Ankit Gandhi, Karteek Alahari, C.V. Jawahar</p><p>Abstract: We aim to decompose a global histogram representation of an image into histograms of its associated objects and regions. This task is formulated as an optimization problem, given a set of linear classifiers, which can effectively discriminate the object categories present in the image. Our decomposition bypasses harder problems associated with accurately localizing and segmenting objects. We evaluate our method on a wide variety of composite histograms, and also compare it with MRF-based solutions. In addition to merely measuring the accuracy of decomposition, we also show the utility of the estimated object and background histograms for the task of image classification on the PASCAL VOC 2007 dataset.</p><p>4 0.12512368 <a title="126-tfidf-4" href="./iccv-2013-Curvature-Aware_Regularization_on_Riemannian_Submanifolds.html">100 iccv-2013-Curvature-Aware Regularization on Riemannian Submanifolds</a></p>
<p>Author: Kwang In Kim, James Tompkin, Christian Theobalt</p><p>Abstract: One fundamental assumption in object recognition as well as in other computer vision and pattern recognition problems is that the data generation process lies on a manifold and that it respects the intrinsic geometry of the manifold. This assumption is held in several successful algorithms for diffusion and regularization, in particular, in graph-Laplacian-based algorithms. We claim that the performance of existing algorithms can be improved if we additionally account for how the manifold is embedded within the ambient space, i.e., if we consider the extrinsic geometry of the manifold. We present a procedure for characterizing the extrinsic (as well as intrinsic) curvature of a manifold M which is described by a sampled point cloud in a high-dimensional Euclidean space. Once estimated, we use this characterization in general diffusion and regularization on M, and form a new regularizer on a point cloud. The resulting re-weighted graph Laplacian demonstrates superior performance over classical graph Laplacian in semisupervised learning and spectral clustering.</p><p>5 0.11782884 <a title="126-tfidf-5" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>Author: Xiao Cai, Feiping Nie, Weidong Cai, Heng Huang</p><p>Abstract: Automatic image categorization has become increasingly important with the development of Internet and the growth in the size of image databases. Although the image categorization can be formulated as a typical multiclass classification problem, two major challenges have been raised by the real-world images. On one hand, though using more labeled training data may improve the prediction performance, obtaining the image labels is a time consuming as well as biased process. On the other hand, more and more visual descriptors have been proposed to describe objects and scenes appearing in images and different features describe different aspects of the visual characteristics. Therefore, how to integrate heterogeneous visual features to do the semi-supervised learning is crucial for categorizing large-scale image data. In this paper, we propose a novel approach to integrate heterogeneous features by performing multi-modal semi-supervised classification on unlabeled as well as unsegmented images. Considering each type of feature as one modality, taking advantage of the large amoun- t of unlabeled data information, our new adaptive multimodal semi-supervised classification (AMMSS) algorithm learns a commonly shared class indicator matrix and the weights for different modalities (image features) simultaneously.</p><p>6 0.11449631 <a title="126-tfidf-6" href="./iccv-2013-Ensemble_Projection_for_Semi-supervised_Image_Classification.html">142 iccv-2013-Ensemble Projection for Semi-supervised Image Classification</a></p>
<p>7 0.10969905 <a title="126-tfidf-7" href="./iccv-2013-A_Convex_Optimization_Framework_for_Active_Learning.html">6 iccv-2013-A Convex Optimization Framework for Active Learning</a></p>
<p>8 0.1065876 <a title="126-tfidf-8" href="./iccv-2013-Discriminative_Label_Propagation_for_Multi-object_Tracking_with_Sporadic_Appearance_Features.html">120 iccv-2013-Discriminative Label Propagation for Multi-object Tracking with Sporadic Appearance Features</a></p>
<p>9 0.10378935 <a title="126-tfidf-9" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>10 0.096900657 <a title="126-tfidf-10" href="./iccv-2013-Motion-Aware_KNN_Laplacian_for_Video_Matting.html">275 iccv-2013-Motion-Aware KNN Laplacian for Video Matting</a></p>
<p>11 0.091360807 <a title="126-tfidf-11" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>12 0.089833342 <a title="126-tfidf-12" href="./iccv-2013-Non-convex_P-Norm_Projection_for_Robust_Sparsity.html">292 iccv-2013-Non-convex P-Norm Projection for Robust Sparsity</a></p>
<p>13 0.085890576 <a title="126-tfidf-13" href="./iccv-2013-Event_Recognition_in_Photo_Collections_with_a_Stopwatch_HMM.html">147 iccv-2013-Event Recognition in Photo Collections with a Stopwatch HMM</a></p>
<p>14 0.078744084 <a title="126-tfidf-14" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>15 0.078066073 <a title="126-tfidf-15" href="./iccv-2013-Uncertainty-Driven_Efficiently-Sampled_Sparse_Graphical_Models_for_Concurrent_Tumor_Segmentation_and_Atlas_Registration.html">432 iccv-2013-Uncertainty-Driven Efficiently-Sampled Sparse Graphical Models for Concurrent Tumor Segmentation and Atlas Registration</a></p>
<p>16 0.074101217 <a title="126-tfidf-16" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>17 0.073628485 <a title="126-tfidf-17" href="./iccv-2013-Fast_High_Dimensional_Vector_Multiplication_Face_Recognition.html">158 iccv-2013-Fast High Dimensional Vector Multiplication Face Recognition</a></p>
<p>18 0.07249523 <a title="126-tfidf-18" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>19 0.072292484 <a title="126-tfidf-19" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>20 0.072264872 <a title="126-tfidf-20" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.201), (1, 0.046), (2, -0.029), (3, -0.041), (4, -0.03), (5, 0.056), (6, -0.031), (7, 0.041), (8, 0.036), (9, -0.075), (10, -0.056), (11, -0.071), (12, -0.041), (13, 0.006), (14, 0.072), (15, -0.0), (16, -0.044), (17, -0.03), (18, -0.028), (19, -0.026), (20, 0.045), (21, -0.003), (22, -0.022), (23, 0.029), (24, 0.004), (25, 0.066), (26, 0.073), (27, 0.047), (28, 0.118), (29, 0.134), (30, -0.019), (31, 0.043), (32, 0.034), (33, -0.001), (34, 0.003), (35, 0.036), (36, 0.042), (37, -0.017), (38, 0.003), (39, 0.017), (40, 0.088), (41, -0.038), (42, -0.062), (43, 0.001), (44, 0.02), (45, -0.084), (46, 0.061), (47, 0.038), (48, 0.037), (49, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95376277 <a title="126-lsi-1" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>Author: Bo Wang, Zhuowen Tu, John K. Tsotsos</p><p>Abstract: In graph-based semi-supervised learning approaches, the classification rate is highly dependent on the size of the availabel labeled data, as well as the accuracy of the similarity measures. Here, we propose a semi-supervised multi-class/multi-label classification scheme, dynamic label propagation (DLP), which performs transductive learning through propagation in a dynamic process. Existing semi-supervised classification methods often have difficulty in dealing with multi-class/multi-label problems due to the lack in consideration of label correlation; our algorithm instead emphasizes dynamic metric fusion with label information. Significant improvement over the state-of-the-art methods is observed on benchmark datasets for both multiclass and multi-label tasks.</p><p>2 0.84250444 <a title="126-lsi-2" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>Author: Xiao Cai, Feiping Nie, Weidong Cai, Heng Huang</p><p>Abstract: Automatic image categorization has become increasingly important with the development of Internet and the growth in the size of image databases. Although the image categorization can be formulated as a typical multiclass classification problem, two major challenges have been raised by the real-world images. On one hand, though using more labeled training data may improve the prediction performance, obtaining the image labels is a time consuming as well as biased process. On the other hand, more and more visual descriptors have been proposed to describe objects and scenes appearing in images and different features describe different aspects of the visual characteristics. Therefore, how to integrate heterogeneous visual features to do the semi-supervised learning is crucial for categorizing large-scale image data. In this paper, we propose a novel approach to integrate heterogeneous features by performing multi-modal semi-supervised classification on unlabeled as well as unsegmented images. Considering each type of feature as one modality, taking advantage of the large amoun- t of unlabeled data information, our new adaptive multimodal semi-supervised classification (AMMSS) algorithm learns a commonly shared class indicator matrix and the weights for different modalities (image features) simultaneously.</p><p>3 0.75541383 <a title="126-lsi-3" href="./iccv-2013-New_Graph_Structured_Sparsity_Model_for_Multi-label_Image_Annotations.html">290 iccv-2013-New Graph Structured Sparsity Model for Multi-label Image Annotations</a></p>
<p>Author: Xiao Cai, Feiping Nie, Weidong Cai, Heng Huang</p><p>Abstract: In multi-label image annotations, because each image is associated to multiple categories, the semantic terms (label classes) are not mutually exclusive. Previous research showed that such label correlations can largely boost the annotation accuracy. However, all existing methods only directly apply the label correlation matrix to enhance the label inference and assignment without further learning the structural information among classes. In this paper, we model the label correlations using the relational graph, and propose a novel graph structured sparse learning model to incorporate the topological constraints of relation graph in multi-label classifications. As a result, our new method will capture and utilize the hidden class structures in relational graph to improve the annotation results. In proposed objective, a large number of structured sparsity-inducing norms are utilized, thus the optimization becomes difficult. To solve this problem, we derive an efficient optimization algorithm with proved convergence. We perform extensive experiments on six multi-label image annotation benchmark data sets. In all empirical results, our new method shows better annotation results than the state-of-the-art approaches.</p><p>4 0.75181442 <a title="126-lsi-4" href="./iccv-2013-From_Point_to_Set%3A_Extend_the_Learning_of_Distance_Metrics.html">177 iccv-2013-From Point to Set: Extend the Learning of Distance Metrics</a></p>
<p>Author: Pengfei Zhu, Lei Zhang, Wangmeng Zuo, David Zhang</p><p>Abstract: Most of the current metric learning methods are proposed for point-to-point distance (PPD) based classification. In many computer vision tasks, however, we need to measure the point-to-set distance (PSD) and even set-to-set distance (SSD) for classification. In this paper, we extend the PPD based Mahalanobis distance metric learning to PSD and SSD based ones, namely point-to-set distance metric learning (PSDML) and set-to-set distance metric learning (SSDML), and solve them under a unified optimization framework. First, we generate positive and negative sample pairs by computing the PSD and SSD between training samples. Then, we characterize each sample pair by its covariance matrix, and propose a covariance kernel based discriminative function. Finally, we tackle the PSDML and SSDMLproblems by using standard support vector machine solvers, making the metric learning very efficient for multiclass visual classification tasks. Experiments on gender classification, digit recognition, object categorization and face recognition show that the proposed metric learning methods can effectively enhance the performance of PSD and SSD based classification.</p><p>5 0.74904132 <a title="126-lsi-5" href="./iccv-2013-Ensemble_Projection_for_Semi-supervised_Image_Classification.html">142 iccv-2013-Ensemble Projection for Semi-supervised Image Classification</a></p>
<p>Author: Dengxin Dai, Luc Van_Gool</p><p>Abstract: This paper investigates the problem of semi-supervised classification. Unlike previous methods to regularize classifying boundaries with unlabeled data, our method learns a new image representation from all available data (labeled and unlabeled) andperformsplain supervised learning with the new feature. In particular, an ensemble of image prototype sets are sampled automatically from the available data, to represent a rich set of visual categories/attributes. Discriminative functions are then learned on these prototype sets, and image are represented by the concatenation of their projected values onto the prototypes (similarities to them) for further classification. Experiments on four standard datasets show three interesting phenomena: (1) our method consistently outperforms previous methods for semi-supervised image classification; (2) our method lets itself combine well with these methods; and (3) our method works well for self-taught image classification where unlabeled data are not coming from the same distribution as la- beled ones, but rather from a random collection of images.</p><p>6 0.74385035 <a title="126-lsi-6" href="./iccv-2013-Joint_Learning_of_Discriminative_Prototypes_and_Large_Margin_Nearest_Neighbor_Classifiers.html">222 iccv-2013-Joint Learning of Discriminative Prototypes and Large Margin Nearest Neighbor Classifiers</a></p>
<p>7 0.73212469 <a title="126-lsi-7" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>8 0.71942627 <a title="126-lsi-8" href="./iccv-2013-Quadruplet-Wise_Image_Similarity_Learning.html">332 iccv-2013-Quadruplet-Wise Image Similarity Learning</a></p>
<p>9 0.70671898 <a title="126-lsi-9" href="./iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning.html">212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</a></p>
<p>10 0.70024562 <a title="126-lsi-10" href="./iccv-2013-From_Semi-supervised_to_Transfer_Counting_of_Crowds.html">178 iccv-2013-From Semi-supervised to Transfer Counting of Crowds</a></p>
<p>11 0.69704634 <a title="126-lsi-11" href="./iccv-2013-Unbiased_Metric_Learning%3A_On_the_Utilization_of_Multiple_Datasets_and_Web_Images_for_Softening_Bias.html">431 iccv-2013-Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias</a></p>
<p>12 0.67959172 <a title="126-lsi-12" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>13 0.6492157 <a title="126-lsi-13" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>14 0.64860713 <a title="126-lsi-14" href="./iccv-2013-Drosophila_Embryo_Stage_Annotation_Using_Label_Propagation.html">125 iccv-2013-Drosophila Embryo Stage Annotation Using Label Propagation</a></p>
<p>15 0.64856452 <a title="126-lsi-15" href="./iccv-2013-A_Convex_Optimization_Framework_for_Active_Learning.html">6 iccv-2013-A Convex Optimization Framework for Active Learning</a></p>
<p>16 0.63655949 <a title="126-lsi-16" href="./iccv-2013-Learning_CRFs_for_Image_Parsing_with_Adaptive_Subgradient_Descent.html">234 iccv-2013-Learning CRFs for Image Parsing with Adaptive Subgradient Descent</a></p>
<p>17 0.62772942 <a title="126-lsi-17" href="./iccv-2013-A_Novel_Earth_Mover%27s_Distance_Methodology_for_Image_Matching_with_Gaussian_Mixture_Models.html">25 iccv-2013-A Novel Earth Mover's Distance Methodology for Image Matching with Gaussian Mixture Models</a></p>
<p>18 0.61717397 <a title="126-lsi-18" href="./iccv-2013-Class-Specific_Simplex-Latent_Dirichlet_Allocation_for_Image_Classification.html">73 iccv-2013-Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification</a></p>
<p>19 0.59955955 <a title="126-lsi-19" href="./iccv-2013-Perceptual_Fidelity_Aware_Mean_Squared_Error.html">312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</a></p>
<p>20 0.59784842 <a title="126-lsi-20" href="./iccv-2013-Linear_Sequence_Discriminant_Analysis%3A_A_Model-Based_Dimensionality_Reduction_Method_for_Vector_Sequences.html">253 iccv-2013-Linear Sequence Discriminant Analysis: A Model-Based Dimensionality Reduction Method for Vector Sequences</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.081), (7, 0.025), (12, 0.015), (26, 0.087), (27, 0.031), (31, 0.065), (34, 0.016), (40, 0.015), (42, 0.102), (47, 0.165), (48, 0.026), (64, 0.049), (73, 0.028), (78, 0.023), (89, 0.151), (98, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84834427 <a title="126-lda-1" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>Author: Bo Wang, Zhuowen Tu, John K. Tsotsos</p><p>Abstract: In graph-based semi-supervised learning approaches, the classification rate is highly dependent on the size of the availabel labeled data, as well as the accuracy of the similarity measures. Here, we propose a semi-supervised multi-class/multi-label classification scheme, dynamic label propagation (DLP), which performs transductive learning through propagation in a dynamic process. Existing semi-supervised classification methods often have difficulty in dealing with multi-class/multi-label problems due to the lack in consideration of label correlation; our algorithm instead emphasizes dynamic metric fusion with label information. Significant improvement over the state-of-the-art methods is observed on benchmark datasets for both multiclass and multi-label tasks.</p><p>2 0.80847728 <a title="126-lda-2" href="./iccv-2013-ACTIVE%3A_Activity_Concept_Transitions_in_Video_Event_Classification.html">4 iccv-2013-ACTIVE: Activity Concept Transitions in Video Event Classification</a></p>
<p>Author: Chen Sun, Ram Nevatia</p><p>Abstract: The goal of high level event classification from videos is to assign a single, high level event label to each query video. Traditional approaches represent each video as a set of low level features and encode it into a fixed length feature vector (e.g. Bag-of-Words), which leave a big gap between low level visual features and high level events. Our paper tries to address this problem by exploiting activity concept transitions in video events (ACTIVE). A video is treated as a sequence of short clips, all of which are observations corresponding to latent activity concept variables in a Hidden Markov Model (HMM). We propose to apply Fisher Kernel techniques so that the concept transitions over time can be encoded into a compact and fixed length feature vector very efficiently. Our approach can utilize concept annotations from independent datasets, and works well even with a very small number of training samples. Experiments on the challenging NIST TRECVID Multimedia Event Detection (MED) dataset shows our approach performs favorably over the state-of-the-art.</p><p>3 0.8013581 <a title="126-lda-3" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<p>Author: Anand Mishra, Karteek Alahari, C.V. Jawahar</p><p>Abstract: We present an approach for the text-to-image retrieval problem based on textual content present in images. Given the recent developments in understanding text in images, an appealing approach to address this problem is to localize and recognize the text, and then query the database, as in a text retrieval problem. We show that such an approach, despite being based on state-of-the-artmethods, is insufficient, and propose a method, where we do not rely on an exact localization and recognition pipeline. We take a query-driven search approach, where we find approximate locations of characters in the text query, and then impose spatial constraints to generate a ranked list of images in the database. The retrieval performance is evaluated on public scene text datasets as well as three large datasets, namely IIIT scene text retrieval, Sports-10K and TV series-1M, we introduce.</p><p>4 0.79601461 <a title="126-lda-4" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>Author: Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, Philip S. Yu</p><p>Abstract: Transfer learning is established as an effective technology in computer visionfor leveraging rich labeled data in the source domain to build an accurate classifier for the target domain. However, most prior methods have not simultaneously reduced the difference in both the marginal distribution and conditional distribution between domains. In this paper, we put forward a novel transfer learning approach, referred to as Joint Distribution Adaptation (JDA). Specifically, JDA aims to jointly adapt both the marginal distribution and conditional distribution in a principled dimensionality reduction procedure, and construct new feature representation that is effective and robustfor substantial distribution difference. Extensive experiments verify that JDA can significantly outperform several state-of-the-art methods on four types of cross-domain image classification problems.</p><p>5 0.79477787 <a title="126-lda-5" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>Author: S. Karthikeyan, Vignesh Jagadeesh, Renuka Shenoy, Miguel Ecksteinz, B.S. Manjunath</p><p>Abstract: Eye movement studies have confirmed that overt attention is highly biased towards faces and text regions in images. In this paper we explore a novel problem of predicting face and text regions in images using eye tracking data from multiple subjects. The problem is challenging as we aim to predict the semantics (face/text/background) only from eye tracking data without utilizing any image information. The proposed algorithm spatially clusters eye tracking data obtained in an image into different coherent groups and subsequently models the likelihood of the clusters containing faces and text using afully connectedMarkov Random Field (MRF). Given the eye tracking datafrom a test image, itpredicts potential face/head (humans, dogs and cats) and text locations reliably. Furthermore, the approach can be used to select regions of interest for further analysis by object detectors for faces and text. The hybrid eye position/object detector approach achieves better detection performance and reduced computation time compared to using only the object detection algorithm. We also present a new eye tracking dataset on 300 images selected from ICDAR, Street-view, Flickr and Oxford-IIIT Pet Dataset from 15 subjects.</p><p>6 0.78838128 <a title="126-lda-6" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>7 0.78453159 <a title="126-lda-7" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>8 0.78329903 <a title="126-lda-8" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>9 0.78300595 <a title="126-lda-9" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>10 0.78278059 <a title="126-lda-10" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>11 0.78183031 <a title="126-lda-11" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>12 0.78101575 <a title="126-lda-12" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>13 0.78083682 <a title="126-lda-13" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>14 0.7795881 <a title="126-lda-14" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>15 0.77951968 <a title="126-lda-15" href="./iccv-2013-Class-Specific_Simplex-Latent_Dirichlet_Allocation_for_Image_Classification.html">73 iccv-2013-Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification</a></p>
<p>16 0.77940881 <a title="126-lda-16" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>17 0.77937961 <a title="126-lda-17" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>18 0.77902448 <a title="126-lda-18" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>19 0.77884066 <a title="126-lda-19" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>20 0.77866566 <a title="126-lda-20" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
