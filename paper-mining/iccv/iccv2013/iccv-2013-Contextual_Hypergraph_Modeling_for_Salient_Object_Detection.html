<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-91" href="#">iccv2013-91</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</h1>
<br/><p>Source: <a title="iccv-2013-91-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Li_Contextual_Hypergraph_Modeling_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Xi Li, Yao Li, Chunhua Shen, Anthony Dick, Anton Van_Den_Hengel</p><p>Abstract: Salient object detection aims to locate objects that capture human attention within images. Previous approaches often pose this as a problem of image contrast analysis. In this work, we model an image as a hypergraph that utilizes a set of hyperedges to capture the contextual properties of image pixels or regions. As a result, the problem of salient object detection becomes one of finding salient vertices and hyperedges in the hypergraph. The main advantage of hypergraph modeling is that it takes into account each pixel’s (or region ’s) affinity with its neighborhood as well as its separation from image background. Furthermore, we propose an alternative approach based on centerversus-surround contextual contrast analysis, which performs salient object detection by optimizing a cost-sensitive support vector machine (SVM) objective function. Experimental results on four challenging datasets demonstrate the effectiveness of the proposed approaches against the stateof-the-art approaches to salient object detection.</p><p>Reference: <a title="iccv-2013-91-reference" href="../iccv2013_reference/iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this work, we model an image as a hypergraph that utilizes a set of hyperedges to capture the contextual properties of image pixels or regions. [sent-3, score-0.813]
</p><p>2 As a result, the problem of salient object detection becomes one of finding salient vertices and hyperedges in the hypergraph. [sent-4, score-0.995]
</p><p>3 The main advantage of hypergraph modeling is that it takes into account each pixel’s (or region ’s) affinity with its neighborhood as well as its separation from image background. [sent-5, score-0.454]
</p><p>4 Furthermore, we propose an alternative approach based on centerversus-surround contextual contrast analysis, which performs salient object detection by optimizing a cost-sensitive support vector machine (SVM) objective function. [sent-6, score-0.461]
</p><p>5 Experimental results on four challenging datasets demonstrate the effectiveness of the proposed approaches against the stateof-the-art approaches to salient object detection. [sent-7, score-0.352]
</p><p>6 Introduction  Image saliency detection aims to effectively identify important and informative regions in images. [sent-9, score-0.704]
</p><p>7 Recently, a large body of work concentrates on salient object detection [4–17], whose goal is to discover the most salient and attention-grabbing object in an image. [sent-11, score-0.7]
</p><p>8 Because it is difficult to define saliency analytically, the performance of salient object detection is evaluated on datasets containing human-labeled bounding boxes or foreground masks. [sent-13, score-1.017]
</p><p>9 estimates the saliency degree of an image region by computing the contrast against its local neighborhood. [sent-15, score-0.672]
</p><p>10 Various contrast measures have been proposed, including mutual information [22], incremental coding length [3], and center-  ImageSVM saliencyHypergraph saliency Figure 1: Illustration of our approaches  to  salient object detection. [sent-16, score-0.981]
</p><p>11 Global salient object detection approaches [4,5,7, 11, 12] estimate the saliency of a particular image region by measuring its uniqueness in the entire image. [sent-18, score-1.038]
</p><p>12 Therefore, the definition of object saliency depends on the choice of context. [sent-20, score-0.643]
</p><p>13 Global saliency defines the context as the entire image, whereas local saliency requires the definition of a local context. [sent-21, score-1.244]
</p><p>14 In this work, we first show that within a fixed context, a cost-sensitive SVM can accurately measure saliency by capturing centre-surround contrast information. [sent-22, score-0.646]
</p><p>15 We then show that the use of a hypergraph captures more comprehensive contextual information, and therefore enhances the accuracy of salient object detection. [sent-23, score-0.805]
</p><p>16 Here, we propose two approaches to salient object detection based on hypergraph modeling and imbalanced max-  margin learning. [sent-24, score-0.896]
</p><p>17 We introduce hypergraph modeling into the process of image saliency detection for the first time. [sent-27, score-1.139]
</p><p>18 A hypergraph is a rich, structured image representation modeling pixels (or superpixels) by their contexts rather than their individual values. [sent-28, score-0.488]
</p><p>19 This additional structural information enables more accurate saliency measurement. [sent-29, score-0.631]
</p><p>20 The problem of saliency detection is naturally cast as 33332281  Center Surroundings  saliency results. [sent-30, score-1.298]
</p><p>21 (3) based  on  the SVM classification  that of detecting salient vertices and hyperedges in a hypergraph at multiple scales. [sent-32, score-1.042]
</p><p>22 We formulate the centre-surround contrast approach to saliency as a cost-sensitive max-margin classification problem. [sent-34, score-0.63]
</p><p>23 Consequently, the saliency degree of an image region is measured by its associated normalized SVM coding length. [sent-35, score-0.698]
</p><p>24 Example results of our approaches to salient object detection are shown in Fig. [sent-36, score-0.405]
</p><p>25 Cost-sensitive SVM saliency detection As illustrated in [9, 23], saliency detection is typically posed as the problem of center-versus-surround contextual contrast analysis. [sent-41, score-1.445]
</p><p>26 To address this problem, we propose a saliency detection method based on imbalanced maxmargin learning, which is capable of effectively discovering the local salient image regions that significantly differ from their surrounding image regions. [sent-42, score-1.067]
</p><p>27 Tgh pea saliency degree of x1 is determined by its inter-class separability from {x? [sent-60, score-0.689]
</p><p>28 N, then it is deemecdo utlod b bee s eaaliseinlyt; eotphaerarwteidse f,r oitms saliency degree is low. [sent-71, score-0.655]
</p><p>29 Using the weighted LS-SVM classifier f(x), we define the saliency score as:  SSa(x1) =N −1 1? [sent-129, score-0.613]
</p><p>30 saliency score SSa(x1) can be viewed as a normalized SVM coding length (i. [sent-138, score-0.631]
</p><p>31 Note that, this max-margin learning framework can be easily extended to perform saliency detection on a global scale. [sent-163, score-0.685]
</p><p>32 By running the max-margin learning procedure over such training samples, the saliency degree of each 33332292  Figure 3: Illustration of hypergraph modeling for saliency detection using nonparametric clustering. [sent-165, score-1.815]
</p><p>33 The middle columns display the multi-scale hyperedges (constructed by nonparametric clustering on the superpixels) and their corresponding results of hyperedge saliency evaluation. [sent-167, score-1.305]
</p><p>34 The rightmost image shows the final saliency map HSa generated by multi-scale hyperedge saliency fusion. [sent-168, score-1.597]
</p><p>35 In theory, our hypergraph modeling can also work on pixels in a similar way. [sent-172, score-0.47]
</p><p>36 ImageHyperg aphsaliencyStandardgraphsaliency  Figure 4: Illustration of salient object detection using two different types of graphs (i. [sent-173, score-0.386]
</p><p>37 Clearly, our hypergraph saliency measure is able to accurately capture the intrinsic structural properties of the salient object. [sent-176, score-1.419]
</p><p>38 Example saliency maps derived from this measure are shown in Figs. [sent-178, score-0.613]
</p><p>39 Although they accurately locate the salient object in each case, they also suffer from “fuzziness” or lack of precision around object boundaries and in locally  homogeneous regions. [sent-180, score-0.36]
</p><p>40 Hypergraph modeling for saliency detection To more effectively find salient object regions, we propose a hypergraph modeling based saliency detection method that forms contexts of superpixels to capture both internal consistency and external separation. [sent-184, score-2.263]
</p><p>41 As illustrated in [26], a hypergraph is a graph comprising a set of vertices and hyperedges. [sent-187, score-0.492]
</p><p>42 In contrast to the pairwise edge in a standard graph, the hyperedge in a hypergraph is a high-order edge associated with a vertex clique linking more than two vertices. [sent-188, score-0.906]
</p><p>43 Effectively constructing such hyperedges is crucial for encoding the intrinsic contextual information on the vertices in the hypergraph. [sent-189, score-0.414]
</p><p>44 Hypergraph modeling In our method, an image I is modeled by a hypergraph G = (V, E), where V = {vi} is the vertex set corresponding to the image superpixels and E = {ej } is the hyp? [sent-190, score-0.556]
</p><p>45 Each clique corresponds to a collection of superpixels having some common visual properties, and works as a hyperedge of the hypergraph G. [sent-197, score-0.88]
</p><p>46 A hyperedge can also be viewed as a high-order context that enforces the contextual constraints on each superpixels in the hyperedge. [sent-200, score-0.496]
</p><p>47 As a result, the saliency of each superpixel, as measured by the hyperedges it belongs to, is not only determined by the superpixel itself but also influenced by its associated contexts. [sent-201, score-0.991]
</p><p>48 Due to such contextual constraints on each superpixel, we simply convert the original saliency detection problem to that of detecting salient vertices and hyperedges in the hypergraph G. [sent-202, score-1.785]
</p><p>49 Mathematically, the hypergraph G is associated with a |V| | E | incidcealnlyc,e tmheat hriyxp Herg r=a p(hH G(v iis, ej s))o |V| | E | :  ×  H(vi,ej) =? [sent-203, score-0.478]
</p><p>50 , The saliency value of any vertex  vi  in G is defined as:  HSa(vi) = ? [sent-205, score-0.695]
</p><p>51 e∈E  (4) (5)  where Γ(e) encodes the saliency information on the hyperedge e. [sent-207, score-0.966]
</p><p>52 In essence, our hypergraph saliency measure (5) is a generalization of the standard pairwise saliency measure defined as:  PSa(vi)  =  ? [sent-208, score-1.675]
</p><p>53 =i}  33332303  Figure 5: Illustration of the gradient magnitude information for hyperedge saliency evaluation. [sent-216, score-1.005]
</p><p>54 where Nvi stands for the neighborhood of vi, d(vi,vj ) measures the saliency degree of the pairwise edge (vi, vj), and Ie is the pairwise adjacency indicator (s. [sent-218, score-0.703]
</p><p>55 Instead of using simple pairwis∈e edges, our hypergraph saliency measure takes advantage ofthe higher-order hyperedges (i. [sent-221, score-1.328]
</p><p>56 , superpixel cliques) to effectively capture the intrinsic structural properties of the salient object, as shown in Fig. [sent-223, score-0.447]
</p><p>57 To implement this approach, we need to address the following two key issues: 1) how to adaptively construct the hyperedge set E; and 2) how to accurately measure the saliency degree Γ(e) of each hyperedge. [sent-225, score-1.024]
</p><p>58 Adaptive hyperedge construction A hyperedge in the hypergraph G is actually a superpixel clique whose elements have some common visual properties. [sent-226, score-1.237]
</p><p>59 To capture the hier-  archial visual saliency information, we construct a set of hyperedges by adaptively grouping the superpixels according to their visual similarities at multiple scales. [sent-227, score-0.962]
</p><p>60 Hyperedge saliency evaluation By construction, a hy-  peredge defines a group of pixels that is internally consistent. [sent-252, score-0.664]
</p><p>61 In addition, a salient hyperedge should have the following two properties: 1) it should be enclosed by strong image edges; and 2) its intersection with the image boundaries ought to be small [5, 13]. [sent-253, score-0.637]
</p><p>62 Therefore, we measure the saliency degree of a scale-specific hyperedge e by summing up the corresponding gradient magnitudes of the pixels (within a narrow band) along the boundary of the hyperedge. [sent-254, score-1.087]
</p><p>63 If the hyperedge touches the image boundaries, we decrease its saliency degree by a penalty factor. [sent-255, score-1.008]
</p><p>64 As a result, the saliency value of the hyperedge e is computed as: Γ(e) =  ωe  ? [sent-265, score-0.966]
</p><p>65 Figure 7: PR curves based on three different configurations: 1) using the SVM saliency approach only; and 2) using the hypergraph saliency approach only; 3) combining the SVM and hypergraph saliency approaches. [sent-315, score-2.721]
</p><p>66 Clearly, the saliency detection performance of using the third configuration outperform that of using the first and second configurations. [sent-316, score-0.685]
</p><p>67 a narrow band) along the boundary of the hyperedge e, ◦ is tah nea erlreomwe bnatwndis)e a dloont product operator, athned hρy(pee) eisd a penalty factor that is equal to the number of the image boundary pixels shared by the hyperedge e. [sent-318, score-0.785]
</p><p>68 (5), we obtain the hypergraph saliency measure HSa(vi) for any vertex vi in the hypergraph G. [sent-320, score-1.561]
</p><p>69 After both SVM and hypergraph saliency detection, we obtain the corresponding saliency maps. [sent-321, score-1.659]
</p><p>70 Each element of these saliency maps is mapped into [0, 255] by linear normalization, leading to the normalized saliency maps. [sent-322, score-1.226]
</p><p>71 Finally, the final saliency map is obtained by linearly combining the SVM and hypergraph saliency detection results. [sent-323, score-1.749]
</p><p>72 Each image in the aforementioned datasets contains a human-labelled foreground mask used as ground truth for salient object detection. [sent-332, score-0.371]
</p><p>73 Evaluation criterion For a given saliency map, we adopt four criteria to evaluate the quantitative performance of different approaches: precision-recall (PR) curves, Fmeasures, receiver operating characteristic (ROC) curves, and VOC overlap scores. [sent-333, score-0.64]
</p><p>74 Specifically, the PR curve is obtained by binarizing the saliency map using a number of thresholds ranging from 0 to 255, as in [4, 7, 12, 11]. [sent-334, score-0.689]
</p><p>75 Here, P and R are the precision +an1d) recall rates obtained by binarizing the saliency map using an adaptive threshold that is twice the overall mean saliency value [4]. [sent-336, score-1.321]
</p><p>76 is the object segmentation mask obtained by binarizing the saliency map using the same adaptive threshold during the calculation of F-measure. [sent-341, score-0.797]
</p><p>77 Implementation details In the experiments, costsensitive SVM saliency detection on an image is performed at different scales, each of which corresponds to a scalespecific image patch size for center-versus-surround contrast analysis. [sent-342, score-0.731]
</p><p>78 The final SVM saliency map is obtained by averaging the multi-scale saliency detection results. [sent-343, score-1.316]
</p><p>79 In the experiments, the final saliency detection results are further refined by graph-based manifold propagation. [sent-357, score-0.685]
</p><p>80 7 shows their quantitative results of salient object detection in the aspect of PR curves. [sent-364, score-0.386]
</p><p>81 7, it is clearly  seen that the saliency detection performance of only using the SVM saliency approach is significantly enhanced after combining the hypergraph saliency approach. [sent-366, score-2.371]
</p><p>82 The reason is that the hypergraph saliency approach captures 33332325  ? [sent-367, score-1.046]
</p><p>83 Comparison of saliency detection approaches  ilhawonypeg uiertnosgearctanhxpe,imhtbesprailmtosvceanol idctesfyn). [sent-500, score-0.704]
</p><p>84 It is clear that our approach obtains the visually more consistent saliency detection results than the other competing approaches. [sent-505, score-0.772]
</p><p>85 From left to right: input images, ground truth, saliency maps, segmentation results. [sent-535, score-0.637]
</p><p>86 8 shows the quantitative saliency detection performance of the proposed approach against the twelve competing approaches in the PR and ROC curves on the four datasets. [sent-539, score-0.767]
</p><p>87 10 shows several salient object detection examples of all the thirteen approaches. [sent-549, score-0.449]
</p><p>88 10 that our approach obtain visually more feasible saliency detection results than the other competing approaches. [sent-551, score-0.734]
</p><p>89 11gives three intuitive examples of salient object segmentation (i. [sent-557, score-0.338]
</p><p>90 As shown in [18], saliency detection plays an important role in image retargeting. [sent-565, score-0.685]
</p><p>91 Following the work of [18], we directly replace its saliency detection component with ours while keeping the other components fixed. [sent-566, score-0.685]
</p><p>92 This indicates that our approach is capable of effectively preserving the intrinsic structural information on salient objects during image retargeting. [sent-575, score-0.352]
</p><p>93 Conclusion In this work, we have proposed two salient object detec-  tion approaches based on hypergraph modeling and centerversus-surround max-margin learning. [sent-577, score-0.787]
</p><p>94 Specifically, we have designed a hypergraph modeling approach that captures the intrinsic contextual saliency information on image pixels/superpixels by detecting salient vertices and hyperedges in a hypergraph. [sent-578, score-1.765]
</p><p>95 Furthermore, we have developed a local salient object detection approach based on centerversus-surround max-margin learning, which solves an imbalanced cost-sensitive SVM optimization problem. [sent-579, score-0.423]
</p><p>96 Compared with the twelve state-of-the-art approaches, we have empirically shown that the fusion of our approaches is able to achieve more accurate and robust results of salient object detection. [sent-580, score-0.352]
</p><p>97 A unified approach to salient object detection via low rank matrix recovery. [sent-718, score-0.386]
</p><p>98 Automatic salient object segmentation based on context and shape prior. [sent-733, score-0.356]
</p><p>99 Fusing generic objectness and visual saliency for salient object detection. [sent-757, score-0.927]
</p><p>100 Visual saliency based on scale-space analysis in the frequency domain. [sent-935, score-0.613]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('saliency', 0.613), ('hypergraph', 0.433), ('hyperedge', 0.353), ('salient', 0.284), ('hyperedges', 0.282), ('ig', 0.083), ('retargeting', 0.075), ('detection', 0.072), ('superpixel', 0.071), ('superpixels', 0.067), ('thirteen', 0.063), ('sod', 0.062), ('binarizing', 0.058), ('contextual', 0.058), ('ssa', 0.055), ('nvi', 0.053), ('pages', 0.053), ('svm', 0.05), ('gs', 0.05), ('vi', 0.047), ('hsa', 0.043), ('vertices', 0.043), ('pr', 0.043), ('degree', 0.042), ('surrounding', 0.042), ('roc', 0.04), ('obtains', 0.038), ('imbalanced', 0.037), ('voc', 0.037), ('peredge', 0.035), ('mg', 0.035), ('vertex', 0.035), ('ieee', 0.034), ('separability', 0.034), ('band', 0.031), ('intrinsic', 0.031), ('cliques', 0.03), ('object', 0.03), ('patch', 0.029), ('competing', 0.028), ('narrow', 0.027), ('overlap', 0.027), ('clique', 0.027), ('clearly', 0.027), ('subfigure', 0.026), ('vj', 0.026), ('pi', 0.025), ('associated', 0.025), ('segmentation', 0.024), ('surroundings', 0.024), ('properties', 0.024), ('vc', 0.023), ('neural', 0.022), ('center', 0.022), ('magnitude', 0.021), ('visually', 0.021), ('modeling', 0.021), ('nonparametric', 0.021), ('uniqueness', 0.02), ('aforementioned', 0.02), ('clustering', 0.02), ('ej', 0.02), ('patches', 0.02), ('zheng', 0.02), ('iq', 0.02), ('agglomerative', 0.02), ('approaches', 0.019), ('effectively', 0.019), ('adaptive', 0.019), ('twelve', 0.019), ('mask', 0.019), ('ie', 0.019), ('foreground', 0.018), ('illustration', 0.018), ('gradient', 0.018), ('structural', 0.018), ('coding', 0.018), ('context', 0.018), ('contexts', 0.018), ('map', 0.018), ('boundary', 0.018), ('positive', 0.017), ('contrast', 0.017), ('lu', 0.017), ('display', 0.016), ('pairwise', 0.016), ('curves', 0.016), ('comprising', 0.016), ('pixels', 0.016), ('accurately', 0.016), ('displays', 0.016), ('stands', 0.016), ('configurations', 0.016), ('calculation', 0.016), ('mathematically', 0.016), ('thumbnail', 0.016), ('ekd', 0.016), ('anthony', 0.016), ('centersurround', 0.016), ('cific', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="91-tfidf-1" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>Author: Xi Li, Yao Li, Chunhua Shen, Anthony Dick, Anton Van_Den_Hengel</p><p>Abstract: Salient object detection aims to locate objects that capture human attention within images. Previous approaches often pose this as a problem of image contrast analysis. In this work, we model an image as a hypergraph that utilizes a set of hyperedges to capture the contextual properties of image pixels or regions. As a result, the problem of salient object detection becomes one of finding salient vertices and hyperedges in the hypergraph. The main advantage of hypergraph modeling is that it takes into account each pixel’s (or region ’s) affinity with its neighborhood as well as its separation from image background. Furthermore, we propose an alternative approach based on centerversus-surround contextual contrast analysis, which performs salient object detection by optimizing a cost-sensitive support vector machine (SVM) objective function. Experimental results on four challenging datasets demonstrate the effectiveness of the proposed approaches against the stateof-the-art approaches to salient object detection.</p><p>2 0.56836528 <a title="91-tfidf-2" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>Author: Yangqing Jia, Mei Han</p><p>Abstract: It is known that purely low-level saliency cues such as frequency does not lead to a good salient object detection result, requiring high-level knowledge to be adopted for successful discovery of task-independent salient objects. In this paper, we propose an efficient way to combine such high-level saliency priors and low-level appearance models. We obtain the high-level saliency prior with the objectness algorithm to find potential object candidates without the need of category information, and then enforce the consistency among the salient regions using a Gaussian MRF with the weights scaled by diverse density that emphasizes the influence of potential foreground pixels. Our model obtains saliency maps that assign high scores for the whole salient object, and achieves state-of-the-art performance on benchmark datasets covering various foreground statistics.</p><p>3 0.53416485 <a title="91-tfidf-3" href="./iccv-2013-Saliency_Detection_via_Dense_and_Sparse_Reconstruction.html">372 iccv-2013-Saliency Detection via Dense and Sparse Reconstruction</a></p>
<p>Author: Xiaohui Li, Huchuan Lu, Lihe Zhang, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: In this paper, we propose a visual saliency detection algorithm from the perspective of reconstruction errors. The image boundaries are first extracted via superpixels as likely cues for background templates, from which dense and sparse appearance models are constructed. For each image region, we first compute dense and sparse reconstruction errors. Second, the reconstruction errors are propagated based on the contexts obtained from K-means clustering. Third, pixel-level saliency is computed by an integration of multi-scale reconstruction errors and refined by an object-biased Gaussian model. We apply the Bayes formula to integrate saliency measures based on dense and sparse reconstruction errors. Experimental results show that the proposed algorithm performs favorably against seventeen state-of-the-art methods in terms of precision and recall. In addition, the proposed algorithm is demonstrated to be more effective in highlighting salient objects uniformly and robust to background noise.</p><p>4 0.38792107 <a title="91-tfidf-4" href="./iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction.html">50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</a></p>
<p>Author: Ali Borji, Hamed R. Tavakoli, Dicky N. Sihite, Laurent Itti</p><p>Abstract: Significant recent progress has been made in developing high-quality saliency models. However, less effort has been undertaken on fair assessment of these models, over large standardized datasets and correctly addressing confounding factors. In this study, we pursue a critical and quantitative look at challenges (e.g., center-bias, map smoothing) in saliency modeling and the way they affect model accuracy. We quantitatively compare 32 state-of-the-art models (using the shuffled AUC score to discount center-bias) on 4 benchmark eye movement datasets, for prediction of human fixation locations and scanpath sequence. We also account for the role of map smoothing. We find that, although model rankings vary, some (e.g., AWS, LG, AIM, and HouNIPS) consistently outperform other models over all datasets. Some models work well for prediction of both fixation locations and scanpath sequence (e.g., Judd, GBVS). Our results show low prediction accuracy for models over emotional stimuli from the NUSEF dataset. Our last benchmark, for the first time, gauges the ability of models to decode the stimulus category from statistics of fixations, saccades, and model saliency values at fixated locations. In this test, ITTI and AIM models win over other models. Our benchmark provides a comprehensive high-level picture of the strengths and weaknesses of many popular models, and suggests future research directions in saliency modeling.</p><p>5 0.37203154 <a title="91-tfidf-5" href="./iccv-2013-Saliency_and_Human_Fixations%3A_State-of-the-Art_and_Study_of_Comparison_Metrics.html">373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</a></p>
<p>Author: Nicolas Riche, Matthieu Duvinage, Matei Mancas, Bernard Gosselin, Thierry Dutoit</p><p>Abstract: Visual saliency has been an increasingly active research area in the last ten years with dozens of saliency models recently published. Nowadays, one of the big challenges in the field is to find a way to fairly evaluate all of these models. In this paper, on human eye fixations ,we compare the ranking of 12 state-of-the art saliency models using 12 similarity metrics. The comparison is done on Jian Li ’s database containing several hundreds of natural images. Based on Kendall concordance coefficient, it is shown that some of the metrics are strongly correlated leading to a redundancy in the performance metrics reported in the available benchmarks. On the other hand, other metrics provide a more diverse picture of models ’ overall performance. As a recommendation, three similarity metrics should be used to obtain a complete point of view of saliency model performance.</p><p>6 0.36035717 <a title="91-tfidf-6" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>7 0.33756223 <a title="91-tfidf-7" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>8 0.32650298 <a title="91-tfidf-8" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>9 0.3198936 <a title="91-tfidf-9" href="./iccv-2013-Saliency_Detection_via_Absorbing_Markov_Chain.html">371 iccv-2013-Saliency Detection via Absorbing Markov Chain</a></p>
<p>10 0.30922401 <a title="91-tfidf-10" href="./iccv-2013-Saliency_Detection_in_Large_Point_Sets.html">370 iccv-2013-Saliency Detection in Large Point Sets</a></p>
<p>11 0.27482754 <a title="91-tfidf-11" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>12 0.26728457 <a title="91-tfidf-12" href="./iccv-2013-Saliency_Detection%3A_A_Boolean_Map_Approach.html">369 iccv-2013-Saliency Detection: A Boolean Map Approach</a></p>
<p>13 0.18167971 <a title="91-tfidf-13" href="./iccv-2013-Semantically-Based_Human_Scanpath_Estimation_with_HMMs.html">381 iccv-2013-Semantically-Based Human Scanpath Estimation with HMMs</a></p>
<p>14 0.12289812 <a title="91-tfidf-14" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>15 0.095834285 <a title="91-tfidf-15" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>16 0.093669154 <a title="91-tfidf-16" href="./iccv-2013-Predicting_Primary_Gaze_Behavior_Using_Social_Saliency_Fields.html">325 iccv-2013-Predicting Primary Gaze Behavior Using Social Saliency Fields</a></p>
<p>17 0.085445158 <a title="91-tfidf-17" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>18 0.077903472 <a title="91-tfidf-18" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>19 0.074337438 <a title="91-tfidf-19" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<p>20 0.073997818 <a title="91-tfidf-20" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.162), (1, -0.063), (2, 0.571), (3, -0.283), (4, -0.178), (5, 0.003), (6, 0.041), (7, -0.05), (8, 0.024), (9, -0.014), (10, -0.035), (11, 0.042), (12, 0.0), (13, -0.016), (14, 0.022), (15, -0.066), (16, 0.11), (17, -0.035), (18, -0.047), (19, 0.105), (20, -0.001), (21, -0.012), (22, 0.021), (23, 0.008), (24, 0.02), (25, -0.04), (26, 0.008), (27, -0.003), (28, 0.0), (29, -0.022), (30, -0.013), (31, 0.009), (32, -0.004), (33, 0.008), (34, 0.027), (35, -0.026), (36, -0.005), (37, 0.0), (38, -0.01), (39, 0.003), (40, -0.002), (41, -0.021), (42, -0.009), (43, -0.009), (44, 0.006), (45, -0.017), (46, 0.012), (47, -0.005), (48, 0.007), (49, 0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97162253 <a title="91-lsi-1" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>Author: Xi Li, Yao Li, Chunhua Shen, Anthony Dick, Anton Van_Den_Hengel</p><p>Abstract: Salient object detection aims to locate objects that capture human attention within images. Previous approaches often pose this as a problem of image contrast analysis. In this work, we model an image as a hypergraph that utilizes a set of hyperedges to capture the contextual properties of image pixels or regions. As a result, the problem of salient object detection becomes one of finding salient vertices and hyperedges in the hypergraph. The main advantage of hypergraph modeling is that it takes into account each pixel’s (or region ’s) affinity with its neighborhood as well as its separation from image background. Furthermore, we propose an alternative approach based on centerversus-surround contextual contrast analysis, which performs salient object detection by optimizing a cost-sensitive support vector machine (SVM) objective function. Experimental results on four challenging datasets demonstrate the effectiveness of the proposed approaches against the stateof-the-art approaches to salient object detection.</p><p>2 0.93970615 <a title="91-lsi-2" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>Author: Yangqing Jia, Mei Han</p><p>Abstract: It is known that purely low-level saliency cues such as frequency does not lead to a good salient object detection result, requiring high-level knowledge to be adopted for successful discovery of task-independent salient objects. In this paper, we propose an efficient way to combine such high-level saliency priors and low-level appearance models. We obtain the high-level saliency prior with the objectness algorithm to find potential object candidates without the need of category information, and then enforce the consistency among the salient regions using a Gaussian MRF with the weights scaled by diverse density that emphasizes the influence of potential foreground pixels. Our model obtains saliency maps that assign high scores for the whole salient object, and achieves state-of-the-art performance on benchmark datasets covering various foreground statistics.</p><p>3 0.93083841 <a title="91-lsi-3" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>Author: Peng Jiang, Haibin Ling, Jingyi Yu, Jingliang Peng</p><p>Abstract: The goal of saliency detection is to locate important pixels or regions in an image which attract humans ’ visual attention the most. This is a fundamental task whose output may serve as the basis for further computer vision tasks like segmentation, resizing, tracking and so forth. In this paper we propose a novel salient region detection algorithm by integrating three important visual cues namely uniqueness, focusness and objectness (UFO). In particular, uniqueness captures the appearance-derived visual contrast; focusness reflects the fact that salient regions are often photographed in focus; and objectness helps keep completeness of detected salient regions. While uniqueness has been used for saliency detection for long, it is new to integrate focusness and objectness for this purpose. In fact, focusness and objectness both provide important saliency information complementary of uniqueness. In our experiments using public benchmark datasets, we show that, even with a simple pixel level combination of the three components, the proposed approach yields significant improve- ment compared with previously reported methods.</p><p>4 0.92273986 <a title="91-lsi-4" href="./iccv-2013-Saliency_Detection%3A_A_Boolean_Map_Approach.html">369 iccv-2013-Saliency Detection: A Boolean Map Approach</a></p>
<p>Author: Jianming Zhang, Stan Sclaroff</p><p>Abstract: A novel Boolean Map based Saliency (BMS) model is proposed. An image is characterized by a set of binary images, which are generated by randomly thresholding the image ’s color channels. Based on a Gestalt principle of figure-ground segregation, BMS computes saliency maps by analyzing the topological structure of Boolean maps. BMS is simple to implement and efficient to run. Despite its simplicity, BMS consistently achieves state-of-the-art performance compared with ten leading methods on five eye tracking datasets. Furthermore, BMS is also shown to be advantageous in salient object detection.</p><p>5 0.90866649 <a title="91-lsi-5" href="./iccv-2013-Saliency_and_Human_Fixations%3A_State-of-the-Art_and_Study_of_Comparison_Metrics.html">373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</a></p>
<p>Author: Nicolas Riche, Matthieu Duvinage, Matei Mancas, Bernard Gosselin, Thierry Dutoit</p><p>Abstract: Visual saliency has been an increasingly active research area in the last ten years with dozens of saliency models recently published. Nowadays, one of the big challenges in the field is to find a way to fairly evaluate all of these models. In this paper, on human eye fixations ,we compare the ranking of 12 state-of-the art saliency models using 12 similarity metrics. The comparison is done on Jian Li ’s database containing several hundreds of natural images. Based on Kendall concordance coefficient, it is shown that some of the metrics are strongly correlated leading to a redundancy in the performance metrics reported in the available benchmarks. On the other hand, other metrics provide a more diverse picture of models ’ overall performance. As a recommendation, three similarity metrics should be used to obtain a complete point of view of saliency model performance.</p><p>6 0.90430236 <a title="91-lsi-6" href="./iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction.html">50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</a></p>
<p>7 0.90157312 <a title="91-lsi-7" href="./iccv-2013-Saliency_Detection_via_Dense_and_Sparse_Reconstruction.html">372 iccv-2013-Saliency Detection via Dense and Sparse Reconstruction</a></p>
<p>8 0.87399292 <a title="91-lsi-8" href="./iccv-2013-Saliency_Detection_via_Absorbing_Markov_Chain.html">371 iccv-2013-Saliency Detection via Absorbing Markov Chain</a></p>
<p>9 0.86309952 <a title="91-lsi-9" href="./iccv-2013-Saliency_Detection_in_Large_Point_Sets.html">370 iccv-2013-Saliency Detection in Large Point Sets</a></p>
<p>10 0.84423566 <a title="91-lsi-10" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>11 0.75352567 <a title="91-lsi-11" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>12 0.61769933 <a title="91-lsi-12" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>13 0.46463197 <a title="91-lsi-13" href="./iccv-2013-Semantically-Based_Human_Scanpath_Estimation_with_HMMs.html">381 iccv-2013-Semantically-Based Human Scanpath Estimation with HMMs</a></p>
<p>14 0.31492215 <a title="91-lsi-14" href="./iccv-2013-Predicting_Primary_Gaze_Behavior_Using_Social_Saliency_Fields.html">325 iccv-2013-Predicting Primary Gaze Behavior Using Social Saliency Fields</a></p>
<p>15 0.30952135 <a title="91-lsi-15" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>16 0.27986664 <a title="91-lsi-16" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>17 0.25154582 <a title="91-lsi-17" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>18 0.24494943 <a title="91-lsi-18" href="./iccv-2013-Detecting_Curved_Symmetric_Parts_Using_a_Deformable_Disc_Model.html">110 iccv-2013-Detecting Curved Symmetric Parts Using a Deformable Disc Model</a></p>
<p>19 0.23641422 <a title="91-lsi-19" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>20 0.22065014 <a title="91-lsi-20" href="./iccv-2013-Heterogeneous_Auto-similarities_of_Characteristics_%28HASC%29%3A_Exploiting_Relational_Information_for_Classification.html">193 iccv-2013-Heterogeneous Auto-similarities of Characteristics (HASC): Exploiting Relational Information for Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.09), (7, 0.023), (13, 0.016), (21, 0.19), (26, 0.076), (27, 0.011), (31, 0.04), (40, 0.011), (42, 0.083), (48, 0.011), (64, 0.034), (73, 0.029), (89, 0.162), (97, 0.094), (98, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81970453 <a title="91-lda-1" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>Author: Xi Li, Yao Li, Chunhua Shen, Anthony Dick, Anton Van_Den_Hengel</p><p>Abstract: Salient object detection aims to locate objects that capture human attention within images. Previous approaches often pose this as a problem of image contrast analysis. In this work, we model an image as a hypergraph that utilizes a set of hyperedges to capture the contextual properties of image pixels or regions. As a result, the problem of salient object detection becomes one of finding salient vertices and hyperedges in the hypergraph. The main advantage of hypergraph modeling is that it takes into account each pixel’s (or region ’s) affinity with its neighborhood as well as its separation from image background. Furthermore, we propose an alternative approach based on centerversus-surround contextual contrast analysis, which performs salient object detection by optimizing a cost-sensitive support vector machine (SVM) objective function. Experimental results on four challenging datasets demonstrate the effectiveness of the proposed approaches against the stateof-the-art approaches to salient object detection.</p><p>2 0.81824934 <a title="91-lda-2" href="./iccv-2013-A_Color_Constancy_Model_with_Double-Opponency_Mechanisms.html">5 iccv-2013-A Color Constancy Model with Double-Opponency Mechanisms</a></p>
<p>Author: Shaobing Gao, Kaifu Yang, Chaoyi Li, Yongjie Li</p><p>Abstract: The double-opponent color-sensitive cells in the primary visual cortex (V1) of the human visual system (HVS) have long been recognized as the physiological basis of color constancy. We introduce a new color constancy model by imitating the functional properties of the HVS from the retina to the double-opponent cells in V1. The idea behind the model originates from the observation that the color distribution of the responses of double-opponent cells to the input color-biased images coincides well with the light source direction. Then the true illuminant color of a scene is easily estimated by searching for the maxima of the separate RGB channels of the responses of double-opponent cells in the RGB space. Our systematical experimental evaluations on two commonly used image datasets show that the proposed model can produce competitive results in comparison to the complex state-of-the-art approaches, but with a simple implementation and without the need for training.</p><p>3 0.81245542 <a title="91-lda-3" href="./iccv-2013-Person_Re-identification_by_Salience_Matching.html">313 iccv-2013-Person Re-identification by Salience Matching</a></p>
<p>Author: Rui Zhao, Wanli Ouyang, Xiaogang Wang</p><p>Abstract: Human salience is distinctive and reliable information in matching pedestrians across disjoint camera views. In this paper, we exploit the pairwise salience distribution relationship between pedestrian images, and solve the person re-identification problem by proposing a salience matching strategy. To handle the misalignment problem in pedestrian images, patch matching is adopted and patch salience is estimated. Matching patches with inconsistent salience brings penalty. Images of the same person are recognized by minimizing the salience matching cost. Furthermore, our salience matching is tightly integrated with patch matching in a unified structural RankSVM learning framework. The effectiveness of our approach is validated on the VIPeR dataset and the CUHK Campus dataset. It outperforms the state-of-the-art methods on both datasets.</p><p>4 0.81192911 <a title="91-lda-4" href="./iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction.html">50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</a></p>
<p>Author: Ali Borji, Hamed R. Tavakoli, Dicky N. Sihite, Laurent Itti</p><p>Abstract: Significant recent progress has been made in developing high-quality saliency models. However, less effort has been undertaken on fair assessment of these models, over large standardized datasets and correctly addressing confounding factors. In this study, we pursue a critical and quantitative look at challenges (e.g., center-bias, map smoothing) in saliency modeling and the way they affect model accuracy. We quantitatively compare 32 state-of-the-art models (using the shuffled AUC score to discount center-bias) on 4 benchmark eye movement datasets, for prediction of human fixation locations and scanpath sequence. We also account for the role of map smoothing. We find that, although model rankings vary, some (e.g., AWS, LG, AIM, and HouNIPS) consistently outperform other models over all datasets. Some models work well for prediction of both fixation locations and scanpath sequence (e.g., Judd, GBVS). Our results show low prediction accuracy for models over emotional stimuli from the NUSEF dataset. Our last benchmark, for the first time, gauges the ability of models to decode the stimulus category from statistics of fixations, saccades, and model saliency values at fixated locations. In this test, ITTI and AIM models win over other models. Our benchmark provides a comprehensive high-level picture of the strengths and weaknesses of many popular models, and suggests future research directions in saliency modeling.</p><p>5 0.76883179 <a title="91-lda-5" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>Author: Hesamoddin Salehian, Guang Cheng, Baba C. Vemuri, Jeffrey Ho</p><p>Abstract: Symmetric positive-definite (SPD) matrices are ubiquitous in Computer Vision, Machine Learning and Medical Image Analysis. Finding the center/average of a population of such matrices is a common theme in many algorithms such as clustering, segmentation, principal geodesic analysis, etc. The center of a population of such matrices can be defined using a variety of distance/divergence measures as the minimizer of the sum of squared distances/divergences from the unknown center to the members of the population. It is well known that the computation of the Karcher mean for the space of SPD matrices which is a negativelycurved Riemannian manifold is computationally expensive. Recently, the LogDet divergence-based center was shown to be a computationally attractive alternative. However, the LogDet-based mean of more than two matrices can not be computed in closed form, which makes it computationally less attractive for large populations. In this paper we present a novel recursive estimator for center based on the Stein distance which is the square root of the LogDet di– vergence that is significantly faster than the batch mode computation of this center. The key theoretical contribution is a closed-form solution for the weighted Stein center of two SPD matrices, which is used in the recursive computation of the Stein center for a population of SPD matrices. Additionally, we show experimental evidence of the convergence of our recursive Stein center estimator to the batch mode Stein center. We present applications of our recursive estimator to K-means clustering and image indexing depicting significant time gains over corresponding algorithms that use the batch mode computations. For the latter application, we develop novel hashing functions using the Stein distance and apply it to publicly available data sets, and experimental results have shown favorable com– ∗This research was funded in part by the NIH grant NS066340 to BCV. †Corresponding author parisons to other competing methods.</p><p>6 0.76492977 <a title="91-lda-6" href="./iccv-2013-Saliency_and_Human_Fixations%3A_State-of-the-Art_and_Study_of_Comparison_Metrics.html">373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</a></p>
<p>7 0.76423264 <a title="91-lda-7" href="./iccv-2013-Synergistic_Clustering_of_Image_and_Segment_Descriptors_for_Unsupervised_Scene_Understanding.html">412 iccv-2013-Synergistic Clustering of Image and Segment Descriptors for Unsupervised Scene Understanding</a></p>
<p>8 0.76108932 <a title="91-lda-8" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>9 0.75996429 <a title="91-lda-9" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>10 0.75074023 <a title="91-lda-10" href="./iccv-2013-Saliency_Detection_via_Dense_and_Sparse_Reconstruction.html">372 iccv-2013-Saliency Detection via Dense and Sparse Reconstruction</a></p>
<p>11 0.74385351 <a title="91-lda-11" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>12 0.7393828 <a title="91-lda-12" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>13 0.73931617 <a title="91-lda-13" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>14 0.73926437 <a title="91-lda-14" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>15 0.73823982 <a title="91-lda-15" href="./iccv-2013-Saliency_Detection%3A_A_Boolean_Map_Approach.html">369 iccv-2013-Saliency Detection: A Boolean Map Approach</a></p>
<p>16 0.73471272 <a title="91-lda-16" href="./iccv-2013-Saliency_Detection_via_Absorbing_Markov_Chain.html">371 iccv-2013-Saliency Detection via Absorbing Markov Chain</a></p>
<p>17 0.72921222 <a title="91-lda-17" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>18 0.72765738 <a title="91-lda-18" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>19 0.7203486 <a title="91-lda-19" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>20 0.71849036 <a title="91-lda-20" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
