<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-9" href="#">iccv2013-9</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</h1>
<br/><p>Source: <a title="iccv-2013-9-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Thomas_A_Flexible_Scene_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Diego Thomas, Akihiro Sugimoto</p><p>Abstract: Updating a global 3D model with live RGB-D measurements has proven to be successful for 3D reconstruction of indoor scenes. Recently, a Truncated Signed Distance Function (TSDF) volumetric model and a fusion algorithm have been introduced (KinectFusion), showing significant advantages such as computational speed and accuracy of the reconstructed scene. This algorithm, however, is expensive in memory when constructing and updating the global model. As a consequence, the method is not well scalable to large scenes. We propose a new flexible 3D scene representation using a set of planes that is cheap in memory use and, nevertheless, achieves accurate reconstruction of indoor scenes from RGB-D image sequences. Projecting the scene onto different planes reduces significantly the size of the scene representation and thus it allows us to generate a global textured 3D model with lower memory requirement while keeping accuracy and easiness to update with live RGB-D measurements. Experimental results demonstrate that our proposed flexible 3D scene representation achieves accurate reconstruction, while keeping the scalability for large indoor scenes.</p><p>Reference: <a title="iccv-2013-9-reference" href="../iccv2013_reference/iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract Updating a global 3D model with live RGB-D measurements has proven to be successful for 3D reconstruction of indoor scenes. [sent-3, score-0.531]
</p><p>2 We propose a new flexible 3D scene representation using a set of planes that is cheap in memory use and, nevertheless, achieves accurate reconstruction of indoor scenes from RGB-D image sequences. [sent-7, score-0.818]
</p><p>3 Projecting the scene onto different planes reduces significantly the size of the scene representation and thus it allows us to generate a global textured 3D model with lower memory requirement while keeping accuracy and easiness to update with live  RGB-D measurements. [sent-8, score-1.033]
</p><p>4 Experimental results demonstrate that our proposed flexible 3D scene representation achieves accurate reconstruction, while keeping the scalability for large indoor scenes. [sent-9, score-0.335]
</p><p>5 Introduction The fine 3D reconstruction of large indoor scenes from RGB-D measurements is of wide interest for the computer vision community, with various potential applications. [sent-11, score-0.32]
</p><p>6 With recent efforts on developing inexpensive depth sensors such as the Microsoft Kinect camera or the Asus Xtion Pro camera (also called RGB-D cameras), capturing depth information in indoor environments becomes an easy task. [sent-13, score-0.614]
</p><p>7 This new set-up opens new possibilities for 3D reconstruction, and several softwares have been already proposed to realize live 3D reconstruction using RGB-D cameras. [sent-14, score-0.306]
</p><p>8 In general, the live 3D reconstruction process can be divided into 4 steps: (1) RGB-D measurement acquisition; Akihiro Sugimoto National Institute of Informatics Chiyoda, Tokyo, Japan sugimot o @ nii . [sent-15, score-0.301]
</p><p>9 j p (2) camera tracking; (3) integration (or fusion) of aligned  RGB-D measurements and (4) 3D modeling (3D textured mesh generation for example). [sent-17, score-0.4]
</p><p>10 Each incoming frame is aligned to its previous frame, then newly aligned data are integrated into a global model (using surfels [15] for example), which is triangulated in a final post-process (using poisson surface reconstruction [12] for example). [sent-24, score-0.411]
</p><p>11 In this approach, only the current and previous frames are loaded on the GPU memory for camera tracking and, therefore, the GPU memory use is low and tracking is fast. [sent-29, score-0.417]
</p><p>12 In this strategy, a single high-quality global 3D model is updated along with live depth measurements, where the global model is represented as a signed distance function that is discretized into a volume covering the scene to be 2800  reconstructed. [sent-31, score-0.702]
</p><p>13 Incoming depth images are then aligned to high-quality predicted depth images generated from the global model. [sent-32, score-0.406]
</p><p>14 Using a global model allows to reduce the error propagation, however, the volumetric representation of the scene is highly memory consuming. [sent-33, score-0.405]
</p><p>15 As a consequence, most of the GPU memory space is used to update the global model and generate depth images. [sent-34, score-0.302]
</p><p>16 Extensions of KinectFusion have been proposed where the volume is moved along with the camera  to reconstruct large indoor scenes. [sent-39, score-0.316]
</p><p>17 In this paper, we introduce a new 3D scene representation using a set ofplanes1 that requires low memory use and, nevertheless, realizes accurate and efficient 3D reconstruction of a scene from an RGB-D image sequence, even when some parts of the sequence have poor geometric features. [sent-42, score-0.721]
</p><p>18 The main idea is to represent a static scene as a set of planes to each of which we attach as its attributes three images describing the local geometry and color. [sent-44, score-0.514]
</p><p>19 This representation is flexible in that we can easily build and update the scene representation and more importantly, with the attributes, we can always recover the full 3D information of the scene while reducing required memory. [sent-45, score-0.45]
</p><p>20 The low memory use required to maintain our 3D scene representation allows us to use both geometry and color for camera tracking with standard graphic cards. [sent-47, score-0.615]
</p><p>21 Moreover, our 3D scene rep-  resentation enables us to directly output roughly segmented dense 3D textured meshes from a large sequence of RGB-D images, even when parts of the sequences lack in rich geometric features. [sent-52, score-0.511]
</p><p>22 1In this paper we use planes only for simplicity, but the same approach can be used straightforwardly with other parametric shapes such as spheres, cylinders or cones, once they are detected from the image sequence. [sent-54, score-0.345]
</p><p>23 Methods using such a strategy build and update a global 3D model with higher resolution than that of input depth images in an online manner along with live measurements. [sent-59, score-0.355]
</p><p>24 As addressed in [20], however, the use of surfels has the advantage that it can be easily updated with live measurements while keeping consistency of the global model. [sent-63, score-0.443]
</p><p>25 [13] proposed KinectFusion where the global 3D model is represented as a signed distance function that is discretized into a volume covering a scene to be reconstructed. [sent-68, score-0.308]
</p><p>26 Incoming depth images are then aligned to a high-quality predicted depth image generated  from the global model. [sent-69, score-0.406]
</p><p>27 The advantage of using a volumetric representation of the scene is that dense depth images can be rendered for a given camera position. [sent-72, score-0.588]
</p><p>28 Namely, for a given resolution, the data size for the volume becomes too large and memory 2801  consuming when the target scene becomes large. [sent-75, score-0.304]
</p><p>29 Extensions of KinectFusion to a large scene have been proposed where the volume is moved along with the tracked camera motion. [sent-76, score-0.297]
</p><p>30 However, rich geometric features in the RGB-D image sequence are required for accurate scene re-  construction. [sent-85, score-0.354]
</p><p>31 When a scene does not have rich geometric features, this method fails in tracking the camera motion, resulting in poor reconstruction. [sent-86, score-0.454]
</p><p>32 By contrast, our proposed method requires only three 2D images for each plane to model the scene, which allows more compact representation of the scene. [sent-93, score-0.309]
</p><p>33 As the size of the scene to be reconstructed becomes large, representing the whole scene using clouds of points or volumes becomes unpractical. [sent-94, score-0.424]
</p><p>34 In this paper we propose a scene representation that allows to directly build roughly segmented 3D textured meshes from an RGB-D image sequence. [sent-96, score-0.315]
</p><p>35 Scene representation using planes For 3D scene reconstruction using RGB-D cameras, how to represent the target scene is of crucial importance. [sent-98, score-0.702]
</p><p>36 The representation should be (1) compact, so that the user can reconstruct large scenes, (2) accurate for fine reconstruction, (3) easy to update with live measurements for fast pro-  cessing, and (4) capable of predicting dense high-quality RGB-D images for accurate camera tracking. [sent-99, score-0.513]
</p><p>37 We reason that parametric shapes such as planes can be used to describe an indoor scene because it is mainly composed of man-made objects (such as table, wall and storage for example), with rather simple shapes. [sent-101, score-0.55]
</p><p>38 We thus propose to represent a scene as a set of planes (again only for simplicity) having attributes. [sent-102, score-0.409]
</p><p>39 To each plane2 detected in the scene, we attach as its attributes three 2D images in addition to information that identifies the plane (planar patch). [sent-103, score-0.334]
</p><p>40 We choose quads rather than triangles because the quadrangulation is a natural meshing for a 2D grid (also the number of quads becomes smaller than that of triangles when generating meshes). [sent-113, score-0.322]
</p><p>41 We can thus render 3D textured meshes from an estimated camera pose to obtain a high-quality dense predicted RGB-D image from the global model using 3D renderers such as OpenGL. [sent-114, score-0.383]
</p><p>42 Plane detection We detect different planes that decompose the target scene by modifying the method proposed in [4] (we employ the Floyd-Warshall algorithm [7] to detect local maxima). [sent-118, score-0.409]
</p><p>43 Note that for our 3D representation the accuracy ofdetected plane parameters is not critical. [sent-120, score-0.309]
</p><p>44 For each point in a given depth image, we first compute its normal vector and generate a histogram of normals from the depth image. [sent-122, score-0.374]
</p><p>45 We thus ob-  2Since a visible region of a plane in the scene has a finite size, a planar patch may be more appropriate, however, we use the word plane for simplicity. [sent-126, score-0.657]
</p><p>46 We remark that, in our method, each plane is identified by its plane equation and a bounding box. [sent-127, score-0.559]
</p><p>47 Thereafter, for each normal vector in the set, we compute for each point in its corresponding list the plane that passes the point and that has the normal vector and then compute the distance dorigin of the origin from this plane. [sent-131, score-0.466]
</p><p>48 By identifying the median of distances for each connected class, we finally obtain a set of plane parameters for the input depth image. [sent-134, score-0.402]
</p><p>49 When a plane has been visible a sufficient number of times, we refine the plane parameters (in the experiments, we refined plane parameters when the plane has been visible twice, 20 times and 80 times). [sent-135, score-1.032]
</p><p>50 After obtaining new plane parameters, we recompute all attributes of the plane. [sent-136, score-0.302]
</p><p>51 For a given plane, the Bump image is obtained by identifying points around the plane and projecting them onto the discretized plane. [sent-140, score-0.428]
</p><p>52 Figure 2 depicts our simple example, where 3D points are randomly distributed around a plane defined by ( n? [sent-144, score-0.301]
</p><p>53 , d), where is the normal vector of the plane and d is the distance of the plane from the origin. [sent-145, score-0.571]
</p><p>54 Our objective is then to compute the Bump image for the plane  n  and to accurately recover the set of 3D points using only the plane parameters and the computed Bump image. [sent-146, score-0.559]
</p><p>55 , n) for the plane, with the origin being the projection onto the plane of the origin of the global coordinate system. [sent-148, score-0.429]
</p><p>56 p onto the plane to obtain its 2D coordinates π(? [sent-175, score-0.313]
</p><p>57 Given a plane and its Bump image, it is straightforward to compute the exact position of a point corresponding to a pixel (k, l) in the global 3D coordinate system. [sent-239, score-0.373]
</p><p>58 The Mask image is then updated with live measurements as explained in Section 4. [sent-250, score-0.313]
</p><p>59 Scene reconstruction The implicit quadrangulation given by the 2D discretization of each plane allows us to maintain our 3D mesh representation of the scene. [sent-257, score-0.668]
</p><p>60 The global mesh is used to render a dense and high-quality predicted depth image (see Section 4. [sent-258, score-0.394]
</p><p>61 Our scene representation is then updated with aligned live measurements, and newly detected planes, if any, are added as the camera moves through the scene. [sent-260, score-0.598]
</p><p>62 Camera tracking Accurately tracking the camera is of crucial importance for any 3D reconstruction method. [sent-265, score-0.29]
</p><p>63 Thanks to low memory use for our scene representation, we still have the GPU memory space available enough to combine the linearized GICP with the SIFT-GPU [2] algorithm in the same way as [10]. [sent-267, score-0.411]
</p><p>64 This al-  lows us to reconstruct scene even when parts of the RGB-D image sequence are poor in geometric features. [sent-268, score-0.362]
</p><p>65 For the linearized GICP to work well, a key issue is to align incoming frames with a dense and high-quality predicted depth image. [sent-269, score-0.306]
</p><p>66 For this reason, directly projecting all the points of the global model into the current camera plane is not appropriate. [sent-271, score-0.492]
</p><p>67 This is because (1) parts ofthe scene that are close to the camera would produce relatively sparse depth information and (2) handling occlusions would require significant efforts. [sent-272, score-0.428]
</p><p>68 Instead, we take advantage of the natural quadrangulation given by the 2D image discretization to render a depth image using meshes. [sent-273, score-0.344]
</p><p>69 quads that have positive mask values at their four summits). [sent-276, score-0.296]
</p><p>70 This straightforwardly gives us a quadrangulation for each plane (vertices are computed using Equation (1)), which can be quickly rendered into the current camera plane using a 3D renderer such as OpenGL. [sent-277, score-0.858]
</p><p>71 Note that for a given camera pose, only planes intersecting with the perspective frustum of the current camera pose are rendered, which eases computation. [sent-278, score-0.482]
</p><p>72 This is because small errors in camera tracking, misalignments between depth and color images and motion blur tend to blur the color image, which is fatal when extracting and matching SIFT features. [sent-280, score-0.419]
</p><p>73 Updating model with live measurements With the estimated camera pose, and the live RGB-D measurements, we can now update and refine our 3D scene representation. [sent-284, score-0.689]
</p><p>74 The currently aligned depth image is merged with the predicted depth image using a rendered Mask image. [sent-287, score-0.517]
</p><p>75 The newly obtained 3D points are then projected onto different planes and the attributes of each plane are updated by replacing mask, color and bump values with newly computed ones (if available). [sent-288, score-1.497]
</p><p>76 In contrast, straightforwardly projecting live measurements onto the different planes and then updating the mask, color and bump values at the projected pixel does not give nice results. [sent-289, score-1.447]
</p><p>77 This is because the point corresponding to a pixel of the input depth image and the point represented at the projected pixel in the plane do not likely represent the same point at the surface. [sent-290, score-0.613]
</p><p>78 Note that color and depth values are merged similarly. [sent-297, score-0.307]
</p><p>79 , then mask, color and bump values are all replaced by the newly computed values for the point p? [sent-302, score-0.727]
</p><p>80 All points that do not belong to any existing planes are kept in a residual image from which new planes are detected. [sent-305, score-0.579]
</p><p>81 Resizing planes From a single depth image, only some parts of planes are visible in general (like the ground for example). [sent-308, score-0.716]
</p><p>82 Merging planes Because of the noise in the depth images, false planes may be detected. [sent-317, score-0.68]
</p><p>83 However, since the Bump image records exact 3D point positions, after refining the plane parameters (see Section 3. [sent-318, score-0.289]
</p><p>84 Then multiple planes may represent  the same parts of the scene, which must be avoided. [sent-320, score-0.304]
</p><p>85 We identify such planes by computing the intersection of the bounding boxes of planes having similar parameters. [sent-321, score-0.579]
</p><p>86 If two planes have non-empty intersection between their bounding boxes, then the two planes are merged. [sent-322, score-0.579]
</p><p>87 , y) of the largest plane and compute the union of the two bounding boxes of the largest plane and a plane to be merged in order to create the bounding box of the merged plane. [sent-325, score-1.056]
</p><p>88 Thereafter, points from the two planes are projected onto the merged plane with their respective color and mask values. [sent-326, score-1.025]
</p><p>89 Note that if two points project onto the same pixel in the merged plane, only the one with the maximum mask value is kept. [sent-327, score-0.436]
</p><p>90 The visibility of the merged plane is set to the one of the largest plane. [sent-328, score-0.356]
</p><p>91 We evaluate the accuracy of our method to reconstruct an indoor scene in fine details by comparing our results with those by KinFuLargeScale. [sent-339, score-0.301]
</p><p>92 To illustrate the usefulness of combining both depth and color images, we captured a large scene called Room1 containing parts that do not have rich geometric features. [sent-352, score-0.522]
</p><p>93 However, by combining both depth and color images, as expected, we could successfully track the camera and reconstruct the 3D scene using the frame-to-global-model strategy as shown in Fig. [sent-357, score-0.505]
</p><p>94 The obtained reconstructed scene with our method was of 1532 K vertices and 2423 K faces and our scene representation required 34 MB. [sent-364, score-0.422]
</p><p>95 We remark that,  like data Room1, KinFuLargeScale failed to reconstruct the scene as parts of the sequence did not have rich geometric features. [sent-379, score-0.432]
</p><p>96 By using standard 2D painting softwares such as Photoshop, it is easy to edit the Bump, Mask and Color images ofdifferent planes that compose the scene. [sent-382, score-0.306]
</p><p>97 After editing the Bump, Mask and Color images of the plane correspond-  ing to the ground we could obtain the original scene even though it was featureless, as shown in Fig. [sent-402, score-0.399]
</p><p>98 Conclusions We proposed a new 3D scene representation using a set of planes that achieves accurate and efficient 3D reconstruction of an indoor scene from an RGB-D image sequence, even when the scene has poor geometric features. [sent-405, score-1.05]
</p><p>99 In this paper, we focused only on planes to represent the scene, however, our approach can be directly applied to other parametric shapes such as spheres and even to a combination of parametric shapes. [sent-407, score-0.326]
</p><p>100 The 3d hough transform for plane detection in point clouds: A review and new accumulator design. [sent-426, score-0.289]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bump', 0.575), ('planes', 0.268), ('plane', 0.258), ('mask', 0.2), ('live', 0.167), ('kinfulargescale', 0.151), ('depth', 0.144), ('scene', 0.141), ('kinectfusion', 0.133), ('quadrangulation', 0.13), ('memory', 0.114), ('indoor', 0.112), ('measurements', 0.107), ('camera', 0.107), ('reconstruction', 0.101), ('merged', 0.098), ('mesh', 0.097), ('quads', 0.096), ('dmerged', 0.086), ('gicp', 0.086), ('surfels', 0.086), ('tsdf', 0.077), ('meshes', 0.071), ('rich', 0.07), ('gpu', 0.067), ('thereafter', 0.067), ('geometric', 0.066), ('color', 0.065), ('rendered', 0.057), ('newly', 0.056), ('normal', 0.055), ('volumetric', 0.055), ('onto', 0.055), ('reconstructed', 0.054), ('textured', 0.052), ('representation', 0.051), ('henry', 0.05), ('transaction', 0.05), ('incoming', 0.05), ('volume', 0.049), ('straightforwardly', 0.048), ('reconstruct', 0.048), ('volumes', 0.045), ('attributes', 0.044), ('updating', 0.044), ('global', 0.044), ('bounding', 0.043), ('chiyoda', 0.043), ('kintinuous', 0.043), ('yobjects', 0.043), ('kinect', 0.043), ('points', 0.043), ('sequence', 0.042), ('cm', 0.042), ('signed', 0.042), ('linearized', 0.042), ('tracking', 0.041), ('pixel', 0.04), ('projecting', 0.04), ('votes', 0.04), ('updated', 0.039), ('render', 0.039), ('record', 0.038), ('fatal', 0.038), ('softwares', 0.038), ('tjh', 0.038), ('projected', 0.038), ('aligned', 0.037), ('predicted', 0.037), ('origin', 0.036), ('parts', 0.036), ('pcl', 0.035), ('surfel', 0.035), ('whelan', 0.035), ('dist', 0.035), ('required', 0.035), ('dense', 0.033), ('nii', 0.033), ('registration', 0.033), ('processed', 0.032), ('discretized', 0.032), ('attach', 0.032), ('graphic', 0.032), ('resizing', 0.032), ('point', 0.031), ('flexible', 0.031), ('discretization', 0.031), ('davison', 0.031), ('izadi', 0.031), ('weise', 0.031), ('encodes', 0.03), ('fps', 0.03), ('failed', 0.029), ('diego', 0.029), ('japan', 0.029), ('tokyo', 0.029), ('parametric', 0.029), ('poor', 0.029), ('geometry', 0.029), ('bins', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="9-tfidf-1" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>Author: Diego Thomas, Akihiro Sugimoto</p><p>Abstract: Updating a global 3D model with live RGB-D measurements has proven to be successful for 3D reconstruction of indoor scenes. Recently, a Truncated Signed Distance Function (TSDF) volumetric model and a fusion algorithm have been introduced (KinectFusion), showing significant advantages such as computational speed and accuracy of the reconstructed scene. This algorithm, however, is expensive in memory when constructing and updating the global model. As a consequence, the method is not well scalable to large scenes. We propose a new flexible 3D scene representation using a set of planes that is cheap in memory use and, nevertheless, achieves accurate reconstruction of indoor scenes from RGB-D image sequences. Projecting the scene onto different planes reduces significantly the size of the scene representation and thus it allows us to generate a global textured 3D model with lower memory requirement while keeping accuracy and easiness to update with live RGB-D measurements. Experimental results demonstrate that our proposed flexible 3D scene representation achieves accurate reconstruction, while keeping the scalability for large indoor scenes.</p><p>2 0.17469811 <a title="9-tfidf-2" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>Author: Carl Yuheng Ren, Victor Prisacariu, David Murray, Ian Reid</p><p>Abstract: We introduce a probabilistic framework for simultaneous tracking and reconstruction of 3D rigid objects using an RGB-D camera. The tracking problem is handled using a bag-of-pixels representation and a back-projection scheme. Surface and background appearance models are learned online, leading to robust tracking in the presence of heavy occlusion and outliers. In both our tracking and reconstruction modules, the 3D object is implicitly embedded using a 3D level-set function. The framework is initialized with a simple shape primitive model (e.g. a sphere or a cube), and the real 3D object shape is tracked and reconstructed online. Unlike existing depth-based 3D reconstruction works, which either rely on calibrated/fixed camera set up or use the observed world map to track the depth camera, our framework can simultaneously track and reconstruct small moving objects. We use both qualitative and quantitative results to demonstrate the superior performance of both tracking and reconstruction of our method.</p><p>3 0.14228331 <a title="9-tfidf-3" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>Author: Jianxiong Xiao, Andrew Owens, Antonio Torralba</p><p>Abstract: Existing scene understanding datasets contain only a limited set of views of a place, and they lack representations of complete 3D spaces. In this paper, we introduce SUN3D, a large-scale RGB-D video database with camera pose and object labels, capturing the full 3D extent of many places. The tasks that go into constructing such a dataset are difficult in isolation hand-labeling videos is painstaking, and structure from motion (SfM) is unreliable for large spaces. But if we combine them together, we make the dataset construction task much easier. First, we introduce an intuitive labeling tool that uses a partial reconstruction to propagate labels from one frame to another. Then we use the object labels to fix errors in the reconstruction. For this, we introduce a generalization of bundle adjustment that incorporates object-to-object correspondences. This algorithm works by constraining points for the same object from different frames to lie inside a fixed-size bounding box, parameterized by its rotation and translation. The SUN3D database, the source code for the generalized bundle adjustment, and the web-based 3D annotation tool are all avail– able at http://sun3d.cs.princeton.edu.</p><p>4 0.14098005 <a title="9-tfidf-4" href="./iccv-2013-Large-Scale_Multi-resolution_Surface_Reconstruction_from_RGB-D_Sequences.html">228 iccv-2013-Large-Scale Multi-resolution Surface Reconstruction from RGB-D Sequences</a></p>
<p>Author: Frank Steinbrücker, Christian Kerl, Daniel Cremers</p><p>Abstract: We propose a method to generate highly detailed, textured 3D models of large environments from RGB-D sequences. Our system runs in real-time on a standard desktop PC with a state-of-the-art graphics card. To reduce the memory consumption, we fuse the acquired depth maps and colors in a multi-scale octree representation of a signed distance function. To estimate the camera poses, we construct a pose graph and use dense image alignment to determine the relative pose between pairs of frames. We add edges between nodes when we detect loop-closures and optimize the pose graph to correct for long-term drift. Our implementation is highly parallelized on graphics hardware to achieve real-time performance. More specifically, we can reconstruct, store, and continuously update a colored 3D model of an entire corridor of nine rooms at high levels of detail in real-time on a single GPU with 2.5GB.</p><p>5 0.13689658 <a title="9-tfidf-5" href="./iccv-2013-Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization.html">284 iccv-2013-Multiview Photometric Stereo Using Planar Mesh Parameterization</a></p>
<p>Author: Jaesik Park, Sudipta N. Sinha, Yasuyuki Matsushita, Yu-Wing Tai, In So Kweon</p><p>Abstract: We propose a method for accurate 3D shape reconstruction using uncalibrated multiview photometric stereo. A coarse mesh reconstructed using multiview stereo is first parameterized using a planar mesh parameterization technique. Subsequently, multiview photometric stereo is performed in the 2D parameter domain of the mesh, where all geometric and photometric cues from multiple images can be treated uniformly. Unlike traditional methods, there is no need for merging view-dependent surface normal maps. Our key contribution is a new photometric stereo based mesh refinement technique that can efficiently reconstruct meshes with extremely fine geometric details by directly estimating a displacement texture map in the 2D parameter domain. We demonstrate that intricate surface geometry can be reconstructed using several challenging datasets containing surfaces with specular reflections, multiple albedos and complex topologies.</p><p>6 0.13667926 <a title="9-tfidf-6" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>7 0.13603093 <a title="9-tfidf-7" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>8 0.1351919 <a title="9-tfidf-8" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>9 0.12692216 <a title="9-tfidf-9" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>10 0.12664892 <a title="9-tfidf-10" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>11 0.126127 <a title="9-tfidf-11" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>12 0.12382463 <a title="9-tfidf-12" href="./iccv-2013-Live_Metric_3D_Reconstruction_on_Mobile_Phones.html">254 iccv-2013-Live Metric 3D Reconstruction on Mobile Phones</a></p>
<p>13 0.11912008 <a title="9-tfidf-13" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>14 0.11811993 <a title="9-tfidf-14" href="./iccv-2013-Image_Guided_Depth_Upsampling_Using_Anisotropic_Total_Generalized_Variation.html">209 iccv-2013-Image Guided Depth Upsampling Using Anisotropic Total Generalized Variation</a></p>
<p>15 0.11582498 <a title="9-tfidf-15" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>16 0.11355853 <a title="9-tfidf-16" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>17 0.11008099 <a title="9-tfidf-17" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>18 0.1094335 <a title="9-tfidf-18" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>19 0.10915612 <a title="9-tfidf-19" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<p>20 0.10758572 <a title="9-tfidf-20" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.208), (1, -0.197), (2, -0.041), (3, 0.031), (4, 0.025), (5, -0.03), (6, -0.008), (7, -0.153), (8, -0.034), (9, 0.041), (10, 0.028), (11, 0.003), (12, -0.046), (13, 0.038), (14, -0.021), (15, -0.079), (16, -0.001), (17, -0.017), (18, -0.061), (19, -0.043), (20, -0.084), (21, 0.025), (22, 0.009), (23, 0.033), (24, -0.035), (25, 0.003), (26, -0.072), (27, 0.059), (28, 0.01), (29, 0.06), (30, 0.012), (31, -0.017), (32, 0.002), (33, -0.043), (34, -0.068), (35, 0.023), (36, 0.04), (37, 0.034), (38, 0.063), (39, 0.08), (40, -0.042), (41, -0.015), (42, -0.037), (43, 0.028), (44, -0.03), (45, -0.017), (46, 0.012), (47, 0.008), (48, 0.014), (49, -0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95883578 <a title="9-lsi-1" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>Author: Diego Thomas, Akihiro Sugimoto</p><p>Abstract: Updating a global 3D model with live RGB-D measurements has proven to be successful for 3D reconstruction of indoor scenes. Recently, a Truncated Signed Distance Function (TSDF) volumetric model and a fusion algorithm have been introduced (KinectFusion), showing significant advantages such as computational speed and accuracy of the reconstructed scene. This algorithm, however, is expensive in memory when constructing and updating the global model. As a consequence, the method is not well scalable to large scenes. We propose a new flexible 3D scene representation using a set of planes that is cheap in memory use and, nevertheless, achieves accurate reconstruction of indoor scenes from RGB-D image sequences. Projecting the scene onto different planes reduces significantly the size of the scene representation and thus it allows us to generate a global textured 3D model with lower memory requirement while keeping accuracy and easiness to update with live RGB-D measurements. Experimental results demonstrate that our proposed flexible 3D scene representation achieves accurate reconstruction, while keeping the scalability for large indoor scenes.</p><p>2 0.89147502 <a title="9-lsi-2" href="./iccv-2013-Large-Scale_Multi-resolution_Surface_Reconstruction_from_RGB-D_Sequences.html">228 iccv-2013-Large-Scale Multi-resolution Surface Reconstruction from RGB-D Sequences</a></p>
<p>Author: Frank Steinbrücker, Christian Kerl, Daniel Cremers</p><p>Abstract: We propose a method to generate highly detailed, textured 3D models of large environments from RGB-D sequences. Our system runs in real-time on a standard desktop PC with a state-of-the-art graphics card. To reduce the memory consumption, we fuse the acquired depth maps and colors in a multi-scale octree representation of a signed distance function. To estimate the camera poses, we construct a pose graph and use dense image alignment to determine the relative pose between pairs of frames. We add edges between nodes when we detect loop-closures and optimize the pose graph to correct for long-term drift. Our implementation is highly parallelized on graphics hardware to achieve real-time performance. More specifically, we can reconstruct, store, and continuously update a colored 3D model of an entire corridor of nine rooms at high levels of detail in real-time on a single GPU with 2.5GB.</p><p>3 0.82629716 <a title="9-lsi-3" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>Author: Benjamin Ummenhofer, Thomas Brox</p><p>Abstract: 3D reconstruction deals with the problem of finding the shape of an object from a set of images. Thin objects that have virtually no volumepose a special challengefor reconstruction with respect to shape representation and fusion of depth information. In this paper we present a dense pointbased reconstruction method that can deal with this special class of objects. We seek to jointly optimize a set of depth maps by treating each pixel as a point in space. Points are pulled towards a common surface by pairwise forces in an iterative scheme. The method also handles the problem of opposed surfaces by means of penalty forces. Efficient optimization is achieved by grouping points to superpixels and a spatial hashing approach for fast neighborhood queries. We show that the approach is on a par with state-of-the-art methods for standard multi view stereo settings and gives superior results for thin objects.</p><p>4 0.79238695 <a title="9-lsi-4" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>Author: Qian-Yi Zhou, Stephen Miller, Vladlen Koltun</p><p>Abstract: We present an approach to reconstruction of detailed scene geometry from range video. Range data produced by commodity handheld cameras suffers from high-frequency errors and low-frequency distortion. Our approach deals with both sources of error by reconstructing locally smooth scene fragments and letting these fragments deform in order to align to each other. We develop a volumetric registration formulation that leverages the smoothness of the deformation to make optimization practical for large scenes. Experimental results demonstrate that our approach substantially increases the fidelity of complex scene geometry reconstructed with commodity handheld cameras.</p><p>5 0.78166366 <a title="9-lsi-5" href="./iccv-2013-Live_Metric_3D_Reconstruction_on_Mobile_Phones.html">254 iccv-2013-Live Metric 3D Reconstruction on Mobile Phones</a></p>
<p>Author: Petri Tanskanen, Kalin Kolev, Lorenz Meier, Federico Camposeco, Olivier Saurer, Marc Pollefeys</p><p>Abstract: unkown-abstract</p><p>6 0.76746702 <a title="9-lsi-6" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>7 0.76432383 <a title="9-lsi-7" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>8 0.75451159 <a title="9-lsi-8" href="./iccv-2013-3D_Scene_Understanding_by_Voxel-CRF.html">2 iccv-2013-3D Scene Understanding by Voxel-CRF</a></p>
<p>9 0.71206743 <a title="9-lsi-9" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>10 0.708224 <a title="9-lsi-10" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>11 0.69910407 <a title="9-lsi-11" href="./iccv-2013-Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization.html">284 iccv-2013-Multiview Photometric Stereo Using Planar Mesh Parameterization</a></p>
<p>12 0.69503635 <a title="9-lsi-12" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>13 0.66672266 <a title="9-lsi-13" href="./iccv-2013-Shape_Anchors_for_Data-Driven_Multi-view_Reconstruction.html">387 iccv-2013-Shape Anchors for Data-Driven Multi-view Reconstruction</a></p>
<p>14 0.66109329 <a title="9-lsi-14" href="./iccv-2013-Depth_from_Combining_Defocus_and_Correspondence_Using_Light-Field_Cameras.html">108 iccv-2013-Depth from Combining Defocus and Correspondence Using Light-Field Cameras</a></p>
<p>15 0.64201218 <a title="9-lsi-15" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>16 0.64165938 <a title="9-lsi-16" href="./iccv-2013-Image_Guided_Depth_Upsampling_Using_Anisotropic_Total_Generalized_Variation.html">209 iccv-2013-Image Guided Depth Upsampling Using Anisotropic Total Generalized Variation</a></p>
<p>17 0.6121062 <a title="9-lsi-17" href="./iccv-2013-Data-Driven_3D_Primitives_for_Single_Image_Understanding.html">102 iccv-2013-Data-Driven 3D Primitives for Single Image Understanding</a></p>
<p>18 0.59540021 <a title="9-lsi-18" href="./iccv-2013-Refractive_Structure-from-Motion_on_Underwater_Images.html">348 iccv-2013-Refractive Structure-from-Motion on Underwater Images</a></p>
<p>19 0.59008807 <a title="9-lsi-19" href="./iccv-2013-Dynamic_Probabilistic_Volumetric_Models.html">128 iccv-2013-Dynamic Probabilistic Volumetric Models</a></p>
<p>20 0.5898391 <a title="9-lsi-20" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.063), (12, 0.026), (18, 0.153), (26, 0.083), (31, 0.038), (34, 0.012), (35, 0.023), (40, 0.017), (42, 0.094), (64, 0.042), (73, 0.03), (89, 0.288), (95, 0.012), (98, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93089664 <a title="9-lda-1" href="./iccv-2013-Conservation_Tracking.html">87 iccv-2013-Conservation Tracking</a></p>
<p>Author: Martin Schiegg, Philipp Hanslovsky, Bernhard X. Kausler, Lars Hufnagel, Fred A. Hamprecht</p><p>Abstract: The quality of any tracking-by-assignment hinges on the accuracy of the foregoing target detection / segmentation step. In many kinds of images, errors in this first stage are unavoidable. These errors then propagate to, and corrupt, the tracking result. Our main contribution is the first probabilistic graphical model that can explicitly account for over- and undersegmentation errors even when the number of tracking targets is unknown and when they may divide, as in cell cultures. The tracking model we present implements global consistency constraints for the number of targets comprised by each detection and is solved to global optimality on reasonably large 2D+t and 3D+t datasets. In addition, we empirically demonstrate the effectiveness of a postprocessing that allows to establish target identity even across occlusion / undersegmentation. The usefulness and efficiency of this new tracking method is demonstrated on three different and challenging 2D+t and 3D+t datasets from developmental biology.</p><p>same-paper 2 0.92411673 <a title="9-lda-2" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>Author: Diego Thomas, Akihiro Sugimoto</p><p>Abstract: Updating a global 3D model with live RGB-D measurements has proven to be successful for 3D reconstruction of indoor scenes. Recently, a Truncated Signed Distance Function (TSDF) volumetric model and a fusion algorithm have been introduced (KinectFusion), showing significant advantages such as computational speed and accuracy of the reconstructed scene. This algorithm, however, is expensive in memory when constructing and updating the global model. As a consequence, the method is not well scalable to large scenes. We propose a new flexible 3D scene representation using a set of planes that is cheap in memory use and, nevertheless, achieves accurate reconstruction of indoor scenes from RGB-D image sequences. Projecting the scene onto different planes reduces significantly the size of the scene representation and thus it allows us to generate a global textured 3D model with lower memory requirement while keeping accuracy and easiness to update with live RGB-D measurements. Experimental results demonstrate that our proposed flexible 3D scene representation achieves accurate reconstruction, while keeping the scalability for large indoor scenes.</p><p>3 0.9181878 <a title="9-lda-3" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>Author: Christoph Vogel, Konrad Schindler, Stefan Roth</p><p>Abstract: Estimating dense 3D scene flow from stereo sequences remains a challenging task, despite much progress in both classical disparity and 2D optical flow estimation. To overcome the limitations of existing techniques, we introduce a novel model that represents the dynamic 3D scene by a collection of planar, rigidly moving, local segments. Scene flow estimation then amounts to jointly estimating the pixelto-segment assignment, and the 3D position, normal vector, and rigid motion parameters of a plane for each segment. The proposed energy combines an occlusion-sensitive data term with appropriate shape, motion, and segmentation regularizers. Optimization proceeds in two stages: Starting from an initial superpixelization, we estimate the shape and motion parameters of all segments by assigning a proposal from a set of moving planes. Then the pixel-to-segment assignment is updated, while holding the shape and motion parameters of the moving planes fixed. We demonstrate the benefits of our model on different real-world image sets, including the challenging KITTI benchmark. We achieve leading performance levels, exceeding competing 3D scene flow methods, and even yielding better 2D motion estimates than all tested dedicated optical flow techniques.</p><p>4 0.89525682 <a title="9-lda-4" href="./iccv-2013-Random_Forests_of_Local_Experts_for_Pedestrian_Detection.html">336 iccv-2013-Random Forests of Local Experts for Pedestrian Detection</a></p>
<p>Author: Javier Marín, David Vázquez, Antonio M. López, Jaume Amores, Bastian Leibe</p><p>Abstract: Pedestrian detection is one of the most challenging tasks in computer vision, and has received a lot of attention in the last years. Recently, some authors have shown the advantages of using combinations of part/patch-based detectors in order to cope with the large variability of poses and the existence of partial occlusions. In this paper, we propose a pedestrian detection method that efficiently combines multiple local experts by means of a Random Forest ensemble. The proposed method works with rich block-based representations such as HOG and LBP, in such a way that the same features are reused by the multiple local experts, so that no extra computational cost is needed with respect to a holistic method. Furthermore, we demonstrate how to integrate the proposed approach with a cascaded architecture in order to achieve not only high accuracy but also an acceptable efficiency. In particular, the resulting detector operates at five frames per second using a laptop machine. We tested the proposed method with well-known challenging datasets such as Caltech, ETH, Daimler, and INRIA. The method proposed in this work consistently ranks among the top performers in all the datasets, being either the best method or having a small difference with the best one.</p><p>5 0.89522409 <a title="9-lda-5" href="./iccv-2013-Shape_Anchors_for_Data-Driven_Multi-view_Reconstruction.html">387 iccv-2013-Shape Anchors for Data-Driven Multi-view Reconstruction</a></p>
<p>Author: Andrew Owens, Jianxiong Xiao, Antonio Torralba, William Freeman</p><p>Abstract: We present a data-driven method for building dense 3D reconstructions using a combination of recognition and multi-view cues. Our approach is based on the idea that there are image patches that are so distinctive that we can accurately estimate their latent 3D shapes solely using recognition. We call these patches shape anchors, and we use them as the basis of a multi-view reconstruction system that transfers dense, complex geometry between scenes. We “anchor” our 3D interpretation from these patches, using them to predict geometry for parts of the scene that are relatively ambiguous. The resulting algorithm produces dense reconstructions from stereo point clouds that are sparse and noisy, and we demonstrate it on a challenging dataset of real-world, indoor scenes.</p><p>6 0.89370966 <a title="9-lda-6" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>7 0.89339387 <a title="9-lda-7" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>8 0.89300579 <a title="9-lda-8" href="./iccv-2013-Estimating_the_3D_Layout_of_Indoor_Scenes_and_Its_Clutter_from_Depth_Sensors.html">144 iccv-2013-Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors</a></p>
<p>9 0.89286625 <a title="9-lda-9" href="./iccv-2013-Sequential_Bayesian_Model_Update_under_Structured_Scene_Prior_for_Semantic_Road_Scenes_Labeling.html">386 iccv-2013-Sequential Bayesian Model Update under Structured Scene Prior for Semantic Road Scenes Labeling</a></p>
<p>10 0.89259666 <a title="9-lda-10" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>11 0.89247638 <a title="9-lda-11" href="./iccv-2013-Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization.html">284 iccv-2013-Multiview Photometric Stereo Using Planar Mesh Parameterization</a></p>
<p>12 0.89220268 <a title="9-lda-12" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>13 0.89214188 <a title="9-lda-13" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>14 0.89106363 <a title="9-lda-14" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<p>15 0.89093512 <a title="9-lda-15" href="./iccv-2013-Handling_Occlusions_with_Franken-Classifiers.html">190 iccv-2013-Handling Occlusions with Franken-Classifiers</a></p>
<p>16 0.8903203 <a title="9-lda-16" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>17 0.89029276 <a title="9-lda-17" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>18 0.88990188 <a title="9-lda-18" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>19 0.8898083 <a title="9-lda-19" href="./iccv-2013-Volumetric_Semantic_Segmentation_Using_Pyramid_Context_Features.html">447 iccv-2013-Volumetric Semantic Segmentation Using Pyramid Context Features</a></p>
<p>20 0.88959014 <a title="9-lda-20" href="./iccv-2013-Depth_from_Combining_Defocus_and_Correspondence_Using_Light-Field_Cameras.html">108 iccv-2013-Depth from Combining Defocus and Correspondence Using Light-Field Cameras</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
