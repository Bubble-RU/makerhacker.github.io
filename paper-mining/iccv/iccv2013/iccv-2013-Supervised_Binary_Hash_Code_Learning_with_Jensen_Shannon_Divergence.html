<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-409" href="#">iccv2013-409</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</h1>
<br/><p>Source: <a title="iccv-2013-409-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Fan_Supervised_Binary_Hash_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Lixin Fan</p><p>Abstract: This paper proposes to learn binary hash codes within a statistical learning framework, in which an upper bound of the probability of Bayes decision errors is derived for different forms of hash functions and a rigorous proof of the convergence of the upper bound is presented. Consequently, minimizing such an upper bound leads to consistent performance improvements of existing hash code learning algorithms, regardless of whether original algorithms are unsupervised or supervised. This paper also illustrates a fast hash coding method that exploits simple binary tests to achieve orders of magnitude improvement in coding speed as compared to projection based methods.</p><p>Reference: <a title="iccv-2013-409-reference" href="../iccv2013_reference/iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 org  Abstract This paper proposes to learn binary hash codes within a statistical learning framework, in which an upper bound of the probability of Bayes decision errors is derived for different forms of hash functions and a rigorous proof of the convergence of the upper bound is presented. [sent-2, score-1.981]
</p><p>2 Consequently, minimizing such an upper bound leads to consistent performance improvements of existing hash code learning algorithms, regardless of whether original algorithms are unsupervised or supervised. [sent-3, score-0.89]
</p><p>3 This paper also illustrates a fast hash coding method that exploits simple binary tests to achieve orders of magnitude improvement in coding speed as compared to projection based methods. [sent-4, score-0.975]
</p><p>4 Introduction Mapping high dimensional image data into binary codes plays an indispensable role for efficient storing and searching of large scale image databases. [sent-6, score-0.19]
</p><p>5 Owing to storage efficiency and sublinear search time, compact binary codes have been widely applied to various vision applications such as object recognition [20], image retrieval [23], lo-  cal descriptor matching [6, 19] and binary feature matching [2, 9, 17, 21] etc. [sent-7, score-0.325]
</p><p>6 Learning compact binary code has been traditionally treated as a “similarity-preserving” problem with objective to map similar data points into similar binary codes. [sent-8, score-0.311]
</p><p>7 Often original data points are assumed to reside in a highdimensional Euclidean space while (dis-)similarity between binary codes is quantified by Hamming distance. [sent-9, score-0.19]
</p><p>8 Recent work demonstrate considerable performance improvements with a discernible shift of research focus to (semi-)supervised approaches, in which the learning of hash functions is invariably governed by some forms of label information. [sent-14, score-0.773]
</p><p>9 Despite empirical justifications with large databases, the choice of objective functions for these methods is to some extent heuristic and it remains an open question to exploit label information in a consistent and theoretically sound manner. [sent-16, score-0.121]
</p><p>10 The research presented in this paper, therefore, proposes  to formulate binary hash code learning within a statistical learning framework, in which an upper bound of the probability of Bayes decision errors is derived for different forms of hash functions (Section 3). [sent-17, score-1.768]
</p><p>11 Furthermore, a rigours proof of the convergence of the upper bound for arbitrary sequential learning algorithms is presented in Theorem 1. [sent-18, score-0.331]
</p><p>12 Consequently, minimizing the upper bound for existing hash code learning algorithms leads to consistent performance improvements, regardless of whether original algorithms are unsupervised or supervised (Section 4). [sent-19, score-0.923]
</p><p>13 The processing time of converting a query data point into binary codes (i. [sent-20, score-0.224]
</p><p>14 coding time) is a crucial performance parameter for nearest neighbour search algorithms. [sent-22, score-0.126]
</p><p>15 Having short coding time is of particular interest for vision applications such as fast keypoint recognition [16] and image localization [18] etc, in which query images may contain up to 10K features and the coding time amounts to a significant portion of the total processing time. [sent-23, score-0.246]
</p><p>16 While the coding time is considered a constant for projection based hash functions, we demonstrate in Section 4. [sent-24, score-0.707]
</p><p>17 4 that it actually can be reduced by order of magnitudes by exploiting simple yet discriminative binary tests ofinput feature vectors. [sent-25, score-0.191]
</p><p>18 The proposed Haar-Like hash function, albeit suboptimal in terms of precision rate, is preferred due to its high speed and computational simplicity for real-time applications running on mobile devices. [sent-26, score-0.728]
</p><p>19 Related Work  We review below only supervised hash code learning methods and refer readers to two journal articles [19, 4] for thorough reviews of this active research area. [sent-28, score-0.783]
</p><p>20 2616  Supervised hash learning methods: Kullis and Darrell proposed to minimize reconstruction errors between both similar and dissimilar data points [7]. [sent-29, score-0.668]
</p><p>21 [12] used a supervised term to minimize empirical errors between data point pairs associated with three types of label i. [sent-32, score-0.095]
</p><p>22 Motivated by the hinger loss in SVMs, Norouzi and Fleet proposed to minimize the upper bound on empirical loss of similar (or dissimilar) point pairs [13], and extended the method to incorporate ranking loss defined on triplets of binary codes [14]. [sent-35, score-0.481]
</p><p>23 Similar loss function was used in [10] to encode proximity comparison information of labelled data. [sent-36, score-0.079]
</p><p>24 Binary tests: There are abundant literatures related to binary test descriptors. [sent-39, score-0.103]
</p><p>25 Owing to its fast calculation speed, binary test has long been adopted to extract Haar-like features for real-time object detection [22]. [sent-40, score-0.103]
</p><p>26 BRIEF descriptor completely abandoned the training phase, but had to resort to long code lengths ranging from 128 to 256 bits [2]. [sent-43, score-0.166]
</p><p>27 More recent ORB descriptor decorrelated BRIEF features by using a learning step to search for binary tests with means near 0. [sent-44, score-0.212]
</p><p>28 D-Brief descriptors [21] was trained to reduce code lengths while maintaining high discriminative power. [sent-46, score-0.115]
</p><p>29 Theoretic Analysis This section lays down theoretic foundation for a sequential binary hash code learning framework and presents a rigorous proof of the convergence property of the framework. [sent-50, score-1.073]
</p><p>30 A Statistical Learning Framework for Binary Code Learning We treat learning optimal binary codes as a typical multiclass classification problem, in which a p-dimensional observation x = (x1, x2 , . [sent-53, score-0.239]
</p><p>31 A hash function h : Rp → {0, 1} is often treated as a mapping of an observation x →to { a single bo fitt binary ceodd aes. [sent-71, score-0.694]
</p><p>32 Depending on the outcome of the hash function, the entire sample space S = Rp is partitioned into two complementary B-subsets that are defined as follows: [b]hS = {x ∈ S | h(x) = b }, b = 0 or 1. [sent-72, score-0.591]
</p><p>33 Thi∪s d[1e]finition of B-s∩ub [s1e]ts i=s generic sapnedcaciaclolym [m∅]o∅dates to different families of hash functions such as linear transform, kernelized or more complex hash functions used e. [sent-74, score-1.374]
</p><p>34 , hK} partitions the space S into 2K non-overlapping B-sub}se ptsa1r , which are intersections of B-subsets of each hash function:  [b1b2. [sent-80, score-0.591]
</p><p>35 When ambiguity seems unlikely, hK and S are omitted and binary codes are denoted as b1. [sent-94, score-0.19]
</p><p>36 bK] is observed, the posterior probability of class Cm is given by Bayes Theorem: πm ? [sent-101, score-0.059]
</p><p>37 K  The probability of Bayes decision error by choosing the class MAP estimate C m˜ that has the largest posterior probability is: π m˜ ? [sent-114, score-0.135]
</p><p>38 =1 K  and the total probability of error for a given set of hash functions hK is: P(e | hK)  =  ? [sent-125, score-0.721]
</p><p>39 Our goal is to seek a set of hash functions that minimizes the total probability of Bayes decision errors:  h∗K = argmhiKnP(e | hK). [sent-133, score-0.763]
</p><p>40 (2)  1Depending on different hash functions, certain binary codes may correspond to empty B-subsets. [sent-134, score-0.781]
</p><p>41 2617  Minimizing the objective function (2) is akin to minimizing  empirical training error in recent supervised hashing learning methods [24, 23, 12]. [sent-136, score-0.213]
</p><p>42 ) function in the class MAP estimate makes it difficult to analyze the convergence property of the minimization of (2). [sent-138, score-0.058]
</p><p>43 In the rest of the paper, we first present a rigorous proof of the convergence of an upper bound on P(e) and then illustrate how to use the upper bound to supervise a variety of hash code learning algorithms. [sent-139, score-1.143]
</p><p>44 Since H(π) is a constant for a given problem, it immediately follows that the upper bound of the probability of error is minimized by maximizing (4):  h∗K = argmhaKxJSDπ(p1,. [sent-193, score-0.174]
</p><p>45 (5)  Remark 1: The JSD measure as an objective function can be intuitively interpreted as the combination of an unsupervised and a supervised terms: maximizing the first  term H? [sent-197, score-0.07]
</p><p>46 ,N, K, L begin  hK = ∅; πi = NNi; Ii∅ = 1Ni×1; where Ni := #data points in class i; hl = sgn(xwl) and H = sgn(xW) , where W = [w1, . [sent-214, score-0.093]
</p><p>47 These two opposing terms conspire to minimize the upper bound of the probability of MAP decision errors. [sent-226, score-0.216]
</p><p>48 Remark 2: Owing to the convexity of KL divergence, Theorem 1 below proves that the upper bound of the Bayesian decision error is monotonically decreasing when a sequential learning approach repeatedly adds more hash functions except for some pathological cases. [sent-227, score-0.989]
</p><p>49 =  Remark 3: the condition above is satisfied even for random hash functions and thus Theorem 1 also proves the convergence property of the well-known Locality Sensitive Hashing (LSH) method [5]. [sent-314, score-0.72]
</p><p>50 in the following section, to select the hash function which maximizes JSD at each step is a preferred option to random selection of hash functions. [sent-321, score-1.214]
</p><p>51 Experiments Section 3 lays down theoretic foundation for a sequential binary hash code learning framework. [sent-323, score-0.965]
</p><p>52 In this section, we demonstrate how to adopt this generic framework to train existing learning algorithms with labelled datasets. [sent-324, score-0.086]
</p><p>53 Four publicly available datasets, namely CIFAR10, CIFAR100, LabelMe22k and SIFT10K, are used to compare the proposed hash code learning approach against existing methods. [sent-336, score-0.713]
</p><p>54 100 nearest neighbours are provided for each class and there is one query SIFT descriptor per class that is used for testing in our experiments. [sent-350, score-0.084]
</p><p>55 Two examples precision curves measured at Hamming radius 0 and 1for hash codes lengths ranging from 8 to 64 bits are illustrated in Figure 1 (left column). [sent-353, score-0.851]
</p><p>56 Mean average precision (mAP) are often used to quantify performances of different methods. [sent-354, score-0.117]
</p><p>57 In this work, we measure mAP only for Hamming radius no greater than 3 since high precision rates prevent unnecessarily long lists of retrieved items. [sent-355, score-0.127]
</p><p>58 Sequential Learning with JSD We first demonstrate how to supervise the well-known LSH with a sequential learning algorithm 1. [sent-358, score-0.162]
</p><p>59 A set of L candidate linear projections wl ∈ Rp×1, l= 1, . [sent-359, score-0.121]
</p><p>60 L are randomly generated and applied to∈ tRhe whole dataset hl = sgn(xwl)2. [sent-362, score-0.068]
</p><p>61 Outcomes of candidate projections are concatenated into a binary matrix H ∈ {0, 1}N×L. [sent-363, score-0.193]
</p><p>62 Ibi1  Binary vector K ∈ {0, 1}Ni 1 indicates those points that are associated with∈ cl {a0s,s1 1i} and binary code b1. [sent-370, score-0.176]
</p><p>63 This  pbi1 K∩[1]hl  way can be efficiently computed by counting “1” bits in the intersection of K and corresponding column vectors hl, and thereafter normalizing the count with  Ibi1  pib1. [sent-374, score-0.081]
</p><p>64 The whole JSD-based learning process is implemented as a binary tree growing algorithm in MATLAB codes. [sent-379, score-0.152]
</p><p>65 This supervised approach leads to significant performance improvement as compared to random projection adopted in LSH. [sent-383, score-0.106]
</p><p>66 Figure 1 illustrates improvements in precision rates measured on CIFAR10 dataset with both Euclidean neighbour and semantic labels. [sent-384, score-0.234]
</p><p>67 Figure 2 (left) illustrates how the precision of JSD algorithm is positively related to the number of candidate hash functions L. [sent-386, score-0.804]
</p><p>68 Whereas a small number of 200 candidates lead to significant improvement over random projections in LSH, 50 times more candidates improves about 10% precision only. [sent-387, score-0.133]
</p><p>69 Figure 3 shows that there is a graceful precision loss due to noisy or partial labels. [sent-389, score-0.122]
</p><p>70 With 50% random labels, the precision of JSD learning decreases about 20% yet still outperforming LSH. [sent-390, score-0.129]
</p><p>71 Training with 1% labels, the incurred precision loss is no more than 5%. [sent-391, score-0.151]
</p><p>72 JSD improvements of other methods Algorithm 1 can also be used to improve other binary code learning methods with a minor modification, i. [sent-394, score-0.262]
</p><p>73 by appending outputs of existing methods He ∈ {0, 1}N×K to candidate projection outcomes H ← H ∪ H∈e. [sent-396, score-0.103]
</p><p>74 { 0T,h1is} generic approach o pfr exploiting ltacobmel eisn fHor ←ma tHion ∪ can be applied to any hash coding learning algorithms in a consistent manner. [sent-397, score-0.72]
</p><p>75 Figures 4 illustrates performance improvements of one unsupervised (SH [26]) and three supervised methods (BRE [7], ITQ [3] and KSH [12]) measured on CIFAR10 dataset with Euclidean labels. [sent-399, score-0.107]
</p><p>76 Quantitative performance comparison, in terms of mean average precision (mAP), are summarized in Figure 5 in which six sets of data/labels are used. [sent-401, score-0.08]
</p><p>77 It is observed that performances of JSD-based learning approaches compare favourably with those of original methods in the majority of comparisons. [sent-402, score-0.086]
</p><p>78 In the bottom two panels, however, performances in KSH/JSD column no longer excel but JSD learning methods still outperform the original methods in the majority of cases (with minor exceptions marked by “‡”). [sent-408, score-0.116]
</p><p>79 stance, Table 1 summarizes numbers of hash functions that are selected from different methods in one example run. [sent-416, score-0.687]
</p><p>80 In particular, the last column shows percentage of hash functions that are selected from corresponding learning methods, averaged over different lengths of hash codes. [sent-417, score-1.399]
</p><p>81 Noticeably, KSH contributes the majority of selected hash functions and this seems in accordance with high precision rates of KSH reported in Figure 4 as well as Table 4. [sent-418, score-0.814]
</p><p>82 First, the preparation time (Tp) of candidate matrix H is  approximately proportional to the number of candidates L (also see Figure 2 right). [sent-422, score-0.085]
</p><p>83 Instead, the processing time of converting a query data point into binary codes (i. [sent-444, score-0.224]
</p><p>84 coding time) is a crucial performance parameter for nearest neighbour search algorithms. [sent-446, score-0.126]
</p><p>85 While the coding time is often considered a constant for projection based hash functions, it actually can be reduced by order of magnitudes by exploiting simple yet discriminative binary tests of input feature vectors. [sent-447, score-0.898]
</p><p>86 This family ∈o fR hash functions constitute a special subset of the well-known linear projections that have only two non-zero elements (1 and −1 respectively) in each column 2621  Figure 6. [sent-454, score-0.77]
</p><p>87 Three columns on left: Precision-recall curves with varying code lengths. [sent-456, score-0.073]
</p><p>88 Random selection of HALFs (rHALF) does not necessarily guarantee satisfactory precision rates, instead, we adopt the proposed JSD learning algorithm to boost precision rates of random HALFs (denoted as rHALF-JSD hereafter). [sent-465, score-0.256]
</p><p>89 As compared with other methods, Figure 6 shows that the  curve of rHALF-JSD (red solid line) lies almost exactly between unsupervised (LSH, rHALF and SH) and supervised methods (BRE, ITQ and KSH). [sent-467, score-0.07]
</p><p>90 The practical value of rHALF-JSD lies in its extremely short coding time Tc (see Table 3). [sent-470, score-0.08]
</p><p>91 Since there is no matrix multiplication involved, the per query coding time is at least two orders of magnitude faster than other projection based methods for 5 12-dimensional GIST vectors. [sent-471, score-0.175]
</p><p>92 This speedup is particularly useful for real-time applications such as fast keypoint recognition [16] and image localization [18] in which query images may contain up to 10K features and the coding time amounts to a significant portion of the total processing time. [sent-472, score-0.166]
</p><p>93 First, we formulated binary hash coding learning within a statistical learning framework in which an upper bound of the proba-  Table3. [sent-476, score-1.012]
</p><p>94 bility of Bayes decision errors is derived for arbitrary hash functions. [sent-485, score-0.633]
</p><p>95 Since the convergence property of such a sequential learning method is rigorously proved, one is able to freely apply the JSD-based learning approach to different hash functions as long as sufficient number of labelled data are available. [sent-486, score-0.926]
</p><p>96 Second, we proposed a very simple and fast hash function which is able to reduce the coding time by two order of magnitudes as compared with other projection based methods. [sent-488, score-0.735]
</p><p>97 As for future work, we advocate the JSD-based learning approach as a generic framework to combine different learning algorithms. [sent-490, score-0.098]
</p><p>98 Iterative quantization: A procrustean approach to learning binary codes. [sent-743, score-0.177]
</p><p>99 Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval. [sent-750, score-0.264]
</p><p>100 Small codes and large image  [21] [22] [23] [24] [25] [26]  databases for recognition. [sent-862, score-0.087]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hash', 0.591), ('jsd', 0.509), ('hk', 0.22), ('hs', 0.143), ('ksh', 0.132), ('halfs', 0.106), ('binary', 0.103), ('functions', 0.096), ('codes', 0.087), ('rhalf', 0.085), ('precision', 0.08), ('coding', 0.08), ('code', 0.073), ('lsh', 0.072), ('sequential', 0.071), ('supervised', 0.07), ('bound', 0.07), ('upper', 0.07), ('hashing', 0.069), ('hl', 0.068), ('dhk', 0.064), ('hamming', 0.064), ('euclidean', 0.063), ('bayes', 0.061), ('tests', 0.06), ('bre', 0.06), ('vs', 0.059), ('divergence', 0.054), ('pi', 0.054), ('projections', 0.053), ('keypoint', 0.052), ('theorem', 0.051), ('bits', 0.051), ('rp', 0.051), ('learning', 0.049), ('itq', 0.049), ('jensen', 0.049), ('shannon', 0.049), ('preparation', 0.048), ('rates', 0.047), ('neighbour', 0.046), ('ts', 0.044), ('lengths', 0.042), ('supervise', 0.042), ('xwl', 0.042), ('decision', 0.042), ('norouzi', 0.042), ('loss', 0.042), ('sh', 0.04), ('theoretic', 0.04), ('owing', 0.04), ('dx', 0.038), ('pm', 0.038), ('sgn', 0.038), ('panels', 0.038), ('lays', 0.038), ('neighbourhoods', 0.038), ('proof', 0.038), ('improvements', 0.037), ('rigorous', 0.037), ('performances', 0.037), ('labelled', 0.037), ('candidate', 0.037), ('projection', 0.036), ('probability', 0.034), ('query', 0.034), ('remark', 0.034), ('convergence', 0.033), ('orb', 0.033), ('nni', 0.033), ('ipi', 0.033), ('font', 0.033), ('preferred', 0.032), ('compact', 0.032), ('wl', 0.031), ('lepetit', 0.031), ('calonder', 0.031), ('column', 0.03), ('outcomes', 0.03), ('incurred', 0.029), ('dissimilar', 0.028), ('bronstein', 0.028), ('magnitudes', 0.028), ('bit', 0.028), ('tp', 0.028), ('ln', 0.027), ('pb', 0.026), ('strecha', 0.026), ('brevity', 0.026), ('labels', 0.026), ('mobile', 0.025), ('empirical', 0.025), ('class', 0.025), ('procrustean', 0.025), ('orders', 0.025), ('priori', 0.025), ('ni', 0.025), ('ih', 0.025), ('fleet', 0.024), ('semantic', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="409-tfidf-1" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>Author: Lixin Fan</p><p>Abstract: This paper proposes to learn binary hash codes within a statistical learning framework, in which an upper bound of the probability of Bayes decision errors is derived for different forms of hash functions and a rigorous proof of the convergence of the upper bound is presented. Consequently, minimizing such an upper bound leads to consistent performance improvements of existing hash code learning algorithms, regardless of whether original algorithms are unsupervised or supervised. This paper also illustrates a fast hash coding method that exploits simple binary tests to achieve orders of magnitude improvement in coding speed as compared to projection based methods.</p><p>2 0.52912039 <a title="409-tfidf-2" href="./iccv-2013-A_General_Two-Step_Approach_to_Learning-Based_Hashing.html">13 iccv-2013-A General Two-Step Approach to Learning-Based Hashing</a></p>
<p>Author: Guosheng Lin, Chunhua Shen, David Suter, Anton van_den_Hengel</p><p>Abstract: Most existing approaches to hashing apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to respond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. This framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problemspecific hashing methods. Our framework decomposes the hashing learning problem into two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically be formulated as binary quadratic problems, and the second step can be accomplished by training standard binary classifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate that the proposed framework is effective, flexible and outperforms the state-of-the-art.</p><p>3 0.42523131 <a title="409-tfidf-3" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>Author: Guangnan Ye, Dong Liu, Jun Wang, Shih-Fu Chang</p><p>Abstract: Recently, learning based hashing methods have become popular for indexing large-scale media data. Hashing methods map high-dimensional features to compact binary codes that are efficient to match and robust in preserving original similarity. However, most of the existing hashing methods treat videos as a simple aggregation of independent frames and index each video through combining the indexes of frames. The structure information of videos, e.g., discriminative local visual commonality and temporal consistency, is often neglected in the design of hash functions. In this paper, we propose a supervised method that explores the structure learning techniques to design efficient hash functions. The proposed video hashing method formulates a minimization problem over a structure-regularized empirical loss. In particular, the structure regularization exploits the common local visual patterns occurring in video frames that are associated with the same semantic class, and simultaneously preserves the temporal consistency over successive frames from the same video. We show that the minimization objective can be efficiently solved by an Acceler- ated Proximal Gradient (APG) method. Extensive experiments on two large video benchmark datasets (up to around 150K video clips with over 12 million frames) show that the proposed method significantly outperforms the state-ofthe-art hashing methods.</p><p>4 0.42067966 <a title="409-tfidf-4" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>Author: Jun Wang, Wei Liu, Andy X. Sun, Yu-Gang Jiang</p><p>Abstract: Hashing techniques have been intensively investigated in the design of highly efficient search engines for largescale computer vision applications. Compared with prior approximate nearest neighbor search approaches like treebased indexing, hashing-based search schemes have prominent advantages in terms of both storage and computational efficiencies. Moreover, the procedure of devising hash functions can be easily incorporated into sophisticated machine learning tools, leading to data-dependent and task-specific compact hash codes. Therefore, a number of learning paradigms, ranging from unsupervised to supervised, have been applied to compose appropriate hash functions. How- ever, most of the existing hash function learning methods either treat hash function design as a classification problem or generate binary codes to satisfy pairwise supervision, and have not yet directly optimized the search accuracy. In this paper, we propose to leverage listwise supervision into a principled hash function learning framework. In particular, the ranking information is represented by a set of rank triplets that can be used to assess the quality of ranking. Simple linear projection-based hash functions are solved efficiently through maximizing the ranking quality over the training data. We carry out experiments on large image datasets with size up to one million and compare with the state-of-the-art hashing techniques. The extensive results corroborate that our learned hash codes via listwise supervision can provide superior search accuracy without incurring heavy computational overhead.</p><p>5 0.33413237 <a title="409-tfidf-5" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>Author: Zhongming Jin, Yao Hu, Yue Lin, Debing Zhang, Shiding Lin, Deng Cai, Xuelong Li</p><p>Abstract: Recently, hashing techniques have been widely applied to solve the approximate nearest neighbors search problem in many vision applications. Generally, these hashing approaches generate 2c buckets, where c is the length of the hash code. A good hashing method should satisfy the following two requirements: 1) mapping the nearby data points into the same bucket or nearby (measured by xue long l i opt . ac . cn @ a(a)b(b) the Hamming distance) buckets. 2) all the data points are evenly distributed among all the buckets. In this paper, we propose a novel algorithm named Complementary Projection Hashing (CPH) to find the optimal hashing functions which explicitly considers the above two requirements. Specifically, CPHaims at sequentiallyfinding a series ofhyperplanes (hashing functions) which cross the sparse region of the data. At the same time, the data points are evenly distributed in the hypercubes generated by these hyperplanes. The experiments comparing with the state-of-the-art hashing methods demonstrate the effectiveness of the proposed method.</p><p>6 0.15670878 <a title="409-tfidf-6" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>7 0.12895291 <a title="409-tfidf-7" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>8 0.079362988 <a title="409-tfidf-8" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>9 0.078241237 <a title="409-tfidf-9" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>10 0.072789542 <a title="409-tfidf-10" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>11 0.066234328 <a title="409-tfidf-11" href="./iccv-2013-Fast_Neighborhood_Graph_Search_Using_Cartesian_Concatenation.html">159 iccv-2013-Fast Neighborhood Graph Search Using Cartesian Concatenation</a></p>
<p>12 0.062824808 <a title="409-tfidf-12" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>13 0.062204313 <a title="409-tfidf-13" href="./iccv-2013-Low-Rank_Sparse_Coding_for_Image_Classification.html">258 iccv-2013-Low-Rank Sparse Coding for Image Classification</a></p>
<p>14 0.060878634 <a title="409-tfidf-14" href="./iccv-2013-Joint_Inverted_Indexing.html">221 iccv-2013-Joint Inverted Indexing</a></p>
<p>15 0.058269896 <a title="409-tfidf-15" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>16 0.054136414 <a title="409-tfidf-16" href="./iccv-2013-Find_the_Best_Path%3A_An_Efficient_and_Accurate_Classifier_for_Image_Hierarchies.html">165 iccv-2013-Find the Best Path: An Efficient and Accurate Classifier for Image Hierarchies</a></p>
<p>17 0.051874094 <a title="409-tfidf-17" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>18 0.051753752 <a title="409-tfidf-18" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>19 0.050701525 <a title="409-tfidf-19" href="./iccv-2013-Elastic_Net_Constraints_for_Shape_Matching.html">140 iccv-2013-Elastic Net Constraints for Shape Matching</a></p>
<p>20 0.05009589 <a title="409-tfidf-20" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.147), (1, 0.07), (2, -0.074), (3, -0.102), (4, -0.055), (5, 0.337), (6, 0.003), (7, -0.002), (8, -0.27), (9, 0.147), (10, -0.234), (11, 0.135), (12, -0.02), (13, -0.169), (14, 0.01), (15, 0.049), (16, -0.139), (17, 0.173), (18, -0.151), (19, 0.075), (20, 0.001), (21, 0.042), (22, 0.033), (23, 0.01), (24, 0.036), (25, -0.036), (26, -0.024), (27, 0.002), (28, -0.002), (29, -0.021), (30, 0.023), (31, 0.003), (32, -0.002), (33, 0.01), (34, -0.033), (35, -0.004), (36, -0.014), (37, 0.008), (38, -0.0), (39, -0.002), (40, -0.021), (41, 0.028), (42, -0.01), (43, -0.01), (44, -0.024), (45, 0.007), (46, -0.014), (47, -0.001), (48, -0.01), (49, -0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96520251 <a title="409-lsi-1" href="./iccv-2013-A_General_Two-Step_Approach_to_Learning-Based_Hashing.html">13 iccv-2013-A General Two-Step Approach to Learning-Based Hashing</a></p>
<p>Author: Guosheng Lin, Chunhua Shen, David Suter, Anton van_den_Hengel</p><p>Abstract: Most existing approaches to hashing apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to respond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. This framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problemspecific hashing methods. Our framework decomposes the hashing learning problem into two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically be formulated as binary quadratic problems, and the second step can be accomplished by training standard binary classifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate that the proposed framework is effective, flexible and outperforms the state-of-the-art.</p><p>2 0.96026313 <a title="409-lsi-2" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>Author: Zhongming Jin, Yao Hu, Yue Lin, Debing Zhang, Shiding Lin, Deng Cai, Xuelong Li</p><p>Abstract: Recently, hashing techniques have been widely applied to solve the approximate nearest neighbors search problem in many vision applications. Generally, these hashing approaches generate 2c buckets, where c is the length of the hash code. A good hashing method should satisfy the following two requirements: 1) mapping the nearby data points into the same bucket or nearby (measured by xue long l i opt . ac . cn @ a(a)b(b) the Hamming distance) buckets. 2) all the data points are evenly distributed among all the buckets. In this paper, we propose a novel algorithm named Complementary Projection Hashing (CPH) to find the optimal hashing functions which explicitly considers the above two requirements. Specifically, CPHaims at sequentiallyfinding a series ofhyperplanes (hashing functions) which cross the sparse region of the data. At the same time, the data points are evenly distributed in the hypercubes generated by these hyperplanes. The experiments comparing with the state-of-the-art hashing methods demonstrate the effectiveness of the proposed method.</p><p>3 0.94881845 <a title="409-lsi-3" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>Author: Jun Wang, Wei Liu, Andy X. Sun, Yu-Gang Jiang</p><p>Abstract: Hashing techniques have been intensively investigated in the design of highly efficient search engines for largescale computer vision applications. Compared with prior approximate nearest neighbor search approaches like treebased indexing, hashing-based search schemes have prominent advantages in terms of both storage and computational efficiencies. Moreover, the procedure of devising hash functions can be easily incorporated into sophisticated machine learning tools, leading to data-dependent and task-specific compact hash codes. Therefore, a number of learning paradigms, ranging from unsupervised to supervised, have been applied to compose appropriate hash functions. How- ever, most of the existing hash function learning methods either treat hash function design as a classification problem or generate binary codes to satisfy pairwise supervision, and have not yet directly optimized the search accuracy. In this paper, we propose to leverage listwise supervision into a principled hash function learning framework. In particular, the ranking information is represented by a set of rank triplets that can be used to assess the quality of ranking. Simple linear projection-based hash functions are solved efficiently through maximizing the ranking quality over the training data. We carry out experiments on large image datasets with size up to one million and compare with the state-of-the-art hashing techniques. The extensive results corroborate that our learned hash codes via listwise supervision can provide superior search accuracy without incurring heavy computational overhead.</p><p>same-paper 4 0.92852974 <a title="409-lsi-4" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>Author: Lixin Fan</p><p>Abstract: This paper proposes to learn binary hash codes within a statistical learning framework, in which an upper bound of the probability of Bayes decision errors is derived for different forms of hash functions and a rigorous proof of the convergence of the upper bound is presented. Consequently, minimizing such an upper bound leads to consistent performance improvements of existing hash code learning algorithms, regardless of whether original algorithms are unsupervised or supervised. This paper also illustrates a fast hash coding method that exploits simple binary tests to achieve orders of magnitude improvement in coding speed as compared to projection based methods.</p><p>5 0.88090014 <a title="409-lsi-5" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>Author: Guangnan Ye, Dong Liu, Jun Wang, Shih-Fu Chang</p><p>Abstract: Recently, learning based hashing methods have become popular for indexing large-scale media data. Hashing methods map high-dimensional features to compact binary codes that are efficient to match and robust in preserving original similarity. However, most of the existing hashing methods treat videos as a simple aggregation of independent frames and index each video through combining the indexes of frames. The structure information of videos, e.g., discriminative local visual commonality and temporal consistency, is often neglected in the design of hash functions. In this paper, we propose a supervised method that explores the structure learning techniques to design efficient hash functions. The proposed video hashing method formulates a minimization problem over a structure-regularized empirical loss. In particular, the structure regularization exploits the common local visual patterns occurring in video frames that are associated with the same semantic class, and simultaneously preserves the temporal consistency over successive frames from the same video. We show that the minimization objective can be efficiently solved by an Acceler- ated Proximal Gradient (APG) method. Extensive experiments on two large video benchmark datasets (up to around 150K video clips with over 12 million frames) show that the proposed method significantly outperforms the state-ofthe-art hashing methods.</p><p>6 0.47452861 <a title="409-lsi-6" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>7 0.44213176 <a title="409-lsi-7" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>8 0.40514085 <a title="409-lsi-8" href="./iccv-2013-Joint_Inverted_Indexing.html">221 iccv-2013-Joint Inverted Indexing</a></p>
<p>9 0.33256784 <a title="409-lsi-9" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>10 0.3012352 <a title="409-lsi-10" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>11 0.30018404 <a title="409-lsi-11" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<p>12 0.29128507 <a title="409-lsi-12" href="./iccv-2013-Fast_Neighborhood_Graph_Search_Using_Cartesian_Concatenation.html">159 iccv-2013-Fast Neighborhood Graph Search Using Cartesian Concatenation</a></p>
<p>13 0.27257672 <a title="409-lsi-13" href="./iccv-2013-Fix_Structured_Learning_of_2013_ICCV_paper_k2opt.pdf.html">171 iccv-2013-Fix Structured Learning of 2013 ICCV paper k2opt.pdf</a></p>
<p>14 0.25309086 <a title="409-lsi-14" href="./iccv-2013-Quadruplet-Wise_Image_Similarity_Learning.html">332 iccv-2013-Quadruplet-Wise Image Similarity Learning</a></p>
<p>15 0.24853018 <a title="409-lsi-15" href="./iccv-2013-Learning_to_Rank_Using_Privileged_Information.html">248 iccv-2013-Learning to Rank Using Privileged Information</a></p>
<p>16 0.24110384 <a title="409-lsi-16" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<p>17 0.23938054 <a title="409-lsi-17" href="./iccv-2013-Detecting_Irregular_Curvilinear_Structures_in_Gray_Scale_and_Color_Imagery_Using_Multi-directional_Oriented_Flux.html">112 iccv-2013-Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-directional Oriented Flux</a></p>
<p>18 0.23492394 <a title="409-lsi-18" href="./iccv-2013-Abnormal_Event_Detection_at_150_FPS_in_MATLAB.html">34 iccv-2013-Abnormal Event Detection at 150 FPS in MATLAB</a></p>
<p>19 0.23220499 <a title="409-lsi-19" href="./iccv-2013-Low-Rank_Sparse_Coding_for_Image_Classification.html">258 iccv-2013-Low-Rank Sparse Coding for Image Classification</a></p>
<p>20 0.23039895 <a title="409-lsi-20" href="./iccv-2013-Dynamic_Structured_Model_Selection.html">130 iccv-2013-Dynamic Structured Model Selection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.185), (7, 0.28), (12, 0.011), (26, 0.056), (31, 0.029), (35, 0.01), (40, 0.017), (42, 0.078), (64, 0.027), (73, 0.027), (89, 0.153), (95, 0.011), (98, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83970875 <a title="409-lda-1" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>Author: Lixin Fan</p><p>Abstract: This paper proposes to learn binary hash codes within a statistical learning framework, in which an upper bound of the probability of Bayes decision errors is derived for different forms of hash functions and a rigorous proof of the convergence of the upper bound is presented. Consequently, minimizing such an upper bound leads to consistent performance improvements of existing hash code learning algorithms, regardless of whether original algorithms are unsupervised or supervised. This paper also illustrates a fast hash coding method that exploits simple binary tests to achieve orders of magnitude improvement in coding speed as compared to projection based methods.</p><p>2 0.83730757 <a title="409-lda-2" href="./iccv-2013-From_Semi-supervised_to_Transfer_Counting_of_Crowds.html">178 iccv-2013-From Semi-supervised to Transfer Counting of Crowds</a></p>
<p>Author: Chen Change Loy, Shaogang Gong, Tao Xiang</p><p>Abstract: Regression-based techniques have shown promising results for people counting in crowded scenes. However, most existing techniques require expensive and laborious data annotation for model training. In this study, we propose to address this problem from three perspectives: (1) Instead of exhaustively annotating every single frame, the most informative frames are selected for annotation automatically and actively. (2) Rather than learning from only labelled data, the abundant unlabelled data are exploited. (3) Labelled data from other scenes are employed to further alleviate the burden for data annotation. All three ideas are implemented in a unified active and semi-supervised regression framework with ability to perform transfer learning, by exploiting the underlying geometric structure of crowd patterns via manifold analysis. Extensive experiments validate the effectiveness of our approach.</p><p>3 0.83171427 <a title="409-lda-3" href="./iccv-2013-Text_Localization_in_Natural_Images_Using_Stroke_Feature_Transform_and_Text_Covariance_Descriptors.html">415 iccv-2013-Text Localization in Natural Images Using Stroke Feature Transform and Text Covariance Descriptors</a></p>
<p>Author: Weilin Huang, Zhe Lin, Jianchao Yang, Jue Wang</p><p>Abstract: In this paper, we present a new approach for text localization in natural images, by discriminating text and non-text regions at three levels: pixel, component and textline levels. Firstly, a powerful low-level filter called the Stroke Feature Transform (SFT) is proposed, which extends the widely-used Stroke Width Transform (SWT) by incorporating color cues of text pixels, leading to significantly enhanced performance on inter-component separation and intra-component connection. Secondly, based on the output of SFT, we apply two classifiers, a text component classifier and a text-line classifier, sequentially to extract text regions, eliminating the heuristic procedures that are commonly used in previous approaches. The two classifiers are built upon two novel Text Covariance Descriptors (TCDs) that encode both the heuristic properties and the statistical characteristics of text stokes. Finally, text regions are located by simply thresholding the text-line confident map. Our method was evaluated on two benchmark datasets: ICDAR 2005 and ICDAR 2011, and the corresponding F- , measure values are 0. 72 and 0. 73, respectively, surpassing previous methods in accuracy by a large margin.</p><p>4 0.82511169 <a title="409-lda-4" href="./iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning.html">212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</a></p>
<p>Author: Jiwen Lu, Gang Wang, Pierre Moulin</p><p>Abstract: This paper presents a new approach for image set classification, where each training and testing example contains a set of image instances of an object captured from varying viewpoints or under varying illuminations. While a number of image set classification methods have been proposed in recent years, most of them model each image set as a single linear subspace or mixture of linear subspaces, which may lose some discriminative information for classification. To address this, we propose exploring multiple order statistics as features of image sets, and develop a localized multikernel metric learning (LMKML) algorithm to effectively combine different order statistics information for classification. Our method achieves the state-of-the-art performance on four widely used databases including the Honda/UCSD, CMU Mobo, and Youtube face datasets, and the ETH-80 object dataset.</p><p>5 0.7791903 <a title="409-lda-5" href="./iccv-2013-Pose_Estimation_with_Unknown_Focal_Length_Using_Points%2C_Directions_and_Lines.html">323 iccv-2013-Pose Estimation with Unknown Focal Length Using Points, Directions and Lines</a></p>
<p>Author: Yubin Kuang, Kalle Åström</p><p>Abstract: In this paper, we study the geometry problems of estimating camera pose with unknown focal length using combination of geometric primitives. We consider points, lines and also rich features such as quivers, i.e. points with one or more directions. We formulate the problems as polynomial systems where the constraints for different primitives are handled in a unified way. We develop efficient polynomial solvers for each of the derived cases with different combinations of primitives. The availability of these solvers enables robust pose estimation with unknown focal length for wider classes of features. Such rich features allow for fewer feature correspondences and generate larger inlier sets with higher probability. We demonstrate in synthetic experiments that our solvers are fast and numerically stable. For real images, we show that our solvers can be used in RANSAC loops to provide good initial solutions.</p><p>6 0.77244246 <a title="409-lda-6" href="./iccv-2013-No_Matter_Where_You_Are%3A_Flexible_Graph-Guided_Multi-task_Learning_for_Multi-view_Head_Pose_Classification_under_Target_Motion.html">291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</a></p>
<p>7 0.7543875 <a title="409-lda-7" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>8 0.70904309 <a title="409-lda-8" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>9 0.70132196 <a title="409-lda-9" href="./iccv-2013-A_General_Two-Step_Approach_to_Learning-Based_Hashing.html">13 iccv-2013-A General Two-Step Approach to Learning-Based Hashing</a></p>
<p>10 0.69922411 <a title="409-lda-10" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>11 0.6977607 <a title="409-lda-11" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<p>12 0.69549644 <a title="409-lda-12" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>13 0.69258738 <a title="409-lda-13" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>14 0.69107372 <a title="409-lda-14" href="./iccv-2013-Improving_Graph_Matching_via_Density_Maximization.html">214 iccv-2013-Improving Graph Matching via Density Maximization</a></p>
<p>15 0.69068688 <a title="409-lda-15" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>16 0.68671143 <a title="409-lda-16" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>17 0.68460548 <a title="409-lda-17" href="./iccv-2013-Linear_Sequence_Discriminant_Analysis%3A_A_Model-Based_Dimensionality_Reduction_Method_for_Vector_Sequences.html">253 iccv-2013-Linear Sequence Discriminant Analysis: A Model-Based Dimensionality Reduction Method for Vector Sequences</a></p>
<p>18 0.68142968 <a title="409-lda-18" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>19 0.67832685 <a title="409-lda-19" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>20 0.67729288 <a title="409-lda-20" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
