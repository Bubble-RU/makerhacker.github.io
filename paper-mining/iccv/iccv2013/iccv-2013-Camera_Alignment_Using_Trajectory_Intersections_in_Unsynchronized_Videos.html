<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>68 iccv-2013-Camera Alignment Using Trajectory Intersections in Unsynchronized Videos</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-68" href="#">iccv2013-68</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>68 iccv-2013-Camera Alignment Using Trajectory Intersections in Unsynchronized Videos</h1>
<br/><p>Source: <a title="iccv-2013-68-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Kuo_Camera_Alignment_Using_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Thomas Kuo, Santhoshkumar Sunderrajan, B.S. Manjunath</p><p>Abstract: This paper addresses the novel and challenging problem of aligning camera views that are unsynchronized by low and/or variable frame rates using object trajectories. Unlike existing trajectory-based alignment methods, our method does not require frame-to-frame synchronization. Instead, we propose using the intersections of corresponding object trajectories to match views. To find these intersections, we introduce a novel trajectory matching algorithm based on matching Spatio-Temporal Context Graphs (STCGs). These graphs represent the distances between trajectories in time and space within a view, and are matched to an STCG from another view to find the corresponding trajectories. To the best of our knowledge, this is one of the first attempts to align views that are unsynchronized with variable frame rates. The results on simulated and real-world datasets show trajectory intersections area viablefeatureforcamera alignment, and that the trajectory matching method performs well in real-world scenarios.</p><p>Reference: <a title="iccv-2013-68-reference" href="../iccv2013_reference/iccv-2013-Camera_Alignment_Using_Trajectory_Intersections_in_Unsynchronized_Videos_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract This paper addresses the novel and challenging problem of aligning camera views that are unsynchronized by low and/or variable frame rates using object trajectories. [sent-5, score-0.816]
</p><p>2 Unlike existing trajectory-based alignment methods, our method does not require frame-to-frame synchronization. [sent-6, score-0.142]
</p><p>3 Instead, we propose using the intersections of corresponding object trajectories to match views. [sent-7, score-0.848]
</p><p>4 To find these intersections, we introduce a novel trajectory matching algorithm based on matching Spatio-Temporal Context Graphs (STCGs). [sent-8, score-0.379]
</p><p>5 These graphs represent the distances between trajectories in time and space within a view, and are matched to an STCG from another view to find the corresponding trajectories. [sent-9, score-0.66]
</p><p>6 To the best of our knowledge, this is one of the first attempts to align views that are unsynchronized with variable frame rates. [sent-10, score-0.654]
</p><p>7 The results on simulated and real-world datasets show trajectory intersections area viablefeatureforcamera alignment, and that the trajectory matching method performs well in real-world scenarios. [sent-11, score-0.889]
</p><p>8 Introduction Networks of wireless cameras are very useful in applications such as surveillance and intelligent environments [1] because they require less infrastructure. [sent-13, score-0.272]
</p><p>9 However, a wireless channel typically has less bandwidth and more disruptions such as dropped frames. [sent-14, score-0.089]
</p><p>10 These limits can be compensated for by varying both the quality of the image and the frame rate of the video according to the needed and available bandwidth. [sent-15, score-0.133]
</p><p>11 These compromises make camera calibration and alignment more difficult. [sent-16, score-0.306]
</p><p>12 Variable frame rates affect methods that match points in trajectories, since these methods typically require frame-to-frame synchronization [19, 11, 16] and/or fixed frame rates [7, 3]. [sent-18, score-0.784]
</p><p>13 In this context, we contribute one of the first methods to align unsynchronized videos that have variable frame rates. [sent-19, score-0.598]
</p><p>14 We propose the novel use of the intersections of ground plane trajectories to find the homography between cameras. [sent-20, score-0.929]
</p><p>15 Additionally, to find corresponding intersections, we propose a new method for matching trajectories that represents trajectories in a Spatio-Temporal Context Graph (STCG). [sent-21, score-1.006]
</p><p>16 Our method, see Figure 1, starts with the ground plane trajectories of two views. [sent-22, score-0.502]
</p><p>17 Then, the spatial and temporal relationships between trajectories in one view are cap-  tured in an STCG, which we use to find the best matching trajectories in anotherview. [sent-23, score-1.226]
</p><p>18 Finally, the corresponding intersections ofcorresponding trajectories are used to compute a homography. [sent-24, score-0.77]
</p><p>19 Experiments show that our method performs as well as state-of-the-art methods on synchronized videos, and better on unsynchronized videos. [sent-25, score-0.369]
</p><p>20 The paper is divided as follows: Section 2 overviews prior work in calibration, alignment, and synchronization. [sent-26, score-0.045]
</p><p>21 Section 3 describes STCGs and how to use them to match trajectories. [sent-27, score-0.078]
</p><p>22 Section 4 describes the procedure for using trajectory intersections to align camera views. [sent-28, score-0.678]
</p><p>23 In Section 5, we present results on both simulated and realworld datasets, and then discuss the advantages and disadvantages of the method in Section 6. [sent-29, score-0.076]
</p><p>24 Prior Work Camera calibration, image alignment, and synchronization has been well covered in the literature. [sent-31, score-0.245]
</p><p>25 Camera calibration and image alignment typically follows a pattern of finding a set of potential corresponding points across views, and then extracting a geometric model, e. [sent-32, score-0.258]
</p><p>26 a homography or fundamental matrix, using a RANSACbased method. [sent-34, score-0.124]
</p><p>27 These corresponding points are usually found in static images, and matched using feature descriptors such as SIFT [8], SURF [2], and MSER [9]. [sent-35, score-0.086]
</p><p>28 In videos with low image quality orwide-baselines, im-  age features do not work well. [sent-39, score-0.076]
</p><p>29 [7] select object centroids across camera views that occur simultaneously in a small time window as potential corresponding points. [sent-43, score-0.304]
</p><p>30 Stauffer and Tieu [19] probabilistically match simultaneous points that have similar appearances. [sent-44, score-0.153]
</p><p>31 [3] use a RANSAC variant to select corresponding trajectories and compute a homography or fundamental matrix from the observations in the trajectories. [sent-48, score-0.591]
</p><p>32 [11] use bipartite graph matching to compute the epipolar constraints of synchronized cameras using the trajectories as features. [sent-50, score-0.87]
</p><p>33 They create a digraph that finds the maximum likelihood estimate of the inter-frame homographies. [sent-52, score-0.043]
</p><p>34 Two of the these methods handle one form of synchro-  nization: a constant time shift. [sent-53, score-0.066]
</p><p>35 [7] re-align the videos with different time shifts and select the shift with the least error. [sent-55, score-0.109]
</p><p>36 [3] estimates the homography and time shift simultaneously in RANSAC. [sent-57, score-0.187]
</p><p>37 The literature on synchronization overlaps with our goal of aligning unsynchronized videos. [sent-58, score-0.597]
</p><p>38 They also tend to assume videos with constant frame rate, and often require calibration. [sent-59, score-0.286]
</p><p>39 [21] formalize temporal synchronization in cases when different cameras have different constant frame rates. [sent-61, score-0.586]
</p><p>40 Their proposed synchronization method requires the camera geometry of3 views, and first roughly synchronizes cameras using the points of maximum curvature. [sent-62, score-0.475]
</p><p>41 Then, they refine the synchronization to subframe accuracy using the epipolar lines of the inflection points. [sent-63, score-0.42]
</p><p>42 Pundik and Moses [15] also use epipolar lines from calibrated cameras by matching temporal signals along the epipolar lines. [sent-64, score-0.466]
</p><p>43 Wolf and Zomet [22] do not assume an existing calibration, but assume the videos have equal and constant frame rates. [sent-65, score-0.244]
</p><p>44 They synchronize views by rank constraints on matrices that capture either the linear combination between points in two views or the brightness measurements of image patches. [sent-66, score-0.419]
</p><p>45 Sinha and Pollefeys [17] simultaneously calibrate and synchronize cameras in a network, but require the silhouette of a person instead of the trajectory alone. [sent-67, score-0.525]
</p><p>46 These methods are not designed for unsynchronized, variable frame rate videos. [sent-68, score-0.149]
</p><p>47 Most of the alignment methods require manual synchronization or only handle a  constant time shift. [sent-69, score-0.453]
</p><p>48 Most of the synchronization methods assume constant frame rates or are dependent on an existing alignment. [sent-70, score-0.483]
</p><p>49 This paper addresses unsynchronized cameras with variable frame rates. [sent-71, score-0.602]
</p><p>50 It proposes the novel use of trajectory intersections as corresponding points to align these views, and a method to match trajectories based on the the spatial and temporal context between neighboring trajectories. [sent-72, score-1.323]
</p><p>51 Unlike, the existing methods, these do not required constant frame rates or frame-to-frame synchronization. [sent-73, score-0.238]
</p><p>52 Trajectory Matching using STCGs Our approach requires as input ground plane object trajectories in each camera view. [sent-75, score-0.581]
</p><p>53 We express each trajectory j in camera ias a sequence of observations Tji = {(xji [n],yji [n], tji [n])}n∈? [sent-76, score-0.618]
</p><p>54 , where xji [n] and yji [n] are the {im(xage coordina[tnes]) of an observation and tji [n] is the timestamp of an observation. [sent-77, score-0.426]
</p><p>55 Typically, tji [n] tji [n 1] is not constant for all n, nor is it synchronize[dn a]−crtos[sn v−ie1w]s is. [sent-78, score-0.612]
</p><p>56 Methods for obtaining these trajectories are numerous and outside the scope of this work. [sent-79, score-0.467]
</p><p>57 We match trajectories across views by applying Balanced Graph Matching [4] on the graphs of the views. [sent-82, score-0.778]
</p><p>58 jectory intersections that can be used to compute the homography. [sent-83, score-0.348]
</p><p>59 Our novel trajectory matching technique first codifies for each view the spatio-temporal relationships between the trajectories using a graph. [sent-84, score-0.854]
</p><p>60 Then we use existing graph matching techniques to match trajectories across views. [sent-85, score-0.698]
</p><p>61 For each camera i, we build a fully-connected, attributed multi-graph Gi = (Vi, Ei,Ai) with vertices Vi = {1, . [sent-86, score-0.12]
</p><p>62 re , t lhikee etr tahjeec etoxarimesp lien sah ocawmne irna, Tijgi-, and each edge, (j, k), has an attribute that is the distance between trajectories j and k, d(Tji, Tki). [sent-100, score-0.603]
</p><p>63 One represents a spatial distance, dspatial and the other a temporal distance, dtemporal. [sent-102, score-0.099]
</p><p>64 The graphs are matched using the Balanced Graph Matching method by Cour et al. [sent-103, score-0.111]
</p><p>65 Spatial Trajectory Distances Spatial trajectory distances measure the distance between trajectories in a view. [sent-107, score-0.811]
</p><p>66 Intuitively, trajectories that are near each other should remain nearby. [sent-108, score-0.467]
</p><p>67 However, different viewpoints may reveal different relationships between the trajectories, and some homographies may severely affect the arrangement of distances. [sent-109, score-0.139]
</p><p>68 In most surveillance networks where cameras are mounted with similar heights and orientations, these types of homographies are less likely. [sent-110, score-0.268]
</p><p>69 We experimented with four commonly used spatial distance metrics [13] . [sent-111, score-0.097]
</p><p>70 The Euclidean distance is computed as the average Euclidean distance between points on two trajectories. [sent-113, score-0.172]
</p><p>71 This requires that they have the same number of points, and thus the trajectory has to be resampled. [sent-114, score-0.235]
</p><p>72 In our experiments, we resample to 100 evenly-spaced points. [sent-115, score-0.036]
</p><p>73 In the PCA+Euclidean distance, the (x,y) coordinates of the trajectories are transformed using PCA to capture 95% of the variation. [sent-116, score-0.467]
</p><p>74 The distance is the Euclidean distance between these coefficients. [sent-117, score-0.128]
</p><p>75 The trajectories must  again be of equal length and so are resampled to 100 points. [sent-118, score-0.467]
</p><p>76 Dynamic Time Warping (DTW) finds the optimal time warping that minimizes the total distance between matching points. [sent-119, score-0.218]
</p><p>77 Unlike the previous distances, DTW does not require that trajectories have the same length. [sent-120, score-0.509]
</p><p>78 The Longest Common Subsequence (LCSS) distance determines the longest subsequence that is common to both trajectories. [sent-121, score-0.199]
</p><p>79 The distance in Table 1 is based on the LCSS, which is found using the algorithm in Equation 1. [sent-123, score-0.064]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('trajectories', 0.467), ('unsynchronized', 0.312), ('intersections', 0.303), ('tji', 0.273), ('synchronization', 0.245), ('trajectory', 0.235), ('stcg', 0.151), ('stcgs', 0.151), ('lcss', 0.134), ('views', 0.132), ('homography', 0.124), ('synchronize', 0.111), ('cameras', 0.107), ('frame', 0.102), ('alignment', 0.1), ('wireless', 0.089), ('epipolar', 0.087), ('santa', 0.083), ('calibration', 0.082), ('camera', 0.079), ('match', 0.078), ('videos', 0.076), ('caspi', 0.074), ('matching', 0.072), ('xji', 0.071), ('rates', 0.07), ('graphs', 0.069), ('dtw', 0.069), ('longest', 0.069), ('barbara', 0.069), ('subsequence', 0.066), ('constant', 0.066), ('temporal', 0.066), ('homographies', 0.065), ('distance', 0.064), ('align', 0.061), ('synchronized', 0.057), ('euclidean', 0.05), ('graph', 0.049), ('variable', 0.047), ('lines', 0.047), ('balanced', 0.045), ('compromises', 0.045), ('jectory', 0.045), ('nization', 0.045), ('overviews', 0.045), ('pundik', 0.045), ('timestamp', 0.045), ('distances', 0.045), ('simulated', 0.044), ('points', 0.044), ('finds', 0.043), ('relationships', 0.043), ('require', 0.042), ('matched', 0.042), ('subframe', 0.041), ('tured', 0.041), ('vertices', 0.041), ('aligning', 0.04), ('warping', 0.039), ('lien', 0.039), ('zomet', 0.039), ('meingast', 0.039), ('mser', 0.039), ('stauffer', 0.039), ('tieu', 0.039), ('axl', 0.037), ('moses', 0.037), ('yji', 0.037), ('view', 0.037), ('context', 0.036), ('manjunath', 0.036), ('kuo', 0.036), ('resample', 0.036), ('plane', 0.035), ('surveillance', 0.034), ('addresses', 0.034), ('ace', 0.033), ('etr', 0.033), ('struck', 0.033), ('att', 0.033), ('vtd', 0.033), ('shift', 0.033), ('spatial', 0.033), ('disadvantages', 0.032), ('ainn', 0.032), ('across', 0.032), ('compensated', 0.031), ('ias', 0.031), ('pets', 0.031), ('networks', 0.031), ('affect', 0.031), ('heights', 0.031), ('thomas', 0.031), ('bipartite', 0.031), ('centroids', 0.031), ('probabilistically', 0.031), ('sinha', 0.03), ('simultaneously', 0.03), ('vi', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="68-tfidf-1" href="./iccv-2013-Camera_Alignment_Using_Trajectory_Intersections_in_Unsynchronized_Videos.html">68 iccv-2013-Camera Alignment Using Trajectory Intersections in Unsynchronized Videos</a></p>
<p>Author: Thomas Kuo, Santhoshkumar Sunderrajan, B.S. Manjunath</p><p>Abstract: This paper addresses the novel and challenging problem of aligning camera views that are unsynchronized by low and/or variable frame rates using object trajectories. Unlike existing trajectory-based alignment methods, our method does not require frame-to-frame synchronization. Instead, we propose using the intersections of corresponding object trajectories to match views. To find these intersections, we introduce a novel trajectory matching algorithm based on matching Spatio-Temporal Context Graphs (STCGs). These graphs represent the distances between trajectories in time and space within a view, and are matched to an STCG from another view to find the corresponding trajectories. To the best of our knowledge, this is one of the first attempts to align views that are unsynchronized with variable frame rates. The results on simulated and real-world datasets show trajectory intersections area viablefeatureforcamera alignment, and that the trajectory matching method performs well in real-world scenarios.</p><p>2 0.32490593 <a title="68-tfidf-2" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>Author: Feng Shi, Zhong Zhou, Jiangjian Xiao, Wei Wu</p><p>Abstract: Due to occlusions and objects ’ non-rigid deformation in the scene, the obtained motion trajectories from common trackers may contain a number of missing or mis-associated entries. To cluster such corrupted point based trajectories into multiple motions is still a hard problem. In this paper, we present an approach that exploits temporal and spatial characteristics from tracked points to facilitate segmentation of incomplete and corrupted trajectories, thereby obtain highly robust results against severe data missing and noises. Our method first uses the Discrete Cosine Transform (DCT) bases as a temporal smoothness constraint on trajectory projection to ensure the validity of resulting components to repair pathological trajectories. Then, based on an observation that the trajectories of foreground and background in a scene may have different spatial distributions, we propose a two-stage clustering strategy that first performs foreground-background separation then segments remaining foreground trajectories. We show that, with this new clustering strategy, sequences with complex motions can be accurately segmented by even using a simple trans- lational model. Finally, a series of experiments on Hopkins 155 dataset andBerkeley motion segmentation dataset show the advantage of our method over other state-of-the-art motion segmentation algorithms in terms of both effectiveness and robustness.</p><p>3 0.31869817 <a title="68-tfidf-3" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>Author: Ali Elqursh, Ahmed Elgammal</p><p>Abstract: The vast majority of work on motion segmentation adopts the affine camera model due to its simplicity. Under the affine model, the motion segmentation problem becomes that of subspace separation. Due to this assumption, such methods are mainly offline and exhibit poor performance when the assumption is not satisfied. This is made evident in state-of-the-art methods that relax this assumption by using piecewise affine spaces and spectral clustering techniques to achieve better results. In this paper, we formulate the problem of motion segmentation as that of manifold separation. We then show how label propagation can be used in an online framework to achieve manifold separation. The performance of our framework is evaluated on a benchmark dataset and achieves competitive performance while being online.</p><p>4 0.30671078 <a title="68-tfidf-4" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>Author: Jiaming Guo, Zhuwen Li, Loong-Fah Cheong, Steven Zhiying Zhou</p><p>Abstract: Given a pair of videos having a common action, our goal is to simultaneously segment this pair of videos to extract this common action. As a preprocessing step, we first remove background trajectories by a motion-based figureground segmentation. To remove the remaining background and those extraneous actions, we propose the trajectory cosaliency measure, which captures the notion that trajectories recurring in all the videos should have their mutual saliency boosted. This requires a trajectory matching process which can compare trajectories with different lengths and not necessarily spatiotemporally aligned, and yet be discriminative enough despite significant intra-class variation in the common action. We further leverage the graph matching to enforce geometric coherence between regions so as to reduce feature ambiguity and matching errors. Finally, to classify the trajectories into common action and action outliers, we formulate the problem as a binary labeling of a Markov Random Field, in which the data term is measured by the trajectory co-saliency and the smooth- ness term is measured by the spatiotemporal consistency between trajectories. To evaluate the performance of our framework, we introduce a dataset containing clips that have animal actions as well as human actions. Experimental results show that the proposed method performs well in common action extraction.</p><p>5 0.25286156 <a title="68-tfidf-5" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>Author: Heng Wang, Cordelia Schmid</p><p>Abstract: Recently dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets. This paper improves their performance by taking into account camera motion to correct them. To estimate camera motion, we match feature points between frames using SURF descriptors and dense optical flow, which are shown to be complementary. These matches are, then, used to robustly estimate a homography with RANSAC. Human motion is in general different from camera motion and generates inconsistent matches. To improve the estimation, a human detector is employed to remove these matches. Given the estimated camera motion, we remove trajectories consistent with it. We also use this estimation to cancel out camera motion from the optical flow. This significantly improves motion-based descriptors, such as HOF and MBH. Experimental results onfour challenging action datasets (i.e., Hollywood2, HMDB51, Olympic Sports and UCF50) significantly outperform the current state of the art.</p><p>6 0.18424946 <a title="68-tfidf-6" href="./iccv-2013-Measuring_Flow_Complexity_in_Videos.html">263 iccv-2013-Measuring Flow Complexity in Videos</a></p>
<p>7 0.17000645 <a title="68-tfidf-7" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>8 0.15767334 <a title="68-tfidf-8" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>9 0.15629399 <a title="68-tfidf-9" href="./iccv-2013-Space-Time_Tradeoffs_in_Photo_Sequencing.html">397 iccv-2013-Space-Time Tradeoffs in Photo Sequencing</a></p>
<p>10 0.14310683 <a title="68-tfidf-10" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>11 0.12013207 <a title="68-tfidf-11" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>12 0.10026534 <a title="68-tfidf-12" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>13 0.099394888 <a title="68-tfidf-13" href="./iccv-2013-Lifting_3D_Manhattan_Lines_from_a_Single_Image.html">250 iccv-2013-Lifting 3D Manhattan Lines from a Single Image</a></p>
<p>14 0.087881505 <a title="68-tfidf-14" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>15 0.087525912 <a title="68-tfidf-15" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>16 0.085537352 <a title="68-tfidf-16" href="./iccv-2013-Unsupervised_Random_Forest_Manifold_Alignment_for_Lipreading.html">437 iccv-2013-Unsupervised Random Forest Manifold Alignment for Lipreading</a></p>
<p>17 0.081746869 <a title="68-tfidf-17" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>18 0.080878235 <a title="68-tfidf-18" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>19 0.077621482 <a title="68-tfidf-19" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>20 0.073625162 <a title="68-tfidf-20" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.165), (1, -0.049), (2, 0.029), (3, 0.191), (4, -0.031), (5, 0.116), (6, 0.027), (7, 0.044), (8, 0.193), (9, 0.135), (10, 0.059), (11, 0.042), (12, 0.036), (13, 0.029), (14, 0.013), (15, 0.037), (16, 0.048), (17, 0.114), (18, 0.001), (19, 0.022), (20, -0.168), (21, -0.087), (22, 0.128), (23, 0.261), (24, 0.057), (25, 0.182), (26, -0.034), (27, 0.025), (28, 0.028), (29, 0.067), (30, 0.038), (31, 0.115), (32, 0.022), (33, -0.009), (34, 0.075), (35, -0.098), (36, -0.034), (37, -0.041), (38, 0.041), (39, 0.004), (40, 0.033), (41, 0.003), (42, 0.039), (43, 0.031), (44, -0.006), (45, 0.056), (46, 0.01), (47, 0.098), (48, -0.003), (49, 0.096)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97055739 <a title="68-lsi-1" href="./iccv-2013-Camera_Alignment_Using_Trajectory_Intersections_in_Unsynchronized_Videos.html">68 iccv-2013-Camera Alignment Using Trajectory Intersections in Unsynchronized Videos</a></p>
<p>Author: Thomas Kuo, Santhoshkumar Sunderrajan, B.S. Manjunath</p><p>Abstract: This paper addresses the novel and challenging problem of aligning camera views that are unsynchronized by low and/or variable frame rates using object trajectories. Unlike existing trajectory-based alignment methods, our method does not require frame-to-frame synchronization. Instead, we propose using the intersections of corresponding object trajectories to match views. To find these intersections, we introduce a novel trajectory matching algorithm based on matching Spatio-Temporal Context Graphs (STCGs). These graphs represent the distances between trajectories in time and space within a view, and are matched to an STCG from another view to find the corresponding trajectories. To the best of our knowledge, this is one of the first attempts to align views that are unsynchronized with variable frame rates. The results on simulated and real-world datasets show trajectory intersections area viablefeatureforcamera alignment, and that the trajectory matching method performs well in real-world scenarios.</p><p>2 0.79729068 <a title="68-lsi-2" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>Author: Feng Shi, Zhong Zhou, Jiangjian Xiao, Wei Wu</p><p>Abstract: Due to occlusions and objects ’ non-rigid deformation in the scene, the obtained motion trajectories from common trackers may contain a number of missing or mis-associated entries. To cluster such corrupted point based trajectories into multiple motions is still a hard problem. In this paper, we present an approach that exploits temporal and spatial characteristics from tracked points to facilitate segmentation of incomplete and corrupted trajectories, thereby obtain highly robust results against severe data missing and noises. Our method first uses the Discrete Cosine Transform (DCT) bases as a temporal smoothness constraint on trajectory projection to ensure the validity of resulting components to repair pathological trajectories. Then, based on an observation that the trajectories of foreground and background in a scene may have different spatial distributions, we propose a two-stage clustering strategy that first performs foreground-background separation then segments remaining foreground trajectories. We show that, with this new clustering strategy, sequences with complex motions can be accurately segmented by even using a simple trans- lational model. Finally, a series of experiments on Hopkins 155 dataset andBerkeley motion segmentation dataset show the advantage of our method over other state-of-the-art motion segmentation algorithms in terms of both effectiveness and robustness.</p><p>3 0.75745267 <a title="68-lsi-3" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>Author: Ali Elqursh, Ahmed Elgammal</p><p>Abstract: The vast majority of work on motion segmentation adopts the affine camera model due to its simplicity. Under the affine model, the motion segmentation problem becomes that of subspace separation. Due to this assumption, such methods are mainly offline and exhibit poor performance when the assumption is not satisfied. This is made evident in state-of-the-art methods that relax this assumption by using piecewise affine spaces and spectral clustering techniques to achieve better results. In this paper, we formulate the problem of motion segmentation as that of manifold separation. We then show how label propagation can be used in an online framework to achieve manifold separation. The performance of our framework is evaluated on a benchmark dataset and achieves competitive performance while being online.</p><p>4 0.69674224 <a title="68-lsi-4" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>Author: Jiaming Guo, Zhuwen Li, Loong-Fah Cheong, Steven Zhiying Zhou</p><p>Abstract: Given a pair of videos having a common action, our goal is to simultaneously segment this pair of videos to extract this common action. As a preprocessing step, we first remove background trajectories by a motion-based figureground segmentation. To remove the remaining background and those extraneous actions, we propose the trajectory cosaliency measure, which captures the notion that trajectories recurring in all the videos should have their mutual saliency boosted. This requires a trajectory matching process which can compare trajectories with different lengths and not necessarily spatiotemporally aligned, and yet be discriminative enough despite significant intra-class variation in the common action. We further leverage the graph matching to enforce geometric coherence between regions so as to reduce feature ambiguity and matching errors. Finally, to classify the trajectories into common action and action outliers, we formulate the problem as a binary labeling of a Markov Random Field, in which the data term is measured by the trajectory co-saliency and the smooth- ness term is measured by the spatiotemporal consistency between trajectories. To evaluate the performance of our framework, we introduce a dataset containing clips that have animal actions as well as human actions. Experimental results show that the proposed method performs well in common action extraction.</p><p>5 0.68938643 <a title="68-lsi-5" href="./iccv-2013-Measuring_Flow_Complexity_in_Videos.html">263 iccv-2013-Measuring Flow Complexity in Videos</a></p>
<p>Author: Saad Ali</p><p>Abstract: In this paper a notion of flow complexity that measures the amount of interaction among objects is introduced and an approach to compute it directly from a video sequence is proposed. The approach employs particle trajectories as the input representation of motion and maps it into a ‘braid’ based representation. The mapping is based on the observation that 2D trajectories of particles take the form of a braid in space-time due to the intermingling among particles over time. As a result of this mapping, the problem of estimating the flow complexity from particle trajectories becomes the problem of estimating braid complexity, which in turn can be computed by measuring the topological entropy of a braid. For this purpose recently developed mathematical tools from braid theory are employed which allow rapid computation of topological entropy of braids. The approach is evaluated on a dataset consisting of open source videos depicting variations in terms of types of moving objects, scene layout, camera view angle, motion patterns, and object densities. The results show that the proposed approach is able to quantify the complexity of the flow, and at the same time provides useful insights about the sources of the complexity.</p><p>6 0.66292453 <a title="68-lsi-6" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>7 0.6577847 <a title="68-lsi-7" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>8 0.61041695 <a title="68-lsi-8" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>9 0.50801021 <a title="68-lsi-9" href="./iccv-2013-Space-Time_Tradeoffs_in_Photo_Sequencing.html">397 iccv-2013-Space-Time Tradeoffs in Photo Sequencing</a></p>
<p>10 0.47180116 <a title="68-lsi-10" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>11 0.47166261 <a title="68-lsi-11" href="./iccv-2013-Manipulation_Pattern_Discovery%3A_A_Nonparametric_Bayesian_Approach.html">260 iccv-2013-Manipulation Pattern Discovery: A Nonparametric Bayesian Approach</a></p>
<p>12 0.44921169 <a title="68-lsi-12" href="./iccv-2013-The_Way_They_Move%3A_Tracking_Multiple_Targets_with_Similar_Appearance.html">418 iccv-2013-The Way They Move: Tracking Multiple Targets with Similar Appearance</a></p>
<p>13 0.4313257 <a title="68-lsi-13" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>14 0.3968412 <a title="68-lsi-14" href="./iccv-2013-Unsupervised_Random_Forest_Manifold_Alignment_for_Lipreading.html">437 iccv-2013-Unsupervised Random Forest Manifold Alignment for Lipreading</a></p>
<p>15 0.37326041 <a title="68-lsi-15" href="./iccv-2013-Estimating_the_Material_Properties_of_Fabric_from_Video.html">145 iccv-2013-Estimating the Material Properties of Fabric from Video</a></p>
<p>16 0.37133601 <a title="68-lsi-16" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>17 0.36838132 <a title="68-lsi-17" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>18 0.35787061 <a title="68-lsi-18" href="./iccv-2013-Optimal_Orthogonal_Basis_and_Image_Assimilation%3A_Motion_Modeling.html">301 iccv-2013-Optimal Orthogonal Basis and Image Assimilation: Motion Modeling</a></p>
<p>19 0.35054302 <a title="68-lsi-19" href="./iccv-2013-A_Global_Linear_Method_for_Camera_Pose_Registration.html">17 iccv-2013-A Global Linear Method for Camera Pose Registration</a></p>
<p>20 0.3380174 <a title="68-lsi-20" href="./iccv-2013-Lifting_3D_Manhattan_Lines_from_a_Single_Image.html">250 iccv-2013-Lifting 3D Manhattan Lines from a Single Image</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.056), (7, 0.014), (26, 0.06), (31, 0.037), (34, 0.011), (42, 0.076), (54, 0.283), (64, 0.063), (73, 0.035), (89, 0.245), (97, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81630552 <a title="68-lda-1" href="./iccv-2013-Camera_Alignment_Using_Trajectory_Intersections_in_Unsynchronized_Videos.html">68 iccv-2013-Camera Alignment Using Trajectory Intersections in Unsynchronized Videos</a></p>
<p>Author: Thomas Kuo, Santhoshkumar Sunderrajan, B.S. Manjunath</p><p>Abstract: This paper addresses the novel and challenging problem of aligning camera views that are unsynchronized by low and/or variable frame rates using object trajectories. Unlike existing trajectory-based alignment methods, our method does not require frame-to-frame synchronization. Instead, we propose using the intersections of corresponding object trajectories to match views. To find these intersections, we introduce a novel trajectory matching algorithm based on matching Spatio-Temporal Context Graphs (STCGs). These graphs represent the distances between trajectories in time and space within a view, and are matched to an STCG from another view to find the corresponding trajectories. To the best of our knowledge, this is one of the first attempts to align views that are unsynchronized with variable frame rates. The results on simulated and real-world datasets show trajectory intersections area viablefeatureforcamera alignment, and that the trajectory matching method performs well in real-world scenarios.</p><p>2 0.81319225 <a title="68-lda-2" href="./iccv-2013-Shortest_Paths_with_Curvature_and_Torsion.html">389 iccv-2013-Shortest Paths with Curvature and Torsion</a></p>
<p>Author: Petter Strandmark, Johannes Ulén, Fredrik Kahl, Leo Grady</p><p>Abstract: This paper describes a method of finding thin, elongated structures in images and volumes. We use shortest paths to minimize very general functionals of higher-order curve properties, such as curvature and torsion. Our globally optimal method uses line graphs and its runtime is polynomial in the size of the discretization, often in the order of seconds on a single computer. To our knowledge, we are the first to perform experiments in three dimensions with curvature and torsion regularization. The largest graphs we process have almost one hundred billion arcs. Experiments on medical images and in multi-view reconstruction show the significance and practical usefulness of regularization based on curvature while torsion is still only tractable for small-scale problems.</p><p>3 0.80069268 <a title="68-lda-3" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<p>Author: Nicolas Martin, Vincent Couture, Sébastien Roy</p><p>Abstract: We present a scanning method that recovers dense subpixel camera-projector correspondence without requiring any photometric calibration nor preliminary knowledge of their relative geometry. Subpixel accuracy is achieved by considering several zero-crossings defined by the difference between pairs of unstructured patterns. We use gray-level band-pass white noise patterns that increase robustness to indirect lighting and scene discontinuities. Simulated and experimental results show that our method recovers scene geometry with high subpixel precision, and that it can handle many challenges of active reconstruction systems. We compare our results to state of the art methods such as micro phase shifting and modulated phase shifting.</p><p>4 0.77266681 <a title="68-lda-4" href="./iccv-2013-Nonparametric_Blind_Super-resolution.html">293 iccv-2013-Nonparametric Blind Super-resolution</a></p>
<p>Author: Tomer Michaeli, Michal Irani</p><p>Abstract: Super resolution (SR) algorithms typically assume that the blur kernel is known (either the Point Spread Function ‘PSF’ of the camera, or some default low-pass filter, e.g. a Gaussian). However, the performance of SR methods significantly deteriorates when the assumed blur kernel deviates from the true one. We propose a general framework for “blind” super resolution. In particular, we show that: (i) Unlike the common belief, the PSF of the camera is the wrong blur kernel to use in SR algorithms. (ii) We show how the correct SR blur kernel can be recovered directly from the low-resolution image. This is done by exploiting the inherent recurrence property of small natural image patches (either internally within the same image, or externally in a collection of other natural images). In particular, we show that recurrence of small patches across scales of the low-res image (which forms the basis for single-image SR), can also be used for estimating the optimal blur kernel. This leads to significant improvement in SR results.</p><p>5 0.74430501 <a title="68-lda-5" href="./iccv-2013-Event_Recognition_in_Photo_Collections_with_a_Stopwatch_HMM.html">147 iccv-2013-Event Recognition in Photo Collections with a Stopwatch HMM</a></p>
<p>Author: Lukas Bossard, Matthieu Guillaumin, Luc Van_Gool</p><p>Abstract: The task of recognizing events in photo collections is central for automatically organizing images. It is also very challenging, because of the ambiguity of photos across different event classes and because many photos do not convey enough relevant information. Unfortunately, the field still lacks standard evaluation data sets to allow comparison of different approaches. In this paper, we introduce and release a novel data set of personal photo collections containing more than 61,000 images in 807 collections, annotated with 14 diverse social event classes. Casting collections as sequential data, we build upon recent and state-of-the-art work in event recognition in videos to propose a latent sub-event approach for event recognition in photo collections. However, photos in collections are sparsely sampled over time and come in bursts from which transpires the importance of specific moments for the photographers. Thus, we adapt a discriminative hidden Markov model to allow the transitions between states to be a function of the time gap between consecutive images, which we coin as Stopwatch Hidden Markov model (SHMM). In our experiments, we show that our proposed model outperforms approaches based only on feature pooling or a classical hidden Markov model. With an average accuracy of 56%, we also highlight the difficulty of the data set and the need for future advances in event recognition in photo collections.</p><p>6 0.69959217 <a title="68-lda-6" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>7 0.6992923 <a title="68-lda-7" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>8 0.69906151 <a title="68-lda-8" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>9 0.69846511 <a title="68-lda-9" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>10 0.6981647 <a title="68-lda-10" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>11 0.69789582 <a title="68-lda-11" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>12 0.69716012 <a title="68-lda-12" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>13 0.69663417 <a title="68-lda-13" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>14 0.69639945 <a title="68-lda-14" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>15 0.69577199 <a title="68-lda-15" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>16 0.69535649 <a title="68-lda-16" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>17 0.69511366 <a title="68-lda-17" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>18 0.6950016 <a title="68-lda-18" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>19 0.69469947 <a title="68-lda-19" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>20 0.69432455 <a title="68-lda-20" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
