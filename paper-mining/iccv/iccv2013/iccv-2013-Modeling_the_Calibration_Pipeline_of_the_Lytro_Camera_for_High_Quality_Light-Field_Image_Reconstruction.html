<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-271" href="#">iccv2013-271</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</h1>
<br/><p>Source: <a title="iccv-2013-271-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Cho_Modeling_the_Calibration_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Donghyeon Cho, Minhaeng Lee, Sunyeong Kim, Yu-Wing Tai</p><p>Abstract: Light-field imaging systems have got much attention recently as the next generation camera model. A light-field imaging system consists of three parts: data acquisition, manipulation, and application. Given an acquisition system, it is important to understand how a light-field camera converts from its raw image to its resulting refocused image. In this paper, using the Lytro camera as an example, we describe step-by-step procedures to calibrate a raw light-field image. In particular, we are interested in knowing the spatial and angular coordinates of the micro lens array and the resampling process for image reconstruction. Since Lytro uses a hexagonal arrangement of a micro lens image, additional treatments in calibration are required. After calibration, we analyze and compare the performances of several resampling methods for image reconstruction with and without calibration. Finally, a learning based interpolation method is proposed which demonstrates a higher quality image reconstruction than previous interpolation methods including a method used in Lytro software.</p><p>Reference: <a title="iccv-2013-271-reference" href="../iccv2013_reference/iccv-2013-Modeling_the_Calibration_Pipeline_of_the_Lytro_Camera_for_High_Quality_Light-Field_Image_Reconstruction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, using the Lytro camera as an example, we describe step-by-step procedures to calibrate a raw light-field image. [sent-4, score-0.299]
</p><p>2 In particular, we are interested in knowing the spatial and angular coordinates of the micro lens array and the resampling process for image reconstruction. [sent-5, score-1.133]
</p><p>3 Since Lytro uses a hexagonal arrangement of a micro lens image, additional treatments in calibration are required. [sent-6, score-1.18]
</p><p>4 Finally, a learning based interpolation method is proposed which demonstrates a higher quality image reconstruction than previous interpolation methods including a method used in Lytro software. [sent-8, score-0.513]
</p><p>5 Coordinates in the st and uv planes represent the intersection of incoming light from different view perspectives and we denote it as L(s, t, u, v). [sent-15, score-0.297]
</p><p>6 In practice, light field images captured by a light field camera are not perfect. [sent-17, score-0.806]
</p><p>7 To accurately convert a light field raw image into the representation in L(s, t, u, v) requires careful calibration and resampling. [sent-23, score-0.737]
</p><p>8 In this paper, using the Lytro camera as an example, we describe step-by-step procedures to calibrate and to convert the raw image into L(s, t, u, v) representation. [sent-24, score-0.342]
</p><p>9 We model the calibration pipeline of the Lytro lightfield camera and describe step-by-step procedures to achieve accurate calibration. [sent-36, score-0.332]
</p><p>10 We analyze and evaluate several interpolation techniques for pixel resampling in L(s, t, u, v). [sent-38, score-0.382]
</p><p>11 We show that direct interpolation in RAW images for hexagonal  grid produces better interpolation than first making a low resolution regular grid image followed by interpolation. [sent-39, score-0.814]
</p><p>12 A dictionary learning based interpolation technique is proposed which demonstrates a higher quality image 33227803  the micro lens array is not parallel to image coordinate. [sent-41, score-1.316]
</p><p>13 reconstruction than previous interpolation methods including method used in Lytro software. [sent-42, score-0.244]
</p><p>14 [17] presented the prototype light-field camera utilizing micro lens array, many progresses have been made in plenoptic camera developments [19, 12, 13, 18, 14, 5, 7]. [sent-46, score-1.109]
</p><p>15 A major application of light field camera is the post-digital refocusing which changes focus on a image after a picture is taken. [sent-47, score-0.454]
</p><p>16 To overcome this limitation, many light field super-resolution algorithms have been developed [2, 3, 8, 13, 10]. [sent-49, score-0.379]
</p><p>17 use ray tracing in light field to get a high resolution focal stack image. [sent-51, score-0.578]
</p><p>18 They utilize light ray from different direction to obtain sub-pixel details. [sent-52, score-0.271]
</p><p>19 [12, 13] consider the trade off between spatial and angular information in light field capturing. [sent-54, score-0.414]
</p><p>20 They developed the focused plenoptic camera called plenoptic 2. [sent-55, score-0.404]
</p><p>21 0 which places the micro lens array behind the main lens image plane and with a small distance in front of image sensor. [sent-56, score-1.37]
</p><p>22 There are also works that utilize light field representation for super-resolution which is independent of hardware configuration knowledge. [sent-66, score-0.379]
</p><p>23 [10] suggest a dimensionality gap prior in the 4D frequency domain of light field for view synthesis and to enhance resolution through frequency domain interpolation without using depth information. [sent-71, score-0.934]
</p><p>24 Some of the works assume their initial input is from the light field L(s, t, u, v) representation. [sent-75, score-0.379]
</p><p>25 In this paper, we systematically analyze the quality of RAW images from the Lytro camera and describe step-by-step procedures to convert RAW to L(s, t, u, v). [sent-76, score-0.225]
</p><p>26 To this end, a dictionary learning based interpolation method is presented for high quality light field image reconstruction. [sent-78, score-0.722]
</p><p>27 RAW data analysis and calibration In this section, we analyze the RAW data from the Lytro camera and describe our calibration procedures to correct the misalignment error between micro lens array and image sensor. [sent-80, score-1.416]
</p><p>28 In the next section, we evaluate different resampling methods and propose our learning based interpolation method for high quality light field image reconstruction. [sent-81, score-0.773]
</p><p>29 lfp file contains camera parameters such as focal length in the file header and a RAW image file as shown in Figure 2. [sent-87, score-0.282]
</p><p>30 For each micro lens, the diameter is around  ×  10 pixels and the physical size of each micro lens is around 1. [sent-91, score-1.355]
</p><p>31 f4 m×ic1r0o lens (assuming grid based micro lens array), the effective resolution of the reconstructed light field image is 328 328. [sent-94, score-1.832]
</p><p>32 aTn chei sth iem resolution of rendered images instead of using a naive method to reconstruct a low resolution light-field image for rendering. [sent-98, score-0.296]
</p><p>33 Calibration In order to convert the RAW image file to the light field image representation effectively, we need to calibrate the RAW image. [sent-101, score-0.526]
</p><p>34 The main goal of this calibration is to identify center point locations of each micro-lens sub-image and rearrange them in a regular basis for better resampling which will be described in the next section. [sent-102, score-0.377]
</p><p>35 Note the periodic pattern of coefficients due to the repetition micro lens image. [sent-114, score-0.877]
</p><p>36 (b) Initial rotation of micro lens image in RAW, (c) Rotation compensated micro lens image. [sent-115, score-1.699]
</p><p>37 Our next step is to estimate the rotation of micro lens array to compensate the misalignments between micro lens array and image sensor. [sent-117, score-1.975]
</p><p>38 We estimate the rotation of micro lens image by looking for a local maxima coefficient closest to the zero frequency location as shown in Figure 5(a). [sent-120, score-0.948]
</p><p>39 micro lens image, and hence we get the rotation of micro lens array. [sent-123, score-1.699]
</p><p>40 Note that if the micro lens array aligns with  pixel axis, the peak frequency should be in vertical or horizontal direction, but we barely find such case in our calibration. [sent-124, score-1.055]
</p><p>41 Finally, we estimate the center point of micro lens by ap-  ×  plying the eroding operation as shown in Figure 6(a). [sent-128, score-0.93]
</p><p>42 The non-uniformity of micro-lens center can be due to manufacturing defection where each micro lens have slightly different shape. [sent-129, score-0.955]
</p><p>43 Figure 6(c) shows the estimated center points of each micro lens image. [sent-134, score-0.895]
</p><p>44 Lastly, we use the Delaunay Triangulation to fit a regular triangle grid to the estimated center points of micro lens image and shift the micro lens image locally to obtain our calibrated image. [sent-135, score-1.88]
</p><p>45 diameter of micro lens) to obtain a well sampled low resolution regular grid light field image at a resolution of 328 328. [sent-150, score-1.179]
</p><p>46 Then, we use bicubic interpolation stoo upsample t2h8e× ×lo3w28 r. [sent-151, score-0.263]
</p><p>47 es Tolhuetni,on w light efie blidcu ibmicage to the target resolution. [sent-152, score-0.247]
</p><p>48 In addition, some high frequency details are lost in the downsampled light field image. [sent-155, score-0.435]
</p><p>49 Barycentric interpolation at target resolution To fully utilize the hexagonal layout of the micro lens array, we resize the triangular grid from the calibrated data to the target resolution. [sent-158, score-1.409]
</p><p>50 Then, we apply Barycentric interpolation to directly interpolate the pixel values from the micro lens centers at triangle corners. [sent-159, score-1.093]
</p><p>51 The Barycentric interpolation produces higher quality interpolation comparing to the previous method since it does not involve any downsampling. [sent-161, score-0.496]
</p><p>52 Also, the hexagonal layout of the micro lens array gives smoother edges with less aliasing artifacts. [sent-162, score-1.235]
</p><p>53 Refinement using Multiple-Views The Barycentric reconstruction uses only one pixel per micro lens image to reconstruct the light field image. [sent-165, score-1.343]
</p><p>54 In order to reconstruct a higher quality light field image, we can use more pixels from each micro lens image for reconstruction. [sent-166, score-1.371]
</p><p>55 Since pixels in a micro lens image represent rays from  isn me[sa7sl]o. [sent-167, score-0.864]
</p><p>56 Top left: Epipolar image from the Barycentric reconstruct light field image, Bottom left: Red points are from other view. [sent-171, score-0.446]
</p><p>57 Figure 7(Bottom Left) illustrates the copied pixels from adjacent views which follow the hexag-  onal arrangement of micro lens array in the Lytro camera. [sent-178, score-1.041]
</p><p>58 After this multi-view refinement, we obtain more details in the reconstructed light field image. [sent-181, score-0.467]
</p><p>59 Learning based Interpolation The multi-view refined light field image still contains aliasing which is unnatural. [sent-184, score-0.496]
</p><p>60 In this section, we adopt a learning based technique to train a dictionary that encodes natural image structures and use it to reconstruct our light field image. [sent-185, score-0.52]
</p><p>61 Our learning based interpolation is inspired by the work in [22, 9] in which they use dictionary learning with sparse coding to reconstruct super-resolved image from a low quality and low resolution image. [sent-186, score-0.553]
</p><p>62 After that, we use the Barycentric interpolation to re-interpolate the pixel values to get a synthesized image after the multi-view refinement. [sent-188, score-0.234]
</p><p>63 1  (3)  ×  where D = {Dh, Dl} is the trained dictionary which conwsishtesr of high quality an}d i slo twhe quality dictionary pair, iTch his c our training examples, and α is the sparse coefficient. [sent-193, score-0.27]
</p><p>64 In the reconstruction phase, we estimate the sparse coefficients which can faithfully reconstruct the multi-view refined light field image using the low quality dictionary by solving the following equation:  argmφin? [sent-195, score-0.636]
</p><p>65 (4)  Next, we substitute the low quality dictionary with the high quality dictionary and then reconstruct the light field image again using the high quality dictionary and the estimated sparse coefficients. [sent-200, score-0.87]
</p><p>66 After the learning based interpolation, our reconstructed light field images are of high quality which contains high resolution details without any aliasing. [sent-201, score-0.633]
</p><p>67 Experimental Results This section shows our reconstructed light field images from the Lytro RAW data. [sent-203, score-0.467]
</p><p>68 We examine the effects of the calibration by comparing the reconstructed light-field images with and without the calibration. [sent-204, score-0.311]
</p><p>69 In our experiment, we reconstruct light field images, L(s, t, u, v), with size 7 7 by using only the pixels around the calibrated cent7er × points uosfi nmgi ocrnol yle tnhes images. [sent-205, score-0.521]
</p><p>70 oTuhnisd tish e be ccaaliubsrae ethde meni--  ×  cro lens has vignetting and other non-uniform effects which greatly degrade the reconstructed light field image from the border pixels of micro lens images. [sent-206, score-1.748]
</p><p>71 Also, 7 7 light ftiheeld b images are already souf lfeicnsie nimt fagore post-focusing 7m leigthhtods [17, 16] and many light field super-resolution algorithms [3, 8, 10, 15, 21, 2]. [sent-207, score-0.626]
</p><p>72 We compute the results with and without calibration by assuming the positions of each center pixel of micro lens which are fixed on a given hexagonal grid. [sent-209, score-1.227]
</p><p>73 As shown in the leftmost column, results without calibration have blur, aliasing and color shift artifacts. [sent-211, score-0.278]
</p><p>74 This is because the reconstructed images without calibration can contain pixels from other view perspective. [sent-212, score-0.309]
</p><p>75 After calibration, the aliasing artifacts are reduced and edges are sharper as shown in the center images respectively. [sent-213, score-0.251]
</p><p>76 We examine the reconstructed center view with and without sub-pixel precision estimation of center points 33227847  Figure8. [sent-216, score-0.259]
</p><p>77 Results without calibration has many artifacts compared with calibrated results. [sent-219, score-0.26]
</p><p>78 Since the micro-lens array does not fully align with image sensors, using the integer pixel unit to represent micro lens centers can cause large errors especially when each of the micro lens is very small. [sent-222, score-1.834]
</p><p>79 Comparisons of different resampling methods In order to examine the effect on different resampling, we compare the reconstructed center view from the bicubic interpolation method described in Section 4. [sent-225, score-0.587]
</p><p>80 3, and the dictionary learning based interpolation method described in Section 4. [sent-228, score-0.282]
</p><p>81 In Figure 10 (b), blur and aliasing artifacts appear particularly in the edge region of resolution chart because some high frequency details have lost in the downsamping process. [sent-230, score-0.355]
</p><p>82 The Barycentric reconstruction at the target resolution with downsampling shows distinguishable lines in the resolution chart in Figure 10 (c) and better results in Figure 11. [sent-231, score-0.289]
</p><p>83 We also apply learning based interpolation on top of calibration and sub-pixel precision processes. [sent-233, score-0.369]
</p><p>84 Since a low resolution image is directly replaced by high resolution based on the dictionary, it has less aliasing artifacts, while other results based on the interpolation method still have jagged  (Right) sub-pixel precision estimation of micro lens center. [sent-235, score-1.418]
</p><p>85 We can also see that the dictionary learning interpolation outperforms Lytro software results with more details and less aliasing. [sent-239, score-0.306]
</p><p>86 Note that our results are of higher resolution and with more details and less aliasing artifacts. [sent-242, score-0.222]
</p><p>87 Conclusion and discussion We have presented the calibration pipeline of Lytro and several resampling algorithms for light field image reconstruction. [sent-244, score-0.665]
</p><p>88 Although this work is mostly engineering, it gives a good case study to understand the process of calibration and demonstrate the importance of developing better light field reconstruction algorithm for converting RAW to L(s, t, u, v). [sent-245, score-0.576]
</p><p>89 In the calibration, the Lytro RAW data is converted into the light-field representation L(s, t, u, v) and we estimate the center points in raw data which has a hexagonal formation. [sent-246, score-0.359]
</p><p>90 To reconstruct high quality light field images, we design the learning based interpolation algorithm and demonstrate that our learning based algorithm outperforms other resampling methods including results from the Lytro software. [sent-248, score-0.84]
</p><p>91 (a) Extracted pixels on hexagonal grid, (b) Bicubic interpolation on low resolution image, (c) Barycentric interpolation, (d) Using multiple images, (e) our learning based method, (f) Lytro built-in. [sent-251, score-0.506]
</p><p>92 knowing calibration parameters for high quality light-field reconstruction. [sent-252, score-0.222]
</p><p>93 While most previous works assume that the light field representation is given from plenotic cameras, the quality of light field images can vary a lot and hence can greatly affect the performances of post-processing algorithms. [sent-253, score-0.837]
</p><p>94 In the future, we plan to extend our work to combine with other light-field super-resolution algorithms to further enhance the resolution and quality of the light field image. [sent-254, score-0.565]
</p><p>95 The light field camera: Extended depth of field, aliasing, and superresolution. [sent-273, score-0.414]
</p><p>96 Linear view synthesis using a di-  mensionality gap light field prior. [sent-328, score-0.41]
</p><p>97 (From left to right) Bicubic interpolation on rectangle grid at low resolution, barycentric interpolation hexagonal grid, multiple images interpolation, learning based method, Lytro built-in method. [sent-332, score-0.832]
</p><p>98 Light field denoising, light field superresolution and stereo camera based refocussing us-  [16]  [17]  [18] [19]  [20] [21]  [22]  ing a gmm light field patch prior. [sent-356, score-0.97]
</p><p>99 Dappled photography: Mask enhanced cameras for heterodyned light fields and coded aperture refocusing. [sent-384, score-0.247]
</p><p>100 Spatial and angular variational super-resolution of 4d light fields. [sent-395, score-0.282]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('micro', 0.459), ('lytro', 0.397), ('lens', 0.376), ('light', 0.247), ('interpolation', 0.208), ('barycentric', 0.203), ('plenoptic', 0.178), ('calibration', 0.161), ('raw', 0.154), ('hexagonal', 0.145), ('array', 0.138), ('field', 0.132), ('resampling', 0.125), ('aliasing', 0.117), ('dansereau', 0.106), ('resolution', 0.105), ('reconstructed', 0.088), ('dictionary', 0.074), ('lightfield', 0.073), ('reconstruct', 0.067), ('quality', 0.061), ('center', 0.06), ('bishop', 0.058), ('file', 0.057), ('frequency', 0.056), ('bicubic', 0.055), ('artifacts', 0.053), ('paraboloid', 0.053), ('procedures', 0.05), ('grid', 0.049), ('camera', 0.048), ('calibrate', 0.047), ('lumsdaine', 0.047), ('calibrated', 0.046), ('wanner', 0.044), ('convert', 0.043), ('periodic', 0.042), ('rectification', 0.04), ('arrangement', 0.039), ('delaunay', 0.038), ('reconstruction', 0.036), ('defection', 0.035), ('eroding', 0.035), ('header', 0.035), ('nava', 0.035), ('depth', 0.035), ('angular', 0.035), ('white', 0.034), ('gamma', 0.034), ('refinement', 0.034), ('superresolution', 0.032), ('diameter', 0.032), ('demosaicking', 0.031), ('georgiev', 0.031), ('zanetti', 0.031), ('regular', 0.031), ('view', 0.031), ('epipolar', 0.031), ('jagged', 0.029), ('pixels', 0.029), ('rotation', 0.029), ('maxima', 0.028), ('triangulation', 0.028), ('focal', 0.028), ('refocusing', 0.027), ('pixel', 0.026), ('kaist', 0.026), ('sweep', 0.026), ('levoy', 0.025), ('manufacturing', 0.025), ('chart', 0.024), ('stretching', 0.024), ('triangle', 0.024), ('software', 0.024), ('ray', 0.024), ('effects', 0.023), ('analyze', 0.023), ('domain', 0.022), ('tracing', 0.022), ('adobe', 0.022), ('sharper', 0.021), ('plane', 0.021), ('triangular', 0.021), ('iccp', 0.021), ('decoding', 0.021), ('examine', 0.02), ('toolbox', 0.02), ('stack', 0.02), ('rightmost', 0.02), ('conversion', 0.02), ('enhance', 0.02), ('comparing', 0.019), ('ieee', 0.019), ('low', 0.019), ('downsampling', 0.019), ('uv', 0.019), ('performances', 0.018), ('cro', 0.018), ('levin', 0.018), ('photography', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="271-tfidf-1" href="./iccv-2013-Modeling_the_Calibration_Pipeline_of_the_Lytro_Camera_for_High_Quality_Light-Field_Image_Reconstruction.html">271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</a></p>
<p>Author: Donghyeon Cho, Minhaeng Lee, Sunyeong Kim, Yu-Wing Tai</p><p>Abstract: Light-field imaging systems have got much attention recently as the next generation camera model. A light-field imaging system consists of three parts: data acquisition, manipulation, and application. Given an acquisition system, it is important to understand how a light-field camera converts from its raw image to its resulting refocused image. In this paper, using the Lytro camera as an example, we describe step-by-step procedures to calibrate a raw light-field image. In particular, we are interested in knowing the spatial and angular coordinates of the micro lens array and the resampling process for image reconstruction. Since Lytro uses a hexagonal arrangement of a micro lens image, additional treatments in calibration are required. After calibration, we analyze and compare the performances of several resampling methods for image reconstruction with and without calibration. Finally, a learning based interpolation method is proposed which demonstrates a higher quality image reconstruction than previous interpolation methods including a method used in Lytro software.</p><p>2 0.17332926 <a title="271-tfidf-2" href="./iccv-2013-Depth_from_Combining_Defocus_and_Correspondence_Using_Light-Field_Cameras.html">108 iccv-2013-Depth from Combining Defocus and Correspondence Using Light-Field Cameras</a></p>
<p>Author: Michael W. Tao, Sunil Hadap, Jitendra Malik, Ravi Ramamoorthi</p><p>Abstract: Light-field cameras have recently become available to the consumer market. An array of micro-lenses captures enough information that one can refocus images after acquisition, as well as shift one ’s viewpoint within the subapertures of the main lens, effectively obtaining multiple views. Thus, depth cues from both defocus and correspondence are available simultaneously in a single capture. Previously, defocus could be achieved only through multiple image exposures focused at different depths, while correspondence cues needed multiple exposures at different viewpoints or multiple cameras; moreover, both cues could not easily be obtained together. In this paper, we present a novel simple and principled algorithm that computes dense depth estimation by combining both defocus and correspondence depth cues. We analyze the x-u 2D epipolar image (EPI), where by convention we assume the spatial x coordinate is horizontal and the angular u coordinate is vertical (our final algorithm uses the full 4D EPI). We show that defocus depth cues are obtained by computing the horizontal (spatial) variance after vertical (angular) integration, and correspondence depth cues by computing the vertical (angular) variance. We then show how to combine the two cues into a high quality depth map, suitable for computer vision applications such as matting, full control of depth-of-field, and surface reconstruction.</p><p>3 0.16735449 <a title="271-tfidf-3" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<p>Author: Salil Tambe, Ashok Veeraraghavan, Amit Agrawal</p><p>Abstract: Current Light Field (LF) cameras offer fixed resolution in space, time and angle which is decided a-priori and is independent of the scene. These cameras either trade-off spatial resolution to capture single-shot LF [20, 27, 12] or tradeoff temporal resolution by assuming a static scene to capture high spatial resolution LF [18, 3]. Thus, capturing high spatial resolution LF video for dynamic scenes remains an open and challenging problem. We present the concept, design and implementation of a LF video camera that allows capturing high resolution LF video. The spatial, angular and temporal resolution are not fixed a-priori and we exploit the scene-specific redundancy in space, time and angle. Our reconstruction is motion-aware and offers a continuum of resolution tradeoff with increasing motion in the scene. The key idea is (a) to design efficient multiplexing matrices that allow resolution tradeoffs, (b) use dictionary learning and sparse repre- sentations for robust reconstruction, and (c) perform local motion-aware adaptive reconstruction. We perform extensive analysis and characterize the performance of our motion-aware reconstruction algorithm. We show realistic simulations using a graphics simulator as well as real results using a LCoS based programmable camera. We demonstrate novel results such as high resolution digital refocusing for dynamic moving objects.</p><p>4 0.13729203 <a title="271-tfidf-4" href="./iccv-2013-Line_Assisted_Light_Field_Triangulation_and_Stereo_Matching.html">252 iccv-2013-Line Assisted Light Field Triangulation and Stereo Matching</a></p>
<p>Author: Zhan Yu, Xinqing Guo, Haibing Lin, Andrew Lumsdaine, Jingyi Yu</p><p>Abstract: Light fields are image-based representations that use densely sampled rays as a scene description. In this paper, we explore geometric structures of 3D lines in ray space for improving light field triangulation and stereo matching. The triangulation problem aims to fill in the ray space with continuous and non-overlapping simplices anchored at sampled points (rays). Such a triangulation provides a piecewise-linear interpolant useful for light field superresolution. We show that the light field space is largely bilinear due to 3D line segments in the scene, and direct triangulation of these bilinear subspaces leads to large errors. We instead present a simple but effective algorithm to first map bilinear subspaces to line constraints and then apply Constrained Delaunay Triangulation (CDT). Based on our analysis, we further develop a novel line-assisted graphcut (LAGC) algorithm that effectively encodes 3D line constraints into light field stereo matching. Experiments on synthetic and real data show that both our triangulation and LAGC algorithms outperform state-of-the-art solutions in accuracy and visual quality.</p><p>5 0.12444385 <a title="271-tfidf-5" href="./iccv-2013-Structured_Light_in_Sunlight.html">405 iccv-2013-Structured Light in Sunlight</a></p>
<p>Author: Mohit Gupta, Qi Yin, Shree K. Nayar</p><p>Abstract: Strong ambient illumination severely degrades the performance of structured light based techniques. This is especially true in outdoor scenarios, where the structured light sources have to compete with sunlight, whose power is often 2-5 orders of magnitude larger than the projected light. In this paper, we propose the concept of light-concentration to overcome strong ambient illumination. Our key observation is that given a fixed light (power) budget, it is always better to allocate it sequentially in several portions of the scene, as compared to spreading it over the entire scene at once. For a desired level of accuracy, we show that by distributing light appropriately, the proposed approach requires 1-2 orders lower acquisition time than existing approaches. Our approach is illumination-adaptive as the optimal light distribution is determined based on a measurement of the ambient illumination level. Since current light sources have a fixed light distribution, we have built a prototype light source that supports flexible light distribution by controlling the scanning speed of a laser scanner. We show several high quality 3D scanning results in a wide range of outdoor scenarios. The proposed approach will benefit 3D vision systems that need to operate outdoors under extreme ambient illumination levels on a limited time and power budget.</p><p>6 0.10844391 <a title="271-tfidf-6" href="./iccv-2013-Unsupervised_Intrinsic_Calibration_from_a_Single_Frame_Using_a_%22Plumb-Line%22_Approach.html">436 iccv-2013-Unsupervised Intrinsic Calibration from a Single Frame Using a "Plumb-Line" Approach</a></p>
<p>7 0.093809552 <a title="271-tfidf-7" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>8 0.092974082 <a title="271-tfidf-8" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<p>9 0.091381706 <a title="271-tfidf-9" href="./iccv-2013-Multi-view_3D_Reconstruction_from_Uncalibrated_Radially-Symmetric_Cameras.html">280 iccv-2013-Multi-view 3D Reconstruction from Uncalibrated Radially-Symmetric Cameras</a></p>
<p>10 0.083868362 <a title="271-tfidf-10" href="./iccv-2013-Image_Guided_Depth_Upsampling_Using_Anisotropic_Total_Generalized_Variation.html">209 iccv-2013-Image Guided Depth Upsampling Using Anisotropic Total Generalized Variation</a></p>
<p>11 0.080486991 <a title="271-tfidf-11" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>12 0.071659081 <a title="271-tfidf-12" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>13 0.07147935 <a title="271-tfidf-13" href="./iccv-2013-A_Rotational_Stereo_Model_Based_on_XSlit_Imaging.html">28 iccv-2013-A Rotational Stereo Model Based on XSlit Imaging</a></p>
<p>14 0.071236789 <a title="271-tfidf-14" href="./iccv-2013-High_Quality_Shape_from_a_Single_RGB-D_Image_under_Uncalibrated_Natural_Illumination.html">199 iccv-2013-High Quality Shape from a Single RGB-D Image under Uncalibrated Natural Illumination</a></p>
<p>15 0.070712656 <a title="271-tfidf-15" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>16 0.070442796 <a title="271-tfidf-16" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>17 0.068934724 <a title="271-tfidf-17" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>18 0.06869901 <a title="271-tfidf-18" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>19 0.068322152 <a title="271-tfidf-19" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>20 0.065828986 <a title="271-tfidf-20" href="./iccv-2013-Rolling_Shutter_Stereo.html">363 iccv-2013-Rolling Shutter Stereo</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.114), (1, -0.1), (2, -0.051), (3, 0.016), (4, -0.094), (5, -0.025), (6, -0.03), (7, -0.15), (8, -0.009), (9, -0.01), (10, 0.013), (11, -0.026), (12, 0.001), (13, -0.003), (14, -0.028), (15, -0.044), (16, -0.045), (17, 0.004), (18, -0.016), (19, 0.05), (20, -0.014), (21, -0.017), (22, -0.077), (23, 0.011), (24, -0.066), (25, 0.036), (26, -0.052), (27, -0.074), (28, 0.024), (29, -0.022), (30, 0.064), (31, 0.037), (32, 0.064), (33, -0.017), (34, -0.016), (35, 0.072), (36, -0.109), (37, -0.113), (38, -0.051), (39, 0.118), (40, 0.126), (41, -0.007), (42, 0.077), (43, 0.003), (44, 0.008), (45, -0.06), (46, -0.0), (47, 0.139), (48, -0.024), (49, -0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95354348 <a title="271-lsi-1" href="./iccv-2013-Modeling_the_Calibration_Pipeline_of_the_Lytro_Camera_for_High_Quality_Light-Field_Image_Reconstruction.html">271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</a></p>
<p>Author: Donghyeon Cho, Minhaeng Lee, Sunyeong Kim, Yu-Wing Tai</p><p>Abstract: Light-field imaging systems have got much attention recently as the next generation camera model. A light-field imaging system consists of three parts: data acquisition, manipulation, and application. Given an acquisition system, it is important to understand how a light-field camera converts from its raw image to its resulting refocused image. In this paper, using the Lytro camera as an example, we describe step-by-step procedures to calibrate a raw light-field image. In particular, we are interested in knowing the spatial and angular coordinates of the micro lens array and the resampling process for image reconstruction. Since Lytro uses a hexagonal arrangement of a micro lens image, additional treatments in calibration are required. After calibration, we analyze and compare the performances of several resampling methods for image reconstruction with and without calibration. Finally, a learning based interpolation method is proposed which demonstrates a higher quality image reconstruction than previous interpolation methods including a method used in Lytro software.</p><p>2 0.84362292 <a title="271-lsi-2" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<p>Author: Salil Tambe, Ashok Veeraraghavan, Amit Agrawal</p><p>Abstract: Current Light Field (LF) cameras offer fixed resolution in space, time and angle which is decided a-priori and is independent of the scene. These cameras either trade-off spatial resolution to capture single-shot LF [20, 27, 12] or tradeoff temporal resolution by assuming a static scene to capture high spatial resolution LF [18, 3]. Thus, capturing high spatial resolution LF video for dynamic scenes remains an open and challenging problem. We present the concept, design and implementation of a LF video camera that allows capturing high resolution LF video. The spatial, angular and temporal resolution are not fixed a-priori and we exploit the scene-specific redundancy in space, time and angle. Our reconstruction is motion-aware and offers a continuum of resolution tradeoff with increasing motion in the scene. The key idea is (a) to design efficient multiplexing matrices that allow resolution tradeoffs, (b) use dictionary learning and sparse repre- sentations for robust reconstruction, and (c) perform local motion-aware adaptive reconstruction. We perform extensive analysis and characterize the performance of our motion-aware reconstruction algorithm. We show realistic simulations using a graphics simulator as well as real results using a LCoS based programmable camera. We demonstrate novel results such as high resolution digital refocusing for dynamic moving objects.</p><p>3 0.79317021 <a title="271-lsi-3" href="./iccv-2013-Structured_Light_in_Sunlight.html">405 iccv-2013-Structured Light in Sunlight</a></p>
<p>Author: Mohit Gupta, Qi Yin, Shree K. Nayar</p><p>Abstract: Strong ambient illumination severely degrades the performance of structured light based techniques. This is especially true in outdoor scenarios, where the structured light sources have to compete with sunlight, whose power is often 2-5 orders of magnitude larger than the projected light. In this paper, we propose the concept of light-concentration to overcome strong ambient illumination. Our key observation is that given a fixed light (power) budget, it is always better to allocate it sequentially in several portions of the scene, as compared to spreading it over the entire scene at once. For a desired level of accuracy, we show that by distributing light appropriately, the proposed approach requires 1-2 orders lower acquisition time than existing approaches. Our approach is illumination-adaptive as the optimal light distribution is determined based on a measurement of the ambient illumination level. Since current light sources have a fixed light distribution, we have built a prototype light source that supports flexible light distribution by controlling the scanning speed of a laser scanner. We show several high quality 3D scanning results in a wide range of outdoor scenarios. The proposed approach will benefit 3D vision systems that need to operate outdoors under extreme ambient illumination levels on a limited time and power budget.</p><p>4 0.66677552 <a title="271-lsi-4" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<p>Author: Nicolas Martin, Vincent Couture, Sébastien Roy</p><p>Abstract: We present a scanning method that recovers dense subpixel camera-projector correspondence without requiring any photometric calibration nor preliminary knowledge of their relative geometry. Subpixel accuracy is achieved by considering several zero-crossings defined by the difference between pairs of unstructured patterns. We use gray-level band-pass white noise patterns that increase robustness to indirect lighting and scene discontinuities. Simulated and experimental results show that our method recovers scene geometry with high subpixel precision, and that it can handle many challenges of active reconstruction systems. We compare our results to state of the art methods such as micro phase shifting and modulated phase shifting.</p><p>5 0.64345264 <a title="271-lsi-5" href="./iccv-2013-Line_Assisted_Light_Field_Triangulation_and_Stereo_Matching.html">252 iccv-2013-Line Assisted Light Field Triangulation and Stereo Matching</a></p>
<p>Author: Zhan Yu, Xinqing Guo, Haibing Lin, Andrew Lumsdaine, Jingyi Yu</p><p>Abstract: Light fields are image-based representations that use densely sampled rays as a scene description. In this paper, we explore geometric structures of 3D lines in ray space for improving light field triangulation and stereo matching. The triangulation problem aims to fill in the ray space with continuous and non-overlapping simplices anchored at sampled points (rays). Such a triangulation provides a piecewise-linear interpolant useful for light field superresolution. We show that the light field space is largely bilinear due to 3D line segments in the scene, and direct triangulation of these bilinear subspaces leads to large errors. We instead present a simple but effective algorithm to first map bilinear subspaces to line constraints and then apply Constrained Delaunay Triangulation (CDT). Based on our analysis, we further develop a novel line-assisted graphcut (LAGC) algorithm that effectively encodes 3D line constraints into light field stereo matching. Experiments on synthetic and real data show that both our triangulation and LAGC algorithms outperform state-of-the-art solutions in accuracy and visual quality.</p><p>6 0.6268034 <a title="271-lsi-6" href="./iccv-2013-Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illumination_in_the_Spectral_Domain.html">385 iccv-2013-Separating Reflective and Fluorescent Components Using High Frequency Illumination in the Spectral Domain</a></p>
<p>7 0.55854672 <a title="271-lsi-7" href="./iccv-2013-Illuminant_Chromaticity_from_Image_Sequences.html">207 iccv-2013-Illuminant Chromaticity from Image Sequences</a></p>
<p>8 0.53948104 <a title="271-lsi-8" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>9 0.51471812 <a title="271-lsi-9" href="./iccv-2013-Efficient_Image_Dehazing_with_Boundary_Constraint_and_Contextual_Regularization.html">135 iccv-2013-Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</a></p>
<p>10 0.50436246 <a title="271-lsi-10" href="./iccv-2013-Depth_from_Combining_Defocus_and_Correspondence_Using_Light-Field_Cameras.html">108 iccv-2013-Depth from Combining Defocus and Correspondence Using Light-Field Cameras</a></p>
<p>11 0.5023244 <a title="271-lsi-11" href="./iccv-2013-A_Color_Constancy_Model_with_Double-Opponency_Mechanisms.html">5 iccv-2013-A Color Constancy Model with Double-Opponency Mechanisms</a></p>
<p>12 0.47248128 <a title="271-lsi-12" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>13 0.4702642 <a title="271-lsi-13" href="./iccv-2013-A_Rotational_Stereo_Model_Based_on_XSlit_Imaging.html">28 iccv-2013-A Rotational Stereo Model Based on XSlit Imaging</a></p>
<p>14 0.44795808 <a title="271-lsi-14" href="./iccv-2013-Refractive_Structure-from-Motion_on_Underwater_Images.html">348 iccv-2013-Refractive Structure-from-Motion on Underwater Images</a></p>
<p>15 0.44432098 <a title="271-lsi-15" href="./iccv-2013-Image_Guided_Depth_Upsampling_Using_Anisotropic_Total_Generalized_Variation.html">209 iccv-2013-Image Guided Depth Upsampling Using Anisotropic Total Generalized Variation</a></p>
<p>16 0.43594882 <a title="271-lsi-16" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>17 0.42833427 <a title="271-lsi-17" href="./iccv-2013-Toward_Guaranteed_Illumination_Models_for_Non-convex_Objects.html">422 iccv-2013-Toward Guaranteed Illumination Models for Non-convex Objects</a></p>
<p>18 0.42546907 <a title="271-lsi-18" href="./iccv-2013-Local_Signal_Equalization_for_Correspondence_Matching.html">255 iccv-2013-Local Signal Equalization for Correspondence Matching</a></p>
<p>19 0.41977274 <a title="271-lsi-19" href="./iccv-2013-Extrinsic_Camera_Calibration_without_a_Direct_View_Using_Spherical_Mirror.html">152 iccv-2013-Extrinsic Camera Calibration without a Direct View Using Spherical Mirror</a></p>
<p>20 0.41642332 <a title="271-lsi-20" href="./iccv-2013-A_Joint_Intensity_and_Depth_Co-sparse_Analysis_Model_for_Depth_Map_Super-resolution.html">18 iccv-2013-A Joint Intensity and Depth Co-sparse Analysis Model for Depth Map Super-resolution</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.029), (7, 0.015), (26, 0.105), (27, 0.011), (31, 0.046), (42, 0.069), (64, 0.017), (73, 0.027), (78, 0.012), (89, 0.181), (98, 0.372)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77862561 <a title="271-lda-1" href="./iccv-2013-Modeling_the_Calibration_Pipeline_of_the_Lytro_Camera_for_High_Quality_Light-Field_Image_Reconstruction.html">271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</a></p>
<p>Author: Donghyeon Cho, Minhaeng Lee, Sunyeong Kim, Yu-Wing Tai</p><p>Abstract: Light-field imaging systems have got much attention recently as the next generation camera model. A light-field imaging system consists of three parts: data acquisition, manipulation, and application. Given an acquisition system, it is important to understand how a light-field camera converts from its raw image to its resulting refocused image. In this paper, using the Lytro camera as an example, we describe step-by-step procedures to calibrate a raw light-field image. In particular, we are interested in knowing the spatial and angular coordinates of the micro lens array and the resampling process for image reconstruction. Since Lytro uses a hexagonal arrangement of a micro lens image, additional treatments in calibration are required. After calibration, we analyze and compare the performances of several resampling methods for image reconstruction with and without calibration. Finally, a learning based interpolation method is proposed which demonstrates a higher quality image reconstruction than previous interpolation methods including a method used in Lytro software.</p><p>2 0.77388853 <a title="271-lda-2" href="./iccv-2013-Unsupervised_Domain_Adaptation_by_Domain_Invariant_Projection.html">435 iccv-2013-Unsupervised Domain Adaptation by Domain Invariant Projection</a></p>
<p>Author: Mahsa Baktashmotlagh, Mehrtash T. Harandi, Brian C. Lovell, Mathieu Salzmann</p><p>Abstract: Domain-invariant representations are key to addressing the domain shift problem where the training and test examples follow different distributions. Existing techniques that have attempted to match the distributions of the source and target domains typically compare these distributions in the original feature space. This space, however, may not be directly suitable for such a comparison, since some of the features may have been distorted by the domain shift, or may be domain specific. In this paper, we introduce a Domain Invariant Projection approach: An unsupervised domain adaptation method that overcomes this issue by extracting the information that is invariant across the source and target domains. More specifically, we learn a projection of the data to a low-dimensional latent space where the distance between the empirical distributions of the source and target examples is minimized. We demonstrate the effectiveness of our approach on the task of visual object recognition and show that it outperforms state-of-the-art methods on a standard domain adaptation benchmark dataset.</p><p>3 0.75689173 <a title="271-lda-3" href="./iccv-2013-Unifying_Nuclear_Norm_and_Bilinear_Factorization_Approaches_for_Low-Rank_Matrix_Decomposition.html">434 iccv-2013-Unifying Nuclear Norm and Bilinear Factorization Approaches for Low-Rank Matrix Decomposition</a></p>
<p>Author: Ricardo Cabral, Fernando De_La_Torre, João P. Costeira, Alexandre Bernardino</p><p>Abstract: Low rank models have been widely usedfor the representation of shape, appearance or motion in computer vision problems. Traditional approaches to fit low rank models make use of an explicit bilinear factorization. These approaches benefit from fast numerical methods for optimization and easy kernelization. However, they suffer from serious local minima problems depending on the loss function and the amount/type of missing data. Recently, these lowrank models have alternatively been formulated as convex problems using the nuclear norm regularizer; unlike factorization methods, their numerical solvers are slow and it is unclear how to kernelize them or to impose a rank a priori. This paper proposes a unified approach to bilinear factorization and nuclear norm regularization, that inherits the benefits of both. We analyze the conditions under which these approaches are equivalent. Moreover, based on this analysis, we propose a new optimization algorithm and a “rank continuation ” strategy that outperform state-of-theart approaches for Robust PCA, Structure from Motion and Photometric Stereo with outliers and missing data.</p><p>4 0.7545042 <a title="271-lda-4" href="./iccv-2013-Unbiased_Metric_Learning%3A_On_the_Utilization_of_Multiple_Datasets_and_Web_Images_for_Softening_Bias.html">431 iccv-2013-Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias</a></p>
<p>Author: Chen Fang, Ye Xu, Daniel N. Rockmore</p><p>Abstract: Many standard computer vision datasets exhibit biases due to a variety of sources including illumination condition, imaging system, and preference of dataset collectors. Biases like these can have downstream effects in the use of vision datasets in the construction of generalizable techniques, especially for the goal of the creation of a classification system capable of generalizing to unseen and novel datasets. In this work we propose Unbiased Metric Learning (UML), a metric learning approach, to achieve this goal. UML operates in the following two steps: (1) By varying hyperparameters, it learns a set of less biased candidate distance metrics on training examples from multiple biased datasets. The key idea is to learn a neighborhood for each example, which consists of not only examples of the same category from the same dataset, but those from other datasets. The learning framework is based on structural SVM. (2) We do model validation on a set of weakly-labeled web images retrieved by issuing class labels as keywords to search engine. The metric with best validationperformance is selected. Although the web images sometimes have noisy labels, they often tend to be less biased, which makes them suitable for the validation set in our task. Cross-dataset image classification experiments are carried out. Results show significant performance improvement on four well-known computer vision datasets.</p><p>5 0.74949408 <a title="271-lda-5" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>Author: Bryan Klingner, David Martin, James Roseborough</p><p>Abstract: We describe a structure-from-motion framework that handles “generalized” cameras, such as moving rollingshutter cameras, and works at an unprecedented scale— billions of images covering millions of linear kilometers of roads—by exploiting a good relative pose prior along vehicle paths. We exhibit a planet-scale, appearanceaugmented point cloud constructed with our framework and demonstrate its practical use in correcting the pose of a street-level image collection.</p><p>6 0.7476685 <a title="271-lda-6" href="./iccv-2013-A_Learning-Based_Approach_to_Reduce_JPEG_Artifacts_in_Image_Matting.html">19 iccv-2013-A Learning-Based Approach to Reduce JPEG Artifacts in Image Matting</a></p>
<p>7 0.71664917 <a title="271-lda-7" href="./iccv-2013-A_Unified_Video_Segmentation_Benchmark%3A_Annotation%2C_Metrics_and_Analysis.html">33 iccv-2013-A Unified Video Segmentation Benchmark: Annotation, Metrics and Analysis</a></p>
<p>8 0.61743665 <a title="271-lda-8" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<p>9 0.61190426 <a title="271-lda-9" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>10 0.60443509 <a title="271-lda-10" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>11 0.59766394 <a title="271-lda-11" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>12 0.59343642 <a title="271-lda-12" href="./iccv-2013-Unsupervised_Visual_Domain_Adaptation_Using_Subspace_Alignment.html">438 iccv-2013-Unsupervised Visual Domain Adaptation Using Subspace Alignment</a></p>
<p>13 0.56693345 <a title="271-lda-13" href="./iccv-2013-Geometric_Registration_Based_on_Distortion_Estimation.html">183 iccv-2013-Geometric Registration Based on Distortion Estimation</a></p>
<p>14 0.5631249 <a title="271-lda-14" href="./iccv-2013-Joint_Learning_of_Discriminative_Prototypes_and_Large_Margin_Nearest_Neighbor_Classifiers.html">222 iccv-2013-Joint Learning of Discriminative Prototypes and Large Margin Nearest Neighbor Classifiers</a></p>
<p>15 0.56008685 <a title="271-lda-15" href="./iccv-2013-Line_Assisted_Light_Field_Triangulation_and_Stereo_Matching.html">252 iccv-2013-Line Assisted Light Field Triangulation and Stereo Matching</a></p>
<p>16 0.55462664 <a title="271-lda-16" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<p>17 0.55405718 <a title="271-lda-17" href="./iccv-2013-Rolling_Shutter_Stereo.html">363 iccv-2013-Rolling Shutter Stereo</a></p>
<p>18 0.5537194 <a title="271-lda-18" href="./iccv-2013-Bayesian_Robust_Matrix_Factorization_for_Image_and_Video_Processing.html">60 iccv-2013-Bayesian Robust Matrix Factorization for Image and Video Processing</a></p>
<p>19 0.5537141 <a title="271-lda-19" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>20 0.55313045 <a title="271-lda-20" href="./iccv-2013-Depth_from_Combining_Defocus_and_Correspondence_Using_Light-Field_Cameras.html">108 iccv-2013-Depth from Combining Defocus and Correspondence Using Light-Field Cameras</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
