<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-179" href="#">iccv2013-179</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</h1>
<br/><p>Source: <a title="iccv-2013-179-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Lan_From_Subcategories_to_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Tian Lan, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: The appearance of an object changes profoundly with pose, camera view and interactions of the object with other objects in the scene. This makes it challenging to learn detectors based on an object-level label (e.g., “car”). We postulate that having a richer set oflabelings (at different levels of granularity) for an object, including finer-grained subcategories, consistent in appearance and view, and higherorder composites – contextual groupings of objects consistent in their spatial layout and appearance, can significantly alleviate these problems. However, obtaining such a rich set of annotations, including annotation of an exponentially growing set of object groupings, is simply not feasible. We propose a weakly-supervised framework for object detection where we discover subcategories and the composites automatically with only traditional object-level category labels as input. To this end, we first propose an exemplar-SVM-based clustering approach, with latent SVM refinement, that discovers a variable length set of discriminative subcategories for each object class. We then develop a structured model for object detection that captures interactions among object subcategories and automatically discovers semantically meaningful and discriminatively relevant visual composites. We show that this model produces state-of-the-art performance on UIUC phrase object detection benchmark.</p><p>Reference: <a title="iccv-2013-179-reference" href="../iccv2013_reference/iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We propose a weakly-supervised framework for object detection where we discover subcategories and the composites automatically with only traditional object-level category labels as input. [sent-7, score-1.177]
</p><p>2 To this end, we first propose an exemplar-SVM-based clustering approach, with latent SVM refinement, that discovers a variable length set of discriminative subcategories for each object class. [sent-8, score-0.645]
</p><p>3 We then develop a structured model for object detection that captures  interactions among object subcategories and automatically discovers semantically meaningful and discriminatively relevant visual composites. [sent-9, score-0.884]
</p><p>4 We show that this model produces state-of-the-art performance on UIUC phrase object detection benchmark. [sent-10, score-0.388]
</p><p>5 In this paper, we detect basic-level objects from images, and simultaneously discover the unobserved labeling space that includes the low-level fine-grained subcategory labels and the high-level visual composite labels. [sent-26, score-0.697]
</p><p>6 Anoveri wof urdet cionpi eln :Inthe rain g  phase (top row), we first discover the object subcategories from each basic-level category, then we train subcategory classifiers in the latent SVM framework. [sent-35, score-1.103]
</p><p>7 The relational model takes the outputs of these subcategory classifiers as input, and carries out reasoning on top of these responses from a structured perspective. [sent-36, score-0.513]
</p><p>8 In the test phase (bottom row), we run the subcategory detectors to generate a set of bounding boxes. [sent-37, score-0.5]
</p><p>9 , person riding a horse or  lying on a sofa has very different appearance from a generic person walking). [sent-46, score-0.702]
</p><p>10 Considering the difficulties in obtain such labelings, we advocate a weakly supervised setting where only the basic-level categories are provided in training, and the fine-grained subcategories as well as the high-level visual composites are automatically discovered from the training data. [sent-48, score-1.1]
</p><p>11 The semantic labelings consist of lower-level object subcategories as well as higher-level visual contextual composites, modeling relationships that the object has with other objects in the scene. [sent-50, score-1.059]
</p><p>12 Object subcategories are visual clusters that capture a wide range of appearance variations of an object. [sent-51, score-0.597]
</p><p>13 We propose an exemplar-SVM based clustering algorithm to discover the subcategory labels. [sent-52, score-0.456]
</p><p>14 The subcategories are then treated as mixture components in a latent SVM framework, and refined during learning. [sent-53, score-0.557]
</p><p>15 Our model detects possibly multiple object instances in a single image and generates detailed fine-grained subcategory labels for each instance. [sent-54, score-0.5]
</p><p>16 Related Work: Our framework relates to both multicomponent object detection and, recently introduced, contextual visual composite models. [sent-62, score-0.448]
</p><p>17 These labelings contain both lower-level subcategories and higherlevel visual composites. [sent-71, score-0.592]
</p><p>18 3) We develop a structured model for object detection that captures interactions among object subcategories and automatically discovers discriminative visual composites. [sent-73, score-0.862]
</p><p>19 We also  show that our discovered visual composites are semantically meaningful. [sent-74, score-0.598]
</p><p>20 In this work, we use exemplar SVM to discover an initial pool of subcategories which we then refine, through merging, and train part-based  models for each resulting subcategory. [sent-88, score-0.585]
</p><p>21 Bourdev [1] introduces poselets for person detection, but poselets encode visual composites of parts rather than global objects and also require keypoint annotation. [sent-92, score-0.734]
</p><p>22 The core difference between our approach and these methods is that we model interactions among object subcategories rather than the basic-level object categories. [sent-94, score-0.655]
</p><p>23 For example, when a person partakes in an interaction with a bicycle, when riding, the appearance of both the bicycle and the person exhibit viewconsistent appearance changes (e. [sent-96, score-0.536]
</p><p>24 [15] manually annotate a list of visual phrases and train global phrase templates for detection. [sent-102, score-0.512]
</p><p>25 In [13], higher-order visual composites are automatically discovered based on the spatial/scale/aspect consistency of –  objects. [sent-103, score-0.609]
</p><p>26 In contrast, we use subcategories and spatial relations to reason about object interactions. [sent-106, score-0.539]
</p><p>27 The two key requirements for good object subcategories are: (1) inclusivity – subcategories should cover all, or most, variations in object appearance and (2) discriminability – subcategories should be useful for detecting the class. [sent-111, score-1.563]
</p><p>28 We argue that the number of subcategories per object class should be driven by the appearance variations within that class, not a fixed global parameter. [sent-115, score-0.583]
</p><p>29 Note that, due to our discriminative training strategy, objects within each subcategory are highly consistent in appearance. [sent-150, score-0.478]
</p><p>30 Learning Subcategories Given the set of subcategories obtained from the previous section, we learn a mixture model based on DPM [7], where the mixture components correspond to subcategories 371  stances that are tightly clustered in appearance space. [sent-152, score-1.023]
</p><p>31 The figure shows the discovered subcategories for some of the basic-level object categories including horse, person, bicycle, car, bottle and dog. [sent-153, score-0.776]
</p><p>32 Mixture models: It is straightforward to train DPM for each subcategory independently [16]. [sent-177, score-0.414]
</p><p>33 In addition, subcategories discovered in the previous section might be noisy and should be cleaned up during learning. [sent-179, score-0.529]
</p><p>34 In this work, we train the subcategory classifiers in the latent SVM framework, where the training of the classifiers are coupled and the subcategory labels are refined in the latent step. [sent-180, score-1.042]
</p><p>35 The subcategory labels in our method correspond to the mixture components in DPM. [sent-181, score-0.48]
</p><p>36 Due to the non-convex nature of latent SVM, initialization of subcategories is a key step of learning a good detector. [sent-184, score-0.495]
</p><p>37 Here we naturally use the subcategories discovered in the last section to initialize the mixture components, and allow the subcategory labels to refine during the latent step. [sent-185, score-1.073]
</p><p>38 The output of this step is a set of candidate windows where each  window is associated with a fine-grained subcategory label. [sent-187, score-0.438]
</p><p>39 Unlike these methods, however, we propose to build contextual basic-level category models based on the subcategory classifiers (not aggregate object detections, as in [13], or composed templates [15]). [sent-191, score-0.761]
</p><p>40 This allows our model to be attuned to visual and view-based correlations between subcategories of objects. [sent-192, score-0.484]
</p><p>41 However, instead of treating all objects in the same image as context, we introduce binary latent variables to discriminatively select which objects have strong interactions with the central object and should be included in our model. [sent-195, score-0.467]
</p><p>42 We call the star graph that includes the candidate central object and the contextual objects a visual composite. [sent-196, score-0.497]
</p><p>43 At the end of the inference, our goal is to obtain a rich set of visual composites that are not only highly characteristic ofthe object class, but also highly discriminative compared to other classes. [sent-197, score-0.617]
</p><p>44 c,tKor} t bhaet select the i-th bounding box, where K is the total number of subcategories in our dataset. [sent-223, score-0.523]
</p><p>45 An object hypothesis is represented by a star graph, which specifies an object bounding box and a set of bounding boxes of contextual objects. [sent-224, score-0.66]
</p><p>46 Finding Candidate Visual Composites In training, we discover a set of discriminative visual composites automatically. [sent-231, score-0.573]
</p><p>47 We define visual composites as consisting of two or more objects. [sent-232, score-0.473]
</p><p>48 Our goal is to discover such composites that exhibit consistently occurring object layout patterns in a set of images. [sent-235, score-0.631]
</p><p>49 The key insight is that based on our definition, contextual objects windows (leaves of the graph) should all have consistent layout with the central object window; in other words, contextual objects should be able to consistently predict a bounding box for the central object under consideration. [sent-237, score-1.04]
</p><p>50 With that as an insight, at training time, we first learn spatial layout relationships between potential contextual objects and central object. [sent-238, score-0.441]
</p><p>51 Based on these learned relations an initial graph of a visual composite is constructed (by only considering object detections that are consistent in predicting considered central object’s bounding box). [sent-239, score-0.402]
</p><p>52 Notably, we can easily produce a hypothesis for a bounding box of a central object by conditioning the learned mixture model on the bounding box of a contextual object. [sent-242, score-0.704]
</p><p>53 Given an image, we can use this model to determine the set of possible contextual objects for each central object window. [sent-243, score-0.404]
</p><p>54 Given a central object window we consider contextual objects to be windows that, given a learned spatial Gaussian mixture model, can predict the central object window to > 0. [sent-244, score-0.741]
</p><p>55 5 overlap to the true object annotation), and in this way obtain the visual composites (central object + contextual objects) that have tight spatial configuration coupling. [sent-247, score-0.875]
</p><p>56 If we naively include all spatially consistent detection windows as contextual objects for a given candidate central object, we may include many false positives and thus hurt the performance. [sent-249, score-0.393]
</p><p>57 Thus during inference, we introduce a binary latent variable for each candidate contextual window to discriminatively select if it will be included in our composite object model. [sent-250, score-0.465]
</p><p>58 For an object window i, we use hi to denote the binary latent variables for all contextual objects with indices in the set Li. [sent-251, score-0.484]
</p><p>59 ixi · yc,i: We simply use the output of the subcategory detecto·r as the single feature. [sent-264, score-0.412]
</p><p>60 αpi is the two-dimensional  weight that corresponds to the subcategory class of the i-th bounding box. [sent-266, score-0.477]
</p><p>61 jdij · hij: We write dij = [xj, gij] to represent the objects in context, where xj is the appearance feature (detection score) of the j-th bounding box and gij is the spatial feature computed based on the relative position and scale of the j-th bounding box w. [sent-269, score-0.489]
</p><p>62 hij is a binary latent variable that determines whether the contextual object is discriminative and should be included into the context model. [sent-273, score-0.437]
</p><p>63 ipjxj ·hij: This term captures the “prior” over subcategory combi·nhations. [sent-275, score-0.42]
</p><p>64 The intuition is that certain pairs of subcategories tend to co-occur while others do not, for example, a bicycle with side view tends to cooccur with a rider with the same viewpoint, and a horse tends to co-occur with a horse rider instead of a person walking. [sent-276, score-1.322]
</p><p>65 box i, the inference is on a star graph where we jointly infer the presence or absence of the c-th category yc,i as well as the corresponding binary latent variables hi of the contextual objects. [sent-284, score-0.487]
</p><p>66 We emphasize that our inference procedure returns both object labels and the visual composites that tells the closely related contextual objects for each object window. [sent-287, score-0.934]
</p><p>67 Subcategory Templates: Examples of learned three object categories: bicycle, horse and person (one category per row). [sent-289, score-0.403]
</p><p>68 For object categories, bicycle, bottle, car, chair, dog, horse, person and sofa, the number of discovered subcategories (through affinity propagation) for our mixture models are: 13, 6, 15, 6, 13, 12, 59 and 13 respectively. [sent-291, score-0.852]
</p><p>69 We note that during training the contextual binary variables hi are observable based on the ground truth visual composites (see Section 5. [sent-320, score-0.717]
</p><p>70 Experiments We present results of object detection on a standard object benchmark dataset: UIUC phrase dataset [15]. [sent-324, score-0.47]
</p><p>71 The UIUC phrase dataset contains 2796 images which consist of a subset of PASCAL images and images for phrases collected from the web. [sent-325, score-0.405]
</p><p>72 The images are labeled with 8 of the 20 PASCAL categories, and a list of 17 visual phrases such as person riding bicycle, dog lying on sofa, etc. [sent-326, score-0.522]
</p><p>73 We picked UIUC phrase because it contains a rich set of visual composites (phrases plus a few higher-order composites such as person drinking bottle sitting in a chair). [sent-332, score-1.529]
</p><p>74 We compare our results to state-of-the-art performance results of [13, 15], as well as the detection performance of our subcategory classifiers. [sent-335, score-0.433]
</p><p>75 To fairly compare with the reported results, we use the same version of deformable part models to train our subcategory classifiers [6]. [sent-341, score-0.488]
</p><p>76 The trained subcategory classifiers are highly discriminative and address intra-class variations among basiclevel categories. [sent-350, score-0.488]
</p><p>77 This is demonstrated by the performance gain of subcategory classifiers in Table 2. [sent-351, score-0.426]
</p><p>78 Some of the learned subcategory templates are visualized in Figure 4. [sent-352, score-0.41]
</p><p>79 2) Modeling visual composites improve performance for some hard-to-detect object categories such as bottle and chair. [sent-353, score-0.72]
</p><p>80 We think the poor performance of classifying bottle and chair by our subcategory classifiers is mainly due to over-fitting. [sent-354, score-0.562]
</p><p>81 We have fewer training examples for these two classes, and further dividing these examples into subcategories will easily over-fit the data. [sent-355, score-0.431]
</p><p>82 A detailed discussion of visual composites is provided in the next section. [sent-371, score-0.473]
</p><p>83 Compared to phrases, higher order composites can be more effective because multiple confident contextual object responses in tight spatial layout with the single central object detection tend to be more reliable and indicative. [sent-378, score-1.076]
</p><p>84 6 (top right) shows a composite of person riding bicycle with a car nearby, where the confident responses of car and person will jointly boost the detection of the bicycle. [sent-380, score-0.86]
</p><p>85 Visual phrase retrieval: Since the UIUC phrase dataset is annotated with visual phrases, we wish to evaluate how well our method performs in applications designed for those  phrases even without phrase annotations during training. [sent-381, score-1.009]
</p><p>86 However, our method is not directly applicable to visual phrase detection as in [15], since the output of our method are bounding boxes of objects instead of visual phrases. [sent-382, score-0.607]
</p><p>87 UIUC phrase contains annotations of 12 visual phrases that describe interactions between two objects (e. [sent-385, score-0.617]
</p><p>88 We compare our method with deformable part models [7] and our subcategory detectors. [sent-389, score-0.418]
</p><p>89 Then a testing image will be scored by the sum of the maximum person detector score and maximum bicycle detector score on this image. [sent-402, score-0.397]
</p><p>90 Note that our method does not require any visual phrase annotations, and is trained purely on basic-level object categories, but can still reliably retrieve a list of visual phrases: “Per-  son riding bicycle”, “Person riding horse” and “Horse and rider jumping”. [sent-406, score-0.861]
</p><p>91 We believe this is because our method automatically discovers subcategories corresponding to “horse rider” and “jumping horse” and learns discriminative subcategory templates. [sent-410, score-0.922]
</p><p>92 Our full model also further improves the results of subcategory classifiers, particularly in the phrases with tight spatial configurations, such as “Person sitting on sofa” and “Horse and rider jumping”. [sent-411, score-0.813]
</p><p>93 We believe this is because our subcategory classifiers for bottle over-fit (see Section 6. [sent-413, score-0.531]
</p><p>94 Objectandvisualcompositedet ction:Thecentral  objects, object we want to re-score, are shown in red (only the top 3 detection responses are visualized), and the automatically discovered contextual objects in blue; green rectangles label the visual composites. [sent-415, score-0.591]
</p><p>95 For each detection response, we also show the confidence score before and after applying our relational model denoted by s : t, where s is the output of subcategory detector and t is the output of our full model. [sent-416, score-0.524]
</p><p>96 Conclusion In this paper, we propose a multi-level framework for  detecting and labeling objects with basic object-level categories and multiple automatically discovered semantic labelings including the fine-grained subcategories as well as the high-level visual composites. [sent-424, score-0.882]
</p><p>97 Our experiments on the UIUC phrase dataset show that the proposed method outperforms multiple state-of-the-art methods in object detection, and the automatically discovered visual composites are semantically meaningful. [sent-426, score-0.976]
</p><p>98 We further show that our method can be applied to retrieve images with visual phrase queries even without visual phrase annotations during training, with significant improvement over baselines. [sent-427, score-0.657]
</p><p>99 As can be seen, composites are often are semantically meaningful. [sent-430, score-0.447]
</p><p>100 Besides visual phrases, we also  discover higher-order composites such as: person-bicycle-car and person-bicycle-person (see examples of the second row). [sent-431, score-0.544]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('subcategories', 0.431), ('composites', 0.42), ('subcategory', 0.385), ('phrase', 0.258), ('bicycle', 0.18), ('contextual', 0.174), ('rider', 0.159), ('phrases', 0.147), ('person', 0.141), ('horse', 0.126), ('uiuc', 0.125), ('riding', 0.117), ('labelings', 0.108), ('bottle', 0.105), ('sofa', 0.099), ('discovered', 0.098), ('bounding', 0.092), ('composite', 0.091), ('central', 0.084), ('object', 0.082), ('discover', 0.071), ('objects', 0.064), ('latent', 0.064), ('mixture', 0.062), ('hij', 0.06), ('interactions', 0.06), ('categories', 0.06), ('box', 0.059), ('sitting', 0.058), ('layout', 0.058), ('category', 0.054), ('exemplar', 0.054), ('visual', 0.053), ('relational', 0.053), ('dpm', 0.053), ('detection', 0.048), ('hi', 0.045), ('jumping', 0.044), ('clusters', 0.043), ('drinking', 0.041), ('classifiers', 0.041), ('lying', 0.041), ('star', 0.04), ('boxes', 0.039), ('car', 0.039), ('discovers', 0.039), ('tight', 0.038), ('affinity', 0.038), ('automatically', 0.038), ('response', 0.038), ('detector', 0.038), ('appearance', 0.037), ('discriminability', 0.036), ('cluster', 0.035), ('annotations', 0.035), ('ipjxj', 0.035), ('jdij', 0.035), ('posites', 0.035), ('santosh', 0.035), ('relationships', 0.035), ('responses', 0.034), ('variations', 0.033), ('labels', 0.033), ('rich', 0.033), ('deformable', 0.033), ('profoundly', 0.031), ('alexei', 0.031), ('template', 0.031), ('desai', 0.031), ('svm', 0.031), ('chair', 0.031), ('confident', 0.03), ('semantic', 0.03), ('window', 0.03), ('propagation', 0.03), ('train', 0.029), ('atomic', 0.029), ('notably', 0.029), ('gu', 0.029), ('discriminative', 0.029), ('poselets', 0.028), ('context', 0.028), ('ixi', 0.027), ('mog', 0.027), ('semantically', 0.027), ('pascal', 0.027), ('interacting', 0.027), ('yn', 0.027), ('inference', 0.026), ('spatial', 0.026), ('templates', 0.025), ('groupings', 0.025), ('gij', 0.025), ('variables', 0.025), ('discriminatively', 0.024), ('detectors', 0.023), ('viewpoint', 0.023), ('dog', 0.023), ('windows', 0.023), ('son', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="179-tfidf-1" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<p>Author: Tian Lan, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: The appearance of an object changes profoundly with pose, camera view and interactions of the object with other objects in the scene. This makes it challenging to learn detectors based on an object-level label (e.g., “car”). We postulate that having a richer set oflabelings (at different levels of granularity) for an object, including finer-grained subcategories, consistent in appearance and view, and higherorder composites – contextual groupings of objects consistent in their spatial layout and appearance, can significantly alleviate these problems. However, obtaining such a rich set of annotations, including annotation of an exponentially growing set of object groupings, is simply not feasible. We propose a weakly-supervised framework for object detection where we discover subcategories and the composites automatically with only traditional object-level category labels as input. To this end, we first propose an exemplar-SVM-based clustering approach, with latent SVM refinement, that discovers a variable length set of discriminative subcategories for each object class. We then develop a structured model for object detection that captures interactions among object subcategories and automatically discovers semantically meaningful and discriminatively relevant visual composites. We show that this model produces state-of-the-art performance on UIUC phrase object detection benchmark.</p><p>2 0.32067657 <a title="179-tfidf-2" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>Author: Jiongxin Liu, Peter N. Belhumeur</p><p>Abstract: In this paper, we propose a novel approach for bird part localization, targeting fine-grained categories with wide variations in appearance due to different poses (including aspect and orientation) and subcategories. As it is challenging to represent such variations across a large set of diverse samples with tractable parametric models, we turn to individual exemplars. Specifically, we extend the exemplarbased models in [4] by enforcing pose and subcategory consistency at the parts. During training, we build posespecific detectors scoring part poses across subcategories, and subcategory-specific detectors scoring part appearance across poses. At the testing stage, likely exemplars are matched to the image, suggesting part locations whose pose and subcategory consistency are well-supported by the image cues. From these hypotheses, part configuration can be predicted with very high accuracy. Experimental results demonstrate significantperformance gainsfrom our method on an extensive dataset: CUB-200-2011 [30], for both localization and classification tasks.</p><p>3 0.23985794 <a title="179-tfidf-3" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>Author: Limin Wang, Yu Qiao, Xiaoou Tang</p><p>Abstract: This paper proposes motion atom and phrase as a midlevel temporal “part” for representing and classifying complex action. Motion atom is defined as an atomic part of action, and captures the motion information of action video in a short temporal scale. Motion phrase is a temporal composite of multiple motion atoms with an AND/OR structure, which further enhances the discriminative ability of motion atoms by incorporating temporal constraints in a longer scale. Specifically, given a set of weakly labeled action videos, we firstly design a discriminative clustering method to automatically discovera set ofrepresentative motion atoms. Then, based on these motion atoms, we mine effective motion phrases with high discriminative and representativepower. We introduce a bottom-upphrase construction algorithm and a greedy selection method for this mining task. We examine the classification performance of the motion atom and phrase based representation on two complex action datasets: Olympic Sports and UCF50. Experimental results show that our method achieves superior performance over recent published methods on both datasets.</p><p>4 0.18184453 <a title="179-tfidf-4" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>Author: Arash Vahdat, Kevin Cannons, Greg Mori, Sangmin Oh, Ilseo Kim</p><p>Abstract: We present a compositional model for video event detection. A video is modeled using a collection of both global and segment-level features and kernel functions are employed for similarity comparisons. The locations of salient, discriminative video segments are treated as a latent variable, allowing the model to explicitly ignore portions of the video that are unimportant for classification. A novel, multiple kernel learning (MKL) latent support vector machine (SVM) is defined, that is used to combine and re-weight multiple feature types in a principled fashion while simultaneously operating within the latent variable framework. The compositional nature of the proposed model allows it to respond directly to the challenges of temporal clutter and intra-class variation, which are prevalent in unconstrained internet videos. Experimental results on the TRECVID Multimedia Event Detection 2011 (MED11) dataset demonstrate the efficacy of the method.</p><p>5 0.1256714 <a title="179-tfidf-5" href="./iccv-2013-Group_Norm_for_Learning_Structured_SVMs_with_Unstructured_Latent_Variables.html">187 iccv-2013-Group Norm for Learning Structured SVMs with Unstructured Latent Variables</a></p>
<p>Author: Daozheng Chen, Dhruv Batra, William T. Freeman</p><p>Abstract: Latent variables models have been applied to a number of computer vision problems. However, the complexity of the latent space is typically left as a free design choice. A larger latent space results in a more expressive model, but such models are prone to overfitting and are slower to perform inference with. The goal of this paper is to regularize the complexity of the latent space and learn which hidden states are really relevant for prediction. Specifically, we propose using group-sparsity-inducing regularizers such as ?1-?2 to estimate the parameters of Structured SVMs with unstructured latent variables. Our experiments on digit recognition and object detection show that our approach is indeed able to control the complexity of latent space without any significant loss in accuracy of the learnt model.</p><p>6 0.1216533 <a title="179-tfidf-6" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>7 0.12155093 <a title="179-tfidf-7" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>8 0.11816856 <a title="179-tfidf-8" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>9 0.1123089 <a title="179-tfidf-9" href="./iccv-2013-NEIL%3A_Extracting_Visual_Knowledge_from_Web_Data.html">285 iccv-2013-NEIL: Extracting Visual Knowledge from Web Data</a></p>
<p>10 0.10635129 <a title="179-tfidf-10" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>11 0.09942814 <a title="179-tfidf-11" href="./iccv-2013-Building_Part-Based_Object_Detectors_via_3D_Geometry.html">66 iccv-2013-Building Part-Based Object Detectors via 3D Geometry</a></p>
<p>12 0.097245887 <a title="179-tfidf-12" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>13 0.095127515 <a title="179-tfidf-13" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>14 0.092667319 <a title="179-tfidf-14" href="./iccv-2013-Decomposing_Bag_of_Words_Histograms.html">104 iccv-2013-Decomposing Bag of Words Histograms</a></p>
<p>15 0.09104564 <a title="179-tfidf-15" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>16 0.090680808 <a title="179-tfidf-16" href="./iccv-2013-Holistic_Scene_Understanding_for_3D_Object_Detection_with_RGBD_Cameras.html">201 iccv-2013-Holistic Scene Understanding for 3D Object Detection with RGBD Cameras</a></p>
<p>17 0.090267316 <a title="179-tfidf-17" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>18 0.085776061 <a title="179-tfidf-18" href="./iccv-2013-Allocentric_Pose_Estimation.html">46 iccv-2013-Allocentric Pose Estimation</a></p>
<p>19 0.085189521 <a title="179-tfidf-19" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>20 0.084880948 <a title="179-tfidf-20" href="./iccv-2013-Recognising_Human-Object_Interaction_via_Exemplar_Based_Modelling.html">344 iccv-2013-Recognising Human-Object Interaction via Exemplar Based Modelling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.185), (1, 0.082), (2, 0.027), (3, -0.02), (4, 0.138), (5, -0.037), (6, -0.049), (7, 0.034), (8, -0.061), (9, -0.067), (10, 0.062), (11, 0.016), (12, -0.09), (13, -0.099), (14, -0.081), (15, -0.039), (16, 0.041), (17, 0.069), (18, 0.072), (19, -0.02), (20, -0.038), (21, 0.056), (22, 0.02), (23, 0.025), (24, 0.068), (25, 0.003), (26, 0.001), (27, -0.067), (28, 0.04), (29, -0.029), (30, 0.029), (31, -0.089), (32, -0.028), (33, 0.02), (34, -0.014), (35, 0.012), (36, -0.037), (37, -0.015), (38, -0.047), (39, 0.023), (40, 0.021), (41, -0.072), (42, 0.062), (43, -0.123), (44, -0.049), (45, 0.044), (46, -0.027), (47, 0.127), (48, 0.11), (49, 0.1)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90128893 <a title="179-lsi-1" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<p>Author: Tian Lan, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: The appearance of an object changes profoundly with pose, camera view and interactions of the object with other objects in the scene. This makes it challenging to learn detectors based on an object-level label (e.g., “car”). We postulate that having a richer set oflabelings (at different levels of granularity) for an object, including finer-grained subcategories, consistent in appearance and view, and higherorder composites – contextual groupings of objects consistent in their spatial layout and appearance, can significantly alleviate these problems. However, obtaining such a rich set of annotations, including annotation of an exponentially growing set of object groupings, is simply not feasible. We propose a weakly-supervised framework for object detection where we discover subcategories and the composites automatically with only traditional object-level category labels as input. To this end, we first propose an exemplar-SVM-based clustering approach, with latent SVM refinement, that discovers a variable length set of discriminative subcategories for each object class. We then develop a structured model for object detection that captures interactions among object subcategories and automatically discovers semantically meaningful and discriminatively relevant visual composites. We show that this model produces state-of-the-art performance on UIUC phrase object detection benchmark.</p><p>2 0.74325162 <a title="179-lsi-2" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>Author: Ning Zhang, Ryan Farrell, Forrest Iandola, Trevor Darrell</p><p>Abstract: Recognizing objects in fine-grained domains can be extremely challenging due to the subtle differences between subcategories. Discriminative markings are often highly localized, leading traditional object recognition approaches to struggle with the large pose variation often present in these domains. Pose-normalization seeks to align training exemplars, either piecewise by part or globally for the whole object, effectively factoring out differences in pose and in viewing angle. Prior approaches relied on computationally-expensive filter ensembles for part localization and required extensive supervision. This paper proposes two pose-normalized descriptors based on computationally-efficient deformable part models. The first leverages the semantics inherent in strongly-supervised DPM parts. The second exploits weak semantic annotations to learn cross-component correspondences, computing pose-normalized descriptors from the latent parts of a weakly-supervised DPM. These representations enable pooling across pose and viewpoint, in turn facilitating tasks such as fine-grained recognition and attribute prediction. Experiments conducted on the Caltech-UCSD Birds 200 dataset and Berkeley Human Attribute dataset demonstrate significant improvements over state-of-art algorithms.</p><p>3 0.73050332 <a title="179-lsi-3" href="./iccv-2013-Building_Part-Based_Object_Detectors_via_3D_Geometry.html">66 iccv-2013-Building Part-Based Object Detectors via 3D Geometry</a></p>
<p>Author: Abhinav Shrivastava, Abhinav Gupta</p><p>Abstract: This paper proposes a novel part-based representation for modeling object categories. Our representation combines the effectiveness of deformable part-based models with the richness of geometric representation by defining parts based on consistent underlying 3D geometry. Our key hypothesis is that while the appearance and the arrangement of parts might vary across the instances of object categories, the constituent parts will still have consistent underlying 3D geometry. We propose to learn this geometrydriven deformable part-based model (gDPM) from a set of labeled RGBD images. We also demonstrate how the geometric representation of gDPM can help us leverage depth data during training and constrain the latent model learning problem. But most importantly, a joint geometric and appearance based representation not only allows us to achieve state-of-the-art results on object detection but also allows us to tackle the grand challenge of understanding 3D objects from 2D images.</p><p>4 0.6877358 <a title="179-lsi-4" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>Author: Jian Sun, Jean Ponce</p><p>Abstract: In this paper, we address the problem of learning discriminative part detectors from image sets with category labels. We propose a novel latent SVM model regularized by group sparsity to learn these part detectors. Starting from a large set of initial parts, the group sparsity regularizer forces the model to jointly select and optimize a set of discriminative part detectors in a max-margin framework. We propose a stochastic version of a proximal algorithm to solve the corresponding optimization problem. We apply the proposed method to image classification and cosegmentation, and quantitative experiments with standard benchmarks show that it matches or improves upon the state of the art.</p><p>5 0.68303782 <a title="179-lsi-5" href="./iccv-2013-Detecting_Avocados_to_Zucchinis%3A_What_Have_We_Done%2C_and_Where_Are_We_Going%3F.html">109 iccv-2013-Detecting Avocados to Zucchinis: What Have We Done, and Where Are We Going?</a></p>
<p>Author: Olga Russakovsky, Jia Deng, Zhiheng Huang, Alexander C. Berg, Li Fei-Fei</p><p>Abstract: The growth of detection datasets and the multiple directions of object detection research provide both an unprecedented need and a great opportunity for a thorough evaluation of the current state of the field of categorical object detection. In this paper we strive to answer two key questions. First, where are we currently as a field: what have we done right, what still needs to be improved? Second, where should we be going in designing the next generation of object detectors? Inspired by the recent work of Hoiem et al. [10] on the standard PASCAL VOC detection dataset, we perform a large-scale study on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) data. First, we quantitatively demonstrate that this dataset provides many of the same detection challenges as the PASCAL VOC. Due to its scale of 1000 object categories, ILSVRC also provides an excellent testbed for understanding the performance of detectors as a function of several key properties of the object classes. We conduct a series of analyses looking at how different detection methods perform on a number of imagelevel and object-class-levelproperties such as texture, color, deformation, and clutter. We learn important lessons of the current object detection methods and propose a number of insights for designing the next generation object detectors.</p><p>6 0.67832476 <a title="179-lsi-6" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>7 0.65813202 <a title="179-lsi-7" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>8 0.64129853 <a title="179-lsi-8" href="./iccv-2013-How_Do_You_Tell_a_Blackbird_from_a_Crow%3F.html">202 iccv-2013-How Do You Tell a Blackbird from a Crow?</a></p>
<p>9 0.63607693 <a title="179-lsi-9" href="./iccv-2013-NEIL%3A_Extracting_Visual_Knowledge_from_Web_Data.html">285 iccv-2013-NEIL: Extracting Visual Knowledge from Web Data</a></p>
<p>10 0.63607413 <a title="179-lsi-10" href="./iccv-2013-Fine-Grained_Categorization_by_Alignments.html">169 iccv-2013-Fine-Grained Categorization by Alignments</a></p>
<p>11 0.62662369 <a title="179-lsi-11" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>12 0.61780053 <a title="179-lsi-12" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>13 0.60483873 <a title="179-lsi-13" href="./iccv-2013-Modeling_Occlusion_by_Discriminative_AND-OR_Structures.html">269 iccv-2013-Modeling Occlusion by Discriminative AND-OR Structures</a></p>
<p>14 0.59660673 <a title="179-lsi-14" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>15 0.59193552 <a title="179-lsi-15" href="./iccv-2013-Group_Norm_for_Learning_Structured_SVMs_with_Unstructured_Latent_Variables.html">187 iccv-2013-Group Norm for Learning Structured SVMs with Unstructured Latent Variables</a></p>
<p>16 0.58299541 <a title="179-lsi-16" href="./iccv-2013-Shufflets%3A_Shared_Mid-level_Parts_for_Fast_Object_Detection.html">390 iccv-2013-Shufflets: Shared Mid-level Parts for Fast Object Detection</a></p>
<p>17 0.56864703 <a title="179-lsi-17" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>18 0.55768728 <a title="179-lsi-18" href="./iccv-2013-Recognising_Human-Object_Interaction_via_Exemplar_Based_Modelling.html">344 iccv-2013-Recognising Human-Object Interaction via Exemplar Based Modelling</a></p>
<p>19 0.55647236 <a title="179-lsi-19" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>20 0.55252987 <a title="179-lsi-20" href="./iccv-2013-HOGgles%3A_Visualizing_Object_Detection_Features.html">189 iccv-2013-HOGgles: Visualizing Object Detection Features</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.096), (7, 0.014), (12, 0.011), (13, 0.013), (24, 0.204), (26, 0.1), (31, 0.047), (42, 0.128), (64, 0.072), (73, 0.024), (78, 0.032), (89, 0.131), (98, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85026407 <a title="179-lda-1" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>Author: Chaochao Lu, Deli Zhao, Xiaoou Tang</p><p>Abstract: When face images are taken in the wild, the large variations in facial pose, illumination, and expression make face recognition challenging. The most fundamental problem for face recognition is to measure the similarity between faces. The traditional measurements such as various mathematical norms, Hausdorff distance, and approximate geodesic distance cannot accurately capture the structural information between faces in such complex circumstances. To address this issue, we develop a novel face patch network, based on which we define a new similarity measure called the random path (RP) measure. The RP measure is derivedfrom the collective similarity ofpaths by performing random walks in the network. It can globally characterize the contextual and curved structures of the face space. To apply the RP measure, we construct two kinds of networks: . cuhk . edu . hk the in-face network and the out-face network. The in-face network is drawn from any two face images and captures the local structural information. The out-face network is constructed from all the training face patches, thereby modeling the global structures of face space. The two face networks are structurally complementary and can be combined together to improve the recognition performance. Experiments on the Multi-PIE and LFW benchmarks show that the RP measure outperforms most of the state-of-art algorithms for face recognition.</p><p>same-paper 2 0.82122743 <a title="179-lda-2" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<p>Author: Tian Lan, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: The appearance of an object changes profoundly with pose, camera view and interactions of the object with other objects in the scene. This makes it challenging to learn detectors based on an object-level label (e.g., “car”). We postulate that having a richer set oflabelings (at different levels of granularity) for an object, including finer-grained subcategories, consistent in appearance and view, and higherorder composites – contextual groupings of objects consistent in their spatial layout and appearance, can significantly alleviate these problems. However, obtaining such a rich set of annotations, including annotation of an exponentially growing set of object groupings, is simply not feasible. We propose a weakly-supervised framework for object detection where we discover subcategories and the composites automatically with only traditional object-level category labels as input. To this end, we first propose an exemplar-SVM-based clustering approach, with latent SVM refinement, that discovers a variable length set of discriminative subcategories for each object class. We then develop a structured model for object detection that captures interactions among object subcategories and automatically discovers semantically meaningful and discriminatively relevant visual composites. We show that this model produces state-of-the-art performance on UIUC phrase object detection benchmark.</p><p>3 0.81068128 <a title="179-lda-3" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<p>Author: Nakamasa Inoue, Koichi Shinoda</p><p>Abstract: Assigning a visual code to a low-level image descriptor, which we call code assignment, is the most computationally expensive part of image classification algorithms based on the bag of visual word (BoW) framework. This paper proposes a fast computation method, Neighbor-toNeighbor (NTN) search, for this code assignment. Based on the fact that image features from an adjacent region are usually similar to each other, this algorithm effectively reduces the cost of calculating the distance between a codeword and a feature vector. This method can be applied not only to a hard codebook constructed by vector quantization (NTN-VQ), but also to a soft codebook, a Gaussian mixture model (NTN-GMM). We evaluated this method on the PASCAL VOC 2007 classification challenge task. NTN-VQ reduced the assignment cost by 77.4% in super-vector coding, and NTN-GMM reduced it by 89.3% in Fisher-vector coding, without any significant degradation in classification performance.</p><p>4 0.77698016 <a title="179-lda-4" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<p>Author: Federico Tombari, Alessandro Franchi, Luigi Di_Stefano</p><p>Abstract: Object detection in images withstanding significant clutter and occlusion is still a challenging task whenever the object surface is characterized by poor informative content. We propose to tackle this problem by a compact and distinctive representation of groups of neighboring line segments aggregated over limited spatial supports and invariant to rotation, translation and scale changes. Peculiarly, our proposal allows for leveraging on the inherent strengths of descriptor-based approaches, i.e. robustness to occlusion and clutter and scalability with respect to the size of the model library, also when dealing with scarcely textured objects.</p><p>5 0.74397445 <a title="179-lda-5" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>Author: S. Karthikeyan, Vignesh Jagadeesh, Renuka Shenoy, Miguel Ecksteinz, B.S. Manjunath</p><p>Abstract: Eye movement studies have confirmed that overt attention is highly biased towards faces and text regions in images. In this paper we explore a novel problem of predicting face and text regions in images using eye tracking data from multiple subjects. The problem is challenging as we aim to predict the semantics (face/text/background) only from eye tracking data without utilizing any image information. The proposed algorithm spatially clusters eye tracking data obtained in an image into different coherent groups and subsequently models the likelihood of the clusters containing faces and text using afully connectedMarkov Random Field (MRF). Given the eye tracking datafrom a test image, itpredicts potential face/head (humans, dogs and cats) and text locations reliably. Furthermore, the approach can be used to select regions of interest for further analysis by object detectors for faces and text. The hybrid eye position/object detector approach achieves better detection performance and reduced computation time compared to using only the object detection algorithm. We also present a new eye tracking dataset on 300 images selected from ICDAR, Street-view, Flickr and Oxford-IIIT Pet Dataset from 15 subjects.</p><p>6 0.73971474 <a title="179-lda-6" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>7 0.73531735 <a title="179-lda-7" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>8 0.73521215 <a title="179-lda-8" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>9 0.73459798 <a title="179-lda-9" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>10 0.73424768 <a title="179-lda-10" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>11 0.73364842 <a title="179-lda-11" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>12 0.73296291 <a title="179-lda-12" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>13 0.73208654 <a title="179-lda-13" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>14 0.73092455 <a title="179-lda-14" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>15 0.72996348 <a title="179-lda-15" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>16 0.72977638 <a title="179-lda-16" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<p>17 0.72842407 <a title="179-lda-17" href="./iccv-2013-Bayesian_Joint_Topic_Modelling_for_Weakly_Supervised_Object_Localisation.html">59 iccv-2013-Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation</a></p>
<p>18 0.72838569 <a title="179-lda-18" href="./iccv-2013-New_Graph_Structured_Sparsity_Model_for_Multi-label_Image_Annotations.html">290 iccv-2013-New Graph Structured Sparsity Model for Multi-label Image Annotations</a></p>
<p>19 0.72822392 <a title="179-lda-19" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>20 0.72765887 <a title="179-lda-20" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
