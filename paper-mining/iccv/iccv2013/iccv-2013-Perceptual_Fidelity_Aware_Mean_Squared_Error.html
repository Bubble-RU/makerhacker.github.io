<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-312" href="#">iccv2013-312</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</h1>
<br/><p>Source: <a title="iccv-2013-312-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Xue_Perceptual_Fidelity_Aware_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Wufeng Xue, Xuanqin Mou, Lei Zhang, Xiangchu Feng</p><p>Abstract: How to measure the perceptual quality of natural images is an important problem in low level vision. It is known that the Mean Squared Error (MSE) is not an effective index to describe the perceptual fidelity of images. Numerous perceptual fidelity indices have been developed, while the representatives include the Structural SIMilarity (SSIM) index and its variants. However, most of those perceptual measures are nonlinear, and they cannot be easily adopted as an objective function to minimize in various low level vision tasks. Can MSE be perceptual fidelity aware after some minor adaptation ? In this paper we propose a simple framework to enhance the perceptual fidelity awareness of MSE by introducing an l2-norm structural error term to it. Such a Structural MSE (SMSE) can lead to very competitive image quality assessment (IQA) results. More surprisingly, we show that by using certain structure extractors, SMSE can befurther turned into a Gaussian smoothed MSE (i.e., the Euclidean distance between the original and distorted images after Gaussian , smooth filtering), which is much simpler to calculate but achieves rather better IQA performance than SSIM. The socalled Perceptual-fidelity Aware MSE (PAMSE) can have great potentials in applications such as perceptual image coding and perceptual image restoration.</p><p>Reference: <a title="iccv-2013-312-reference" href="../iccv2013_reference/iccv-2013-Perceptual_Fidelity_Aware_Mean_Squared_Error_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 hk Abstract How to measure the perceptual quality of natural images is an important problem in low level vision. [sent-8, score-0.204]
</p><p>2 It is known that the Mean Squared Error (MSE) is not an effective index to describe the perceptual fidelity of images. [sent-9, score-0.311]
</p><p>3 Numerous perceptual fidelity indices have been developed, while the representatives include the Structural SIMilarity (SSIM) index and its variants. [sent-10, score-0.311]
</p><p>4 However, most of those perceptual measures are nonlinear, and they cannot be easily adopted as an objective function to minimize in various low level vision tasks. [sent-11, score-0.149]
</p><p>5 Can MSE be perceptual fidelity aware after some minor adaptation ? [sent-12, score-0.331]
</p><p>6 In this paper we propose a simple framework to enhance the perceptual fidelity awareness of MSE by introducing an l2-norm structural error term to it. [sent-13, score-0.369]
</p><p>7 Such a Structural MSE (SMSE) can lead to very competitive image quality assessment (IQA) results. [sent-14, score-0.116]
</p><p>8 , the Euclidean distance between the original and distorted images after Gaussian  ,  smooth filtering), which is much simpler to calculate but achieves rather better IQA performance than SSIM. [sent-17, score-0.084]
</p><p>9 The socalled Perceptual-fidelity Aware MSE (PAMSE) can have great potentials in applications such as perceptual image coding and perceptual image restoration. [sent-18, score-0.313]
</p><p>10 Introduction In many image processing and low level vision tasks, it is indispensable to find a suitable fidelity measure to measure the fidelity index of the underlying visual signal. [sent-20, score-0.345]
</p><p>11 cn use of fidelity measure can be generally classified into the following categories. [sent-27, score-0.164]
</p><p>12 First of all, the fidelity measure is ubiquitously used to evaluate the performance of competing algorithms and to guide the parameter selection, for example, in image denoising [9] and medical image reconstruction [29]. [sent-28, score-0.179]
</p><p>13 Second, the fidelity measure is used to design the objective function for minimization in applications such as image coding [12,23] and image restoration [17]. [sent-29, score-0.194]
</p><p>14 Third, the fidelity measure can be used as the rule to make decisions, for example, in content based image retrieval [21] and block matching [16, 27]. [sent-30, score-0.164]
</p><p>15 The most popular and widely used signal fidelity measure may be the classical Mean Squared Error (MSE),  which measures the fidelity of a signal d ∈ RN by ? [sent-31, score-0.349]
</p><p>16 This is mainly because MSE is pixel-wise and ignores the structural relationship in a neighborhood. [sent-41, score-0.055]
</p><p>17 The past decades have witnessed the rapid development of image quality assessment (IQA) methods, which aim to gauge the perceptual quality of natural images like human −  770055  does. [sent-43, score-0.287]
</p><p>18 In the case that the reference image is accessible, the resulting full reference (FR) IQA model can be utilized as an image perceptual fidelity term to mimic the human perception of image quality. [sent-44, score-0.33]
</p><p>19 Though these IQA models can better predict the perceptual quality of a test image than MSE, they do not profit much the applications such as perceptual image coding, perceptual image representation and restoration, etc. [sent-46, score-0.483]
</p><p>20 Most of them are not a valid distance metric and have much higher computational complexity than MSE. [sent-48, score-0.06]
</p><p>21 In perceptual image coding and restoration, more visually comfortable results have been reported by using SSIM to guide the algorithm design [16, 17, 23]. [sent-51, score-0.164]
</p><p>22 The underlying assumption of SSIM is that the human vision system is highly adapted to extract structural information in the viewing field. [sent-52, score-0.055]
</p><p>23 For a pair of reference image r and distorted image d, SSIM estimates the perceptual quality of d from three aspects: luminance, contrast and structure. [sent-53, score-0.243]
</p><p>24 It uses the following formula to compute the perceptual fidelity index of d:  SSIM(r,d) =μ2r2μ+rμ μdd2++ c c11·σ2r2σ+r, σdd2++ c2 c2  (1)  where μr and μd are the local mean luminance of r and d; σr2 and σd2 are the local variance; σr,d is the local covariance  between r and d. [sent-54, score-0.323]
</p><p>25 It is highly demanded to find an MSE-like fidelity measure, which could inherit some of the appealing merits of MSE while being highly perceptual fidelity aware. [sent-57, score-0.451]
</p><p>26 This will not only simplify the computation in IQA applications, but also facilitate significantly the use of perceptual fidelity measure in applications such perceptual image coding and perceptual image restoration. [sent-58, score-0.626]
</p><p>27 With the above considerations, in this work we aim to develop an MSE-like l2-norm perceptual fidelity measure. [sent-59, score-0.294]
</p><p>28 We propose to introduce an l2-norm structural error term to the original MSE so that the resulting measure can be more perceptual fidelity aware. [sent-60, score-0.388]
</p><p>29 The structural error term can be simply designed by using the linear gradient operator or Laplacian of Gaussian operator. [sent-61, score-0.152]
</p><p>30 , the Euclidean distance between reference image r and distorted image d after Gaussian smooth filtering. [sent-65, score-0.102]
</p><p>31 We call the resulting measure  Perceptual-fidelity Aware MSE (PAMSE), which provides a very simple MSE-like formula to calculate the image perceptual fidelity and achieves rather better perceptual consistency than SSIM. [sent-66, score-0.462]
</p><p>32 In addition, PAMSE can have great potentials in perceptual image coding and restoration. [sent-67, score-0.164]
</p><p>33 Section 2 presents the framework of SMSE and the setting of its structural term. [sent-68, score-0.055]
</p><p>34 Since human visual system is sensitive to image local structures, we propose to amend MSE a little so that the modified MSE can count more the structural information in the fidelity estimation. [sent-76, score-0.2]
</p><p>35 2 2 )  (2)  where α is a (negative or positive) constant to adjust the contribution of structural error term ? [sent-91, score-0.075]
</p><p>36 SMSE is an amendment of MSE by introducing a structural error term. [sent-97, score-0.075]
</p><p>37 The framework of Structure MSE (SMSE) and Perceptual fidelity Aware MSE (PAMSE). [sent-101, score-0.145]
</p><p>38 22  (6)  The matrix P can be viewed as a new feature extractor (a linear projection/transform) which is able to simultaneously measure the pixel-wise energy preservation and local neighborhood-wise image structure preservation. [sent-119, score-0.108]
</p><p>39 Linear structure extractor S There are many candidates for the linear structure operator S in the proposed SMSE framework. [sent-123, score-0.146]
</p><p>40 For instances, S can be chosen as the gradient operator which outputs the  abrupt changes of image intensity; S can also be chosen as the Laplacian or Laplacian of Gaussian(LOG) operator which mimics the receptive field of the ganglion cells and the lateral geniculate nucleus (LGN) cells [5]. [sent-124, score-0.133]
</p><p>41 The linear transforms such as wavelet transform and principle component analysis can also be employed as the feature extractor S. [sent-125, score-0.064]
</p><p>42 For simplicity, in this paper we only consider the gradient operator and the Laplacian operator. [sent-126, score-0.077]
</p><p>43 1 Gradient operators Image gradient is a good feature for low level and higher level vision tasks, and gradient priors are widely used in image restoration. [sent-129, score-0.083]
</p><p>44 Therefore, it is a good choice to employ gradient operators as S in the proposed SMSE framework. [sent-131, score-0.062]
</p><p>45 That is, we can apply a filter f = [1, −1] horizontally and vertically to the images r afnd = =d 1to, get th hoeirri gradient maps. [sent-134, score-0.056]
</p><p>46 tTichael yfor towta rhde dimif-ference filtering can be written as a matrix operator, denoted by Sd = [Sxd; Sdy], where Sdx and Sdy denote the corresponding operators along horizontal and vertical directions, respectively. [sent-135, score-0.094]
</p><p>47 The forward difference operator can be too sensitive to small intensity changes. [sent-136, score-0.07]
</p><p>48 A more commonly used gradient operator is the Gaussian gradient operator, which actual-  ly smoothes the image by using a Gaussian smooth filter h before applying the forward difference filter f. [sent-137, score-0.228]
</p><p>49 2  Laplacian operators  The gradient operator computes the first derivative of images. [sent-141, score-0.132]
</p><p>50 In comparison, the Laplacian operator, denoted by Sl, exploits the second derivative of images by filtering the images with the Laplacian filter l = [0, 1, 0; 1, −4, 1; 0, 1, 0] . [sent-142, score-0.076]
</p><p>51 aTghees Laplacian operator nS fli tise very se [0n,si1t,i0ve; 1to, −no4,is1e; a0n,d1 i,m0]-. [sent-143, score-0.056]
</p><p>52 age small changes, and hence the LOG operator, denoted by Slog, is proposed to smooth the image by Gaussian smooth filter h before applying l. [sent-144, score-0.095]
</p><p>53 The zero770077  crossings of the LOG filtering response indicate the edge locations, and the LOG based edge map has been successfully used to predict image quality [28, 3 1]. [sent-146, score-0.063]
</p><p>54 3  The determination of α  Given a linear structure extractor S, we need to determine the parameter α to make the proposed SMSE a valid distance metric. [sent-149, score-0.119]
</p><p>55 Since the operators Sd, Sg, Sl, and Slog can be interpreted as filtering operations, they can be written as a circulant matrix (each row is a cyclic shift of another) of size N N, whose rows are repeated svherisftio onfs a nofo tthheer corresponding Nfil,te wr template. [sent-157, score-0.132]
</p><p>56 Gaussian smooth MSE: an inherent perceptual fidelity aware MSE In Section. [sent-178, score-0.361]
</p><p>57 2, we proposed a framework to make MSE be able to characterize image local structural changes while measuring the global energy of image pixel-wise error. [sent-179, score-0.055]
</p><p>58 With some linear structure extractor S and the associated parameter α determined in Eq. [sent-180, score-0.077]
</p><p>59 9, the new image fidelity measure SMSE is however still a valid distance metric. [sent-181, score-0.206]
</p><p>60 The metric is actually characterized by the symmetric PSD matrix M = I αSTS, which can be written as + M = PTP, and consequently the SMSE metric can be written as SMSE(r, d) = N1 ? [sent-182, score-0.076]
</p><p>61 matrix P is a circulant matrix and each row of it comes from a filter, then the feature extraction by Pr and Pd becomes the linear filtering of images r and d by this filter. [sent-187, score-0.089]
</p><p>62 The operator P depends on the used linear feature extractor S. [sent-190, score-0.12]
</p><p>63 We can also use more than one operator in the pro−  posed SMSE framework. [sent-193, score-0.056]
</p><p>64 Suppose that we use the difference operator Sd and the Laplacian operator Sl, and hence the SMSE measure becomes SMSE(r,d) =  N1(? [sent-194, score-0.131]
</p><p>65 Interestingly, it can be proved that (please refer to Appendix A) if we set αd = −2σ2 and αl = σ4, where σ is the scale parameter of a G−a2uσssian smooth filter h and σ is small, then we have  S+MσS4E? [sent-201, score-0.065]
</p><p>66 on: we can simply make MSE perceptual fidelity aware by filtering the images r and d with a Gaussian smooth filter. [sent-211, score-0.388]
</p><p>67 We call such an image fidelity measure Perceptual-fidelity Aware MSE (PAMSE), defined as  PAMSE(r,d) =N1? [sent-212, score-0.164]
</p><p>68 12 can be written as PAMSE(r, d) = N1(r − d)TPhTPh(r − d), where Ph is the matrix form of filtering by h, PAM(SrE− −is always a (pseudo-)distance metric because PhTPh is a PSD matrix. [sent-217, score-0.071]
</p><p>69 The images in these databases are generated through different distortion channels and are all assigned with a subjective quality/distortion score. [sent-222, score-0.087]
</p><p>70 scores and the predicted scores can be examined in terms of Spear rank order correlation coefficient (SRC), Pearson correlation coefficient (PCC) and the root mean squared error (RMSE). [sent-226, score-0.064]
</p><p>71 Note that PCC and RMSE are calculated after a logistic regression between the predicted scores and the subjective scores [8]. [sent-227, score-0.061]
</p><p>72 The LIVE database consists of 29 reference images and 779 distorted images generated from JPEG compression (JPEG), JPEG2000 compression (JP2K), additive white noise (AWN), Gaussian blur (GB), and simulated fast fading Rayleigh channel. [sent-229, score-0.058]
</p><p>73 The CSIQ database consistes of 30  reference images and 866 distorted images of JPEG2000, JPEG, AWN, GB, additive pink Gaussian noise (PGN) and contrast change. [sent-230, score-0.058]
</p><p>74 There are 25 reference images and 1,700 distorted images of 17 distortion types. [sent-233, score-0.078]
</p><p>75 Since these operators are actually the matrix form of spatial filters, in implementation we use spatial convolution to compute the structural features Sr and Sd. [sent-238, score-0.108]
</p><p>76 For SMSE with Sg, we will smooth the images r and d by using a Gaussian smooth filter h(i, j) = 2π1σ2 before applying the forward difference filter efx. [sent-262, score-0.144]
</p><p>77 For SMSE with Slog, we also need to set the scale parameter of the Gaussian smooth filter h. [sent-293, score-0.065]
</p><p>78 Implementation and results of PAMSE The implementation of PAMSE is even simpler than SMSE because we only need to set the scale parameter σ of the Gaussian smooth filter h. [sent-325, score-0.065]
</p><p>79 The key message we would like to convey is that with some small adaptation, an MSE-like image perceptual fidelity measure can be obtained, which is very easy and efficient to implement while offering very competitive IQA results. [sent-343, score-0.327]
</p><p>80 4 shows the scatter plots of the subjective score versus the predicted score by SSIM and PAMSE on the CSIQ database. [sent-358, score-0.074]
</p><p>81 MSE measures image distortion equally in all frequencies, and thus over-estimates the perceptual distortion. [sent-403, score-0.169]
</p><p>82 r d) with the filter corresponding to operator Sro,r a singdn tahle (nr calculating thhee f il 2t-erno cromrr eosfp tohned ifnilgte troing response. [sent-417, score-0.091]
</p><p>83 The scatter plots of the subjective score versus the results of IQA models on the CSIQ database. [sent-426, score-0.074]
</p><p>84 Conclusions By adding an l2-norm structure error term to the original Mean Squared Error (MSE) index, in this paper we proposed a simple yet very effective framework, namely Structural MSE (SMSE), for image quality assessment (IQA). [sent-436, score-0.135]
</p><p>85 When using difference operator and Laplacian operator to extract the structure error, SMSE becomes the MSE between Gaussian smoothed reference and distorted images, and we call the corresponding SMSE measure Percetualfidelity Aware MSE (PAMSE). [sent-437, score-0.214]
</p><p>86 Meanwhile, SMSE and PAMSE have good potentials to be used as objective functions in perceptual quality based image processing tasks. [sent-440, score-0.185]
</p><p>87 f Wdifefe uresenc ∇e operator Sod d aenndo Laplacian operator Sl, respectively. [sent-444, score-0.112]
</p><p>88 On the mathematical properties of the structural similarity index. [sent-478, score-0.07]
</p><p>89 Perceptual image quality assessment using a geometric structural distortion model. [sent-486, score-0.177]
</p><p>90 Visible differences predictor: an algorithm for the assessment of image fidelity. [sent-496, score-0.066]
</p><p>91 Final report from the video quality experts group on the validation of objective models of video quality assessment, phase II. [sent-516, score-0.072]
</p><p>92 Most apparent distortion: fullreference image quality assessment and the role of strategy. [sent-526, score-0.102]
</p><p>93 The effects of a visual fidelity criterion of the encoding of images. [sent-540, score-0.145]
</p><p>94 TID2008-a database for evaluation offull-reference visual quality assessment metrics. [sent-564, score-0.102]
</p><p>95 An information fidelity criterion for image quality assessment using natural scene statistics. [sent-588, score-0.247]
</p><p>96 Image quality assessment: From error visibility to structural similarity. [sent-631, score-0.111]
</p><p>97 Maximum differentiation (mad) competition: A methodology for comparing computational models of perceptual quantities. [sent-637, score-0.149]
</p><p>98 Patch-based probabilistic image quality assessment for face selection and improved video-based face recognition. [sent-646, score-0.102]
</p><p>99 An image quality assessment metric based on non-shift edge. [sent-651, score-0.12]
</p><p>100 Non-shift edge based ratio (nser): An image quality assessment metric based on early vision features. [sent-672, score-0.12]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('smse', 0.69), ('iqa', 0.388), ('pamse', 0.37), ('mse', 0.288), ('perceptual', 0.149), ('fidelity', 0.145), ('ssim', 0.103), ('csiq', 0.078), ('slog', 0.069), ('assessment', 0.066), ('extractor', 0.064), ('src', 0.06), ('sd', 0.059), ('operator', 0.056), ('structural', 0.055), ('sl', 0.05), ('sg', 0.044), ('fsim', 0.043), ('laplacian', 0.041), ('operators', 0.041), ('distorted', 0.04), ('pcc', 0.039), ('circulant', 0.038), ('aware', 0.037), ('subjective', 0.037), ('quality', 0.036), ('filter', 0.035), ('sts', 0.032), ('smooth', 0.03), ('mad', 0.03), ('databases', 0.03), ('ifc', 0.029), ('rehman', 0.029), ('rnse', 0.029), ('valid', 0.028), ('gaussian', 0.027), ('filtering', 0.027), ('live', 0.027), ('extractors', 0.026), ('psd', 0.025), ('vif', 0.024), ('tip', 0.022), ('gradient', 0.021), ('versus', 0.021), ('rmse', 0.021), ('error', 0.02), ('sheikh', 0.02), ('squared', 0.02), ('signal', 0.02), ('distortion', 0.02), ('awn', 0.019), ('csf', 0.019), ('danatdab', 0.019), ('diq', 0.019), ('sdy', 0.019), ('measure', 0.019), ('reference', 0.018), ('jpeg', 0.018), ('metric', 0.018), ('mou', 0.017), ('differentiability', 0.017), ('index', 0.017), ('electronic', 0.017), ('eigenvalues', 0.016), ('scatter', 0.016), ('brunet', 0.016), ('smoothes', 0.016), ('appendix', 0.016), ('mathematical', 0.015), ('competing', 0.015), ('sensitivity', 0.015), ('hvs', 0.015), ('resu', 0.015), ('ptp', 0.015), ('dv', 0.015), ('coding', 0.015), ('restoration', 0.015), ('written', 0.014), ('forward', 0.014), ('competitive', 0.014), ('distance', 0.014), ('derivative', 0.014), ('curves', 0.013), ('counterpart', 0.013), ('bovik', 0.013), ('icip', 0.013), ('structure', 0.013), ('inherit', 0.012), ('china', 0.012), ('smoothed', 0.012), ('ses', 0.012), ('scores', 0.012), ('cs', 0.012), ('plot', 0.012), ('imaging', 0.012), ('mai', 0.012), ('luminance', 0.012), ('cti', 0.012), ('fr', 0.012), ('matrix', 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="312-tfidf-1" href="./iccv-2013-Perceptual_Fidelity_Aware_Mean_Squared_Error.html">312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</a></p>
<p>Author: Wufeng Xue, Xuanqin Mou, Lei Zhang, Xiangchu Feng</p><p>Abstract: How to measure the perceptual quality of natural images is an important problem in low level vision. It is known that the Mean Squared Error (MSE) is not an effective index to describe the perceptual fidelity of images. Numerous perceptual fidelity indices have been developed, while the representatives include the Structural SIMilarity (SSIM) index and its variants. However, most of those perceptual measures are nonlinear, and they cannot be easily adopted as an objective function to minimize in various low level vision tasks. Can MSE be perceptual fidelity aware after some minor adaptation ? In this paper we propose a simple framework to enhance the perceptual fidelity awareness of MSE by introducing an l2-norm structural error term to it. Such a Structural MSE (SMSE) can lead to very competitive image quality assessment (IQA) results. More surprisingly, we show that by using certain structure extractors, SMSE can befurther turned into a Gaussian smoothed MSE (i.e., the Euclidean distance between the original and distorted images after Gaussian , smooth filtering), which is much simpler to calculate but achieves rather better IQA performance than SSIM. The socalled Perceptual-fidelity Aware MSE (PAMSE) can have great potentials in applications such as perceptual image coding and perceptual image restoration.</p><p>2 0.16472185 <a title="312-tfidf-2" href="./iccv-2013-A_New_Image_Quality_Metric_for_Image_Auto-denoising.html">23 iccv-2013-A New Image Quality Metric for Image Auto-denoising</a></p>
<p>Author: Xiangfei Kong, Kuan Li, Qingxiong Yang, Liu Wenyin, Ming-Hsuan Yang</p><p>Abstract: This paper proposes a new non-reference image quality metric that can be adopted by the state-of-the-art image/video denoising algorithms for auto-denoising. The proposed metric is extremely simple and can be implemented in four lines of Matlab code1. The basic assumption employed by the proposed metric is that the noise should be independent of the original image. A direct measurement of this dependence is, however, impractical due to the relatively low accuracy of existing denoising method. The proposed metric thus aims at maximizing the structure similarity between the input noisy image and the estimated image noise around homogeneous regions and the structure similarity between the input noisy image and the denoised image around highly-structured regions, and is computed as the linear correlation coefficient of the two corresponding structure similarity maps. Numerous experimental results demonstrate that the proposed metric not only outperforms the current state-of-the-art non-reference quality metric quantitatively and qualitatively, but also better maintains temporal coherence when used for video denoising. ˜</p><p>3 0.049122095 <a title="312-tfidf-3" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>Author: Zhaowen Wang, Jianchao Yang, Nasser Nasrabadi, Thomas Huang</p><p>Abstract: Sparse Representation-based Classification (SRC) is a powerful tool in distinguishing signal categories which lie on different subspaces. Despite its wide application to visual recognition tasks, current understanding of SRC is solely based on a reconstructive perspective, which neither offers any guarantee on its classification performance nor provides any insight on how to design a discriminative dictionary for SRC. In this paper, we present a novel perspective towards SRC and interpret it as a margin classifier. The decision boundary and margin of SRC are analyzed in local regions where the support of sparse code is stable. Based on the derived margin, we propose a hinge loss function as the gauge for the classification performance of SRC. A stochastic gradient descent algorithm is implemented to maximize the margin of SRC and obtain more discriminative dictionaries. Experiments validate the effectiveness of the proposed approach in predicting classification performance and improving dictionary quality over reconstructive ones. Classification results competitive with other state-ofthe-art sparse coding methods are reported on several data sets.</p><p>4 0.049096026 <a title="312-tfidf-4" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>Author: Bo Wang, Zhuowen Tu, John K. Tsotsos</p><p>Abstract: In graph-based semi-supervised learning approaches, the classification rate is highly dependent on the size of the availabel labeled data, as well as the accuracy of the similarity measures. Here, we propose a semi-supervised multi-class/multi-label classification scheme, dynamic label propagation (DLP), which performs transductive learning through propagation in a dynamic process. Existing semi-supervised classification methods often have difficulty in dealing with multi-class/multi-label problems due to the lack in consideration of label correlation; our algorithm instead emphasizes dynamic metric fusion with label information. Significant improvement over the state-of-the-art methods is observed on benchmark datasets for both multiclass and multi-label tasks.</p><p>5 0.037526775 <a title="312-tfidf-5" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>Author: Yu-Tseh Chi, Mohsen Ali, Muhammad Rushdi, Jeffrey Ho</p><p>Abstract: This paper proposes a novel approach for sparse coding that further improves upon the sparse representation-based classification (SRC) framework. The proposed framework, Affine-Constrained Group Sparse Coding (ACGSC), extends the current SRC framework to classification problems with multiple input samples. Geometrically, the affineconstrained group sparse coding essentially searches for the vector in the convex hull spanned by the input vectors that can best be sparse coded using the given dictionary. The resulting objectivefunction is still convex and can be efficiently optimized using iterative block-coordinate descent scheme that is guaranteed to converge. Furthermore, we provide a form of sparse recovery result that guarantees, at least theoretically, that the classification performance of the constrained group sparse coding should be at least as good as the group sparse coding. We have evaluated the proposed approach using three different recognition experiments that involve illumination variation of faces and textures, and face recognition under occlusions. Prelimi- nary experiments have demonstrated the effectiveness of the proposed approach, and in particular, the results from the recognition/occlusion experiment are surprisingly accurate and robust.</p><p>6 0.037085127 <a title="312-tfidf-6" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>7 0.033271238 <a title="312-tfidf-7" href="./iccv-2013-A_Joint_Intensity_and_Depth_Co-sparse_Analysis_Model_for_Depth_Map_Super-resolution.html">18 iccv-2013-A Joint Intensity and Depth Co-sparse Analysis Model for Depth Map Super-resolution</a></p>
<p>8 0.03212801 <a title="312-tfidf-8" href="./iccv-2013-Beyond_Hard_Negative_Mining%3A_Efficient_Detector_Learning_via_Block-Circulant_Decomposition.html">61 iccv-2013-Beyond Hard Negative Mining: Efficient Detector Learning via Block-Circulant Decomposition</a></p>
<p>9 0.031600799 <a title="312-tfidf-9" href="./iccv-2013-From_Semi-supervised_to_Transfer_Counting_of_Crowds.html">178 iccv-2013-From Semi-supervised to Transfer Counting of Crowds</a></p>
<p>10 0.029860999 <a title="312-tfidf-10" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<p>11 0.028190734 <a title="312-tfidf-11" href="./iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding.html">14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</a></p>
<p>12 0.028161088 <a title="312-tfidf-12" href="./iccv-2013-Dynamic_Probabilistic_Volumetric_Models.html">128 iccv-2013-Dynamic Probabilistic Volumetric Models</a></p>
<p>13 0.02789283 <a title="312-tfidf-13" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>14 0.027619008 <a title="312-tfidf-14" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>15 0.027538273 <a title="312-tfidf-15" href="./iccv-2013-A_Simple_Model_for_Intrinsic_Image_Decomposition_with_Depth_Cues.html">30 iccv-2013-A Simple Model for Intrinsic Image Decomposition with Depth Cues</a></p>
<p>16 0.026655616 <a title="312-tfidf-16" href="./iccv-2013-Curvature-Aware_Regularization_on_Riemannian_Submanifolds.html">100 iccv-2013-Curvature-Aware Regularization on Riemannian Submanifolds</a></p>
<p>17 0.025949873 <a title="312-tfidf-17" href="./iccv-2013-Constant_Time_Weighted_Median_Filtering_for_Stereo_Matching_and_Beyond.html">88 iccv-2013-Constant Time Weighted Median Filtering for Stereo Matching and Beyond</a></p>
<p>18 0.025195239 <a title="312-tfidf-18" href="./iccv-2013-From_Point_to_Set%3A_Extend_the_Learning_of_Distance_Metrics.html">177 iccv-2013-From Point to Set: Extend the Learning of Distance Metrics</a></p>
<p>19 0.023996795 <a title="312-tfidf-19" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>20 0.021663485 <a title="312-tfidf-20" href="./iccv-2013-A_Color_Constancy_Model_with_Double-Opponency_Mechanisms.html">5 iccv-2013-A Color Constancy Model with Double-Opponency Mechanisms</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.059), (1, -0.007), (2, -0.016), (3, -0.019), (4, -0.03), (5, 0.008), (6, 0.002), (7, -0.014), (8, -0.0), (9, -0.024), (10, -0.01), (11, -0.031), (12, 0.006), (13, -0.002), (14, 0.005), (15, 0.016), (16, -0.013), (17, -0.021), (18, -0.002), (19, 0.025), (20, 0.013), (21, 0.026), (22, -0.011), (23, -0.035), (24, 0.002), (25, 0.053), (26, 0.055), (27, -0.011), (28, 0.013), (29, 0.032), (30, -0.028), (31, -0.039), (32, 0.022), (33, 0.066), (34, 0.016), (35, -0.021), (36, 0.022), (37, -0.036), (38, 0.017), (39, -0.023), (40, 0.028), (41, 0.048), (42, -0.016), (43, -0.039), (44, 0.053), (45, 0.003), (46, 0.052), (47, 0.001), (48, -0.034), (49, 0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88665503 <a title="312-lsi-1" href="./iccv-2013-Perceptual_Fidelity_Aware_Mean_Squared_Error.html">312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</a></p>
<p>Author: Wufeng Xue, Xuanqin Mou, Lei Zhang, Xiangchu Feng</p><p>Abstract: How to measure the perceptual quality of natural images is an important problem in low level vision. It is known that the Mean Squared Error (MSE) is not an effective index to describe the perceptual fidelity of images. Numerous perceptual fidelity indices have been developed, while the representatives include the Structural SIMilarity (SSIM) index and its variants. However, most of those perceptual measures are nonlinear, and they cannot be easily adopted as an objective function to minimize in various low level vision tasks. Can MSE be perceptual fidelity aware after some minor adaptation ? In this paper we propose a simple framework to enhance the perceptual fidelity awareness of MSE by introducing an l2-norm structural error term to it. Such a Structural MSE (SMSE) can lead to very competitive image quality assessment (IQA) results. More surprisingly, we show that by using certain structure extractors, SMSE can befurther turned into a Gaussian smoothed MSE (i.e., the Euclidean distance between the original and distorted images after Gaussian , smooth filtering), which is much simpler to calculate but achieves rather better IQA performance than SSIM. The socalled Perceptual-fidelity Aware MSE (PAMSE) can have great potentials in applications such as perceptual image coding and perceptual image restoration.</p><p>2 0.81544477 <a title="312-lsi-2" href="./iccv-2013-A_New_Image_Quality_Metric_for_Image_Auto-denoising.html">23 iccv-2013-A New Image Quality Metric for Image Auto-denoising</a></p>
<p>Author: Xiangfei Kong, Kuan Li, Qingxiong Yang, Liu Wenyin, Ming-Hsuan Yang</p><p>Abstract: This paper proposes a new non-reference image quality metric that can be adopted by the state-of-the-art image/video denoising algorithms for auto-denoising. The proposed metric is extremely simple and can be implemented in four lines of Matlab code1. The basic assumption employed by the proposed metric is that the noise should be independent of the original image. A direct measurement of this dependence is, however, impractical due to the relatively low accuracy of existing denoising method. The proposed metric thus aims at maximizing the structure similarity between the input noisy image and the estimated image noise around homogeneous regions and the structure similarity between the input noisy image and the denoised image around highly-structured regions, and is computed as the linear correlation coefficient of the two corresponding structure similarity maps. Numerous experimental results demonstrate that the proposed metric not only outperforms the current state-of-the-art non-reference quality metric quantitatively and qualitatively, but also better maintains temporal coherence when used for video denoising. ˜</p><p>3 0.60524261 <a title="312-lsi-3" href="./iccv-2013-Single-Patch_Low-Rank_Prior_for_Non-pointwise_Impulse_Noise_Removal.html">394 iccv-2013-Single-Patch Low-Rank Prior for Non-pointwise Impulse Noise Removal</a></p>
<p>Author: Ruixuan Wang, Emanuele Trucco</p><p>Abstract: This paper introduces a ‘low-rank prior’ for small oriented noise-free image patches: considering an oriented patch as a matrix, a low-rank matrix approximation is enough to preserve the texture details in the properly oriented patch. Based on this prior, we propose a single-patch method within a generalized joint low-rank and sparse matrix recovery framework to simultaneously detect and remove non-pointwise random-valued impulse noise (e.g., very small blobs). A weighting matrix is incorporated in the framework to encode an initial estimate of the spatial noise distribution. An accelerated proximal gradient method is adapted to estimate the optimal noise-free image patches. Experiments show the effectiveness of our framework in removing non-pointwise random-valued impulse noise.</p><p>4 0.5451839 <a title="312-lsi-4" href="./iccv-2013-A_Learning-Based_Approach_to_Reduce_JPEG_Artifacts_in_Image_Matting.html">19 iccv-2013-A Learning-Based Approach to Reduce JPEG Artifacts in Image Matting</a></p>
<p>Author: Inchang Choi, Sunyeong Kim, Michael S. Brown, Yu-Wing Tai</p><p>Abstract: Single image matting techniques assume high-quality input images. The vast majority of images on the web and in personal photo collections are encoded using JPEG compression. JPEG images exhibit quantization artifacts that adversely affect the performance of matting algorithms. To address this situation, we propose a learning-based post-processing method to improve the alpha mattes extracted from JPEG images. Our approach learns a set of sparse dictionaries from training examples that are used to transfer details from high-quality alpha mattes to alpha mattes corrupted by JPEG compression. Three different dictionaries are defined to accommodate different object structure (long hair, short hair, and sharp boundaries). A back-projection criteria combined within an MRF framework is used to automatically select the best dictionary to apply on the object’s local boundary. We demonstrate that our method can produces superior results over existing state-of-the-art matting algorithms on a variety of inputs and compression levels.</p><p>5 0.53909665 <a title="312-lsi-5" href="./iccv-2013-Joint_Noise_Level_Estimation_from_Personal_Photo_Collections.html">223 iccv-2013-Joint Noise Level Estimation from Personal Photo Collections</a></p>
<p>Author: Yichang Shih, Vivek Kwatra, Troy Chinen, Hui Fang, Sergey Ioffe</p><p>Abstract: Personal photo albums are heavily biased towards faces of people, but most state-of-the-art algorithms for image denoising and noise estimation do not exploit facial information. We propose a novel technique for jointly estimating noise levels of all face images in a photo collection. Photos in a personal album are likely to contain several faces of the same people. While some of these photos would be clean and high quality, others may be corrupted by noise. Our key idea is to estimate noise levels by comparing multiple images of the same content that differ predominantly in their noise content. Specifically, we compare geometrically and photometrically aligned face images of the same person. Our estimation algorithm is based on a probabilistic formulation that seeks to maximize the joint probability of estimated noise levels across all images. We propose an approximate solution that decomposes this joint maximization into a two-stage optimization. The first stage determines the relative noise between pairs of images by pooling estimates from corresponding patch pairs in a probabilistic fashion. The second stage then jointly optimizes for all absolute noise parameters by conditioning them upon relative noise levels, which allows for a pairwise factorization of the probability distribution. We evaluate our noise estimation method using quantitative experiments to measure accuracy on synthetic data. Additionally, we employ the estimated noise levels for automatic denoising using “BM3D”, and evaluate the quality of denoising on real-world photos through a user study.</p><p>6 0.53145152 <a title="312-lsi-6" href="./iccv-2013-Total_Variation_Regularization_for_Functions_with_Values_in_a_Manifold.html">421 iccv-2013-Total Variation Regularization for Functions with Values in a Manifold</a></p>
<p>7 0.52481723 <a title="312-lsi-7" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>8 0.51842403 <a title="312-lsi-8" href="./iccv-2013-DCSH_-_Matching_Patches_in_RGBD_Images.html">101 iccv-2013-DCSH - Matching Patches in RGBD Images</a></p>
<p>9 0.50908703 <a title="312-lsi-9" href="./iccv-2013-A_Novel_Earth_Mover%27s_Distance_Methodology_for_Image_Matching_with_Gaussian_Mixture_Models.html">25 iccv-2013-A Novel Earth Mover's Distance Methodology for Image Matching with Gaussian Mixture Models</a></p>
<p>10 0.49581248 <a title="312-lsi-10" href="./iccv-2013-From_Point_to_Set%3A_Extend_the_Learning_of_Distance_Metrics.html">177 iccv-2013-From Point to Set: Extend the Learning of Distance Metrics</a></p>
<p>11 0.49426311 <a title="312-lsi-11" href="./iccv-2013-SGTD%3A_Structure_Gradient_and_Texture_Decorrelating_Regularization_for_Image_Decomposition.html">364 iccv-2013-SGTD: Structure Gradient and Texture Decorrelating Regularization for Image Decomposition</a></p>
<p>12 0.49194822 <a title="312-lsi-12" href="./iccv-2013-Joint_Learning_of_Discriminative_Prototypes_and_Large_Margin_Nearest_Neighbor_Classifiers.html">222 iccv-2013-Joint Learning of Discriminative Prototypes and Large Margin Nearest Neighbor Classifiers</a></p>
<p>13 0.48641181 <a title="312-lsi-13" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>14 0.47525293 <a title="312-lsi-14" href="./iccv-2013-Quadruplet-Wise_Image_Similarity_Learning.html">332 iccv-2013-Quadruplet-Wise Image Similarity Learning</a></p>
<p>15 0.47463915 <a title="312-lsi-15" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>16 0.45584601 <a title="312-lsi-16" href="./iccv-2013-A_Generalized_Low-Rank_Appearance_Model_for_Spatio-temporally_Correlated_Rain_Streaks.html">15 iccv-2013-A Generalized Low-Rank Appearance Model for Spatio-temporally Correlated Rain Streaks</a></p>
<p>17 0.45502803 <a title="312-lsi-17" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>18 0.43329656 <a title="312-lsi-18" href="./iccv-2013-Non-convex_P-Norm_Projection_for_Robust_Sparsity.html">292 iccv-2013-Non-convex P-Norm Projection for Robust Sparsity</a></p>
<p>19 0.43111134 <a title="312-lsi-19" href="./iccv-2013-Motion-Aware_KNN_Laplacian_for_Video_Matting.html">275 iccv-2013-Motion-Aware KNN Laplacian for Video Matting</a></p>
<p>20 0.43027905 <a title="312-lsi-20" href="./iccv-2013-Detecting_Irregular_Curvilinear_Structures_in_Gray_Scale_and_Color_Imagery_Using_Multi-directional_Oriented_Flux.html">112 iccv-2013-Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-directional Oriented Flux</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.055), (7, 0.022), (12, 0.011), (15, 0.192), (25, 0.053), (26, 0.061), (31, 0.048), (42, 0.075), (48, 0.033), (55, 0.105), (64, 0.028), (73, 0.036), (89, 0.097), (97, 0.018), (98, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74320149 <a title="312-lda-1" href="./iccv-2013-Perceptual_Fidelity_Aware_Mean_Squared_Error.html">312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</a></p>
<p>Author: Wufeng Xue, Xuanqin Mou, Lei Zhang, Xiangchu Feng</p><p>Abstract: How to measure the perceptual quality of natural images is an important problem in low level vision. It is known that the Mean Squared Error (MSE) is not an effective index to describe the perceptual fidelity of images. Numerous perceptual fidelity indices have been developed, while the representatives include the Structural SIMilarity (SSIM) index and its variants. However, most of those perceptual measures are nonlinear, and they cannot be easily adopted as an objective function to minimize in various low level vision tasks. Can MSE be perceptual fidelity aware after some minor adaptation ? In this paper we propose a simple framework to enhance the perceptual fidelity awareness of MSE by introducing an l2-norm structural error term to it. Such a Structural MSE (SMSE) can lead to very competitive image quality assessment (IQA) results. More surprisingly, we show that by using certain structure extractors, SMSE can befurther turned into a Gaussian smoothed MSE (i.e., the Euclidean distance between the original and distorted images after Gaussian , smooth filtering), which is much simpler to calculate but achieves rather better IQA performance than SSIM. The socalled Perceptual-fidelity Aware MSE (PAMSE) can have great potentials in applications such as perceptual image coding and perceptual image restoration.</p><p>2 0.64122534 <a title="312-lda-2" href="./iccv-2013-PhotoOCR%3A_Reading_Text_in_Uncontrolled_Conditions.html">315 iccv-2013-PhotoOCR: Reading Text in Uncontrolled Conditions</a></p>
<p>Author: Alessandro Bissacco, Mark Cummins, Yuval Netzer, Hartmut Neven</p><p>Abstract: We describe PhotoOCR, a system for text extraction from images. Our particular focus is reliable text extraction from smartphone imagery, with the goal of text recognition as a user input modality similar to speech recognition. Commercially available OCR performs poorly on this task. Recent progress in machine learning has substantially improved isolated character classification; we build on this progress by demonstrating a complete OCR system using these techniques. We also incorporate modern datacenter-scale distributed language modelling. Our approach is capable of recognizing text in a variety of challenging imaging conditions where traditional OCR systems fail, notably in the presence of substantial blur, low resolution, low contrast, high image noise and other distortions. It also operates with low latency; mean processing time is 600 ms per image. We evaluate our system on public benchmark datasets for text extraction and outperform all previously reported results, more than halving the error rate on multiple benchmarks. The system is currently in use in many applications at Google, and is available as a user input modality in Google Translate for Android.</p><p>3 0.62320805 <a title="312-lda-3" href="./iccv-2013-Joint_Optimization_for_Consistent_Multiple_Graph_Matching.html">224 iccv-2013-Joint Optimization for Consistent Multiple Graph Matching</a></p>
<p>Author: Junchi Yan, Yu Tian, Hongyuan Zha, Xiaokang Yang, Ya Zhang, Stephen M. Chu</p><p>Abstract: The problem of graph matching in general is NP-hard and approaches have been proposed for its suboptimal solution, most focusing on finding the one-to-one node mapping between two graphs. A more general and challenging problem arises when one aims to find consistent mappings across a number of graphs more than two. Conventional graph pair matching methods often result in mapping inconsistency since the mapping between two graphs can either be determined by pair mapping or by an additional anchor graph. To address this issue, a novel formulation is derived which is maximized via alternating optimization. Our method enjoys several advantages: 1) the mappings are jointly optimized rather than sequentially performed by applying pair matching, allowing the global affinity information across graphs can be propagated and explored; 2) the number of concerned variables to optimize is in linear with the number of graphs, being superior to local pair matching resulting in O(n2) variables; 3) the mapping consistency constraints are analytically satisfied during optimization; and 4) off-the-shelf graph pair matching solvers can be reused under the proposed framework in an ‘out-of-thebox’ fashion. Competitive results on both the synthesized data and the real data are reported, by varying the level of deformation, outliers and edge densities. ∗Corresponding author. The work is supported by NSF IIS1116886, NSF IIS-1049694, NSFC 61129001/F010403 and the 111 Project (B07022). Yu Tian Shanghai Jiao Tong University Shanghai, China, 200240 yut ian @ s j tu . edu .cn Xiaokang Yang Shanghai Jiao Tong University Shanghai, China, 200240 xkyang@ s j tu .edu . cn Stephen M. Chu IBM T.J. Waston Research Center Yorktown Heights, NY USA, 10598 s chu @u s . ibm . com</p><p>4 0.61036134 <a title="312-lda-4" href="./iccv-2013-A_New_Image_Quality_Metric_for_Image_Auto-denoising.html">23 iccv-2013-A New Image Quality Metric for Image Auto-denoising</a></p>
<p>Author: Xiangfei Kong, Kuan Li, Qingxiong Yang, Liu Wenyin, Ming-Hsuan Yang</p><p>Abstract: This paper proposes a new non-reference image quality metric that can be adopted by the state-of-the-art image/video denoising algorithms for auto-denoising. The proposed metric is extremely simple and can be implemented in four lines of Matlab code1. The basic assumption employed by the proposed metric is that the noise should be independent of the original image. A direct measurement of this dependence is, however, impractical due to the relatively low accuracy of existing denoising method. The proposed metric thus aims at maximizing the structure similarity between the input noisy image and the estimated image noise around homogeneous regions and the structure similarity between the input noisy image and the denoised image around highly-structured regions, and is computed as the linear correlation coefficient of the two corresponding structure similarity maps. Numerous experimental results demonstrate that the proposed metric not only outperforms the current state-of-the-art non-reference quality metric quantitatively and qualitatively, but also better maintains temporal coherence when used for video denoising. ˜</p><p>5 0.60691398 <a title="312-lda-5" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>Author: Yong Jae Lee, Alexei A. Efros, Martial Hebert</p><p>Abstract: We present a weakly-supervised visual data mining approach that discovers connections between recurring midlevel visual elements in historic (temporal) and geographic (spatial) image collections, and attempts to capture the underlying visual style. In contrast to existing discovery methods that mine for patterns that remain visually consistent throughout the dataset, our goal is to discover visual elements whose appearance changes due to change in time or location; i.e., exhibit consistent stylistic variations across the label space (date or geo-location). To discover these elements, we first identify groups of patches that are stylesensitive. We then incrementally build correspondences to find the same element across the entire dataset. Finally, we train style-aware regressors that model each element’s range of stylistic differences. We apply our approach to date and geo-location prediction and show substantial improvement over several baselines that do not model visual style. We also demonstrate the method’s effectiveness on the related task of fine-grained classification.</p><p>6 0.57377476 <a title="312-lda-6" href="./iccv-2013-Image_Segmentation_with_Cascaded_Hierarchical_Models_and_Logistic_Disjunctive_Normal_Networks.html">211 iccv-2013-Image Segmentation with Cascaded Hierarchical Models and Logistic Disjunctive Normal Networks</a></p>
<p>7 0.57293224 <a title="312-lda-7" href="./iccv-2013-Learning_CRFs_for_Image_Parsing_with_Adaptive_Subgradient_Descent.html">234 iccv-2013-Learning CRFs for Image Parsing with Adaptive Subgradient Descent</a></p>
<p>8 0.57091814 <a title="312-lda-8" href="./iccv-2013-Geometric_Registration_Based_on_Distortion_Estimation.html">183 iccv-2013-Geometric Registration Based on Distortion Estimation</a></p>
<p>9 0.56971139 <a title="312-lda-9" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>10 0.56856102 <a title="312-lda-10" href="./iccv-2013-Efficient_Image_Dehazing_with_Boundary_Constraint_and_Contextual_Regularization.html">135 iccv-2013-Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</a></p>
<p>11 0.56813645 <a title="312-lda-11" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>12 0.56408101 <a title="312-lda-12" href="./iccv-2013-Rolling_Shutter_Stereo.html">363 iccv-2013-Rolling Shutter Stereo</a></p>
<p>13 0.56325811 <a title="312-lda-13" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>14 0.56315178 <a title="312-lda-14" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>15 0.56303358 <a title="312-lda-15" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>16 0.56142801 <a title="312-lda-16" href="./iccv-2013-A_Unified_Rolling_Shutter_and_Motion_Blur_Model_for_3D_Visual_Registration.html">32 iccv-2013-A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration</a></p>
<p>17 0.56102693 <a title="312-lda-17" href="./iccv-2013-A_Simple_Model_for_Intrinsic_Image_Decomposition_with_Depth_Cues.html">30 iccv-2013-A Simple Model for Intrinsic Image Decomposition with Depth Cues</a></p>
<p>18 0.56005597 <a title="312-lda-18" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>19 0.55781734 <a title="312-lda-19" href="./iccv-2013-Bounded_Labeling_Function_for_Global_Segmentation_of_Multi-part_Objects_with_Geometric_Constraints.html">63 iccv-2013-Bounded Labeling Function for Global Segmentation of Multi-part Objects with Geometric Constraints</a></p>
<p>20 0.55739999 <a title="312-lda-20" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
