<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-51" href="#">iccv2013-51</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</h1>
<br/><p>Source: <a title="iccv-2013-51-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Timofte_Anchored_Neighborhood_Regression_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Radu Timofte, Vincent De_Smet, Luc Van_Gool</p><p>Abstract: Recently there have been significant advances in image upscaling or image super-resolution based on a dictionary of low and high resolution exemplars. The running time of the methods is often ignored despite the fact that it is a critical factor for real applications. This paper proposes fast super-resolution methods while making no compromise on quality. First, we support the use of sparse learned dictionaries in combination with neighbor embedding methods. In this case, the nearest neighbors are computed using the correlation with the dictionary atoms rather than the Euclidean distance. Moreover, we show that most of the current approaches reach top performance for the right parameters. Second, we show that using global collaborative coding has considerable speed advantages, reducing the super-resolution mapping to a precomputed projective matrix. Third, we propose the anchored neighborhood regression. That is to anchor the neighborhood embedding of a low resolution patch to the nearest atom in the dictionary and to precompute the corresponding embedding matrix. These proposals are contrasted with current state-of- the-art methods on standard images. We obtain similar or improved quality and one or two orders of magnitude speed improvements.</p><p>Reference: <a title="iccv-2013-51-reference" href="../iccv2013_reference/iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 First, we support the use of sparse learned dictionaries in combination with neighbor embedding methods. [sent-4, score-0.414]
</p><p>2 In this case, the nearest neighbors are computed using the correlation with the dictionary atoms rather than the Euclidean distance. [sent-5, score-0.53]
</p><p>3 Second, we show that using global collaborative coding has considerable speed advantages, reducing the super-resolution mapping to a precomputed projective matrix. [sent-7, score-0.226]
</p><p>4 That is to anchor the neighborhood embedding of a low resolution patch to the nearest atom in the dictionary and to precompute the corresponding embedding matrix. [sent-9, score-1.123]
</p><p>5 In general, it takes one or more low resolution (LR) images as input and maps them to a high resolution (HR) output image. [sent-14, score-0.295]
</p><p>6 Dictionary-based methods use a patch- or feature-based approach to learn the relationship between local image details in low resolution and high resolution versions of the same scene. [sent-31, score-0.273]
</p><p>7 By searching for nearest neighbors in a low resolution dictionary, a number of corresponding high resolution candidates can be retrieved. [sent-33, score-0.37]
</p><p>8 Several methods have been proposed to overcome this problem, most notably neighbor embedding and sparse encoding approaches. [sent-37, score-0.293]
</p><p>9 Sparse coding methods [17, 18] try to find a sparse coding for the input patches based on a compact dictionary (created by applying K-means or a similar algorithm to the training 11992200  patches). [sent-39, score-0.636]
</p><p>10 [3] compare the running times for several example-based super-resolution algorithms, mentioning minutes to hours for most state-of-the-art methods, depending on the size of the input image. [sent-42, score-0.134]
</p><p>11 We propose a new method for example-based superresolution that focuses on low computation time while keeping the qualitative performance of recent state-of-the-art approaches. [sent-43, score-0.114]
</p><p>12 The remainder of the paper is organized as follows: we  take a closer look at recent approaches for neighbor embedding and sparse coding for super-resolution in Section 2, we explain our proposed method in Section 3 and show experimental results in Section 4. [sent-45, score-0.322]
</p><p>13 As we have briefly presented Freeman’s original method [8] in the introduction, here we focus on neighbor embedding and sparse coding approaches. [sent-49, score-0.322]
</p><p>14 Neighbor embedding approaches Neighbor embedding (NE) approaches assume that small image patches from a low resolution image and its high resolution counterpart form low-dimensional nonlinear manifolds with similar local geometry. [sent-52, score-0.642]
</p><p>15 The LLE algorithm assumes that when enough samples are available, each sample and its neighbors lie on or near a locally linear patch of the manifold. [sent-55, score-0.148]
</p><p>16 search for a set of K nearest neighbors for each input patch in LR feature space, compute K appropriate weights for reconstructing the LR patch by finding a constrained least squares solution, and eventually create an HR patch by applying these weights in HR feature space. [sent-58, score-0.4]
</p><p>17 It is based on the assumption that the local nonnegative least squares decomposition weights over the local neighborhood in LR space also hold for the corresponding neighborhood in HR space. [sent-61, score-0.438]
</p><p>18 Sparse coding approaches The NE approaches from the previous section use a dictionary of sampled patches from low and high resolution image pairs. [sent-64, score-0.655]
</p><p>19 These dictionaries can quickly become very large, especially when more or bigger training images are added to improve performance. [sent-65, score-0.112]
</p><p>20 Sparse coding (SC) approaches try to overcome this by using a learned compact dictionary based on sparse signal representation. [sent-66, score-0.499]
</p><p>21 Low resolution patches are sparsely reconstructed from a learned dictionary using the following formulation: mαin ? [sent-69, score-0.576]
</p><p>22 0,  (1)  where F is a feature extraction operator, Dl is the learned low resolution dictionary, α is the sparse representation, y is the low resolution input patch and λ is a weighting factor. [sent-73, score-0.47]
</p><p>23 Sparse dictionaries are jointly learned for low and high resolution image patches, with the goal of having the same sparse representation for low resolution patches as their corresponding high resolution patches. [sent-76, score-0.732]
</p><p>24 This goal is reached for a set of training image patch pairs Xh, Yl (high and low resolution patches resp. [sent-77, score-0.313]
</p><p>25 2  (2)  where N and M are the dimensionality of the low and high resolution patches and Z is the coefficient vector representing the sparsity constraint. [sent-83, score-0.288]
</p><p>26 The resulting dictionary has a fixed size and thus the algorithm has the capability of learning from many training patches while avoiding long processing times due to an ever growing dictionary. [sent-84, score-0.515]
</p><p>27 [18] build upon this framework and improve  the execution speed by adding several modifications. [sent-87, score-0.117]
</p><p>28 Proposed Methods We propose an anchored neighborhood regression method that conveys two situations, one being the general behavior where a neighborhood size is set and the other being the so called global case, where the neighborhood coincides with the whole dictionary in use. [sent-92, score-1.105]
</p><p>29 We can reformulate the problem  as a least squares regression regularized by the l2-norm of the coefficients. [sent-99, score-0.119]
</p><p>30 2,  (3)  where Nl corresponds to the neighborhood in LR space that we choose to solve this problem, which in the case of neighborhood embedding would refer to the K nearest neighbors of the input feature yF and in the case of sparse coding would refer to the LR dictionary. [sent-105, score-0.696]
</p><p>31 (4)  The HR patches can then be computed using the same coefficients on the high resolution neighborhood Nh x = Nhβ, (5) where x is the HR output patch and Nh the HR neighborhood corresponding to Nl . [sent-108, score-0.642]
</p><p>32 If we use the whole LR dictionary for this, meaning (Nh, Nl) = (Dh, Dl), we get a global solution for the problem. [sent-109, score-0.364]
</p><p>33 This means that during the execution of the SR algorithm we only need to multiply the precomputed projection matrix PG with the LR input feature vector, yF, to calculate the HR output patches x. [sent-111, score-0.257]
</p><p>34 Anchored Neighborhood Regression The Global Regression approach reduces the superresolution process to a projection of each input feature into the HR space by multiplication with a precomputed matrix. [sent-115, score-0.141]
</p><p>35 If instead of considering the whole dictionary as starting point for computing the projective matrix we consider local neighborhoods of a given size we allow more flexibility of the approach at the expense of increased computation – we will have more than one projective matrix and neighborhoods. [sent-117, score-0.492]
</p><p>36 We start by grouping the dictionary atoms into neighborhoods. [sent-118, score-0.433]
</p><p>37 More specifically, for each atom in the dictionary we compute its K nearest neighbors, which will represent its neighborhood. [sent-119, score-0.448]
</p><p>38 If we start from a learned sparse dictionary, as in the sparsity approaches ofYang et al. [sent-120, score-0.132]
</p><p>39 [18], we find the nearest neighbors based on the correlation between the dictionary atoms rather than the Euclidean  distance. [sent-122, score-0.53]
</p><p>40 The reason for this is that the atoms are a learned basis consisting of l2-normalized vectors. [sent-123, score-0.125]
</p><p>41 If, conversely, we have a dictionary of features taken straight from the training patches, like in the NE approaches of Chang et al. [sent-124, score-0.387]
</p><p>42 Once the neighborhoods are defined, we can calculate a separate projection matrix Pj for each dictionary atom dj, based on its own neighborhood. [sent-127, score-0.491]
</p><p>43 This can be done in the same way as in the previous section by using only the dictionary atoms that occur in the neighborhood rather than the entire dictionary, and can again be computed offline. [sent-128, score-0.605]
</p><p>44 The super-resolution problem can then be solved by calculating for each input patch feature yiF its nearest neighbor atom, dj, in the dictionary, followed by the mapping to HR space using the stored projection matrix Pj : xi  = PjyiF. [sent-129, score-0.253]
</p><p>45 (8)  This is a close approximation of the NE approach, with a very low complexity and thus a vast improvement in execution time. [sent-130, score-0.109]
</p><p>46 We call our approach the Anchored Neighborhood Regression (ANR), since the neighborhoods are anchored to the dictionary atoms and not directly to the low resolution patches as in the other NE approaches. [sent-131, score-0.856]
</p><p>47 l4096 Dictoinary szie  1  03  102  Dictoinary szie  a) trained dictionary  28. [sent-141, score-0.569]
</p><p>48 6482 16324182561 024N0AGbE4 NRci8+ RuLNb SciEL4096 Dictoinary szie  103  102  e(s)gtim1 0 1  n iuRn1 0 − 12bNAGN ENREcEi+R+u+L bNSLciNELS  ×  10−3 163264128256512102420484096 Dictoinary szie  b) random dictionary  Figure 2. [sent-143, score-0.569]
</p><p>49 average PSNR and average running time performance on the 14 images from Set14 with magnification 3. [sent-145, score-0.181]
</p><p>50 t aheve mraegtheo PdSsN were dus aedve wraigthe trhuenirn i bnegst t neighborhood sciez eo. [sent-150, score-0.172]
</p><p>51 For the running times we subtracted the shared processing time (collecting patches, combining the output) for  all the methods. [sent-154, score-0.112]
</p><p>52 details surrounding the algorithm such as used features, dictionary choices, similarity measures, size for neighborhood calculation and different patch embeddings. [sent-155, score-0.611]
</p><p>53 These features are almost always calculated from the luminance component of the image, while the color components are interpolated using a regular interpolation algorithm such as bicubic interpolation [9, 18, 17, 4, 3]. [sent-161, score-0.239]
</p><p>54 This is because the human visual system is much less sensitive to high frequency color changes than high frequency intensity changes, so for the magnification factors used in most papers the perceived difference between bicubic interpolation and SR of the color channels is negligible. [sent-162, score-0.298]
</p><p>55 This usually leads to  ×  features of about 30 dimensions for upscaling factor 3 and 3 3 low resolution patch sizes. [sent-173, score-0.261]
</p><p>56 2  Embeddings  Apart from our comparisons with the sparse methods of Yang et al. [sent-182, score-0.099]
</p><p>57 [18], we also compare our results to neighbor embedding approaches adapted to our dictionary choices. [sent-184, score-0.557]
</p><p>58 [4] does not use a learned dictionary, instead the dictionary consists simply of the training patches themselves. [sent-186, score-0.469]
</p><p>59 This makes direct comparison to our method and the sparse methods difficult, because the question then arises “which dictionary should we use to have a fair comparison? [sent-187, score-0.394]
</p><p>60 The same can be said when we wish to compare to the nonnegative neighbor embedding of Bevilacqua et al. [sent-189, score-0.34]
</p><p>61 The solution is to use the same learned dictionary as Zeyde et al. [sent-191, score-0.42]
</p><p>62 l em  10−13 248163264128256512 Neighborhood szie  a) trained dictionary of size 1024 28. [sent-197, score-0.479]
</p><p>63 68241 bNGANciENREu+ RbLNci SEL48163241825612,04  ×  Neighborhood szie  ()e s111000123 m itg 100  nniuRn1100−−12bNAGNNENREcEi+R+u+LLbNSLciNELS 10−3 12481632641282565121,024 Neighborhood szie  b) random dictionary of size 1024  Figure 3. [sent-201, score-0.593]
</p><p>64 average PSNR and average running time performance on the 14 images from Set14 with magnification 3. [sent-203, score-0.181]
</p><p>65 uses the original dictionary of 1022, while the other ×m3e. [sent-208, score-0.341]
</p><p>66 tuhsee running tiinmael oifc tiohen a AryNR of m10e2th2,od w hfriolme t h raen odtohmer dictionary experiments is caused by applying a subsampling step of max{1, neighbo3r0hoodsize } for the anchor atoms, while for the trained one tohnea step pise r1i . [sent-213, score-0.428]
</p><p>67 Feontrs st ihse c running yti ampepsl we s aub sturbascatemdp tlhineg s hstaerped o processing time (collecting patches, combining t,h we output) hfoer aralli ntehed dictionary based methods, leaving only the encoding time for each method. [sent-214, score-0.446]
</p><p>68 3  Dictionaries  ×  The choice of the dictionary is critical for the performance of any SR method. [sent-218, score-0.341]
</p><p>69 Usually, the larger the dictionary the better the performance, however this comes with a higher computational cost. [sent-219, score-0.341]
</p><p>70 The dictionary can be built using the LR input image itself, in this case we have an “internal” dictionary. [sent-220, score-0.363]
</p><p>71 Also, we consider both randomly sampled dictionaries and learned dictionaries. [sent-227, score-0.145]
</p><p>72 2 we depict the effect of the dictionary on the performance. [sent-231, score-0.341]
</p><p>73 Moreover, we see again that using a learned dictionary is highly beneficial for all the methods it allows for a reasonably high performance for small dictionary sizes. [sent-233, score-0.737]
</p><p>74 One needs a 16 larger random sampled dictionary t soi z reesa. [sent-234, score-0.341]
</p><p>75 Most of the methods exhibit a similar log-linear increasing trend with the dictionary size and the PSNR difference among ANR, Zeyde et al. [sent-236, score-0.411]
</p><p>76 and the NE approaches is quite small for their best settings (using optimal neighborhood size). [sent-237, score-0.172]
</p><p>77 GR is the fastest method, but as a global method it has its weaknesses, and for large dictionaries tends not to reach competitive PSNR levels. [sent-239, score-0.197]
</p><p>78 2, our ANR algorithm finds the nearest neighbor (atom) in the dictionary for each input feature and borrows the neighborhood and the precomputed projection matrix from this neighbor. [sent-243, score-0.736]
</p><p>79 The NE approaches also rely on the neighborhood to the input LR feature. [sent-244, score-0.194]
</p><p>80 The performance of the embedding methods, and hence the performance of the SR method, depends on the dimensionality of these neighborhoods. [sent-245, score-0.154]
</p><p>81 The computation of the nearest neighbors is based on a similarity measure. [sent-246, score-0.097]
</p><p>82 In the case of the learned sparse dictionaries, we obtain l2-normalized atoms meant to form a basis spanning the space of the training samples while minimizing the reconstruction error. [sent-249, score-0.178]
</p><p>83 sT ohef PmSeNthRod (sd sBh)a raen tdh reu same gtr taiimneed ( dictionary goef o10n2 t4h,e except B diactuabseict. [sent-255, score-0.371]
</p><p>84 27s on average, being 13 times faster than Zeyde et al. [sent-262, score-0.112]
</p><p>85 [18]GRANRNE+LSNE+NNLSNE+LLE  imagesPSNRTimePSNRTimePSNRTimePSNRTimePSNRTimePSNRTimePSNRTimePSNRTime  The neighborhood size is the major parameter for the NE techniques and for ANR as well. [sent-266, score-0.196]
</p><p>86 We show the effect of this size in Figure 3 for dictionaries of size 1024. [sent-267, score-0.16]
</p><p>87 On the learned dictionary, NN + LS peaks at 12, NE + LLE and NE + NNLS at 24, while ANR peaks at 40. [sent-270, score-0.127]
</p><p>88 We will use these neighborhood sizes for the further experiments. [sent-272, score-0.172]
</p><p>89 More specifically, we compare our results to the sparse coding algorithms of Yang et al. [sent-278, score-0.152]
</p><p>90 The effect of dictionary size is explored in Fig. [sent-288, score-0.365]
</p><p>91 3 shows the relationship between neighborhood size, PSNR and time. [sent-290, score-0.172]
</p><p>92 1  Quality  When using the optimal neighborhood size for each method the PSNR of Zeyde et al. [sent-293, score-0.242]
</p><p>93 [3], which is in the order of 10 seconds for a magnification factor of 3 . [sent-312, score-0.124]
</p><p>94 compare with their algorithm because it is a very recent method aimed at low complexity and high processing speed while still keeping high quality results, and is therefore an ideal candidate for reference. [sent-315, score-0.15]
</p><p>95 63 seconds of the algorithms (pre/post processing, bicubic interpolation, patch extraction, etc. [sent-319, score-0.127]
</p><p>96 As lol fth PeS mNReth (oddBs) s ahnadre r tuhnen same mtraein (se)d p dictionary oonf t1h0e24 Se, except sBeitc. [sent-329, score-0.341]
</p><p>97 For upscaling factor 3, ANR is 5 times faster than Zeyde et al. [sent-334, score-0.155]
</p><p>98 Conclusions  We proposed a new example-based method for superresolution called Anchored Neighbor Regression which focuses on fast execution while retaining the qualitative performance of recent state-of-the-art methods. [sent-429, score-0.149]
</p><p>99 We also proposed an extreme variant of this called Global Regression which focuses purely on high execution speed in exchange for some visual quality loss. [sent-430, score-0.19]
</p><p>100 K-SVD: an algorithm for designing overcomplete dictionaries for sparse representation. [sent-439, score-0.165]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('zeyde', 0.43), ('dictionary', 0.341), ('anr', 0.31), ('ne', 0.258), ('bevilacqua', 0.211), ('hr', 0.208), ('nnls', 0.19), ('neighborhood', 0.172), ('lle', 0.162), ('psnr', 0.153), ('lr', 0.142), ('embedding', 0.127), ('magnification', 0.124), ('anchored', 0.121), ('szie', 0.114), ('dictionaries', 0.112), ('resolution', 0.107), ('sr', 0.096), ('patches', 0.095), ('gr', 0.094), ('atoms', 0.092), ('neighbor', 0.089), ('dictoinary', 0.084), ('regression', 0.08), ('interpolation', 0.077), ('ls', 0.075), ('patch', 0.074), ('execution', 0.072), ('atom', 0.063), ('neighborhoods', 0.063), ('running', 0.057), ('nonnegative', 0.055), ('sparse', 0.053), ('neighbors', 0.053), ('coding', 0.053), ('bicubic', 0.053), ('superresolution', 0.051), ('chang', 0.05), ('db', 0.048), ('peaks', 0.047), ('et', 0.046), ('nh', 0.045), ('speed', 0.045), ('precomputed', 0.044), ('nearest', 0.044), ('upscaling', 0.043), ('elad', 0.043), ('bicubically', 0.042), ('dltdl', 0.042), ('granrne', 0.042), ('lsne', 0.042), ('nnlsne', 0.042), ('yf', 0.042), ('yang', 0.042), ('reach', 0.04), ('squares', 0.039), ('low', 0.037), ('nl', 0.037), ('faster', 0.035), ('learned', 0.033), ('lanczos', 0.033), ('projective', 0.032), ('dh', 0.032), ('interpolated', 0.032), ('times', 0.031), ('precompute', 0.031), ('subdivided', 0.031), ('bh', 0.03), ('raen', 0.03), ('timofte', 0.03), ('collaborative', 0.029), ('freeman', 0.028), ('external', 0.028), ('pg', 0.028), ('iminds', 0.028), ('dimensionality', 0.027), ('focuses', 0.026), ('embeddings', 0.026), ('euclidean', 0.026), ('glasner', 0.026), ('extreme', 0.025), ('mrf', 0.024), ('processing', 0.024), ('encoding', 0.024), ('projection', 0.024), ('size', 0.024), ('said', 0.023), ('global', 0.023), ('fastest', 0.022), ('high', 0.022), ('yl', 0.022), ('ridge', 0.022), ('input', 0.022), ('locally', 0.021), ('profile', 0.02), ('manifolds', 0.02), ('dl', 0.02), ('eventually', 0.02), ('dj', 0.019), ('try', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="51-tfidf-1" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>Author: Radu Timofte, Vincent De_Smet, Luc Van_Gool</p><p>Abstract: Recently there have been significant advances in image upscaling or image super-resolution based on a dictionary of low and high resolution exemplars. The running time of the methods is often ignored despite the fact that it is a critical factor for real applications. This paper proposes fast super-resolution methods while making no compromise on quality. First, we support the use of sparse learned dictionaries in combination with neighbor embedding methods. In this case, the nearest neighbors are computed using the correlation with the dictionary atoms rather than the Euclidean distance. Moreover, we show that most of the current approaches reach top performance for the right parameters. Second, we show that using global collaborative coding has considerable speed advantages, reducing the super-resolution mapping to a precomputed projective matrix. Third, we propose the anchored neighborhood regression. That is to anchor the neighborhood embedding of a low resolution patch to the nearest atom in the dictionary and to precompute the corresponding embedding matrix. These proposals are contrasted with current state-of- the-art methods on standard images. We obtain similar or improved quality and one or two orders of magnitude speed improvements.</p><p>2 0.32198802 <a title="51-tfidf-2" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>Author: Chenglong Bao, Jian-Feng Cai, Hui Ji</p><p>Abstract: In recent years, how to learn a dictionary from input images for sparse modelling has been one very active topic in image processing and recognition. Most existing dictionary learning methods consider an over-complete dictionary, e.g. the K-SVD method. Often they require solving some minimization problem that is very challenging in terms of computational feasibility and efficiency. However, if the correlations among dictionary atoms are not well constrained, the redundancy of the dictionary does not necessarily improve the performance of sparse coding. This paper proposed a fast orthogonal dictionary learning method for sparse image representation. With comparable performance on several image restoration tasks, the proposed method is much more computationally efficient than the over-complete dictionary based learning methods.</p><p>3 0.30368674 <a title="51-tfidf-3" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>Author: Chih-Yuan Yang, Ming-Hsuan Yang</p><p>Abstract: The goal of single-image super-resolution is to generate a high-quality high-resolution image based on a given low-resolution input. It is an ill-posed problem which requires exemplars or priors to better reconstruct the missing high-resolution image details. In this paper, we propose to split the feature space into numerous subspaces and collect exemplars to learn priors for each subspace, thereby creating effective mapping functions. The use of split input space facilitates both feasibility of using simple functionsfor super-resolution, and efficiency ofgenerating highresolution results. High-quality high-resolution images are reconstructed based on the effective learned priors. Experimental results demonstrate that theproposed algorithmperforms efficiently and effectively over state-of-the-art methods.</p><p>4 0.28980431 <a title="51-tfidf-4" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>Author: Hua Wang, Feiping Nie, Weidong Cai, Heng Huang</p><p>Abstract: Representing the raw input of a data set by a set of relevant codes is crucial to many computer vision applications. Due to the intrinsic sparse property of real-world data, dictionary learning, in which the linear decomposition of a data point uses a set of learned dictionary bases, i.e., codes, has demonstrated state-of-the-art performance. However, traditional dictionary learning methods suffer from three weaknesses: sensitivity to noisy and outlier samples, difficulty to determine the optimal dictionary size, and incapability to incorporate supervision information. In this paper, we address these weaknesses by learning a Semi-Supervised Robust Dictionary (SSR-D). Specifically, we use the ℓ2,0+ norm as the loss function to improve the robustness against outliers, and develop a new structured sparse regularization com, , tom. . cai@sydney . edu . au , heng@uta .edu make the learning tasks easier to deal with and reduce the computational cost. For example, in image tagging, instead of using the raw pixel-wise features, semi-local or patch- based features, such as SIFT and geometric blur, are usually more desirable to achieve better performance. In practice, finding a set of compact features bases, also referred to as dictionary, with enhanced representative and discriminative power, plays a significant role in building a successful computer vision system. In this paper, we explore this important problem by proposing a novel formulation and its solution for learning Semi-Supervised Robust Dictionary (SSRD), where we examine the challenges in dictionary learning, and seek opportunities to overcome them and improve the dictionary qualities. 1.1. Challenges in Dictionary Learning to incorporate the supervision information in dictionary learning, without incurring additional parameters. Moreover, the optimal dictionary size is automatically learned from the input data. Minimizing the derived objective function is challenging because it involves many non-smooth ℓ2,0+ -norm terms. We present an efficient algorithm to solve the problem with a rigorous proof of the convergence of the algorithm. Extensive experiments are presented to show the superior performance of the proposed method.</p><p>5 0.22173204 <a title="51-tfidf-5" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>Author: Chen-Kuo Chiang, Te-Feng Su, Chih Yen, Shang-Hong Lai</p><p>Abstract: We present a multi-attributed dictionary learning algorithm for sparse coding. Considering training samples with multiple attributes, a new distance matrix is proposed by jointly incorporating data and attribute similarities. Then, an objective function is presented to learn categorydependent dictionaries that are compact (closeness of dictionary atoms based on data distance and attribute similarity), reconstructive (low reconstruction error with correct dictionary) and label-consistent (encouraging the labels of dictionary atoms to be similar). We have demonstrated our algorithm on action classification and face recognition tasks on several publicly available datasets. Experimental results with improved performance over previous dictionary learning methods are shown to validate the effectiveness of the proposed algorithm.</p><p>6 0.20619917 <a title="51-tfidf-6" href="./iccv-2013-Coupled_Dictionary_and_Feature_Space_Learning_with_Applications_to_Cross-Domain_Image_Synthesis_and_Recognition.html">96 iccv-2013-Coupled Dictionary and Feature Space Learning with Applications to Cross-Domain Image Synthesis and Recognition</a></p>
<p>7 0.19957282 <a title="51-tfidf-7" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>8 0.19696186 <a title="51-tfidf-8" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>9 0.18949932 <a title="51-tfidf-9" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>10 0.17978673 <a title="51-tfidf-10" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>11 0.15423304 <a title="51-tfidf-11" href="./iccv-2013-A_Joint_Intensity_and_Depth_Co-sparse_Analysis_Model_for_Depth_Map_Super-resolution.html">18 iccv-2013-A Joint Intensity and Depth Co-sparse Analysis Model for Depth Map Super-resolution</a></p>
<p>12 0.15413977 <a title="51-tfidf-12" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>13 0.15118511 <a title="51-tfidf-13" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>14 0.130142 <a title="51-tfidf-14" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>15 0.12818982 <a title="51-tfidf-15" href="./iccv-2013-Dictionary_Learning_and_Sparse_Coding_on_Grassmann_Manifolds%3A_An_Extrinsic_Solution.html">114 iccv-2013-Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution</a></p>
<p>16 0.10689303 <a title="51-tfidf-16" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>17 0.10555744 <a title="51-tfidf-17" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>18 0.10372724 <a title="51-tfidf-18" href="./iccv-2013-Nonparametric_Blind_Super-resolution.html">293 iccv-2013-Nonparametric Blind Super-resolution</a></p>
<p>19 0.098104171 <a title="51-tfidf-19" href="./iccv-2013-Learning_a_Dictionary_of_Shape_Epitomes_with_Applications_to_Image_Labeling.html">245 iccv-2013-Learning a Dictionary of Shape Epitomes with Applications to Image Labeling</a></p>
<p>20 0.091888025 <a title="51-tfidf-20" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.161), (1, 0.091), (2, -0.101), (3, -0.014), (4, -0.327), (5, -0.109), (6, -0.179), (7, -0.114), (8, -0.091), (9, -0.064), (10, 0.009), (11, 0.022), (12, 0.083), (13, 0.018), (14, -0.062), (15, 0.069), (16, -0.026), (17, -0.076), (18, 0.017), (19, 0.042), (20, -0.031), (21, 0.012), (22, 0.059), (23, 0.118), (24, -0.005), (25, 0.069), (26, 0.015), (27, -0.07), (28, -0.153), (29, -0.085), (30, -0.085), (31, -0.051), (32, 0.07), (33, 0.05), (34, -0.043), (35, -0.007), (36, 0.015), (37, -0.038), (38, 0.038), (39, -0.018), (40, 0.127), (41, -0.022), (42, 0.01), (43, -0.005), (44, -0.005), (45, -0.02), (46, -0.043), (47, 0.036), (48, 0.072), (49, -0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96167022 <a title="51-lsi-1" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>Author: Radu Timofte, Vincent De_Smet, Luc Van_Gool</p><p>Abstract: Recently there have been significant advances in image upscaling or image super-resolution based on a dictionary of low and high resolution exemplars. The running time of the methods is often ignored despite the fact that it is a critical factor for real applications. This paper proposes fast super-resolution methods while making no compromise on quality. First, we support the use of sparse learned dictionaries in combination with neighbor embedding methods. In this case, the nearest neighbors are computed using the correlation with the dictionary atoms rather than the Euclidean distance. Moreover, we show that most of the current approaches reach top performance for the right parameters. Second, we show that using global collaborative coding has considerable speed advantages, reducing the super-resolution mapping to a precomputed projective matrix. Third, we propose the anchored neighborhood regression. That is to anchor the neighborhood embedding of a low resolution patch to the nearest atom in the dictionary and to precompute the corresponding embedding matrix. These proposals are contrasted with current state-of- the-art methods on standard images. We obtain similar or improved quality and one or two orders of magnitude speed improvements.</p><p>2 0.7938382 <a title="51-lsi-2" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>Author: Chenglong Bao, Jian-Feng Cai, Hui Ji</p><p>Abstract: In recent years, how to learn a dictionary from input images for sparse modelling has been one very active topic in image processing and recognition. Most existing dictionary learning methods consider an over-complete dictionary, e.g. the K-SVD method. Often they require solving some minimization problem that is very challenging in terms of computational feasibility and efficiency. However, if the correlations among dictionary atoms are not well constrained, the redundancy of the dictionary does not necessarily improve the performance of sparse coding. This paper proposed a fast orthogonal dictionary learning method for sparse image representation. With comparable performance on several image restoration tasks, the proposed method is much more computationally efficient than the over-complete dictionary based learning methods.</p><p>3 0.74291867 <a title="51-lsi-3" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>Author: Chih-Yuan Yang, Ming-Hsuan Yang</p><p>Abstract: The goal of single-image super-resolution is to generate a high-quality high-resolution image based on a given low-resolution input. It is an ill-posed problem which requires exemplars or priors to better reconstruct the missing high-resolution image details. In this paper, we propose to split the feature space into numerous subspaces and collect exemplars to learn priors for each subspace, thereby creating effective mapping functions. The use of split input space facilitates both feasibility of using simple functionsfor super-resolution, and efficiency ofgenerating highresolution results. High-quality high-resolution images are reconstructed based on the effective learned priors. Experimental results demonstrate that theproposed algorithmperforms efficiently and effectively over state-of-the-art methods.</p><p>4 0.71039379 <a title="51-lsi-4" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>Author: Hua Wang, Feiping Nie, Weidong Cai, Heng Huang</p><p>Abstract: Representing the raw input of a data set by a set of relevant codes is crucial to many computer vision applications. Due to the intrinsic sparse property of real-world data, dictionary learning, in which the linear decomposition of a data point uses a set of learned dictionary bases, i.e., codes, has demonstrated state-of-the-art performance. However, traditional dictionary learning methods suffer from three weaknesses: sensitivity to noisy and outlier samples, difficulty to determine the optimal dictionary size, and incapability to incorporate supervision information. In this paper, we address these weaknesses by learning a Semi-Supervised Robust Dictionary (SSR-D). Specifically, we use the ℓ2,0+ norm as the loss function to improve the robustness against outliers, and develop a new structured sparse regularization com, , tom. . cai@sydney . edu . au , heng@uta .edu make the learning tasks easier to deal with and reduce the computational cost. For example, in image tagging, instead of using the raw pixel-wise features, semi-local or patch- based features, such as SIFT and geometric blur, are usually more desirable to achieve better performance. In practice, finding a set of compact features bases, also referred to as dictionary, with enhanced representative and discriminative power, plays a significant role in building a successful computer vision system. In this paper, we explore this important problem by proposing a novel formulation and its solution for learning Semi-Supervised Robust Dictionary (SSRD), where we examine the challenges in dictionary learning, and seek opportunities to overcome them and improve the dictionary qualities. 1.1. Challenges in Dictionary Learning to incorporate the supervision information in dictionary learning, without incurring additional parameters. Moreover, the optimal dictionary size is automatically learned from the input data. Minimizing the derived objective function is challenging because it involves many non-smooth ℓ2,0+ -norm terms. We present an efficient algorithm to solve the problem with a rigorous proof of the convergence of the algorithm. Extensive experiments are presented to show the superior performance of the proposed method.</p><p>5 0.70635551 <a title="51-lsi-5" href="./iccv-2013-Coupled_Dictionary_and_Feature_Space_Learning_with_Applications_to_Cross-Domain_Image_Synthesis_and_Recognition.html">96 iccv-2013-Coupled Dictionary and Feature Space Learning with Applications to Cross-Domain Image Synthesis and Recognition</a></p>
<p>Author: De-An Huang, Yu-Chiang Frank Wang</p><p>Abstract: Cross-domain image synthesis and recognition are typically considered as two distinct tasks in the areas of computer vision and pattern recognition. Therefore, it is not clear whether approaches addressing one task can be easily generalized or extended for solving the other. In this paper, we propose a unified model for coupled dictionary and feature space learning. The proposed learning model not only observes a common feature space for associating cross-domain image data for recognition purposes, the derived feature space is able to jointly update the dictionaries in each image domain for improved representation. This is why our method can be applied to both cross-domain image synthesis and recognition problems. Experiments on a variety of synthesis and recognition tasks such as single image super-resolution, cross-view action recognition, and sketchto-photo face recognition would verify the effectiveness of our proposed learning model.</p><p>6 0.69730645 <a title="51-lsi-6" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>7 0.63346368 <a title="51-lsi-7" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>8 0.60028106 <a title="51-lsi-8" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>9 0.56920975 <a title="51-lsi-9" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>10 0.56385964 <a title="51-lsi-10" href="./iccv-2013-Dictionary_Learning_and_Sparse_Coding_on_Grassmann_Manifolds%3A_An_Extrinsic_Solution.html">114 iccv-2013-Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution</a></p>
<p>11 0.5465309 <a title="51-lsi-11" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>12 0.52303606 <a title="51-lsi-12" href="./iccv-2013-A_Learning-Based_Approach_to_Reduce_JPEG_Artifacts_in_Image_Matting.html">19 iccv-2013-A Learning-Based Approach to Reduce JPEG Artifacts in Image Matting</a></p>
<p>13 0.50898498 <a title="51-lsi-13" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>14 0.48953766 <a title="51-lsi-14" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>15 0.47964537 <a title="51-lsi-15" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>16 0.4794251 <a title="51-lsi-16" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>17 0.45272356 <a title="51-lsi-17" href="./iccv-2013-Single-Patch_Low-Rank_Prior_for_Non-pointwise_Impulse_Noise_Removal.html">394 iccv-2013-Single-Patch Low-Rank Prior for Non-pointwise Impulse Noise Removal</a></p>
<p>18 0.44433007 <a title="51-lsi-18" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>19 0.44241309 <a title="51-lsi-19" href="./iccv-2013-SIFTpack%3A_A_Compact_Representation_for_Efficient_SIFT_Matching.html">365 iccv-2013-SIFTpack: A Compact Representation for Efficient SIFT Matching</a></p>
<p>20 0.4381147 <a title="51-lsi-20" href="./iccv-2013-Log-Euclidean_Kernels_for_Sparse_Representation_and_Dictionary_Learning.html">257 iccv-2013-Log-Euclidean Kernels for Sparse Representation and Dictionary Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.031), (7, 0.012), (26, 0.519), (31, 0.047), (42, 0.108), (64, 0.019), (73, 0.024), (89, 0.119), (97, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93672538 <a title="51-lda-1" href="./iccv-2013-Structured_Light_in_Sunlight.html">405 iccv-2013-Structured Light in Sunlight</a></p>
<p>Author: Mohit Gupta, Qi Yin, Shree K. Nayar</p><p>Abstract: Strong ambient illumination severely degrades the performance of structured light based techniques. This is especially true in outdoor scenarios, where the structured light sources have to compete with sunlight, whose power is often 2-5 orders of magnitude larger than the projected light. In this paper, we propose the concept of light-concentration to overcome strong ambient illumination. Our key observation is that given a fixed light (power) budget, it is always better to allocate it sequentially in several portions of the scene, as compared to spreading it over the entire scene at once. For a desired level of accuracy, we show that by distributing light appropriately, the proposed approach requires 1-2 orders lower acquisition time than existing approaches. Our approach is illumination-adaptive as the optimal light distribution is determined based on a measurement of the ambient illumination level. Since current light sources have a fixed light distribution, we have built a prototype light source that supports flexible light distribution by controlling the scanning speed of a laser scanner. We show several high quality 3D scanning results in a wide range of outdoor scenarios. The proposed approach will benefit 3D vision systems that need to operate outdoors under extreme ambient illumination levels on a limited time and power budget.</p><p>same-paper 2 0.90319777 <a title="51-lda-2" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>Author: Radu Timofte, Vincent De_Smet, Luc Van_Gool</p><p>Abstract: Recently there have been significant advances in image upscaling or image super-resolution based on a dictionary of low and high resolution exemplars. The running time of the methods is often ignored despite the fact that it is a critical factor for real applications. This paper proposes fast super-resolution methods while making no compromise on quality. First, we support the use of sparse learned dictionaries in combination with neighbor embedding methods. In this case, the nearest neighbors are computed using the correlation with the dictionary atoms rather than the Euclidean distance. Moreover, we show that most of the current approaches reach top performance for the right parameters. Second, we show that using global collaborative coding has considerable speed advantages, reducing the super-resolution mapping to a precomputed projective matrix. Third, we propose the anchored neighborhood regression. That is to anchor the neighborhood embedding of a low resolution patch to the nearest atom in the dictionary and to precompute the corresponding embedding matrix. These proposals are contrasted with current state-of- the-art methods on standard images. We obtain similar or improved quality and one or two orders of magnitude speed improvements.</p><p>3 0.89509821 <a title="51-lda-3" href="./iccv-2013-Slice_Sampling_Particle_Belief_Propagation.html">395 iccv-2013-Slice Sampling Particle Belief Propagation</a></p>
<p>Author: Oliver Müller, Michael Ying Yang, Bodo Rosenhahn</p><p>Abstract: Inference in continuous label Markov random fields is a challenging task. We use particle belief propagation (PBP) for solving the inference problem in continuous label space. Sampling particles from the belief distribution is typically done by using Metropolis-Hastings (MH) Markov chain Monte Carlo (MCMC) methods which involves sampling from a proposal distribution. This proposal distribution has to be carefully designed depending on the particular model and input data to achieve fast convergence. We propose to avoid dependence on a proposal distribution by introducing a slice sampling based PBP algorithm. The proposed approach shows superior convergence performance on an image denoising toy example. Our findings are validated on a challenging relational 2D feature tracking application.</p><p>4 0.88284576 <a title="51-lda-4" href="./iccv-2013-Drosophila_Embryo_Stage_Annotation_Using_Label_Propagation.html">125 iccv-2013-Drosophila Embryo Stage Annotation Using Label Propagation</a></p>
<p>Author: Tomáš Kazmar, Evgeny Z. Kvon, Alexander Stark, Christoph H. Lampert</p><p>Abstract: In this work we propose a system for automatic classification of Drosophila embryos into developmental stages. While the system is designed to solve an actual problem in biological research, we believe that the principle underlying it is interesting not only for biologists, but also for researchers in computer vision. The main idea is to combine two orthogonal sources of information: one is a classifier trained on strongly invariant features, which makes it applicable to images of very different conditions, but also leads to rather noisy predictions. The other is a label propagation step based on a more powerful similarity measure that however is only consistent within specific subsets of the data at a time. In our biological setup, the information sources are the shape and the staining patterns of embryo images. We show experimentally that while neither of the methods can be used by itself to achieve satisfactory results, their combination achieves prediction quality comparable to human per- formance.</p><p>5 0.86474514 <a title="51-lda-5" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>Author: Abdelaziz Djelouah, Jean-Sébastien Franco, Edmond Boyer, François Le_Clerc, Patrick Pérez</p><p>Abstract: In this paper, we address the problem of object segmentation in multiple views or videos when two or more viewpoints of the same scene are available. We propose a new approach that propagates segmentation coherence information in both space and time, hence allowing evidences in one image to be shared over the complete set. To this aim the segmentation is cast as a single efficient labeling problem over space and time with graph cuts. In contrast to most existing multi-view segmentation methods that rely on some form of dense reconstruction, ours only requires a sparse 3D sampling to propagate information between viewpoints. The approach is thoroughly evaluated on standard multiview datasets, as well as on videos. With static views, results compete with state of the art methods but they are achieved with significantly fewer viewpoints. With multiple videos, we report results that demonstrate the benefit of segmentation propagation through temporal cues.</p><p>6 0.86165196 <a title="51-lda-6" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>7 0.85867709 <a title="51-lda-7" href="./iccv-2013-Refractive_Structure-from-Motion_on_Underwater_Images.html">348 iccv-2013-Refractive Structure-from-Motion on Underwater Images</a></p>
<p>8 0.80061424 <a title="51-lda-8" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>9 0.7680189 <a title="51-lda-9" href="./iccv-2013-A_Deformable_Mixture_Parsing_Model_with_Parselets.html">8 iccv-2013-A Deformable Mixture Parsing Model with Parselets</a></p>
<p>10 0.76739305 <a title="51-lda-10" href="./iccv-2013-Data-Driven_3D_Primitives_for_Single_Image_Understanding.html">102 iccv-2013-Data-Driven 3D Primitives for Single Image Understanding</a></p>
<p>11 0.69207346 <a title="51-lda-11" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>12 0.67629641 <a title="51-lda-12" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>13 0.66173828 <a title="51-lda-13" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>14 0.65978271 <a title="51-lda-14" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>15 0.64313066 <a title="51-lda-15" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>16 0.63675123 <a title="51-lda-16" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>17 0.62275541 <a title="51-lda-17" href="./iccv-2013-Uncertainty-Driven_Efficiently-Sampled_Sparse_Graphical_Models_for_Concurrent_Tumor_Segmentation_and_Atlas_Registration.html">432 iccv-2013-Uncertainty-Driven Efficiently-Sampled Sparse Graphical Models for Concurrent Tumor Segmentation and Atlas Registration</a></p>
<p>18 0.61679411 <a title="51-lda-18" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<p>19 0.61635673 <a title="51-lda-19" href="./iccv-2013-Progressive_Multigrid_Eigensolvers_for_Multiscale_Spectral_Segmentation.html">329 iccv-2013-Progressive Multigrid Eigensolvers for Multiscale Spectral Segmentation</a></p>
<p>20 0.61432284 <a title="51-lda-20" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
