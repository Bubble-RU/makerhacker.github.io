<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>153 iccv-2013-Face Recognition Using Face Patch Networks</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-153" href="#">iccv2013-153</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>153 iccv-2013-Face Recognition Using Face Patch Networks</h1>
<br/><p>Source: <a title="iccv-2013-153-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Lu_Face_Recognition_Using_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Chaochao Lu, Deli Zhao, Xiaoou Tang</p><p>Abstract: When face images are taken in the wild, the large variations in facial pose, illumination, and expression make face recognition challenging. The most fundamental problem for face recognition is to measure the similarity between faces. The traditional measurements such as various mathematical norms, Hausdorff distance, and approximate geodesic distance cannot accurately capture the structural information between faces in such complex circumstances. To address this issue, we develop a novel face patch network, based on which we define a new similarity measure called the random path (RP) measure. The RP measure is derivedfrom the collective similarity ofpaths by performing random walks in the network. It can globally characterize the contextual and curved structures of the face space. To apply the RP measure, we construct two kinds of networks: . cuhk . edu . hk the in-face network and the out-face network. The in-face network is drawn from any two face images and captures the local structural information. The out-face network is constructed from all the training face patches, thereby modeling the global structures of face space. The two face networks are structurally complementary and can be combined together to improve the recognition performance. Experiments on the Multi-PIE and LFW benchmarks show that the RP measure outperforms most of the state-of-art algorithms for face recognition.</p><p>Reference: <a title="iccv-2013-153-reference" href="../iccv2013_reference/iccv-2013-Face_Recognition_Using_Face_Patch_Networks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract When face images are taken in the wild, the large variations in facial pose, illumination, and expression make face recognition challenging. [sent-2, score-0.507]
</p><p>2 The most fundamental problem for face recognition is to measure the similarity between faces. [sent-3, score-0.421]
</p><p>3 The traditional measurements such as various mathematical norms, Hausdorff distance, and approximate geodesic distance cannot accurately capture the structural information between faces in such complex circumstances. [sent-4, score-0.325]
</p><p>4 To address this issue, we develop a novel face patch network, based on which we define a new similarity measure called the random path (RP) measure. [sent-5, score-0.848]
</p><p>5 hk  the in-face network and the out-face network. [sent-11, score-0.386]
</p><p>6 The in-face network is drawn from any two face images and captures the local structural information. [sent-12, score-0.766]
</p><p>7 The out-face network is constructed from all the training face patches, thereby modeling the global structures of face space. [sent-13, score-0.929]
</p><p>8 The two face networks are structurally complementary and can be combined together to improve the recognition performance. [sent-14, score-0.423]
</p><p>9 Experiments on the Multi-PIE and LFW benchmarks show  that the RP measure outperforms most of the state-of-art algorithms for face recognition. [sent-15, score-0.339]
</p><p>10 Illustration of the superiority of our random path (RP) measure over other measures (for example, Euclidean (E) measure and the shortest path (SP) measure). [sent-24, score-0.775]
</p><p>11 , pose, illumination, and expression), there may be underlying structures in face space (denoted by the red and blue clusters). [sent-27, score-0.299]
</p><p>12 > dSAPB if measured by Euclidean measure (solid green line) and the shortest path measure (solid yellow line). [sent-30, score-0.534]
</p><p>13 If we consider their underlying structures and compute their similarity by our random path measure (dashed yellow line), we get dRAPA? [sent-33, score-0.479]
</p><p>14 Seminal studies in [27, 23, 20, 22] have revealed 33228881  that the diverse distributions of face images for one person may form the underlying manifold structures. [sent-41, score-0.29]
</p><p>15 For instance, computing the length of the shortest path in a network is very sensitive to noisy nodes. [sent-47, score-0.701]
</p><p>16 This paper reports on a new face similarity measure called random path (RP) measure, which was designed to overcome the above-mentioned problems. [sent-48, score-0.639]
</p><p>17 We first construct two novel face patch networks: the in-face network and the out-face network, as shown in Figures 2 and 3. [sent-49, score-0.866]
</p><p>18 The in-face network is defined for any pair of faces. [sent-51, score-0.414]
</p><p>19 For each pair of faces, at each patch location, we use the two corresponding patch pairs and their eight neighboring patches to form a KNN graph, which we  call the in-face network. [sent-52, score-0.599]
</p><p>20 For each such in-face network, we propose a random path (RP) measure as the patch similarity 33228892  of the corresponding patch pair. [sent-53, score-0.815]
</p><p>21 The RP measure includes all paths of different lengths in the network, which enables it to capture more discriminative information in faces and significantly reduce the effect of noise and outliers. [sent-55, score-0.322]
</p><p>22 Since the network is only constructed within two faces in this approach, we call it the in-face network. [sent-57, score-0.509]
</p><p>23 The out-face network is built in a similar fashion. [sent-58, score-0.386]
</p><p>24 Instead of using local neighboring patches to form the network, for each patch we search a database of face patches and find similar patches in the same location neighbors of the patch to form the patch pair network. [sent-59, score-1.336]
</p><p>25 Since the search is conducted globally over the training space, the outface network captures more global structural information. [sent-60, score-0.573]
</p><p>26 Because the two networks describe the local and global structural information respectively, the similarities derived from the RP measure on these two networks can be combined to boost the recognition performance. [sent-61, score-0.555]
</p><p>27 By means of the RP measure on the in-face and outface networks, our RP measure performs significantly better than existing measures for face verification on two challenging face  datasets, LFW [11] and Multi-PIE [9]. [sent-62, score-0.775]
</p><p>28 Related Work From the point of view of data structures, there are two types of measures for face recognition: non-structure-based measures [1, 25, 32, 10, 4] and structure-based measures [29, 27, 21]. [sent-65, score-0.386]
</p><p>29 Studies on similarity measure in face recognition mainly focus on the non-structured metrics. [sent-68, score-0.421]
</p><p>30 In [4], the component similarity is measured by L2 distance between the corresponding descriptors of the face pair. [sent-70, score-0.361]
</p><p>31 A partial Hausdorff distance measure was defined in [25], where a pixel in one face could be matched with any other pixel with the same local  binary pattern in the other face. [sent-71, score-0.376]
</p><p>32 Similarly, in [10], a robust elastic and partial matching metric was presented, where each descriptor in one face is matched with its spatial neighboring descriptors in another face and the minimal distance is regarded as their dis-similarity. [sent-72, score-0.559]
</p><p>33 Isomap models the structural proximity between two data points by the geodesic distance that can be approximated by the shortest path length. [sent-75, score-0.543]
</p><p>34 Although these approaches can capture the manifold structures, their performance often degrades significantly with the existence of noise or outliers, because the shortest path is sensitive to noisy perturbations. [sent-79, score-0.339]
</p><p>35 Even for a patch p  ×  cropped from the face, its micro-structure is continuously connected with that of patches around patch p. [sent-82, score-0.59]
</p><p>36 Therefore, it is more convincing to take the neighboring patches into consideration when locally comparing the similarity between two faces from patch pair. [sent-84, score-0.57]
</p><p>37 To do this, we sample the r neighboring patches around patch p. [sent-85, score-0.362]
</p><p>38 Figure 2 (b) shows the patch examples in the case of r = 8 for face a and face b. [sent-86, score-0.693]
</p><p>39 Figure 2 (c) shows a 2-NN network of facial patches. [sent-92, score-0.409]
</p><p>40 The network constructed between two faces is called the in-face network. [sent-93, score-0.509]
</p><p>41 For instance, each entry of (I zP) −1 presents the global −  33228903  similarity for the corresponding nodes [13], where P is the adjacency matrix of the network. [sent-97, score-0.364]
</p><p>42 The nodal similarity is defined by all the connected paths between nodes. [sent-98, score-0.305]
</p><p>43 With the similarity matrix (I−zP)−1, we can measure the structural compactness roixf (tIhe− znPet)work by the average similarity between all nodes. [sent-99, score-0.456]
</p><p>44 To this end, we define the path centrality CG = − zP)−11. [sent-100, score-0.488]
</p><p>45 The more compact the network is, the larger (thIe − path centrality is. [sent-101, score-0.874]
</p><p>46 With path centrality CG, it will be easy to measure the similarity of patch pair in the network framework. [sent-102, score-1.29]
</p><p>47 To make the analysis clearer, we let Gpa ∪ Gqb denote the network constructed from patch p in face∪ a Gand patch q in face b, as shown in Figure 2 (a), where Gpa is the sub-network of patch p and Gqb is the sub-network of patch q. [sent-103, score-1.49]
</p><p>48 It is straightforward to know that the more similar the patch p and the patch q are, the more mutually connected paths there are between Gpa and Gqb. [sent-104, score-0.576]
</p><p>49 To model such structural correlation, we further construct a network from the patches at the same positions of all faces. [sent-110, score-0.668]
</p><p>50 To integrate these  component networks, for each patch in a face, we link it with its r most similar patches in the neighbor position in the training data, as Figures 3 (c) shows. [sent-112, score-0.324]
</p><p>51 Thus we derive a global network that cover the structural correlation between faces and within faces. [sent-113, score-0.644]
</p><p>52 The random path measure will be adopted both in the in-face and out-face networks. [sent-115, score-0.315]
</p><p>53 So we first present the formulation of the random path measure and then the construction of the in-face and out-face networks follows. [sent-116, score-0.46]
</p><p>54 The Random Path Measure Let G denote a network with N nodes {x1, . [sent-119, score-0.443]
</p><p>55 Inspired by concepts in social network analysis [17], we introduce the definition of path centrality CG for the network G. [sent-128, score-1.34]
</p><p>56 ∈ St  = i∈,  vt  (2)  = j  which is the sum of the products of the weights over all paths of length t that start at node xi and end at node xj in G. [sent-144, score-0.324]
</p><p>57 From this point of view, the path centrality is to measure the structural compactness of the network G by all paths of all lengths between all the connected nodes in G. [sent-149, score-1.351]
</p><p>58 With the definition of path centrality, the RP measure can be naturally used to compute the similarity between two networks. [sent-151, score-0.447]
</p><p>59 From the definition of path centrality, it makes sense that the two sub-networks in G have the most similar  structures in the sense of path centrality if they share the most paths. [sent-152, score-0.789]
</p><p>60 Therefore, for two given networks Gi and Gj, the definition of our RP measure can be defined as follows. [sent-154, score-0.292]
</p><p>61 In the definition above, the union path centrality CGi∪Gj is written as CGi∪Gj =  |Gi∪1 Gj|1T(I − zPGi∪Gj)−11. [sent-156, score-0.573]
</p><p>62 The RP measure ΦGi∪Gj embodies the structural information about all paths between Gi and Gj . [sent-158, score-0.363]
</p><p>63 CGi∪Gj measures not only the structural information within Gi and Gj, but also that through all paths between Gi and Gj . [sent-163, score-0.314]
</p><p>64 The larger the value of ΦGi∪Gj , the more structural information the two networks share, meaning that these two networks have more similar structures. [sent-164, score-0.428]
</p><p>65 Therefore, ΦGi∪Gj can be exploited to measure the structural similarity between two networks. [sent-165, score-0.317]
</p><p>66 The RP measure takes all paths between two networks into consideration to measure their similarity, not only the shortest path such as [29, 27]. [sent-166, score-0.811]
</p><p>67 Besides, we take the average value of nodal centrality (I − zP) −1 1. [sent-168, score-0.335]
</p><p>68 In-face Network  Figure 2 presents the in-face network pipeline. [sent-172, score-0.386]
</p><p>69 We densely partition the face image into M = K K overlapping patches of size n n (n = 16 in our settings). [sent-173, score-0.357]
</p><p>70 To build an in-face network for the patches at (i, j), we take fiaj in Fa and in Fb. [sent-185, score-0.697]
</p><p>71 Thus, we get the (2 + 2r) feature vectors of patches that are utilized to construct a KNN network Gij for the patch pair of fiaj and . [sent-189, score-0.963]
</p><p>72 Therefore, the adjacency matrix PGiaj of the network Giaj corresponding to fiaj and its r neighbors  fibj  fibj  fibj  is the sub-matrix of Gij identified by the indices of fiaj and its r neighbors. [sent-191, score-1.301]
</p><p>73 For the patch pair of fiaj and we calculate their path centralities as follows:  fibj,  CGiaj CGbij  =|G1iaj|1T(I − zPGiaj)−11, =|G1ibj|1T(I − zPGibj)−11,  CGiaj∪Gibj = CGij =  (5)  |G1ij|1T(I − zPGij)−11. [sent-194, score-0.7]
</p><p>74 Applying the RP measure gives the similarity measure of the patch pair  Siinj = ΦGiaj∪Gibj  = CGiaj∪Gibj − (CGiaj  + CGbij). [sent-195, score-0.513]
</p><p>75 (7)  completes the process of applying the RP measure on the in-face network for two face images. [sent-204, score-0.725]
</p><p>76 We refer to the network presented above as the in-face network because the network is only constructed within two  face images. [sent-205, score-1.426]
</p><p>77 Only the structural information of patch pair and their neighborhoods is considered; therefore, the in-face network mainly conveys the local information. [sent-206, score-0.785]
</p><p>78 Out-face Network The proposed out-face network pipeline is shown in Figure 3. [sent-209, score-0.386]
</p><p>79 Unlike the in-face network, the construction of the out-face network requires the training data in an unsupervised way. [sent-210, score-0.386]
</p><p>80 , at (i,j) in the training  fiTj} set to construct a KNN network Gigjlobal. [sent-221, score-0.415]
</p><p>81 Further, to preserve the structural proximity between fitj and its neighbors at (i, j) in each face, we connect fitj with all of its 8 neighbors. [sent-223, score-0.378]
</p><p>82 Therefore, by the connections of neighborhoods, the M independent Gigjlobal  ×  are linked together to form the final global network Gglobal with the weighted adjacency matrix . [sent-225, score-0.583]
</p><p>83 Given a test face image a, we search its rNN most similar patches in for each fiaj, and then for each selected patch, we also select its 8 neighbor patches together to form the initial Ga. [sent-226, score-0.472]
</p><p>84 This search method can guarantee that the acquired similar patches are among the spatially semantic neighbors of fiaj in other face images. [sent-227, score-0.609]
</p><p>85 + W 1e) d×el Mete some duplicates ffirnoamll yth seemle catendd use the remaining nodes to extract the sub-network Ga from Gglobal with its corresponding sub-matrix PGa from Gb and PGb can be acquired in the same way for face b. [sent-229, score-0.299]
</p><p>86 By merging nodes in Ga and Gb, we can draw the union network Ga ∪ Gb and its adjacency matrix PGa∪Gb from Gglobal and Pg∪lo Gbal. [sent-230, score-0.651]
</p><p>87 After acquiring PGa , PGb , and PGa∪Gb for face a and face b, it is straightforward to compute ∪thGeir path centralities: CGa , CGb , and CGa∪Gb according to Definition 1. [sent-231, score-0.702]
</p><p>88 (9) sout describes the structural information of two face images from the global view. [sent-233, score-0.454]
</p><p>89 Since the construction of this network requires the training data and the each test face needs to be projected on it, we call the network the out-face network. [sent-234, score-1.014]
</p><p>90 sout  Pglobal  for the nearest neighbors for each patch is fast because the search operation is only made in Gigjlobal instead of Gglobal. [sent-236, score-0.339]
</p><p>91 The Fusion Method From the analysis above, it is clear that the in-face network and the out-face network are structurally complementary. [sent-239, score-0.808]
</p><p>92 To improve the discriminative capability of the networks, we present a simple fusion method to combine them sfinal = [αsin, (1 − α)sout], (10) where sfinal is the combined similarity vector of two face images, and α is a free parameter learned from the training data. [sent-240, score-0.496]
</p><p>93 This fusion method can effectively combine the advantages of the in-face network and the out-face network. [sent-241, score-0.41]
</p><p>94 Weighted Adjacency Matrix The weight P(i, j) of the edge connecting node xi and node xj in the network is defined as  P(i,j) =? [sent-245, score-0.553]
</p><p>95 eriments In this section, we conduct experiments on face verification to validate the effectiveness of our RP measure based on the in-face and out-face networks. [sent-258, score-0.339]
</p><p>96 The face data we use are two widely used face databases: the MultiPIE dataset [9] and the LFW dataset [11]. [sent-259, score-0.484]
</p><p>97 The Strain is applied to construct the global network Gglobal employed in Section 4. [sent-264, score-0.415]
</p><p>98 To perform the fair comparison with the recent algorithms in face recognition, we follow the procedures in [4] to crop  ×  faces and each cropped face is resized to 84 96 pixels fwacithes sth aen eyes ahn cdr ompopuethd corners aligned. [sent-274, score-0.608]
</p><p>99 ’s method [10], and the shortest path [26, 29], on four popular descriptors in face recognition: LBP [18], HOG [7], Gabor [32], and LE [4]. [sent-276, score-0.557]
</p><p>100 The first two parameters are the number of nearest  1  Kin  1There are also three relatively unimportant parameters: the size of the patch (n), the patch sampling step (s), and the number of the patch’s neighbors (r). [sent-281, score-0.474]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('network', 0.386), ('centrality', 0.27), ('rp', 0.259), ('gj', 0.255), ('face', 0.242), ('path', 0.218), ('patch', 0.209), ('fiaj', 0.196), ('zp', 0.151), ('cgi', 0.147), ('gibj', 0.147), ('networks', 0.145), ('adjacency', 0.143), ('structural', 0.138), ('paths', 0.128), ('gi', 0.118), ('patches', 0.115), ('cgiaj', 0.098), ('fibj', 0.098), ('gglobal', 0.098), ('giaj', 0.098), ('gigjlobal', 0.098), ('shortest', 0.097), ('faces', 0.097), ('measure', 0.097), ('pga', 0.087), ('similarity', 0.082), ('lfw', 0.076), ('gb', 0.074), ('sfinal', 0.074), ('sout', 0.074), ('gij', 0.069), ('gpa', 0.065), ('fitj', 0.065), ('nodal', 0.065), ('hausdorff', 0.06), ('cg', 0.058), ('nodes', 0.057), ('neighbors', 0.056), ('entry', 0.052), ('definition', 0.05), ('centralities', 0.049), ('cga', 0.049), ('cgbij', 0.049), ('cgj', 0.049), ('gqb', 0.049), ('nik', 0.049), ('outface', 0.049), ('pgb', 0.049), ('rnn', 0.049), ('siinj', 0.049), ('xj', 0.048), ('measures', 0.048), ('node', 0.047), ('knn', 0.047), ('ga', 0.046), ('strain', 0.044), ('folders', 0.044), ('isomap', 0.04), ('neighboring', 0.038), ('fij', 0.038), ('distance', 0.037), ('cuhk', 0.036), ('structurally', 0.036), ('figures', 0.036), ('union', 0.035), ('mmd', 0.035), ('multipie', 0.035), ('structures', 0.033), ('similarities', 0.03), ('connected', 0.03), ('matrix', 0.03), ('social', 0.03), ('connect', 0.03), ('geodesic', 0.029), ('consideration', 0.029), ('vt', 0.029), ('construct', 0.029), ('kong', 0.028), ('pair', 0.028), ('pt', 0.028), ('compactness', 0.027), ('dist', 0.027), ('cropped', 0.027), ('walks', 0.026), ('constructed', 0.026), ('fb', 0.025), ('yellow', 0.025), ('fa', 0.025), ('xi', 0.025), ('measurements', 0.024), ('fusion', 0.024), ('neighborhoods', 0.024), ('weighted', 0.024), ('proximity', 0.024), ('underlying', 0.024), ('manifold', 0.024), ('manifolds', 0.023), ('facial', 0.023), ('correlation', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="153-tfidf-1" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>Author: Chaochao Lu, Deli Zhao, Xiaoou Tang</p><p>Abstract: When face images are taken in the wild, the large variations in facial pose, illumination, and expression make face recognition challenging. The most fundamental problem for face recognition is to measure the similarity between faces. The traditional measurements such as various mathematical norms, Hausdorff distance, and approximate geodesic distance cannot accurately capture the structural information between faces in such complex circumstances. To address this issue, we develop a novel face patch network, based on which we define a new similarity measure called the random path (RP) measure. The RP measure is derivedfrom the collective similarity ofpaths by performing random walks in the network. It can globally characterize the contextual and curved structures of the face space. To apply the RP measure, we construct two kinds of networks: . cuhk . edu . hk the in-face network and the out-face network. The in-face network is drawn from any two face images and captures the local structural information. The out-face network is constructed from all the training face patches, thereby modeling the global structures of face space. The two face networks are structurally complementary and can be combined together to improve the recognition performance. Experiments on the Multi-PIE and LFW benchmarks show that the RP measure outperforms most of the state-of-art algorithms for face recognition.</p><p>2 0.1817036 <a title="153-tfidf-2" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>Author: Yizhe Zhang, Ming Shao, Edward K. Wong, Yun Fu</p><p>Abstract: One of the most challenging task in face recognition is to identify people with varied poses. Namely, the test faces have significantly different poses compared with the registered faces. In this paper, we propose a high-level feature learning scheme to extract pose-invariant identity feature for face recognition. First, we build a single-hiddenlayer neural network with sparse constraint, to extractposeinvariant feature in a supervised fashion. Second, we further enhance the discriminative capability of the proposed feature by using multiple random faces as the target values for multiple encoders. By enforcing the target values to be uniquefor inputfaces over differentposes, the learned highlevel feature that is represented by the neurons in the hidden layer is pose free and only relevant to the identity information. Finally, we conduct face identification on CMU MultiPIE, and verification on Labeled Faces in the Wild (LFW) databases, where identification rank-1 accuracy and face verification accuracy with ROC curve are reported. These experiments demonstrate that our model is superior to oth- er state-of-the-art approaches on handling pose variations.</p><p>3 0.17529632 <a title="153-tfidf-3" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>Author: David Eigen, Dilip Krishnan, Rob Fergus</p><p>Abstract: Photographs taken through a window are often compromised by dirt or rain present on the window surface. Common cases of this include pictures taken from inside a vehicle, or outdoor security cameras mounted inside a protective enclosure. At capture time, defocus can be used to remove the artifacts, but this relies on achieving a shallow depth-of-field and placement of the camera close to the window. Instead, we present a post-capture image processing solution that can remove localized rain and dirt artifacts from a single image. We collect a dataset of clean/corrupted image pairs which are then used to train a specialized form of convolutional neural network. This learns how to map corrupted image patches to clean ones, implicitly capturing the characteristic appearance of dirt and water droplets in natural images. Our models demonstrate effective removal of dirt and rain in outdoor test conditions.</p><p>4 0.16898789 <a title="153-tfidf-4" href="./iccv-2013-Video_Motion_for_Every_Visible_Point.html">441 iccv-2013-Video Motion for Every Visible Point</a></p>
<p>Author: Susanna Ricco, Carlo Tomasi</p><p>Abstract: Dense motion of image points over many video frames can provide important information about the world. However, occlusions and drift make it impossible to compute long motionpaths by merely concatenating opticalflow vectors between consecutive frames. Instead, we solve for entire paths directly, and flag the frames in which each is visible. As in previous work, we anchor each path to a unique pixel which guarantees an even spatial distribution of paths. Unlike earlier methods, we allow paths to be anchored in any frame. By explicitly requiring that at least one visible path passes within a small neighborhood of every pixel, we guarantee complete coverage of all visible points in all frames. We achieve state-of-the-art results on real sequences including both rigid and non-rigid motions with significant occlusions.</p><p>5 0.15931271 <a title="153-tfidf-5" href="./iccv-2013-A_Non-parametric_Bayesian_Network_Prior_of_Human_Pose.html">24 iccv-2013-A Non-parametric Bayesian Network Prior of Human Pose</a></p>
<p>Author: Andreas M. Lehrmann, Peter V. Gehler, Sebastian Nowozin</p><p>Abstract: Having a sensible prior of human pose is a vital ingredient for many computer vision applications, including tracking and pose estimation. While the application of global non-parametric approaches and parametric models has led to some success, finding the right balance in terms of flexibility and tractability, as well as estimating model parameters from data has turned out to be challenging. In this work, we introduce a sparse Bayesian network model of human pose that is non-parametric with respect to the estimation of both its graph structure and its local distributions. We describe an efficient sampling scheme for our model and show its tractability for the computation of exact log-likelihoods. We empirically validate our approach on the Human 3.6M dataset and demonstrate superior performance to global models and parametric networks. We further illustrate our model’s ability to represent and compose poses not present in the training set (compositionality) and describe a speed-accuracy trade-off that allows realtime scoring of poses.</p><p>6 0.14595132 <a title="153-tfidf-6" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>7 0.13293365 <a title="153-tfidf-7" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>8 0.12953272 <a title="153-tfidf-8" href="./iccv-2013-Hybrid_Deep_Learning_for_Face_Verification.html">206 iccv-2013-Hybrid Deep Learning for Face Verification</a></p>
<p>9 0.12846152 <a title="153-tfidf-9" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>10 0.11870322 <a title="153-tfidf-10" href="./iccv-2013-Deep_Learning_Identity-Preserving_Face_Space.html">106 iccv-2013-Deep Learning Identity-Preserving Face Space</a></p>
<p>11 0.11764824 <a title="153-tfidf-11" href="./iccv-2013-Find_the_Best_Path%3A_An_Efficient_and_Accurate_Classifier_for_Image_Hierarchies.html">165 iccv-2013-Find the Best Path: An Efficient and Accurate Classifier for Image Hierarchies</a></p>
<p>12 0.11559611 <a title="153-tfidf-12" href="./iccv-2013-Tree_Shape_Priors_with_Connectivity_Constraints_Using_Convex_Relaxation_on_General_Graphs.html">429 iccv-2013-Tree Shape Priors with Connectivity Constraints Using Convex Relaxation on General Graphs</a></p>
<p>13 0.1096741 <a title="153-tfidf-13" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>14 0.099973395 <a title="153-tfidf-14" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>15 0.092965364 <a title="153-tfidf-15" href="./iccv-2013-Single-Patch_Low-Rank_Prior_for_Non-pointwise_Impulse_Noise_Removal.html">394 iccv-2013-Single-Patch Low-Rank Prior for Non-pointwise Impulse Noise Removal</a></p>
<p>16 0.092659421 <a title="153-tfidf-16" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>17 0.092614084 <a title="153-tfidf-17" href="./iccv-2013-Coupling_Alignments_with_Recognition_for_Still-to-Video_Face_Recognition.html">97 iccv-2013-Coupling Alignments with Recognition for Still-to-Video Face Recognition</a></p>
<p>18 0.092041157 <a title="153-tfidf-18" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>19 0.091916457 <a title="153-tfidf-19" href="./iccv-2013-Pose-Free_Facial_Landmark_Fitting_via_Optimized_Part_Mixtures_and_Cascaded_Deformable_Shape_Model.html">321 iccv-2013-Pose-Free Facial Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Shape Model</a></p>
<p>20 0.090929657 <a title="153-tfidf-20" href="./iccv-2013-DCSH_-_Matching_Patches_in_RGBD_Images.html">101 iccv-2013-DCSH - Matching Patches in RGBD Images</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.174), (1, 0.022), (2, -0.071), (3, -0.091), (4, -0.032), (5, -0.043), (6, 0.159), (7, 0.077), (8, 0.025), (9, -0.052), (10, -0.068), (11, 0.014), (12, 0.039), (13, 0.043), (14, 0.038), (15, 0.109), (16, -0.041), (17, -0.098), (18, -0.033), (19, 0.091), (20, -0.054), (21, -0.079), (22, 0.107), (23, -0.067), (24, -0.045), (25, 0.099), (26, -0.06), (27, -0.043), (28, -0.022), (29, 0.098), (30, -0.01), (31, -0.015), (32, 0.068), (33, -0.011), (34, -0.064), (35, -0.007), (36, 0.01), (37, -0.092), (38, 0.123), (39, -0.131), (40, -0.132), (41, -0.008), (42, 0.09), (43, 0.005), (44, 0.087), (45, -0.083), (46, 0.138), (47, 0.044), (48, 0.046), (49, -0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96694756 <a title="153-lsi-1" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>Author: Chaochao Lu, Deli Zhao, Xiaoou Tang</p><p>Abstract: When face images are taken in the wild, the large variations in facial pose, illumination, and expression make face recognition challenging. The most fundamental problem for face recognition is to measure the similarity between faces. The traditional measurements such as various mathematical norms, Hausdorff distance, and approximate geodesic distance cannot accurately capture the structural information between faces in such complex circumstances. To address this issue, we develop a novel face patch network, based on which we define a new similarity measure called the random path (RP) measure. The RP measure is derivedfrom the collective similarity ofpaths by performing random walks in the network. It can globally characterize the contextual and curved structures of the face space. To apply the RP measure, we construct two kinds of networks: . cuhk . edu . hk the in-face network and the out-face network. The in-face network is drawn from any two face images and captures the local structural information. The out-face network is constructed from all the training face patches, thereby modeling the global structures of face space. The two face networks are structurally complementary and can be combined together to improve the recognition performance. Experiments on the Multi-PIE and LFW benchmarks show that the RP measure outperforms most of the state-of-art algorithms for face recognition.</p><p>2 0.6297183 <a title="153-lsi-2" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>Author: David Eigen, Dilip Krishnan, Rob Fergus</p><p>Abstract: Photographs taken through a window are often compromised by dirt or rain present on the window surface. Common cases of this include pictures taken from inside a vehicle, or outdoor security cameras mounted inside a protective enclosure. At capture time, defocus can be used to remove the artifacts, but this relies on achieving a shallow depth-of-field and placement of the camera close to the window. Instead, we present a post-capture image processing solution that can remove localized rain and dirt artifacts from a single image. We collect a dataset of clean/corrupted image pairs which are then used to train a specialized form of convolutional neural network. This learns how to map corrupted image patches to clean ones, implicitly capturing the characteristic appearance of dirt and water droplets in natural images. Our models demonstrate effective removal of dirt and rain in outdoor test conditions.</p><p>3 0.61886448 <a title="153-lsi-3" href="./iccv-2013-Face_Recognition_via_Archetype_Hull_Ranking.html">154 iccv-2013-Face Recognition via Archetype Hull Ranking</a></p>
<p>Author: Yuanjun Xiong, Wei Liu, Deli Zhao, Xiaoou Tang</p><p>Abstract: The archetype hull model is playing an important role in large-scale data analytics and mining, but rarely applied to vision problems. In this paper, we migrate such a geometric model to address face recognition and verification together through proposing a unified archetype hull ranking framework. Upon a scalable graph characterized by a compact set of archetype exemplars whose convex hull encompasses most of the training images, the proposed framework explicitly captures the relevance between any query and the stored archetypes, yielding a rank vector over the archetype hull. The archetype hull ranking is then executed on every block of face images to generate a blockwise similarity measure that is achieved by comparing two different rank vectors with respect to the same archetype hull. After integrating blockwise similarity measurements with learned importance weights, we accomplish a sensible face similarity measure which can support robust and effective face recognition and verification. We evaluate the face similarity measure in terms of experiments performed on three benchmark face databases Multi-PIE, Pubfig83, and LFW, demonstrat- ing its performance superior to the state-of-the-arts.</p><p>4 0.60988939 <a title="153-lsi-4" href="./iccv-2013-Hybrid_Deep_Learning_for_Face_Verification.html">206 iccv-2013-Hybrid Deep Learning for Face Verification</a></p>
<p>Author: Yi Sun, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: This paper proposes a hybrid convolutional network (ConvNet)-Restricted Boltzmann Machine (RBM) model for face verification in wild conditions. A key contribution of this work is to directly learn relational visual features, which indicate identity similarities, from raw pixels of face pairs with a hybrid deep network. The deep ConvNets in our model mimic the primary visual cortex to jointly extract local relational visual features from two face images compared with the learned filter pairs. These relational features are further processed through multiple layers to extract high-level and global features. Multiple groups of ConvNets are constructed in order to achieve robustness and characterize face similarities from different aspects. The top-layerRBMperforms inferencefrom complementary high-level features extracted from different ConvNet groups with a two-level average pooling hierarchy. The entire hybrid deep network is jointly fine-tuned to optimize for the task of face verification. Our model achieves competitive face verification performance on the LFW dataset.</p><p>5 0.60283244 <a title="153-lsi-5" href="./iccv-2013-Markov_Network-Based_Unified_Classifier_for_Face_Identification.html">261 iccv-2013-Markov Network-Based Unified Classifier for Face Identification</a></p>
<p>Author: Wonjun Hwang, Kyungshik Roh, Junmo Kim</p><p>Abstract: We propose a novel unifying framework using a Markov network to learn the relationship between multiple classifiers in face recognition. We assume that we have several complementary classifiers and assign observation nodes to the features of a query image and hidden nodes to the features of gallery images. We connect each hidden node to its corresponding observation node and to the hidden nodes of other neighboring classifiers. For each observation-hidden node pair, we collect a set of gallery candidates that are most similar to the observation instance, and the relationship between the hidden nodes is captured in terms of the similarity matrix between the collected gallery images. Posterior probabilities in the hidden nodes are computed by the belief-propagation algorithm. The novelty of the proposed framework is the method that takes into account the classifier dependency using the results of each neighboring classifier. We present extensive results on two different evaluation protocols, known and unknown image variation tests, using three different databases, which shows that the proposed framework always leads to good accuracy in face recognition.</p><p>6 0.5995335 <a title="153-lsi-6" href="./iccv-2013-Deep_Learning_Identity-Preserving_Face_Space.html">106 iccv-2013-Deep Learning Identity-Preserving Face Space</a></p>
<p>7 0.58686584 <a title="153-lsi-7" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>8 0.5764119 <a title="153-lsi-8" href="./iccv-2013-Fast_High_Dimensional_Vector_Multiplication_Face_Recognition.html">158 iccv-2013-Fast High Dimensional Vector Multiplication Face Recognition</a></p>
<p>9 0.55980641 <a title="153-lsi-9" href="./iccv-2013-Hidden_Factor_Analysis_for_Age_Invariant_Face_Recognition.html">195 iccv-2013-Hidden Factor Analysis for Age Invariant Face Recognition</a></p>
<p>10 0.55313694 <a title="153-lsi-10" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>11 0.54956639 <a title="153-lsi-11" href="./iccv-2013-Video_Motion_for_Every_Visible_Point.html">441 iccv-2013-Video Motion for Every Visible Point</a></p>
<p>12 0.52059305 <a title="153-lsi-12" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>13 0.50174129 <a title="153-lsi-13" href="./iccv-2013-Coupling_Alignments_with_Recognition_for_Still-to-Video_Face_Recognition.html">97 iccv-2013-Coupling Alignments with Recognition for Still-to-Video Face Recognition</a></p>
<p>14 0.48452383 <a title="153-lsi-14" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>15 0.48343128 <a title="153-lsi-15" href="./iccv-2013-Complex_3D_General_Object_Reconstruction_from_Line_Drawings.html">84 iccv-2013-Complex 3D General Object Reconstruction from Line Drawings</a></p>
<p>16 0.48145032 <a title="153-lsi-16" href="./iccv-2013-Joint_Noise_Level_Estimation_from_Personal_Photo_Collections.html">223 iccv-2013-Joint Noise Level Estimation from Personal Photo Collections</a></p>
<p>17 0.47662485 <a title="153-lsi-17" href="./iccv-2013-Single-Patch_Low-Rank_Prior_for_Non-pointwise_Impulse_Noise_Removal.html">394 iccv-2013-Single-Patch Low-Rank Prior for Non-pointwise Impulse Noise Removal</a></p>
<p>18 0.46819195 <a title="153-lsi-18" href="./iccv-2013-A_Generalized_Low-Rank_Appearance_Model_for_Spatio-temporally_Correlated_Rain_Streaks.html">15 iccv-2013-A Generalized Low-Rank Appearance Model for Spatio-temporally Correlated Rain Streaks</a></p>
<p>19 0.46780318 <a title="153-lsi-19" href="./iccv-2013-Modifying_the_Memorability_of_Face_Photographs.html">272 iccv-2013-Modifying the Memorability of Face Photographs</a></p>
<p>20 0.46191418 <a title="153-lsi-20" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.121), (7, 0.024), (13, 0.013), (24, 0.276), (26, 0.083), (31, 0.033), (34, 0.01), (42, 0.115), (64, 0.056), (73, 0.023), (89, 0.117), (98, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77325982 <a title="153-lda-1" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>Author: Chaochao Lu, Deli Zhao, Xiaoou Tang</p><p>Abstract: When face images are taken in the wild, the large variations in facial pose, illumination, and expression make face recognition challenging. The most fundamental problem for face recognition is to measure the similarity between faces. The traditional measurements such as various mathematical norms, Hausdorff distance, and approximate geodesic distance cannot accurately capture the structural information between faces in such complex circumstances. To address this issue, we develop a novel face patch network, based on which we define a new similarity measure called the random path (RP) measure. The RP measure is derivedfrom the collective similarity ofpaths by performing random walks in the network. It can globally characterize the contextual and curved structures of the face space. To apply the RP measure, we construct two kinds of networks: . cuhk . edu . hk the in-face network and the out-face network. The in-face network is drawn from any two face images and captures the local structural information. The out-face network is constructed from all the training face patches, thereby modeling the global structures of face space. The two face networks are structurally complementary and can be combined together to improve the recognition performance. Experiments on the Multi-PIE and LFW benchmarks show that the RP measure outperforms most of the state-of-art algorithms for face recognition.</p><p>2 0.71431279 <a title="153-lda-2" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<p>Author: Tian Lan, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: The appearance of an object changes profoundly with pose, camera view and interactions of the object with other objects in the scene. This makes it challenging to learn detectors based on an object-level label (e.g., “car”). We postulate that having a richer set oflabelings (at different levels of granularity) for an object, including finer-grained subcategories, consistent in appearance and view, and higherorder composites – contextual groupings of objects consistent in their spatial layout and appearance, can significantly alleviate these problems. However, obtaining such a rich set of annotations, including annotation of an exponentially growing set of object groupings, is simply not feasible. We propose a weakly-supervised framework for object detection where we discover subcategories and the composites automatically with only traditional object-level category labels as input. To this end, we first propose an exemplar-SVM-based clustering approach, with latent SVM refinement, that discovers a variable length set of discriminative subcategories for each object class. We then develop a structured model for object detection that captures interactions among object subcategories and automatically discovers semantically meaningful and discriminatively relevant visual composites. We show that this model produces state-of-the-art performance on UIUC phrase object detection benchmark.</p><p>3 0.71168953 <a title="153-lda-3" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<p>Author: Nakamasa Inoue, Koichi Shinoda</p><p>Abstract: Assigning a visual code to a low-level image descriptor, which we call code assignment, is the most computationally expensive part of image classification algorithms based on the bag of visual word (BoW) framework. This paper proposes a fast computation method, Neighbor-toNeighbor (NTN) search, for this code assignment. Based on the fact that image features from an adjacent region are usually similar to each other, this algorithm effectively reduces the cost of calculating the distance between a codeword and a feature vector. This method can be applied not only to a hard codebook constructed by vector quantization (NTN-VQ), but also to a soft codebook, a Gaussian mixture model (NTN-GMM). We evaluated this method on the PASCAL VOC 2007 classification challenge task. NTN-VQ reduced the assignment cost by 77.4% in super-vector coding, and NTN-GMM reduced it by 89.3% in Fisher-vector coding, without any significant degradation in classification performance.</p><p>4 0.66891742 <a title="153-lda-4" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<p>Author: Federico Tombari, Alessandro Franchi, Luigi Di_Stefano</p><p>Abstract: Object detection in images withstanding significant clutter and occlusion is still a challenging task whenever the object surface is characterized by poor informative content. We propose to tackle this problem by a compact and distinctive representation of groups of neighboring line segments aggregated over limited spatial supports and invariant to rotation, translation and scale changes. Peculiarly, our proposal allows for leveraging on the inherent strengths of descriptor-based approaches, i.e. robustness to occlusion and clutter and scalability with respect to the size of the model library, also when dealing with scarcely textured objects.</p><p>5 0.61414182 <a title="153-lda-5" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>Author: S. Karthikeyan, Vignesh Jagadeesh, Renuka Shenoy, Miguel Ecksteinz, B.S. Manjunath</p><p>Abstract: Eye movement studies have confirmed that overt attention is highly biased towards faces and text regions in images. In this paper we explore a novel problem of predicting face and text regions in images using eye tracking data from multiple subjects. The problem is challenging as we aim to predict the semantics (face/text/background) only from eye tracking data without utilizing any image information. The proposed algorithm spatially clusters eye tracking data obtained in an image into different coherent groups and subsequently models the likelihood of the clusters containing faces and text using afully connectedMarkov Random Field (MRF). Given the eye tracking datafrom a test image, itpredicts potential face/head (humans, dogs and cats) and text locations reliably. Furthermore, the approach can be used to select regions of interest for further analysis by object detectors for faces and text. The hybrid eye position/object detector approach achieves better detection performance and reduced computation time compared to using only the object detection algorithm. We also present a new eye tracking dataset on 300 images selected from ICDAR, Street-view, Flickr and Oxford-IIIT Pet Dataset from 15 subjects.</p><p>6 0.61371189 <a title="153-lda-6" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>7 0.60822666 <a title="153-lda-7" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>8 0.60822076 <a title="153-lda-8" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>9 0.60801268 <a title="153-lda-9" href="./iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation.html">383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</a></p>
<p>10 0.60606921 <a title="153-lda-10" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>11 0.60552913 <a title="153-lda-11" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>12 0.60531861 <a title="153-lda-12" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>13 0.6051349 <a title="153-lda-13" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<p>14 0.60459697 <a title="153-lda-14" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>15 0.60430682 <a title="153-lda-15" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>16 0.60392672 <a title="153-lda-16" href="./iccv-2013-Quadruplet-Wise_Image_Similarity_Learning.html">332 iccv-2013-Quadruplet-Wise Image Similarity Learning</a></p>
<p>17 0.60384774 <a title="153-lda-17" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>18 0.60335863 <a title="153-lda-18" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>19 0.60278803 <a title="153-lda-19" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>20 0.60202181 <a title="153-lda-20" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
