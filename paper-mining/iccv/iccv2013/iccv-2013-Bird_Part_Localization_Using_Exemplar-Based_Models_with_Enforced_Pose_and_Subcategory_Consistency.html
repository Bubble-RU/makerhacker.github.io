<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-62" href="#">iccv2013-62</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</h1>
<br/><p>Source: <a title="iccv-2013-62-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Liu_Bird_Part_Localization_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Jiongxin Liu, Peter N. Belhumeur</p><p>Abstract: In this paper, we propose a novel approach for bird part localization, targeting fine-grained categories with wide variations in appearance due to different poses (including aspect and orientation) and subcategories. As it is challenging to represent such variations across a large set of diverse samples with tractable parametric models, we turn to individual exemplars. Specifically, we extend the exemplarbased models in [4] by enforcing pose and subcategory consistency at the parts. During training, we build posespecific detectors scoring part poses across subcategories, and subcategory-specific detectors scoring part appearance across poses. At the testing stage, likely exemplars are matched to the image, suggesting part locations whose pose and subcategory consistency are well-supported by the image cues. From these hypotheses, part configuration can be predicted with very high accuracy. Experimental results demonstrate significantperformance gainsfrom our method on an extensive dataset: CUB-200-2011 [30], for both localization and classification tasks.</p><p>Reference: <a title="iccv-2013-62-reference" href="../iccv2013_reference/iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu iu  Abstract In this paper, we propose a novel approach for bird part localization, targeting fine-grained categories with wide variations in appearance due to different poses (including aspect and orientation) and subcategories. [sent-4, score-0.403]
</p><p>2 Specifically, we extend the exemplarbased models in [4] by enforcing pose and subcategory consistency at the parts. [sent-6, score-1.093]
</p><p>3 During training, we build posespecific detectors scoring part poses across subcategories, and subcategory-specific detectors scoring part appearance across poses. [sent-7, score-0.681]
</p><p>4 At the testing stage, likely exemplars are matched to the image, suggesting part locations whose pose and subcategory consistency are well-supported by the image cues. [sent-8, score-1.467]
</p><p>5 ), parts capture useful information to differentiate subcategories, and generally the accuracy of part localization has a significant impact on the effectiveness of localized features representing the object. [sent-16, score-0.456]
</p><p>6 In this paper, we focus on birds as the test case with the goal of localizing the parts across different bird species. [sent-18, score-0.496]
</p><p>7 What makes detecting birds and bird parts difficult are the extreme variations in pose (e. [sent-19, score-0.73]
</p><p>8 As it is challenging to model the appearance variations of birds accurately with tractable parametric models, we turn to individual exemplars as in [4]. [sent-33, score-0.4]
</p><p>9 In contrast to [4] where exemplars only dictate the layout of parts, we propose to enforce pose and subcategory consistency at the parts. [sent-35, score-1.167]
</p><p>10 To do this, we design pose-specific detectors to score the pose types for each part, similar to [32]. [sent-36, score-0.471]
</p><p>11 Our novelty lies in that instead ofparameterizing the pose types in an objective function, we associate them with non-parametric exemplars, by which we strictly constrain the co-occurrence of part poses of a bird. [sent-37, score-0.52]
</p><p>12 Subcategory consistency means that the appearance at the detected parts should agree on the class membership. [sent-38, score-0.319]
</p><p>13 To enforce such consistency, we make use of the species labels to build species-specific part detectors. [sent-39, score-0.367]
</p><p>14 At the testing stage, we predict the part locations by matching likely exemplars to the image so that the estimated parts not only form a globally plausi-  ble configuration, but also satisfy the pose and subcategory consistency well. [sent-41, score-1.652]
</p><p>15 In this way, we achieve significantly more accurate part localization than previous methods, as shown in Fig. [sent-42, score-0.309]
</p><p>16 We propose the idea of enforcing subcategory consistency for part localization. [sent-46, score-0.964]
</p><p>17 We show how to impose strong constraints on the parts by using pose and subcategory consistency and associating them with exemplars. [sent-48, score-1.137]
</p><p>18 We produce state-of-the-art performance on an extensive bird dataset: CUB-200-201 1 [30], for both part localization and species classification. [sent-50, score-0.656]
</p><p>19 [2] combines top-down scanning part detectors and bottom-up region hypotheses to gen-  erate region-based features for semantic segmentation of objects; while we combine pose-specific and subcategoryspecific part detectors to predict part locations. [sent-56, score-0.909]
</p><p>20 Accurate part localization requires prior knowledge about the global shape. [sent-57, score-0.309]
</p><p>21 We also use exemplars, but we impose much stronger constraints on the parts by enforcing pose and subcategory consistency. [sent-67, score-1.074]
</p><p>22 In addition, we predict part visibilities which are not considered in [4]. [sent-68, score-0.287]
</p><p>23 Poselets [6] can also predict the part locations given a cluster of poselet activations. [sent-72, score-0.388]
</p><p>24 However, by design, Poselets do not target part localization, and the rough prediction of part locations from each poselet activation may deviate greatly from the correct positions. [sent-73, score-0.489]
</p><p>25 Also, its heuristic way of rescoring and grouping activations usually cannot generate the optimal group of activations, in which case part localization will be hurt much more than object detection. [sent-74, score-0.488]
</p><p>26 Pose and Subcategory Detectors As the building block of our method, we build part detectors that score pose-specific and subcategory-specific features. [sent-76, score-0.311]
</p><p>27 To do this, we group the samples of each part based on their poses and species memberships. [sent-77, score-0.417]
</p><p>28 Pose Grouping We obtain the pose grouping by using part annotations, as the keypoint configuration around a part roughly captures its pose, including aspect and orientation. [sent-80, score-0.704]
</p><p>29 Examples of the pose clusters and subcategory clusters for the part “Back”, which is marked with a red dot in each image. [sent-82, score-1.095]
</p><p>30 of part visibilities and locations for visible parts. [sent-84, score-0.387]
</p><p>31 Taking part iof Xk as an example, we represent its pose with a  [? [sent-85, score-0.449]
</p><p>32 2 (a) shows several examples of pose clusters for the  xkj ? [sent-113, score-0.273]
</p><p>33 For each pose cluster of each part, a detector is built using the samples in that cluster as positive samples, where a much larger set of negative samples are randomly drawn from image regions not containing that part of any type. [sent-118, score-0.546]
</p><p>34 So, by design, the detectors are trained to score the local pose across subcategories. [sent-119, score-0.39]
</p><p>35 Subcategory Grouping The underlying assumption of subcategory grouping is that samples from the same subcategory have similar appearance at the parts like colors, textures, etc. [sent-122, score-1.511]
</p><p>36 Given these labels, it is straightforward to obtain the subcategory grouping for each part. [sent-124, score-0.679]
</p><p>37 Similar to pose detectors, a subcategory detector is built for each cluster of each part. [sent-128, score-0.931]
</p><p>38 To make the subcategory detectors learn species-specific features in an effective way, we do two things during training: we first normalize the orientation of parts to reduce the noise in the features. [sent-129, score-0.919]
</p><p>39 The normalization is done by aligning each part sample to a reference part sample using Procrustes analysis with “reflection” enabled based on their local shape vectors defined in Sec. [sent-130, score-0.378]
</p><p>40 Secondly, we run the pose detectors exhaustively on the training images, and collect false activations (which  ×  are off the correct part locations) to form the negative training samples. [sent-133, score-0.695]
</p><p>41 Therefore, the subcategory detectors are able to learn subcategory-specific features across poses. [sent-134, score-0.772]
</p><p>42 Implementation Details  We use linear SVMs implemented in LIBSVM [10] to build pose and subcategory detectors. [sent-137, score-0.874]
</p><p>43 Because pose and subcategory detectors play different roles in our method (see Sec. [sent-143, score-1.018]
</p><p>44 For subcategory detectors, we extract three additional color histograms using 64 color bins, which are obtained through k-means in the RGB 22552222  color space of training images. [sent-146, score-0.628]
</p><p>45 Part Localization Approach  We cast the problem of part localization as fitting likely exemplars to an image, with the assumption that we can always find a similar configuration to the testing sample from a sufficiently large training set. [sent-149, score-0.599]
</p><p>46 1  where n denotes the number of parts, xik,t is the image location of part i, and di is the corresponding response map. [sent-157, score-0.301]
</p><p>47 Besides addressing the above issues, our major contribution is enforcing pose and subcategory consistency on Xk,t to obtain a more accurate estimation of P(Xk,t |I) . [sent-159, score-1.043]
</p><p>48 Pose Consistency To evaluate P(Xk,t |I) based on pose consistency, we generate a collection o|fI response maps sfeor c aonlls tihstee parts ×e agelln tehrea pose types, odenn oofte rde as Dp. [sent-162, score-0.792]
</p><p>49 mThapes key point ies ptharatts sf o×r each exemplar Xk, we know the visibility of each part; if a part is visible, we also know its pose type. [sent-163, score-0.532]
</p><p>50 So in evaluating P(Xk,t |Dp), we choose the response maps corresponding to the particular pose types of Xk. [sent-164, score-0.507]
</p><p>51 i1vki, (2) where vki denotes the visibility flag, dip[cik, sik,t] denotes the response map for pose type cik at scale sik,t. [sent-167, score-0.463]
</p><p>52 Because the exemplars usually cannot fit the configuration of a testing sample perfectly, the probability maps are smoothed before evaluating P(Xk,t |Dp). [sent-171, score-0.362]
</p><p>53 The filter radius is estimated by measuring the deviation between two corresponding parts from different exemplars after global alignment. [sent-173, score-0.324]
</p><p>54 Also, because of the reduced visual complexity in each pose cluster, each response map can give fairly accurate estimation of the part locations. [sent-175, score-0.52]
</p><p>55 , evaluating a fixed number of Xk,t’s) is independent of the number of pose types, as opposed to [32, 36]. [sent-180, score-0.273]
</p><p>56 Therefore, we can increase the number of pose types a lot. [sent-181, score-0.327]
</p><p>57 Subcategory Consistency Subcategory Consistency means that the appearance at all the parts should agree with each other on the subcategory membership. [sent-184, score-0.831]
</p><p>58 Here, we assume that the image cues are contained in Ds, a collection of response maps for all the parts all the subcategories. [sent-185, score-0.3]
</p><p>59 Given a subcategory l, we e pvaarltusat ×e tahlel tlhikeel siuhboocadt eogfo trhiee image region occupied by Xk,t matching a sample from that subcategory as  P(Xk,t|l,Ds) =⎛⎝i,v? [sent-186, score-1.256]
</p><p>60 ivki,  1  (3) where dsi[l, sik,t, θki,t] denotes the response map for part iof subcategory l, at scale sik,t and in orientation θki,t. [sent-188, score-0.938]
</p><p>61 We use the same  method as pose detector calibration to convert the response maps to probability maps. [sent-190, score-0.455]
</p><p>62 After computing P(Xk,t |l, Ds) for all possible l’s, P(Xk,t |I) based on subcategory c|ol,nDsistency is defined as P(Xk,t|Ds) =  malx  P(Xk,t|l, Ds). [sent-191, score-0.628]
</p><p>63 Select the two response maps for the chosen parts at the corresponding scales. [sent-199, score-0.3]
</p><p>64 Because we extract the features at the part location|sD dictated by the models, the errors in the part locations lead to underestimation of P(Xk,t |Ds), in which case incorrect models may rank higher than| correct ones. [sent-214, score-0.431]
</p><p>65 ,M, we first predict the visibility flag vi for each part i} through voting:  vi=? [sent-228, score-0.332]
</p><p>66 If a part is predicted as visible, we use the same method as [4] to estimate its location by combining the hypotheses and the probability maps corresponding to the relevant pose types. [sent-233, score-0.595]
</p><p>67 As can be seen here, pose detectors mainly play the role in finding the parts while subcategory detectors focus on verifying the hypotheses suggested by pose detectors. [sent-234, score-1.637]
</p><p>68 Dataset and Evaluation Metrics We test our method on CUB-200-201 1 [30] dataset, which contains 11,788 uncropped images of 200 bird species (about 60 images per species). [sent-238, score-0.347]
</p><p>69 A total of 15 parts were annotated by pixel location and visibility flag in each image through Mechanical Turk. [sent-241, score-0.301]
</p><p>70 “False Visibility Rate” is the percentage of parts that are incorrectly estimated as visible; “False Invisibility Rate” is the percentage of parts that are incorrectly estimated as invisible. [sent-246, score-0.34]
</p><p>71 How The Number of Pose Types Matters To examine the effect of the number of pose types, we  only consider pose consistency here (i. [sent-250, score-0.608]
</p><p>72 The best performance is achieved with 500 pose types for each part. [sent-263, score-0.327]
</p><p>73 granularity of pose types makes the constraints on pose consistency stronger. [sent-268, score-0.689]
</p><p>74 As the number of pose types goes beyond 1, 000, the performance becomes slightly worse, possibly due to the fact that there are much fewer positive training samples. [sent-269, score-0.327]
</p><p>75 Given more than 50 pose types, our method sig-  ×  nificantly outperforms [4] where a single non-linear detector is used. [sent-270, score-0.275]
</p><p>76 i6n×t by collapsing 0th0e0 probability maps of all 200 pose types for each part to a single probability map by taking pixel-wise maximum, thus reducing the number of types to 1. [sent-275, score-0.675]
</p><p>77 But the accuracy drops a lot, demonstrating the effect of enforcing pose consistency. [sent-276, score-0.299]
</p><p>78 As the visual appearance is coupled with pose, [14] actually groups samples similar in pose but with more noise than our pose clustering. [sent-279, score-0.549]
</p><p>79 For Poselets-based part localization, we obtain the poselet activations from the authors of [34], and follow [6] to predict the location of each part as the average prediction from its corresponding poselet activations. [sent-285, score-0.668]
</p><p>80 2, our part localization outperforms state-of-the-art techniques on all the metrics. [sent-299, score-0.309]
</p><p>81 1, our full model incorporating the subcategory consistency achieves remarkable improvement on AE. [sent-302, score-0.768]
</p><p>82 It indicates that pose consistency and subcategory consistency are complementary to each other. [sent-309, score-1.106]
</p><p>83 Some examples of our bird part localization are shown in Fig. [sent-318, score-0.456]
</p><p>84 Although birds have very wide variations in appearance and pose, and birds reside in very different environments, our method is still able to detect most of the parts correctly. [sent-320, score-0.53]
</p><p>85 Part-Based Species Classification To demonstrate how the accuracy of part localization affects the species classification, we feed the estimated part locations to our part-based classification method [20]. [sent-323, score-0.78]
</p><p>86 Specifically, we center 12 SIFT windows at the 15 parts (for symmetrical parts like left/right eyes, we randomly choose one if both are visible), and the features for invisible parts are zeroed out. [sent-326, score-0.441]
</p><p>87 (b) gives more examples of part localization using our method. [sent-334, score-0.309]
</p><p>88 Cumulative Match Characteristic (CMC) curves for bird species classification. [sent-338, score-0.347]
</p><p>89 The comparison between our full model and our partial model with only pose consistency shows that adding the subcategory consistency leads  to about 3% increase in the Rank 1accuracy. [sent-342, score-1.106]
</p><p>90 Moreover, we achieve state-of-the-art performance on the dataset in a fully automatic setting (without using groundtruth bounding boxes or ground-truth part locations from the testing data). [sent-346, score-0.272]
</p><p>91 Based on the experiment, we do feel accurate part localization goes a long way towards building a working system for fine-grained classification. [sent-347, score-0.309]
</p><p>92 Conclusion In this paper, we propose a simple and novel approach for bird part localization, as the test case of fine-grained categories. [sent-349, score-0.314]
</p><p>93 We introduce the idea of enforcing subcategory consistency at the parts, and show how to generate likely hypotheses of part configurations using exemplar-based models with enforced pose and subcategory consistency. [sent-350, score-2.029]
</p><p>94 The improved hypotheses over [4] enable us to better estimate 22552266  Method200 species14 species  PTBOeoiumrsdepl oatsoe[l1ibn7a]g keinrge[l3 ]4 28. [sent-351, score-0.282]
</p><p>95 Experimental results demonstrate that our method achieves state-of-the-art performance for both part localization and species classification on the challenging dataset CUB-200-201 1 [30]. [sent-357, score-0.544]
</p><p>96 Poselets: Body part detectors trained using 3d human pose annotations. [sent-409, score-0.557]
</p><p>97 How important are deformable parts in the deformable parts model? [sent-460, score-0.36]
</p><p>98 Articulated pose estimation with parts connectivity using discriminative local oriented contours. [sent-562, score-0.393]
</p><p>99 Multiclass recognition and part localization with humans in the loop. [sent-577, score-0.309]
</p><p>100 Face detection, pose estimation, and landmark localization in the wild. [sent-626, score-0.42]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('subcategory', 0.628), ('pose', 0.246), ('species', 0.2), ('exemplars', 0.177), ('part', 0.167), ('birds', 0.16), ('bird', 0.147), ('parts', 0.147), ('detectors', 0.144), ('localization', 0.142), ('poselets', 0.135), ('dp', 0.132), ('fvr', 0.126), ('consistency', 0.116), ('response', 0.107), ('activations', 0.097), ('xk', 0.088), ('poselet', 0.086), ('visibilities', 0.082), ('hypotheses', 0.082), ('ds', 0.082), ('types', 0.081), ('visibility', 0.071), ('visible', 0.069), ('locations', 0.069), ('pcp', 0.069), ('consensus', 0.062), ('invisibility', 0.062), ('ae', 0.06), ('flag', 0.056), ('cmc', 0.053), ('enforcing', 0.053), ('grouping', 0.051), ('vkj', 0.05), ('fir', 0.05), ('subcategories', 0.05), ('configuration', 0.049), ('exemplar', 0.048), ('maps', 0.046), ('vlfeat', 0.045), ('shape', 0.044), ('localizing', 0.042), ('false', 0.041), ('ranked', 0.04), ('branson', 0.04), ('cik', 0.039), ('dip', 0.039), ('predict', 0.038), ('testing', 0.036), ('iof', 0.036), ('classification', 0.035), ('articulated', 0.034), ('xik', 0.034), ('perona', 0.033), ('deformable', 0.033), ('farrell', 0.033), ('appearance', 0.033), ('dpms', 0.032), ('bourdev', 0.032), ('landmark', 0.032), ('dsi', 0.031), ('transformation', 0.031), ('hurt', 0.031), ('variations', 0.03), ('configurations', 0.03), ('trees', 0.03), ('detector', 0.029), ('toolbox', 0.029), ('cluster', 0.028), ('models', 0.028), ('active', 0.028), ('likely', 0.028), ('location', 0.027), ('clusters', 0.027), ('probability', 0.027), ('evaluating', 0.027), ('scaling', 0.027), ('wah', 0.026), ('subsequent', 0.026), ('poses', 0.026), ('libsvm', 0.026), ('ensemble', 0.025), ('jacobs', 0.025), ('rate', 0.025), ('please', 0.025), ('keypoint', 0.024), ('remarkable', 0.024), ('samples', 0.024), ('facial', 0.024), ('mixture', 0.024), ('cumulative', 0.023), ('detection', 0.023), ('pictorial', 0.023), ('incorrectly', 0.023), ('relations', 0.023), ('enforced', 0.023), ('agree', 0.023), ('exemplarbased', 0.022), ('lbox', 0.022), ('butterflies', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="62-tfidf-1" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>Author: Jiongxin Liu, Peter N. Belhumeur</p><p>Abstract: In this paper, we propose a novel approach for bird part localization, targeting fine-grained categories with wide variations in appearance due to different poses (including aspect and orientation) and subcategories. As it is challenging to represent such variations across a large set of diverse samples with tractable parametric models, we turn to individual exemplars. Specifically, we extend the exemplarbased models in [4] by enforcing pose and subcategory consistency at the parts. During training, we build posespecific detectors scoring part poses across subcategories, and subcategory-specific detectors scoring part appearance across poses. At the testing stage, likely exemplars are matched to the image, suggesting part locations whose pose and subcategory consistency are well-supported by the image cues. From these hypotheses, part configuration can be predicted with very high accuracy. Experimental results demonstrate significantperformance gainsfrom our method on an extensive dataset: CUB-200-2011 [30], for both localization and classification tasks.</p><p>2 0.32067657 <a title="62-tfidf-2" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<p>Author: Tian Lan, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: The appearance of an object changes profoundly with pose, camera view and interactions of the object with other objects in the scene. This makes it challenging to learn detectors based on an object-level label (e.g., “car”). We postulate that having a richer set oflabelings (at different levels of granularity) for an object, including finer-grained subcategories, consistent in appearance and view, and higherorder composites – contextual groupings of objects consistent in their spatial layout and appearance, can significantly alleviate these problems. However, obtaining such a rich set of annotations, including annotation of an exponentially growing set of object groupings, is simply not feasible. We propose a weakly-supervised framework for object detection where we discover subcategories and the composites automatically with only traditional object-level category labels as input. To this end, we first propose an exemplar-SVM-based clustering approach, with latent SVM refinement, that discovers a variable length set of discriminative subcategories for each object class. We then develop a structured model for object detection that captures interactions among object subcategories and automatically discovers semantically meaningful and discriminatively relevant visual composites. We show that this model produces state-of-the-art performance on UIUC phrase object detection benchmark.</p><p>3 0.23497003 <a title="62-tfidf-3" href="./iccv-2013-How_Do_You_Tell_a_Blackbird_from_a_Crow%3F.html">202 iccv-2013-How Do You Tell a Blackbird from a Crow?</a></p>
<p>Author: Thomas Berg, Peter N. Belhumeur</p><p>Abstract: How do you tell a blackbirdfrom a crow? There has been great progress toward automatic methods for visual recognition, including fine-grained visual categorization in which the classes to be distinguished are very similar. In a task such as bird species recognition, automatic recognition systems can now exceed the performance of non-experts – most people are challenged to name a couple dozen bird species, let alone identify them. This leads us to the question, “Can a recognition system show humans what to look for when identifying classes (in this case birds)? ” In the context of fine-grained visual categorization, we show that we can automatically determine which classes are most visually similar, discover what visual features distinguish very similar classes, and illustrate the key features in a way meaningful to humans. Running these methods on a dataset of bird images, we can generate a visual field guide to birds which includes a tree of similarity that displays the similarity relations between all species, pages for each species showing the most similar other species, and pages for each pair of similar species illustrating their differences.</p><p>4 0.22072378 <a title="62-tfidf-4" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>Author: Ning Zhang, Ryan Farrell, Forrest Iandola, Trevor Darrell</p><p>Abstract: Recognizing objects in fine-grained domains can be extremely challenging due to the subtle differences between subcategories. Discriminative markings are often highly localized, leading traditional object recognition approaches to struggle with the large pose variation often present in these domains. Pose-normalization seeks to align training exemplars, either piecewise by part or globally for the whole object, effectively factoring out differences in pose and in viewing angle. Prior approaches relied on computationally-expensive filter ensembles for part localization and required extensive supervision. This paper proposes two pose-normalized descriptors based on computationally-efficient deformable part models. The first leverages the semantics inherent in strongly-supervised DPM parts. The second exploits weak semantic annotations to learn cross-component correspondences, computing pose-normalized descriptors from the latent parts of a weakly-supervised DPM. These representations enable pooling across pose and viewpoint, in turn facilitating tasks such as fine-grained recognition and attribute prediction. Experiments conducted on the Caltech-UCSD Birds 200 dataset and Berkeley Human Attribute dataset demonstrate significant improvements over state-of-art algorithms.</p><p>5 0.19669414 <a title="62-tfidf-5" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>Author: Jian Sun, Jean Ponce</p><p>Abstract: In this paper, we address the problem of learning discriminative part detectors from image sets with category labels. We propose a novel latent SVM model regularized by group sparsity to learn these part detectors. Starting from a large set of initial parts, the group sparsity regularizer forces the model to jointly select and optimize a set of discriminative part detectors in a max-margin framework. We propose a stochastic version of a proximal algorithm to solve the corresponding optimization problem. We apply the proposed method to image classification and cosegmentation, and quantitative experiments with standard benchmarks show that it matches or improves upon the state of the art.</p><p>6 0.19607891 <a title="62-tfidf-6" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>7 0.18368159 <a title="62-tfidf-7" href="./iccv-2013-Strong_Appearance_and_Expressive_Spatial_Models_for_Human_Pose_Estimation.html">403 iccv-2013-Strong Appearance and Expressive Spatial Models for Human Pose Estimation</a></p>
<p>8 0.17867503 <a title="62-tfidf-8" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>9 0.17487647 <a title="62-tfidf-9" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>10 0.16265126 <a title="62-tfidf-10" href="./iccv-2013-Fine-Grained_Categorization_by_Alignments.html">169 iccv-2013-Fine-Grained Categorization by Alignments</a></p>
<p>11 0.15996356 <a title="62-tfidf-11" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>12 0.15043455 <a title="62-tfidf-12" href="./iccv-2013-Allocentric_Pose_Estimation.html">46 iccv-2013-Allocentric Pose Estimation</a></p>
<p>13 0.13804214 <a title="62-tfidf-13" href="./iccv-2013-Monocular_Image_3D_Human_Pose_Estimation_under_Self-Occlusion.html">273 iccv-2013-Monocular Image 3D Human Pose Estimation under Self-Occlusion</a></p>
<p>14 0.13016717 <a title="62-tfidf-14" href="./iccv-2013-How_Related_Exemplars_Help_Complex_Event_Detection_in_Web_Videos%3F.html">203 iccv-2013-How Related Exemplars Help Complex Event Detection in Web Videos?</a></p>
<p>15 0.12081681 <a title="62-tfidf-15" href="./iccv-2013-Recognising_Human-Object_Interaction_via_Exemplar_Based_Modelling.html">344 iccv-2013-Recognising Human-Object Interaction via Exemplar Based Modelling</a></p>
<p>16 0.11664697 <a title="62-tfidf-16" href="./iccv-2013-Interactive_Markerless_Articulated_Hand_Motion_Tracking_Using_RGB_and_Depth_Data.html">218 iccv-2013-Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth Data</a></p>
<p>17 0.11205805 <a title="62-tfidf-17" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>18 0.111608 <a title="62-tfidf-18" href="./iccv-2013-Parsing_IKEA_Objects%3A_Fine_Pose_Estimation.html">308 iccv-2013-Parsing IKEA Objects: Fine Pose Estimation</a></p>
<p>19 0.10956758 <a title="62-tfidf-19" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>20 0.10807318 <a title="62-tfidf-20" href="./iccv-2013-No_Matter_Where_You_Are%3A_Flexible_Graph-Guided_Multi-task_Learning_for_Multi-view_Head_Pose_Classification_under_Target_Motion.html">291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.231), (1, 0.026), (2, 0.0), (3, -0.04), (4, 0.162), (5, -0.116), (6, 0.023), (7, 0.048), (8, -0.079), (9, -0.015), (10, 0.066), (11, 0.05), (12, -0.162), (13, -0.183), (14, -0.148), (15, 0.077), (16, 0.067), (17, 0.021), (18, 0.086), (19, 0.024), (20, 0.15), (21, 0.089), (22, 0.067), (23, 0.07), (24, 0.035), (25, 0.052), (26, 0.001), (27, -0.05), (28, 0.06), (29, -0.06), (30, 0.025), (31, -0.107), (32, 0.042), (33, -0.052), (34, 0.003), (35, 0.016), (36, 0.006), (37, -0.103), (38, 0.006), (39, -0.026), (40, -0.001), (41, -0.113), (42, 0.145), (43, -0.044), (44, -0.071), (45, 0.054), (46, -0.086), (47, 0.091), (48, 0.078), (49, 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95210451 <a title="62-lsi-1" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>Author: Jiongxin Liu, Peter N. Belhumeur</p><p>Abstract: In this paper, we propose a novel approach for bird part localization, targeting fine-grained categories with wide variations in appearance due to different poses (including aspect and orientation) and subcategories. As it is challenging to represent such variations across a large set of diverse samples with tractable parametric models, we turn to individual exemplars. Specifically, we extend the exemplarbased models in [4] by enforcing pose and subcategory consistency at the parts. During training, we build posespecific detectors scoring part poses across subcategories, and subcategory-specific detectors scoring part appearance across poses. At the testing stage, likely exemplars are matched to the image, suggesting part locations whose pose and subcategory consistency are well-supported by the image cues. From these hypotheses, part configuration can be predicted with very high accuracy. Experimental results demonstrate significantperformance gainsfrom our method on an extensive dataset: CUB-200-2011 [30], for both localization and classification tasks.</p><p>2 0.77442867 <a title="62-lsi-2" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>Author: Ning Zhang, Ryan Farrell, Forrest Iandola, Trevor Darrell</p><p>Abstract: Recognizing objects in fine-grained domains can be extremely challenging due to the subtle differences between subcategories. Discriminative markings are often highly localized, leading traditional object recognition approaches to struggle with the large pose variation often present in these domains. Pose-normalization seeks to align training exemplars, either piecewise by part or globally for the whole object, effectively factoring out differences in pose and in viewing angle. Prior approaches relied on computationally-expensive filter ensembles for part localization and required extensive supervision. This paper proposes two pose-normalized descriptors based on computationally-efficient deformable part models. The first leverages the semantics inherent in strongly-supervised DPM parts. The second exploits weak semantic annotations to learn cross-component correspondences, computing pose-normalized descriptors from the latent parts of a weakly-supervised DPM. These representations enable pooling across pose and viewpoint, in turn facilitating tasks such as fine-grained recognition and attribute prediction. Experiments conducted on the Caltech-UCSD Birds 200 dataset and Berkeley Human Attribute dataset demonstrate significant improvements over state-of-art algorithms.</p><p>3 0.76230389 <a title="62-lsi-3" href="./iccv-2013-Strong_Appearance_and_Expressive_Spatial_Models_for_Human_Pose_Estimation.html">403 iccv-2013-Strong Appearance and Expressive Spatial Models for Human Pose Estimation</a></p>
<p>Author: Leonid Pishchulin, Mykhaylo Andriluka, Peter Gehler, Bernt Schiele</p><p>Abstract: Typical approaches to articulated pose estimation combine spatial modelling of the human body with appearance modelling of body parts. This paper aims to push the state-of-the-art in articulated pose estimation in two ways. First we explore various types of appearance representations aiming to substantially improve the bodypart hypotheses. And second, we draw on and combine several recently proposed powerful ideas such as more flexible spatial models as well as image-conditioned spatial models. In a series of experiments we draw several important conclusions: (1) we show that the proposed appearance representations are complementary; (2) we demonstrate that even a basic tree-structure spatial human body model achieves state-ofthe-art performance when augmented with the proper appearance representation; and (3) we show that the combination of the best performing appearance model with a flexible image-conditioned spatial model achieves the best result, significantly improving over the state of the art, on the “Leeds Sports Poses ” and “Parse ” benchmarks.</p><p>4 0.72649699 <a title="62-lsi-4" href="./iccv-2013-Fine-Grained_Categorization_by_Alignments.html">169 iccv-2013-Fine-Grained Categorization by Alignments</a></p>
<p>Author: E. Gavves, B. Fernando, C.G.M. Snoek, A.W.M. Smeulders, T. Tuytelaars</p><p>Abstract: The aim of this paper is fine-grained categorization without human interaction. Different from prior work, which relies on detectors for specific object parts, we propose to localize distinctive details by roughly aligning the objects using just the overall shape, since implicit to fine-grained categorization is the existence of a super-class shape shared among all classes. The alignments are then used to transfer part annotations from training images to test images (supervised alignment), or to blindly yet consistently segment the object in a number of regions (unsupervised alignment). We furthermore argue that in the distinction of finegrained sub-categories, classification-oriented encodings like Fisher vectors are better suited for describing localized information than popular matching oriented features like HOG. We evaluate the method on the CU-2011 Birds and Stanford Dogs fine-grained datasets, outperforming the state-of-the-art.</p><p>5 0.72304159 <a title="62-lsi-5" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>Author: Jian Sun, Jean Ponce</p><p>Abstract: In this paper, we address the problem of learning discriminative part detectors from image sets with category labels. We propose a novel latent SVM model regularized by group sparsity to learn these part detectors. Starting from a large set of initial parts, the group sparsity regularizer forces the model to jointly select and optimize a set of discriminative part detectors in a max-margin framework. We propose a stochastic version of a proximal algorithm to solve the corresponding optimization problem. We apply the proposed method to image classification and cosegmentation, and quantitative experiments with standard benchmarks show that it matches or improves upon the state of the art.</p><p>6 0.69440025 <a title="62-lsi-6" href="./iccv-2013-How_Do_You_Tell_a_Blackbird_from_a_Crow%3F.html">202 iccv-2013-How Do You Tell a Blackbird from a Crow?</a></p>
<p>7 0.6673376 <a title="62-lsi-7" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<p>8 0.66489887 <a title="62-lsi-8" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>9 0.66231543 <a title="62-lsi-9" href="./iccv-2013-Recognising_Human-Object_Interaction_via_Exemplar_Based_Modelling.html">344 iccv-2013-Recognising Human-Object Interaction via Exemplar Based Modelling</a></p>
<p>10 0.65690166 <a title="62-lsi-10" href="./iccv-2013-Discovering_Object_Functionality.html">118 iccv-2013-Discovering Object Functionality</a></p>
<p>11 0.64925712 <a title="62-lsi-11" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>12 0.63497341 <a title="62-lsi-12" href="./iccv-2013-Building_Part-Based_Object_Detectors_via_3D_Geometry.html">66 iccv-2013-Building Part-Based Object Detectors via 3D Geometry</a></p>
<p>13 0.63067544 <a title="62-lsi-13" href="./iccv-2013-Allocentric_Pose_Estimation.html">46 iccv-2013-Allocentric Pose Estimation</a></p>
<p>14 0.61687601 <a title="62-lsi-14" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>15 0.60921931 <a title="62-lsi-15" href="./iccv-2013-No_Matter_Where_You_Are%3A_Flexible_Graph-Guided_Multi-task_Learning_for_Multi-view_Head_Pose_Classification_under_Target_Motion.html">291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</a></p>
<p>16 0.60134059 <a title="62-lsi-16" href="./iccv-2013-A_Method_of_Perceptual-Based_Shape_Decomposition.html">21 iccv-2013-A Method of Perceptual-Based Shape Decomposition</a></p>
<p>17 0.60129058 <a title="62-lsi-17" href="./iccv-2013-Parsing_IKEA_Objects%3A_Fine_Pose_Estimation.html">308 iccv-2013-Parsing IKEA Objects: Fine Pose Estimation</a></p>
<p>18 0.60037333 <a title="62-lsi-18" href="./iccv-2013-Shufflets%3A_Shared_Mid-level_Parts_for_Fast_Object_Detection.html">390 iccv-2013-Shufflets: Shared Mid-level Parts for Fast Object Detection</a></p>
<p>19 0.59070194 <a title="62-lsi-19" href="./iccv-2013-Monocular_Image_3D_Human_Pose_Estimation_under_Self-Occlusion.html">273 iccv-2013-Monocular Image 3D Human Pose Estimation under Self-Occlusion</a></p>
<p>20 0.58207065 <a title="62-lsi-20" href="./iccv-2013-Human_Re-identification_by_Matching_Compositional_Template_with_Cluster_Sampling.html">205 iccv-2013-Human Re-identification by Matching Compositional Template with Cluster Sampling</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.061), (7, 0.011), (8, 0.124), (26, 0.089), (31, 0.034), (34, 0.057), (35, 0.028), (42, 0.151), (48, 0.013), (64, 0.047), (73, 0.032), (77, 0.018), (78, 0.031), (89, 0.182), (98, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92575699 <a title="62-lda-1" href="./iccv-2013-Modifying_the_Memorability_of_Face_Photographs.html">272 iccv-2013-Modifying the Memorability of Face Photographs</a></p>
<p>Author: Aditya Khosla, Wilma A. Bainbridge, Antonio Torralba, Aude Oliva</p><p>Abstract: Contemporary life bombards us with many new images of faces every day, which poses non-trivial constraints on human memory. The vast majority of face photographs are intended to be remembered, either because of personal relevance, commercial interests or because the pictures were deliberately designed to be memorable. Can we make aportrait more memorable or more forgettable automatically? Here, we provide a method to modify the memorability of individual face photographs, while keeping the identity and other facial traits (e.g. age, attractiveness, and emotional magnitude) of the individual fixed. We show that face photographs manipulated to be more memorable (or more forgettable) are indeed more often remembered (or forgotten) in a crowd-sourcing experiment with an accuracy of 74%. Quantifying and modifying the ‘memorability ’ of a face lends itself to many useful applications in computer vision and graphics, such as mnemonic aids for learning, photo editing applications for social networks and tools for designing memorable advertisements.</p><p>2 0.90926546 <a title="62-lda-2" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>Author: C. Lawrence Zitnick, Devi Parikh, Lucy Vanderwende</p><p>Abstract: Sentences that describe visual scenes contain a wide variety of information pertaining to the presence of objects, their attributes and their spatial relations. In this paper we learn the visual features that correspond to semantic phrases derived from sentences. Specifically, we extract predicate tuples that contain two nouns and a relation. The relation may take several forms, such as a verb, preposition, adjective or their combination. We model a scene using a Conditional Random Field (CRF) formulation where each node corresponds to an object, and the edges to their relations. We determine the potentials of the CRF using the tuples extracted from the sentences. We generate novel scenes depicting the sentences’ visual meaning by sampling from the CRF. The CRF is also used to score a set of scenes for a text-based image retrieval task. Our results show we can generate (retrieve) scenes that convey the desired semantic meaning, even when scenes (queries) are described by multiple sentences. Significant improvement is found over several baseline approaches.</p><p>3 0.90814114 <a title="62-lda-3" href="./iccv-2013-3D_Sub-query_Expansion_for_Improving_Sketch-Based_Multi-view_Image_Retrieval.html">3 iccv-2013-3D Sub-query Expansion for Improving Sketch-Based Multi-view Image Retrieval</a></p>
<p>Author: Yen-Liang Lin, Cheng-Yu Huang, Hao-Jeng Wang, Winston Hsu</p><p>Abstract: We propose a 3D sub-query expansion approach for boosting sketch-based multi-view image retrieval. The core idea of our method is to automatically convert two (guided) 2D sketches into an approximated 3D sketch model, and then generate multi-view sketches as expanded sub-queries to improve the retrieval performance. To learn the weights among synthesized views (sub-queries), we present a new multi-query feature to model the similarity between subqueries and dataset images, and formulate it into a convex optimization problem. Our approach shows superior performance compared with the state-of-the-art approach on a public multi-view image dataset. Moreover, we also conduct sensitivity tests to analyze the parameters of our approach based on the gathered user sketches.</p><p>same-paper 4 0.90162373 <a title="62-lda-4" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>Author: Jiongxin Liu, Peter N. Belhumeur</p><p>Abstract: In this paper, we propose a novel approach for bird part localization, targeting fine-grained categories with wide variations in appearance due to different poses (including aspect and orientation) and subcategories. As it is challenging to represent such variations across a large set of diverse samples with tractable parametric models, we turn to individual exemplars. Specifically, we extend the exemplarbased models in [4] by enforcing pose and subcategory consistency at the parts. During training, we build posespecific detectors scoring part poses across subcategories, and subcategory-specific detectors scoring part appearance across poses. At the testing stage, likely exemplars are matched to the image, suggesting part locations whose pose and subcategory consistency are well-supported by the image cues. From these hypotheses, part configuration can be predicted with very high accuracy. Experimental results demonstrate significantperformance gainsfrom our method on an extensive dataset: CUB-200-2011 [30], for both localization and classification tasks.</p><p>5 0.88551176 <a title="62-lda-5" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>Author: Xiaoyang Wang, Qiang Ji</p><p>Abstract: This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. As a list of semantically meaningful properties of objects, attributes generally relate to each other statistically. In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and objectindependent attribute relationships. The model utilizes the captured relationships to benefit both attribute prediction and object recognition. Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases.</p><p>6 0.885149 <a title="62-lda-6" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>7 0.88402414 <a title="62-lda-7" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<p>8 0.88273764 <a title="62-lda-8" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>9 0.87706983 <a title="62-lda-9" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>10 0.8768096 <a title="62-lda-10" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>11 0.87577641 <a title="62-lda-11" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>12 0.87511075 <a title="62-lda-12" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>13 0.874331 <a title="62-lda-13" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>14 0.87403387 <a title="62-lda-14" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>15 0.87334055 <a title="62-lda-15" href="./iccv-2013-Box_in_the_Box%3A_Joint_3D_Layout_and_Object_Reasoning_from_Single_Images.html">64 iccv-2013-Box in the Box: Joint 3D Layout and Object Reasoning from Single Images</a></p>
<p>16 0.87295479 <a title="62-lda-16" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>17 0.87217385 <a title="62-lda-17" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>18 0.87191486 <a title="62-lda-18" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>19 0.8706975 <a title="62-lda-19" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<p>20 0.87037313 <a title="62-lda-20" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
