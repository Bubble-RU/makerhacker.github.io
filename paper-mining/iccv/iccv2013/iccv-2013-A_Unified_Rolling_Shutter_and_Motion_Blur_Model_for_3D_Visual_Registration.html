<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>32 iccv-2013-A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-32" href="#">iccv2013-32</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>32 iccv-2013-A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration</h1>
<br/><p>Source: <a title="iccv-2013-32-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Meilland_A_Unified_Rolling_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Maxime Meilland, Tom Drummond, Andrew I. Comport</p><p>Abstract: Motion blur and rolling shutter deformations both inhibit visual motion registration, whether it be due to a moving sensor or a moving target. Whilst both deformations exist simultaneously, no models have been proposed to handle them together. Furthermore, neither deformation has been consideredpreviously in the context of monocularfullimage 6 degrees of freedom registration or RGB-D structure and motion. As will be shown, rolling shutter deformation is observed when a camera moves faster than a single pixel in parallax between subsequent scan-lines. Blur is a function of the pixel exposure time and the motion vector. In this paper a complete dense 3D registration model will be derived to accountfor both motion blur and rolling shutter deformations simultaneously. Various approaches will be compared with respect to ground truth and live real-time performance will be demonstratedfor complex scenarios where both blur and shutter deformations are dominant.</p><p>Reference: <a title="iccv-2013-32-reference" href="../iccv2013_reference/iccv-2013-A_Unified_Rolling_Shutter_and_Motion_Blur_Model_for_3D_Visual_Registration_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 fr l 3 Abstract Motion blur and rolling shutter deformations both inhibit visual motion registration, whether it be due to a moving sensor or a moving target. [sent-6, score-1.484]
</p><p>2 As will be shown, rolling shutter deformation is observed when a camera moves faster than a single pixel in parallax between subsequent scan-lines. [sent-9, score-1.139]
</p><p>3 Blur is a function of the pixel exposure time and the motion vector. [sent-10, score-0.241]
</p><p>4 In this paper a complete dense 3D registration model will be derived to accountfor both motion blur and rolling shutter deformations simultaneously. [sent-11, score-1.504]
</p><p>5 Various approaches will be compared with respect to ground truth and live real-time performance will be demonstratedfor complex scenarios where both blur and shutter deformations are dominant. [sent-12, score-0.868]
</p><p>6 Introduction  Electronic rolling shutter (RS) cameras have been becoming increasingly present in a wide number of applications and devices due to their low cost, low power consumption and continual read-out properties. [sent-14, score-0.993]
</p><p>7 Unfortunately RS cameras capture deformed images if the camera is in motion or objects move in the scene. [sent-18, score-0.228]
</p><p>8 On the other hand, image motion blur (MB) affects a large range of algorithms that deal with moving sensors (i. [sent-19, score-0.387]
</p><p>9 MB depends directly on each pixel’s exposure period (electronic shutter interval) and even with small motions some amount Andrew . [sent-23, score-0.652]
</p><p>10 Subsequently if there is enough motion to produce RS deformations then there is imperatively enough motion to create MB effects also. [sent-26, score-0.347]
</p><p>11 In [14] 6 + N 6 degrees of freedom (dof) are estimated by tracking groups of scan-lines  independently to model non-uniform motion and more recently the same authors proposed a polynomial projection model. [sent-41, score-0.196]
</p><p>12 The deformations were treated as an underlying high-frequency jitter of the camera and this highfrequency motion is estimated using optic-flow point feature correspondences. [sent-46, score-0.292]
</p><p>13 Lastly, in [9] RS rectification is simplified to rotation only and the authors use this to perform structure from motion estimation and Bundle Adjustment respectively. [sent-49, score-0.196]
</p><p>14 Motion Blur Deformation Surprisingly all these previous works have only considered  RS deformation but none have handled motion blur. [sent-52, score-0.174]
</p><p>15 MB, in its general form, varies with respect to the full 6 dof motion of the camera. [sent-53, score-0.294]
</p><p>16 In [12], an inertial sensor was used to estimate motion and perform pose tracking in the presence of MB. [sent-58, score-0.313]
</p><p>17 In this case 8 parameters of the SL(3) Lie group are estimated using 8 dof estimation for the homography parameters plus either 8 dof for the MB direction or 1 additional dof for the MB magnitude (limited to high frame-rate). [sent-63, score-0.571]
</p><p>18 Rolling Shutter and Motion Blur None of the previously cited papers on RS and MB deformations have, however, attempted to simultaneously correct for both rolling shutter and blur distortion. [sent-70, score-1.296]
</p><p>19 The dual problem that should be considered is to both: •  •  correct for rolling shutter distortions that are induced by sensor motion or moving objects, correct for image blur induced by integrating moving light rays during the sensor exposure period. [sent-71, score-1.626]
</p><p>20 Underneath this threshold only a small amount of blur will be observable and it will depend on the pixel exposure time and the motion observed in the image. [sent-74, score-0.479]
</p><p>21 In [23] different analog and digital imaging shutter mechanics are presented and the coupled effect of motion-blur and rolling shutter deformations is discussed. [sent-75, score-1.576]
</p><p>22 Overview In this paper a unified model is proposed for monocular direct 6 dof pose tracking from a dense 3D model in the presence of both RS and MB deformations. [sent-91, score-0.377]
</p><p>23 The main contributions are: •  •  A unified approach for both rolling shutter and motion blur estimation, via a 6 dof state model that improves on [1] for RS and [15] for MB. [sent-93, score-1.517]
</p><p>24 2017  •  •  Motion blur model is also valid for global-shutter cameras by simply setting the camera readout time to zero. [sent-95, score-0.435]
</p><p>25 In essence, the additional 6 dof corresponds to estimating the 3D model pose, however, as we show it can be calibrated only once in the first image. [sent-99, score-0.192]
</p><p>26 Both 6 and 12 dof models will be compared and detailed further in the article and the proposed approach will be shown to be valid for monocular model based registration. [sent-101, score-0.206]
</p><p>27 Dense image observation model Live direct 3D model-based tracking will be defined for monocular cameras using a dense large-scale world model that has been acquired in real-time by an automatic mapping process. [sent-103, score-0.205]
</p><p>28 The paper will also consider real-time 3D model acquisition by performing dense structure and motion (SaM) estimation with RGB-D sensors. [sent-104, score-0.185]
</p><p>29 In the present paper a graph of RGB-D key-frames is stored to represent the 3D model within which 6 dof poses are the edges in the graph. [sent-106, score-0.17]
</p><p>30 In this context consider a calibrated camera sensor with a colour brightness function I: Ω R+ → R+ ; (p) → I(p, t), where Ω = [1, n] [1, m] ⊂ R2, P = (p1, p2, . [sent-108, score-0.227]
</p><p>31 Each raw of the sensor is sequentially exposed during a fixed exposure time te. [sent-124, score-0.23]
</p><p>32 The total readout time tr is the delay between the readout of the first and the last row. [sent-125, score-0.257]
</p><p>33 The frame period tp is the time delay between the readout of the same raw of the image. [sent-126, score-0.194]
</p><p>34 All rows of the image are exposed simultaneously during a fixed exposure time te. [sent-128, score-0.166]
</p><p>35 Global shutter With a global shutter camera, all the pixels of the sensor are simultaneously exposed during the acquisition period tp (see Figure 1). [sent-136, score-1.264]
</p><p>36 Under the assumption of brightness consistency and assuming that the exposure time of the sensor te is infinitesimally small, if the true pose T? [sent-137, score-0.317]
</p><p>37 Rolling shutter  Now considering that the current image I been acquired has with a RS camera, under constant linear and angular velocities xv = (υ, ω) ∈ R6. [sent-160, score-0.741]
</p><p>38 As depicted in Figure 1, each row of a RS sensor is exposed sequentially with a time delay tΔ = where tr is the total readout time and n is the number of rows in the image. [sent-161, score-0.303]
</p><p>39 ,  (4)  where the first warping wT1 ((·τ) xis the standard global shutter warping of equation (3). [sent-173, score-0.652]
</p><p>40 The scalar value τ is the time constant for a particular scanline but since the reference pixels have been warped with a 6dof transformation, their coordinates no longer have a integer correspondence with a scan-line in the current image and they are scattered. [sent-175, score-0.172]
</p><p>41 Due to the associative properties of the warping functions, the rolling shutter projection can be denoted as  ? [sent-191, score-1.023]
</p><p>42 Motion blur The blurring model detailed in this section is based on [15] for planar homography patches parametrized on SL(3). [sent-198, score-0.26]
</p><p>43 Reconsidering the case of a global shutter camera and focusing on an image I corrupted by motion blur. [sent-200, score-0.717]
</p><p>44 In image-based tracking the reference frame is usually maintained untouched to avoid corrupting the measurements and the aim is to transform and de-blur the current image such that it is equal to the reference as in equation (2): I∗(p∗) = Iu  ? [sent-210, score-0.186]
</p><p>45 It is therefore more efficient to introduce motion blur into the reference image so as to maintain this equality in the presence of blur. [sent-214, score-0.442]
</p><p>46 The current blurred image must still be warped to the reference according to equation (2). [sent-216, score-0.194]
</p><p>47 This blur generation technique correspond to warping M images and averaging their values into a single image and is valid for constant velocity and under brightness consistency assumption. [sent-228, score-0.469]
</p><p>48 Unified model Now considering that the current image I acquired with is a RS camera, under the exposure period te, the following equality is obtained by combining equations (4) and (10):  I? [sent-231, score-0.222]
</p><p>49 dt This models consists in simultaneously warping the current image with RS distortions to a virtually blurred reference 2019  frame. [sent-241, score-0.248]
</p><p>50 Global shutter sensors are also handled, by simply setting the total readout time tr = 0, as well as non-blurred images by setting the exposure time to an infinitesimally small value te = ? [sent-242, score-0.836]
</p><p>51 , (12) where Iw is the current (naturally blurred) warped image with RS distortions given by (7) and Ib∗ is the reference (virtually blurred) image of equation (10). [sent-272, score-0.179]
</p><p>52 The derivation of the 12 dof state RS model as it was first proposed by [1] assumes that the pose and the velocity are not coupled. [sent-274, score-0.347]
</p><p>53 In a live tracking framework, the pose increment  ×  T(x) at time ti is usually initialized with the last estimated pose at time ti−1 . [sent-276, score-0.223]
</p><p>54 If the time constant between ti−1 and ti is known, then the true velocity can be obtained from the instantaneous velocity twist that parametrizes the pose. [sent-277, score-0.327]
</p><p>55 Therefore the state vector can be reduced to only 6 dof by assuming a constant velocity during the frame period tp, leading to x? [sent-278, score-0.367]
</p><p>56 4 for real-time structure and motion estimation using an RGB-D sensor (projective light, stereo or other). [sent-297, score-0.228]
</p><p>57 It is assumed that both the colour image and the depth im-  ×  age have synchronised rolling shutter cameras so that the same 6 velocity parameters can be used to rectify both images. [sent-298, score-1.143]
</p><p>58 In the following experiments, the motion blur generation of equation (10) is performed with M = 20 samples, which appears to be sufficient to minimize aliasing and allows real-time computation. [sent-308, score-0.401]
</p><p>59 A more efficient strategy would be to adjust the number of samples with the camera velocity and exposure. [sent-309, score-0.202]
</p><p>60 Motion blur is obtained by invoking the rendering pipeline M times during the exposure time te, and the resulting images are averaged into a single image. [sent-316, score-0.363]
</p><p>61 In order to generate realistic motion blur and to avoid aliasing effects, 100 samples are used. [sent-317, score-0.382]
</p><p>62 Three sequences of 445 images were generated using the same input trajectory computed from 6 dof velocity increments integrated over the frame period te = 0. [sent-319, score-0.384]
</p><p>63 The first sequence simulates a global shutter camera with motion blur (te = 0. [sent-321, score-1.025]
</p><p>64 The second sequence simulates a non-blurred rolling shutter camera (te = 0. [sent-324, score-1.111]
</p><p>65 026s), and the third sequence simulates a rolling shutter camera with motion blur (te = 0. [sent-326, score-1.473]
</p><p>66 Synthetic scene observed with a constant velocity from the same viewpoint using different camera models. [sent-331, score-0.229]
</p><p>67 (a) is a perfect global shutter camera, (b) is a global shutter camera with motion blur, (c) is a rolling shutter camera without motion blur and (d) is a rolling shutter  camera with motion blur. [sent-332, score-3.802]
</p><p>68 Simulation  Absoulte angualr veloctiy (rad/s)  (b) RS sequence: angular error. [sent-370, score-0.239]
</p><p>69 Absoulte angualr veloctiy (rad/s)  (c) RS and MB sequence: angular error. [sent-372, score-0.239]
</p><p>70 Absoulte angualr veloctiy (rad/s)  (e) RS sequence: RMSE. [sent-386, score-0.18]
</p><p>71 Absoulte angualr veloctiy (rad/s)  (f) RS and MB sequence: RMSE. [sent-387, score-0.18]
</p><p>72 Figures 3(a),(b),(c) report the absolute angular error with respect to the absolute angular velocity of the camera. [sent-391, score-0.244]
</p><p>73 Figures 3(d),(e),(f) report the root mean squared error of the objective function (12) with respect to the absolute angular velocity of the camera. [sent-392, score-0.185]
</p><p>74 For the first sequence (only corrupted by motion blur) (a),(d), it can be seen that modelling motion blur (GS+MB) considerably improves the accuracy of the pose estimation compared to the standard model (GS). [sent-394, score-0.636]
</p><p>75 In the second sequence (only corrupted by rolling shutter perturbation) (b),(e) the same analysis can be made, modelling rolling shutter (RS) also improves the accuracy  of the pose estimation compared to the standard model (GS). [sent-395, score-2.08]
</p><p>76 In the third sequence containing both motion blur and rolling shutter effects (c),(f), it appears that only modelling motion blur (GS+MB) or only modelling rolling shutter effects (RS) do not improve the accuracy even if the image re-projection error (RMSE) is smaller than the standard model (GS). [sent-396, score-2.828]
</p><p>77 This emphasizes the correlation between rolling shutter and motion blur effects in the image projection subsequently creating a false minimum. [sent-397, score-1.354]
</p><p>78 When blur and rolling shutter effects are simultaneously estimated, pose estimation remains accurate even with high velocities. [sent-398, score-1.321]
</p><p>79 The 12 dof model gives similar results to 6 dof but requires inverting a larger Jacobian and takes longer to converge. [sent-399, score-0.34]
</p><p>80 Real data For the experiments, a calibrated Asus Xtion Pro Live RGBD camera was used as RGB-D for SFM and as a monocular camera for model-based registration. [sent-406, score-0.21]
</p><p>81 The readout time tr of the rolling shutter which was calibrated in [22] was used for the purposes of the following experiments. [sent-407, score-1.12]
</p><p>82 (a) is the virtual reference frame used for registration generated from the dense 3D model shown in (c). [sent-420, score-0.161]
</p><p>83 (b) is the current image undergoing rolling shutter and motion blur distortions. [sent-421, score-1.374]
</p><p>84 (e) is the current image after rolling shutter effect removal. [sent-423, score-0.987]
</p><p>85 In order to build a dense 3D model the real-time SaM approach proposed in [16] was used with the proposed rolling shutter and motion blur deformation model as introduced in Section 4. [sent-426, score-1.419]
</p><p>86 The camera was waved around the environment with very fast movements in each of the 6 dof and the estimated trajectory is also shown. [sent-433, score-0.294]
</p><p>87 In practice for the same camera velocity, expo2022  sure value and read-out constants, the motion blur for this setup gives a much larger deformation than the rolling shutter effects. [sent-445, score-1.453]
</p><p>88 Finally, the 12 dof RS+MB only model gives  average performance and high noise sensitivity characteristics are present. [sent-446, score-0.17]
</p><p>89 Conclusions This paper has addressed the problem of model-based 6 dof motion estimation using a consumer-level rolling shutter camera undergoing fast movements within large scenes. [sent-450, score-1.407]
</p><p>90 A unified solution for simultaneously estimating both motion blur and rolling shutter deformations was proposed within a direct dense registration framework that does not require feature extraction and matching. [sent-451, score-1.57]
</p><p>91 It has been shown that it is only necessary to estimate the velocity twist of the camera motion to estimate rolling shutter, motion blur and camera pose information. [sent-454, score-1.284]
</p><p>92 This is an improvement over previous rolling shutter approaches because none handle motion blur nor do they parametrise the system with 6 dof therefore improving precision, robustness and computational efficiency. [sent-455, score-1.497]
</p><p>93 Exploiting rolling shutter distortions for simultaneous object pose and velocity computation using a single view. [sent-461, score-1.18]
</p><p>94 Structure and kinematics triangulation with a rolling shutter stereo rig. [sent-468, score-0.965]
</p><p>95 Visual tracking in the pres-  [11]  [12]  [13] [14]  [15]  [16]  [17] [18] [19]  [20]  [21]  ence of motion blur. [sent-534, score-0.157]
</p><p>96 A generic rolling shutter camera model and its application to dynamical pose estimation. [sent-559, score-1.092]
</p><p>97 Modeling and generating complex motion blur for real-time tracking. [sent-565, score-0.362]
</p><p>98 Esm-blur: Handling &  [22]  [23]  [24]  [25]  [26]  rendering blur in 3d tracking and augmentation. [sent-613, score-0.305]
</p><p>99 Scan rectification for structured light range sensors with rolling shutters. [sent-621, score-0.507]
</p><p>100 Modeling of image shutters and motion blur in analog and digital camera systems. [sent-630, score-0.46]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('shutter', 0.517), ('rolling', 0.448), ('mb', 0.322), ('rs', 0.298), ('blur', 0.238), ('dof', 0.17), ('gs', 0.161), ('velocity', 0.126), ('motion', 0.124), ('readout', 0.093), ('exposure', 0.091), ('absoulte', 0.09), ('angualr', 0.09), ('veloctiy', 0.09), ('sensor', 0.085), ('camera', 0.076), ('deformations', 0.072), ('registration', 0.063), ('angular', 0.059), ('warping', 0.058), ('rmse', 0.056), ('reference', 0.056), ('sam', 0.054), ('exposed', 0.054), ('blurred', 0.053), ('pose', 0.051), ('deformation', 0.05), ('xv', 0.045), ('period', 0.044), ('te', 0.044), ('warped', 0.044), ('dense', 0.042), ('acquired', 0.041), ('live', 0.041), ('sequence', 0.04), ('tr', 0.04), ('modelling', 0.04), ('distortions', 0.038), ('monocular', 0.036), ('pw', 0.034), ('rectification', 0.034), ('rendering', 0.034), ('tracking', 0.033), ('delay', 0.031), ('rotational', 0.03), ('andreff', 0.03), ('antipolis', 0.03), ('monash', 0.03), ('sophia', 0.03), ('ttiei', 0.03), ('wrs', 0.03), ('iw', 0.03), ('simulates', 0.03), ('velocities', 0.03), ('klt', 0.029), ('cameras', 0.028), ('movements', 0.028), ('effects', 0.027), ('constant', 0.027), ('ti', 0.027), ('nm', 0.027), ('comport', 0.026), ('drummond', 0.026), ('infinitesimally', 0.026), ('meilland', 0.026), ('posters', 0.026), ('pixel', 0.026), ('translational', 0.026), ('tp', 0.026), ('undergoing', 0.025), ('direct', 0.025), ('sensors', 0.025), ('deblurring', 0.025), ('colour', 0.024), ('equality', 0.024), ('scanline', 0.023), ('ieee', 0.022), ('homography', 0.022), ('calibrated', 0.022), ('analog', 0.022), ('parallax', 0.022), ('current', 0.022), ('iu', 0.022), ('automation', 0.021), ('ib', 0.021), ('twist', 0.021), ('simultaneously', 0.021), ('estimated', 0.02), ('brightness', 0.02), ('rectifying', 0.02), ('unified', 0.02), ('aliasing', 0.02), ('inertial', 0.02), ('transfers', 0.02), ('estimation', 0.019), ('authors', 0.019), ('sl', 0.019), ('andrew', 0.019), ('whilst', 0.019), ('equation', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="32-tfidf-1" href="./iccv-2013-A_Unified_Rolling_Shutter_and_Motion_Blur_Model_for_3D_Visual_Registration.html">32 iccv-2013-A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration</a></p>
<p>Author: Maxime Meilland, Tom Drummond, Andrew I. Comport</p><p>Abstract: Motion blur and rolling shutter deformations both inhibit visual motion registration, whether it be due to a moving sensor or a moving target. Whilst both deformations exist simultaneously, no models have been proposed to handle them together. Furthermore, neither deformation has been consideredpreviously in the context of monocularfullimage 6 degrees of freedom registration or RGB-D structure and motion. As will be shown, rolling shutter deformation is observed when a camera moves faster than a single pixel in parallax between subsequent scan-lines. Blur is a function of the pixel exposure time and the motion vector. In this paper a complete dense 3D registration model will be derived to accountfor both motion blur and rolling shutter deformations simultaneously. Various approaches will be compared with respect to ground truth and live real-time performance will be demonstratedfor complex scenarios where both blur and shutter deformations are dominant.</p><p>2 0.7003215 <a title="32-tfidf-2" href="./iccv-2013-Rolling_Shutter_Stereo.html">363 iccv-2013-Rolling Shutter Stereo</a></p>
<p>Author: Olivier Saurer, Kevin Köser, Jean-Yves Bouguet, Marc Pollefeys</p><p>Abstract: A huge fraction of cameras used nowadays is based on CMOS sensors with a rolling shutter that exposes the image line by line. For dynamic scenes/cameras this introduces undesired effects like stretch, shear and wobble. It has been shown earlier that rotational shake induced rolling shutter effects in hand-held cell phone capture can be compensated based on an estimate of the camera rotation. In contrast, we analyse the case of significant camera motion, e.g. where a bypassing streetlevel capture vehicle uses a rolling shutter camera in a 3D reconstruction framework. The introduced error is depth dependent and cannot be compensated based on camera motion/rotation alone, invalidating also rectification for stereo camera systems. On top, significant lens distortion as often present in wide angle cameras intertwines with rolling shutter effects as it changes the time at which a certain 3D point is seen. We show that naive 3D reconstructions (assuming global shutter) will deliver biased geometry already for very mild assumptions on vehicle speed and resolution. We then develop rolling shutter dense multiview stereo algorithms that solve for time of exposure and depth at the same time, even in the presence of lens distortion and perform an evaluation on ground truth laser scan models as well as on real street-level data.</p><p>3 0.34819886 <a title="32-tfidf-3" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>Author: Bryan Klingner, David Martin, James Roseborough</p><p>Abstract: We describe a structure-from-motion framework that handles “generalized” cameras, such as moving rollingshutter cameras, and works at an unprecedented scale— billions of images covering millions of linear kilometers of roads—by exploiting a good relative pose prior along vehicle paths. We exhibit a planet-scale, appearanceaugmented point cloud constructed with our framework and demonstrate its practical use in correcting the pose of a street-level image collection.</p><p>4 0.18530916 <a title="32-tfidf-4" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>Author: Tae Hyun Kim, Byeongjoo Ahn, Kyoung Mu Lee</p><p>Abstract: Most conventional single image deblurring methods assume that the underlying scene is static and the blur is caused by only camera shake. In this paper, in contrast to this restrictive assumption, we address the deblurring problem of general dynamic scenes which contain multiple moving objects as well as camera shake. In case of dynamic scenes, moving objects and background have different blur motions, so the segmentation of the motion blur is required for deblurring each distinct blur motion accurately. Thus, we propose a novel energy model designed with the weighted sum of multiple blur data models, which estimates different motion blurs and their associated pixelwise weights, and resulting sharp image. In this framework, the local weights are determined adaptively and get high values when the corresponding data models have high data fidelity. And, the weight information is used for the segmentation of the motion blur. Non-local regularization of weights are also incorporated to produce more reliable segmentation results. A convex optimization-based method is used for the solution of the proposed energy model. Exper- imental results demonstrate that our method outperforms conventional approaches in deblurring both dynamic scenes and static scenes.</p><p>5 0.18156853 <a title="32-tfidf-5" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>Author: Shicheng Zheng, Li Xu, Jiaya Jia</p><p>Abstract: We handle a special type of motion blur considering that cameras move primarily forward or backward. Solving this type of blur is of unique practical importance since nearly all car, traffic and bike-mounted cameras follow out-ofplane translational motion. We start with the study of geometric models and analyze the difficulty of existing methods to deal with them. We also propose a solution accounting for depth variation. Homographies associated with different 3D planes are considered and solved for in an optimization framework. Our method is verified on several natural image examples that cannot be satisfyingly dealt with by previous methods.</p><p>6 0.14533356 <a title="32-tfidf-6" href="./iccv-2013-Deblurring_by_Example_Using_Dense_Correspondence.html">103 iccv-2013-Deblurring by Example Using Dense Correspondence</a></p>
<p>7 0.12142143 <a title="32-tfidf-7" href="./iccv-2013-Fluttering_Pattern_Generation_Using_Modified_Legendre_Sequence_for_Coded_Exposure_Imaging.html">173 iccv-2013-Fluttering Pattern Generation Using Modified Legendre Sequence for Coded Exposure Imaging</a></p>
<p>8 0.093586177 <a title="32-tfidf-8" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>9 0.082414873 <a title="32-tfidf-9" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>10 0.081028283 <a title="32-tfidf-10" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>11 0.078854293 <a title="32-tfidf-11" href="./iccv-2013-Fibonacci_Exposure_Bracketing_for_High_Dynamic_Range_Imaging.html">164 iccv-2013-Fibonacci Exposure Bracketing for High Dynamic Range Imaging</a></p>
<p>12 0.077132083 <a title="32-tfidf-12" href="./iccv-2013-Live_Metric_3D_Reconstruction_on_Mobile_Phones.html">254 iccv-2013-Live Metric 3D Reconstruction on Mobile Phones</a></p>
<p>13 0.07485956 <a title="32-tfidf-13" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>14 0.071222201 <a title="32-tfidf-14" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>15 0.070633017 <a title="32-tfidf-15" href="./iccv-2013-Large-Scale_Multi-resolution_Surface_Reconstruction_from_RGB-D_Sequences.html">228 iccv-2013-Large-Scale Multi-resolution Surface Reconstruction from RGB-D Sequences</a></p>
<p>16 0.06636595 <a title="32-tfidf-16" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>17 0.065668941 <a title="32-tfidf-17" href="./iccv-2013-Real-Time_Body_Tracking_with_One_Depth_Camera_and_Inertial_Sensors.html">341 iccv-2013-Real-Time Body Tracking with One Depth Camera and Inertial Sensors</a></p>
<p>18 0.06490767 <a title="32-tfidf-18" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>19 0.064374357 <a title="32-tfidf-19" href="./iccv-2013-Efficient_Hand_Pose_Estimation_from_a_Single_Depth_Image.html">133 iccv-2013-Efficient Hand Pose Estimation from a Single Depth Image</a></p>
<p>20 0.064351201 <a title="32-tfidf-20" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.14), (1, -0.171), (2, -0.024), (3, 0.084), (4, -0.034), (5, 0.016), (6, 0.034), (7, -0.119), (8, 0.066), (9, 0.057), (10, -0.005), (11, -0.144), (12, 0.061), (13, -0.181), (14, -0.096), (15, 0.116), (16, 0.1), (17, 0.02), (18, -0.047), (19, 0.004), (20, -0.061), (21, -0.168), (22, -0.13), (23, 0.135), (24, -0.097), (25, -0.263), (26, -0.227), (27, -0.023), (28, 0.264), (29, 0.168), (30, -0.105), (31, -0.26), (32, 0.108), (33, 0.26), (34, 0.007), (35, 0.101), (36, -0.008), (37, -0.023), (38, -0.118), (39, -0.136), (40, 0.052), (41, 0.012), (42, 0.019), (43, -0.029), (44, 0.032), (45, 0.109), (46, -0.045), (47, -0.058), (48, -0.019), (49, -0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93391091 <a title="32-lsi-1" href="./iccv-2013-Rolling_Shutter_Stereo.html">363 iccv-2013-Rolling Shutter Stereo</a></p>
<p>Author: Olivier Saurer, Kevin Köser, Jean-Yves Bouguet, Marc Pollefeys</p><p>Abstract: A huge fraction of cameras used nowadays is based on CMOS sensors with a rolling shutter that exposes the image line by line. For dynamic scenes/cameras this introduces undesired effects like stretch, shear and wobble. It has been shown earlier that rotational shake induced rolling shutter effects in hand-held cell phone capture can be compensated based on an estimate of the camera rotation. In contrast, we analyse the case of significant camera motion, e.g. where a bypassing streetlevel capture vehicle uses a rolling shutter camera in a 3D reconstruction framework. The introduced error is depth dependent and cannot be compensated based on camera motion/rotation alone, invalidating also rectification for stereo camera systems. On top, significant lens distortion as often present in wide angle cameras intertwines with rolling shutter effects as it changes the time at which a certain 3D point is seen. We show that naive 3D reconstructions (assuming global shutter) will deliver biased geometry already for very mild assumptions on vehicle speed and resolution. We then develop rolling shutter dense multiview stereo algorithms that solve for time of exposure and depth at the same time, even in the presence of lens distortion and perform an evaluation on ground truth laser scan models as well as on real street-level data.</p><p>same-paper 2 0.93071544 <a title="32-lsi-2" href="./iccv-2013-A_Unified_Rolling_Shutter_and_Motion_Blur_Model_for_3D_Visual_Registration.html">32 iccv-2013-A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration</a></p>
<p>Author: Maxime Meilland, Tom Drummond, Andrew I. Comport</p><p>Abstract: Motion blur and rolling shutter deformations both inhibit visual motion registration, whether it be due to a moving sensor or a moving target. Whilst both deformations exist simultaneously, no models have been proposed to handle them together. Furthermore, neither deformation has been consideredpreviously in the context of monocularfullimage 6 degrees of freedom registration or RGB-D structure and motion. As will be shown, rolling shutter deformation is observed when a camera moves faster than a single pixel in parallax between subsequent scan-lines. Blur is a function of the pixel exposure time and the motion vector. In this paper a complete dense 3D registration model will be derived to accountfor both motion blur and rolling shutter deformations simultaneously. Various approaches will be compared with respect to ground truth and live real-time performance will be demonstratedfor complex scenarios where both blur and shutter deformations are dominant.</p><p>3 0.70121872 <a title="32-lsi-3" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>Author: Bryan Klingner, David Martin, James Roseborough</p><p>Abstract: We describe a structure-from-motion framework that handles “generalized” cameras, such as moving rollingshutter cameras, and works at an unprecedented scale— billions of images covering millions of linear kilometers of roads—by exploiting a good relative pose prior along vehicle paths. We exhibit a planet-scale, appearanceaugmented point cloud constructed with our framework and demonstrate its practical use in correcting the pose of a street-level image collection.</p><p>4 0.61482137 <a title="32-lsi-4" href="./iccv-2013-Fluttering_Pattern_Generation_Using_Modified_Legendre_Sequence_for_Coded_Exposure_Imaging.html">173 iccv-2013-Fluttering Pattern Generation Using Modified Legendre Sequence for Coded Exposure Imaging</a></p>
<p>Author: Hae-Gon Jeon, Joon-Young Lee, Yudeog Han, Seon Joo Kim, In So Kweon</p><p>Abstract: Finding a good binary sequence is critical in determining theperformance ofthe coded exposure imaging, butprevious methods mostly rely on a random search for finding the binary codes, which could easily fail to find good long sequences due to the exponentially growing search space. In this paper, we present a new computationally efficient algorithm for generating the binary sequence, which is especially well suited for longer sequences. We show that the concept of the low autocorrelation binary sequence that has been well exploited in the information theory community can be applied for generating the fluttering patterns of the shutter, propose a new measure of a good binary sequence, and present a new algorithm by modifying the Legendre sequence for the coded exposure imaging. Experiments using both synthetic and real data show that our new algorithm consistently generates better binary sequencesfor the coded exposure problem, yielding better deblurring and resolution enhancement results compared to the previous methods for generating the binary codes.</p><p>5 0.51120692 <a title="32-lsi-5" href="./iccv-2013-Fibonacci_Exposure_Bracketing_for_High_Dynamic_Range_Imaging.html">164 iccv-2013-Fibonacci Exposure Bracketing for High Dynamic Range Imaging</a></p>
<p>Author: Mohit Gupta, Daisuke Iso, Shree K. Nayar</p><p>Abstract: Exposure bracketing for high dynamic range (HDR) imaging involves capturing several images of the scene at different exposures. If either the camera or the scene moves during capture, the captured images must be registered. Large exposure differences between bracketed images lead to inaccurate registration, resulting in artifacts such as ghosting (multiple copies of scene objects) and blur. We present two techniques, one for image capture (Fibonacci exposure bracketing) and one for image registration (generalized registration), to prevent such motion-related artifacts. Fibonacci bracketing involves capturing a sequence of images such that each exposure time is the sum of the previous N(N > 1) exposures. Generalized registration involves estimating motion between sums of contiguous sets of frames, instead of between individual frames. Together, the two techniques ensure that motion is always estimated betweenframes of the same total exposure time. This results in HDR images and videos which have both a large dynamic range andminimal motion-relatedartifacts. We show, by results for several real-world indoor and outdoor scenes, that theproposed approach significantly outperforms several ex- isting bracketing schemes.</p><p>6 0.31749701 <a title="32-lsi-6" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>7 0.2892814 <a title="32-lsi-7" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>8 0.28837928 <a title="32-lsi-8" href="./iccv-2013-Refractive_Structure-from-Motion_on_Underwater_Images.html">348 iccv-2013-Refractive Structure-from-Motion on Underwater Images</a></p>
<p>9 0.27598861 <a title="32-lsi-9" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>10 0.27085167 <a title="32-lsi-10" href="./iccv-2013-Deblurring_by_Example_Using_Dense_Correspondence.html">103 iccv-2013-Deblurring by Example Using Dense Correspondence</a></p>
<p>11 0.26347804 <a title="32-lsi-11" href="./iccv-2013-Live_Metric_3D_Reconstruction_on_Mobile_Phones.html">254 iccv-2013-Live Metric 3D Reconstruction on Mobile Phones</a></p>
<p>12 0.24164362 <a title="32-lsi-12" href="./iccv-2013-Optimal_Orthogonal_Basis_and_Image_Assimilation%3A_Motion_Modeling.html">301 iccv-2013-Optimal Orthogonal Basis and Image Assimilation: Motion Modeling</a></p>
<p>13 0.2412689 <a title="32-lsi-13" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>14 0.23377745 <a title="32-lsi-14" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>15 0.22492929 <a title="32-lsi-15" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>16 0.2231456 <a title="32-lsi-16" href="./iccv-2013-Multi-view_3D_Reconstruction_from_Uncalibrated_Radially-Symmetric_Cameras.html">280 iccv-2013-Multi-view 3D Reconstruction from Uncalibrated Radially-Symmetric Cameras</a></p>
<p>17 0.22069943 <a title="32-lsi-17" href="./iccv-2013-Large-Scale_Multi-resolution_Surface_Reconstruction_from_RGB-D_Sequences.html">228 iccv-2013-Large-Scale Multi-resolution Surface Reconstruction from RGB-D Sequences</a></p>
<p>18 0.21270235 <a title="32-lsi-18" href="./iccv-2013-Modeling_Self-Occlusions_in_Dynamic_Shape_and_Appearance_Tracking.html">270 iccv-2013-Modeling Self-Occlusions in Dynamic Shape and Appearance Tracking</a></p>
<p>19 0.20391946 <a title="32-lsi-19" href="./iccv-2013-Dynamic_Probabilistic_Volumetric_Models.html">128 iccv-2013-Dynamic Probabilistic Volumetric Models</a></p>
<p>20 0.20356008 <a title="32-lsi-20" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.031), (7, 0.014), (12, 0.012), (26, 0.058), (31, 0.033), (35, 0.01), (42, 0.065), (55, 0.27), (64, 0.042), (73, 0.051), (89, 0.254), (98, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84058326 <a title="32-lda-1" href="./iccv-2013-A_Unified_Rolling_Shutter_and_Motion_Blur_Model_for_3D_Visual_Registration.html">32 iccv-2013-A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration</a></p>
<p>Author: Maxime Meilland, Tom Drummond, Andrew I. Comport</p><p>Abstract: Motion blur and rolling shutter deformations both inhibit visual motion registration, whether it be due to a moving sensor or a moving target. Whilst both deformations exist simultaneously, no models have been proposed to handle them together. Furthermore, neither deformation has been consideredpreviously in the context of monocularfullimage 6 degrees of freedom registration or RGB-D structure and motion. As will be shown, rolling shutter deformation is observed when a camera moves faster than a single pixel in parallax between subsequent scan-lines. Blur is a function of the pixel exposure time and the motion vector. In this paper a complete dense 3D registration model will be derived to accountfor both motion blur and rolling shutter deformations simultaneously. Various approaches will be compared with respect to ground truth and live real-time performance will be demonstratedfor complex scenarios where both blur and shutter deformations are dominant.</p><p>2 0.83546311 <a title="32-lda-2" href="./iccv-2013-Joint_Optimization_for_Consistent_Multiple_Graph_Matching.html">224 iccv-2013-Joint Optimization for Consistent Multiple Graph Matching</a></p>
<p>Author: Junchi Yan, Yu Tian, Hongyuan Zha, Xiaokang Yang, Ya Zhang, Stephen M. Chu</p><p>Abstract: The problem of graph matching in general is NP-hard and approaches have been proposed for its suboptimal solution, most focusing on finding the one-to-one node mapping between two graphs. A more general and challenging problem arises when one aims to find consistent mappings across a number of graphs more than two. Conventional graph pair matching methods often result in mapping inconsistency since the mapping between two graphs can either be determined by pair mapping or by an additional anchor graph. To address this issue, a novel formulation is derived which is maximized via alternating optimization. Our method enjoys several advantages: 1) the mappings are jointly optimized rather than sequentially performed by applying pair matching, allowing the global affinity information across graphs can be propagated and explored; 2) the number of concerned variables to optimize is in linear with the number of graphs, being superior to local pair matching resulting in O(n2) variables; 3) the mapping consistency constraints are analytically satisfied during optimization; and 4) off-the-shelf graph pair matching solvers can be reused under the proposed framework in an ‘out-of-thebox’ fashion. Competitive results on both the synthesized data and the real data are reported, by varying the level of deformation, outliers and edge densities. ∗Corresponding author. The work is supported by NSF IIS1116886, NSF IIS-1049694, NSFC 61129001/F010403 and the 111 Project (B07022). Yu Tian Shanghai Jiao Tong University Shanghai, China, 200240 yut ian @ s j tu . edu .cn Xiaokang Yang Shanghai Jiao Tong University Shanghai, China, 200240 xkyang@ s j tu .edu . cn Stephen M. Chu IBM T.J. Waston Research Center Yorktown Heights, NY USA, 10598 s chu @u s . ibm . com</p><p>3 0.82080883 <a title="32-lda-3" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>Author: Feng Liu, Yuzhen Niu, Hailin Jin</p><p>Abstract: Shaky stereoscopic video is not only unpleasant to watch but may also cause 3D fatigue. Stabilizing the left and right view of a stereoscopic video separately using a monocular stabilization method tends to both introduce undesirable vertical disparities and damage horizontal disparities, which may destroy the stereoscopic viewing experience. In this paper, we present a joint subspace stabilization method for stereoscopic video. We prove that the low-rank subspace constraint for monocular video [10] also holds for stereoscopic video. Particularly, the feature trajectories from the left and right video share the same subspace. Based on this proof, we develop a stereo subspace stabilization method that jointly computes a common subspace from the left and right video and uses it to stabilize the two videos simultaneously. Our method meets the stereoscopic constraints without 3D reconstruction or explicit left-right correspondence. We test our method on a variety of stereoscopic videos with different scene content and camera motion. The experiments show that our method achieves high-quality stabilization for stereoscopic video in a robust and efficient way.</p><p>4 0.82031363 <a title="32-lda-4" href="./iccv-2013-Rolling_Shutter_Stereo.html">363 iccv-2013-Rolling Shutter Stereo</a></p>
<p>Author: Olivier Saurer, Kevin Köser, Jean-Yves Bouguet, Marc Pollefeys</p><p>Abstract: A huge fraction of cameras used nowadays is based on CMOS sensors with a rolling shutter that exposes the image line by line. For dynamic scenes/cameras this introduces undesired effects like stretch, shear and wobble. It has been shown earlier that rotational shake induced rolling shutter effects in hand-held cell phone capture can be compensated based on an estimate of the camera rotation. In contrast, we analyse the case of significant camera motion, e.g. where a bypassing streetlevel capture vehicle uses a rolling shutter camera in a 3D reconstruction framework. The introduced error is depth dependent and cannot be compensated based on camera motion/rotation alone, invalidating also rectification for stereo camera systems. On top, significant lens distortion as often present in wide angle cameras intertwines with rolling shutter effects as it changes the time at which a certain 3D point is seen. We show that naive 3D reconstructions (assuming global shutter) will deliver biased geometry already for very mild assumptions on vehicle speed and resolution. We then develop rolling shutter dense multiview stereo algorithms that solve for time of exposure and depth at the same time, even in the presence of lens distortion and perform an evaluation on ground truth laser scan models as well as on real street-level data.</p><p>5 0.81944525 <a title="32-lda-5" href="./iccv-2013-Geometric_Registration_Based_on_Distortion_Estimation.html">183 iccv-2013-Geometric Registration Based on Distortion Estimation</a></p>
<p>Author: Wei Zeng, Mayank Goswami, Feng Luo, Xianfeng Gu</p><p>Abstract: Surface registration plays a fundamental role in many applications in computer vision and aims at finding a oneto-one correspondence between surfaces. Conformal mapping based surface registration methods conformally map 2D/3D surfaces onto 2D canonical domains and perform the matching on the 2D plane. This registration framework reduces dimensionality, and the result is intrinsic to Riemannian metric and invariant under isometric deformation. However, conformal mapping will be affected by inconsistent boundaries and non-isometric deformations of surfaces. In this work, we quantify the effects of boundary variation and non-isometric deformation to conformal mappings, and give the theoretical upper bounds for the distortions of conformal mappings under these two factors. Besides giving the thorough theoretical proofs of the theorems, we verified them by concrete experiments using 3D human facial scans with dynamic expressions and varying boundaries. Furthermore, we used the distortion estimates for reducing search range in feature matching of surface registration applications. The experimental results are consistent with the theoreticalpredictions and also demonstrate the performance improvements in feature tracking.</p><p>6 0.77444941 <a title="32-lda-6" href="./iccv-2013-Estimating_the_3D_Layout_of_Indoor_Scenes_and_Its_Clutter_from_Depth_Sensors.html">144 iccv-2013-Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors</a></p>
<p>7 0.77284342 <a title="32-lda-7" href="./iccv-2013-A_New_Image_Quality_Metric_for_Image_Auto-denoising.html">23 iccv-2013-A New Image Quality Metric for Image Auto-denoising</a></p>
<p>8 0.76511961 <a title="32-lda-8" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>9 0.75540298 <a title="32-lda-9" href="./iccv-2013-Perceptual_Fidelity_Aware_Mean_Squared_Error.html">312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</a></p>
<p>10 0.71028686 <a title="32-lda-10" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>11 0.71019208 <a title="32-lda-11" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>12 0.70876527 <a title="32-lda-12" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>13 0.70809203 <a title="32-lda-13" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>14 0.70697045 <a title="32-lda-14" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>15 0.70689982 <a title="32-lda-15" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>16 0.70659667 <a title="32-lda-16" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<p>17 0.70649797 <a title="32-lda-17" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>18 0.7063579 <a title="32-lda-18" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>19 0.70575941 <a title="32-lda-19" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>20 0.70561975 <a title="32-lda-20" href="./iccv-2013-Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization.html">284 iccv-2013-Multiview Photometric Stereo Using Planar Mesh Parameterization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
