<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>235 iccv-2013-Learning Coupled Feature Spaces for Cross-Modal Matching</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-235" href="#">iccv2013-235</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>235 iccv-2013-Learning Coupled Feature Spaces for Cross-Modal Matching</h1>
<br/><p>Source: <a title="iccv-2013-235-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Wang_Learning_Coupled_Feature_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Kaiye Wang, Ran He, Wei Wang, Liang Wang, Tieniu Tan</p><p>Abstract: Cross-modal matching has recently drawn much attention due to the widespread existence of multimodal data. It aims to match data from different modalities, and generally involves two basic problems: the measure of relevance and coupled feature selection. Most previous works mainly focus on solving the first problem. In this paper, we propose a novel coupled linear regression framework to deal with both problems. Our method learns two projection matrices to map multimodal data into a common feature space, in which cross-modal data matching can be performed. And in the learning procedure, the ?21-norm penalties are imposed on the two projection matrices separately, which leads to select relevant and discriminative features from coupled feature spaces simultaneously. A trace norm is further imposed on the projected data as a low-rank constraint, which enhances the relevance of different modal data with connections. We also present an iterative algorithm based on halfquadratic minimization to solve the proposed regularized linear regression problem. The experimental results on two challenging cross-modal datasets demonstrate that the proposed method outperforms the state-of-the-art approaches.</p><p>Reference: <a title="iccv-2013-235-reference" href="../iccv2013_reference/iccv-2013-Learning_Coupled_Feature_Spaces_for_Cross-Modal_Matching_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 It aims to match data from different modalities, and generally involves two basic problems: the measure of relevance and coupled feature selection. [sent-3, score-0.457]
</p><p>2 In this paper, we propose a novel coupled linear regression framework to deal with both problems. [sent-5, score-0.387]
</p><p>3 Our method learns two projection matrices to map multimodal data into a common feature space, in which cross-modal data matching can be performed. [sent-6, score-0.31]
</p><p>4 21-norm penalties are imposed on the two projection matrices separately, which leads to select relevant and discriminative features from coupled feature spaces simultaneously. [sent-8, score-0.682]
</p><p>5 A trace norm is further imposed on the projected data as a low-rank constraint, which enhances the relevance of different modal data with connections. [sent-9, score-0.603]
</p><p>6 We also present an iterative algorithm based on halfquadratic minimization to solve the proposed regularized linear regression problem. [sent-10, score-0.197]
</p><p>7 The task of cross-modal matching is to predict whether a pair of data points from two different modalities represent the same underlying content or object. [sent-14, score-0.306]
</p><p>8 Take multimedia retrieval for example, one often seeks to find the picture (or video) that best illustrates a given text, or find the text that best describes a given picture (or video). [sent-16, score-0.296]
</p><p>9 Most of them just focus on learning a common latent subspace to make all data comparable. [sent-18, score-0.165]
</p><p>10 UA and UB are projection matrices learned using our method on space A and B. [sent-24, score-0.112]
</p><p>11 21-norm and trace norm are used for coupled feature selection and low-rank relevance measure respectively. [sent-26, score-0.857]
</p><p>12 portant problem, how to simultaneously select relevant and discriminative features from two different feature spaces, is usually ignored. [sent-27, score-0.181]
</p><p>13 Although various feature selection methods [26] have been developed for the single modality data analysis, they are not extended to the case of multi-modality data. [sent-29, score-0.225]
</p><p>14 21-norm has been proved to be a powerful tool for the feature selection problem [5, 8, 15], and trace norm [1, 3, 4, 6] is used to encode the correlation of the design matrix or prior knowledge by enforcing a low-rank solution. [sent-31, score-0.485]
</p><p>15 Motivated by these recent advances, this paper proposes a  novel regularization framework (as shown in Figure 1) for the cross-modal matching problem, by combining common subspace learning and coupled feature selection. [sent-32, score-0.575]
</p><p>16 First, inspired by the potential relationship between Canonical Correlation Analysis (CCA) and linear least squares [23], coupled linear regression is used to project data from different modalities into a common subspace that is defined by label information. [sent-33, score-0.758]
</p><p>17 21-norm is used to select the relevant and discriminative features from coupled 2088  modalities, and the trace norm regularization enforces the relevance of the projected data with potentially connections. [sent-35, score-0.939]
</p><p>18 Second, based on the alternative formulation for the trace norm [4] and the half-quadratic analysis for ? [sent-36, score-0.33]
</p><p>19 21-norm and trace norm into a generic minimization formulation so that subspace learning and coupled feature selection can be performed simultaneously. [sent-41, score-0.909]
</p><p>20 2) An iterative algorithm is presented to efficiently solve such kind of complex minimization problems. [sent-42, score-0.119]
</p><p>21 Section 3 describes our proposed regularized linear regression framework for cross-modal matching, along with an iterative algorithm to solve this problem. [sent-51, score-0.129]
</p><p>22 Based on the hypothesis that there is a ben-  efit to explicitly model correlations between two modalities, CCA is used to learn a common subspace by maximizing the correlation between the two modalities. [sent-59, score-0.174]
</p><p>23 Then, a semantic space is learned to measure the similarity of different modal features. [sent-60, score-0.151]
</p><p>24 They use CCA to learn a common space in which the possibility of whether two non-corresponding face regions belong to the same face can be measured. [sent-63, score-0.134]
</p><p>25 low-resolution photos, Sharma and Jacobs [21] use PLS to linearly map images in different modalities to a common linear subspace in which they are highly correlated. [sent-68, score-0.384]
</p><p>26 They use PLS to switch the image features into the text space, then learn a semantic space for the measure of similarity between two different modalities . [sent-71, score-0.432]
</p><p>27 In [24], Tenenbaum and Freeman propose a bilinear model (BLM) to derive a common space for cross-modal face recognition, and BLM is also used for text-image retrieval in [22]. [sent-72, score-0.205]
</p><p>28 Lei and Li [12] propose coupled spectral regression to learn two associated projections, which project heterogeneous data into a common space respectively in which classification is performed. [sent-74, score-0.451]
</p><p>29 , Generalized Multiview LDA (GMLDA) and Generalized Multiview MFA (GMMFA), and apply them to deal with the cross-media retrieval problem. [sent-81, score-0.102]
</p><p>30 All above methods can be categorized into two classes: one is to learn a common latent space in which both modalities are projected, and the other is to map data of one modality into the space of another one. [sent-82, score-0.437]
</p><p>31 Hence, how to simultaneously select the relevant and discriminative features for different modalities of data is very important. [sent-87, score-0.357]
</p><p>32 Accordingly, we aim to jointly perform common subspace learning and coupled feature selection. [sent-88, score-0.487]
</p><p>33 To achieve this goal, we propose a generic minimization formulation by coupled linear regressions, ? [sent-89, score-0.376]
</p><p>34 21-norm and trace norm, which will be detailed in the next section. [sent-90, score-0.222]
</p><p>35 Learning Coupled Feature Spaces In this section, we present a novel framework for the cross-modal matching problem, which can be formulated  as a minimization problem. [sent-92, score-0.118]
</p><p>36 Then, an iterative algorithm based on half-quadratic optimization is given to solve this minimization problem. [sent-93, score-0.119]
</p><p>37 The Frobenius norm of the matrix M is defined as  ? [sent-98, score-0.108]
</p><p>38 Given a query from one modality, the goal of the cross-modal matching is to return the closest match in another modality. [sent-143, score-0.187]
</p><p>39 As shown in Figure 1, the cross-modal matching generally involves two problems: 1) The first problem is how to measure the relevance of data from different modalities. [sent-144, score-0.154]
</p><p>40 2) The second one is how to select the relevant and discriminative features from the coupled feature spaces, simultaneously. [sent-145, score-0.461]
</p><p>41 They project data from different modalities into a latent space, in which the possibility of whether two different modal data represent the same semantic concept can be measured. [sent-147, score-0.403]
</p><p>42 Compared to dimensionality reduction or feature selection methods performed on the two feature spaces separately, coupled feature selection is more likely to find the most relevant features. [sent-149, score-0.771]
</p><p>43 Based on this consideration, we propose that the feature selection procedure should be performed on coupled feature spaces simultaneously for better matching. [sent-150, score-0.605]
</p><p>44 , xbn] ∈ Rd2×n,  each modality ]h a∈s n samples embedded in diffe]re ∈nt Rdimensional spaces (d1 and d2), and each pair {xia, xib} represents the same underlying content and belongs to the same class. [sent-157, score-0.254]
</p><p>45 Our model aims to learn two projection matrices to map the data of the coupled spaces into the common space defined by class labels. [sent-162, score-0.604]
</p><p>46 21-norm on the projection matrices for coupled feature selection, and impose a lowrank constraint, defined by the trace norm, on the projected data. [sent-164, score-0.738]
</p><p>47 The first term is coupled linear regression, which is used to learn two projection matrices for mapping different modal data into a common space. [sent-194, score-0.584]
</p><p>48 21-norms that play a role of feature selection on two feature spaces simultaneously. [sent-196, score-0.269]
</p><p>49 And the trace norm is to enforce the relevance of projected data with connections. [sent-197, score-0.485]
</p><p>50 Here,  an iterative algorithm based on the half-quadratic minimization [8, 9] is proposed to solve this problem. [sent-202, score-0.119]
</p><p>51 Toward this end, we first need to introduce a variational formulation for the trace norm [4]: Lemma 1. [sent-203, score-0.33]
</p><p>52 and the infimum is attained for S = Using this lemma, we can reform? [sent-211, score-0.139]
</p><p>53 Otherwise, the infimum over S could be attained at a non-invertible S, leading to a non-convergent algorithm. [sent-222, score-0.139]
</p><p>54 The infimum over S is then attained for S = (XTaUaUaTXa + XbTUbUbTXb + μI)1/2  (9)  If we define φ(x) = √x2+ ε , we can replace ? [sent-223, score-0.139]
</p><p>55 Step 1and Step 2 correspond to the trace norm, which is expected to reinforce the relevance of projected data of different modalities with connections. [sent-314, score-0.598]
</p><p>56 21-norms and play an important role in coupled feature selection. [sent-316, score-0.353]
</p><p>57 376)) approximately, where k is the number of iteration needed to converge, n is the number of training samples, and d = max(d1, d2), d1 and d2 are the dimensions of the two modality data, respectively. [sent-352, score-0.148]
</p><p>58 Experimental Results Given a cross-modal problem, we can learn two projection matrices on the training set using the iterative algorithm given by Algorithm 1. [sent-354, score-0.163]
</p><p>59 Then, using the two projection  matrices we can project each pair of data into the common subspace defined by class labels, in which the relevance of projected data from different modalities can be easily measured. [sent-355, score-0.622]
</p><p>60 In the testing phase, we take one modality data of the testing set as the query set to retrieve the other modality data. [sent-356, score-0.357]
</p><p>61 Experimental settings We compare the proposed LCFS approach with several related methods, namely, PLS [21], BLM [22, 24], CCA [7, 19], GMMFA and GMLDA [22], for two cross-modal retrieval tasks: (1) Image query vs. [sent-363, score-0.211]
</p><p>62 The top nine images retrieved by our method on the Pascal VOC dataset, given the tags “boat+water”. [sent-379, score-0.17]
</p><p>63 task is to find the nearest neighbors from the text (or image) database. [sent-380, score-0.178]
</p><p>64 We want more correct matches in the top K documents for a better retrieval. [sent-381, score-0.12]
</p><p>65 cision (AP) of a set of N retrieved documents by AP = T1 ? [sent-384, score-0.203]
</p><p>66 ents in the retrieved set, P(r) denotes the precision of th? [sent-386, score-0.113]
</p><p>67 e top r retrieved documents, and δ(r) = 1 if the rth retrieved document is relevant (where relevant means belonging to the class of the query) and δ(r) = 0 otherwise. [sent-387, score-0.414]
</p><p>68 The MAP is then computed by averaging the AP values over all queries in the query set. [sent-388, score-0.182]
</p><p>69 The precision-recall curve is a classical measure of information retrieval performance, but some researchers [18] consider the characterization of retrieval performance by curves of  precision-scope more expressive for multimedia retrieval. [sent-391, score-0.278]
</p><p>70 The image features are 5 12-dimensional Gist features [11], and the text features are 399-dimensional word frequency features. [sent-404, score-0.178]
</p><p>71 As we mentioned in Section 2, the compared methods just focus on the common subspace learning, so Principal Component Analysis (PCA) is performed on the orig-  inal features to remove redundant features. [sent-405, score-0.168]
</p><p>72 Our method can perform coupled feature selection, so we do not perform PCA on the original features for our method. [sent-406, score-0.353]
</p><p>73 This may be because our method selects the relevant and discriminative features from the two modalities simultaneously, and the learnt common space is more compact and effective. [sent-410, score-0.375]
</p><p>74 This may be because the text features of the Pascal VOC dataset are very sparse, which maybe does not agree the assumption of GMLDA. [sent-415, score-0.178]
</p><p>75 Figure 2 shows the top nine retrieval images using a tag vector containing “boat+water” as query. [sent-416, score-0.15]
</p><p>76 Firstly, tag vectors and image feature vectors are projected into the common space by the proposed method. [sent-417, score-0.188]
</p><p>77 Then, for a tag vector, we return the nearest K images as the retrieved results. [sent-418, score-0.159]
</p><p>78 We can see that most retrieved images are very relevant to the given query. [sent-419, score-0.192]
</p><p>79 The corresponding precision-scope curves and precision-recall curves are plotted in Figure 3. [sent-420, score-0.094]
</p><p>80 , the top K retrieved items) for the precisionscope curves varies from K=50 to 1000. [sent-423, score-0.19]
</p><p>81 The top row shows the performance of different methods based on the precision-scope curves for both forms of cross-modal retrieval tasks, i. [sent-424, score-0.151]
</p><p>82 In each pair, the text is an article describing people, places or some events and the image is closely related to the  content of the article. [sent-440, score-0.213]
</p><p>83 The representation of the text with 10 dimensions is derived from a latent Dirichlet allocation model. [sent-443, score-0.247]
</p><p>84 Due to the low dimensions of image and text features themselves, PCA is not used to reduce the dimensions of the original features here. [sent-445, score-0.254]
</p><p>85 2141 for the image query and text query respectively, only a little bit better than those of GMMFA and GMLDA. [sent-449, score-0.452]
</p><p>86 The reason is that the dimensions of image and text features are low, so the ? [sent-450, score-0.216]
</p><p>87 21-norms of our method for coupled feature selection could hardly take effect. [sent-451, score-0.423]
</p><p>88 We can see that for both forms of cross-modal retrieval, our method finds more correct matches in the top K documents than its compared methods. [sent-455, score-0.12]
</p><p>89 Figure 5 shows two examples of text queries and the top five images retrieved by our method. [sent-456, score-0.366]
</p><p>90 In each case, the query text and its paired image  MethodsImage queryText queryAverage  TableLPGCB2LMC. [sent-457, score-0.343]
</p><p>91 The retrieved images are perceived as belonging to the same category of the query text (“Geography & places” at the top, “Warfare” at the bottom). [sent-466, score-0.428]
</p><p>92 Conclusion In this paper, we have proposed a general regularization framework to solve the problem of cross-modal matching, which consists of coupled subspace learning for different modalities, the ? [sent-468, score-0.434]
</p><p>93 21-norms for coupled feature selection , and the trace norm for the measurement of relevance. [sent-469, score-0.753]
</p><p>94 Under the framework, different projection matrices are learnt to project different modal data into a common subspace defined by label information, and relevant and discriminative  features for the coupled spaces are selected simultaneously in the projection procedure. [sent-470, score-0.972]
</p><p>95 To solve this complex regularization problem, we have harnessed an alternative formulation of the trace norm, and reformulated ? [sent-471, score-0.292]
</p><p>96 Two examples of text queries (the first column) and the top five images (columns  3-7) retrieved by our method on the Wiki  dataset. [sent-511, score-0.366]
</p><p>97 The second column contains the paired images of the text queries . [sent-512, score-0.251]
</p><p>98 Continuum  [3]  [4]  [5] [6]  [7]  [8]  [9]  [10] [11]  [12] [13]  regression for cross-modal multimedia retrieval. [sent-525, score-0.095]
</p><p>99 Trace lasso: a trace norm regularization for correlated designs. [sent-537, score-0.368]
</p><p>100 The heterogeneous feature selection with structual sparsity for multimedia annotation and hashing: a survey. [sent-689, score-0.205]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('coupled', 0.308), ('pls', 0.233), ('gmmfa', 0.23), ('trace', 0.222), ('modalities', 0.221), ('cca', 0.215), ('blm', 0.204), ('gmlda', 0.202), ('ub', 0.193), ('ua', 0.18), ('wiki', 0.179), ('text', 0.178), ('query', 0.137), ('modal', 0.118), ('lcfs', 0.115), ('retrieved', 0.113), ('modality', 0.11), ('spaces', 0.109), ('norm', 0.108), ('uit', 0.106), ('relevance', 0.104), ('qt', 0.095), ('voc', 0.091), ('documents', 0.09), ('subspace', 0.088), ('infimum', 0.086), ('pascal', 0.08), ('lemma', 0.08), ('relevant', 0.079), ('tr', 0.077), ('retrieval', 0.074), ('ft', 0.073), ('selection', 0.07), ('minimization', 0.068), ('rasiwasia', 0.067), ('pages', 0.064), ('sharma', 0.06), ('kaiye', 0.058), ('xatuaxbtub', 0.058), ('xbtub', 0.058), ('xbtububtxb', 0.058), ('xtauauatxa', 0.058), ('matrices', 0.057), ('projection', 0.055), ('attained', 0.053), ('iterative', 0.051), ('regression', 0.051), ('vdiag', 0.051), ('mfa', 0.051), ('projected', 0.051), ('matching', 0.05), ('pca', 0.048), ('regressions', 0.047), ('correntropy', 0.047), ('curves', 0.047), ('common', 0.046), ('heterogeneous', 0.046), ('tag', 0.046), ('multiview', 0.045), ('queries', 0.045), ('feature', 0.045), ('xib', 0.045), ('squares', 0.044), ('face', 0.044), ('multimedia', 0.044), ('bilinear', 0.041), ('correlation', 0.04), ('shan', 0.04), ('rn', 0.039), ('curve', 0.039), ('dimensions', 0.038), ('regularization', 0.038), ('quadrianto', 0.037), ('boat', 0.036), ('wikipedia', 0.036), ('photos', 0.035), ('content', 0.035), ('tenenbaum', 0.035), ('redundant', 0.034), ('xia', 0.033), ('semantic', 0.033), ('reformulated', 0.032), ('lei', 0.032), ('xb', 0.032), ('latent', 0.031), ('canonical', 0.03), ('xa', 0.03), ('top', 0.03), ('water', 0.029), ('map', 0.029), ('discriminative', 0.029), ('simultaneously', 0.028), ('deal', 0.028), ('multimodal', 0.028), ('paired', 0.028), ('objective', 0.027), ('tags', 0.027), ('china', 0.027), ('ui', 0.027), ('regularized', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="235-tfidf-1" href="./iccv-2013-Learning_Coupled_Feature_Spaces_for_Cross-Modal_Matching.html">235 iccv-2013-Learning Coupled Feature Spaces for Cross-Modal Matching</a></p>
<p>Author: Kaiye Wang, Ran He, Wei Wang, Liang Wang, Tieniu Tan</p><p>Abstract: Cross-modal matching has recently drawn much attention due to the widespread existence of multimodal data. It aims to match data from different modalities, and generally involves two basic problems: the measure of relevance and coupled feature selection. Most previous works mainly focus on solving the first problem. In this paper, we propose a novel coupled linear regression framework to deal with both problems. Our method learns two projection matrices to map multimodal data into a common feature space, in which cross-modal data matching can be performed. And in the learning procedure, the ?21-norm penalties are imposed on the two projection matrices separately, which leads to select relevant and discriminative features from coupled feature spaces simultaneously. A trace norm is further imposed on the projected data as a low-rank constraint, which enhances the relevance of different modal data with connections. We also present an iterative algorithm based on halfquadratic minimization to solve the proposed regularized linear regression problem. The experimental results on two challenging cross-modal datasets demonstrate that the proposed method outperforms the state-of-the-art approaches.</p><p>2 0.14372055 <a title="235-tfidf-2" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<p>Author: Anand Mishra, Karteek Alahari, C.V. Jawahar</p><p>Abstract: We present an approach for the text-to-image retrieval problem based on textual content present in images. Given the recent developments in understanding text in images, an appealing approach to address this problem is to localize and recognize the text, and then query the database, as in a text retrieval problem. We show that such an approach, despite being based on state-of-the-artmethods, is insufficient, and propose a method, where we do not rely on an exact localization and recognition pipeline. We take a query-driven search approach, where we find approximate locations of characters in the text query, and then impose spatial constraints to generate a ranked list of images in the database. The retrieval performance is evaluated on public scene text datasets as well as three large datasets, namely IIIT scene text retrieval, Sports-10K and TV series-1M, we introduce.</p><p>3 0.13329983 <a title="235-tfidf-3" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>Author: Xiao Cai, Feiping Nie, Weidong Cai, Heng Huang</p><p>Abstract: Automatic image categorization has become increasingly important with the development of Internet and the growth in the size of image databases. Although the image categorization can be formulated as a typical multiclass classification problem, two major challenges have been raised by the real-world images. On one hand, though using more labeled training data may improve the prediction performance, obtaining the image labels is a time consuming as well as biased process. On the other hand, more and more visual descriptors have been proposed to describe objects and scenes appearing in images and different features describe different aspects of the visual characteristics. Therefore, how to integrate heterogeneous visual features to do the semi-supervised learning is crucial for categorizing large-scale image data. In this paper, we propose a novel approach to integrate heterogeneous features by performing multi-modal semi-supervised classification on unlabeled as well as unsegmented images. Considering each type of feature as one modality, taking advantage of the large amoun- t of unlabeled data information, our new adaptive multimodal semi-supervised classification (AMMSS) algorithm learns a commonly shared class indicator matrix and the weights for different modalities (image features) simultaneously.</p><p>4 0.1278218 <a title="235-tfidf-4" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>Author: Basura Fernando, Tinne Tuytelaars</p><p>Abstract: In this paper we present a new method for object retrieval starting from multiple query images. The use of multiple queries allows for a more expressive formulation of the query object including, e.g., different viewpoints and/or viewing conditions. This, in turn, leads to more diverse and more accurate retrieval results. When no query images are available to the user, they can easily be retrieved from the internet using a standard image search engine. In particular, we propose a new method based on pattern mining. Using the minimal description length principle, we derive the most suitable set of patterns to describe the query object, with patterns corresponding to local feature configurations. This results in apowerful object-specific mid-level image representation. The archive can then be searched efficiently for similar images based on this representation, using a combination of two inverted file systems. Since the patterns already encode local spatial information, good results on several standard image retrieval datasets are obtained even without costly re-ranking based on geometric verification.</p><p>5 0.12065308 <a title="235-tfidf-5" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>Author: S. Karthikeyan, Vignesh Jagadeesh, Renuka Shenoy, Miguel Ecksteinz, B.S. Manjunath</p><p>Abstract: Eye movement studies have confirmed that overt attention is highly biased towards faces and text regions in images. In this paper we explore a novel problem of predicting face and text regions in images using eye tracking data from multiple subjects. The problem is challenging as we aim to predict the semantics (face/text/background) only from eye tracking data without utilizing any image information. The proposed algorithm spatially clusters eye tracking data obtained in an image into different coherent groups and subsequently models the likelihood of the clusters containing faces and text using afully connectedMarkov Random Field (MRF). Given the eye tracking datafrom a test image, itpredicts potential face/head (humans, dogs and cats) and text locations reliably. Furthermore, the approach can be used to select regions of interest for further analysis by object detectors for faces and text. The hybrid eye position/object detector approach achieves better detection performance and reduced computation time compared to using only the object detection algorithm. We also present a new eye tracking dataset on 300 images selected from ICDAR, Street-view, Flickr and Oxford-IIIT Pet Dataset from 15 subjects.</p><p>6 0.11901881 <a title="235-tfidf-6" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>7 0.11720092 <a title="235-tfidf-7" href="./iccv-2013-Robust_Subspace_Clustering_via_Half-Quadratic_Minimization.html">360 iccv-2013-Robust Subspace Clustering via Half-Quadratic Minimization</a></p>
<p>8 0.10903054 <a title="235-tfidf-8" href="./iccv-2013-Coupled_Dictionary_and_Feature_Space_Learning_with_Applications_to_Cross-Domain_Image_Synthesis_and_Recognition.html">96 iccv-2013-Coupled Dictionary and Feature Space Learning with Applications to Cross-Domain Image Synthesis and Recognition</a></p>
<p>9 0.10753414 <a title="235-tfidf-9" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<p>10 0.10287926 <a title="235-tfidf-10" href="./iccv-2013-Latent_Space_Sparse_Subspace_Clustering.html">232 iccv-2013-Latent Space Sparse Subspace Clustering</a></p>
<p>11 0.10066897 <a title="235-tfidf-11" href="./iccv-2013-Non-convex_P-Norm_Projection_for_Robust_Sparsity.html">292 iccv-2013-Non-convex P-Norm Projection for Robust Sparsity</a></p>
<p>12 0.097257093 <a title="235-tfidf-12" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>13 0.096408695 <a title="235-tfidf-13" href="./iccv-2013-Correntropy_Induced_L2_Graph_for_Robust_Subspace_Clustering.html">94 iccv-2013-Correntropy Induced L2 Graph for Robust Subspace Clustering</a></p>
<p>14 0.09332376 <a title="235-tfidf-14" href="./iccv-2013-PhotoOCR%3A_Reading_Text_in_Uncontrolled_Conditions.html">315 iccv-2013-PhotoOCR: Reading Text in Uncontrolled Conditions</a></p>
<p>15 0.088976666 <a title="235-tfidf-15" href="./iccv-2013-Correlation_Adaptive_Subspace_Segmentation_by_Trace_Lasso.html">93 iccv-2013-Correlation Adaptive Subspace Segmentation by Trace Lasso</a></p>
<p>16 0.083758913 <a title="235-tfidf-16" href="./iccv-2013-New_Graph_Structured_Sparsity_Model_for_Multi-label_Image_Annotations.html">290 iccv-2013-New Graph Structured Sparsity Model for Multi-label Image Annotations</a></p>
<p>17 0.083157316 <a title="235-tfidf-17" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>18 0.081017308 <a title="235-tfidf-18" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>19 0.079955287 <a title="235-tfidf-19" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>20 0.079798289 <a title="235-tfidf-20" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.182), (1, 0.062), (2, -0.071), (3, -0.096), (4, -0.047), (5, 0.124), (6, 0.045), (7, 0.021), (8, 0.009), (9, -0.001), (10, 0.154), (11, -0.044), (12, 0.001), (13, 0.04), (14, -0.008), (15, -0.013), (16, 0.008), (17, -0.031), (18, -0.011), (19, 0.003), (20, 0.014), (21, 0.025), (22, -0.05), (23, -0.068), (24, 0.037), (25, 0.004), (26, 0.06), (27, 0.011), (28, 0.012), (29, -0.001), (30, 0.008), (31, 0.037), (32, 0.029), (33, 0.04), (34, -0.026), (35, 0.025), (36, 0.014), (37, 0.076), (38, 0.019), (39, 0.031), (40, 0.047), (41, -0.134), (42, 0.003), (43, -0.01), (44, 0.067), (45, 0.017), (46, 0.016), (47, -0.049), (48, -0.024), (49, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93757242 <a title="235-lsi-1" href="./iccv-2013-Learning_Coupled_Feature_Spaces_for_Cross-Modal_Matching.html">235 iccv-2013-Learning Coupled Feature Spaces for Cross-Modal Matching</a></p>
<p>Author: Kaiye Wang, Ran He, Wei Wang, Liang Wang, Tieniu Tan</p><p>Abstract: Cross-modal matching has recently drawn much attention due to the widespread existence of multimodal data. It aims to match data from different modalities, and generally involves two basic problems: the measure of relevance and coupled feature selection. Most previous works mainly focus on solving the first problem. In this paper, we propose a novel coupled linear regression framework to deal with both problems. Our method learns two projection matrices to map multimodal data into a common feature space, in which cross-modal data matching can be performed. And in the learning procedure, the ?21-norm penalties are imposed on the two projection matrices separately, which leads to select relevant and discriminative features from coupled feature spaces simultaneously. A trace norm is further imposed on the projected data as a low-rank constraint, which enhances the relevance of different modal data with connections. We also present an iterative algorithm based on halfquadratic minimization to solve the proposed regularized linear regression problem. The experimental results on two challenging cross-modal datasets demonstrate that the proposed method outperforms the state-of-the-art approaches.</p><p>2 0.66262054 <a title="235-lsi-2" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<p>Author: Anand Mishra, Karteek Alahari, C.V. Jawahar</p><p>Abstract: We present an approach for the text-to-image retrieval problem based on textual content present in images. Given the recent developments in understanding text in images, an appealing approach to address this problem is to localize and recognize the text, and then query the database, as in a text retrieval problem. We show that such an approach, despite being based on state-of-the-artmethods, is insufficient, and propose a method, where we do not rely on an exact localization and recognition pipeline. We take a query-driven search approach, where we find approximate locations of characters in the text query, and then impose spatial constraints to generate a ranked list of images in the database. The retrieval performance is evaluated on public scene text datasets as well as three large datasets, namely IIIT scene text retrieval, Sports-10K and TV series-1M, we introduce.</p><p>3 0.64070678 <a title="235-lsi-3" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<p>Author: Shi Qiu, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: This paper proposes modeling the complex web image collections with an automatically generated graph structure called visual semantic complex network (VSCN). The nodes on this complex network are clusters of images with both visual and semantic consistency, called semantic concepts. These nodes are connected based on the visual and semantic correlations. Our VSCN with 33, 240 concepts is generated from a collection of 10 million web images. 1 A great deal of valuable information on the structures of the web image collections can be revealed by exploring the VSCN, such as the small-world behavior, concept community, indegree distribution, hubs, and isolated concepts. It not only helps us better understand the web image collections at a macroscopic level, but also has many important practical applications. This paper presents two application examples: content-based image retrieval and image browsing. Experimental results show that the VSCN leads to significant improvement on both the precision of image retrieval (over 200%) and user experience for image browsing.</p><p>4 0.6368404 <a title="235-lsi-4" href="./iccv-2013-3D_Sub-query_Expansion_for_Improving_Sketch-Based_Multi-view_Image_Retrieval.html">3 iccv-2013-3D Sub-query Expansion for Improving Sketch-Based Multi-view Image Retrieval</a></p>
<p>Author: Yen-Liang Lin, Cheng-Yu Huang, Hao-Jeng Wang, Winston Hsu</p><p>Abstract: We propose a 3D sub-query expansion approach for boosting sketch-based multi-view image retrieval. The core idea of our method is to automatically convert two (guided) 2D sketches into an approximated 3D sketch model, and then generate multi-view sketches as expanded sub-queries to improve the retrieval performance. To learn the weights among synthesized views (sub-queries), we present a new multi-query feature to model the similarity between subqueries and dataset images, and formulate it into a convex optimization problem. Our approach shows superior performance compared with the state-of-the-art approach on a public multi-view image dataset. Moreover, we also conduct sensitivity tests to analyze the parameters of our approach based on the gathered user sketches.</p><p>5 0.60193515 <a title="235-lsi-5" href="./iccv-2013-Non-convex_P-Norm_Projection_for_Robust_Sparsity.html">292 iccv-2013-Non-convex P-Norm Projection for Robust Sparsity</a></p>
<p>Author: Mithun Das Gupta, Sanjeev Kumar</p><p>Abstract: In this paper, we investigate the properties of Lp norm (p ≤ 1) within a projection framework. We start with the (KpK T≤ equations of the neoctni-olnin efraarm optimization problem a thnde then use its key properties to arrive at an algorithm for Lp norm projection on the non-negative simplex. We compare with L1projection which needs prior knowledge of the true norm, as well as hard thresholding based sparsificationproposed in recent compressed sensing literature. We show performance improvements compared to these techniques across different vision applications.</p><p>6 0.59726483 <a title="235-lsi-6" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>7 0.59506899 <a title="235-lsi-7" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>8 0.58983666 <a title="235-lsi-8" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>9 0.58691943 <a title="235-lsi-9" href="./iccv-2013-Correntropy_Induced_L2_Graph_for_Robust_Subspace_Clustering.html">94 iccv-2013-Correntropy Induced L2 Graph for Robust Subspace Clustering</a></p>
<p>10 0.57906878 <a title="235-lsi-10" href="./iccv-2013-Query-Adaptive_Asymmetrical_Dissimilarities_for_Visual_Object_Retrieval.html">334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</a></p>
<p>11 0.57898104 <a title="235-lsi-11" href="./iccv-2013-New_Graph_Structured_Sparsity_Model_for_Multi-label_Image_Annotations.html">290 iccv-2013-New Graph Structured Sparsity Model for Multi-label Image Annotations</a></p>
<p>12 0.56094205 <a title="235-lsi-12" href="./iccv-2013-Distributed_Low-Rank_Subspace_Segmentation.html">122 iccv-2013-Distributed Low-Rank Subspace Segmentation</a></p>
<p>13 0.56025094 <a title="235-lsi-13" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>14 0.55925888 <a title="235-lsi-14" href="./iccv-2013-Paper_Doll_Parsing%3A_Retrieving_Similar_Styles_to_Parse_Clothing_Items.html">306 iccv-2013-Paper Doll Parsing: Retrieving Similar Styles to Parse Clothing Items</a></p>
<p>15 0.55103749 <a title="235-lsi-15" href="./iccv-2013-Correlation_Adaptive_Subspace_Segmentation_by_Trace_Lasso.html">93 iccv-2013-Correlation Adaptive Subspace Segmentation by Trace Lasso</a></p>
<p>16 0.54925591 <a title="235-lsi-16" href="./iccv-2013-PhotoOCR%3A_Reading_Text_in_Uncontrolled_Conditions.html">315 iccv-2013-PhotoOCR: Reading Text in Uncontrolled Conditions</a></p>
<p>17 0.54689425 <a title="235-lsi-17" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<p>18 0.53930259 <a title="235-lsi-18" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>19 0.53783762 <a title="235-lsi-19" href="./iccv-2013-GOSUS%3A_Grassmannian_Online_Subspace_Updates_with_Structured-Sparsity.html">182 iccv-2013-GOSUS: Grassmannian Online Subspace Updates with Structured-Sparsity</a></p>
<p>20 0.53558093 <a title="235-lsi-20" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.081), (7, 0.022), (12, 0.015), (13, 0.011), (26, 0.062), (27, 0.016), (31, 0.049), (42, 0.181), (64, 0.05), (68, 0.249), (73, 0.022), (89, 0.147)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80158055 <a title="235-lda-1" href="./iccv-2013-Learning_Coupled_Feature_Spaces_for_Cross-Modal_Matching.html">235 iccv-2013-Learning Coupled Feature Spaces for Cross-Modal Matching</a></p>
<p>Author: Kaiye Wang, Ran He, Wei Wang, Liang Wang, Tieniu Tan</p><p>Abstract: Cross-modal matching has recently drawn much attention due to the widespread existence of multimodal data. It aims to match data from different modalities, and generally involves two basic problems: the measure of relevance and coupled feature selection. Most previous works mainly focus on solving the first problem. In this paper, we propose a novel coupled linear regression framework to deal with both problems. Our method learns two projection matrices to map multimodal data into a common feature space, in which cross-modal data matching can be performed. And in the learning procedure, the ?21-norm penalties are imposed on the two projection matrices separately, which leads to select relevant and discriminative features from coupled feature spaces simultaneously. A trace norm is further imposed on the projected data as a low-rank constraint, which enhances the relevance of different modal data with connections. We also present an iterative algorithm based on halfquadratic minimization to solve the proposed regularized linear regression problem. The experimental results on two challenging cross-modal datasets demonstrate that the proposed method outperforms the state-of-the-art approaches.</p><p>2 0.7776401 <a title="235-lda-2" href="./iccv-2013-How_Related_Exemplars_Help_Complex_Event_Detection_in_Web_Videos%3F.html">203 iccv-2013-How Related Exemplars Help Complex Event Detection in Web Videos?</a></p>
<p>Author: Yi Yang, Zhigang Ma, Zhongwen Xu, Shuicheng Yan, Alexander G. Hauptmann</p><p>Abstract: Compared to visual concepts such as actions, scenes and objects, complex event is a higher level abstraction of longer video sequences. For example, a “marriage proposal” event is described by multiple objects (e.g., ring, faces), scenes (e.g., in a restaurant, outdoor) and actions (e.g., kneeling down). The positive exemplars which exactly convey the precise semantic of an event are hard to obtain. It would be beneficial to utilize the related exemplars for complex event detection. However, the semantic correlations between related exemplars and the target event vary substantially as relatedness assessment is subjective. Two related exemplars can be about completely different events, e.g., in the TRECVID MED dataset, both bicycle riding and equestrianism are labeled as related to “attempting a bike trick” event. To tackle the subjectiveness of human assessment, our algorithm automatically evaluates how positive the related exemplars are for the detection of an event and uses them on an exemplar-specific basis. Experiments demonstrate that our algorithm is able to utilize related exemplars adaptively, and the algorithm gains good perform- z. ance for complex event detection.</p><p>3 0.74576694 <a title="235-lda-3" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<p>Author: Fan Wang, Qixing Huang, Leonidas J. Guibas</p><p>Abstract: Joint segmentation of image sets has great importance for object recognition, image classification, and image retrieval. In this paper, we aim to jointly segment a set of images starting from a small number of labeled images or none at all. To allow the images to share segmentation information with each other, we build a network that contains segmented as well as unsegmented images, and extract functional maps between connected image pairs based on image appearance features. These functional maps act as general property transporters between the images and, in particular, are used to transfer segmentations. We define and operate in a reduced functional space optimized so that the functional maps approximately satisfy cycle-consistency under composition in the network. A joint optimization framework is proposed to simultaneously generate all segmentation functions over the images so that they both align with local segmentation cues in each particular image, and agree with each other under network transportation. This formulation allows us to extract segmentations even with no training data, but can also exploit such data when available. The collective effect of the joint processing using functional maps leads to accurate information sharing among images and yields superior segmentation results, as shown on the iCoseg, MSRC, and PASCAL data sets.</p><p>4 0.72907066 <a title="235-lda-4" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<p>Author: Jiyan Pan, Takeo Kanade</p><p>Abstract: Objects in a real world image cannot have arbitrary appearance, sizes and locations due to geometric constraints in 3D space. Such a 3D geometric context plays an important role in resolving visual ambiguities and achieving coherent object detection. In this paper, we develop a RANSAC-CRF framework to detect objects that are geometrically coherent in the 3D world. Different from existing methods, we propose a novel generalized RANSAC algorithm to generate global 3D geometry hypothesesfrom local entities such that outlier suppression and noise reduction is achieved simultaneously. In addition, we evaluate those hypotheses using a CRF which considers both the compatibility of individual objects under global 3D geometric context and the compatibility between adjacent objects under local 3D geometric context. Experiment results show that our approach compares favorably with the state of the art.</p><p>5 0.71727616 <a title="235-lda-5" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>Author: Jiang Wang, Ying Wu</p><p>Abstract: Temporal misalignment and duration variation in video actions largely influence the performance of action recognition, but it is very difficult to specify effective temporal alignment on action sequences. To address this challenge, this paper proposes a novel discriminative learning-based temporal alignment method, called maximum margin temporal warping (MMTW), to align two action sequences and measure their matching score. Based on the latent structure SVM formulation, the proposed MMTW method is able to learn a phantom action template to represent an action class for maximum discrimination against other classes. The recognition of this action class is based on the associated learned alignment of the input action. Extensive experiments on five benchmark datasets have demonstrated that this MMTW model is able to significantly promote the accuracy and robustness of action recognition under temporal misalignment and variations.</p><p>6 0.71478778 <a title="235-lda-6" href="./iccv-2013-Unsupervised_Intrinsic_Calibration_from_a_Single_Frame_Using_a_%22Plumb-Line%22_Approach.html">436 iccv-2013-Unsupervised Intrinsic Calibration from a Single Frame Using a "Plumb-Line" Approach</a></p>
<p>7 0.71360457 <a title="235-lda-7" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>8 0.71282971 <a title="235-lda-8" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>9 0.71120179 <a title="235-lda-9" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>10 0.70850545 <a title="235-lda-10" href="./iccv-2013-Latent_Multitask_Learning_for_View-Invariant_Action_Recognition.html">231 iccv-2013-Latent Multitask Learning for View-Invariant Action Recognition</a></p>
<p>11 0.70826077 <a title="235-lda-11" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>12 0.70817178 <a title="235-lda-12" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>13 0.70733392 <a title="235-lda-13" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>14 0.706743 <a title="235-lda-14" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>15 0.70601773 <a title="235-lda-15" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>16 0.70597649 <a title="235-lda-16" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>17 0.70502973 <a title="235-lda-17" href="./iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding.html">14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</a></p>
<p>18 0.70498145 <a title="235-lda-18" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>19 0.70418131 <a title="235-lda-19" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>20 0.70365787 <a title="235-lda-20" href="./iccv-2013-Correlation_Adaptive_Subspace_Segmentation_by_Trace_Lasso.html">93 iccv-2013-Correlation Adaptive Subspace Segmentation by Trace Lasso</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
