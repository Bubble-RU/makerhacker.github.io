<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-98" href="#">iccv2013-98</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</h1>
<br/><p>Source: <a title="iccv-2013-98-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Yan_Cross-Field_Joint_Image_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Qiong Yan, Xiaoyong Shen, Li Xu, Shaojie Zhuo, Xiaopeng Zhang, Liang Shen, Jiaya Jia</p><p>Abstract: Color, infrared, and flash images captured in different fields can be employed to effectively eliminate noise and other visual artifacts. We propose a two-image restoration framework considering input images in different fields, for example, one noisy color image and one dark-flashed nearinfrared image. The major issue in such a framework is to handle structure divergence and find commonly usable edges and smooth transition for visually compelling image reconstruction. We introduce a scale map as a competent representation to explicitly model derivative-level confidence and propose new functions and a numerical solver to effectively infer it following new structural observations. Our method is general and shows a principled way for cross-field restoration.</p><p>Reference: <a title="iccv-2013-98-reference" href="../iccv2013_reference/iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 hk / leo j i /pro j ect s / cro s s fie ld/ a  Abstract Color, infrared, and flash images captured in different fields can be employed to effectively eliminate noise and other visual artifacts. [sent-5, score-0.326]
</p><p>2 We propose a two-image restoration framework considering input images in different fields, for example, one noisy color image and one dark-flashed nearinfrared image. [sent-6, score-0.379]
</p><p>3 The major issue in such a framework is to handle structure divergence and find commonly usable edges and smooth transition for visually compelling image reconstruction. [sent-7, score-0.063]
</p><p>4 We introduce a scale map as a competent representation to explicitly model derivative-level confidence and propose new functions and a numerical solver to effectively infer it following new structural observations. [sent-8, score-0.116]
</p><p>5 They could be very noisy when increasing ISO in a short  exposure duration. [sent-12, score-0.091]
</p><p>6 Using flash might improve lighting; but it creates unwanted shadow and highlight, or changes tone of the image. [sent-13, score-0.387]
</p><p>7 The methods of [6, 14, 1] restore a color image based on flash and non-flash inputs of the same scene. [sent-14, score-0.339]
</p><p>8 Recently, because of the popularity of other imaging devices, more computational photography and computer vision solutions based on images captured under different configurations were developed. [sent-15, score-0.07]
</p><p>9 For example, near infrared (NIR) images are with a single channel recording infrared light reflected from objects with spectrum ranging from 700nm-1000nmin wavelength. [sent-16, score-0.277]
</p><p>10 This enables a configuration to take an NIR image with less noisy details by dark flash [11] to guide corresponding noisy color image restoration. [sent-18, score-0.559]
</p><p>11 The main advantage is on only using NIR flash invisible to naked human eyes, making  (a) RGB Image(b) NIR Image  (c) Close-up Comparison Figure 1. [sent-19, score-0.261]
</p><p>12 it a suitable way for daily portrait photography and of remarkable practical importance. [sent-25, score-0.07]
</p><p>13 [11] used gradients ofa dark-flashed image, capturing ultraviolet (UV) and NIR light to guide noise removal in the color image. [sent-27, score-0.2]
</p><p>14 In [21] and [16], the detail layer was manipulated differently for RGB and haze image enhancement. [sent-30, score-0.111]
</p><p>15 Several methods also explore other image fusion applications in two-image deblurring [19], matting [17], tone  mapping [7], upsampling [10], context enhancement [15], relighting [2], to name a few. [sent-31, score-0.138]
</p><p>16 11553377  We note existing methods work well for their respective applications by handling different detail layers or gradients from multiple images. [sent-34, score-0.057]
</p><p>17 tIt r oisw wdu oef to varied reflectance to infrared and visible light. [sent-43, score-0.096]
</p><p>18 If one uses flash only ofowr t ahned dN HIRig image, bity inevitably generates highlight/shadow that is not contained in the other image. [sent-51, score-0.261]
</p><p>19 These issues are caused by inherent discrepancy of structures in different types of images, which we call crossfield problems. [sent-53, score-0.083]
</p><p>20 Simple joint image filtering [18, 8] could blur weak edges due to the inherent smoothing property. [sent-55, score-0.109]
</p><p>21 Directly transferring guidance gradients to the noisy field also results in unnatural appearance. [sent-56, score-0.387]
</p><p>22 In this paper, we propose a framework via novel scale map construction. [sent-57, score-0.059]
</p><p>23 This map captures the nature of structure discrepancy between images and has clear statistical and numerical meanings. [sent-58, score-0.139]
</p><p>24 Based on its analysis, we design functions to form an optimal scale map considering adaptive smoothing, edge preservation, and guidance strength manipulation. [sent-59, score-0.26]
</p><p>25 We also develop an effective solver via robust function approximation and problem decomposition, which converges in less than 5 passes compared to other gradient decent alternatives that may need tens or hundreds of iterations. [sent-61, score-0.129]
</p><p>26 Modeling and Formulation Our system takes the input of a noisy RGB image I0 and a guidance image G captured from the same camera position. [sent-63, score-0.318]
</p><p>27 Plot (b) contains gradients along the vertical line in the top two patches. [sent-74, score-0.057]
</p><p>28 We introduce an auxiliary map s with the same size as G, which is key to our method, to adapt structure of G to that of I∗ – the ground truth noise-free image. [sent-79, score-0.059]
</p><p>29 Era fcohr meilnegmae nt v si irn map s, wnhde ryei indexes pixels, is a scalar, measuring robust difference between corresponding gradients in the two images. [sent-84, score-0.282]
</p><p>30 Simply put, s is a ratio map between the guidance and latent images. [sent-85, score-0.26]
</p><p>31 Property of s First, sign of each si can be either positive or negative. [sent-91, score-0.199]
</p><p>32 A negative si means edges exist in the two images, but with opposite directions, as demonstrated in Fig. [sent-92, score-0.199]
</p><p>33 Second, when the guidance image G contains extra shadow and highlight caused by flash, which are absent in ∇I∗ , si with value 0 can help ignore them. [sent-94, score-0.538]
</p><p>34 Finally, si can be any value when ∇Gi = 0 – that is, guidance edge nd boees annoyt e vxailsut,e su wchhe as ∇thGe red letters in Fig. [sent-95, score-0.367]
</p><p>35 In this case, under local smoothness, si being 0 is a good choice. [sent-97, score-0.166]
</p><p>36 In short, an optimal s map should be able to represent  all these structure discrepancies. [sent-98, score-0.059]
</p><p>37 Data Term about s In |si∇Gi − ∇Ii |, where iindexes pixels, ∇Gi can be analogously regarded as a rscea ile i map fsor p si lds,ue ∇ toG the dual relation between si and ∇Gi. [sent-110, score-0.391]
</p><p>38 It controls the penalty when computing si for diafnfedre ∇ntG pixels. [sent-111, score-0.205]
</p><p>39 Freumrtohvere sto t ahveo iudn tehxep eexcttreedm sec asiltiunagti eonff wechten ca ∇usxeGdi boyr ∇ ∇yGGi Fisu rclthoseer ttoo zveoriod, tahned e xentrleimst eth seit aubatiliiotny wtoh erenj ∇ect outolier r∇s, we define our data term as wremhiochve iss  E1(s,I) =? [sent-120, score-0.094]
</p><p>40 Result in (b) from anisotropic smoothing contains higher contrast structure. [sent-126, score-0.173]
</p><p>41 pi,k, where k ∈ {x, y}, is a truncation function  pi,k=sign(∇kGi) · m1ax(|∇kGi|,ε),  (6)  where sign(x) is the sign operator, outputting 1 if ∇kGi is positive or zero and outputting -1 otherwise. [sent-132, score-0.129]
</p><p>42 i  where ρ is the same robust function and I0,i is the color of pixel iin I0. [sent-140, score-0.074]
</p><p>43 E2 (I) requires the restoration result not to wildly deviate from the input noisy image I0 especially along salient edges. [sent-141, score-0.335]
</p><p>44 Regularization Term Our regularization term is defined with anisotropic gradient tensors [13, 4]. [sent-145, score-0.211]
</p><p>45 4, uniformly smoothing s in all directions blurs sharp edges. [sent-149, score-0.138]
</p><p>46 Our anisotropic tensor scheme preserves sharp edges  according to gradient directions of G. [sent-150, score-0.303]
</p><p>47 By a few algebraic operations, an anisotropic tensor is expressed as  D(∇Gi) = (∇Gi)21+ 2η2((∇Gi⊥)(∇Gi⊥)T+ η21),  (8)  where ∇Gi⊥ = (∇yGi, −∇xGi)T is a vectorperpendicular wtoh e∇rGe∇i, G1 is= an ∇ident,it−y ∇matrix and scalar η controls the isotropic smoothness. [sent-151, score-0.293]
</p><p>48 (12)  Different smoothing penalties are controlled by μi,1 and μi,2 in directions vi,1 and vi,2, across and along edges respectively. [sent-165, score-0.142]
</p><p>49 The final smoothing term is thus defined as E3(∇s)  = ? [sent-167, score-0.102]
</p><p>50 Final Objective Function The final objective function to estimate the s map and restore image I written as is E(s, I) = E1(s, I) + λE2(I) + βE3(∇s),  (14)  where λ controls the confidence on noisy image I0, and β corresponds to smoothness of s. [sent-174, score-0.279]
</p><p>51 Naive gradient in decent cannot guarantee optimality and leads to very slow convergence even for a local minimum. [sent-178, score-0.098]
</p><p>52 We contrarily propose an iterative method, which finds constraints to shape the s map according to its characteristics and yields the effect to remove intensive noise from input I0. [sent-179, score-0.119]
</p><p>53 tPox ,c Py, Autxe, Ay agned g gBra are diagonal em xa−tric aensd, wy−hdosiere ic-ttiho diagonal elements are defined as (Px)ii  = pi,x, = pi,y,  (Ax)ii  (Py)ii (Ay)ii Bii = φ(Ii − I0,i). [sent-198, score-0.058]
</p><p>54 = φ(si = φ(si  − pi,x∇xIi), − pi,y∇yIi),  Among them, Ax, Ay and B account for the re-weighting process and are typically computed using estimates from previous iterations Px and Py are normalization terms from the guidance image. [sent-199, score-0.231]
</p><p>55 Note the last term sTLs controls spatial smoothness of s, where matrix L is a smoothing Laplacian, expressed as –  L = CxT(Σ1Vx2 + Σ2Vy2)Cx + CyT(Σ2Vx2 + Σ1Vy2)Cy + 2CTy(Σ1 − Σ2)VxVyCx  (18)  after a bit complicated derivations. [sent-202, score-0.197]
</p><p>56 1:input: noisy image I0, guidance image G, parameters β and λ 2: 3: 4: 5: 6: 7:  initialize I I0, s ← 1 ← repeat estimate s according to Eq. [sent-207, score-0.292]
</p><p>57 (23) until convergence output: s map and restored image I  Analysis We note L is actually an inhomogeneous term, reflecting the anisotropic property of our smoothing regularizer. [sent-209, score-0.232]
</p><p>58 The resulting s map is therefore smooth in all directions. [sent-212, score-0.059]
</p><p>59 But in natural images, ∇G on an edge is not isotropic and should be with inmonaugnesi,fo ∇rmG regularization strength. [sent-213, score-0.067]
</p><p>60 By setting all initial si to 1s, total smoothness is obtained. [sent-220, score-0.222]
</p><p>61 Usually, 4-6 iterations are enough to generate visually compelling results. [sent-227, score-0.06]
</p><p>62 I(t+1)  Solve for I given by is  gradient  Similarly, the energy function to solve for  E˜(I) =(s(t+1) − PxCxI)TAtx+1,t(s(t+1)  − PxCxI)  + (s(t+1) − PyCyI)TAty+1,t(s(t+1) + λ(I − I0)TBt+1,t(I − I0),  − PyCyI) (22)  where Atx+1,t and Aty+1,t are calculated with available s(t+1)  I(t)  and . [sent-236, score-0.063]
</p><p>63 (21), the resulting si for pixel iis a weighted average of pi,x∇xIi ≈ ∇xIi/∇xGi and pi,y∇yIi ≈ ∇yIi/∇yGi, whose weights are de/te∇rmined by (A∇x)ii an≈d (Ay)ii. [sent-246, score-0.166]
</p><p>64 ∇Even if these weights are quite different due to noise or other aforementioned issues described in Section 1, our method can still get a reasonable solution. [sent-247, score-0.063]
</p><p>65 (23), si reduces the gradient in the x-direction and increases the other so that ∇Ii lies icnlo tshee t ox sd∇ireGctii. [sent-250, score-0.229]
</p><p>66 Then caflotesre e toach s∇ ∇itGeration, a less noisy I put into Eq. [sent-252, score-0.091]
</p><p>67 (21) helps avoid discontinuity in the s map along edges of G . [sent-256, score-0.092]
</p><p>68 Initially the map is noisy because of confusing  or  contradictive gradient magnitudes and directions in the 11554411  (a)ImageI0withAd itveNoise(b)NIRImageG(c)EstimatedI(d)GroundTruth (e)InitalsMap(f)MapsatIeration1(g)MapsatIeration2(h)FinalResult  Figure 5. [sent-260, score-0.246]
</p><p>69 Given image pairs in (a) and (b), our method can get the high-quality restoration result in (c). [sent-262, score-0.218]
</p><p>70 Handling shadow and highlight only existing in the guidance image G. [sent-265, score-0.372]
</p><p>71 Our final scale map adapts the gradients of G to match I0 with noise removed. [sent-269, score-0.15]
</p><p>72 Experiments  Suppose the two input images one is noisy and the other is clean are aligned. [sent-271, score-0.117]
</p><p>73 We explain our algorithm on noisy RGB and flashed NIR images due to its generality of structure discrepancy. [sent-273, score-0.227]
</p><p>74 Experiment Setting and Running Time Our method has two parameters β and λ, controlling smoothness of s and confidence of the noisy input. [sent-274, score-0.147]
</p><p>75 5, some gradients of guidance NIR image are reversed or weak compared to the noisy color image. [sent-281, score-0.447]
</p><p>76 Reversed gradients for the letter “D” are corrected with the negative values in the resulting scale map s. [sent-282, score-0.116]
</p><p>77 6, we show another example with highlight and  shadow only in the flashed NIR image. [sent-284, score-0.307]
</p><p>78 Our estimated s map shown in (c) contains large values along object boundaries, and has close-to-zero values for highlight and shadow. [sent-285, score-0.139]
</p><p>79 The restoration result shown in (d) is with much less highlight and shadow, which is impossible to achieve by gradient transfer or joint filtering. [sent-286, score-0.361]
</p><p>80 7 gives comparisons with BM3D [5] and the method of [21], which do not handle gradient variation. [sent-288, score-0.063]
</p><p>81 We also compare our result with the one presented in [11], which was generated by taking both UV and IR flashed image as guidance. [sent-290, score-0.136]
</p><p>82 Our method, by only taking the IR flashed image as G, accomplishes the comparable result shown in Fig. [sent-291, score-0.136]
</p><p>83 Flash and Non-Flash Images Our method is applicable to image restoration using flash/non-flash image pairs. [sent-293, score-0.218]
</p><p>84 Since the two input images are color ones under visible light, we use each channel from the flash image to guide image restoration in the corresponding channel of the nonflash noisy image. [sent-294, score-0.77]
</p><p>85 Without handling it, it is hard to preserve these sharp edges as gradients averaging  (a)Non-FlashNoiseInput(b)FlashImage (c)Resultof[14](d)OurResult  Figure 9. [sent-305, score-0.119]
</p><p>86 We apply it to cross-field dehazing with color and NIR images captured in haze. [sent-315, score-0.133]
</p><p>87 An image recovered from low visibility caused by haze could suffer from noise and compression artifacts due to significant gradient enhancement in low contrast regions. [sent-316, score-0.281]
</p><p>88 There is no guidance structure in the rectangle of (b), making restoration less-constrained. [sent-319, score-0.419]
</p><p>89 By applying our method to singleimage dehazing result that is noisy and the NIR input, we can improve the quality. [sent-323, score-0.18]
</p><p>90 The single-image dehazing result of [9] contains noise, and the result of [16], differently, changes the tone. [sent-326, score-0.089]
</p><p>91 Our restoration result with an NIR image as guidance G is more visually pleasing. [sent-327, score-0.419]
</p><p>92 More results from our system are available in the project website (see the title page), including those of depth image enhancement using Kinect. [sent-328, score-0.073]
</p><p>93 Unlike transferring details or applying joint filtering, we explicitly take the possible structural discrepancy between input images into consideration. [sent-331, score-0.118]
</p><p>94 It is encoded in a scale map s that can represent all challenging cases. [sent-332, score-0.059]
</p><p>95 Our objective functions and optimization make good use of the guidance from other domains and preserve necessary details and edges. [sent-333, score-0.201]
</p><p>96 The limitation of our current method is on the situation that the guidance does not exist, corresponding to zero ∇G tahnadt non-zero n∇ceI∗ d pixels. [sent-334, score-0.201]
</p><p>97 B neocna-uzseero oth ∇e guidance does not exist, image restoration naturally degrades to single-image denoising. [sent-337, score-0.419]
</p><p>98 Removing photography artifacts using gradient projection and flashexposure sampling. [sent-347, score-0.133]
</p><p>99 Flash cut: Foreground extraction with flash and no-flash image pairs. [sent-481, score-0.261]
</p><p>100 Enhancing low light images using near infrared flash images. [sent-509, score-0.392]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nir', 0.534), ('flash', 0.261), ('restoration', 0.218), ('pxcxi', 0.217), ('pycyi', 0.217), ('guidance', 0.201), ('gi', 0.181), ('si', 0.166), ('flashed', 0.136), ('kgi', 0.136), ('xii', 0.136), ('haze', 0.111), ('anisotropic', 0.097), ('yii', 0.096), ('infrared', 0.096), ('noisy', 0.091), ('shadow', 0.091), ('dehazing', 0.089), ('stls', 0.081), ('highlight', 0.08), ('ii', 0.078), ('smoothing', 0.076), ('enhancement', 0.073), ('ay', 0.073), ('photography', 0.07), ('gradient', 0.063), ('py', 0.063), ('map', 0.059), ('vy', 0.058), ('gradients', 0.057), ('rgb', 0.057), ('smoothness', 0.056), ('cyt', 0.054), ('gradientshop', 0.054), ('nirimageg', 0.054), ('pcg', 0.054), ('reversed', 0.054), ('tax', 0.054), ('ygi', 0.054), ('discrepancy', 0.054), ('channel', 0.05), ('cxt', 0.048), ('ggi', 0.048), ('outputting', 0.048), ('tensor', 0.048), ('cy', 0.046), ('bt', 0.045), ('tay', 0.045), ('xgi', 0.045), ('zhuo', 0.045), ('color', 0.044), ('cohen', 0.044), ('dark', 0.042), ('ax', 0.042), ('px', 0.042), ('isotropic', 0.042), ('bhat', 0.04), ('controls', 0.039), ('wtoh', 0.038), ('agrawala', 0.038), ('transferring', 0.038), ('vx', 0.037), ('thing', 0.036), ('raskar', 0.036), ('light', 0.035), ('decent', 0.035), ('irls', 0.035), ('tone', 0.035), ('restore', 0.034), ('lischinski', 0.034), ('noise', 0.034), ('edges', 0.033), ('directions', 0.033), ('sign', 0.033), ('ect', 0.031), ('cx', 0.031), ('solver', 0.031), ('krishnan', 0.031), ('deblurring', 0.03), ('bilateral', 0.03), ('iss', 0.03), ('compelling', 0.03), ('iterations', 0.03), ('acm', 0.03), ('iin', 0.03), ('guide', 0.03), ('scalar', 0.029), ('sharp', 0.029), ('issues', 0.029), ('diagonal', 0.029), ('uv', 0.029), ('enhancing', 0.027), ('numerical', 0.026), ('zeros', 0.026), ('input', 0.026), ('term', 0.026), ('regularization', 0.025), ('sun', 0.025), ('pages', 0.025), ('solvers', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="98-tfidf-1" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>Author: Qiong Yan, Xiaoyong Shen, Li Xu, Shaojie Zhuo, Xiaopeng Zhang, Liang Shen, Jiaya Jia</p><p>Abstract: Color, infrared, and flash images captured in different fields can be employed to effectively eliminate noise and other visual artifacts. We propose a two-image restoration framework considering input images in different fields, for example, one noisy color image and one dark-flashed nearinfrared image. The major issue in such a framework is to handle structure divergence and find commonly usable edges and smooth transition for visually compelling image reconstruction. We introduce a scale map as a competent representation to explicitly model derivative-level confidence and propose new functions and a numerical solver to effectively infer it following new structural observations. Our method is general and shows a principled way for cross-field restoration.</p><p>2 0.12902126 <a title="98-tfidf-2" href="./iccv-2013-Efficient_Image_Dehazing_with_Boundary_Constraint_and_Contextual_Regularization.html">135 iccv-2013-Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</a></p>
<p>Author: Gaofeng Meng, Ying Wang, Jiangyong Duan, Shiming Xiang, Chunhong Pan</p><p>Abstract: unkown-abstract</p><p>3 0.093127012 <a title="98-tfidf-3" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>Author: Wei Xia, Csaba Domokos, Jian Dong, Loong-Fah Cheong, Shuicheng Yan</p><p>Abstract: Numerous existing object segmentation frameworks commonly utilize the object bounding box as a prior. In this paper, we address semantic segmentation assuming that object bounding boxes are provided by object detectors, but no training data with annotated segments are available. Based on a set of segment hypotheses, we introduce a simple voting scheme to estimate shape guidance for each bounding box. The derived shape guidance is used in the subsequent graph-cut-based figure-ground segmentation. The final segmentation result is obtained by merging the segmentation results in the bounding boxes. We conduct an extensive analysis of the effect of object bounding box accuracy. Comprehensive experiments on both the challenging PASCAL VOC object segmentation dataset and GrabCut50 image segmentation dataset show that the proposed approach achieves competitive results compared to previous detection or bounding box prior based methods, as well as other state-of-the-art semantic segmentation methods.</p><p>4 0.083642676 <a title="98-tfidf-4" href="./iccv-2013-Abnormal_Event_Detection_at_150_FPS_in_MATLAB.html">34 iccv-2013-Abnormal Event Detection at 150 FPS in MATLAB</a></p>
<p>Author: Cewu Lu, Jianping Shi, Jiaya Jia</p><p>Abstract: Speedy abnormal event detection meets the growing demand to process an enormous number of surveillance videos. Based on inherent redundancy of video structures, we propose an efficient sparse combination learning framework. It achieves decent performance in the detection phase without compromising result quality. The short running time is guaranteed because the new method effectively turns the original complicated problem to one in which only a few costless small-scale least square optimization steps are involved. Our method reaches high detection rates on benchmark datasets at a speed of 140∼150 frames per soenc obnednc on average wsehtesn a computing on an ordinary desktop PC using MATLAB.</p><p>5 0.077766143 <a title="98-tfidf-5" href="./iccv-2013-Potts_Model%2C_Parametric_Maxflow_and_K-Submodular_Functions.html">324 iccv-2013-Potts Model, Parametric Maxflow and K-Submodular Functions</a></p>
<p>Author: Igor Gridchyn, Vladimir Kolmogorov</p><p>Abstract: The problem of minimizing the Potts energy function frequently occurs in computer vision applications. One way to tackle this NP-hard problem was proposed by Kovtun [20, 21]. It identifies a part of an optimal solution by running k maxflow computations, where k is the number of labels. The number of “labeled” pixels can be significant in some applications, e.g. 50-93% in our tests for stereo. We show how to reduce the runtime to O(log k) maxflow computations (or one parametric maxflow computation). Furthermore, the output of our algorithm allows to speed-up the subsequent alpha expansion for the unlabeled part, or can be used as it is for time-critical applications. To derive our technique, we generalize the algorithm of Felzenszwalb et al. [7] for Tree Metrics. We also show a connection to k-submodular functions from combinatorial optimization, and discuss k-submodular relaxations for general energy functions.</p><p>6 0.07557492 <a title="98-tfidf-6" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>7 0.070708774 <a title="98-tfidf-7" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>8 0.068401083 <a title="98-tfidf-8" href="./iccv-2013-Deblurring_by_Example_Using_Dense_Correspondence.html">103 iccv-2013-Deblurring by Example Using Dense Correspondence</a></p>
<p>9 0.062993482 <a title="98-tfidf-9" href="./iccv-2013-Image_Guided_Depth_Upsampling_Using_Anisotropic_Total_Generalized_Variation.html">209 iccv-2013-Image Guided Depth Upsampling Using Anisotropic Total Generalized Variation</a></p>
<p>10 0.061837543 <a title="98-tfidf-10" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>11 0.06096597 <a title="98-tfidf-11" href="./iccv-2013-Coupled_Dictionary_and_Feature_Space_Learning_with_Applications_to_Cross-Domain_Image_Synthesis_and_Recognition.html">96 iccv-2013-Coupled Dictionary and Feature Space Learning with Applications to Cross-Domain Image Synthesis and Recognition</a></p>
<p>12 0.059098504 <a title="98-tfidf-12" href="./iccv-2013-Exploiting_Reflection_Change_for_Automatic_Reflection_Removal.html">151 iccv-2013-Exploiting Reflection Change for Automatic Reflection Removal</a></p>
<p>13 0.058427833 <a title="98-tfidf-13" href="./iccv-2013-Robust_Non-parametric_Data_Fitting_for_Correspondence_Modeling.html">358 iccv-2013-Robust Non-parametric Data Fitting for Correspondence Modeling</a></p>
<p>14 0.055558957 <a title="98-tfidf-14" href="./iccv-2013-New_Graph_Structured_Sparsity_Model_for_Multi-label_Image_Annotations.html">290 iccv-2013-New Graph Structured Sparsity Model for Multi-label Image Annotations</a></p>
<p>15 0.054168697 <a title="98-tfidf-15" href="./iccv-2013-Curvature-Aware_Regularization_on_Riemannian_Submanifolds.html">100 iccv-2013-Curvature-Aware Regularization on Riemannian Submanifolds</a></p>
<p>16 0.05194664 <a title="98-tfidf-16" href="./iccv-2013-Single-Patch_Low-Rank_Prior_for_Non-pointwise_Impulse_Noise_Removal.html">394 iccv-2013-Single-Patch Low-Rank Prior for Non-pointwise Impulse Noise Removal</a></p>
<p>17 0.051878572 <a title="98-tfidf-17" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>18 0.051338054 <a title="98-tfidf-18" href="./iccv-2013-Joint_Noise_Level_Estimation_from_Personal_Photo_Collections.html">223 iccv-2013-Joint Noise Level Estimation from Personal Photo Collections</a></p>
<p>19 0.050847296 <a title="98-tfidf-19" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>20 0.05047844 <a title="98-tfidf-20" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.128), (1, -0.043), (2, -0.025), (3, -0.014), (4, -0.039), (5, 0.01), (6, -0.022), (7, -0.019), (8, 0.016), (9, -0.077), (10, -0.028), (11, -0.049), (12, 0.033), (13, -0.034), (14, -0.016), (15, 0.027), (16, -0.019), (17, -0.007), (18, -0.008), (19, 0.022), (20, -0.005), (21, 0.02), (22, -0.031), (23, -0.041), (24, -0.006), (25, 0.055), (26, 0.049), (27, -0.013), (28, 0.004), (29, -0.039), (30, 0.003), (31, 0.053), (32, 0.027), (33, 0.074), (34, 0.027), (35, 0.029), (36, 0.013), (37, 0.009), (38, 0.014), (39, 0.074), (40, -0.002), (41, 0.007), (42, -0.016), (43, 0.0), (44, 0.019), (45, 0.062), (46, 0.067), (47, 0.054), (48, -0.021), (49, -0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91836625 <a title="98-lsi-1" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>Author: Qiong Yan, Xiaoyong Shen, Li Xu, Shaojie Zhuo, Xiaopeng Zhang, Liang Shen, Jiaya Jia</p><p>Abstract: Color, infrared, and flash images captured in different fields can be employed to effectively eliminate noise and other visual artifacts. We propose a two-image restoration framework considering input images in different fields, for example, one noisy color image and one dark-flashed nearinfrared image. The major issue in such a framework is to handle structure divergence and find commonly usable edges and smooth transition for visually compelling image reconstruction. We introduce a scale map as a competent representation to explicitly model derivative-level confidence and propose new functions and a numerical solver to effectively infer it following new structural observations. Our method is general and shows a principled way for cross-field restoration.</p><p>2 0.7325834 <a title="98-lsi-2" href="./iccv-2013-Efficient_Image_Dehazing_with_Boundary_Constraint_and_Contextual_Regularization.html">135 iccv-2013-Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</a></p>
<p>Author: Gaofeng Meng, Ying Wang, Jiangyong Duan, Shiming Xiang, Chunhong Pan</p><p>Abstract: unkown-abstract</p><p>3 0.70777273 <a title="98-lsi-3" href="./iccv-2013-SGTD%3A_Structure_Gradient_and_Texture_Decorrelating_Regularization_for_Image_Decomposition.html">364 iccv-2013-SGTD: Structure Gradient and Texture Decorrelating Regularization for Image Decomposition</a></p>
<p>Author: Qiegen Liu, Jianbo Liu, Pei Dong, Dong Liang</p><p>Abstract: This paper presents a novel structure gradient and texture decorrelating regularization (SGTD) for image decomposition. The motivation of the idea is under the assumption that the structure gradient and texture components should be properly decorrelated for a successful decomposition. The proposed model consists of the data fidelity term, total variation regularization and the SGTD regularization. An augmented Lagrangian method is proposed to address this optimization issue, by first transforming the unconstrained problem to an equivalent constrained problem and then applying an alternating direction method to iteratively solve the subproblems. Experimental results demonstrate that the proposed method presents better or comparable performance as state-of-the-art methods do.</p><p>4 0.66189331 <a title="98-lsi-4" href="./iccv-2013-A_Generalized_Low-Rank_Appearance_Model_for_Spatio-temporally_Correlated_Rain_Streaks.html">15 iccv-2013-A Generalized Low-Rank Appearance Model for Spatio-temporally Correlated Rain Streaks</a></p>
<p>Author: Yi-Lei Chen, Chiou-Ting Hsu</p><p>Abstract: In this paper, we propose a novel low-rank appearance model for removing rain streaks. Different from previous work, our method needs neither rain pixel detection nor time-consuming dictionary learning stage. Instead, as rain streaks usually reveal similar and repeated patterns on imaging scene, we propose and generalize a low-rank model from matrix to tensor structure in order to capture the spatio-temporally correlated rain streaks. With the appearance model, we thus remove rain streaks from image/video (and also other high-order image structure) in a unified way. Our experimental results demonstrate competitive (or even better) visual quality and efficient run-time in comparison with state of the art.</p><p>5 0.63232905 <a title="98-lsi-5" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>Author: Carlos Fernandez-Granda, Emmanuel J. Candès</p><p>Abstract: We present a framework to super-resolve planar regions found in urban scenes and other man-made environments by taking into account their 3D geometry. Such regions have highly structured straight edges, but this prior is challenging to exploit due to deformations induced by the projection onto the imaging plane. Our method factors out such deformations by using recently developed tools based on convex optimization to learn a transform that maps the image to a domain where its gradient has a simple group-sparse structure. This allows to obtain a novel convex regularizer that enforces global consistency constraints between the edges of the image. Computational experiments with real images show that this data-driven approach to the design of regularizers promoting transform-invariant group sparsity is very effective at high super-resolution factors. We view our approach as complementary to most recent superresolution methods, which tend to focus on hallucinating high-frequency textures.</p><p>6 0.62262768 <a title="98-lsi-6" href="./iccv-2013-Single-Patch_Low-Rank_Prior_for_Non-pointwise_Impulse_Noise_Removal.html">394 iccv-2013-Single-Patch Low-Rank Prior for Non-pointwise Impulse Noise Removal</a></p>
<p>7 0.61407208 <a title="98-lsi-7" href="./iccv-2013-Perceptual_Fidelity_Aware_Mean_Squared_Error.html">312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</a></p>
<p>8 0.60344678 <a title="98-lsi-8" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>9 0.57806283 <a title="98-lsi-9" href="./iccv-2013-A_New_Image_Quality_Metric_for_Image_Auto-denoising.html">23 iccv-2013-A New Image Quality Metric for Image Auto-denoising</a></p>
<p>10 0.55755615 <a title="98-lsi-10" href="./iccv-2013-Target-Driven_Moire_Pattern_Synthesis_by_Phase_Modulation.html">413 iccv-2013-Target-Driven Moire Pattern Synthesis by Phase Modulation</a></p>
<p>11 0.55556577 <a title="98-lsi-11" href="./iccv-2013-Total_Variation_Regularization_for_Functions_with_Values_in_a_Manifold.html">421 iccv-2013-Total Variation Regularization for Functions with Values in a Manifold</a></p>
<p>12 0.55519098 <a title="98-lsi-12" href="./iccv-2013-Bayesian_Robust_Matrix_Factorization_for_Image_and_Video_Processing.html">60 iccv-2013-Bayesian Robust Matrix Factorization for Image and Video Processing</a></p>
<p>13 0.54924536 <a title="98-lsi-13" href="./iccv-2013-Illuminant_Chromaticity_from_Image_Sequences.html">207 iccv-2013-Illuminant Chromaticity from Image Sequences</a></p>
<p>14 0.54455972 <a title="98-lsi-14" href="./iccv-2013-Non-convex_P-Norm_Projection_for_Robust_Sparsity.html">292 iccv-2013-Non-convex P-Norm Projection for Robust Sparsity</a></p>
<p>15 0.54312307 <a title="98-lsi-15" href="./iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding.html">14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</a></p>
<p>16 0.53746688 <a title="98-lsi-16" href="./iccv-2013-A_Simple_Model_for_Intrinsic_Image_Decomposition_with_Depth_Cues.html">30 iccv-2013-A Simple Model for Intrinsic Image Decomposition with Depth Cues</a></p>
<p>17 0.53698045 <a title="98-lsi-17" href="./iccv-2013-Joint_Noise_Level_Estimation_from_Personal_Photo_Collections.html">223 iccv-2013-Joint Noise Level Estimation from Personal Photo Collections</a></p>
<p>18 0.53428686 <a title="98-lsi-18" href="./iccv-2013-Shape_Index_Descriptors_Applied_to_Texture-Based_Galaxy_Analysis.html">388 iccv-2013-Shape Index Descriptors Applied to Texture-Based Galaxy Analysis</a></p>
<p>19 0.52846211 <a title="98-lsi-19" href="./iccv-2013-Progressive_Multigrid_Eigensolvers_for_Multiscale_Spectral_Segmentation.html">329 iccv-2013-Progressive Multigrid Eigensolvers for Multiscale Spectral Segmentation</a></p>
<p>20 0.52663982 <a title="98-lsi-20" href="./iccv-2013-Potts_Model%2C_Parametric_Maxflow_and_K-Submodular_Functions.html">324 iccv-2013-Potts Model, Parametric Maxflow and K-Submodular Functions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.042), (7, 0.01), (25, 0.015), (26, 0.047), (31, 0.033), (40, 0.016), (42, 0.087), (64, 0.03), (73, 0.47), (89, 0.147)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82901585 <a title="98-lda-1" href="./iccv-2013-Single-Patch_Low-Rank_Prior_for_Non-pointwise_Impulse_Noise_Removal.html">394 iccv-2013-Single-Patch Low-Rank Prior for Non-pointwise Impulse Noise Removal</a></p>
<p>Author: Ruixuan Wang, Emanuele Trucco</p><p>Abstract: This paper introduces a ‘low-rank prior’ for small oriented noise-free image patches: considering an oriented patch as a matrix, a low-rank matrix approximation is enough to preserve the texture details in the properly oriented patch. Based on this prior, we propose a single-patch method within a generalized joint low-rank and sparse matrix recovery framework to simultaneously detect and remove non-pointwise random-valued impulse noise (e.g., very small blobs). A weighting matrix is incorporated in the framework to encode an initial estimate of the spatial noise distribution. An accelerated proximal gradient method is adapted to estimate the optimal noise-free image patches. Experiments show the effectiveness of our framework in removing non-pointwise random-valued impulse noise.</p><p>same-paper 2 0.81995732 <a title="98-lda-2" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>Author: Qiong Yan, Xiaoyong Shen, Li Xu, Shaojie Zhuo, Xiaopeng Zhang, Liang Shen, Jiaya Jia</p><p>Abstract: Color, infrared, and flash images captured in different fields can be employed to effectively eliminate noise and other visual artifacts. We propose a two-image restoration framework considering input images in different fields, for example, one noisy color image and one dark-flashed nearinfrared image. The major issue in such a framework is to handle structure divergence and find commonly usable edges and smooth transition for visually compelling image reconstruction. We introduce a scale map as a competent representation to explicitly model derivative-level confidence and propose new functions and a numerical solver to effectively infer it following new structural observations. Our method is general and shows a principled way for cross-field restoration.</p><p>3 0.75013554 <a title="98-lda-3" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>Author: Ernesto Brau, Jinyan Guan, Kyle Simek, Luca Del Pero, Colin Reimer Dawson, Kobus Barnard</p><p>Abstract: Jinyan Guan† j guan1 @ emai l ari z ona . edu . Kyle Simek† ks imek@ emai l ari z ona . edu . Colin Reimer Dawson‡ cdaws on@ emai l ari z ona . edu . ‡School of Information University of Arizona Kobus Barnard‡ kobus @ s i sta . ari z ona . edu ∗School of Informatics University of Edinburgh for tracking an unknown and changing number of people in a scene using video taken from a single, fixed viewpoint. We develop a Bayesian modeling approach for tracking people in 3D from monocular video with unknown cameras. Modeling in 3D provides natural explanations for occlusions and smoothness discontinuities that result from projection, and allows priors on velocity and smoothness to be grounded in physical quantities: meters and seconds vs. pixels and frames. We pose the problem in the context of data association, in which observations are assigned to tracks. A correct application of Bayesian inference to multitarget tracking must address the fact that the model’s dimension changes as tracks are added or removed, and thus, posterior densities of different hypotheses are not comparable. We address this by marginalizing out the trajectory parameters so the resulting posterior over data associations has constant dimension. This is made tractable by using (a) Gaussian process priors for smooth trajectories and (b) approximately Gaussian likelihood functions. Our approach provides a principled method for incorporating multiple sources of evidence; we present results using both optical flow and object detector outputs. Results are comparable to recent work on 3D tracking and, unlike others, our method requires no pre-calibrated cameras.</p><p>4 0.72772294 <a title="98-lda-4" href="./iccv-2013-Multiple_Non-rigid_Surface_Detection_and_Registration.html">283 iccv-2013-Multiple Non-rigid Surface Detection and Registration</a></p>
<p>Author: Yi Wu, Yoshihisa Ijiri, Ming-Hsuan Yang</p><p>Abstract: Detecting and registering nonrigid surfaces are two important research problems for computer vision. Much work has been done with the assumption that there exists only one instance in the image. In this work, we propose an algorithm that detects and registers multiple nonrigid instances of given objects in a cluttered image. Specifically, after we use low level feature points to obtain the initial matches between templates and the input image, a novel high-order affinity graph is constructed to model the consistency of local topology. A hierarchical clustering approach is then used to locate the nonrigid surfaces. To remove the outliers in the cluster, we propose a deterministic annealing approach based on the Thin Plate Spline (TPS) model. The proposed method achieves high accuracy even when the number of outliers is nineteen times larger than the inliers. As the matches may appear sparsely in each instance, we propose a TPS based match growing approach to propagate the matches. Finally, an approach that fuses feature and appearance information is proposed to register each nonrigid surface. Extensive experiments and evaluations demonstrate that the proposed algorithm achieves promis- ing results in detecting and registering multiple non-rigid surfaces in a cluttered scene.</p><p>5 0.70628989 <a title="98-lda-5" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>Author: Jim Braux-Zin, Romain Dupont, Adrien Bartoli</p><p>Abstract: Dense motion field estimation (typically Romain Dupont1 romain . dupont @ cea . fr Adrien Bartoli2 adrien . bart o l @ gmai l com i . 2 ISIT, Universit e´ d’Auvergne/CNRS, France sions are explicitly modeled [32, 13]. Coarse-to-fine warping improves global convergence by making the assumption that optical flow, the motion of smaller structures is similar to the motion of stereo disparity and surface registration) is a key computer vision problem. Many solutions have been proposed to compute small or large displacements, narrow or wide baseline stereo disparity, but a unified methodology is still lacking. We here introduce a general framework that robustly combines direct and feature-based matching. The feature-based cost is built around a novel robust distance function that handles keypoints and “weak” features such as segments. It allows us to use putative feature matches which may contain mismatches to guide dense motion estimation out of local minima. Our framework uses a robust direct data term (AD-Census). It is implemented with a powerful second order Total Generalized Variation regularization with external and self-occlusion reasoning. Our framework achieves state of the art performance in several cases (standard optical flow benchmarks, wide-baseline stereo and non-rigid surface registration). Our framework has a modular design that customizes to specific application needs.</p><p>6 0.59614998 <a title="98-lda-6" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>7 0.58307242 <a title="98-lda-7" href="./iccv-2013-Joint_Noise_Level_Estimation_from_Personal_Photo_Collections.html">223 iccv-2013-Joint Noise Level Estimation from Personal Photo Collections</a></p>
<p>8 0.5704385 <a title="98-lda-8" href="./iccv-2013-A_New_Image_Quality_Metric_for_Image_Auto-denoising.html">23 iccv-2013-A New Image Quality Metric for Image Auto-denoising</a></p>
<p>9 0.55887222 <a title="98-lda-9" href="./iccv-2013-Bayesian_Robust_Matrix_Factorization_for_Image_and_Video_Processing.html">60 iccv-2013-Bayesian Robust Matrix Factorization for Image and Video Processing</a></p>
<p>10 0.55076373 <a title="98-lda-10" href="./iccv-2013-Robust_Non-parametric_Data_Fitting_for_Correspondence_Modeling.html">358 iccv-2013-Robust Non-parametric Data Fitting for Correspondence Modeling</a></p>
<p>11 0.52324742 <a title="98-lda-11" href="./iccv-2013-A_Robust_Analytical_Solution_to_Isometric_Shape-from-Template_with_Focal_Length_Calibration.html">27 iccv-2013-A Robust Analytical Solution to Isometric Shape-from-Template with Focal Length Calibration</a></p>
<p>12 0.50997663 <a title="98-lda-12" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<p>13 0.50733829 <a title="98-lda-13" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>14 0.5061217 <a title="98-lda-14" href="./iccv-2013-Exploiting_Reflection_Change_for_Automatic_Reflection_Removal.html">151 iccv-2013-Exploiting Reflection Change for Automatic Reflection Removal</a></p>
<p>15 0.50096339 <a title="98-lda-15" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>16 0.49567479 <a title="98-lda-16" href="./iccv-2013-Direct_Optimization_of_Frame-to-Frame_Rotation.html">115 iccv-2013-Direct Optimization of Frame-to-Frame Rotation</a></p>
<p>17 0.4935869 <a title="98-lda-17" href="./iccv-2013-Modeling_Self-Occlusions_in_Dynamic_Shape_and_Appearance_Tracking.html">270 iccv-2013-Modeling Self-Occlusions in Dynamic Shape and Appearance Tracking</a></p>
<p>18 0.49199289 <a title="98-lda-18" href="./iccv-2013-PM-Huber%3A_PatchMatch_with_Huber_Regularization_for_Stereo_Matching.html">304 iccv-2013-PM-Huber: PatchMatch with Huber Regularization for Stereo Matching</a></p>
<p>19 0.49166843 <a title="98-lda-19" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>20 0.49060494 <a title="98-lda-20" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
