<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>160 iccv-2013-Fast Object Segmentation in Unconstrained Video</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-160" href="#">iccv2013-160</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>160 iccv-2013-Fast Object Segmentation in Unconstrained Video</h1>
<br/><p>Source: <a title="iccv-2013-160-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Papazoglou_Fast_Object_Segmentation_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Anestis Papazoglou, Vittorio Ferrari</p><p>Abstract: We present a technique for separating foreground objects from the background in a video. Our method isfast, , fully automatic, and makes minimal assumptions about the video. This enables handling essentially unconstrained settings, including rapidly moving background, arbitrary object motion and appearance, and non-rigid deformations and articulations. In experiments on two datasets containing over 1400 video shots, our method outperforms a state-of-theart background subtraction technique [4] as well as methods based on clustering point tracks [6, 18, 19]. Moreover, it performs comparably to recent video object segmentation methods based on objectproposals [14, 16, 27], while being orders of magnitude faster.</p><p>Reference: <a title="iccv-2013-160-reference" href="../iccv2013_reference/iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Fast object segmentation in unconstrained video  Anestis Papazoglou University of Edinburgh Abstract We present a technique for separating foreground objects from the background in a video. [sent-1, score-0.637]
</p><p>2 This enables handling essentially unconstrained settings, including rapidly moving background, arbitrary object motion and appearance, and non-rigid deformations and articulations. [sent-3, score-0.477]
</p><p>3 In experiments on two datasets containing over 1400 video shots, our method outperforms a state-of-theart background subtraction technique [4] as well as methods based on clustering point tracks [6, 18, 19]. [sent-4, score-0.397]
</p><p>4 Moreover, it performs comparably to recent video object segmentation methods based on objectproposals [14, 16, 27], while being orders of magnitude faster. [sent-5, score-0.431]
</p><p>5 Introduction Video object segmentation is the task of separating foreground objects from the background in a video [14, 18, 26]. [sent-7, score-0.595]
</p><p>6 Moreover, the general unconstrained setting might include rapidly moving backgrounds and objects, non-rigid deformations and articulations (fig. [sent-12, score-0.296]
</p><p>7 In this paper we propose a technique for fully automatic video object segmentation in unconstrained settings. [sent-14, score-0.382]
</p><p>8 Our method is computationally efficient and makes minimal assumptions about the video: the only requirement is for the object to move differently from its surrounding background in a good fraction of the video. [sent-15, score-0.317]
</p><p>9 The object can be static in a portion of the video and only part of it can be moving in some other portion (e. [sent-16, score-0.326]
</p><p>10 Our method does not require a static or slowly moving background (as opposed to classic backVittorio Ferrari University of Edinburgh ground subtraction methods [9, 4, 7]). [sent-19, score-0.361]
</p><p>11 Moreover, it does not assume the object follows a particular motion model, nor that all its points move homogeneously (as opposed to  methods based on clustering point tracks [6, 17, 18]). [sent-20, score-0.413]
</p><p>12 The key new element in our approach is a rapid technique to produce a rough estimate of which pixels are inside the object based on motion boundaries in pairs of subsequent frames (sec. [sent-23, score-0.699]
</p><p>13 This second stage automatically bootstraps an appearance model based on the initial foreground estimate, and uses it to refine the spatial accuracy of the segmentation and to also segment the object in frames where it does not move (sec. [sent-27, score-0.866]
</p><p>14 Several methods for video object segmentation require the user to manually annotate a few frames with object segmentations and then propagate these annotations to all other frames [3, 20, 26]. [sent-38, score-0.676]
</p><p>15 Classic background subtraction methods model the appearance of the background at each pixel and consider pixels that change rapidly to be 11777777  foreground. [sent-41, score-0.596]
</p><p>16 The background should change slowly in order for the model to update safely without generating false-positive foreground detections. [sent-43, score-0.302]
</p><p>17 Several automatic video segmentation methods track points over several frames and then cluster the resulting tracks based on pairwise [6, 17] or triplet [18] similarity measures. [sent-45, score-0.427]
</p><p>18 The object only needs to move sufficiently differently from the background to generate motion boundaries along most of its physical boundary. [sent-49, score-0.651]
</p><p>19 [13] oversegment a video into spatio-temporal regions of uniform motion and appearance, analog to still-image superpixels [15]. [sent-62, score-0.455]
</p><p>20 While this is a useful basis for later processing, it does not solve the video object segmentation task on its own. [sent-63, score-0.34]
</p><p>21 Our approach The goal of our work is to segment objects that move differently than their surroundings. [sent-65, score-0.258]
</p><p>22 The goal of the first stage is to rapidly produce an initial estimate of which pixels might be inside the object based purely on motion. [sent-73, score-0.404]
</p><p>23 We compute the optical flow between pairs of subsequent frames and detect motion boundaries. [sent-74, score-0.568]
</p><p>24 Ideally, the motion boundaries will form a complete closed curve coinciding with the object boundaries. [sent-75, score-0.424]
</p><p>25 However, due to inaccuracies in the flow estimation, the motion boundaries  flow f? [sent-76, score-0.655]
</p><p>26 (c) Motion boundaries bpm, based on the magnitude of the gradient of the optical flow. [sent-79, score-0.404]
</p><p>27 (d) Motion boundaries bpθ, based on difference in direction between a pixel and its neighbours. [sent-80, score-0.314]
</p><p>28 (f) Final, binary motion boundaries after thresholding, overlaid on the first frame. [sent-82, score-0.334]
</p><p>29 are typically incomplete and do not align perfectly with object boundaries (fig. [sent-83, score-0.285]
</p><p>30 As they are purely based on motion boundaries, the insideoutside maps produced by the first stage typically only approximately indicate where the object is. [sent-89, score-0.517]
</p><p>31 Furthermore, (parts of) the object might be static in some frames, or the inside-outside maps may miss it due to incorrect optical flow estimation. [sent-91, score-0.598]
</p><p>32 The goal of the second stage is to refine the spatial accuracy of the inside-outside maps and to segment the whole object in all frames. [sent-92, score-0.309]
</p><p>33 Moreover, after learning the object appearance in the frames where the 11777788  of time. [sent-95, score-0.315]
</p><p>34 In this case,  (x, y) = S(x  − 1,  y) =  The number of intersections for the ray 1, and for the right ray as Xright  both rays vote for x being inside( xth,ey object. [sent-100, score-0.388]
</p><p>35 (x, y) =  inside-outside maps found it, the second stage uses it to segment the object in frames where it was initially missed (e. [sent-102, score-0.48]
</p><p>36 We begin by computing optical flow between pairs of subsequent frames (t, t + 1) using the stateof-the-art algorithm [6, 22]. [sent-108, score-0.429]
</p><p>37 image points where the optical flow field changes abruptly. [sent-114, score-0.306]
</p><p>38 Motion boundaries reveal the location of occlusion boundaries, which very often correspond to physical object boundaries [23]. [sent-115, score-0.527]
</p><p>39 The simplest way to estimate motion boundaries is by computing the magnitude of the gradient of the optical flow field:  f? [sent-117, score-0.684]
</p><p>40 p||)  (1)  where bpm ∈ [0, 1] is the strength of the motion boundary at pixel p; λm∈ i [s0 a parameter controlling t mheo steepness oarfy yth aet  function. [sent-119, score-0.443]
</p><p>41 While this measure correctly detects boundaries at rapidly moving pixels, where bpm is close to 1, it is unreliable for pixels with intermediate bpm values around 0. [sent-120, score-0.766]
</p><p>42 5, which could be explained either as boundaries or errors due to inaccuracies in the optical flow (fig. [sent-121, score-0.54]
</p><p>43 where δθp,q denotes the angle between and The idea is that if n is moving in a different direction than all its neighbours, it is likely to be a motion boundary. [sent-126, score-0.256]
</p><p>44 This estimator can correctly detect boundaries even when the object is moving at a modest velocity, as long as it goes in a different direction than the background. [sent-127, score-0.402]
</p><p>45 However, it tends to produce false-positives in static image regions, as the direction of the optical flow is noisy at points with little or no motion (fig. [sent-128, score-0.539]
</p><p>46 The produced motion boundaries typically do not completely cover the whole object boundary. [sent-137, score-0.465]
</p><p>47 Moreover, there might be false positive boundaries, due to inaccurracy of the optical flow estimation. [sent-138, score-0.354]
</p><p>48 The algorithm estimates whether a pixel is inside the object based on the point-in-polygon problem [12] from computational geometry. [sent-140, score-0.258]
</p><p>49 The key observation is that any ray starting from a point inside the polygon (or any closed curve) will intersect the boundary of the polygon an odd number of times. [sent-141, score-0.399]
</p><p>50 Since the motion boundaries are typically incomplete, a single ray is not sufficient to determine whether a pixel lies inside the object. [sent-143, score-0.596]
</p><p>51 Each ray casts a vote on whether the pixel is inside or outside. [sent-145, score-0.303]
</p><p>52 a pixel with 5 or more rays intersecting the boundaries an odd number of times is deemed inside. [sent-148, score-0.418]
</p><p>53 Aen c entry aS m(xat, ryix) oSf o otfh iths em saatmrixe isnizdeic Wate ×s t Hhe number of boundary intersections along the line going from the image border up to pixel (x, y). [sent-160, score-0.299]
</p><p>54 We then move rightwards one pixel at a time and increment S(x, y) by 1each time we transition from a non-boundary pixel to a boundary pixel. [sent-165, score-0.289]
</p><p>55 We can now determine the number of intersections X for both horizontal rays (left→right, right→left) emanating f froorm b a pixel (zox,n yta)l i rna ycson (lsetaft→nt rtiigmhet by Xleft (x, y) = S(x 1, y) Xright(x, y) = S(W, y) − S(x, y) −  (4) (5)  where W is the width of the image, i. [sent-170, score-0.276]
</p><p>56 Foreground-background labelling refinement We formulate video segmentation as a pixel labelling problem with two labels (foreground and background). [sent-182, score-0.541]
</p><p>57 We  oversegment each frame into superpixels St [15], which greatly rmedenutce esa computational efficiency sa nSd memory usage, enabling to segment much longer videos. [sent-183, score-0.406]
</p><p>58 Each superpixel sit ∈ St can take a label lit ∈ {0, 1}. [sent-184, score-0.483]
</p><p>59 A labelling L = {lti}t,i o∈f a Sll superpixels in all fra∈m {e0s, represlaebnetsll a segmentation of the video. [sent-185, score-0.404]
</p><p>60 ,t)∈Et  At is a unary potential evaluating how likely a superpixel is to be foreground or background according to the appearance model of frame t. [sent-195, score-0.597]
</p><p>61 The second unary potential Lt is based on a location prior model encouraging foreground labellings in areas where independent motion has been observed. [sent-196, score-0.4]
</p><p>62 T Ewo superpixels sit, sjt+1 in subsequent frames are connected if there at least one pixel of sit moves into sjt+1 according to the optical flow (fig. [sent-207, score-0.894]
</p><p>63 The factor that differs from the standard definition is φ, which is the percentage of pixels within the two superpixels that are connected by the optical flow. [sent-210, score-0.425]
</p><p>64 The appearance model consists of two Gaussian Mixture Models over RGB colour values1 ,  one for the foreground (fg) and one for the background (bg). [sent-213, score-0.357]
</p><p>65 At each frame t we estimate a fg model from all superpixels in the video, weighted by how likely they are to be foreground and by how close in time they are to t. [sent-221, score-0.449]
</p><p>66 After estimating the foreground-background appearance models, the unary potential Ait (lti) is the log-probability of sit to take label lit under the appropriate model (i. [sent-229, score-0.527]
</p><p>67 the foreground model if lit = 1and the background one otherwise). [sent-231, score-0.398]
</p><p>68 ergy (6) enables to segment the object more accurately than possible from motion alone, as motion estimation is inherently inaccurate near occlusion boundaries. [sent-238, score-0.491]
</p><p>69 Moreover, the appearance models are integrated over large image regions and over many frames, and therefore can robustly estimate the appearance of the object, despite faults in the insideoutside maps. [sent-239, score-0.355]
</p><p>70 The appearance models then transfer this knowledge to other positions within a frame and to other frames, by altering towards foreground the unary potential of pixels with object-like appearance, even if the insideoutside maps missed them. [sent-240, score-0.713]
</p><p>71 This enables completing the segmentation in frames where only part of the object is moving, and helps segmenting it even in frames where it does move at all. [sent-241, score-0.564]
</p><p>72 When based only on appearance, the segmentation could be distracted by background regions with similar colour to the foreground (even with perfect appearance models). [sent-243, score-0.494]
</p><p>73 Fortunately, the inside-outside maps can provide a valuable location prior to anchor the segmentation to image areas likely to contain the object, as they move 1As the basic units erage RGB value. [sent-244, score-0.334]
</p><p>74 However, in some frames (part of) the object may be static, and in others the inside-outside map might miss it because of incorrect optical flow estimation (fig. [sent-246, score-0.612]
</p><p>75 Therefore, directly plugging the inside-outside maps as unary potentials in Lt  would further encourage an all-background segmentation in frames where they missed the object. [sent-248, score-0.477]
</p><p>76 We propose here to propagate the per-frame insideoutside maps over time to build a more complete location prior Lt. [sent-249, score-0.257]
</p><p>77 The key observation is that ‘inside’ classifications are more reliable than ‘outside’ ones: the true object boundaries might not form a near-closed motion boundary due to the reasons above, but accidental near-closed boundaries rarely form out of noise. [sent-250, score-0.715]
</p><p>78 Therefore, our algorithm accumulates inside points over the entire video sequence, following the optical flow (fig. [sent-251, score-0.512]
</p><p>79 The value of the location prior at a superpixel sit is initially Lit := rit, i. [sent-254, score-0.387]
</p><p>80 the percentage of its pixels that are inside the object according to the inside-outside map Mt. [sent-256, score-0.284]
</p><p>81 We start propagating from frame 1 to frame 2, then move to frame 3 and so on. [sent-257, score-0.331]
</p><p>82 all superpixels in frame t; the connection weight φ is the percentage of pixels in superpixel sit that connect to superpixel sjt+1 by following the optical flow (fig. [sent-261, score-1.095]
</p><p>83 tr 3a)n;sf γer ∈ quality measure, down-weighting propagation if the optical flow for sit is deemed unreliable  ψw  ψ(sit) = exp(−λψ ? [sent-263, score-0.581]
</p><p>84 ∈sit  (12)  In essence, ψ measures the sum of the flow gradients in sit; large gradients can indicate depth discontinuities, where the optical flow is often inaccurate, or that sit might cover bits of two different objects. [sent-266, score-0.726]
</p><p>85 It contains 6 videos (monkeydog, girl, birdfall, parachute, cheetah, penguin) and pixel-level ground-truth for the foreground object in every frame. [sent-284, score-0.338]
</p><p>86 The video object segmentation method of Lee at al. [sent-294, score-0.34]
</p><p>87 In contrast, our method directly returns a single foreground segment, as it discovers the foreground object automatically. [sent-298, score-0.416]
</p><p>88 We also compare to a state-of-the-art background subtraction method [4] and with two state-of-the-art clustering point tracks based methods [6, 18]. [sent-300, score-0.284]
</p><p>89 Our method considerably outperforms [6, 4, 18] in all videos, as it handles non-rigid objects better, and tightly integrates appearance along with motion as segmentation cues. [sent-317, score-0.417]
</p><p>90 Note the high quality of  the segmentation on monkeydog and cheetah, which feature fast camera motion and strong non-rigid deformations. [sent-331, score-0.356]
</p><p>91 All methods lock on the object in all videos and accuracy differences between methods are due to finer localization of the object boundaries. [sent-333, score-0.265]
</p><p>92 The dataset also provides ground-truth bounding-boxes on the object of interest in one frame for each of 1407 video shots. [sent-365, score-0.283]
</p><p>93 [19] automatically select one segment per shot among those produced by [6], based on its appearance similarity to segments selected in other videos of the same object class, and on how likely it is to cover an object according to a class-generic objectness measure [2]. [sent-378, score-0.595]
</p><p>94 As it returns a single foreground segment per shot, this method is directly comparable to ours. [sent-379, score-0.286]
</p><p>95 edu / yl 3 6 6 3 / ˜ylee / shows our method correctly segment objects even if largely clipped by the image border in some frames, as it automatically transfers object appearance learned in other frames. [sent-397, score-0.446]
</p><p>96 Runtime Given optical flow and superpixels, our method takes 0. [sent-400, score-0.306]
</p><p>97 0GHz), and exclude optical flow computation, which all methods require as input. [sent-407, score-0.306]
</p><p>98 High quality optical flow can be computed rapidly using [22] (< 1 sec/frame). [sent-408, score-0.383]
</p><p>99 Vibe: A universal background subtraction algorithm for video sequences. [sent-438, score-0.304]
</p><p>100 Video object segmentation through spatially accurate and temporally dense extraction of primary object regions. [sent-595, score-0.317]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sjt', 0.241), ('sit', 0.231), ('boundaries', 0.195), ('segtrack', 0.192), ('bpm', 0.181), ('optical', 0.165), ('foreground', 0.163), ('superpixels', 0.159), ('insideoutside', 0.151), ('lit', 0.143), ('flow', 0.141), ('motion', 0.139), ('segmentation', 0.137), ('lti', 0.123), ('frames', 0.123), ('segment', 0.123), ('video', 0.113), ('superpixel', 0.109), ('labelling', 0.108), ('ljt', 0.107), ('appearance', 0.102), ('cheetah', 0.099), ('intersections', 0.099), ('subtraction', 0.099), ('ray', 0.094), ('inside', 0.093), ('background', 0.092), ('move', 0.091), ('corloc', 0.091), ('object', 0.09), ('bp', 0.088), ('videos', 0.085), ('monkeydog', 0.08), ('parachute', 0.08), ('frame', 0.08), ('border', 0.077), ('rapidly', 0.077), ('pixel', 0.075), ('moving', 0.073), ('shots', 0.067), ('penguin', 0.067), ('segments', 0.064), ('proposals', 0.061), ('birdfall', 0.06), ('calvin', 0.06), ('ltj', 0.06), ('vitj', 0.06), ('witj', 0.06), ('xleft', 0.06), ('xright', 0.06), ('polygon', 0.06), ('rays', 0.06), ('maps', 0.059), ('potentials', 0.059), ('pixels', 0.059), ('deformations', 0.056), ('bg', 0.055), ('tracks', 0.054), ('sti', 0.054), ('clipped', 0.054), ('unary', 0.051), ('static', 0.05), ('edinburgh', 0.05), ('col', 0.049), ('boundary', 0.048), ('might', 0.048), ('missed', 0.048), ('orders', 0.047), ('slowly', 0.047), ('fg', 0.047), ('location', 0.047), ('turbopixels', 0.047), ('grabcut', 0.045), ('miss', 0.045), ('oversegment', 0.044), ('deemed', 0.044), ('odd', 0.044), ('smoothness', 0.044), ('differently', 0.044), ('magnitude', 0.044), ('direction', 0.044), ('rit', 0.043), ('ochs', 0.043), ('unconstrained', 0.042), ('horizontal', 0.042), ('percentage', 0.042), ('vote', 0.041), ('prest', 0.041), ('exp', 0.041), ('produced', 0.041), ('timings', 0.04), ('clustering', 0.039), ('cat', 0.039), ('integrates', 0.039), ('inaccuracies', 0.039), ('bird', 0.038), ('stage', 0.037), ('brox', 0.037), ('mt', 0.037), ('misses', 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="160-tfidf-1" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>Author: Anestis Papazoglou, Vittorio Ferrari</p><p>Abstract: We present a technique for separating foreground objects from the background in a video. Our method isfast, , fully automatic, and makes minimal assumptions about the video. This enables handling essentially unconstrained settings, including rapidly moving background, arbitrary object motion and appearance, and non-rigid deformations and articulations. In experiments on two datasets containing over 1400 video shots, our method outperforms a state-of-theart background subtraction technique [4] as well as methods based on clustering point tracks [6, 18, 19]. Moreover, it performs comparably to recent video object segmentation methods based on objectproposals [14, 16, 27], while being orders of magnitude faster.</p><p>2 0.3008337 <a title="160-tfidf-2" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>Author: Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, James M. Rehg</p><p>Abstract: We propose an unsupervised video segmentation approach by simultaneously tracking multiple holistic figureground segments. Segment tracks are initialized from a pool of segment proposals generated from a figure-ground segmentation algorithm. Then, online non-local appearance models are trained incrementally for each track using a multi-output regularized least squares formulation. By using the same set of training examples for all segment tracks, a computational trick allows us to track hundreds of segment tracks efficiently, as well as perform optimal online updates in closed-form. Besides, a new composite statistical inference approach is proposed for refining the obtained segment tracks, which breaks down the initial segment proposals and recombines for better ones by utilizing highorder statistic estimates from the appearance model and enforcing temporal consistency. For evaluating the algorithm, a dataset, SegTrack v2, is collected with about 1,000 frames with pixel-level annotations. The proposed framework outperforms state-of-the-art approaches in the dataset, show- ing its efficiency and robustness to challenges in different video sequences.</p><p>3 0.29511309 <a title="160-tfidf-3" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>Author: Manjunath Narayana, Allen Hanson, Erik Learned-Miller</p><p>Abstract: In moving camera videos, motion segmentation is commonly performed using the image plane motion of pixels, or optical flow. However, objects that are at different depths from the camera can exhibit different optical flows even if they share the same real-world motion. This can cause a depth-dependent segmentation of the scene. Our goal is to develop a segmentation algorithm that clusters pixels that have similar real-world motion irrespective of their depth in the scene. Our solution uses optical flow orientations instead of the complete vectors and exploits the well-known property that under camera translation, optical flow orientations are independent of object depth. We introduce a probabilistic model that automatically estimates the number of observed independent motions and results in a labeling that is consistent with real-world motion in the scene. The result of our system is that static objects are correctly identified as one segment, even if they are at different depths. Color features and information from previous frames in the video sequence are used to correct occasional errors due to the orientation-based segmentation. We present results on more than thirty videos from different benchmarks. The system is particularly robust on complex background scenes containing objects at significantly different depths.</p><p>4 0.2613121 <a title="160-tfidf-4" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>Author: Christoph Vogel, Konrad Schindler, Stefan Roth</p><p>Abstract: Estimating dense 3D scene flow from stereo sequences remains a challenging task, despite much progress in both classical disparity and 2D optical flow estimation. To overcome the limitations of existing techniques, we introduce a novel model that represents the dynamic 3D scene by a collection of planar, rigidly moving, local segments. Scene flow estimation then amounts to jointly estimating the pixelto-segment assignment, and the 3D position, normal vector, and rigid motion parameters of a plane for each segment. The proposed energy combines an occlusion-sensitive data term with appropriate shape, motion, and segmentation regularizers. Optimization proceeds in two stages: Starting from an initial superpixelization, we estimate the shape and motion parameters of all segments by assigning a proposal from a set of moving planes. Then the pixel-to-segment assignment is updated, while holding the shape and motion parameters of the moving planes fixed. We demonstrate the benefits of our model on different real-world image sets, including the challenging KITTI benchmark. We achieve leading performance levels, exceeding competing 3D scene flow methods, and even yielding better 2D motion estimates than all tested dedicated optical flow techniques.</p><p>5 0.24066207 <a title="160-tfidf-5" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>Author: Abdelaziz Djelouah, Jean-Sébastien Franco, Edmond Boyer, François Le_Clerc, Patrick Pérez</p><p>Abstract: In this paper, we address the problem of object segmentation in multiple views or videos when two or more viewpoints of the same scene are available. We propose a new approach that propagates segmentation coherence information in both space and time, hence allowing evidences in one image to be shared over the complete set. To this aim the segmentation is cast as a single efficient labeling problem over space and time with graph cuts. In contrast to most existing multi-view segmentation methods that rely on some form of dense reconstruction, ours only requires a sparse 3D sampling to propagate information between viewpoints. The approach is thoroughly evaluated on standard multiview datasets, as well as on videos. With static views, results compete with state of the art methods but they are achieved with significantly fewer viewpoints. With multiple videos, we report results that demonstrate the benefit of segmentation propagation through temporal cues.</p><p>6 0.20916297 <a title="160-tfidf-6" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>7 0.18499869 <a title="160-tfidf-7" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>8 0.17930457 <a title="160-tfidf-8" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>9 0.17785183 <a title="160-tfidf-9" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>10 0.16895206 <a title="160-tfidf-10" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>11 0.16482618 <a title="160-tfidf-11" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>12 0.16218308 <a title="160-tfidf-12" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>13 0.15609361 <a title="160-tfidf-13" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>14 0.15496939 <a title="160-tfidf-14" href="./iccv-2013-A_Unified_Video_Segmentation_Benchmark%3A_Annotation%2C_Metrics_and_Analysis.html">33 iccv-2013-A Unified Video Segmentation Benchmark: Annotation, Metrics and Analysis</a></p>
<p>15 0.14941326 <a title="160-tfidf-15" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<p>16 0.14434275 <a title="160-tfidf-16" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<p>17 0.1407436 <a title="160-tfidf-17" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>18 0.1360594 <a title="160-tfidf-18" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>19 0.13416742 <a title="160-tfidf-19" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>20 0.13388938 <a title="160-tfidf-20" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.298), (1, -0.13), (2, 0.119), (3, 0.155), (4, 0.081), (5, 0.075), (6, -0.119), (7, 0.133), (8, 0.067), (9, -0.005), (10, 0.001), (11, 0.164), (12, 0.179), (13, -0.013), (14, -0.12), (15, -0.037), (16, -0.109), (17, -0.018), (18, -0.052), (19, -0.018), (20, 0.122), (21, -0.116), (22, 0.0), (23, -0.034), (24, -0.033), (25, -0.016), (26, 0.017), (27, 0.005), (28, -0.047), (29, -0.005), (30, -0.047), (31, 0.019), (32, -0.05), (33, -0.008), (34, -0.067), (35, 0.057), (36, 0.049), (37, 0.07), (38, -0.068), (39, -0.04), (40, 0.023), (41, -0.013), (42, -0.053), (43, 0.014), (44, -0.025), (45, 0.019), (46, 0.026), (47, 0.01), (48, 0.086), (49, 0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96902293 <a title="160-lsi-1" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>Author: Anestis Papazoglou, Vittorio Ferrari</p><p>Abstract: We present a technique for separating foreground objects from the background in a video. Our method isfast, , fully automatic, and makes minimal assumptions about the video. This enables handling essentially unconstrained settings, including rapidly moving background, arbitrary object motion and appearance, and non-rigid deformations and articulations. In experiments on two datasets containing over 1400 video shots, our method outperforms a state-of-theart background subtraction technique [4] as well as methods based on clustering point tracks [6, 18, 19]. Moreover, it performs comparably to recent video object segmentation methods based on objectproposals [14, 16, 27], while being orders of magnitude faster.</p><p>2 0.86374289 <a title="160-lsi-2" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>Author: Manjunath Narayana, Allen Hanson, Erik Learned-Miller</p><p>Abstract: In moving camera videos, motion segmentation is commonly performed using the image plane motion of pixels, or optical flow. However, objects that are at different depths from the camera can exhibit different optical flows even if they share the same real-world motion. This can cause a depth-dependent segmentation of the scene. Our goal is to develop a segmentation algorithm that clusters pixels that have similar real-world motion irrespective of their depth in the scene. Our solution uses optical flow orientations instead of the complete vectors and exploits the well-known property that under camera translation, optical flow orientations are independent of object depth. We introduce a probabilistic model that automatically estimates the number of observed independent motions and results in a labeling that is consistent with real-world motion in the scene. The result of our system is that static objects are correctly identified as one segment, even if they are at different depths. Color features and information from previous frames in the video sequence are used to correct occasional errors due to the orientation-based segmentation. We present results on more than thirty videos from different benchmarks. The system is particularly robust on complex background scenes containing objects at significantly different depths.</p><p>3 0.78354043 <a title="160-lsi-3" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>Author: Abdelaziz Djelouah, Jean-Sébastien Franco, Edmond Boyer, François Le_Clerc, Patrick Pérez</p><p>Abstract: In this paper, we address the problem of object segmentation in multiple views or videos when two or more viewpoints of the same scene are available. We propose a new approach that propagates segmentation coherence information in both space and time, hence allowing evidences in one image to be shared over the complete set. To this aim the segmentation is cast as a single efficient labeling problem over space and time with graph cuts. In contrast to most existing multi-view segmentation methods that rely on some form of dense reconstruction, ours only requires a sparse 3D sampling to propagate information between viewpoints. The approach is thoroughly evaluated on standard multiview datasets, as well as on videos. With static views, results compete with state of the art methods but they are achieved with significantly fewer viewpoints. With multiple videos, we report results that demonstrate the benefit of segmentation propagation through temporal cues.</p><p>4 0.77837902 <a title="160-lsi-4" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>Author: Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, James M. Rehg</p><p>Abstract: We propose an unsupervised video segmentation approach by simultaneously tracking multiple holistic figureground segments. Segment tracks are initialized from a pool of segment proposals generated from a figure-ground segmentation algorithm. Then, online non-local appearance models are trained incrementally for each track using a multi-output regularized least squares formulation. By using the same set of training examples for all segment tracks, a computational trick allows us to track hundreds of segment tracks efficiently, as well as perform optimal online updates in closed-form. Besides, a new composite statistical inference approach is proposed for refining the obtained segment tracks, which breaks down the initial segment proposals and recombines for better ones by utilizing highorder statistic estimates from the appearance model and enforcing temporal consistency. For evaluating the algorithm, a dataset, SegTrack v2, is collected with about 1,000 frames with pixel-level annotations. The proposed framework outperforms state-of-the-art approaches in the dataset, show- ing its efficiency and robustness to challenges in different video sequences.</p><p>5 0.77198672 <a title="160-lsi-5" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>Author: Christoph Vogel, Konrad Schindler, Stefan Roth</p><p>Abstract: Estimating dense 3D scene flow from stereo sequences remains a challenging task, despite much progress in both classical disparity and 2D optical flow estimation. To overcome the limitations of existing techniques, we introduce a novel model that represents the dynamic 3D scene by a collection of planar, rigidly moving, local segments. Scene flow estimation then amounts to jointly estimating the pixelto-segment assignment, and the 3D position, normal vector, and rigid motion parameters of a plane for each segment. The proposed energy combines an occlusion-sensitive data term with appropriate shape, motion, and segmentation regularizers. Optimization proceeds in two stages: Starting from an initial superpixelization, we estimate the shape and motion parameters of all segments by assigning a proposal from a set of moving planes. Then the pixel-to-segment assignment is updated, while holding the shape and motion parameters of the moving planes fixed. We demonstrate the benefits of our model on different real-world image sets, including the challenging KITTI benchmark. We achieve leading performance levels, exceeding competing 3D scene flow methods, and even yielding better 2D motion estimates than all tested dedicated optical flow techniques.</p><p>6 0.76491231 <a title="160-lsi-6" href="./iccv-2013-A_Unified_Video_Segmentation_Benchmark%3A_Annotation%2C_Metrics_and_Analysis.html">33 iccv-2013-A Unified Video Segmentation Benchmark: Annotation, Metrics and Analysis</a></p>
<p>7 0.72886699 <a title="160-lsi-7" href="./iccv-2013-Motion-Aware_KNN_Laplacian_for_Video_Matting.html">275 iccv-2013-Motion-Aware KNN Laplacian for Video Matting</a></p>
<p>8 0.72616595 <a title="160-lsi-8" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>9 0.69461179 <a title="160-lsi-9" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>10 0.64423442 <a title="160-lsi-10" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<p>11 0.61893141 <a title="160-lsi-11" href="./iccv-2013-Estimating_the_Material_Properties_of_Fabric_from_Video.html">145 iccv-2013-Estimating the Material Properties of Fabric from Video</a></p>
<p>12 0.60559243 <a title="160-lsi-12" href="./iccv-2013-Bounded_Labeling_Function_for_Global_Segmentation_of_Multi-part_Objects_with_Geometric_Constraints.html">63 iccv-2013-Bounded Labeling Function for Global Segmentation of Multi-part Objects with Geometric Constraints</a></p>
<p>13 0.60507613 <a title="160-lsi-13" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<p>14 0.58499622 <a title="160-lsi-14" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>15 0.58488256 <a title="160-lsi-15" href="./iccv-2013-Two-Point_Gait%3A_Decoupling_Gait_from_Body_Shape.html">430 iccv-2013-Two-Point Gait: Decoupling Gait from Body Shape</a></p>
<p>16 0.58423078 <a title="160-lsi-16" href="./iccv-2013-Measuring_Flow_Complexity_in_Videos.html">263 iccv-2013-Measuring Flow Complexity in Videos</a></p>
<p>17 0.57819641 <a title="160-lsi-17" href="./iccv-2013-Modeling_Self-Occlusions_in_Dynamic_Shape_and_Appearance_Tracking.html">270 iccv-2013-Modeling Self-Occlusions in Dynamic Shape and Appearance Tracking</a></p>
<p>18 0.57771897 <a title="160-lsi-18" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>19 0.5745365 <a title="160-lsi-19" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>20 0.56903666 <a title="160-lsi-20" href="./iccv-2013-A_New_Adaptive_Segmental_Matching_Measure_for_Human_Activity_Recognition.html">22 iccv-2013-A New Adaptive Segmental Matching Measure for Human Activity Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.065), (7, 0.015), (12, 0.017), (26, 0.121), (31, 0.042), (34, 0.011), (40, 0.185), (42, 0.078), (48, 0.014), (64, 0.096), (73, 0.027), (78, 0.02), (89, 0.21), (98, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96162271 <a title="160-lda-1" href="./iccv-2013-Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illumination_in_the_Spectral_Domain.html">385 iccv-2013-Separating Reflective and Fluorescent Components Using High Frequency Illumination in the Spectral Domain</a></p>
<p>Author: Ying Fu, Antony Lam, Imari Sato, Takahiro Okabe, Yoichi Sato</p><p>Abstract: Hyperspectral imaging is beneficial to many applications but current methods do not consider fluorescent effects which are present in everyday items ranging from paper, to clothing, to even our food. Furthermore, everyday fluorescent items exhibit a mix of reflectance and fluorescence. So proper separation of these components is necessary for analyzing them. In this paper, we demonstrate efficient separation and recovery of reflective and fluorescent emission spectra through the use of high frequency illumination in the spectral domain. With the obtained fluorescent emission spectra from our high frequency illuminants, we then present to our knowledge, the first method for estimating the fluorescent absorption spectrum of a material given its emission spectrum. Conventional bispectral measurement of absorption and emission spectra needs to examine all combinations of incident and observed light wavelengths. In contrast, our method requires only two hyperspectral images. The effectiveness of our proposed methods are then evaluated through a combination of simulation and real experiments. We also demonstrate an application of our method to synthetic relighting of real scenes.</p><p>2 0.9524526 <a title="160-lda-2" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>Author: Stefan Duffner, Christophe Garcia</p><p>Abstract: In this paper, we present a novel algorithm for fast tracking of generic objects in videos. The algorithm uses two components: a detector that makes use of the generalised Hough transform with pixel-based descriptors, and a probabilistic segmentation method based on global models for foreground and background. These components are used for tracking in a combined way, and they adapt each other in a co-training manner. Through effective model adaptation and segmentation, the algorithm is able to track objects that undergo rigid and non-rigid deformations and considerable shape and appearance variations. The proposed tracking method has been thoroughly evaluated on challenging standard videos, and outperforms state-of-theart tracking methods designed for the same task. Finally, the proposed models allow for an extremely efficient implementation, and thus tracking is very fast.</p><p>3 0.91013575 <a title="160-lda-3" href="./iccv-2013-Total_Variation_Regularization_for_Functions_with_Values_in_a_Manifold.html">421 iccv-2013-Total Variation Regularization for Functions with Values in a Manifold</a></p>
<p>Author: Jan Lellmann, Evgeny Strekalovskiy, Sabrina Koetter, Daniel Cremers</p><p>Abstract: While total variation is among the most popular regularizers for variational problems, its extension to functions with values in a manifold is an open problem. In this paper, we propose the first algorithm to solve such problems which applies to arbitrary Riemannian manifolds. The key idea is to reformulate the variational problem as a multilabel optimization problem with an infinite number of labels. This leads to a hard optimization problem which can be approximately solved using convex relaxation techniques. The framework can be easily adapted to different manifolds including spheres and three-dimensional rotations, and allows to obtain accurate solutions even with a relatively coarse discretization. With numerous examples we demonstrate that the proposed framework can be applied to variational models that incorporate chromaticity values, normal fields, or camera trajectories.</p><p>same-paper 4 0.88515472 <a title="160-lda-4" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>Author: Anestis Papazoglou, Vittorio Ferrari</p><p>Abstract: We present a technique for separating foreground objects from the background in a video. Our method isfast, , fully automatic, and makes minimal assumptions about the video. This enables handling essentially unconstrained settings, including rapidly moving background, arbitrary object motion and appearance, and non-rigid deformations and articulations. In experiments on two datasets containing over 1400 video shots, our method outperforms a state-of-theart background subtraction technique [4] as well as methods based on clustering point tracks [6, 18, 19]. Moreover, it performs comparably to recent video object segmentation methods based on objectproposals [14, 16, 27], while being orders of magnitude faster.</p><p>5 0.86531413 <a title="160-lda-5" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>Author: Olaf Kähler, Ian Reid</p><p>Abstract: We address the problem of 3D scene labeling in a structured learning framework. Unlike previous work which uses structured Support VectorMachines, we employ the recently described Decision Tree Field and Regression Tree Field frameworks, which learn the unary and binary terms of a Conditional Random Field from training data. We show this has significant advantages in terms of inference speed, while maintaining similar accuracy. We also demonstrate empirically the importance for overall labeling accuracy of features that make use of prior knowledge about the coarse scene layout such as the location of the ground plane. We show how this coarse layout can be estimated by our framework automatically, and that this information can be used to bootstrap improved accuracy in the detailed labeling.</p><p>6 0.85268229 <a title="160-lda-6" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>7 0.83424556 <a title="160-lda-7" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>8 0.83169806 <a title="160-lda-8" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>9 0.83032322 <a title="160-lda-9" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>10 0.82784384 <a title="160-lda-10" href="./iccv-2013-Alternating_Regression_Forests_for_Object_Detection_and_Pose_Estimation.html">47 iccv-2013-Alternating Regression Forests for Object Detection and Pose Estimation</a></p>
<p>11 0.82644206 <a title="160-lda-11" href="./iccv-2013-Real-Time_Articulated_Hand_Pose_Estimation_Using_Semi-supervised_Transductive_Regression_Forests.html">340 iccv-2013-Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests</a></p>
<p>12 0.82619584 <a title="160-lda-12" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>13 0.82421678 <a title="160-lda-13" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>14 0.82296503 <a title="160-lda-14" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>15 0.81881207 <a title="160-lda-15" href="./iccv-2013-Volumetric_Semantic_Segmentation_Using_Pyramid_Context_Features.html">447 iccv-2013-Volumetric Semantic Segmentation Using Pyramid Context Features</a></p>
<p>16 0.81840873 <a title="160-lda-16" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>17 0.81697226 <a title="160-lda-17" href="./iccv-2013-Flattening_Supervoxel_Hierarchies_by_the_Uniform_Entropy_Slice.html">172 iccv-2013-Flattening Supervoxel Hierarchies by the Uniform Entropy Slice</a></p>
<p>18 0.81606483 <a title="160-lda-18" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>19 0.81572276 <a title="160-lda-19" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>20 0.81522256 <a title="160-lda-20" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
