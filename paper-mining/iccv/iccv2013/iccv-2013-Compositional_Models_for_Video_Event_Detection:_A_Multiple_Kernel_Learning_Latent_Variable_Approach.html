<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-85" href="#">iccv2013-85</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</h1>
<br/><p>Source: <a title="iccv-2013-85-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Vahdat_Compositional_Models_for_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Arash Vahdat, Kevin Cannons, Greg Mori, Sangmin Oh, Ilseo Kim</p><p>Abstract: We present a compositional model for video event detection. A video is modeled using a collection of both global and segment-level features and kernel functions are employed for similarity comparisons. The locations of salient, discriminative video segments are treated as a latent variable, allowing the model to explicitly ignore portions of the video that are unimportant for classification. A novel, multiple kernel learning (MKL) latent support vector machine (SVM) is defined, that is used to combine and re-weight multiple feature types in a principled fashion while simultaneously operating within the latent variable framework. The compositional nature of the proposed model allows it to respond directly to the challenges of temporal clutter and intra-class variation, which are prevalent in unconstrained internet videos. Experimental results on the TRECVID Multimedia Event Detection 2011 (MED11) dataset demonstrate the efficacy of the method.</p><p>Reference: <a title="iccv-2013-85-reference" href="../iccv2013_reference/iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ca Abstract We present a compositional model for video event detection. [sent-2, score-0.581]
</p><p>2 A video is modeled using a collection of both global and segment-level features and kernel functions are employed for similarity comparisons. [sent-3, score-0.403]
</p><p>3 The locations of salient, discriminative video segments are treated as a latent variable, allowing the model to explicitly ignore portions of the video that are unimportant for classification. [sent-4, score-0.75]
</p><p>4 A novel, multiple kernel learning (MKL) latent support vector machine (SVM) is defined, that is used to combine and re-weight multiple feature types in a principled fashion while simultaneously operating within the latent variable framework. [sent-5, score-1.161]
</p><p>5 The compositional nature of the proposed model allows it to respond directly to the challenges of temporal clutter and intra-class variation, which are prevalent in unconstrained internet videos. [sent-6, score-0.439]
</p><p>6 Introduction Multimedia event detection in unconstrained video collections is a challenging problem. [sent-9, score-0.433]
</p><p>7 This video contains segments focusing on the snowboard, the person jumping, is shot in an outdoor, ski-resort scene, and has fast-paced theme music. [sent-14, score-0.315]
</p><p>8 In this example, parts of the test video from the board trick event are similar to three different videos in terms of motion and sound (green), pure motion (purple) or motion and texture (yellow). [sent-29, score-0.68]
</p><p>9 Arguably, such a model must reason about which temporal segments within the video contain relevant evidence. [sent-31, score-0.393]
</p><p>10 Grouping segments into their relevant scene types can improve recognition. [sent-34, score-0.314]
</p><p>11 In this paper we present a novel, compositional model for video event detection. [sent-36, score-0.581]
</p><p>12 Our model uses a latent variable framework to localize the discriminative temporal segments of a video. [sent-37, score-0.647]
</p><p>13 These temporal segments are matched to training segments of the same scene type via kernels that combine information from several feature modalities. [sent-38, score-0.773]
</p><p>14 The proposed compositional method has two key novel aspects: (1) a weakly supervised method for localizing only the most salient evidence for classification in a video sequence. [sent-41, score-0.377]
</p><p>15 (2) A novel multiple kernel learning algorithm with structured latent variables that permits the principled combination of multiple different low-level features in a single integrated framework. [sent-43, score-0.831]
</p><p>16 Furthermore, our multiple kernel learning algorithm offers an extension that allows for such feature combination in conjunction with latent SVMs. [sent-50, score-0.596]
</p><p>17 With this novel approach, more detailed comparisons between latently selected video segments can be considered. [sent-51, score-0.472]
</p><p>18 [8], who developed a related model for human action recognition, but used a fixed, single temporal ordering of key poses around anchor points which may break down in internet videos due to temporal clutter. [sent-53, score-0.4]
</p><p>19 In contrast to the above, our method focuses on intra-class variation and temporal scatter of an event by using latent variables to compose a test video in a kernelized framework. [sent-58, score-1.319]
</p><p>20 Technically, the proposed approach is most closely related to [18, 20, 3], but differentiates itself by presenting a novel multiple kernel learning approach that accommodates structured latent variables. [sent-63, score-0.597]
</p><p>21 [20] developed kernelized variants of the latent support vector machine [2, 2 1]. [sent-65, score-0.735]
</p><p>22 However, the algorithms for learning kernelized latent SVMs in these papers have two drawbacks: they are limited to cases where one can enumerate the set of latent variables and they are restricted to a single kernel or a set of summed kernels. [sent-66, score-1.3]
</p><p>23 A body of work has aimed at providing efficient training and evaluation with kernelized classifiers via algorithmic optimizations or additive linear approximations [15, 6, 10]. [sent-72, score-0.39]
</p><p>24 This line of work is promising, but has yet to be extended to latent variable models, as is done here. [sent-73, score-0.378]
</p><p>25 ,  the evidence of a complex event can occur in small, isolated video segments) and intra-class variation. [sent-78, score-0.437]
</p><p>26 In this paper, we target both the intra-class variation and temporal clutter challenges by leveraging a compositional model. [sent-79, score-0.349]
</p><p>27 In such systems, the standard kernelized SVM can be thought of as a form of intelligent template matching, whereby a test video is compared directly against the set of support vectors. [sent-82, score-0.516]
</p><p>28 By introducing latent variables in our proposed method, kernelized latent SVMs are constructed that select particularly salient video segments. [sent-84, score-1.269]
</p><p>29 This approach provides our compositional model with the additional flexibility to mix and match segments from the pool of training videos when evaluating a test video, directly addressing the challenges of clutter and intra-class variation. [sent-86, score-0.608]
</p><p>30 We further extend our model to combine multiple kernel learning with the kernelized latent SVM framework, adding the ability to weight feature types based  on their relative importance. [sent-88, score-0.965]
</p><p>31 It is assumed that each event category contains several subcategories (e. [sent-93, score-0.4]
</p><p>32 The global model captures the subcategories of an event, and the scene model represents the different scene types observed in the category. [sent-98, score-0.387]
</p><p>33 The presence of a subcategory or scene type is represented using binary variables (bc, zs). [sent-99, score-0.516]
</p><p>34 The temporal position of scene types in a video is denoted by ts. [sent-100, score-0.389]
</p><p>35 The second part of our formulation is a “scene type model”  that represents an event by a set of segment-level features. [sent-103, score-0.366]
</p><p>36 We consider eight second segments that correspond to scenes observed within the event category (e. [sent-107, score-0.476]
</p><p>37 A weakly supervised setting is considered, meaning that we are only given a binary event label for each video that indicates the presence of a complex event in the sequence; the subcategory labels, scene type labels, and temporal locations of scene types are not provided. [sent-110, score-1.33]
</p><p>38 These are modeled as hidden variables and we employ a latent max-margin approach [2] to infer them during training. [sent-111, score-0.466]
</p><p>39 Concretely, assume we are given a video sequence x, and want to classify it into an event category. [sent-112, score-0.436]
</p><p>40 The variables C and S denote the number of subcategories and scene types for an event, respectively. [sent-113, score-0.361]
</p><p>41 1  where wcg is the learned weight vector for the cth subcategory model on the global feature φg (·), and wsl is the weight vector for the sth scene type mo(d·)e,l adnedfin wed on the segment-level feature φl (·). [sent-144, score-0.515]
</p><p>42 al location for the sth scene type is shared among all segment-level features types – they are all extracted from the same temporal window in the sequence. [sent-165, score-0.392]
</p><p>43 del configurations, hs, are latent variables, unobserved on both training and testing data. [sent-171, score-0.388]
</p><p>44 Next, we develop a novel multiple kernel learning approach for learning with these latent variables. [sent-172, score-0.564]
</p><p>45 However, both [20, 18] assumed simple models with few latent variables that could be enumerated during inference. [sent-180, score-0.466]
</p><p>46 In our proposed model, latent variables are defined in a structured framework such that enumeration is not tractable. [sent-181, score-0.499]
</p><p>47 We require a training framework that can accommodate both latent variables and feature re-scaling simultaneously. [sent-194, score-0.543]
</p><p>48 We propose a novel multiple kernel latent SVM framework that extends standard MKL and can be used to train models of the form proposed in this paper. [sent-195, score-0.564]
</p><p>49 Note that our multiple kernel latent SVM framework becomes a standard latent SVM [2] if the kernel coefficients, di, are set to one and will become a standard MKL classifier if the hidden variables vn are observed. [sent-218, score-1.316]
</p><p>50 2 is not convex; however, convexity is attained if the latent variables for positive samples are available (sem√i-convexity of latent SVM [2]) and if wi is replaced with √diwi. [sent-220, score-0.894]
</p><p>51 Here we limit the possible latent variables of positive samples to a single configuration Vn = {v∗n} ∀n : yn = 1, but allow negative samples to Vcon=side {rv all} po∀ssnib :le y latent variables, Vn ∀n : yn = −1. [sent-221, score-1.096]
</p><p>52 If the latent variables are structured, |V| will be exponential. [sent-237, score-0.466]
</p><p>53 We use the cutting plane algorithm to extract the set of most violated constraints for negative samples during training, while the latent variables of positive videos remain fixed. [sent-241, score-0.688]
</p><p>54 Here, denotes the set of current active constraints (instead of VVn, which represents all the constraints defined (oivnestr aaldl poof sVsible latent variables). [sent-242, score-0.343]
</p><p>55 ∈V˜n where αn,v is the Lagrangian variable for the nth sample and the latent variables, v. [sent-249, score-0.378]
</p><p>56 ) that measures the similarity of xn and xm, given their latent configurations. [sent-278, score-0.399]
</p><p>57 Given the final α∗ and d∗ (which together represent w), we infer the latent v? [sent-299, score-0.343]
</p><p>58 f positive samples and learning the latent SVM model parameters will minimize the objective function to a local optimum [2 1, 2]. [sent-305, score-0.38]
</p><p>59 The same argument holds for multiple kernel latent SVM. [sent-306, score-0.564]
</p><p>60 Kernelized Model We use multiple kernel latent SVM to train the parameters of our model defined in Eq. [sent-310, score-0.564]
</p><p>61 1 global models were defined on G global features while scene type models employed L segment-level feature types. [sent-315, score-0.366]
</p><p>62 Specifically, the 1188  Algorithm 1 Training a multiple kernel latent SVM  . [sent-316, score-0.564]
</p><p>63 , Kg measures the kernelized similarity of their global feature if they belong to the same subcategory; otherwise, it assigns zero similarity. [sent-353, score-0.395]
</p><p>64 Analogously, Kl measures the kernelized similarity of segment-  level feature lfor sequences x and x? [sent-354, score-0.337]
</p><p>65 ,  V˜n  where (hn, bn) ∈ are latent variables defined for the nth training sam)p ∈le. [sent-376, score-0.511]
</p><p>66 Given the sequence, x, maximization matches the sequence to the training videos by choosing segment locations, h, and the subcategory model, b, that are well-explained by the training videos. [sent-379, score-0.463]
</p><p>67 A test video, x, is assigned a high score for an event category if it is similar to its associated positive training videos using two criteria. [sent-380, score-0.564]
</p><p>68 First, the global features from the test video should be similar to the global features from training videos. [sent-381, score-0.395]
</p><p>69 Second, the test video should contain segments that are similar to those in the training set. [sent-382, score-0.368]
</p><p>70 Under this framework, the test video can be composed using components from numerous training videos at both the global and segment scale. [sent-383, score-0.392]
</p><p>71 Implementation Details Simple heuristics are used to initialize the latent variables for the positive samples. [sent-388, score-0.503]
</p><p>72 For the subcategory labels, we cluster the concatenated global features of the positive videos into C clusters. [sent-389, score-0.465]
</p><p>73 For the scene models, we similarly cluster the concatenated segment-level features of all segments from the positive training videos. [sent-391, score-0.367]
</p><p>74 Inference: For inferring latent variables, we first need to compute the global and scene model scores for each subcategory and scene type. [sent-393, score-0.793]
</p><p>75 Now, given global and scene type model scores, we need to infer the subcategory  variables bc and temporal locations ts of the K best scene type models. [sent-403, score-1.009]
</p><p>76 For a video with T segments, the best location for each scene type is found in O(T), and then the K best scenes are selected in O(S log(K)) using a min heap. [sent-405, score-0.345]
</p><p>77 The DEV-T dataset consists of 10,723 videos including videos from five event categories: board trick (E1), feeding animal (E2), landing fish (E3), wedding ceremony (E4), and woodworking project (E5). [sent-413, score-0.79]
</p><p>78 A classifier is trained for each event category versus all other categories, similar to [12]. [sent-421, score-0.317]
</p><p>79 Two variants of our proposed model were considered: Linear-LSVM, using a linear latent SVM, and KLSVM, using a HIK latent SVM. [sent-432, score-0.726]
</p><p>80 We used the kernelized version of our model with a HIK kernel to choose the best parameters on DEV-T (E1 to E5) and fixed them for all subsequent experiments using our model in this paper. [sent-434, score-0.491]
</p><p>81 In particular, our latent model with selected parameters (C = 8, S = 16, K = 4) outperforms the standard kernelized SVM (C = 1, S = 0) by 6. [sent-437, score-0.686]
</p><p>82 It appears that use of a kernelized SVM is critical for the task of accurate event detection. [sent-449, score-0.579]
</p><p>83 Note that a kernelized version of Tang was not considered because it is not clear how the computationally  expensive inference could be done for an extension to kernel SVMs, especially for a large data collection. [sent-461, score-0.491]
</p><p>84 Best results for a particular event category are shown in bold. [sent-473, score-0.317]
</p><p>85 In the KLSVM, the weights of all kernels are fixed to one, while in the MKLKLSVM, the kernel weights are learned. [sent-483, score-0.351]
</p><p>86 KLSVM improves performance by incorporating our proposed compositional model that performs latent segment selection. [sent-487, score-0.526]
</p><p>87 Finally, when considering the full model, MKL-KLSVM, which allows the various kernel weights to be adapted for the global and segment components across multiple features, highest overall accuracy is attained. [sent-488, score-0.319]
</p><p>88 The frames that are latently selected tend to be discriminative and ignore temporal clutter inherent in many test videos. [sent-492, score-0.438]
</p><p>89 For example, in the sewing project video, the latter frames where the individual is walking in an outdoor environment are not selected because such scenes are not typically associated with a video of a sewing project. [sent-493, score-0.361]
</p><p>90 For example, in the changing a tire test sequence, two of the top three support vector videos offer good matches for three of the latently selected frames in the test sequence (corresponding to the test frames highlighted with red, yellow, and blue boxes). [sent-500, score-0.655]
</p><p>91 The proposed model is able to accumulate evidence for classification from different video segments in the pool of training videos. [sent-502, score-0.367]
</p><p>92 Conclusion We presented a novel, compositional model for video event detection that leverages a novel multiple kernel learning algorithm that incorporates structured latent variables. [sent-504, score-1.178]
</p><p>93 The kernelized latent variable framework allows the model to select and match test video segments with those that are extracted from the pool of training of videos. [sent-505, score-1.051]
</p><p>94 The compositional nature of the model allows it to respond to the challenges of intra-class variation and temporal clutter, which 1191  subfigure shows frames from a testing video along with frames from the three support vectors that produce the overall best match to that test video (i. [sent-506, score-0.714]
</p><p>95 For a test video, the K = 4 frames that were latently selected are highlighted with colored boxes, where color denotes the particular scene type model. [sent-509, score-0.455]
</p><p>96 Latently selected frames from the the top three support vectors are grouped using colored boxes, where color corresponds to the same scene types selected for the test video. [sent-510, score-0.361]
</p><p>97 From top-to-bottom, left-to-right, the testing videos correspond to changing tire (E7), grooming animal (E10), repairing appliance (E14), and sewing project (E15). [sent-511, score-0.428]
</p><p>98 Additionally, since multiple feature types are required to attain state-ofthe-art performance on TRECVID MED1 1, a principled approach to feature fusion via multiple kernel learning with structured  latent variables  is proposed. [sent-515, score-0.988]
</p><p>99 Multimodal feature fusion for robust event detection in web videos. [sent-576, score-0.339]
</p><p>100 A discriminative latent model of object classes and attributes. [sent-646, score-0.343]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('latent', 0.343), ('kernelized', 0.305), ('event', 0.274), ('subcategory', 0.21), ('kernel', 0.186), ('compositional', 0.183), ('klsvm', 0.176), ('trecvid', 0.165), ('segments', 0.159), ('ksvm', 0.151), ('latently', 0.151), ('videos', 0.125), ('yn', 0.125), ('video', 0.124), ('variables', 0.123), ('temporal', 0.11), ('svm', 0.107), ('vn', 0.1), ('mkl', 0.1), ('bc', 0.098), ('zs', 0.097), ('type', 0.092), ('scene', 0.091), ('niebles', 0.09), ('kernels', 0.085), ('svms', 0.083), ('subcategories', 0.083), ('hik', 0.083), ('sewing', 0.078), ('tang', 0.064), ('bn', 0.064), ('types', 0.064), ('trick', 0.06), ('cutting', 0.06), ('grooming', 0.058), ('global', 0.058), ('board', 0.057), ('xn', 0.056), ('clutter', 0.056), ('kg', 0.056), ('internet', 0.055), ('ceremony', 0.053), ('baselines', 0.052), ('mori', 0.051), ('newton', 0.051), ('diwit', 0.05), ('mklklsvm', 0.05), ('tire', 0.05), ('wedding', 0.048), ('animal', 0.048), ('wi', 0.048), ('support', 0.047), ('training', 0.045), ('kitware', 0.045), ('segmentlevel', 0.045), ('snowboard', 0.045), ('pooling', 0.044), ('hn', 0.044), ('ts', 0.044), ('category', 0.043), ('qp', 0.043), ('frames', 0.043), ('wit', 0.042), ('maxv', 0.041), ('vahdat', 0.041), ('di', 0.041), ('principled', 0.041), ('additive', 0.04), ('variants', 0.04), ('weights', 0.04), ('test', 0.04), ('events', 0.04), ('evidence', 0.039), ('hs', 0.039), ('selected', 0.038), ('sequence', 0.038), ('appliance', 0.037), ('positive', 0.037), ('unconstrained', 0.035), ('xm', 0.035), ('variable', 0.035), ('base', 0.035), ('lagrangian', 0.035), ('multiple', 0.035), ('features', 0.035), ('natarajan', 0.034), ('fusion', 0.033), ('saddle', 0.033), ('izadinia', 0.033), ('smo', 0.033), ('structured', 0.033), ('kl', 0.032), ('iarpa', 0.032), ('repairing', 0.032), ('theme', 0.032), ('feature', 0.032), ('visualizations', 0.031), ('salient', 0.031), ('attain', 0.031), ('park', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="85-tfidf-1" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>Author: Arash Vahdat, Kevin Cannons, Greg Mori, Sangmin Oh, Ilseo Kim</p><p>Abstract: We present a compositional model for video event detection. A video is modeled using a collection of both global and segment-level features and kernel functions are employed for similarity comparisons. The locations of salient, discriminative video segments are treated as a latent variable, allowing the model to explicitly ignore portions of the video that are unimportant for classification. A novel, multiple kernel learning (MKL) latent support vector machine (SVM) is defined, that is used to combine and re-weight multiple feature types in a principled fashion while simultaneously operating within the latent variable framework. The compositional nature of the proposed model allows it to respond directly to the challenges of temporal clutter and intra-class variation, which are prevalent in unconstrained internet videos. Experimental results on the TRECVID Multimedia Event Detection 2011 (MED11) dataset demonstrate the efficacy of the method.</p><p>2 0.30466959 <a title="85-tfidf-2" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>Author: Weixin Li, Qian Yu, Ajay Divakaran, Nuno Vasconcelos</p><p>Abstract: The problem of adaptively selecting pooling regions for the classification of complex video events is considered. Complex events are defined as events composed of several characteristic behaviors, whose temporal configuration can change from sequence to sequence. A dynamic pooling operator is defined so as to enable a unified solution to the problems of event specific video segmentation, temporal structure modeling, and event detection. Video is decomposed into segments, and the segments most informative for detecting a given event are identified, so as to dynamically determine the pooling operator most suited for each sequence. This dynamic pooling is implemented by treating the locations of characteristic segments as hidden information, which is inferred, on a sequence-by-sequence basis, via a large-margin classification rule with latent variables. Although the feasible set of segment selections is combinatorial, it is shown that a globally optimal solution to the inference problem can be obtained efficiently, through the solution of a series of linear programs. Besides the coarselevel location of segments, a finer model of video struc- ture is implemented by jointly pooling features of segmenttuples. Experimental evaluation demonstrates that the re- sulting event detector has state-of-the-art performance on challenging video datasets.</p><p>3 0.25795785 <a title="85-tfidf-3" href="./iccv-2013-Event_Recognition_in_Photo_Collections_with_a_Stopwatch_HMM.html">147 iccv-2013-Event Recognition in Photo Collections with a Stopwatch HMM</a></p>
<p>Author: Lukas Bossard, Matthieu Guillaumin, Luc Van_Gool</p><p>Abstract: The task of recognizing events in photo collections is central for automatically organizing images. It is also very challenging, because of the ambiguity of photos across different event classes and because many photos do not convey enough relevant information. Unfortunately, the field still lacks standard evaluation data sets to allow comparison of different approaches. In this paper, we introduce and release a novel data set of personal photo collections containing more than 61,000 images in 807 collections, annotated with 14 diverse social event classes. Casting collections as sequential data, we build upon recent and state-of-the-art work in event recognition in videos to propose a latent sub-event approach for event recognition in photo collections. However, photos in collections are sparsely sampled over time and come in bursts from which transpires the importance of specific moments for the photographers. Thus, we adapt a discriminative hidden Markov model to allow the transitions between states to be a function of the time gap between consecutive images, which we coin as Stopwatch Hidden Markov model (SHMM). In our experiments, we show that our proposed model outperforms approaches based only on feature pooling or a classical hidden Markov model. With an average accuracy of 56%, we also highlight the difficulty of the data set and the need for future advances in event recognition in photo collections.</p><p>4 0.2376139 <a title="85-tfidf-4" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>Author: Kevin Tang, Bangpeng Yao, Li Fei-Fei, Daphne Koller</p><p>Abstract: In this paper, we tackle the problem of combining features extracted from video for complex event recognition. Feature combination is an especially relevant task in video data, as there are many features we can extract, ranging from image features computed from individual frames to video features that take temporal information into account. To combine features effectively, we propose a method that is able to be selective of different subsets of features, as some features or feature combinations may be uninformative for certain classes. We introduce a hierarchical method for combining features based on the AND/OR graph structure, where nodes in the graph represent combinations of different sets of features. Our method automatically learns the structure of the AND/OR graph using score-based structure learning, and we introduce an inference procedure that is able to efficiently compute structure scores. We present promising results and analysis on the difficult and large-scale 2011 TRECVID Multimedia Event Detection dataset [17].</p><p>5 0.21868806 <a title="85-tfidf-5" href="./iccv-2013-Feature_Weighting_via_Optimal_Thresholding_for_Video_Analysis.html">163 iccv-2013-Feature Weighting via Optimal Thresholding for Video Analysis</a></p>
<p>Author: Zhongwen Xu, Yi Yang, Ivor Tsang, Nicu Sebe, Alexander G. Hauptmann</p><p>Abstract: Fusion of multiple features can boost the performance of large-scale visual classification and detection tasks like TRECVID Multimedia Event Detection (MED) competition [1]. In this paper, we propose a novel feature fusion approach, namely Feature Weighting via Optimal Thresholding (FWOT) to effectively fuse various features. FWOT learns the weights, thresholding and smoothing parameters in a joint framework to combine the decision values obtained from all the individual features and the early fusion. To the best of our knowledge, this is the first work to consider the weight and threshold factors of fusion problem simultaneously. Compared to state-of-the-art fusion algorithms, our approach achieves promising improvements on HMDB [8] action recognition dataset and CCV [5] video classification dataset. In addition, experiments on two TRECVID MED 2011 collections show that our approach outperforms the state-of-the-art fusion methods for complex event detection.</p><p>6 0.21841273 <a title="85-tfidf-6" href="./iccv-2013-Event_Detection_in_Complex_Scenes_Using_Interval_Temporal_Constraints.html">146 iccv-2013-Event Detection in Complex Scenes Using Interval Temporal Constraints</a></p>
<p>7 0.21733689 <a title="85-tfidf-7" href="./iccv-2013-Group_Norm_for_Learning_Structured_SVMs_with_Unstructured_Latent_Variables.html">187 iccv-2013-Group Norm for Learning Structured SVMs with Unstructured Latent Variables</a></p>
<p>8 0.21717446 <a title="85-tfidf-8" href="./iccv-2013-How_Related_Exemplars_Help_Complex_Event_Detection_in_Web_Videos%3F.html">203 iccv-2013-How Related Exemplars Help Complex Event Detection in Web Videos?</a></p>
<p>9 0.20957062 <a title="85-tfidf-9" href="./iccv-2013-Modeling_4D_Human-Object_Interactions_for_Event_and_Object_Recognition.html">268 iccv-2013-Modeling 4D Human-Object Interactions for Event and Object Recognition</a></p>
<p>10 0.20760825 <a title="85-tfidf-10" href="./iccv-2013-Learning_to_Share_Latent_Tasks_for_Action_Recognition.html">249 iccv-2013-Learning to Share Latent Tasks for Action Recognition</a></p>
<p>11 0.1961865 <a title="85-tfidf-11" href="./iccv-2013-ACTIVE%3A_Activity_Concept_Transitions_in_Video_Event_Classification.html">4 iccv-2013-ACTIVE: Activity Concept Transitions in Video Event Classification</a></p>
<p>12 0.18184453 <a title="85-tfidf-12" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<p>13 0.17867503 <a title="85-tfidf-13" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>14 0.15980254 <a title="85-tfidf-14" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>15 0.15955803 <a title="85-tfidf-15" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>16 0.15714526 <a title="85-tfidf-16" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>17 0.15425542 <a title="85-tfidf-17" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>18 0.14709765 <a title="85-tfidf-18" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>19 0.14497949 <a title="85-tfidf-19" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>20 0.1302252 <a title="85-tfidf-20" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.268), (1, 0.183), (2, 0.059), (3, 0.118), (4, 0.083), (5, 0.076), (6, 0.071), (7, -0.025), (8, -0.048), (9, -0.177), (10, -0.155), (11, -0.187), (12, -0.05), (13, 0.049), (14, -0.228), (15, -0.088), (16, 0.048), (17, 0.074), (18, 0.055), (19, -0.007), (20, 0.052), (21, 0.027), (22, 0.016), (23, 0.009), (24, 0.063), (25, -0.018), (26, 0.029), (27, 0.045), (28, 0.053), (29, 0.045), (30, 0.002), (31, 0.037), (32, -0.12), (33, -0.054), (34, -0.023), (35, 0.016), (36, 0.003), (37, 0.012), (38, 0.05), (39, -0.085), (40, 0.035), (41, -0.053), (42, 0.089), (43, -0.082), (44, -0.005), (45, 0.076), (46, -0.015), (47, 0.087), (48, -0.069), (49, 0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95415097 <a title="85-lsi-1" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>Author: Arash Vahdat, Kevin Cannons, Greg Mori, Sangmin Oh, Ilseo Kim</p><p>Abstract: We present a compositional model for video event detection. A video is modeled using a collection of both global and segment-level features and kernel functions are employed for similarity comparisons. The locations of salient, discriminative video segments are treated as a latent variable, allowing the model to explicitly ignore portions of the video that are unimportant for classification. A novel, multiple kernel learning (MKL) latent support vector machine (SVM) is defined, that is used to combine and re-weight multiple feature types in a principled fashion while simultaneously operating within the latent variable framework. The compositional nature of the proposed model allows it to respond directly to the challenges of temporal clutter and intra-class variation, which are prevalent in unconstrained internet videos. Experimental results on the TRECVID Multimedia Event Detection 2011 (MED11) dataset demonstrate the efficacy of the method.</p><p>2 0.83989251 <a title="85-lsi-2" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>Author: Weixin Li, Qian Yu, Ajay Divakaran, Nuno Vasconcelos</p><p>Abstract: The problem of adaptively selecting pooling regions for the classification of complex video events is considered. Complex events are defined as events composed of several characteristic behaviors, whose temporal configuration can change from sequence to sequence. A dynamic pooling operator is defined so as to enable a unified solution to the problems of event specific video segmentation, temporal structure modeling, and event detection. Video is decomposed into segments, and the segments most informative for detecting a given event are identified, so as to dynamically determine the pooling operator most suited for each sequence. This dynamic pooling is implemented by treating the locations of characteristic segments as hidden information, which is inferred, on a sequence-by-sequence basis, via a large-margin classification rule with latent variables. Although the feasible set of segment selections is combinatorial, it is shown that a globally optimal solution to the inference problem can be obtained efficiently, through the solution of a series of linear programs. Besides the coarselevel location of segments, a finer model of video struc- ture is implemented by jointly pooling features of segmenttuples. Experimental evaluation demonstrates that the re- sulting event detector has state-of-the-art performance on challenging video datasets.</p><p>3 0.8018676 <a title="85-lsi-3" href="./iccv-2013-Event_Recognition_in_Photo_Collections_with_a_Stopwatch_HMM.html">147 iccv-2013-Event Recognition in Photo Collections with a Stopwatch HMM</a></p>
<p>Author: Lukas Bossard, Matthieu Guillaumin, Luc Van_Gool</p><p>Abstract: The task of recognizing events in photo collections is central for automatically organizing images. It is also very challenging, because of the ambiguity of photos across different event classes and because many photos do not convey enough relevant information. Unfortunately, the field still lacks standard evaluation data sets to allow comparison of different approaches. In this paper, we introduce and release a novel data set of personal photo collections containing more than 61,000 images in 807 collections, annotated with 14 diverse social event classes. Casting collections as sequential data, we build upon recent and state-of-the-art work in event recognition in videos to propose a latent sub-event approach for event recognition in photo collections. However, photos in collections are sparsely sampled over time and come in bursts from which transpires the importance of specific moments for the photographers. Thus, we adapt a discriminative hidden Markov model to allow the transitions between states to be a function of the time gap between consecutive images, which we coin as Stopwatch Hidden Markov model (SHMM). In our experiments, we show that our proposed model outperforms approaches based only on feature pooling or a classical hidden Markov model. With an average accuracy of 56%, we also highlight the difficulty of the data set and the need for future advances in event recognition in photo collections.</p><p>4 0.76272261 <a title="85-lsi-4" href="./iccv-2013-How_Related_Exemplars_Help_Complex_Event_Detection_in_Web_Videos%3F.html">203 iccv-2013-How Related Exemplars Help Complex Event Detection in Web Videos?</a></p>
<p>Author: Yi Yang, Zhigang Ma, Zhongwen Xu, Shuicheng Yan, Alexander G. Hauptmann</p><p>Abstract: Compared to visual concepts such as actions, scenes and objects, complex event is a higher level abstraction of longer video sequences. For example, a “marriage proposal” event is described by multiple objects (e.g., ring, faces), scenes (e.g., in a restaurant, outdoor) and actions (e.g., kneeling down). The positive exemplars which exactly convey the precise semantic of an event are hard to obtain. It would be beneficial to utilize the related exemplars for complex event detection. However, the semantic correlations between related exemplars and the target event vary substantially as relatedness assessment is subjective. Two related exemplars can be about completely different events, e.g., in the TRECVID MED dataset, both bicycle riding and equestrianism are labeled as related to “attempting a bike trick” event. To tackle the subjectiveness of human assessment, our algorithm automatically evaluates how positive the related exemplars are for the detection of an event and uses them on an exemplar-specific basis. Experiments demonstrate that our algorithm is able to utilize related exemplars adaptively, and the algorithm gains good perform- z. ance for complex event detection.</p><p>5 0.73925889 <a title="85-lsi-5" href="./iccv-2013-Event_Detection_in_Complex_Scenes_Using_Interval_Temporal_Constraints.html">146 iccv-2013-Event Detection in Complex Scenes Using Interval Temporal Constraints</a></p>
<p>Author: Yifan Zhang, Qiang Ji, Hanqing Lu</p><p>Abstract: In complex scenes with multiple atomic events happening sequentially or in parallel, detecting each individual event separately may not always obtain robust and reliable result. It is essential to detect them in a holistic way which incorporates the causality and temporal dependency among them to compensate the limitation of current computer vision techniques. In this paper, we propose an interval temporal constrained dynamic Bayesian network to extendAllen ’s interval algebra network (IAN) [2]from a deterministic static model to a probabilistic dynamic system, which can not only capture the complex interval temporal relationships, but also model the evolution dynamics and handle the uncertainty from the noisy visual observation. In the model, the topology of the IAN on each time slice and the interlinks between the time slices are discovered by an advanced structure learning method. The duration of the event and the unsynchronized time lags between two correlated event intervals are captured by a duration model, so that we can better determine the temporal boundary of the event. Empirical results on two real world datasets show the power of the proposed interval temporal constrained model.</p><p>6 0.72588235 <a title="85-lsi-6" href="./iccv-2013-ACTIVE%3A_Activity_Concept_Transitions_in_Video_Event_Classification.html">4 iccv-2013-ACTIVE: Activity Concept Transitions in Video Event Classification</a></p>
<p>7 0.70843583 <a title="85-lsi-7" href="./iccv-2013-Feature_Weighting_via_Optimal_Thresholding_for_Video_Analysis.html">163 iccv-2013-Feature Weighting via Optimal Thresholding for Video Analysis</a></p>
<p>8 0.70840418 <a title="85-lsi-8" href="./iccv-2013-Modeling_4D_Human-Object_Interactions_for_Event_and_Object_Recognition.html">268 iccv-2013-Modeling 4D Human-Object Interactions for Event and Object Recognition</a></p>
<p>9 0.61945915 <a title="85-lsi-9" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>10 0.61530471 <a title="85-lsi-10" href="./iccv-2013-Group_Norm_for_Learning_Structured_SVMs_with_Unstructured_Latent_Variables.html">187 iccv-2013-Group Norm for Learning Structured SVMs with Unstructured Latent Variables</a></p>
<p>11 0.61165839 <a title="85-lsi-11" href="./iccv-2013-Learning_Slow_Features_for_Behaviour_Analysis.html">243 iccv-2013-Learning Slow Features for Behaviour Analysis</a></p>
<p>12 0.59330362 <a title="85-lsi-12" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<p>13 0.5748558 <a title="85-lsi-13" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>14 0.55671108 <a title="85-lsi-14" href="./iccv-2013-Learning_to_Share_Latent_Tasks_for_Action_Recognition.html">249 iccv-2013-Learning to Share Latent Tasks for Action Recognition</a></p>
<p>15 0.53545594 <a title="85-lsi-15" href="./iccv-2013-Facial_Action_Unit_Event_Detection_by_Cascade_of_Tasks.html">155 iccv-2013-Facial Action Unit Event Detection by Cascade of Tasks</a></p>
<p>16 0.52046168 <a title="85-lsi-16" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>17 0.5058651 <a title="85-lsi-17" href="./iccv-2013-Abnormal_Event_Detection_at_150_FPS_in_MATLAB.html">34 iccv-2013-Abnormal Event Detection at 150 FPS in MATLAB</a></p>
<p>18 0.48093292 <a title="85-lsi-18" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>19 0.47188625 <a title="85-lsi-19" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>20 0.46091375 <a title="85-lsi-20" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.097), (4, 0.023), (7, 0.014), (13, 0.011), (26, 0.065), (31, 0.037), (42, 0.085), (48, 0.013), (64, 0.085), (73, 0.021), (77, 0.197), (78, 0.017), (89, 0.235), (98, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9036181 <a title="85-lda-1" href="./iccv-2013-Relative_Attributes_for_Large-Scale_Abandoned_Object_Detection.html">350 iccv-2013-Relative Attributes for Large-Scale Abandoned Object Detection</a></p>
<p>Author: Quanfu Fan, Prasad Gabbur, Sharath Pankanti</p><p>Abstract: Effective reduction of false alarms in large-scale video surveillance is rather challenging, especially for applications where abnormal events of interest rarely occur, such as abandoned object detection. We develop an approach to prioritize alerts by ranking them, and demonstrate its great effectiveness in reducing false positives while keeping good detection accuracy. Our approach benefits from a novel representation of abandoned object alerts by relative attributes, namely staticness, foregroundness and abandonment. The relative strengths of these attributes are quantified using a ranking function[19] learnt on suitably designed low-level spatial and temporal features.These attributes of varying strengths are not only powerful in distinguishing abandoned objects from false alarms such as people and light artifacts, but also computationally efficient for large-scale deployment. With these features, we apply a linear ranking algorithm to sort alerts according to their relevance to the end-user. We test the effectiveness of our approach on both public data sets and large ones collected from the real world.</p><p>same-paper 2 0.87917966 <a title="85-lda-2" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>Author: Arash Vahdat, Kevin Cannons, Greg Mori, Sangmin Oh, Ilseo Kim</p><p>Abstract: We present a compositional model for video event detection. A video is modeled using a collection of both global and segment-level features and kernel functions are employed for similarity comparisons. The locations of salient, discriminative video segments are treated as a latent variable, allowing the model to explicitly ignore portions of the video that are unimportant for classification. A novel, multiple kernel learning (MKL) latent support vector machine (SVM) is defined, that is used to combine and re-weight multiple feature types in a principled fashion while simultaneously operating within the latent variable framework. The compositional nature of the proposed model allows it to respond directly to the challenges of temporal clutter and intra-class variation, which are prevalent in unconstrained internet videos. Experimental results on the TRECVID Multimedia Event Detection 2011 (MED11) dataset demonstrate the efficacy of the method.</p><p>3 0.87606597 <a title="85-lda-3" href="./iccv-2013-Ensemble_Projection_for_Semi-supervised_Image_Classification.html">142 iccv-2013-Ensemble Projection for Semi-supervised Image Classification</a></p>
<p>Author: Dengxin Dai, Luc Van_Gool</p><p>Abstract: This paper investigates the problem of semi-supervised classification. Unlike previous methods to regularize classifying boundaries with unlabeled data, our method learns a new image representation from all available data (labeled and unlabeled) andperformsplain supervised learning with the new feature. In particular, an ensemble of image prototype sets are sampled automatically from the available data, to represent a rich set of visual categories/attributes. Discriminative functions are then learned on these prototype sets, and image are represented by the concatenation of their projected values onto the prototypes (similarities to them) for further classification. Experiments on four standard datasets show three interesting phenomena: (1) our method consistently outperforms previous methods for semi-supervised image classification; (2) our method lets itself combine well with these methods; and (3) our method works well for self-taught image classification where unlabeled data are not coming from the same distribution as la- beled ones, but rather from a random collection of images.</p><p>4 0.87190717 <a title="85-lda-4" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>Author: Zhongming Jin, Yao Hu, Yue Lin, Debing Zhang, Shiding Lin, Deng Cai, Xuelong Li</p><p>Abstract: Recently, hashing techniques have been widely applied to solve the approximate nearest neighbors search problem in many vision applications. Generally, these hashing approaches generate 2c buckets, where c is the length of the hash code. A good hashing method should satisfy the following two requirements: 1) mapping the nearby data points into the same bucket or nearby (measured by xue long l i opt . ac . cn @ a(a)b(b) the Hamming distance) buckets. 2) all the data points are evenly distributed among all the buckets. In this paper, we propose a novel algorithm named Complementary Projection Hashing (CPH) to find the optimal hashing functions which explicitly considers the above two requirements. Specifically, CPHaims at sequentiallyfinding a series ofhyperplanes (hashing functions) which cross the sparse region of the data. At the same time, the data points are evenly distributed in the hypercubes generated by these hyperplanes. The experiments comparing with the state-of-the-art hashing methods demonstrate the effectiveness of the proposed method.</p><p>5 0.84517157 <a title="85-lda-5" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<p>Author: Tatiana Tommasi, Barbara Caputo</p><p>Abstract: Over the last years, several authors have signaled that state of the art categorization methods fail to perform well when trained and tested on data from different databases. The general consensus in the literature is that this issue, known as domain adaptation and/or dataset bias, is due to a distribution mismatch between data collections. Methods addressing it go from max-margin classifiers to learning how to modify the features and obtain a more robust representation. The large majority of these works use BOW feature descriptors, and learning methods based on imageto-image distance functions. Following the seminal work of [6], in this paper we challenge these two assumptions. We experimentally show that using the NBNN classifier over existing domain adaptation databases achieves always very strong performances. We build on this result, and present an NBNN-based domain adaptation algorithm that learns iteratively a class metric while inducing, for each sample, a large margin separation among classes. To the best of our knowledge, this is the first work casting the domain adaptation problem within the NBNN framework. Experiments show that our method achieves the state of the art, both in the unsupervised and semi-supervised settings.</p><p>6 0.84163487 <a title="85-lda-6" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>7 0.82834142 <a title="85-lda-7" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>8 0.82685786 <a title="85-lda-8" href="./iccv-2013-Enhanced_Continuous_Tabu_Search_for_Parameter_Estimation_in_Multiview_Geometry.html">141 iccv-2013-Enhanced Continuous Tabu Search for Parameter Estimation in Multiview Geometry</a></p>
<p>9 0.8254931 <a title="85-lda-9" href="./iccv-2013-ACTIVE%3A_Activity_Concept_Transitions_in_Video_Event_Classification.html">4 iccv-2013-ACTIVE: Activity Concept Transitions in Video Event Classification</a></p>
<p>10 0.82026708 <a title="85-lda-10" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>11 0.81588006 <a title="85-lda-11" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>12 0.81421673 <a title="85-lda-12" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>13 0.81379402 <a title="85-lda-13" href="./iccv-2013-From_Actemes_to_Action%3A_A_Strongly-Supervised_Representation_for_Detailed_Action_Understanding.html">175 iccv-2013-From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding</a></p>
<p>14 0.81333297 <a title="85-lda-14" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>15 0.81245565 <a title="85-lda-15" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>16 0.81198275 <a title="85-lda-16" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>17 0.81158221 <a title="85-lda-17" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>18 0.81151152 <a title="85-lda-18" href="./iccv-2013-Higher_Order_Matching_for_Consistent_Multiple_Target_Tracking.html">200 iccv-2013-Higher Order Matching for Consistent Multiple Target Tracking</a></p>
<p>19 0.81134617 <a title="85-lda-19" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>20 0.81075567 <a title="85-lda-20" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
