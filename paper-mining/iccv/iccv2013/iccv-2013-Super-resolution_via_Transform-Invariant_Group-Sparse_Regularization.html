<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-408" href="#">iccv2013-408</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</h1>
<br/><p>Source: <a title="iccv-2013-408-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Fernandez-Granda_Super-resolution_via_Transform-Invariant_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Carlos Fernandez-Granda, Emmanuel J. Candès</p><p>Abstract: We present a framework to super-resolve planar regions found in urban scenes and other man-made environments by taking into account their 3D geometry. Such regions have highly structured straight edges, but this prior is challenging to exploit due to deformations induced by the projection onto the imaging plane. Our method factors out such deformations by using recently developed tools based on convex optimization to learn a transform that maps the image to a domain where its gradient has a simple group-sparse structure. This allows to obtain a novel convex regularizer that enforces global consistency constraints between the edges of the image. Computational experiments with real images show that this data-driven approach to the design of regularizers promoting transform-invariant group sparsity is very effective at high super-resolution factors. We view our approach as complementary to most recent superresolution methods, which tend to focus on hallucinating high-frequency textures.</p><p>Reference: <a title="iccv-2013-408-reference" href="../iccv2013_reference/iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Abstract We present a framework to super-resolve planar regions found in urban scenes and other man-made environments by taking into account their 3D geometry. [sent-2, score-0.19]
</p><p>2 Such regions have highly structured straight edges, but this prior is challenging to exploit due to deformations induced by the projection onto the imaging plane. [sent-3, score-0.314]
</p><p>3 Our method factors out such deformations by using recently developed tools based on convex optimization to learn a transform that maps the image to a domain where its gradient has a simple group-sparse structure. [sent-4, score-0.398]
</p><p>4 This allows to obtain a novel convex regularizer that enforces global consistency constraints between the edges of the image. [sent-5, score-0.436]
</p><p>5 Computational experiments with real images show that this data-driven approach to the design of regularizers promoting transform-invariant group sparsity is very effective at high super-resolution factors. [sent-6, score-0.143]
</p><p>6 The aim is to obtain a higher-resolution image by upsampling a single image. [sent-12, score-0.289]
</p><p>7 We can only hope to reconstruct certain very specific structures (see [4] for theoretical results on the super-resolution of pointwise objects) or to hallucinate high-frequency textures that are visually pleasing [1]. [sent-14, score-0.214]
</p><p>8 In this work we consider planar regions taken from 3D scenes that have straight edges aligned in a few main directions, such as the one in Figure 1. [sent-15, score-0.518]
</p><p>9 They are ubiquitous in urban environments and recent large-scale urban 3D mapping efforts (such as the Apple 3D map) make such data readily availEmmanuel J. [sent-17, score-0.096]
</p><p>10 Existing super-resolution techniques can be applied to this class of textures to obtain reasonably good upsampling results up to factors of three or four. [sent-22, score-0.496]
</p><p>11 In this work, we explore the possibility of attaining higher upsampling factors by harnessing such prior knowledge for images with structured edges. [sent-24, score-0.414]
</p><p>12 Unfortunately, the non-uniform blur and deformations induced by the projection of 3D surfaces onto the imaging  plane make it very challenging to exploit prior knowledge about the structure of the data directly. [sent-25, score-0.163]
</p><p>13 In fact, in our quest to super-resolve regions with structured edges we face two fundamental questions that are at the core ofmany problems in computer vision: 1. [sent-26, score-0.301]
</p><p>14 A solution in the case of highly structured 3D scenes is to use recent advances in the recovery of low-rank textures [19], defined as low-rank structures deformed by affine or projective transformations. [sent-36, score-0.335]
</p><p>15 In the case of images with highly structured edges, the sparsity pattern of the gradient tends to follow a lowrank pattern, as illustrated by Figure 1. [sent-38, score-0.184]
</p><p>16 Learning the domain transform that reveals the lowrank structure of the data. [sent-40, score-0.238]
</p><p>17 Under the assumptions that the edge structure of the 3D scene is approximately low rank, this data-driven procedure produces a convex regularizer that allows to super-resolve the image very effectively. [sent-43, score-0.37]
</p><p>18 These example-based methods enforce local consistency to produce sharp-looking edges and are able to hallucinate high-frequency textures very effectively at moderate upsampling factors, especially if prior knowledge about these textures is available [9, 7, 14]. [sent-47, score-0.849]
</p><p>19 However, they are not well adapted to deal with global features, such as the straight edges in Figure 1. [sent-48, score-0.39]
</p><p>20 To recapitulate, our main contribution is a principled methodology for the super-resolution of planar regions with regular structures, which achieves high-quality results at upsampling factors that are problematic for other methods. [sent-51, score-0.538]
</p><p>21 Directional total variation Consider the problem of designing a regularizer adapted to the problem of super-resolving images with sharp edges oriented in a few main directions. [sent-59, score-0.582]
</p><p>22 1 norm of the gradient [11], also known as the total variation (TV) of the image, or related non-convex penalties [10] in order to obtain an estimate with a sparse gradient (see also [15] for a recent approach that takes discretization into account). [sent-61, score-0.19]
</p><p>23 Unfortunately, minimizing the total variation often fails to superresolve two-dimensional edges, even in the case of very simple piecewise-constant images such as the checkerboard shown in Figure 3. [sent-62, score-0.13]
</p><p>24 This failure is largely due to the fact that the regularizer is agnostic to the orientation of the edges in the image, and in particular to the correlation between the orientation of nearby edges. [sent-63, score-0.377]
</p><p>25 This suggests resorting to a regularizer that is better adapted to the high-level structure of the image gradient. [sent-64, score-0.257]
</p><p>26 Let us assume that, as is the case for the checkerboard in Figure 3, we happen to know the directions of most edges in the image. [sent-65, score-0.333]
</p><p>27 In this case, the gradient in the image is not only sparse, but group sparse [18], since its nonzero elements are grouped along horizontal and vertical lines. [sent-66, score-0.246]
</p><p>28 As a result, a more suitable regularizer is the directional total variation (DTV) of the image, defined as  DTV(I) =x? [sent-67, score-0.296]
</p><p>29 It is designed to favor edges that are aligned horizontally and vertically. [sent-83, score-0.337]
</p><p>30 A similar regularizer has been proposed for multiple change-point detection in time-series analysis [3]. [sent-84, score-0.181]
</p><p>31 The top of Figure 3 compares the results of minimizing the TV and DTV cost functions to perform nonblind deblurring of a checkerboard image. [sent-85, score-0.251]
</p><p>32 Transform-invariant regularization  21 Gaussian  The DTV regularizer proposed in Section 2. [sent-93, score-0.276]
</p><p>33 We seldom encounter images where the edges are perfectly aligned horizontally and vertically. [sent-95, score-0.337]
</p><p>34 We consider images such that there exists an affine or projective transform τ for which most of the edges of I◦τ are aligned vertically and horizontally. [sent-97, score-0.536]
</p><p>35 eInd general yiitn igs not straightforward to design a regularizer adapted to such a model. [sent-100, score-0.257]
</p><p>36 The reason is that the gradient is no longer group sparse along a few main directions. [sent-101, score-0.142]
</p><p>37 However, it is group sparse modulo the transform τ. [sent-102, score-0.208]
</p><p>38 2)  ×  a cost function that promotes straight edges in the transformed image, where TI-DTV stands for transforminvariant directional total variation. [sent-104, score-0.47]
</p><p>39 2) is invariant to affine or projective transforms of I, as long as we are able to estimate them a priori. [sent-106, score-0.117]
</p><p>40 As we will see, this allows to factor out significant deformations induced by the camera projection. [sent-107, score-0.148]
</p><p>41 At the bottom of Figure 4 we can see the results of minimizing the TI-DTV cost function to perform nonblind deblurring of a tilted checkerboard. [sent-108, score-0.158]
</p><p>42 Transform-invariant low-rank textures  21  In order to use the cost function proposed in (2. [sent-116, score-0.195]
</p><p>43 2), it is necessary to learn a transform “τ” mapping the image to a domain where its edges are mostly aligned vertically and horizontally. [sent-117, score-0.464]
</p><p>44 We propose doing this by exploiting the fact that images with vertical and horizontal edges tend to be approximately low rank when viewed as a matrix. [sent-118, score-0.35]
</p><p>45 This is obviously the case for the checkerboard in Figure 4, but holds much more broadly. [sent-119, score-0.093]
</p><p>46 The main edges are indeed aligned horizontally and vertically by the transformation associated to the low-rank texture. [sent-121, score-0.416]
</p><p>47 An image with low-rank structured edges might lose its sharpness at low resolutions, but it remains an approximately low-rank texture. [sent-122, score-0.314]
</p><p>48 This is crucial for our interests, since we can consequently use TILT to learn the transform associated to the edge structure and then apply the regularizer proposed in Section 2. [sent-123, score-0.339]
</p><p>49 The authors of [19] develop robust computational tools that allow to extract low-rank textures distorted by affine or projective transformations. [sent-126, score-0.353]
</p><p>50 In essence, TILT al33333381  lows to compute a transform τ from an image I such that I τ = L + E, where L is a low-rank matrix and E ac◦ cIo ◦un τts = =for L sparse d wevheiaretio Lns i sfro am lo twh-er alnowk- mraantkri xm oanddel. [sent-128, score-0.203]
</p><p>51 We refer to [19] for more details on transform-invariant low-rank textures and on how to solve Problem (2. [sent-135, score-0.15]
</p><p>52 3), but we would like to mention that the presence of the sparse term E is of vital importance if we apply TILT to retrieve low-rank textures from low-resolution inputs. [sent-136, score-0.24]
</p><p>53 The reason is that it accounts for artifacts caused by blur and pixelation. [sent-137, score-0.105]
</p><p>54 This is illustrated by Figure 6, which shows the low-rank and sparse components obtained from a blurry image. [sent-138, score-0.147]
</p><p>55 Super-resolution via TI-DTV regularization We finally have all the tools to tackle the problem of super-resolving an image obtained from a 3D scene with  structured edges. [sent-144, score-0.216]
</p><p>56 We propose to leverage a data-driven convex regularizer adapted to the 3D geometry revealed by TILT. [sent-145, score-0.316]
</p><p>57 4)  for a downsampling operator D : RN1×N2 → Rn1×n2 and a blurring ksaemrnpelli nKg ∈p rRatNo1r × DN2 :. [sent-147, score-0.146]
</p><p>58 We apply TILT to the low-resolution image ILR in order to obtain a transform τ which reveals the low-rank edge structure of the image. [sent-150, score-0.201]
</p><p>59 In practice, we upsample ILR using bicubic interpolation before learning the transformation. [sent-151, score-0.39]
</p><p>60 5)  where λ and β are regularization parameters,TV represents the usual total variation operator for discrete images (i. [sent-181, score-0.186]
</p><p>61 the sum of the horizontal and vertical finite differences) and Aτ is a linear operator that maps the image to the domain where we seek to penalize the  directional total variation. [sent-183, score-0.281]
</p><p>62 For color images we apply this procedure to the illuminance channel and upsample the chrominance components Cb and Cr using bicubic interpolation. [sent-184, score-0.362]
</p><p>63 In practice, we use a Gaussian kernel with a standard deviation σ slightly greater than the upsampling factor divided by two. [sent-191, score-0.346]
</p><p>64 This framework consists in casting the problem as a conic program, determining the dual problem and applying a firstorder method, such as accelerated gradient descent, to solve a smoothed version of the dual. [sent-199, score-0.133]
</p><p>65 5) by implementing functions to apply the operator Aτ and to compute the dual of the mixed ? [sent-204, score-0.163]
</p><p>66 This can be done by implementing Aτ with a sparse matrix that samples the transformed image on a new grid using bilinear interpolation. [sent-211, score-0.096]
</p><p>67 5), another option to super-resolve our class of images of interest is to work directly in the rectified domain penalizing the DTV norm. [sent-226, score-0.1]
</p><p>68 More precisely, one can compute the rectified lowresolution image ILτR = ILR ◦ τ and then solve the optimiza-  tion problem  mI˜HτiRn? [sent-227, score-0.135]
</p><p>69 Adequate modeling of the downsampling operator is crucial to super-resolve effectively [12], so it not surprising that the results for this alternative method are not as sharp as those obtained with TI-DTV regularization, as shown in Figure 7. [sent-249, score-0.144]
</p><p>70 6) to super-resolve at an upsampling factor of 8 using geometric information obtained from the low-resolution image. [sent-253, score-0.346]
</p><p>71 We focus on qualitative comparisons, since there is no clear metric capable of quantifying the quality of super-resolved images (for instance, at high upsampling factors the mean-square error can be better for blurry images that do not enhance any features of interest). [sent-263, score-0.482]
</p><p>72 In our first example, we take large planar regions from five images in the SUN database [16],  shown in Figure 8, and compare our method with other representative super-resolution methods developed in the literature. [sent-265, score-0.142]
</p><p>73 Although we apply the algorithms to the whole planar region, zoomed-in areas are shown due to space limitations. [sent-266, score-0.138]
</p><p>74 Interpolation algorithms are represented by bicubic interpolation, which we compute using the Matlab function imresize. [sent-267, score-0.249]
</p><p>75 eter, since TV regularization is usually the method of choice to promote sharp edges in image processing. [sent-289, score-0.343]
</p><p>76 It is actually noted in [17] that exemplar-based methods have difficulties dealing with highly structured textures such as building facades, because it is difficult to build a dictionary that can exhaust edges in all directions and scales. [sent-292, score-0.458]
</p><p>77 Planar regions with structured edges extracted from images in the SUN database [16]. [sent-295, score-0.301]
</p><p>78 33333403  online for this algorithm, which allows to apply an upsampling factor of4. [sent-297, score-0.379]
</p><p>79 For the rest ofthe methods, including ours, we apply an upsampling factor of 8. [sent-298, score-0.379]
</p><p>80 In all cases bicubic interpolation produces images that are very blurry. [sent-300, score-0.379]
</p><p>81 The results for TV regularization are sharper, but they contain significant artifacts which make edges appear wobbly instead of straight. [sent-301, score-0.359]
</p><p>82 Despite its reduced upsampling factor, the sparse-coding algorithm is also not capable of super-resolving edges effectively and its results are only slightly better than those of bicubic interpolation. [sent-302, score-0.78]
</p><p>83 In contrast, TI-DTV regularization produces clear straight edges  that correspond to the global geometry of the planar surface, yielding upsampled images that are significantly sharper than the rest. [sent-303, score-0.669]
</p><p>84 As expected the edges align mostly horizontally and vertically following the low-rank structure. [sent-305, score-0.354]
</p><p>85 For the top example in Figure 12, where the lowresolution image has size 120x136, the running time required by TI-DTV regularization is of 123. [sent-306, score-0.175]
</p><p>86 Super-resolution of text Text follows the model that we consider to some extent, since most letters contain horizontal or vertical edges. [sent-313, score-0.104]
</p><p>87 As a result, TI-DTV regularization is capable of effectively super-resolving letter or characters printed on distorted surfaces. [sent-314, score-0.174]
</p><p>88 To demonstrate this, Figure 11 shows four such examples and compares the results obtained from bicubic interpolation and our method. [sent-315, score-0.342]
</p><p>89 TI-DTV regularization is clearly superior in all cases, despite the fact that some letters have edges that are not aligned with the low-rank structure (see the following section). [sent-317, score-0.353]
</p><p>90 Limitations As we have made clear throughout  this paper, our  method is geared to the super-resolution of planar surfaces that are approximately low-rank and have straight edges that are oriented following the low-rank structure. [sent-320, score-0.504]
</p><p>91 If these conditions are not met, the algorithm might produce artifacts in regions that resemble horizontal or vertical edges. [sent-321, score-0.209]
</p><p>92 In any case, the method often degrades gracefully in regions that do not have straight edges with the right orientation. [sent-324, score-0.351]
</p><p>93 In Figure 10, for example, TI-DTV does not produce any artifacts around the arc and at the same time super-resolves sharply the edges in the rest of the image. [sent-325, score-0.298]
</p><p>94 In comparison, TV makes the arc look sharper but generates obvious artifacts. [sent-326, score-0.099]
</p><p>95 Bicubic  TV  TI-DTV  that does not completely conform to the transform-invariant lowrank model using bicubic interpolation, TV regularization and TIDTV regularization. [sent-329, score-0.412]
</p><p>96 Conclusion and extensions We believe that developing tools capable of constraining non-local image structure is a crucial step towards achieving high-quality super-resolution at large upsampling factors. [sent-332, score-0.388]
</p><p>97 Our contributions are the introduction of a principled methodology in which such constraints are imposed through data-driven non-parametric regularizers and a robust implementation of this methodology for a particular class of images, which yields state-of-the-art results. [sent-333, score-0.161]
</p><p>98 Templates for convex cone problems with applications to sparse signal recovery. [sent-362, score-0.116]
</p><p>99 Results from super-resolving the images in Figure 8 using bicubic interpolation, total-variation regularization, sparse coding [17] and our proposed algorithm. [sent-495, score-0.306]
</p><p>100 The upsampling factor was 4 for sparse coding and 8 for the rest of the methods. [sent-496, score-0.403]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dtv', 0.327), ('upsampling', 0.289), ('ilr', 0.254), ('bicubic', 0.249), ('tv', 0.205), ('tilt', 0.198), ('edges', 0.196), ('regularizer', 0.181), ('tfocs', 0.181), ('textures', 0.15), ('straight', 0.118), ('srf', 0.109), ('planar', 0.105), ('hr', 0.105), ('regularization', 0.095), ('interpolation', 0.093), ('checkerboard', 0.093), ('blurry', 0.09), ('cand', 0.086), ('transform', 0.082), ('lowresolution', 0.08), ('horizontally', 0.079), ('vertically', 0.079), ('directional', 0.078), ('adapted', 0.076), ('projective', 0.074), ('fgranda', 0.073), ('hirn', 0.073), ('nonblind', 0.073), ('artifacts', 0.068), ('structured', 0.068), ('lowrank', 0.068), ('sharper', 0.065), ('hallucinate', 0.064), ('aligned', 0.062), ('regularizers', 0.061), ('convex', 0.059), ('sparse', 0.057), ('factors', 0.057), ('factor', 0.057), ('rectified', 0.055), ('operator', 0.054), ('blurring', 0.054), ('deformations', 0.054), ('vertical', 0.054), ('upsampled', 0.053), ('tools', 0.053), ('sharp', 0.052), ('approximately', 0.05), ('methodology', 0.05), ('horizontal', 0.05), ('urban', 0.048), ('upsample', 0.048), ('conic', 0.048), ('ford', 0.048), ('gradient', 0.048), ('capable', 0.046), ('cost', 0.045), ('domain', 0.045), ('promoting', 0.045), ('directions', 0.044), ('superresolution', 0.044), ('affine', 0.043), ('reveals', 0.043), ('edge', 0.043), ('sun', 0.042), ('deblurring', 0.04), ('designing', 0.04), ('implementing', 0.039), ('downsampling', 0.038), ('mi', 0.037), ('regions', 0.037), ('variation', 0.037), ('produces', 0.037), ('blur', 0.037), ('dual', 0.037), ('induced', 0.037), ('group', 0.037), ('super', 0.036), ('stanford', 0.035), ('surfaces', 0.035), ('mathematical', 0.035), ('arc', 0.034), ('apply', 0.033), ('distorted', 0.033), ('stands', 0.033), ('solvers', 0.033), ('tog', 0.032), ('blurred', 0.032), ('hallucinating', 0.032), ('kme', 0.032), ('sfro', 0.032), ('chrominance', 0.032), ('mraantkri', 0.032), ('otfo', 0.032), ('modulo', 0.032), ('anford', 0.032), ('truck', 0.032), ('lns', 0.032), ('tracted', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="408-tfidf-1" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>Author: Carlos Fernandez-Granda, Emmanuel J. Candès</p><p>Abstract: We present a framework to super-resolve planar regions found in urban scenes and other man-made environments by taking into account their 3D geometry. Such regions have highly structured straight edges, but this prior is challenging to exploit due to deformations induced by the projection onto the imaging plane. Our method factors out such deformations by using recently developed tools based on convex optimization to learn a transform that maps the image to a domain where its gradient has a simple group-sparse structure. This allows to obtain a novel convex regularizer that enforces global consistency constraints between the edges of the image. Computational experiments with real images show that this data-driven approach to the design of regularizers promoting transform-invariant group sparsity is very effective at high super-resolution factors. We view our approach as complementary to most recent superresolution methods, which tend to focus on hallucinating high-frequency textures.</p><p>2 0.20754518 <a title="408-tfidf-2" href="./iccv-2013-Image_Guided_Depth_Upsampling_Using_Anisotropic_Total_Generalized_Variation.html">209 iccv-2013-Image Guided Depth Upsampling Using Anisotropic Total Generalized Variation</a></p>
<p>Author: David Ferstl, Christian Reinbacher, Rene Ranftl, Matthias Ruether, Horst Bischof</p><p>Abstract: In this work we present a novel method for the challenging problem of depth image upsampling. Modern depth cameras such as Kinect or Time of Flight cameras deliver dense, high quality depth measurements but are limited in their lateral resolution. To overcome this limitation we formulate a convex optimization problem using higher order regularization for depth image upsampling. In this optimization an anisotropic diffusion tensor, calculated from a high resolution intensity image, is used to guide the upsampling. We derive a numerical algorithm based on a primaldual formulation that is efficiently parallelized and runs at multiple frames per second. We show that this novel upsampling clearly outperforms state of the art approaches in terms of speed and accuracy on the widely used Middlebury 2007 datasets. Furthermore, we introduce novel datasets with highly accurate groundtruth, which, for the first time, enable to benchmark depth upsampling methods using real sensor data.</p><p>3 0.20205586 <a title="408-tfidf-3" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>Author: Chih-Yuan Yang, Ming-Hsuan Yang</p><p>Abstract: The goal of single-image super-resolution is to generate a high-quality high-resolution image based on a given low-resolution input. It is an ill-posed problem which requires exemplars or priors to better reconstruct the missing high-resolution image details. In this paper, we propose to split the feature space into numerous subspaces and collect exemplars to learn priors for each subspace, thereby creating effective mapping functions. The use of split input space facilitates both feasibility of using simple functionsfor super-resolution, and efficiency ofgenerating highresolution results. High-quality high-resolution images are reconstructed based on the effective learned priors. Experimental results demonstrate that theproposed algorithmperforms efficiently and effectively over state-of-the-art methods.</p><p>4 0.14714007 <a title="408-tfidf-4" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>Author: Netalee Efrat, Daniel Glasner, Alexander Apartsin, Boaz Nadler, Anat Levin</p><p>Abstract: Over the past decade, single image Super-Resolution (SR) research has focused on developing sophisticated image priors, leading to significant advances. Estimating and incorporating the blur model, that relates the high-res and low-res images, has received much less attention, however. In particular, the reconstruction constraint, namely that the blurred and downsampled high-res output should approximately equal the low-res input image, has been either ignored or applied with default fixed blur models. In this work, we examine the relative importance ofthe imageprior and the reconstruction constraint. First, we show that an accurate reconstruction constraint combined with a simple gradient regularization achieves SR results almost as good as those of state-of-the-art algorithms with sophisticated image priors. Second, we study both empirically and theoretically the sensitivity of SR algorithms to the blur model assumed in the reconstruction constraint. We find that an accurate blur model is more important than a sophisticated image prior. Finally, using real camera data, we demonstrate that the default blur models of various SR algorithms may differ from the camera blur, typically leading to over- smoothed results. Our findings highlight the importance of accurately estimating camera blur in reconstructing raw low- res images acquired by an actual camera.</p><p>5 0.11577915 <a title="408-tfidf-5" href="./iccv-2013-Deblurring_by_Example_Using_Dense_Correspondence.html">103 iccv-2013-Deblurring by Example Using Dense Correspondence</a></p>
<p>Author: Yoav Hacohen, Eli Shechtman, Dani Lischinski</p><p>Abstract: This paper presents a new method for deblurring photos using a sharp reference example that contains some shared content with the blurry photo. Most previous deblurring methods that exploit information from other photos require an accurately registered photo of the same static scene. In contrast, our method aims to exploit reference images where the shared content may have undergone substantial photometric and non-rigid geometric transformations, as these are the kind of reference images most likely to be found in personal photo albums. Our approach builds upon a recent method for examplebased deblurring using non-rigid dense correspondence (NRDC) [11] and extends it in two ways. First, we suggest exploiting information from the reference image not only for blur kernel estimation, but also as a powerful local prior for the non-blind deconvolution step. Second, we introduce a simple yet robust technique for spatially varying blur estimation, rather than assuming spatially uniform blur. Unlike the aboveprevious method, which hasproven successful only with simple deblurring scenarios, we demonstrate that our method succeeds on a variety of real-world examples. We provide quantitative and qualitative evaluation of our method and show that it outperforms the state-of-the-art.</p><p>6 0.10495016 <a title="408-tfidf-6" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>7 0.10410433 <a title="408-tfidf-7" href="./iccv-2013-A_Joint_Intensity_and_Depth_Co-sparse_Analysis_Model_for_Depth_Map_Super-resolution.html">18 iccv-2013-A Joint Intensity and Depth Co-sparse Analysis Model for Depth Map Super-resolution</a></p>
<p>8 0.093795024 <a title="408-tfidf-8" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>9 0.08799386 <a title="408-tfidf-9" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>10 0.08781863 <a title="408-tfidf-10" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>11 0.086327121 <a title="408-tfidf-11" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>12 0.082793124 <a title="408-tfidf-12" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>13 0.080072112 <a title="408-tfidf-13" href="./iccv-2013-Total_Variation_Regularization_for_Functions_with_Values_in_a_Manifold.html">421 iccv-2013-Total Variation Regularization for Functions with Values in a Manifold</a></p>
<p>14 0.077681489 <a title="408-tfidf-14" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>15 0.076532193 <a title="408-tfidf-15" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>16 0.074199691 <a title="408-tfidf-16" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<p>17 0.072882876 <a title="408-tfidf-17" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>18 0.071810134 <a title="408-tfidf-18" href="./iccv-2013-Toward_Guaranteed_Illumination_Models_for_Non-convex_Objects.html">422 iccv-2013-Toward Guaranteed Illumination Models for Non-convex Objects</a></p>
<p>19 0.070489012 <a title="408-tfidf-19" href="./iccv-2013-Nonparametric_Blind_Super-resolution.html">293 iccv-2013-Nonparametric Blind Super-resolution</a></p>
<p>20 0.070442796 <a title="408-tfidf-20" href="./iccv-2013-Modeling_the_Calibration_Pipeline_of_the_Lytro_Camera_for_High_Quality_Light-Field_Image_Reconstruction.html">271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.181), (1, -0.075), (2, -0.061), (3, -0.017), (4, -0.097), (5, 0.021), (6, -0.021), (7, -0.086), (8, 0.028), (9, -0.101), (10, -0.001), (11, -0.091), (12, 0.04), (13, -0.069), (14, -0.017), (15, 0.046), (16, -0.017), (17, -0.034), (18, -0.028), (19, -0.009), (20, 0.003), (21, 0.065), (22, -0.016), (23, -0.063), (24, 0.015), (25, 0.063), (26, 0.048), (27, -0.062), (28, -0.055), (29, -0.069), (30, -0.014), (31, -0.018), (32, 0.004), (33, 0.064), (34, -0.036), (35, -0.023), (36, -0.075), (37, -0.021), (38, 0.042), (39, -0.018), (40, 0.124), (41, -0.083), (42, 0.042), (43, 0.058), (44, -0.049), (45, 0.034), (46, -0.094), (47, 0.101), (48, -0.026), (49, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92026305 <a title="408-lsi-1" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>Author: Carlos Fernandez-Granda, Emmanuel J. Candès</p><p>Abstract: We present a framework to super-resolve planar regions found in urban scenes and other man-made environments by taking into account their 3D geometry. Such regions have highly structured straight edges, but this prior is challenging to exploit due to deformations induced by the projection onto the imaging plane. Our method factors out such deformations by using recently developed tools based on convex optimization to learn a transform that maps the image to a domain where its gradient has a simple group-sparse structure. This allows to obtain a novel convex regularizer that enforces global consistency constraints between the edges of the image. Computational experiments with real images show that this data-driven approach to the design of regularizers promoting transform-invariant group sparsity is very effective at high super-resolution factors. We view our approach as complementary to most recent superresolution methods, which tend to focus on hallucinating high-frequency textures.</p><p>2 0.73483878 <a title="408-lsi-2" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>Author: Chih-Yuan Yang, Ming-Hsuan Yang</p><p>Abstract: The goal of single-image super-resolution is to generate a high-quality high-resolution image based on a given low-resolution input. It is an ill-posed problem which requires exemplars or priors to better reconstruct the missing high-resolution image details. In this paper, we propose to split the feature space into numerous subspaces and collect exemplars to learn priors for each subspace, thereby creating effective mapping functions. The use of split input space facilitates both feasibility of using simple functionsfor super-resolution, and efficiency ofgenerating highresolution results. High-quality high-resolution images are reconstructed based on the effective learned priors. Experimental results demonstrate that theproposed algorithmperforms efficiently and effectively over state-of-the-art methods.</p><p>3 0.69324982 <a title="408-lsi-3" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>Author: Netalee Efrat, Daniel Glasner, Alexander Apartsin, Boaz Nadler, Anat Levin</p><p>Abstract: Over the past decade, single image Super-Resolution (SR) research has focused on developing sophisticated image priors, leading to significant advances. Estimating and incorporating the blur model, that relates the high-res and low-res images, has received much less attention, however. In particular, the reconstruction constraint, namely that the blurred and downsampled high-res output should approximately equal the low-res input image, has been either ignored or applied with default fixed blur models. In this work, we examine the relative importance ofthe imageprior and the reconstruction constraint. First, we show that an accurate reconstruction constraint combined with a simple gradient regularization achieves SR results almost as good as those of state-of-the-art algorithms with sophisticated image priors. Second, we study both empirically and theoretically the sensitivity of SR algorithms to the blur model assumed in the reconstruction constraint. We find that an accurate blur model is more important than a sophisticated image prior. Finally, using real camera data, we demonstrate that the default blur models of various SR algorithms may differ from the camera blur, typically leading to over- smoothed results. Our findings highlight the importance of accurately estimating camera blur in reconstructing raw low- res images acquired by an actual camera.</p><p>4 0.63215178 <a title="408-lsi-4" href="./iccv-2013-SGTD%3A_Structure_Gradient_and_Texture_Decorrelating_Regularization_for_Image_Decomposition.html">364 iccv-2013-SGTD: Structure Gradient and Texture Decorrelating Regularization for Image Decomposition</a></p>
<p>Author: Qiegen Liu, Jianbo Liu, Pei Dong, Dong Liang</p><p>Abstract: This paper presents a novel structure gradient and texture decorrelating regularization (SGTD) for image decomposition. The motivation of the idea is under the assumption that the structure gradient and texture components should be properly decorrelated for a successful decomposition. The proposed model consists of the data fidelity term, total variation regularization and the SGTD regularization. An augmented Lagrangian method is proposed to address this optimization issue, by first transforming the unconstrained problem to an equivalent constrained problem and then applying an alternating direction method to iteratively solve the subproblems. Experimental results demonstrate that the proposed method presents better or comparable performance as state-of-the-art methods do.</p><p>5 0.62824994 <a title="408-lsi-5" href="./iccv-2013-Efficient_Image_Dehazing_with_Boundary_Constraint_and_Contextual_Regularization.html">135 iccv-2013-Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</a></p>
<p>Author: Gaofeng Meng, Ying Wang, Jiangyong Duan, Shiming Xiang, Chunhong Pan</p><p>Abstract: unkown-abstract</p><p>6 0.62338281 <a title="408-lsi-6" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>7 0.57995898 <a title="408-lsi-7" href="./iccv-2013-Detecting_Irregular_Curvilinear_Structures_in_Gray_Scale_and_Color_Imagery_Using_Multi-directional_Oriented_Flux.html">112 iccv-2013-Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-directional Oriented Flux</a></p>
<p>8 0.57904702 <a title="408-lsi-8" href="./iccv-2013-Nonparametric_Blind_Super-resolution.html">293 iccv-2013-Nonparametric Blind Super-resolution</a></p>
<p>9 0.56557637 <a title="408-lsi-9" href="./iccv-2013-Modeling_the_Calibration_Pipeline_of_the_Lytro_Camera_for_High_Quality_Light-Field_Image_Reconstruction.html">271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</a></p>
<p>10 0.56000209 <a title="408-lsi-10" href="./iccv-2013-Non-convex_P-Norm_Projection_for_Robust_Sparsity.html">292 iccv-2013-Non-convex P-Norm Projection for Robust Sparsity</a></p>
<p>11 0.55955738 <a title="408-lsi-11" href="./iccv-2013-A_Joint_Intensity_and_Depth_Co-sparse_Analysis_Model_for_Depth_Map_Super-resolution.html">18 iccv-2013-A Joint Intensity and Depth Co-sparse Analysis Model for Depth Map Super-resolution</a></p>
<p>12 0.54862857 <a title="408-lsi-12" href="./iccv-2013-Automatic_Kronecker_Product_Model_Based_Detection_of_Repeated_Patterns_in_2D_Urban_Images.html">55 iccv-2013-Automatic Kronecker Product Model Based Detection of Repeated Patterns in 2D Urban Images</a></p>
<p>13 0.5483734 <a title="408-lsi-13" href="./iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding.html">14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</a></p>
<p>14 0.54496843 <a title="408-lsi-14" href="./iccv-2013-Target-Driven_Moire_Pattern_Synthesis_by_Phase_Modulation.html">413 iccv-2013-Target-Driven Moire Pattern Synthesis by Phase Modulation</a></p>
<p>15 0.51456338 <a title="408-lsi-15" href="./iccv-2013-Example-Based_Facade_Texture_Synthesis.html">148 iccv-2013-Example-Based Facade Texture Synthesis</a></p>
<p>16 0.51217127 <a title="408-lsi-16" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>17 0.51074284 <a title="408-lsi-17" href="./iccv-2013-Coupled_Dictionary_and_Feature_Space_Learning_with_Applications_to_Cross-Domain_Image_Synthesis_and_Recognition.html">96 iccv-2013-Coupled Dictionary and Feature Space Learning with Applications to Cross-Domain Image Synthesis and Recognition</a></p>
<p>18 0.50521398 <a title="408-lsi-18" href="./iccv-2013-Perceptual_Fidelity_Aware_Mean_Squared_Error.html">312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</a></p>
<p>19 0.49678025 <a title="408-lsi-19" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<p>20 0.49432355 <a title="408-lsi-20" href="./iccv-2013-Toward_Guaranteed_Illumination_Models_for_Non-convex_Objects.html">422 iccv-2013-Toward Guaranteed Illumination Models for Non-convex Objects</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.037), (7, 0.013), (26, 0.065), (31, 0.506), (42, 0.096), (64, 0.024), (73, 0.029), (89, 0.155)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90600336 <a title="408-lda-1" href="./iccv-2013-Recognizing_Text_with_Perspective_Distortion_in_Natural_Scenes.html">345 iccv-2013-Recognizing Text with Perspective Distortion in Natural Scenes</a></p>
<p>Author: Trung Quy Phan, Palaiahnakote Shivakumara, Shangxuan Tian, Chew Lim Tan</p><p>Abstract: This paper presents an approach to text recognition in natural scene images. Unlike most existing works which assume that texts are horizontal and frontal parallel to the image plane, our method is able to recognize perspective texts of arbitrary orientations. For individual character recognition, we adopt a bag-of-keypoints approach, in which Scale Invariant Feature Transform (SIFT) descriptors are extracted densely and quantized using a pre-trained vocabulary. Following [1, 2], the context information is utilized through lexicons. We formulate word recognition as finding the optimal alignment between the set of characters and the list of lexicon words. Furthermore, we introduce a new dataset called StreetViewText-Perspective, which contains texts in street images with a great variety of viewpoints. Experimental results on public datasets and the proposed dataset show that our method significantly outperforms the state-of-the-art on perspective texts of arbitrary orientations.</p><p>same-paper 2 0.87411302 <a title="408-lda-2" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>Author: Carlos Fernandez-Granda, Emmanuel J. Candès</p><p>Abstract: We present a framework to super-resolve planar regions found in urban scenes and other man-made environments by taking into account their 3D geometry. Such regions have highly structured straight edges, but this prior is challenging to exploit due to deformations induced by the projection onto the imaging plane. Our method factors out such deformations by using recently developed tools based on convex optimization to learn a transform that maps the image to a domain where its gradient has a simple group-sparse structure. This allows to obtain a novel convex regularizer that enforces global consistency constraints between the edges of the image. Computational experiments with real images show that this data-driven approach to the design of regularizers promoting transform-invariant group sparsity is very effective at high super-resolution factors. We view our approach as complementary to most recent superresolution methods, which tend to focus on hallucinating high-frequency textures.</p><p>3 0.84167439 <a title="408-lda-3" href="./iccv-2013-Characterizing_Layouts_of_Outdoor_Scenes_Using_Spatial_Topic_Processes.html">72 iccv-2013-Characterizing Layouts of Outdoor Scenes Using Spatial Topic Processes</a></p>
<p>Author: Dahua Lin, Jianxiong Xiao</p><p>Abstract: In this paper, we develop a generative model to describe the layouts of outdoor scenes the spatial configuration of regions. Specifically, the layout of an image is represented as a composite of regions, each associated with a semantic topic. At the heart of this model is a novel stochastic process called Spatial Topic Process, which generates a spatial map of topics from a set of coupled Gaussian processes, thus allowing the distributions of topics to vary continuously across the image plane. A key aspect that distinguishes this model from previous ones consists in its capability of capturing dependencies across both locations and topics while allowing substantial variations in the layouts. We demonstrate the practical utility of the proposed model by testing it on scene classification, semantic segmentation, and layout hallucination. –</p><p>4 0.82430422 <a title="408-lda-4" href="./iccv-2013-Robust_Matrix_Factorization_with_Unknown_Noise.html">357 iccv-2013-Robust Matrix Factorization with Unknown Noise</a></p>
<p>Author: Deyu Meng, Fernando De_La_Torre</p><p>Abstract: Many problems in computer vision can be posed as recovering a low-dimensional subspace from highdimensional visual data. Factorization approaches to lowrank subspace estimation minimize a loss function between an observed measurement matrix and a bilinear factorization. Most popular loss functions include the L2 and L1 losses. L2 is optimal for Gaussian noise, while L1 is for Laplacian distributed noise. However, real data is often corrupted by an unknown noise distribution, which is unlikely to be purely Gaussian or Laplacian. To address this problem, this paper proposes a low-rank matrix factorization problem with a Mixture of Gaussians (MoG) noise model. The MoG model is a universal approximator for any continuous distribution, and hence is able to model a wider range of noise distributions. The parameters of the MoG model can be estimated with a maximum likelihood method, while the subspace is computed with standard approaches. We illustrate the benefits of our approach in extensive syn- thetic and real-world experiments including structure from motion, face modeling and background subtraction.</p><p>5 0.81599569 <a title="408-lda-5" href="./iccv-2013-Action_Recognition_with_Actons.html">38 iccv-2013-Action Recognition with Actons</a></p>
<p>Author: Jun Zhu, Baoyuan Wang, Xiaokang Yang, Wenjun Zhang, Zhuowen Tu</p><p>Abstract: With the improved accessibility to an exploding amount of video data and growing demands in a wide range of video analysis applications, video-based action recognition/classification becomes an increasingly important task in computer vision. In this paper, we propose a two-layer structure for action recognition to automatically exploit a mid-level “acton ” representation. The weakly-supervised actons are learned via a new max-margin multi-channel multiple instance learning framework, which can capture multiple mid-level action concepts simultaneously. The learned actons (with no requirement for detailed manual annotations) observe theproperties ofbeing compact, informative, discriminative, and easy to scale. The experimental results demonstrate the effectiveness ofapplying the learned actons in our two-layer structure, and show the state-ofthe-art recognition performance on two challenging action datasets, i.e., Youtube and HMDB51.</p><p>6 0.7234112 <a title="408-lda-6" href="./iccv-2013-Motion-Aware_KNN_Laplacian_for_Video_Matting.html">275 iccv-2013-Motion-Aware KNN Laplacian for Video Matting</a></p>
<p>7 0.68209589 <a title="408-lda-7" href="./iccv-2013-Modeling_Occlusion_by_Discriminative_AND-OR_Structures.html">269 iccv-2013-Modeling Occlusion by Discriminative AND-OR Structures</a></p>
<p>8 0.62741601 <a title="408-lda-8" href="./iccv-2013-Class-Specific_Simplex-Latent_Dirichlet_Allocation_for_Image_Classification.html">73 iccv-2013-Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification</a></p>
<p>9 0.6215151 <a title="408-lda-9" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>10 0.59562576 <a title="408-lda-10" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>11 0.58926082 <a title="408-lda-11" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<p>12 0.58922148 <a title="408-lda-12" href="./iccv-2013-PhotoOCR%3A_Reading_Text_in_Uncontrolled_Conditions.html">315 iccv-2013-PhotoOCR: Reading Text in Uncontrolled Conditions</a></p>
<p>13 0.56395936 <a title="408-lda-13" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>14 0.55817902 <a title="408-lda-14" href="./iccv-2013-Text_Localization_in_Natural_Images_Using_Stroke_Feature_Transform_and_Text_Covariance_Descriptors.html">415 iccv-2013-Text Localization in Natural Images Using Stroke Feature Transform and Text Covariance Descriptors</a></p>
<p>15 0.55433846 <a title="408-lda-15" href="./iccv-2013-Fluttering_Pattern_Generation_Using_Modified_Legendre_Sequence_for_Coded_Exposure_Imaging.html">173 iccv-2013-Fluttering Pattern Generation Using Modified Legendre Sequence for Coded Exposure Imaging</a></p>
<p>16 0.55021191 <a title="408-lda-16" href="./iccv-2013-A_Learning-Based_Approach_to_Reduce_JPEG_Artifacts_in_Image_Matting.html">19 iccv-2013-A Learning-Based Approach to Reduce JPEG Artifacts in Image Matting</a></p>
<p>17 0.54486406 <a title="408-lda-17" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<p>18 0.54248315 <a title="408-lda-18" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<p>19 0.54148388 <a title="408-lda-19" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>20 0.53720129 <a title="408-lda-20" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
