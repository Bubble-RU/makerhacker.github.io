<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>280 iccv-2013-Multi-view 3D Reconstruction from Uncalibrated Radially-Symmetric Cameras</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-280" href="#">iccv2013-280</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>280 iccv-2013-Multi-view 3D Reconstruction from Uncalibrated Radially-Symmetric Cameras</h1>
<br/><p>Source: <a title="iccv-2013-280-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Kim_Multi-view_3D_Reconstruction_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Jae-Hak Kim, Yuchao Dai, Hongdong Li, Xin Du, Jonghyuk Kim</p><p>Abstract: We present a new multi-view 3D Euclidean reconstruction method for arbitrary uncalibrated radially-symmetric cameras, which needs no calibration or any camera model parameters other than radial symmetry. It is built on the radial 1D camera model [25], a unified mathematical abstraction to different types of radially-symmetric cameras. We formulate the problem of multi-view reconstruction for radial 1D cameras as a matrix rank minimization problem. Efficient implementation based on alternating direction continuation is proposed to handle scalability issue for real-world applications. Our method applies to a wide range of omnidirectional cameras including both dioptric and catadioptric (central and non-central) cameras. Additionally, our method deals with complete and incomplete measurements under a unified framework elegantly. Experiments on both synthetic and real images from various types of cameras validate the superior performance of our new method, in terms of numerical accuracy and robustness.</p><p>Reference: <a title="iccv-2013-280-reference" href="../iccv2013_reference/iccv-2013-Multi-view_3D_Reconstruction_from_Uncalibrated_Radially-Symmetric_Cameras_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 It is built on the radial 1D camera model [25], a unified mathematical abstraction to different types of radially-symmetric cameras. [sent-2, score-0.564]
</p><p>2 We formulate the problem of multi-view reconstruction for radial 1D cameras as a matrix rank minimization problem. [sent-3, score-0.836]
</p><p>3 Our method applies to a wide range of omnidirectional cameras including both dioptric and catadioptric (central and non-central) cameras. [sent-5, score-0.905]
</p><p>4 Introduction  Having a wide field of view, omnidirectional cameras can be used to reconstruct broad scenes from few views, thus have been widely deployed to applications such as surveillance, robot navigation and 3D modeling of street scene. [sent-9, score-0.436]
</p><p>5 It is highly desired to have a unified and efficient reconstruction method for omnidirectional cameras. [sent-12, score-0.479]
</p><p>6 This paper proposes a new multi-view 3D Euclidean reconstruction method for generic types of uncalibrated radially-symmetric cameras. [sent-13, score-0.264]
</p><p>7 It is built on the radial 1D camera model originally developed by Thirthala and Pollefeys [25]. [sent-14, score-0.471]
</p><p>8 Our method recovers 3D structure via matrix rank min-  imization from general types of uncalibrated radially-symmetric cameras e. [sent-16, score-0.374]
</p><p>9 fisheye lens cameras, concave shape mirror based catadioptric cameras, noncentral cameras including spherical mirror or any radially-symmetric mirror shape based cameras, and multiple relflection surfaces based Sony RPU camera. [sent-18, score-1.221]
</p><p>10 First, we formulate the problem of multiview reconstruction for radial 1D cameras as a matrix rank minimization problem, and solve it through convex optimization (and semi-definite programming in particular followed by an efficient alternating direction continuation method). [sent-28, score-0.951]
</p><p>11 Second, the multi-view reconstruction is upgraded from projective to Euclidean by exploiting the internal constraints. [sent-29, score-0.324]
</p><p>12 Modeling radially-symmetric cameras Due to the various types of omnidirectional camera de-  sign and construction, e. [sent-33, score-0.583]
</p><p>13 dioptric (lens-based) or catadioptric (mirror-lens system), central or non-central, most 3D reconstruction methods are specially designed for one 11889966  or a few particular types, and thus are not universally applicable to others. [sent-35, score-0.723]
</p><p>14 Meanwhile, novel types of omnidirectional optical devices are emerging too (e. [sent-36, score-0.25]
</p><p>15 1), which also calls for a unified 3D reconstruction procedure. [sent-39, score-0.229]
</p><p>16 There indeed exists a unified mathematical model to represent various types of omnidirectional cameras, so-called generalized camera model (GCM) [19, 22, 11], which models cameras as unconstrained sets of projection rays. [sent-40, score-0.681]
</p><p>17 However, the GCM does not suggest a unified way to handle 3D reconstruction from uncalibrated cameras. [sent-41, score-0.308]
</p><p>18 The radial 1D camera model studied in this paper is applicable to both central and non-central cases, and it is in fact a special case (symmetry version) of the GCM (i. [sent-43, score-0.54]
</p><p>19 In practice, very often omnidirectional cameras will manifest a certain type of symmetry, where radial symmetry being the dominant form. [sent-46, score-0.789]
</p><p>20 This is reasonable, because it is convenient to design, to manufacture and to use, an omnidirectional camera with a radially-symmetric field of view. [sent-47, score-0.397]
</p><p>21 There are two major classes of omnidirectional cam-  eras: dioptric and catadioptric. [sent-48, score-0.333]
</p><p>22 The former one includes a wide-angle lens (fish-eye lens), and the latter one often consists of a perspective camera plus a curved mirror. [sent-49, score-0.29]
</p><p>23 Most of the commonly used omnidirectional cameras belong to this class, and this is the main focus of this paper. [sent-51, score-0.436]
</p><p>24 To express radially-symmetric cameras in a unified way, Thirthala and Pollefeys [25] proposed the novel concept of “radial 1D camera” that maps a 3D point to a radial line. [sent-52, score-0.554]
</p><p>25 The axial camera is an abstraction of stereo systems, non-central catadioptric cameras and pushbroom cameras. [sent-57, score-0.846]
</p><p>26 Related works To recover structure and motion for cameras with a wide circular field of view, Miˇ cuˇ s ı´k and Pajdla [17] estimated epipolar geometry of radially-symmetric cameras by solving a polynomial eigenvalue problem. [sent-60, score-0.401]
</p><p>27 Lhuillier [12] presented fully automatic methods for estimating  scene structure and camera motion from an image sequence acquired by a catadioptric system, where bundle adjustment is applied to both central and non-central models. [sent-62, score-0.651]
</p><p>28 When a specific camera model is available, 3D reconstruction can be achieved in a tailor-made style. [sent-63, score-0.332]
</p><p>29 For catadioptric cameras, with the information of the mirror (model and parameters), calibration and reconstruction can be done through computing the forward and backward projection. [sent-64, score-0.765]
</p><p>30 Geyer and Daniilidis [8] introduced the circle space representation for an image of points and lines in central catadioptric cameras, from which the epipolar constraint and catadioptric fundamental matrix are derived. [sent-65, score-0.917]
</p><p>31 Miˇ cuˇ s ı´k and Pajdla [16] developed accurate non-central and suitable approximate central models for specific mirrors, thus allowing to build a 3D metric reconstruction from two uncalibrated non-central catadioptric images. [sent-67, score-0.719]
</p><p>32 The analytical forward projection leads to 3D reconstruction via bundle adjustment. [sent-70, score-0.329]
</p><p>33 The plane based calibration for radially-symmetric cameras have also been studied in [20] and [24]. [sent-74, score-0.265]
</p><p>34 Hartley  ×  and Kang [9] proposed a parameter-free method to simultaneously calibrate the radial distortion function of a camera and other internal calibration parameters by using a planar calibration grid. [sent-76, score-0.733]
</p><p>35 However, their model is restricted to central cameras and assumes a known calibration grid. [sent-77, score-0.334]
</p><p>36 When the distortion model is available, radial distortion calibration and multi-view geometry can be solved with algebraic minimization methods such as [7]. [sent-78, score-0.697]
</p><p>37 Radial 1D camera model  The radial 1D camera model [25] (Fig. [sent-80, score-0.618]
</p><p>38 2) is a much more general mathematical abstraction, which encompasses most of the fisheye cameras, central and non-central catadioptric cameras, perspective and affine cameras. [sent-81, score-0.647]
</p><p>39 Definition: The radial 1D camera expresses the mapping of a 3D point in P3 onto a radial line in the image plane. [sent-82, score-0.795]
</p><p>40 e Under the radial 1D camera model, a 3D point Xj = [xj , yj , zj , 1]T is mapped to a distorted image measurement xidj = [uidj, vidj]T by a radial camera Pi ∈ R2×4: PiXj =  φijxidj  ,  (1)  where φij is a scale factor. [sent-84, score-1.248]
</p><p>41 11889977  The backward projection of the line is the plane containing the 3D point Xj and the ray passing through center of distortion ci and xidj . [sent-87, score-0.417]
</p><p>42 , a scaling projection matrix for each radial camera and scaling scene point individually will not change the 2D image measurements. [sent-93, score-0.657]
</p><p>43 Nevertheless, we can achieve 3D Euclidean reconstruction without ambiguity as shown in the following sections. [sent-94, score-0.233]
</p><p>44 The radial 1D camera can be thought of as projecting a bundle of planes containing the optical axis onto a bundle of radial lines passing through the radial centre in the image plane (Fig. [sent-96, score-1.253]
</p><p>45 The radial 1D camera model encompasses most ofthe central and non-central omnidirectional cameras. [sent-98, score-0.79]
</p><p>46 This is because the only essential requirement in this model is that all points lie in one plane, of the bundle around the optical axis, project onto the same radial line (passing through the radial centre). [sent-99, score-0.727]
</p><p>47 Multi-view reconstruction upto projectivity In this paper, we target at multi-view Euclidean reconstruction from arbitrary radially-symmetric cameras. [sent-101, score-0.441]
</p><p>48 First, we achieve multi-view reconstruction upto projectivity through factorization, which offers great simplicity and elegancy. [sent-103, score-0.256]
</p><p>49 Second, by exploiting intrinsic constraints, the projective reconstruction is upgraded to Euclidean reconstruction. [sent-104, score-0.324]
</p><p>50 In this way, we do not need any specific camera and distortion model other than the radially-symmetric condition. [sent-105, score-0.251]
</p><p>51 Thus, we have reached a factorization formulation similar to the factorization model for perspective cameras. [sent-120, score-0.351]
</p><p>52 Actually, a perspective camera model with or without distortion falls exactly into the radial 1D camera model. [sent-121, score-0.771]
</p><p>53 -(3) can handle non-central cameras as well since it is a projection model by radial 1D cameras in [25]. [sent-123, score-0.75]
</p><p>54 Hadamard factorization based solution Recall that the multi-view factorization model for radially symmetric cameras is expressed as: PX = Φ ? [sent-126, score-0.61]
</p><p>55 For general wide view angle omnidirectional cameras, the coefficient φij is positive. [sent-129, score-0.336]
</p><p>56 Taking all the constraints into consideration, mathematically multi-view factorization for radially symmetric cameras is formulated as: Problem 3. [sent-130, score-0.459]
</p><p>57 ×  Once the scaling matrix Φ is recovered, the multi-view factorization problem can be solved via the singular value decomposition (SVD) as W = PX. [sent-134, score-0.24]
</p><p>58 Minimization objective: Under the affine camera model, multi-view factorization achieves the maximum likelihood estimation (MLE) [10]. [sent-137, score-0.298]
</p><p>59 For the perspective camera model, multi-view factorization is actually minimizing an algebraic error, which can be viewed as an approximation to the geometric reprojection error. [sent-138, score-0.401]
</p><p>60 Under the radial 1D camera model, we cannot measure the geometric reprojection error but only an angular error. [sent-139, score-0.586]
</p><p>61 We define the angular error corresponding to a distorted image measurement xidj as eij, which measures the angle between the measured ray and the reconstructed ray, i. [sent-140, score-0.502]
</p><p>62 Column-wise and row-wise normalization of φij deal with scale ambiguity with P and X respectively, which has been used in projective factorization problems e. [sent-190, score-0.338]
</p><p>63 In solving the multi-view factorization problem, we are actually relaxing the objective function to an algebraic error to approximate the geometric angular error. [sent-193, score-0.32]
</p><p>64 To achieve a good approximation, we propose to normalize all the image measurements xidj to unit norm xidj ← xidj/ ? [sent-194, score-0.553]
</p><p>65 , th aes  factorization formulation expresses as: PiXj = φijxidj , where xidj is a unit norm vector (direction). [sent-198, score-0.414]
</p><p>66 Multi-view Euclidean Reconstruction In this section, we upgrade the multi-view reconstruction from radially-symmetric camera to Euclidean reconstruction by exploiting constraints on the intrinsic camera matrix. [sent-210, score-0.664]
</p><p>67 Once we have recovered the scaling matrix Φ and the weighted measurement matrix W, the projection matrix P and scene structure X can be recovered through SVD as W = PX. [sent-214, score-0.388]
</p><p>68 If a reasonable upgrading matrix H is achieved, the Euclidean structure and motion can be recovered from structure matrix Xˆ and projection matrix Pˆ. [sent-218, score-0.293]
</p><p>69 Efficient Implementations In this section, we propose to solve multi-view projective reconstruction Problem 3. [sent-232, score-0.288]
</p><p>70 Semi-definite Programming Under noiseless and complete measurement case, our multi-view factorization Problem 3. [sent-236, score-0.234]
</p><p>71 Recently, nuclear norm minimization has been widely used in low-rank modeling such as projective factorization [5, 3] and robust principal component analysis [4]. [sent-249, score-0.379]
</p><p>72 Therefore, we obtain an alternating direction continuation based algorithm for Hadamard factorization formulation for radially symmetric camera reconstruction, which alternates between updating U, V and Φ. [sent-336, score-0.506]
</p><p>73 Dealing with incomplete measurements With a wide field of view camera, we can reconstruct 3D scene from a few images, which will create an incomplete measurement matrix in general and the SVD based method cannot be applied. [sent-340, score-0.365]
</p><p>74 ×  In this subsection, we extend our Hadamard factorization formulation from complete measurements case to incomplete measurements case. [sent-341, score-0.349]
</p><p>75 With missing data setting, we are given an incomplete measurement matrix M = [xidj] , where the missing elements are completed with 0. [sent-342, score-0.332]
</p><p>76 e, With these notations, the imaging process for radiallysymmetric cameras with missing data can be compactly expressed as: ? [sent-345, score-0.278]
</p><p>77 Additionally, our alternating adtirreixct wiointh hc nonot einn-uation based efficient implementation can also be extended to incomplete measurement case directly, achieving missing points handling ALM method (MALM). [sent-363, score-0.263]
</p><p>78 To generate the synthetic data we created 100 3D points placed randomly on 3 walls intersecting each other in 90 degrees, and a catadioptric system consisting of a perspective camera and a spherical mirror axially aligned is placed to capture the image of the 3D points on the walls. [sent-370, score-0.908]
</p><p>79 The catadioptric system is moved to capture 20 images at different views. [sent-371, score-0.386]
</p><p>80 Note that this reflection by the spherical mirror creates axially aligned radial distortion in the image. [sent-373, score-0.702]
</p><p>81 The 2D angle error is the angle between the estimated radial line and the input radial line in the image. [sent-383, score-0.782]
</p><p>82 DTh peo sikntews e error tise obtained from the estimated camera calibration matrix. [sent-386, score-0.27]
</p><p>83 (Left)  A catadioptric system consists of a spherical mirror and a camera (green frustrum) in 6 textured walls. [sent-400, score-0.715]
</p><p>84 (Third and fourth) The same views of 3D reconstruction by MALM and missing points are recovered. [sent-418, score-0.267]
</p><p>85 We generated a dataset ‘MirrorBall’ which is an image sequence rendered by a computer graphics application to simulate a catadioptric system in the configuration shown in Fig. [sent-425, score-0.421]
</p><p>86 A spherical mirror is placed in front of a perspective camera inside 6 textured walls, and the reflection on the spherical mirror is rendered. [sent-427, score-0.612]
</p><p>87 Sony RPU camera module consisting of multiple reflection surfaces and a camera to create a panoramic image was used to capture a ‘rooftop’ sequence of 17 images. [sent-480, score-0.346]
</p><p>88 An omnidirectional image sequence captured by a catadioptric camera (Kumotek VS-C14U-80-ST) was used in our experiment. [sent-487, score-0.783]
</p><p>89 4(Lef6t)  An image of a cylinder shape of a paper captured by the omnidirectional camera (catadioptric). [sent-490, score-0.397]
</p><p>90 A top view of the reconstruction by SIESTA (171 frames, 40 points) and the area of the reconstruction in the building map. [sent-499, score-0.411]
</p><p>91 der shape, then the catadioptric camera was inserted inside the rolled paper. [sent-503, score-0.569]
</p><p>92 Image sequences were captured by Canon fisheye lens at indoor scenes such as corridors and a room as shown in Fig. [sent-508, score-0.237]
</p><p>93 The reports show that our proposed ALM achieves less than 5% error of 3D reconstruction compared with the ground truth. [sent-530, score-0.229]
</p><p>94 With multi-view input, our method overcomes the previous theoretical boundary of 3D reconstruction from radial 1D cameras. [sent-536, score-0.509]
</p><p>95 one view from a fish eye lens and another view from catadioptric camera), theoretically this is identical and solvable. [sent-539, score-0.562]
</p><p>96 A current limitation of our method is that the centre of distortion needs to be known or estimated from lens outer edges. [sent-540, score-0.234]
</p><p>97 Analytical forward projection for axial non-central dioptric and catadioptric cameras. [sent-549, score-0.601]
</p><p>98 Beyond Alhazen’s problem: Analytical projection model for non-central catadioptric cameras with quadric mirrors. [sent-556, score-0.626]
</p><p>99 Parameter-free radial distortion correction with center of distortion estimation. [sent-597, score-0.532]
</p><p>100 Automatic scene structure and camera motion using a catadioptric system. [sent-613, score-0.533]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('catadioptric', 0.386), ('radial', 0.324), ('omnidirectional', 0.25), ('xidj', 0.223), ('alm', 0.195), ('cameras', 0.186), ('reconstruction', 0.185), ('sdp', 0.172), ('malm', 0.162), ('factorization', 0.151), ('camera', 0.147), ('fisheye', 0.143), ('siesta', 0.142), ('rpu', 0.121), ('sony', 0.116), ('mirror', 0.115), ('ij', 0.112), ('distortion', 0.104), ('projective', 0.103), ('pixj', 0.101), ('lens', 0.094), ('radially', 0.09), ('measurement', 0.083), ('dioptric', 0.083), ('euclidean', 0.08), ('uncalibrated', 0.079), ('calibration', 0.079), ('axial', 0.078), ('angular', 0.071), ('central', 0.069), ('spherical', 0.067), ('measurements', 0.067), ('upgrading', 0.066), ('tardif', 0.066), ('lagrangian', 0.065), ('incomplete', 0.064), ('rank', 0.063), ('gcm', 0.061), ('thirthala', 0.061), ('algebraic', 0.054), ('projection', 0.054), ('nuclear', 0.053), ('reflection', 0.052), ('continuation', 0.052), ('missing', 0.052), ('px', 0.052), ('uvt', 0.05), ('abstraction', 0.049), ('perspective', 0.049), ('multiplier', 0.049), ('bundle', 0.049), ('ambiguity', 0.048), ('canon', 0.047), ('matrix', 0.046), ('sturm', 0.045), ('hadamard', 0.045), ('angle', 0.045), ('error', 0.044), ('walls', 0.044), ('unified', 0.044), ('scaling', 0.043), ('ramalingam', 0.041), ('view', 0.041), ('analytical', 0.041), ('axially', 0.04), ('headings', 0.04), ('ijxidj', 0.04), ('mirrorball', 0.04), ('projectivity', 0.04), ('radiallysymmetric', 0.04), ('rooftop', 0.04), ('stratification', 0.04), ('uidj', 0.04), ('norm', 0.04), ('svd', 0.04), ('cu', 0.04), ('pages', 0.039), ('augmented', 0.037), ('centre', 0.036), ('ray', 0.036), ('upgraded', 0.036), ('rolled', 0.036), ('nonsingular', 0.036), ('normalization', 0.036), ('rendered', 0.035), ('completed', 0.035), ('recovered', 0.035), ('hartley', 0.034), ('alternating', 0.034), ('war', 0.033), ('agrawal', 0.032), ('minimization', 0.032), ('symmetric', 0.032), ('australian', 0.031), ('upto', 0.031), ('points', 0.03), ('taguchi', 0.03), ('symmetry', 0.029), ('circular', 0.029), ('programming', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="280-tfidf-1" href="./iccv-2013-Multi-view_3D_Reconstruction_from_Uncalibrated_Radially-Symmetric_Cameras.html">280 iccv-2013-Multi-view 3D Reconstruction from Uncalibrated Radially-Symmetric Cameras</a></p>
<p>Author: Jae-Hak Kim, Yuchao Dai, Hongdong Li, Xin Du, Jonghyuk Kim</p><p>Abstract: We present a new multi-view 3D Euclidean reconstruction method for arbitrary uncalibrated radially-symmetric cameras, which needs no calibration or any camera model parameters other than radial symmetry. It is built on the radial 1D camera model [25], a unified mathematical abstraction to different types of radially-symmetric cameras. We formulate the problem of multi-view reconstruction for radial 1D cameras as a matrix rank minimization problem. Efficient implementation based on alternating direction continuation is proposed to handle scalability issue for real-world applications. Our method applies to a wide range of omnidirectional cameras including both dioptric and catadioptric (central and non-central) cameras. Additionally, our method deals with complete and incomplete measurements under a unified framework elegantly. Experiments on both synthetic and real images from various types of cameras validate the superior performance of our new method, in terms of numerical accuracy and robustness.</p><p>2 0.24613944 <a title="280-tfidf-2" href="./iccv-2013-Extrinsic_Camera_Calibration_without_a_Direct_View_Using_Spherical_Mirror.html">152 iccv-2013-Extrinsic Camera Calibration without a Direct View Using Spherical Mirror</a></p>
<p>Author: Amit Agrawal</p><p>Abstract: We consider the problem of estimating the extrinsic parameters (pose) of a camera with respect to a reference 3D object without a direct view. Since the camera does not view the object directly, previous approaches have utilized reflections in a planar mirror to solve this problem. However, a planar mirror based approach requires a minimum of three reflections and has degenerate configurations where estimation fails. In this paper, we show that the pose can be obtained using a single reflection in a spherical mirror of known radius. This makes our approach simpler and easier in practice. In addition, unlike planar mirrors, the spherical mirror based approach does not have any degenerate configurations, leading to a robust algorithm. While a planar mirror reflection results in a virtual perspective camera, a spherical mirror reflection results in a non-perspective axial camera. The axial nature of rays allows us to compute the axis (direction of sphere center) and few pose parameters in a linear fashion. We then derive an analytical solution to obtain the distance to the sphere cen- ter and remaining pose parameters and show that it corresponds to solving a 16th degree equation. We present comparisons with a recent method that use planar mirrors and show that our approach recovers more accurate pose in the presence of noise. Extensive simulations and results on real data validate our algorithm.</p><p>3 0.22850381 <a title="280-tfidf-3" href="./iccv-2013-Real-Time_Solution_to_the_Absolute_Pose_Problem_with_Unknown_Radial_Distortion_and_Focal_Length.html">342 iccv-2013-Real-Time Solution to the Absolute Pose Problem with Unknown Radial Distortion and Focal Length</a></p>
<p>Author: Zuzana Kukelova, Martin Bujnak, Tomas Pajdla</p><p>Abstract: Theproblem ofdetermining the absoluteposition andorientation of a camera from a set of 2D-to-3D point correspondences is one of the most important problems in computer vision with a broad range of applications. In this paper we present a new solution to the absolute pose problem for camera with unknown radial distortion and unknown focal length from five 2D-to-3D point correspondences. Our new solver is numerically more stable, more accurate, and significantly faster than the existing state-of-the-art minimal fourpoint absolutepose solvers for this problem. Moreover, our solver results in less solutions and can handle larger radial distortions. The new solver is straightforward and uses only simple concepts from linear algebra. Therefore it is simpler than the state-of-the-art Gr¨ obner basis solvers. We compare our new solver with the existing state-of-theart solvers and show its usefulness on synthetic and real datasets. 1</p><p>4 0.19831523 <a title="280-tfidf-4" href="./iccv-2013-Unifying_Nuclear_Norm_and_Bilinear_Factorization_Approaches_for_Low-Rank_Matrix_Decomposition.html">434 iccv-2013-Unifying Nuclear Norm and Bilinear Factorization Approaches for Low-Rank Matrix Decomposition</a></p>
<p>Author: Ricardo Cabral, Fernando De_La_Torre, João P. Costeira, Alexandre Bernardino</p><p>Abstract: Low rank models have been widely usedfor the representation of shape, appearance or motion in computer vision problems. Traditional approaches to fit low rank models make use of an explicit bilinear factorization. These approaches benefit from fast numerical methods for optimization and easy kernelization. However, they suffer from serious local minima problems depending on the loss function and the amount/type of missing data. Recently, these lowrank models have alternatively been formulated as convex problems using the nuclear norm regularizer; unlike factorization methods, their numerical solvers are slow and it is unclear how to kernelize them or to impose a rank a priori. This paper proposes a unified approach to bilinear factorization and nuclear norm regularization, that inherits the benefits of both. We analyze the conditions under which these approaches are equivalent. Moreover, based on this analysis, we propose a new optimization algorithm and a “rank continuation ” strategy that outperform state-of-theart approaches for Robust PCA, Structure from Motion and Photometric Stereo with outliers and missing data.</p><p>5 0.16995393 <a title="280-tfidf-5" href="./iccv-2013-Unsupervised_Intrinsic_Calibration_from_a_Single_Frame_Using_a_%22Plumb-Line%22_Approach.html">436 iccv-2013-Unsupervised Intrinsic Calibration from a Single Frame Using a "Plumb-Line" Approach</a></p>
<p>Author: R. Melo, M. Antunes, J.P. Barreto, G. Falcão, N. Gonçalves</p><p>Abstract: Estimating the amount and center ofdistortionfrom lines in the scene has been addressed in the literature by the socalled “plumb-line ” approach. In this paper we propose a new geometric method to estimate not only the distortion parameters but the entire camera calibration (up to an “angular” scale factor) using a minimum of 3 lines. We propose a new framework for the unsupervised simultaneous detection of natural image of lines and camera parameters estimation, enabling a robust calibration from a single image. Comparative experiments with existing automatic approaches for the distortion estimation and with ground truth data are presented.</p><p>6 0.15567826 <a title="280-tfidf-6" href="./iccv-2013-An_Enhanced_Structure-from-Motion_Paradigm_Based_on_the_Absolute_Dual_Quadric_and_Images_of_Circular_Points.html">49 iccv-2013-An Enhanced Structure-from-Motion Paradigm Based on the Absolute Dual Quadric and Images of Circular Points</a></p>
<p>7 0.13793468 <a title="280-tfidf-7" href="./iccv-2013-A_Global_Linear_Method_for_Camera_Pose_Registration.html">17 iccv-2013-A Global Linear Method for Camera Pose Registration</a></p>
<p>8 0.10816436 <a title="280-tfidf-8" href="./iccv-2013-Pose_Estimation_with_Unknown_Focal_Length_Using_Points%2C_Directions_and_Lines.html">323 iccv-2013-Pose Estimation with Unknown Focal Length Using Points, Directions and Lines</a></p>
<p>9 0.10673786 <a title="280-tfidf-9" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>10 0.10337541 <a title="280-tfidf-10" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>11 0.10307068 <a title="280-tfidf-11" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>12 0.10298225 <a title="280-tfidf-12" href="./iccv-2013-Rectangling_Stereographic_Projection_for_Wide-Angle_Image_Visualization.html">346 iccv-2013-Rectangling Stereographic Projection for Wide-Angle Image Visualization</a></p>
<p>13 0.099461637 <a title="280-tfidf-13" href="./iccv-2013-Refractive_Structure-from-Motion_on_Underwater_Images.html">348 iccv-2013-Refractive Structure-from-Motion on Underwater Images</a></p>
<p>14 0.093237825 <a title="280-tfidf-14" href="./iccv-2013-Space-Time_Tradeoffs_in_Photo_Sequencing.html">397 iccv-2013-Space-Time Tradeoffs in Photo Sequencing</a></p>
<p>15 0.092973173 <a title="280-tfidf-15" href="./iccv-2013-Partial_Sum_Minimization_of_Singular_Values_in_RPCA_for_Low-Level_Vision.html">310 iccv-2013-Partial Sum Minimization of Singular Values in RPCA for Low-Level Vision</a></p>
<p>16 0.091528594 <a title="280-tfidf-16" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>17 0.091381706 <a title="280-tfidf-17" href="./iccv-2013-Modeling_the_Calibration_Pipeline_of_the_Lytro_Camera_for_High_Quality_Light-Field_Image_Reconstruction.html">271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</a></p>
<p>18 0.091252677 <a title="280-tfidf-18" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>19 0.09080036 <a title="280-tfidf-19" href="./iccv-2013-Global_Fusion_of_Relative_Motions_for_Robust%2C_Accurate_and_Scalable_Structure_from_Motion.html">184 iccv-2013-Global Fusion of Relative Motions for Robust, Accurate and Scalable Structure from Motion</a></p>
<p>20 0.090399131 <a title="280-tfidf-20" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.168), (1, -0.165), (2, -0.063), (3, 0.025), (4, -0.082), (5, 0.045), (6, 0.042), (7, -0.122), (8, 0.072), (9, 0.023), (10, 0.03), (11, -0.041), (12, -0.152), (13, 0.001), (14, -0.0), (15, 0.051), (16, 0.074), (17, 0.148), (18, -0.056), (19, 0.045), (20, 0.007), (21, -0.138), (22, -0.089), (23, -0.029), (24, -0.054), (25, 0.063), (26, 0.059), (27, -0.082), (28, -0.077), (29, -0.004), (30, 0.011), (31, 0.056), (32, -0.038), (33, -0.017), (34, -0.054), (35, 0.011), (36, -0.033), (37, 0.014), (38, 0.072), (39, 0.014), (40, 0.048), (41, -0.13), (42, 0.01), (43, 0.096), (44, 0.0), (45, 0.104), (46, -0.071), (47, -0.04), (48, 0.104), (49, -0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95697302 <a title="280-lsi-1" href="./iccv-2013-Multi-view_3D_Reconstruction_from_Uncalibrated_Radially-Symmetric_Cameras.html">280 iccv-2013-Multi-view 3D Reconstruction from Uncalibrated Radially-Symmetric Cameras</a></p>
<p>Author: Jae-Hak Kim, Yuchao Dai, Hongdong Li, Xin Du, Jonghyuk Kim</p><p>Abstract: We present a new multi-view 3D Euclidean reconstruction method for arbitrary uncalibrated radially-symmetric cameras, which needs no calibration or any camera model parameters other than radial symmetry. It is built on the radial 1D camera model [25], a unified mathematical abstraction to different types of radially-symmetric cameras. We formulate the problem of multi-view reconstruction for radial 1D cameras as a matrix rank minimization problem. Efficient implementation based on alternating direction continuation is proposed to handle scalability issue for real-world applications. Our method applies to a wide range of omnidirectional cameras including both dioptric and catadioptric (central and non-central) cameras. Additionally, our method deals with complete and incomplete measurements under a unified framework elegantly. Experiments on both synthetic and real images from various types of cameras validate the superior performance of our new method, in terms of numerical accuracy and robustness.</p><p>2 0.86906099 <a title="280-lsi-2" href="./iccv-2013-An_Enhanced_Structure-from-Motion_Paradigm_Based_on_the_Absolute_Dual_Quadric_and_Images_of_Circular_Points.html">49 iccv-2013-An Enhanced Structure-from-Motion Paradigm Based on the Absolute Dual Quadric and Images of Circular Points</a></p>
<p>Author: Lilian Calvet, Pierre Gurdjos</p><p>Abstract: This work aims at introducing a new unified Structurefrom-Motion (SfM) paradigm in which images of circular point-pairs can be combined with images of natural points. An imaged circular point-pair encodes the 2D Euclidean structure of a world plane and can easily be derived from the image of a planar shape, especially those including circles. A classical SfM method generally runs two steps: first a projective factorization of all matched image points (into projective cameras and points) and second a camera selfcalibration that updates the obtained world from projective to Euclidean. This work shows how to introduce images of circular points in these two SfM steps while its key contribution is to provide the theoretical foundations for combining “classical” linear self-calibration constraints with additional ones derived from such images. We show that the two proposed SfM steps clearly contribute to better results than the classical approach. We validate our contributions on synthetic and real images.</p><p>3 0.81084317 <a title="280-lsi-3" href="./iccv-2013-Unsupervised_Intrinsic_Calibration_from_a_Single_Frame_Using_a_%22Plumb-Line%22_Approach.html">436 iccv-2013-Unsupervised Intrinsic Calibration from a Single Frame Using a "Plumb-Line" Approach</a></p>
<p>Author: R. Melo, M. Antunes, J.P. Barreto, G. Falcão, N. Gonçalves</p><p>Abstract: Estimating the amount and center ofdistortionfrom lines in the scene has been addressed in the literature by the socalled “plumb-line ” approach. In this paper we propose a new geometric method to estimate not only the distortion parameters but the entire camera calibration (up to an “angular” scale factor) using a minimum of 3 lines. We propose a new framework for the unsupervised simultaneous detection of natural image of lines and camera parameters estimation, enabling a robust calibration from a single image. Comparative experiments with existing automatic approaches for the distortion estimation and with ground truth data are presented.</p><p>4 0.79786497 <a title="280-lsi-4" href="./iccv-2013-Extrinsic_Camera_Calibration_without_a_Direct_View_Using_Spherical_Mirror.html">152 iccv-2013-Extrinsic Camera Calibration without a Direct View Using Spherical Mirror</a></p>
<p>Author: Amit Agrawal</p><p>Abstract: We consider the problem of estimating the extrinsic parameters (pose) of a camera with respect to a reference 3D object without a direct view. Since the camera does not view the object directly, previous approaches have utilized reflections in a planar mirror to solve this problem. However, a planar mirror based approach requires a minimum of three reflections and has degenerate configurations where estimation fails. In this paper, we show that the pose can be obtained using a single reflection in a spherical mirror of known radius. This makes our approach simpler and easier in practice. In addition, unlike planar mirrors, the spherical mirror based approach does not have any degenerate configurations, leading to a robust algorithm. While a planar mirror reflection results in a virtual perspective camera, a spherical mirror reflection results in a non-perspective axial camera. The axial nature of rays allows us to compute the axis (direction of sphere center) and few pose parameters in a linear fashion. We then derive an analytical solution to obtain the distance to the sphere cen- ter and remaining pose parameters and show that it corresponds to solving a 16th degree equation. We present comparisons with a recent method that use planar mirrors and show that our approach recovers more accurate pose in the presence of noise. Extensive simulations and results on real data validate our algorithm.</p><p>5 0.78316653 <a title="280-lsi-5" href="./iccv-2013-Refractive_Structure-from-Motion_on_Underwater_Images.html">348 iccv-2013-Refractive Structure-from-Motion on Underwater Images</a></p>
<p>Author: Anne Jordt-Sedlazeck, Reinhard Koch</p><p>Abstract: In underwater environments, cameras need to be confined in an underwater housing, viewing the scene through a piece of glass. In case of flat port underwater housings, light rays entering the camera housing are refracted twice, due to different medium densities of water, glass, and air. This causes the usually linear rays of light to bend and the commonly used pinhole camera model to be invalid. When using the pinhole camera model without explicitly modeling refraction in Structure-from-Motion (SfM) methods, a systematic model error occurs. Therefore, in this paper, we propose a system for computing camera path and 3D points with explicit incorporation of refraction using new methods for pose estimation. Additionally, a new error function is introduced for non-linear optimization, especially bundle adjustment. The proposed method allows to increase reconstruction accuracy and is evaluated in a set of experiments, where the proposed method’s performance is compared to SfM with the perspective camera model.</p><p>6 0.7685219 <a title="280-lsi-6" href="./iccv-2013-Real-Time_Solution_to_the_Absolute_Pose_Problem_with_Unknown_Radial_Distortion_and_Focal_Length.html">342 iccv-2013-Real-Time Solution to the Absolute Pose Problem with Unknown Radial Distortion and Focal Length</a></p>
<p>7 0.71750516 <a title="280-lsi-7" href="./iccv-2013-Pose_Estimation_with_Unknown_Focal_Length_Using_Points%2C_Directions_and_Lines.html">323 iccv-2013-Pose Estimation with Unknown Focal Length Using Points, Directions and Lines</a></p>
<p>8 0.68844497 <a title="280-lsi-8" href="./iccv-2013-Rectangling_Stereographic_Projection_for_Wide-Angle_Image_Visualization.html">346 iccv-2013-Rectangling Stereographic Projection for Wide-Angle Image Visualization</a></p>
<p>9 0.66304576 <a title="280-lsi-9" href="./iccv-2013-Lifting_3D_Manhattan_Lines_from_a_Single_Image.html">250 iccv-2013-Lifting 3D Manhattan Lines from a Single Image</a></p>
<p>10 0.64840627 <a title="280-lsi-10" href="./iccv-2013-A_Robust_Analytical_Solution_to_Isometric_Shape-from-Template_with_Focal_Length_Calibration.html">27 iccv-2013-A Robust Analytical Solution to Isometric Shape-from-Template with Focal Length Calibration</a></p>
<p>11 0.63780183 <a title="280-lsi-11" href="./iccv-2013-Revisiting_the_PnP_Problem%3A_A_Fast%2C_General_and_Optimal_Solution.html">353 iccv-2013-Revisiting the PnP Problem: A Fast, General and Optimal Solution</a></p>
<p>12 0.62855101 <a title="280-lsi-12" href="./iccv-2013-A_Global_Linear_Method_for_Camera_Pose_Registration.html">17 iccv-2013-A Global Linear Method for Camera Pose Registration</a></p>
<p>13 0.62811977 <a title="280-lsi-13" href="./iccv-2013-Unifying_Nuclear_Norm_and_Bilinear_Factorization_Approaches_for_Low-Rank_Matrix_Decomposition.html">434 iccv-2013-Unifying Nuclear Norm and Bilinear Factorization Approaches for Low-Rank Matrix Decomposition</a></p>
<p>14 0.61361796 <a title="280-lsi-14" href="./iccv-2013-Non-convex_P-Norm_Projection_for_Robust_Sparsity.html">292 iccv-2013-Non-convex P-Norm Projection for Robust Sparsity</a></p>
<p>15 0.60313755 <a title="280-lsi-15" href="./iccv-2013-Enhanced_Continuous_Tabu_Search_for_Parameter_Estimation_in_Multiview_Geometry.html">141 iccv-2013-Enhanced Continuous Tabu Search for Parameter Estimation in Multiview Geometry</a></p>
<p>16 0.58193177 <a title="280-lsi-16" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>17 0.58100116 <a title="280-lsi-17" href="./iccv-2013-Space-Time_Tradeoffs_in_Photo_Sequencing.html">397 iccv-2013-Space-Time Tradeoffs in Photo Sequencing</a></p>
<p>18 0.56575972 <a title="280-lsi-18" href="./iccv-2013-Direct_Optimization_of_Frame-to-Frame_Rotation.html">115 iccv-2013-Direct Optimization of Frame-to-Frame Rotation</a></p>
<p>19 0.55956012 <a title="280-lsi-19" href="./iccv-2013-Partial_Sum_Minimization_of_Singular_Values_in_RPCA_for_Low-Level_Vision.html">310 iccv-2013-Partial Sum Minimization of Singular Values in RPCA for Low-Level Vision</a></p>
<p>20 0.52581149 <a title="280-lsi-20" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.036), (7, 0.028), (26, 0.063), (27, 0.051), (31, 0.07), (42, 0.106), (48, 0.018), (64, 0.028), (67, 0.225), (73, 0.031), (89, 0.186), (94, 0.01), (98, 0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81751454 <a title="280-lda-1" href="./iccv-2013-Multi-view_3D_Reconstruction_from_Uncalibrated_Radially-Symmetric_Cameras.html">280 iccv-2013-Multi-view 3D Reconstruction from Uncalibrated Radially-Symmetric Cameras</a></p>
<p>Author: Jae-Hak Kim, Yuchao Dai, Hongdong Li, Xin Du, Jonghyuk Kim</p><p>Abstract: We present a new multi-view 3D Euclidean reconstruction method for arbitrary uncalibrated radially-symmetric cameras, which needs no calibration or any camera model parameters other than radial symmetry. It is built on the radial 1D camera model [25], a unified mathematical abstraction to different types of radially-symmetric cameras. We formulate the problem of multi-view reconstruction for radial 1D cameras as a matrix rank minimization problem. Efficient implementation based on alternating direction continuation is proposed to handle scalability issue for real-world applications. Our method applies to a wide range of omnidirectional cameras including both dioptric and catadioptric (central and non-central) cameras. Additionally, our method deals with complete and incomplete measurements under a unified framework elegantly. Experiments on both synthetic and real images from various types of cameras validate the superior performance of our new method, in terms of numerical accuracy and robustness.</p><p>2 0.78979689 <a title="280-lda-2" href="./iccv-2013-Multi-stage_Contextual_Deep_Learning_for_Pedestrian_Detection.html">279 iccv-2013-Multi-stage Contextual Deep Learning for Pedestrian Detection</a></p>
<p>Author: Xingyu Zeng, Wanli Ouyang, Xiaogang Wang</p><p>Abstract: Cascaded classifiers1 have been widely used in pedestrian detection and achieved great success. These classifiers are trained sequentially without joint optimization. In this paper, we propose a new deep model that can jointly train multi-stage classifiers through several stages of backpropagation. It keeps the score map output by a classifier within a local region and uses it as contextual information to support the decision at the next stage. Through a specific design of the training strategy, this deep architecture is able to simulate the cascaded classifiers by mining hard samples to train the network stage-by-stage. Each classifier handles samples at a different difficulty level. Unsupervised pre-training and specifically designed stage-wise supervised training are used to regularize the optimization problem. Both theoretical analysis and experimental results show that the training strategy helps to avoid overfitting. Experimental results on three datasets (Caltech, ETH and TUD-Brussels) show that our approach outperforms the state-of-the-art approaches.</p><p>3 0.74351925 <a title="280-lda-3" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>Author: Yu-Tseh Chi, Mohsen Ali, Muhammad Rushdi, Jeffrey Ho</p><p>Abstract: This paper proposes a novel approach for sparse coding that further improves upon the sparse representation-based classification (SRC) framework. The proposed framework, Affine-Constrained Group Sparse Coding (ACGSC), extends the current SRC framework to classification problems with multiple input samples. Geometrically, the affineconstrained group sparse coding essentially searches for the vector in the convex hull spanned by the input vectors that can best be sparse coded using the given dictionary. The resulting objectivefunction is still convex and can be efficiently optimized using iterative block-coordinate descent scheme that is guaranteed to converge. Furthermore, we provide a form of sparse recovery result that guarantees, at least theoretically, that the classification performance of the constrained group sparse coding should be at least as good as the group sparse coding. We have evaluated the proposed approach using three different recognition experiments that involve illumination variation of faces and textures, and face recognition under occlusions. Prelimi- nary experiments have demonstrated the effectiveness of the proposed approach, and in particular, the results from the recognition/occlusion experiment are surprisingly accurate and robust.</p><p>4 0.73843288 <a title="280-lda-4" href="./iccv-2013-Joint_Deep_Learning_for_Pedestrian_Detection.html">220 iccv-2013-Joint Deep Learning for Pedestrian Detection</a></p>
<p>Author: Wanli Ouyang, Xiaogang Wang</p><p>Abstract: Feature extraction, deformation handling, occlusion handling, and classi?cation are four important components in pedestrian detection. Existing methods learn or design these components either individually or sequentially. The interaction among these components is not yet well explored. This paper proposes that they should be jointly learned in order to maximize their strengths through cooperation. We formulate these four components into a joint deep learning framework and propose a new deep network architecture1. By establishing automatic, mutual interaction among components, the deep model achieves a 9% reduction in the average miss rate compared with the current best-performing pedestrian detection approaches on the largest Caltech benchmark dataset.</p><p>5 0.71996021 <a title="280-lda-5" href="./iccv-2013-Partial_Sum_Minimization_of_Singular_Values_in_RPCA_for_Low-Level_Vision.html">310 iccv-2013-Partial Sum Minimization of Singular Values in RPCA for Low-Level Vision</a></p>
<p>Author: Tae-Hyun Oh, Hyeongwoo Kim, Yu-Wing Tai, Jean-Charles Bazin, In So Kweon</p><p>Abstract: Robust Principal Component Analysis (RPCA) via rank minimization is a powerful tool for recovering underlying low-rank structure of clean data corrupted with sparse noise/outliers. In many low-level vision problems, not only it is known that the underlying structure of clean data is low-rank, but the exact rank of clean data is also known. Yet, when applying conventional rank minimization for those problems, the objective function is formulated in a way that does not fully utilize a priori target rank information about the problems. This observation motivates us to investigate whether there is a better alternative solution when using rank minimization. In this paper, instead of minimizing the nuclear norm, we propose to minimize the partial sum of singular values. The proposed objective function implicitly encourages the target rank constraint in rank minimization. Our experimental analyses show that our approach performs better than conventional rank minimization when the number of samples is deficient, while the solutions obtained by the two approaches are almost identical when the number of samples is more than sufficient. We apply our approach to various low-level vision problems, e.g. high dynamic range imaging, photometric stereo and image alignment, and show that our results outperform those obtained by the conventional nuclear norm rank minimization method.</p><p>6 0.71858776 <a title="280-lda-6" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>7 0.71804559 <a title="280-lda-7" href="./iccv-2013-An_Adaptive_Descriptor_Design_for_Object_Recognition_in_the_Wild.html">48 iccv-2013-An Adaptive Descriptor Design for Object Recognition in the Wild</a></p>
<p>8 0.71604198 <a title="280-lda-8" href="./iccv-2013-Unifying_Nuclear_Norm_and_Bilinear_Factorization_Approaches_for_Low-Rank_Matrix_Decomposition.html">434 iccv-2013-Unifying Nuclear Norm and Bilinear Factorization Approaches for Low-Rank Matrix Decomposition</a></p>
<p>9 0.71379918 <a title="280-lda-9" href="./iccv-2013-Non-convex_P-Norm_Projection_for_Robust_Sparsity.html">292 iccv-2013-Non-convex P-Norm Projection for Robust Sparsity</a></p>
<p>10 0.7125932 <a title="280-lda-10" href="./iccv-2013-A_Learning-Based_Approach_to_Reduce_JPEG_Artifacts_in_Image_Matting.html">19 iccv-2013-A Learning-Based Approach to Reduce JPEG Artifacts in Image Matting</a></p>
<p>11 0.71090996 <a title="280-lda-11" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>12 0.71037066 <a title="280-lda-12" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>13 0.70577073 <a title="280-lda-13" href="./iccv-2013-Unsupervised_Intrinsic_Calibration_from_a_Single_Frame_Using_a_%22Plumb-Line%22_Approach.html">436 iccv-2013-Unsupervised Intrinsic Calibration from a Single Frame Using a "Plumb-Line" Approach</a></p>
<p>14 0.70463765 <a title="280-lda-14" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>15 0.7045911 <a title="280-lda-15" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>16 0.70457017 <a title="280-lda-16" href="./iccv-2013-Building_Part-Based_Object_Detectors_via_3D_Geometry.html">66 iccv-2013-Building Part-Based Object Detectors via 3D Geometry</a></p>
<p>17 0.70428473 <a title="280-lda-17" href="./iccv-2013-Unsupervised_Visual_Domain_Adaptation_Using_Subspace_Alignment.html">438 iccv-2013-Unsupervised Visual Domain Adaptation Using Subspace Alignment</a></p>
<p>18 0.70312762 <a title="280-lda-18" href="./iccv-2013-Group_Norm_for_Learning_Structured_SVMs_with_Unstructured_Latent_Variables.html">187 iccv-2013-Group Norm for Learning Structured SVMs with Unstructured Latent Variables</a></p>
<p>19 0.70306212 <a title="280-lda-19" href="./iccv-2013-Revisiting_the_PnP_Problem%3A_A_Fast%2C_General_and_Optimal_Solution.html">353 iccv-2013-Revisiting the PnP Problem: A Fast, General and Optimal Solution</a></p>
<p>20 0.7024498 <a title="280-lda-20" href="./iccv-2013-Potts_Model%2C_Parametric_Maxflow_and_K-Submodular_Functions.html">324 iccv-2013-Potts Model, Parametric Maxflow and K-Submodular Functions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
