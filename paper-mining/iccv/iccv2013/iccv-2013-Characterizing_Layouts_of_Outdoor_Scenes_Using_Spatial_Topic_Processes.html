<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>72 iccv-2013-Characterizing Layouts of Outdoor Scenes Using Spatial Topic Processes</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-72" href="#">iccv2013-72</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>72 iccv-2013-Characterizing Layouts of Outdoor Scenes Using Spatial Topic Processes</h1>
<br/><p>Source: <a title="iccv-2013-72-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Lin_Characterizing_Layouts_of_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Dahua Lin, Jianxiong Xiao</p><p>Abstract: In this paper, we develop a generative model to describe the layouts of outdoor scenes the spatial configuration of regions. Specifically, the layout of an image is represented as a composite of regions, each associated with a semantic topic. At the heart of this model is a novel stochastic process called Spatial Topic Process, which generates a spatial map of topics from a set of coupled Gaussian processes, thus allowing the distributions of topics to vary continuously across the image plane. A key aspect that distinguishes this model from previous ones consists in its capability of capturing dependencies across both locations and topics while allowing substantial variations in the layouts. We demonstrate the practical utility of the proposed model by testing it on scene classification, semantic segmentation, and layout hallucination. –</p><p>Reference: <a title="iccv-2013-72-reference" href="../iccv2013_reference/iccv-2013-Characterizing_Layouts_of_Outdoor_Scenes_Using_Spatial_Topic_Processes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu c Abstract In this paper, we develop a generative model to describe the layouts of outdoor scenes the spatial configuration of regions. [sent-2, score-0.642]
</p><p>2 Specifically, the layout of an image is represented as a composite of regions, each associated with a semantic topic. [sent-3, score-0.394]
</p><p>3 At the heart of this model is a novel stochastic process called Spatial Topic Process, which generates a spatial map of topics from a set of coupled Gaussian processes, thus allowing the distributions of topics to vary continuously across the image plane. [sent-4, score-0.744]
</p><p>4 A key aspect that distinguishes this model from previous ones consists in its capability of capturing dependencies across both locations and topics while allowing substantial variations in the layouts. [sent-5, score-0.355]
</p><p>5 We demonstrate the practical utility of the proposed model by testing it on scene classification, semantic segmentation, and layout hallucination. [sent-6, score-0.546]
</p><p>6 As illustrated in Figure 1, layouts convey significant information for both semantic interpretation (e. [sent-9, score-0.385]
</p><p>7 Therefore, a good model of layouts is of fundamental importance. [sent-14, score-0.319]
</p><p>8 Our primary goal here is to develop a layout model that can capture the common structures of outdoor scenes while allowing flexible variations. [sent-15, score-0.61]
</p><p>9 In this work, we develop a generative model of layouts, which can be used in various vision tasks, including scene classification and semantic segmentation. [sent-25, score-0.296]
</p><p>10 Moreover, leveraging the scene structures captured by the model, one can extrapolate the scenes beyond the visible scope. [sent-26, score-0.402]
</p><p>11 Instead of modeling layouts explicitly, these models typically utilize spatial relations via potentials that couple semantic labels at different sites. [sent-30, score-0.549]
</p><p>12 Generative models, unlike discriminative ones, often resort to hierarchical Bayesian models to describe a scene [1, 5, 14, 23]. [sent-33, score-0.144]
</p><p>13 Taking advantage of the flexibility of graphi884411  cal models, they are able to express various relations in a complex scene, such as the ones between a scene and its parts [1, 23] and those between concurrent objects [5, 14]. [sent-34, score-0.239]
</p><p>14 Since the introduction of Latent Dirichlet Allocation [3] to scene categorization [6], topic models have also been widely used in scene understanding [4, 16, 19, 26, 29]. [sent-35, score-0.692]
</p><p>15 Whereas some of them take into account spatial relations, their treatment is often simplified focusing only on pairwise relations between objects or making assumptions that ignore important spatial dependencies. [sent-36, score-0.266]
</p><p>16 Hence, the resultant models are generally not the most appropriate choices for characterizing the layouts of outdoor scenes. [sent-37, score-0.42]
</p><p>17 Towards the goal of providing an effective layout model, we develop the Spatial topicprocess, a new formulation that –  builds upon topic models and goes beyond by allowing distributions of topics to vary continuously across the image plane. [sent-38, score-1.233]
</p><p>18 Specifically, to capture the statistical dependencies across both spatial locations and visual categories, we introduce a set of Gaussian processes (GPs) to generate a map of topic distributions. [sent-39, score-0.803]
</p><p>19 These GPs are coupled via a latent representation that encodes the global scene structure. [sent-40, score-0.239]
</p><p>20 This model provides a rich representation that can express layout variations through pixel-dependent topic distributions, and on the other hand ensures both local coherence and global structural consistency via the use of coupled GPs. [sent-41, score-0.944]
</p><p>21 This new layout model is useful for a variety of vision problems. [sent-42, score-0.328]
</p><p>22 We demonstrate its practical utility on three applications: (1) scene classification using the layout representation, (2) semantic segmentation based on spatially varying topic distributions, and (3) layout hallucination, a task trying to extrapolate beyond the visible part of a scene. [sent-43, score-1.681]
</p><p>23 Related Work This work is related to several models developed in recent years that try to incorporate spatial relations into topic models. [sent-45, score-0.634]
</p><p>24 Wang and Grimson proposed Spatial LDA [26], where each pixel is assigned a topic chosen from a local document. [sent-46, score-0.47]
</p><p>25 This model enables spatial variation of topics, but ignores the dependencies between topic assignments by assuming that they are independently chosen. [sent-47, score-0.657]
</p><p>26 [29] goes one step further by introducing an MRF to encourage coherent topic  assignment. [sent-49, score-0.47]
</p><p>27 [19] proposed a reconfigurable model for scene recognition, which treats a scene as a composite of a fixed number of rectangular regions, each governed by a topic. [sent-52, score-0.267]
</p><p>28 While allowing flexible topic-region association, it does not take into account the dependencies between topic assignments either. [sent-53, score-0.625]
</p><p>29 There has been other work that combines latent GPs for spatially coherent segmentation [8, 21, 22]. [sent-54, score-0.185]
</p><p>30 Sudderth and Jordan [22] proposed a formulation of dependent PitmanYor processes (DPY), where spatial dependencies are induced via thresholded GPs. [sent-55, score-0.3]
</p><p>31 It is, however, important to note that there is a fundamental aspect that distinguishes our work from this paper: we aim to learn a generative model that is able to capture the prior structure of outdoor scenes, such that one can sample new scenes from it or infers missing parts of a scene. [sent-56, score-0.251]
</p><p>32 Generative Model of Layouts Following the paradigm of topic models, we characterize an image by a set of visual worlds: S = {(xi , yi, wi)}in=1 . [sent-60, score-0.503]
</p><p>33 aHne rime, xi a bnyd yi are ft vheis pixel ocrolodrsd:i nSat =es {o(fx the i-th )v}isual word, and wi is the quantized label. [sent-61, score-0.176]
</p><p>34 We aim to develop a generative model to explain the spatial configuration of S. [sent-62, score-0.186]
</p><p>35 Given zi, one can draw the visual word wi from the corresponding topic. [sent-67, score-0.226]
</p><p>36 Therefore, it is desirable to jointly model the distributions of zi over the entire image so as to capture the correlations between them. [sent-71, score-0.232]
</p><p>37 In particular, we develop a probabilistic model called Spatial Topic Process that can generate a continuous map of topic distributions based on a set of coupled Gaussian processes. [sent-72, score-0.741]
</p><p>38 (1) Generate a continuous map of  topic distributions as θ ∼ θ|λ. [sent-77, score-0.645]
</p><p>39 Here, θ(x, y) is the predicted tdoisptircib duitsitorinb uotfi topics θa t∼ (x θ,| λy). [sent-78, score-0.156]
</p><p>40 (3) Draw the topic indicator zi at each location from θi ? [sent-82, score-0.546]
</p><p>41 (4) Draw the visual word wi from the corresponding topic βzi . [sent-84, score-0.696]
</p><p>42 884422  from a set of coupled Gaussian processes (with parameter λ), where each map corresponds to a topic. [sent-85, score-0.173]
</p><p>43 Finally, at each sample point  (xi ,yi), a topic  is chosen according to  θi, and then a visual word wi is drawn from the word distribution of the corresponding topic, i. [sent-87, score-0.825]
</p><p>44 To begin with, we first consider a simpler problem – devising a joint distribution to incorporate correlations between a finite set of discrete distributions θ1 , . [sent-93, score-0.222]
</p><p>45 By further extending the finite dimensional Gaussian distributions in Eq. [sent-112, score-0.191]
</p><p>46 Within each topic are links between values at neighboring grid points. [sent-128, score-0.604]
</p><p>47 There are also links (depicted in orange color) between values for different topics at corresponding grid points. [sent-129, score-0.29]
</p><p>48 However, this is not an appropriate design in the context of scene layout modeling, where both the mean and the variance are location dependent. [sent-134, score-0.439]
</p><p>49 We first define a Gaussian distribution over a finite grid and then extend it to a Gaussian process via smooth interpolation. [sent-136, score-0.161]
</p><p>50 =1  884433  Here, wj (v) = exp(−d(v, sj)2/σ2g) is a weight value that reflects t(hev )in =flu eexnpce(− odf( tvh,es j-th seed to v. [sent-154, score-0.207]
</p><p>51 (3) and (4) introduce K Gaussian processes each can be characterized by a finite dimensional Gaussian distributions using the grid-based parametrization as above. [sent-162, score-0.304]
</p><p>52 (5) ensures local coherence, while Gaussian distributions over the grid capture long range spatial relations. [sent-164, score-0.322]
</p><p>53 To capture such relations, it is desirable to further couple all GPs this can be achieved through a joint distributions over the grid values for all topics. [sent-169, score-0.22]
</p><p>54 Empirical testings showed that a 6-by-6 grid suffices to express most variations in the –  –  layout of natural scenes, and regions roughly fall into 20 to 30 categories (e. [sent-171, score-0.489]
</p><p>55 ,k,l  Here, g is an mK-dimensional vector that contains all values at grid points, which we call the latent layout representation, and is the value for the k-th topic at the i-th grid point. [sent-184, score-1.056]
</p><p>56 As shown in Figure 3, this GMRF comprises two types of links: the ones between values for the same topic at neighboring sites (i ∼ j indicates i auends j are neighbors), ca antd n ethigosheb oberitnwge seinte vsa (liue ∼s fjo irn ddiifcfaetreesnt i topics at the same site. [sent-186, score-0.626]
</p><p>57 Given β (the word distributions) and λ (the param-  eter of the Spatial Topic Process), the joint probability of these visual words and their associated topic indicators is ? [sent-192, score-0.712]
</p><p>58 (6), is the prior of tHheer ela,te pn(tg layout representation. [sent-197, score-0.362]
</p><p>59 p(zj |xj , yj , igs) hise th prei topic probability at (xj , yj), which is define|xd by Eq. [sent-198, score-0.61]
</p><p>60 p(wj |zj ; β) is the probability of choosing visual word wj from th|ez topic βzj . [sent-202, score-0.839]
</p><p>61 Inference and Learning Algorithms This section presents algorithms to infer layouts of images and to learn model parameters. [sent-204, score-0.356]
</p><p>62 Inferring Layouts Given the model parameters, including λ and β, we can derive the latent layout representation g of a new image as follows. [sent-207, score-0.396]
</p><p>63 Each word is represented by a triple (xj , yj , wj ), and is associated with a hidden variable zj that assigns it to a topic. [sent-209, score-0.63]
</p><p>64 zK=1  Here, p(wj |xj , yj , g) = p(wj |z)p(z|xj , yj , g) . [sent-214, score-0.28]
</p><p>65 Learning Model Parameters The goal of learning is to estimate the word distribution βk of each topic, and the GP parameter λ that governs the spatially varying topic distribution. [sent-234, score-0.64]
</p><p>66 Suppose pixel-wise topic labeling is provided for each training image. [sent-237, score-0.517]
</p><p>67 Here, (xj , yj) is the coordinate, wj is the word label, and zj is the topic label. [sent-239, score-0.96]
</p><p>68 both the model parameter λ and the latent layout representations g1, . [sent-247, score-0.396]
</p><p>69 The basic idea is to treat the topic indicators for such images as hidden variables, and use E-steps to infer the expected probabilities of their values, as in Eq. [sent-272, score-0.547]
</p><p>70 Applications and Experiments We conducted experiments on three applications scene classification, semantic segmentation, and layout hallucination to test the practical utility of the proposed model. [sent-276, score-0.636]
</p><p>71 In the legend, SPM-Lk refers to spatial pyramid matching with k-levels, STP-k refers to spatial topic process on a k k grid. [sent-290, score-0.706]
</p><p>72 We found empirically that this feature tends to achieve better performance than dense SIFT in outdoor scenes, as significant parts of such scenes are textured regions instead of objects. [sent-294, score-0.17]
</p><p>73 We learned the layout models from the training set following the procedure described in section 4. [sent-296, score-0.328]
</p><p>74 In specific, we set the prior count α to 10−4 in estimating the word distributions of each topic. [sent-297, score-0.288]
</p><p>75 We learned the spatial topic processes on three grid sizes 3 3, 4 4 and 6 6 over a standard image soinzteh 2re56e g×r i2d5s6i,z aensd3 ×set3 σg ×to4 8a0n,d d660×, a6ndov 4e0r respectively. [sent-298, score-0.78]
</p><p>76 Scene Classification  Given an image I, one can infer the latent layout representation g using the optimization algorithm presented in section 4. [sent-302, score-0.433]
</p><p>77 We observe: (1) For the proposed method (STP), the classification accuracy increases when using finer grids, which suggests that local scale variations in the layouts convey useful information for classification. [sent-310, score-0.354]
</p><p>78 (2) STP outperforms SPM when using a 4 4 or 6 6 grid, which indifcoartemss sth SaPtM Mdis wchriemnin uastiinvge ain 4fo ×rm 4at oiorn 6 i s× effectively captured by the layout representation. [sent-311, score-0.36]
</p><p>79 dimnso104SPTM105  image, the inferred layout (using a 4 4 grid), the result by our method (based on the inferred layout), and the result by SLDA. [sent-315, score-0.388]
</p><p>80 Particularly, timhea image t ion fveirsrueadli lzaey tohuet i(nusfeirnrge da layout irisd generated by mixing tehtheo cdol (obrass eodf doinf tfehere nintf topics using t,h aen probabilities θ(x, y) as weights. [sent-316, score-0.484]
</p><p>81 However, it is interesting to notice that such increase is much faster for STP than for others a small subset of visual words is sufficient to estimate the layout reliably. [sent-320, score-0.401]
</p><p>82 Given an image I, we first oversegment it into super-pixels using SLIC [2], and then obtain a semantic segmentation by assigning a label to each super On MSRC  On SUN  0 . [sent-335, score-0.186]
</p><p>83 Note that one can derive a continuous map θ of topic distributions from the layout representation g using Eq. [sent-343, score-0.973]
</p><p>84 We can then combine this prior with the visual words within a super pixel to infer its topic label. [sent-345, score-0.658]
</p><p>85 Specifically, let zs denote the label of a super pixel s, then its posterior distribution is given by p(zs|s; θ)  ∝  ? [sent-346, score-0.156]
</p><p>86 s  Here, we use i∈ s to indicate the i-th visual word is within tHheer super-pixel s. [sent-351, score-0.162]
</p><p>87 For comparison, we also implemented a evxapri(aηnt of spatial LDA [26, 29], which incorporates an MRF to enforce coherence between topics allocated to neighboring pixels. [sent-355, score-0.311]
</p><p>88 Figure 6 shows part of the segmentation results obtained on the SUN dataset, which accurately reflect the scene structures and have very good spatial coherence. [sent-359, score-0.328]
</p><p>89 As we can see, the inferred layouts capture the spatial structures very well, thus substantially reducing the ambiguities of labeling. [sent-362, score-0.521]
</p><p>90 Layout Hallucination It is an interesting phenomenon in human vision system that people often remember seeing a surrounding region of a scene that was not visible in the view. [sent-370, score-0.2]
</p><p>91 These findings lead us to the belief that a model that effectively captures the visual structures of a scene category should be able to extrapolate beyond the input images. [sent-373, score-0.31]
</p><p>92 Specifically, we solve the optimal layout representation g based on a subset of visual words extracted from the visible part, and use it to generate the entire layout. [sent-376, score-0.459]
</p><p>93 As more regions are revealed, the true layout is gradually recovered. [sent-380, score-0.361]
</p><p>94 These results demonstrates our model’s capability of extrapolating layouts beyond the visible part. [sent-383, score-0.463]
</p><p>95 Conclusions  We presented a novel approach to layout modeling. [sent-385, score-0.328]
</p><p>96 At the heart of this model is a spatial topic process which uses a set of coupled Gaussian processes to generate topic distributions that vary continuously across the image plane. [sent-386, score-1.417]
</p><p>97 Using the grid-based parameterization, we further derived a finite dimensional representation of layouts that captures the correlations across both locations and topics. [sent-387, score-0.416]
</p><p>98 The experiments on both scene classification and semantic segmentation showed that the proposed methods achieve considerable improvement over state-of-the-art, which is owning to the strong structural prior provided by the layout model. [sent-388, score-0.65]
</p><p>99 We also performed experiments on layout hallucination, which demonstrates that our model is able to extrapolate scene layouts beyond the visible part. [sent-389, score-0.943]
</p><p>100 Shared segmentation of natural scenes using dependent pitman-yor processes. [sent-542, score-0.143]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('topic', 0.47), ('layout', 0.328), ('layouts', 0.319), ('wj', 0.207), ('topics', 0.156), ('zj', 0.154), ('yj', 0.14), ('word', 0.129), ('distributions', 0.125), ('processes', 0.113), ('zs', 0.112), ('scene', 0.111), ('spatial', 0.102), ('xj', 0.098), ('grid', 0.095), ('msrc', 0.093), ('hallucination', 0.09), ('gps', 0.089), ('extrapolate', 0.086), ('dependencies', 0.085), ('stp', 0.082), ('gaussian', 0.077), ('zi', 0.076), ('segmentation', 0.076), ('gi', 0.076), ('outdoor', 0.07), ('latent', 0.068), ('scenes', 0.067), ('semantic', 0.066), ('finite', 0.066), ('wi', 0.064), ('dirichlet', 0.063), ('sudderth', 0.062), ('relations', 0.062), ('coupled', 0.06), ('visible', 0.058), ('eext', 0.058), ('hills', 0.058), ('sun', 0.054), ('coherence', 0.053), ('exp', 0.053), ('continuous', 0.05), ('generative', 0.048), ('eint', 0.048), ('gmrf', 0.048), ('parizi', 0.048), ('labeling', 0.047), ('yi', 0.047), ('logp', 0.046), ('reconfigurable', 0.045), ('capability', 0.045), ('super', 0.044), ('zij', 0.043), ('covariance', 0.042), ('beyond', 0.041), ('spatially', 0.041), ('superparsing', 0.041), ('tighe', 0.041), ('utility', 0.041), ('words', 0.04), ('lda', 0.04), ('continuously', 0.04), ('indicators', 0.04), ('softmax', 0.04), ('links', 0.039), ('structures', 0.039), ('thheer', 0.038), ('qj', 0.038), ('heart', 0.037), ('allowing', 0.037), ('infer', 0.037), ('meanings', 0.036), ('develop', 0.036), ('dg', 0.035), ('classification', 0.035), ('prior', 0.034), ('mrf', 0.034), ('regions', 0.033), ('express', 0.033), ('visual', 0.033), ('flexible', 0.033), ('xi', 0.033), ('slic', 0.033), ('concurrent', 0.033), ('hierarchical', 0.033), ('quantized', 0.032), ('pyramid', 0.032), ('sth', 0.032), ('distinguishes', 0.032), ('devised', 0.032), ('jt', 0.032), ('jordan', 0.032), ('stochastic', 0.031), ('characterizing', 0.031), ('correlations', 0.031), ('holistic', 0.031), ('gp', 0.031), ('seeing', 0.031), ('substantially', 0.031), ('inferred', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="72-tfidf-1" href="./iccv-2013-Characterizing_Layouts_of_Outdoor_Scenes_Using_Spatial_Topic_Processes.html">72 iccv-2013-Characterizing Layouts of Outdoor Scenes Using Spatial Topic Processes</a></p>
<p>Author: Dahua Lin, Jianxiong Xiao</p><p>Abstract: In this paper, we develop a generative model to describe the layouts of outdoor scenes the spatial configuration of regions. Specifically, the layout of an image is represented as a composite of regions, each associated with a semantic topic. At the heart of this model is a novel stochastic process called Spatial Topic Process, which generates a spatial map of topics from a set of coupled Gaussian processes, thus allowing the distributions of topics to vary continuously across the image plane. A key aspect that distinguishes this model from previous ones consists in its capability of capturing dependencies across both locations and topics while allowing substantial variations in the layouts. We demonstrate the practical utility of the proposed model by testing it on scene classification, semantic segmentation, and layout hallucination. –</p><p>2 0.28801784 <a title="72-tfidf-2" href="./iccv-2013-Class-Specific_Simplex-Latent_Dirichlet_Allocation_for_Image_Classification.html">73 iccv-2013-Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification</a></p>
<p>Author: Mandar Dixit, Nikhil Rasiwasia, Nuno Vasconcelos</p><p>Abstract: An extension of the latent Dirichlet allocation (LDA), denoted class-specific-simplex LDA (css-LDA), is proposed for image classification. An analysis of the supervised LDA models currently used for this task shows that the impact of class information on the topics discovered by these models is very weak in general. This implies that the discovered topics are driven by general image regularities, rather than the semantic regularities of interest for classification. To address this, we introduce a model that induces supervision in topic discovery, while retaining the original flexibility of LDA to account for unanticipated structures of interest. The proposed css-LDA is an LDA model with class supervision at the level of image features. In css-LDA topics are discovered per class, i.e. a single set of topics shared across classes is replaced by multiple class-specific topic sets. This model can be used for generative classification using the Bayes decision rule or even extended to discriminative classification with support vector machines (SVMs). A css-LDA model can endow an image with a vector of class and topic specific count statistics that are similar to the Bag-of-words (BoW) histogram. SVM-based discriminants can be learned for classes in the space of these histograms. The effectiveness of css-LDA model in both generative and discriminative classification frameworks is demonstrated through an extensive experimental evaluation, involving multiple benchmark datasets, where it is shown to outperform all existing LDA based image classification approaches.</p><p>3 0.26716447 <a title="72-tfidf-3" href="./iccv-2013-Estimating_the_3D_Layout_of_Indoor_Scenes_and_Its_Clutter_from_Depth_Sensors.html">144 iccv-2013-Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors</a></p>
<p>Author: Jian Zhang, Chen Kan, Alexander G. Schwing, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly estimate the layout ofrooms as well as the clutterpresent in the scene using RGB-D data. Towards this goal, we propose an effective model that is able to exploit both depth and appearance features, which are complementary. Furthermore, our approach is efficient as we exploit the inherent decomposition of additive potentials. We demonstrate the effectiveness of our approach on the challenging NYU v2 dataset and show that employing depth reduces the layout error by 6% and the clutter estimation by 13%.</p><p>4 0.23981112 <a title="72-tfidf-4" href="./iccv-2013-Box_in_the_Box%3A_Joint_3D_Layout_and_Object_Reasoning_from_Single_Images.html">64 iccv-2013-Box in the Box: Joint 3D Layout and Object Reasoning from Single Images</a></p>
<p>Author: Alexander G. Schwing, Sanja Fidler, Marc Pollefeys, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly infer the room layout as well as the objects present in the scene. Towards this goal, we propose a branch and bound algorithm which is guaranteed to retrieve the global optimum of the joint problem. The main difficulty resides in taking into account occlusion in order to not over-count the evidence. We introduce a new decomposition method, which generalizes integral geometry to triangular shapes, and allows us to bound the different terms in constant time. We exploit both geometric cues and object detectors as image features and show large improvements in 2D and 3D object detection over state-of-the-art deformable part-based models.</p><p>5 0.23767075 <a title="72-tfidf-5" href="./iccv-2013-Bayesian_Joint_Topic_Modelling_for_Weakly_Supervised_Object_Localisation.html">59 iccv-2013-Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation</a></p>
<p>Author: Zhiyuan Shi, Timothy M. Hospedales, Tao Xiang</p><p>Abstract: We address the problem of localisation of objects as bounding boxes in images with weak labels. This weakly supervised object localisation problem has been tackled in the past using discriminative models where each object class is localised independently from other classes. We propose a novel framework based on Bayesian joint topic modelling. Our framework has three distinctive advantages over previous works: (1) All object classes and image backgrounds are modelled jointly together in a single generative model so that “explaining away” inference can resolve ambiguity and lead to better learning and localisation. (2) The Bayesian formulation of the model enables easy integration of prior knowledge about object appearance to compensate for limited supervision. (3) Our model can be learned with a mixture of weakly labelled and unlabelled data, allowing the large volume of unlabelled images on the Internet to be exploited for learning. Extensive experiments on the challenging VOC dataset demonstrate that our approach outperforms the state-of-the-art competitors.</p><p>6 0.14230481 <a title="72-tfidf-6" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>7 0.11864164 <a title="72-tfidf-7" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>8 0.099886641 <a title="72-tfidf-8" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>9 0.099563114 <a title="72-tfidf-9" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>10 0.098194018 <a title="72-tfidf-10" href="./iccv-2013-Holistic_Scene_Understanding_for_3D_Object_Detection_with_RGBD_Cameras.html">201 iccv-2013-Holistic Scene Understanding for 3D Object Detection with RGBD Cameras</a></p>
<p>11 0.08829774 <a title="72-tfidf-11" href="./iccv-2013-New_Graph_Structured_Sparsity_Model_for_Multi-label_Image_Annotations.html">290 iccv-2013-New Graph Structured Sparsity Model for Multi-label Image Annotations</a></p>
<p>12 0.088199429 <a title="72-tfidf-12" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>13 0.085422434 <a title="72-tfidf-13" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<p>14 0.084581077 <a title="72-tfidf-14" href="./iccv-2013-Group_Norm_for_Learning_Structured_SVMs_with_Unstructured_Latent_Variables.html">187 iccv-2013-Group Norm for Learning Structured SVMs with Unstructured Latent Variables</a></p>
<p>15 0.083268747 <a title="72-tfidf-15" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>16 0.082125217 <a title="72-tfidf-16" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>17 0.081776857 <a title="72-tfidf-17" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>18 0.08145801 <a title="72-tfidf-18" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>19 0.080481477 <a title="72-tfidf-19" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<p>20 0.08016292 <a title="72-tfidf-20" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.223), (1, 0.018), (2, 0.007), (3, -0.027), (4, 0.071), (5, 0.031), (6, -0.065), (7, -0.006), (8, -0.043), (9, -0.129), (10, 0.057), (11, 0.005), (12, -0.073), (13, 0.0), (14, -0.031), (15, -0.049), (16, -0.087), (17, 0.001), (18, -0.04), (19, -0.077), (20, -0.106), (21, -0.067), (22, 0.118), (23, -0.084), (24, 0.117), (25, -0.094), (26, 0.175), (27, 0.026), (28, 0.061), (29, 0.153), (30, 0.027), (31, 0.001), (32, 0.009), (33, 0.174), (34, -0.011), (35, 0.003), (36, -0.17), (37, -0.022), (38, -0.016), (39, 0.039), (40, 0.028), (41, 0.016), (42, -0.056), (43, -0.124), (44, -0.075), (45, -0.071), (46, -0.029), (47, 0.017), (48, 0.115), (49, -0.121)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95740163 <a title="72-lsi-1" href="./iccv-2013-Characterizing_Layouts_of_Outdoor_Scenes_Using_Spatial_Topic_Processes.html">72 iccv-2013-Characterizing Layouts of Outdoor Scenes Using Spatial Topic Processes</a></p>
<p>Author: Dahua Lin, Jianxiong Xiao</p><p>Abstract: In this paper, we develop a generative model to describe the layouts of outdoor scenes the spatial configuration of regions. Specifically, the layout of an image is represented as a composite of regions, each associated with a semantic topic. At the heart of this model is a novel stochastic process called Spatial Topic Process, which generates a spatial map of topics from a set of coupled Gaussian processes, thus allowing the distributions of topics to vary continuously across the image plane. A key aspect that distinguishes this model from previous ones consists in its capability of capturing dependencies across both locations and topics while allowing substantial variations in the layouts. We demonstrate the practical utility of the proposed model by testing it on scene classification, semantic segmentation, and layout hallucination. –</p><p>2 0.81294334 <a title="72-lsi-2" href="./iccv-2013-Class-Specific_Simplex-Latent_Dirichlet_Allocation_for_Image_Classification.html">73 iccv-2013-Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification</a></p>
<p>Author: Mandar Dixit, Nikhil Rasiwasia, Nuno Vasconcelos</p><p>Abstract: An extension of the latent Dirichlet allocation (LDA), denoted class-specific-simplex LDA (css-LDA), is proposed for image classification. An analysis of the supervised LDA models currently used for this task shows that the impact of class information on the topics discovered by these models is very weak in general. This implies that the discovered topics are driven by general image regularities, rather than the semantic regularities of interest for classification. To address this, we introduce a model that induces supervision in topic discovery, while retaining the original flexibility of LDA to account for unanticipated structures of interest. The proposed css-LDA is an LDA model with class supervision at the level of image features. In css-LDA topics are discovered per class, i.e. a single set of topics shared across classes is replaced by multiple class-specific topic sets. This model can be used for generative classification using the Bayes decision rule or even extended to discriminative classification with support vector machines (SVMs). A css-LDA model can endow an image with a vector of class and topic specific count statistics that are similar to the Bag-of-words (BoW) histogram. SVM-based discriminants can be learned for classes in the space of these histograms. The effectiveness of css-LDA model in both generative and discriminative classification frameworks is demonstrated through an extensive experimental evaluation, involving multiple benchmark datasets, where it is shown to outperform all existing LDA based image classification approaches.</p><p>3 0.71099466 <a title="72-lsi-3" href="./iccv-2013-Estimating_the_3D_Layout_of_Indoor_Scenes_and_Its_Clutter_from_Depth_Sensors.html">144 iccv-2013-Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors</a></p>
<p>Author: Jian Zhang, Chen Kan, Alexander G. Schwing, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly estimate the layout ofrooms as well as the clutterpresent in the scene using RGB-D data. Towards this goal, we propose an effective model that is able to exploit both depth and appearance features, which are complementary. Furthermore, our approach is efficient as we exploit the inherent decomposition of additive potentials. We demonstrate the effectiveness of our approach on the challenging NYU v2 dataset and show that employing depth reduces the layout error by 6% and the clutter estimation by 13%.</p><p>4 0.70319533 <a title="72-lsi-4" href="./iccv-2013-Bayesian_Joint_Topic_Modelling_for_Weakly_Supervised_Object_Localisation.html">59 iccv-2013-Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation</a></p>
<p>Author: Zhiyuan Shi, Timothy M. Hospedales, Tao Xiang</p><p>Abstract: We address the problem of localisation of objects as bounding boxes in images with weak labels. This weakly supervised object localisation problem has been tackled in the past using discriminative models where each object class is localised independently from other classes. We propose a novel framework based on Bayesian joint topic modelling. Our framework has three distinctive advantages over previous works: (1) All object classes and image backgrounds are modelled jointly together in a single generative model so that “explaining away” inference can resolve ambiguity and lead to better learning and localisation. (2) The Bayesian formulation of the model enables easy integration of prior knowledge about object appearance to compensate for limited supervision. (3) Our model can be learned with a mixture of weakly labelled and unlabelled data, allowing the large volume of unlabelled images on the Internet to be exploited for learning. Extensive experiments on the challenging VOC dataset demonstrate that our approach outperforms the state-of-the-art competitors.</p><p>5 0.69531608 <a title="72-lsi-5" href="./iccv-2013-Box_in_the_Box%3A_Joint_3D_Layout_and_Object_Reasoning_from_Single_Images.html">64 iccv-2013-Box in the Box: Joint 3D Layout and Object Reasoning from Single Images</a></p>
<p>Author: Alexander G. Schwing, Sanja Fidler, Marc Pollefeys, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly infer the room layout as well as the objects present in the scene. Towards this goal, we propose a branch and bound algorithm which is guaranteed to retrieve the global optimum of the joint problem. The main difficulty resides in taking into account occlusion in order to not over-count the evidence. We introduce a new decomposition method, which generalizes integral geometry to triangular shapes, and allows us to bound the different terms in constant time. We exploit both geometric cues and object detectors as image features and show large improvements in 2D and 3D object detection over state-of-the-art deformable part-based models.</p><p>6 0.63286084 <a title="72-lsi-6" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>7 0.59750426 <a title="72-lsi-7" href="./iccv-2013-Holistic_Scene_Understanding_for_3D_Object_Detection_with_RGBD_Cameras.html">201 iccv-2013-Holistic Scene Understanding for 3D Object Detection with RGBD Cameras</a></p>
<p>8 0.56815076 <a title="72-lsi-8" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>9 0.56337607 <a title="72-lsi-9" href="./iccv-2013-Sequential_Bayesian_Model_Update_under_Structured_Scene_Prior_for_Semantic_Road_Scenes_Labeling.html">386 iccv-2013-Sequential Bayesian Model Update under Structured Scene Prior for Semantic Road Scenes Labeling</a></p>
<p>10 0.56295019 <a title="72-lsi-10" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>11 0.5405218 <a title="72-lsi-11" href="./iccv-2013-3D_Scene_Understanding_by_Voxel-CRF.html">2 iccv-2013-3D Scene Understanding by Voxel-CRF</a></p>
<p>12 0.53760588 <a title="72-lsi-12" href="./iccv-2013-Pyramid_Coding_for_Functional_Scene_Element_Recognition_in_Video_Scenes.html">331 iccv-2013-Pyramid Coding for Functional Scene Element Recognition in Video Scenes</a></p>
<p>13 0.5205161 <a title="72-lsi-13" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>14 0.51689386 <a title="72-lsi-14" href="./iccv-2013-Active_MAP_Inference_in_CRFs_for_Efficient_Semantic_Segmentation.html">42 iccv-2013-Active MAP Inference in CRFs for Efficient Semantic Segmentation</a></p>
<p>15 0.50515229 <a title="72-lsi-15" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>16 0.47597939 <a title="72-lsi-16" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>17 0.47593379 <a title="72-lsi-17" href="./iccv-2013-Incorporating_Cloud_Distribution_in_Sky_Representation.html">215 iccv-2013-Incorporating Cloud Distribution in Sky Representation</a></p>
<p>18 0.46451455 <a title="72-lsi-18" href="./iccv-2013-Synergistic_Clustering_of_Image_and_Segment_Descriptors_for_Unsupervised_Scene_Understanding.html">412 iccv-2013-Synergistic Clustering of Image and Segment Descriptors for Unsupervised Scene Understanding</a></p>
<p>19 0.4637627 <a title="72-lsi-19" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<p>20 0.458729 <a title="72-lsi-20" href="./iccv-2013-Learning_CRFs_for_Image_Parsing_with_Adaptive_Subgradient_Descent.html">234 iccv-2013-Learning CRFs for Image Parsing with Adaptive Subgradient Descent</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.073), (7, 0.018), (12, 0.011), (26, 0.081), (31, 0.436), (42, 0.07), (64, 0.033), (73, 0.022), (89, 0.158)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92606843 <a title="72-lda-1" href="./iccv-2013-Recognizing_Text_with_Perspective_Distortion_in_Natural_Scenes.html">345 iccv-2013-Recognizing Text with Perspective Distortion in Natural Scenes</a></p>
<p>Author: Trung Quy Phan, Palaiahnakote Shivakumara, Shangxuan Tian, Chew Lim Tan</p><p>Abstract: This paper presents an approach to text recognition in natural scene images. Unlike most existing works which assume that texts are horizontal and frontal parallel to the image plane, our method is able to recognize perspective texts of arbitrary orientations. For individual character recognition, we adopt a bag-of-keypoints approach, in which Scale Invariant Feature Transform (SIFT) descriptors are extracted densely and quantized using a pre-trained vocabulary. Following [1, 2], the context information is utilized through lexicons. We formulate word recognition as finding the optimal alignment between the set of characters and the list of lexicon words. Furthermore, we introduce a new dataset called StreetViewText-Perspective, which contains texts in street images with a great variety of viewpoints. Experimental results on public datasets and the proposed dataset show that our method significantly outperforms the state-of-the-art on perspective texts of arbitrary orientations.</p><p>2 0.89176905 <a title="72-lda-2" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>Author: Carlos Fernandez-Granda, Emmanuel J. Candès</p><p>Abstract: We present a framework to super-resolve planar regions found in urban scenes and other man-made environments by taking into account their 3D geometry. Such regions have highly structured straight edges, but this prior is challenging to exploit due to deformations induced by the projection onto the imaging plane. Our method factors out such deformations by using recently developed tools based on convex optimization to learn a transform that maps the image to a domain where its gradient has a simple group-sparse structure. This allows to obtain a novel convex regularizer that enforces global consistency constraints between the edges of the image. Computational experiments with real images show that this data-driven approach to the design of regularizers promoting transform-invariant group sparsity is very effective at high super-resolution factors. We view our approach as complementary to most recent superresolution methods, which tend to focus on hallucinating high-frequency textures.</p><p>same-paper 3 0.87660354 <a title="72-lda-3" href="./iccv-2013-Characterizing_Layouts_of_Outdoor_Scenes_Using_Spatial_Topic_Processes.html">72 iccv-2013-Characterizing Layouts of Outdoor Scenes Using Spatial Topic Processes</a></p>
<p>Author: Dahua Lin, Jianxiong Xiao</p><p>Abstract: In this paper, we develop a generative model to describe the layouts of outdoor scenes the spatial configuration of regions. Specifically, the layout of an image is represented as a composite of regions, each associated with a semantic topic. At the heart of this model is a novel stochastic process called Spatial Topic Process, which generates a spatial map of topics from a set of coupled Gaussian processes, thus allowing the distributions of topics to vary continuously across the image plane. A key aspect that distinguishes this model from previous ones consists in its capability of capturing dependencies across both locations and topics while allowing substantial variations in the layouts. We demonstrate the practical utility of the proposed model by testing it on scene classification, semantic segmentation, and layout hallucination. –</p><p>4 0.8511132 <a title="72-lda-4" href="./iccv-2013-Action_Recognition_with_Actons.html">38 iccv-2013-Action Recognition with Actons</a></p>
<p>Author: Jun Zhu, Baoyuan Wang, Xiaokang Yang, Wenjun Zhang, Zhuowen Tu</p><p>Abstract: With the improved accessibility to an exploding amount of video data and growing demands in a wide range of video analysis applications, video-based action recognition/classification becomes an increasingly important task in computer vision. In this paper, we propose a two-layer structure for action recognition to automatically exploit a mid-level “acton ” representation. The weakly-supervised actons are learned via a new max-margin multi-channel multiple instance learning framework, which can capture multiple mid-level action concepts simultaneously. The learned actons (with no requirement for detailed manual annotations) observe theproperties ofbeing compact, informative, discriminative, and easy to scale. The experimental results demonstrate the effectiveness ofapplying the learned actons in our two-layer structure, and show the state-ofthe-art recognition performance on two challenging action datasets, i.e., Youtube and HMDB51.</p><p>5 0.8475821 <a title="72-lda-5" href="./iccv-2013-Robust_Matrix_Factorization_with_Unknown_Noise.html">357 iccv-2013-Robust Matrix Factorization with Unknown Noise</a></p>
<p>Author: Deyu Meng, Fernando De_La_Torre</p><p>Abstract: Many problems in computer vision can be posed as recovering a low-dimensional subspace from highdimensional visual data. Factorization approaches to lowrank subspace estimation minimize a loss function between an observed measurement matrix and a bilinear factorization. Most popular loss functions include the L2 and L1 losses. L2 is optimal for Gaussian noise, while L1 is for Laplacian distributed noise. However, real data is often corrupted by an unknown noise distribution, which is unlikely to be purely Gaussian or Laplacian. To address this problem, this paper proposes a low-rank matrix factorization problem with a Mixture of Gaussians (MoG) noise model. The MoG model is a universal approximator for any continuous distribution, and hence is able to model a wider range of noise distributions. The parameters of the MoG model can be estimated with a maximum likelihood method, while the subspace is computed with standard approaches. We illustrate the benefits of our approach in extensive syn- thetic and real-world experiments including structure from motion, face modeling and background subtraction.</p><p>6 0.76749468 <a title="72-lda-6" href="./iccv-2013-Motion-Aware_KNN_Laplacian_for_Video_Matting.html">275 iccv-2013-Motion-Aware KNN Laplacian for Video Matting</a></p>
<p>7 0.72223568 <a title="72-lda-7" href="./iccv-2013-Modeling_Occlusion_by_Discriminative_AND-OR_Structures.html">269 iccv-2013-Modeling Occlusion by Discriminative AND-OR Structures</a></p>
<p>8 0.68723631 <a title="72-lda-8" href="./iccv-2013-Class-Specific_Simplex-Latent_Dirichlet_Allocation_for_Image_Classification.html">73 iccv-2013-Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification</a></p>
<p>9 0.66951632 <a title="72-lda-9" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>10 0.65136141 <a title="72-lda-10" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>11 0.64560443 <a title="72-lda-11" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<p>12 0.64236152 <a title="72-lda-12" href="./iccv-2013-PhotoOCR%3A_Reading_Text_in_Uncontrolled_Conditions.html">315 iccv-2013-PhotoOCR: Reading Text in Uncontrolled Conditions</a></p>
<p>13 0.61931145 <a title="72-lda-13" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>14 0.61459064 <a title="72-lda-14" href="./iccv-2013-Text_Localization_in_Natural_Images_Using_Stroke_Feature_Transform_and_Text_Covariance_Descriptors.html">415 iccv-2013-Text Localization in Natural Images Using Stroke Feature Transform and Text Covariance Descriptors</a></p>
<p>15 0.60734046 <a title="72-lda-15" href="./iccv-2013-Fluttering_Pattern_Generation_Using_Modified_Legendre_Sequence_for_Coded_Exposure_Imaging.html">173 iccv-2013-Fluttering Pattern Generation Using Modified Legendre Sequence for Coded Exposure Imaging</a></p>
<p>16 0.59960669 <a title="72-lda-16" href="./iccv-2013-A_Learning-Based_Approach_to_Reduce_JPEG_Artifacts_in_Image_Matting.html">19 iccv-2013-A Learning-Based Approach to Reduce JPEG Artifacts in Image Matting</a></p>
<p>17 0.59760308 <a title="72-lda-17" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<p>18 0.59436214 <a title="72-lda-18" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<p>19 0.59182477 <a title="72-lda-19" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>20 0.58150744 <a title="72-lda-20" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
