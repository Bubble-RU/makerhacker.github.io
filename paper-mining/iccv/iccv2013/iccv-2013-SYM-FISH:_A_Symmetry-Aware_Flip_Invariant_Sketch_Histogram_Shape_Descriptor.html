<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>368 iccv-2013-SYM-FISH: A Symmetry-Aware Flip Invariant Sketch Histogram Shape Descriptor</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-368" href="#">iccv2013-368</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>368 iccv-2013-SYM-FISH: A Symmetry-Aware Flip Invariant Sketch Histogram Shape Descriptor</h1>
<br/><p>Source: <a title="iccv-2013-368-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Cao_SYM-FISH_A_Symmetry-Aware_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Xiaochun Cao, Hua Zhang, Si Liu, Xiaojie Guo, Liang Lin</p><p>Abstract: Recently, studies on sketch, such as sketch retrieval and sketch classification, have received more attention in the computer vision community. One of its most fundamental and essential problems is how to more effectively describe a sketch image. Many existing descriptors, such as shape context, have achieved great success. In this paper, we propose a new descriptor, namely Symmetric-aware Flip Invariant Sketch Histogram (SYM-FISH) to refine the shape context feature. Its extraction process includes three steps. First the Flip Invariant Sketch Histogram (FISH) descriptor is extracted on the input image, which is a flip-invariant version of the shape context feature. Then we explore the symmetry character of the image by calculating the kurtosis coefficient. Finally, the SYM-FISH is generated by constructing a symmetry table. The new SYM-FISH descriptor supplements the original shape context by encoding the symmetric information, which is a pervasive characteristic of natural scene and objects. We evaluate the efficacy of the novel descriptor in two applications, i.e., sketch retrieval and sketch classification. Extensive experiments on three datasets well demonstrate the effectiveness and robustness of the proposed SYM-FISH descriptor.</p><p>Reference: <a title="iccv-2013-368-reference" href="../iccv2013_reference/iccv-2013-SYM-FISH%3A_A_Symmetry-Aware_Flip_Invariant_Sketch_Histogram_Shape_Descriptor_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 org  Abstract Recently, studies on sketch, such as sketch retrieval and sketch classification, have received more attention in the computer vision community. [sent-11, score-1.344]
</p><p>2 One of its most fundamental and essential problems is how to more effectively describe a sketch image. [sent-12, score-0.615]
</p><p>3 Then we explore the symmetry character of the image by calculating the kurtosis coefficient. [sent-17, score-0.738]
</p><p>4 Finally, the SYM-FISH is generated by constructing a symmetry table. [sent-18, score-0.555]
</p><p>5 The new SYM-FISH descriptor supplements the original shape context by encoding the symmetric information, which is a pervasive characteristic of natural scene and objects. [sent-19, score-0.305]
</p><p>6 iPad and Microsoft Surface, sketch related studies become unprecedented popular nowadays. [sent-27, score-0.615]
</p><p>7 The sketches drawn by users are used as queries to feed into any of the sketch retrieval system. [sent-29, score-0.817]
</p><p>8 The first query is non-symmetric, the second query is bilateral symmetric while the last one is rotation symmetric. [sent-33, score-0.354]
</p><p>9 For each query, the retrieval results of three kinds of shape descriptors: shape context, FISH and SYM-FISH are shown sequentially in different rows. [sent-34, score-0.258]
</p><p>10 The first column is the query sketch images, and the remaining columns are returned real life images. [sent-35, score-0.78]
</p><p>11 Actually, beside sketch retrieval [5], many other edge related tasks, such as sketch detection [23] and sketch recognition [2] are also extensively studied. [sent-39, score-1.959]
</p><p>12 Moreover the symmetry is of scale invariance as well as translation invariance. [sent-44, score-0.596]
</p><p>13 Although there is a long history of symmetry study [9, 13, 21], it has rarely been integrated into descriptors in a unified framework. [sent-45, score-0.603]
</p><p>14 First, the image is represented by a flip invariant descriptor. [sent-48, score-0.254]
</p><p>15 Then we minimize the energy measurement to determine the symmetry directions in each image patch. [sent-56, score-0.555]
</p><p>16 Finally, we incorporate the symmetry character into image representation. [sent-57, score-0.6]
</p><p>17 We construct a graph named symmetry table to describe the symmetry character and generate the SYMmetric-aware Flip Invariant Sketch Histogram (SYM-  FISH)(Section 4. [sent-58, score-1.155]
</p><p>18 Please note that, based on the graph, we can handle both the case with and without symmetry property in the image. [sent-60, score-0.586]
</p><p>19 To validate the effectiveness of our proposed approach, we apply it on two applications: sketch retrieval [5] and sketch classification [15]. [sent-61, score-1.405]
</p><p>20 The sketch retrieval task is quite challenging because of the huge gap between the sketch query and real life repository images. [sent-62, score-1.557]
</p><p>21 Some of the representative works on sketch retrieval are MindFinder [4] [5] and Sketch2photo [6]. [sent-63, score-0.729]
</p><p>22 However, little attention is paid on studying the symmetry character of the images. [sent-65, score-0.6]
</p><p>23 Furthermore, we do the experiment on two benchmark datasets: ETH shape dataset [8] and a large scale sketch retrieval dataset [17]. [sent-68, score-0.887]
</p><p>24 , sketch images from the same category may have certain common preference to symmetry. [sent-73, score-0.635]
</p><p>25 We conduct extensive experiments on the sketch dataset [15]. [sent-75, score-0.63]
</p><p>26 Experimental results show that the proposed SYM-FISH descriptor is more discriminating than standard descriptors, such as shape context, and can significantly improve sketch classification performance. [sent-76, score-0.791]
</p><p>27 Related work There are limited related works on sketch classification. [sent-78, score-0.615]
</p><p>28 However, all the aforementioned descriptors do not enforce the descriptors to be flip invariant, while our propose SYM-FISH can handle the flip cases well. [sent-84, score-0.555]
</p><p>29 Therefore, we propose a novel sketch descriptors which can handle various transformations e. [sent-90, score-0.678]
</p><p>30 Thus, in our proposed method, we introduce the symmetry structure of the image to compensate such shortcoming. [sent-95, score-0.555]
</p><p>31 A recent related work is [10], which proposed a symmetry score approach to find the symmetry feature points, afterwards constructed a symmetry descriptor for building matching. [sent-97, score-1.804]
</p><p>32 Our proposed symmetry visual word phase is robust to the distortions and noise. [sent-106, score-0.622]
</p><p>33 FISH descriptor construction process: sampling feature points, mapping sampled points in the log-polar coordinate, and developing the descriptor representation. [sent-109, score-0.244]
</p><p>34 metry character of the image by analyzing matching scores  and 3) constructing a symmetry table combined with FISH descriptor to finally generate the SYM-FISH. [sent-112, score-0.71]
</p><p>35 Although shape context has achieved great success, it cannot handle the flip case. [sent-121, score-0.382]
</p><p>36 2 (a) are quite similar (only under flip changes). [sent-124, score-0.236]
</p><p>37 To handle the flip variations, we propose a FISH descriptor, which can be viewed as a post-processing procedure after shape context feature is extracted. [sent-125, score-0.388]
</p><p>38 More specifically, we re-order all the bins in the shape context by two steps: determine the reference bin and the rotation orientation sequentially. [sent-126, score-0.284]
</p><p>39 To sum up, we can roughly align the FISH features by re-ordering the bins of shape context according to the inferred reference bin and the rotation orientation. [sent-140, score-0.262]
</p><p>40 However there is no benchmark sketch dataset specially for matching, a sketch pairs database is collected by ourselves. [sent-160, score-1.301]
</p><p>41 each pair is consisted of original image and its rotation, flip and scale version. [sent-162, score-0.239]
</p><p>42 In the sketch pair database, the orientation angle is randomly selected from (0,360). [sent-163, score-0.652]
</p><p>43 For the flip situation, we flip the whole original image. [sent-165, score-0.444]
</p><p>44 The setting of the matching experiment is as follows: 300 feature points are sampled on two sketch images, then an adjacency matrix is constructed by computing the similarity among their FISH descriptors. [sent-166, score-0.735]
</p><p>45 The red line indicates the detected symmetry axis of the original image. [sent-179, score-0.592]
</p><p>46 For bilateral symmetry image (a), the kurtosis value is large and the distribution has a peak (d). [sent-180, score-0.767]
</p><p>47 While the rotation symmetry image (b) has a small kurtosis value and a flat distribution (e). [sent-181, score-0.762]
</p><p>48 Specially, two separated parts could be mapped to each other by the angle of the symmetry axis. [sent-195, score-0.57]
</p><p>49 2n-fold rotation symmetry: There exist more than two symmetry lines of a sketch, while these lines are intersected in one point. [sent-198, score-0.624]
</p><p>50 In our problem, if a sketch include two symmetry lines, which are not orthogonal, we also define it as rotation symmetry. [sent-199, score-1.239]
</p><p>51 Thus, discovering local symmetry of a region can be converted to search the symmetric axis of the sketch. [sent-202, score-0.702]
</p><p>52 To detect the symmetry axis on the input sketch, we propose a compact energy minimization method. [sent-203, score-0.575]
</p><p>53 The whole strategy is 1The curved symmetry may be considered as another symmetry categories. [sent-204, score-1.125]
</p><p>54 But we think the curved symmetry is a kind of piecewise bilateral symmetry. [sent-205, score-0.658]
</p><p>55 symmetric 14: end ifimage Ii is not symmetric 15: end if 16: Output: its symmetry type and the symmetric points;  shown in Algorithm 1. [sent-221, score-0.834]
</p><p>56 O represents tahgee symmetry sdpirlaeycstio tnhes, Ewuhcilcidhe aisn f idxiesdta fnrcoem. [sent-235, score-0.555]
</p><p>57 Then, we select the minimum scores of each orientation o as the potential symmetry orientation, We accumulate all the scores to generate a 36dim vector, shown in line 6 of Algorithm 1. [sent-237, score-0.594]
</p><p>58 We observe that for the bilateral symmetry sketch the matching score S coreIi(O) has an unimodal distribution, while for the rotation symmetry, S coreIi(O) has a multimodal distribution. [sent-238, score-1.359]
</p><p>59 λ(λ − μλ)4ScoreIi  (3)  3 16  where σ is the standard deviation and λ represents the angel of the symmetry axis. [sent-241, score-0.555]
</p><p>60 Usually, the bilateral symmetry produces much higher Kurt score comparing with rotation symmetric and non-symmetric, as shown in Fig. [sent-244, score-0.81]
</p><p>61 Evaluation of symmetry discovering: We test the effectiveness Algorithm 1on a subset of sketch database [17]. [sent-257, score-1.203]
</p><p>62 The validation database is composed of 3 1human drawing sketches, which contain bilateral symmetry sketch, rotation symmetry sketch and non-symmetry sketch. [sent-258, score-1.884]
</p><p>63 We would like to know whether the symmetry type can be correctly classified. [sent-259, score-0.555]
</p><p>64 Symmetry-aware togram  Flip Invariant  Sketch His-  In image retrieval and classification the local descriptors, such as SIFT [14], shape context [2] and FISH, will not be directly used for the image representation. [sent-265, score-0.264]
</p><p>65 Usually, we summarize all the local descriptors in a sketch image with the Bags of words (BoWs) representation. [sent-266, score-0.678]
</p><p>66 Thus,  in this section, we will illustrate how to fuse the symmetry property among feature points into the visual word representation. [sent-267, score-0.669]
</p><p>67 In this paper we only focus on symmetry and do not exploit other spatial structure of the sketch images. [sent-270, score-1.17]
</p><p>68 We propose to use a symmetry table to capture the symmetry relations among visual words. [sent-271, score-1.124]
</p><p>69 Thirdly, we map the symmetry of feature points to symmetry of visual words. [sent-278, score-1.175]
</p><p>70 Finally, we construct a symmetry table Y ∈ {0, 1}, which is a N×N matrix, where N is the number oYf ∈vi {s0u,a1l w,o wrhdisc. [sent-280, score-0.555]
</p><p>71 h Y is i as an Nind mexa mrixa,tr wixh, wreh Nose is e thleem neunmt Yi,j indicates whether the visual word Vi and Vj are symmetric in the sketch images. [sent-281, score-0.755]
</p><p>72 With the symmetry table, the symmetry relationship is transferred from feature points level to the visual words level. [sent-283, score-1.19]
</p><p>73 To sum up, besides the original BoWs feature, for each sketch image, we have a new structural feature called SYMmetry-aware Flip In-  variant Sketch Histogram (SYM-FISH) shape feature. [sent-284, score-0.709]
</p><p>74 It is the combination of original FISH feature and a symmetry table. [sent-285, score-0.577]
</p><p>75 Thus the distance between two symmetry table is just humming distance. [sent-287, score-0.575]
</p><p>76 SYM-FISH descriptor in sketch retrieval Searching the real life images by using a sketch query is not an easy task. [sent-291, score-1.574]
</p><p>77 For example, sketch images convey information mostly by edges while real life images always have rich texture. [sent-293, score-0.703]
</p><p>78 The SYM-FISH is used in the sketch retrieval task by re-ranking the original ranking list. [sent-294, score-0.742]
</p><p>79 For the SYM-FISH reranking, we first extract the symmetry table for all the images in the repository. [sent-296, score-0.555]
</p><p>80 Then the original list is reorder by the distances between symmetry table. [sent-297, score-0.555]
</p><p>81 SYM-FISH descriptor in sketch classification In the traditional classification approach [15] , chi-square distance is usually used to compute the similarity between different images while the symmetry character of the sketches is not considered. [sent-320, score-1.448]
</p><p>82 In our approach, we combine the distance between visual word representation and the similarity of symmetry table. [sent-322, score-0.622]
</p><p>83 Formally, we have: D(Ii, Ij) = χ2(Ii, Ij) + λ  ∗  S T(Ii, Ij)  (6)  where χ2(Ii, Ij) computes the chi-square distance between different images, and S T(Ii, Ij) is the similarity of symmetry table between different images. [sent-323, score-0.575]
</p><p>84 Experiments We evaluate our proposed descriptor FISH and SYM-  FISH on two applications : sketch retrieval and sketch classification. [sent-329, score-1.427]
</p><p>85 In the sketch retrieval experiment, we first check the performance of FISH and SYM-FISH on ETH shape database [8]. [sent-330, score-0.817]
</p><p>86 The further validation of these two descriptors is on a large scale sketch retrieval dataset [17]. [sent-331, score-0.809]
</p><p>87 In the sketch classification experiment, we test the performance of proposed descriptors on sketch classification dataset [15]. [sent-332, score-1.335]
</p><p>88 The ETH dataset provides one representative sketch image for each class, which is used as the input query in our experiment. [sent-341, score-0.689]
</p><p>89 The possible explanation is that the proposed SYM-FISH descriptor becomes more robustness because of the novel encoding approach and can better handle the flip situation. [sent-348, score-0.32]
</p><p>90 Moreover, the introduced symmetry property enables the representation more discriminative. [sent-349, score-0.571]
</p><p>91 The benchmark dataset contains 3 1benchmark sketches as well as 40 corresponding images for each sketch while the distractor image dataset contains 100,000 creative commons images. [sent-361, score-0.787]
</p><p>92 1 2684-7F026I189SH  In this section, we test the effectiveness of the proposed SYM-FISH on the sketch classification task. [sent-380, score-0.653]
</p><p>93 Each sketch class contains about 80 images with different styles. [sent-382, score-0.615]
</p><p>94 To train the sketch model, we randomly divide the dataset into 2 subset: 58 images from each category are randomly selected as the training set and the remaining images are used as testing set. [sent-383, score-0.65]
</p><p>95 The first column is the query sketch image, while the remaining columns correspond to the retrieved real life images. [sent-390, score-0.762]
</p><p>96 The reasons can be summarized as follows: firstly there exists many flip situations in the dataset 3 19  Table 3. [sent-407, score-0.259]
</p><p>97 Secondly, the symmetry table can better preserve the symmetry properties of the sketches which both decrease the intra-category distances and increase inter-category distances. [sent-411, score-1.198]
</p><p>98 Conclusion and Future Work  In this paper, we propose a novel shape descriptor SYMFISH which can handle the flip changes and encode image’s symmetric property. [sent-413, score-0.485]
</p><p>99 We thoroughly analyze its characteristics on two applications: sketch retrieval and classification. [sent-415, score-0.729]
</p><p>100 Although we only validate the effectiveness of the descriptor on sketch retrieval and classification tasks in this paper, we believe that it can also be used in other tasks, such as sketch detection. [sent-417, score-1.488]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sketch', 0.615), ('symmetry', 0.555), ('fish', 0.29), ('flip', 0.222), ('kurtosis', 0.138), ('retrieval', 0.114), ('symmetric', 0.093), ('mdb', 0.089), ('sketches', 0.088), ('descriptor', 0.083), ('bilateral', 0.074), ('shape', 0.072), ('life', 0.069), ('rotation', 0.069), ('kurt', 0.066), ('vscore', 0.059), ('query', 0.059), ('context', 0.057), ('subwindows', 0.057), ('repository', 0.052), ('descriptors', 0.048), ('character', 0.045), ('coreii', 0.044), ('smdb', 0.044), ('symfish', 0.044), ('bows', 0.04), ('bin', 0.04), ('mindfinder', 0.039), ('word', 0.033), ('invariant', 0.032), ('distractor', 0.03), ('concordant', 0.03), ('kristian', 0.03), ('tamy', 0.03), ('tianjin', 0.03), ('till', 0.029), ('points', 0.029), ('mathias', 0.028), ('matching', 0.027), ('sampled', 0.027), ('ii', 0.026), ('iq', 0.025), ('bins', 0.024), ('histogram', 0.024), ('benchmark', 0.024), ('translation', 0.024), ('highlighted', 0.023), ('stars', 0.023), ('validate', 0.023), ('eth', 0.023), ('cao', 0.022), ('orientation', 0.022), ('feature', 0.022), ('firstly', 0.022), ('discovering', 0.021), ('classification', 0.021), ('ij', 0.021), ('phase', 0.02), ('distance', 0.02), ('category', 0.02), ('axis', 0.02), ('academy', 0.02), ('dissimilar', 0.02), ('traverse', 0.02), ('score', 0.019), ('real', 0.019), ('china', 0.019), ('chinese', 0.019), ('returned', 0.018), ('secondly', 0.017), ('line', 0.017), ('scale', 0.017), ('effectiveness', 0.017), ('dictionary', 0.017), ('database', 0.016), ('great', 0.016), ('property', 0.016), ('objectness', 0.016), ('specially', 0.016), ('words', 0.015), ('inverted', 0.015), ('afterwards', 0.015), ('curved', 0.015), ('dataset', 0.015), ('sciences', 0.015), ('experiment', 0.015), ('tpami', 0.015), ('angle', 0.015), ('handle', 0.015), ('vj', 0.014), ('guo', 0.014), ('think', 0.014), ('windows', 0.014), ('quite', 0.014), ('visual', 0.014), ('converted', 0.013), ('star', 0.013), ('ranking', 0.013), ('coefficient', 0.013), ('singapore', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="368-tfidf-1" href="./iccv-2013-SYM-FISH%3A_A_Symmetry-Aware_Flip_Invariant_Sketch_Histogram_Shape_Descriptor.html">368 iccv-2013-SYM-FISH: A Symmetry-Aware Flip Invariant Sketch Histogram Shape Descriptor</a></p>
<p>Author: Xiaochun Cao, Hua Zhang, Si Liu, Xiaojie Guo, Liang Lin</p><p>Abstract: Recently, studies on sketch, such as sketch retrieval and sketch classification, have received more attention in the computer vision community. One of its most fundamental and essential problems is how to more effectively describe a sketch image. Many existing descriptors, such as shape context, have achieved great success. In this paper, we propose a new descriptor, namely Symmetric-aware Flip Invariant Sketch Histogram (SYM-FISH) to refine the shape context feature. Its extraction process includes three steps. First the Flip Invariant Sketch Histogram (FISH) descriptor is extracted on the input image, which is a flip-invariant version of the shape context feature. Then we explore the symmetry character of the image by calculating the kurtosis coefficient. Finally, the SYM-FISH is generated by constructing a symmetry table. The new SYM-FISH descriptor supplements the original shape context by encoding the symmetric information, which is a pervasive characteristic of natural scene and objects. We evaluate the efficacy of the novel descriptor in two applications, i.e., sketch retrieval and sketch classification. Extensive experiments on three datasets well demonstrate the effectiveness and robustness of the proposed SYM-FISH descriptor.</p><p>2 0.37130237 <a title="368-tfidf-2" href="./iccv-2013-3D_Sub-query_Expansion_for_Improving_Sketch-Based_Multi-view_Image_Retrieval.html">3 iccv-2013-3D Sub-query Expansion for Improving Sketch-Based Multi-view Image Retrieval</a></p>
<p>Author: Yen-Liang Lin, Cheng-Yu Huang, Hao-Jeng Wang, Winston Hsu</p><p>Abstract: We propose a 3D sub-query expansion approach for boosting sketch-based multi-view image retrieval. The core idea of our method is to automatically convert two (guided) 2D sketches into an approximated 3D sketch model, and then generate multi-view sketches as expanded sub-queries to improve the retrieval performance. To learn the weights among synthesized views (sub-queries), we present a new multi-query feature to model the similarity between subqueries and dataset images, and formulate it into a convex optimization problem. Our approach shows superior performance compared with the state-of-the-art approach on a public multi-view image dataset. Moreover, we also conduct sensitivity tests to analyze the parameters of our approach based on the gathered user sketches.</p><p>3 0.2008058 <a title="368-tfidf-3" href="./iccv-2013-Detecting_Curved_Symmetric_Parts_Using_a_Deformable_Disc_Model.html">110 iccv-2013-Detecting Curved Symmetric Parts Using a Deformable Disc Model</a></p>
<p>Author: Tom Sie Ho Lee, Sanja Fidler, Sven Dickinson</p><p>Abstract: Symmetry is a powerful shape regularity that’s been exploited by perceptual grouping researchers in both human and computer vision to recover part structure from an image without a priori knowledge of scene content. Drawing on the concept of a medial axis, defined as the locus of centers of maximal inscribed discs that sweep out a symmetric part, we model part recovery as the search for a sequence of deformable maximal inscribed disc hypotheses generated from a multiscale superpixel segmentation, a framework proposed by [13]. However, we learn affinities between adjacent superpixels in a space that’s invariant to bending and tapering along the symmetry axis, enabling us to capture a wider class of symmetric parts. Moreover, we introduce a global cost that perceptually integrates the hypothesis space by combining a pairwise and a higher-level smoothing term, which we minimize globally using dynamic programming. The new framework is demonstrated on two datasets, and is shown to significantly outperform the baseline [13].</p><p>4 0.14990091 <a title="368-tfidf-4" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>Author: Jifeng Dai, Ying Nian Wu, Jie Zhou, Song-Chun Zhu</p><p>Abstract: Cosegmentation refers to theproblem ofsegmenting multiple images simultaneously by exploiting the similarities between the foreground and background regions in these images. The key issue in cosegmentation is to align common objects between these images. To address this issue, we propose an unsupervised learning framework for cosegmentation, by coupling cosegmentation with what we call “cosketch ”. The goal of cosketch is to automatically discover a codebook of deformable shape templates shared by the input images. These shape templates capture distinct image patterns and each template is matched to similar image patches in different images. Thus the cosketch of the images helps to align foreground objects, thereby providing crucial information for cosegmentation. We present a statistical model whose energy function couples cosketch and cosegmentation. We then present an unsupervised learning algorithm that performs cosketch and cosegmentation by energy minimization. Experiments show that our method outperforms state of the art methods for cosegmentation on the challenging MSRC and iCoseg datasets. We also illustrate our method on a new dataset called Coseg-Rep where cosegmentation can be performed within a single image with repetitive patterns.</p><p>5 0.095560551 <a title="368-tfidf-5" href="./iccv-2013-Revisiting_the_PnP_Problem%3A_A_Fast%2C_General_and_Optimal_Solution.html">353 iccv-2013-Revisiting the PnP Problem: A Fast, General and Optimal Solution</a></p>
<p>Author: Yinqiang Zheng, Yubin Kuang, Shigeki Sugimoto, Kalle Åström, Masatoshi Okutomi</p><p>Abstract: In this paper, we revisit the classical perspective-n-point (PnP) problem, and propose the first non-iterative O(n) solution that is fast, generally applicable and globally optimal. Our basic idea is to formulate the PnP problem into a functional minimization problem and retrieve all its stationary points by using the Gr¨ obner basis technique. The novelty lies in a non-unit quaternion representation to parameterize the rotation and a simple but elegant formulation of the PnP problem into an unconstrained optimization problem. Interestingly, the polynomial system arising from its first-order optimality condition assumes two-fold symmetry, a nice property that can be utilized to improve speed and numerical stability of a Gr¨ obner basis solver. Experiment results have demonstrated that, in terms of accuracy, our proposed solution is definitely better than the state-ofthe-art O(n) methods, and even comparable with the reprojection error minimization method.</p><p>6 0.081340723 <a title="368-tfidf-6" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<p>7 0.074691482 <a title="368-tfidf-7" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>8 0.069033064 <a title="368-tfidf-8" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>9 0.06606207 <a title="368-tfidf-9" href="./iccv-2013-Human_Re-identification_by_Matching_Compositional_Template_with_Cluster_Sampling.html">205 iccv-2013-Human Re-identification by Matching Compositional Template with Cluster Sampling</a></p>
<p>10 0.065981559 <a title="368-tfidf-10" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>11 0.058988098 <a title="368-tfidf-11" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>12 0.058441352 <a title="368-tfidf-12" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>13 0.058249161 <a title="368-tfidf-13" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>14 0.056828454 <a title="368-tfidf-14" href="./iccv-2013-Content-Aware_Rotation.html">90 iccv-2013-Content-Aware Rotation</a></p>
<p>15 0.055337302 <a title="368-tfidf-15" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>16 0.051617742 <a title="368-tfidf-16" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>17 0.050734673 <a title="368-tfidf-17" href="./iccv-2013-Query-Adaptive_Asymmetrical_Dissimilarities_for_Visual_Object_Retrieval.html">334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</a></p>
<p>18 0.049415793 <a title="368-tfidf-18" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>19 0.048447493 <a title="368-tfidf-19" href="./iccv-2013-Quantize_and_Conquer%3A_A_Dimensionality-Recursive_Solution_to_Clustering%2C_Vector_Quantization%2C_and_Image_Retrieval.html">333 iccv-2013-Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval</a></p>
<p>20 0.04841071 <a title="368-tfidf-20" href="./iccv-2013-Recognizing_Text_with_Perspective_Distortion_in_Natural_Scenes.html">345 iccv-2013-Recognizing Text with Perspective Distortion in Natural Scenes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.114), (1, 0.011), (2, -0.022), (3, -0.056), (4, 0.011), (5, 0.063), (6, 0.007), (7, -0.032), (8, -0.051), (9, -0.01), (10, 0.103), (11, 0.014), (12, 0.017), (13, 0.044), (14, 0.017), (15, 0.032), (16, 0.106), (17, -0.031), (18, 0.067), (19, -0.068), (20, 0.07), (21, -0.051), (22, -0.047), (23, 0.039), (24, 0.007), (25, 0.065), (26, 0.03), (27, 0.065), (28, -0.036), (29, 0.015), (30, -0.009), (31, 0.049), (32, -0.001), (33, 0.008), (34, 0.021), (35, 0.002), (36, -0.012), (37, 0.035), (38, -0.089), (39, -0.13), (40, -0.049), (41, -0.125), (42, -0.075), (43, -0.008), (44, 0.044), (45, -0.057), (46, 0.047), (47, 0.081), (48, -0.146), (49, -0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93051809 <a title="368-lsi-1" href="./iccv-2013-SYM-FISH%3A_A_Symmetry-Aware_Flip_Invariant_Sketch_Histogram_Shape_Descriptor.html">368 iccv-2013-SYM-FISH: A Symmetry-Aware Flip Invariant Sketch Histogram Shape Descriptor</a></p>
<p>Author: Xiaochun Cao, Hua Zhang, Si Liu, Xiaojie Guo, Liang Lin</p><p>Abstract: Recently, studies on sketch, such as sketch retrieval and sketch classification, have received more attention in the computer vision community. One of its most fundamental and essential problems is how to more effectively describe a sketch image. Many existing descriptors, such as shape context, have achieved great success. In this paper, we propose a new descriptor, namely Symmetric-aware Flip Invariant Sketch Histogram (SYM-FISH) to refine the shape context feature. Its extraction process includes three steps. First the Flip Invariant Sketch Histogram (FISH) descriptor is extracted on the input image, which is a flip-invariant version of the shape context feature. Then we explore the symmetry character of the image by calculating the kurtosis coefficient. Finally, the SYM-FISH is generated by constructing a symmetry table. The new SYM-FISH descriptor supplements the original shape context by encoding the symmetric information, which is a pervasive characteristic of natural scene and objects. We evaluate the efficacy of the novel descriptor in two applications, i.e., sketch retrieval and sketch classification. Extensive experiments on three datasets well demonstrate the effectiveness and robustness of the proposed SYM-FISH descriptor.</p><p>2 0.81789792 <a title="368-lsi-2" href="./iccv-2013-3D_Sub-query_Expansion_for_Improving_Sketch-Based_Multi-view_Image_Retrieval.html">3 iccv-2013-3D Sub-query Expansion for Improving Sketch-Based Multi-view Image Retrieval</a></p>
<p>Author: Yen-Liang Lin, Cheng-Yu Huang, Hao-Jeng Wang, Winston Hsu</p><p>Abstract: We propose a 3D sub-query expansion approach for boosting sketch-based multi-view image retrieval. The core idea of our method is to automatically convert two (guided) 2D sketches into an approximated 3D sketch model, and then generate multi-view sketches as expanded sub-queries to improve the retrieval performance. To learn the weights among synthesized views (sub-queries), we present a new multi-query feature to model the similarity between subqueries and dataset images, and formulate it into a convex optimization problem. Our approach shows superior performance compared with the state-of-the-art approach on a public multi-view image dataset. Moreover, we also conduct sensitivity tests to analyze the parameters of our approach based on the gathered user sketches.</p><p>3 0.53529167 <a title="368-lsi-3" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<p>Author: Shi Qiu, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: This paper proposes modeling the complex web image collections with an automatically generated graph structure called visual semantic complex network (VSCN). The nodes on this complex network are clusters of images with both visual and semantic consistency, called semantic concepts. These nodes are connected based on the visual and semantic correlations. Our VSCN with 33, 240 concepts is generated from a collection of 10 million web images. 1 A great deal of valuable information on the structures of the web image collections can be revealed by exploring the VSCN, such as the small-world behavior, concept community, indegree distribution, hubs, and isolated concepts. It not only helps us better understand the web image collections at a macroscopic level, but also has many important practical applications. This paper presents two application examples: content-based image retrieval and image browsing. Experimental results show that the VSCN leads to significant improvement on both the precision of image retrieval (over 200%) and user experience for image browsing.</p><p>4 0.52297658 <a title="368-lsi-4" href="./iccv-2013-Paper_Doll_Parsing%3A_Retrieving_Similar_Styles_to_Parse_Clothing_Items.html">306 iccv-2013-Paper Doll Parsing: Retrieving Similar Styles to Parse Clothing Items</a></p>
<p>Author: Kota Yamaguchi, M. Hadi Kiapour, Tamara L. Berg</p><p>Abstract: Clothing recognition is an extremely challenging problem due to wide variation in clothing item appearance, layering, and style. In this paper, we tackle the clothing parsing problem using a retrieval based approach. For a query image, we find similar styles from a large database of tagged fashion images and use these examples to parse the query. Our approach combines parsing from: pre-trained global clothing models, local clothing models learned on theflyfrom retrieved examples, and transferredparse masks (paper doll item transfer) from retrieved examples. Experimental evaluation shows that our approach significantly outperforms state of the art in parsing accuracy.</p><p>5 0.52081466 <a title="368-lsi-5" href="./iccv-2013-Content-Aware_Rotation.html">90 iccv-2013-Content-Aware Rotation</a></p>
<p>Author: Kaiming He, Huiwen Chang, Jian Sun</p><p>Abstract: We present an image editing tool called Content-Aware Rotation. Casually shot photos can appear tilted, and are often corrected by rotation and cropping. This trivial solution may remove desired content and hurt image integrity. Instead of doing rigid rotation, we propose a warping method that creates the perception of rotation and avoids cropping. Human vision studies suggest that the perception of rotation is mainly due to horizontal/vertical lines. We design an optimization-based method that preserves the rotation of horizontal/vertical lines, maintains the completeness of the image content, and reduces the warping distortion. An efficient algorithm is developed to address the challenging optimization. We demonstrate our content-aware rotation method on a variety of practical cases.</p><p>6 0.51287919 <a title="368-lsi-6" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>7 0.49469993 <a title="368-lsi-7" href="./iccv-2013-Query-Adaptive_Asymmetrical_Dissimilarities_for_Visual_Object_Retrieval.html">334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</a></p>
<p>8 0.47886038 <a title="368-lsi-8" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>9 0.45735407 <a title="368-lsi-9" href="./iccv-2013-To_Aggregate_or_Not_to_aggregate%3A_Selective_Match_Kernels_for_Image_Search.html">419 iccv-2013-To Aggregate or Not to aggregate: Selective Match Kernels for Image Search</a></p>
<p>10 0.45709121 <a title="368-lsi-10" href="./iccv-2013-Detecting_Curved_Symmetric_Parts_Using_a_Deformable_Disc_Model.html">110 iccv-2013-Detecting Curved Symmetric Parts Using a Deformable Disc Model</a></p>
<p>11 0.44860181 <a title="368-lsi-11" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>12 0.44301239 <a title="368-lsi-12" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>13 0.43696412 <a title="368-lsi-13" href="./iccv-2013-Example-Based_Facade_Texture_Synthesis.html">148 iccv-2013-Example-Based Facade Texture Synthesis</a></p>
<p>14 0.43056694 <a title="368-lsi-14" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>15 0.42770839 <a title="368-lsi-15" href="./iccv-2013-Learning_Coupled_Feature_Spaces_for_Cross-Modal_Matching.html">235 iccv-2013-Learning Coupled Feature Spaces for Cross-Modal Matching</a></p>
<p>16 0.42431647 <a title="368-lsi-16" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>17 0.41931626 <a title="368-lsi-17" href="./iccv-2013-Optimization_Problems_for_Fast_AAM_Fitting_in-the-Wild.html">302 iccv-2013-Optimization Problems for Fast AAM Fitting in-the-Wild</a></p>
<p>18 0.40754795 <a title="368-lsi-18" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>19 0.40721747 <a title="368-lsi-19" href="./iccv-2013-Multi-scale_Topological_Features_for_Hand_Posture_Representation_and_Analysis.html">278 iccv-2013-Multi-scale Topological Features for Hand Posture Representation and Analysis</a></p>
<p>20 0.40531847 <a title="368-lsi-20" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.15), (4, 0.01), (5, 0.196), (7, 0.023), (8, 0.018), (12, 0.016), (26, 0.057), (31, 0.035), (34, 0.016), (42, 0.103), (48, 0.01), (58, 0.013), (64, 0.029), (73, 0.024), (78, 0.019), (89, 0.166)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82279491 <a title="368-lda-1" href="./iccv-2013-SYM-FISH%3A_A_Symmetry-Aware_Flip_Invariant_Sketch_Histogram_Shape_Descriptor.html">368 iccv-2013-SYM-FISH: A Symmetry-Aware Flip Invariant Sketch Histogram Shape Descriptor</a></p>
<p>Author: Xiaochun Cao, Hua Zhang, Si Liu, Xiaojie Guo, Liang Lin</p><p>Abstract: Recently, studies on sketch, such as sketch retrieval and sketch classification, have received more attention in the computer vision community. One of its most fundamental and essential problems is how to more effectively describe a sketch image. Many existing descriptors, such as shape context, have achieved great success. In this paper, we propose a new descriptor, namely Symmetric-aware Flip Invariant Sketch Histogram (SYM-FISH) to refine the shape context feature. Its extraction process includes three steps. First the Flip Invariant Sketch Histogram (FISH) descriptor is extracted on the input image, which is a flip-invariant version of the shape context feature. Then we explore the symmetry character of the image by calculating the kurtosis coefficient. Finally, the SYM-FISH is generated by constructing a symmetry table. The new SYM-FISH descriptor supplements the original shape context by encoding the symmetric information, which is a pervasive characteristic of natural scene and objects. We evaluate the efficacy of the novel descriptor in two applications, i.e., sketch retrieval and sketch classification. Extensive experiments on three datasets well demonstrate the effectiveness and robustness of the proposed SYM-FISH descriptor.</p><p>2 0.77899325 <a title="368-lda-2" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>Author: Peng Jiang, Haibin Ling, Jingyi Yu, Jingliang Peng</p><p>Abstract: The goal of saliency detection is to locate important pixels or regions in an image which attract humans ’ visual attention the most. This is a fundamental task whose output may serve as the basis for further computer vision tasks like segmentation, resizing, tracking and so forth. In this paper we propose a novel salient region detection algorithm by integrating three important visual cues namely uniqueness, focusness and objectness (UFO). In particular, uniqueness captures the appearance-derived visual contrast; focusness reflects the fact that salient regions are often photographed in focus; and objectness helps keep completeness of detected salient regions. While uniqueness has been used for saliency detection for long, it is new to integrate focusness and objectness for this purpose. In fact, focusness and objectness both provide important saliency information complementary of uniqueness. In our experiments using public benchmark datasets, we show that, even with a simple pixel level combination of the three components, the proposed approach yields significant improve- ment compared with previously reported methods.</p><p>3 0.77863628 <a title="368-lda-3" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>Author: Arash Vahdat, Greg Mori</p><p>Abstract: Gathering accurate training data for recognizing a set of attributes or tags on images or videos is a challenge. Obtaining labels via manual effort or from weakly-supervised data typically results in noisy training labels. We develop the FlipSVM, a novel algorithm for handling these noisy, structured labels. The FlipSVM models label noise by “flipping ” labels on training examples. We show empirically that the FlipSVM is effective on images-and-attributes and video tagging datasets.</p><p>4 0.77827668 <a title="368-lda-4" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>Author: Jun Wang, Wei Liu, Andy X. Sun, Yu-Gang Jiang</p><p>Abstract: Hashing techniques have been intensively investigated in the design of highly efficient search engines for largescale computer vision applications. Compared with prior approximate nearest neighbor search approaches like treebased indexing, hashing-based search schemes have prominent advantages in terms of both storage and computational efficiencies. Moreover, the procedure of devising hash functions can be easily incorporated into sophisticated machine learning tools, leading to data-dependent and task-specific compact hash codes. Therefore, a number of learning paradigms, ranging from unsupervised to supervised, have been applied to compose appropriate hash functions. How- ever, most of the existing hash function learning methods either treat hash function design as a classification problem or generate binary codes to satisfy pairwise supervision, and have not yet directly optimized the search accuracy. In this paper, we propose to leverage listwise supervision into a principled hash function learning framework. In particular, the ranking information is represented by a set of rank triplets that can be used to assess the quality of ranking. Simple linear projection-based hash functions are solved efficiently through maximizing the ranking quality over the training data. We carry out experiments on large image datasets with size up to one million and compare with the state-of-the-art hashing techniques. The extensive results corroborate that our learned hash codes via listwise supervision can provide superior search accuracy without incurring heavy computational overhead.</p><p>5 0.77692127 <a title="368-lda-5" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<p>Author: Shi Qiu, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: This paper proposes modeling the complex web image collections with an automatically generated graph structure called visual semantic complex network (VSCN). The nodes on this complex network are clusters of images with both visual and semantic consistency, called semantic concepts. These nodes are connected based on the visual and semantic correlations. Our VSCN with 33, 240 concepts is generated from a collection of 10 million web images. 1 A great deal of valuable information on the structures of the web image collections can be revealed by exploring the VSCN, such as the small-world behavior, concept community, indegree distribution, hubs, and isolated concepts. It not only helps us better understand the web image collections at a macroscopic level, but also has many important practical applications. This paper presents two application examples: content-based image retrieval and image browsing. Experimental results show that the VSCN leads to significant improvement on both the precision of image retrieval (over 200%) and user experience for image browsing.</p><p>6 0.77416348 <a title="368-lda-6" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>7 0.77309668 <a title="368-lda-7" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>8 0.77138448 <a title="368-lda-8" href="./iccv-2013-Improving_Graph_Matching_via_Density_Maximization.html">214 iccv-2013-Improving Graph Matching via Density Maximization</a></p>
<p>9 0.77034509 <a title="368-lda-9" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>10 0.76873654 <a title="368-lda-10" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>11 0.76847684 <a title="368-lda-11" href="./iccv-2013-Learning_to_Rank_Using_Privileged_Information.html">248 iccv-2013-Learning to Rank Using Privileged Information</a></p>
<p>12 0.76832104 <a title="368-lda-12" href="./iccv-2013-Revisiting_Example_Dependent_Cost-Sensitive_Learning_with_Decision_Trees.html">352 iccv-2013-Revisiting Example Dependent Cost-Sensitive Learning with Decision Trees</a></p>
<p>13 0.76759851 <a title="368-lda-13" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>14 0.76516557 <a title="368-lda-14" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>15 0.76247227 <a title="368-lda-15" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>16 0.76093763 <a title="368-lda-16" href="./iccv-2013-Person_Re-identification_by_Salience_Matching.html">313 iccv-2013-Person Re-identification by Salience Matching</a></p>
<p>17 0.76056147 <a title="368-lda-17" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>18 0.75951588 <a title="368-lda-18" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>19 0.75941312 <a title="368-lda-19" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>20 0.75886846 <a title="368-lda-20" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
