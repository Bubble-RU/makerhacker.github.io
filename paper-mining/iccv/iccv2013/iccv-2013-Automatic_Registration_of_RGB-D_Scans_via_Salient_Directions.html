<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-56" href="#">iccv2013-56</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</h1>
<br/><p>Source: <a title="iccv-2013-56-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Zeisl_Automatic_Registration_of_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Bernhard Zeisl, Kevin Köser, Marc Pollefeys</p><p>Abstract: We address the problem of wide-baseline registration of RGB-D data, such as photo-textured laser scans without any artificial targets or prediction on the relative motion. Our approach allows to fully automatically register scans taken in GPS-denied environments such as urban canyon, industrial facilities or even indoors. We build upon image features which are plenty, localized well and much more discriminative than geometry features; however, they suffer from viewpoint distortions and request for normalization. We utilize the principle of salient directions present in the geometry and propose to extract (several) directions from the distribution of surface normals or other cues such as observable symmetries. Compared to previous work we pose no requirements on the scanned scene (like containing large textured planes) and can handle arbitrary surface shapes. Rendering the whole scene from these repeatable directions using an orthographic camera generates textures which are identical up to 2D similarity transformations. This ambiguity is naturally handled by 2D features and allows to find stable correspondences among scans. For geometric pose estimation from tentative matches we propose a fast and robust 2 point sample consensus scheme integrating an early rejection phase. We evaluate our approach on different challenging real world scenes.</p><p>Reference: <a title="iccv-2013-56-reference" href="../iccv2013_reference/iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ch  Abstract We address the problem of wide-baseline registration of RGB-D data, such as photo-textured laser scans without any artificial targets or prediction on the relative motion. [sent-6, score-0.904]
</p><p>2 Our approach allows to fully automatically register scans taken in GPS-denied environments such as urban canyon, industrial facilities or even indoors. [sent-7, score-0.552]
</p><p>3 We utilize the principle of salient directions present in the geometry and propose to extract (several) directions from the distribution of surface normals or other cues such as observable symmetries. [sent-9, score-0.926]
</p><p>4 Compared to previous work we pose no requirements on the scanned scene (like containing large textured planes) and can handle arbitrary surface shapes. [sent-10, score-0.22]
</p><p>5 Rendering the whole scene from these repeatable directions using an orthographic camera generates textures which are identical up to 2D similarity transformations. [sent-11, score-0.388]
</p><p>6 For geometric pose estimation from tentative matches we propose a  fast and robust 2 point sample consensus scheme integrating an early rejection phase. [sent-13, score-0.316]
</p><p>7 Introduction When surveying construction sites, historical buildings or industrial facilities laser scanning is the state-of-the-art technique to obtain accurate three-dimensional models. [sent-16, score-0.359]
</p><p>8 Usually a scanner is positioned at different places, in- or outdoors, in order to minimize scan shadows and to obtain a model as complete as possible. [sent-21, score-0.344]
</p><p>9 Since scanning is a time-consuming and therefore expensive task the number of scans is usually kept as small as possible, leading to a wide baseline setting between ∗This work was done while K. [sent-22, score-0.482]
</p><p>10 was  employed  at  the Institute for  gorithm from 5 individual scans (CHURCH dataset). [sent-24, score-0.392]
</p><p>11 We achieve entirely automatic registration of arbitrary geometry from largely different viewpoints by exploiting depth and image data jointly. [sent-25, score-0.393]
</p><p>12 Not only scanning, but also the registration ofindividual scans takes a lot oftime - either afterwards by manually aligning models, or on site by carefully positioning targets (artificial markers) in the scene, which are spotted and automatically detected from several scan positions. [sent-27, score-0.87]
</p><p>13 If one desires to rescan the facility at another point in time and align current data with an older model, exploiting artificial markers for registration is impossible. [sent-28, score-0.309]
</p><p>14 As a result there is a quest for automatic registration methods which do  not rely on any artificial landmarks, but can generate accurate registration results by exploiting the scan data itself. [sent-29, score-0.712]
</p><p>15 GPS and magnetic compass can simplify the registration problem, but they fail under bridges, inside buildings, urban canyon or close to metallic or electric installations, respectively. [sent-31, score-0.371]
</p><p>16 In this work we propose to become independent of the original sensor viewpoint by exploiting characteristic salient directions of the scene, which are repeatable among different scans. [sent-35, score-0.616]
</p><p>17 Examples include peaks in the distribution of the surface normals, vanishing points, symmetry, gravity or other directions that can be reliably obtained from the sensor or the scene. [sent-36, score-0.466]
</p><p>18 Each salient direction is then exploited to render an orthographic view, and by this way remov-  ing the perspective effects that had been introduced by the particular scanner position. [sent-37, score-0.738]
</p><p>19 Importantly, for corresponding salient directions between scans generated images are identical (for jointly seen Lambertian scene parts) up to a 2D similarity transformation! [sent-38, score-0.883]
</p><p>20 Compared to earlier approaches proposed for consumer depth cameras [25] or stereo systems [22, 4] our approach does not pose any requirements on the presence of particular geometric shapes. [sent-40, score-0.231]
</p><p>21 This is an important aspect if the visible overlap between scans is small. [sent-42, score-0.392]
</p><p>22 Finally, we propose a novel 2-point solution for the restricted 4 DoF registration problem, allowing for a greedy rejection of outliercontaminated hypotheses in a sample consensus framework. [sent-47, score-0.334]
</p><p>23 The remainder of the paper is structured as follows: After a discussion of existing registration techniques in the next section, we show how to obtain viewpoint invariance from salient directions in Sec. [sent-48, score-0.819]
</p><p>24 5 cover details of our approach for salient direction detection  and pose estimation. [sent-52, score-0.436]
</p><p>25 This registration can be performed by targets or calibration patterns [21] or by maximizing mutual information between reflectance and color [17]. [sent-57, score-0.3]
</p><p>26 However, this repeatability is likely to decrease with increasing surface complexity because of self-occlusions. [sent-76, score-0.251]
</p><p>27 For planar scenes like facades with clearly visible straight lines, vanishing points can be used, even if no depth information is available [18, 3, 1]. [sent-82, score-0.247]
</p><p>28 For more general scenes, it was shown that the sole usage of affine features can be improved, if they are normalized with respect to the local surface normal rather than to the affine shape [11]. [sent-83, score-0.295]
</p><p>29 Recently, this local approach has been generalized from planes to parametric developable surfaces, allowing also to use cylinders and cones [25]. [sent-86, score-0.239]
</p><p>30 Feature matching and thus registration from these images fails in most cases. [sent-90, score-0.269]
</p><p>31 (middle, right:) Generated salient direction rectified (SDR) renderings along corresponding salient directions. [sent-91, score-0.733]
</p><p>32 Viewpoint Invariance via Salient Directions Our novel approach to register widely separated scans builds upon image features rather than 3D geometry features, because image features are plenty, well localized and discriminative. [sent-96, score-0.47]
</p><p>33 We eliminate effects of viewpoint to allow for wide baseline registration of scans without a prediction on relative pose. [sent-97, score-0.794]
</p><p>34 Instead we exploit the entire scene information by the concept of salient directions. [sent-101, score-0.321]
</p><p>35 Let us now define what we mean by a salient direction. [sent-102, score-0.284]
</p><p>36 The pose of a laser scanner in the world coordinate system is specified by the mapping of a point X from world to scanner coordinates via Xi = siRiX+ti = siRiX −siRiCi . [sent-103, score-0.645]
</p><p>37 Here, Ci represents the origin of the scanner in Xwo−rlsd coordinates, while Ri represents its orientation and si is the scaling. [sent-104, score-0.246]
</p><p>38 A salient direction is a real-world direction in global coordinates dsal that can be observed locally as disal, djsal in independent scans iand j: dsal = RTidisal = RTjdjsal. [sent-107, score-1.182]
</p><p>39 (1)  Intuitively, imagine dsal is the north direction, that is repre-  Figure 3: Orthographic renderings along a salient direction. [sent-108, score-0.437]
</p><p>40 The scene overlap of planar (red) and free form (blue) surface will be rendered identically along dsal for each scanner. [sent-109, score-0.388]
</p><p>41 sented in scans iand j as disal and djsal respectively. [sent-110, score-0.681]
</p><p>42 5D depth and image data, either from a laser scanner or from a consumer depth device or stereo system. [sent-112, score-0.571]
</p><p>43 Then, for the depth data, local normals are estimated and we will call the set of range data, color data and normals taken from one position a scan. [sent-114, score-0.351]
</p><p>44 A salient direction rectified (SDR) image, is an image which is obtained by rendering the scene along a salient direction disal with orthographic projection matrix  Pi=? [sent-120, score-1.166]
</p><p>45 Given a salient direction d sal with corresponding local directions disal, djsal in scans iand j, then corresponding points in the two SDR-images relate to each other via a 2D similarity transformation. [sent-130, score-1.046]
</p><p>46 Without loss of generality we set the ith scanner pose [I, 0] and denote [sjRj , tj] = [sR, t] . [sent-138, score-0.266]
</p><p>47 If absolute scale is known – as for laser scans the freedom reduces to a 2D euclidean transformation. [sent-157, score-0.526]
</p><p>48 Given that a global direction g is known commonly among scans in local coordinates as gi and that ˜ ri,1  ×  is chosen as ˜ ri,1 = (gi disal)/|gi images differ only in tra×nsdl atio)n/. [sent-164, score-0.524]
</p><p>49 |  disal |, then generated  Defining ˜ ri,1 as above and setting ˜ ri,2 orthogonal to it via r˜i,2 = (disal ˜r i,1)/|disal ˜ ri,1 | ensures that g appears upright in the× SDR-images. [sent-165, score-0.276]
</p><p>50 Normalization of image data with respect to salient directions (per direction per scan) 3. [sent-176, score-0.55]
</p><p>51 Geometric verification and concurrent pose estimation (for a scan pair) 4. [sent-178, score-0.224]
</p><p>52 Salient Direction Detection and Image Normalization Given a salient world direction that can be identified in two different scans, we have shown that we can transform the image content in a way that it becomes virtually invari-  ant with respect to the unknown pose. [sent-179, score-0.38]
</p><p>53 Depending on the scene type several possibilities exist how to identify salient directions, including vanishing points [1] in modern architecture, directions of repetitions or symmetries [12] in historical buildings or north direction from the sky or the time and the sun [14] in outdoor scenes. [sent-180, score-0.723]
</p><p>54 However, in this contribution we demonstrate the idea using salient directions derived from characteristics of geometric structures, that is peaks in the distribution of surface normals (cf. [sent-181, score-0.813]
</p><p>55 successful registration only a single peak needs to be consistent, while remaining modes can be different. [sent-185, score-0.302]
</p><p>56 Dominant normal directions Potentially disjoint, locally planar surfaces give rise to dominant surface normals. [sent-186, score-0.48]
</p><p>57 The algorithm now performs gradient descent on the density estimate fˆ(nk) and sample trajectories reach stable points at peaks of the density function. [sent-193, score-0.171]
</p><p>58 As a distance measure between normals we use their orientation agreement. [sent-194, score-0.17]
</p><p>59 The sampling density of points on a surface highly depends on the distance of the surface from the scanner, as well as the slant of the surface wrt. [sent-203, score-0.449]
</p><p>60 Thus, if we used raw 3D points x (and their normals n) as generated from the scanner much higher emphasis would  be given to surfaces close to the scanner and parallel to the scanning direction. [sent-205, score-0.73]
</p><p>61 −  −  −  281 1  dered from salient directions highly supported by structures near the scanner, and repeatability of salient directions between scans would be degraded. [sent-220, score-1.424]
</p><p>62 (7)  Here a(x) denotes the surface area orthogonal to the scanning direction rx. [sent-227, score-0.313]
</p><p>63 For a depth map it is the projected pixel footprint at depth [x]z, while for a laser-scan it relates to the projected 2D scan interval (given by the angular scan resolution) at distance ? [sent-228, score-0.434]
</p><p>64 c Alosud a ar ensdu are aeb glee ntoe rdaettee arm sipnaesalient directions bias-free. [sent-235, score-0.17]
</p><p>65 Within the first category are holes which are caused by occluders in the original scanner viewpoint placed in front of the surface to render, e. [sent-243, score-0.517]
</p><p>66 Second, are holes which are caused by missing data in the scanning process (e. [sent-248, score-0.174]
</p><p>67 For the relative registration of two scans we augment each feature by its 3D position and normal in the local coordinate system and denote points as ps and pt (in the following indices s and t indicate source and target scan, respectively). [sent-266, score-0.94]
</p><p>68 For a laser scanner the gravity direction is usually known (assumed to be aligned with the z-axis in the following), so we need to estimate only 4 parameters; however, for a hand-held RGB-D sensor 6 DoF need to be estimated. [sent-268, score-0.504]
</p><p>69 For upright features the latter is fixed by the gravity direction and the local coordinate system is defined as [n, n ez , (n ez) n]). [sent-275, score-0.48]
</p><p>70 The rotation angle θ around the gravity direction ez is computed between normals ns , nt projected in the x-y plane n¯s = ns ? [sent-278, score-0.571]
</p><p>71 28 12  Algorithm  1  2-point  geometric  pose  verification  Algorithm 12-point geometric pose verification  {p(si) pt(i)  Require: set m = [m1, . [sent-294, score-0.242]
</p><p>72 K do uniformly sample 2 matches mi , mj from m vs ← vt ← if | ? [sent-300, score-0.174]
</p><p>73 The orientation of these vectors is more precisely compared to normals due to their much larger spatial extent. [sent-325, score-0.17]
</p><p>74 3D points in the source in the target scene form vectors vs and vt respectively, connecting the 2 points in the local scans. [sent-332, score-0.218]
</p><p>75 ps(i), p(sj)  and p(ti), p(tj)  Full 6 DoF transformation To estimate all 6 DoF of a 3D rigid body transformation, at minimum 3 corresponding points are required (if normals and feature orientations should be avoided). [sent-339, score-0.25]
</p><p>76 Experimental Evaluation For evaluation we recorded 3 different datasets with different scene characteristics which are typical for laser scanning scenarios. [sent-343, score-0.261]
</p><p>77 CHURCH is an indoor dataset of an old church consisting of 5 scans and exhibiting many vaults. [sent-344, score-0.511]
</p><p>78 Note that there exists a sign ambiguity for the symmetry plane normal, thus we use both possible normal directions as salient direction. [sent-346, score-0.556]
</p><p>79 For CITY we captured 3 scans in an urban area showing a high number of structured facades (e. [sent-347, score-0.446]
</p><p>80 Repeatability of Salient Directions It is essential for successful registration that we extract at least one salient direction (up to small variation) in both viewpoints. [sent-356, score-0.649]
</p><p>81 For evaluation we have taken scans with known rel-  ative pose and rendered the source scene into the viewpoint of the target scene. [sent-358, score-0.623]
</p><p>82 Thus, in these regions corresponding salient directions (defined as directions differing by 10◦ at maximum) can and should get support. [sent-363, score-0.624]
</p><p>83 We now determine repeatability scores by comparing the number of corresponding salient directions to the total number of detected salient directions. [sent-364, score-0.9]
</p><p>84 Registration performance To demonstrate the registration performance of our approach we compare it against state-of-the-art planar RGB-D rectification [22, 4]. [sent-367, score-0.399]
</p><p>85 cube face images), but registration fails in more than half of the cases. [sent-370, score-0.309]
</p><p>86 salient directions can be established from a untextured white wall, while the features for matching originate from some other textured free-form surface. [sent-388, score-0.454]
</p><p>87 In addition Fig 1and Fig 6 illustrates the global registration results for CHURCH and CITY, respectively. [sent-389, score-0.269]
</p><p>88 Previously pair-wise estimated relative poses form a graph connecting  the scans with successful registration. [sent-390, score-0.429]
</p><p>89 An initial solution for the absolute pose of each scans is obtained by construction of a minimum spanning tree (MST) in the graph and concatenating relative transformations accordingly. [sent-391, score-0.485]
</p><p>90 Conclusion In this work we have presented the novel concept of obtaining viewpoint invariance by means of an orthographic projection along detected salient directions in range data. [sent-398, score-0.703]
</p><p>91 We have proven that resulting salient direction rectified (SDR) images for corresponding salient directions in different scans are identical up to a 2D similarity transformation in the general case or even more restricted in special, but common cases. [sent-399, score-1.347]
</p><p>92 This allows to exploit texture and features not only on parametric objects like planes, cones or cylinders, but on any free-form surface in the scene. [sent-400, score-0.205]
</p><p>93 We have proposed to utilize modes in the distribution of surface normals for salient direction detection. [sent-401, score-0.674]
</p><p>94 Compared to model fitting approaches for the parametric surfaces, estimating modes via mean-shift is robust, which is reflected by the high repeatability scores we achieve. [sent-402, score-0.19]
</p><p>95 We have evaluated  the algorithm on challenging scenes with wide baseline and little overlap and demonstrated superior registration performance. [sent-403, score-0.302]
</p><p>96 Future work will explore fully automatic registration of scans taken at different points in time or in different lighting, seasons or weather conditions. [sent-404, score-0.693]
</p><p>97 Registration of multiple range scans as a location recognition problem: hypothesis generation, refinement and verification. [sent-474, score-0.392]
</p><p>98 (Lower left parts) Repeatability scores for salient directions, i. [sent-530, score-0.284]
</p><p>99 the ration of found and present salient directions in the scan overlap. [sent-532, score-0.588]
</p><p>100 Surface Signatures: An orientation independent free-form surface representation scheme for the purpose of objects registration and matching. [sent-590, score-0.432]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('scans', 0.392), ('salient', 0.284), ('registration', 0.269), ('ez', 0.226), ('disal', 0.217), ('scanner', 0.21), ('directions', 0.17), ('laser', 0.134), ('normals', 0.134), ('scan', 0.134), ('surface', 0.127), ('repeatability', 0.124), ('dsal', 0.121), ('church', 0.119), ('rz', 0.119), ('orthographic', 0.115), ('tentative', 0.107), ('direction', 0.096), ('viewpoint', 0.096), ('scanning', 0.09), ('holes', 0.084), ('transformation', 0.084), ('depth', 0.083), ('djsal', 0.072), ('sdr', 0.072), ('claim', 0.071), ('planes', 0.07), ('rectification', 0.069), ('normal', 0.068), ('peaks', 0.067), ('repeatable', 0.066), ('vt', 0.066), ('rejection', 0.065), ('oser', 0.064), ('gravity', 0.064), ('dof', 0.063), ('planar', 0.061), ('ps', 0.061), ('consumer', 0.061), ('upright', 0.059), ('matches', 0.057), ('pose', 0.056), ('surfaces', 0.054), ('urban', 0.054), ('castle', 0.053), ('cylinders', 0.051), ('vs', 0.051), ('rotation', 0.051), ('affine', 0.05), ('canyon', 0.048), ('ntni', 0.048), ('sirix', 0.048), ('zeisl', 0.048), ('correspondences', 0.046), ('pt', 0.046), ('cones', 0.045), ('ser', 0.044), ('zurich', 0.044), ('inpainting', 0.043), ('rendered', 0.042), ('geometry', 0.041), ('cube', 0.04), ('artificial', 0.04), ('developable', 0.04), ('geomar', 0.04), ('translation', 0.038), ('vanishing', 0.038), ('detected', 0.038), ('facilities', 0.037), ('spotted', 0.037), ('scene', 0.037), ('relative', 0.037), ('register', 0.037), ('rectified', 0.037), ('density', 0.036), ('orientation', 0.036), ('gi', 0.036), ('plenty', 0.036), ('statue', 0.036), ('city', 0.035), ('handled', 0.035), ('coordinate', 0.035), ('symmetry', 0.034), ('verification', 0.034), ('scenes', 0.033), ('mst', 0.033), ('historical', 0.033), ('panoramic', 0.033), ('roof', 0.033), ('parametric', 0.033), ('render', 0.033), ('buildings', 0.033), ('modes', 0.033), ('points', 0.032), ('renderings', 0.032), ('arccos', 0.032), ('industrial', 0.032), ('ch', 0.032), ('calibration', 0.031), ('geometric', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999899 <a title="56-tfidf-1" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>Author: Bernhard Zeisl, Kevin Köser, Marc Pollefeys</p><p>Abstract: We address the problem of wide-baseline registration of RGB-D data, such as photo-textured laser scans without any artificial targets or prediction on the relative motion. Our approach allows to fully automatically register scans taken in GPS-denied environments such as urban canyon, industrial facilities or even indoors. We build upon image features which are plenty, localized well and much more discriminative than geometry features; however, they suffer from viewpoint distortions and request for normalization. We utilize the principle of salient directions present in the geometry and propose to extract (several) directions from the distribution of surface normals or other cues such as observable symmetries. Compared to previous work we pose no requirements on the scanned scene (like containing large textured planes) and can handle arbitrary surface shapes. Rendering the whole scene from these repeatable directions using an orthographic camera generates textures which are identical up to 2D similarity transformations. This ambiguity is naturally handled by 2D features and allows to find stable correspondences among scans. For geometric pose estimation from tentative matches we propose a fast and robust 2 point sample consensus scheme integrating an early rejection phase. We evaluate our approach on different challenging real world scenes.</p><p>2 0.16796446 <a title="56-tfidf-2" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>Author: Bastien Jacquet, Christian Häne, Kevin Köser, Marc Pollefeys</p><p>Abstract: Although specular objects have gained interest in recent years, virtually no approaches exist for markerless reconstruction of reflective scenes in the wild. In this work, we present a practical approach to capturing normal maps in real-world scenes using video only. We focus on nearly planar surfaces such as windows, facades from glass or metal, or frames, screens and other indoor objects and show how normal maps of these can be obtained without the use of an artificial calibration object. Rather, we track the reflections of real-world straight lines, while moving with a hand-held or vehicle-mounted camera in front of the object. In contrast to error-prone local edge tracking, we obtain the reflections by a robust, global segmentation technique of an ortho-rectified 3D video cube that also naturally allows efficient user interaction. Then, at each point of the reflective surface, the resulting 2D-curve to 3D-line correspondence provides a novel quadratic constraint on the local surface normal. This allows to globally solve for the shape by integrability and smoothness constraints and easily supports the usage of multiple lines. We demonstrate the technique on several objects and facades.</p><p>3 0.15679692 <a title="56-tfidf-3" href="./iccv-2013-Uncertainty-Driven_Efficiently-Sampled_Sparse_Graphical_Models_for_Concurrent_Tumor_Segmentation_and_Atlas_Registration.html">432 iccv-2013-Uncertainty-Driven Efficiently-Sampled Sparse Graphical Models for Concurrent Tumor Segmentation and Atlas Registration</a></p>
<p>Author: Sarah Parisot, William Wells_III, Stéphane Chemouny, Hugues Duffau, Nikos Paragios</p><p>Abstract: Graph-based methods have become popular in recent years and have successfully addressed tasks like segmentation and deformable registration. Their main strength is optimality of the obtained solution while their main limitation is the lack of precision due to the grid-like representations and the discrete nature of the quantized search space. In this paper we introduce a novel approach for combined segmentation/registration of brain tumors that adapts graph and sampling resolution according to the image content. To this end we estimate the segmentation and registration marginals towards adaptive graph resolution and intelligent definition of the search space. This information is considered in a hierarchical framework where uncertainties are propagated in a natural manner. State of the art results in the joint segmentation/registration of brain images with low-grade gliomas demonstrate the potential of our approach.</p><p>4 0.14835124 <a title="56-tfidf-4" href="./iccv-2013-Saliency_Detection_in_Large_Point_Sets.html">370 iccv-2013-Saliency Detection in Large Point Sets</a></p>
<p>Author: Elizabeth Shtrom, George Leifman, Ayellet Tal</p><p>Abstract: While saliency in images has been extensively studied in recent years, there is very little work on saliency of point sets. This is despite the fact that point sets and range data are becoming ever more widespread and have myriad applications. In this paper we present an algorithm for detecting the salient points in unorganized 3D point sets. Our algorithm is designed to cope with extremely large sets, which may contain tens of millions of points. Such data is typical of urban scenes, which have recently become commonly available on the web. No previous work has handled such data. For general data sets, we show that our results are competitive with those of saliency detection of surfaces, although we do not have any connectivity information. We demonstrate the utility of our algorithm in two applications: producing a set of the most informative viewpoints and suggesting an informative city tour given a city scan.</p><p>5 0.14384393 <a title="56-tfidf-5" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>Author: Scott Satkin, Martial Hebert</p><p>Abstract: We present a new algorithm 3DNN (3D NearestNeighbor), which is capable of matching an image with 3D data, independently of the viewpoint from which the image was captured. By leveraging rich annotations associated with each image, our algorithm can automatically produce precise and detailed 3D models of a scene from a single image. Moreover, we can transfer information across images to accurately label and segment objects in a scene. The true benefit of 3DNN compared to a traditional 2D nearest-neighbor approach is that by generalizing across viewpoints, we free ourselves from the need to have training examples captured from all possible viewpoints. Thus, we are able to achieve comparable results using orders of magnitude less data, and recognize objects from never-beforeseen viewpoints. In this work, we describe the 3DNN algorithm and rigorously evaluate its performance for the tasks of geometry estimation and object detection/segmentation. By decoupling the viewpoint and the geometry of an image, we develop a scene matching approach which is truly 100% viewpoint invariant, yielding state-of-the-art performance on challenging data.</p><p>6 0.14257294 <a title="56-tfidf-6" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>7 0.14094186 <a title="56-tfidf-7" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>8 0.13623431 <a title="56-tfidf-8" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>9 0.13548332 <a title="56-tfidf-9" href="./iccv-2013-Multiple_Non-rigid_Surface_Detection_and_Registration.html">283 iccv-2013-Multiple Non-rigid Surface Detection and Registration</a></p>
<p>10 0.12426072 <a title="56-tfidf-10" href="./iccv-2013-Accurate_and_Robust_3D_Facial_Capture_Using_a_Single_RGBD_Camera.html">36 iccv-2013-Accurate and Robust 3D Facial Capture Using a Single RGBD Camera</a></p>
<p>11 0.12385441 <a title="56-tfidf-11" href="./iccv-2013-Go-ICP%3A_Solving_3D_Registration_Efficiently_and_Globally_Optimally.html">185 iccv-2013-Go-ICP: Solving 3D Registration Efficiently and Globally Optimally</a></p>
<p>12 0.12044571 <a title="56-tfidf-12" href="./iccv-2013-Geometric_Registration_Based_on_Distortion_Estimation.html">183 iccv-2013-Geometric Registration Based on Distortion Estimation</a></p>
<p>13 0.11966816 <a title="56-tfidf-13" href="./iccv-2013-High_Quality_Shape_from_a_Single_RGB-D_Image_under_Uncalibrated_Natural_Illumination.html">199 iccv-2013-High Quality Shape from a Single RGB-D Image under Uncalibrated Natural Illumination</a></p>
<p>14 0.1162549 <a title="56-tfidf-14" href="./iccv-2013-A_Generic_Deformation_Model_for_Dense_Non-rigid_Surface_Registration%3A_A_Higher-Order_MRF-Based_Approach.html">16 iccv-2013-A Generic Deformation Model for Dense Non-rigid Surface Registration: A Higher-Order MRF-Based Approach</a></p>
<p>15 0.11498395 <a title="56-tfidf-15" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>16 0.10830726 <a title="56-tfidf-16" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<p>17 0.10758572 <a title="56-tfidf-17" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>18 0.10680857 <a title="56-tfidf-18" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>19 0.10521436 <a title="56-tfidf-19" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>20 0.10286738 <a title="56-tfidf-20" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.226), (1, -0.196), (2, 0.004), (3, -0.033), (4, -0.008), (5, -0.01), (6, 0.062), (7, -0.149), (8, -0.009), (9, -0.026), (10, 0.002), (11, 0.035), (12, -0.07), (13, 0.038), (14, 0.066), (15, -0.033), (16, 0.074), (17, 0.049), (18, -0.006), (19, -0.034), (20, 0.011), (21, 0.035), (22, 0.076), (23, 0.025), (24, -0.002), (25, -0.081), (26, -0.027), (27, 0.091), (28, -0.004), (29, -0.037), (30, 0.058), (31, -0.045), (32, 0.056), (33, -0.029), (34, 0.011), (35, -0.049), (36, 0.031), (37, 0.064), (38, 0.031), (39, -0.0), (40, 0.07), (41, 0.096), (42, -0.051), (43, -0.045), (44, 0.006), (45, -0.027), (46, -0.007), (47, 0.038), (48, -0.048), (49, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94438475 <a title="56-lsi-1" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>Author: Bernhard Zeisl, Kevin Köser, Marc Pollefeys</p><p>Abstract: We address the problem of wide-baseline registration of RGB-D data, such as photo-textured laser scans without any artificial targets or prediction on the relative motion. Our approach allows to fully automatically register scans taken in GPS-denied environments such as urban canyon, industrial facilities or even indoors. We build upon image features which are plenty, localized well and much more discriminative than geometry features; however, they suffer from viewpoint distortions and request for normalization. We utilize the principle of salient directions present in the geometry and propose to extract (several) directions from the distribution of surface normals or other cues such as observable symmetries. Compared to previous work we pose no requirements on the scanned scene (like containing large textured planes) and can handle arbitrary surface shapes. Rendering the whole scene from these repeatable directions using an orthographic camera generates textures which are identical up to 2D similarity transformations. This ambiguity is naturally handled by 2D features and allows to find stable correspondences among scans. For geometric pose estimation from tentative matches we propose a fast and robust 2 point sample consensus scheme integrating an early rejection phase. We evaluate our approach on different challenging real world scenes.</p><p>2 0.82175392 <a title="56-lsi-2" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>Author: Qian-Yi Zhou, Stephen Miller, Vladlen Koltun</p><p>Abstract: We present an approach to reconstruction of detailed scene geometry from range video. Range data produced by commodity handheld cameras suffers from high-frequency errors and low-frequency distortion. Our approach deals with both sources of error by reconstructing locally smooth scene fragments and letting these fragments deform in order to align to each other. We develop a volumetric registration formulation that leverages the smoothness of the deformation to make optimization practical for large scenes. Experimental results demonstrate that our approach substantially increases the fidelity of complex scene geometry reconstructed with commodity handheld cameras.</p><p>3 0.76902092 <a title="56-lsi-3" href="./iccv-2013-Geometric_Registration_Based_on_Distortion_Estimation.html">183 iccv-2013-Geometric Registration Based on Distortion Estimation</a></p>
<p>Author: Wei Zeng, Mayank Goswami, Feng Luo, Xianfeng Gu</p><p>Abstract: Surface registration plays a fundamental role in many applications in computer vision and aims at finding a oneto-one correspondence between surfaces. Conformal mapping based surface registration methods conformally map 2D/3D surfaces onto 2D canonical domains and perform the matching on the 2D plane. This registration framework reduces dimensionality, and the result is intrinsic to Riemannian metric and invariant under isometric deformation. However, conformal mapping will be affected by inconsistent boundaries and non-isometric deformations of surfaces. In this work, we quantify the effects of boundary variation and non-isometric deformation to conformal mappings, and give the theoretical upper bounds for the distortions of conformal mappings under these two factors. Besides giving the thorough theoretical proofs of the theorems, we verified them by concrete experiments using 3D human facial scans with dynamic expressions and varying boundaries. Furthermore, we used the distortion estimates for reducing search range in feature matching of surface registration applications. The experimental results are consistent with the theoreticalpredictions and also demonstrate the performance improvements in feature tracking.</p><p>4 0.76891398 <a title="56-lsi-4" href="./iccv-2013-A_Generic_Deformation_Model_for_Dense_Non-rigid_Surface_Registration%3A_A_Higher-Order_MRF-Based_Approach.html">16 iccv-2013-A Generic Deformation Model for Dense Non-rigid Surface Registration: A Higher-Order MRF-Based Approach</a></p>
<p>Author: Yun Zeng, Chaohui Wang, Xianfeng Gu, Dimitris Samaras, Nikos Paragios</p><p>Abstract: We propose a novel approach for dense non-rigid 3D surface registration, which brings together Riemannian geometry and graphical models. To this end, we first introduce a generic deformation model, called Canonical Distortion Coefficients (CDCs), by characterizing the deformation of every point on a surface using the distortions along its two principle directions. This model subsumes the deformation groups commonly used in surface registration such as isometry and conformality, and is able to handle more complex deformations. We also derive its discrete counterpart which can be computed very efficiently in a closed form. Based on these, we introduce a higher-order Markov Random Field (MRF) model which seamlessly integrates our deformation model and a geometry/texture similarity metric. Then we jointly establish the optimal correspondences for all the points via maximum a posteriori (MAP) inference. Moreover, we develop a parallel optimization algorithm to efficiently perform the inference for the proposed higher-order MRF model. The resulting registration algorithm outperforms state-of-the-art methods in both dense non-rigid 3D surface registration and tracking.</p><p>5 0.73570162 <a title="56-lsi-5" href="./iccv-2013-Go-ICP%3A_Solving_3D_Registration_Efficiently_and_Globally_Optimally.html">185 iccv-2013-Go-ICP: Solving 3D Registration Efficiently and Globally Optimally</a></p>
<p>Author: Jiaolong Yang, Hongdong Li, Yunde Jia</p><p>Abstract: Registration is a fundamental task in computer vision. The Iterative Closest Point (ICP) algorithm is one of the widely-used methods for solving the registration problem. Based on local iteration, ICP is however well-known to suffer from local minima. Its performance critically relies on the quality of initialization, and only local optimality is guaranteed. This paper provides the very first globally optimal solution to Euclidean registration of two 3D pointsets or two 3D surfaces under the L2 error. Our method is built upon ICP, but combines it with a branch-and-bound (BnB) scheme which searches the 3D motion space SE(3) efficiently. By exploiting the special structure of the underlying geometry, we derive novel upper and lower bounds for the ICP error function. The integration of local ICP and global BnB enables the new method to run efficiently in practice, and its optimality is exactly guaranteed. We also discuss extensions, addressing the issue of outlier robustness.</p><p>6 0.73160803 <a title="56-lsi-6" href="./iccv-2013-Multiple_Non-rigid_Surface_Detection_and_Registration.html">283 iccv-2013-Multiple Non-rigid Surface Detection and Registration</a></p>
<p>7 0.73044956 <a title="56-lsi-7" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>8 0.72651136 <a title="56-lsi-8" href="./iccv-2013-Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization.html">284 iccv-2013-Multiview Photometric Stereo Using Planar Mesh Parameterization</a></p>
<p>9 0.7068423 <a title="56-lsi-9" href="./iccv-2013-Large-Scale_Multi-resolution_Surface_Reconstruction_from_RGB-D_Sequences.html">228 iccv-2013-Large-Scale Multi-resolution Surface Reconstruction from RGB-D Sequences</a></p>
<p>10 0.67969739 <a title="56-lsi-10" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>11 0.66390711 <a title="56-lsi-11" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>12 0.6423825 <a title="56-lsi-12" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>13 0.63438386 <a title="56-lsi-13" href="./iccv-2013-Data-Driven_3D_Primitives_for_Single_Image_Understanding.html">102 iccv-2013-Data-Driven 3D Primitives for Single Image Understanding</a></p>
<p>14 0.59970862 <a title="56-lsi-14" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>15 0.59967864 <a title="56-lsi-15" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<p>16 0.59045815 <a title="56-lsi-16" href="./iccv-2013-Content-Aware_Rotation.html">90 iccv-2013-Content-Aware Rotation</a></p>
<p>17 0.58477432 <a title="56-lsi-17" href="./iccv-2013-Saliency_Detection_in_Large_Point_Sets.html">370 iccv-2013-Saliency Detection in Large Point Sets</a></p>
<p>18 0.57909405 <a title="56-lsi-18" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>19 0.56541342 <a title="56-lsi-19" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>20 0.55698788 <a title="56-lsi-20" href="./iccv-2013-Live_Metric_3D_Reconstruction_on_Mobile_Phones.html">254 iccv-2013-Live Metric 3D Reconstruction on Mobile Phones</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.035), (26, 0.048), (31, 0.03), (35, 0.012), (40, 0.012), (42, 0.088), (64, 0.031), (73, 0.028), (89, 0.631)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99954683 <a title="56-lda-1" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>Author: Qian-Yi Zhou, Stephen Miller, Vladlen Koltun</p><p>Abstract: We present an approach to reconstruction of detailed scene geometry from range video. Range data produced by commodity handheld cameras suffers from high-frequency errors and low-frequency distortion. Our approach deals with both sources of error by reconstructing locally smooth scene fragments and letting these fragments deform in order to align to each other. We develop a volumetric registration formulation that leverages the smoothness of the deformation to make optimization practical for large scenes. Experimental results demonstrate that our approach substantially increases the fidelity of complex scene geometry reconstructed with commodity handheld cameras.</p><p>2 0.99817562 <a title="56-lda-2" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>Author: Kevin Tang, Bangpeng Yao, Li Fei-Fei, Daphne Koller</p><p>Abstract: In this paper, we tackle the problem of combining features extracted from video for complex event recognition. Feature combination is an especially relevant task in video data, as there are many features we can extract, ranging from image features computed from individual frames to video features that take temporal information into account. To combine features effectively, we propose a method that is able to be selective of different subsets of features, as some features or feature combinations may be uninformative for certain classes. We introduce a hierarchical method for combining features based on the AND/OR graph structure, where nodes in the graph represent combinations of different sets of features. Our method automatically learns the structure of the AND/OR graph using score-based structure learning, and we introduce an inference procedure that is able to efficiently compute structure scores. We present promising results and analysis on the difficult and large-scale 2011 TRECVID Multimedia Event Detection dataset [17].</p><p>3 0.99783742 <a title="56-lda-3" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>Author: Dan Xie, Sinisa Todorovic, Song-Chun Zhu</p><p>Abstract: This paper presents an approach to localizing functional objects in surveillance videos without domain knowledge about semantic object classes that may appear in the scene. Functional objects do not have discriminative appearance and shape, but they affect behavior of people in the scene. For example, they “attract” people to approach them for satisfying certain needs (e.g., vending machines could quench thirst), or “repel” people to avoid them (e.g., grass lawns). Therefore, functional objects can be viewed as “dark matter”, emanating “dark energy ” that affects people ’s trajectories in the video. To detect “dark matter” and infer their “dark energy ” field, we extend the Lagrangian mechanics. People are treated as particle-agents with latent intents to approach “dark matter” and thus satisfy their needs, where their motions are subject to a composite “dark energy ” field of all functional objects in the scene. We make the assumption that people take globally optimal paths toward the intended “dark matter” while avoiding latent obstacles. A Bayesian framework is used to probabilistically model: people ’s trajectories and intents, constraint map of the scene, and locations of functional objects. A data-driven Markov Chain Monte Carlo (MCMC) process is used for inference. Our evaluation on videos of public squares and courtyards demonstrates our effectiveness in localizing functional objects and predicting people ’s trajectories in unobserved parts of the video footage.</p><p>4 0.99612582 <a title="56-lda-4" href="./iccv-2013-Deblurring_by_Example_Using_Dense_Correspondence.html">103 iccv-2013-Deblurring by Example Using Dense Correspondence</a></p>
<p>Author: Yoav Hacohen, Eli Shechtman, Dani Lischinski</p><p>Abstract: This paper presents a new method for deblurring photos using a sharp reference example that contains some shared content with the blurry photo. Most previous deblurring methods that exploit information from other photos require an accurately registered photo of the same static scene. In contrast, our method aims to exploit reference images where the shared content may have undergone substantial photometric and non-rigid geometric transformations, as these are the kind of reference images most likely to be found in personal photo albums. Our approach builds upon a recent method for examplebased deblurring using non-rigid dense correspondence (NRDC) [11] and extends it in two ways. First, we suggest exploiting information from the reference image not only for blur kernel estimation, but also as a powerful local prior for the non-blind deconvolution step. Second, we introduce a simple yet robust technique for spatially varying blur estimation, rather than assuming spatially uniform blur. Unlike the aboveprevious method, which hasproven successful only with simple deblurring scenarios, we demonstrate that our method succeeds on a variety of real-world examples. We provide quantitative and qualitative evaluation of our method and show that it outperforms the state-of-the-art.</p><p>5 0.99542743 <a title="56-lda-5" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>Author: Heng Wang, Cordelia Schmid</p><p>Abstract: Recently dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets. This paper improves their performance by taking into account camera motion to correct them. To estimate camera motion, we match feature points between frames using SURF descriptors and dense optical flow, which are shown to be complementary. These matches are, then, used to robustly estimate a homography with RANSAC. Human motion is in general different from camera motion and generates inconsistent matches. To improve the estimation, a human detector is employed to remove these matches. Given the estimated camera motion, we remove trajectories consistent with it. We also use this estimation to cancel out camera motion from the optical flow. This significantly improves motion-based descriptors, such as HOF and MBH. Experimental results onfour challenging action datasets (i.e., Hollywood2, HMDB51, Olympic Sports and UCF50) significantly outperform the current state of the art.</p><p>6 0.9947443 <a title="56-lda-6" href="./iccv-2013-Optimization_Problems_for_Fast_AAM_Fitting_in-the-Wild.html">302 iccv-2013-Optimization Problems for Fast AAM Fitting in-the-Wild</a></p>
<p>same-paper 7 0.99426681 <a title="56-lda-7" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>8 0.99260086 <a title="56-lda-8" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>9 0.98677588 <a title="56-lda-9" href="./iccv-2013-3D_Scene_Understanding_by_Voxel-CRF.html">2 iccv-2013-3D Scene Understanding by Voxel-CRF</a></p>
<p>10 0.98320645 <a title="56-lda-10" href="./iccv-2013-Shufflets%3A_Shared_Mid-level_Parts_for_Fast_Object_Detection.html">390 iccv-2013-Shufflets: Shared Mid-level Parts for Fast Object Detection</a></p>
<p>11 0.97769624 <a title="56-lda-11" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>12 0.96589702 <a title="56-lda-12" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>13 0.96496314 <a title="56-lda-13" href="./iccv-2013-Large-Scale_Multi-resolution_Surface_Reconstruction_from_RGB-D_Sequences.html">228 iccv-2013-Large-Scale Multi-resolution Surface Reconstruction from RGB-D Sequences</a></p>
<p>14 0.96197075 <a title="56-lda-14" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>15 0.96063471 <a title="56-lda-15" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>16 0.95918745 <a title="56-lda-16" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>17 0.95789695 <a title="56-lda-17" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>18 0.95605528 <a title="56-lda-18" href="./iccv-2013-Active_MAP_Inference_in_CRFs_for_Efficient_Semantic_Segmentation.html">42 iccv-2013-Active MAP Inference in CRFs for Efficient Semantic Segmentation</a></p>
<p>19 0.95574826 <a title="56-lda-19" href="./iccv-2013-Saliency_Detection_in_Large_Point_Sets.html">370 iccv-2013-Saliency Detection in Large Point Sets</a></p>
<p>20 0.95553541 <a title="56-lda-20" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
