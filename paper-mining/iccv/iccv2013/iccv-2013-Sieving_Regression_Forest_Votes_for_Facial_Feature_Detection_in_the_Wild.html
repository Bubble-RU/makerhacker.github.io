<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-391" href="#">iccv2013-391</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</h1>
<br/><p>Source: <a title="iccv-2013-391-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Yang_Sieving_Regression_Forest_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Heng Yang, Ioannis Patras</p><p>Abstract: In this paper we propose a method for the localization of multiple facial features on challenging face images. In the regression forests (RF) framework, observations (patches) that are extracted at several image locations cast votes for the localization of several facial features. In order to filter out votes that are not relevant, we pass them through two types of sieves, that are organised in a cascade, and which enforce geometric constraints. The first sieve filters out votes that are not consistent with a hypothesis for the location of the face center. Several sieves of the second type, one associated with each individual facial point, filter out distant votes. We propose a method that adjusts onthe-fly the proximity threshold of each second type sieve by applying a classifier which, based on middle-level features extracted from voting maps for the facial feature in question, makes a sequence of decisions on whether the threshold should be reduced or not. We validate our proposed method on two challenging datasets with images collected from the Internet in which we obtain state of the art results without resorting to explicit facial shape models. We also show the benefits of our method for proximity threshold adjustment especially on ’difficult’ face images.</p><p>Reference: <a title="iccv-2013-391-reference" href="../iccv2013_reference/iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract In this paper we propose a method for the localization of multiple facial features on challenging face images. [sent-6, score-0.596]
</p><p>2 In the regression forests (RF) framework, observations (patches) that are extracted at several image locations cast votes for the localization of several facial features. [sent-7, score-1.13]
</p><p>3 In order to filter out votes that are not relevant, we pass them through two types of sieves, that are organised in a cascade, and which enforce geometric constraints. [sent-8, score-0.45]
</p><p>4 The first sieve filters out votes that are not consistent with a hypothesis for the location of the face center. [sent-9, score-1.12]
</p><p>5 Several sieves of the second type, one associated with each individual facial point, filter out distant votes. [sent-10, score-0.516]
</p><p>6 We also  show the benefits of our method for proximity threshold adjustment especially on ’difficult’ face images. [sent-13, score-0.501]
</p><p>7 Introduction Detecting semantic facial features on face images is often the first step for many tasks in Facial Analysis including face and facial expression recognition [16, 21]. [sent-15, score-1.053]
</p><p>8 Recent works attempt to make a transition from facial images recorded in laboratory settings or in controlled conditions to face images “in the wild” [4, 10, 12, 30, 6]. [sent-16, score-0.516]
</p><p>9 In this paper, we address the problem using random forests, given their good performance in various challenging computer vision tasks like video synopsis[29], action recognition [13], human pose estimation [23], object recognition [26] and facial feature detection [10]. [sent-18, score-0.392]
</p><p>10 er0  Figure 1: Our approach estimates the facial feature points in 2D images using votes from random regression forest. [sent-20, score-0.878]
</p><p>11 Before accumulating the votes to a Hough map, our method refines them by a cascade of sieves. [sent-21, score-0.512]
</p><p>12 either a facial feature, or the center of the face). [sent-25, score-0.424]
</p><p>13 Essentially, each sieve operates as a filter that rejects votes that are not relevant/useful for the estimation of the variable associated with the sieve in question. [sent-26, score-1.393]
</p><p>14 The sieve that is associated with the hidden variable (i. [sent-27, score-0.469]
</p><p>15 the face center) rejects votes that are not consistent with hypotheses that are generated by a search for local maxima in the voting space for the variable in question. [sent-29, score-0.879]
</p><p>16 This introduces global consistency, imposes geometric constraints in the form of implicit facial shape models, and deals with irrelevant votes due to, for example, occlusions. [sent-30, score-0.759]
</p><p>17 This differentiates our method from classic regression forests that treat votes in a completely independent way, i. [sent-31, score-0.659]
</p><p>18 The sieves associated with the individual facial features are responsible of rejecting votes with low accuracy. [sent-34, score-0.944]
</p><p>19 Each sieve adjusts a threshold that controls the minimum allowed proximity (equivalently the maximum allowed offset) between  the facial feature in question and the location at which the observations are extracted. [sent-35, score-1.093]
</p><p>20 With a large threshold, that is at high proximity, we select votes with small offsets to the facial feature in question. [sent-36, score-0.816]
</p><p>21 With a small threshold, we select votes with large offsets and in this way introduce facial shape constraints and robustness to occlusions. [sent-38, score-0.828]
</p><p>22 In contrast to using a fixed threshold that is learned during training, in our work we learn a classifier who controls a procedure in which the proximity threshold is gradually reduced. [sent-40, score-0.377]
</p><p>23 In this procedure, the decision on whether the threshold should be decreased or not is taken by a classifier that is built on middle-level features that are extracted from the current voting map for the location of the feature in question. [sent-41, score-0.412]
</p><p>24 Finally, the detection is carried out on Hough voting maps formed by the cast votes after they are filtered by the cascade of sieves. [sent-42, score-0.806]
</p><p>25 Related work Two different sources of information are usually exploited for facial feature detection: face appearance and spatial shape. [sent-47, score-0.538]
</p><p>26 In particular, [3] modelled the joint distribution over all the votes and the hypotheses in a probabilistic way, rather than simply accumulating the votes. [sent-55, score-0.47]
</p><p>27 [17] studied the geometric compatibilities of the votes in a pairwise fashion within a game-theoretic setting. [sent-56, score-0.43]
</p><p>28 These methods were developed for person/object detection and focused on intra-class geometrical agreement, while the consistency in our problem is inter-class, since we consider the localization of all facial points. [sent-58, score-0.431]
</p><p>29 Sieving Random Forest votes Our method can be built on top of any part-based voting approaches. [sent-60, score-0.633]
</p><p>30 In this section, we first describe two baseline regression forests and then present our proposal to use a cascade of sieves to refine votes. [sent-61, score-0.445]
</p><p>31 Once a regression forest (or Hough forest) is trained, at testing stage, observations arrive at tree leaves and cast votes for the hidden or target variables [9]. [sent-65, score-0.724]
</p><p>32 1  CRF-D [10]  CRF-D introduces conditional regression forests that model the appearance and location of facial feature points conditioned on the head pose yaw angle. [sent-69, score-0.722]
</p><p>33 At leaf nodes, the relative offsets to each facial feature point are summarized by a weighted offset vector. [sent-97, score-0.531]
</p><p>34 This vector is used to cast votes for observations that arrive at the leaf in question during testing. [sent-98, score-0.588]
</p><p>35 Formally, at a leaf note, the vote associated with the ith facial feature point is given by:  vi  = (Δi,  ωi,  Δi0, ωi0),  (2)  11993377  where Δi = d¯i, ωi = trace1(Σi) with d¯i and Σi are, respectively, the mean and the covariance matrix of the offsets of the ith facial feature point. [sent-100, score-0.933]
</p><p>36 Δi0 is the mean value calculated on the offsets of the face center and ωi0 is the corresponding weight. [sent-102, score-0.441]
</p><p>37 Figure 1 shows some voting elements, each with a red arrow to the face center and a green arrow to one point target. [sent-103, score-0.66]
</p><p>38 Then, a relative vote associated with ith facial feature point in state k is given as: vik = (Δik, ωik, Δi0k, ωi0k) (3) where Δik is the mean-shift mode of the largest cluster and ωik is the relative size of the cluster. [sent-110, score-0.498]
</p><p>39 Δi0k and ωi0k that are related to the offsets to the center of the face are learned in the same way. [sent-111, score-0.419]
</p><p>40 3  Inference from votes  During testing, the patches extracted from a test image are fed to the forest and when they arrive at a leaf note, they cast votes for the localization of the facial features. [sent-121, score-1.458]
</p><p>41 Let us denote by Vi = {vi} the set of votes for the facial point i. [sent-122, score-0.754]
</p><p>42 , [14, 6, 10], we construct a Hough map for each facial feature point by accumulating the votes. [sent-128, score-0.407]
</p><p>43 δ(·) is the Dirac delta function that only allows votes for wδh(·ic)h i tsh teh proximity test, using the proximity threshold λi, is satisfied. [sent-140, score-0.867]
</p><p>44 This vote selection process is regarded as a sieve, which in this case is associated with an individual facial point. [sent-141, score-0.405]
</p><p>45 In this work, instead of explicitly learning shape models to constrain the local detection, as in [7, 27], we propose to use a cascade of sieves that impose geometric constraints and filter the votes in order to remove false hypotheses. [sent-145, score-0.68]
</p><p>46 (a) A vote consists of two offset vectors, one to the target point (green arrow) and the other to face center (red arrow). [sent-161, score-0.502]
</p><p>47 (b) Original set of votes for the left brow center. [sent-162, score-0.517]
</p><p>48 (c) The absolute face center votes, those in green are regarded as consistent to the face center. [sent-163, score-0.571]
</p><p>49 (d) The remaining voting elements filtered by the face center sieve. [sent-164, score-0.613]
</p><p>50 (e) All voting elements are used to localize the face center (red dot). [sent-165, score-0.591]
</p><p>51 (f) and (h) are the Hough maps generated from votes of (b) and (d) respectively. [sent-166, score-0.43]
</p><p>52 A face center sieve is used to reject votes that are not consistent with the localization of the facial center. [sent-168, score-1.624]
</p><p>53 Recall that each voting element also contains a weighted relative vector to the face center, i. [sent-169, score-0.446]
</p><p>54 Then, the absolute vote to the face center is yi0 = zi+Δi0. [sent-172, score-0.437]
</p><p>55 As shown in Figure 2e when calculating the face center, voting elements from all the facial points are accumulated into V0 = {(Δi0, ωi0) }iN=1 . [sent-173, score-0.784]
</p><p>56 A mean-shift algorithm is employed to find the mode in the voting map that is the estimate of the face center ˆy 0. [sent-186, score-0.595]
</p><p>57 Since the face center is calculated using votes from the whole image, its localization is very robust to partial occlusions even if the center itself is heavily occluded. [sent-187, score-1.047]
</p><p>58 Consequently, we choose to use it as a stable point that is used by our sieve to reject voting elements that cast votes far from it. [sent-188, score-1.221]
</p><p>59 From left to right, the first row shows the original face image, all votes for the point (λ = 0. [sent-194, score-0.705]
</p><p>60 35), votes passed center sieve and the votes with updated threshold (λ = 0. [sent-195, score-1.553]
</p><p>61 the estimated center yˆ0 are not consistent with the global hypothesis and therefore unlikely to contribute to the correct localization of facial features. [sent-199, score-0.504]
</p><p>62 Formally, the sieve re-weighs a voting element vi, according to the proximity between the location of the vote for the face center (i. [sent-201, score-1.27]
</p><p>63 yi0) and the estimated face center ˆy 0, that is: ωi  :=  ωi  · δ(f(|yi0 −ˆ y 0|) > λ0)  (6)  where f(·) is the proximity function and δ(·) is the delta  fwuhnecrtieo fn. [sent-203, score-0.531]
</p><p>64 ( ·A)s i ssh tohwen p ironx Figure f2udn, catiftoenr filtering by th thee s ideevleta, voting elements that violate the face center consistency and vote for other face center hypotheses, are assigned a zero weight, i. [sent-204, score-1.069]
</p><p>65 The ones that do not violate the face center consistency are kept. [sent-207, score-0.391]
</p><p>66 In what follows we will denote set of the remaining votes by VF. [sent-208, score-0.43]
</p><p>67 Individual sieve threshold updating Recall that when collecting the votes for an individual feature point, a threshold is applied. [sent-212, score-1.16]
</p><p>68 This works as a sieve that prohibits long distant voting. [sent-213, score-0.493]
</p><p>69 As shown in Figure 3, in that case, only few valid voting elements remain after the sieve of the face center is applied and those voting elements may contain wrong information. [sent-216, score-1.301]
</p><p>70 In order to determine an image-dependent proximity threshold λi for the sieve i, we propose a iterative scheme  Figure4:Comparisonofvoteorientaionhistogramsho. [sent-219, score-0.725]
</p><p>71 decrease) the threshold is taken by a classifier who is applied on features that are extracted from the voting map calculated using the current threshold value. [sent-225, score-0.482]
</p><p>72 If a decision to adjust the threshold is taken, then a new voting map is calculated considering all the voting elements that pass through the sieve, and the classifier is again applied on features that are extracted from the new voting map. [sent-226, score-0.876]
</p><p>73 For each facial point we collect a set of training instances (xi , τ) consisting of a feature vector xi that is extracted from the voting map, as explained below, and an adjustment operation label τ ∈ {1, 0}. [sent-228, score-0.613]
</p><p>74 In contrast to other methods that explicitly try to detect occlusions using image features, we propose to use middle-level features that are extracted directly from the voting maps that are formed using the current threshold before and after the face center sieve is applied, i. [sent-237, score-1.195]
</p><p>75 vi∈ViF|VifF(||yi− ¯yi|), the concentration of the votes  xi2 = after face center sieve. [sent-245, score-0.78]
</p><p>76 xi3 = ho, the orientation histogram of the votes after filtering. [sent-249, score-0.43]
</p><p>77 Then, for each facial feature, we decrease the value of proximity threshold λi (i. [sent-255, score-0.578]
</p><p>78 For each facial feature point, we find the optimal value of the threshold ˆλi, i. [sent-258, score-0.412]
</p><p>79 Algorithm summary The voting map of a feature point is re-estimated if the threshold of its sieve has been updated (Figure 1). [sent-268, score-0.839]
</p><p>80 Then, the new votes are filtered by the face center sieve. [sent-269, score-0.802]
</p><p>81 =l λ ∅i a∈n dΛ M Mdoa 4: collect∈ voting elements Vi based on λi 5: apply face center sieve and obtain ViF 6: calculate the middle level feature xi 7: τ ← svmi (xi) ? [sent-276, score-1.102]
</p><p>82 The first, contains the 13,233 images of the Labelled Face in the Wild (LFW) dataset [15] that are annotated [10] with the location of 10 facial points and the face bounding boxes. [sent-280, score-0.571]
</p><p>83 At each leaf node, the trained model provides offset vectors to 10 facial points and it also provides a mean patch offset vector to the center of the bounding box, i. [sent-296, score-0.581]
</p><p>84 Sieve parameters The parameters that are related to the face center sieve are the bandwidth for the face center h0 in Eq. [sent-311, score-1.169]
</p><p>85 A key parameter is the face center proximity threshold λ0. [sent-315, score-0.606]
</p><p>86 Large values of λ0 impose larger consistency in the votes for the face center, while smaller values relax the constraint and allow more votes to pass through the sieve. [sent-316, score-1.122]
</p><p>87 We first evaluate the contribution of our face center sieve by comparing with CRF-D [10]. [sent-329, score-0.819]
</p><p>88 1 (663 face images) and into the AFLW TestII the rest (337 face images). [sent-352, score-0.442]
</p><p>89 Since our proposed scheme is based on the face center detection, we evaluate the stability by measuring the nose tip localization error. [sent-355, score-0.465]
</p><p>90 The procedure of updating individual sieve thresholds does not have much effect. [sent-390, score-0.563]
</p><p>91 The average relative improvement in mean error by using the face center sieve is 37% (0. [sent-392, score-0.888]
</p><p>92 (a) Distribution of face  center  localization  error. [sent-405, score-0.43]
</p><p>93 Tree) [30], (2) the structured-output regression forests (SO-RF) in [27], (3) the regression forests based CLM (RF-CLM) [7] and (4) betaface. [sent-426, score-0.458]
</p><p>94 Tree models combine face detection and landmarks detection, for fair comparison we build our algorithm on top of a Viola-Jones face detector from the Matlab computer vision toolbox. [sent-430, score-0.522]
</p><p>95 In Figure 8c we compare the average localization error of all the 18 internal points on a face (the chin center is excluded) of our method with SO-RF and RF-CLM. [sent-449, score-0.505]
</p><p>96 Conclusion and Future Work In this paper we have proposed a cascade of sieves that are able to refine votes from regression forests for facial feature detection. [sent-464, score-1.192]
</p><p>97 Other than the face center consistency, can we develop more efficient ways to measure the quality of the votes before accumulating them into the Hough map? [sent-468, score-0.82]
</p><p>98 Can we extract more useful middle-level features from the votes for high-level vision tasks such as to measure the face similarity and to recognize the facial expression? [sent-469, score-0.946]
</p><p>99 Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization. [sent-592, score-0.671]
</p><p>100 Privileged information-based conditional regression forests for facial feature detection. [sent-645, score-0.573]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sieve', 0.469), ('votes', 0.43), ('facial', 0.295), ('aflw', 0.278), ('face', 0.221), ('voting', 0.203), ('sieves', 0.174), ('proximity', 0.161), ('testi', 0.139), ('testii', 0.139), ('hough', 0.136), ('center', 0.129), ('forests', 0.125), ('lfw', 0.108), ('regression', 0.104), ('threshold', 0.095), ('vote', 0.087), ('localization', 0.08), ('offsets', 0.069), ('forest', 0.069), ('brow', 0.062), ('head', 0.06), ('wild', 0.059), ('leaf', 0.058), ('vif', 0.057), ('vi', 0.056), ('cast', 0.052), ('eye', 0.048), ('sieving', 0.046), ('landmarks', 0.045), ('thresholds', 0.045), ('dji', 0.043), ('cascade', 0.042), ('accumulating', 0.04), ('pose', 0.04), ('rf', 0.039), ('arrow', 0.039), ('pj', 0.038), ('elements', 0.038), ('heng', 0.037), ('landmark', 0.036), ('offset', 0.036), ('occlusions', 0.036), ('detection', 0.035), ('nose', 0.035), ('maxiter', 0.035), ('sorf', 0.035), ('shape', 0.034), ('validation', 0.03), ('faces', 0.03), ('point', 0.029), ('yi', 0.028), ('annotated', 0.028), ('points', 0.027), ('conditional', 0.027), ('decrease', 0.027), ('adjusts', 0.027), ('patras', 0.027), ('error', 0.027), ('classifier', 0.026), ('ik', 0.026), ('pantic', 0.026), ('synopsis', 0.026), ('updating', 0.026), ('left', 0.025), ('hair', 0.025), ('decision', 0.025), ('rejects', 0.025), ('observations', 0.024), ('distant', 0.024), ('arrive', 0.024), ('adjustment', 0.024), ('gall', 0.024), ('resorting', 0.023), ('individual', 0.023), ('calculated', 0.022), ('razavi', 0.022), ('formed', 0.022), ('feature', 0.022), ('tpami', 0.022), ('filtered', 0.022), ('relative', 0.022), ('yaw', 0.022), ('rejecting', 0.022), ('mode', 0.021), ('consistency', 0.021), ('ho', 0.021), ('expression', 0.021), ('chin', 0.021), ('tree', 0.021), ('map', 0.021), ('improvement', 0.02), ('jp', 0.02), ('kohli', 0.02), ('extracted', 0.02), ('xi', 0.02), ('delta', 0.02), ('violate', 0.02), ('namely', 0.02), ('pass', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="391-tfidf-1" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>Author: Heng Yang, Ioannis Patras</p><p>Abstract: In this paper we propose a method for the localization of multiple facial features on challenging face images. In the regression forests (RF) framework, observations (patches) that are extracted at several image locations cast votes for the localization of several facial features. In order to filter out votes that are not relevant, we pass them through two types of sieves, that are organised in a cascade, and which enforce geometric constraints. The first sieve filters out votes that are not consistent with a hypothesis for the location of the face center. Several sieves of the second type, one associated with each individual facial point, filter out distant votes. We propose a method that adjusts onthe-fly the proximity threshold of each second type sieve by applying a classifier which, based on middle-level features extracted from voting maps for the facial feature in question, makes a sequence of decisions on whether the threshold should be reduced or not. We validate our proposed method on two challenging datasets with images collected from the Internet in which we obtain state of the art results without resorting to explicit facial shape models. We also show the benefits of our method for proximity threshold adjustment especially on ’difficult’ face images.</p><p>2 0.2573303 <a title="391-tfidf-2" href="./iccv-2013-Cascaded_Shape_Space_Pruning_for_Robust_Facial_Landmark_Detection.html">70 iccv-2013-Cascaded Shape Space Pruning for Robust Facial Landmark Detection</a></p>
<p>Author: Xiaowei Zhao, Shiguang Shan, Xiujuan Chai, Xilin Chen</p><p>Abstract: In this paper, we propose a novel cascaded face shape space pruning algorithm for robust facial landmark detection. Through progressively excluding the incorrect candidate shapes, our algorithm can accurately and efficiently achieve the globally optimal shape configuration. Specifically, individual landmark detectors are firstly applied to eliminate wrong candidates for each landmark. Then, the candidate shape space is further pruned by jointly removing incorrect shape configurations. To achieve this purpose, a discriminative structure classifier is designed to assess the candidate shape configurations. Based on the learned discriminative structure classifier, an efficient shape space pruning strategy is proposed to quickly reject most incorrect candidate shapes while preserve the true shape. The proposed algorithm is carefully evaluated on a large set of real world face images. In addition, comparison results on the publicly available BioID and LFW face databases demonstrate that our algorithm outperforms some state-of-the-art algorithms.</p><p>3 0.24146575 <a title="391-tfidf-3" href="./iccv-2013-Accurate_and_Robust_3D_Facial_Capture_Using_a_Single_RGBD_Camera.html">36 iccv-2013-Accurate and Robust 3D Facial Capture Using a Single RGBD Camera</a></p>
<p>Author: Yen-Lin Chen, Hsiang-Tao Wu, Fuhao Shi, Xin Tong, Jinxiang Chai</p><p>Abstract: This paper presents an automatic and robust approach that accurately captures high-quality 3D facial performances using a single RGBD camera. The key of our approach is to combine the power of automatic facial feature detection and image-based 3D nonrigid registration techniques for 3D facial reconstruction. In particular, we develop a robust and accurate image-based nonrigid registration algorithm that incrementally deforms a 3D template mesh model to best match observed depth image data and important facial features detected from single RGBD images. The whole process is fully automatic and robust because it is based on single frame facial registration framework. The system is flexible because it does not require any strong 3D facial priors such as blendshape models. We demonstrate the power of our approach by capturing a wide range of 3D facial expressions using a single RGBD camera and achieve state-of-the-art accuracy by comparing against alternative methods.</p><p>4 0.19555597 <a title="391-tfidf-4" href="./iccv-2013-Pose-Free_Facial_Landmark_Fitting_via_Optimized_Part_Mixtures_and_Cascaded_Deformable_Shape_Model.html">321 iccv-2013-Pose-Free Facial Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Shape Model</a></p>
<p>Author: Xiang Yu, Junzhou Huang, Shaoting Zhang, Wang Yan, Dimitris N. Metaxas</p><p>Abstract: This paper addresses the problem of facial landmark localization and tracking from a single camera. We present a two-stage cascaded deformable shape model to effectively and efficiently localize facial landmarks with large head pose variations. For face detection, we propose a group sparse learning method to automatically select the most salient facial landmarks. By introducing 3D face shape model, we use procrustes analysis to achieve pose-free facial landmark initialization. For deformation, the first step uses mean-shift local search with constrained local model to rapidly approach the global optimum. The second step uses component-wise active contours to discriminatively refine the subtle shape variation. Our framework can simultaneously handle face detection, pose-free landmark localization and tracking in real time. Extensive experiments are conducted on both laboratory environmental face databases and face-in-the-wild databases. All results demonstrate that our approach has certain advantages over state-of-theart methods in handling pose variations1.</p><p>5 0.19369748 <a title="391-tfidf-5" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>Author: Yizhe Zhang, Ming Shao, Edward K. Wong, Yun Fu</p><p>Abstract: One of the most challenging task in face recognition is to identify people with varied poses. Namely, the test faces have significantly different poses compared with the registered faces. In this paper, we propose a high-level feature learning scheme to extract pose-invariant identity feature for face recognition. First, we build a single-hiddenlayer neural network with sparse constraint, to extractposeinvariant feature in a supervised fashion. Second, we further enhance the discriminative capability of the proposed feature by using multiple random faces as the target values for multiple encoders. By enforcing the target values to be uniquefor inputfaces over differentposes, the learned highlevel feature that is represented by the neurons in the hidden layer is pose free and only relevant to the identity information. Finally, we conduct face identification on CMU MultiPIE, and verification on Labeled Faces in the Wild (LFW) databases, where identification rank-1 accuracy and face verification accuracy with ROC curve are reported. These experiments demonstrate that our model is superior to oth- er state-of-the-art approaches on handling pose variations.</p><p>6 0.18588018 <a title="391-tfidf-6" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>7 0.16349936 <a title="391-tfidf-7" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>8 0.13073733 <a title="391-tfidf-8" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>9 0.1267741 <a title="391-tfidf-9" href="./iccv-2013-Alternating_Regression_Forests_for_Object_Detection_and_Pose_Estimation.html">47 iccv-2013-Alternating Regression Forests for Object Detection and Pose Estimation</a></p>
<p>10 0.12558894 <a title="391-tfidf-10" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>11 0.12540177 <a title="391-tfidf-11" href="./iccv-2013-Exemplar-Based_Graph_Matching_for_Robust_Facial_Landmark_Localization.html">149 iccv-2013-Exemplar-Based Graph Matching for Robust Facial Landmark Localization</a></p>
<p>12 0.12506135 <a title="391-tfidf-12" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>13 0.11828941 <a title="391-tfidf-13" href="./iccv-2013-No_Matter_Where_You_Are%3A_Flexible_Graph-Guided_Multi-task_Learning_for_Multi-view_Head_Pose_Classification_under_Target_Motion.html">291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</a></p>
<p>14 0.11746189 <a title="391-tfidf-14" href="./iccv-2013-Capturing_Global_Semantic_Relationships_for_Facial_Action_Unit_Recognition.html">69 iccv-2013-Capturing Global Semantic Relationships for Facial Action Unit Recognition</a></p>
<p>15 0.11379202 <a title="391-tfidf-15" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>16 0.10903456 <a title="391-tfidf-16" href="./iccv-2013-Real-Time_Articulated_Hand_Pose_Estimation_Using_Semi-supervised_Transductive_Regression_Forests.html">340 iccv-2013-Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests</a></p>
<p>17 0.10651843 <a title="391-tfidf-17" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>18 0.10549697 <a title="391-tfidf-18" href="./iccv-2013-Like_Father%2C_Like_Son%3A_Facial_Expression_Dynamics_for_Kinship_Verification.html">251 iccv-2013-Like Father, Like Son: Facial Expression Dynamics for Kinship Verification</a></p>
<p>19 0.10449245 <a title="391-tfidf-19" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>20 0.10358167 <a title="391-tfidf-20" href="./iccv-2013-Facial_Action_Unit_Event_Detection_by_Cascade_of_Tasks.html">155 iccv-2013-Facial Action Unit Event Detection by Cascade of Tasks</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.187), (1, 0.013), (2, -0.064), (3, -0.087), (4, 0.02), (5, -0.166), (6, 0.272), (7, 0.114), (8, -0.034), (9, 0.009), (10, -0.046), (11, 0.089), (12, 0.045), (13, 0.03), (14, 0.01), (15, 0.035), (16, -0.0), (17, -0.052), (18, -0.098), (19, -0.029), (20, -0.039), (21, 0.045), (22, -0.03), (23, 0.097), (24, -0.037), (25, -0.048), (26, 0.009), (27, -0.058), (28, 0.031), (29, -0.042), (30, -0.024), (31, 0.078), (32, -0.087), (33, 0.023), (34, -0.097), (35, -0.002), (36, -0.012), (37, -0.031), (38, -0.025), (39, 0.013), (40, 0.01), (41, 0.023), (42, -0.022), (43, -0.048), (44, -0.007), (45, -0.041), (46, -0.057), (47, -0.086), (48, 0.098), (49, -0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94854248 <a title="391-lsi-1" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>Author: Heng Yang, Ioannis Patras</p><p>Abstract: In this paper we propose a method for the localization of multiple facial features on challenging face images. In the regression forests (RF) framework, observations (patches) that are extracted at several image locations cast votes for the localization of several facial features. In order to filter out votes that are not relevant, we pass them through two types of sieves, that are organised in a cascade, and which enforce geometric constraints. The first sieve filters out votes that are not consistent with a hypothesis for the location of the face center. Several sieves of the second type, one associated with each individual facial point, filter out distant votes. We propose a method that adjusts onthe-fly the proximity threshold of each second type sieve by applying a classifier which, based on middle-level features extracted from voting maps for the facial feature in question, makes a sequence of decisions on whether the threshold should be reduced or not. We validate our proposed method on two challenging datasets with images collected from the Internet in which we obtain state of the art results without resorting to explicit facial shape models. We also show the benefits of our method for proximity threshold adjustment especially on ’difficult’ face images.</p><p>2 0.86215985 <a title="391-lsi-2" href="./iccv-2013-Like_Father%2C_Like_Son%3A_Facial_Expression_Dynamics_for_Kinship_Verification.html">251 iccv-2013-Like Father, Like Son: Facial Expression Dynamics for Kinship Verification</a></p>
<p>Author: Hamdi Dibeklioglu, Albert Ali Salah, Theo Gevers</p><p>Abstract: Kinship verification from facial appearance is a difficult problem. This paper explores the possibility of employing facial expression dynamics in this problem. By using features that describe facial dynamics and spatio-temporal appearance over smile expressions, we show that it is possible to improve the state ofthe art in thisproblem, and verify that it is indeed possible to recognize kinship by resemblance of facial expressions. The proposed method is tested on different kin relationships. On the average, 72.89% verification accuracy is achieved on spontaneous smiles.</p><p>3 0.82038939 <a title="391-lsi-3" href="./iccv-2013-Cascaded_Shape_Space_Pruning_for_Robust_Facial_Landmark_Detection.html">70 iccv-2013-Cascaded Shape Space Pruning for Robust Facial Landmark Detection</a></p>
<p>Author: Xiaowei Zhao, Shiguang Shan, Xiujuan Chai, Xilin Chen</p><p>Abstract: In this paper, we propose a novel cascaded face shape space pruning algorithm for robust facial landmark detection. Through progressively excluding the incorrect candidate shapes, our algorithm can accurately and efficiently achieve the globally optimal shape configuration. Specifically, individual landmark detectors are firstly applied to eliminate wrong candidates for each landmark. Then, the candidate shape space is further pruned by jointly removing incorrect shape configurations. To achieve this purpose, a discriminative structure classifier is designed to assess the candidate shape configurations. Based on the learned discriminative structure classifier, an efficient shape space pruning strategy is proposed to quickly reject most incorrect candidate shapes while preserve the true shape. The proposed algorithm is carefully evaluated on a large set of real world face images. In addition, comparison results on the publicly available BioID and LFW face databases demonstrate that our algorithm outperforms some state-of-the-art algorithms.</p><p>4 0.7880308 <a title="391-lsi-4" href="./iccv-2013-Pose-Free_Facial_Landmark_Fitting_via_Optimized_Part_Mixtures_and_Cascaded_Deformable_Shape_Model.html">321 iccv-2013-Pose-Free Facial Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Shape Model</a></p>
<p>Author: Xiang Yu, Junzhou Huang, Shaoting Zhang, Wang Yan, Dimitris N. Metaxas</p><p>Abstract: This paper addresses the problem of facial landmark localization and tracking from a single camera. We present a two-stage cascaded deformable shape model to effectively and efficiently localize facial landmarks with large head pose variations. For face detection, we propose a group sparse learning method to automatically select the most salient facial landmarks. By introducing 3D face shape model, we use procrustes analysis to achieve pose-free facial landmark initialization. For deformation, the first step uses mean-shift local search with constrained local model to rapidly approach the global optimum. The second step uses component-wise active contours to discriminatively refine the subtle shape variation. Our framework can simultaneously handle face detection, pose-free landmark localization and tracking in real time. Extensive experiments are conducted on both laboratory environmental face databases and face-in-the-wild databases. All results demonstrate that our approach has certain advantages over state-of-theart methods in handling pose variations1.</p><p>5 0.74002266 <a title="391-lsi-5" href="./iccv-2013-Accurate_and_Robust_3D_Facial_Capture_Using_a_Single_RGBD_Camera.html">36 iccv-2013-Accurate and Robust 3D Facial Capture Using a Single RGBD Camera</a></p>
<p>Author: Yen-Lin Chen, Hsiang-Tao Wu, Fuhao Shi, Xin Tong, Jinxiang Chai</p><p>Abstract: This paper presents an automatic and robust approach that accurately captures high-quality 3D facial performances using a single RGBD camera. The key of our approach is to combine the power of automatic facial feature detection and image-based 3D nonrigid registration techniques for 3D facial reconstruction. In particular, we develop a robust and accurate image-based nonrigid registration algorithm that incrementally deforms a 3D template mesh model to best match observed depth image data and important facial features detected from single RGBD images. The whole process is fully automatic and robust because it is based on single frame facial registration framework. The system is flexible because it does not require any strong 3D facial priors such as blendshape models. We demonstrate the power of our approach by capturing a wide range of 3D facial expressions using a single RGBD camera and achieve state-of-the-art accuracy by comparing against alternative methods.</p><p>6 0.72842884 <a title="391-lsi-6" href="./iccv-2013-Exemplar-Based_Graph_Matching_for_Robust_Facial_Landmark_Localization.html">149 iccv-2013-Exemplar-Based Graph Matching for Robust Facial Landmark Localization</a></p>
<p>7 0.72125763 <a title="391-lsi-7" href="./iccv-2013-Robust_Face_Landmark_Estimation_under_Occlusion.html">355 iccv-2013-Robust Face Landmark Estimation under Occlusion</a></p>
<p>8 0.71782708 <a title="391-lsi-8" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>9 0.67527813 <a title="391-lsi-9" href="./iccv-2013-Modifying_the_Memorability_of_Face_Photographs.html">272 iccv-2013-Modifying the Memorability of Face Photographs</a></p>
<p>10 0.65952468 <a title="391-lsi-10" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>11 0.65204465 <a title="391-lsi-11" href="./iccv-2013-Facial_Action_Unit_Event_Detection_by_Cascade_of_Tasks.html">155 iccv-2013-Facial Action Unit Event Detection by Cascade of Tasks</a></p>
<p>12 0.62142098 <a title="391-lsi-12" href="./iccv-2013-Capturing_Global_Semantic_Relationships_for_Facial_Action_Unit_Recognition.html">69 iccv-2013-Capturing Global Semantic Relationships for Facial Action Unit Recognition</a></p>
<p>13 0.61921185 <a title="391-lsi-13" href="./iccv-2013-Hidden_Factor_Analysis_for_Age_Invariant_Face_Recognition.html">195 iccv-2013-Hidden Factor Analysis for Age Invariant Face Recognition</a></p>
<p>14 0.61166525 <a title="391-lsi-14" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>15 0.59835678 <a title="391-lsi-15" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>16 0.53546733 <a title="391-lsi-16" href="./iccv-2013-Alternating_Regression_Forests_for_Object_Detection_and_Pose_Estimation.html">47 iccv-2013-Alternating Regression Forests for Object Detection and Pose Estimation</a></p>
<p>17 0.5128383 <a title="391-lsi-17" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>18 0.50797164 <a title="391-lsi-18" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>19 0.5070098 <a title="391-lsi-19" href="./iccv-2013-Face_Recognition_via_Archetype_Hull_Ranking.html">154 iccv-2013-Face Recognition via Archetype Hull Ranking</a></p>
<p>20 0.49944085 <a title="391-lsi-20" href="./iccv-2013-Fast_High_Dimensional_Vector_Multiplication_Face_Recognition.html">158 iccv-2013-Fast High Dimensional Vector Multiplication Face Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.056), (7, 0.022), (23, 0.154), (26, 0.086), (31, 0.024), (34, 0.012), (35, 0.016), (40, 0.091), (42, 0.17), (64, 0.055), (73, 0.034), (78, 0.014), (89, 0.16), (98, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8664903 <a title="391-lda-1" href="./iccv-2013-SGTD%3A_Structure_Gradient_and_Texture_Decorrelating_Regularization_for_Image_Decomposition.html">364 iccv-2013-SGTD: Structure Gradient and Texture Decorrelating Regularization for Image Decomposition</a></p>
<p>Author: Qiegen Liu, Jianbo Liu, Pei Dong, Dong Liang</p><p>Abstract: This paper presents a novel structure gradient and texture decorrelating regularization (SGTD) for image decomposition. The motivation of the idea is under the assumption that the structure gradient and texture components should be properly decorrelated for a successful decomposition. The proposed model consists of the data fidelity term, total variation regularization and the SGTD regularization. An augmented Lagrangian method is proposed to address this optimization issue, by first transforming the unconstrained problem to an equivalent constrained problem and then applying an alternating direction method to iteratively solve the subproblems. Experimental results demonstrate that the proposed method presents better or comparable performance as state-of-the-art methods do.</p><p>same-paper 2 0.86641222 <a title="391-lda-2" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>Author: Heng Yang, Ioannis Patras</p><p>Abstract: In this paper we propose a method for the localization of multiple facial features on challenging face images. In the regression forests (RF) framework, observations (patches) that are extracted at several image locations cast votes for the localization of several facial features. In order to filter out votes that are not relevant, we pass them through two types of sieves, that are organised in a cascade, and which enforce geometric constraints. The first sieve filters out votes that are not consistent with a hypothesis for the location of the face center. Several sieves of the second type, one associated with each individual facial point, filter out distant votes. We propose a method that adjusts onthe-fly the proximity threshold of each second type sieve by applying a classifier which, based on middle-level features extracted from voting maps for the facial feature in question, makes a sequence of decisions on whether the threshold should be reduced or not. We validate our proposed method on two challenging datasets with images collected from the Internet in which we obtain state of the art results without resorting to explicit facial shape models. We also show the benefits of our method for proximity threshold adjustment especially on ’difficult’ face images.</p><p>3 0.85692203 <a title="391-lda-3" href="./iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F.html">316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</a></p>
<p>Author: Elisabeta Marinoiu, Dragos Papava, Cristian Sminchisescu</p><p>Abstract: Human motion analysis in images and video is a central computer vision problem. Yet, there are no studies that reveal how humans perceive other people in images and how accurate they are. In this paper we aim to unveil some of the processing–as well as the levels of accuracy–involved in the 3D perception of people from images by assessing the human performance. Our contributions are: (1) the construction of an experimental apparatus that relates perception and measurement, in particular the visual and kinematic performance with respect to 3D ground truth when the human subject is presented an image of a person in a given pose; (2) the creation of a dataset containing images, articulated 2D and 3D pose ground truth, as well as synchronized eye movement recordings of human subjects, shown a variety of human body configurations, both easy and difficult, as well as their ‘re-enacted’ 3D poses; (3) quantitative analysis revealing the human performance in 3D pose reenactment tasks, the degree of stability in the visual fixation patterns of human subjects, and the way it correlates with different poses. We also discuss the implications of our find- ings for the construction of visual human sensing systems.</p><p>4 0.83178967 <a title="391-lda-4" href="./iccv-2013-Total_Variation_Regularization_for_Functions_with_Values_in_a_Manifold.html">421 iccv-2013-Total Variation Regularization for Functions with Values in a Manifold</a></p>
<p>Author: Jan Lellmann, Evgeny Strekalovskiy, Sabrina Koetter, Daniel Cremers</p><p>Abstract: While total variation is among the most popular regularizers for variational problems, its extension to functions with values in a manifold is an open problem. In this paper, we propose the first algorithm to solve such problems which applies to arbitrary Riemannian manifolds. The key idea is to reformulate the variational problem as a multilabel optimization problem with an infinite number of labels. This leads to a hard optimization problem which can be approximately solved using convex relaxation techniques. The framework can be easily adapted to different manifolds including spheres and three-dimensional rotations, and allows to obtain accurate solutions even with a relatively coarse discretization. With numerous examples we demonstrate that the proposed framework can be applied to variational models that incorporate chromaticity values, normal fields, or camera trajectories.</p><p>5 0.8253268 <a title="391-lda-5" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>Author: Stefan Duffner, Christophe Garcia</p><p>Abstract: In this paper, we present a novel algorithm for fast tracking of generic objects in videos. The algorithm uses two components: a detector that makes use of the generalised Hough transform with pixel-based descriptors, and a probabilistic segmentation method based on global models for foreground and background. These components are used for tracking in a combined way, and they adapt each other in a co-training manner. Through effective model adaptation and segmentation, the algorithm is able to track objects that undergo rigid and non-rigid deformations and considerable shape and appearance variations. The proposed tracking method has been thoroughly evaluated on challenging standard videos, and outperforms state-of-theart tracking methods designed for the same task. Finally, the proposed models allow for an extremely efficient implementation, and thus tracking is very fast.</p><p>6 0.82359034 <a title="391-lda-6" href="./iccv-2013-Network_Principles_for_SfM%3A_Disambiguating_Repeated_Structures_with_Local_Context.html">289 iccv-2013-Network Principles for SfM: Disambiguating Repeated Structures with Local Context</a></p>
<p>7 0.8168996 <a title="391-lda-7" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>8 0.81652415 <a title="391-lda-8" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>9 0.81648982 <a title="391-lda-9" href="./iccv-2013-Low-Rank_Sparse_Coding_for_Image_Classification.html">258 iccv-2013-Low-Rank Sparse Coding for Image Classification</a></p>
<p>10 0.81551617 <a title="391-lda-10" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>11 0.81550628 <a title="391-lda-11" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>12 0.81418085 <a title="391-lda-12" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>13 0.81007582 <a title="391-lda-13" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>14 0.80873007 <a title="391-lda-14" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>15 0.80811822 <a title="391-lda-15" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>16 0.80720633 <a title="391-lda-16" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>17 0.80592227 <a title="391-lda-17" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>18 0.80576634 <a title="391-lda-18" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>19 0.80546099 <a title="391-lda-19" href="./iccv-2013-Correlation_Adaptive_Subspace_Segmentation_by_Trace_Lasso.html">93 iccv-2013-Correlation Adaptive Subspace Segmentation by Trace Lasso</a></p>
<p>20 0.80519009 <a title="391-lda-20" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
