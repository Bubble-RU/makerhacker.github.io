<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>164 iccv-2013-Fibonacci Exposure Bracketing for High Dynamic Range Imaging</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-164" href="#">iccv2013-164</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>164 iccv-2013-Fibonacci Exposure Bracketing for High Dynamic Range Imaging</h1>
<br/><p>Source: <a title="iccv-2013-164-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Gupta_Fibonacci_Exposure_Bracketing_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Mohit Gupta, Daisuke Iso, Shree K. Nayar</p><p>Abstract: Exposure bracketing for high dynamic range (HDR) imaging involves capturing several images of the scene at different exposures. If either the camera or the scene moves during capture, the captured images must be registered. Large exposure differences between bracketed images lead to inaccurate registration, resulting in artifacts such as ghosting (multiple copies of scene objects) and blur. We present two techniques, one for image capture (Fibonacci exposure bracketing) and one for image registration (generalized registration), to prevent such motion-related artifacts. Fibonacci bracketing involves capturing a sequence of images such that each exposure time is the sum of the previous N(N > 1) exposures. Generalized registration involves estimating motion between sums of contiguous sets of frames, instead of between individual frames. Together, the two techniques ensure that motion is always estimated betweenframes of the same total exposure time. This results in HDR images and videos which have both a large dynamic range andminimal motion-relatedartifacts. We show, by results for several real-world indoor and outdoor scenes, that theproposed approach significantly outperforms several ex- isting bracketing schemes.</p><p>Reference: <a title="iccv-2013-164-reference" href="../iccv2013_reference/iccv-2013-Fibonacci_Exposure_Bracketing_for_High_Dynamic_Range_Imaging_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Exposure bracketing for high dynamic range (HDR) imaging involves capturing several images of the scene at different exposures. [sent-8, score-0.791]
</p><p>2 Large exposure differences between bracketed images lead to inaccurate registration, resulting in artifacts such as ghosting (multiple copies of scene objects) and blur. [sent-10, score-0.603]
</p><p>3 We present two techniques, one for image capture (Fibonacci exposure bracketing) and one for image registration (generalized registration), to prevent such motion-related artifacts. [sent-11, score-0.6]
</p><p>4 Fibonacci bracketing involves capturing a sequence of images such that each exposure time is the sum of the previous N(N > 1) exposures. [sent-12, score-1.087]
</p><p>5 Together, the two techniques ensure that motion is always estimated betweenframes of the same total exposure time. [sent-14, score-0.436]
</p><p>6 We show, by results for several real-world indoor and outdoor scenes, that theproposed approach significantly outperforms several ex-  isting bracketing schemes. [sent-16, score-0.613]
</p><p>7 Exposure bracketing [9, 2] is the most popular technique for HDR digital imaging. [sent-20, score-0.613]
</p><p>8 Because of its ease of implementation, bracketing for HDR is now available as a standard feature in most digital cameras, including cell-phones. [sent-23, score-0.613]
</p><p>9 Despite its simplicity, exposure bracketing is not used in several real-world scenarios because it is prone to errors when there is scene or camera motion. [sent-24, score-1.05]
</p><p>10 However, since the frames have different exposure times 1, they have different amounts ofmotion blur and noise. [sent-32, score-0.487]
</p><p>11 These differences cannot be removed by normalizing the image intensities by their exposure times 2. [sent-33, score-0.444]
</p><p>12 This presents a fundamental tradeoff - while a large difference in image exposures is required  to capture a wide intensity dynamic range, it also results in strong motion-related artifacts. [sent-38, score-0.385]
</p><p>13 In this paper, we present new exposure bracketing and image registration techniques for handling scene and camera motion while also capturing a wide dynamic range (DR). [sent-39, score-1.463]
</p><p>14 We propose an exposure bracketing scheme called Fibonacci bracketing where the exposure times follow the Fibonacci property, i. [sent-43, score-2.033]
</p><p>15 , each exposure time is the sum of the previous N(N > 1) exposure times. [sent-45, score-0.768]
</p><p>16 Together, Fibonacci bracketing and generalized registration ensure that 1In this paper, image exposure is changed only by varying the camera shutter-time. [sent-46, score-1.309]
</p><p>17 The exposure times in a Fibonacci sequence grow exponentially, thus capturing a large DR as well. [sent-50, score-0.482]
</p><p>18 Hardware Prototype: For generalized registration, a sensor that allows exposure bracketing with a negligible  inter-frame time-gap is required. [sent-53, score-1.117]
</p><p>19 Although most current cameras support exposure bracketing, there is often a large inter-frame gap (50 − 200ms). [sent-54, score-0.415]
</p><p>20 1ms while exposure times are changed from one frame to the next. [sent-57, score-0.415]
</p><p>21 For the same time-budget, Fibonacci bracketing and generalized registration produce images of significantly higher quality as compared to existing techniques. [sent-59, score-0.89]
</p><p>22 We also extend our techniques to capture HDR video at up to 15 fps while adapting the bracketing sequence to scene brightness and motion. [sent-60, score-0.751]
</p><p>23 Related Work Exposure Bracketing: One of the most widely used bracketing schemes for HDR imaging is the exponential scheme [9, 2], where a sequence of images with exponentially increasing shutter times are used. [sent-69, score-0.806]
</p><p>24 [6] proposed a bracketing sequence of alternating short and long exposures. [sent-71, score-0.709]
</p><p>25 [21] proposed capturing and registering a sequence of several very short and same exposure images. [sent-74, score-0.499]
</p><p>26 We propose using exposure times that have the Fibonacci property, i. [sent-75, score-0.4]
</p><p>27 , each exposure is the sum of previous N(N > 1) exposures. [sent-77, score-0.392]
</p><p>28 Recently, there has been a lot of work in devising sceneadaptive exposure bracketing techniques [5]. [sent-78, score-1.017]
</p><p>29 These techniques attempt to maximize the signal-to-noise-ratio of the final HDR image by adapting the bracketing sequence to the scene’s brightness distribution. [sent-79, score-0.714]
</p><p>30 For most consumer cameras, especially the point-and-shoot and cell-phone ones, exposure bracketing remains the cheapest and the most viable HDR imaging option. [sent-92, score-1.037]
</p><p>31 Bracketing  Given a time-budget T for acquiring a single HDR image, an exposure bracketing sequence is defined as a set of frame exposures E = {e1, e2 , . [sent-95, score-1.311]
</p><p>32 Given a time-budget, there are infinite possible bracketing sequences. [sent-108, score-0.613]
</p><p>33 Which bracketing sequence achieves the highest quality HDR image? [sent-109, score-0.666]
</p><p>34 The dynamic range achieved by a bracketing scheme is given as [11]: DR  =  logIImmaixneemmaixn,  (2)  where emax and emin are the maximum and the minimum exposures in the bracketing scheme, respectively. [sent-110, score-1.698]
</p><p>35 2, it is clear that in order to maximize the dynamic range, a bracketing scheme should have a large range of exposures so that 11447744  eemax stacmkin captured using  the ratio is maximized. [sent-113, score-1.042]
</p><p>36 The LDR frames in the exposure such bracketing sequences will have large differences in exposures. [sent-114, score-1.101]
</p><p>37 How-  ×  ever, because of different exposure times, images have different amount of motion blur and noise, and hence, image features are not preserved. [sent-118, score-0.432]
</p><p>38 Although normalizing the images with their exposure times maintains brightness constancy between them, it does not remove the differences in motion blur. [sent-119, score-0.477]
</p><p>39 For each patch, two images with different exposure times are generated, using an affine image noise model. [sent-123, score-0.4]
</p><p>40 As shown, images with the same exposures have the minimum registration errors. [sent-126, score-0.474]
</p><p>41 Qualitative comparison of existing bracketing schemes: The exponential scheme [2] achieves good DR as the exposures grow exponentially. [sent-127, score-0.942]
</p><p>42 The alternating scheme [6] uses long and short exposures (ratio between the exposures is 16). [sent-128, score-0.582]
</p><p>43 Thus, in the context of exposure bracketing for HDR imaging, there is a fundamental tradeoff between the dynamic range and registration accuracy. [sent-133, score-1.299]
</p><p>44 However, large differences in exposures of images can result in strong registration artifacts. [sent-135, score-0.484]
</p><p>45 How can we create a bracketing scheme that achieves high dynamic range while minimizing the likelihood of registration artifacts? [sent-136, score-0.954]
</p><p>46 Generalized Registration for Bracketed Image Sequences  Exposure  Consider a sequence of exposure bracketed images that are to be registered using optical flow. [sent-138, score-0.537]
</p><p>47 We call the flow between sums of frames as generalized flow and the process of estimating generalized flow as generalized registration. [sent-140, score-0.494]
</p><p>48 In the next section, we will show that with the correct choice of exposure sequence, generalized registration allows computing flow between sums of frames having the same total exposure, while also achieving a high dynamic range. [sent-143, score-0.873]
</p><p>49 For each patch, two images with different exposure times were generated, normalized, and optical flow was computed between them. [sent-173, score-0.498]
</p><p>50 Images with the same exposures have the minimum registration errors. [sent-178, score-0.474]
</p><p>51 fi are the bracketed LDR frames, ei are the exposure times and o are the flows between frames. [sent-416, score-0.6]
</p><p>52 Differences in exposures of the frames result in registration artifacts. [sent-419, score-0.522]
</p><p>53 By choosing the exposure times appropriately, generalized registration ensures that the flow is always computed between sums of frames with the same total exposure times. [sent-421, score-1.221]
</p><p>54 Fibonacci Exposure Bracketing In this section, we propose an exposure bracketing scheme that exploits generalized registration to ensure that optical flow is always computed between frames of the same exposure time. [sent-425, score-1.834]
</p><p>55 To formalize this, we define the isoexposure property for an exposure sequence:  Definition 1 An exposure sequence {e1, e2 , . [sent-426, score-0.864]
</p><p>56 i+ 1  If an exposure sequence has order (ns , nt) iso-exposure property, it is possible to make generalized frames Fis and Fit (using Eqs. [sent-439, score-0.564]
</p><p>57 Order (2, 1) iso-exposure property is achieved if every exposure ei is equal to the sum of two previous exposures, i. [sent-451, score-0.45]
</p><p>58 We call the bracketing scheme with exposures forming a Fibonacci sequence as Fibonacci exposure bracketing. [sent-459, score-1.327]
</p><p>59 While it may appear that the DR of Fibonacci bracketing (DRfib) is small due to a relatively small growth factor, it turns out that DRfib is always within a small additive constant of the maximum achievable dynamic range. [sent-465, score-0.741]
</p><p>60 1+2√5,  Lemma 1 For any given time-budget T, the dynamic range achieved by Fibonacci bracketing DRfib is within 1. [sent-466, score-0.718]
</p><p>61 Proof 1 Consider an exposure s√equence where the ratio of successive exposures is φ = This sequence follows the Fibonacci property. [sent-468, score-0.716]
</p><p>62 (12)  Thus, Fibonacci bracketing achieves both - a high dynamic range (close to maximum achievable) as well as robustness to registration errors. [sent-489, score-0.923]
</p><p>63 For example, a Fibonacci sequence constructed with a total time budget of 33ms and a minimum exposure of 0. [sent-490, score-0.444]
</p><p>64 The blue-colored bars in Figure 4 (b) represent the intensity difference Dgen (R) for Fibonacci bracketing and generalized registration. [sent-502, score-0.715]
</p><p>65 A sequence of exposures has the (N, 1) order isoexposure property if the exposure times are from an order-  N Fibonacci sequence (or N-bonacci sequence). [sent-509, score-0.819]
</p><p>66 Robustness of Fibonacci bracketing to non-linear camera response functions. [sent-535, score-0.694]
</p><p>67 Differences for Fibonacci bracketing based generalized registration is always less than that of conventional registration. [sent-538, score-0.93]
</p><p>68 In Section 6, we show results of tribonacci exposure bracketing. [sent-550, score-0.438]
</p><p>69 Hardware Prototype and Results Several consumer cameras support exposure bracketing. [sent-552, score-0.415]
</p><p>70 It is possible to capture a sequence of images while varying the exposure time. [sent-553, score-0.448]
</p><p>71 On the other hand, while there is negligible time gap between successive images of a video stream captured by a video camera, it is not possible to change exposure time during capture. [sent-555, score-0.467]
</p><p>72 For generalized registration, ideally, a sensor that allows varying exposures with a negligible inter-frame gap is required. [sent-556, score-0.404]
</p><p>73 1ms) while varying exposure from one frame to the next. [sent-560, score-0.391]
</p><p>74 Results: Figure 6 shows the result of Fibonacci bracketing and generalized registration for an outdoor night scene. [sent-562, score-0.89]
</p><p>75 The Fibonacci exposure sequence obeying these constraints  is {0. [sent-566, score-0.429]
</p><p>76 Each exposure is the sum of the previous two; the ratio between successive exposures is 1. [sent-578, score-0.679]
</p><p>77 The images for exponential bracketing were normalized by their exposure times before registration. [sent-592, score-1.057]
</p><p>78 The HDR image computed using exponential bracketing and conventional registration has artifacts due to large differences in the exposures. [sent-598, score-0.983]
</p><p>79 Comparisons with existing bracketing schemes: Figure 7 shows comparisons of Fibonacci bracketing with the burst (of short exposures) scheme [21] and the alternating (long and short exposure) scheme [6]. [sent-600, score-1.389]
</p><p>80 For the burst scheme, 36 frames were captured, each with an exposure of 0. [sent-603, score-0.478]
</p><p>81 The alternating scheme suffers from strong registration artifacts because of the large exposure differences and cannot reconstruct mid-tones of the scenes (table and flowers). [sent-609, score-0.719]
</p><p>82 With the same capture time, HDR images created using Fibonacci bracketing have a significantly better quality. [sent-612, score-0.632]
</p><p>83 Results oftribonacci bracketing: Figure 8 shows HDR results computed using tribonacci exposure bracketing for the same scenes as in Figure 7. [sent-614, score-1.085]
</p><p>84 The ratio of consecutive exposures in a tribonacci sequence is φ3 = 1. [sent-616, score-0.411]
</p><p>85 For both, the same LDR frames were used (11 frames of a Fibonacci bracketing se-  quence). [sent-622, score-0.739]
</p><p>86 Evaluating the effect of non-linear camera response  era used to capture exposure bracketed images. [sent-627, score-0.565]
</p><p>87 The bracketing sequence is changed according to scene characteristics (intensity and motion) as they vary during video capture. [sent-645, score-0.684]
</p><p>88 We use the following simple function that can be 11447788  (Best exposure)  (No registration)  bracketing + conv. [sent-658, score-0.613]
</p><p>89 (c) HDR image computed using exponential bracketing and conventional registration has strong registration artifacts. [sent-666, score-1.123]
</p><p>90 (d) HDR image obtained using the proposed Fibonacci bracketing and generalized registration techniques. [sent-667, score-0.89]
</p><p>91 registration)  between Fibonacci and two existing bracketing  birthday party. [sent-671, score-0.613]
</p><p>92 (d) HDR images created using Fibonacci bracketing and generalized registration. [sent-678, score-0.685]
</p><p>93 We used a Miro M3 10 camera (see Figure 5) for capturing exposure bracketed images. [sent-688, score-0.537]
</p><p>94 A Point-Grey Flea3 camera was used for ‘scene-metering’ - intensity and motion information was computed on images captured by the Flea3 camera for determining the bracketing sequence. [sent-689, score-0.801]
</p><p>95 Iˆ  Mˆ  Since image analysis and capture steps are parallelized and the total capture time for each bracketing sequence is about 60ms, our system captures HDR video at 15 fps. [sent-690, score-0.704]
</p><p>96 Ours is the first exposure bracketing scheme that is designed to deal with dynamic camera and objects. [sent-694, score-1.131]
</p><p>97 Fibonac i bracketing +Fibonac i bracketing + Conventional registration Generalized registration Figure 9. [sent-697, score-1.636]
</p><p>98 The same LDR images (11 frames of a Fibonacci bracketing sequence) were used for both cases. [sent-699, score-0.676]
</p><p>99 High dynamic range imaging by varying exposure time, gain and aperture of a video camera. [sent-815, score-0.522]
</p><p>100 Space-variant blur deconvolution and denoising in the dual exposure problem. [sent-854, score-0.4]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bracketing', 0.613), ('exposure', 0.376), ('fibonacci', 0.346), ('hdr', 0.336), ('exposures', 0.254), ('registration', 0.205), ('ldr', 0.165), ('bracketed', 0.089), ('fis', 0.078), ('generalized', 0.072), ('dynamic', 0.068), ('frames', 0.063), ('flow', 0.063), ('tribonacci', 0.062), ('ghosting', 0.055), ('sequence', 0.053), ('dr', 0.051), ('emin', 0.051), ('ns', 0.049), ('fi', 0.049), ('drfib', 0.044), ('exponential', 0.044), ('camera', 0.043), ('artifacts', 0.04), ('conventional', 0.04), ('burst', 0.039), ('response', 0.038), ('range', 0.037), ('isoexposure', 0.036), ('ei', 0.035), ('achievable', 0.033), ('motion', 0.032), ('hardware', 0.032), ('scheme', 0.031), ('intensity', 0.03), ('capturing', 0.029), ('sensor', 0.029), ('nt', 0.028), ('techniques', 0.028), ('growth', 0.027), ('flows', 0.027), ('exposed', 0.027), ('consecutive', 0.027), ('negligible', 0.027), ('fibonac', 0.027), ('tetranacci', 0.027), ('imaging', 0.026), ('sums', 0.026), ('differences', 0.025), ('captured', 0.024), ('times', 0.024), ('blur', 0.024), ('sequences', 0.024), ('alternating', 0.024), ('oii', 0.024), ('sen', 0.023), ('property', 0.023), ('consumer', 0.022), ('modifications', 0.022), ('registering', 0.022), ('gap', 0.022), ('fit', 0.021), ('brightness', 0.02), ('intensities', 0.019), ('optical', 0.019), ('short', 0.019), ('contiguous', 0.019), ('capture', 0.019), ('scenes', 0.018), ('successive', 0.018), ('arduino', 0.018), ('dconv', 0.018), ('drmax', 0.018), ('exponentialbracketing', 0.018), ('flames', 0.018), ('hdrimage', 0.018), ('iimmainx', 0.018), ('miro', 0.018), ('tocci', 0.018), ('scene', 0.018), ('bright', 0.017), ('cameras', 0.017), ('sum', 0.016), ('computed', 0.016), ('emax', 0.016), ('emulated', 0.016), ('externally', 0.016), ('ward', 0.016), ('faithfully', 0.015), ('schemes', 0.015), ('frame', 0.015), ('columbia', 0.015), ('minimum', 0.015), ('aperture', 0.015), ('ratio', 0.015), ('candle', 0.015), ('saturated', 0.015), ('wide', 0.014), ('nayar', 0.014), ('dark', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="164-tfidf-1" href="./iccv-2013-Fibonacci_Exposure_Bracketing_for_High_Dynamic_Range_Imaging.html">164 iccv-2013-Fibonacci Exposure Bracketing for High Dynamic Range Imaging</a></p>
<p>Author: Mohit Gupta, Daisuke Iso, Shree K. Nayar</p><p>Abstract: Exposure bracketing for high dynamic range (HDR) imaging involves capturing several images of the scene at different exposures. If either the camera or the scene moves during capture, the captured images must be registered. Large exposure differences between bracketed images lead to inaccurate registration, resulting in artifacts such as ghosting (multiple copies of scene objects) and blur. We present two techniques, one for image capture (Fibonacci exposure bracketing) and one for image registration (generalized registration), to prevent such motion-related artifacts. Fibonacci bracketing involves capturing a sequence of images such that each exposure time is the sum of the previous N(N > 1) exposures. Generalized registration involves estimating motion between sums of contiguous sets of frames, instead of between individual frames. Together, the two techniques ensure that motion is always estimated betweenframes of the same total exposure time. This results in HDR images and videos which have both a large dynamic range andminimal motion-relatedartifacts. We show, by results for several real-world indoor and outdoor scenes, that theproposed approach significantly outperforms several ex- isting bracketing schemes.</p><p>2 0.17471211 <a title="164-tfidf-2" href="./iccv-2013-Fluttering_Pattern_Generation_Using_Modified_Legendre_Sequence_for_Coded_Exposure_Imaging.html">173 iccv-2013-Fluttering Pattern Generation Using Modified Legendre Sequence for Coded Exposure Imaging</a></p>
<p>Author: Hae-Gon Jeon, Joon-Young Lee, Yudeog Han, Seon Joo Kim, In So Kweon</p><p>Abstract: Finding a good binary sequence is critical in determining theperformance ofthe coded exposure imaging, butprevious methods mostly rely on a random search for finding the binary codes, which could easily fail to find good long sequences due to the exponentially growing search space. In this paper, we present a new computationally efficient algorithm for generating the binary sequence, which is especially well suited for longer sequences. We show that the concept of the low autocorrelation binary sequence that has been well exploited in the information theory community can be applied for generating the fluttering patterns of the shutter, propose a new measure of a good binary sequence, and present a new algorithm by modifying the Legendre sequence for the coded exposure imaging. Experiments using both synthetic and real data show that our new algorithm consistently generates better binary sequencesfor the coded exposure problem, yielding better deblurring and resolution enhancement results compared to the previous methods for generating the binary codes.</p><p>3 0.13023302 <a title="164-tfidf-3" href="./iccv-2013-Partial_Sum_Minimization_of_Singular_Values_in_RPCA_for_Low-Level_Vision.html">310 iccv-2013-Partial Sum Minimization of Singular Values in RPCA for Low-Level Vision</a></p>
<p>Author: Tae-Hyun Oh, Hyeongwoo Kim, Yu-Wing Tai, Jean-Charles Bazin, In So Kweon</p><p>Abstract: Robust Principal Component Analysis (RPCA) via rank minimization is a powerful tool for recovering underlying low-rank structure of clean data corrupted with sparse noise/outliers. In many low-level vision problems, not only it is known that the underlying structure of clean data is low-rank, but the exact rank of clean data is also known. Yet, when applying conventional rank minimization for those problems, the objective function is formulated in a way that does not fully utilize a priori target rank information about the problems. This observation motivates us to investigate whether there is a better alternative solution when using rank minimization. In this paper, instead of minimizing the nuclear norm, we propose to minimize the partial sum of singular values. The proposed objective function implicitly encourages the target rank constraint in rank minimization. Our experimental analyses show that our approach performs better than conventional rank minimization when the number of samples is deficient, while the solutions obtained by the two approaches are almost identical when the number of samples is more than sufficient. We apply our approach to various low-level vision problems, e.g. high dynamic range imaging, photometric stereo and image alignment, and show that our results outperform those obtained by the conventional nuclear norm rank minimization method.</p><p>4 0.10117516 <a title="164-tfidf-4" href="./iccv-2013-Uncertainty-Driven_Efficiently-Sampled_Sparse_Graphical_Models_for_Concurrent_Tumor_Segmentation_and_Atlas_Registration.html">432 iccv-2013-Uncertainty-Driven Efficiently-Sampled Sparse Graphical Models for Concurrent Tumor Segmentation and Atlas Registration</a></p>
<p>Author: Sarah Parisot, William Wells_III, Stéphane Chemouny, Hugues Duffau, Nikos Paragios</p><p>Abstract: Graph-based methods have become popular in recent years and have successfully addressed tasks like segmentation and deformable registration. Their main strength is optimality of the obtained solution while their main limitation is the lack of precision due to the grid-like representations and the discrete nature of the quantized search space. In this paper we introduce a novel approach for combined segmentation/registration of brain tumors that adapts graph and sampling resolution according to the image content. To this end we estimate the segmentation and registration marginals towards adaptive graph resolution and intelligent definition of the search space. This information is considered in a hierarchical framework where uncertainties are propagated in a natural manner. State of the art results in the joint segmentation/registration of brain images with low-grade gliomas demonstrate the potential of our approach.</p><p>5 0.078854293 <a title="164-tfidf-5" href="./iccv-2013-A_Unified_Rolling_Shutter_and_Motion_Blur_Model_for_3D_Visual_Registration.html">32 iccv-2013-A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration</a></p>
<p>Author: Maxime Meilland, Tom Drummond, Andrew I. Comport</p><p>Abstract: Motion blur and rolling shutter deformations both inhibit visual motion registration, whether it be due to a moving sensor or a moving target. Whilst both deformations exist simultaneously, no models have been proposed to handle them together. Furthermore, neither deformation has been consideredpreviously in the context of monocularfullimage 6 degrees of freedom registration or RGB-D structure and motion. As will be shown, rolling shutter deformation is observed when a camera moves faster than a single pixel in parallax between subsequent scan-lines. Blur is a function of the pixel exposure time and the motion vector. In this paper a complete dense 3D registration model will be derived to accountfor both motion blur and rolling shutter deformations simultaneously. Various approaches will be compared with respect to ground truth and live real-time performance will be demonstratedfor complex scenarios where both blur and shutter deformations are dominant.</p><p>6 0.072778091 <a title="164-tfidf-6" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>7 0.062918611 <a title="164-tfidf-7" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>8 0.062498417 <a title="164-tfidf-8" href="./iccv-2013-Go-ICP%3A_Solving_3D_Registration_Efficiently_and_Globally_Optimally.html">185 iccv-2013-Go-ICP: Solving 3D Registration Efficiently and Globally Optimally</a></p>
<p>9 0.060012657 <a title="164-tfidf-9" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>10 0.059570167 <a title="164-tfidf-10" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>11 0.059303068 <a title="164-tfidf-11" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>12 0.058570147 <a title="164-tfidf-12" href="./iccv-2013-Accurate_and_Robust_3D_Facial_Capture_Using_a_Single_RGBD_Camera.html">36 iccv-2013-Accurate and Robust 3D Facial Capture Using a Single RGBD Camera</a></p>
<p>13 0.058419947 <a title="164-tfidf-13" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>14 0.053648219 <a title="164-tfidf-14" href="./iccv-2013-Rolling_Shutter_Stereo.html">363 iccv-2013-Rolling Shutter Stereo</a></p>
<p>15 0.050554525 <a title="164-tfidf-15" href="./iccv-2013-Geometric_Registration_Based_on_Distortion_Estimation.html">183 iccv-2013-Geometric Registration Based on Distortion Estimation</a></p>
<p>16 0.050482504 <a title="164-tfidf-16" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<p>17 0.048549909 <a title="164-tfidf-17" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>18 0.047184918 <a title="164-tfidf-18" href="./iccv-2013-Multiple_Non-rigid_Surface_Detection_and_Registration.html">283 iccv-2013-Multiple Non-rigid Surface Detection and Registration</a></p>
<p>19 0.046793789 <a title="164-tfidf-19" href="./iccv-2013-A_Generic_Deformation_Model_for_Dense_Non-rigid_Surface_Registration%3A_A_Higher-Order_MRF-Based_Approach.html">16 iccv-2013-A Generic Deformation Model for Dense Non-rigid Surface Registration: A Higher-Order MRF-Based Approach</a></p>
<p>20 0.046641205 <a title="164-tfidf-20" href="./iccv-2013-Depth_from_Combining_Defocus_and_Correspondence_Using_Light-Field_Cameras.html">108 iccv-2013-Depth from Combining Defocus and Correspondence Using Light-Field Cameras</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.078), (1, -0.074), (2, -0.012), (3, 0.04), (4, -0.015), (5, 0.016), (6, 0.021), (7, -0.034), (8, 0.036), (9, 0.006), (10, -0.02), (11, -0.021), (12, 0.07), (13, -0.03), (14, -0.017), (15, 0.019), (16, 0.007), (17, 0.054), (18, 0.01), (19, -0.017), (20, 0.04), (21, -0.026), (22, -0.045), (23, 0.01), (24, -0.046), (25, -0.085), (26, -0.005), (27, 0.01), (28, 0.072), (29, -0.0), (30, 0.04), (31, -0.077), (32, 0.029), (33, 0.09), (34, 0.012), (35, -0.005), (36, 0.005), (37, 0.059), (38, 0.043), (39, 0.031), (40, 0.058), (41, 0.067), (42, 0.009), (43, 0.058), (44, -0.003), (45, 0.0), (46, 0.04), (47, 0.054), (48, 0.054), (49, -0.001)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91883606 <a title="164-lsi-1" href="./iccv-2013-Fibonacci_Exposure_Bracketing_for_High_Dynamic_Range_Imaging.html">164 iccv-2013-Fibonacci Exposure Bracketing for High Dynamic Range Imaging</a></p>
<p>Author: Mohit Gupta, Daisuke Iso, Shree K. Nayar</p><p>Abstract: Exposure bracketing for high dynamic range (HDR) imaging involves capturing several images of the scene at different exposures. If either the camera or the scene moves during capture, the captured images must be registered. Large exposure differences between bracketed images lead to inaccurate registration, resulting in artifacts such as ghosting (multiple copies of scene objects) and blur. We present two techniques, one for image capture (Fibonacci exposure bracketing) and one for image registration (generalized registration), to prevent such motion-related artifacts. Fibonacci bracketing involves capturing a sequence of images such that each exposure time is the sum of the previous N(N > 1) exposures. Generalized registration involves estimating motion between sums of contiguous sets of frames, instead of between individual frames. Together, the two techniques ensure that motion is always estimated betweenframes of the same total exposure time. This results in HDR images and videos which have both a large dynamic range andminimal motion-relatedartifacts. We show, by results for several real-world indoor and outdoor scenes, that theproposed approach significantly outperforms several ex- isting bracketing schemes.</p><p>2 0.70866793 <a title="164-lsi-2" href="./iccv-2013-Fluttering_Pattern_Generation_Using_Modified_Legendre_Sequence_for_Coded_Exposure_Imaging.html">173 iccv-2013-Fluttering Pattern Generation Using Modified Legendre Sequence for Coded Exposure Imaging</a></p>
<p>Author: Hae-Gon Jeon, Joon-Young Lee, Yudeog Han, Seon Joo Kim, In So Kweon</p><p>Abstract: Finding a good binary sequence is critical in determining theperformance ofthe coded exposure imaging, butprevious methods mostly rely on a random search for finding the binary codes, which could easily fail to find good long sequences due to the exponentially growing search space. In this paper, we present a new computationally efficient algorithm for generating the binary sequence, which is especially well suited for longer sequences. We show that the concept of the low autocorrelation binary sequence that has been well exploited in the information theory community can be applied for generating the fluttering patterns of the shutter, propose a new measure of a good binary sequence, and present a new algorithm by modifying the Legendre sequence for the coded exposure imaging. Experiments using both synthetic and real data show that our new algorithm consistently generates better binary sequencesfor the coded exposure problem, yielding better deblurring and resolution enhancement results compared to the previous methods for generating the binary codes.</p><p>3 0.63313794 <a title="164-lsi-3" href="./iccv-2013-A_Unified_Rolling_Shutter_and_Motion_Blur_Model_for_3D_Visual_Registration.html">32 iccv-2013-A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration</a></p>
<p>Author: Maxime Meilland, Tom Drummond, Andrew I. Comport</p><p>Abstract: Motion blur and rolling shutter deformations both inhibit visual motion registration, whether it be due to a moving sensor or a moving target. Whilst both deformations exist simultaneously, no models have been proposed to handle them together. Furthermore, neither deformation has been consideredpreviously in the context of monocularfullimage 6 degrees of freedom registration or RGB-D structure and motion. As will be shown, rolling shutter deformation is observed when a camera moves faster than a single pixel in parallax between subsequent scan-lines. Blur is a function of the pixel exposure time and the motion vector. In this paper a complete dense 3D registration model will be derived to accountfor both motion blur and rolling shutter deformations simultaneously. Various approaches will be compared with respect to ground truth and live real-time performance will be demonstratedfor complex scenarios where both blur and shutter deformations are dominant.</p><p>4 0.52768618 <a title="164-lsi-4" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>Author: Supreeth Achar, Stephen T. Nuske, Srinivasa G. Narasimhan</p><p>Abstract: Separating the direct and global components of radiance can aid shape recovery algorithms and can provide useful information about materials in a scene. Practical methods for finding the direct and global components use multiple images captured under varying illumination patterns and require the scene, light source and camera to remain stationary during the image acquisition process. In this paper, we develop a motion compensation method that relaxes this condition and allows direct-global separation to beperformed on video sequences of dynamic scenes captured by moving projector-camera systems. Key to our method is being able to register frames in a video sequence to each other in the presence of time varying, high frequency active illumination patterns. We compare our motion compensated method to alternatives such as single shot separation and frame interleaving as well as ground truth. We present results on challenging video sequences that include various types of motions and deformations in scenes that contain complex materials like fabric, skin, leaves and wax.</p><p>5 0.52451897 <a title="164-lsi-5" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>Author: Qian-Yi Zhou, Stephen Miller, Vladlen Koltun</p><p>Abstract: We present an approach to reconstruction of detailed scene geometry from range video. Range data produced by commodity handheld cameras suffers from high-frequency errors and low-frequency distortion. Our approach deals with both sources of error by reconstructing locally smooth scene fragments and letting these fragments deform in order to align to each other. We develop a volumetric registration formulation that leverages the smoothness of the deformation to make optimization practical for large scenes. Experimental results demonstrate that our approach substantially increases the fidelity of complex scene geometry reconstructed with commodity handheld cameras.</p><p>6 0.49618793 <a title="164-lsi-6" href="./iccv-2013-Rolling_Shutter_Stereo.html">363 iccv-2013-Rolling Shutter Stereo</a></p>
<p>7 0.49185374 <a title="164-lsi-7" href="./iccv-2013-Geometric_Registration_Based_on_Distortion_Estimation.html">183 iccv-2013-Geometric Registration Based on Distortion Estimation</a></p>
<p>8 0.48752254 <a title="164-lsi-8" href="./iccv-2013-Go-ICP%3A_Solving_3D_Registration_Efficiently_and_Globally_Optimally.html">185 iccv-2013-Go-ICP: Solving 3D Registration Efficiently and Globally Optimally</a></p>
<p>9 0.46995342 <a title="164-lsi-9" href="./iccv-2013-A_Generic_Deformation_Model_for_Dense_Non-rigid_Surface_Registration%3A_A_Higher-Order_MRF-Based_Approach.html">16 iccv-2013-A Generic Deformation Model for Dense Non-rigid Surface Registration: A Higher-Order MRF-Based Approach</a></p>
<p>10 0.44846642 <a title="164-lsi-10" href="./iccv-2013-Uncertainty-Driven_Efficiently-Sampled_Sparse_Graphical_Models_for_Concurrent_Tumor_Segmentation_and_Atlas_Registration.html">432 iccv-2013-Uncertainty-Driven Efficiently-Sampled Sparse Graphical Models for Concurrent Tumor Segmentation and Atlas Registration</a></p>
<p>11 0.44352362 <a title="164-lsi-11" href="./iccv-2013-Robust_Non-parametric_Data_Fitting_for_Correspondence_Modeling.html">358 iccv-2013-Robust Non-parametric Data Fitting for Correspondence Modeling</a></p>
<p>12 0.44096571 <a title="164-lsi-12" href="./iccv-2013-Modeling_Self-Occlusions_in_Dynamic_Shape_and_Appearance_Tracking.html">270 iccv-2013-Modeling Self-Occlusions in Dynamic Shape and Appearance Tracking</a></p>
<p>13 0.4397831 <a title="164-lsi-13" href="./iccv-2013-Optimal_Orthogonal_Basis_and_Image_Assimilation%3A_Motion_Modeling.html">301 iccv-2013-Optimal Orthogonal Basis and Image Assimilation: Motion Modeling</a></p>
<p>14 0.41993913 <a title="164-lsi-14" href="./iccv-2013-Space-Time_Tradeoffs_in_Photo_Sequencing.html">397 iccv-2013-Space-Time Tradeoffs in Photo Sequencing</a></p>
<p>15 0.41950798 <a title="164-lsi-15" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<p>16 0.41225883 <a title="164-lsi-16" href="./iccv-2013-Multiple_Non-rigid_Surface_Detection_and_Registration.html">283 iccv-2013-Multiple Non-rigid Surface Detection and Registration</a></p>
<p>17 0.4012118 <a title="164-lsi-17" href="./iccv-2013-Motion-Aware_KNN_Laplacian_for_Video_Matting.html">275 iccv-2013-Motion-Aware KNN Laplacian for Video Matting</a></p>
<p>18 0.3991749 <a title="164-lsi-18" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>19 0.39325035 <a title="164-lsi-19" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>20 0.38701773 <a title="164-lsi-20" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.02), (7, 0.01), (26, 0.081), (27, 0.026), (31, 0.031), (40, 0.013), (42, 0.041), (64, 0.044), (73, 0.051), (76, 0.357), (89, 0.163), (98, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73532635 <a title="164-lda-1" href="./iccv-2013-Fibonacci_Exposure_Bracketing_for_High_Dynamic_Range_Imaging.html">164 iccv-2013-Fibonacci Exposure Bracketing for High Dynamic Range Imaging</a></p>
<p>Author: Mohit Gupta, Daisuke Iso, Shree K. Nayar</p><p>Abstract: Exposure bracketing for high dynamic range (HDR) imaging involves capturing several images of the scene at different exposures. If either the camera or the scene moves during capture, the captured images must be registered. Large exposure differences between bracketed images lead to inaccurate registration, resulting in artifacts such as ghosting (multiple copies of scene objects) and blur. We present two techniques, one for image capture (Fibonacci exposure bracketing) and one for image registration (generalized registration), to prevent such motion-related artifacts. Fibonacci bracketing involves capturing a sequence of images such that each exposure time is the sum of the previous N(N > 1) exposures. Generalized registration involves estimating motion between sums of contiguous sets of frames, instead of between individual frames. Together, the two techniques ensure that motion is always estimated betweenframes of the same total exposure time. This results in HDR images and videos which have both a large dynamic range andminimal motion-relatedartifacts. We show, by results for several real-world indoor and outdoor scenes, that theproposed approach significantly outperforms several ex- isting bracketing schemes.</p><p>2 0.6427117 <a title="164-lda-2" href="./iccv-2013-Joint_Inverted_Indexing.html">221 iccv-2013-Joint Inverted Indexing</a></p>
<p>Author: Yan Xia, Kaiming He, Fang Wen, Jian Sun</p><p>Abstract: Inverted indexing is a popular non-exhaustive solution to large scale search. An inverted file is built by a quantizer such as k-means or a tree structure. It has been found that multiple inverted files, obtained by multiple independent random quantizers, are able to achieve practically good recall and speed. Instead of computing the multiple quantizers independently, we present a method that creates them jointly. Our method jointly optimizes all codewords in all quantizers. Then it assigns these codewords to the quantizers. In experiments this method shows significant improvement over various existing methods that use multiple independent quantizers. On the one-billion set of SIFT vectors, our method is faster and more accurate than a recent state-of-the-art inverted indexing method.</p><p>3 0.58166653 <a title="164-lda-3" href="./iccv-2013-Discovering_Object_Functionality.html">118 iccv-2013-Discovering Object Functionality</a></p>
<p>Author: Bangpeng Yao, Jiayuan Ma, Li Fei-Fei</p><p>Abstract: Object functionality refers to the quality of an object that allows humans to perform some specific actions. It has been shown in psychology that functionality (affordance) is at least as essential as appearance in object recognition by humans. In computer vision, most previous work on functionality either assumes exactly one functionality for each object, or requires detailed annotation of human poses and objects. In this paper, we propose a weakly supervised approach to discover all possible object functionalities. Each object functionality is represented by a specific type of human-object interaction. Our method takes any possible human-object interaction into consideration, and evaluates image similarity in 3D rather than 2D in order to cluster human-object interactions more coherently. Experimental results on a dataset of people interacting with musical instruments show the effectiveness of our approach.</p><p>4 0.57461226 <a title="164-lda-4" href="./iccv-2013-Manipulation_Pattern_Discovery%3A_A_Nonparametric_Bayesian_Approach.html">260 iccv-2013-Manipulation Pattern Discovery: A Nonparametric Bayesian Approach</a></p>
<p>Author: Bingbing Ni, Pierre Moulin</p><p>Abstract: We aim to unsupervisedly discover human’s action (motion) patterns of manipulating various objects in scenarios such as assisted living. We are motivated by two key observations. First, large variation exists in motion patterns associated with various types of objects being manipulated, thus manually defining motion primitives is infeasible. Second, some motion patterns are shared among different objects being manipulated while others are object specific. We therefore propose a nonparametric Bayesian method that adopts a hierarchical Dirichlet process prior to learn representative manipulation (motion) patterns in an unsupervised manner. Taking easy-to-obtain object detection score maps and dense motion trajectories as inputs, the proposed probabilistic model can discover motion pattern groups associated with different types of objects being manipulated with a shared manipulation pattern dictionary. The size of the learned dictionary is automatically inferred. Com- prehensive experiments on two assisted living benchmarks and a cooking motion dataset demonstrate superiority of our learned manipulation pattern dictionary in representing manipulation actions for recognition.</p><p>5 0.55671394 <a title="164-lda-5" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>Author: Matthias Reso, Jörn Jachalsky, Bodo Rosenhahn, Jörn Ostermann</p><p>Abstract: Superpixel algorithms represent a very useful and increasingly popular preprocessing step for a wide range of computer vision applications, as they offer the potential to boost efficiency and effectiveness. In this regards, this paper presents a highly competitive approach for temporally consistent superpixelsfor video content. The approach is based on energy-minimizing clustering utilizing a novel hybrid clustering strategy for a multi-dimensional feature space working in a global color subspace and local spatial subspaces. Moreover, a new contour evolution based strategy is introduced to ensure spatial coherency of the generated superpixels. For a thorough evaluation the proposed approach is compared to state of the art supervoxel algorithms using established benchmarks and shows a superior performance.</p><p>6 0.48331347 <a title="164-lda-6" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>7 0.4788624 <a title="164-lda-7" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>8 0.47827339 <a title="164-lda-8" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>9 0.47787344 <a title="164-lda-9" href="./iccv-2013-Image_Guided_Depth_Upsampling_Using_Anisotropic_Total_Generalized_Variation.html">209 iccv-2013-Image Guided Depth Upsampling Using Anisotropic Total Generalized Variation</a></p>
<p>10 0.47584713 <a title="164-lda-10" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>11 0.47376099 <a title="164-lda-11" href="./iccv-2013-Bayesian_Robust_Matrix_Factorization_for_Image_and_Video_Processing.html">60 iccv-2013-Bayesian Robust Matrix Factorization for Image and Video Processing</a></p>
<p>12 0.47331533 <a title="164-lda-12" href="./iccv-2013-Uncertainty-Driven_Efficiently-Sampled_Sparse_Graphical_Models_for_Concurrent_Tumor_Segmentation_and_Atlas_Registration.html">432 iccv-2013-Uncertainty-Driven Efficiently-Sampled Sparse Graphical Models for Concurrent Tumor Segmentation and Atlas Registration</a></p>
<p>13 0.47279224 <a title="164-lda-13" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<p>14 0.47215766 <a title="164-lda-14" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>15 0.47040939 <a title="164-lda-15" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<p>16 0.4700878 <a title="164-lda-16" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>17 0.46996361 <a title="164-lda-17" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>18 0.46964407 <a title="164-lda-18" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>19 0.4689385 <a title="164-lda-19" href="./iccv-2013-Data-Driven_3D_Primitives_for_Single_Image_Understanding.html">102 iccv-2013-Data-Driven 3D Primitives for Single Image Understanding</a></p>
<p>20 0.46844926 <a title="164-lda-20" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
