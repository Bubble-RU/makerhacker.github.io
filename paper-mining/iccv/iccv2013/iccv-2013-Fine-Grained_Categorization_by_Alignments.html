<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>169 iccv-2013-Fine-Grained Categorization by Alignments</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-169" href="#">iccv2013-169</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>169 iccv-2013-Fine-Grained Categorization by Alignments</h1>
<br/><p>Source: <a title="iccv-2013-169-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Gavves_Fine-Grained_Categorization_by_2013_ICCV_paper.pdf">pdf</a></p><p>Author: E. Gavves, B. Fernando, C.G.M. Snoek, A.W.M. Smeulders, T. Tuytelaars</p><p>Abstract: The aim of this paper is fine-grained categorization without human interaction. Different from prior work, which relies on detectors for specific object parts, we propose to localize distinctive details by roughly aligning the objects using just the overall shape, since implicit to fine-grained categorization is the existence of a super-class shape shared among all classes. The alignments are then used to transfer part annotations from training images to test images (supervised alignment), or to blindly yet consistently segment the object in a number of regions (unsupervised alignment). We furthermore argue that in the distinction of finegrained sub-categories, classification-oriented encodings like Fisher vectors are better suited for describing localized information than popular matching oriented features like HOG. We evaluate the method on the CU-2011 Birds and Stanford Dogs fine-grained datasets, outperforming the state-of-the-art.</p><p>Reference: <a title="iccv-2013-169-reference" href="../iccv2013_reference/iccv-2013-Fine-Grained_Categorization_by_Alignments_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Different from prior work, which relies on detectors for specific object parts, we propose to localize distinctive details by roughly aligning the objects using just the overall shape, since implicit to fine-grained categorization is the existence of a super-class shape shared among all classes. [sent-11, score-0.319]
</p><p>2 The alignments are then used to transfer part annotations from training images to test images (supervised alignment), or to blindly yet consistently segment the object in a number of regions (unsupervised alignment). [sent-12, score-0.862]
</p><p>3 We furthermore argue that in the distinction of finegrained sub-categories, classification-oriented encodings like Fisher vectors are better suited for describing localized information than popular matching oriented features like HOG. [sent-13, score-0.328]
</p><p>4 Introduction  Fine-grained categorization relies on identifying the subtle differences in appearance of specific object parts. [sent-16, score-0.176]
</p><p>5 Humans learn to distinguish different types of birds by addressing the differences in specific details. [sent-18, score-0.255]
</p><p>6 Based on example images like these, fine-grained categorization tries to answer the question: what fine-grained bird category do we have in the third image? [sent-25, score-0.194]
</p><p>7 Rather than directly trying to localize parts (be it distinctive or intrinsic), we show in this paper that better results can be obtained if one first tries to align the birds based on their global  shape, ignoring the actual bird categories. [sent-26, score-0.54]
</p><p>8 Yet, it remains unclear what is the most critical aspect of “parts” in a fine-grained categorization context: is it the ability to accurately localize corresponding locations over object instances, or simply the ability to capture detailed information? [sent-29, score-0.282]
</p><p>9 We argue that a very precise “part” localization is not necessary and rough alignments suffice, as long as one manages to capture the finegrained details in the appearance. [sent-31, score-0.823]
</p><p>10 Parts may be divided in intrinsic parts [3, 16] such as the head of a dog or the body of a bird, and distinctive parts [32, 31] specific to few sub-categories. [sent-32, score-0.341]
</p><p>11 Recovering intrinsic parts implies that such parts are seen throughout the whole dataset. [sent-33, score-0.261]
</p><p>12 Furthermore, rough alignment is not sub-category specific, thus the object representation becomes independent of the number of classes or training images [33, 32]. [sent-42, score-0.204]
</p><p>13 In the unsupervised case, we use alignments to delineate corresponding object regions that we will use in the differential classification. [sent-47, score-0.857]
</p><p>14 In contrast to the raw SIFT or template features preferred in the fine-grained literature [16, 3 1, 32], such localized feature encodings are less sensitive to misalignments. [sent-51, score-0.172]
</p><p>15 We present two methods for recovering alignments that require varying levels of part supervision during training. [sent-53, score-0.783]
</p><p>16 The results vouch for unsupervised alignments, which outperform previous published results. [sent-55, score-0.152]
</p><p>17 Different from the above works, we propose to use strong classification- and not matchingoriented, encodings to describe the alignment parts and regions. [sent-66, score-0.29]
</p><p>18 However, we use Fisher vectors not only as global, object level representations, but also as localized appearance descriptors. [sent-71, score-0.197]
</p><p>19 The detection of objects in a fine-grained categorization setting ranges from the segmentation of the object of interest [19, 5, 6] to fitting ellipsoids [10] and detecting individual parts and templates [33, 34, 32, 3 1, 16]. [sent-73, score-0.348]
</p><p>20 propose to use deformable part models to detect the head of cats and dogs and in [1] Berg and Belhumeur learn discriminative parts from pairwise comparisons between classes. [sent-85, score-0.293]
</p><p>21 propose to share parts between classes to arrive at accurate part localization. [sent-87, score-0.284]
</p><p>22 Based on this alignment, we then derive a small number of predicted parts (supervised) or regions (unsupervised). [sent-89, score-0.171]
</p><p>23 The computation of the segmentation mask can be accurate as in the left, ok as in the middle or completely fail as in the right image. [sent-96, score-0.194]
</p><p>24 We say an image is aligned with other images if we have identified a local frame of reference in the image that is consistent with (a subset of) the frames of reference found in other images. [sent-113, score-0.149]
</p><p>25 As is common in fine-grained categorization [33, 32, 3 1], we have available both at training and at test time the bounding box locations of the object of interest. [sent-115, score-0.368]
</p><p>26 , all birds are usually either on trees or flying in the sky. [sent-119, score-0.255]
</p><p>27 The rectangular bounding box around an object allows for extracting important information, such as the approximate shape of the object. [sent-120, score-0.214]
</p><p>28 More specifically, we use GrabCut [25] on the bounding box to compute an accurate figure-  ground segmentation. [sent-121, score-0.166]
</p><p>29 Supervised alignments In the supervised scenario the ground truth locations of basic object parts, such as the beak or the tail of the birds, are available in the training set. [sent-126, score-1.102]
</p><p>30 Then, we can use the common frame of reference to predict the part locations in the test image. [sent-129, score-0.242]
</p><p>31 Our first goal is to retrieve a small number of training pictures that have a similar shape as the object in the test image. [sent-130, score-0.154]
</p><p>32 To this end, we first obtain the segmentation mask of the object as described before. [sent-132, score-0.158]
</p><p>33 We are now in position to use the ground truth locations of the parts in the training images and predict the corresponding locations in the test image. [sent-143, score-0.421]
</p><p>34 To calculate the positions of the same parts on the test image, one may apply several methods of varying sophistication, ranging from simple average pooling of part locations to local, independent optimization of parts based on HOG convolutions. [sent-144, score-0.444]
</p><p>35 To ensure maximum compatibility we repeat the above procedure for all training and testing images in the dataset, thus predicting part locations for all the objects in the dataset. [sent-146, score-0.198]
</p><p>36 On the right, we have the nearest neighbor training images, their ground truth part locations and their HOG shape representations, based on which they were retrieved. [sent-149, score-0.31]
</p><p>37 Unsupervised alignments In the unsupervised scenario no ground truth information of the training part locations is available. [sent-154, score-1.062]
</p><p>38 However, we still have the bounding box that surrounds the object, based on which we can derive a shape mask per object. [sent-155, score-0.264]
</p><p>39 Since no ground truth part locations are available, it does not make sense to align the test image to a small subset of training images. [sent-156, score-0.264]
</p><p>40 While not as accurate as the alignments in the previous subsection, this procedure allows us to obtain robust and consistent alignments over the entire database. [sent-158, score-1.426]
</p><p>41 More specifically, we fit an ellipse to the pixels X of the segmentation mask and compute the local 2-d geometry in the form of the two principal axes aj  = ¯x  + ej? [sent-159, score-0.262]
</p><p>42 To this end we extract the principal axes using all the foreground pixels of the shape mask. [sent-166, score-0.181]
</p><p>43 For objects that have an elliptical shape the longer axis is usually the principal axis. [sent-167, score-0.158]
</p><p>44 We therefore decide not to use the ancillary axis in the generation of consistent regions. [sent-170, score-0.155]
</p><p>45 Relative to this frame of reference, we can define different locations or regions at will. [sent-173, score-0.156]
</p><p>46 Here, we divide the principal axis equally from the origin to the end in a fixed number of segments, and define regions as the part of the foreground mask that falls within one such segment. [sent-174, score-0.384]
</p><p>47 Given accurate segmentation masks, the corresponding locations in different fine-grained objects are visited in the same order, thus resulting in pose-normalized representations, see Fig. [sent-175, score-0.178]
</p><p>48 Final Image Representation Our alignments are designed to be rough. [sent-180, score-0.676]
</p><p>49 Note that although the second approach is theoretically more accurate in capturing only the object appearance details, at the same time it might either include background pixels or omit foreground pixels, since segmentation masks are not perfect. [sent-189, score-0.201]
</p><p>50 After fitting an ellipse, we obtain the two axes in the middle column pictures, the principal green and the ancillary magenta ones. [sent-192, score-0.193]
</p><p>51 After the gravity vector assumption [22] we assume the origin of the principal axis to be the highest point in the direction of the green arrow. [sent-193, score-0.169]
</p><p>52 Based on this frame of reference, we split equally in the right column pictures the principal axis to obtain consistent regions. [sent-194, score-0.24]
</p><p>53 We use  the ground truth part annotations only during learning, unless stated otherwise. [sent-205, score-0.164]
</p><p>54 Fisher vectors are better equipped in describing part appearance than HOG for fine-grained categorization. [sent-222, score-0.208]
</p><p>55 Matching vs Classification Descriptors In this first experiment we evaluate what are good descriptors for describing parts in a fine-grained categoriza-  ×  tion setting. [sent-225, score-0.22]
</p><p>56 In order to ensure a fair comparison, as well as to test the maximum recognition capacity of parts for such a task, we use the ground truth part annotations both in training and in testing, as if an oracle algorithm for the part locations was available. [sent-226, score-0.531]
</p><p>57 If Fisher vectors outperform HOG on perfectly aligned ground truth parts, then we expect this to be the case even more for less accurate parts. [sent-227, score-0.217]
</p><p>58 In order to avoid a too strong correlation between the parts and also control the dimensionality of the final feature vector we use only the following 7 parts, which cover the bird silhouette: beak, belly, forehead, left wing, right wing, tail and throat. [sent-229, score-0.201]
</p><p>59 The Fisher vectors from the 7 parts are concatenated with a Fisher vector from the whole bounding box to arrive at the final object representation. [sent-232, score-0.35]
</p><p>60 Similarly, for the HOG object descriptors we also compute a HOG vector using the bounding box, rescaled to 100 100 pixels. [sent-233, score-0.154]
</p><p>61 As we see in Table 1, Fisher vectors are much better in describing parts for fine-grained categorization than matching based descriptors like HOG. [sent-234, score-0.408]
</p><p>62 5 the individual accuracies per class for Fisher vectors and for HOG, noticing that Fisher vectors outperform for 184 out of the 200 sub-categories. [sent-242, score-0.184]
</p><p>63 In the following experiments we report results using only Fisher vectors for describing the appearance of parts and alignments. [sent-243, score-0.261]
</p><p>64 Supervised alignments In the second experiment we test whether supervised alignments actually benefit the recognition of fine-grained categories, as compared to a standard classification pipeline. [sent-246, score-1.467]
</p><p>65 The difference is not that big (2%), but note that for Fisher vector unsupervised alignments no ground truth part locations are required. [sent-251, score-1.033]
</p><p>66 ××  Part selection  [2 2] spatial pyramid kernel Supervised alignment on beak only Supervised alignments  Fisher vectors 39. [sent-252, score-0.95]
</p><p>67 Supervised alignments are more accurate than a spatial pyramid kernel and an alignment based on the beak of a bird only, while being rather close to the theoretical accuracy of the oracle parts in Table 1. [sent-256, score-1.169]
</p><p>68 Here our supervised alignments use ground truth part annotations only in training. [sent-257, score-0.955]
</p><p>69 We use the same 7 parts as in the previous experiment plus a Fisher vector extracted from the whole bounding box. [sent-258, score-0.171]
</p><p>70 We predict their location by averaging the locations of the parts in the top 20 nearest neighbors. [sent-259, score-0.222]
</p><p>71 We compare our proposed supervised alignment method against a 2 2 spatial pyramid using Fisher vectors computed from all SIFT descriptors in the bounding box. [sent-261, score-0.403]
</p><p>72 As we observe in Table 2, parts bring an 17% accuracy improvement over a standard spatial pyramid classification approach, since they better capture the little nuances that  ×  differentiate sub-classes that are otherwise visually very similar. [sent-265, score-0.184]
</p><p>73 Furthermore, we note that extracting Fisher vectors on the supervised alignments is 47. [sent-266, score-0.912]
</p><p>74 5% obtained when extracting Fisher vectors on the parts provided by the ground truth. [sent-268, score-0.271]
</p><p>75 This indicates that we capture the part locations well enough for an appearance descriptor like the Fisher vector. [sent-269, score-0.2]
</p><p>76 In fact, the mean squared error between our estimated parts and the ground truth ones is 12%, after normalizing the respective locations with respect to the bounding box geometry. [sent-270, score-0.378]
</p><p>77 Our supervised alignments perform consistently better for 141 out of the 200 classes. [sent-273, score-0.791]
</p><p>78 We conclude that extracting localized information in the form of alignments or parts matters in a fine-grained categorization setting. [sent-274, score-1.004]
</p><p>79 Unsupervised Alignments  In this experiment we compare the unsupervised alignments with the supervised ones. [sent-277, score-0.913]
</p><p>80 After extracting the principal axis we split the bird mask into four regions, starting from the highest point, considering only the pixels within the segmentation mask. [sent-278, score-0.363]
</p><p>81 We observe that describing the object based on the unsupervised alignments results in more accurate predictions compared to the supervised case (49. [sent-281, score-1.026]
</p><p>82 We observe that birds in these sub-classes have consistent appearance. [sent-286, score-0.285]
</p><p>83 ×  Part selection Supervised alignments [4 1] spatial pyramid kernel Fisher vector from the foreground mask only Unsupervised alignments  Fisher vectors 47. [sent-287, score-1.621]
</p><p>84 Unsupervised alignments are more accurate than supervised ones, while at the same time requiring no supervision at all. [sent-292, score-0.877]
</p><p>85 Note that unsupervised alignments use no ground truth part annotations, neither in training nor in testing. [sent-295, score-0.958]
</p><p>86 We, furthermore, plot the individual accuracy differences per class for supervised and unsupervised alignments in the right picture in Fig. [sent-302, score-0.997]
</p><p>87 The distribution of classes is split roughly equally for supervised and unsupervised alignments, with unsupervised alignments having slightly larger accuracy differences. [sent-304, score-1.087]
</p><p>88 We conclude that compared to supervised parts, unsupervised alignments describe the localized appearance of fine-grained objects at least as good, often better. [sent-305, score-0.999]
</p><p>89 Birds  Accuracy  Pose pooling kernels [34] Pooling feature learning [12] POOF [1] This paper: Unsupervised alignments  28. [sent-306, score-0.715]
</p><p>90 Unsupervised alignments with Fisher vectors outperform the state-ofthe-art considerably. [sent-312, score-0.783]
</p><p>91 Dogs Discriminative Color Descriptors [14] Edge templates [3 1] This paper: Unsupervised alignments  Accuracy 28. [sent-313, score-0.731]
</p><p>92 Unsupervised alignments with Fisher vectors outperform the state-ofthe-art considerably. [sent-318, score-0.783]
</p><p>93 State-of-the-art comparison In experiment 4, we compare our unsupervised alignments with state-of-the-art methods reported on CU-201 1 Birds and Stanford Dogs. [sent-321, score-0.798]
</p><p>94 Compared to the very recently published POOF features [1], unsupervised color alignments are 10% more accurate, while not requiring ground truth part annotations. [sent-324, score-0.929]
</p><p>95 Compared to the pose pooling kernels, unsupervised alignments recognize bird subcategories 84% more accurately. [sent-325, score-0.92]
</p><p>96 And compared to learned  features proposed in [12] unsupervised alignments perform 36. [sent-326, score-0.798]
</p><p>97 Also for Stanford Dogs we outperform the state-of-the-art, in spite of the larger shape and pose variation among the dogs compared to the birds, see Table 5. [sent-328, score-0.186]
</p><p>98 6 we plot pictures from four categories for which alignments reach high accuracy, i. [sent-340, score-0.748]
</p><p>99 We conclude that rough alignments lead to accurate fine-grained categorization. [sent-356, score-0.78]
</p><p>100 Improving the fisher kernel  [24]  [25]  [26] [27] [28] [29] [30] [3 1] [32] [33] [34]  for large-scale image classification. [sent-523, score-0.383]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('alignments', 0.676), ('fisher', 0.355), ('birds', 0.255), ('unsupervised', 0.122), ('parts', 0.118), ('encodings', 0.117), ('supervised', 0.115), ('categorization', 0.111), ('dogs', 0.11), ('locations', 0.104), ('mask', 0.094), ('bird', 0.083), ('beak', 0.078), ('hog', 0.077), ('vectors', 0.077), ('stanford', 0.074), ('branson', 0.07), ('descriptors', 0.067), ('ancillary', 0.066), ('part', 0.065), ('rough', 0.06), ('axis', 0.059), ('picture', 0.057), ('templates', 0.055), ('localized', 0.055), ('alignment', 0.055), ('principal', 0.053), ('bounding', 0.053), ('oracle', 0.051), ('distinctive', 0.051), ('chai', 0.05), ('axes', 0.048), ('wah', 0.046), ('reference', 0.046), ('shape', 0.046), ('pictures', 0.045), ('accurate', 0.044), ('grabcut', 0.044), ('aligning', 0.044), ('heermann', 0.044), ('shrikes', 0.044), ('extracting', 0.044), ('farrell', 0.044), ('finegrained', 0.044), ('precise', 0.043), ('supervision', 0.042), ('poof', 0.042), ('perona', 0.039), ('bobolink', 0.039), ('gull', 0.039), ('pooling', 0.039), ('ellipse', 0.037), ('box', 0.037), ('yao', 0.037), ('pyramid', 0.036), ('describing', 0.035), ('truth', 0.034), ('surrounds', 0.034), ('fgvc', 0.034), ('foreground', 0.034), ('object', 0.034), ('annotations', 0.033), ('localize', 0.033), ('ground', 0.032), ('appearance', 0.031), ('sanchez', 0.031), ('arriving', 0.031), ('arrive', 0.031), ('segmentation', 0.03), ('nuances', 0.03), ('nilsback', 0.03), ('amsterdam', 0.03), ('outperform', 0.03), ('consistent', 0.03), ('belong', 0.029), ('gravity', 0.029), ('training', 0.029), ('dog', 0.029), ('kernel', 0.028), ('wing', 0.028), ('parkhi', 0.028), ('predicted', 0.028), ('masks', 0.028), ('origin', 0.028), ('frame', 0.027), ('impression', 0.027), ('nameable', 0.027), ('plot', 0.027), ('classes', 0.026), ('user', 0.026), ('equally', 0.026), ('sift', 0.026), ('middle', 0.026), ('attributes', 0.026), ('interactive', 0.026), ('intrinsic', 0.025), ('parikh', 0.025), ('regions', 0.025), ('zisserman', 0.025), ('novelty', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="169-tfidf-1" href="./iccv-2013-Fine-Grained_Categorization_by_Alignments.html">169 iccv-2013-Fine-Grained Categorization by Alignments</a></p>
<p>Author: E. Gavves, B. Fernando, C.G.M. Snoek, A.W.M. Smeulders, T. Tuytelaars</p><p>Abstract: The aim of this paper is fine-grained categorization without human interaction. Different from prior work, which relies on detectors for specific object parts, we propose to localize distinctive details by roughly aligning the objects using just the overall shape, since implicit to fine-grained categorization is the existence of a super-class shape shared among all classes. The alignments are then used to transfer part annotations from training images to test images (supervised alignment), or to blindly yet consistently segment the object in a number of regions (unsupervised alignment). We furthermore argue that in the distinction of finegrained sub-categories, classification-oriented encodings like Fisher vectors are better suited for describing localized information than popular matching oriented features like HOG. We evaluate the method on the CU-2011 Birds and Stanford Dogs fine-grained datasets, outperforming the state-of-the-art.</p><p>2 0.21820959 <a title="169-tfidf-2" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>Author: Yuning Chai, Victor Lempitsky, Andrew Zisserman</p><p>Abstract: We propose a new method for the task of fine-grained visual categorization. The method builds a model of the baselevel category that can be fitted to images, producing highquality foreground segmentation and mid-level part localizations. The model can be learnt from the typical datasets available for fine-grained categorization, where the only annotation provided is a loose bounding box around the instance (e.g. bird) in each image. Both segmentation and part localizations are then used to encode the image content into a highly-discriminative visual signature. The model is symbiotic in that part discovery/localization is helped by segmentation and, conversely, the segmentation is helped by the detection (e.g. part layout). Our model builds on top of the part-based object category detector of Felzenszwalb et al., and also on the powerful GrabCut segmentation algorithm of Rother et al., and adds a simple spatial saliency coupling between them. In our evaluation, the model improves the categorization accuracy over the state-of-the-art. It also improves over what can be achieved with an analogous system that runs segmentation and part-localization independently.</p><p>3 0.19976221 <a title="169-tfidf-3" href="./iccv-2013-Coupling_Alignments_with_Recognition_for_Still-to-Video_Face_Recognition.html">97 iccv-2013-Coupling Alignments with Recognition for Still-to-Video Face Recognition</a></p>
<p>Author: Zhiwu Huang, Xiaowei Zhao, Shiguang Shan, Ruiping Wang, Xilin Chen</p><p>Abstract: The Still-to-Video (S2V) face recognition systems typically need to match faces in low-quality videos captured under unconstrained conditions against high quality still face images, which is very challenging because of noise, image blur, lowface resolutions, varying headpose, complex lighting, and alignment difficulty. To address the problem, one solution is to select the frames of ‘best quality ’ from videos (hereinafter called quality alignment in this paper). Meanwhile, the faces in the selected frames should also be geometrically aligned to the still faces offline well-aligned in the gallery. In this paper, we discover that the interactions among the three tasks–quality alignment, geometric alignment and face recognition–can benefit from each other, thus should be performed jointly. With this in mind, we propose a Coupling Alignments with Recognition (CAR) method to tightly couple these tasks via low-rank regularized sparse representation in a unified framework. Our method makes the three tasks promote mutually by a joint optimization in an Augmented Lagrange Multiplier routine. Extensive , experiments on two challenging S2V datasets demonstrate that our method outperforms the state-of-the-art methods impressively.</p><p>4 0.18899383 <a title="169-tfidf-4" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>Author: Ning Zhang, Ryan Farrell, Forrest Iandola, Trevor Darrell</p><p>Abstract: Recognizing objects in fine-grained domains can be extremely challenging due to the subtle differences between subcategories. Discriminative markings are often highly localized, leading traditional object recognition approaches to struggle with the large pose variation often present in these domains. Pose-normalization seeks to align training exemplars, either piecewise by part or globally for the whole object, effectively factoring out differences in pose and in viewing angle. Prior approaches relied on computationally-expensive filter ensembles for part localization and required extensive supervision. This paper proposes two pose-normalized descriptors based on computationally-efficient deformable part models. The first leverages the semantics inherent in strongly-supervised DPM parts. The second exploits weak semantic annotations to learn cross-component correspondences, computing pose-normalized descriptors from the latent parts of a weakly-supervised DPM. These representations enable pooling across pose and viewpoint, in turn facilitating tasks such as fine-grained recognition and attribute prediction. Experiments conducted on the Caltech-UCSD Birds 200 dataset and Berkeley Human Attribute dataset demonstrate significant improvements over state-of-art algorithms.</p><p>5 0.16814503 <a title="169-tfidf-5" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>Author: Ramazan Gokberk Cinbis, Jakob Verbeek, Cordelia Schmid</p><p>Abstract: We present an object detection system based on the Fisher vector (FV) image representation computed over SIFT and color descriptors. For computational and storage efficiency, we use a recent segmentation-based method to generate class-independent object detection hypotheses, in combination with data compression techniques. Our main contribution is a method to produce tentative object segmentation masks to suppress background clutter in the features. Re-weighting the local image features based on these masks is shown to improve object detection significantly. We also exploit contextual features in the form of a full-image FV descriptor, and an inter-category rescoring mechanism. Our experiments on the PASCAL VOC 2007 and 2010 datasets show that our detector improves over the current state-of-the-art detection results.</p><p>6 0.16265126 <a title="169-tfidf-6" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>7 0.14733946 <a title="169-tfidf-7" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>8 0.14669605 <a title="169-tfidf-8" href="./iccv-2013-Codemaps_-_Segment%2C_Classify_and_Search_Objects_Locally.html">77 iccv-2013-Codemaps - Segment, Classify and Search Objects Locally</a></p>
<p>9 0.14330173 <a title="169-tfidf-9" href="./iccv-2013-How_Do_You_Tell_a_Blackbird_from_a_Crow%3F.html">202 iccv-2013-How Do You Tell a Blackbird from a Crow?</a></p>
<p>10 0.14259429 <a title="169-tfidf-10" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>11 0.10118464 <a title="169-tfidf-11" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>12 0.10095645 <a title="169-tfidf-12" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>13 0.097732455 <a title="169-tfidf-13" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>14 0.09261097 <a title="169-tfidf-14" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>15 0.086506091 <a title="169-tfidf-15" href="./iccv-2013-Parsing_IKEA_Objects%3A_Fine_Pose_Estimation.html">308 iccv-2013-Parsing IKEA Objects: Fine Pose Estimation</a></p>
<p>16 0.086089253 <a title="169-tfidf-16" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>17 0.084584497 <a title="169-tfidf-17" href="./iccv-2013-Discriminatively_Trained_Templates_for_3D_Object_Detection%3A_A_Real_Time_Scalable_Approach.html">121 iccv-2013-Discriminatively Trained Templates for 3D Object Detection: A Real Time Scalable Approach</a></p>
<p>18 0.080718331 <a title="169-tfidf-18" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>19 0.079533383 <a title="169-tfidf-19" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>20 0.077103071 <a title="169-tfidf-20" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.196), (1, 0.042), (2, 0.015), (3, -0.052), (4, 0.102), (5, -0.023), (6, -0.036), (7, 0.025), (8, -0.041), (9, -0.04), (10, 0.083), (11, 0.074), (12, -0.022), (13, -0.125), (14, -0.084), (15, -0.021), (16, 0.047), (17, 0.036), (18, 0.071), (19, -0.058), (20, 0.076), (21, 0.014), (22, -0.022), (23, 0.051), (24, -0.007), (25, 0.158), (26, -0.047), (27, 0.017), (28, 0.031), (29, 0.04), (30, 0.123), (31, -0.136), (32, -0.049), (33, -0.088), (34, -0.1), (35, -0.011), (36, -0.025), (37, -0.099), (38, 0.038), (39, 0.019), (40, -0.095), (41, -0.022), (42, 0.046), (43, 0.019), (44, -0.071), (45, 0.013), (46, 0.012), (47, 0.009), (48, 0.06), (49, -0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92747682 <a title="169-lsi-1" href="./iccv-2013-Fine-Grained_Categorization_by_Alignments.html">169 iccv-2013-Fine-Grained Categorization by Alignments</a></p>
<p>Author: E. Gavves, B. Fernando, C.G.M. Snoek, A.W.M. Smeulders, T. Tuytelaars</p><p>Abstract: The aim of this paper is fine-grained categorization without human interaction. Different from prior work, which relies on detectors for specific object parts, we propose to localize distinctive details by roughly aligning the objects using just the overall shape, since implicit to fine-grained categorization is the existence of a super-class shape shared among all classes. The alignments are then used to transfer part annotations from training images to test images (supervised alignment), or to blindly yet consistently segment the object in a number of regions (unsupervised alignment). We furthermore argue that in the distinction of finegrained sub-categories, classification-oriented encodings like Fisher vectors are better suited for describing localized information than popular matching oriented features like HOG. We evaluate the method on the CU-2011 Birds and Stanford Dogs fine-grained datasets, outperforming the state-of-the-art.</p><p>2 0.8216325 <a title="169-lsi-2" href="./iccv-2013-How_Do_You_Tell_a_Blackbird_from_a_Crow%3F.html">202 iccv-2013-How Do You Tell a Blackbird from a Crow?</a></p>
<p>Author: Thomas Berg, Peter N. Belhumeur</p><p>Abstract: How do you tell a blackbirdfrom a crow? There has been great progress toward automatic methods for visual recognition, including fine-grained visual categorization in which the classes to be distinguished are very similar. In a task such as bird species recognition, automatic recognition systems can now exceed the performance of non-experts – most people are challenged to name a couple dozen bird species, let alone identify them. This leads us to the question, “Can a recognition system show humans what to look for when identifying classes (in this case birds)? ” In the context of fine-grained visual categorization, we show that we can automatically determine which classes are most visually similar, discover what visual features distinguish very similar classes, and illustrate the key features in a way meaningful to humans. Running these methods on a dataset of bird images, we can generate a visual field guide to birds which includes a tree of similarity that displays the similarity relations between all species, pages for each species showing the most similar other species, and pages for each pair of similar species illustrating their differences.</p><p>3 0.81855148 <a title="169-lsi-3" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>Author: Lingxi Xie, Qi Tian, Richang Hong, Shuicheng Yan, Bo Zhang</p><p>Abstract: As a special topic in computer vision, , fine-grained visual categorization (FGVC) has been attracting growing attention these years. Different with traditional image classification tasks in which objects have large inter-class variation, the visual concepts in the fine-grained datasets, such as hundreds of bird species, often have very similar semantics. Due to the large inter-class similarity, it is very difficult to classify the objects without locating really discriminative features, therefore it becomes more important for the algorithm to make full use of the part information in order to train a robust model. In this paper, we propose a powerful flowchart named Hierarchical Part Matching (HPM) to cope with finegrained classification tasks. We extend the Bag-of-Features (BoF) model by introducing several novel modules to integrate into image representation, including foreground inference and segmentation, Hierarchical Structure Learn- ing (HSL), and Geometric Phrase Pooling (GPP). We verify in experiments that our algorithm achieves the state-ofthe-art classification accuracy in the Caltech-UCSD-Birds200-2011 dataset by making full use of the ground-truth part annotations.</p><p>4 0.75758821 <a title="169-lsi-4" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>Author: Yuning Chai, Victor Lempitsky, Andrew Zisserman</p><p>Abstract: We propose a new method for the task of fine-grained visual categorization. The method builds a model of the baselevel category that can be fitted to images, producing highquality foreground segmentation and mid-level part localizations. The model can be learnt from the typical datasets available for fine-grained categorization, where the only annotation provided is a loose bounding box around the instance (e.g. bird) in each image. Both segmentation and part localizations are then used to encode the image content into a highly-discriminative visual signature. The model is symbiotic in that part discovery/localization is helped by segmentation and, conversely, the segmentation is helped by the detection (e.g. part layout). Our model builds on top of the part-based object category detector of Felzenszwalb et al., and also on the powerful GrabCut segmentation algorithm of Rother et al., and adds a simple spatial saliency coupling between them. In our evaluation, the model improves the categorization accuracy over the state-of-the-art. It also improves over what can be achieved with an analogous system that runs segmentation and part-localization independently.</p><p>5 0.75267935 <a title="169-lsi-5" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>Author: Ning Zhang, Ryan Farrell, Forrest Iandola, Trevor Darrell</p><p>Abstract: Recognizing objects in fine-grained domains can be extremely challenging due to the subtle differences between subcategories. Discriminative markings are often highly localized, leading traditional object recognition approaches to struggle with the large pose variation often present in these domains. Pose-normalization seeks to align training exemplars, either piecewise by part or globally for the whole object, effectively factoring out differences in pose and in viewing angle. Prior approaches relied on computationally-expensive filter ensembles for part localization and required extensive supervision. This paper proposes two pose-normalized descriptors based on computationally-efficient deformable part models. The first leverages the semantics inherent in strongly-supervised DPM parts. The second exploits weak semantic annotations to learn cross-component correspondences, computing pose-normalized descriptors from the latent parts of a weakly-supervised DPM. These representations enable pooling across pose and viewpoint, in turn facilitating tasks such as fine-grained recognition and attribute prediction. Experiments conducted on the Caltech-UCSD Birds 200 dataset and Berkeley Human Attribute dataset demonstrate significant improvements over state-of-art algorithms.</p><p>6 0.72887188 <a title="169-lsi-6" href="./iccv-2013-Codemaps_-_Segment%2C_Classify_and_Search_Objects_Locally.html">77 iccv-2013-Codemaps - Segment, Classify and Search Objects Locally</a></p>
<p>7 0.64527339 <a title="169-lsi-7" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>8 0.6390143 <a title="169-lsi-8" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>9 0.61530924 <a title="169-lsi-9" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>10 0.60829836 <a title="169-lsi-10" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>11 0.56755453 <a title="169-lsi-11" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<p>12 0.56613904 <a title="169-lsi-12" href="./iccv-2013-Detecting_Avocados_to_Zucchinis%3A_What_Have_We_Done%2C_and_Where_Are_We_Going%3F.html">109 iccv-2013-Detecting Avocados to Zucchinis: What Have We Done, and Where Are We Going?</a></p>
<p>13 0.55712742 <a title="169-lsi-13" href="./iccv-2013-Shufflets%3A_Shared_Mid-level_Parts_for_Fast_Object_Detection.html">390 iccv-2013-Shufflets: Shared Mid-level Parts for Fast Object Detection</a></p>
<p>14 0.55676275 <a title="169-lsi-14" href="./iccv-2013-Decomposing_Bag_of_Words_Histograms.html">104 iccv-2013-Decomposing Bag of Words Histograms</a></p>
<p>15 0.55582076 <a title="169-lsi-15" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>16 0.54790545 <a title="169-lsi-16" href="./iccv-2013-Nested_Shape_Descriptors.html">288 iccv-2013-Nested Shape Descriptors</a></p>
<p>17 0.53540283 <a title="169-lsi-17" href="./iccv-2013-A_Method_of_Perceptual-Based_Shape_Decomposition.html">21 iccv-2013-A Method of Perceptual-Based Shape Decomposition</a></p>
<p>18 0.53093433 <a title="169-lsi-18" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>19 0.52964848 <a title="169-lsi-19" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>20 0.52131343 <a title="169-lsi-20" href="./iccv-2013-An_Adaptive_Descriptor_Design_for_Object_Recognition_in_the_Wild.html">48 iccv-2013-An Adaptive Descriptor Design for Object Recognition in the Wild</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.081), (4, 0.014), (7, 0.013), (13, 0.012), (26, 0.115), (31, 0.031), (34, 0.079), (40, 0.028), (42, 0.108), (48, 0.011), (64, 0.049), (66, 0.096), (73, 0.027), (77, 0.017), (89, 0.222)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93588805 <a title="169-lda-1" href="./iccv-2013-Fine-Grained_Categorization_by_Alignments.html">169 iccv-2013-Fine-Grained Categorization by Alignments</a></p>
<p>Author: E. Gavves, B. Fernando, C.G.M. Snoek, A.W.M. Smeulders, T. Tuytelaars</p><p>Abstract: The aim of this paper is fine-grained categorization without human interaction. Different from prior work, which relies on detectors for specific object parts, we propose to localize distinctive details by roughly aligning the objects using just the overall shape, since implicit to fine-grained categorization is the existence of a super-class shape shared among all classes. The alignments are then used to transfer part annotations from training images to test images (supervised alignment), or to blindly yet consistently segment the object in a number of regions (unsupervised alignment). We furthermore argue that in the distinction of finegrained sub-categories, classification-oriented encodings like Fisher vectors are better suited for describing localized information than popular matching oriented features like HOG. We evaluate the method on the CU-2011 Birds and Stanford Dogs fine-grained datasets, outperforming the state-of-the-art.</p><p>2 0.92477345 <a title="169-lda-2" href="./iccv-2013-How_Do_You_Tell_a_Blackbird_from_a_Crow%3F.html">202 iccv-2013-How Do You Tell a Blackbird from a Crow?</a></p>
<p>Author: Thomas Berg, Peter N. Belhumeur</p><p>Abstract: How do you tell a blackbirdfrom a crow? There has been great progress toward automatic methods for visual recognition, including fine-grained visual categorization in which the classes to be distinguished are very similar. In a task such as bird species recognition, automatic recognition systems can now exceed the performance of non-experts – most people are challenged to name a couple dozen bird species, let alone identify them. This leads us to the question, “Can a recognition system show humans what to look for when identifying classes (in this case birds)? ” In the context of fine-grained visual categorization, we show that we can automatically determine which classes are most visually similar, discover what visual features distinguish very similar classes, and illustrate the key features in a way meaningful to humans. Running these methods on a dataset of bird images, we can generate a visual field guide to birds which includes a tree of similarity that displays the similarity relations between all species, pages for each species showing the most similar other species, and pages for each pair of similar species illustrating their differences.</p><p>3 0.92307401 <a title="169-lda-3" href="./iccv-2013-Multi-scale_Topological_Features_for_Hand_Posture_Representation_and_Analysis.html">278 iccv-2013-Multi-scale Topological Features for Hand Posture Representation and Analysis</a></p>
<p>Author: Kaoning Hu, Lijun Yin</p><p>Abstract: In this paper, we propose a multi-scale topological feature representation for automatic analysis of hand posture. Such topological features have the advantage of being posture-dependent while being preserved under certain variations of illumination, rotation, personal dependency, etc. Our method studies the topology of the holes between the hand region and its convex hull. Inspired by the principle of Persistent Homology, which is the theory of computational topology for topological feature analysis over multiple scales, we construct the multi-scale Betti Numbers matrix (MSBNM) for the topological feature representation. In our experiments, we used 12 different hand postures and compared our features with three popular features (HOG, MCT, and Shape Context) on different data sets. In addition to hand postures, we also extend the feature representations to arm postures. The results demonstrate the feasibility and reliability of the proposed method.</p><p>4 0.92206997 <a title="169-lda-4" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>Author: Xiaoyang Wang, Qiang Ji</p><p>Abstract: This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. As a list of semantically meaningful properties of objects, attributes generally relate to each other statistically. In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and objectindependent attribute relationships. The model utilizes the captured relationships to benefit both attribute prediction and object recognition. Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases.</p><p>5 0.92180663 <a title="169-lda-5" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>Author: Ning Zhang, Ryan Farrell, Forrest Iandola, Trevor Darrell</p><p>Abstract: Recognizing objects in fine-grained domains can be extremely challenging due to the subtle differences between subcategories. Discriminative markings are often highly localized, leading traditional object recognition approaches to struggle with the large pose variation often present in these domains. Pose-normalization seeks to align training exemplars, either piecewise by part or globally for the whole object, effectively factoring out differences in pose and in viewing angle. Prior approaches relied on computationally-expensive filter ensembles for part localization and required extensive supervision. This paper proposes two pose-normalized descriptors based on computationally-efficient deformable part models. The first leverages the semantics inherent in strongly-supervised DPM parts. The second exploits weak semantic annotations to learn cross-component correspondences, computing pose-normalized descriptors from the latent parts of a weakly-supervised DPM. These representations enable pooling across pose and viewpoint, in turn facilitating tasks such as fine-grained recognition and attribute prediction. Experiments conducted on the Caltech-UCSD Birds 200 dataset and Berkeley Human Attribute dataset demonstrate significant improvements over state-of-art algorithms.</p><p>6 0.91852558 <a title="169-lda-6" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>7 0.91684449 <a title="169-lda-7" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>8 0.91640949 <a title="169-lda-8" href="./iccv-2013-Box_in_the_Box%3A_Joint_3D_Layout_and_Object_Reasoning_from_Single_Images.html">64 iccv-2013-Box in the Box: Joint 3D Layout and Object Reasoning from Single Images</a></p>
<p>9 0.9103477 <a title="169-lda-9" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>10 0.90827322 <a title="169-lda-10" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>11 0.90821373 <a title="169-lda-11" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>12 0.90756691 <a title="169-lda-12" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>13 0.90618598 <a title="169-lda-13" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>14 0.90423292 <a title="169-lda-14" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>15 0.90400481 <a title="169-lda-15" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>16 0.90346396 <a title="169-lda-16" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>17 0.90277493 <a title="169-lda-17" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>18 0.90276521 <a title="169-lda-18" href="./iccv-2013-Detecting_Curved_Symmetric_Parts_Using_a_Deformable_Disc_Model.html">110 iccv-2013-Detecting Curved Symmetric Parts Using a Deformable Disc Model</a></p>
<p>19 0.90244889 <a title="169-lda-19" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>20 0.90229225 <a title="169-lda-20" href="./iccv-2013-A_Convex_Optimization_Framework_for_Active_Learning.html">6 iccv-2013-A Convex Optimization Framework for Active Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
