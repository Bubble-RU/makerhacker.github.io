<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-445" href="#">iccv2013-445</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</h1>
<br/><p>Source: <a title="iccv-2013-445-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Deng_Visual_Reranking_through_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Cheng Deng, Rongrong Ji, Wei Liu, Dacheng Tao, Xinbo Gao</p><p>Abstract: Visual reranking has been widely deployed to refine the quality of conventional content-based image retrieval engines. The current trend lies in employing a crowd of retrieved results stemming from multiple feature modalities to boost the overall performance of visual reranking. However, a major challenge pertaining to current reranking methods is how to take full advantage of the complementary property of distinct feature modalities. Given a query image and one feature modality, a regular visual reranking framework treats the top-ranked images as pseudo positive instances which are inevitably noisy, difficult to reveal this complementary property, and thus lead to inferior ranking performance. This paper proposes a novel image reranking approach by introducing a Co-Regularized Multi-Graph Learning (Co-RMGL) framework, in which the intra-graph and inter-graph constraints are simultaneously imposed to encode affinities in a single graph and consistency across different graphs. Moreover, weakly supervised learning driven by image attributes is performed to denoise the pseudo- labeled instances, thereby highlighting the unique strength of individual feature modality. Meanwhile, such learning can yield a few anchors in graphs that vitally enable the alignment and fusion of multiple graphs. As a result, an edge weight matrix learned from the fused graph automatically gives the ordering to the initially retrieved results. We evaluate our approach on four benchmark image retrieval datasets, demonstrating a significant performance gain over the state-of-the-arts.</p><p>Reference: <a title="iccv-2013-445-reference" href="../iccv2013_reference/iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract Visual reranking has been widely deployed to refine the quality of conventional content-based image retrieval engines. [sent-6, score-0.58]
</p><p>2 The current trend lies in employing a crowd of retrieved results stemming from multiple feature modalities to boost the overall performance of visual reranking. [sent-7, score-0.194]
</p><p>3 However, a major challenge pertaining to current reranking methods is how to take full advantage of the complementary property of distinct feature modalities. [sent-8, score-0.516]
</p><p>4 Given a query image and one feature modality, a regular visual reranking framework treats the top-ranked images as pseudo positive instances which are inevitably noisy, difficult to reveal this complementary property, and thus lead to inferior ranking performance. [sent-9, score-1.045]
</p><p>5 This paper proposes a novel image reranking approach by introducing a Co-Regularized Multi-Graph Learning (Co-RMGL) framework, in which the intra-graph and inter-graph constraints are simultaneously imposed to encode affinities in a single graph and consistency across different graphs. [sent-10, score-0.598]
</p><p>6 Moreover, weakly supervised learning driven by image attributes is performed to denoise the pseudo-  labeled instances, thereby highlighting the unique strength of individual feature modality. [sent-11, score-0.717]
</p><p>7 Meanwhile, such learning can yield a few anchors in graphs that vitally enable the alignment and fusion of multiple graphs. [sent-12, score-0.707]
</p><p>8 As a result, an edge weight matrix learned from the fused graph automatically gives the ordering to the initially retrieved results. [sent-13, score-0.263]
</p><p>9 , refining an image ranking list initially returned by a textual or visual query, has been intensively studied in image search engines and beyond [1]. [sent-18, score-0.166]
</p><p>10 Under such a circumstance, the initial ranking list is reordered  (a) (b) Figure 1. [sent-19, score-0.164]
</p><p>11 The user gap: given the positive labels as shown in light blue rectangles, how can the machine interpret the user’s labeling intention? [sent-20, score-0.193]
</p><p>12 by exploiting some inherent visual similarity measure, typically accomplished through learning a refined ranking function from accurately or noisily labeled instances. [sent-21, score-0.214]
</p><p>13 Such labeled instances are gathered via relevance judgments with respect to the query, and these judgments can be determined by an automatic scheme, e. [sent-22, score-0.274]
</p><p>14 top-ranked images as pseudo positive (relevant) instances [2], or a manual scheme, e. [sent-24, score-0.258]
</p><p>15 positive instances specified by a user [3] or click data [4]. [sent-26, score-0.261]
</p><p>16 To make relevance judgements as sufficient as possible, various methods have been investigated, ranging from a straightforward means like query expansion [5][6] to sophisticated skills such as recently developed query-relative classifier learning [7] and irrelevant image elimination [8]. [sent-27, score-0.285]
</p><p>17 On the other hand, a successful reranking also relies on the credibility of involved labeled instances. [sent-30, score-0.521]
</p><p>18 For example, taking the top-ranked images as pseudo positive instances tends to be unreliable because there may exist false positive samples (a. [sent-32, score-0.294]
</p><p>19 [2] assigned pseudo positive labels to a few top-ranked images, and then selected sparse smooth eigenbases of the normalized graph Laplacian, that is built on the working set of higher ranked images, to filter out the outliers and hence achieve the reliable positive labels. [sent-37, score-0.282]
</p><p>20 Nevertheless, even provided with sufficient positive instances for labeling, how to discover the user’s search intention remains open, which is referred to as the “user gap” issue [6]. [sent-39, score-0.261]
</p><p>21 Instead, a higher-level mining to discover the underlying semantic attributes of images, if feasible, could help a lot. [sent-43, score-0.268]
</p><p>22 It turns out that such a semantic mining step is specially valuable for enhancing the reranking performance. [sent-44, score-0.586]
</p><p>23 To this end, we propose an image attribute driven learning framework, namely Weakly Supervised Multi-Graph Learning, to  address visual reranking. [sent-45, score-0.24]
</p><p>24 In this scenario, the (pseudo) labeled instances are not directly used as seed labels for reranking, but undergo a selective procedure like the scenario of weakly supervised learning [10][1 1]. [sent-46, score-0.642]
</p><p>25 The mined image attributes are subsequently leveraged to learn a refined ranking function by applying proper off-the-shelf learning algorithms such as multi-view learning and graph-based semisupervised learning. [sent-47, score-0.407]
</p><p>26 We follow a state-of-the-art method [9] for multi-feature fusion based visual reranking, upon which we conduct graph-based learning rather than straightforward feature learning, aiming at capturing the intrinsic manifold structure underlying the images to be reranked. [sent-48, score-0.266]
</p><p>27 In our proposed framework, multiple retrieved image sets stemming from different modalities of visual features are expressed into multiple graphs, which are aligned and then fused towards learning an optimal similarity metric across multiple graphs for sensible reranking. [sent-49, score-0.345]
</p><p>28 The weakly supervised learning driven by image attributes can yield critical graph anchors within each graph, which enable the effective alignment and fusion across multiple graphs. [sent-50, score-1.328]
</p><p>29 , Oxford, Paris, INRIA Holidays and UKBench, bear out that the proposed reranking approach outperforms the state-ofthe-arts [9][12][13] by a significant margin in terms of ro-  Figure 2. [sent-53, score-0.516]
</p><p>30 Section 3 presents the proposed visual reranking framework based on multi-graph learning. [sent-58, score-0.519]
</p><p>31 Section 4 describes the graph anchor seeking procedure using weakly supervised learning via co-occurred attribute mining. [sent-59, score-0.991]
</p><p>32 Graph-based image reranking methods have shown promising performance recently. [sent-63, score-0.488]
</p><p>33 It targets at refining the initial ranking list by propagating the initial rank scores of seed (or anchor) nodes to the other nodes in a graph [2][8]. [sent-64, score-0.303]
</p><p>34 In [14], the video reranking process  was modeled as a modified PageRank over a set of graphs to propagate the final ranking scores. [sent-65, score-0.677]
</p><p>35 [9] proposed a graph-based query specific fusion approach, where multiple retrieval sets from different visual cues were merged and reranked by link analysis on a fused graph. [sent-69, score-0.469]
</p><p>36 In [17], semantic attributes and non-semantic attributes were learned for recognizing objects within categories and across categories. [sent-72, score-0.311]
</p><p>37 Moreover, attributes can be used as mid-level features for scene recognition [19], face recognition [20], and image retrieval [21]. [sent-73, score-0.2]
</p><p>38 There also exist some recent endeavors aiming to discover attributes interactively [22] or from noisy web-crawled data [23]. [sent-74, score-0.199]
</p><p>39 Weakly supervised learning methods have been extensively studied in the recent literature. [sent-76, score-0.246]
</p><p>40 For instance, [10] unified weakly supervised learning into undirected graphical models for object recognition; [11] learned object categories in a weakly supervised manner for object recognition. [sent-77, score-0.985]
</p><p>41 In [24], weakly supervised information was integrated with latent SVMs to conduct object localization. [sent-78, score-0.434]
</p><p>42 In [25], accurate semantic segmentation with a multi-image model was achieved by supplementing weakly supervised classes to training images. [sent-79, score-0.471]
</p><p>43 Given a query image Iq and its initial topN ranking list I {Ii}iN=1 . [sent-82, score-0.307]
</p><p>44 is edge weight m(i,ajtr)ix}  wm  where each wimj represents ∈th Re edge weight over (i, j) to be learned. [sent-92, score-0.393]
</p><p>45 from wm will be used as the final scores to refine the initi? [sent-94, score-0.223]
</p><p>46 r vector to label all images, w =he {rez zi = 1 means that the image Ii is a graph anchor and zi = 0 otherwise, with anchor feature set X? [sent-97, score-0.658]
</p><p>47 ation to describe the procedwuhreer oef “ graph tahnech inodri csaelteocrt oiopne, aAt ioisn nth teo a dnecshcroirb neu tmhebe prr, oacnedxˆm is the anchor feature vector for the m-th feature channel. [sent-104, score-0.398]
</p><p>48 Second, we introduce inter-graph constraints where the  pairwise distribution between pairs of anchors and nonanchors across graphs should behave consistently. [sent-109, score-0.411]
</p><p>49 CoRMGL can be interpreted as a multiple graphs fusion algorithm via graph anchor alignment, as described in Eq. [sent-111, score-0.588]
</p><p>50 Given a set of graph anchor 1 for the m-th feature channel, the intra-graph learning aims to obtain a new edge weight matrix to minimize the distances between the query and the anchors, as well as between pairwise anchors. [sent-113, score-0.641]
</p><p>51 Here, because the whole graph can be approximated as a set of overlapped linear neighborhood patches, we instead exploit the locally linear reconstruction (LLR) method (like [26]) to describe the distance constraints encoded into the weight matrix. [sent-114, score-0.152]
</p><p>52 This results in the following objective function for intra-graph learning:  wm  ? [sent-115, score-0.194]
</p><p>53 a given anchor xˆim using other anchors xˆjm (j i), and the second term is the reconstruction error of t(hje ? [sent-137, score-0.551]
</p><p>54 =  1The selection of anchors will be detailed later in Section 4. [sent-139, score-0.353]
</p><p>55 As far as we know, this is the first time to conduct multi-graph alignment with graph anchors. [sent-215, score-0.171]
</p><p>56 For a better alignment, as shown in Figure 2, the common anchors across multiple graphs are retained as many as possible, while uncommon ones are omitted. [sent-216, score-0.411]
</p><p>57 Our formulation is general enough to unify several existing graph fusion techniques developed for reranking and beyond [2][9]. [sent-217, score-0.752]
</p><p>58 Graph anchors are first introduced for graphbased SSL in [29] where K-Means clustering centers are used for graph anchors. [sent-221, score-0.429]
</p><p>59 While the most straightforward approach is to treat the pseudo labeled instances as anchors, 2602  Algorithm 1: Co-RMGL for Multi-Graph Learning. [sent-222, score-0.255]
</p><p>60 Our solution is to discover the intrinsic attributes among the labeled instances, upon which we seek a better anchor set. [sent-235, score-0.435]
</p><p>61 This is accomplished by mining discriminative attributes from all attribute vectors of the initially retrieved results, via the cutting-edge image descrip-  tors like Classemes [30] or ObjectBank [3 1]. [sent-236, score-0.366]
</p><p>62 The mined attributes are then utilized to select top-ranked images with the maximum responses as the target anchors. [sent-237, score-0.244]
</p><p>63 We introduce an effective yet efficient attributes discovery scheme based on Aprior [32] over the attribute vectors detected from all retrieved results, which, as shown in our subsequent experiments, has superior performance over the straightforward attribute vectors intersection scheme. [sent-239, score-0.469]
</p><p>64 Formally speaking, we use Classemes to derive the middle-level attribute vector set A from the initial retrieval smetid d I. [sent-240, score-0.216]
</p><p>65 atheadv insgcmri axinmautimverve spcto nrsCes 9 10  end Output: graph anchor set  ? [sent-260, score-0.342]
</p><p>66 Given the mined attributes, we then select A images with the maximum responses as the anchors for graph alignment and fusion in Section 3. [sent-273, score-0.751]
</p><p>67 Intuitively, we utilize the co-occurred attributes to generate an associated discriminative vector, with which the images having the maximum responses are found as the anchors. [sent-274, score-0.177]
</p><p>68 Algorithm 2 gives a procedure, namely Weakly Supervised Anchor Seeking (WSAS), to yield the desirable anchor set. [sent-279, score-0.26]
</p><p>69 Note that both Classemes based attribute description and its transaction set are done offline. [sent-281, score-0.162]
</p><p>70 100,000 and 1M images randomly downloaded from Flickr are respectively added as distractors to form the Oxford105k and INRIA 1M, which test the performance of our reranking approach. [sent-291, score-0.488]
</p><p>71 Following the state-of-the-art setting in multi-feature fusion based reranking, we design the following feature channels. [sent-299, score-0.182]
</p><p>72 mAP on Oxford5k with different numbers of anchors A. [sent-307, score-0.319]
</p><p>73 (2) supervised manner: the manually selected L images from the initial ranking list as labeling instances. [sent-315, score-0.443]
</p><p>74 For both case, we run our weakly supervised anchor learning as in Algorithm 2 to come up with an extended and purified label set, i. [sent-316, score-0.719]
</p><p>75 In our method, the top-N dataset candidates for the query image Iq are considered to evaluate reranking performance. [sent-320, score-0.631]
</p><p>76 when N becomes larger, the mAP of each feature channel and fusion continues to decrease. [sent-326, score-0.237]
</p><p>77 With N increasing from 20 to 200, the mAP of fusion drops from 0. [sent-327, score-0.186]
</p><p>78 The mAP of direct fusion is lower than the one of GIST-based reranking since the complementary properties of different feature channels are not exploited. [sent-330, score-0.716]
</p><p>79 The number of anchors A is directly related to the accuracy and scalability of our scheme. [sent-335, score-0.319]
</p><p>80 6747 varies with A on Oxford5k under the case of supervised labeling selection when N = 200. [sent-364, score-0.313]
</p><p>81 Figure 5 and Figure 6 together further compare the performance of unsupervised and supervised labeling instances selections on four datasets when the number of labeling instances L = 30. [sent-372, score-0.67]
</p><p>82 We find that both labels selection criteria achieve relatively good performance with either unsupervised or supervised, which demonstrates that our method is generalized and compatible for different labeling instances selection schemes. [sent-373, score-0.31]
</p><p>83 In addition, the mAP of supervised criteria is improved by nearly 2% over the unsupervised one. [sent-374, score-0.227]
</p><p>84 In weakly supervised attribute learning, there are two methods to select anchors. [sent-375, score-0.558]
</p><p>85 The other is selection of top-ranked images with maximum responses as anchors via attribute mining (AM), named Co-RMGL+AM. [sent-377, score-0.578]
</p><p>86 We also verify “Baseline III+Classemes” which directly use Classemes to align graphs on Baseline III without attributebased anchor selection. [sent-378, score-0.324]
</p><p>87 We further compare our LLR based metric with unsupervised distance metric learning (UDML) for the stage of intra-graph learning, the latter of which learns similarity metrics in individual feature channels, potentially with a fusion operation to achieve reranking. [sent-402, score-0.297]
</p><p>88 Figure 8 shows some visualized results of our Co-RGML+AM reranking on Oxford5k, INRIA and Paris respectively, with comparisons with Baseline III and 1We  pre-compute  and  store  all of these features offline. [sent-421, score-0.523]
</p><p>89 Comparisons mAP of four datasets under “unsupervised labeling instances selection” when we set A = 10, in which each group respects different stages of our approach, such as Baseline III, Baseline IV, graph alignment and anchor learning (Co-RMGL+AI). [sent-423, score-0.72]
</p><p>90 Comparisons mAP of four datasets under “supervised labeling instances selection” when we set A = 10, in which each group respects different stages of our approach, such as Baseline III, Baseline IV, graph alignment and anchor learning (Co-RMGL+AI). [sent-425, score-0.72]
</p><p>91 For example, for the query “hertford” on Oxford5k, our scheme can rank relevance images into top 9, because it can automatically beling instances  refine the initial la-  and extend the mined relevance  to other  images that can not be recognized by previous methods. [sent-429, score-0.528]
</p><p>92 Conclusion In this paper we propose a novel visual reranking approach through performing weakly supervised multi-graph learning. [sent-431, score-0.953]
</p><p>93 Chang Noise resistant graph ranking for improved web image search. [sent-449, score-0.25]
</p><p>94 Learning to re-rank: Query-dependent image reranking using click data. [sent-462, score-0.52]
</p><p>95 The last column lists some important attributes mined by our proposed WSAS). [sent-504, score-0.204]
</p><p>96 Weakly supervised learning of part-based spatial models for visual object recognition. [sent-508, score-0.277]
</p><p>97 Weakly supervised scale-invariant learning of models for visual recognition. [sent-514, score-0.277]
</p><p>98 Schmid Combining attributes ans Fisher vectors for efficient image retrieval CVPR, 2011. [sent-583, score-0.2]
</p><p>99 Automatic attribute discovery and characterization from noisy web data. [sent-593, score-0.167]
</p><p>100 Scene recognition and weakly supervised object localization with deformable part-based models. [sent-598, score-0.434]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reranking', 0.488), ('anchors', 0.319), ('weakly', 0.241), ('anchor', 0.232), ('wm', 0.194), ('supervised', 0.193), ('fusion', 0.154), ('query', 0.143), ('inria', 0.142), ('attributes', 0.137), ('attribute', 0.124), ('instances', 0.122), ('ukbench', 0.121), ('graph', 0.11), ('iii', 0.105), ('classemes', 0.101), ('pseudo', 0.1), ('ranking', 0.097), ('graphs', 0.092), ('labeling', 0.086), ('cormgl', 0.074), ('paris', 0.073), ('baseline', 0.072), ('user', 0.071), ('intention', 0.07), ('mined', 0.067), ('skm', 0.065), ('llr', 0.065), ('retrieval', 0.063), ('alignment', 0.061), ('mining', 0.061), ('oxford', 0.06), ('modalities', 0.059), ('xim', 0.057), ('channel', 0.055), ('learning', 0.053), ('chum', 0.05), ('relevance', 0.049), ('aprior', 0.049), ('maxres', 0.049), ('rongrong', 0.049), ('udml', 0.049), ('wimj', 0.049), ('wsas', 0.049), ('iv', 0.047), ('channels', 0.046), ('ai', 0.044), ('retrieved', 0.044), ('reranked', 0.044), ('mikul', 0.044), ('dacheng', 0.044), ('web', 0.043), ('weight', 0.042), ('scheme', 0.04), ('qc', 0.04), ('classeme', 0.04), ('judgements', 0.04), ('holidays', 0.04), ('responses', 0.04), ('transaction', 0.038), ('list', 0.038), ('seeking', 0.038), ('semantic', 0.037), ('manner', 0.037), ('perd', 0.036), ('positive', 0.036), ('comparisons', 0.035), ('judgments', 0.035), ('och', 0.035), ('selection', 0.034), ('unsupervised', 0.034), ('fused', 0.034), ('discover', 0.033), ('edge', 0.033), ('labeled', 0.033), ('stemming', 0.032), ('hua', 0.032), ('drops', 0.032), ('driven', 0.032), ('xq', 0.032), ('click', 0.032), ('map', 0.031), ('visual', 0.031), ('vm', 0.031), ('initial', 0.029), ('interactively', 0.029), ('hsv', 0.029), ('refine', 0.029), ('respects', 0.029), ('jm', 0.029), ('operation', 0.028), ('yield', 0.028), ('bear', 0.028), ('feature', 0.028), ('zi', 0.028), ('iq', 0.027), ('undirected', 0.027), ('flower', 0.027), ('philbin', 0.027), ('datasets', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="445-tfidf-1" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>Author: Cheng Deng, Rongrong Ji, Wei Liu, Dacheng Tao, Xinbo Gao</p><p>Abstract: Visual reranking has been widely deployed to refine the quality of conventional content-based image retrieval engines. The current trend lies in employing a crowd of retrieved results stemming from multiple feature modalities to boost the overall performance of visual reranking. However, a major challenge pertaining to current reranking methods is how to take full advantage of the complementary property of distinct feature modalities. Given a query image and one feature modality, a regular visual reranking framework treats the top-ranked images as pseudo positive instances which are inevitably noisy, difficult to reveal this complementary property, and thus lead to inferior ranking performance. This paper proposes a novel image reranking approach by introducing a Co-Regularized Multi-Graph Learning (Co-RMGL) framework, in which the intra-graph and inter-graph constraints are simultaneously imposed to encode affinities in a single graph and consistency across different graphs. Moreover, weakly supervised learning driven by image attributes is performed to denoise the pseudo- labeled instances, thereby highlighting the unique strength of individual feature modality. Meanwhile, such learning can yield a few anchors in graphs that vitally enable the alignment and fusion of multiple graphs. As a result, an edge weight matrix learned from the fused graph automatically gives the ordering to the initially retrieved results. We evaluate our approach on four benchmark image retrieval datasets, demonstrating a significant performance gain over the state-of-the-arts.</p><p>2 0.23097351 <a title="445-tfidf-2" href="./iccv-2013-Shape_Anchors_for_Data-Driven_Multi-view_Reconstruction.html">387 iccv-2013-Shape Anchors for Data-Driven Multi-view Reconstruction</a></p>
<p>Author: Andrew Owens, Jianxiong Xiao, Antonio Torralba, William Freeman</p><p>Abstract: We present a data-driven method for building dense 3D reconstructions using a combination of recognition and multi-view cues. Our approach is based on the idea that there are image patches that are so distinctive that we can accurately estimate their latent 3D shapes solely using recognition. We call these patches shape anchors, and we use them as the basis of a multi-view reconstruction system that transfers dense, complex geometry between scenes. We “anchor” our 3D interpretation from these patches, using them to predict geometry for parts of the scene that are relatively ambiguous. The resulting algorithm produces dense reconstructions from stereo point clouds that are sparse and noisy, and we demonstrate it on a challenging dataset of real-world, indoor scenes.</p><p>3 0.17924701 <a title="445-tfidf-3" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>Author: Adriana Kovashka, Kristen Grauman</p><p>Abstract: Current methods learn monolithic attribute predictors, with the assumption that a single model is sufficient to reflect human understanding of a visual attribute. However, in reality, humans vary in how they perceive the association between a named property and image content. For example, two people may have slightly different internal models for what makes a shoe look “formal”, or they may disagree on which of two scenes looks “more cluttered”. Rather than discount these differences as noise, we propose to learn user-specific attribute models. We adapt a generic model trained with annotations from multiple users, tailoring it to satisfy user-specific labels. Furthermore, we propose novel techniques to infer user-specific labels based on transitivity and contradictions in the user’s search history. We demonstrate that adapted attributes improve accuracy over both existing monolithic models as well as models that learn from scratch with user-specific data alone. In addition, we show how adapted attributes are useful to personalize image search, whether with binary or relative attributes.</p><p>4 0.17312066 <a title="445-tfidf-4" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>Author: Shiliang Zhang, Ming Yang, Xiaoyu Wang, Yuanqing Lin, Qi Tian</p><p>Abstract: Inverted indexes in image retrieval not only allow fast access to database images but also summarize all knowledge about the database, so that their discriminative capacity largely determines the retrieval performance. In this paper, for vocabulary tree based image retrieval, we propose a semantic-aware co-indexing algorithm to jointly San Antonio, TX 78249 . j dl@gmai l com qit ian@cs .ut sa . edu . The query embed two strong cues into the inverted indexes: 1) local invariant features that are robust to delineate low-level image contents, and 2) semantic attributes from large-scale object recognition that may reveal image semantic meanings. For an initial set of inverted indexes of local features, we utilize 1000 semantic attributes to filter out isolated images and insert semantically similar images to the initial set. Encoding these two distinct cues together effectively enhances the discriminative capability of inverted indexes. Such co-indexing operations are totally off-line and introduce small computation overhead to online query cause only local features but no semantic attributes are used for query. Experiments and comparisons with recent retrieval methods on 3 datasets, i.e., UKbench, Holidays, Oxford5K, and 1.3 million images from Flickr as distractors, manifest the competitive performance of our method 1.</p><p>5 0.16906191 <a title="445-tfidf-5" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>Author: Xiaoyang Wang, Qiang Ji</p><p>Abstract: This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. As a list of semantically meaningful properties of objects, attributes generally relate to each other statistically. In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and objectindependent attribute relationships. The model utilizes the captured relationships to benefit both attribute prediction and object recognition. Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases.</p><p>6 0.1652451 <a title="445-tfidf-6" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>7 0.1604052 <a title="445-tfidf-7" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>8 0.14796761 <a title="445-tfidf-8" href="./iccv-2013-Learning_Graph_Matching%3A_Oriented_to_Category_Modeling_from_Cluttered_Scenes.html">237 iccv-2013-Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes</a></p>
<p>9 0.14184566 <a title="445-tfidf-9" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>10 0.1275969 <a title="445-tfidf-10" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>11 0.12200733 <a title="445-tfidf-11" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>12 0.11987584 <a title="445-tfidf-12" href="./iccv-2013-Bayesian_Joint_Topic_Modelling_for_Weakly_Supervised_Object_Localisation.html">59 iccv-2013-Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation</a></p>
<p>13 0.11472183 <a title="445-tfidf-13" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>14 0.1095712 <a title="445-tfidf-14" href="./iccv-2013-Video_Motion_for_Every_Visible_Point.html">441 iccv-2013-Video Motion for Every Visible Point</a></p>
<p>15 0.10850823 <a title="445-tfidf-15" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>16 0.10319988 <a title="445-tfidf-16" href="./iccv-2013-Feature_Weighting_via_Optimal_Thresholding_for_Video_Analysis.html">163 iccv-2013-Feature Weighting via Optimal Thresholding for Video Analysis</a></p>
<p>17 0.09881749 <a title="445-tfidf-17" href="./iccv-2013-Query-Adaptive_Asymmetrical_Dissimilarities_for_Visual_Object_Retrieval.html">334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</a></p>
<p>18 0.09432441 <a title="445-tfidf-18" href="./iccv-2013-3D_Sub-query_Expansion_for_Improving_Sketch-Based_Multi-view_Image_Retrieval.html">3 iccv-2013-3D Sub-query Expansion for Improving Sketch-Based Multi-view Image Retrieval</a></p>
<p>19 0.093872502 <a title="445-tfidf-19" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>20 0.092454538 <a title="445-tfidf-20" href="./iccv-2013-Implied_Feedback%3A_Learning_Nuances_of_User_Behavior_in_Image_Search.html">213 iccv-2013-Implied Feedback: Learning Nuances of User Behavior in Image Search</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.21), (1, 0.114), (2, -0.069), (3, -0.147), (4, 0.103), (5, 0.099), (6, -0.052), (7, -0.113), (8, 0.06), (9, 0.083), (10, 0.002), (11, 0.035), (12, 0.061), (13, 0.067), (14, 0.019), (15, 0.009), (16, 0.057), (17, -0.066), (18, 0.05), (19, -0.04), (20, -0.046), (21, 0.003), (22, -0.065), (23, -0.028), (24, 0.06), (25, 0.05), (26, -0.001), (27, 0.002), (28, 0.027), (29, 0.055), (30, -0.02), (31, 0.04), (32, 0.057), (33, -0.013), (34, 0.026), (35, 0.02), (36, -0.025), (37, 0.017), (38, 0.034), (39, -0.018), (40, -0.028), (41, -0.13), (42, -0.025), (43, 0.017), (44, -0.028), (45, -0.051), (46, 0.019), (47, 0.008), (48, 0.036), (49, -0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94149566 <a title="445-lsi-1" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>Author: Cheng Deng, Rongrong Ji, Wei Liu, Dacheng Tao, Xinbo Gao</p><p>Abstract: Visual reranking has been widely deployed to refine the quality of conventional content-based image retrieval engines. The current trend lies in employing a crowd of retrieved results stemming from multiple feature modalities to boost the overall performance of visual reranking. However, a major challenge pertaining to current reranking methods is how to take full advantage of the complementary property of distinct feature modalities. Given a query image and one feature modality, a regular visual reranking framework treats the top-ranked images as pseudo positive instances which are inevitably noisy, difficult to reveal this complementary property, and thus lead to inferior ranking performance. This paper proposes a novel image reranking approach by introducing a Co-Regularized Multi-Graph Learning (Co-RMGL) framework, in which the intra-graph and inter-graph constraints are simultaneously imposed to encode affinities in a single graph and consistency across different graphs. Moreover, weakly supervised learning driven by image attributes is performed to denoise the pseudo- labeled instances, thereby highlighting the unique strength of individual feature modality. Meanwhile, such learning can yield a few anchors in graphs that vitally enable the alignment and fusion of multiple graphs. As a result, an edge weight matrix learned from the fused graph automatically gives the ordering to the initially retrieved results. We evaluate our approach on four benchmark image retrieval datasets, demonstrating a significant performance gain over the state-of-the-arts.</p><p>2 0.7809546 <a title="445-lsi-2" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>Author: Shiliang Zhang, Ming Yang, Xiaoyu Wang, Yuanqing Lin, Qi Tian</p><p>Abstract: Inverted indexes in image retrieval not only allow fast access to database images but also summarize all knowledge about the database, so that their discriminative capacity largely determines the retrieval performance. In this paper, for vocabulary tree based image retrieval, we propose a semantic-aware co-indexing algorithm to jointly San Antonio, TX 78249 . j dl@gmai l com qit ian@cs .ut sa . edu . The query embed two strong cues into the inverted indexes: 1) local invariant features that are robust to delineate low-level image contents, and 2) semantic attributes from large-scale object recognition that may reveal image semantic meanings. For an initial set of inverted indexes of local features, we utilize 1000 semantic attributes to filter out isolated images and insert semantically similar images to the initial set. Encoding these two distinct cues together effectively enhances the discriminative capability of inverted indexes. Such co-indexing operations are totally off-line and introduce small computation overhead to online query cause only local features but no semantic attributes are used for query. Experiments and comparisons with recent retrieval methods on 3 datasets, i.e., UKbench, Holidays, Oxford5K, and 1.3 million images from Flickr as distractors, manifest the competitive performance of our method 1.</p><p>3 0.74283874 <a title="445-lsi-3" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<p>Author: Shi Qiu, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: This paper proposes modeling the complex web image collections with an automatically generated graph structure called visual semantic complex network (VSCN). The nodes on this complex network are clusters of images with both visual and semantic consistency, called semantic concepts. These nodes are connected based on the visual and semantic correlations. Our VSCN with 33, 240 concepts is generated from a collection of 10 million web images. 1 A great deal of valuable information on the structures of the web image collections can be revealed by exploring the VSCN, such as the small-world behavior, concept community, indegree distribution, hubs, and isolated concepts. It not only helps us better understand the web image collections at a macroscopic level, but also has many important practical applications. This paper presents two application examples: content-based image retrieval and image browsing. Experimental results show that the VSCN leads to significant improvement on both the precision of image retrieval (over 200%) and user experience for image browsing.</p><p>4 0.68256605 <a title="445-lsi-4" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>Author: Adriana Kovashka, Kristen Grauman</p><p>Abstract: Current methods learn monolithic attribute predictors, with the assumption that a single model is sufficient to reflect human understanding of a visual attribute. However, in reality, humans vary in how they perceive the association between a named property and image content. For example, two people may have slightly different internal models for what makes a shoe look “formal”, or they may disagree on which of two scenes looks “more cluttered”. Rather than discount these differences as noise, we propose to learn user-specific attribute models. We adapt a generic model trained with annotations from multiple users, tailoring it to satisfy user-specific labels. Furthermore, we propose novel techniques to infer user-specific labels based on transitivity and contradictions in the user’s search history. We demonstrate that adapted attributes improve accuracy over both existing monolithic models as well as models that learn from scratch with user-specific data alone. In addition, we show how adapted attributes are useful to personalize image search, whether with binary or relative attributes.</p><p>5 0.67782539 <a title="445-lsi-5" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<p>Author: Jon Almazán, Albert Gordo, Alicia Fornés, Ernest Valveny</p><p>Abstract: We propose an approach to multi-writer word spotting, where the goal is to find a query word in a dataset comprised of document images. We propose an attributes-based approach that leads to a low-dimensional, fixed-length representation of the word images that is fast to compute and, especially, fast to compare. This approach naturally leads to an unified representation of word images and strings, which seamlessly allows one to indistinctly perform queryby-example, where the query is an image, and query-bystring, where the query is a string. We also propose a calibration scheme to correct the attributes scores based on Canonical Correlation Analysis that greatly improves the results on a challenging dataset. We test our approach on two public datasets showing state-of-the-art results.</p><p>6 0.66801 <a title="445-lsi-6" href="./iccv-2013-3D_Sub-query_Expansion_for_Improving_Sketch-Based_Multi-view_Image_Retrieval.html">3 iccv-2013-3D Sub-query Expansion for Improving Sketch-Based Multi-view Image Retrieval</a></p>
<p>7 0.65708947 <a title="445-lsi-7" href="./iccv-2013-Fast_Neighborhood_Graph_Search_Using_Cartesian_Concatenation.html">159 iccv-2013-Fast Neighborhood Graph Search Using Cartesian Concatenation</a></p>
<p>8 0.65482569 <a title="445-lsi-8" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>9 0.64026225 <a title="445-lsi-9" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>10 0.63467872 <a title="445-lsi-10" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>11 0.63127154 <a title="445-lsi-11" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>12 0.62831748 <a title="445-lsi-12" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>13 0.61417979 <a title="445-lsi-13" href="./iccv-2013-Query-Adaptive_Asymmetrical_Dissimilarities_for_Visual_Object_Retrieval.html">334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</a></p>
<p>14 0.61235064 <a title="445-lsi-14" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>15 0.589378 <a title="445-lsi-15" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>16 0.58721226 <a title="445-lsi-16" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>17 0.58632094 <a title="445-lsi-17" href="./iccv-2013-To_Aggregate_or_Not_to_aggregate%3A_Selective_Match_Kernels_for_Image_Search.html">419 iccv-2013-To Aggregate or Not to aggregate: Selective Match Kernels for Image Search</a></p>
<p>18 0.57771975 <a title="445-lsi-18" href="./iccv-2013-Learning_Coupled_Feature_Spaces_for_Cross-Modal_Matching.html">235 iccv-2013-Learning Coupled Feature Spaces for Cross-Modal Matching</a></p>
<p>19 0.5667873 <a title="445-lsi-19" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>20 0.55770451 <a title="445-lsi-20" href="./iccv-2013-SYM-FISH%3A_A_Symmetry-Aware_Flip_Invariant_Sketch_Histogram_Shape_Descriptor.html">368 iccv-2013-SYM-FISH: A Symmetry-Aware Flip Invariant Sketch Histogram Shape Descriptor</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.108), (4, 0.033), (7, 0.021), (12, 0.021), (13, 0.016), (26, 0.073), (27, 0.021), (30, 0.012), (31, 0.039), (34, 0.012), (40, 0.149), (42, 0.092), (64, 0.093), (73, 0.037), (89, 0.147), (95, 0.022), (98, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92252392 <a title="445-lda-1" href="./iccv-2013-Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illumination_in_the_Spectral_Domain.html">385 iccv-2013-Separating Reflective and Fluorescent Components Using High Frequency Illumination in the Spectral Domain</a></p>
<p>Author: Ying Fu, Antony Lam, Imari Sato, Takahiro Okabe, Yoichi Sato</p><p>Abstract: Hyperspectral imaging is beneficial to many applications but current methods do not consider fluorescent effects which are present in everyday items ranging from paper, to clothing, to even our food. Furthermore, everyday fluorescent items exhibit a mix of reflectance and fluorescence. So proper separation of these components is necessary for analyzing them. In this paper, we demonstrate efficient separation and recovery of reflective and fluorescent emission spectra through the use of high frequency illumination in the spectral domain. With the obtained fluorescent emission spectra from our high frequency illuminants, we then present to our knowledge, the first method for estimating the fluorescent absorption spectrum of a material given its emission spectrum. Conventional bispectral measurement of absorption and emission spectra needs to examine all combinations of incident and observed light wavelengths. In contrast, our method requires only two hyperspectral images. The effectiveness of our proposed methods are then evaluated through a combination of simulation and real experiments. We also demonstrate an application of our method to synthetic relighting of real scenes.</p><p>2 0.90834653 <a title="445-lda-2" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>Author: Stefan Duffner, Christophe Garcia</p><p>Abstract: In this paper, we present a novel algorithm for fast tracking of generic objects in videos. The algorithm uses two components: a detector that makes use of the generalised Hough transform with pixel-based descriptors, and a probabilistic segmentation method based on global models for foreground and background. These components are used for tracking in a combined way, and they adapt each other in a co-training manner. Through effective model adaptation and segmentation, the algorithm is able to track objects that undergo rigid and non-rigid deformations and considerable shape and appearance variations. The proposed tracking method has been thoroughly evaluated on challenging standard videos, and outperforms state-of-theart tracking methods designed for the same task. Finally, the proposed models allow for an extremely efficient implementation, and thus tracking is very fast.</p><p>3 0.88378429 <a title="445-lda-3" href="./iccv-2013-Total_Variation_Regularization_for_Functions_with_Values_in_a_Manifold.html">421 iccv-2013-Total Variation Regularization for Functions with Values in a Manifold</a></p>
<p>Author: Jan Lellmann, Evgeny Strekalovskiy, Sabrina Koetter, Daniel Cremers</p><p>Abstract: While total variation is among the most popular regularizers for variational problems, its extension to functions with values in a manifold is an open problem. In this paper, we propose the first algorithm to solve such problems which applies to arbitrary Riemannian manifolds. The key idea is to reformulate the variational problem as a multilabel optimization problem with an infinite number of labels. This leads to a hard optimization problem which can be approximately solved using convex relaxation techniques. The framework can be easily adapted to different manifolds including spheres and three-dimensional rotations, and allows to obtain accurate solutions even with a relatively coarse discretization. With numerous examples we demonstrate that the proposed framework can be applied to variational models that incorporate chromaticity values, normal fields, or camera trajectories.</p><p>same-paper 4 0.87270117 <a title="445-lda-4" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>Author: Cheng Deng, Rongrong Ji, Wei Liu, Dacheng Tao, Xinbo Gao</p><p>Abstract: Visual reranking has been widely deployed to refine the quality of conventional content-based image retrieval engines. The current trend lies in employing a crowd of retrieved results stemming from multiple feature modalities to boost the overall performance of visual reranking. However, a major challenge pertaining to current reranking methods is how to take full advantage of the complementary property of distinct feature modalities. Given a query image and one feature modality, a regular visual reranking framework treats the top-ranked images as pseudo positive instances which are inevitably noisy, difficult to reveal this complementary property, and thus lead to inferior ranking performance. This paper proposes a novel image reranking approach by introducing a Co-Regularized Multi-Graph Learning (Co-RMGL) framework, in which the intra-graph and inter-graph constraints are simultaneously imposed to encode affinities in a single graph and consistency across different graphs. Moreover, weakly supervised learning driven by image attributes is performed to denoise the pseudo- labeled instances, thereby highlighting the unique strength of individual feature modality. Meanwhile, such learning can yield a few anchors in graphs that vitally enable the alignment and fusion of multiple graphs. As a result, an edge weight matrix learned from the fused graph automatically gives the ordering to the initially retrieved results. We evaluate our approach on four benchmark image retrieval datasets, demonstrating a significant performance gain over the state-of-the-arts.</p><p>5 0.84030771 <a title="445-lda-5" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>Author: Anestis Papazoglou, Vittorio Ferrari</p><p>Abstract: We present a technique for separating foreground objects from the background in a video. Our method isfast, , fully automatic, and makes minimal assumptions about the video. This enables handling essentially unconstrained settings, including rapidly moving background, arbitrary object motion and appearance, and non-rigid deformations and articulations. In experiments on two datasets containing over 1400 video shots, our method outperforms a state-of-theart background subtraction technique [4] as well as methods based on clustering point tracks [6, 18, 19]. Moreover, it performs comparably to recent video object segmentation methods based on objectproposals [14, 16, 27], while being orders of magnitude faster.</p><p>6 0.82783425 <a title="445-lda-6" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>7 0.82392824 <a title="445-lda-7" href="./iccv-2013-Real-Time_Articulated_Hand_Pose_Estimation_Using_Semi-supervised_Transductive_Regression_Forests.html">340 iccv-2013-Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests</a></p>
<p>8 0.81989598 <a title="445-lda-8" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>9 0.81485277 <a title="445-lda-9" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>10 0.81416726 <a title="445-lda-10" href="./iccv-2013-Alternating_Regression_Forests_for_Object_Detection_and_Pose_Estimation.html">47 iccv-2013-Alternating Regression Forests for Object Detection and Pose Estimation</a></p>
<p>11 0.81237316 <a title="445-lda-11" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>12 0.81151193 <a title="445-lda-12" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>13 0.80784053 <a title="445-lda-13" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>14 0.80447197 <a title="445-lda-14" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>15 0.80246609 <a title="445-lda-15" href="./iccv-2013-Finding_the_Best_from_the_Second_Bests_-_Inhibiting_Subjective_Bias_in_Evaluation_of_Visual_Tracking_Algorithms.html">168 iccv-2013-Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms</a></p>
<p>16 0.80181158 <a title="445-lda-16" href="./iccv-2013-Linear_Sequence_Discriminant_Analysis%3A_A_Model-Based_Dimensionality_Reduction_Method_for_Vector_Sequences.html">253 iccv-2013-Linear Sequence Discriminant Analysis: A Model-Based Dimensionality Reduction Method for Vector Sequences</a></p>
<p>17 0.80171454 <a title="445-lda-17" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>18 0.80157137 <a title="445-lda-18" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>19 0.80145073 <a title="445-lda-19" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>20 0.80135179 <a title="445-lda-20" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
