<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-156" href="#">iccv2013-156</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</h1>
<br/><p>Source: <a title="iccv-2013-156-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Yang_Fast_Direct_Super-Resolution_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Chih-Yuan Yang, Ming-Hsuan Yang</p><p>Abstract: The goal of single-image super-resolution is to generate a high-quality high-resolution image based on a given low-resolution input. It is an ill-posed problem which requires exemplars or priors to better reconstruct the missing high-resolution image details. In this paper, we propose to split the feature space into numerous subspaces and collect exemplars to learn priors for each subspace, thereby creating effective mapping functions. The use of split input space facilitates both feasibility of using simple functionsfor super-resolution, and efficiency ofgenerating highresolution results. High-quality high-resolution images are reconstructed based on the effective learned priors. Experimental results demonstrate that theproposed algorithmperforms efficiently and effectively over state-of-the-art methods.</p><p>Reference: <a title="iccv-2013-156-reference" href="../iccv2013_reference/iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 It is an ill-posed problem which requires exemplars or priors to better reconstruct the missing high-resolution image details. [sent-3, score-0.207]
</p><p>2 In this paper, we propose to split the feature space into numerous subspaces and collect exemplars to learn priors for each subspace, thereby creating effective mapping functions. [sent-4, score-0.507]
</p><p>3 To alleviate this ill-posed problem, it is imperative for most SISR algorithms to exploit additional information such as exemplar images or statistical priors. [sent-11, score-0.225]
</p><p>4 First, there exist fundamental ambiguities between the LR and HR data as significantly different HR image patches may generate very similar LR patches as a result of downsampling process. [sent-14, score-0.523]
</p><p>5 That is, the mapping between HR and LR data is many to one and the reverse process from one single LR image patch alone is inherently ambiguous. [sent-15, score-0.196]
</p><p>6 Second, the success of this approach hinges on the assumption that a high-fidelity HR patch can be found from the LR one (aside from ambiguity which can be alleviated with statistical priors), thereby requiring a large and adequate dataset at our disposal. [sent-16, score-0.388]
</p><p>7 Third, the ensuing problem with a large dataset is how to determine similar patches efficiently. [sent-17, score-0.231]
</p><p>8 Since the priors are learned from numerous examples, they are statistically effective to represent the majority of the training data. [sent-19, score-0.285]
</p><p>9 The computational load of these algorithms is relatively low, as it is not necessary to search exemplars. [sent-20, score-0.165]
</p><p>10 Although the process of learning  statistical priors is time consuming, it can be computed offline and only once for SR applications. [sent-21, score-0.161]
</p><p>11 In this paper, we propose a divide-and-conquer approach [25, 23] to learn statistical priors directly from exemplar patches using a large number of simple functions. [sent-28, score-0.58]
</p><p>12 We show that while sufficient amount of data is collected, the ambiguity problem of the source HR patches is alleviated. [sent-29, score-0.3]
</p><p>13 While LR feature space is properly divided, simple linear functions are sufficient to map LR patches to HR effectively. [sent-30, score-0.342]
</p><p>14 First, we demonstrate a direct single-image superresolution algorithm can be simple and fast when effective exemplars are available in the training phase. [sent-33, score-0.214]
</p><p>15 Second, we effectively split the input domain of low-resolution patches based on exemplar images, thereby facilitating learning simple functions for effective mapping. [sent-34, score-0.511]
</p><p>16 Third, the proposed algorithm generates favorable results with low computational load against existing methods. [sent-35, score-0.218]
</p><p>17 We demonstrate the merits of the proposed algorithm in terms of image quality and computational load by numerous qualitative and quantitative comparisons with the state-of-the-art methods. [sent-36, score-0.261]
</p><p>18 Classic methods render HR images from LR ones through 561  certain mathematical formulations [13, 11] such as bicubic interpolation and back-projection [8]. [sent-39, score-0.245]
</p><p>19 To reduce the ambiguity between LR and HR patches, spacial correlation is exploited to minimize the difference of overlapping HR patches [4, 1, 19]. [sent-43, score-0.275]
</p><p>20 For improving the effectiveness of exemplar images, user guidance is required to prepare precise ones [19, 6]. [sent-44, score-0.192]
</p><p>21 In order to increase the efficiency of reconstructed HR edges, small scaling factors and a compact exemplar patch set are proposed by generating from the input frame [5, 3]. [sent-45, score-0.416]
</p><p>22 For increasing the chance to retrieve effective patches, segments are introduced for multiple-level patch searching [17, 6]. [sent-46, score-0.196]
</p><p>23 Statistical SISR algorithms learn priors from numerous feature vectors to generate a function mapping features  from LR to HR. [sent-47, score-0.316]
</p><p>24 A significant advantage of this approach is the low computational complexity as the load of searching exemplars is alleviated. [sent-48, score-0.285]
</p><p>25 Edge-specific priors focus on reconstructing sharp edges because they are important visual cues for image quality [2, 16]. [sent-50, score-0.233]
</p><p>26 In addition, priors of patch mapping from LR to HR are developed based on dictionaries via sparse representation [24, 21], support vector regression [12], or kernel ridge regression [9]. [sent-51, score-0.54]
</p><p>27 To handle the ambiguity problem using simple features, we spend intensive  computational load during the training phase. [sent-62, score-0.238]
</p><p>28 We collect a large set of LR patches and their corresponding HR source patches. [sent-63, score-0.263]
</p><p>29 A set of functions is learned to map a LR patch to a set of pixels at the central (shaded) region of the corresponding HR patch (instead of the entire HR patch). [sent-66, score-0.395]
</p><p>30 We compute the patch mean of Pl as μ, and extract the features of Ph and Pl as the intensities minus μ to present the high-frequency signals. [sent-73, score-0.178]
</p><p>31 For HR patch Ph, we only extract features for pixels at the central region (e. [sent-74, score-0.168]
</p><p>32 We do not learn mapping functions to predict the HR boundary pixels as the LR patch Pl does not carry sufficient information to predict those pixels. [sent-77, score-0.344]
</p><p>33 We collect a large set of LR patches from natural images to learn K cluster centers of their extracted features. [sent-78, score-0.527]
</p><p>34 Figure 2 shows 4096 cluster centers learned from 2. [sent-79, score-0.223]
</p><p>35 Similar to the heavy-tailed gradient distribution in natural images [7], more populous cluster centers correspond to smoother patches as shown in Figure 3. [sent-81, score-0.541]
</p><p>36 These K cluster centers can be viewed as anchor points to represent the feature space of natural image patches. [sent-82, score-0.281]
</p><p>37 For some regions in the feature space where natural patches appear fairly rarely, it is unnecessary to learn mapping functions to predict patches ofHR from LR. [sent-83, score-0.658]
</p><p>38 Since each cluster represents a subspace, we collect a certain number of exemplar patches in the segmented space to training a mapping function. [sent-84, score-0.608]
</p><p>39 Since natural images are abundant and easily acquired, we can assume that there are sufficient exemplar patches available for each cluster center. [sent-85, score-0.584]
</p><p>40 Suppose there are l LR exemplar patches belonging to the same cluster. [sent-86, score-0.388]
</p><p>41 , l) be vectorized features of the LR and HR patches respectively, in dimensions m and n. [sent-90, score-0.231]
</p><p>42 As the features for clustering are the intensities subtracting patch means, we show the intensities by adding their mean values for visualization purpose. [sent-97, score-0.222]
</p><p>43 The order of cluster centers is sorted by the amounts of clustered patches, as shown in Figure 3. [sent-98, score-0.235]
</p><p>44 2 million natural patches with cluster centers shown in Figure 2. [sent-102, score-0.512]
</p><p>45 While the most populous cluster consists of 18489 patches, the 40 least populous clusters only have one patch. [sent-103, score-0.319]
</p><p>46 u Given a LR test image, we crop each LR patch to compute the LR features and search for the closest cluster center. [sent-124, score-0.231]
</p><p>47 According to the cluster center, we apply the learned coefficients to compute the HR features by w = C∗  ? [sent-125, score-0.173]
</p><p>48 (3)  The predicted HR patch intensity is then reconstructed by adding the LR patch mean to the HR features. [sent-128, score-0.3]
</p><p>49 The proposed method generates effective HR patches because each test LR patch and its exemplar LR patches are highly similar as they belong to the same compact feature subspace. [sent-129, score-0.871]
</p><p>50 The computational load for generating a HR image is low as each HR patch can be generated by a LR patch through a few additions and multiplications. [sent-130, score-0.518]
</p><p>51 The algorithm can easily be executed in parallel because all LR patches are upsampled individually. [sent-131, score-0.298]
</p><p>52 Experimental Results Implementation: For color images, we apply the proposed algorithm on brightness channel (Y) and upsample color channels (UV) by bicubic interpolation as human vision is  ××  more sensitive to brightness change. [sent-134, score-0.342]
</p><p>53 The LR patch size is set as 7 7 pixels, and the LR feature dimension is 45 since faosu 7r corner pixels are tdhiesc LaRrde fde. [sent-138, score-0.168]
</p><p>54 a tTuhree dciemnteranls region 4o5f a HncRe patch is set as 12 12 pixels (as illustrated in Figure 1). [sent-139, score-0.168]
</p><p>55 iSsi ncocvee trheed c by r9a lL rReg patches aRn ids t 3h e× output intensity li isn generated by averaging 9 predicted values, as commonly used in the literature [5, 24, 3, 21]. [sent-141, score-0.29]
</p><p>56 We prepare a training set containing 6152 HR natural images collected from the Berkeley segmentation and LabelMe datasets [10, 14] to generate a LR training image set containing 679 million patches. [sent-142, score-0.22]
</p><p>57 2 million patches to learn a set of 4096 cluster centers, and use the learned cluster centers to label all LR patches in training image set. [sent-144, score-0.89]
</p><p>58 Numbers of patches used to train regression coefficients in our experiments. [sent-150, score-0.329]
</p><p>59 Since some patches are rarely observed in natural images, there are fewer than 1000 patches in some clusters. [sent-151, score-0.542]
</p><p>60 Images best viewed on a high-resolution display where each image is shown with at least 5 12 512 pixels (full resolution). [sent-154, score-0.192]
</p><p>61 Since some patches are rarely observed in natural images, there are fewer than 1000 patches in a few clusters. [sent-157, score-0.542]
</p><p>62 Otherwise, we use bilinear interpolation to map LR patches for such clusters. [sent-162, score-0.316]
</p><p>63 While the low-frequency regions are almost the same, the high-frequency regions of the image generated by more clusters are better in terms of less jaggy artifacts along the face contours. [sent-165, score-0.207]
</p><p>64 With more clusters, the input feature space can be divided into more compact subspaces from which the linear mapping functions can be learned more effectively. [sent-166, score-0.226]
</p><p>65 However, the computational load of SVRs is much higher due to the cost of computing the similarity between each support vector and the test vector. [sent-169, score-0.165]
</p><p>66 While the generated SR images by the proposed method are comparable to those by the self-exemplar  SR algorithm [5], the required computational load is much lower (14 seconds vs. [sent-177, score-0.224]
</p><p>67 The evaluations are presented from the four perspectives with comparisons to SR methods using statistical priors [9, 16], fast SR algorithms [8, 15], self-exemplar SR algorithms [5, 3], and SR approaches with dictionary learning [24, 21]. [sent-187, score-0.19]
</p><p>68 SR methods based on statistical priors: As shown in Fig-  ure 6(b)(c), Figure 8(a), Figure 10(c), and Figure 11(b)(c), the proposed algorithm generates textures with better contrast than existing methods using statistical priors [9, 16]. [sent-188, score-0.388]
</p><p>69 However, mid-frequency details at textures may be wrongly reduced and the filtered textures appear unrealistic. [sent-191, score-0.242]
</p><p>70 First, the proposed method upsamples the LR patches directly rather than using an intermediate image generated by bicubic interpolation, and thus there is no loss of texture details. [sent-193, score-0.473]
</p><p>71 8859  Results best viewed on a high-resolution display with adequate zoom level where each image is shown with at least  5 12 pixels (full resolution). [sent-213, score-0.384]
</p><p>72 Fourth, existing methods learn a single regressor for the whole feature space, but the proposed method learns numerous regressors (one for each subspace), thereby making the prediction more effective. [sent-215, score-0.246]
</p><p>73 Fast SR methods: Compared with existing fast SR methods [8, 15] and bicubic interpolation, Figure 6(a)(e)(f), Figure 7, and Figure 11(a)(b) show that the proposed method generates better edges and textures. [sent-216, score-0.289]
</p><p>74 Although bicubic interpolation is the fastest method, the generated edges and textures are always over-smoothed. [sent-217, score-0.486]
</p><p>75 However, the image structures along sharp edges are highly anisotropic, and thus an isotropic kernel wrongly compensates the intensities. [sent-220, score-0.199]
</p><p>76 Thus, over-smoothed textures and jaggy edges are generated by this method. [sent-223, score-0.318]
</p><p>77 The proposed method generates better edges and textures as each LR patch is upsampled by a specific prior learned from a compact subspace of similar patches. [sent-224, score-0.501]
</p><p>78 Such an approach has an advantage of generating sharp and clear edges because it is easy to find similar edge 565  (a) Bicubic Interpolation(b) Back Projection [8](c) Shan [5](d) Proposed  Figure 7. [sent-231, score-0.192]
</p><p>79 Results best viewed on a high-resolution display with adequate zoom level where each image is shown with at least 974  ×  800  ×  (a) Sun [16](b) Glasner [5](c) Wang [21](d) Proposed PSNR / SSIM: 28. [sent-233, score-0.35]
</p><p>80 9071  Results best viewed on a high-resolution display with adequate zoom level where each image is shown with at least  5 12 pixels (full resolution). [sent-243, score-0.384]
</p><p>81 However, the exemplar patches generated by the input image are few. [sent-245, score-0.447]
</p><p>82 To reduce the errors caused by using a small exemplar patch set, the back-projection technique is facilitated as a post-processing in [5] to refine the generated image in each upsampling iteration. [sent-246, score-0.394]
</p><p>83 However, the post-processing may over-compensate SR images and generate artifacts, as shown in Figure 8(b) and Figure 11(c), the edges and textures are over-sharpened and unnatural. [sent-247, score-0.216]
</p><p>84 In addition, since all exemplar patches are generated from the input image, it entails a computationally expensive on-line process to find similar patches and makes the method less suitable for realtime applications. [sent-248, score-0.71]
</p><p>85 An simplified algorithm [3] reduces the computational load by searching local patches only, but the restriction also reduces the image quality. [sent-249, score-0.426]
</p><p>86 In contrast, the proposed method overcomes the dif-  ficulty of finding rare patches by using a huge exemplar set, which improves the probability to find similar edge patches. [sent-252, score-0.414]
</p><p>87 The proposed method exploits the well labeled edge patches in training phase to generated effective SR edges in test phase. [sent-253, score-0.453]
</p><p>88 As shown in Figure 6(d), Figure 7(d), Figure 8(d), Figure 10(d), and Figure 11(d), the proposed method generates SR images with sharp edges effectively. [sent-254, score-0.193]
</p><p>89 The proposed algorithm splits the feature space and learns numerous mapping functions individually, but the existing algorithms [24, 21] learn one mapping function (through the paired dictionaries) for all patches. [sent-257, score-0.334]
</p><p>90 Results best viewed on a high-resolution display with adequate zoom level where each image is shown with at least  ×  (a) Wang [21](b) Freedman [3](c) Sun [16](d) Proposed PSNR / SSIM: 25. [sent-268, score-0.35]
</p><p>91 Results best viewed on a high-resolution display with adequate zoom level where each image is shown with at least  320  480 pixels (full resolution). [sent-278, score-0.384]
</p><p>92 Since patches of sharp edges are less frequent than smooth patches in natural images, blocky edges can be observed in Figure 6(g). [sent-280, score-0.769]
</p><p>93 To improve the accuracy of patch mapping through a pair of dictionaries, an additional transform matrix is proposed in [21] to map LR sparse coefficients to HR ones. [sent-281, score-0.261]
</p><p>94 As shown in Figure 6(h), Figure 8(c) and Figure 10(a), the edges are sharp without blocky artifacts. [sent-282, score-0.186]
</p><p>95 However, the additional transform matrix blurs textures because the mapping of sparse coefficients becomes many-to-many rather than oneto-one, which results in effects of averaging. [sent-283, score-0.233]
</p><p>96 Using simple feature and linear functions, the proposed method generates sharper edges than [24] and richer textures than [21], as shown in  ×  Figure 6(d)(g)(h), Figure 8(c)(d), and Figure 10(a)(d). [sent-285, score-0.235]
</p><p>97 By splitting the feature space into numerous subspaces and collecting sufficient training exemplars to learn simple regression functions, the proposed method generates high-quality SR images with sharp edges and rich textures. [sent-288, score-0.559]
</p><p>98 Results best viewed on a high-resolution display with adequate zoom level where each image is shown with at least  320  480 pixels (full resolution). [sent-298, score-0.384]
</p><p>99 Single image super-resolution by clustered sparse representation and adaptive patch aggregation. [sent-453, score-0.205]
</p><p>100 Single-image superresolution reconstruction via learned geometric dictionaries and clustered sparse coding. [sent-467, score-0.226]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hr', 0.493), ('lr', 0.427), ('sr', 0.245), ('sisr', 0.231), ('patches', 0.231), ('bicubic', 0.16), ('exemplar', 0.157), ('patch', 0.134), ('load', 0.132), ('ssim', 0.112), ('adequate', 0.109), ('textures', 0.106), ('psnr', 0.1), ('cluster', 0.097), ('numerous', 0.096), ('priors', 0.093), ('centers', 0.091), ('exemplars', 0.09), ('interpolation', 0.085), ('display', 0.084), ('zoom', 0.083), ('jaggy', 0.077), ('populous', 0.077), ('edges', 0.076), ('statistical', 0.068), ('sharp', 0.064), ('svrs', 0.063), ('superresolution', 0.063), ('mapping', 0.062), ('regressors', 0.061), ('freedman', 0.06), ('generated', 0.059), ('functions', 0.058), ('regression', 0.057), ('dictionaries', 0.057), ('generates', 0.053), ('upsample', 0.051), ('million', 0.048), ('sun', 0.048), ('viewed', 0.048), ('shan', 0.047), ('resolution', 0.047), ('glasner', 0.047), ('clustered', 0.047), ('blocky', 0.046), ('natural', 0.045), ('intensities', 0.044), ('pl', 0.044), ('upsampling', 0.044), ('ambiguity', 0.044), ('ph', 0.043), ('clusters', 0.042), ('coefficients', 0.041), ('tip', 0.04), ('subspaces', 0.038), ('upsampled', 0.038), ('prepare', 0.035), ('learned', 0.035), ('rarely', 0.035), ('generate', 0.034), ('pixels', 0.034), ('scaling', 0.034), ('computational', 0.033), ('thereby', 0.033), ('compact', 0.033), ('entails', 0.032), ('effective', 0.032), ('reconstructed', 0.032), ('collect', 0.032), ('learn', 0.031), ('searching', 0.03), ('iccp', 0.03), ('wrongly', 0.03), ('ih', 0.03), ('artifacts', 0.029), ('kernel', 0.029), ('training', 0.029), ('dictionary', 0.029), ('abundant', 0.029), ('executed', 0.029), ('shaded', 0.029), ('properly', 0.028), ('downsampling', 0.027), ('facilitates', 0.027), ('ridge', 0.027), ('generating', 0.026), ('least', 0.026), ('labelme', 0.026), ('interpolated', 0.026), ('subspace', 0.026), ('edge', 0.026), ('super', 0.026), ('profile', 0.025), ('regressor', 0.025), ('paired', 0.025), ('sufficient', 0.025), ('sparse', 0.024), ('reconstruct', 0.024), ('brightness', 0.023), ('intermediate', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="156-tfidf-1" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>Author: Chih-Yuan Yang, Ming-Hsuan Yang</p><p>Abstract: The goal of single-image super-resolution is to generate a high-quality high-resolution image based on a given low-resolution input. It is an ill-posed problem which requires exemplars or priors to better reconstruct the missing high-resolution image details. In this paper, we propose to split the feature space into numerous subspaces and collect exemplars to learn priors for each subspace, thereby creating effective mapping functions. The use of split input space facilitates both feasibility of using simple functionsfor super-resolution, and efficiency ofgenerating highresolution results. High-quality high-resolution images are reconstructed based on the effective learned priors. Experimental results demonstrate that theproposed algorithmperforms efficiently and effectively over state-of-the-art methods.</p><p>2 0.32009333 <a title="156-tfidf-2" href="./iccv-2013-A_Joint_Intensity_and_Depth_Co-sparse_Analysis_Model_for_Depth_Map_Super-resolution.html">18 iccv-2013-A Joint Intensity and Depth Co-sparse Analysis Model for Depth Map Super-resolution</a></p>
<p>Author: Martin Kiechle, Simon Hawe, Martin Kleinsteuber</p><p>Abstract: High-resolution depth maps can be inferred from lowresolution depth measurements and an additional highresolution intensity image of the same scene. To that end, we introduce a bimodal co-sparse analysis model, which is able to capture the interdependency of registered intensity . go l e i um . de . .t ities together with the knowledge of the relative positions between all views. Despite very active research in this area and significant improvements over the past years, stereo methods still struggle with noise, texture-less regions, repetitive texture, and occluded areas. For an overview of stereo methods, the reader is referred to [25]. and depth information. This model is based on the assumption that the co-supports of corresponding bimodal image structures are aligned when computed by a suitable pair of analysis operators. No analytic form of such operators ex- ist and we propose a method for learning them from a set of registered training signals. This learning process is done offline and returns a bimodal analysis operator that is universally applicable to natural scenes. We use this to exploit the bimodal co-sparse analysis model as a prior for solving inverse problems, which leads to an efficient algorithm for depth map super-resolution.</p><p>3 0.30368674 <a title="156-tfidf-3" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>Author: Radu Timofte, Vincent De_Smet, Luc Van_Gool</p><p>Abstract: Recently there have been significant advances in image upscaling or image super-resolution based on a dictionary of low and high resolution exemplars. The running time of the methods is often ignored despite the fact that it is a critical factor for real applications. This paper proposes fast super-resolution methods while making no compromise on quality. First, we support the use of sparse learned dictionaries in combination with neighbor embedding methods. In this case, the nearest neighbors are computed using the correlation with the dictionary atoms rather than the Euclidean distance. Moreover, we show that most of the current approaches reach top performance for the right parameters. Second, we show that using global collaborative coding has considerable speed advantages, reducing the super-resolution mapping to a precomputed projective matrix. Third, we propose the anchored neighborhood regression. That is to anchor the neighborhood embedding of a low resolution patch to the nearest atom in the dictionary and to precompute the corresponding embedding matrix. These proposals are contrasted with current state-of- the-art methods on standard images. We obtain similar or improved quality and one or two orders of magnitude speed improvements.</p><p>4 0.29335946 <a title="156-tfidf-4" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>Author: Netalee Efrat, Daniel Glasner, Alexander Apartsin, Boaz Nadler, Anat Levin</p><p>Abstract: Over the past decade, single image Super-Resolution (SR) research has focused on developing sophisticated image priors, leading to significant advances. Estimating and incorporating the blur model, that relates the high-res and low-res images, has received much less attention, however. In particular, the reconstruction constraint, namely that the blurred and downsampled high-res output should approximately equal the low-res input image, has been either ignored or applied with default fixed blur models. In this work, we examine the relative importance ofthe imageprior and the reconstruction constraint. First, we show that an accurate reconstruction constraint combined with a simple gradient regularization achieves SR results almost as good as those of state-of-the-art algorithms with sophisticated image priors. Second, we study both empirically and theoretically the sensitivity of SR algorithms to the blur model assumed in the reconstruction constraint. We find that an accurate blur model is more important than a sophisticated image prior. Finally, using real camera data, we demonstrate that the default blur models of various SR algorithms may differ from the camera blur, typically leading to over- smoothed results. Our findings highlight the importance of accurately estimating camera blur in reconstructing raw low- res images acquired by an actual camera.</p><p>5 0.22186378 <a title="156-tfidf-5" href="./iccv-2013-Coupled_Dictionary_and_Feature_Space_Learning_with_Applications_to_Cross-Domain_Image_Synthesis_and_Recognition.html">96 iccv-2013-Coupled Dictionary and Feature Space Learning with Applications to Cross-Domain Image Synthesis and Recognition</a></p>
<p>Author: De-An Huang, Yu-Chiang Frank Wang</p><p>Abstract: Cross-domain image synthesis and recognition are typically considered as two distinct tasks in the areas of computer vision and pattern recognition. Therefore, it is not clear whether approaches addressing one task can be easily generalized or extended for solving the other. In this paper, we propose a unified model for coupled dictionary and feature space learning. The proposed learning model not only observes a common feature space for associating cross-domain image data for recognition purposes, the derived feature space is able to jointly update the dictionaries in each image domain for improved representation. This is why our method can be applied to both cross-domain image synthesis and recognition problems. Experiments on a variety of synthesis and recognition tasks such as single image super-resolution, cross-view action recognition, and sketchto-photo face recognition would verify the effectiveness of our proposed learning model.</p><p>6 0.20205586 <a title="156-tfidf-6" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>7 0.19270654 <a title="156-tfidf-7" href="./iccv-2013-Nonparametric_Blind_Super-resolution.html">293 iccv-2013-Nonparametric Blind Super-resolution</a></p>
<p>8 0.11372752 <a title="156-tfidf-8" href="./iccv-2013-Translating_Video_Content_to_Natural_Language_Descriptions.html">428 iccv-2013-Translating Video Content to Natural Language Descriptions</a></p>
<p>9 0.1125456 <a title="156-tfidf-9" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>10 0.11108253 <a title="156-tfidf-10" href="./iccv-2013-Single-Patch_Low-Rank_Prior_for_Non-pointwise_Impulse_Noise_Removal.html">394 iccv-2013-Single-Patch Low-Rank Prior for Non-pointwise Impulse Noise Removal</a></p>
<p>11 0.10602347 <a title="156-tfidf-11" href="./iccv-2013-DCSH_-_Matching_Patches_in_RGBD_Images.html">101 iccv-2013-DCSH - Matching Patches in RGBD Images</a></p>
<p>12 0.09480115 <a title="156-tfidf-12" href="./iccv-2013-A_New_Image_Quality_Metric_for_Image_Auto-denoising.html">23 iccv-2013-A New Image Quality Metric for Image Auto-denoising</a></p>
<p>13 0.091269441 <a title="156-tfidf-13" href="./iccv-2013-Recognising_Human-Object_Interaction_via_Exemplar_Based_Modelling.html">344 iccv-2013-Recognising Human-Object Interaction via Exemplar Based Modelling</a></p>
<p>14 0.089959852 <a title="156-tfidf-14" href="./iccv-2013-Shape_Anchors_for_Data-Driven_Multi-view_Reconstruction.html">387 iccv-2013-Shape Anchors for Data-Driven Multi-view Reconstruction</a></p>
<p>15 0.087352291 <a title="156-tfidf-15" href="./iccv-2013-Ensemble_Projection_for_Semi-supervised_Image_Classification.html">142 iccv-2013-Ensemble Projection for Semi-supervised Image Classification</a></p>
<p>16 0.083976768 <a title="156-tfidf-16" href="./iccv-2013-How_Related_Exemplars_Help_Complex_Event_Detection_in_Web_Videos%3F.html">203 iccv-2013-How Related Exemplars Help Complex Event Detection in Web Videos?</a></p>
<p>17 0.077920228 <a title="156-tfidf-17" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>18 0.075607672 <a title="156-tfidf-18" href="./iccv-2013-Partial_Enumeration_and_Curvature_Regularization.html">309 iccv-2013-Partial Enumeration and Curvature Regularization</a></p>
<p>19 0.074581333 <a title="156-tfidf-19" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>20 0.065932691 <a title="156-tfidf-20" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.16), (1, -0.007), (2, -0.047), (3, -0.027), (4, -0.131), (5, -0.017), (6, -0.043), (7, -0.091), (8, -0.02), (9, -0.112), (10, -0.021), (11, -0.089), (12, 0.068), (13, -0.082), (14, -0.066), (15, 0.038), (16, -0.053), (17, -0.16), (18, -0.047), (19, 0.016), (20, -0.057), (21, 0.124), (22, 0.027), (23, 0.104), (24, -0.009), (25, 0.157), (26, 0.05), (27, -0.109), (28, -0.313), (29, -0.138), (30, -0.085), (31, -0.123), (32, 0.212), (33, 0.119), (34, -0.107), (35, -0.005), (36, -0.011), (37, -0.052), (38, 0.055), (39, -0.051), (40, 0.233), (41, -0.084), (42, 0.056), (43, 0.003), (44, 0.012), (45, 0.008), (46, -0.137), (47, 0.062), (48, 0.036), (49, -0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.950414 <a title="156-lsi-1" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>Author: Chih-Yuan Yang, Ming-Hsuan Yang</p><p>Abstract: The goal of single-image super-resolution is to generate a high-quality high-resolution image based on a given low-resolution input. It is an ill-posed problem which requires exemplars or priors to better reconstruct the missing high-resolution image details. In this paper, we propose to split the feature space into numerous subspaces and collect exemplars to learn priors for each subspace, thereby creating effective mapping functions. The use of split input space facilitates both feasibility of using simple functionsfor super-resolution, and efficiency ofgenerating highresolution results. High-quality high-resolution images are reconstructed based on the effective learned priors. Experimental results demonstrate that theproposed algorithmperforms efficiently and effectively over state-of-the-art methods.</p><p>2 0.67577857 <a title="156-lsi-2" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>Author: Netalee Efrat, Daniel Glasner, Alexander Apartsin, Boaz Nadler, Anat Levin</p><p>Abstract: Over the past decade, single image Super-Resolution (SR) research has focused on developing sophisticated image priors, leading to significant advances. Estimating and incorporating the blur model, that relates the high-res and low-res images, has received much less attention, however. In particular, the reconstruction constraint, namely that the blurred and downsampled high-res output should approximately equal the low-res input image, has been either ignored or applied with default fixed blur models. In this work, we examine the relative importance ofthe imageprior and the reconstruction constraint. First, we show that an accurate reconstruction constraint combined with a simple gradient regularization achieves SR results almost as good as those of state-of-the-art algorithms with sophisticated image priors. Second, we study both empirically and theoretically the sensitivity of SR algorithms to the blur model assumed in the reconstruction constraint. We find that an accurate blur model is more important than a sophisticated image prior. Finally, using real camera data, we demonstrate that the default blur models of various SR algorithms may differ from the camera blur, typically leading to over- smoothed results. Our findings highlight the importance of accurately estimating camera blur in reconstructing raw low- res images acquired by an actual camera.</p><p>3 0.62521875 <a title="156-lsi-3" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>Author: Radu Timofte, Vincent De_Smet, Luc Van_Gool</p><p>Abstract: Recently there have been significant advances in image upscaling or image super-resolution based on a dictionary of low and high resolution exemplars. The running time of the methods is often ignored despite the fact that it is a critical factor for real applications. This paper proposes fast super-resolution methods while making no compromise on quality. First, we support the use of sparse learned dictionaries in combination with neighbor embedding methods. In this case, the nearest neighbors are computed using the correlation with the dictionary atoms rather than the Euclidean distance. Moreover, we show that most of the current approaches reach top performance for the right parameters. Second, we show that using global collaborative coding has considerable speed advantages, reducing the super-resolution mapping to a precomputed projective matrix. Third, we propose the anchored neighborhood regression. That is to anchor the neighborhood embedding of a low resolution patch to the nearest atom in the dictionary and to precompute the corresponding embedding matrix. These proposals are contrasted with current state-of- the-art methods on standard images. We obtain similar or improved quality and one or two orders of magnitude speed improvements.</p><p>4 0.60834914 <a title="156-lsi-4" href="./iccv-2013-Nonparametric_Blind_Super-resolution.html">293 iccv-2013-Nonparametric Blind Super-resolution</a></p>
<p>Author: Tomer Michaeli, Michal Irani</p><p>Abstract: Super resolution (SR) algorithms typically assume that the blur kernel is known (either the Point Spread Function ‘PSF’ of the camera, or some default low-pass filter, e.g. a Gaussian). However, the performance of SR methods significantly deteriorates when the assumed blur kernel deviates from the true one. We propose a general framework for “blind” super resolution. In particular, we show that: (i) Unlike the common belief, the PSF of the camera is the wrong blur kernel to use in SR algorithms. (ii) We show how the correct SR blur kernel can be recovered directly from the low-resolution image. This is done by exploiting the inherent recurrence property of small natural image patches (either internally within the same image, or externally in a collection of other natural images). In particular, we show that recurrence of small patches across scales of the low-res image (which forms the basis for single-image SR), can also be used for estimating the optimal blur kernel. This leads to significant improvement in SR results.</p><p>5 0.56935203 <a title="156-lsi-5" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>Author: Carlos Fernandez-Granda, Emmanuel J. Candès</p><p>Abstract: We present a framework to super-resolve planar regions found in urban scenes and other man-made environments by taking into account their 3D geometry. Such regions have highly structured straight edges, but this prior is challenging to exploit due to deformations induced by the projection onto the imaging plane. Our method factors out such deformations by using recently developed tools based on convex optimization to learn a transform that maps the image to a domain where its gradient has a simple group-sparse structure. This allows to obtain a novel convex regularizer that enforces global consistency constraints between the edges of the image. Computational experiments with real images show that this data-driven approach to the design of regularizers promoting transform-invariant group sparsity is very effective at high super-resolution factors. We view our approach as complementary to most recent superresolution methods, which tend to focus on hallucinating high-frequency textures.</p><p>6 0.52080268 <a title="156-lsi-6" href="./iccv-2013-A_Joint_Intensity_and_Depth_Co-sparse_Analysis_Model_for_Depth_Map_Super-resolution.html">18 iccv-2013-A Joint Intensity and Depth Co-sparse Analysis Model for Depth Map Super-resolution</a></p>
<p>7 0.51482815 <a title="156-lsi-7" href="./iccv-2013-Coupled_Dictionary_and_Feature_Space_Learning_with_Applications_to_Cross-Domain_Image_Synthesis_and_Recognition.html">96 iccv-2013-Coupled Dictionary and Feature Space Learning with Applications to Cross-Domain Image Synthesis and Recognition</a></p>
<p>8 0.47663763 <a title="156-lsi-8" href="./iccv-2013-Single-Patch_Low-Rank_Prior_for_Non-pointwise_Impulse_Noise_Removal.html">394 iccv-2013-Single-Patch Low-Rank Prior for Non-pointwise Impulse Noise Removal</a></p>
<p>9 0.47633392 <a title="156-lsi-9" href="./iccv-2013-DCSH_-_Matching_Patches_in_RGBD_Images.html">101 iccv-2013-DCSH - Matching Patches in RGBD Images</a></p>
<p>10 0.44900328 <a title="156-lsi-10" href="./iccv-2013-Detecting_Irregular_Curvilinear_Structures_in_Gray_Scale_and_Color_Imagery_Using_Multi-directional_Oriented_Flux.html">112 iccv-2013-Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-directional Oriented Flux</a></p>
<p>11 0.41408467 <a title="156-lsi-11" href="./iccv-2013-Example-Based_Facade_Texture_Synthesis.html">148 iccv-2013-Example-Based Facade Texture Synthesis</a></p>
<p>12 0.379044 <a title="156-lsi-12" href="./iccv-2013-Perceptual_Fidelity_Aware_Mean_Squared_Error.html">312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</a></p>
<p>13 0.378205 <a title="156-lsi-13" href="./iccv-2013-Efficient_Image_Dehazing_with_Boundary_Constraint_and_Contextual_Regularization.html">135 iccv-2013-Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</a></p>
<p>14 0.374771 <a title="156-lsi-14" href="./iccv-2013-Modeling_the_Calibration_Pipeline_of_the_Lytro_Camera_for_High_Quality_Light-Field_Image_Reconstruction.html">271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</a></p>
<p>15 0.37181085 <a title="156-lsi-15" href="./iccv-2013-A_New_Image_Quality_Metric_for_Image_Auto-denoising.html">23 iccv-2013-A New Image Quality Metric for Image Auto-denoising</a></p>
<p>16 0.36742967 <a title="156-lsi-16" href="./iccv-2013-Translating_Video_Content_to_Natural_Language_Descriptions.html">428 iccv-2013-Translating Video Content to Natural Language Descriptions</a></p>
<p>17 0.35450542 <a title="156-lsi-17" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>18 0.32758129 <a title="156-lsi-18" href="./iccv-2013-Local_Signal_Equalization_for_Correspondence_Matching.html">255 iccv-2013-Local Signal Equalization for Correspondence Matching</a></p>
<p>19 0.31969288 <a title="156-lsi-19" href="./iccv-2013-Image_Guided_Depth_Upsampling_Using_Anisotropic_Total_Generalized_Variation.html">209 iccv-2013-Image Guided Depth Upsampling Using Anisotropic Total Generalized Variation</a></p>
<p>20 0.31403244 <a title="156-lsi-20" href="./iccv-2013-Log-Euclidean_Kernels_for_Sparse_Representation_and_Dictionary_Learning.html">257 iccv-2013-Log-Euclidean Kernels for Sparse Representation and Dictionary Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.062), (7, 0.025), (26, 0.193), (31, 0.112), (42, 0.133), (45, 0.092), (55, 0.016), (64, 0.026), (73, 0.038), (78, 0.022), (89, 0.166), (98, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92445838 <a title="156-lda-1" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>Author: Chih-Yuan Yang, Ming-Hsuan Yang</p><p>Abstract: The goal of single-image super-resolution is to generate a high-quality high-resolution image based on a given low-resolution input. It is an ill-posed problem which requires exemplars or priors to better reconstruct the missing high-resolution image details. In this paper, we propose to split the feature space into numerous subspaces and collect exemplars to learn priors for each subspace, thereby creating effective mapping functions. The use of split input space facilitates both feasibility of using simple functionsfor super-resolution, and efficiency ofgenerating highresolution results. High-quality high-resolution images are reconstructed based on the effective learned priors. Experimental results demonstrate that theproposed algorithmperforms efficiently and effectively over state-of-the-art methods.</p><p>2 0.90343165 <a title="156-lda-2" href="./iccv-2013-Drosophila_Embryo_Stage_Annotation_Using_Label_Propagation.html">125 iccv-2013-Drosophila Embryo Stage Annotation Using Label Propagation</a></p>
<p>Author: Tomáš Kazmar, Evgeny Z. Kvon, Alexander Stark, Christoph H. Lampert</p><p>Abstract: In this work we propose a system for automatic classification of Drosophila embryos into developmental stages. While the system is designed to solve an actual problem in biological research, we believe that the principle underlying it is interesting not only for biologists, but also for researchers in computer vision. The main idea is to combine two orthogonal sources of information: one is a classifier trained on strongly invariant features, which makes it applicable to images of very different conditions, but also leads to rather noisy predictions. The other is a label propagation step based on a more powerful similarity measure that however is only consistent within specific subsets of the data at a time. In our biological setup, the information sources are the shape and the staining patterns of embryo images. We show experimentally that while neither of the methods can be used by itself to achieve satisfactory results, their combination achieves prediction quality comparable to human per- formance.</p><p>3 0.89875782 <a title="156-lda-3" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>Author: Radu Timofte, Vincent De_Smet, Luc Van_Gool</p><p>Abstract: Recently there have been significant advances in image upscaling or image super-resolution based on a dictionary of low and high resolution exemplars. The running time of the methods is often ignored despite the fact that it is a critical factor for real applications. This paper proposes fast super-resolution methods while making no compromise on quality. First, we support the use of sparse learned dictionaries in combination with neighbor embedding methods. In this case, the nearest neighbors are computed using the correlation with the dictionary atoms rather than the Euclidean distance. Moreover, we show that most of the current approaches reach top performance for the right parameters. Second, we show that using global collaborative coding has considerable speed advantages, reducing the super-resolution mapping to a precomputed projective matrix. Third, we propose the anchored neighborhood regression. That is to anchor the neighborhood embedding of a low resolution patch to the nearest atom in the dictionary and to precompute the corresponding embedding matrix. These proposals are contrasted with current state-of- the-art methods on standard images. We obtain similar or improved quality and one or two orders of magnitude speed improvements.</p><p>4 0.89636064 <a title="156-lda-4" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>Author: Lingxi Xie, Qi Tian, Richang Hong, Shuicheng Yan, Bo Zhang</p><p>Abstract: As a special topic in computer vision, , fine-grained visual categorization (FGVC) has been attracting growing attention these years. Different with traditional image classification tasks in which objects have large inter-class variation, the visual concepts in the fine-grained datasets, such as hundreds of bird species, often have very similar semantics. Due to the large inter-class similarity, it is very difficult to classify the objects without locating really discriminative features, therefore it becomes more important for the algorithm to make full use of the part information in order to train a robust model. In this paper, we propose a powerful flowchart named Hierarchical Part Matching (HPM) to cope with finegrained classification tasks. We extend the Bag-of-Features (BoF) model by introducing several novel modules to integrate into image representation, including foreground inference and segmentation, Hierarchical Structure Learn- ing (HSL), and Geometric Phrase Pooling (GPP). We verify in experiments that our algorithm achieves the state-ofthe-art classification accuracy in the Caltech-UCSD-Birds200-2011 dataset by making full use of the ground-truth part annotations.</p><p>5 0.89434791 <a title="156-lda-5" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>Author: Stefanos Zafeiriou, Irene Kotsia</p><p>Abstract: Kernels have been a common tool of machine learning and computer vision applications for modeling nonlinearities and/or the design of robust1 similarity measures between objects. Arguably, the class of positive semidefinite (psd) kernels, widely known as Mercer’s Kernels, constitutes one of the most well-studied cases. For every psd kernel there exists an associated feature map to an arbitrary dimensional Hilbert space H, the so-called feature space. Tdihme mnsaiionn reason ebreth sipnadc ep s Hd ,ke threne slos’-c c aplolpedul aferiattyu rise the fact that classification/regression techniques (such as Support Vector Machines (SVMs)) and component analysis algorithms (such as Kernel Principal Component Analysis (KPCA)) can be devised in H, without an explicit defisnisiti (oKnP of t)h)e c feature map, only by using athne xkperlniceitl (dtehfeso-called kernel trick). Recently, due to the development of very efficient solutions for large scale linear SVMs and for incremental linear component analysis, the research to- wards finding feature map approximations for classes of kernels has attracted significant interest. In this paper, we attempt the derivation of explicit feature maps of a recently proposed class of kernels, the so-called one-shot similarity kernels. We show that for this class of kernels either there exists an explicit representation in feature space or the kernel can be expressed in such a form that allows for exact incremental learning. We theoretically explore the properties of these kernels and show how these kernels can be used for the development of robust visual tracking, recognition and deformable fitting algorithms. 1Robustness may refer to either the presence of outliers and noise the robustness to a class of transformations (e.g., translation). or to ∗ Irene Kotsia ,†,? ∗Electronics Laboratory, Department of Physics, University of Patras, Greece ?School of Science and Technology, Middlesex University, London i .kot s i @mdx . ac .uk a</p><p>6 0.89260221 <a title="156-lda-6" href="./iccv-2013-Slice_Sampling_Particle_Belief_Propagation.html">395 iccv-2013-Slice Sampling Particle Belief Propagation</a></p>
<p>7 0.88917232 <a title="156-lda-7" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>8 0.88272345 <a title="156-lda-8" href="./iccv-2013-Data-Driven_3D_Primitives_for_Single_Image_Understanding.html">102 iccv-2013-Data-Driven 3D Primitives for Single Image Understanding</a></p>
<p>9 0.88100207 <a title="156-lda-9" href="./iccv-2013-A_Deformable_Mixture_Parsing_Model_with_Parselets.html">8 iccv-2013-A Deformable Mixture Parsing Model with Parselets</a></p>
<p>10 0.87780786 <a title="156-lda-10" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>11 0.87648201 <a title="156-lda-11" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>12 0.87469888 <a title="156-lda-12" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>13 0.87455559 <a title="156-lda-13" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>14 0.87406909 <a title="156-lda-14" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>15 0.87283653 <a title="156-lda-15" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>16 0.87241757 <a title="156-lda-16" href="./iccv-2013-Structured_Light_in_Sunlight.html">405 iccv-2013-Structured Light in Sunlight</a></p>
<p>17 0.86792761 <a title="156-lda-17" href="./iccv-2013-Refractive_Structure-from-Motion_on_Underwater_Images.html">348 iccv-2013-Refractive Structure-from-Motion on Underwater Images</a></p>
<p>18 0.86710042 <a title="156-lda-18" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>19 0.86695504 <a title="156-lda-19" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>20 0.86317909 <a title="156-lda-20" href="./iccv-2013-Revisiting_the_PnP_Problem%3A_A_Fast%2C_General_and_Optimal_Solution.html">353 iccv-2013-Revisiting the PnP Problem: A Fast, General and Optimal Solution</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
