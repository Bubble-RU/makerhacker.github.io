<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-40" href="#">iccv2013-40</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</h1>
<br/><p>Source: <a title="iccv-2013-40-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Oneata_Action_and_Event_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Dan Oneata, Jakob Verbeek, Cordelia Schmid</p><p>Abstract: Action recognition in uncontrolled video is an important and challenging computer vision problem. Recent progress in this area is due to new local features and models that capture spatio-temporal structure between local features, or human-object interactions. Instead of working towards more complex models, we focus on the low-level features and their encoding. We evaluate the use of Fisher vectors as an alternative to bag-of-word histograms to aggregate a small set of state-of-the-art low-level descriptors, in combination with linear classifiers. We present a large and varied set of evaluations, considering (i) classification of short actions in five datasets, (ii) localization of such actions in feature-length movies, and (iii) large-scale recognition of complex events. We find that for basic action recognition and localization MBH features alone are enough for stateof-the-art performance. For complex events we find that SIFT and MFCC features provide complementary cues. On all three problems we obtain state-of-the-art results, while using fewer features and less complex models.</p><p>Reference: <a title="iccv-2013-40-reference" href="../iccv2013_reference/iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 fr a –  Abstract Action recognition in uncontrolled video is an important and challenging computer vision problem. [sent-3, score-0.121]
</p><p>2 Instead of working towards more complex models, we focus on the low-level features and their encoding. [sent-5, score-0.101]
</p><p>3 We evaluate the use of Fisher vectors as an alternative to bag-of-word histograms to aggregate a small set of state-of-the-art low-level descriptors, in combination with linear classifiers. [sent-6, score-0.103]
</p><p>4 We present a large and varied set of evaluations, considering (i) classification of short actions in five datasets, (ii) localization of such actions in feature-length movies, and (iii) large-scale recognition of complex events. [sent-7, score-0.466]
</p><p>5 We find that for basic action recognition and localization MBH features alone are enough for stateof-the-art performance. [sent-8, score-0.415]
</p><p>6 For complex events we find that SIFT and MFCC features provide complementary cues. [sent-9, score-0.187]
</p><p>7 On all three problems we obtain state-of-the-art results, while using fewer features and less complex models. [sent-10, score-0.128]
</p><p>8 Introduction Action and event recognition in uncontrolled video are extremely challenging due to the large amount of intra-class variation caused by factors such as the style and duration of the performed action. [sent-12, score-0.294]
</p><p>9 Recently significant progress has been made in action and event recognition. [sent-15, score-0.395]
</p><p>10 Most features are carefully engineered, while some recent work explores learning the low-level features from data [20, 45]. [sent-21, score-0.12]
</p><p>11 Once local features are extracted, often methods similar to those used for object recognition are employed. [sent-22, score-0.1]
</p><p>12 Typically, local features are quantized, and their overall distribution in a video is repre-  sented by means of bag-of-visual-word (BoV) histograms. [sent-23, score-0.109]
</p><p>13 Possibly, to capture spatio-temporal layout in the spirit of [19], a concatenation of several such histograms is used, which are computed over several space-time cells overlaid on the video [17]. [sent-24, score-0.159]
</p><p>14 As for object recognition, the combination of various complementary feature types has been explored. [sent-26, score-0.111]
</p><p>15 For example, [2] considers feature pooling based on scene-types, where video frames are assigned to scene types and their features are aggregated in the corresponding scene-specific representation. [sent-27, score-0.186]
</p><p>16 Others not only include object detector responses, but also use speech recognition, and character recognition systems to extract additional high-level features [27]. [sent-29, score-0.132]
</p><p>17 A complementary line of work has focused on considering more sophisticated models for action recognition that go beyond simple bag-of-word representations, and instead aim to explicitly capture the spatial and temporal structure of actions, see e. [sent-30, score-0.423]
</p><p>18 [10] use a tree-structured CRF to model co-occurrence relations among sub-events and complex event categories, but require additional labeling of the sub-events unlike Tang et al. [sent-40, score-0.184]
</p><p>19 Structured models for action recognition seem promising to model basic actions such as drinking, answer phone, or get out of car, which could be decomposed into more basic action units, e. [sent-42, score-0.705]
</p><p>20 To sidestep these potential disadvantages of more complex models, we instead explore the potential of recent advances in robust feature pooling strategies developed in the object recognition literature. [sent-48, score-0.158]
</p><p>21 In particular, in this paper we explore the potential of the Fisher vector (FV) encoding [35] as a robust feature pooling technique that has proven to be among the most effective for object recognition [3]. [sent-49, score-0.229]
</p><p>22 As low-level features we use the dense motion boundary histogram (MBH) features of [41],  and evaluate the effect of adding SIFT descriptors to encode appearance information not captured by MBH. [sent-50, score-0.294]
</p><p>23 While recently FVs have been explored by others for action recognition [39, 44], we are the first to use them in a large, diverse, and comprehensive evaluation. [sent-51, score-0.292]
</p><p>24 [11] complemented the dense trajectory descriptors with new features computed from optical flow, and encode them using vectors of aggregated local descriptors (VLAD), a simplified version of the Fisher vector. [sent-53, score-0.293]
</p><p>25 First, we consider the classification of basic action categories using five of the most challenging recent datasets. [sent-56, score-0.295]
</p><p>26 Second, we consider the localization of actions in feature length movies, using the four actions drinking, smoking, sit down, and open door from [4, 18]. [sent-57, score-0.542]
</p><p>27 Third, we consider classification of more high-level complex event categories using the TrecVid MED 2011 dataset [29]. [sent-58, score-0.227]
</p><p>28 For action localization in full length movies we also propose a modified non-maximumsuppression technique that avoids a bias towards selecting shorter segments. [sent-60, score-0.44]
</p><p>29 Video representation  ×  In this section we first present our feature extraction and encoding pipeline. [sent-66, score-0.114]
</p><p>30 Then, we discuss how we include weak location information of local features, and finally we discuss non-maximum suppression for action localization. [sent-67, score-0.283]
</p><p>31 Feature extraction  We encode the low level visual content using static appearance features as well as motion features. [sent-70, score-0.138]
</p><p>32 We compute SIFT descriptors every tenth video frame, at multiple scales on a dense grid (21 21 patches daeto o4f pixel steps alntidp 5e ssccaalleess)o. [sent-74, score-0.145]
</p><p>33 We capture motion information using the recently introduced dense trajectory Motion Boundary Histogram (MBH) features of [41],1 with default parameters: trajectories of length 15 frames extracted on a dense grid with 5 pixel spacing. [sent-75, score-0.288]
</p><p>34 The MBH feature is similar to SIFT, but computes gradient orientation histograms over both the vertical and horizontal spatial derivatives of the optical flow. [sent-76, score-0.114]
</p><p>35 Feature encoding Once the two local low-level features sets are extracted, we use them to construct a signature to characterize the video. [sent-81, score-0.2]
</p><p>36 For this step we use the Fisher vector (FV) representation [35], which was found to be the most effective one in a recent evaluation study of feature pooling techniques for object recognition [3], which included FVs, bag-of-words, sparse coding techniques, and several variations thereof. [sent-82, score-0.142]
</p><p>37 This leads to a signature with dimension K(2D + 1) for K quantization cells and D dimensional descriptors. [sent-86, score-0.151]
</p><p>38 use inr i lpe s f r a l e to paper an corrects  1188 11 88  more information is stored per cell, a smaller number of quantization cells can be used than for BoV. [sent-91, score-0.097]
</p><p>39 As the assignment of local descriptors to quantization cells is the main computational cost, the FV signature is faster to compute. [sent-92, score-0.214]
</p><p>40 Local descriptors are then assigned not only to a single quantization cell, but in a weighted manner to multiple clusters using the posterior component probability given the descriptor. [sent-94, score-0.109]
</p><p>41 For the MBH features we use the first option, since the local features overlap in time. [sent-110, score-0.145]
</p><p>42 The code to aggregate the MBH features in-memory into FVs, and to add SPM and SFV, is available online at http : / / lear . [sent-118, score-0.152]
</p><p>43 Non-maximum-suppression for localization For the action localization task we employ a temporal sliding window approach. [sent-123, score-0.424]
</p><p>44 In practice, we use candidate windows of length 30, 60, 90, and 120 frames, and slide the windows in steps of 30 frames. [sent-126, score-0.147]
</p><p>45 This effect is due to the fact that if a relatively long action appears, it is likely that there are short candidate windows that just contain the most characteristic features for the action. [sent-128, score-0.396]
</p><p>46 Longer windows might better cover the action, but are likely to include less characteristic features (even if they lead to positive classification by themselves), and might include background features due to imperfect temporal alignment. [sent-129, score-0.25]
</p><p>47 The Hollywood2 [23] dataset is used for a detailed evaluation of the feature encoding parameters. [sent-142, score-0.139]
</p><p>48 This dataset contains clips of 12 action categories which have been collected from movies. [sent-143, score-0.295]
</p><p>49 Across all actions there are 810 training samples and 884 test samples; the train and test clips have been selected from different movies. [sent-144, score-0.161]
</p><p>50 For a comparison to the state of the art we also present experimental results on four of the most challenging action 11881199  recognition datasets: UCF50 [33], HMDB51 [16], YouTube [21], and Olympics [28]. [sent-146, score-0.32]
</p><p>51 The first dataset we consider for action localization is based on the movie Coffee and Cigarettes, and contains annotations for the actions drinking and smoking [18]. [sent-150, score-0.662]
</p><p>52 Additional training examples (32 and 8 respectively) come from  the movie Sea of Love, and another 33 lab-recorded drinking examples are included. [sent-152, score-0.124]
</p><p>53 The test sets consist of about 20 minutes from Coffee and Cigarettes for drinking, with 38 positive examples; for smoking a sequence of about 18 minutes is used that contains 42 positive examples. [sent-153, score-0.116]
</p><p>54 [4] contains annotations for the actions sit down, and open door. [sent-155, score-0.233]
</p><p>55 The test data contains three full movies (Living in Oblivion, The Crying Game, and The Graduate), which in total last for about 250 minutes, and contain 86 sit down, and 91 open door samples. [sent-157, score-0.196]
</p><p>56 The 2011 dataset consists of consumer videos from 15 categories that are more complex than the basic actions considered in the other datasets, e. [sent-161, score-0.295]
</p><p>57 The number of extracted features is roughly proportional to the video size; therefore video rescaling lin-  early speeds up the feature extraction and encoding time. [sent-182, score-0.317]
</p><p>58 Our experiments show that FVs using 50 visual words are comparable to BoV histograms for 4000 visual words; confirming that for FVs fewer visual words are needed than for BoV histograms. [sent-209, score-0.117]
</p><p>59 This shows that FVs are more efficient for large-scale applications, since the feature encoding step is one of the main computational bottlenecks and it scales linearly with the dictionary size. [sent-210, score-0.114]
</p><p>60 to the state of the art in Table 2 on five action recognition datasets. [sent-221, score-0.32]
</p><p>61 The SIFT features perform significantly worse, and carry relatively little useful complementary information. [sent-223, score-0.148]
</p><p>62 The comparison to [41] shows the effectiveness of the FV representation: they used 4000 visual words with χ2RBF kernels and in addition to MBH also included HOG,  HOF and trajectory features as well as a spatio-temporal grid. [sent-224, score-0.134]
</p><p>63 [1] represent videos as a temporal sequence of poses and use an exemplar-based recognition at test time. [sent-228, score-0.136]
</p><p>64 [14] encode local motion patterns by matching patches across successive video frames, and aggregate the quatized motion patterns in a BoV reprensentation. [sent-230, score-0.222]
</p><p>65 [44] uses sparse coding with sum-pooling over the STIP+HOG/HOF features of [17], which they found to work slightly better than FVs (albeit using 64 times fewer visual words for the FVs). [sent-232, score-0.115]
</p><p>66 [12] use the dense trajectory features of [41] and use an extended BoV encoding over pairs oflocal features to explicitly cancel common (camera) motion patterns. [sent-234, score-0.335]
</p><p>67 Mathe and Sminchisescu [24] use multiple-kernel learning to combine 14 descriptors sampled on human attention maps with the dense trajectory features of [41]. [sent-237, score-0.202]
</p><p>68 Action localization experiments In our second set of experiments we consider the localization of four actions in feature length movies. [sent-248, score-0.344]
</p><p>69 On three of the four actions we obtain substantially better results, despite the fact that previous work used more elaborate techniques. [sent-255, score-0.161]
</p><p>70 For example, [13] relied on a person detector, while [6] requires finer annotations that indicate the position of characteristic moments of the actions (actoms). [sent-256, score-0.186]
</p><p>71 As for the action recognition datasets, we also find that the SIFT features carry little complementary information, and are actually detrimental when combined with the MBH features by late fusion. [sent-257, score-0.525]
</p><p>72 Event recognition experiments In our last set of experiments we consider the TrecVid MED 2011 event recognition dataset. [sent-261, score-0.223]
</p><p>73 [27] combine many features from different modalities, including audio features, and high-level features obtained from object detector responses, automatic speech recognition, and video text recognition. [sent-273, score-0.284]
</p><p>74 We did, however, experiment with adding audio features: the mel-frequency cepstral coefficients (MFCC) and their first and second derivatives [32]. [sent-275, score-0.109]
</p><p>75 With the inclusion of the audio features our results are comparable or better on eight of the ten categories, and also better on average. [sent-278, score-0.143]
</p><p>76 [39] also use FVs for dense trajectories, but include four types of descriptors (MBH, HOG, HOF and the shape of the trajectories) as well as use a spatial pyramid and a Gaussian kernel, whereas we only use FVs with MBH descriptors and linear classifiers, but use more visual words. [sent-285, score-0.185]
</p><p>77 Our results follow a similar trend as in the previous experiment, Table 5: the main gain is due to the SIFT descriptors (8% mAP) and adding the audio further increases the score by 4% mAP. [sent-287, score-0.146]
</p><p>78 Conclusions We presented an efficient action recognition system that combines three state-of-the-art low-level descriptors (MBH, SIFT, MFCC) with the recent Fisher vector representation. [sent-315, score-0.355]
</p><p>79 In our experimental evaluation we considered action recognition, action localization in movies, and complex event recognition. [sent-316, score-0.776]
</p><p>80 For the first two tasks, we observed that MBH motion features carry much more discriminative information than SIFT features, and that the latter bring little or no complementary information. [sent-317, score-0.198]
</p><p>81 A detailed evaluation on the Hollywood2 action recognition dataset showed the effectiveness and complementarity of SPM and SFV to include weak geometric information, and that FVs provide a more efficient feature encoding method than BoV histograms since fewer visual words are needed. [sent-318, score-0.551]
</p><p>82 We found that action localization results can be substantially improved by using a simple re-scoring technique before applying NMS, to suppress a bias for shorter windows. [sent-319, score-0.315]
</p><p>83 For recognition of event categories, we find that the SIFT features do bring useful contextual information, as do MFCC audio features. [sent-320, score-0.326]
</p><p>84 Our experimental evaluation is among the most extensive and diverse ones to date, including five of the most chal-  lenging action recognition benchmarks, action localization in feature length movies, and large-scale event recognition with a test set of more than 1,000 hours of video. [sent-321, score-0.872]
</p><p>85 Across all these datasets the combination of FVs with state-of-the-art descriptors outperforms the current state of the art, while using less features and less complex models. [sent-322, score-0.246]
</p><p>86 Therefore we believe that, currently, the presented system is the most effective one for deployment in large-scale action and event recognition problems, such as encountered in practice in broadcast archives or user-generated content archives. [sent-323, score-0.459]
</p><p>87 The devil is in the details: an evaluation of recent feature encoding methods. [sent-346, score-0.139]
</p><p>88 Auto-  [5]  [6] [7] [8]  [9]  [10] [11] [12]  [13]  [14]  [15] [16]  matic annotation of human actions in video. [sent-354, score-0.161]
</p><p>89 Object, scene and actions: combining multiple features for human action recognition. [sent-385, score-0.312]
</p><p>90 Recognizing complex events using large margin joint low-level event model. [sent-390, score-0.211]
</p><p>91 Trajectory-based modeling of human actions with motion reference points. [sent-406, score-0.211]
</p><p>92 Motion interchange patterns for action recognition in unconstrained videos. [sent-420, score-0.292]
</p><p>93 [17]  [18] [19]  [20]  [21] [22] [23] [24]  [25]  [26] [27]  HMDB: a large video database for human motion recognition. [sent-434, score-0.099]
</p><p>94 Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis. [sent-459, score-0.352]
</p><p>95 Dynamic eye movement datasets and learnt saliency models for visual action recognition. [sent-480, score-0.282]
</p><p>96 Representing pairwise spatial and temporal relations for action recognition. [sent-486, score-0.324]
</p><p>97 Multimodal feature fusion for robust event detection in web videos. [sent-502, score-0.171]
</p><p>98 Large-scale web video event classification by use of Fisher vectors. [sent-580, score-0.192]
</p><p>99 Dense trajectories and motion boundary descriptors for action recognition. [sent-594, score-0.402]
</p><p>100 A comparative study of encoding, pooling and normalization methods for action recognition. [sent-613, score-0.335]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mbh', 0.399), ('fvs', 0.359), ('bov', 0.348), ('sfv', 0.301), ('action', 0.252), ('fv', 0.195), ('actions', 0.161), ('med', 0.155), ('event', 0.143), ('spm', 0.128), ('nms', 0.113), ('fisher', 0.107), ('trecvid', 0.101), ('sift', 0.1), ('movies', 0.096), ('drinking', 0.09), ('mfcc', 0.09), ('encoding', 0.086), ('audio', 0.083), ('descriptors', 0.063), ('localization', 0.063), ('smoking', 0.062), ('features', 0.06), ('windows', 0.059), ('complementary', 0.059), ('laptev', 0.058), ('gaidon', 0.058), ('signature', 0.054), ('cells', 0.051), ('motion', 0.05), ('videos', 0.05), ('pooling', 0.049), ('video', 0.049), ('lear', 0.047), ('sit', 0.047), ('cigarettes', 0.046), ('trajectory', 0.046), ('quantization', 0.046), ('temporal', 0.046), ('rescaling', 0.045), ('aggregate', 0.045), ('categories', 0.043), ('normalizations', 0.041), ('complex', 0.041), ('recognition', 0.04), ('coffee', 0.038), ('aser', 0.037), ('trajectories', 0.037), ('mathe', 0.036), ('hof', 0.035), ('perronnin', 0.034), ('movie', 0.034), ('normalization', 0.034), ('histograms', 0.034), ('actom', 0.033), ('dense', 0.033), ('uncontrolled', 0.032), ('schmid', 0.032), ('speech', 0.032), ('natarajan', 0.032), ('recognizing', 0.032), ('setup', 0.031), ('weak', 0.031), ('izadinia', 0.031), ('tire', 0.031), ('gaussians', 0.03), ('kl', 0.03), ('duration', 0.03), ('marsza', 0.03), ('inri', 0.03), ('datasets', 0.03), ('carry', 0.029), ('length', 0.029), ('jain', 0.028), ('feature', 0.028), ('door', 0.028), ('words', 0.028), ('encode', 0.028), ('state', 0.028), ('minutes', 0.027), ('events', 0.027), ('six', 0.027), ('realistic', 0.027), ('brendel', 0.027), ('fewer', 0.027), ('duchenne', 0.026), ('derivatives', 0.026), ('proven', 0.026), ('harchaoui', 0.026), ('spatial', 0.026), ('overlap', 0.025), ('late', 0.025), ('anchez', 0.025), ('layout', 0.025), ('characteristic', 0.025), ('evaluation', 0.025), ('open', 0.025), ('cell', 0.025), ('encountered', 0.024), ('combination', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="40-tfidf-1" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>Author: Dan Oneata, Jakob Verbeek, Cordelia Schmid</p><p>Abstract: Action recognition in uncontrolled video is an important and challenging computer vision problem. Recent progress in this area is due to new local features and models that capture spatio-temporal structure between local features, or human-object interactions. Instead of working towards more complex models, we focus on the low-level features and their encoding. We evaluate the use of Fisher vectors as an alternative to bag-of-word histograms to aggregate a small set of state-of-the-art low-level descriptors, in combination with linear classifiers. We present a large and varied set of evaluations, considering (i) classification of short actions in five datasets, (ii) localization of such actions in feature-length movies, and (iii) large-scale recognition of complex events. We find that for basic action recognition and localization MBH features alone are enough for stateof-the-art performance. For complex events we find that SIFT and MFCC features provide complementary cues. On all three problems we obtain state-of-the-art results, while using fewer features and less complex models.</p><p>2 0.35042214 <a title="40-tfidf-2" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>Author: Heng Wang, Cordelia Schmid</p><p>Abstract: Recently dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets. This paper improves their performance by taking into account camera motion to correct them. To estimate camera motion, we match feature points between frames using SURF descriptors and dense optical flow, which are shown to be complementary. These matches are, then, used to robustly estimate a homography with RANSAC. Human motion is in general different from camera motion and generates inconsistent matches. To improve the estimation, a human detector is employed to remove these matches. Given the estimated camera motion, we remove trajectories consistent with it. We also use this estimation to cancel out camera motion from the optical flow. This significantly improves motion-based descriptors, such as HOF and MBH. Experimental results onfour challenging action datasets (i.e., Hollywood2, HMDB51, Olympic Sports and UCF50) significantly outperform the current state of the art.</p><p>3 0.24775569 <a title="40-tfidf-3" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>Author: Jiaming Guo, Zhuwen Li, Loong-Fah Cheong, Steven Zhiying Zhou</p><p>Abstract: Given a pair of videos having a common action, our goal is to simultaneously segment this pair of videos to extract this common action. As a preprocessing step, we first remove background trajectories by a motion-based figureground segmentation. To remove the remaining background and those extraneous actions, we propose the trajectory cosaliency measure, which captures the notion that trajectories recurring in all the videos should have their mutual saliency boosted. This requires a trajectory matching process which can compare trajectories with different lengths and not necessarily spatiotemporally aligned, and yet be discriminative enough despite significant intra-class variation in the common action. We further leverage the graph matching to enforce geometric coherence between regions so as to reduce feature ambiguity and matching errors. Finally, to classify the trajectories into common action and action outliers, we formulate the problem as a binary labeling of a Markov Random Field, in which the data term is measured by the trajectory co-saliency and the smooth- ness term is measured by the spatiotemporal consistency between trajectories. To evaluate the performance of our framework, we introduce a dataset containing clips that have animal actions as well as human actions. Experimental results show that the proposed method performs well in common action extraction.</p><p>4 0.23147744 <a title="40-tfidf-4" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>Author: Mihai Zanfir, Marius Leordeanu, Cristian Sminchisescu</p><p>Abstract: Human action recognition under low observational latency is receiving a growing interest in computer vision due to rapidly developing technologies in human-robot interaction, computer gaming and surveillance. In this paper we propose a fast, simple, yet powerful non-parametric Moving Pose (MP)frameworkfor low-latency human action and activity recognition. Central to our methodology is a moving pose descriptor that considers both pose information as well as differential quantities (speed and acceleration) of the human body joints within a short time window around the current frame. The proposed descriptor is used in conjunction with a modified kNN classifier that considers both the temporal location of a particular frame within the action sequence as well as the discrimination power of its moving pose descriptor compared to other frames in the training set. The resulting method is non-parametric and enables low-latency recognition, one-shot learning, and action detection in difficult unsegmented sequences. Moreover, the framework is real-time, scalable, and outperforms more sophisticated approaches on challenging benchmarks like MSR-Action3D or MSR-DailyActivities3D.</p><p>5 0.22334218 <a title="40-tfidf-5" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>Author: Ping Wei, Nanning Zheng, Yibiao Zhao, Song-Chun Zhu</p><p>Abstract: Action recognition has often been posed as a classification problem, which assumes that a video sequence only have one action class label and different actions are independent. However, a single human body can perform multiple concurrent actions at the same time, and different actions interact with each other. This paper proposes a concurrent action detection model where the action detection is formulated as a structural prediction problem. In this model, an interval in a video sequence can be described by multiple action labels. An detected action interval is determined both by the unary local detector and the relations with other actions. We use a wavelet feature to represent the action sequence, and design a composite temporal logic descriptor to describe the action relations. The model parameters are trained by structural SVM learning. Given a long video sequence, a sequential decision window search algorithm is designed to detect the actions. Experiments on our new collected concurrent action dataset demonstrate the strength of our method.</p><p>6 0.21398598 <a title="40-tfidf-6" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>7 0.21069013 <a title="40-tfidf-7" href="./iccv-2013-Feature_Weighting_via_Optimal_Thresholding_for_Video_Analysis.html">163 iccv-2013-Feature Weighting via Optimal Thresholding for Video Analysis</a></p>
<p>8 0.20585896 <a title="40-tfidf-8" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>9 0.19787207 <a title="40-tfidf-9" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>10 0.19435322 <a title="40-tfidf-10" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>11 0.19076228 <a title="40-tfidf-11" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>12 0.18865308 <a title="40-tfidf-12" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>13 0.17589836 <a title="40-tfidf-13" href="./iccv-2013-Finding_Actors_and_Actions_in_Movies.html">166 iccv-2013-Finding Actors and Actions in Movies</a></p>
<p>14 0.17031075 <a title="40-tfidf-14" href="./iccv-2013-Learning_to_Share_Latent_Tasks_for_Action_Recognition.html">249 iccv-2013-Learning to Share Latent Tasks for Action Recognition</a></p>
<p>15 0.16944814 <a title="40-tfidf-15" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>16 0.15855865 <a title="40-tfidf-16" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>17 0.1565022 <a title="40-tfidf-17" href="./iccv-2013-How_Related_Exemplars_Help_Complex_Event_Detection_in_Web_Videos%3F.html">203 iccv-2013-How Related Exemplars Help Complex Event Detection in Web Videos?</a></p>
<p>18 0.15556823 <a title="40-tfidf-18" href="./iccv-2013-From_Actemes_to_Action%3A_A_Strongly-Supervised_Representation_for_Detailed_Action_Understanding.html">175 iccv-2013-From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding</a></p>
<p>19 0.14497949 <a title="40-tfidf-19" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>20 0.14361748 <a title="40-tfidf-20" href="./iccv-2013-ACTIVE%3A_Activity_Concept_Transitions_in_Video_Event_Classification.html">4 iccv-2013-ACTIVE: Activity Concept Transitions in Video Event Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.244), (1, 0.23), (2, 0.127), (3, 0.268), (4, 0.031), (5, 0.067), (6, 0.112), (7, -0.081), (8, -0.031), (9, -0.03), (10, 0.041), (11, 0.005), (12, 0.043), (13, 0.017), (14, -0.001), (15, -0.059), (16, 0.046), (17, 0.052), (18, 0.062), (19, 0.015), (20, -0.01), (21, 0.03), (22, -0.011), (23, 0.03), (24, -0.005), (25, 0.067), (26, -0.041), (27, 0.047), (28, -0.003), (29, 0.036), (30, -0.02), (31, -0.014), (32, -0.036), (33, 0.045), (34, -0.034), (35, -0.069), (36, 0.029), (37, -0.113), (38, 0.006), (39, 0.065), (40, -0.066), (41, 0.008), (42, 0.017), (43, 0.127), (44, 0.016), (45, -0.025), (46, 0.004), (47, -0.116), (48, -0.043), (49, -0.089)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94713259 <a title="40-lsi-1" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>Author: Dan Oneata, Jakob Verbeek, Cordelia Schmid</p><p>Abstract: Action recognition in uncontrolled video is an important and challenging computer vision problem. Recent progress in this area is due to new local features and models that capture spatio-temporal structure between local features, or human-object interactions. Instead of working towards more complex models, we focus on the low-level features and their encoding. We evaluate the use of Fisher vectors as an alternative to bag-of-word histograms to aggregate a small set of state-of-the-art low-level descriptors, in combination with linear classifiers. We present a large and varied set of evaluations, considering (i) classification of short actions in five datasets, (ii) localization of such actions in feature-length movies, and (iii) large-scale recognition of complex events. We find that for basic action recognition and localization MBH features alone are enough for stateof-the-art performance. For complex events we find that SIFT and MFCC features provide complementary cues. On all three problems we obtain state-of-the-art results, while using fewer features and less complex models.</p><p>2 0.77943707 <a title="40-lsi-2" href="./iccv-2013-Feature_Weighting_via_Optimal_Thresholding_for_Video_Analysis.html">163 iccv-2013-Feature Weighting via Optimal Thresholding for Video Analysis</a></p>
<p>Author: Zhongwen Xu, Yi Yang, Ivor Tsang, Nicu Sebe, Alexander G. Hauptmann</p><p>Abstract: Fusion of multiple features can boost the performance of large-scale visual classification and detection tasks like TRECVID Multimedia Event Detection (MED) competition [1]. In this paper, we propose a novel feature fusion approach, namely Feature Weighting via Optimal Thresholding (FWOT) to effectively fuse various features. FWOT learns the weights, thresholding and smoothing parameters in a joint framework to combine the decision values obtained from all the individual features and the early fusion. To the best of our knowledge, this is the first work to consider the weight and threshold factors of fusion problem simultaneously. Compared to state-of-the-art fusion algorithms, our approach achieves promising improvements on HMDB [8] action recognition dataset and CCV [5] video classification dataset. In addition, experiments on two TRECVID MED 2011 collections show that our approach outperforms the state-of-the-art fusion methods for complex event detection.</p><p>3 0.75889468 <a title="40-lsi-3" href="./iccv-2013-Action_Recognition_with_Actons.html">38 iccv-2013-Action Recognition with Actons</a></p>
<p>Author: Jun Zhu, Baoyuan Wang, Xiaokang Yang, Wenjun Zhang, Zhuowen Tu</p><p>Abstract: With the improved accessibility to an exploding amount of video data and growing demands in a wide range of video analysis applications, video-based action recognition/classification becomes an increasingly important task in computer vision. In this paper, we propose a two-layer structure for action recognition to automatically exploit a mid-level “acton ” representation. The weakly-supervised actons are learned via a new max-margin multi-channel multiple instance learning framework, which can capture multiple mid-level action concepts simultaneously. The learned actons (with no requirement for detailed manual annotations) observe theproperties ofbeing compact, informative, discriminative, and easy to scale. The experimental results demonstrate the effectiveness ofapplying the learned actons in our two-layer structure, and show the state-ofthe-art recognition performance on two challenging action datasets, i.e., Youtube and HMDB51.</p><p>4 0.75458699 <a title="40-lsi-4" href="./iccv-2013-From_Actemes_to_Action%3A_A_Strongly-Supervised_Representation_for_Detailed_Action_Understanding.html">175 iccv-2013-From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding</a></p>
<p>Author: Weiyu Zhang, Menglong Zhu, Konstantinos G. Derpanis</p><p>Abstract: This paper presents a novel approach for analyzing human actions in non-scripted, unconstrained video settings based on volumetric, x-y-t, patch classifiers, termed actemes. Unlike previous action-related work, the discovery of patch classifiers is posed as a strongly-supervised process. Specifically, keypoint labels (e.g., position) across spacetime are used in a data-driven training process to discover patches that are highly clustered in the spacetime keypoint configuration space. To support this process, a new human action dataset consisting of challenging consumer videos is introduced, where notably the action label, the 2D position of a set of keypoints and their visibilities are provided for each video frame. On a novel input video, each acteme is used in a sliding volume scheme to yield a set of sparse, non-overlapping detections. These detections provide the intermediate substrate for segmenting out the action. For action classification, the proposed representation shows significant improvement over state-of-the-art low-level features, while providing spatiotemporal localiza- tion as additional output. This output sheds further light into detailed action understanding.</p><p>5 0.71959966 <a title="40-lsi-5" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>Author: Ping Wei, Nanning Zheng, Yibiao Zhao, Song-Chun Zhu</p><p>Abstract: Action recognition has often been posed as a classification problem, which assumes that a video sequence only have one action class label and different actions are independent. However, a single human body can perform multiple concurrent actions at the same time, and different actions interact with each other. This paper proposes a concurrent action detection model where the action detection is formulated as a structural prediction problem. In this model, an interval in a video sequence can be described by multiple action labels. An detected action interval is determined both by the unary local detector and the relations with other actions. We use a wavelet feature to represent the action sequence, and design a composite temporal logic descriptor to describe the action relations. The model parameters are trained by structural SVM learning. Given a long video sequence, a sequential decision window search algorithm is designed to detect the actions. Experiments on our new collected concurrent action dataset demonstrate the strength of our method.</p><p>6 0.71117747 <a title="40-lsi-6" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>7 0.71060717 <a title="40-lsi-7" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>8 0.6894809 <a title="40-lsi-8" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>9 0.67949843 <a title="40-lsi-9" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>10 0.67284554 <a title="40-lsi-10" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>11 0.66442931 <a title="40-lsi-11" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>12 0.66431081 <a title="40-lsi-12" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>13 0.64637649 <a title="40-lsi-13" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>14 0.63498676 <a title="40-lsi-14" href="./iccv-2013-ACTIVE%3A_Activity_Concept_Transitions_in_Video_Event_Classification.html">4 iccv-2013-ACTIVE: Activity Concept Transitions in Video Event Classification</a></p>
<p>15 0.6333276 <a title="40-lsi-15" href="./iccv-2013-Manipulation_Pattern_Discovery%3A_A_Nonparametric_Bayesian_Approach.html">260 iccv-2013-Manipulation Pattern Discovery: A Nonparametric Bayesian Approach</a></p>
<p>16 0.60301685 <a title="40-lsi-16" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>17 0.59643471 <a title="40-lsi-17" href="./iccv-2013-Latent_Multitask_Learning_for_View-Invariant_Action_Recognition.html">231 iccv-2013-Latent Multitask Learning for View-Invariant Action Recognition</a></p>
<p>18 0.59558207 <a title="40-lsi-18" href="./iccv-2013-Finding_Actors_and_Actions_in_Movies.html">166 iccv-2013-Finding Actors and Actions in Movies</a></p>
<p>19 0.5866127 <a title="40-lsi-19" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>20 0.56983089 <a title="40-lsi-20" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.074), (4, 0.046), (7, 0.012), (12, 0.013), (13, 0.049), (26, 0.073), (31, 0.049), (42, 0.066), (48, 0.011), (64, 0.094), (73, 0.02), (77, 0.012), (86, 0.1), (89, 0.27)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94350076 <a title="40-lda-1" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>Author: Dan Oneata, Jakob Verbeek, Cordelia Schmid</p><p>Abstract: Action recognition in uncontrolled video is an important and challenging computer vision problem. Recent progress in this area is due to new local features and models that capture spatio-temporal structure between local features, or human-object interactions. Instead of working towards more complex models, we focus on the low-level features and their encoding. We evaluate the use of Fisher vectors as an alternative to bag-of-word histograms to aggregate a small set of state-of-the-art low-level descriptors, in combination with linear classifiers. We present a large and varied set of evaluations, considering (i) classification of short actions in five datasets, (ii) localization of such actions in feature-length movies, and (iii) large-scale recognition of complex events. We find that for basic action recognition and localization MBH features alone are enough for stateof-the-art performance. For complex events we find that SIFT and MFCC features provide complementary cues. On all three problems we obtain state-of-the-art results, while using fewer features and less complex models.</p><p>2 0.93915975 <a title="40-lda-2" href="./iccv-2013-YouTube2Text%3A_Recognizing_and_Describing_Arbitrary_Activities_Using_Semantic_Hierarchies_and_Zero-Shot_Recognition.html">452 iccv-2013-YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition</a></p>
<p>Author: Sergio Guadarrama, Niveda Krishnamoorthy, Girish Malkarnenkar, Subhashini Venugopalan, Raymond Mooney, Trevor Darrell, Kate Saenko</p><p>Abstract: Despite a recent push towards large-scale object recognition, activity recognition remains limited to narrow domains and small vocabularies of actions. In this paper, we tackle the challenge of recognizing and describing activities “in-the-wild”. We present a solution that takes a short video clip and outputs a brief sentence that sums up the main activity in the video, such as the actor, the action and its object. Unlike previous work, our approach works on out-of-domain actions: it does not require training videos of the exact activity. If it cannot find an accurate prediction for a pre-trained model, it finds a less specific answer that is also plausible from a pragmatic standpoint. We use semantic hierarchies learned from the data to help to choose an appropriate level of generalization, and priors learned from web-scale natural language corpora to penalize unlikely combinations of actors/actions/objects; we also use a web-scale language model to “fill in ” novel verbs, i.e. when the verb does not appear in the training set. We evaluate our method on a large YouTube corpus and demonstrate it is able to generate short sentence descriptions of video clips better than baseline approaches.</p><p>3 0.93705356 <a title="40-lda-3" href="./iccv-2013-Coarse-to-Fine_Semantic_Video_Segmentation_Using_Supervoxel_Trees.html">76 iccv-2013-Coarse-to-Fine Semantic Video Segmentation Using Supervoxel Trees</a></p>
<p>Author: Aastha Jain, Shuanak Chatterjee, René Vidal</p><p>Abstract: We propose an exact, general and efficient coarse-to-fine energy minimization strategy for semantic video segmentation. Our strategy is based on a hierarchical abstraction of the supervoxel graph that allows us to minimize an energy defined at the finest level of the hierarchy by minimizing a series of simpler energies defined over coarser graphs. The strategy is exact, i.e., it produces the same solution as minimizing over the finest graph. It is general, i.e., it can be used to minimize any energy function (e.g., unary, pairwise, and higher-order terms) with any existing energy minimization algorithm (e.g., graph cuts and belief propagation). It also gives significant speedups in inference for several datasets with varying degrees of spatio-temporal continuity. We also discuss the strengths and weaknesses of our strategy relative to existing hierarchical approaches, and the kinds of image and video data that provide the best speedups.</p><p>4 0.92490679 <a title="40-lda-4" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>Author: Limin Wang, Yu Qiao, Xiaoou Tang</p><p>Abstract: This paper proposes motion atom and phrase as a midlevel temporal “part” for representing and classifying complex action. Motion atom is defined as an atomic part of action, and captures the motion information of action video in a short temporal scale. Motion phrase is a temporal composite of multiple motion atoms with an AND/OR structure, which further enhances the discriminative ability of motion atoms by incorporating temporal constraints in a longer scale. Specifically, given a set of weakly labeled action videos, we firstly design a discriminative clustering method to automatically discovera set ofrepresentative motion atoms. Then, based on these motion atoms, we mine effective motion phrases with high discriminative and representativepower. We introduce a bottom-upphrase construction algorithm and a greedy selection method for this mining task. We examine the classification performance of the motion atom and phrase based representation on two complex action datasets: Olympic Sports and UCF50. Experimental results show that our method achieves superior performance over recent published methods on both datasets.</p><p>5 0.92262453 <a title="40-lda-5" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>Author: Arash Vahdat, Kevin Cannons, Greg Mori, Sangmin Oh, Ilseo Kim</p><p>Abstract: We present a compositional model for video event detection. A video is modeled using a collection of both global and segment-level features and kernel functions are employed for similarity comparisons. The locations of salient, discriminative video segments are treated as a latent variable, allowing the model to explicitly ignore portions of the video that are unimportant for classification. A novel, multiple kernel learning (MKL) latent support vector machine (SVM) is defined, that is used to combine and re-weight multiple feature types in a principled fashion while simultaneously operating within the latent variable framework. The compositional nature of the proposed model allows it to respond directly to the challenges of temporal clutter and intra-class variation, which are prevalent in unconstrained internet videos. Experimental results on the TRECVID Multimedia Event Detection 2011 (MED11) dataset demonstrate the efficacy of the method.</p><p>6 0.92182994 <a title="40-lda-6" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>7 0.92048782 <a title="40-lda-7" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>8 0.91839063 <a title="40-lda-8" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>9 0.9178471 <a title="40-lda-9" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>10 0.91777307 <a title="40-lda-10" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>11 0.91771686 <a title="40-lda-11" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>12 0.91594326 <a title="40-lda-12" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>13 0.91521049 <a title="40-lda-13" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>14 0.91457695 <a title="40-lda-14" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>15 0.91407382 <a title="40-lda-15" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>16 0.91398227 <a title="40-lda-16" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>17 0.91350394 <a title="40-lda-17" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>18 0.91305035 <a title="40-lda-18" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>19 0.91219145 <a title="40-lda-19" href="./iccv-2013-Feature_Weighting_via_Optimal_Thresholding_for_Video_Analysis.html">163 iccv-2013-Feature Weighting via Optimal Thresholding for Video Analysis</a></p>
<p>20 0.91106427 <a title="40-lda-20" href="./iccv-2013-Higher_Order_Matching_for_Consistent_Multiple_Target_Tracking.html">200 iccv-2013-Higher Order Matching for Consistent Multiple Target Tracking</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
