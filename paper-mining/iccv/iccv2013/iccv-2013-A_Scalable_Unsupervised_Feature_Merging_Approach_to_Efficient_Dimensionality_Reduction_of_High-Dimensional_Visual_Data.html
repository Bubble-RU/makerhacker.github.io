<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-29" href="#">iccv2013-29</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</h1>
<br/><p>Source: <a title="iccv-2013-29-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Liu_A_Scalable_Unsupervised_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Lingqiao Liu, Lei Wang</p><p>Abstract: To achieve a good trade-off between recognition accuracy and computational efficiency, it is often needed to reduce high-dimensional visual data to medium-dimensional ones. For this task, even applying a simple full-matrixbased linear projection causes significant computation and memory use. When the number of visual data is large, how to efficiently learn such a projection could even become a problem. The recent feature merging approach offers an efficient way to reduce the dimensionality, which only requires a single scan of features to perform reduction. However, existing merging algorithms do not scale well with highdimensional data, especially in the unsupervised case. To address this problem, we formulate unsupervised feature merging as a PCA problem imposed with a special structure constraint. By exploiting its connection with kmeans, we transform this constrained PCA problem into a feature clustering problem. Moreover, we employ the hashing technique to improve its scalability. These produce a scalable feature merging algorithm for our dimensional- ity reduction task. In addition, we develop an extension of this method by leveraging the neighborhood structure in the data to further improve dimensionality reduction performance. In further, we explore the incorporation of bipolar merging a variant of merging function which allows the subtraction operation into our algorithms. Through three applications in visual recognition, we demonstrate that our methods can not only achieve good dimensionality reduction performance with little computational cost but also help to create more powerful representation at both image level and local feature level. – –</p><p>Reference: <a title="iccv-2013-29-reference" href="../iccv2013_reference/iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 For this task, even applying a simple full-matrixbased linear projection causes significant computation and memory use. [sent-6, score-0.198]
</p><p>2 The recent feature merging approach offers an efficient way to reduce the dimensionality, which only requires a single scan of features to perform reduction. [sent-8, score-0.611]
</p><p>3 However, existing merging algorithms do not scale well with highdimensional data, especially in the unsupervised case. [sent-9, score-0.657]
</p><p>4 To address this problem, we formulate unsupervised feature merging as a PCA problem imposed with a special structure constraint. [sent-10, score-0.705]
</p><p>5 Moreover, we employ the hashing technique to improve its scalability. [sent-12, score-0.214]
</p><p>6 These produce a scalable feature merging algorithm for our dimensional-  ity reduction task. [sent-13, score-0.88]
</p><p>7 In addition, we develop an extension of this method by leveraging the neighborhood structure in the data to further improve dimensionality reduction performance. [sent-14, score-0.602]
</p><p>8 In further, we explore the incorporation of bipolar merging a variant of merging function which allows the subtraction operation into our algorithms. [sent-15, score-1.305]
</p><p>9 Through three applications in visual recognition, we demonstrate that our methods can not only achieve good dimensionality reduction performance with little computational cost but also help to create more powerful representation at both image level and local feature level. [sent-16, score-0.626]
</p><p>10 highdimensional histograms or pooled coding vectors [2], le iw@uow . [sent-20, score-0.158]
</p><p>11 au Fisher vectors [15] and histogram of local binary patterns with a large-sized neighborhood [7]. [sent-22, score-0.142]
</p><p>12 Although higherdimensional representations help achieving better recognition performance, it is often desirable to reduce the dimensionality for the sake of saving computational load and memory usage. [sent-23, score-0.493]
</p><p>13 However,  ×  this requirement makes traditional dimensionality reduction methods, even the simple full-matrix linear projection, inefficient in performing the reduction in terms of computational load and memory usage. [sent-27, score-1.033]
</p><p>14 For instance, the above example needs one million multiplication operations to project a single sample and incurs 500MB memory to save the double-precision projection matrix. [sent-28, score-0.271]
</p><p>15 As a result, the dimensionality reduction may become unaffordable for the applications requiring real-time performance, e. [sent-29, score-0.542]
</p><p>16 Moreover, when both the number of feature dimensions and samples are large, how to efficiently learn the reduction function will become a problem. [sent-34, score-0.374]
</p><p>17 The recent feature merging approach offers an efficient way to perform the dimensionality reduction. [sent-35, score-0.831]
</p><p>18 The idea of feature merging is to group features into clusters and use the sum of the features within each cluster as each reduced feature. [sent-36, score-0.681]
</p><p>19 Compared with traditional dimensionality reduction, merging is much more efficient in performing the reduction and incurs much lower memory usage. [sent-38, score-1.241]
</p><p>20 It only needs a scan over all D features to accomplish the reduction and a 1 D sized table to store the merging indexes. [sent-39, score-0.808]
</p><p>21 However, existing dfe taatbulere merging, especially tnhdee unsupervised merging algorithms do not scale well with highdimensional data. [sent-40, score-0.657]
</p><p>22 Their poor scalability is caused by two  factors: (1) the clustering frameworks adopted in these methods incur super-linear computational load growth with 333000000888  the feature dimensions, e. [sent-41, score-0.24]
</p><p>23 (2) besides the scalability issue in their clustering frameworks, the existing unsupervised merging algorithms [10, 11] require the calculation of pairwise feature similarity by some measures whose computational cost grows linearly with the number of samples, e. [sent-44, score-0.826]
</p><p>24 Compared with supervised merging, unsupervised merging has much wider applications (e. [sent-48, score-0.634]
</p><p>25 To overcome this problem, in this paper we propose an unsupervised feature merging algorithm which scales well with both high feature dimensionality and large sample size. [sent-52, score-0.98]
</p><p>26 In this algorithm, we formulate unsupervised feature merging as a principal component analysis (PCA) problem imposed with a special structure constraint. [sent-53, score-0.705]
</p><p>27 To handle this situation, we employ the hashing technique to improve its scalability. [sent-56, score-0.214]
</p><p>28 The combination of the above two techniques finally gives us a scalable feature merging algorithm. [sent-57, score-0.611]
</p><p>29 Also, inspired by kernel alignment which is commonly used in supervised learning, we further propose an extension of the above basic method. [sent-58, score-0.2]
</p><p>30 More specifically, we integrate the neighborhood structure in data to generate ‘pseudo-supervised’ information and utilize this information with kernel alignment to formulate a new merging function learning problem, which can be efficiently solved by our basic algorithm with a slight modification. [sent-59, score-0.725]
</p><p>31 In addition, we explore to incorporate a variant of merging function called bipolar merging which allows the subtraction operation in the merging process. [sent-60, score-1.844]
</p><p>32 To show the significance of our methods, we introduce three applications dimensionality reduction of the bag-offeatures (BoF) based image representation, learning better compact local binary pattern and dimensionality reduction of high-dimensional local features. [sent-61, score-1.097]
</p><p>33 Through the comparison with other alternatives in each application, we demonstrate that our methods can not only achieve good dimensionality reduction performance with little computational cost but also help to create more powerful representation at both image level and local feature level. [sent-62, score-0.626]
</p><p>34 Related Work Dimensionality reduction is a classic topic in machine learning and has many applications in computer vision. [sent-64, score-0.269]
</p><p>35 Related to efficient dimensionality reduction, the existing methods [5, 1] mainly focus on the time and space complexity of learning the reduction function rather than that of performing the reduction. [sent-66, score-0.554]
</p><p>36 For example, the popular random projection method [1] almost has zero cost in learning the projection function but still has high storage and computational complexity in performing the reduction. [sent-67, score-0.189]
</p><p>37 One recent work on reducing the complexity in performing dimensionality reduction is Hashing [16]. [sent-68, score-0.554]
</p><p>38 It calculates the projection matrix via hash function and results in a algorithm which has both low time and space complexity in performing the reduction. [sent-69, score-0.162]
</p><p>39 Feature merging can be categorized into supervised and unsupervised approaches. [sent-70, score-0.634]
</p><p>40 Examples of supervised merging algorithms include the methods in [19, 18, 6, 13]. [sent-71, score-0.566]
</p><p>41 The use of feature merging as an efficient dimensionality reduction is well demonstrated in [6], where the merging reduction is employed to accelerate the integral histogram calculation for object detection. [sent-72, score-1.964]
</p><p>42 Unsupervised merging algorithms have been also developed in [11, 10]. [sent-73, score-0.539]
</p><p>43 The method in [10] directly extends the supervised merging method in [6] by substituting the class-conditional probability with the probability of a word occurring in each image. [sent-74, score-0.566]
</p><p>44 This substitution makes the pairwise feature similarity calculation required in its hierarchical clustering framework grow linearly with the sample size. [sent-75, score-0.179]
</p><p>45 As discussed in the intro-  duction section, this feature similarity evaluation together with the inefficient clustering framework significantly affect their scalability and limit their potential applications. [sent-77, score-0.185]
</p><p>46 The Basic Method Formulation: Our basic algorithm is inspired by PCA the most popular unsupervised dimensionality reduction method. [sent-81, score-0.627]
</p><p>47 In a nutshell, we express the merging operation as a linear projection operator with a special structure constraint and try to incorporate this structure constraint into the PCA formulation. [sent-82, score-0.651]
</p><p>48 Let Y ∈ Rd×N represent the data obtained by applying tehte merging ? [sent-92, score-0.575]
</p><p>49 This merging operation is 333000000999  also equivalent to multiplying X with a special structured W: (1) each entry of W is either ‘0’ or ‘ 1’ . [sent-98, score-0.641]
</p><p>50 Fortunately, if we slightly modify the merging function, we could arrive at a formulation which is equivalent to k-means cluster? [sent-110, score-0.566]
</p><p>51 Translating this merging function into the structure constraint on W, we obtain:  W ∈ Ω = {W|wi,j∈ {0,√1λi}  ∀i, WWT= I}  (2)  λi = ? [sent-115, score-0.539]
</p><p>52 (2) Applying the proposed normalization factor √1|Gj| on the merging function is actually beneficial in the sense of preserving the pairwise data distance which is desirable for dimensionality reduction. [sent-133, score-0.787]
</p><p>53 eIf E we apply th deen toratedi etxiopneaclt merging fvu anrci--  ××  tion, the above ED after merging becomes 2Var{nxk} = 2tinon2,V tahre{x abko}v, eth EatD Dis a, tteher EmDer giisn enlarged by 2tVhea group }siz =e n. [sent-148, score-1.078]
</p><p>54 HVoarw{exve}r,, tihf we apply our sm eondlaifrigeedd merging rfouunpcti siozne, the ED after merging will be = 2nVar{xk} which is identical to the one before merging. [sent-149, score-1.078]
</p><p>55 Thus, we can see that for the traditional merging function the distance between the merged data will be distorted by the different group sizes but the proposed normalization factor can alleviate this issue. [sent-150, score-0.567]
</p><p>56 2Var{n√xnk}  Algorithm 1Our basic merging algorithm 1: 2: 3: 4: 5: 6: 7: 8:  Set S ← 0, where 0 is a ds D all ‘0’ matrix. [sent-153, score-0.623]
</p><p>57 s(k :) h← s(k, ) ,+ α αxi end for end for Run k-means on S to obtain the merging indexes. [sent-155, score-0.539]
</p><p>58 ,  −  :  Handle large N with hashing: To handle the issue in remark (3), we propose to adopt hashing [16] to reduce the dimensionality of XT. [sent-156, score-0.514]
</p><p>59 Its advantage is that the projection matrix can be analytically worked out so we do not need to allocate memory for storing the projection matrix. [sent-158, score-0.249]
</p><p>60 We use the hashing function provided by the work in [16]. [sent-162, score-0.214]
</p><p>61 s is the seed of hashing function and different seeds define different hashing functions. [sent-163, score-0.455]
</p><p>62 Applying H to reduce the dimensionality of XT, we obtained a matrix S = HXT ∈ Rds×D. [sent-166, score-0.302]
</p><p>63 = our }b a vsiica merging msethod is shown in Algorithm 1. [sent-185, score-0.539]
</p><p>64 To evaluate this alignment, we choose unnormalized centralized kernel alignment [3] as the objective: max? [sent-194, score-0.178]
</p><p>65 Thus we choose the unnormalized centralized kernel alignment which has also shown good performance in [3]. [sent-208, score-0.178]
</p><p>66 To handle this problem, we can firstly apply the basic method to obtain intermediate lower-dimensional data and then construct the knn graph from it. [sent-223, score-0.138]
</p><p>67 After that, we can run the PKA algorithm to attain the final reduction function. [sent-224, score-0.269]
</p><p>68 Exploring Bipolar Merging Function Traditional merging functions only allow the addition operation. [sent-229, score-0.539]
</p><p>69 We call this scheme of feature merging ‘Bipolar Merging’ . [sent-239, score-0.608]
</p><p>70 Application I: Dimensionality Reduction for BoF Based Image Representation Problem Introduction: Large-sized codebooks and the use of Spatial Pyramid (SPM) often make the dimensionality of BoF image representation very high. [sent-313, score-0.248]
</p><p>71 In this application, we evaluate various reduction methods via the classification accuracy after reduction, time of performing reduction on whole dataset and the memory usage. [sent-314, score-0.706]
</p><p>72 Seven methods are compared, including PCA, Hashing [16], AIB [10], the proposed basic merging algorithm (BSC in short), PKA and their bipolar variants (BSCB, PKAB in short). [sent-321, score-0.75]
</p><p>73 For PKA and PKAB, we firstly create the intermediate representation by using our BSC algorithm to reduce the dimensionality (to 200 for Scene 15 and 4000 for Pascal). [sent-322, score-0.364]
</p><p>74 Then we build the knn graph based on the intermediate features and re-run the dimensionality reduction with PKA and PKAB. [sent-323, score-0.588]
</p><p>75 As seen: (1) Merging-based reduction requires much less computational time than Hashing and PCA. [sent-327, score-0.299]
</p><p>76 For merging and Hashing, their computational time is independent of the reduced dimensionality while the time used by PCA linearly increases with it. [sent-328, score-0.874]
</p><p>77 (3) In terms of classification performance, PCA tends to perform slightly better when the reduced dimensionality is low while our methods outperform PCA at higher dimensions. [sent-333, score-0.335]
</p><p>78 This is because hashing cannot leverage any data-dependent information for dimensionality reduction as our methods do. [sent-339, score-0.731]
</p><p>79 This is probably because Pascal-2007 is a challenging dataset and the neighborhood of a sample obtained via low-dimensional intermediate features is less likely to share the same class label. [sent-343, score-0.159]
</p><p>80 However, a larger neighborhood will exponentially increase the number of possible binary patterns and result in very high-dimensional histograms (the number of possible patterns for LBP-D5 is 216 = 65536). [sent-359, score-0.157]
</p><p>81 This motivates us to apply our merging methods to this problem since they can exploit the co-occurrence information between features. [sent-368, score-0.539]
</p><p>82 Experimental Setting: We compare the performance of our methods with VQ, PCA and Hashing in reducing the dimensionality of LBP-D5 histogram. [sent-369, score-0.248]
</p><p>83 In particular, to test their generalization performance we learn the reduction function (PCA projection and merging indexes) on a separate dataset and evaluate the classification performance on Scene-15. [sent-389, score-0.899]
</p><p>84 As expected, the performance of hashing is not good enough due to its data-independent nature. [sent-395, score-0.214]
</p><p>85 For example, it will be difficult to load a large number of HDLF into memory to learn a dictionary (20K LBP-D5 feature will need 9. [sent-409, score-0.223]
</p><p>86 Moreover, since each image generates a large number of HDLF, the efficiency of performing dimensionality reduction becomes very important. [sent-412, score-0.554]
</p><p>87 Thus, the proposed merging methods will have significant advantages. [sent-413, score-0.539]
</p><p>88 How to choose the dimensionality reduction methods for this task? [sent-415, score-0.517]
</p><p>89 6B Table 3: Classification performance of the LBP baselines and the methods using HDLF-II with different dimensionality reduction approaches on Scene-15 dataset. [sent-424, score-0.517]
</p><p>90 To extract HDLF-I, We firstly follow [22] to extract color-SIFT feature and encode them by LSC coding [12] with a 1000-word dictionary. [sent-432, score-0.144]
</p><p>91 n Wcoed eth tehne e re lodcuacle g ietsdimensionality to 512 and encode it with sparse coding [21]  ×  since we find that it significantly outperforms other coding methods in encoding HDLF-I. [sent-435, score-0.15]
</p><p>92 Again, its dimensionality will be reduced to 512 for later processing. [sent-438, score-0.305]
</p><p>93 We emphasize that this achievement relies on the right choice of dimensionality reduction method. [sent-447, score-0.517]
</p><p>94 In contrast, the other three dimensionality reduction methods (PCA, VQ and Hashing) fail to maintain the good descriptive power of LBP-D5 after the reduction. [sent-460, score-0.517]
</p><p>95 Discussion The bipolar merging is designed to further group the negatively correlated features together and thus ‘save more dimensions’ for the informative features. [sent-464, score-0.755]
</p><p>96 However, in the above experiments, we do not observe significant improvement over the BSC and PKA by using their bipolar variants. [sent-465, score-0.169]
</p><p>97 This result and the comparable performance of BSC and BSCB in the previous experiments also suggest that utilizing bipolar merging function is a safe choice: it will not decrease the performance and may obtain additional improvement in certain circumstances. [sent-474, score-0.708]
</p><p>98 Conclusion In this paper, we propose a scalable unsupervised merging algorithm and one extension to achieve the efficient dimensionality reduction. [sent-476, score-0.919]
</p><p>99 Random projection in dimensionality reduction: applications to image and text data. [sent-483, score-0.309]
</p><p>100 Fast and efficient dimensionality reduction using structurally random matrices. [sent-513, score-0.517]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('merging', 0.539), ('pka', 0.304), ('reduction', 0.269), ('dimensionality', 0.248), ('hashing', 0.214), ('hdlf', 0.183), ('bsc', 0.183), ('bipolar', 0.169), ('spm', 0.149), ('pca', 0.127), ('bscb', 0.114), ('memory', 0.101), ('gj', 0.1), ('vq', 0.094), ('pkab', 0.092), ('coding', 0.075), ('lsc', 0.071), ('wma', 0.069), ('wxxtwt', 0.069), ('trace', 0.069), ('unsupervised', 0.068), ('clustering', 0.067), ('lbp', 0.064), ('projection', 0.061), ('bof', 0.059), ('reduced', 0.057), ('centralized', 0.056), ('wwt', 0.053), ('load', 0.052), ('alignment', 0.051), ('highdimensional', 0.05), ('neighborhood', 0.049), ('scalability', 0.047), ('negatively', 0.047), ('incurs', 0.047), ('aib', 0.046), ('hxt', 0.046), ('probably', 0.045), ('xt', 0.045), ('kernel', 0.044), ('feature', 0.044), ('knn', 0.043), ('signature', 0.043), ('ds', 0.042), ('basic', 0.042), ('cluster', 0.041), ('rds', 0.041), ('patterns', 0.04), ('mwax', 0.038), ('hash', 0.038), ('usage', 0.037), ('sample', 0.037), ('performing', 0.037), ('extension', 0.036), ('dimensions', 0.036), ('applying', 0.036), ('flipped', 0.035), ('jegou', 0.035), ('compact', 0.035), ('create', 0.035), ('saving', 0.034), ('pooled', 0.033), ('indexes', 0.033), ('subtraction', 0.032), ('calculate', 0.031), ('calculation', 0.031), ('xk', 0.03), ('classification', 0.03), ('computational', 0.03), ('imposed', 0.029), ('merged', 0.028), ('scalable', 0.028), ('binary', 0.028), ('reduce', 0.028), ('intermediate', 0.028), ('inefficient', 0.027), ('seeds', 0.027), ('supervised', 0.027), ('equivalent', 0.027), ('unnormalized', 0.027), ('targeted', 0.027), ('ww', 0.027), ('operation', 0.026), ('impractical', 0.026), ('membership', 0.026), ('matrix', 0.026), ('dictionary', 0.026), ('become', 0.025), ('call', 0.025), ('histogram', 0.025), ('rd', 0.025), ('pth', 0.025), ('firstly', 0.025), ('dimension', 0.025), ('save', 0.025), ('implementing', 0.025), ('special', 0.025), ('entry', 0.024), ('remark', 0.024), ('ith', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999869 <a title="29-tfidf-1" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>Author: Lingqiao Liu, Lei Wang</p><p>Abstract: To achieve a good trade-off between recognition accuracy and computational efficiency, it is often needed to reduce high-dimensional visual data to medium-dimensional ones. For this task, even applying a simple full-matrixbased linear projection causes significant computation and memory use. When the number of visual data is large, how to efficiently learn such a projection could even become a problem. The recent feature merging approach offers an efficient way to reduce the dimensionality, which only requires a single scan of features to perform reduction. However, existing merging algorithms do not scale well with highdimensional data, especially in the unsupervised case. To address this problem, we formulate unsupervised feature merging as a PCA problem imposed with a special structure constraint. By exploiting its connection with kmeans, we transform this constrained PCA problem into a feature clustering problem. Moreover, we employ the hashing technique to improve its scalability. These produce a scalable feature merging algorithm for our dimensional- ity reduction task. In addition, we develop an extension of this method by leveraging the neighborhood structure in the data to further improve dimensionality reduction performance. In further, we explore the incorporation of bipolar merging a variant of merging function which allows the subtraction operation into our algorithms. Through three applications in visual recognition, we demonstrate that our methods can not only achieve good dimensionality reduction performance with little computational cost but also help to create more powerful representation at both image level and local feature level. – –</p><p>2 0.17335373 <a title="29-tfidf-2" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>Author: Zhongming Jin, Yao Hu, Yue Lin, Debing Zhang, Shiding Lin, Deng Cai, Xuelong Li</p><p>Abstract: Recently, hashing techniques have been widely applied to solve the approximate nearest neighbors search problem in many vision applications. Generally, these hashing approaches generate 2c buckets, where c is the length of the hash code. A good hashing method should satisfy the following two requirements: 1) mapping the nearby data points into the same bucket or nearby (measured by xue long l i opt . ac . cn @ a(a)b(b) the Hamming distance) buckets. 2) all the data points are evenly distributed among all the buckets. In this paper, we propose a novel algorithm named Complementary Projection Hashing (CPH) to find the optimal hashing functions which explicitly considers the above two requirements. Specifically, CPHaims at sequentiallyfinding a series ofhyperplanes (hashing functions) which cross the sparse region of the data. At the same time, the data points are evenly distributed in the hypercubes generated by these hyperplanes. The experiments comparing with the state-of-the-art hashing methods demonstrate the effectiveness of the proposed method.</p><p>3 0.17023939 <a title="29-tfidf-3" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>Author: Guangnan Ye, Dong Liu, Jun Wang, Shih-Fu Chang</p><p>Abstract: Recently, learning based hashing methods have become popular for indexing large-scale media data. Hashing methods map high-dimensional features to compact binary codes that are efficient to match and robust in preserving original similarity. However, most of the existing hashing methods treat videos as a simple aggregation of independent frames and index each video through combining the indexes of frames. The structure information of videos, e.g., discriminative local visual commonality and temporal consistency, is often neglected in the design of hash functions. In this paper, we propose a supervised method that explores the structure learning techniques to design efficient hash functions. The proposed video hashing method formulates a minimization problem over a structure-regularized empirical loss. In particular, the structure regularization exploits the common local visual patterns occurring in video frames that are associated with the same semantic class, and simultaneously preserves the temporal consistency over successive frames from the same video. We show that the minimization objective can be efficiently solved by an Acceler- ated Proximal Gradient (APG) method. Extensive experiments on two large video benchmark datasets (up to around 150K video clips with over 12 million frames) show that the proposed method significantly outperforms the state-ofthe-art hashing methods.</p><p>4 0.14182793 <a title="29-tfidf-4" href="./iccv-2013-A_General_Two-Step_Approach_to_Learning-Based_Hashing.html">13 iccv-2013-A General Two-Step Approach to Learning-Based Hashing</a></p>
<p>Author: Guosheng Lin, Chunhua Shen, David Suter, Anton van_den_Hengel</p><p>Abstract: Most existing approaches to hashing apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to respond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. This framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problemspecific hashing methods. Our framework decomposes the hashing learning problem into two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically be formulated as binary quadratic problems, and the second step can be accomplished by training standard binary classifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate that the proposed framework is effective, flexible and outperforms the state-of-the-art.</p><p>5 0.1167255 <a title="29-tfidf-5" href="./iccv-2013-Fast_High_Dimensional_Vector_Multiplication_Face_Recognition.html">158 iccv-2013-Fast High Dimensional Vector Multiplication Face Recognition</a></p>
<p>Author: Oren Barkan, Jonathan Weill, Lior Wolf, Hagai Aronowitz</p><p>Abstract: This paper advances descriptor-based face recognition by suggesting a novel usage of descriptors to form an over-complete representation, and by proposing a new metric learning pipeline within the same/not-same framework. First, the Over-Complete Local Binary Patterns (OCLBP) face representation scheme is introduced as a multi-scale modified version of the Local Binary Patterns (LBP) scheme. Second, we propose an efficient matrix-vector multiplication-based recognition system. The system is based on Linear Discriminant Analysis (LDA) coupled with Within Class Covariance Normalization (WCCN). This is further extended to the unsupervised case by proposing an unsupervised variant of WCCN. Lastly, we introduce Diffusion Maps (DM) for non-linear dimensionality reduction as an alternative to the Whitened Principal Component Analysis (WPCA) method which is often used in face recognition. We evaluate the proposed framework on the LFW face recognition dataset under the restricted, unrestricted and unsupervised protocols. In all three cases we achieve very competitive results.</p><p>6 0.10991157 <a title="29-tfidf-6" href="./iccv-2013-Latent_Space_Sparse_Subspace_Clustering.html">232 iccv-2013-Latent Space Sparse Subspace Clustering</a></p>
<p>7 0.10318388 <a title="29-tfidf-7" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>8 0.096802659 <a title="29-tfidf-8" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>9 0.090013683 <a title="29-tfidf-9" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<p>10 0.088122331 <a title="29-tfidf-10" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>11 0.08648248 <a title="29-tfidf-11" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>12 0.080494538 <a title="29-tfidf-12" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>13 0.080274105 <a title="29-tfidf-13" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>14 0.078241237 <a title="29-tfidf-14" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>15 0.077040389 <a title="29-tfidf-15" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>16 0.074856289 <a title="29-tfidf-16" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>17 0.07249523 <a title="29-tfidf-17" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>18 0.068220563 <a title="29-tfidf-18" href="./iccv-2013-Low-Rank_Sparse_Coding_for_Image_Classification.html">258 iccv-2013-Low-Rank Sparse Coding for Image Classification</a></p>
<p>19 0.066882804 <a title="29-tfidf-19" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>20 0.064182937 <a title="29-tfidf-20" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.17), (1, 0.064), (2, -0.062), (3, -0.056), (4, -0.088), (5, 0.148), (6, -0.004), (7, 0.039), (8, -0.076), (9, 0.018), (10, -0.049), (11, 0.028), (12, -0.024), (13, -0.058), (14, -0.006), (15, 0.006), (16, -0.027), (17, 0.032), (18, -0.021), (19, -0.001), (20, 0.009), (21, 0.03), (22, 0.032), (23, 0.029), (24, -0.023), (25, 0.027), (26, 0.013), (27, 0.022), (28, 0.007), (29, 0.084), (30, 0.025), (31, -0.021), (32, -0.038), (33, -0.023), (34, -0.029), (35, 0.029), (36, 0.05), (37, -0.085), (38, 0.008), (39, -0.014), (40, -0.012), (41, -0.027), (42, -0.019), (43, 0.043), (44, -0.028), (45, -0.076), (46, -0.013), (47, -0.004), (48, 0.054), (49, -0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92684108 <a title="29-lsi-1" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>Author: Lingqiao Liu, Lei Wang</p><p>Abstract: To achieve a good trade-off between recognition accuracy and computational efficiency, it is often needed to reduce high-dimensional visual data to medium-dimensional ones. For this task, even applying a simple full-matrixbased linear projection causes significant computation and memory use. When the number of visual data is large, how to efficiently learn such a projection could even become a problem. The recent feature merging approach offers an efficient way to reduce the dimensionality, which only requires a single scan of features to perform reduction. However, existing merging algorithms do not scale well with highdimensional data, especially in the unsupervised case. To address this problem, we formulate unsupervised feature merging as a PCA problem imposed with a special structure constraint. By exploiting its connection with kmeans, we transform this constrained PCA problem into a feature clustering problem. Moreover, we employ the hashing technique to improve its scalability. These produce a scalable feature merging algorithm for our dimensional- ity reduction task. In addition, we develop an extension of this method by leveraging the neighborhood structure in the data to further improve dimensionality reduction performance. In further, we explore the incorporation of bipolar merging a variant of merging function which allows the subtraction operation into our algorithms. Through three applications in visual recognition, we demonstrate that our methods can not only achieve good dimensionality reduction performance with little computational cost but also help to create more powerful representation at both image level and local feature level. – –</p><p>2 0.77411562 <a title="29-lsi-2" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>Author: Lixin Fan</p><p>Abstract: This paper proposes to learn binary hash codes within a statistical learning framework, in which an upper bound of the probability of Bayes decision errors is derived for different forms of hash functions and a rigorous proof of the convergence of the upper bound is presented. Consequently, minimizing such an upper bound leads to consistent performance improvements of existing hash code learning algorithms, regardless of whether original algorithms are unsupervised or supervised. This paper also illustrates a fast hash coding method that exploits simple binary tests to achieve orders of magnitude improvement in coding speed as compared to projection based methods.</p><p>3 0.73955572 <a title="29-lsi-3" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>Author: Zhongming Jin, Yao Hu, Yue Lin, Debing Zhang, Shiding Lin, Deng Cai, Xuelong Li</p><p>Abstract: Recently, hashing techniques have been widely applied to solve the approximate nearest neighbors search problem in many vision applications. Generally, these hashing approaches generate 2c buckets, where c is the length of the hash code. A good hashing method should satisfy the following two requirements: 1) mapping the nearby data points into the same bucket or nearby (measured by xue long l i opt . ac . cn @ a(a)b(b) the Hamming distance) buckets. 2) all the data points are evenly distributed among all the buckets. In this paper, we propose a novel algorithm named Complementary Projection Hashing (CPH) to find the optimal hashing functions which explicitly considers the above two requirements. Specifically, CPHaims at sequentiallyfinding a series ofhyperplanes (hashing functions) which cross the sparse region of the data. At the same time, the data points are evenly distributed in the hypercubes generated by these hyperplanes. The experiments comparing with the state-of-the-art hashing methods demonstrate the effectiveness of the proposed method.</p><p>4 0.73923033 <a title="29-lsi-4" href="./iccv-2013-A_General_Two-Step_Approach_to_Learning-Based_Hashing.html">13 iccv-2013-A General Two-Step Approach to Learning-Based Hashing</a></p>
<p>Author: Guosheng Lin, Chunhua Shen, David Suter, Anton van_den_Hengel</p><p>Abstract: Most existing approaches to hashing apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to respond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. This framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problemspecific hashing methods. Our framework decomposes the hashing learning problem into two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically be formulated as binary quadratic problems, and the second step can be accomplished by training standard binary classifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate that the proposed framework is effective, flexible and outperforms the state-of-the-art.</p><p>5 0.71124572 <a title="29-lsi-5" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>Author: Jun Wang, Wei Liu, Andy X. Sun, Yu-Gang Jiang</p><p>Abstract: Hashing techniques have been intensively investigated in the design of highly efficient search engines for largescale computer vision applications. Compared with prior approximate nearest neighbor search approaches like treebased indexing, hashing-based search schemes have prominent advantages in terms of both storage and computational efficiencies. Moreover, the procedure of devising hash functions can be easily incorporated into sophisticated machine learning tools, leading to data-dependent and task-specific compact hash codes. Therefore, a number of learning paradigms, ranging from unsupervised to supervised, have been applied to compose appropriate hash functions. How- ever, most of the existing hash function learning methods either treat hash function design as a classification problem or generate binary codes to satisfy pairwise supervision, and have not yet directly optimized the search accuracy. In this paper, we propose to leverage listwise supervision into a principled hash function learning framework. In particular, the ranking information is represented by a set of rank triplets that can be used to assess the quality of ranking. Simple linear projection-based hash functions are solved efficiently through maximizing the ranking quality over the training data. We carry out experiments on large image datasets with size up to one million and compare with the state-of-the-art hashing techniques. The extensive results corroborate that our learned hash codes via listwise supervision can provide superior search accuracy without incurring heavy computational overhead.</p><p>6 0.7007888 <a title="29-lsi-6" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>7 0.69107521 <a title="29-lsi-7" href="./iccv-2013-Low-Rank_Sparse_Coding_for_Image_Classification.html">258 iccv-2013-Low-Rank Sparse Coding for Image Classification</a></p>
<p>8 0.68468857 <a title="29-lsi-8" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<p>9 0.63419122 <a title="29-lsi-9" href="./iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning.html">212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</a></p>
<p>10 0.62110239 <a title="29-lsi-10" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>11 0.6195488 <a title="29-lsi-11" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>12 0.60699373 <a title="29-lsi-12" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>13 0.60618347 <a title="29-lsi-13" href="./iccv-2013-Fast_High_Dimensional_Vector_Multiplication_Face_Recognition.html">158 iccv-2013-Fast High Dimensional Vector Multiplication Face Recognition</a></p>
<p>14 0.60237867 <a title="29-lsi-14" href="./iccv-2013-Quadruplet-Wise_Image_Similarity_Learning.html">332 iccv-2013-Quadruplet-Wise Image Similarity Learning</a></p>
<p>15 0.59468007 <a title="29-lsi-15" href="./iccv-2013-Stacked_Predictive_Sparse_Coding_for_Classification_of_Distinct_Regions_in_Tumor_Histopathology.html">401 iccv-2013-Stacked Predictive Sparse Coding for Classification of Distinct Regions in Tumor Histopathology</a></p>
<p>16 0.58939809 <a title="29-lsi-16" href="./iccv-2013-Joint_Learning_of_Discriminative_Prototypes_and_Large_Margin_Nearest_Neighbor_Classifiers.html">222 iccv-2013-Joint Learning of Discriminative Prototypes and Large Margin Nearest Neighbor Classifiers</a></p>
<p>17 0.58883959 <a title="29-lsi-17" href="./iccv-2013-Quantize_and_Conquer%3A_A_Dimensionality-Recursive_Solution_to_Clustering%2C_Vector_Quantization%2C_and_Image_Retrieval.html">333 iccv-2013-Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval</a></p>
<p>18 0.58221132 <a title="29-lsi-18" href="./iccv-2013-SIFTpack%3A_A_Compact_Representation_for_Efficient_SIFT_Matching.html">365 iccv-2013-SIFTpack: A Compact Representation for Efficient SIFT Matching</a></p>
<p>19 0.57736564 <a title="29-lsi-19" href="./iccv-2013-From_Point_to_Set%3A_Extend_the_Learning_of_Distance_Metrics.html">177 iccv-2013-From Point to Set: Extend the Learning of Distance Metrics</a></p>
<p>20 0.57702726 <a title="29-lsi-20" href="./iccv-2013-Nested_Shape_Descriptors.html">288 iccv-2013-Nested Shape Descriptors</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.115), (4, 0.022), (7, 0.033), (13, 0.025), (24, 0.01), (26, 0.091), (31, 0.059), (42, 0.114), (48, 0.012), (64, 0.042), (73, 0.031), (78, 0.016), (85, 0.176), (89, 0.132), (95, 0.011), (98, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83041996 <a title="29-lda-1" href="./iccv-2013-Potts_Model%2C_Parametric_Maxflow_and_K-Submodular_Functions.html">324 iccv-2013-Potts Model, Parametric Maxflow and K-Submodular Functions</a></p>
<p>Author: Igor Gridchyn, Vladimir Kolmogorov</p><p>Abstract: The problem of minimizing the Potts energy function frequently occurs in computer vision applications. One way to tackle this NP-hard problem was proposed by Kovtun [20, 21]. It identifies a part of an optimal solution by running k maxflow computations, where k is the number of labels. The number of “labeled” pixels can be significant in some applications, e.g. 50-93% in our tests for stereo. We show how to reduce the runtime to O(log k) maxflow computations (or one parametric maxflow computation). Furthermore, the output of our algorithm allows to speed-up the subsequent alpha expansion for the unlabeled part, or can be used as it is for time-critical applications. To derive our technique, we generalize the algorithm of Felzenszwalb et al. [7] for Tree Metrics. We also show a connection to k-submodular functions from combinatorial optimization, and discuss k-submodular relaxations for general energy functions.</p><p>same-paper 2 0.81304502 <a title="29-lda-2" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>Author: Lingqiao Liu, Lei Wang</p><p>Abstract: To achieve a good trade-off between recognition accuracy and computational efficiency, it is often needed to reduce high-dimensional visual data to medium-dimensional ones. For this task, even applying a simple full-matrixbased linear projection causes significant computation and memory use. When the number of visual data is large, how to efficiently learn such a projection could even become a problem. The recent feature merging approach offers an efficient way to reduce the dimensionality, which only requires a single scan of features to perform reduction. However, existing merging algorithms do not scale well with highdimensional data, especially in the unsupervised case. To address this problem, we formulate unsupervised feature merging as a PCA problem imposed with a special structure constraint. By exploiting its connection with kmeans, we transform this constrained PCA problem into a feature clustering problem. Moreover, we employ the hashing technique to improve its scalability. These produce a scalable feature merging algorithm for our dimensional- ity reduction task. In addition, we develop an extension of this method by leveraging the neighborhood structure in the data to further improve dimensionality reduction performance. In further, we explore the incorporation of bipolar merging a variant of merging function which allows the subtraction operation into our algorithms. Through three applications in visual recognition, we demonstrate that our methods can not only achieve good dimensionality reduction performance with little computational cost but also help to create more powerful representation at both image level and local feature level. – –</p><p>3 0.79622042 <a title="29-lda-3" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>Author: Vidit Jain, Sachin Sudhakar Farfade</p><p>Abstract: Classification cascades have been very effective for object detection. Such a cascade fails to perform well in data domains with variations in appearances that may not be captured in the training examples. This limited generalization severely restricts the domains for which they can be used effectively. A common approach to address this limitation is to train a new cascade of classifiers from scratch for each of the new domains. Building separate detectors for each of the different domains requires huge annotation and computational effort, making it not scalable to a large number of data domains. Here we present an algorithm for quickly adapting a pre-trained cascade of classifiers using a small number oflabeledpositive instancesfrom a different yet similar data domain. In our experiments with images of human babies and human-like characters from movies, we demonstrate that the adapted cascade significantly outperforms both of the original cascade and the one trained from scratch using the given training examples. –</p><p>4 0.77737284 <a title="29-lda-4" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>Author: S. Karthikeyan, Vignesh Jagadeesh, Renuka Shenoy, Miguel Ecksteinz, B.S. Manjunath</p><p>Abstract: Eye movement studies have confirmed that overt attention is highly biased towards faces and text regions in images. In this paper we explore a novel problem of predicting face and text regions in images using eye tracking data from multiple subjects. The problem is challenging as we aim to predict the semantics (face/text/background) only from eye tracking data without utilizing any image information. The proposed algorithm spatially clusters eye tracking data obtained in an image into different coherent groups and subsequently models the likelihood of the clusters containing faces and text using afully connectedMarkov Random Field (MRF). Given the eye tracking datafrom a test image, itpredicts potential face/head (humans, dogs and cats) and text locations reliably. Furthermore, the approach can be used to select regions of interest for further analysis by object detectors for faces and text. The hybrid eye position/object detector approach achieves better detection performance and reduced computation time compared to using only the object detection algorithm. We also present a new eye tracking dataset on 300 images selected from ICDAR, Street-view, Flickr and Oxford-IIIT Pet Dataset from 15 subjects.</p><p>5 0.76894999 <a title="29-lda-5" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>Author: Hua Wang, Feiping Nie, Weidong Cai, Heng Huang</p><p>Abstract: Representing the raw input of a data set by a set of relevant codes is crucial to many computer vision applications. Due to the intrinsic sparse property of real-world data, dictionary learning, in which the linear decomposition of a data point uses a set of learned dictionary bases, i.e., codes, has demonstrated state-of-the-art performance. However, traditional dictionary learning methods suffer from three weaknesses: sensitivity to noisy and outlier samples, difficulty to determine the optimal dictionary size, and incapability to incorporate supervision information. In this paper, we address these weaknesses by learning a Semi-Supervised Robust Dictionary (SSR-D). Specifically, we use the ℓ2,0+ norm as the loss function to improve the robustness against outliers, and develop a new structured sparse regularization com, , tom. . cai@sydney . edu . au , heng@uta .edu make the learning tasks easier to deal with and reduce the computational cost. For example, in image tagging, instead of using the raw pixel-wise features, semi-local or patch- based features, such as SIFT and geometric blur, are usually more desirable to achieve better performance. In practice, finding a set of compact features bases, also referred to as dictionary, with enhanced representative and discriminative power, plays a significant role in building a successful computer vision system. In this paper, we explore this important problem by proposing a novel formulation and its solution for learning Semi-Supervised Robust Dictionary (SSRD), where we examine the challenges in dictionary learning, and seek opportunities to overcome them and improve the dictionary qualities. 1.1. Challenges in Dictionary Learning to incorporate the supervision information in dictionary learning, without incurring additional parameters. Moreover, the optimal dictionary size is automatically learned from the input data. Minimizing the derived objective function is challenging because it involves many non-smooth ℓ2,0+ -norm terms. We present an efficient algorithm to solve the problem with a rigorous proof of the convergence of the algorithm. Extensive experiments are presented to show the superior performance of the proposed method.</p><p>6 0.76859617 <a title="29-lda-6" href="./iccv-2013-Class-Specific_Simplex-Latent_Dirichlet_Allocation_for_Image_Classification.html">73 iccv-2013-Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification</a></p>
<p>7 0.76844841 <a title="29-lda-7" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>8 0.76727164 <a title="29-lda-8" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>9 0.76386416 <a title="29-lda-9" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>10 0.76362199 <a title="29-lda-10" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>11 0.76350403 <a title="29-lda-11" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>12 0.76275522 <a title="29-lda-12" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>13 0.76245624 <a title="29-lda-13" href="./iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation.html">383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</a></p>
<p>14 0.7603752 <a title="29-lda-14" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<p>15 0.75964874 <a title="29-lda-15" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>16 0.75914162 <a title="29-lda-16" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>17 0.75851279 <a title="29-lda-17" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>18 0.75822258 <a title="29-lda-18" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>19 0.75801444 <a title="29-lda-19" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<p>20 0.75788879 <a title="29-lda-20" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
