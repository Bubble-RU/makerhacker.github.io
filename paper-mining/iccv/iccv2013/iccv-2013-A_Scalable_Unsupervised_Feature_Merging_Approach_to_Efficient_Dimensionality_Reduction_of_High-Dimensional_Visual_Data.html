<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-29" href="#">iccv2013-29</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</h1>
<br/><p>Source: <a title="iccv-2013-29-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Liu_A_Scalable_Unsupervised_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Lingqiao Liu, Lei Wang</p><p>Abstract: To achieve a good trade-off between recognition accuracy and computational efficiency, it is often needed to reduce high-dimensional visual data to medium-dimensional ones. For this task, even applying a simple full-matrixbased linear projection causes significant computation and memory use. When the number of visual data is large, how to efficiently learn such a projection could even become a problem. The recent feature merging approach offers an efficient way to reduce the dimensionality, which only requires a single scan of features to perform reduction. However, existing merging algorithms do not scale well with highdimensional data, especially in the unsupervised case. To address this problem, we formulate unsupervised feature merging as a PCA problem imposed with a special structure constraint. By exploiting its connection with kmeans, we transform this constrained PCA problem into a feature clustering problem. Moreover, we employ the hashing technique to improve its scalability. These produce a scalable feature merging algorithm for our dimensional- ity reduction task. In addition, we develop an extension of this method by leveraging the neighborhood structure in the data to further improve dimensionality reduction performance. In further, we explore the incorporation of bipolar merging a variant of merging function which allows the subtraction operation into our algorithms. Through three applications in visual recognition, we demonstrate that our methods can not only achieve good dimensionality reduction performance with little computational cost but also help to create more powerful representation at both image level and local feature level. – –</p><p>Reference: <a title="iccv-2013-29-reference" href="../iccv2013_reference/iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pka', 0.423), ('hash', 0.333), ('hdlf', 0.254), ('bsc', 0.254), ('bipol', 0.235), ('spm', 0.207), ('pca', 0.176), ('dimend', 0.175), ('bscb', 0.159), ('gj', 0.139), ('mem', 0.135), ('vq', 0.13), ('pkab', 0.127), ('reduc', 0.107), ('lsc', 0.098), ('load', 0.096), ('wma', 0.095), ('wxxtwt', 0.095), ('unsuperv', 0.094), ('lbp', 0.088)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="29-tfidf-1" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>Author: Lingqiao Liu, Lei Wang</p><p>Abstract: To achieve a good trade-off between recognition accuracy and computational efficiency, it is often needed to reduce high-dimensional visual data to medium-dimensional ones. For this task, even applying a simple full-matrixbased linear projection causes significant computation and memory use. When the number of visual data is large, how to efficiently learn such a projection could even become a problem. The recent feature merging approach offers an efficient way to reduce the dimensionality, which only requires a single scan of features to perform reduction. However, existing merging algorithms do not scale well with highdimensional data, especially in the unsupervised case. To address this problem, we formulate unsupervised feature merging as a PCA problem imposed with a special structure constraint. By exploiting its connection with kmeans, we transform this constrained PCA problem into a feature clustering problem. Moreover, we employ the hashing technique to improve its scalability. These produce a scalable feature merging algorithm for our dimensional- ity reduction task. In addition, we develop an extension of this method by leveraging the neighborhood structure in the data to further improve dimensionality reduction performance. In further, we explore the incorporation of bipolar merging a variant of merging function which allows the subtraction operation into our algorithms. Through three applications in visual recognition, we demonstrate that our methods can not only achieve good dimensionality reduction performance with little computational cost but also help to create more powerful representation at both image level and local feature level. – –</p><p>2 0.31927419 <a title="29-tfidf-2" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>Author: Guangnan Ye, Dong Liu, Jun Wang, Shih-Fu Chang</p><p>Abstract: Recently, learning based hashing methods have become popular for indexing large-scale media data. Hashing methods map high-dimensional features to compact binary codes that are efficient to match and robust in preserving original similarity. However, most of the existing hashing methods treat videos as a simple aggregation of independent frames and index each video through combining the indexes of frames. The structure information of videos, e.g., discriminative local visual commonality and temporal consistency, is often neglected in the design of hash functions. In this paper, we propose a supervised method that explores the structure learning techniques to design efficient hash functions. The proposed video hashing method formulates a minimization problem over a structure-regularized empirical loss. In particular, the structure regularization exploits the common local visual patterns occurring in video frames that are associated with the same semantic class, and simultaneously preserves the temporal consistency over successive frames from the same video. We show that the minimization objective can be efficiently solved by an Acceler- ated Proximal Gradient (APG) method. Extensive experiments on two large video benchmark datasets (up to around 150K video clips with over 12 million frames) show that the proposed method significantly outperforms the state-ofthe-art hashing methods.</p><p>3 0.30969268 <a title="29-tfidf-3" href="./iccv-2013-A_General_Two-Step_Approach_to_Learning-Based_Hashing.html">13 iccv-2013-A General Two-Step Approach to Learning-Based Hashing</a></p>
<p>Author: Guosheng Lin, Chunhua Shen, David Suter, Anton van_den_Hengel</p><p>Abstract: Most existing approaches to hashing apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to respond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. This framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problemspecific hashing methods. Our framework decomposes the hashing learning problem into two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically be formulated as binary quadratic problems, and the second step can be accomplished by training standard binary classifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate that the proposed framework is effective, flexible and outperforms the state-of-the-art.</p><p>4 0.28879109 <a title="29-tfidf-4" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>Author: Zhongming Jin, Yao Hu, Yue Lin, Debing Zhang, Shiding Lin, Deng Cai, Xuelong Li</p><p>Abstract: Recently, hashing techniques have been widely applied to solve the approximate nearest neighbors search problem in many vision applications. Generally, these hashing approaches generate 2c buckets, where c is the length of the hash code. A good hashing method should satisfy the following two requirements: 1) mapping the nearby data points into the same bucket or nearby (measured by xue long l i opt . ac . cn @ a(a)b(b) the Hamming distance) buckets. 2) all the data points are evenly distributed among all the buckets. In this paper, we propose a novel algorithm named Complementary Projection Hashing (CPH) to find the optimal hashing functions which explicitly considers the above two requirements. Specifically, CPHaims at sequentiallyfinding a series ofhyperplanes (hashing functions) which cross the sparse region of the data. At the same time, the data points are evenly distributed in the hypercubes generated by these hyperplanes. The experiments comparing with the state-of-the-art hashing methods demonstrate the effectiveness of the proposed method.</p><p>5 0.25581157 <a title="29-tfidf-5" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>Author: Jun Wang, Wei Liu, Andy X. Sun, Yu-Gang Jiang</p><p>Abstract: Hashing techniques have been intensively investigated in the design of highly efficient search engines for largescale computer vision applications. Compared with prior approximate nearest neighbor search approaches like treebased indexing, hashing-based search schemes have prominent advantages in terms of both storage and computational efficiencies. Moreover, the procedure of devising hash functions can be easily incorporated into sophisticated machine learning tools, leading to data-dependent and task-specific compact hash codes. Therefore, a number of learning paradigms, ranging from unsupervised to supervised, have been applied to compose appropriate hash functions. How- ever, most of the existing hash function learning methods either treat hash function design as a classification problem or generate binary codes to satisfy pairwise supervision, and have not yet directly optimized the search accuracy. In this paper, we propose to leverage listwise supervision into a principled hash function learning framework. In particular, the ranking information is represented by a set of rank triplets that can be used to assess the quality of ranking. Simple linear projection-based hash functions are solved efficiently through maximizing the ranking quality over the training data. We carry out experiments on large image datasets with size up to one million and compare with the state-of-the-art hashing techniques. The extensive results corroborate that our learned hash codes via listwise supervision can provide superior search accuracy without incurring heavy computational overhead.</p><p>6 0.24019733 <a title="29-tfidf-6" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>7 0.15266493 <a title="29-tfidf-7" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>8 0.13504715 <a title="29-tfidf-8" href="./iccv-2013-Modifying_the_Memorability_of_Face_Photographs.html">272 iccv-2013-Modifying the Memorability of Face Photographs</a></p>
<p>9 0.12830319 <a title="29-tfidf-9" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>10 0.11894895 <a title="29-tfidf-10" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>11 0.11127669 <a title="29-tfidf-11" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>12 0.10128445 <a title="29-tfidf-12" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>13 0.09978731 <a title="29-tfidf-13" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>14 0.09575703 <a title="29-tfidf-14" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<p>15 0.085970506 <a title="29-tfidf-15" href="./iccv-2013-Fine-Grained_Categorization_by_Alignments.html">169 iccv-2013-Fine-Grained Categorization by Alignments</a></p>
<p>16 0.082755379 <a title="29-tfidf-16" href="./iccv-2013-Fast_Neighborhood_Graph_Search_Using_Cartesian_Concatenation.html">159 iccv-2013-Fast Neighborhood Graph Search Using Cartesian Concatenation</a></p>
<p>17 0.082415983 <a title="29-tfidf-17" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>18 0.080193318 <a title="29-tfidf-18" href="./iccv-2013-Fast_High_Dimensional_Vector_Multiplication_Face_Recognition.html">158 iccv-2013-Fast High Dimensional Vector Multiplication Face Recognition</a></p>
<p>19 0.077215903 <a title="29-tfidf-19" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>20 0.075469591 <a title="29-tfidf-20" href="./iccv-2013-Latent_Space_Sparse_Subspace_Clustering.html">232 iccv-2013-Latent Space Sparse Subspace Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.18), (1, -0.072), (2, -0.11), (3, 0.088), (4, 0.3), (5, -0.089), (6, -0.062), (7, 0.023), (8, 0.009), (9, 0.036), (10, 0.135), (11, -0.134), (12, 0.057), (13, 0.025), (14, 0.009), (15, 0.045), (16, 0.008), (17, -0.0), (18, 0.01), (19, -0.031), (20, -0.003), (21, -0.02), (22, 0.002), (23, 0.026), (24, 0.019), (25, 0.013), (26, -0.017), (27, -0.057), (28, 0.01), (29, 0.038), (30, 0.014), (31, 0.002), (32, -0.02), (33, 0.04), (34, -0.074), (35, 0.025), (36, -0.046), (37, 0.009), (38, -0.007), (39, 0.02), (40, 0.039), (41, 0.046), (42, -0.06), (43, -0.006), (44, 0.0), (45, 0.051), (46, -0.035), (47, 0.068), (48, -0.025), (49, -0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91476107 <a title="29-lsi-1" href="./iccv-2013-A_General_Two-Step_Approach_to_Learning-Based_Hashing.html">13 iccv-2013-A General Two-Step Approach to Learning-Based Hashing</a></p>
<p>Author: Guosheng Lin, Chunhua Shen, David Suter, Anton van_den_Hengel</p><p>Abstract: Most existing approaches to hashing apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to respond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. This framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problemspecific hashing methods. Our framework decomposes the hashing learning problem into two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically be formulated as binary quadratic problems, and the second step can be accomplished by training standard binary classifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate that the proposed framework is effective, flexible and outperforms the state-of-the-art.</p><p>2 0.8977806 <a title="29-lsi-2" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>Author: Zhongming Jin, Yao Hu, Yue Lin, Debing Zhang, Shiding Lin, Deng Cai, Xuelong Li</p><p>Abstract: Recently, hashing techniques have been widely applied to solve the approximate nearest neighbors search problem in many vision applications. Generally, these hashing approaches generate 2c buckets, where c is the length of the hash code. A good hashing method should satisfy the following two requirements: 1) mapping the nearby data points into the same bucket or nearby (measured by xue long l i opt . ac . cn @ a(a)b(b) the Hamming distance) buckets. 2) all the data points are evenly distributed among all the buckets. In this paper, we propose a novel algorithm named Complementary Projection Hashing (CPH) to find the optimal hashing functions which explicitly considers the above two requirements. Specifically, CPHaims at sequentiallyfinding a series ofhyperplanes (hashing functions) which cross the sparse region of the data. At the same time, the data points are evenly distributed in the hypercubes generated by these hyperplanes. The experiments comparing with the state-of-the-art hashing methods demonstrate the effectiveness of the proposed method.</p><p>3 0.87633389 <a title="29-lsi-3" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>Author: Lixin Fan</p><p>Abstract: This paper proposes to learn binary hash codes within a statistical learning framework, in which an upper bound of the probability of Bayes decision errors is derived for different forms of hash functions and a rigorous proof of the convergence of the upper bound is presented. Consequently, minimizing such an upper bound leads to consistent performance improvements of existing hash code learning algorithms, regardless of whether original algorithms are unsupervised or supervised. This paper also illustrates a fast hash coding method that exploits simple binary tests to achieve orders of magnitude improvement in coding speed as compared to projection based methods.</p><p>4 0.85401887 <a title="29-lsi-4" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>Author: Jun Wang, Wei Liu, Andy X. Sun, Yu-Gang Jiang</p><p>Abstract: Hashing techniques have been intensively investigated in the design of highly efficient search engines for largescale computer vision applications. Compared with prior approximate nearest neighbor search approaches like treebased indexing, hashing-based search schemes have prominent advantages in terms of both storage and computational efficiencies. Moreover, the procedure of devising hash functions can be easily incorporated into sophisticated machine learning tools, leading to data-dependent and task-specific compact hash codes. Therefore, a number of learning paradigms, ranging from unsupervised to supervised, have been applied to compose appropriate hash functions. How- ever, most of the existing hash function learning methods either treat hash function design as a classification problem or generate binary codes to satisfy pairwise supervision, and have not yet directly optimized the search accuracy. In this paper, we propose to leverage listwise supervision into a principled hash function learning framework. In particular, the ranking information is represented by a set of rank triplets that can be used to assess the quality of ranking. Simple linear projection-based hash functions are solved efficiently through maximizing the ranking quality over the training data. We carry out experiments on large image datasets with size up to one million and compare with the state-of-the-art hashing techniques. The extensive results corroborate that our learned hash codes via listwise supervision can provide superior search accuracy without incurring heavy computational overhead.</p><p>same-paper 5 0.83233583 <a title="29-lsi-5" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>Author: Lingqiao Liu, Lei Wang</p><p>Abstract: To achieve a good trade-off between recognition accuracy and computational efficiency, it is often needed to reduce high-dimensional visual data to medium-dimensional ones. For this task, even applying a simple full-matrixbased linear projection causes significant computation and memory use. When the number of visual data is large, how to efficiently learn such a projection could even become a problem. The recent feature merging approach offers an efficient way to reduce the dimensionality, which only requires a single scan of features to perform reduction. However, existing merging algorithms do not scale well with highdimensional data, especially in the unsupervised case. To address this problem, we formulate unsupervised feature merging as a PCA problem imposed with a special structure constraint. By exploiting its connection with kmeans, we transform this constrained PCA problem into a feature clustering problem. Moreover, we employ the hashing technique to improve its scalability. These produce a scalable feature merging algorithm for our dimensional- ity reduction task. In addition, we develop an extension of this method by leveraging the neighborhood structure in the data to further improve dimensionality reduction performance. In further, we explore the incorporation of bipolar merging a variant of merging function which allows the subtraction operation into our algorithms. Through three applications in visual recognition, we demonstrate that our methods can not only achieve good dimensionality reduction performance with little computational cost but also help to create more powerful representation at both image level and local feature level. – –</p><p>6 0.82101792 <a title="29-lsi-6" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>7 0.59883845 <a title="29-lsi-7" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>8 0.54378849 <a title="29-lsi-8" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>9 0.51577109 <a title="29-lsi-9" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>10 0.50360119 <a title="29-lsi-10" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<p>11 0.50282365 <a title="29-lsi-11" href="./iccv-2013-Fast_Neighborhood_Graph_Search_Using_Cartesian_Concatenation.html">159 iccv-2013-Fast Neighborhood Graph Search Using Cartesian Concatenation</a></p>
<p>12 0.48559281 <a title="29-lsi-12" href="./iccv-2013-Low-Rank_Sparse_Coding_for_Image_Classification.html">258 iccv-2013-Low-Rank Sparse Coding for Image Classification</a></p>
<p>13 0.48193434 <a title="29-lsi-13" href="./iccv-2013-SIFTpack%3A_A_Compact_Representation_for_Efficient_SIFT_Matching.html">365 iccv-2013-SIFTpack: A Compact Representation for Efficient SIFT Matching</a></p>
<p>14 0.47066039 <a title="29-lsi-14" href="./iccv-2013-Joint_Inverted_Indexing.html">221 iccv-2013-Joint Inverted Indexing</a></p>
<p>15 0.45800486 <a title="29-lsi-15" href="./iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning.html">212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</a></p>
<p>16 0.44808629 <a title="29-lsi-16" href="./iccv-2013-A_Novel_Earth_Mover%27s_Distance_Methodology_for_Image_Matching_with_Gaussian_Mixture_Models.html">25 iccv-2013-A Novel Earth Mover's Distance Methodology for Image Matching with Gaussian Mixture Models</a></p>
<p>17 0.44572949 <a title="29-lsi-17" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>18 0.44349569 <a title="29-lsi-18" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>19 0.43765697 <a title="29-lsi-19" href="./iccv-2013-Nested_Shape_Descriptors.html">288 iccv-2013-Nested Shape Descriptors</a></p>
<p>20 0.43541473 <a title="29-lsi-20" href="./iccv-2013-Quantize_and_Conquer%3A_A_Dimensionality-Recursive_Solution_to_Clustering%2C_Vector_Quantization%2C_and_Image_Retrieval.html">333 iccv-2013-Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(12, 0.015), (13, 0.057), (18, 0.018), (20, 0.101), (25, 0.034), (42, 0.129), (48, 0.165), (50, 0.029), (55, 0.024), (59, 0.304), (77, 0.037), (83, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.75420868 <a title="29-lda-1" href="./iccv-2013-Detecting_Irregular_Curvilinear_Structures_in_Gray_Scale_and_Color_Imagery_Using_Multi-directional_Oriented_Flux.html">112 iccv-2013-Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-directional Oriented Flux</a></p>
<p>Author: Engin Türetken, Carlos Becker, Przemyslaw Glowacki, Fethallah Benmansour, Pascal Fua</p><p>Abstract: We propose a new approach to detecting irregular curvilinear structures in noisy image stacks. In contrast to earlier approaches that rely on circular models of the crosssections, ours allows for the arbitrarily-shaped ones that are prevalent in biological imagery. This is achieved by maximizing the image gradient flux along multiple directions and radii, instead of only two with a unique radius as is usually done. This yields a more complex optimization problem for which we propose a computationally efficient solution. We demonstrate the effectiveness of our approach on a wide range ofchallenging gray scale and color datasets and show that it outperforms existing techniques, especially on very irregular structures.</p><p>2 0.70881051 <a title="29-lda-2" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>Author: Guangnan Ye, Dong Liu, Jun Wang, Shih-Fu Chang</p><p>Abstract: Recently, learning based hashing methods have become popular for indexing large-scale media data. Hashing methods map high-dimensional features to compact binary codes that are efficient to match and robust in preserving original similarity. However, most of the existing hashing methods treat videos as a simple aggregation of independent frames and index each video through combining the indexes of frames. The structure information of videos, e.g., discriminative local visual commonality and temporal consistency, is often neglected in the design of hash functions. In this paper, we propose a supervised method that explores the structure learning techniques to design efficient hash functions. The proposed video hashing method formulates a minimization problem over a structure-regularized empirical loss. In particular, the structure regularization exploits the common local visual patterns occurring in video frames that are associated with the same semantic class, and simultaneously preserves the temporal consistency over successive frames from the same video. We show that the minimization objective can be efficiently solved by an Acceler- ated Proximal Gradient (APG) method. Extensive experiments on two large video benchmark datasets (up to around 150K video clips with over 12 million frames) show that the proposed method significantly outperforms the state-ofthe-art hashing methods.</p><p>3 0.70178986 <a title="29-lda-3" href="./iccv-2013-Image_Guided_Depth_Upsampling_Using_Anisotropic_Total_Generalized_Variation.html">209 iccv-2013-Image Guided Depth Upsampling Using Anisotropic Total Generalized Variation</a></p>
<p>Author: David Ferstl, Christian Reinbacher, Rene Ranftl, Matthias Ruether, Horst Bischof</p><p>Abstract: In this work we present a novel method for the challenging problem of depth image upsampling. Modern depth cameras such as Kinect or Time of Flight cameras deliver dense, high quality depth measurements but are limited in their lateral resolution. To overcome this limitation we formulate a convex optimization problem using higher order regularization for depth image upsampling. In this optimization an anisotropic diffusion tensor, calculated from a high resolution intensity image, is used to guide the upsampling. We derive a numerical algorithm based on a primaldual formulation that is efficiently parallelized and runs at multiple frames per second. We show that this novel upsampling clearly outperforms state of the art approaches in terms of speed and accuracy on the widely used Middlebury 2007 datasets. Furthermore, we introduce novel datasets with highly accurate groundtruth, which, for the first time, enable to benchmark depth upsampling methods using real sensor data.</p><p>same-paper 4 0.69698042 <a title="29-lda-4" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>Author: Lingqiao Liu, Lei Wang</p><p>Abstract: To achieve a good trade-off between recognition accuracy and computational efficiency, it is often needed to reduce high-dimensional visual data to medium-dimensional ones. For this task, even applying a simple full-matrixbased linear projection causes significant computation and memory use. When the number of visual data is large, how to efficiently learn such a projection could even become a problem. The recent feature merging approach offers an efficient way to reduce the dimensionality, which only requires a single scan of features to perform reduction. However, existing merging algorithms do not scale well with highdimensional data, especially in the unsupervised case. To address this problem, we formulate unsupervised feature merging as a PCA problem imposed with a special structure constraint. By exploiting its connection with kmeans, we transform this constrained PCA problem into a feature clustering problem. Moreover, we employ the hashing technique to improve its scalability. These produce a scalable feature merging algorithm for our dimensional- ity reduction task. In addition, we develop an extension of this method by leveraging the neighborhood structure in the data to further improve dimensionality reduction performance. In further, we explore the incorporation of bipolar merging a variant of merging function which allows the subtraction operation into our algorithms. Through three applications in visual recognition, we demonstrate that our methods can not only achieve good dimensionality reduction performance with little computational cost but also help to create more powerful representation at both image level and local feature level. – –</p><p>5 0.67723006 <a title="29-lda-5" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>Author: Zhongming Jin, Yao Hu, Yue Lin, Debing Zhang, Shiding Lin, Deng Cai, Xuelong Li</p><p>Abstract: Recently, hashing techniques have been widely applied to solve the approximate nearest neighbors search problem in many vision applications. Generally, these hashing approaches generate 2c buckets, where c is the length of the hash code. A good hashing method should satisfy the following two requirements: 1) mapping the nearby data points into the same bucket or nearby (measured by xue long l i opt . ac . cn @ a(a)b(b) the Hamming distance) buckets. 2) all the data points are evenly distributed among all the buckets. In this paper, we propose a novel algorithm named Complementary Projection Hashing (CPH) to find the optimal hashing functions which explicitly considers the above two requirements. Specifically, CPHaims at sequentiallyfinding a series ofhyperplanes (hashing functions) which cross the sparse region of the data. At the same time, the data points are evenly distributed in the hypercubes generated by these hyperplanes. The experiments comparing with the state-of-the-art hashing methods demonstrate the effectiveness of the proposed method.</p><p>6 0.67542714 <a title="29-lda-6" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>7 0.66978461 <a title="29-lda-7" href="./iccv-2013-A_General_Two-Step_Approach_to_Learning-Based_Hashing.html">13 iccv-2013-A General Two-Step Approach to Learning-Based Hashing</a></p>
<p>8 0.66858262 <a title="29-lda-8" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>9 0.6576798 <a title="29-lda-9" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>10 0.61602831 <a title="29-lda-10" href="./iccv-2013-Quantize_and_Conquer%3A_A_Dimensionality-Recursive_Solution_to_Clustering%2C_Vector_Quantization%2C_and_Image_Retrieval.html">333 iccv-2013-Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval</a></p>
<p>11 0.61401206 <a title="29-lda-11" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>12 0.61259878 <a title="29-lda-12" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>13 0.61247021 <a title="29-lda-13" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>14 0.61233371 <a title="29-lda-14" href="./iccv-2013-Unsupervised_Random_Forest_Manifold_Alignment_for_Lipreading.html">437 iccv-2013-Unsupervised Random Forest Manifold Alignment for Lipreading</a></p>
<p>15 0.61213791 <a title="29-lda-15" href="./iccv-2013-Live_Metric_3D_Reconstruction_on_Mobile_Phones.html">254 iccv-2013-Live Metric 3D Reconstruction on Mobile Phones</a></p>
<p>16 0.61209244 <a title="29-lda-16" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>17 0.6119644 <a title="29-lda-17" href="./iccv-2013-Joint_Inverted_Indexing.html">221 iccv-2013-Joint Inverted Indexing</a></p>
<p>18 0.61152709 <a title="29-lda-18" href="./iccv-2013-Discriminant_Tracking_Using_Tensor_Representation_with_Semi-supervised_Improvement.html">119 iccv-2013-Discriminant Tracking Using Tensor Representation with Semi-supervised Improvement</a></p>
<p>19 0.61103284 <a title="29-lda-19" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>20 0.61096692 <a title="29-lda-20" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
