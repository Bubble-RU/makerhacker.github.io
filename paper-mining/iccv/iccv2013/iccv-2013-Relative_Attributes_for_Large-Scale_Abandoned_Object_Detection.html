<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>350 iccv-2013-Relative Attributes for Large-Scale Abandoned Object Detection</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-350" href="#">iccv2013-350</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>350 iccv-2013-Relative Attributes for Large-Scale Abandoned Object Detection</h1>
<br/><p>Source: <a title="iccv-2013-350-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Fan_Relative_Attributes_for_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Quanfu Fan, Prasad Gabbur, Sharath Pankanti</p><p>Abstract: Effective reduction of false alarms in large-scale video surveillance is rather challenging, especially for applications where abnormal events of interest rarely occur, such as abandoned object detection. We develop an approach to prioritize alerts by ranking them, and demonstrate its great effectiveness in reducing false positives while keeping good detection accuracy. Our approach benefits from a novel representation of abandoned object alerts by relative attributes, namely staticness, foregroundness and abandonment. The relative strengths of these attributes are quantified using a ranking function[19] learnt on suitably designed low-level spatial and temporal features.These attributes of varying strengths are not only powerful in distinguishing abandoned objects from false alarms such as people and light artifacts, but also computationally efficient for large-scale deployment. With these features, we apply a linear ranking algorithm to sort alerts according to their relevance to the end-user. We test the effectiveness of our approach on both public data sets and large ones collected from the real world.</p><p>Reference: <a title="iccv-2013-350-reference" href="../iccv2013_reference/iccv-2013-Relative_Attributes_for_Large-Scale_Abandoned_Object_Detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Effective reduction of false alarms in large-scale video surveillance is rather challenging, especially for applications where abnormal events of interest rarely occur, such as abandoned object detection. [sent-10, score-0.832]
</p><p>2 We develop an approach to prioritize alerts by ranking them, and demonstrate its great effectiveness in reducing false positives while keeping good detection accuracy. [sent-11, score-0.878]
</p><p>3 Our approach benefits from a novel representation of abandoned object alerts by relative attributes, namely staticness, foregroundness and abandonment. [sent-12, score-1.292]
</p><p>4 The relative strengths of these attributes are quantified using a ranking function[19] learnt on suitably designed low-level spatial and temporal features. [sent-13, score-0.358]
</p><p>5 These attributes of varying strengths are not only powerful in distinguishing abandoned objects from false alarms such as people and light artifacts, but also computationally efficient for large-scale deployment. [sent-14, score-1.056]
</p><p>6 With these features, we apply a linear ranking algorithm to sort alerts according to their relevance to the end-user. [sent-15, score-0.722]
</p><p>7 Introduction We present a robust and efficient approach to prioritize alerts in abandoned object detection (AOD) for large scale video surveillance. [sent-18, score-1.164]
</p><p>8 An abandoned object tends to indicate high staticness (S), foregroundness (F) and abandonment (A). [sent-28, score-1.157]
</p><p>9 These high-level attributes are then fed to a second level ranker to prioritize the importance (I) of an object. [sent-30, score-0.229]
</p><p>10 Secondly, many other things can be confused with abandoned objects. [sent-35, score-0.503]
</p><p>11 2) are considered as true drops, which account for only a tiny portion of the total number of alerts triggered in a system. [sent-41, score-0.591]
</p><p>12 Such an extremely high imbalance between true and false alarms demands the system to have good hit rates while at the same time working at low FPRs. [sent-42, score-0.274]
</p><p>13 In this paper, we address the challenges aforementioned 2736  by prioritizing abandoned object alerts using ranking techniques. [sent-43, score-1.196]
</p><p>14 Ranking is well suited for the problem of AOD , where false alarms dominate detection results. [sent-44, score-0.276]
</p><p>15 It has the ability to move up alerts of higher importance to the top of the adjudication process while significantly suppressing false alarms. [sent-45, score-0.664]
</p><p>16 In order to make alert prioritization feasible, we first propose a novel representation of abandoned objects by visual attributes, namely, staticness, foregroundness and abandonment. [sent-46, score-0.869]
</p><p>17 In general, abandoned objects are essentially foreground objects that remain motionless over a certain period of time in the scene. [sent-47, score-0.631]
</p><p>18 One more attribute that abandoned objects possess uniquely is abandonment, which is referred to in [9] as some associated human activity or behavior around an object just before it is dropped and left in isolation. [sent-52, score-0.64]
</p><p>19 Motivated by the recent work of relative attributes by Parikh and Grauman [19], we further specify the relative strengths of these attributes on different types of alerts raised by various objects in the scene, and apply the technique of [19] to score the attributes. [sent-53, score-1.031]
</p><p>20 As demonstrated later, these high-level semantic features are intuitively discriminative in separating abandoned objects from other types of alerts. [sent-54, score-0.545]
</p><p>21 Since static objects are of primary interest in AOD, we integrate the tracker with the approach of [8] which models temporarily static objects by a finite state machine. [sent-57, score-0.375]
</p><p>22 This information enables effective extraction of spatio-temporal features for staticness and abandonment analysis. [sent-59, score-0.45]
</p><p>23 We finally use these learnt attributes as input to a ranker to sort alerts by importance. [sent-61, score-0.778]
</p><p>24 The degree of importance for  an alert is given in the order of bags > people > other alerts. [sent-62, score-0.297]
</p><p>25 Here bags refer to true abandoned objects or true alerts. [sent-63, score-0.682]
</p><p>26 We enforce such a relationship of ordering between alerts in the ranker largely because people are the most confusing alerts to bags and other alerts such as light artifacts and shadows are of the least interest to the users. [sent-64, score-2.012]
</p><p>27 We again adopt the technique of [19] for alert ranking due to its simplicity and efficiency. [sent-65, score-0.268]
</p><p>28 To the best of our knowledge, this work is the first to propose a general representation of abandoned objects by quantifiable visual attributes. [sent-69, score-0.545]
</p><p>29 While some of these attributes (or concepts) have been tried in previous work [9] for false alarm reduction, they were used qualitatively and mostly in a heuristic way. [sent-70, score-0.285]
</p><p>30 In our experiments, we thoroughly validated the effectiveness and robustness of our approach under various challenging urban scenarios, using both public data sets with staged drops and a data set collected from deployed cameras including natural drops. [sent-72, score-0.315]
</p><p>31 Some work such as [25, 7] focus on detection of abandoned and removed objects, but these approaches usually do not handle lighting changes very well or are susceptible to low texturedness and cluttered background. [sent-78, score-0.568]
</p><p>32 The idea of tracking has been applied to abandoned objection detection in [23, 6, 13] for owner identification. [sent-79, score-0.642]
</p><p>33 Recently, some works have attempted to address the issue of false positives in a more systematic way to meet the requirement of large-scale deployment of abandoned object detection. [sent-83, score-0.692]
</p><p>34 For example, in [9], a sequence of robust filters were developed to address different types of false alarms by doing foreground and abandonment analysis. [sent-84, score-0.525]
</p><p>35 Abandoned Object Alerts In the context of PETS2006 [3], an abandoned object is defined as an item of luggage that has been left behind by its owner. [sent-88, score-0.606]
</p><p>36 In this work, we consider abandoned objects as stationary objects that are physically isolated from other foreground objects in the scene for some time. [sent-89, score-0.724]
</p><p>37 In practice, in addition to bags or luggage, interesting drops picked up by a system include natural items such as bikes, garbage cans and traffic cones (Fig. [sent-91, score-0.296]
</p><p>38 For convenience, we refer to all of them as bags (or true drops) in this paper as opposed to false alarms described below. [sent-93, score-0.359]
</p><p>39 Among false alarms, people and quick lighting changes are two dominant sources (Fig. [sent-94, score-0.247]
</p><p>40 2), followed by shadows and ghosts (spurious foreground objects detected after temporarily static objects move again in a scene). [sent-95, score-0.401]
</p><p>41 This technique features a finite state machine (FSM) that tracks temporarily static objects robustly even under occlusion. [sent-100, score-0.232]
</p><p>42 Figure 2: Typical abandoned object alerts in video surveillance. [sent-103, score-1.1]
</p><p>43 a) a sample staged drop from PETS2006 b) a sample staged drop from iLIDS c) two natural drops (trash cans and traffic cones) d) a non-occluded sitting person e) an occluded sitting person f) a light artifact 4. [sent-104, score-0.414]
</p><p>44 The superscript ’+’ denotes the class of true drops and ’-’ denotes false alarms. [sent-106, score-0.275]
</p><p>45 We take a similar approach here and design three physically expressible features (attributes) that seem plausible for abandoned object detection. [sent-117, score-0.568]
</p><p>46 Specifically, our attributes are called staticness, foregroundness and abandonment, as mentioned previously. [sent-118, score-0.297]
</p><p>47 Similarly, foregroundness refers to the distinctiveness of the object relative to the background based on its appearance. [sent-120, score-0.269]
</p><p>48 Finally, abandonment expresses the notion of the object being left in isolation after remaining in possession or vicinity of some other entity. [sent-121, score-0.298]
</p><p>49 In our work, the level of abandonment for an object is related to the magnitude of external motion around the object right before it is left in isolation. [sent-122, score-0.301]
</p><p>50 In such a way, we bypass the problem of solving the challenge of owner identification and tracking in crowded scenes and instead focus on analyzing the motion around the abandonment of an object. [sent-123, score-0.409]
</p><p>51 It is possible to describe the relative strengths of different kinds of objects associated with alerts in terms of the above attributes (Table 1). [sent-124, score-0.802]
</p><p>52 We expect that a truly abandoned object (B+) such as a bag or a piece of luggage remains static in the scene for a long time (high staticness), is very different from the background (high foregroundness) and has been previously in the possession of its owner (high abandonment). [sent-125, score-0.909]
</p><p>53 In addition, he can be part of a group initially in the scene and isolated later exhibiting abandonment somewhere between a bag and a static background (medium abandonment). [sent-128, score-0.396]
</p><p>54 Similarly other situations associated with false alarms such as lighting changes (L−), shadows (S−) and ghosts (G−) exhibit different degrees of the proposed attributes and hence different relative rankings as shown in Table 1. [sent-129, score-0.638]
</p><p>55 3 shows a small sample of our data points represented in the relative attributes space (3D) as learned by attribute rankers (Section 5). [sent-132, score-0.231]
</p><p>56 Figure 3 : Attribute  scores  of staticness (ST), foregroundness (FG) and  ×  abandonment (AB) learned from two data sets CITY and S-iLIDS for bags(◦), people(+), lighting artifacts( ), shadows (? [sent-134, score-0.739]
</p><p>57 Ranking Using Relative Attributes We adopt the relative attribute framework [19] to rank order our data points in terms of their degree of staticness, foregroundness and abandonment. [sent-137, score-0.329]
</p><p>58 Object Tracking And Low-level Features One of the main challenges is to deal with alerts raised by people, which often exhibit high similarity to abandoned objects. [sent-153, score-1.104]
</p><p>59 Two useful clues for separating people from bags are how an object arrives at the current location and how it remains static in the same location. [sent-154, score-0.29]
</p><p>60 Even if one can only track an object for a short period of time prior to its being static, such informa-  tion turns out to be helpful for staticness and abandonment analysis when combined with other BGS-related information, as described later. [sent-157, score-0.528]
</p><p>61 Different from other tracking-based approaches [23], tracking in our approach is not intended to identify the owner of an abandoned object. [sent-158, score-0.614]
</p><p>62 Instead it aims to provide sufficient evidence for differentiating people from truly static objects for the purpose of suppressing false alarms. [sent-159, score-0.362]
</p><p>63 For the purpose of abandonment analysis, we further search for the blob Ra that maximally overlaps with Re right before the object gets tracked by the mini-tracker. [sent-184, score-0.301]
</p><p>64 | |Le − Ls | |; the total length of the track; the aspect ratio of the static region; the ratio of the area of the static region over that of the start region, i. [sent-195, score-0.222]
</p><p>65 solid box (red or blue): abandoned object; yellow box: the start position of an object; cyan box: the foreground object from which the object is split. [sent-213, score-0.635]
</p><p>66 For foreground analysis, we directly adopt the feature set developed in [9], which has demonstrated superior performance in separating foreground objects and background artifacts related to lighting changes. [sent-215, score-0.242]
</p><p>67 The features described above are used as input for the attribute ranker discussed in Section 5 to compute the ranking scores of staticness, foregroundness and abandonment. [sent-217, score-0.436]
</p><p>68 Alert Ranking We design a second level ranker to sort alerts using the attribute scores learnt previously in Section 5. [sent-221, score-0.734]
</p><p>69 In practice, some types of false alarms are more important than others to the end user. [sent-222, score-0.248]
</p><p>70 It is found that investigating irrelevant alerts caused by shadows and lighting artifacts leads to wasteful utilization of a security officer’s time and effort. [sent-223, score-0.692]
</p><p>71 While alerts raised by activities of people in the scene are also less interesting, investigating such alerts sometimes can be useful in detecting potentially harmful situations. [sent-224, score-1.225]
</p><p>72 Moreover, people alerts present more ambiguity to true drops than other alerts (see Fig. [sent-225, score-1.297]
</p><p>73 This suggests a relative ordering of alerts themselves based on both their relevance to the end user and their separability, i. [sent-227, score-0.662]
</p><p>74 We enforce such a relationship of ordering between alerts in a ranker. [sent-230, score-0.576]
</p><p>75 Due to its simplicity and efficiency, we adopt the technique of [19] again for alert ranking by treating relevance as one single attribute. [sent-231, score-0.311]
</p><p>76 The data set has a total of 60 staged drops, and was selected in a way to ensure that the baseline approach used for comparison can detect a reasonably good portion of the drops in the video. [sent-245, score-0.229]
</p><p>77 The second is a challenging data set( CITY) used in [9], containing 255 staged drops within over 70 hours of video footage captured from 30 cameras in typical urban scenarios such as streets and parks. [sent-246, score-0.361]
</p><p>78 Note that the number of bags may be smaller than the number of drops in Table 2 due to detection failures in [8]. [sent-255, score-0.24]
</p><p>79 The first one is an alert ranker using high-level attributes (HL-RANK) as described in Section 7, and the other two are basically binary SVMs using low-level features (LL-SVM) and high-level attributes (HL-SVM) respectively. [sent-261, score-0.447]
</p><p>80 The two SVMs treat bags as positive labels and other alerts as negative and are trained with a linear kernel. [sent-262, score-0.627]
</p><p>81 At the same recall, all our approaches show a significant reduction of false alarms in comparison with the baseline. [sent-281, score-0.248]
</p><p>82 7 shows some examples of true and false positive alerts detected by our system(HL-RANK). [sent-316, score-0.69]
</p><p>83 Our approach is able to eliminate difficult people alerts such as those two illustrated in the top of the figure. [sent-317, score-0.602]
</p><p>84 As a comparison, all our proposed approaches yield very few false alarms (high precision). [sent-325, score-0.248]
</p><p>85 Natural drops vary from staged ones in many aspects. [sent-331, score-0.229]
</p><p>86 Note that NATS only includes few natural drops and we do not expect that our approaches can rank them as high as those staged drops, so it makes sense in this case to evaluate the ranking quality of our ap-  ×  proaches with MAP or NDCG [14]. [sent-343, score-0.376]
</p><p>87 To better understand how much our system can benefit from a ranking technique, we turned HL-RANK into a classifier by thresholding the ranking scores by 0. [sent-350, score-0.256]
</p><p>88 In Table 5, HL-RANK demonstrates clear advantages over the baseline by reducing half of the false alarms while still achieving a comparable detection rate with the baseline. [sent-352, score-0.276]
</p><p>89 2742  Figure 7: Example alerts provided by our system. [sent-360, score-0.542]
</p><p>90 The top row shows correct detections while the bottom row illustrates false detections (a false positive is highlighted with a red bounding box around the image while a green box indicates a false negative). [sent-361, score-0.406]
</p><p>91 Conclusions We propose a novel approach to abandoned object detection using the framework of relative attributes. [sent-366, score-0.608]
</p><p>92 Specifically, we design three physically interpretable attributes (staticness, foregroundness and abandonment) to characterize different kinds of alerts raised by various objects in the scene. [sent-367, score-0.97]
</p><p>93 We learn ranking functions for each of the attributes to rank order the alerts based on their strengths on the correspond-  ing attributes. [sent-368, score-0.864]
</p><p>94 The attributes are used as input to an alert prioritization method which performs a ranking using alert importance. [sent-369, score-0.525]
</p><p>95 Discrimination of abandoned and stolen object based on active contours. [sent-414, score-0.537]
</p><p>96 Modeling oftemporarily static objects for robust abandoned object detection in urban  [9]  [10] [11] [12] [13]  [14] [15]  [16]  [17]  [18]  [19]  surveillance. [sent-419, score-0.759]
</p><p>97 Robust foreground and abandonment analysis for large-scale abandoned object detection. [sent-424, score-0.814]
</p><p>98 A localized approach to abandoned luggage detection with foreground-mask sampling. [sent-444, score-0.6]
</p><p>99 Feature extraction techniques for abandoned object classification in video surveillance. [sent-472, score-0.558]
</p><p>100 Real-time detection of abandoned and removed objects in complex environments. [sent-512, score-0.573]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('alerts', 0.542), ('abandoned', 0.503), ('abandonment', 0.233), ('staticness', 0.217), ('foregroundness', 0.17), ('drops', 0.127), ('attributes', 0.127), ('alert', 0.127), ('alarms', 0.126), ('aod', 0.124), ('false', 0.122), ('ranking', 0.117), ('static', 0.111), ('fsm', 0.108), ('staged', 0.102), ('city', 0.091), ('bags', 0.085), ('owner', 0.082), ('ghosts', 0.069), ('luggage', 0.069), ('ranker', 0.066), ('crowded', 0.065), ('avss', 0.063), ('attribute', 0.061), ('shadows', 0.06), ('people', 0.06), ('artifacts', 0.053), ('strengths', 0.048), ('harmful', 0.046), ('tpr', 0.046), ('track', 0.044), ('foreground', 0.044), ('relevance', 0.043), ('relative', 0.043), ('objects', 0.042), ('urban', 0.041), ('nats', 0.041), ('ibm', 0.04), ('lighting', 0.037), ('tracker', 0.036), ('alarm', 0.036), ('ndcg', 0.036), ('bikes', 0.036), ('prioritize', 0.036), ('raised', 0.035), ('blob', 0.034), ('ordering', 0.034), ('object', 0.034), ('temporarily', 0.033), ('positives', 0.033), ('orderings', 0.032), ('fpr', 0.032), ('physically', 0.031), ('bgs', 0.031), ('cans', 0.031), ('loitering', 0.031), ('possession', 0.031), ('rvilfamnpekr', 0.031), ('subway', 0.031), ('bag', 0.03), ('weather', 0.03), ('rankings', 0.03), ('rank', 0.03), ('tracking', 0.029), ('cones', 0.029), ('quick', 0.028), ('light', 0.028), ('detection', 0.028), ('prioritization', 0.027), ('urgent', 0.027), ('scenarios', 0.027), ('truly', 0.027), ('surveillance', 0.026), ('true', 0.026), ('trash', 0.025), ('ilids', 0.025), ('movement', 0.025), ('degree', 0.025), ('traffic', 0.024), ('technique', 0.024), ('busy', 0.024), ('exhibit', 0.024), ('learnt', 0.023), ('public', 0.023), ('tps', 0.023), ('interpretable', 0.023), ('triggered', 0.023), ('rs', 0.023), ('tracks', 0.022), ('scores', 0.022), ('cameras', 0.022), ('background', 0.022), ('video', 0.021), ('watson', 0.021), ('streets', 0.021), ('industry', 0.021), ('sort', 0.02), ('stationary', 0.02), ('box', 0.02), ('pets', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="350-tfidf-1" href="./iccv-2013-Relative_Attributes_for_Large-Scale_Abandoned_Object_Detection.html">350 iccv-2013-Relative Attributes for Large-Scale Abandoned Object Detection</a></p>
<p>Author: Quanfu Fan, Prasad Gabbur, Sharath Pankanti</p><p>Abstract: Effective reduction of false alarms in large-scale video surveillance is rather challenging, especially for applications where abnormal events of interest rarely occur, such as abandoned object detection. We develop an approach to prioritize alerts by ranking them, and demonstrate its great effectiveness in reducing false positives while keeping good detection accuracy. Our approach benefits from a novel representation of abandoned object alerts by relative attributes, namely staticness, foregroundness and abandonment. The relative strengths of these attributes are quantified using a ranking function[19] learnt on suitably designed low-level spatial and temporal features.These attributes of varying strengths are not only powerful in distinguishing abandoned objects from false alarms such as people and light artifacts, but also computationally efficient for large-scale deployment. With these features, we apply a linear ranking algorithm to sort alerts according to their relevance to the end-user. We test the effectiveness of our approach on both public data sets and large ones collected from the real world.</p><p>2 0.11592255 <a title="350-tfidf-2" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>Author: Adriana Kovashka, Kristen Grauman</p><p>Abstract: Current methods learn monolithic attribute predictors, with the assumption that a single model is sufficient to reflect human understanding of a visual attribute. However, in reality, humans vary in how they perceive the association between a named property and image content. For example, two people may have slightly different internal models for what makes a shoe look “formal”, or they may disagree on which of two scenes looks “more cluttered”. Rather than discount these differences as noise, we propose to learn user-specific attribute models. We adapt a generic model trained with annotations from multiple users, tailoring it to satisfy user-specific labels. Furthermore, we propose novel techniques to infer user-specific labels based on transitivity and contradictions in the user’s search history. We demonstrate that adapted attributes improve accuracy over both existing monolithic models as well as models that learn from scratch with user-specific data alone. In addition, we show how adapted attributes are useful to personalize image search, whether with binary or relative attributes.</p><p>3 0.10421946 <a title="350-tfidf-3" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>Author: Xiaoyang Wang, Qiang Ji</p><p>Abstract: This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. As a list of semantically meaningful properties of objects, attributes generally relate to each other statistically. In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and objectindependent attribute relationships. The model utilizes the captured relationships to benefit both attribute prediction and object recognition. Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases.</p><p>4 0.091331221 <a title="350-tfidf-4" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>Author: Sukrit Shankar, Joan Lasenby, Roberto Cipolla</p><p>Abstract: Relative (comparative) attributes are promising for thematic ranking of visual entities, which also aids in recognition tasks [19, 23]. However, attribute rank learning often requires a substantial amount of relational supervision, which is highly tedious, and apparently impracticalfor realworld applications. In this paper, we introduce the Semantic Transform, which under minimal supervision, adaptively finds a semantic feature space along with a class ordering that is related in the best possible way. Such a semantic space is found for every attribute category. To relate the classes under weak supervision, the class ordering needs to be refined according to a cost function in an iterative procedure. This problem is ideally NP-hard, and we thus propose a constrained search tree formulation for the same. Driven by the adaptive semantic feature space representation, our model achieves the best results to date for all of the tasks of relative, absolute and zero-shot classification on two popular datasets.</p><p>5 0.086654998 <a title="350-tfidf-5" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Devi Parikh, Tsuhan Chen</p><p>Abstract: In recent years, there has been a great deal of progress in describing objects with attributes. Attributes have proven useful for object recognition, image search, face verification, image description, and zero-shot learning. Typically, attributes are either binary or relative: they describe either the presence or absence of a descriptive characteristic, or the relative magnitude of the characteristic when comparing two exemplars. However, prior work fails to model the actual way in which humans use these attributes in descriptive statements of images. Specifically, it does not address the important interactions between the binary and relative aspects of an attribute. In this work we propose a spoken attribute classifier which models a more natural way of using an attribute in a description. For each attribute we train a classifier which captures the specific way this attribute should be used. We show that as a result of using this model, we produce descriptions about images of people that are more natural and specific than past systems.</p><p>6 0.085755371 <a title="350-tfidf-6" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>7 0.077999175 <a title="350-tfidf-7" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>8 0.077534549 <a title="350-tfidf-8" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>9 0.071578637 <a title="350-tfidf-9" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>10 0.070750318 <a title="350-tfidf-10" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>11 0.066003099 <a title="350-tfidf-11" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>12 0.065150611 <a title="350-tfidf-12" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>13 0.062233865 <a title="350-tfidf-13" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>14 0.061842266 <a title="350-tfidf-14" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>15 0.061006602 <a title="350-tfidf-15" href="./iccv-2013-Finding_the_Best_from_the_Second_Bests_-_Inhibiting_Subjective_Bias_in_Evaluation_of_Visual_Tracking_Algorithms.html">168 iccv-2013-Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms</a></p>
<p>16 0.059639648 <a title="350-tfidf-16" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>17 0.058197077 <a title="350-tfidf-17" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>18 0.055165004 <a title="350-tfidf-18" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>19 0.053237129 <a title="350-tfidf-19" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>20 0.053202733 <a title="350-tfidf-20" href="./iccv-2013-Learning_Graph_Matching%3A_Oriented_to_Category_Modeling_from_Cluttered_Scenes.html">237 iccv-2013-Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.129), (1, 0.033), (2, -0.007), (3, -0.05), (4, 0.074), (5, -0.025), (6, -0.044), (7, -0.033), (8, 0.064), (9, 0.098), (10, -0.023), (11, -0.002), (12, 0.029), (13, 0.003), (14, -0.021), (15, -0.017), (16, 0.016), (17, 0.058), (18, -0.038), (19, -0.001), (20, -0.054), (21, 0.014), (22, -0.015), (23, -0.043), (24, -0.017), (25, 0.035), (26, 0.012), (27, -0.032), (28, 0.0), (29, -0.015), (30, 0.003), (31, 0.039), (32, 0.006), (33, -0.01), (34, 0.035), (35, 0.015), (36, 0.073), (37, -0.01), (38, -0.014), (39, 0.019), (40, 0.03), (41, 0.042), (42, -0.059), (43, 0.005), (44, -0.039), (45, 0.002), (46, 0.042), (47, 0.013), (48, 0.015), (49, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89574593 <a title="350-lsi-1" href="./iccv-2013-Relative_Attributes_for_Large-Scale_Abandoned_Object_Detection.html">350 iccv-2013-Relative Attributes for Large-Scale Abandoned Object Detection</a></p>
<p>Author: Quanfu Fan, Prasad Gabbur, Sharath Pankanti</p><p>Abstract: Effective reduction of false alarms in large-scale video surveillance is rather challenging, especially for applications where abnormal events of interest rarely occur, such as abandoned object detection. We develop an approach to prioritize alerts by ranking them, and demonstrate its great effectiveness in reducing false positives while keeping good detection accuracy. Our approach benefits from a novel representation of abandoned object alerts by relative attributes, namely staticness, foregroundness and abandonment. The relative strengths of these attributes are quantified using a ranking function[19] learnt on suitably designed low-level spatial and temporal features.These attributes of varying strengths are not only powerful in distinguishing abandoned objects from false alarms such as people and light artifacts, but also computationally efficient for large-scale deployment. With these features, we apply a linear ranking algorithm to sort alerts according to their relevance to the end-user. We test the effectiveness of our approach on both public data sets and large ones collected from the real world.</p><p>2 0.72976434 <a title="350-lsi-2" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>Author: Naman Turakhia, Devi Parikh</p><p>Abstract: When we look at an image, some properties or attributes of the image stand out more than others. When describing an image, people are likely to describe these dominant attributes first. Attribute dominance is a result of a complex interplay between the various properties present or absent in the image. Which attributes in an image are more dominant than others reveals rich information about the content of the image. In this paper we tap into this information by modeling attribute dominance. We show that this helps improve the performance of vision systems on a variety of human-centric applications such as zero-shot learning, image search and generating textual descriptions of images.</p><p>3 0.71946484 <a title="350-lsi-3" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Devi Parikh, Tsuhan Chen</p><p>Abstract: In recent years, there has been a great deal of progress in describing objects with attributes. Attributes have proven useful for object recognition, image search, face verification, image description, and zero-shot learning. Typically, attributes are either binary or relative: they describe either the presence or absence of a descriptive characteristic, or the relative magnitude of the characteristic when comparing two exemplars. However, prior work fails to model the actual way in which humans use these attributes in descriptive statements of images. Specifically, it does not address the important interactions between the binary and relative aspects of an attribute. In this work we propose a spoken attribute classifier which models a more natural way of using an attribute in a description. For each attribute we train a classifier which captures the specific way this attribute should be used. We show that as a result of using this model, we produce descriptions about images of people that are more natural and specific than past systems.</p><p>4 0.69709438 <a title="350-lsi-4" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>Author: Xiaoyang Wang, Qiang Ji</p><p>Abstract: This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. As a list of semantically meaningful properties of objects, attributes generally relate to each other statistically. In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and objectindependent attribute relationships. The model utilizes the captured relationships to benefit both attribute prediction and object recognition. Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases.</p><p>5 0.6911298 <a title="350-lsi-5" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>Author: Adriana Kovashka, Kristen Grauman</p><p>Abstract: Current methods learn monolithic attribute predictors, with the assumption that a single model is sufficient to reflect human understanding of a visual attribute. However, in reality, humans vary in how they perceive the association between a named property and image content. For example, two people may have slightly different internal models for what makes a shoe look “formal”, or they may disagree on which of two scenes looks “more cluttered”. Rather than discount these differences as noise, we propose to learn user-specific attribute models. We adapt a generic model trained with annotations from multiple users, tailoring it to satisfy user-specific labels. Furthermore, we propose novel techniques to infer user-specific labels based on transitivity and contradictions in the user’s search history. We demonstrate that adapted attributes improve accuracy over both existing monolithic models as well as models that learn from scratch with user-specific data alone. In addition, we show how adapted attributes are useful to personalize image search, whether with binary or relative attributes.</p><p>6 0.67750639 <a title="350-lsi-6" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>7 0.63549989 <a title="350-lsi-7" href="./iccv-2013-NEIL%3A_Extracting_Visual_Knowledge_from_Web_Data.html">285 iccv-2013-NEIL: Extracting Visual Knowledge from Web Data</a></p>
<p>8 0.62319249 <a title="350-lsi-8" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>9 0.59977365 <a title="350-lsi-9" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>10 0.58882689 <a title="350-lsi-10" href="./iccv-2013-The_Interestingness_of_Images.html">416 iccv-2013-The Interestingness of Images</a></p>
<p>11 0.57212299 <a title="350-lsi-11" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>12 0.54655081 <a title="350-lsi-12" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>13 0.53233999 <a title="350-lsi-13" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>14 0.52735132 <a title="350-lsi-14" href="./iccv-2013-Conservation_Tracking.html">87 iccv-2013-Conservation Tracking</a></p>
<p>15 0.52303702 <a title="350-lsi-15" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>16 0.52095181 <a title="350-lsi-16" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>17 0.51107895 <a title="350-lsi-17" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<p>18 0.50305676 <a title="350-lsi-18" href="./iccv-2013-The_Way_They_Move%3A_Tracking_Multiple_Targets_with_Similar_Appearance.html">418 iccv-2013-The Way They Move: Tracking Multiple Targets with Similar Appearance</a></p>
<p>19 0.50014526 <a title="350-lsi-19" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>20 0.48909453 <a title="350-lsi-20" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.06), (7, 0.011), (12, 0.03), (26, 0.068), (31, 0.033), (42, 0.114), (64, 0.088), (73, 0.03), (77, 0.299), (89, 0.139), (98, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75165552 <a title="350-lda-1" href="./iccv-2013-Relative_Attributes_for_Large-Scale_Abandoned_Object_Detection.html">350 iccv-2013-Relative Attributes for Large-Scale Abandoned Object Detection</a></p>
<p>Author: Quanfu Fan, Prasad Gabbur, Sharath Pankanti</p><p>Abstract: Effective reduction of false alarms in large-scale video surveillance is rather challenging, especially for applications where abnormal events of interest rarely occur, such as abandoned object detection. We develop an approach to prioritize alerts by ranking them, and demonstrate its great effectiveness in reducing false positives while keeping good detection accuracy. Our approach benefits from a novel representation of abandoned object alerts by relative attributes, namely staticness, foregroundness and abandonment. The relative strengths of these attributes are quantified using a ranking function[19] learnt on suitably designed low-level spatial and temporal features.These attributes of varying strengths are not only powerful in distinguishing abandoned objects from false alarms such as people and light artifacts, but also computationally efficient for large-scale deployment. With these features, we apply a linear ranking algorithm to sort alerts according to their relevance to the end-user. We test the effectiveness of our approach on both public data sets and large ones collected from the real world.</p><p>2 0.68726993 <a title="350-lda-2" href="./iccv-2013-Ensemble_Projection_for_Semi-supervised_Image_Classification.html">142 iccv-2013-Ensemble Projection for Semi-supervised Image Classification</a></p>
<p>Author: Dengxin Dai, Luc Van_Gool</p><p>Abstract: This paper investigates the problem of semi-supervised classification. Unlike previous methods to regularize classifying boundaries with unlabeled data, our method learns a new image representation from all available data (labeled and unlabeled) andperformsplain supervised learning with the new feature. In particular, an ensemble of image prototype sets are sampled automatically from the available data, to represent a rich set of visual categories/attributes. Discriminative functions are then learned on these prototype sets, and image are represented by the concatenation of their projected values onto the prototypes (similarities to them) for further classification. Experiments on four standard datasets show three interesting phenomena: (1) our method consistently outperforms previous methods for semi-supervised image classification; (2) our method lets itself combine well with these methods; and (3) our method works well for self-taught image classification where unlabeled data are not coming from the same distribution as la- beled ones, but rather from a random collection of images.</p><p>3 0.67976975 <a title="350-lda-3" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>Author: Zhongming Jin, Yao Hu, Yue Lin, Debing Zhang, Shiding Lin, Deng Cai, Xuelong Li</p><p>Abstract: Recently, hashing techniques have been widely applied to solve the approximate nearest neighbors search problem in many vision applications. Generally, these hashing approaches generate 2c buckets, where c is the length of the hash code. A good hashing method should satisfy the following two requirements: 1) mapping the nearby data points into the same bucket or nearby (measured by xue long l i opt . ac . cn @ a(a)b(b) the Hamming distance) buckets. 2) all the data points are evenly distributed among all the buckets. In this paper, we propose a novel algorithm named Complementary Projection Hashing (CPH) to find the optimal hashing functions which explicitly considers the above two requirements. Specifically, CPHaims at sequentiallyfinding a series ofhyperplanes (hashing functions) which cross the sparse region of the data. At the same time, the data points are evenly distributed in the hypercubes generated by these hyperplanes. The experiments comparing with the state-of-the-art hashing methods demonstrate the effectiveness of the proposed method.</p><p>4 0.66635203 <a title="350-lda-4" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>Author: S. Karthikeyan, Vignesh Jagadeesh, Renuka Shenoy, Miguel Ecksteinz, B.S. Manjunath</p><p>Abstract: Eye movement studies have confirmed that overt attention is highly biased towards faces and text regions in images. In this paper we explore a novel problem of predicting face and text regions in images using eye tracking data from multiple subjects. The problem is challenging as we aim to predict the semantics (face/text/background) only from eye tracking data without utilizing any image information. The proposed algorithm spatially clusters eye tracking data obtained in an image into different coherent groups and subsequently models the likelihood of the clusters containing faces and text using afully connectedMarkov Random Field (MRF). Given the eye tracking datafrom a test image, itpredicts potential face/head (humans, dogs and cats) and text locations reliably. Furthermore, the approach can be used to select regions of interest for further analysis by object detectors for faces and text. The hybrid eye position/object detector approach achieves better detection performance and reduced computation time compared to using only the object detection algorithm. We also present a new eye tracking dataset on 300 images selected from ICDAR, Street-view, Flickr and Oxford-IIIT Pet Dataset from 15 subjects.</p><p>5 0.65788847 <a title="350-lda-5" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<p>Author: Tatiana Tommasi, Barbara Caputo</p><p>Abstract: Over the last years, several authors have signaled that state of the art categorization methods fail to perform well when trained and tested on data from different databases. The general consensus in the literature is that this issue, known as domain adaptation and/or dataset bias, is due to a distribution mismatch between data collections. Methods addressing it go from max-margin classifiers to learning how to modify the features and obtain a more robust representation. The large majority of these works use BOW feature descriptors, and learning methods based on imageto-image distance functions. Following the seminal work of [6], in this paper we challenge these two assumptions. We experimentally show that using the NBNN classifier over existing domain adaptation databases achieves always very strong performances. We build on this result, and present an NBNN-based domain adaptation algorithm that learns iteratively a class metric while inducing, for each sample, a large margin separation among classes. To the best of our knowledge, this is the first work casting the domain adaptation problem within the NBNN framework. Experiments show that our method achieves the state of the art, both in the unsupervised and semi-supervised settings.</p><p>6 0.63731772 <a title="350-lda-6" href="./iccv-2013-Enhanced_Continuous_Tabu_Search_for_Parameter_Estimation_in_Multiview_Geometry.html">141 iccv-2013-Enhanced Continuous Tabu Search for Parameter Estimation in Multiview Geometry</a></p>
<p>7 0.63353956 <a title="350-lda-7" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>8 0.59055114 <a title="350-lda-8" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>9 0.58995372 <a title="350-lda-9" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>10 0.58801603 <a title="350-lda-10" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>11 0.58730328 <a title="350-lda-11" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>12 0.58219326 <a title="350-lda-12" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>13 0.58180493 <a title="350-lda-13" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>14 0.5814991 <a title="350-lda-14" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>15 0.58135182 <a title="350-lda-15" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>16 0.58123267 <a title="350-lda-16" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>17 0.5807879 <a title="350-lda-17" href="./iccv-2013-Incorporating_Cloud_Distribution_in_Sky_Representation.html">215 iccv-2013-Incorporating Cloud Distribution in Sky Representation</a></p>
<p>18 0.57911968 <a title="350-lda-18" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>19 0.57894474 <a title="350-lda-19" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>20 0.57848489 <a title="350-lda-20" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
