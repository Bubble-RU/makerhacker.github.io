<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>327 iccv-2013-Predicting an Object Location Using a Global Image Representation</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-327" href="#">iccv2013-327</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>327 iccv-2013-Predicting an Object Location Using a Global Image Representation</h1>
<br/><p>Source: <a title="iccv-2013-327-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Serrano_Predicting_an_Object_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Jose A. Rodriguez Serrano, Diane Larlus</p><p>Abstract: We tackle the detection of prominent objects in images as a retrieval task: given a global image descriptor, we find the most similar images in an annotated dataset, and transfer the object bounding boxes. We refer to this approach as data driven detection (DDD), that is an alternative to sliding windows. Previous works have used similar notions but with task-independent similarities and representations, i.e. they were not tailored to the end-goal of localization. This article proposes two contributions: (i) a metric learning algorithm and (ii) a representation of images as object probability maps, that are both optimized for detection. We show experimentally that these two contributions are crucial to DDD, do not require costly additional operations, and in some cases yield comparable or better results than state-of-the-art detectors despite conceptual simplicity and increased speed. As an application of prominent object detection, we improve fine-grained categorization by precropping images with the proposed approach.</p><p>Reference: <a title="iccv-2013-327-reference" href="../iccv2013_reference/iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We tackle the detection of prominent objects in images as a retrieval task: given a global image descriptor, we find the most similar images in an annotated dataset, and transfer the object bounding boxes. [sent-6, score-0.677]
</p><p>2 We refer to this approach as data driven detection (DDD), that is an alternative to sliding windows. [sent-7, score-0.29]
</p><p>3 This article proposes two contributions: (i) a metric learning algorithm and (ii) a representation of images as object probability maps, that are both optimized for detection. [sent-11, score-0.354]
</p><p>4 As an application of prominent object detection, we improve fine-grained categorization by precropping images with the proposed approach. [sent-13, score-0.3]
</p><p>5 Introduction  This paper deals with the problem of prominent object detection, where the goal is to predict the region containing the relevant subject (or object of interest) in an image, as opposed to other regions containing background or nonrelevant objects. [sent-15, score-0.3]
</p><p>6 The localization of the prominent object is a key cue that can be used to improve this difficult  Figure1. [sent-26, score-0.27]
</p><p>7 A query image is compared to an annotated set, and the nearest neighbors transfer their annotations. [sent-28, score-0.292]
</p><p>8 For this retrieval process, we use supervision to learn an image representation and a metric geared toward detection. [sent-29, score-0.254]
</p><p>9 As these examples show, the definition of prominent or relevant object is application-dependent, and we assume this is defined by a training set with annotated bounding boxes. [sent-31, score-0.354]
</p><p>10 Research in detection has converged to combining a template descriptor (e. [sent-33, score-0.168]
</p><p>11 HOG [6] or its extension to deformable parts [10]) with sliding windows. [sent-35, score-0.197]
</p><p>12 Although methods for accelerating sliding window search have been proposed, a large number of window descriptors have to be extracted and classified independently. [sent-37, score-0.375]
</p><p>13 In this paper, we aim at an efficient solution, and propose to extract a single global feature for the input image and  to estimate the prominent object location directly from this feature vector, avoiding sliding windows. [sent-38, score-0.48]
</p><p>14 This suggests a simple retrieval-based method for prominent object localization: given an input image, find the nearest images from a database (using the global descriptor), and transfer the bounding box of the most similar image. [sent-40, score-0.49]
</p><p>15 First, detection is performed at the ease and efficiency of a retrieval operation. [sent-42, score-0.211]
</p><p>16 Second, it allows handling any object shape and does not rely on the rectangular and rigid object assumption of the sliding-window approaches. [sent-43, score-0.154]
</p><p>17 Finally, as detection is obtained using a global image descriptor, it intrinsically leverages context for detection. [sent-44, score-0.163]
</p><p>18 We apply metric learning to enforce image pairs with high overlap between detection rectangles to be more similar than images with no (or small) overlap. [sent-52, score-0.336]
</p><p>19 We propose to use a compact image representation that is constructed from the probability ofeach image patch to belong to the object. [sent-54, score-0.242]
</p><p>20 This requires some knowledge about the object to locate, but allows representing the image as a probability map of the object location. [sent-55, score-0.235]
</p><p>21 Since there exists a strong correlation between the true detection and such a probability map, these features are well-suited to data-driven detection. [sent-56, score-0.246]
</p><p>22 Both contributions use supervision to connect the features and similarity to the detection task, by converting an initial assumption (similar images tend to have similar layouts) into an actual optimization step (we learn what makes two layouts similar). [sent-57, score-0.252]
</p><p>23 Finally, we show in fine-grained categorization experiments that we improve classification accuracy by concentrating on the region predicted by a DDD. [sent-64, score-0.151]
</p><p>24 De-facto standard detection methods [6, 10] combine a template representation and a sliding window approach. [sent-76, score-0.419]
</p><p>25 Additional abundant work on detection has been published but the most successful methods cast detection  as a classification problem: a large number of classification operations are performed (each possible sub-window is classified as containing the object or not). [sent-80, score-0.444]
</p><p>26 In contrast, in this paper we would like to take a data-driven approach, and cast detection as a retrieval problem. [sent-81, score-0.211]
</p><p>27 It differs from DDD in spirit (it is not application dependent) and in practice (it is a sliding window approach). [sent-85, score-0.244]
</p><p>28 The concept of transferring bounding boxes (or pixel-level masks) from the nearest neighbors of an image has been successfully exploited in previous work. [sent-87, score-0.28]
</p><p>29 Two main strategies exist: transfer at subwindow level and transfer at full-image level. [sent-88, score-0.16]
</p><p>30 In the first strategy, approaches still perform sliding window search and each sub-window is used as a query for which nearest-neighbor similarity is computed [3 1, 22]. [sent-89, score-0.35]
</p><p>31 A similar case is the figure-ground segmentation of [15] where sliding window search is replaced by an objectness detector [2]. [sent-90, score-0.274]
</p><p>32 Although these works clearly have a data-driven component that is key to their success (the scoring of subwindows), they still compare all sub-windows of an image against a database, and pay a complexity price that it at least as big as the one paid by sliding window approaches. [sent-91, score-0.36]
</p><p>33 All these works boil-down to the same principle: find the  nearest neighbors of the input image; and feed the annotations of those to a more complex method. [sent-93, score-0.177]
</p><p>34 Similarly, [27] uses the neighbors to induce priors over object classes and bounding box locations in a graphical model. [sent-95, score-0.252]
</p><p>35 We observe that for the second strategy, transferring the labels of the neighbors is crucial to guide the algorithm, but as the retrieval step is based on task-independent features and similarities, this is not sufficient. [sent-96, score-0.22]
</p><p>36 In [15] a global neighbor transfer baseline produces poor results in a multi-object binary segmentation task. [sent-98, score-0.165]
</p><p>37 Data-driven detection baseline Our goal is to infer the bounding box of the prominent object from the global feature vector of the image, using a training set with annotated bounding boxes. [sent-101, score-0.675]
</p><p>38 Task-aware metric Our first contribution is a similarity learning algorithm that optimizes a detection criterion. [sent-127, score-0.281]
</p><p>39 We are not aware of previous works applying metric learning on object location labels. [sent-130, score-0.206]
</p><p>40 For object localization, a common similarity is the overlap score, defined as the union-to-intersection area ratio (e. [sent-136, score-0.163]
</p><p>41 More precisely, we consider a similarity function which augments the dot product as: kW(q, x) = qTWx. [sent-145, score-0.143]
</p><p>42 Task-aware representation: patch-level object classifiers are used to represent query images by probability maps. [sent-165, score-0.228]
</p><p>43 Annotations of  training images with similar probability maps are transferred to solve detection. [sent-166, score-0.215]
</p><p>44 Task-aware representation The second contribution is an improved representation built using supervised information of the detection task. [sent-195, score-0.219]
</p><p>45 More precisely, we propose to build a “probability map” indicating probabilities that a certain pixel contains the prominent object. [sent-196, score-0.18]
</p><p>46 This is based on a patch-level classifier that has been pre-trained to distinguish between patches from  prominent objects and from the rest of the image. [sent-197, score-0.219]
</p><p>47 We highlight that probability maps constitute responses of local patch classifiers, which are noisy and smooth, and that the response map of explicit object detectors (e. [sent-198, score-0.358]
</p><p>48 However, object banks suffer from the same limitations as sliding-window based approaches, while the probability map is fast to compute (just one extra dot product on top of the patch encoding). [sent-201, score-0.392]
</p><p>49 Probability maps have been used as an input for object segmentation, to classify super-pixels [5], as the unary potential of a random field [17], or with auto-context algorithms [32]. [sent-202, score-0.195]
</p><p>50 In our case, we use probability maps directly as an image representation within data-driven detection. [sent-203, score-0.259]
</p><p>51 Patches are extracted densely and at multiple scales within images of the training set, and are associated to a binary label depending on their degree of overlap with the annotated object region. [sent-205, score-0.149]
</p><p>52 Patch-level descriptors and their labels are used to train a linear SVM classifier which assigns each patch to the “object” or “background” class. [sent-208, score-0.168]
</p><p>53 These scores are transformed  into probabilities at the pixel level [5], yielding probability maps (see Fig. [sent-214, score-0.215]
</p><p>54 To make them comparable, the probability maps are resized to a small fixed-size image (50x50 pixels), ? [sent-217, score-0.256]
</p><p>55 First, by nature, this representation captures information about the end task, so images having similar representations are more likely to have similar detection annotations. [sent-221, score-0.225]
</p><p>56 This means that, despite the extra cost at training time to learn an object classifier, and the small constant cost at test time to compute the map, the smaller dimension of this representation makes retrieval and consequently detection much faster. [sent-223, score-0.315]
</p><p>57 Finally, one can combine several lowlevel cues (for instance SIFT and color) without increasing the final dimension of the representation by averaging maps computed using different channels [5]. [sent-226, score-0.192]
</p><p>58 Since probability maps are a strong cue for object location, one may wonder why not using directly a sliding win-  ×  dow over the probability map. [sent-227, score-0.549]
</p><p>59 Intuitively, patch level classifiers are far from perfect, but we expect classifier errors to be consistent between similar training and query images. [sent-229, score-0.175]
</p><p>60 Therefore, even for inaccurate probability maps, the closest maps in the database can help transferring the object location reliably. [sent-230, score-0.377]
</p><p>61 In all experiments, the number of neighbors L are determined from the validation set. [sent-245, score-0.143]
</p><p>62 For our task-aware features, the probability maps are built from FVs computed at patch level as in [5]. [sent-249, score-0.298]
</p><p>63 This essentially follows the same process as explained above but computing one FV per patch instead of aggregating the patch contributions. [sent-250, score-0.166]
</p><p>64 The probability maps using FVs computed from lowlevel SIFT descriptors are denoted PM-S IFT. [sent-255, score-0.309]
</p><p>65 We also consider color statistics [25] as low-level descriptors (PM-COL) and the average of both maps (PM-S IFT+COL). [sent-256, score-0.146]
</p><p>66 While the database an-  notations are at the level of body joint locations (knees, elbows, neck, etc), it has been designed so that there is one prominent person (i. [sent-262, score-0.18]
</p><p>67 We would like to evaluate the prominent subject detection task on these challenging images. [sent-265, score-0.311]
</p><p>68 To this end, we transformed the annotation, and obtained ground-truth rectangles by taking the bounding rectangle of the body joint annotations. [sent-266, score-0.206]
</p><p>69 The quality of the detection is evaluated using the overlap score of Eq. [sent-268, score-0.181]
</p><p>70 SIFT), the task-aware representation PM-S IFT still compares favorably to the baseline (+5. [sent-274, score-0.169]
</p><p>71 We also combine probability maps to a sliding window (SW) process. [sent-284, score-0.459]
</p><p>72 The rectangle with the best density score (density inside the rectangle minus density outside) is kept. [sent-285, score-0.146]
</p><p>73 Comparison of DDD for FV (and different spatial pooling G=GWxGH) and PM (for different resized map sizes) with and without metric learning. [sent-288, score-0.168]
</p><p>74 achieves competitive results (confirming that probability maps are powerful representations), but is outperformed by our best DDD strategies. [sent-289, score-0.215]
</p><p>75 The previous results use SIFT as lowlevel descriptor in the probability maps to be comparable to the DPM and DDD baselines that are based on gradients and do not use color. [sent-302, score-0.334]
</p><p>76 When combining low-level SIFT and color descriptors in the probability maps (PM-S IFT+COL) and using metric learning, we obtain the best results with 44. [sent-304, score-0.358]
</p><p>77 ImageNet dogs is a dataset of dog images used for fine-grained classification purposes [1]. [sent-310, score-0.316]
</p><p>78 Here, the dog is the prominent object, and we measure the dog detection task, and the effect of our task-aware method on the final classification accuracy. [sent-311, score-0.596]
</p><p>79 The dataset is composed of 120 different breeds of dogs, making the detection challenging (see Fig. [sent-312, score-0.175]
</p><p>80 As the test annotations are not available, we have split the validation set into 1,000 images  that we use as an actual validation set to find the best parameters, and the remaining 5,000 images are used as our  TOaDbstklhe-2rw. [sent-315, score-0.169]
</p><p>81 We used a generic dog classifier in the task-aware representation, trained using all 120 types of annotations. [sent-318, score-0.151]
</p><p>82 The sliding window (PM-S IFT+SW) is on par with the FV-S IFT. [sent-339, score-0.285]
</p><p>83 We found it tends to fail for dogs in “non-canonical poses”. [sent-343, score-0.143]
</p><p>84 One could also ask whether detection is trivial just because dogs seem centered. [sent-346, score-0.274]
</p><p>85 We assume that the object location is available at train time (as for DDD), and we train classifiers over the 120 breeds of dogs using the cropped images. [sent-357, score-0.296]
</p><p>86 At test time, we use the bounding boxes predicted by our system to crop images, and classify the cropped region. [sent-358, score-0.172]
</p><p>87 Classification results are reported in Table 3, and compared to (i) the classification of full images, and (ii) cropping using the ground-truth detections (to measure how far we are from the classification figure of a perfect detection). [sent-363, score-0.213]
</p><p>88 We show detection results for generic bird detectors (the bird being the prominent object of each image). [sent-370, score-0.515]
</p><p>89 The overlap threshold to compute precision is set 70% for the same reason as in the dog set. [sent-372, score-0.162]
</p><p>90 For the same reason as in the dogs dataset, we concentrate on the task-aware representations. [sent-376, score-0.143]
</p><p>91 In this set, probability maps based on color (PM-COL) yield better results than those based on SIFT (PM-S IFT), probably due to the colorful nature of birds. [sent-377, score-0.215]
</p><p>92 The sliding window (PM-SIFT+SW) yields the same behavior as in the dogs set. [sent-387, score-0.387]
</p><p>93 We also conduct a  fine-grained classification experiment on this set with the same protocol as before, and measure the impact of detection on the final classification accuracy. [sent-389, score-0.253]
</p><p>94 As expected, detection has an impact on fine-grained classification (increase of > 13%). [sent-393, score-0.192]
</p><p>95 But if we inspect the detection precision at 20% overlap (which corresponds to estimating the object location roughly), the DPM is at 95. [sent-398, score-0.29]
</p><p>96 Note the difference in classification accuracy between perfect detection and DPM detection is 4. [sent-416, score-0.362]
</p><p>97 Since the proposed method still reduces to finding nearest neighbors at test time using a single feature vector per image, and the two contributions significantly reduce the dimension of the representation, we avoid sliding window search, and the retrieval process is fast (about 200ms per image). [sent-420, score-0.444]
</p><p>98 DDD compares favorably to a stateof-the art sliding window approach in presence of nonrigid objects. [sent-421, score-0.316]
</p><p>99 It compares less favorably for rigid objects (as birds) but appears to be good enough as pre-cropping  method for fine-grained classification results. [sent-422, score-0.167]
</p><p>100 Mobile product image search by automatic query object extraction. [sent-627, score-0.147]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ddd', 0.584), ('ift', 0.243), ('prominent', 0.18), ('sliding', 0.159), ('dpm', 0.147), ('dogs', 0.143), ('ijk', 0.143), ('detection', 0.131), ('kw', 0.122), ('probability', 0.115), ('dog', 0.112), ('maps', 0.1), ('metric', 0.097), ('fv', 0.094), ('neighbors', 0.087), ('window', 0.085), ('patch', 0.083), ('birds', 0.083), ('transfer', 0.08), ('retrieval', 0.08), ('col', 0.078), ('lsp', 0.076), ('bounding', 0.075), ('rectangle', 0.073), ('bird', 0.072), ('ri', 0.069), ('classification', 0.061), ('triplets', 0.06), ('object', 0.06), ('categorization', 0.06), ('ku', 0.059), ('rectangles', 0.058), ('annotations', 0.057), ('lijk', 0.057), ('taskindependent', 0.057), ('validation', 0.056), ('dot', 0.056), ('fisher', 0.055), ('baseline', 0.053), ('query', 0.053), ('transferring', 0.053), ('similarity', 0.053), ('similarities', 0.052), ('cropping', 0.052), ('sift', 0.051), ('overlap', 0.05), ('dim', 0.05), ('representations', 0.05), ('location', 0.049), ('imagenet', 0.048), ('lowlevel', 0.048), ('unreasonable', 0.047), ('descriptors', 0.046), ('representation', 0.044), ('banks', 0.044), ('fvs', 0.044), ('breeds', 0.044), ('leeds', 0.044), ('pascal', 0.044), ('sw', 0.044), ('resized', 0.041), ('big', 0.041), ('par', 0.041), ('pay', 0.04), ('favorably', 0.04), ('classifier', 0.039), ('annotated', 0.039), ('rd', 0.039), ('perfect', 0.039), ('costly', 0.039), ('pm', 0.039), ('deformable', 0.038), ('article', 0.038), ('descriptor', 0.037), ('dimensionality', 0.036), ('classify', 0.035), ('layouts', 0.035), ('paid', 0.035), ('russell', 0.034), ('confirming', 0.034), ('product', 0.034), ('baselines', 0.034), ('rigid', 0.034), ('supervision', 0.033), ('species', 0.033), ('nearest', 0.033), ('boxes', 0.032), ('compares', 0.032), ('triplet', 0.032), ('ij', 0.032), ('global', 0.032), ('anchez', 0.031), ('loss', 0.031), ('predicted', 0.03), ('objectness', 0.03), ('pooling', 0.03), ('box', 0.03), ('strategy', 0.03), ('localization', 0.03), ('acts', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="327-tfidf-1" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>Author: Jose A. Rodriguez Serrano, Diane Larlus</p><p>Abstract: We tackle the detection of prominent objects in images as a retrieval task: given a global image descriptor, we find the most similar images in an annotated dataset, and transfer the object bounding boxes. We refer to this approach as data driven detection (DDD), that is an alternative to sliding windows. Previous works have used similar notions but with task-independent similarities and representations, i.e. they were not tailored to the end-goal of localization. This article proposes two contributions: (i) a metric learning algorithm and (ii) a representation of images as object probability maps, that are both optimized for detection. We show experimentally that these two contributions are crucial to DDD, do not require costly additional operations, and in some cases yield comparable or better results than state-of-the-art detectors despite conceptual simplicity and increased speed. As an application of prominent object detection, we improve fine-grained categorization by precropping images with the proposed approach.</p><p>2 0.20294988 <a title="327-tfidf-2" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>Author: Ning Zhang, Ryan Farrell, Forrest Iandola, Trevor Darrell</p><p>Abstract: Recognizing objects in fine-grained domains can be extremely challenging due to the subtle differences between subcategories. Discriminative markings are often highly localized, leading traditional object recognition approaches to struggle with the large pose variation often present in these domains. Pose-normalization seeks to align training exemplars, either piecewise by part or globally for the whole object, effectively factoring out differences in pose and in viewing angle. Prior approaches relied on computationally-expensive filter ensembles for part localization and required extensive supervision. This paper proposes two pose-normalized descriptors based on computationally-efficient deformable part models. The first leverages the semantics inherent in strongly-supervised DPM parts. The second exploits weak semantic annotations to learn cross-component correspondences, computing pose-normalized descriptors from the latent parts of a weakly-supervised DPM. These representations enable pooling across pose and viewpoint, in turn facilitating tasks such as fine-grained recognition and attribute prediction. Experiments conducted on the Caltech-UCSD Birds 200 dataset and Berkeley Human Attribute dataset demonstrate significant improvements over state-of-art algorithms.</p><p>3 0.17202625 <a title="327-tfidf-3" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>Author: Ramazan Gokberk Cinbis, Jakob Verbeek, Cordelia Schmid</p><p>Abstract: We present an object detection system based on the Fisher vector (FV) image representation computed over SIFT and color descriptors. For computational and storage efficiency, we use a recent segmentation-based method to generate class-independent object detection hypotheses, in combination with data compression techniques. Our main contribution is a method to produce tentative object segmentation masks to suppress background clutter in the features. Re-weighting the local image features based on these masks is shown to improve object detection significantly. We also exploit contextual features in the form of a full-image FV descriptor, and an inter-category rescoring mechanism. Our experiments on the PASCAL VOC 2007 and 2010 datasets show that our detector improves over the current state-of-the-art detection results.</p><p>4 0.16450244 <a title="327-tfidf-4" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>Author: Yuning Chai, Victor Lempitsky, Andrew Zisserman</p><p>Abstract: We propose a new method for the task of fine-grained visual categorization. The method builds a model of the baselevel category that can be fitted to images, producing highquality foreground segmentation and mid-level part localizations. The model can be learnt from the typical datasets available for fine-grained categorization, where the only annotation provided is a loose bounding box around the instance (e.g. bird) in each image. Both segmentation and part localizations are then used to encode the image content into a highly-discriminative visual signature. The model is symbiotic in that part discovery/localization is helped by segmentation and, conversely, the segmentation is helped by the detection (e.g. part layout). Our model builds on top of the part-based object category detector of Felzenszwalb et al., and also on the powerful GrabCut segmentation algorithm of Rother et al., and adds a simple spatial saliency coupling between them. In our evaluation, the model improves the categorization accuracy over the state-of-the-art. It also improves over what can be achieved with an analogous system that runs segmentation and part-localization independently.</p><p>5 0.14259429 <a title="327-tfidf-5" href="./iccv-2013-Fine-Grained_Categorization_by_Alignments.html">169 iccv-2013-Fine-Grained Categorization by Alignments</a></p>
<p>Author: E. Gavves, B. Fernando, C.G.M. Snoek, A.W.M. Smeulders, T. Tuytelaars</p><p>Abstract: The aim of this paper is fine-grained categorization without human interaction. Different from prior work, which relies on detectors for specific object parts, we propose to localize distinctive details by roughly aligning the objects using just the overall shape, since implicit to fine-grained categorization is the existence of a super-class shape shared among all classes. The alignments are then used to transfer part annotations from training images to test images (supervised alignment), or to blindly yet consistently segment the object in a number of regions (unsupervised alignment). We furthermore argue that in the distinction of finegrained sub-categories, classification-oriented encodings like Fisher vectors are better suited for describing localized information than popular matching oriented features like HOG. We evaluate the method on the CU-2011 Birds and Stanford Dogs fine-grained datasets, outperforming the state-of-the-art.</p><p>6 0.14155956 <a title="327-tfidf-6" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>7 0.11996345 <a title="327-tfidf-7" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>8 0.11907513 <a title="327-tfidf-8" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>9 0.10395043 <a title="327-tfidf-9" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>10 0.10239962 <a title="327-tfidf-10" href="./iccv-2013-Decomposing_Bag_of_Words_Histograms.html">104 iccv-2013-Decomposing Bag of Words Histograms</a></p>
<p>11 0.10063899 <a title="327-tfidf-11" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>12 0.098852523 <a title="327-tfidf-12" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>13 0.097511321 <a title="327-tfidf-13" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>14 0.097419739 <a title="327-tfidf-14" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>15 0.094666429 <a title="327-tfidf-15" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>16 0.093568988 <a title="327-tfidf-16" href="./iccv-2013-Unbiased_Metric_Learning%3A_On_the_Utilization_of_Multiple_Datasets_and_Web_Images_for_Softening_Bias.html">431 iccv-2013-Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias</a></p>
<p>17 0.091964513 <a title="327-tfidf-17" href="./iccv-2013-Modeling_Occlusion_by_Discriminative_AND-OR_Structures.html">269 iccv-2013-Modeling Occlusion by Discriminative AND-OR Structures</a></p>
<p>18 0.091595814 <a title="327-tfidf-18" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>19 0.090159379 <a title="327-tfidf-19" href="./iccv-2013-Strong_Appearance_and_Expressive_Spatial_Models_for_Human_Pose_Estimation.html">403 iccv-2013-Strong Appearance and Expressive Spatial Models for Human Pose Estimation</a></p>
<p>20 0.088011935 <a title="327-tfidf-20" href="./iccv-2013-Discriminatively_Trained_Templates_for_3D_Object_Detection%3A_A_Real_Time_Scalable_Approach.html">121 iccv-2013-Discriminatively Trained Templates for 3D Object Detection: A Real Time Scalable Approach</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.256), (1, 0.053), (2, 0.017), (3, -0.098), (4, 0.094), (5, 0.03), (6, -0.041), (7, 0.02), (8, -0.087), (9, -0.067), (10, 0.104), (11, 0.025), (12, -0.025), (13, -0.091), (14, -0.015), (15, -0.058), (16, 0.049), (17, 0.017), (18, 0.1), (19, -0.012), (20, 0.011), (21, 0.024), (22, -0.016), (23, 0.04), (24, -0.002), (25, 0.13), (26, 0.005), (27, -0.047), (28, 0.035), (29, 0.049), (30, 0.026), (31, -0.099), (32, -0.033), (33, -0.015), (34, -0.034), (35, -0.041), (36, 0.083), (37, -0.038), (38, -0.021), (39, 0.012), (40, 0.005), (41, 0.03), (42, -0.005), (43, 0.018), (44, 0.039), (45, -0.029), (46, 0.055), (47, -0.012), (48, 0.085), (49, -0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94322622 <a title="327-lsi-1" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>Author: Jose A. Rodriguez Serrano, Diane Larlus</p><p>Abstract: We tackle the detection of prominent objects in images as a retrieval task: given a global image descriptor, we find the most similar images in an annotated dataset, and transfer the object bounding boxes. We refer to this approach as data driven detection (DDD), that is an alternative to sliding windows. Previous works have used similar notions but with task-independent similarities and representations, i.e. they were not tailored to the end-goal of localization. This article proposes two contributions: (i) a metric learning algorithm and (ii) a representation of images as object probability maps, that are both optimized for detection. We show experimentally that these two contributions are crucial to DDD, do not require costly additional operations, and in some cases yield comparable or better results than state-of-the-art detectors despite conceptual simplicity and increased speed. As an application of prominent object detection, we improve fine-grained categorization by precropping images with the proposed approach.</p><p>2 0.84080023 <a title="327-lsi-2" href="./iccv-2013-Detecting_Avocados_to_Zucchinis%3A_What_Have_We_Done%2C_and_Where_Are_We_Going%3F.html">109 iccv-2013-Detecting Avocados to Zucchinis: What Have We Done, and Where Are We Going?</a></p>
<p>Author: Olga Russakovsky, Jia Deng, Zhiheng Huang, Alexander C. Berg, Li Fei-Fei</p><p>Abstract: The growth of detection datasets and the multiple directions of object detection research provide both an unprecedented need and a great opportunity for a thorough evaluation of the current state of the field of categorical object detection. In this paper we strive to answer two key questions. First, where are we currently as a field: what have we done right, what still needs to be improved? Second, where should we be going in designing the next generation of object detectors? Inspired by the recent work of Hoiem et al. [10] on the standard PASCAL VOC detection dataset, we perform a large-scale study on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) data. First, we quantitatively demonstrate that this dataset provides many of the same detection challenges as the PASCAL VOC. Due to its scale of 1000 object categories, ILSVRC also provides an excellent testbed for understanding the performance of detectors as a function of several key properties of the object classes. We conduct a series of analyses looking at how different detection methods perform on a number of imagelevel and object-class-levelproperties such as texture, color, deformation, and clutter. We learn important lessons of the current object detection methods and propose a number of insights for designing the next generation object detectors.</p><p>3 0.83278948 <a title="327-lsi-3" href="./iccv-2013-Fine-Grained_Categorization_by_Alignments.html">169 iccv-2013-Fine-Grained Categorization by Alignments</a></p>
<p>Author: E. Gavves, B. Fernando, C.G.M. Snoek, A.W.M. Smeulders, T. Tuytelaars</p><p>Abstract: The aim of this paper is fine-grained categorization without human interaction. Different from prior work, which relies on detectors for specific object parts, we propose to localize distinctive details by roughly aligning the objects using just the overall shape, since implicit to fine-grained categorization is the existence of a super-class shape shared among all classes. The alignments are then used to transfer part annotations from training images to test images (supervised alignment), or to blindly yet consistently segment the object in a number of regions (unsupervised alignment). We furthermore argue that in the distinction of finegrained sub-categories, classification-oriented encodings like Fisher vectors are better suited for describing localized information than popular matching oriented features like HOG. We evaluate the method on the CU-2011 Birds and Stanford Dogs fine-grained datasets, outperforming the state-of-the-art.</p><p>4 0.81008464 <a title="327-lsi-4" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>Author: Xiaoyu Wang, Ming Yang, Shenghuo Zhu, Yuanqing Lin</p><p>Abstract: Generic object detection is confronted by dealing with different degrees of variations in distinct object classes with tractable computations, which demands for descriptive and flexible object representations that are also efficient to evaluate for many locations. In view of this, we propose to model an object class by a cascaded boosting classifier which integrates various types of features from competing local regions, named as regionlets. A regionlet is a base feature extraction region defined proportionally to a detection window at an arbitrary resolution (i.e. size and aspect ratio). These regionlets are organized in small groups with stable relative positions to delineate fine-grained spatial layouts inside objects. Their features are aggregated to a one-dimensional feature within one group so as to tolerate deformations. Then we evaluate the object bounding box proposal in selective search from segmentation cues, limiting the evaluation locations to thousands. Our approach significantly outperforms the state-of-the-art on popular multi-class detection benchmark datasets with a single method, without any contexts. It achieves the detec- tion mean average precision of 41. 7% on the PASCAL VOC 2007 dataset and 39. 7% on the VOC 2010 for 20 object categories. It achieves 14. 7% mean average precision on the ImageNet dataset for 200 object categories, outperforming the latest deformable part-based model (DPM) by 4. 7%.</p><p>5 0.79994845 <a title="327-lsi-5" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>Author: Ramazan Gokberk Cinbis, Jakob Verbeek, Cordelia Schmid</p><p>Abstract: We present an object detection system based on the Fisher vector (FV) image representation computed over SIFT and color descriptors. For computational and storage efficiency, we use a recent segmentation-based method to generate class-independent object detection hypotheses, in combination with data compression techniques. Our main contribution is a method to produce tentative object segmentation masks to suppress background clutter in the features. Re-weighting the local image features based on these masks is shown to improve object detection significantly. We also exploit contextual features in the form of a full-image FV descriptor, and an inter-category rescoring mechanism. Our experiments on the PASCAL VOC 2007 and 2010 datasets show that our detector improves over the current state-of-the-art detection results.</p><p>6 0.79535228 <a title="327-lsi-6" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>7 0.77548337 <a title="327-lsi-7" href="./iccv-2013-Decomposing_Bag_of_Words_Histograms.html">104 iccv-2013-Decomposing Bag of Words Histograms</a></p>
<p>8 0.77262586 <a title="327-lsi-8" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>9 0.76818222 <a title="327-lsi-9" href="./iccv-2013-Codemaps_-_Segment%2C_Classify_and_Search_Objects_Locally.html">77 iccv-2013-Codemaps - Segment, Classify and Search Objects Locally</a></p>
<p>10 0.75435317 <a title="327-lsi-10" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>11 0.74925584 <a title="327-lsi-11" href="./iccv-2013-How_Do_You_Tell_a_Blackbird_from_a_Crow%3F.html">202 iccv-2013-How Do You Tell a Blackbird from a Crow?</a></p>
<p>12 0.73490512 <a title="327-lsi-12" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<p>13 0.71953148 <a title="327-lsi-13" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>14 0.70946783 <a title="327-lsi-14" href="./iccv-2013-Heterogeneous_Auto-similarities_of_Characteristics_%28HASC%29%3A_Exploiting_Relational_Information_for_Classification.html">193 iccv-2013-Heterogeneous Auto-similarities of Characteristics (HASC): Exploiting Relational Information for Classification</a></p>
<p>15 0.69682306 <a title="327-lsi-15" href="./iccv-2013-HOGgles%3A_Visualizing_Object_Detection_Features.html">189 iccv-2013-HOGgles: Visualizing Object Detection Features</a></p>
<p>16 0.68446594 <a title="327-lsi-16" href="./iccv-2013-Shufflets%3A_Shared_Mid-level_Parts_for_Fast_Object_Detection.html">390 iccv-2013-Shufflets: Shared Mid-level Parts for Fast Object Detection</a></p>
<p>17 0.664702 <a title="327-lsi-17" href="./iccv-2013-Unbiased_Metric_Learning%3A_On_the_Utilization_of_Multiple_Datasets_and_Web_Images_for_Softening_Bias.html">431 iccv-2013-Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias</a></p>
<p>18 0.65785068 <a title="327-lsi-18" href="./iccv-2013-NEIL%3A_Extracting_Visual_Knowledge_from_Web_Data.html">285 iccv-2013-NEIL: Extracting Visual Knowledge from Web Data</a></p>
<p>19 0.65367556 <a title="327-lsi-19" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>20 0.64775884 <a title="327-lsi-20" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.134), (2, 0.089), (7, 0.025), (12, 0.016), (13, 0.025), (26, 0.097), (31, 0.075), (34, 0.028), (35, 0.02), (42, 0.121), (48, 0.011), (64, 0.047), (73, 0.035), (89, 0.176), (98, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88772726 <a title="327-lda-1" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>Author: Jose A. Rodriguez Serrano, Diane Larlus</p><p>Abstract: We tackle the detection of prominent objects in images as a retrieval task: given a global image descriptor, we find the most similar images in an annotated dataset, and transfer the object bounding boxes. We refer to this approach as data driven detection (DDD), that is an alternative to sliding windows. Previous works have used similar notions but with task-independent similarities and representations, i.e. they were not tailored to the end-goal of localization. This article proposes two contributions: (i) a metric learning algorithm and (ii) a representation of images as object probability maps, that are both optimized for detection. We show experimentally that these two contributions are crucial to DDD, do not require costly additional operations, and in some cases yield comparable or better results than state-of-the-art detectors despite conceptual simplicity and increased speed. As an application of prominent object detection, we improve fine-grained categorization by precropping images with the proposed approach.</p><p>2 0.88140422 <a title="327-lda-2" href="./iccv-2013-A_Method_of_Perceptual-Based_Shape_Decomposition.html">21 iccv-2013-A Method of Perceptual-Based Shape Decomposition</a></p>
<p>Author: Chang Ma, Zhongqian Dong, Tingting Jiang, Yizhou Wang, Wen Gao</p><p>Abstract: In thispaper, wepropose a novelperception-based shape decomposition method which aims to decompose a shape into semantically meaningful parts. In addition to three popular perception rules (the Minima rule, the Short-cut rule and the Convexity rule) in shape decomposition, we propose a new rule named part-similarity rule to encourage consistent partition of similar parts. The problem is formulated as a quadratically constrained quadratic program (QCQP) problem and is solved by a trust-region method. Experiment results on MPEG-7 dataset show that we can get a more consistent shape decomposition with human perception compared with other state-of-the-art methods both qualitatively and quantitatively. Finally, we show the advantage of semantic parts over non-meaningful parts in object detection on the ETHZ dataset.</p><p>3 0.87423521 <a title="327-lda-3" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>Author: S. Karthikeyan, Vignesh Jagadeesh, Renuka Shenoy, Miguel Ecksteinz, B.S. Manjunath</p><p>Abstract: Eye movement studies have confirmed that overt attention is highly biased towards faces and text regions in images. In this paper we explore a novel problem of predicting face and text regions in images using eye tracking data from multiple subjects. The problem is challenging as we aim to predict the semantics (face/text/background) only from eye tracking data without utilizing any image information. The proposed algorithm spatially clusters eye tracking data obtained in an image into different coherent groups and subsequently models the likelihood of the clusters containing faces and text using afully connectedMarkov Random Field (MRF). Given the eye tracking datafrom a test image, itpredicts potential face/head (humans, dogs and cats) and text locations reliably. Furthermore, the approach can be used to select regions of interest for further analysis by object detectors for faces and text. The hybrid eye position/object detector approach achieves better detection performance and reduced computation time compared to using only the object detection algorithm. We also present a new eye tracking dataset on 300 images selected from ICDAR, Street-view, Flickr and Oxford-IIIT Pet Dataset from 15 subjects.</p><p>4 0.86830914 <a title="327-lda-4" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>Author: Ming-Ming Cheng, Jonathan Warrell, Wen-Yan Lin, Shuai Zheng, Vibhav Vineet, Nigel Crook</p><p>Abstract: Detecting visually salient regions in images is one of the fundamental problems in computer vision. We propose a novel method to decompose an image into large scale perceptually homogeneous elements for efficient salient region detection, using a soft image abstraction representation. By considering both appearance similarity and spatial distribution of image pixels, the proposed representation abstracts out unnecessary image details, allowing the assignment of comparable saliency values across similar regions, and producing perceptually accurate salient region detection. We evaluate our salient region detection approach on the largest publicly available dataset with pixel accurate annotations. The experimental results show that the proposed method outperforms 18 alternate methods, reducing the mean absolute error by 25.2% compared to the previous best result, while being computationally more efficient.</p><p>5 0.86353636 <a title="327-lda-5" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>Author: Gang Hua, Chengjiang Long, Ming Yang, Yan Gao</p><p>Abstract: Active learning is an effective way of engaging users to interactively train models for visual recognition. The vast majority of previous works, if not all of them, focused on active learning with a single human oracle. The problem of active learning with multiple oracles in a collaborative setting has not been well explored. Moreover, most of the previous works assume that the labels provided by the human oracles are noise free, which may often be violated in reality. We present a collaborative computational model for active learning with multiple human oracles. It leads to not only an ensemble kernel machine that is robust to label noises, but also a principled label quality measure to online detect irresponsible labelers. Instead of running independent active learning processes for each individual human oracle, our model captures the inherent correlations among the labelers through shared data among them. Our simulation experiments and experiments with real crowd-sourced noisy labels demonstrated the efficacy of our model.</p><p>6 0.86273974 <a title="327-lda-6" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>7 0.86192662 <a title="327-lda-7" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>8 0.86189044 <a title="327-lda-8" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>9 0.86153197 <a title="327-lda-9" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>10 0.86033577 <a title="327-lda-10" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>11 0.8600105 <a title="327-lda-11" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>12 0.85894465 <a title="327-lda-12" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>13 0.85879493 <a title="327-lda-13" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>14 0.85878289 <a title="327-lda-14" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>15 0.85804081 <a title="327-lda-15" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>16 0.85775805 <a title="327-lda-16" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>17 0.85773444 <a title="327-lda-17" href="./iccv-2013-Class-Specific_Simplex-Latent_Dirichlet_Allocation_for_Image_Classification.html">73 iccv-2013-Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification</a></p>
<p>18 0.85726786 <a title="327-lda-18" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>19 0.85659909 <a title="327-lda-19" href="./iccv-2013-PhotoOCR%3A_Reading_Text_in_Uncontrolled_Conditions.html">315 iccv-2013-PhotoOCR: Reading Text in Uncontrolled Conditions</a></p>
<p>20 0.85536492 <a title="327-lda-20" href="./iccv-2013-Learning_a_Dictionary_of_Shape_Epitomes_with_Applications_to_Image_Labeling.html">245 iccv-2013-Learning a Dictionary of Shape Epitomes with Applications to Image Labeling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
