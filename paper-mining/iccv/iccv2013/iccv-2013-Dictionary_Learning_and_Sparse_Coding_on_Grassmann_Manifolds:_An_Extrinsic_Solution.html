<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>114 iccv-2013-Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-114" href="#">iccv2013-114</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>114 iccv-2013-Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution</h1>
<br/><p>Source: <a title="iccv-2013-114-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Harandi_Dictionary_Learning_and_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Mehrtash Harandi, Conrad Sanderson, Chunhua Shen, Brian Lovell</p><p>Abstract: Recent advances in computer vision and machine learning suggest that a wide range of problems can be addressed more appropriately by considering non-Euclidean geometry. In this paper we explore sparse dictionary learning over the space of linear subspaces, which form Riemannian structures known as Grassmann manifolds. To this end, we propose to embed Grassmann manifolds into the space of symmetric matrices by an isometric mapping, which enables us to devise a closed-form solution for updating a Grassmann dictionary, atom by atom. Furthermore, to handle non-linearity in data, we propose a kernelised version of the dictionary learning algorithm. Experiments on several classification tasks (face recognition, action recognition, dynamic texture classification) show that the proposed approach achieves considerable improvements in discrimination accuracy, in comparison to state-of-the-art methods such as kernelised Affine Hull Method and graphembedding Grassmann discriminant analysis.</p><p>Reference: <a title="iccv-2013-114-reference" href="../iccv2013_reference/iccv-2013-Dictionary_Learning_and_Sparse_Coding_on_Grassmann_Manifolds%3A_An_Extrinsic_Solution_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper we explore sparse dictionary learning over the space of linear subspaces, which form Riemannian structures known as Grassmann manifolds. [sent-3, score-0.367]
</p><p>2 To this end, we propose to embed Grassmann manifolds into the space of symmetric matrices by an isometric mapping, which enables us to devise a closed-form solution for updating a Grassmann dictionary, atom by atom. [sent-4, score-0.682]
</p><p>3 Furthermore, to handle non-linearity in data, we propose a kernelised version of the dictionary learning algorithm. [sent-5, score-0.506]
</p><p>4 Introduction Linear subspaces of Rd can be considered as the core of many inference algorithms in computer vision and machine learning. [sent-8, score-0.169]
</p><p>5 For example, several state-of-the-art methods for matching videos or image sets model given data by subspaces [9, 11, 24]. [sent-9, score-0.194]
</p><p>6 Auto regressive and moving average models, which are typically employed to model dynamics in spatio-temporal processing, can also be expressed as linear subspaces [24]. [sent-10, score-0.244]
</p><p>7 More applications of linear subspaces in computer vision include, but are not limited to, chromatic noise filtering [23], biometrics [20], and domain adaptation [8]. [sent-11, score-0.246]
</p><p>8 , the set of all reflectance functions produced by Lambertian objects lies in a linear subspace), subspaces lie on a special type of Riemannian manifold, namely the Grassmann manifold, which makes their analysis very challenging. [sent-14, score-0.22]
</p><p>9 This paper tackles and provides efficient solutions to the following two fundamental problems for learning on Grassmann manifolds: 1. [sent-15, score-0.114]
</p><p>10 Given a signal X and a dictionary D = {Di}iN=1 with N elements (also known as atoms), Dwh =ere { DX and Di are linear subspaces, how X can be approximated by a combination of a “few” atoms in D ? [sent-17, score-0.407]
</p><p>11 Given a set of measurements {Xi}im=1, how can a dictionary D = {Di}iN=1 be  {leXarn}ed to, represent { aX di}icimt=io1 sparsely =? [sent-20, score-0.282]
</p><p>12 Our main motivation here is to develop new methods for analysing video data and image sets. [sent-21, score-0.064]
</p><p>13 This is inspired by the success of sparse signal modelling that suggests natural signals like images (and hence video and image sets as our concern here) can be efficiently approximated by superposition of atoms of a dictionary, where the coefficients of superposition are usually sparse (i. [sent-22, score-0.566]
</p><p>14 We generalise the traditional sparse coding, which operates on vectors, to sparse coding on subspaces. [sent-25, score-0.344]
</p><p>15 Sparse encoding with the dictionary of subspaces can then be seamlessly used for categorising video data. [sent-26, score-0.505]
</p><p>16 Before we present our main results, we want to highlight that the proposed algorithms outperform state-of-the-art methods on various recognition tasks and in particular has achieved the highest reported accuracy in classifying dynamic textures. [sent-27, score-0.13]
</p><p>17 While significant steps have been taken to develop the theory of the sparse coding and dictionary learning in Euclidean spaces, similar problems on non33 111203  Euclidean geometry have received comparatively little attention [10, 15]. [sent-29, score-0.596]
</p><p>18 To our best knowledge, among a handful of solutions devised on Riemannian manifolds, none is specialised for the Grassmann manifolds which motivates our study. [sent-30, score-0.468]
</p><p>19 In [10], the authors addressed sparse coding and dictio-  nary learning for the Riemannian structure of Symmetric Positive Definite (SPD) matrices or tensors. [sent-31, score-0.329]
</p><p>20 The solution was obtained by embedding the SPD manifold into Reproducing Kernel Hilbert Space (RKHS) using a Riemannian kernel. [sent-32, score-0.189]
</p><p>21 Another approach to learning a Riemannian dictionary is by exploiting the tangent bundle of the manifold, as for example in [15] for the manifold of probability distributions. [sent-33, score-0.525]
</p><p>22 Since the sparse coding has a trivial solution in this approach, an affine constraint has to be added to the problem [15]. [sent-34, score-0.269]
</p><p>23 While having an affine constraint along with sparse coding is welcome in specific tasks (e. [sent-35, score-0.341]
</p><p>24 , clustering [2]), in general, the resulting formulation is restrictive and no longer addresses the original problem. [sent-37, score-0.031]
</p><p>25 Also, working in successive tangent spaces, though common, values only a first-order approximation to the manifold at each step. [sent-38, score-0.209]
</p><p>26 Furthermore, switching back and forth to the tangent spaces of a Grassmann manifold (as required by this formulation) can be computationally very demanding for the problems that we are interested in (e. [sent-39, score-0.386]
</p><p>27 This in turns makes the applicability of such school of thought limited for the Grassmann manifolds arising in vision tasks. [sent-42, score-0.375]
</p><p>28 In light of the above discussion, in this paper, we introduce an extrinsic method for learning a Grassmann dictionary. [sent-44, score-0.126]
</p><p>29 To this end, we propose to embed Grassmann manifolds into the space of symmetric matrices by a diffeomorphism that preserves Grassmann projection distance (a special class of distances on Grassmann mani-  folds). [sent-45, score-0.536]
</p><p>30 We show how sparse coding can be accomplished in the induced space and devise a closed-form solution for updating a Grassmann dictionary atom by atom. [sent-46, score-0.655]
</p><p>31 Furthermore, in order to accommodate non-linearity in data, we propose a kernelised version of our dictionary learning algorithm. [sent-47, score-0.535]
</p><p>32 We propose an extrinsic dictionary learning algorithm for data points on Grassmann manifolds by embedding the manifolds into the space of symmetric matrices. [sent-49, score-1.082]
</p><p>33 We derive a kernelised version of the dictionary learning algorithm which can address the non-linearity in data. [sent-51, score-0.506]
</p><p>34 We apply the proposed Grassmannian dictionary learning methods to several computer vision tasks where the data are videos or image sets. [sent-53, score-0.36]
</p><p>35 Our proposed algorithms outperform state-of-the-art methods on a wide range ofclassification tasks, including face recognition from image sets, action recognition and dynamic texture classification. [sent-54, score-0.122]
</p><p>36 Background Before presenting our algorithms, we review some concepts of Riemannian geometry of Grassmann manifolds, which provide the grounding for the proposed algorithms. [sent-56, score-0.109]
</p><p>37 Details on Grassmann manifolds and related topics can be found in [1]. [sent-57, score-0.325]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('grassmann', 0.715), ('manifolds', 0.297), ('dictionary', 0.255), ('riemannian', 0.192), ('kernelised', 0.187), ('subspaces', 0.169), ('australia', 0.141), ('manifold', 0.128), ('coding', 0.115), ('atoms', 0.093), ('extrinsic', 0.092), ('spd', 0.088), ('superposition', 0.084), ('tangent', 0.081), ('sparse', 0.078), ('symmetric', 0.071), ('atom', 0.068), ('embed', 0.059), ('di', 0.053), ('devise', 0.052), ('affine', 0.051), ('welcome', 0.051), ('diffeomorphism', 0.051), ('regressive', 0.051), ('specialised', 0.051), ('spaces', 0.05), ('mehrtash', 0.047), ('brian', 0.047), ('categorising', 0.047), ('generalise', 0.047), ('grounding', 0.047), ('tasks', 0.046), ('chunhua', 0.044), ('auto', 0.044), ('queensland', 0.044), ('isometric', 0.042), ('chromatic', 0.04), ('rkhs', 0.04), ('analysing', 0.04), ('forth', 0.04), ('adelaide', 0.039), ('grassmannian', 0.039), ('handful', 0.037), ('nary', 0.037), ('biometrics', 0.037), ('embedding', 0.036), ('wide', 0.034), ('matrices', 0.034), ('hilbert', 0.034), ('seamlessly', 0.034), ('concern', 0.034), ('learning', 0.034), ('dynamic', 0.034), ('updating', 0.034), ('comparatively', 0.033), ('geometry', 0.033), ('reproducing', 0.032), ('tackles', 0.032), ('switching', 0.032), ('lambertian', 0.032), ('hull', 0.031), ('demanding', 0.031), ('restrictive', 0.031), ('devised', 0.031), ('addressed', 0.031), ('coefficients', 0.031), ('approximated', 0.03), ('version', 0.03), ('appealing', 0.03), ('australian', 0.03), ('signal', 0.029), ('accommodate', 0.029), ('appropriately', 0.029), ('presenting', 0.029), ('arising', 0.029), ('motivates', 0.028), ('definite', 0.028), ('accomplished', 0.028), ('euclidean', 0.028), ('topics', 0.028), ('ere', 0.028), ('action', 0.028), ('reflectance', 0.027), ('sparsely', 0.027), ('bundle', 0.027), ('folds', 0.027), ('furthermore', 0.027), ('outperform', 0.026), ('thought', 0.026), ('operates', 0.026), ('modelling', 0.025), ('solution', 0.025), ('videos', 0.025), ('special', 0.024), ('problems', 0.024), ('highlight', 0.024), ('solutions', 0.024), ('develop', 0.024), ('dynamics', 0.024), ('applicability', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="114-tfidf-1" href="./iccv-2013-Dictionary_Learning_and_Sparse_Coding_on_Grassmann_Manifolds%3A_An_Extrinsic_Solution.html">114 iccv-2013-Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution</a></p>
<p>Author: Mehrtash Harandi, Conrad Sanderson, Chunhua Shen, Brian Lovell</p><p>Abstract: Recent advances in computer vision and machine learning suggest that a wide range of problems can be addressed more appropriately by considering non-Euclidean geometry. In this paper we explore sparse dictionary learning over the space of linear subspaces, which form Riemannian structures known as Grassmann manifolds. To this end, we propose to embed Grassmann manifolds into the space of symmetric matrices by an isometric mapping, which enables us to devise a closed-form solution for updating a Grassmann dictionary, atom by atom. Furthermore, to handle non-linearity in data, we propose a kernelised version of the dictionary learning algorithm. Experiments on several classification tasks (face recognition, action recognition, dynamic texture classification) show that the proposed approach achieves considerable improvements in discrimination accuracy, in comparison to state-of-the-art methods such as kernelised Affine Hull Method and graphembedding Grassmann discriminant analysis.</p><p>2 0.37866539 <a title="114-tfidf-2" href="./iccv-2013-Efficient_Higher-Order_Clustering_on_the_Grassmann_Manifold.html">134 iccv-2013-Efficient Higher-Order Clustering on the Grassmann Manifold</a></p>
<p>Author: Suraj Jain, Venu Madhav Govindu</p><p>Abstract: The higher-order clustering problem arises when data is drawn from multiple subspaces or when observations fit a higher-order parametric model. Most solutions to this problem either decompose higher-order similarity measures for use in spectral clustering or explicitly use low-rank matrix representations. In this paper we present our approach of Sparse Grassmann Clustering (SGC) that combines attributes of both categories. While we decompose the higherorder similarity tensor, we cluster data by directly finding a low dimensional representation without explicitly building a similarity matrix. By exploiting recent advances in online estimation on the Grassmann manifold (GROUSE) we develop an efficient and accurate algorithm that works with individual columns of similarities or partial observations thereof. Since it avoids the storage and decomposition of large similarity matrices, our method is efficient, scalable and has low memory requirements even for large-scale data. We demonstrate the performance of our SGC method on a variety of segmentation problems including planar seg- mentation of Kinect depth maps and motion segmentation of the Hopkins 155 dataset for which we achieve performance comparable to the state-of-the-art.</p><p>3 0.27254054 <a title="114-tfidf-3" href="./iccv-2013-Unsupervised_Domain_Adaptation_by_Domain_Invariant_Projection.html">435 iccv-2013-Unsupervised Domain Adaptation by Domain Invariant Projection</a></p>
<p>Author: Mahsa Baktashmotlagh, Mehrtash T. Harandi, Brian C. Lovell, Mathieu Salzmann</p><p>Abstract: Domain-invariant representations are key to addressing the domain shift problem where the training and test examples follow different distributions. Existing techniques that have attempted to match the distributions of the source and target domains typically compare these distributions in the original feature space. This space, however, may not be directly suitable for such a comparison, since some of the features may have been distorted by the domain shift, or may be domain specific. In this paper, we introduce a Domain Invariant Projection approach: An unsupervised domain adaptation method that overcomes this issue by extracting the information that is invariant across the source and target domains. More specifically, we learn a projection of the data to a low-dimensional latent space where the distance between the empirical distributions of the source and target examples is minimized. We demonstrate the effectiveness of our approach on the task of visual object recognition and show that it outperforms state-of-the-art methods on a standard domain adaptation benchmark dataset.</p><p>4 0.23289554 <a title="114-tfidf-4" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>Author: Chenglong Bao, Jian-Feng Cai, Hui Ji</p><p>Abstract: In recent years, how to learn a dictionary from input images for sparse modelling has been one very active topic in image processing and recognition. Most existing dictionary learning methods consider an over-complete dictionary, e.g. the K-SVD method. Often they require solving some minimization problem that is very challenging in terms of computational feasibility and efficiency. However, if the correlations among dictionary atoms are not well constrained, the redundancy of the dictionary does not necessarily improve the performance of sparse coding. This paper proposed a fast orthogonal dictionary learning method for sparse image representation. With comparable performance on several image restoration tasks, the proposed method is much more computationally efficient than the over-complete dictionary based learning methods.</p><p>5 0.20959209 <a title="114-tfidf-5" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>Author: Hua Wang, Feiping Nie, Weidong Cai, Heng Huang</p><p>Abstract: Representing the raw input of a data set by a set of relevant codes is crucial to many computer vision applications. Due to the intrinsic sparse property of real-world data, dictionary learning, in which the linear decomposition of a data point uses a set of learned dictionary bases, i.e., codes, has demonstrated state-of-the-art performance. However, traditional dictionary learning methods suffer from three weaknesses: sensitivity to noisy and outlier samples, difficulty to determine the optimal dictionary size, and incapability to incorporate supervision information. In this paper, we address these weaknesses by learning a Semi-Supervised Robust Dictionary (SSR-D). Specifically, we use the ℓ2,0+ norm as the loss function to improve the robustness against outliers, and develop a new structured sparse regularization com, , tom. . cai@sydney . edu . au , heng@uta .edu make the learning tasks easier to deal with and reduce the computational cost. For example, in image tagging, instead of using the raw pixel-wise features, semi-local or patch- based features, such as SIFT and geometric blur, are usually more desirable to achieve better performance. In practice, finding a set of compact features bases, also referred to as dictionary, with enhanced representative and discriminative power, plays a significant role in building a successful computer vision system. In this paper, we explore this important problem by proposing a novel formulation and its solution for learning Semi-Supervised Robust Dictionary (SSRD), where we examine the challenges in dictionary learning, and seek opportunities to overcome them and improve the dictionary qualities. 1.1. Challenges in Dictionary Learning to incorporate the supervision information in dictionary learning, without incurring additional parameters. Moreover, the optimal dictionary size is automatically learned from the input data. Minimizing the derived objective function is challenging because it involves many non-smooth ℓ2,0+ -norm terms. We present an efficient algorithm to solve the problem with a rigorous proof of the convergence of the algorithm. Extensive experiments are presented to show the superior performance of the proposed method.</p><p>6 0.18817815 <a title="114-tfidf-6" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>7 0.16900796 <a title="114-tfidf-7" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>8 0.15131669 <a title="114-tfidf-8" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>9 0.15128103 <a title="114-tfidf-9" href="./iccv-2013-Log-Euclidean_Kernels_for_Sparse_Representation_and_Dictionary_Learning.html">257 iccv-2013-Log-Euclidean Kernels for Sparse Representation and Dictionary Learning</a></p>
<p>10 0.14738914 <a title="114-tfidf-10" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>11 0.14192876 <a title="114-tfidf-11" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>12 0.12818982 <a title="114-tfidf-12" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>13 0.1258799 <a title="114-tfidf-13" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>14 0.12338392 <a title="114-tfidf-14" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>15 0.12267324 <a title="114-tfidf-15" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>16 0.11545324 <a title="114-tfidf-16" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>17 0.11470169 <a title="114-tfidf-17" href="./iccv-2013-Total_Variation_Regularization_for_Functions_with_Values_in_a_Manifold.html">421 iccv-2013-Total Variation Regularization for Functions with Values in a Manifold</a></p>
<p>18 0.10757949 <a title="114-tfidf-18" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>19 0.10356462 <a title="114-tfidf-19" href="./iccv-2013-Curvature-Aware_Regularization_on_Riemannian_Submanifolds.html">100 iccv-2013-Curvature-Aware Regularization on Riemannian Submanifolds</a></p>
<p>20 0.093410648 <a title="114-tfidf-20" href="./iccv-2013-GOSUS%3A_Grassmannian_Online_Subspace_Updates_with_Structured-Sparsity.html">182 iccv-2013-GOSUS: Grassmannian Online Subspace Updates with Structured-Sparsity</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.127), (1, 0.095), (2, -0.094), (3, 0.008), (4, -0.301), (5, -0.037), (6, -0.108), (7, -0.008), (8, 0.017), (9, -0.026), (10, 0.037), (11, 0.004), (12, -0.064), (13, 0.022), (14, 0.011), (15, -0.051), (16, -0.028), (17, -0.013), (18, 0.037), (19, -0.035), (20, 0.06), (21, -0.002), (22, 0.17), (23, 0.077), (24, -0.022), (25, -0.03), (26, -0.041), (27, 0.033), (28, -0.018), (29, 0.018), (30, -0.099), (31, -0.015), (32, -0.171), (33, -0.0), (34, 0.105), (35, 0.126), (36, 0.085), (37, 0.052), (38, -0.154), (39, -0.036), (40, -0.052), (41, 0.044), (42, 0.047), (43, -0.088), (44, -0.09), (45, 0.086), (46, 0.038), (47, -0.027), (48, -0.054), (49, -0.148)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94054222 <a title="114-lsi-1" href="./iccv-2013-Dictionary_Learning_and_Sparse_Coding_on_Grassmann_Manifolds%3A_An_Extrinsic_Solution.html">114 iccv-2013-Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution</a></p>
<p>Author: Mehrtash Harandi, Conrad Sanderson, Chunhua Shen, Brian Lovell</p><p>Abstract: Recent advances in computer vision and machine learning suggest that a wide range of problems can be addressed more appropriately by considering non-Euclidean geometry. In this paper we explore sparse dictionary learning over the space of linear subspaces, which form Riemannian structures known as Grassmann manifolds. To this end, we propose to embed Grassmann manifolds into the space of symmetric matrices by an isometric mapping, which enables us to devise a closed-form solution for updating a Grassmann dictionary, atom by atom. Furthermore, to handle non-linearity in data, we propose a kernelised version of the dictionary learning algorithm. Experiments on several classification tasks (face recognition, action recognition, dynamic texture classification) show that the proposed approach achieves considerable improvements in discrimination accuracy, in comparison to state-of-the-art methods such as kernelised Affine Hull Method and graphembedding Grassmann discriminant analysis.</p><p>2 0.59251869 <a title="114-lsi-2" href="./iccv-2013-Log-Euclidean_Kernels_for_Sparse_Representation_and_Dictionary_Learning.html">257 iccv-2013-Log-Euclidean Kernels for Sparse Representation and Dictionary Learning</a></p>
<p>Author: Peihua Li, Qilong Wang, Wangmeng Zuo, Lei Zhang</p><p>Abstract: The symmetric positive de?nite (SPD) matrices have been widely used in image and vision problems. Recently there are growing interests in studying sparse representation (SR) of SPD matrices, motivated by the great success of SR for vector data. Though the space of SPD matrices is well-known to form a Lie group that is a Riemannian manifold, existing work fails to take full advantage of its geometric structure. This paper attempts to tackle this problem by proposing a kernel based method for SR and dictionary learning (DL) of SPD matrices. We disclose that the space of SPD matrices, with the operations of logarithmic multiplication and scalar logarithmic multiplication de?ned in the Log-Euclidean framework, is a complete inner product space. We can thus develop a broad family of kernels that satis?es Mercer’s condition. These kernels characterize the geodesic distance and can be computed ef?ciently. We also consider the geometric structure in the DL process by updating atom matrices in the Riemannian space instead of in the Euclidean space. The proposed method is evaluated with various vision problems and shows notable per- formance gains over state-of-the-arts.</p><p>3 0.57696742 <a title="114-lsi-3" href="./iccv-2013-Efficient_Higher-Order_Clustering_on_the_Grassmann_Manifold.html">134 iccv-2013-Efficient Higher-Order Clustering on the Grassmann Manifold</a></p>
<p>Author: Suraj Jain, Venu Madhav Govindu</p><p>Abstract: The higher-order clustering problem arises when data is drawn from multiple subspaces or when observations fit a higher-order parametric model. Most solutions to this problem either decompose higher-order similarity measures for use in spectral clustering or explicitly use low-rank matrix representations. In this paper we present our approach of Sparse Grassmann Clustering (SGC) that combines attributes of both categories. While we decompose the higherorder similarity tensor, we cluster data by directly finding a low dimensional representation without explicitly building a similarity matrix. By exploiting recent advances in online estimation on the Grassmann manifold (GROUSE) we develop an efficient and accurate algorithm that works with individual columns of similarities or partial observations thereof. Since it avoids the storage and decomposition of large similarity matrices, our method is efficient, scalable and has low memory requirements even for large-scale data. We demonstrate the performance of our SGC method on a variety of segmentation problems including planar seg- mentation of Kinect depth maps and motion segmentation of the Hopkins 155 dataset for which we achieve performance comparable to the state-of-the-art.</p><p>4 0.56601685 <a title="114-lsi-4" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>Author: Hongteng Xu, Hongyuan Zha</p><p>Abstract: Data sparsity has been a thorny issuefor manifold-based image synthesis, and in this paper we address this critical problem by leveraging ideas from transfer learning. Specifically, we propose methods based on generating auxiliary data in the form of synthetic samples using transformations of the original sparse samples. To incorporate the auxiliary data, we propose a weighted data synthesis method, which adaptively selects from the generated samples for inclusion during the manifold learning process via a weighted iterative algorithm. To demonstrate the feasibility of the proposed method, we apply it to the problem of face image synthesis from sparse samples. Compared with existing methods, the proposed method shows encouraging results with good performance improvements.</p><p>5 0.55368584 <a title="114-lsi-5" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>Author: Chenglong Bao, Jian-Feng Cai, Hui Ji</p><p>Abstract: In recent years, how to learn a dictionary from input images for sparse modelling has been one very active topic in image processing and recognition. Most existing dictionary learning methods consider an over-complete dictionary, e.g. the K-SVD method. Often they require solving some minimization problem that is very challenging in terms of computational feasibility and efficiency. However, if the correlations among dictionary atoms are not well constrained, the redundancy of the dictionary does not necessarily improve the performance of sparse coding. This paper proposed a fast orthogonal dictionary learning method for sparse image representation. With comparable performance on several image restoration tasks, the proposed method is much more computationally efficient than the over-complete dictionary based learning methods.</p><p>6 0.55250144 <a title="114-lsi-6" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>7 0.55132568 <a title="114-lsi-7" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>8 0.50403571 <a title="114-lsi-8" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>9 0.48793799 <a title="114-lsi-9" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>10 0.47828719 <a title="114-lsi-10" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>11 0.47267434 <a title="114-lsi-11" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>12 0.46572906 <a title="114-lsi-12" href="./iccv-2013-Total_Variation_Regularization_for_Functions_with_Values_in_a_Manifold.html">421 iccv-2013-Total Variation Regularization for Functions with Values in a Manifold</a></p>
<p>13 0.45764652 <a title="114-lsi-13" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>14 0.45703518 <a title="114-lsi-14" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>15 0.43977723 <a title="114-lsi-15" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>16 0.42889526 <a title="114-lsi-16" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>17 0.422086 <a title="114-lsi-17" href="./iccv-2013-Coupled_Dictionary_and_Feature_Space_Learning_with_Applications_to_Cross-Domain_Image_Synthesis_and_Recognition.html">96 iccv-2013-Coupled Dictionary and Feature Space Learning with Applications to Cross-Domain Image Synthesis and Recognition</a></p>
<p>18 0.42060465 <a title="114-lsi-18" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>19 0.40561858 <a title="114-lsi-19" href="./iccv-2013-Unsupervised_Domain_Adaptation_by_Domain_Invariant_Projection.html">435 iccv-2013-Unsupervised Domain Adaptation by Domain Invariant Projection</a></p>
<p>20 0.40498009 <a title="114-lsi-20" href="./iccv-2013-From_Semi-supervised_to_Transfer_Counting_of_Crowds.html">178 iccv-2013-From Semi-supervised to Transfer Counting of Crowds</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.082), (7, 0.02), (26, 0.085), (31, 0.025), (38, 0.02), (42, 0.133), (53, 0.236), (64, 0.031), (73, 0.019), (89, 0.143), (95, 0.048), (97, 0.031), (98, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81066638 <a title="114-lda-1" href="./iccv-2013-Dictionary_Learning_and_Sparse_Coding_on_Grassmann_Manifolds%3A_An_Extrinsic_Solution.html">114 iccv-2013-Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution</a></p>
<p>Author: Mehrtash Harandi, Conrad Sanderson, Chunhua Shen, Brian Lovell</p><p>Abstract: Recent advances in computer vision and machine learning suggest that a wide range of problems can be addressed more appropriately by considering non-Euclidean geometry. In this paper we explore sparse dictionary learning over the space of linear subspaces, which form Riemannian structures known as Grassmann manifolds. To this end, we propose to embed Grassmann manifolds into the space of symmetric matrices by an isometric mapping, which enables us to devise a closed-form solution for updating a Grassmann dictionary, atom by atom. Furthermore, to handle non-linearity in data, we propose a kernelised version of the dictionary learning algorithm. Experiments on several classification tasks (face recognition, action recognition, dynamic texture classification) show that the proposed approach achieves considerable improvements in discrimination accuracy, in comparison to state-of-the-art methods such as kernelised Affine Hull Method and graphembedding Grassmann discriminant analysis.</p><p>2 0.70116097 <a title="114-lda-2" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>Author: Kwang Moo Yi, Hawook Jeong, Byeongho Heo, Hyung Jin Chang, Jin Young Choi</p><p>Abstract: In this paper we propose an object tracking method in case of inaccurate initializations. To track objects accurately in such situation, the proposed method uses “motion saliency ” and “descriptor saliency ” of local features and performs tracking based on generalized Hough transform (GHT). The proposed motion saliency of a local feature emphasizes features having distinctive motions, compared to the motions which are not from the target object. The descriptor saliency emphasizes features which are likely to be of the object in terms of its feature descriptors. Through these saliencies, the proposed method tries to “learn and find” the target object rather than looking for what was given at initialization, giving robust results even with inaccurate initializations. Also, our tracking result is obtained by combining the results of each local feature of the target and the surroundings with GHT voting, thus is robust against severe occlusions as well. The proposed method is compared against nine other methods, with nine image sequences, and hundred random initializations. The experimental results show that our method outperforms all other compared methods.</p><p>3 0.69014585 <a title="114-lda-3" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>Author: Zhaowen Wang, Jianchao Yang, Nasser Nasrabadi, Thomas Huang</p><p>Abstract: Sparse Representation-based Classification (SRC) is a powerful tool in distinguishing signal categories which lie on different subspaces. Despite its wide application to visual recognition tasks, current understanding of SRC is solely based on a reconstructive perspective, which neither offers any guarantee on its classification performance nor provides any insight on how to design a discriminative dictionary for SRC. In this paper, we present a novel perspective towards SRC and interpret it as a margin classifier. The decision boundary and margin of SRC are analyzed in local regions where the support of sparse code is stable. Based on the derived margin, we propose a hinge loss function as the gauge for the classification performance of SRC. A stochastic gradient descent algorithm is implemented to maximize the margin of SRC and obtain more discriminative dictionaries. Experiments validate the effectiveness of the proposed approach in predicting classification performance and improving dictionary quality over reconstructive ones. Classification results competitive with other state-ofthe-art sparse coding methods are reported on several data sets.</p><p>4 0.68738419 <a title="114-lda-4" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>Author: Zheyun Feng, Rong Jin, Anil Jain</p><p>Abstract: One of the key challenges in search-based image annotation models is to define an appropriate similarity measure between images. Many kernel distance metric learning (KML) algorithms have been developed in order to capture the nonlinear relationships between visual features and semantics ofthe images. Onefundamental limitation in applying KML to image annotation is that it requires converting image annotations into binary constraints, leading to a significant information loss. In addition, most KML algorithms suffer from high computational cost due to the requirement that the learned matrix has to be positive semi-definitive (PSD). In this paper, we propose a robust kernel metric learning (RKML) algorithm based on the regression technique that is able to directly utilize image annotations. The proposed method is also computationally more efficient because PSD property is automatically ensured by regression. We provide the theoretical guarantee for the proposed algorithm, and verify its efficiency and effectiveness for image annotation by comparing it to state-of-the-art approaches for both distance metric learning and image annotation. ,</p><p>5 0.68398565 <a title="114-lda-5" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>Author: Vidit Jain, Sachin Sudhakar Farfade</p><p>Abstract: Classification cascades have been very effective for object detection. Such a cascade fails to perform well in data domains with variations in appearances that may not be captured in the training examples. This limited generalization severely restricts the domains for which they can be used effectively. A common approach to address this limitation is to train a new cascade of classifiers from scratch for each of the new domains. Building separate detectors for each of the different domains requires huge annotation and computational effort, making it not scalable to a large number of data domains. Here we present an algorithm for quickly adapting a pre-trained cascade of classifiers using a small number oflabeledpositive instancesfrom a different yet similar data domain. In our experiments with images of human babies and human-like characters from movies, we demonstrate that the adapted cascade significantly outperforms both of the original cascade and the one trained from scratch using the given training examples. –</p><p>6 0.68293363 <a title="114-lda-6" href="./iccv-2013-GOSUS%3A_Grassmannian_Online_Subspace_Updates_with_Structured-Sparsity.html">182 iccv-2013-GOSUS: Grassmannian Online Subspace Updates with Structured-Sparsity</a></p>
<p>7 0.68072122 <a title="114-lda-7" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>8 0.6805014 <a title="114-lda-8" href="./iccv-2013-A_Novel_Earth_Mover%27s_Distance_Methodology_for_Image_Matching_with_Gaussian_Mixture_Models.html">25 iccv-2013-A Novel Earth Mover's Distance Methodology for Image Matching with Gaussian Mixture Models</a></p>
<p>9 0.67934304 <a title="114-lda-9" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>10 0.67931658 <a title="114-lda-10" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>11 0.67867649 <a title="114-lda-11" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>12 0.67821485 <a title="114-lda-12" href="./iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation.html">383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</a></p>
<p>13 0.67808533 <a title="114-lda-13" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>14 0.67782354 <a title="114-lda-14" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>15 0.67749488 <a title="114-lda-15" href="./iccv-2013-Learning_Graph_Matching%3A_Oriented_to_Category_Modeling_from_Cluttered_Scenes.html">237 iccv-2013-Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes</a></p>
<p>16 0.67743289 <a title="114-lda-16" href="./iccv-2013-Learning_a_Dictionary_of_Shape_Epitomes_with_Applications_to_Image_Labeling.html">245 iccv-2013-Learning a Dictionary of Shape Epitomes with Applications to Image Labeling</a></p>
<p>17 0.67723441 <a title="114-lda-17" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>18 0.67707169 <a title="114-lda-18" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>19 0.67661828 <a title="114-lda-19" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>20 0.67649001 <a title="114-lda-20" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
