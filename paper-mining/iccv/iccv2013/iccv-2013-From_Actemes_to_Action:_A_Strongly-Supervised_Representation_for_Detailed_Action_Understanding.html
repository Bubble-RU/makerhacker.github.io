<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>175 iccv-2013-From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-175" href="#">iccv2013-175</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>175 iccv-2013-From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding</h1>
<br/><p>Source: <a title="iccv-2013-175-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Zhang_From_Actemes_to_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Weiyu Zhang, Menglong Zhu, Konstantinos G. Derpanis</p><p>Abstract: This paper presents a novel approach for analyzing human actions in non-scripted, unconstrained video settings based on volumetric, x-y-t, patch classifiers, termed actemes. Unlike previous action-related work, the discovery of patch classifiers is posed as a strongly-supervised process. Specifically, keypoint labels (e.g., position) across spacetime are used in a data-driven training process to discover patches that are highly clustered in the spacetime keypoint configuration space. To support this process, a new human action dataset consisting of challenging consumer videos is introduced, where notably the action label, the 2D position of a set of keypoints and their visibilities are provided for each video frame. On a novel input video, each acteme is used in a sliding volume scheme to yield a set of sparse, non-overlapping detections. These detections provide the intermediate substrate for segmenting out the action. For action classification, the proposed representation shows significant improvement over state-of-the-art low-level features, while providing spatiotemporal localiza- tion as additional output. This output sheds further light into detailed action understanding.</p><p>Reference: <a title="iccv-2013-175-reference" href="../iccv2013_reference/iccv-2013-From_Actemes_to_Action%3A_A_Strongly-Supervised_Representation_for_Detailed_Action_Understanding_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract This paper presents a novel approach for analyzing human actions in non-scripted, unconstrained video settings based on volumetric, x-y-t, patch classifiers, termed actemes. [sent-5, score-0.345]
</p><p>2 , position) across spacetime are used in a data-driven training process to discover patches that are highly clustered in the spacetime keypoint configuration space. [sent-9, score-0.56]
</p><p>3 To support this process, a new human action dataset consisting of challenging consumer videos is introduced, where notably the action label, the 2D position of a set of keypoints and their visibilities are provided for each video frame. [sent-10, score-0.947]
</p><p>4 On a novel input video, each acteme is used in a sliding volume scheme to yield a set of sparse, non-overlapping detections. [sent-11, score-0.654]
</p><p>5 For action classification, the proposed representation shows significant improvement over state-of-the-art low-level features, while providing spatiotemporal localiza-  tion as additional output. [sent-13, score-0.549]
</p><p>6 This output sheds further light into detailed action understanding. [sent-14, score-0.34]
</p><p>7 Introduction Human action classification (“What action is present in the video? [sent-16, score-0.68]
</p><p>8 ”) and detection (“Where and when is a particular action performed in the video? [sent-17, score-0.388]
</p><p>9 Another challenge arises from the diversity in observed action dynamics due to performance nuances and varying camera capture settings, such as frame rate. [sent-20, score-0.34]
</p><p>10 ca  coded by the activation of a set of discriminative spatiotemporal patch classifiers, actemes. [sent-27, score-0.306]
</p><p>11 The activations of these mid-level primitives in the input imagery are used to classify videos into different action categories and spatiotemporally localize an action. [sent-29, score-0.484]
</p><p>12 Much of the state-of-the-art work in action classification and detection is based on relating image patterns in a more direct fashion to actions. [sent-35, score-0.415]
</p><p>13 Here, local spatiotemporal image patches are often encoded by a universal dictionary of “visual words”, constructed by clustering a large number of training image patches in an unsupervised manner. [sent-39, score-0.34]
</p><p>14 In practice, most local patches capture generic oriented structures, such as bar and corner-like structures in the spatial [24] and spatiotemporal [18] domains. [sent-41, score-0.284]
</p><p>15 Thus, visual words in isolation generally convey generic information of the local spacetime pattern, such as the velocity and the presence of spatiotemporal discontinuities, rather than distinctive information of the action, including its spacetime location. [sent-42, score-0.549]
</p><p>16 Further, as typically employed, these features are not strictly grounded on actions but rather capture both the action and the scene context which may artificially increase performance on datasets where the actions are highly correlated with the scene context [17]. [sent-43, score-0.57]
</p><p>17 A variety of image measurements have been proposed to populate such templates, including optical flow and spatiotemporal orientations [6, 12, 4, 25]. [sent-45, score-0.272]
</p><p>18 Here, parts have been proposed that capture a neighborhood of spacetime [13, 7, 29, 22], a spatial keyframe [26] or a temporal segment while considering the entire spatial extent of the scene [21, 9]. [sent-48, score-0.297]
</p><p>19 Poselets have been shown to be an effective mid-level representation for a variety of applica-  tions, including action recognition in still imagery [19]. [sent-62, score-0.384]
</p><p>20 Motivated by the success of poselets and other patchbased representations in static contexts, this work presents a set of volumetric patch detectors, termed actemes, that are each selective for a salient spatiotemporal visual pattern. [sent-63, score-0.377]
</p><p>21 An acteme can consist of a single frame outlining a keyframe of the action, a volume outlining the entire action or an intermediate patch size, capturing for instance a portion of a stationary head and a right arm moving upward (see Figure 1top-right for further examples). [sent-64, score-1.108]
</p><p>22 The discovery of actemes in the learning stage is posed as a strongly supervised process. [sent-69, score-0.544]
</p><p>23 Specifically, hand labeled keypoint positions across space and time are used in a data-driven search process to discover patches that are highly clustered in the spacetime keypoint configuration space and yield discriminative, yet representative (i. [sent-70, score-0.494]
</p><p>24 The efficacy of this representation is demonstrated on action classification and action detection. [sent-74, score-0.68]
</p><p>25 A key benefit of the proposed representation is that it captures discriminative action-specific patches and conveys partial spatiotemporal pose information and by extension motion. [sent-76, score-0.282]
</p><p>26 Unlike, local feature points, actemes are designed to capture the action (appearance and motion) with minimal scene context. [sent-78, score-0.851]
</p><p>27 (i) A discriminative multiscale spatiotemporal patch model is developed, termed actemes, that serves as an effective mid-level representation for analyzing human actions in video. [sent-82, score-0.483]
</p><p>28 Exploiting the spatiotemporal context of actemes facilitates both action classification and detection. [sent-83, score-1.06]
</p><p>29 (ii) To realize this patch model, this work introduces a new annotated human action dataset1 containing 15 actions and consisting of 2326 challenging consumer videos. [sent-84, score-0.605]
</p><p>30 The annotations consist of action class labels, 2D keypoint positions (13 in all) in each video frame and their corresponding visibilities, and camera viewpoints. [sent-85, score-0.492]
</p><p>31 This is exemplified in Figure 2, where the aligned action parts are relatively sharp compared to the blurred background. [sent-102, score-0.366]
</p><p>32 The steps for realizing the set of actemes are as follows. [sent-105, score-0.538]
</p><p>33 Generate acteme candidates Given the training videos, a set of random patches are selected, the seed volumes. [sent-120, score-0.736]
</p><p>34 ) For each seed volume, the optimal 2D spatial similarity transform and discrete temporal offset is estimated that aligns the keypoints spatiotemporally in each training video with the corresponding visible points within the seed volume. [sent-122, score-0.46]
</p><p>35 Consequently, each cluster is not only restricted to a given viewpoint and set of body poses but also a limited range of action execution speeds. [sent-124, score-0.365]
</p><p>36 ) Each seed volume is selected randomly from a training video at a uniformly random spatiotemporal position. [sent-133, score-0.448]
</p><p>37 ××  Further, the spatiotemporal dimensions of the seed volume are chosen randomly from a set of four spatial dimensions, {100 100, 100 150, 150 100, 100 200}, and a set {o1f 0th0r ×ee temporal extents, 0{5 ×, 10, 01,51}0. [sent-134, score-0.45]
</p><p>38 For the visible keypoints within the extents of a seed volume, P1, the distance is computed to each video sharing the same action and camera viewpoint, as follows (cf. [sent-138, score-0.594]
</p><p>39 Train acteme candidates Similar to poselets, acteme detection is performed using a linear SVM classifier. [sent-144, score-1.167]
</p><p>40 In this work, each acteme is represented as the concatenation of the normalized and unnormalized histogram of flow (HoF), computed using dense flow [2], and the histogram of spatial gradients (HoG) [8]; alternative features are also possible, e. [sent-145, score-0.646]
</p><p>41 The negative examples are randomly selected from images that have a different action label to the positive examples. [sent-149, score-0.34]
</p><p>42 Ranking and selecting actemes The initial set of actemes are selected in a random fashion and thus may exhibit a wide range of capacities to discriminate different actions and represent a wide range of instances of the same action. [sent-154, score-1.164]
</p><p>43 )  For each acteme candidate, its discrimination capacity is measured by the activation ratio, α, between the in-class and out-of-class detections among the top 1000 detections. [sent-160, score-0.689]
</p><p>44 The key idea is that distinctive actemes should fire more often on imagery of its own action category. [sent-161, score-0.895]
</p><p>45 The representative capacity ofthe i-th acteme is measured by a binary vector fi indicating whether a detection occurs for each of the training instances. [sent-162, score-0.625]
</p><p>46 Initially, the selected acteme set, M, contains tinheg aincstetamnece sw. [sent-163, score-0.548]
</p><p>47 Acteme detection Given an input video, each acteme is considered in a sliding volume manner, i. [sent-172, score-0.676]
</p><p>48 , each detector is applied at all spatiotemporal positions and a set of spatiotemporal scales. [sent-174, score-0.418]
</p><p>49 Each detection di is a detected spacetime volume: di = (si , bi, Ti), where si denotes the detection score, bi is the 2D bounding box projection of the 3D volume (same at every image) and Ti is the life span of the detection. [sent-176, score-0.507]
</p><p>50 The actemes for the same action are strongly correlated in both space and time. [sent-178, score-0.851]
</p><p>51 Exploiting the spatiotemporal context of mutual activations helps to boost the weak detections due to partial occlusion and extreme deformation. [sent-179, score-0.325]
</p><p>52 It also disambiguates similar actemes belonging to different actions, e. [sent-180, score-0.511]
</p><p>53 This is particularly important for action classification since it is desirable that each acteme only responds to instances containing its action. [sent-184, score-0.888]
</p><p>54 Similar to [1], the detection score of each acteme is modified by looking at the detection information of other actemes in the neighboring spacetime vicinity. [sent-185, score-1.359]
</p><p>55 Acteme clustering via semantic compatibility  Given the detected actemes, a grouping of consistent candidates is sought that explains a consistent action hypothesis. [sent-193, score-0.48]
</p><p>56 One obvious approach is to let each candidate vote for the spatiotemporal volume of the entire action and select the best hypothesis in the voting space. [sent-194, score-0.629]
</p><p>57 First, the pairwise compatibility between the actemes is measured, where ideally the acteme detections corresponding to the same action hypothesis should be more compatible. [sent-201, score-1.581]
</p><p>58 Next, these compatibilities are used in an agglomerative clustering process to realize the spatiotemporal bounding volumes. [sent-202, score-0.353]
</p><p>59 Let Pi and Pj be the distribu222255 11  acteme di (green) and dj (blue) overlap at a frame t. [sent-204, score-0.697]
</p><p>60 Right: trajectory trp goes through two actemes (green) not overlapping in time. [sent-206, score-0.608]
</p><p>61 There are two cases to consider for measuring the compatibilities: (i) two detections overlap in time and (ii) two detections do not overlap in time. [sent-212, score-0.258]
</p><p>62 For two acteme detections overlapping in time, the semantic compatibility score is computed by taking the average over their common frames:  Cs(di,dj) =|Ti∩1 Tj|t∈TXi∩Tjc(di,dj,t,0). [sent-213, score-0.792]
</p><p>63 (2)  For two acteme detections not overlapping in time, the underlying actemes could capture different stages in time. [sent-214, score-1.152]
</p><p>64 Each trajectory trp is a series of spacetime points, and trtp is its position at time t. [sent-218, score-0.267]
</p><p>65 If trp goes through the volume of di and dj, we can translate the keypoints using the trajectory displacement, see Figure 4 right. [sent-219, score-0.322]
</p><p>66 The induc−ed t rsemantic compatibility score is: Ct (di, dj , trp) = c(di, dj , t2 −ti0, δp) +c(di, dj , t1−t0j, δp). [sent-223, score-0.321]
</p><p>67 If two actemes are far apart in time, the prediction tends to be unreliable and the compatibility is set to zero. [sent-225, score-0.6]
</p><p>68 As the acteme detections are dense, the transitivity ofcompatibility among them links distant frames. [sent-226, score-0.641]
</p><p>69 The first step selects the detected acteme with the highest score. [sent-229, score-0.548]
</p><p>70 For each acteme cluster, the constituent actemes predict  the human bounding volume within its life span. [sent-232, score-1.214]
</p><p>71 Next, the predicted volume for each cluster is merged to form an action detection. [sent-233, score-0.42]
</p><p>72 For the actemes that are closest to the beginning (end) of the action, a predictor for the start (end) time is trained for the whole action and is used to refine the detection at test time. [sent-234, score-0.899]
</p><p>73 Critical to realizing a set of actemes are the locations and visibility labels for a set of keypoints throughout the 22225522  action performance. [sent-245, score-0.998]
</p><p>74 The dataset consists of the following 15 action classes: baseball pitch, baseball swing, bench press, bowling, clean and jerk, golf swing, jump rope, jumping jacks, pull up, push up, sit up, squat, strum guitar, tennis forehand and tennis serve. [sent-253, score-0.545]
</p><p>75 Challenging aspects of the dataset include large variation in intra-class actor appearance, action execution rate, viewpoint, spatiotemporal resolution and complicated natural backdrops. [sent-254, score-0.549]
</p><p>76 For each action category, 300 actemes are selected (K = 4500 in total). [sent-261, score-0.851]
</p><p>77 During testing, acteme detection is run at eight different spatial scales, four per octave and three temporal scales, i. [sent-262, score-0.672]
</p><p>78 Action classification The effectiveness of the proposed model is first shown for action classification. [sent-276, score-0.34]
</p><p>79 The protocol from a recent action recognition evaluation of feature descriptors is followed [30]. [sent-278, score-0.34]
</p><p>80 At test time, the action label corresponding to the detector with the maximum score is returned as the label. [sent-282, score-0.374]
</p><p>81 We also compare with spacetime cuboids [5] on interest points, dense spacetime histogram of gradients (HoG3D) [14], and dense local motion interchange pattern [15]. [sent-289, score-0.4]
</p><p>82 A bag ofwords model is run under three different scenarios: (i) inside the action bounding volume, (ii) outside bounding volume of action and (iii) the whole video clip. [sent-290, score-0.914]
</p><p>83 As an additional base-  line, we compare against Action Bank [25] which represents actions on the basis of holistic spacetime templates. [sent-291, score-0.311]
</p><p>84 If the detection window is provided, as considered in the VOC Action Classification challenge [19], actemes whose predicted spatiotemporal action bounds overlap sufficiently with the input action boundary are retained for further processing (cf. [sent-292, score-1.484]
</p><p>85 [19]); the overlap ratio is defined by the intersection over union of the input and predicted action volume boundaries. [sent-293, score-0.456]
</p><p>86 The introduction of the overlap criteria enforces a degree of spatiotemporal translation invariance and spatiotemporal organization among the detected actemes within the analysis volume and suppresses spurious detections. [sent-295, score-1.045]
</p><p>87 The maximum response for each acteme is used as the entry in the corresponding component ofthe activation vector, i. [sent-298, score-0.596]
</p><p>88 For each action, a separate linear support vector machine (SVM) classifier is trained on top of the vector of acteme responses. [sent-301, score-0.548]
</p><p>89 When the bounding boxes are not provided, the false positives of the actemes on the background contaminate the activation signal and thus reduce performance. [sent-313, score-0.603]
</p><p>90 Action detection  In addition to classification, the proposed model is evaluated on action detection. [sent-318, score-0.388]
</p><p>91 For each action category, the corresponding actemes are run on the input video. [sent-322, score-0.851]
</p><p>92 Detected actemes are clustered using the semantic compatibility, and non-maximal suppression is performed using a 50% overlap ratio. [sent-323, score-0.633]
</p><p>93 The score of each detection si is computed as the sum of the clustered acteme detection scores. [sent-324, score-0.711]
</p><p>94 The key concept introduced is modeling human action as a composition of discriminative volumetric patches, termed actemes. [sent-338, score-0.451]
</p><p>95 Each patch, found through a data-driven discovery process, implicitly captures the intricate pose, dynamics, self-occlusion and spatial appearance of a subset of body parts in a spatiotemporal neighborhood. [sent-339, score-0.291]
</p><p>96 This approach allows to spatiotemporally localize an action in cluttered scenes by a bounding volume. [sent-346, score-0.426]
</p><p>97 Hough forests for object detection, tracking, and action recognition. [sent-412, score-0.34]
</p><p>98 Motion interchange patterns for action recognition in unconstrained videos. [sent-445, score-0.393]
</p><p>99 Action snippets: How many frames does human action recognition require? [sent-516, score-0.371]
</p><p>100 Evaluation of local spatio-temporal features for action recog-  nition. [sent-543, score-0.34]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('acteme', 0.548), ('actemes', 0.511), ('action', 0.34), ('spatiotemporal', 0.209), ('spacetime', 0.17), ('actions', 0.115), ('keypoint', 0.107), ('keypoints', 0.098), ('detections', 0.093), ('compatibility', 0.089), ('seed', 0.085), ('volume', 0.08), ('trp', 0.073), ('dj', 0.066), ('swing', 0.054), ('temporal', 0.052), ('baseball', 0.052), ('patches', 0.051), ('patch', 0.049), ('termed', 0.049), ('activation', 0.048), ('detection', 0.048), ('di', 0.047), ('video', 0.045), ('bounding', 0.044), ('imagery', 0.044), ('spatiotemporally', 0.042), ('realize', 0.042), ('klaser', 0.04), ('poselets', 0.039), ('metadata', 0.037), ('hof', 0.037), ('flow', 0.037), ('derpanis', 0.037), ('inordinate', 0.037), ('menglong', 0.037), ('rescalings', 0.037), ('squat', 0.037), ('overlap', 0.036), ('videos', 0.035), ('score', 0.034), ('clustered', 0.033), ('discovery', 0.033), ('motion', 0.033), ('jerk', 0.032), ('outlining', 0.032), ('alignment', 0.032), ('volumetric', 0.031), ('human', 0.031), ('agglomerative', 0.031), ('analyzing', 0.03), ('grasp', 0.03), ('visibilities', 0.03), ('unity', 0.03), ('bench', 0.03), ('training', 0.029), ('pennsylvania', 0.028), ('jhuang', 0.028), ('rye', 0.028), ('representational', 0.028), ('substrate', 0.028), ('semantic', 0.028), ('consumer', 0.028), ('golf', 0.027), ('interchange', 0.027), ('compatibilities', 0.027), ('realizing', 0.027), ('keyframe', 0.027), ('fashion', 0.027), ('bank', 0.027), ('yield', 0.026), ('optical', 0.026), ('holistic', 0.026), ('extents', 0.026), ('exemplified', 0.026), ('philadelphia', 0.026), ('unconstrained', 0.026), ('movements', 0.026), ('hough', 0.025), ('pitch', 0.025), ('suppression', 0.025), ('body', 0.025), ('trajectory', 0.024), ('velocities', 0.024), ('spatial', 0.024), ('crowdsourced', 0.023), ('activations', 0.023), ('trajectories', 0.023), ('candidates', 0.023), ('span', 0.023), ('falls', 0.023), ('addressing', 0.022), ('pose', 0.022), ('flipping', 0.022), ('tennis', 0.022), ('visibility', 0.022), ('bag', 0.021), ('ko', 0.021), ('classifiers', 0.021), ('supervision', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="175-tfidf-1" href="./iccv-2013-From_Actemes_to_Action%3A_A_Strongly-Supervised_Representation_for_Detailed_Action_Understanding.html">175 iccv-2013-From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding</a></p>
<p>Author: Weiyu Zhang, Menglong Zhu, Konstantinos G. Derpanis</p><p>Abstract: This paper presents a novel approach for analyzing human actions in non-scripted, unconstrained video settings based on volumetric, x-y-t, patch classifiers, termed actemes. Unlike previous action-related work, the discovery of patch classifiers is posed as a strongly-supervised process. Specifically, keypoint labels (e.g., position) across spacetime are used in a data-driven training process to discover patches that are highly clustered in the spacetime keypoint configuration space. To support this process, a new human action dataset consisting of challenging consumer videos is introduced, where notably the action label, the 2D position of a set of keypoints and their visibilities are provided for each video frame. On a novel input video, each acteme is used in a sliding volume scheme to yield a set of sparse, non-overlapping detections. These detections provide the intermediate substrate for segmenting out the action. For action classification, the proposed representation shows significant improvement over state-of-the-art low-level features, while providing spatiotemporal localiza- tion as additional output. This output sheds further light into detailed action understanding.</p><p>2 0.26650691 <a title="175-tfidf-2" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>Author: Ping Wei, Nanning Zheng, Yibiao Zhao, Song-Chun Zhu</p><p>Abstract: Action recognition has often been posed as a classification problem, which assumes that a video sequence only have one action class label and different actions are independent. However, a single human body can perform multiple concurrent actions at the same time, and different actions interact with each other. This paper proposes a concurrent action detection model where the action detection is formulated as a structural prediction problem. In this model, an interval in a video sequence can be described by multiple action labels. An detected action interval is determined both by the unary local detector and the relations with other actions. We use a wavelet feature to represent the action sequence, and design a composite temporal logic descriptor to describe the action relations. The model parameters are trained by structural SVM learning. Given a long video sequence, a sequential decision window search algorithm is designed to detect the actions. Experiments on our new collected concurrent action dataset demonstrate the strength of our method.</p><p>3 0.25578159 <a title="175-tfidf-3" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>Author: Mihai Zanfir, Marius Leordeanu, Cristian Sminchisescu</p><p>Abstract: Human action recognition under low observational latency is receiving a growing interest in computer vision due to rapidly developing technologies in human-robot interaction, computer gaming and surveillance. In this paper we propose a fast, simple, yet powerful non-parametric Moving Pose (MP)frameworkfor low-latency human action and activity recognition. Central to our methodology is a moving pose descriptor that considers both pose information as well as differential quantities (speed and acceleration) of the human body joints within a short time window around the current frame. The proposed descriptor is used in conjunction with a modified kNN classifier that considers both the temporal location of a particular frame within the action sequence as well as the discrimination power of its moving pose descriptor compared to other frames in the training set. The resulting method is non-parametric and enables low-latency recognition, one-shot learning, and action detection in difficult unsegmented sequences. Moreover, the framework is real-time, scalable, and outperforms more sophisticated approaches on challenging benchmarks like MSR-Action3D or MSR-DailyActivities3D.</p><p>4 0.24017175 <a title="175-tfidf-4" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>Author: Jiang Wang, Ying Wu</p><p>Abstract: Temporal misalignment and duration variation in video actions largely influence the performance of action recognition, but it is very difficult to specify effective temporal alignment on action sequences. To address this challenge, this paper proposes a novel discriminative learning-based temporal alignment method, called maximum margin temporal warping (MMTW), to align two action sequences and measure their matching score. Based on the latent structure SVM formulation, the proposed MMTW method is able to learn a phantom action template to represent an action class for maximum discrimination against other classes. The recognition of this action class is based on the associated learned alignment of the input action. Extensive experiments on five benchmark datasets have demonstrated that this MMTW model is able to significantly promote the accuracy and robustness of action recognition under temporal misalignment and variations.</p><p>5 0.23138805 <a title="175-tfidf-5" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>Author: Sunil Bandla, Kristen Grauman</p><p>Abstract: Collecting and annotating videos of realistic human actions is tedious, yet critical for training action recognition systems. We propose a method to actively request the most useful video annotations among a large set of unlabeled videos. Predicting the utility of annotating unlabeled video is not trivial, since any given clip may contain multiple actions of interest, and it need not be trimmed to temporal regions of interest. To deal with this problem, we propose a detection-based active learner to train action category models. We develop a voting-based framework to localize likely intervals of interest in an unlabeled clip, and use them to estimate the total reduction in uncertainty that annotating that clip would yield. On three datasets, we show our approach can learn accurate action detectors more efficiently than alternative active learning strategies that fail to accommodate the “untrimmed” nature of real video data.</p><p>6 0.22085735 <a title="175-tfidf-6" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>7 0.21813484 <a title="175-tfidf-7" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>8 0.19133215 <a title="175-tfidf-8" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>9 0.18202922 <a title="175-tfidf-9" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>10 0.16820908 <a title="175-tfidf-10" href="./iccv-2013-Learning_to_Share_Latent_Tasks_for_Action_Recognition.html">249 iccv-2013-Learning to Share Latent Tasks for Action Recognition</a></p>
<p>11 0.1675227 <a title="175-tfidf-11" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>12 0.16173057 <a title="175-tfidf-12" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>13 0.15556823 <a title="175-tfidf-13" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>14 0.14062119 <a title="175-tfidf-14" href="./iccv-2013-Finding_Actors_and_Actions_in_Movies.html">166 iccv-2013-Finding Actors and Actions in Movies</a></p>
<p>15 0.1328941 <a title="175-tfidf-15" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>16 0.13289002 <a title="175-tfidf-16" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>17 0.12867637 <a title="175-tfidf-17" href="./iccv-2013-Latent_Multitask_Learning_for_View-Invariant_Action_Recognition.html">231 iccv-2013-Latent Multitask Learning for View-Invariant Action Recognition</a></p>
<p>18 0.11983927 <a title="175-tfidf-18" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>19 0.11725596 <a title="175-tfidf-19" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>20 0.10766621 <a title="175-tfidf-20" href="./iccv-2013-Manipulation_Pattern_Discovery%3A_A_Nonparametric_Bayesian_Approach.html">260 iccv-2013-Manipulation Pattern Discovery: A Nonparametric Bayesian Approach</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.222), (1, 0.175), (2, 0.104), (3, 0.248), (4, 0.02), (5, -0.032), (6, 0.097), (7, -0.084), (8, -0.022), (9, 0.038), (10, 0.055), (11, 0.085), (12, 0.002), (13, -0.088), (14, 0.131), (15, -0.012), (16, 0.005), (17, -0.008), (18, 0.0), (19, -0.036), (20, -0.035), (21, 0.028), (22, -0.017), (23, -0.076), (24, -0.012), (25, 0.0), (26, -0.042), (27, -0.025), (28, -0.046), (29, -0.034), (30, -0.016), (31, -0.04), (32, 0.026), (33, 0.028), (34, -0.009), (35, 0.017), (36, 0.039), (37, -0.029), (38, 0.052), (39, -0.006), (40, -0.02), (41, 0.026), (42, -0.004), (43, -0.021), (44, 0.015), (45, -0.012), (46, 0.001), (47, 0.008), (48, 0.005), (49, -0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9665888 <a title="175-lsi-1" href="./iccv-2013-From_Actemes_to_Action%3A_A_Strongly-Supervised_Representation_for_Detailed_Action_Understanding.html">175 iccv-2013-From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding</a></p>
<p>Author: Weiyu Zhang, Menglong Zhu, Konstantinos G. Derpanis</p><p>Abstract: This paper presents a novel approach for analyzing human actions in non-scripted, unconstrained video settings based on volumetric, x-y-t, patch classifiers, termed actemes. Unlike previous action-related work, the discovery of patch classifiers is posed as a strongly-supervised process. Specifically, keypoint labels (e.g., position) across spacetime are used in a data-driven training process to discover patches that are highly clustered in the spacetime keypoint configuration space. To support this process, a new human action dataset consisting of challenging consumer videos is introduced, where notably the action label, the 2D position of a set of keypoints and their visibilities are provided for each video frame. On a novel input video, each acteme is used in a sliding volume scheme to yield a set of sparse, non-overlapping detections. These detections provide the intermediate substrate for segmenting out the action. For action classification, the proposed representation shows significant improvement over state-of-the-art low-level features, while providing spatiotemporal localiza- tion as additional output. This output sheds further light into detailed action understanding.</p><p>2 0.93215424 <a title="175-lsi-2" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>Author: Ping Wei, Nanning Zheng, Yibiao Zhao, Song-Chun Zhu</p><p>Abstract: Action recognition has often been posed as a classification problem, which assumes that a video sequence only have one action class label and different actions are independent. However, a single human body can perform multiple concurrent actions at the same time, and different actions interact with each other. This paper proposes a concurrent action detection model where the action detection is formulated as a structural prediction problem. In this model, an interval in a video sequence can be described by multiple action labels. An detected action interval is determined both by the unary local detector and the relations with other actions. We use a wavelet feature to represent the action sequence, and design a composite temporal logic descriptor to describe the action relations. The model parameters are trained by structural SVM learning. Given a long video sequence, a sequential decision window search algorithm is designed to detect the actions. Experiments on our new collected concurrent action dataset demonstrate the strength of our method.</p><p>3 0.91972947 <a title="175-lsi-3" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>Author: Jiang Wang, Ying Wu</p><p>Abstract: Temporal misalignment and duration variation in video actions largely influence the performance of action recognition, but it is very difficult to specify effective temporal alignment on action sequences. To address this challenge, this paper proposes a novel discriminative learning-based temporal alignment method, called maximum margin temporal warping (MMTW), to align two action sequences and measure their matching score. Based on the latent structure SVM formulation, the proposed MMTW method is able to learn a phantom action template to represent an action class for maximum discrimination against other classes. The recognition of this action class is based on the associated learned alignment of the input action. Extensive experiments on five benchmark datasets have demonstrated that this MMTW model is able to significantly promote the accuracy and robustness of action recognition under temporal misalignment and variations.</p><p>4 0.86993182 <a title="175-lsi-4" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>Author: Mihai Zanfir, Marius Leordeanu, Cristian Sminchisescu</p><p>Abstract: Human action recognition under low observational latency is receiving a growing interest in computer vision due to rapidly developing technologies in human-robot interaction, computer gaming and surveillance. In this paper we propose a fast, simple, yet powerful non-parametric Moving Pose (MP)frameworkfor low-latency human action and activity recognition. Central to our methodology is a moving pose descriptor that considers both pose information as well as differential quantities (speed and acceleration) of the human body joints within a short time window around the current frame. The proposed descriptor is used in conjunction with a modified kNN classifier that considers both the temporal location of a particular frame within the action sequence as well as the discrimination power of its moving pose descriptor compared to other frames in the training set. The resulting method is non-parametric and enables low-latency recognition, one-shot learning, and action detection in difficult unsegmented sequences. Moreover, the framework is real-time, scalable, and outperforms more sophisticated approaches on challenging benchmarks like MSR-Action3D or MSR-DailyActivities3D.</p><p>5 0.84398794 <a title="175-lsi-5" href="./iccv-2013-Latent_Multitask_Learning_for_View-Invariant_Action_Recognition.html">231 iccv-2013-Latent Multitask Learning for View-Invariant Action Recognition</a></p>
<p>Author: Behrooz Mahasseni, Sinisa Todorovic</p><p>Abstract: This paper presents an approach to view-invariant action recognition, where human poses and motions exhibit large variations across different camera viewpoints. When each viewpoint of a given set of action classes is specified as a learning task then multitask learning appears suitable for achieving view invariance in recognition. We extend the standard multitask learning to allow identifying: (1) latent groupings of action views (i.e., tasks), and (2) discriminative action parts, along with joint learning of all tasks. This is because it seems reasonable to expect that certain distinct views are more correlated than some others, and thus identifying correlated views could improve recognition. Also, part-based modeling is expected to improve robustness against self-occlusion when actors are imaged from different views. Results on the benchmark datasets show that we outperform standard multitask learning by 21.9%, and the state-of-the-art alternatives by 4.5–6%.</p><p>6 0.83845603 <a title="175-lsi-6" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>7 0.82509696 <a title="175-lsi-7" href="./iccv-2013-Action_Recognition_with_Actons.html">38 iccv-2013-Action Recognition with Actons</a></p>
<p>8 0.77490366 <a title="175-lsi-8" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>9 0.76096511 <a title="175-lsi-9" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>10 0.75435412 <a title="175-lsi-10" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>11 0.74450934 <a title="175-lsi-11" href="./iccv-2013-Finding_Actors_and_Actions_in_Movies.html">166 iccv-2013-Finding Actors and Actions in Movies</a></p>
<p>12 0.72468871 <a title="175-lsi-12" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>13 0.72281545 <a title="175-lsi-13" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>14 0.70326191 <a title="175-lsi-14" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>15 0.70228678 <a title="175-lsi-15" href="./iccv-2013-Manipulation_Pattern_Discovery%3A_A_Nonparametric_Bayesian_Approach.html">260 iccv-2013-Manipulation Pattern Discovery: A Nonparametric Bayesian Approach</a></p>
<p>16 0.65208483 <a title="175-lsi-16" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>17 0.63551462 <a title="175-lsi-17" href="./iccv-2013-Learning_to_Share_Latent_Tasks_for_Action_Recognition.html">249 iccv-2013-Learning to Share Latent Tasks for Action Recognition</a></p>
<p>18 0.63475037 <a title="175-lsi-18" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>19 0.58171815 <a title="175-lsi-19" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>20 0.56863827 <a title="175-lsi-20" href="./iccv-2013-Capturing_Global_Semantic_Relationships_for_Facial_Action_Unit_Recognition.html">69 iccv-2013-Capturing Global Semantic Relationships for Facial Action Unit Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.066), (7, 0.019), (12, 0.022), (26, 0.078), (31, 0.033), (35, 0.016), (40, 0.016), (42, 0.084), (64, 0.102), (73, 0.023), (78, 0.241), (89, 0.174), (95, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8834945 <a title="175-lda-1" href="./iccv-2013-Capturing_Global_Semantic_Relationships_for_Facial_Action_Unit_Recognition.html">69 iccv-2013-Capturing Global Semantic Relationships for Facial Action Unit Recognition</a></p>
<p>Author: Ziheng Wang, Yongqiang Li, Shangfei Wang, Qiang Ji</p><p>Abstract: In this paper we tackle the problem of facial action unit (AU) recognition by exploiting the complex semantic relationships among AUs, which carry crucial top-down information yet have not been thoroughly exploited. Towards this goal, we build a hierarchical model that combines the bottom-level image features and the top-level AU relationships to jointly recognize AUs in a principled manner. The proposed model has two major advantages over existing methods. 1) Unlike methods that can only capture local pair-wise AU dependencies, our model is developed upon the restricted Boltzmann machine and therefore can exploit the global relationships among AUs. 2) Although AU relationships are influenced by many related factors such as facial expressions, these factors are generally ignored by the current methods. Our model, however, can successfully capture them to more accurately characterize the AU relationships. Efficient learning and inference algorithms of the proposed model are also developed. Experimental results on benchmark databases demonstrate the effectiveness of the proposed approach in modelling complex AU relationships as well as its superior AU recognition performance over existing approaches.</p><p>2 0.85164791 <a title="175-lda-2" href="./iccv-2013-Recognising_Human-Object_Interaction_via_Exemplar_Based_Modelling.html">344 iccv-2013-Recognising Human-Object Interaction via Exemplar Based Modelling</a></p>
<p>Author: Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai, Shaogang Gong, Tao Xiang</p><p>Abstract: Human action can be recognised from a single still image by modelling Human-object interaction (HOI), which infers the mutual spatial structure information between human and object as well as their appearance. Existing approaches rely heavily on accurate detection of human and object, and estimation of human pose. They are thus sensitive to large variations of human poses, occlusion and unsatisfactory detection of small size objects. To overcome this limitation, a novel exemplar based approach is proposed in this work. Our approach learns a set of spatial pose-object interaction exemplars, which are density functions describing how a person is interacting with a manipulated object for different activities spatially in a probabilistic way. A representation based on our HOI exemplar thus has great potential for being robust to the errors in human/object detection and pose estimation. A new framework consists of a proposed exemplar based HOI descriptor and an activity specific matching model that learns the parameters is formulated for robust human activity recog- nition. Experiments on two benchmark activity datasets demonstrate that the proposed approach obtains state-ofthe-art performance.</p><p>3 0.84924686 <a title="175-lda-3" href="./iccv-2013-Line_Assisted_Light_Field_Triangulation_and_Stereo_Matching.html">252 iccv-2013-Line Assisted Light Field Triangulation and Stereo Matching</a></p>
<p>Author: Zhan Yu, Xinqing Guo, Haibing Lin, Andrew Lumsdaine, Jingyi Yu</p><p>Abstract: Light fields are image-based representations that use densely sampled rays as a scene description. In this paper, we explore geometric structures of 3D lines in ray space for improving light field triangulation and stereo matching. The triangulation problem aims to fill in the ray space with continuous and non-overlapping simplices anchored at sampled points (rays). Such a triangulation provides a piecewise-linear interpolant useful for light field superresolution. We show that the light field space is largely bilinear due to 3D line segments in the scene, and direct triangulation of these bilinear subspaces leads to large errors. We instead present a simple but effective algorithm to first map bilinear subspaces to line constraints and then apply Constrained Delaunay Triangulation (CDT). Based on our analysis, we further develop a novel line-assisted graphcut (LAGC) algorithm that effectively encodes 3D line constraints into light field stereo matching. Experiments on synthetic and real data show that both our triangulation and LAGC algorithms outperform state-of-the-art solutions in accuracy and visual quality.</p><p>4 0.83808661 <a title="175-lda-4" href="./iccv-2013-New_Graph_Structured_Sparsity_Model_for_Multi-label_Image_Annotations.html">290 iccv-2013-New Graph Structured Sparsity Model for Multi-label Image Annotations</a></p>
<p>Author: Xiao Cai, Feiping Nie, Weidong Cai, Heng Huang</p><p>Abstract: In multi-label image annotations, because each image is associated to multiple categories, the semantic terms (label classes) are not mutually exclusive. Previous research showed that such label correlations can largely boost the annotation accuracy. However, all existing methods only directly apply the label correlation matrix to enhance the label inference and assignment without further learning the structural information among classes. In this paper, we model the label correlations using the relational graph, and propose a novel graph structured sparse learning model to incorporate the topological constraints of relation graph in multi-label classifications. As a result, our new method will capture and utilize the hidden class structures in relational graph to improve the annotation results. In proposed objective, a large number of structured sparsity-inducing norms are utilized, thus the optimization becomes difficult. To solve this problem, we derive an efficient optimization algorithm with proved convergence. We perform extensive experiments on six multi-label image annotation benchmark data sets. In all empirical results, our new method shows better annotation results than the state-of-the-art approaches.</p><p>same-paper 5 0.82688451 <a title="175-lda-5" href="./iccv-2013-From_Actemes_to_Action%3A_A_Strongly-Supervised_Representation_for_Detailed_Action_Understanding.html">175 iccv-2013-From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding</a></p>
<p>Author: Weiyu Zhang, Menglong Zhu, Konstantinos G. Derpanis</p><p>Abstract: This paper presents a novel approach for analyzing human actions in non-scripted, unconstrained video settings based on volumetric, x-y-t, patch classifiers, termed actemes. Unlike previous action-related work, the discovery of patch classifiers is posed as a strongly-supervised process. Specifically, keypoint labels (e.g., position) across spacetime are used in a data-driven training process to discover patches that are highly clustered in the spacetime keypoint configuration space. To support this process, a new human action dataset consisting of challenging consumer videos is introduced, where notably the action label, the 2D position of a set of keypoints and their visibilities are provided for each video frame. On a novel input video, each acteme is used in a sliding volume scheme to yield a set of sparse, non-overlapping detections. These detections provide the intermediate substrate for segmenting out the action. For action classification, the proposed representation shows significant improvement over state-of-the-art low-level features, while providing spatiotemporal localiza- tion as additional output. This output sheds further light into detailed action understanding.</p><p>6 0.79732859 <a title="175-lda-6" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>7 0.78124201 <a title="175-lda-7" href="./iccv-2013-Facial_Action_Unit_Event_Detection_by_Cascade_of_Tasks.html">155 iccv-2013-Facial Action Unit Event Detection by Cascade of Tasks</a></p>
<p>8 0.73164332 <a title="175-lda-8" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>9 0.72895682 <a title="175-lda-9" href="./iccv-2013-Modeling_4D_Human-Object_Interactions_for_Event_and_Object_Recognition.html">268 iccv-2013-Modeling 4D Human-Object Interactions for Event and Object Recognition</a></p>
<p>10 0.72002447 <a title="175-lda-10" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>11 0.71827143 <a title="175-lda-11" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>12 0.71083182 <a title="175-lda-12" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>13 0.70955777 <a title="175-lda-13" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>14 0.70786107 <a title="175-lda-14" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>15 0.7031399 <a title="175-lda-15" href="./iccv-2013-Event_Detection_in_Complex_Scenes_Using_Interval_Temporal_Constraints.html">146 iccv-2013-Event Detection in Complex Scenes Using Interval Temporal Constraints</a></p>
<p>16 0.70266289 <a title="175-lda-16" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>17 0.70136583 <a title="175-lda-17" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>18 0.70051026 <a title="175-lda-18" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<p>19 0.70043159 <a title="175-lda-19" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>20 0.69946063 <a title="175-lda-20" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
