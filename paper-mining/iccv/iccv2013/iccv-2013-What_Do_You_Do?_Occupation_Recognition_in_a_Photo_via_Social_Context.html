<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-449" href="#">iccv2013-449</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</h1>
<br/><p>Source: <a title="iccv-2013-449-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Shao_What_Do_You_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Ming Shao, Liangyue Li, Yun Fu</p><p>Abstract: In this paper, we investigate the problem of recognizing occupations of multiple people with arbitrary poses in a photo. Previous work utilizing single person ’s nearly frontal clothing information and fore/background context preliminarily proves that occupation recognition is computationally feasible in computer vision. However, in practice, multiple people with arbitrary poses are common in a photo, and recognizing their occupations is even more challenging. We argue that with appropriately built visual attributes, co-occurrence, and spatial configuration model that is learned through structure SVM, we can recognize multiple people ’s occupations in a photo simultaneously. To evaluate our method’s performance, we conduct extensive experiments on a new well-labeled occupation database with 14 representative occupations and over 7K images. Results on this database validate our method’s effectiveness and show that occupation recognition is solvable in a more general case.</p><p>Reference: <a title="iccv-2013-449-reference" href="../iccv2013_reference/iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu i  Abstract In this paper, we investigate the problem of recognizing occupations of multiple people with arbitrary poses in a photo. [sent-6, score-0.712]
</p><p>2 Previous work utilizing single person ’s nearly frontal clothing information and fore/background context preliminarily proves that occupation recognition is computationally feasible in computer vision. [sent-7, score-1.071]
</p><p>3 However, in practice, multiple people with arbitrary poses are common in a photo, and recognizing their occupations is even more challenging. [sent-8, score-0.712]
</p><p>4 We argue that with appropriately built visual attributes, co-occurrence, and spatial configuration model that is learned through structure SVM, we can recognize multiple people ’s occupations in a photo simultaneously. [sent-9, score-0.771]
</p><p>5 To evaluate our method’s performance, we conduct extensive experiments on a new well-labeled occupation database with 14 representative occupations and over 7K images. [sent-10, score-1.105]
</p><p>6 Results on this database validate our method’s effectiveness and show that occupation recognition is solvable in a more general case. [sent-11, score-0.595]
</p><p>7 , social status, connections, and roles in a particular situation draw great attention since they are the essence of social life. [sent-15, score-0.316]
</p><p>8 In the era of social media, more and more social characteristics can be conveyed via digital carriers, e. [sent-16, score-0.344]
</p><p>9 jointly solving social relation and people identification [3 1]; recognizing people by linked text in videos or images [2]; exploring kinship via transfer learning and semantics [34]. [sent-26, score-0.488]
</p><p>10 Generally, people would like to chat to those with similar occupations or backgrounds. [sent-35, score-0.655]
</p><p>11 Therefore, understanding people’s occupations in customers’ photos can significantly 363 1  improve the experience of social connection recommendations and professional services aimed at a particular group. [sent-36, score-0.752]
</p><p>12 In this paper, we propose a novel framework towards multiple people’s occupations recognition in a photo. [sent-37, score-0.528]
</p><p>13 Unlike [27] that considers images of single person with “nearly frontal” pose, ours can tackle multiple people with arbitrary poses in an image through spatial constraint and occupations cooccurrence. [sent-39, score-0.714]
</p><p>14 Experimental results on this database demonstrate that the proposed method can deal with occupation recognition problem in a more general case, regardless of pose variation, human interaction, and messy background. [sent-41, score-0.595]
</p><p>15 Related Work Occupation prediction has been preliminarily discussed and reasonably solved in [27] where a clothing descriptor based framework is proposed. [sent-44, score-0.391]
</p><p>16 The clothing feature is described via part-based modeling on patches of human body parts, and is semantically represented by informative and noise-tolerant sparse coding [40]. [sent-45, score-0.473]
</p><p>17 Second, other than low-level features, mid-level features like visual attributes are also helpful. [sent-49, score-0.275]
</p><p>18 Third, a person’s occupation is tightly coupled with others’ in the image, by which we can improve overall accuracies of all the people in one photo. [sent-50, score-0.687]
</p><p>19 Second, clothing preference and style are carriers of many social characteristics and demographical information, e. [sent-53, score-0.614]
</p><p>20 Third, content based image retrieval for clothing offers a more flexible way to choose and compare products with high efficiency [21, 36]. [sent-56, score-0.326]
</p><p>21 Finally, clothing recognition has been extended to video surveillance as a real-world practice [38]. [sent-57, score-0.347]
</p><p>22 Visual attributes are descriptive words designed by human to capture visually perceptible properties of objects. [sent-58, score-0.243]
</p><p>23 As semantical mid-level features, visual attributes re-organize the complex relations between low-level features and high-level labels, due to its generality over all objects, i. [sent-60, score-0.275]
</p><p>24 They have been widely used to describe objects’ properties [8], recognize objects [32], verify faces [ 17], and parse clothing [21]. [sent-63, score-0.326]
</p><p>25 Low-Level Feature Representation In [27], people use four learned key points on human body to locate clothing patches, i. [sent-66, score-0.524]
</p><p>26 Notably, recent studies on clothing parsing have attempted to tackle the alignment of clothing  parts through either auxiliary database [21] or superpixels with Conditional Random Field (CRF) [36]. [sent-70, score-0.707]
</p><p>27 Differently, we propose to use dense local patches to yield invariant discriminative features. [sent-72, score-0.182]
</p><p>28 In-reality, however, clothing parts misalignment is common and mainly due to pose variations [2 1]. [sent-74, score-0.326]
</p><p>29 In most cases, the key part of clothing is slightly shifted from the ideal model. [sent-75, score-0.326]
</p><p>30 The generated patches, or an image set, reasonably simulate most possible variations due to human poses, therefore potentially become good candidates of local clothing parts. [sent-77, score-0.326]
</p><p>31 Similar to [3, 27, 21], we use HOG [6], LBP [ 1], color histogram, and skin fraction to represent local clothing patches, and these local descriptors are implemented in dense-grid fashion. [sent-79, score-0.326]
</p><p>32 In fact, background information also proves to be informative in determining occupations [27]. [sent-81, score-0.581]
</p><p>33 Illustration of dense local patches and assembling of discriminative filters. [sent-86, score-0.216]
</p><p>34 First, clothing patches of the policeman are detected through the model in [39]. [sent-87, score-0.438]
</p><p>35 Second, for each clothing patch, we shift its bounding box up, down, right, left by 1/3 of the original size, and yield dense local patches XI as figure illustrated. [sent-88, score-0.466]
</p><p>36 Third, we use the dense local patches as positive samples and patches X1S from standard images (e. [sent-89, score-0.182]
</p><p>37 , fn to form a new visual attribute descriptor  of  fp. [sent-97, score-0.22]
</p><p>38 Motivation Traditionally, visual attributes are learned directly through low-level feature [17] or ranks from a wide-margin ranking function [24]. [sent-102, score-0.275]
</p><p>39 Different from them, we believe the difference between relevant visual attributes is a good descriptor for visual attributes. [sent-103, score-0.368]
</p><p>40 For example, in our clothing description, pattern “plain” is conceptually defined through “striped” or “spotted”, and vice versa. [sent-105, score-0.326]
</p><p>41 Mathematically, this “difference” can be highlighted by decision boundary or discriminative filter in SVM if we consider different visual attributes as data with different labels. [sent-107, score-0.343]
</p><p>42 In addition, our formulation of descriptor for visual attributes avoids the vagueness of ranks. [sent-108, score-0.307]
</p><p>43 Descriptor Formulation In this paper, image sets with relevant yet different attributes are defined as “standard image s”. [sent-114, score-0.272]
</p><p>44 For example, clothing patches in other colors are standard images for clothing patches in red. [sent-115, score-0.8]
</p><p>45 Suppose we have a group of dense local clothing patches XI = [x1, x2 , . [sent-116, score-0.472]
</p><p>46 , XnS], wpahtcerhe, aenadch a aX stSa nisd a group goef m l oXcal patches from different clothing with relevant yet different visual attributes to the input image. [sent-122, score-0.742]
</p><p>47 In geometry, linear filter w and corresponding bias b set up a decision boundary for image set XI and XS, highlighting the difference between XI and XS which could be a better descriptor for visual attribute represented by XI. [sent-130, score-0.217]
</p><p>48 To ease t,h we followingjoint occupation recognition scheme, we predict the probability of each attribute through SVM [4] and its probabilistic output [5], and these visual attributes are now scaled to [0, 1] . [sent-158, score-0.962]
</p><p>49 Prior Knowledge We state our learning model dealing with multiple people’s occupations recognition in a photo, with arbitrary poses and interactions. [sent-166, score-0.551]
</p><p>50 Specifically, as to occupation recognition problem, the co-occurrence is known as: people with the same or relevant occupations seem to appear in the same photo with high probability. [sent-168, score-1.328]
</p><p>51 For example, people with relevant occupations could be “teacher—student” and “waiter—customer”. [sent-169, score-0.684]
</p><p>52 Spatial context indicates the structure of images under some social assumption. [sent-170, score-0.18]
</p><p>53 For example, waiter is standing beside a sitting customer, and a group of people are standing in a line. [sent-171, score-0.305]
</p><p>54 Model Description We propose a score maximum model to find out the most  possible occupations for the people in it. [sent-175, score-0.655]
</p><p>55 Suppose there are K people in a single photo denoted by Z = {zi |i = 1, 2, . [sent-176, score-0.232]
</p><p>56 , C} denotes the occupation label of the i-th people. [sent-184, score-0.539]
</p><p>57 f,oCre}, occupation recognition ainb an image tihs equivalent to maximizing the following score function:  J(Z,Y )  = ? [sent-188, score-0.56]
</p><p>58 For each individual, we use a class template wyi to weight the attribute vector zi, while for pairwise relation of people iand j,we encode their spatial configuration in wyi . [sent-202, score-0.501]
</p><p>59 Inference The proposed greedy search in Algorithm 1is analogous to the non-max suppression (NMS) proposed in [20], however, each local part of the object in original model is replaced by different people in the occupation problem. [sent-212, score-0.712]
</p><p>60 Second, the number of occupations and people in a photo are not large. [sent-218, score-0.739]
</p><p>61 Learning We consider optimizing wyi and wyi in a max-margin learning procedure. [sent-221, score-0.226]
</p><p>62 Therefore, we re-formulate wyi and wyi by wb and wa, favoring the multiple labels and output structure, respectively: ,yj  ,yj  J(Z,Y )  = ? [sent-223, score-0.287]
</p><p>63 There are 14 occupations and over 7K images in total. [sent-268, score-0.507]
</p><p>64 Database To the best of our knowledge, the occupation database2 collected in this paper is so far the largest image database for occupation recognition research in computer vision community. [sent-286, score-1.134]
</p><p>65 We select 14 representative occupation categories (shown in Figure 3) from over 200 well-defined occupations in Wikipedia based on following fundamental criteria. [sent-295, score-1.046]
</p><p>66 First, we choose the occupation category that is visually informative from human perspective. [sent-296, score-0.584]
</p><p>67 As to the attributes annotation, three specialists are involved to label each attribute over all images. [sent-303, score-0.37]
</p><p>68 Their responsibility includes: (1) select or remove images not qualified based on the previous criteria; (2) assign attributes to images via majority voting over three of them. [sent-304, score-0.243]
</p><p>69 Examples of our database for each occupation category is shown in Figure 3. [sent-305, score-0.596]
</p><p>70 Note that we extract lowlevel features for visual attributes only from corresponding body parts, e. [sent-310, score-0.325]
</p><p>71 , hat attributes from head area, upper body attributes from torso and arms. [sent-312, score-0.651]
</p><p>72 For attributes experiment, we use 250 images from each occupation category with one person in each photo for training and test, while for the joint learning framework, we use another 250 images with more than one person in each photo from each category for training and test. [sent-314, score-1.088]
</p><p>73 Evaluation on Visual Attributes We illustrate the visual attributes discussed in this paper in Figure 4. [sent-317, score-0.275]
</p><p>74 The negative samples of each attribute are selected from mutually exclusive samples. [sent-318, score-0.189]
</p><p>75 For instance, the negative samples for hat attribute “Rimless” are samples from other four attributes, namely, “Uniform”, “Helmet”, “Cap”, “No Hat”. [sent-319, score-0.215]
</p><p>76 Please be notified that for upper or lower body clothing attributes, not all attributes are mutually exclusive, but part of them. [sent-320, score-0.678]
</p><p>77 Note that attributes in one group are mutually exclusive, e. [sent-325, score-0.313]
</p><p>78 However, for shape group in upper body and lower body, only attributes with the same color border are mutually exclusive. [sent-328, score-0.39]
</p><p>79 We finally obtain the average precisions for each attribute shown in Figure 5, where “SVM-Visual Attributes” means we directly use low-level feature, while “DF-Visual Attributes” means we use the proposed discriminative filter (DF) as the descriptor. [sent-331, score-0.195]
</p><p>80 In Figure 5, we observe that the proposed visual attributes based on discriminative filters perform better than the attributes based on low-level features plus SVM. [sent-333, score-0.601]
</p><p>81 While for attributes like color that can be easily differentiated through low-level feature, the improvement is acceptable, the improvement on other attributes, e. [sent-334, score-0.243]
</p><p>82 Although most attributes achieve acceptable performances, each single attribute cannot directly determine the occupation category. [sent-338, score-0.909]
</p><p>83 In next experiment, we demonstrate that their combinations offer discriminative features to construct classifiers for both single and multiple people occupation recognition. [sent-339, score-0.729]
</p><p>84 Note “Single” means single person’s occupation recognition which is similar to the method in [27], but not identical, since we use “dense local patches” + “DF-visual attributes” instead of low-level clothing features to deal with pose variations. [sent-378, score-0.886]
</p><p>85 Then we train a one-to-rest binary SVM for each occupation category and use background features in test images as inputs. [sent-382, score-0.582]
</p><p>86 Note that background is also a visual attribute element in the attribute vector used in this paper. [sent-383, score-0.307]
</p><p>87 In addition, we add a hidden occupation “customer” in Table 1, indicating people who have interactions with occupations such as “waiter”, or “doctor”. [sent-384, score-1.194]
</p><p>88 It becomes  significant when people of this occupation tend to show ar-  Figure 6. [sent-386, score-0.687]
</p><p>89 We use different numbers of standard images to learn DF-visual attributes descriptors, and the average performance over color, hat, upper body, and lower-body is shown indexed by different curves. [sent-388, score-0.27]
</p><p>90 Under this situation, clothing patch based method is not stable and many irrelevant factors will be filled into the clothing feature. [sent-392, score-0.652]
</p><p>91 On the other hand, method in [27] performs well when people of this occupation always show the nearly frontal pose, e. [sent-394, score-0.751]
</p><p>92 We also find a significant improvement in occupations tending to show a group of people,  e. [sent-400, score-0.545]
</p><p>93 This proves that our social context based joint learning framework is effective. [sent-403, score-0.232]
</p><p>94 In general, we can see that occupation recognition in an image is still challenging, due to the lack of unique attributes, e. [sent-407, score-0.56]
</p><p>95 , students, teachers, or large variations of clothing style, e. [sent-409, score-0.326]
</p><p>96 Conclusions We proposed a novel framework towards multiple people’s occupations recognition in a photo. [sent-420, score-0.528]
</p><p>97 Second, visual attributes are learned through assembling discriminative filters to bridge the semantic gap between  low-level features and high-level labels occupation categories. [sent-422, score-0.963]
</p><p>98 Extensive experiments on the collected database show that our method works better than the state-of-the-art, especially when there are interactive occupations or a group of people in a photo. [sent-424, score-0.728]
</p><p>99 Street-to-shop: Crossscenario clothing retrieval via parts alignment and auxiliary set. [sent-562, score-0.326]
</p><p>100 Seeing people in social context: Recognizing people and social relationships. [sent-627, score-0.612]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('occupation', 0.539), ('occupations', 0.507), ('clothing', 0.326), ('attributes', 0.243), ('social', 0.158), ('people', 0.148), ('attribute', 0.127), ('wyi', 0.113), ('hat', 0.088), ('photo', 0.084), ('waiter', 0.075), ('patches', 0.074), ('spotted', 0.073), ('pages', 0.071), ('zn', 0.068), ('assembling', 0.066), ('doctor', 0.056), ('firefighter', 0.056), ('body', 0.05), ('yn', 0.05), ('customer', 0.046), ('violated', 0.046), ('yi', 0.043), ('discriminative', 0.042), ('plain', 0.041), ('filters', 0.041), ('wb', 0.038), ('group', 0.038), ('argw', 0.038), ('carriers', 0.038), ('chef', 0.038), ('demographical', 0.038), ('lawyer', 0.038), ('policeman', 0.038), ('soldier', 0.038), ('wyti', 0.038), ('striped', 0.037), ('hypothesis', 0.037), ('person', 0.036), ('gallagher', 0.035), ('player', 0.035), ('zi', 0.035), ('database', 0.035), ('frontal', 0.035), ('pictorial', 0.035), ('dense', 0.034), ('recognizing', 0.034), ('xs', 0.034), ('preliminarily', 0.033), ('teacher', 0.033), ('websites', 0.033), ('visual', 0.032), ('mutually', 0.032), ('yield', 0.032), ('descriptor', 0.032), ('teachers', 0.031), ('wtxi', 0.031), ('photos', 0.031), ('proves', 0.03), ('exclusive', 0.03), ('svm', 0.03), ('xi', 0.029), ('relevant', 0.029), ('nearly', 0.029), ('services', 0.029), ('xns', 0.029), ('northeastern', 0.029), ('fn', 0.029), ('slack', 0.028), ('characteristics', 0.028), ('yun', 0.028), ('office', 0.027), ('joachims', 0.027), ('upper', 0.027), ('recommendations', 0.027), ('lij', 0.027), ('neu', 0.027), ('wa', 0.026), ('style', 0.026), ('profiles', 0.026), ('filter', 0.026), ('greedy', 0.025), ('soccer', 0.025), ('award', 0.024), ('conduct', 0.024), ('poses', 0.023), ('informative', 0.023), ('misclassified', 0.023), ('nms', 0.023), ('favoring', 0.023), ('joint', 0.022), ('lbp', 0.022), ('standing', 0.022), ('student', 0.022), ('category', 0.022), ('context', 0.022), ('background', 0.021), ('recognition', 0.021), ('hog', 0.02), ('parsing', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="449-tfidf-1" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>Author: Ming Shao, Liangyue Li, Yun Fu</p><p>Abstract: In this paper, we investigate the problem of recognizing occupations of multiple people with arbitrary poses in a photo. Previous work utilizing single person ’s nearly frontal clothing information and fore/background context preliminarily proves that occupation recognition is computationally feasible in computer vision. However, in practice, multiple people with arbitrary poses are common in a photo, and recognizing their occupations is even more challenging. We argue that with appropriately built visual attributes, co-occurrence, and spatial configuration model that is learned through structure SVM, we can recognize multiple people ’s occupations in a photo simultaneously. To evaluate our method’s performance, we conduct extensive experiments on a new well-labeled occupation database with 14 representative occupations and over 7K images. Results on this database validate our method’s effectiveness and show that occupation recognition is solvable in a more general case.</p><p>2 0.25440192 <a title="449-tfidf-2" href="./iccv-2013-Paper_Doll_Parsing%3A_Retrieving_Similar_Styles_to_Parse_Clothing_Items.html">306 iccv-2013-Paper Doll Parsing: Retrieving Similar Styles to Parse Clothing Items</a></p>
<p>Author: Kota Yamaguchi, M. Hadi Kiapour, Tamara L. Berg</p><p>Abstract: Clothing recognition is an extremely challenging problem due to wide variation in clothing item appearance, layering, and style. In this paper, we tackle the clothing parsing problem using a retrieval based approach. For a query image, we find similar styles from a large database of tagged fashion images and use these examples to parse the query. Our approach combines parsing from: pre-trained global clothing models, local clothing models learned on theflyfrom retrieved examples, and transferredparse masks (paper doll item transfer) from retrieved examples. Experimental evaluation shows that our approach significantly outperforms state of the art in parsing accuracy.</p><p>3 0.21702504 <a title="449-tfidf-3" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>Author: Xiaoyang Wang, Qiang Ji</p><p>Abstract: This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. As a list of semantically meaningful properties of objects, attributes generally relate to each other statistically. In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and objectindependent attribute relationships. The model utilizes the captured relationships to benefit both attribute prediction and object recognition. Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases.</p><p>4 0.1954267 <a title="449-tfidf-4" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>Author: Adriana Kovashka, Kristen Grauman</p><p>Abstract: Current methods learn monolithic attribute predictors, with the assumption that a single model is sufficient to reflect human understanding of a visual attribute. However, in reality, humans vary in how they perceive the association between a named property and image content. For example, two people may have slightly different internal models for what makes a shoe look “formal”, or they may disagree on which of two scenes looks “more cluttered”. Rather than discount these differences as noise, we propose to learn user-specific attribute models. We adapt a generic model trained with annotations from multiple users, tailoring it to satisfy user-specific labels. Furthermore, we propose novel techniques to infer user-specific labels based on transitivity and contradictions in the user’s search history. We demonstrate that adapted attributes improve accuracy over both existing monolithic models as well as models that learn from scratch with user-specific data alone. In addition, we show how adapted attributes are useful to personalize image search, whether with binary or relative attributes.</p><p>5 0.18744457 <a title="449-tfidf-5" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>Author: Jungseock Joo, Shuo Wang, Song-Chun Zhu</p><p>Abstract: We present a part-based approach to the problem of human attribute recognition from a single image of a human body. To recognize the attributes of human from the body parts, it is important to reliably detect the parts. This is a challenging task due to the geometric variation such as articulation and view-point changes as well as the appearance variation of the parts arisen from versatile clothing types. The prior works have primarily focused on handling . edu . cn ???????????? geometric variation by relying on pre-trained part detectors or pose estimators, which require manual part annotation, but the appearance variation has been relatively neglected in these works. This paper explores the importance of the appearance variation, which is directly related to the main task, attribute recognition. To this end, we propose to learn a rich appearance part dictionary of human with significantly less supervision by decomposing image lattice into overlapping windows at multiscale and iteratively refining local appearance templates. We also present quantitative results in which our proposed method outperforms the existing approaches.</p><p>6 0.15543766 <a title="449-tfidf-6" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>7 0.14711757 <a title="449-tfidf-7" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>8 0.11154144 <a title="449-tfidf-8" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>9 0.11095748 <a title="449-tfidf-9" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>10 0.10995762 <a title="449-tfidf-10" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>11 0.10597574 <a title="449-tfidf-11" href="./iccv-2013-Predicting_Primary_Gaze_Behavior_Using_Social_Saliency_Fields.html">325 iccv-2013-Predicting Primary Gaze Behavior Using Social Saliency Fields</a></p>
<p>12 0.10502437 <a title="449-tfidf-12" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>13 0.10260317 <a title="449-tfidf-13" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>14 0.098529108 <a title="449-tfidf-14" href="./iccv-2013-Learning_Graph_Matching%3A_Oriented_to_Category_Modeling_from_Cluttered_Scenes.html">237 iccv-2013-Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes</a></p>
<p>15 0.093011312 <a title="449-tfidf-15" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>16 0.087204292 <a title="449-tfidf-16" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>17 0.085328661 <a title="449-tfidf-17" href="./iccv-2013-Event_Recognition_in_Photo_Collections_with_a_Stopwatch_HMM.html">147 iccv-2013-Event Recognition in Photo Collections with a Stopwatch HMM</a></p>
<p>18 0.08472088 <a title="449-tfidf-18" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>19 0.083047286 <a title="449-tfidf-19" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>20 0.078043096 <a title="449-tfidf-20" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.17), (1, 0.111), (2, -0.042), (3, -0.112), (4, 0.112), (5, -0.056), (6, -0.03), (7, -0.098), (8, 0.123), (9, 0.115), (10, -0.031), (11, 0.026), (12, 0.006), (13, -0.023), (14, -0.073), (15, 0.046), (16, -0.014), (17, -0.011), (18, 0.008), (19, 0.008), (20, 0.014), (21, 0.036), (22, 0.027), (23, -0.049), (24, -0.01), (25, 0.022), (26, 0.046), (27, -0.006), (28, -0.005), (29, 0.05), (30, -0.007), (31, 0.036), (32, 0.066), (33, 0.016), (34, 0.041), (35, -0.013), (36, 0.055), (37, 0.003), (38, -0.004), (39, -0.042), (40, 0.03), (41, 0.014), (42, -0.062), (43, 0.001), (44, 0.039), (45, 0.015), (46, 0.01), (47, -0.023), (48, -0.036), (49, -0.09)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93541437 <a title="449-lsi-1" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>Author: Ming Shao, Liangyue Li, Yun Fu</p><p>Abstract: In this paper, we investigate the problem of recognizing occupations of multiple people with arbitrary poses in a photo. Previous work utilizing single person ’s nearly frontal clothing information and fore/background context preliminarily proves that occupation recognition is computationally feasible in computer vision. However, in practice, multiple people with arbitrary poses are common in a photo, and recognizing their occupations is even more challenging. We argue that with appropriately built visual attributes, co-occurrence, and spatial configuration model that is learned through structure SVM, we can recognize multiple people ’s occupations in a photo simultaneously. To evaluate our method’s performance, we conduct extensive experiments on a new well-labeled occupation database with 14 representative occupations and over 7K images. Results on this database validate our method’s effectiveness and show that occupation recognition is solvable in a more general case.</p><p>2 0.8526901 <a title="449-lsi-2" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Devi Parikh, Tsuhan Chen</p><p>Abstract: In recent years, there has been a great deal of progress in describing objects with attributes. Attributes have proven useful for object recognition, image search, face verification, image description, and zero-shot learning. Typically, attributes are either binary or relative: they describe either the presence or absence of a descriptive characteristic, or the relative magnitude of the characteristic when comparing two exemplars. However, prior work fails to model the actual way in which humans use these attributes in descriptive statements of images. Specifically, it does not address the important interactions between the binary and relative aspects of an attribute. In this work we propose a spoken attribute classifier which models a more natural way of using an attribute in a description. For each attribute we train a classifier which captures the specific way this attribute should be used. We show that as a result of using this model, we produce descriptions about images of people that are more natural and specific than past systems.</p><p>3 0.8458758 <a title="449-lsi-3" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>Author: Naman Turakhia, Devi Parikh</p><p>Abstract: When we look at an image, some properties or attributes of the image stand out more than others. When describing an image, people are likely to describe these dominant attributes first. Attribute dominance is a result of a complex interplay between the various properties present or absent in the image. Which attributes in an image are more dominant than others reveals rich information about the content of the image. In this paper we tap into this information by modeling attribute dominance. We show that this helps improve the performance of vision systems on a variety of human-centric applications such as zero-shot learning, image search and generating textual descriptions of images.</p><p>4 0.84436101 <a title="449-lsi-4" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>Author: Xiaoyang Wang, Qiang Ji</p><p>Abstract: This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. As a list of semantically meaningful properties of objects, attributes generally relate to each other statistically. In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and objectindependent attribute relationships. The model utilizes the captured relationships to benefit both attribute prediction and object recognition. Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases.</p><p>5 0.80154747 <a title="449-lsi-5" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>Author: Adriana Kovashka, Kristen Grauman</p><p>Abstract: Current methods learn monolithic attribute predictors, with the assumption that a single model is sufficient to reflect human understanding of a visual attribute. However, in reality, humans vary in how they perceive the association between a named property and image content. For example, two people may have slightly different internal models for what makes a shoe look “formal”, or they may disagree on which of two scenes looks “more cluttered”. Rather than discount these differences as noise, we propose to learn user-specific attribute models. We adapt a generic model trained with annotations from multiple users, tailoring it to satisfy user-specific labels. Furthermore, we propose novel techniques to infer user-specific labels based on transitivity and contradictions in the user’s search history. We demonstrate that adapted attributes improve accuracy over both existing monolithic models as well as models that learn from scratch with user-specific data alone. In addition, we show how adapted attributes are useful to personalize image search, whether with binary or relative attributes.</p><p>6 0.78607577 <a title="449-lsi-6" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>7 0.76443326 <a title="449-lsi-7" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>8 0.74861461 <a title="449-lsi-8" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>9 0.65424526 <a title="449-lsi-9" href="./iccv-2013-Relative_Attributes_for_Large-Scale_Abandoned_Object_Detection.html">350 iccv-2013-Relative Attributes for Large-Scale Abandoned Object Detection</a></p>
<p>10 0.61668599 <a title="449-lsi-10" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>11 0.61135787 <a title="449-lsi-11" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<p>12 0.60450184 <a title="449-lsi-12" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>13 0.57607609 <a title="449-lsi-13" href="./iccv-2013-Paper_Doll_Parsing%3A_Retrieving_Similar_Styles_to_Parse_Clothing_Items.html">306 iccv-2013-Paper Doll Parsing: Retrieving Similar Styles to Parse Clothing Items</a></p>
<p>14 0.5646193 <a title="449-lsi-14" href="./iccv-2013-NEIL%3A_Extracting_Visual_Knowledge_from_Web_Data.html">285 iccv-2013-NEIL: Extracting Visual Knowledge from Web Data</a></p>
<p>15 0.56123275 <a title="449-lsi-15" href="./iccv-2013-The_Interestingness_of_Images.html">416 iccv-2013-The Interestingness of Images</a></p>
<p>16 0.54864758 <a title="449-lsi-16" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>17 0.53096837 <a title="449-lsi-17" href="./iccv-2013-Quadruplet-Wise_Image_Similarity_Learning.html">332 iccv-2013-Quadruplet-Wise Image Similarity Learning</a></p>
<p>18 0.52443916 <a title="449-lsi-18" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>19 0.52422339 <a title="449-lsi-19" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>20 0.50086856 <a title="449-lsi-20" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.076), (4, 0.012), (7, 0.03), (12, 0.017), (26, 0.095), (31, 0.041), (34, 0.051), (35, 0.01), (36, 0.172), (42, 0.122), (64, 0.054), (73, 0.021), (89, 0.171), (97, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89378083 <a title="449-lda-1" href="./iccv-2013-Shape_Index_Descriptors_Applied_to_Texture-Based_Galaxy_Analysis.html">388 iccv-2013-Shape Index Descriptors Applied to Texture-Based Galaxy Analysis</a></p>
<p>Author: Kim Steenstrup Pedersen, Kristoffer Stensbo-Smidt, Andrew Zirm, Christian Igel</p><p>Abstract: A texture descriptor based on the shape index and the accompanying curvedness measure is proposed, and it is evaluated for the automated analysis of astronomical image data. A representative sample of images of low-redshift galaxies from the Sloan Digital Sky Survey (SDSS) serves as a testbed. The goal of applying texture descriptors to these data is to extract novel information about galaxies; information which is often lost in more traditional analysis. In this study, we build a regression model for predicting a spectroscopic quantity, the specific star-formation rate (sSFR). As texture features we consider multi-scale gradient orientation histograms as well as multi-scale shape index histograms, which lead to a new descriptor. Our results show that we can successfully predict spectroscopic quantities from the texture in optical multi-band images. We successfully recover the observed bi-modal distribution of galaxies into quiescent and star-forming. The state-ofthe-art for predicting the sSFR is a color-based physical model. We significantly improve its accuracy by augmenting the model with texture information. This study is thefirst step towards enabling the quantification of physical galaxy properties from imaging data alone.</p><p>same-paper 2 0.86341876 <a title="449-lda-2" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>Author: Ming Shao, Liangyue Li, Yun Fu</p><p>Abstract: In this paper, we investigate the problem of recognizing occupations of multiple people with arbitrary poses in a photo. Previous work utilizing single person ’s nearly frontal clothing information and fore/background context preliminarily proves that occupation recognition is computationally feasible in computer vision. However, in practice, multiple people with arbitrary poses are common in a photo, and recognizing their occupations is even more challenging. We argue that with appropriately built visual attributes, co-occurrence, and spatial configuration model that is learned through structure SVM, we can recognize multiple people ’s occupations in a photo simultaneously. To evaluate our method’s performance, we conduct extensive experiments on a new well-labeled occupation database with 14 representative occupations and over 7K images. Results on this database validate our method’s effectiveness and show that occupation recognition is solvable in a more general case.</p><p>3 0.83899534 <a title="449-lda-3" href="./iccv-2013-Nested_Shape_Descriptors.html">288 iccv-2013-Nested Shape Descriptors</a></p>
<p>Author: Jeffrey Byrne, Jianbo Shi</p><p>Abstract: In this paper, we propose a new family of binary local feature descriptors called nested shape descriptors. These descriptors are constructed by pooling oriented gradients over a large geometric structure called the Hawaiian earring, which is constructed with a nested correlation structure that enables a new robust local distance function called the nesting distance. This distance function is unique to the nested descriptor and provides robustness to outliers from order statistics. In this paper, we define the nested shape descriptor family and introduce a specific member called the seed-of-life descriptor. We perform a trade study to determine optimal descriptor parameters for the task of image matching. Finally, we evaluate performance compared to state-of-the-art local feature descriptors on the VGGAffine image matching benchmark, showing significant performance gains. Our descriptor is thefirst binary descriptor to outperform SIFT on this benchmark.</p><p>4 0.81314045 <a title="449-lda-4" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>Author: Xiaoyang Wang, Qiang Ji</p><p>Abstract: This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. As a list of semantically meaningful properties of objects, attributes generally relate to each other statistically. In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and objectindependent attribute relationships. The model utilizes the captured relationships to benefit both attribute prediction and object recognition. Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases.</p><p>5 0.81239069 <a title="449-lda-5" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>Author: S. Karthikeyan, Vignesh Jagadeesh, Renuka Shenoy, Miguel Ecksteinz, B.S. Manjunath</p><p>Abstract: Eye movement studies have confirmed that overt attention is highly biased towards faces and text regions in images. In this paper we explore a novel problem of predicting face and text regions in images using eye tracking data from multiple subjects. The problem is challenging as we aim to predict the semantics (face/text/background) only from eye tracking data without utilizing any image information. The proposed algorithm spatially clusters eye tracking data obtained in an image into different coherent groups and subsequently models the likelihood of the clusters containing faces and text using afully connectedMarkov Random Field (MRF). Given the eye tracking datafrom a test image, itpredicts potential face/head (humans, dogs and cats) and text locations reliably. Furthermore, the approach can be used to select regions of interest for further analysis by object detectors for faces and text. The hybrid eye position/object detector approach achieves better detection performance and reduced computation time compared to using only the object detection algorithm. We also present a new eye tracking dataset on 300 images selected from ICDAR, Street-view, Flickr and Oxford-IIIT Pet Dataset from 15 subjects.</p><p>6 0.80499864 <a title="449-lda-6" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>7 0.80324244 <a title="449-lda-7" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>8 0.80207396 <a title="449-lda-8" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>9 0.80195218 <a title="449-lda-9" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>10 0.80162311 <a title="449-lda-10" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>11 0.80155134 <a title="449-lda-11" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>12 0.80077696 <a title="449-lda-12" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>13 0.80040604 <a title="449-lda-13" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>14 0.80036026 <a title="449-lda-14" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>15 0.80034745 <a title="449-lda-15" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>16 0.8002066 <a title="449-lda-16" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>17 0.80018151 <a title="449-lda-17" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>18 0.80006713 <a title="449-lda-18" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>19 0.80001426 <a title="449-lda-19" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>20 0.79964828 <a title="449-lda-20" href="./iccv-2013-Learning_a_Dictionary_of_Shape_Epitomes_with_Applications_to_Image_Labeling.html">245 iccv-2013-Learning a Dictionary of Shape Epitomes with Applications to Image Labeling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
