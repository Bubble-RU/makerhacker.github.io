<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>355 iccv-2013-Robust Face Landmark Estimation under Occlusion</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-355" href="#">iccv2013-355</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>355 iccv-2013-Robust Face Landmark Estimation under Occlusion</h1>
<br/><p>Source: <a title="iccv-2013-355-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Burgos-Artizzu_Robust_Face_Landmark_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Xavier P. Burgos-Artizzu, Pietro Perona, Piotr Dollár</p><p>Abstract: Human faces captured in real-world conditions present large variations in shape and occlusions due to differences in pose, expression, use of accessories such as sunglasses and hats and interactions with objects (e.g. food). Current face landmark estimation approaches struggle under such conditions since theyfail toprovide aprincipled way ofhandling outliers. We propose a novel method, called Robust Cascaded Pose Regression (RCPR) which reduces exposure to outliers by detecting occlusions explicitly and using robust shape-indexed features. We show that RCPR improves on previous landmark estimation methods on three popular face datasets (LFPW, LFW and HELEN). We further explore RCPR ’s performance by introducing a novel face dataset focused on occlusion, composed of 1,007 faces presenting a wide range of occlusion patterns. RCPR reduces failure cases by half on all four datasets, at the same time as it detects face occlusions with a 80/40% precision/recall.</p><p>Reference: <a title="iccv-2013-355-reference" href="../iccv2013_reference/iccv-2013-Robust_Face_Landmark_Estimation_under_Occlusion_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Robust face landmark estimation under occlusion Xavier P. [sent-1, score-0.399]
</p><p>2 edu  Abstract Human faces captured in real-world conditions present large variations in shape and occlusions due to differences in pose, expression, use of accessories such as sunglasses and hats and interactions with objects (e. [sent-6, score-0.37]
</p><p>3 Current face landmark estimation approaches struggle under such conditions since theyfail toprovide aprincipled way ofhandling outliers. [sent-9, score-0.315]
</p><p>4 We propose a novel method, called Robust Cascaded Pose Regression (RCPR) which reduces exposure to outliers by detecting occlusions explicitly and using robust shape-indexed features. [sent-10, score-0.152]
</p><p>5 We show that RCPR improves on previous landmark estimation methods on three popular face datasets (LFPW, LFW and HELEN). [sent-11, score-0.339]
</p><p>6 We further explore RCPR ’s performance by introducing a novel face dataset focused on occlusion, composed of 1,007 faces presenting a wide range of occlusion patterns. [sent-12, score-0.298]
</p><p>7 RCPR reduces failure cases by half on all four datasets, at the same time as it detects face occlusions with a 80/40% precision/recall. [sent-13, score-0.281]
</p><p>8 The shape of human bodies and human faces has attracted particular attention [43, 46, 33, 2]. [sent-16, score-0.195]
</p><p>9 By shape here we mean the parameters of a model that describe the configuration of an object in the image or, alternatively, the location of a number of parts or landmarks in the image or in 3D space. [sent-17, score-0.17]
</p><p>10 Recently, it has emerged as a particularly effective and accurate approach for estimating face landmarks [7]. [sent-20, score-0.138]
</p><p>11 However, face landmark estimation “in the wild” remains a very challenging task. [sent-21, score-0.279]
</p><p>12 We find that CPR struggles under occlusions and large shape variations. [sent-22, score-0.174]
</p><p>13 RCPR estimates landmark positions as well as their occlusion state (red=occluded, green=unoccluded). [sent-26, score-0.293]
</p><p>14 RCPR improves performance by increasing robustness to occlusions and large shape variations, which occur often in real-world conditions. [sent-28, score-0.235]
</p><p>15 RCPR is able to detect face occlusion at the same time as it estimates the landmark positions. [sent-29, score-0.375]
</p><p>16 The occlusion information helps during learning to select unoccluded features and is exploited dynamically through robust statistics to reduce errors inside the cascade. [sent-30, score-0.193]
</p><p>17 This results in an overall improvement as well as a reduction of failure cases by half when faced with difficult images. [sent-31, score-0.126]
</p><p>18 The main contributions of this work are: 1 A novel cascaded regression method, called Robust Cascaded Pose Regression (RCPR). [sent-32, score-0.214]
</p><p>19 As we show in Section 5, RCPR outperforms previous landmark estimation work on four different, varied face datasets. [sent-33, score-0.279]
</p><p>20 Moreover, RCPR is the first approach capable of detecting occlusions at the same time as it estimates landmarks, see Figure 1. [sent-35, score-0.121]
</p><p>21 2 The introduction of a challenging face landmark dataset: Caltech Occluded Faces in the Wild (COFW). [sent-37, score-0.255]
</p><p>22 This dataset is designed to benchmark face landmark algorithms in realistic conditions, which include heavy occlusions and large shape variations. [sent-38, score-0.447]
</p><p>23 Other popular modern approaches to detect the pose or parts of an object involve first detecting the object parts in-  dependently and then estimating pose and/or shape through flexible parts models [5, 27, 2, 20] or directly from detections [21, 19, 8]. [sent-45, score-0.232]
</p><p>24 However, our experimental results seem to indicate that they are less suited for high accuracy landmark estimation, see Section 5. [sent-47, score-0.173]
</p><p>25 Another option is to tackle shape estimation as a regression problem, learning regressors that directly predict the object shape or the location of its parts, starting from a raw estimate of its position [39, 14, 16, 12, 7, 42, 25, 6]. [sent-49, score-0.465]
</p><p>26 These methods generally use boosted regression [23, 15] and random fern regressors [35]. [sent-50, score-0.313]
</p><p>27 Current regression methods are fast and tolerate a small amount of shape variations but are not robust to occlusions and large shape variations. [sent-53, score-0.401]
</p><p>28 We find that occlusions and large shape variations are quite common in real-world faces. [sent-54, score-0.206]
</p><p>29 In Section 4 we introduce a new, more realistic face dataset, collected with a focus on real-world occlusions and a variety of expressions. [sent-56, score-0.17]
</p><p>30 T that start from a raw initial shape guess S0 and progressiveltyha rte sfitnaert e fsrtoimma atio ranw, o inutitpiautlti shnag pfien aglu eshssap Se estimation ST. [sent-67, score-0.127]
</p><p>31 eAdt e aasch a is teerriaetsio onf, regres-  raw shape estimation S0, and trained cascade regressors sRh1a. [sent-71, score-0.332]
</p><p>32 w During learning, each regressor Rt is trained to attempt to minimize the difference between the true shape and the shape estimate of the previous iteration St−1 . [sent-77, score-0.228]
</p><p>33 The key to CPR lies on computing robust shape-indexed features and training regressors able to progressively reduce the estimation error at each iteration. [sent-79, score-0.222]
</p><p>34 Both [14, 7] use depth 5 random fern regressors as regressors Rt and shape-indexed control point features [35]. [sent-80, score-0.342]
</p><p>35 To speed-up training convergence and improve overall performance, [7] performs regression on all shape parameters at once instead of one parameter at a time, effectively exploiting shape constraints. [sent-84, score-0.267]
</p><p>36 Finally, to improve feature invariance to shape variations, features are referenced locally with respect to their closest landmark instead of globally with respect to global shape as [14] originally proposed. [sent-87, score-0.383]
</p><p>37 Robust Cascaded Pose Regression (RCPR) Both the original CPR [14] and the variant proposed in [7] struggle when faced with occlusions and large shape variations. [sent-90, score-0.24]
</p><p>38 Boosted regressors are unable to handle outliers in a principled way, causing a propagation of errors inside the cascade, harming the whole process. [sent-91, score-0.162]
</p><p>39 (a) example annotation with occlusion information (red=occluded, green=unoccluded) (b) Dataset occlusion statistics, grouped in 9 zones. [sent-96, score-0.24]
</p><p>40 1  Robustness to occlusion  Current approaches struggle under occlusion because they do not treat it in a principled way. [sent-101, score-0.276]
</p><p>41 We propose to incorporate occlusion directly during learning to improve shape estimation. [sent-102, score-0.206]
</p><p>42 Our method requires ground-truth annotations for occlusion in the training set. [sent-103, score-0.12]
</p><p>43 This information can be added with minor cost during the annotation procedure, adding a flag to each landmark encoding its visibility, see Figure 2(a). [sent-104, score-0.173]
</p><p>44 As we will show, this information is not only a richer representation of the object shape, it can also be of great use to better handle occlusions during shape estimation. [sent-113, score-0.174]
</p><p>45 Then, all three dimensions are learnt simultaneously using cascaded regression (treating visibility as a continuous, non-binary variable). [sent-115, score-0.227]
</p><p>46 CPR’s coarse-to-fine nature implies that occlusion estimation starts to be accurate from early in the cascade. [sent-116, score-0.144]
</p><p>47 This suggests that occlusion information can be used at the same time as it is being estimated to help shape estimation. [sent-117, score-0.206]
</p><p>48 We introduce a novel occlusion-centered approach which leverages occlusion information to improve the robustness of shape updates δS at each iteration. [sent-118, score-0.228]
</p><p>49 Given an image, the face (whose location is provided by  a face detector) is divided into a 3x3 grid, see Figure 2(b). [sent-119, score-0.177]
</p><p>50 At each iteration t, the amount of occlusion present in each one ofthe 9 zones can be estimated by projecting the current estimate St−1 = [x1. [sent-120, score-0.167]
</p><p>51 Then, insetsetaimd aotef tSraining a single boosted regressor Rt at each iteration t, we propose to train Stot regressors R1t. [sent-127, score-0.22]
</p><p>52 Stot is combined through a weighted mean voting, δwShere weight is inversely proportional to the total amount of occlusion present in the zones from which the regressor drew features. [sent-133, score-0.208]
</p><p>53 We found that good results are achieved using as little as Stot = 3 regressors (see Supp. [sent-134, score-0.144]
</p><p>54 Tgh fise atullroewss t regressors to learn image occlusions properly. [sent-137, score-0.232]
</p><p>55 The key behind our approach is that we enforce “visually different” regressors to reach consensus, trusting more those using features from non-occluded areas of the image. [sent-147, score-0.175]
</p><p>56 2, adding our occlusion reasoning results in a win-win scenario: it improves both landmark estimation and occlusion detection. [sent-149, score-0.476]
</p><p>57 Shape-indexed features invariant to face scales and poses are key to shape estimation success under these conditions. [sent-154, score-0.207]
</p><p>58 This is more robust than referencing features directly with respect to the global shape as originally proposed in [14]. [sent-156, score-0.167]
</p><p>59 However, these features are still not robust enough against large pose variations and shape deformations. [sent-157, score-0.188]
</p><p>60 These new  features are much more robust to shape variations as shown in Figure 3. [sent-160, score-0.147]
</p><p>61 Furthermore, since there is no longer need to find the closest landmark in the current estimate of the shape for each feature, computation is considerably faster (3x speedup). [sent-162, score-0.259]
</p><p>62 The difference in shape ofthe red curve (longer tailed distribution) suggests that variance can be used to predict failure cases early on. [sent-181, score-0.171]
</p><p>63 (b) Average error (bars) and speed (dashed lines) of the smart restarts (blue) compared with the traditional approach (black) on COFW as a function of the number of initializations used. [sent-182, score-0.183]
</p><p>64 LFPW is one of the most used datasets to benchmark face landmark estimation in unconstrained conditions, and is composed of 1300 images, annotated with 29 landmarks. [sent-202, score-0.344]
</p><p>65 Faces are densely annotated using 194 landmarks, representing a benchmark for high detail face landmark localization. [sent-204, score-0.299]
</p><p>66 However, we could not exploit all the benefits of our method due to the lack of occlusions in these datasets and performance saturation (RCPR reaches results almost on par with humans on LFPW and LFW). [sent-208, score-0.164]
</p><p>67 These datasets are not challenging enough since they do not contain faces showing high variations in pose, expressions and  occlusions which are typical in real-world images. [sent-209, score-0.24]
</p><p>68 Our face dataset is designed to present faces in realworld conditions. [sent-211, score-0.163]
</p><p>69 We wanted faces showing large variations in shape and occlusions due to differences in pose, expression, use of accessories such as sunglasses and hats and interactions with objects (e. [sent-212, score-0.37]
</p><p>70 We annotated both the landmark positions as well as their occluded/unoccluded state, see Figure 2. [sent-220, score-0.199]
</p><p>71 The faces are occluded to different degrees, with large variations in the type of occlusions encountered. [sent-221, score-0.234]
</p><p>72 To increase the number of training images, and since COFW has the exact same landmarks as LFPW, for training we use the original non-augmented 845 LFPW faces + 500 COFW faces (1345 total), and for testing the remaining 507 COFW faces. [sent-223, score-0.218]
</p><p>73 To make sure all images had occlusion labels, we annotated occlusion on the available 845 LFPW  training images, finding an average of only 2% occlusion. [sent-224, score-0.266]
</p><p>74 Implementation details In all experiments, to best replicate results of [7, 1], we simulate the output of a face detector providing the bounding box location and scale of the face with a minimum 80% random overlap with ground truth. [sent-227, score-0.177]
</p><p>75 In our implementation, fewer boosted regressors and more iterations (T = 100, K = 50) perform better than what originally reported by the authors as optimal (T = 10, K = 500). [sent-233, score-0.202]
</p><p>76 When using Stot > 1 regressors for robustness to occlusions, we reduce K accordingly to have approximately the same total number of regressors in the cascade (e. [sent-235, score-0.356]
</p><p>77 Errors in all datasets are measured as the average landmark distance to ground-truth, normalized as percentages with respect to interocular distance. [sent-242, score-0.229]
</p><p>78 LFPW, HELEN and LFW Since the main component of RCPR is occlusioncentered regression and occlusion is virtually non-existent in LFPW, HELEN and LFW datasets, in this section we benchmark a version of RCPR which uses only the new shape-indexed features and smart restarts. [sent-246, score-0.288]
</p><p>79 RCPR improves [7]’s results in all cases, reducing failure cases by half, proving its higher robustness to outliers. [sent-255, score-0.129]
</p><p>80 Figure 6 shows landmark estimation results and Figure 7 shows occlusion detection results for each RCPR variant. [sent-269, score-0.332]
</p><p>81 Each of RCPR’s components contributes to improve landmark estimation. [sent-270, score-0.173]
</p><p>82 The smart restarts also reduce errors while maintaining similar speed performance. [sent-272, score-0.143]
</p><p>83 The occlusion-centered regression further improves landmark estimation at some cost in speed. [sent-273, score-0.331]
</p><p>84 Combining different regressors and weighting them 1155 1177  Method  LFPW failures  error  fps  Method  HELEN failures  error  LFW fps  Method  error  failures  fps  [32]  11. [sent-274, score-0.399]
</p><p>85 Using occlusion-centered regression clearly improves area under the curve for occlusion detection. [sent-305, score-0.254]
</p><p>86 according to their occlusion also improves the area under the curve for occlusion detection around 10%. [sent-306, score-0.294]
</p><p>87 Full RCPR improves on previous cascaded regression approaches [7] by a large margin, especially improving on difficult images, reducing the number of failure cases by 16% (almost half). [sent-307, score-0.305]
</p><p>88 These methods struggle with occlusion because they weren’t trained on  it. [sent-310, score-0.171]
</p><p>89 occlusion  error  − − − − − − −− >  orr Figure 9. [sent-369, score-0.145]
</p><p>90 Results are ordered by increasing landmark estimation error (Y axis) and occlusion error (X axis). [sent-371, score-0.367]
</p><p>91 RCPR succeeds at localizing face landmarks within 10% of their true location in 80% of COFW images, and detects occlusion  with an 80/40% precision/recall. [sent-372, score-0.289]
</p><p>92 Discussion and conclusions Occlusions and high shape variances are a difficult challenge for current face landmark estimation methods. [sent-374, score-0.365]
</p><p>93 RCPR is capable of detecting occlusions explicitly, estimating both the landmark positions and their occlusion. [sent-376, score-0.294]
</p><p>94 This dataset represents a very challenging task due to the large amount and variety of occlusions and large shape variations. [sent-381, score-0.174]
</p><p>95 3D shape regression  [7] [8] [9] [10] [11] [12]  [13]  [14] [15] [16] [17] [18] [19]  [20]  for real-time facial animation. [sent-426, score-0.239]
</p><p>96 Face and landmark detection by using cascade of classifiers. [sent-439, score-0.234]
</p><p>97 Privileged information-based conditional regression forest for facial feature detection. [sent-547, score-0.153]
</p><p>98 Labeled faces in the wild: A database for studying face rec. [sent-554, score-0.163]
</p><p>99 Locating facial features with an extended active shape model. [sent-594, score-0.191]
</p><p>100 Facial point detection using boosted regression and graph models. [sent-640, score-0.145]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rcpr', 0.812), ('cofw', 0.2), ('lfpw', 0.178), ('cpr', 0.177), ('landmark', 0.173), ('regressors', 0.144), ('occlusion', 0.12), ('helen', 0.117), ('cascaded', 0.103), ('lfw', 0.096), ('regression', 0.095), ('occlusions', 0.088), ('shape', 0.086), ('face', 0.082), ('faces', 0.081), ('stot', 0.071), ('restarts', 0.07), ('wild', 0.06), ('facial', 0.058), ('landmarks', 0.056), ('failure', 0.053), ('zones', 0.047), ('cascade', 0.046), ('regressor', 0.041), ('pose', 0.041), ('smart', 0.04), ('fern', 0.039), ('improves', 0.039), ('struggle', 0.036), ('hats', 0.035), ('interocular', 0.035), ('boosted', 0.035), ('caltech', 0.034), ('occluded', 0.033), ('initializations', 0.033), ('fps', 0.033), ('variations', 0.032), ('active', 0.032), ('faced', 0.03), ('fg', 0.03), ('visibility', 0.029), ('referencing', 0.029), ('vpi', 0.029), ('fleuret', 0.028), ('half', 0.028), ('sunglasses', 0.027), ('failures', 0.027), ('annotated', 0.026), ('unoccluded', 0.026), ('restart', 0.025), ('error', 0.025), ('estimation', 0.024), ('originally', 0.023), ('saturation', 0.023), ('sp', 0.022), ('robustness', 0.022), ('rt', 0.022), ('cao', 0.022), ('augmentation', 0.021), ('accessories', 0.021), ('datasets', 0.021), ('doll', 0.021), ('humans', 0.019), ('food', 0.019), ('weber', 0.019), ('detecting', 0.019), ('perona', 0.019), ('asm', 0.019), ('benchmark', 0.018), ('errors', 0.018), ('localizing', 0.018), ('expressions', 0.018), ('martinez', 0.017), ('variance', 0.017), ('raw', 0.017), ('aam', 0.016), ('reach', 0.016), ('threshold', 0.016), ('cootes', 0.016), ('kriegman', 0.016), ('called', 0.016), ('cases', 0.015), ('parts', 0.015), ('features', 0.015), ('speed', 0.015), ('branson', 0.015), ('wah', 0.015), ('reduces', 0.015), ('gross', 0.015), ('welinder', 0.015), ('matthews', 0.015), ('trained', 0.015), ('presenting', 0.015), ('detection', 0.015), ('human', 0.014), ('robust', 0.014), ('speedup', 0.014), ('capable', 0.014), ('par', 0.013), ('location', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="355-tfidf-1" href="./iccv-2013-Robust_Face_Landmark_Estimation_under_Occlusion.html">355 iccv-2013-Robust Face Landmark Estimation under Occlusion</a></p>
<p>Author: Xavier P. Burgos-Artizzu, Pietro Perona, Piotr Dollár</p><p>Abstract: Human faces captured in real-world conditions present large variations in shape and occlusions due to differences in pose, expression, use of accessories such as sunglasses and hats and interactions with objects (e.g. food). Current face landmark estimation approaches struggle under such conditions since theyfail toprovide aprincipled way ofhandling outliers. We propose a novel method, called Robust Cascaded Pose Regression (RCPR) which reduces exposure to outliers by detecting occlusions explicitly and using robust shape-indexed features. We show that RCPR improves on previous landmark estimation methods on three popular face datasets (LFPW, LFW and HELEN). We further explore RCPR ’s performance by introducing a novel face dataset focused on occlusion, composed of 1,007 faces presenting a wide range of occlusion patterns. RCPR reduces failure cases by half on all four datasets, at the same time as it detects face occlusions with a 80/40% precision/recall.</p><p>2 0.22596279 <a title="355-tfidf-2" href="./iccv-2013-Pose-Free_Facial_Landmark_Fitting_via_Optimized_Part_Mixtures_and_Cascaded_Deformable_Shape_Model.html">321 iccv-2013-Pose-Free Facial Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Shape Model</a></p>
<p>Author: Xiang Yu, Junzhou Huang, Shaoting Zhang, Wang Yan, Dimitris N. Metaxas</p><p>Abstract: This paper addresses the problem of facial landmark localization and tracking from a single camera. We present a two-stage cascaded deformable shape model to effectively and efficiently localize facial landmarks with large head pose variations. For face detection, we propose a group sparse learning method to automatically select the most salient facial landmarks. By introducing 3D face shape model, we use procrustes analysis to achieve pose-free facial landmark initialization. For deformation, the first step uses mean-shift local search with constrained local model to rapidly approach the global optimum. The second step uses component-wise active contours to discriminatively refine the subtle shape variation. Our framework can simultaneously handle face detection, pose-free landmark localization and tracking in real time. Extensive experiments are conducted on both laboratory environmental face databases and face-in-the-wild databases. All results demonstrate that our approach has certain advantages over state-of-theart methods in handling pose variations1.</p><p>3 0.21879646 <a title="355-tfidf-3" href="./iccv-2013-Cascaded_Shape_Space_Pruning_for_Robust_Facial_Landmark_Detection.html">70 iccv-2013-Cascaded Shape Space Pruning for Robust Facial Landmark Detection</a></p>
<p>Author: Xiaowei Zhao, Shiguang Shan, Xiujuan Chai, Xilin Chen</p><p>Abstract: In this paper, we propose a novel cascaded face shape space pruning algorithm for robust facial landmark detection. Through progressively excluding the incorrect candidate shapes, our algorithm can accurately and efficiently achieve the globally optimal shape configuration. Specifically, individual landmark detectors are firstly applied to eliminate wrong candidates for each landmark. Then, the candidate shape space is further pruned by jointly removing incorrect shape configurations. To achieve this purpose, a discriminative structure classifier is designed to assess the candidate shape configurations. Based on the learned discriminative structure classifier, an efficient shape space pruning strategy is proposed to quickly reject most incorrect candidate shapes while preserve the true shape. The proposed algorithm is carefully evaluated on a large set of real world face images. In addition, comparison results on the publicly available BioID and LFW face databases demonstrate that our algorithm outperforms some state-of-the-art algorithms.</p><p>4 0.14052592 <a title="355-tfidf-4" href="./iccv-2013-Exemplar-Based_Graph_Matching_for_Robust_Facial_Landmark_Localization.html">149 iccv-2013-Exemplar-Based Graph Matching for Robust Facial Landmark Localization</a></p>
<p>Author: Feng Zhou, Jonathan Brandt, Zhe Lin</p><p>Abstract: Localizing facial landmarks is a fundamental step in facial image analysis. However, the problem is still challenging due to the large variability in pose and appearance, and the existence ofocclusions in real-worldface images. In this paper, we present exemplar-based graph matching (EGM), a robust framework for facial landmark localization. Compared to conventional algorithms, EGM has three advantages: (1) an affine-invariant shape constraint is learned online from similar exemplars to better adapt to the test face; (2) the optimal landmark configuration can be directly obtained by solving a graph matching problem with the learned shape constraint; (3) the graph matching problem can be optimized efficiently by linear programming. To our best knowledge, this is the first attempt to apply a graph matching technique for facial landmark localization. Experiments on several challenging datasets demonstrate the advantages of EGM over state-of-the-art methods.</p><p>5 0.10997465 <a title="355-tfidf-5" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>Author: Yizhe Zhang, Ming Shao, Edward K. Wong, Yun Fu</p><p>Abstract: One of the most challenging task in face recognition is to identify people with varied poses. Namely, the test faces have significantly different poses compared with the registered faces. In this paper, we propose a high-level feature learning scheme to extract pose-invariant identity feature for face recognition. First, we build a single-hiddenlayer neural network with sparse constraint, to extractposeinvariant feature in a supervised fashion. Second, we further enhance the discriminative capability of the proposed feature by using multiple random faces as the target values for multiple encoders. By enforcing the target values to be uniquefor inputfaces over differentposes, the learned highlevel feature that is represented by the neurons in the hidden layer is pose free and only relevant to the identity information. Finally, we conduct face identification on CMU MultiPIE, and verification on Labeled Faces in the Wild (LFW) databases, where identification rank-1 accuracy and face verification accuracy with ROC curve are reported. These experiments demonstrate that our model is superior to oth- er state-of-the-art approaches on handling pose variations.</p><p>6 0.10165193 <a title="355-tfidf-6" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>7 0.097730726 <a title="355-tfidf-7" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>8 0.088432103 <a title="355-tfidf-8" href="./iccv-2013-Handling_Occlusions_with_Franken-Classifiers.html">190 iccv-2013-Handling Occlusions with Franken-Classifiers</a></p>
<p>9 0.082662582 <a title="355-tfidf-9" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>10 0.079298325 <a title="355-tfidf-10" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>11 0.077987924 <a title="355-tfidf-11" href="./iccv-2013-Optimization_Problems_for_Fast_AAM_Fitting_in-the-Wild.html">302 iccv-2013-Optimization Problems for Fast AAM Fitting in-the-Wild</a></p>
<p>12 0.075048335 <a title="355-tfidf-12" href="./iccv-2013-Modeling_Occlusion_by_Discriminative_AND-OR_Structures.html">269 iccv-2013-Modeling Occlusion by Discriminative AND-OR Structures</a></p>
<p>13 0.073922791 <a title="355-tfidf-13" href="./iccv-2013-Rank_Minimization_across_Appearance_and_Shape_for_AAM_Ensemble_Fitting.html">339 iccv-2013-Rank Minimization across Appearance and Shape for AAM Ensemble Fitting</a></p>
<p>14 0.07130938 <a title="355-tfidf-14" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>15 0.068266429 <a title="355-tfidf-15" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>16 0.068131708 <a title="355-tfidf-16" href="./iccv-2013-Monocular_Image_3D_Human_Pose_Estimation_under_Self-Occlusion.html">273 iccv-2013-Monocular Image 3D Human Pose Estimation under Self-Occlusion</a></p>
<p>17 0.063844517 <a title="355-tfidf-17" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>18 0.061121508 <a title="355-tfidf-18" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>19 0.06039026 <a title="355-tfidf-19" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>20 0.059637539 <a title="355-tfidf-20" href="./iccv-2013-Accurate_and_Robust_3D_Facial_Capture_Using_a_Single_RGBD_Camera.html">36 iccv-2013-Accurate and Robust 3D Facial Capture Using a Single RGBD Camera</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.121), (1, -0.01), (2, -0.054), (3, -0.057), (4, 0.023), (5, -0.125), (6, 0.166), (7, 0.095), (8, -0.034), (9, -0.001), (10, -0.018), (11, 0.073), (12, 0.057), (13, -0.023), (14, -0.009), (15, 0.006), (16, 0.026), (17, 0.025), (18, -0.004), (19, -0.019), (20, -0.01), (21, 0.041), (22, -0.006), (23, 0.063), (24, 0.007), (25, -0.046), (26, 0.033), (27, -0.041), (28, -0.029), (29, -0.073), (30, -0.054), (31, -0.007), (32, -0.013), (33, 0.009), (34, 0.033), (35, 0.027), (36, -0.028), (37, -0.051), (38, -0.034), (39, 0.001), (40, -0.029), (41, -0.095), (42, -0.06), (43, 0.061), (44, -0.02), (45, 0.011), (46, 0.065), (47, -0.024), (48, 0.024), (49, -0.005)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91840285 <a title="355-lsi-1" href="./iccv-2013-Robust_Face_Landmark_Estimation_under_Occlusion.html">355 iccv-2013-Robust Face Landmark Estimation under Occlusion</a></p>
<p>Author: Xavier P. Burgos-Artizzu, Pietro Perona, Piotr Dollár</p><p>Abstract: Human faces captured in real-world conditions present large variations in shape and occlusions due to differences in pose, expression, use of accessories such as sunglasses and hats and interactions with objects (e.g. food). Current face landmark estimation approaches struggle under such conditions since theyfail toprovide aprincipled way ofhandling outliers. We propose a novel method, called Robust Cascaded Pose Regression (RCPR) which reduces exposure to outliers by detecting occlusions explicitly and using robust shape-indexed features. We show that RCPR improves on previous landmark estimation methods on three popular face datasets (LFPW, LFW and HELEN). We further explore RCPR ’s performance by introducing a novel face dataset focused on occlusion, composed of 1,007 faces presenting a wide range of occlusion patterns. RCPR reduces failure cases by half on all four datasets, at the same time as it detects face occlusions with a 80/40% precision/recall.</p><p>2 0.87386417 <a title="355-lsi-2" href="./iccv-2013-Pose-Free_Facial_Landmark_Fitting_via_Optimized_Part_Mixtures_and_Cascaded_Deformable_Shape_Model.html">321 iccv-2013-Pose-Free Facial Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Shape Model</a></p>
<p>Author: Xiang Yu, Junzhou Huang, Shaoting Zhang, Wang Yan, Dimitris N. Metaxas</p><p>Abstract: This paper addresses the problem of facial landmark localization and tracking from a single camera. We present a two-stage cascaded deformable shape model to effectively and efficiently localize facial landmarks with large head pose variations. For face detection, we propose a group sparse learning method to automatically select the most salient facial landmarks. By introducing 3D face shape model, we use procrustes analysis to achieve pose-free facial landmark initialization. For deformation, the first step uses mean-shift local search with constrained local model to rapidly approach the global optimum. The second step uses component-wise active contours to discriminatively refine the subtle shape variation. Our framework can simultaneously handle face detection, pose-free landmark localization and tracking in real time. Extensive experiments are conducted on both laboratory environmental face databases and face-in-the-wild databases. All results demonstrate that our approach has certain advantages over state-of-theart methods in handling pose variations1.</p><p>3 0.86528629 <a title="355-lsi-3" href="./iccv-2013-Cascaded_Shape_Space_Pruning_for_Robust_Facial_Landmark_Detection.html">70 iccv-2013-Cascaded Shape Space Pruning for Robust Facial Landmark Detection</a></p>
<p>Author: Xiaowei Zhao, Shiguang Shan, Xiujuan Chai, Xilin Chen</p><p>Abstract: In this paper, we propose a novel cascaded face shape space pruning algorithm for robust facial landmark detection. Through progressively excluding the incorrect candidate shapes, our algorithm can accurately and efficiently achieve the globally optimal shape configuration. Specifically, individual landmark detectors are firstly applied to eliminate wrong candidates for each landmark. Then, the candidate shape space is further pruned by jointly removing incorrect shape configurations. To achieve this purpose, a discriminative structure classifier is designed to assess the candidate shape configurations. Based on the learned discriminative structure classifier, an efficient shape space pruning strategy is proposed to quickly reject most incorrect candidate shapes while preserve the true shape. The proposed algorithm is carefully evaluated on a large set of real world face images. In addition, comparison results on the publicly available BioID and LFW face databases demonstrate that our algorithm outperforms some state-of-the-art algorithms.</p><p>4 0.8264268 <a title="355-lsi-4" href="./iccv-2013-Exemplar-Based_Graph_Matching_for_Robust_Facial_Landmark_Localization.html">149 iccv-2013-Exemplar-Based Graph Matching for Robust Facial Landmark Localization</a></p>
<p>Author: Feng Zhou, Jonathan Brandt, Zhe Lin</p><p>Abstract: Localizing facial landmarks is a fundamental step in facial image analysis. However, the problem is still challenging due to the large variability in pose and appearance, and the existence ofocclusions in real-worldface images. In this paper, we present exemplar-based graph matching (EGM), a robust framework for facial landmark localization. Compared to conventional algorithms, EGM has three advantages: (1) an affine-invariant shape constraint is learned online from similar exemplars to better adapt to the test face; (2) the optimal landmark configuration can be directly obtained by solving a graph matching problem with the learned shape constraint; (3) the graph matching problem can be optimized efficiently by linear programming. To our best knowledge, this is the first attempt to apply a graph matching technique for facial landmark localization. Experiments on several challenging datasets demonstrate the advantages of EGM over state-of-the-art methods.</p><p>5 0.72079706 <a title="355-lsi-5" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>Author: Heng Yang, Ioannis Patras</p><p>Abstract: In this paper we propose a method for the localization of multiple facial features on challenging face images. In the regression forests (RF) framework, observations (patches) that are extracted at several image locations cast votes for the localization of several facial features. In order to filter out votes that are not relevant, we pass them through two types of sieves, that are organised in a cascade, and which enforce geometric constraints. The first sieve filters out votes that are not consistent with a hypothesis for the location of the face center. Several sieves of the second type, one associated with each individual facial point, filter out distant votes. We propose a method that adjusts onthe-fly the proximity threshold of each second type sieve by applying a classifier which, based on middle-level features extracted from voting maps for the facial feature in question, makes a sequence of decisions on whether the threshold should be reduced or not. We validate our proposed method on two challenging datasets with images collected from the Internet in which we obtain state of the art results without resorting to explicit facial shape models. We also show the benefits of our method for proximity threshold adjustment especially on ’difficult’ face images.</p><p>6 0.69050568 <a title="355-lsi-6" href="./iccv-2013-Rank_Minimization_across_Appearance_and_Shape_for_AAM_Ensemble_Fitting.html">339 iccv-2013-Rank Minimization across Appearance and Shape for AAM Ensemble Fitting</a></p>
<p>7 0.6871537 <a title="355-lsi-7" href="./iccv-2013-Like_Father%2C_Like_Son%3A_Facial_Expression_Dynamics_for_Kinship_Verification.html">251 iccv-2013-Like Father, Like Son: Facial Expression Dynamics for Kinship Verification</a></p>
<p>8 0.6536938 <a title="355-lsi-8" href="./iccv-2013-Optimization_Problems_for_Fast_AAM_Fitting_in-the-Wild.html">302 iccv-2013-Optimization Problems for Fast AAM Fitting in-the-Wild</a></p>
<p>9 0.64276934 <a title="355-lsi-9" href="./iccv-2013-Modifying_the_Memorability_of_Face_Photographs.html">272 iccv-2013-Modifying the Memorability of Face Photographs</a></p>
<p>10 0.58485538 <a title="355-lsi-10" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>11 0.55463785 <a title="355-lsi-11" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>12 0.529181 <a title="355-lsi-12" href="./iccv-2013-Accurate_and_Robust_3D_Facial_Capture_Using_a_Single_RGBD_Camera.html">36 iccv-2013-Accurate and Robust 3D Facial Capture Using a Single RGBD Camera</a></p>
<p>13 0.52789533 <a title="355-lsi-13" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>14 0.49706382 <a title="355-lsi-14" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>15 0.49439469 <a title="355-lsi-15" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>16 0.4309063 <a title="355-lsi-16" href="./iccv-2013-Hidden_Factor_Analysis_for_Age_Invariant_Face_Recognition.html">195 iccv-2013-Hidden Factor Analysis for Age Invariant Face Recognition</a></p>
<p>17 0.4205384 <a title="355-lsi-17" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>18 0.41807735 <a title="355-lsi-18" href="./iccv-2013-Face_Recognition_via_Archetype_Hull_Ranking.html">154 iccv-2013-Face Recognition via Archetype Hull Ranking</a></p>
<p>19 0.40872693 <a title="355-lsi-19" href="./iccv-2013-Modeling_Self-Occlusions_in_Dynamic_Shape_and_Appearance_Tracking.html">270 iccv-2013-Modeling Self-Occlusions in Dynamic Shape and Appearance Tracking</a></p>
<p>20 0.40667877 <a title="355-lsi-20" href="./iccv-2013-Hybrid_Deep_Learning_for_Face_Verification.html">206 iccv-2013-Hybrid Deep Learning for Face Verification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.038), (7, 0.017), (12, 0.01), (26, 0.078), (31, 0.031), (34, 0.017), (42, 0.166), (43, 0.196), (48, 0.017), (64, 0.079), (73, 0.021), (78, 0.015), (89, 0.171), (98, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83460253 <a title="355-lda-1" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>Author: Alon Faktor, Michal Irani</p><p>Abstract: Given a set of images which share an object from the same semantic category, we would like to co-segment the shared object. We define ‘good’ co-segments to be ones which can be easily composed (like a puzzle) from large pieces of other co-segments, yet are difficult to compose from remaining image parts. These pieces must not only match well but also be statistically significant (hard to compose at random). This gives rise to co-segmentation of objects in very challenging scenarios with large variations in appearance, shape and large amounts of clutter. We further show how multiple images can collaborate and “score each others ’ co-segments to improve the overall fidelity and accuracy of the co-segmentation. Our co-segmentation can be applied both to large image collections, as well as to very few images (where there is too little data for unsupervised learning). At the extreme, it can be applied even to a single image, to extract its co-occurring objects. Our approach obtains state-of-the-art results on benchmark datasets. We further show very encouraging co-segmentation results on the challenging PASCAL-VOC dataset. ”</p><p>same-paper 2 0.833202 <a title="355-lda-2" href="./iccv-2013-Robust_Face_Landmark_Estimation_under_Occlusion.html">355 iccv-2013-Robust Face Landmark Estimation under Occlusion</a></p>
<p>Author: Xavier P. Burgos-Artizzu, Pietro Perona, Piotr Dollár</p><p>Abstract: Human faces captured in real-world conditions present large variations in shape and occlusions due to differences in pose, expression, use of accessories such as sunglasses and hats and interactions with objects (e.g. food). Current face landmark estimation approaches struggle under such conditions since theyfail toprovide aprincipled way ofhandling outliers. We propose a novel method, called Robust Cascaded Pose Regression (RCPR) which reduces exposure to outliers by detecting occlusions explicitly and using robust shape-indexed features. We show that RCPR improves on previous landmark estimation methods on three popular face datasets (LFPW, LFW and HELEN). We further explore RCPR ’s performance by introducing a novel face dataset focused on occlusion, composed of 1,007 faces presenting a wide range of occlusion patterns. RCPR reduces failure cases by half on all four datasets, at the same time as it detects face occlusions with a 80/40% precision/recall.</p><p>3 0.81023145 <a title="355-lda-3" href="./iccv-2013-Interactive_Markerless_Articulated_Hand_Motion_Tracking_Using_RGB_and_Depth_Data.html">218 iccv-2013-Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth Data</a></p>
<p>Author: Srinath Sridhar, Antti Oulasvirta, Christian Theobalt</p><p>Abstract: Tracking the articulated 3D motion of the hand has important applications, for example, in human–computer interaction and teleoperation. We present a novel method that can capture a broad range of articulated hand motions at interactive rates. Our hybrid approach combines, in a voting scheme, a discriminative, part-based pose retrieval method with a generative pose estimation method based on local optimization. Color information from a multiview RGB camera setup along with a person-specific hand model are used by the generative method to find the pose that best explains the observed images. In parallel, our discriminative pose estimation method uses fingertips detected on depth data to estimate a complete or partial pose of the hand by adopting a part-based pose retrieval strategy. This part-based strategy helps reduce the search space drastically in comparison to a global pose retrieval strategy. Quantitative results show that our method achieves state-of-the-art accuracy on challenging sequences and a near-realtime performance of 10 fps on a desktop computer.</p><p>4 0.79074216 <a title="355-lda-4" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>Author: Claudia Nieuwenhuis, Evgeny Strekalovskiy, Daniel Cremers</p><p>Abstract: We propose a convex multilabel framework for image sequence segmentation which allows to impose proportion priors on object parts in order to preserve their size ratios across multiple images. The key idea is that for strongly deformable objects such as a gymnast the size ratio of respective regions (head versus torso, legs versus full body, etc.) is typically preserved. We propose different ways to impose such priors in a Bayesian framework for image segmentation. We show that near-optimal solutions can be computed using convex relaxation techniques. Extensive qualitative and quantitative evaluations demonstrate that the proportion priors allow for highly accurate segmentations, avoiding seeping-out of regions and preserving semantically relevant small-scale structures such as hands or feet. They naturally apply to multiple object instances such as players in sports scenes, and they can relate different objects instead of object parts, e.g. organs in medical imaging. The algorithm is efficient and easily parallelized leading to proportion-consistent segmentations at runtimes around one second.</p><p>5 0.78692925 <a title="355-lda-5" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>Author: Vidit Jain, Sachin Sudhakar Farfade</p><p>Abstract: Classification cascades have been very effective for object detection. Such a cascade fails to perform well in data domains with variations in appearances that may not be captured in the training examples. This limited generalization severely restricts the domains for which they can be used effectively. A common approach to address this limitation is to train a new cascade of classifiers from scratch for each of the new domains. Building separate detectors for each of the different domains requires huge annotation and computational effort, making it not scalable to a large number of data domains. Here we present an algorithm for quickly adapting a pre-trained cascade of classifiers using a small number oflabeledpositive instancesfrom a different yet similar data domain. In our experiments with images of human babies and human-like characters from movies, we demonstrate that the adapted cascade significantly outperforms both of the original cascade and the one trained from scratch using the given training examples. –</p><p>6 0.78583306 <a title="355-lda-6" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>7 0.78500372 <a title="355-lda-7" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>8 0.78479058 <a title="355-lda-8" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>9 0.78345394 <a title="355-lda-9" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>10 0.78291178 <a title="355-lda-10" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>11 0.78283626 <a title="355-lda-11" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>12 0.78252381 <a title="355-lda-12" href="./iccv-2013-Coupling_Alignments_with_Recognition_for_Still-to-Video_Face_Recognition.html">97 iccv-2013-Coupling Alignments with Recognition for Still-to-Video Face Recognition</a></p>
<p>13 0.78243381 <a title="355-lda-13" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>14 0.78204536 <a title="355-lda-14" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>15 0.78203189 <a title="355-lda-15" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>16 0.78123546 <a title="355-lda-16" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>17 0.78052586 <a title="355-lda-17" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>18 0.78041708 <a title="355-lda-18" href="./iccv-2013-Bayesian_Joint_Topic_Modelling_for_Weakly_Supervised_Object_Localisation.html">59 iccv-2013-Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation</a></p>
<p>19 0.78040141 <a title="355-lda-19" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>20 0.78033292 <a title="355-lda-20" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
