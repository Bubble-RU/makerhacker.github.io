<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>191 iccv-2013-Handling Uncertain Tags in Visual Recognition</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-191" href="#">iccv2013-191</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>191 iccv-2013-Handling Uncertain Tags in Visual Recognition</h1>
<br/><p>Source: <a title="iccv-2013-191-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Vahdat_Handling_Uncertain_Tags_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Arash Vahdat, Greg Mori</p><p>Abstract: Gathering accurate training data for recognizing a set of attributes or tags on images or videos is a challenge. Obtaining labels via manual effort or from weakly-supervised data typically results in noisy training labels. We develop the FlipSVM, a novel algorithm for handling these noisy, structured labels. The FlipSVM models label noise by “flipping ” labels on training examples. We show empirically that the FlipSVM is effective on images-and-attributes and video tagging datasets.</p><p>Reference: <a title="iccv-2013-191-reference" href="../iccv2013_reference/iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ca  Abstract Gathering accurate training data for recognizing a set of attributes or tags on images or videos is a challenge. [sent-3, score-0.778]
</p><p>2 Introduction We present a novel algorithm for predicting a set of tags or attributes that describe an image or video. [sent-9, score-0.649]
</p><p>3 However, there are two important intrinsic problems in tag-based recognition: (1) tag annotations are noisy and (2) tags can be very difficult to observe from visual features. [sent-68, score-1.253]
</p><p>4 There are two standard approaches to obtain tag annotations, either relying on human annotation or imputing them from weakly labeled data. [sent-70, score-0.689]
</p><p>5 Human annotation is subjective and many tags correspond to a very fine level ofdetail on a given image or video. [sent-72, score-0.601]
</p><p>6 A typical solution to this problem is to ask several annotators to tag content followed by a tag aggregation approach. [sent-74, score-1.187]
</p><p>7 Weakly labeled data are often used to obtain tag annotations. [sent-78, score-0.58]
</p><p>8 These data  sources are often of poor quality, with many users entering a small subset of tags or spam tags for a given image/video. [sent-80, score-1.14]
</p><p>9 Beyond this, many tags are very difficult to discern visually. [sent-81, score-0.553]
</p><p>10 Different tags have different degrees of difficulty and often there is a need to rely on contextual information or high-level reasoning. [sent-84, score-0.553]
</p><p>11 This is an essential issue to allow the recovery of detailed image or video descriptions from the typically ambiguous and noisy tag training data available. [sent-86, score-0.751]
</p><p>12 We present a novel structured tag prediction learning algorithm that considers the uncertainty of tags and their inter-relations. [sent-87, score-1.165]
</p><p>13 Second, we automatically extract tag annotations for the TRECVID MED1 1 video dataset based on noisy, weakly supervised video description data. [sent-90, score-0.798]
</p><p>14 We apply our algorithm to learn tags from these data, and show they improve performance for classifying complex events. [sent-91, score-0.553]
</p><p>15 Related Work A substantial body of recent work considers the problem  of labeling images with a set of tags or attributes. [sent-93, score-0.601]
</p><p>16 [ 17] use structural SVM to predict multiple tags while considering their correlation. [sent-105, score-0.634]
</p><p>17 Weakly labeled data are often used to extract tag or attribute data for images or videos. [sent-106, score-0.656]
</p><p>18 [12] have proposed to use Multiple Instance Boosting [23] to train video tag classifiers using noisy web videos. [sent-111, score-0.758]
</p><p>19 However, they learn a classifier for each attribute independently and ignore the fact that tags are highly correlated. [sent-113, score-0.627]
</p><p>20 A wide range of machine learning tools has been used for training tag classifiers. [sent-115, score-0.631]
</p><p>21 Latent SVM [6] is used for object and video classification respectively by Wang and Mori [25] and Izadinia and Shah [8] in which tags are modeled as latent variables. [sent-117, score-0.682]
</p><p>22 For this purpose, we are interested in considering tags which represent different aspects of classes. [sent-126, score-0.581]
</p><p>23 Ultimately, recognition of these tags will provide us with more description of visual content. [sent-127, score-0.58]
</p><p>24 In order to train  a classifier, we are going to use a training data set of images and videos that are annotated with their class and tag labels. [sent-128, score-0.776]
</p><p>25 In this section, our approach for training the classifier from noisy tag labels is presented. [sent-129, score-0.794]
</p><p>26 Here, tags may correspond to attributes of objects or tags/key-words of videos. [sent-131, score-0.649]
</p><p>27 For tag-based classification, we will use a structured model similar to Wang and Mori [25] that considers the interaction between tags and class labels. [sent-132, score-0.664]
</p><p>28 Interaction between tags helps us to recognize tags that are correlated. [sent-133, score-1.139]
</p><p>29 Similarly, classification can be enhanced considering the interaction between tags and class labels. [sent-136, score-0.689]
</p><p>30 x represents animage or avideo, t = {ti}ii==T1 rep773388 resents the presence/absence of T tags with binary labels, ti ∈ {−1, 1}. [sent-139, score-0.622]
</p><p>31 If the tag is present in the image x, we will hav∈e ti =1, 11,} o. [sent-140, score-0.626]
</p><p>32 Similar to Latent SVM framework [6, 24] or Structural SVM [2 1], it is assumed that F(x, y) = maxt wTΦ(x, t,y) where wTΦ(x, t,y) is a linear potential function that scores a configuration of tag labels and a class label for a sample considering their interaction. [sent-146, score-0.762]
</p><p>33 The tag dependency is modeled using a a treestructured graph called tag interactions graph. [sent-147, score-1.16]
</p><p>34 Next, our proposed learning framework for training parameters of a structured model from noisy tags is presented. [sent-148, score-0.722]
</p><p>35 The idea of our approach is that the learning algorithm is allowed to change training tag labels while penalizing the number of changes. [sent-157, score-0.682]
</p><p>36 This way the algorithm may be able to correct some tag label mistakes but it is limited not to change all the labels arbitrarily. [sent-158, score-0.695]
</p><p>37 We call this training framework FlipSVM (FSVM) as it flips some of the labels in the course of training. [sent-159, score-0.224]
</p><p>38 nn − ξn  (1) ∀t,∀y  that minimizes the norm of parameters (||w| |22), structured error ξinn amnidz tsh teh tag lrambe ol change ctoersst (ξ| ? [sent-172, score-0.663]
</p><p>39 Therefore, the goal of training is to find a w that produces a score for refined tag labels t? [sent-177, score-0.682]
</p><p>40 n and ground truth class label yn greater than any other hypothesized labeling with a margin re-scaled with the loss function Δyt,,ty? [sent-178, score-0.223]
</p><p>41 By simplifying the model and ignoring pairwise terms (dashed lines) in most violated labeling inference, maximization over t decomposes to maximization over each ti that depends on individual t? [sent-190, score-0.23]
</p><p>42 The inner maximization finds the most violated labeling and the outer minimization refines the ground truth label such that sum of the most violated labeling margin and the flip cost is minimum. [sent-212, score-0.396]
</p><p>43 2, where wTφ(x, t,y) is defined on a tree structured tag interaction graph, this optimization problem is an NP-hard problem. [sent-214, score-0.652]
</p><p>44 As the number of tags is typically on the order of tens, and due to the structure of the tag interaction graph, the inference problem can be solved very efficiently. [sent-228, score-1.173]
</p><p>45 SSVM trains the parameters of model such that both the class and tag labels of the training data can be predicted accurately. [sent-240, score-0.721]
</p><p>46 In the presence of noisy tags, SSVM does not model the label noise and it may fit to noisy tag labels. [sent-241, score-0.865]
</p><p>47 In contrast, noisy tag labels can be modeled as latent variables using a Latent SVM model. [sent-242, score-0.783]
</p><p>48 In [25, 8], tag information is injected into training by replacing the image features with the score of pre-trained tag classifiers. [sent-244, score-1.211]
</p><p>49 Our approach is different from the Latent SVM approach, as it enables us to train the tag and label classifiers in an unified approach. [sent-245, score-0.702]
</p><p>50 Model In order to be able to compare our training algorithm with previous works on tag-based classification, we use a model similar to Wang and Mori [25] for scoring the configuration of tag labels and class label. [sent-248, score-0.768]
</p><p>51 Their work has been later adopted by Izadinia and Shah [8] to recognize a set of complex events in videos by considering noisy low-level events. [sent-249, score-0.259]
</p><p>52 In our model, the dependency of the tags is represented using an undirected tag interaction graph G = (V, E) with tuhsein vge artnex u snedti rVe =te d{1 ta, 2, 3, . [sent-251, score-1.173]
</p><p>53 jO)u ∈r scoring aftuensc tthioen i measures tdheen compatibility of a class label y and tag labels t in an example x by: wTφ(x,t,y)  = yθTφ(x) +? [sent-258, score-0.814]
</p><p>54 eTrhee w scoring αfunction h}as four parts: The potential terms yθTφ(x) and tiαiTφ(x) measure the compatibility of a global feature φ(x) extracted from example x with class or tag labels. [sent-265, score-0.699]
</p><p>55 βiTjϕ(ti , tj) measures the compatibility between a pair of tags that is connected in our tag interaction graph E. [sent-267, score-1.206]
</p><p>56 Similarly, {νiT(0ϕ,(1t)i,, (y1), measures t(1he, compatibility ybe (ttween a tag and a class label. [sent-269, score-0.652]
</p><p>57 In reality, only a small subset of tag pairs may show a high amount of correlation. [sent-270, score-0.58]
</p><p>58 Therefore, considering all pairs of tags will not be necessary, and would slow down infer-  ence significantly. [sent-271, score-0.581]
</p><p>59 By employing this approach, inference of tags is done quickly using dynamic programming. [sent-274, score-0.553]
</p><p>60 Note that, even in our case that tags are noisy, this approach considers statistics that are robust against noise when we are extracting tag dependencies. [sent-275, score-1.182]
</p><p>61 In our case training tags are noisy, so pre-trained binary classifiers will not be immune to noise. [sent-277, score-0.657]
</p><p>62 Wang and Mori [25] have also an extra 774400  class specific tag model. [sent-279, score-0.619]
</p><p>63 We automatically extract a set of video tags by processing the text description files provided in TRECVID MED1 1 [14] videos. [sent-285, score-0.676]
</p><p>64 In this section, we utilize object attributes as tags for the object classification task. [sent-306, score-0.678]
</p><p>65 Modeling Noisy Labels: In this experiment we measure FlipSVM under the original (often noisy) tag labels, and examine the performance of our proposed approach under different amounts of additional noise. [sent-307, score-0.58]
</p><p>66 We run experiments using the original a-Pascal and a-Yahoo tag annotations, and further introduce different levels of synthetic noise to the ground truth tag labels. [sent-308, score-1.209]
</p><p>67 [12] which generates noise for positive tags by changing the label of some samples with negative tag to positive. [sent-310, score-1.246]
</p><p>68 In X% noise, X% of the training samples with each positive tag are mislabeled samples added from the negative set. [sent-314, score-0.631]
</p><p>69 er Ffo r m e adch ba tasged o orn cl tahses, n ruem-scbaelrin ogf  training samples that have that tag or class. [sent-326, score-0.631]
</p><p>70 nii)  is the number of training  images  whose ith tag is t? [sent-341, score-0.631]
</p><p>71 ss Sesin rcaeth were athrean i tag ecsltaesdse isn, we set C = 25 to have a loss function more sensitive to class than tag error. [sent-344, score-1.245]
</p><p>72 We here rescale the hamming loss such that tags with a large number of positive examples are penalized less. [sent-348, score-0.599]
</p><p>73 This way, we encourage our training algorithm to change frequent tags more. [sent-349, score-0.604]
</p><p>74 We also prevent label flips from a negative tag to a positive tag. [sent-350, score-0.766]
</p><p>75 n ni === 1 t−n,1tin,tin=i= −1 1 774411  Note that the loss functions as well as the tag interaction graph are formed from noisy training tags for each experiment and noise level separately. [sent-360, score-1.445]
</p><p>76 (2) Latent SVM that uses our model equipped with a classspecific tag model discussed in [25]. [sent-362, score-0.58]
</p><p>77 (3) SVM classifier trained on global features ignoring all the information about tag annotation. [sent-363, score-0.606]
</p><p>78 Label Flip Results: We further measure the quality of the label flips that our algorithm produces on the training data, as we have access to the ground truth tag labels before and after adding synthetic noise. [sent-370, score-0.868]
</p><p>79 A true positive label flip is defined as a change in label that changes a noisy tag label back to the original ground truth label. [sent-372, score-0.933]
</p><p>80 The DEV-T dataset consists of 10,723 videos including videos from five event categories: board trick (E1), feeding animal (E2), landing fish (E3), wedding ceremony (E4), and woodworking project (E5). [sent-385, score-0.548]
</p><p>81 Our training algorithm shows robustness against noise in tag annotations. [sent-389, score-0.68]
</p><p>82 Instead of manual labeling, we employ a simple technique to extract tags on this dataset. [sent-411, score-0.581]
</p><p>83 The advantage of our approach is that it is very fast, and it can be used to extract noisy tags on the whole dataset with no manual interaction. [sent-412, score-0.667]
</p><p>84 Moreover, the nature of the tags is similar to those tags that can be extracted by processing user-provided descriptions from social websites in the sense that a subset of present tags are extracted by our method (e. [sent-413, score-1.659]
</p><p>85 [9] show that 50% of annotated tags are actually in the Flickr images). [sent-416, score-0.553]
</p><p>86 This is very limited information as it does not provide an exhaustive tag set denoting the absence of all other tags, but it can still be used to train a tag classifier for the present objects or actions. [sent-421, score-1.214]
</p><p>87 We use a simple approach to extract tags focusing on the objects in the videos. [sent-422, score-0.581]
</p><p>88 This process results in 75 tags on the training dataset. [sent-427, score-0.604]
</p><p>89 Randomly selected examples of the tags are: dance, soccer, lake, river, road, snowboard, girl, street, kitchen, boat, rally, egg, car, etc. [sent-428, score-0.553]
</p><p>90 We also added 5 genres and 20 topic tags provided in judgment files to create a 100-tag set for the TRECVID MED1 1dataset1 . [sent-429, score-0.627]
</p><p>91 Unfortunately, direct comparison to this work is not possible as we do not have access to their features and tag annotation. [sent-434, score-0.58]
</p><p>92 So, the extracted tags are actually present in the video. [sent-445, score-0.553]
</p><p>93 However, sentences may not have all the tags that are in a video. [sent-446, score-0.586]
</p><p>94 Therefore, the label flip loss function is modified to prevent label flips from a positive tag to a negative tag. [sent-447, score-0.951]
</p><p>95 V21895M740 datasets with multiple output tags or attributes which is becoming a common paradigm. [sent-467, score-0.649]
</p><p>96 We showed that the FlipSVM model for la–  bel noise can improve performance at image recognition in the presence of noisy attribute data and video classification from weakly supervised tag sets. [sent-470, score-0.887]
</p><p>97 Here at most 10 tags that have score greater than a threshold are reported. [sent-520, score-0.553]
</p><p>98 Our algorithm not only learns the tags that are associated with each category, but also it discriminates some less common ones such as “park” and “dancing” in wedding ceremony, “feed” in landing fish, or “talk” in repairing appliance. [sent-521, score-0.751]
</p><p>99 Note how the detection of “river”, “lake” and “water” misleads our algorithm to classify two background videos as landing fish category. [sent-522, score-0.257]
</p><p>100 Discriminative tag learning on youtube videos with latent sub-tags. [sent-672, score-0.724]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tag', 0.58), ('tags', 0.553), ('flipsvm', 0.223), ('flips', 0.122), ('fish', 0.102), ('attributes', 0.096), ('noisy', 0.086), ('ceremony', 0.085), ('trecvid', 0.084), ('videos', 0.078), ('landing', 0.077), ('risk', 0.076), ('flip', 0.075), ('mori', 0.07), ('wedding', 0.066), ('latent', 0.066), ('label', 0.064), ('appliance', 0.063), ('river', 0.063), ('event', 0.062), ('lady', 0.061), ('ssvm', 0.061), ('lake', 0.061), ('weakly', 0.061), ('svm', 0.06), ('wt', 0.059), ('repairing', 0.055), ('tn', 0.053), ('structural', 0.053), ('rampsvm', 0.051), ('nn', 0.051), ('labels', 0.051), ('training', 0.051), ('noise', 0.049), ('attribute', 0.048), ('annotation', 0.048), ('labeling', 0.048), ('maximization', 0.047), ('scoring', 0.047), ('ti', 0.046), ('loss', 0.046), ('fsvm', 0.046), ('izadinia', 0.045), ('kit', 0.045), ('leung', 0.044), ('talk', 0.044), ('amateur', 0.042), ('violated', 0.042), ('ni', 0.04), ('judgment', 0.04), ('interaction', 0.04), ('class', 0.039), ('boy', 0.038), ('tagging', 0.036), ('indoors', 0.035), ('water', 0.035), ('video', 0.034), ('annotations', 0.034), ('arti', 0.034), ('fishing', 0.034), ('nrbm', 0.034), ('spam', 0.034), ('topia', 0.034), ('files', 0.034), ('footage', 0.034), ('nouns', 0.034), ('events', 0.034), ('compatibility', 0.033), ('sentences', 0.033), ('recognize', 0.033), ('structured', 0.032), ('boat', 0.032), ('birthday', 0.031), ('farhadi', 0.03), ('ramp', 0.03), ('leg', 0.03), ('inner', 0.03), ('classifiers', 0.03), ('man', 0.029), ('classification', 0.029), ('shah', 0.029), ('ap', 0.029), ('train', 0.028), ('devo', 0.028), ('extract', 0.028), ('considering', 0.028), ('description', 0.027), ('annotators', 0.027), ('drive', 0.027), ('classifier', 0.026), ('yn', 0.026), ('classes', 0.026), ('lsvm', 0.025), ('berg', 0.025), ('kennedy', 0.024), ('russakovsky', 0.024), ('xn', 0.024), ('tj', 0.024), ('shiny', 0.023), ('binary', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="191-tfidf-1" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>Author: Arash Vahdat, Greg Mori</p><p>Abstract: Gathering accurate training data for recognizing a set of attributes or tags on images or videos is a challenge. Obtaining labels via manual effort or from weakly-supervised data typically results in noisy training labels. We develop the FlipSVM, a novel algorithm for handling these noisy, structured labels. The FlipSVM models label noise by “flipping ” labels on training examples. We show empirically that the FlipSVM is effective on images-and-attributes and video tagging datasets.</p><p>2 0.15980254 <a title="191-tfidf-2" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>Author: Arash Vahdat, Kevin Cannons, Greg Mori, Sangmin Oh, Ilseo Kim</p><p>Abstract: We present a compositional model for video event detection. A video is modeled using a collection of both global and segment-level features and kernel functions are employed for similarity comparisons. The locations of salient, discriminative video segments are treated as a latent variable, allowing the model to explicitly ignore portions of the video that are unimportant for classification. A novel, multiple kernel learning (MKL) latent support vector machine (SVM) is defined, that is used to combine and re-weight multiple feature types in a principled fashion while simultaneously operating within the latent variable framework. The compositional nature of the proposed model allows it to respond directly to the challenges of temporal clutter and intra-class variation, which are prevalent in unconstrained internet videos. Experimental results on the TRECVID Multimedia Event Detection 2011 (MED11) dataset demonstrate the efficacy of the method.</p><p>3 0.14756781 <a title="191-tfidf-3" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<p>Author: Xiatian Zhu, Chen Change Loy, Shaogang Gong</p><p>Abstract: Generating coherent synopsis for surveillance video stream remains a formidable challenge due to the ambiguity and uncertainty inherent to visual observations. In contrast to existing video synopsis approaches that rely on visual cues alone, we propose a novel multi-source synopsis framework capable of correlating visual data and independent non-visual auxiliary information to better describe and summarise subtlephysical events in complex scenes. Specifically, our unsupervised framework is capable of seamlessly uncovering latent correlations among heterogeneous types of data sources, despite the non-trivial heteroscedasticity and dimensionality discrepancy problems. Additionally, the proposed model is robust to partial or missing non-visual information. We demonstrate the effectiveness of our framework on two crowded public surveillance datasets.</p><p>4 0.12380076 <a title="191-tfidf-4" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>Author: Zheyun Feng, Rong Jin, Anil Jain</p><p>Abstract: One of the key challenges in search-based image annotation models is to define an appropriate similarity measure between images. Many kernel distance metric learning (KML) algorithms have been developed in order to capture the nonlinear relationships between visual features and semantics ofthe images. Onefundamental limitation in applying KML to image annotation is that it requires converting image annotations into binary constraints, leading to a significant information loss. In addition, most KML algorithms suffer from high computational cost due to the requirement that the learned matrix has to be positive semi-definitive (PSD). In this paper, we propose a robust kernel metric learning (RKML) algorithm based on the regression technique that is able to directly utilize image annotations. The proposed method is also computationally more efficient because PSD property is automatically ensured by regression. We provide the theoretical guarantee for the proposed algorithm, and verify its efficiency and effectiveness for image annotation by comparing it to state-of-the-art approaches for both distance metric learning and image annotation. ,</p><p>5 0.11665285 <a title="191-tfidf-5" href="./iccv-2013-Paper_Doll_Parsing%3A_Retrieving_Similar_Styles_to_Parse_Clothing_Items.html">306 iccv-2013-Paper Doll Parsing: Retrieving Similar Styles to Parse Clothing Items</a></p>
<p>Author: Kota Yamaguchi, M. Hadi Kiapour, Tamara L. Berg</p><p>Abstract: Clothing recognition is an extremely challenging problem due to wide variation in clothing item appearance, layering, and style. In this paper, we tackle the clothing parsing problem using a retrieval based approach. For a query image, we find similar styles from a large database of tagged fashion images and use these examples to parse the query. Our approach combines parsing from: pre-trained global clothing models, local clothing models learned on theflyfrom retrieved examples, and transferredparse masks (paper doll item transfer) from retrieved examples. Experimental evaluation shows that our approach significantly outperforms state of the art in parsing accuracy.</p><p>6 0.11246041 <a title="191-tfidf-6" href="./iccv-2013-Event_Recognition_in_Photo_Collections_with_a_Stopwatch_HMM.html">147 iccv-2013-Event Recognition in Photo Collections with a Stopwatch HMM</a></p>
<p>7 0.1028211 <a title="191-tfidf-7" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>8 0.1013464 <a title="191-tfidf-8" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>9 0.09874098 <a title="191-tfidf-9" href="./iccv-2013-Feature_Weighting_via_Optimal_Thresholding_for_Video_Analysis.html">163 iccv-2013-Feature Weighting via Optimal Thresholding for Video Analysis</a></p>
<p>10 0.096371129 <a title="191-tfidf-10" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>11 0.09547089 <a title="191-tfidf-11" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>12 0.095150642 <a title="191-tfidf-12" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>13 0.092936337 <a title="191-tfidf-13" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>14 0.089127257 <a title="191-tfidf-14" href="./iccv-2013-How_Related_Exemplars_Help_Complex_Event_Detection_in_Web_Videos%3F.html">203 iccv-2013-How Related Exemplars Help Complex Event Detection in Web Videos?</a></p>
<p>15 0.08619047 <a title="191-tfidf-15" href="./iccv-2013-ACTIVE%3A_Activity_Concept_Transitions_in_Video_Event_Classification.html">4 iccv-2013-ACTIVE: Activity Concept Transitions in Video Event Classification</a></p>
<p>16 0.082990631 <a title="191-tfidf-16" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>17 0.081501134 <a title="191-tfidf-17" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>18 0.079039507 <a title="191-tfidf-18" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>19 0.076227263 <a title="191-tfidf-19" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>20 0.075448804 <a title="191-tfidf-20" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.165), (1, 0.139), (2, -0.011), (3, -0.024), (4, 0.091), (5, 0.038), (6, -0.003), (7, -0.062), (8, 0.047), (9, -0.034), (10, -0.058), (11, -0.072), (12, -0.037), (13, 0.053), (14, -0.075), (15, -0.03), (16, -0.015), (17, 0.017), (18, -0.016), (19, 0.028), (20, -0.011), (21, 0.001), (22, -0.009), (23, 0.059), (24, 0.052), (25, -0.0), (26, 0.07), (27, 0.006), (28, 0.033), (29, 0.004), (30, 0.022), (31, 0.012), (32, -0.007), (33, -0.034), (34, -0.014), (35, -0.066), (36, -0.015), (37, 0.057), (38, -0.053), (39, -0.039), (40, -0.03), (41, -0.018), (42, -0.014), (43, -0.032), (44, 0.093), (45, 0.06), (46, 0.087), (47, -0.02), (48, -0.045), (49, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92186004 <a title="191-lsi-1" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>Author: Arash Vahdat, Greg Mori</p><p>Abstract: Gathering accurate training data for recognizing a set of attributes or tags on images or videos is a challenge. Obtaining labels via manual effort or from weakly-supervised data typically results in noisy training labels. We develop the FlipSVM, a novel algorithm for handling these noisy, structured labels. The FlipSVM models label noise by “flipping ” labels on training examples. We show empirically that the FlipSVM is effective on images-and-attributes and video tagging datasets.</p><p>2 0.7027635 <a title="191-lsi-2" href="./iccv-2013-Event_Recognition_in_Photo_Collections_with_a_Stopwatch_HMM.html">147 iccv-2013-Event Recognition in Photo Collections with a Stopwatch HMM</a></p>
<p>Author: Lukas Bossard, Matthieu Guillaumin, Luc Van_Gool</p><p>Abstract: The task of recognizing events in photo collections is central for automatically organizing images. It is also very challenging, because of the ambiguity of photos across different event classes and because many photos do not convey enough relevant information. Unfortunately, the field still lacks standard evaluation data sets to allow comparison of different approaches. In this paper, we introduce and release a novel data set of personal photo collections containing more than 61,000 images in 807 collections, annotated with 14 diverse social event classes. Casting collections as sequential data, we build upon recent and state-of-the-art work in event recognition in videos to propose a latent sub-event approach for event recognition in photo collections. However, photos in collections are sparsely sampled over time and come in bursts from which transpires the importance of specific moments for the photographers. Thus, we adapt a discriminative hidden Markov model to allow the transitions between states to be a function of the time gap between consecutive images, which we coin as Stopwatch Hidden Markov model (SHMM). In our experiments, we show that our proposed model outperforms approaches based only on feature pooling or a classical hidden Markov model. With an average accuracy of 56%, we also highlight the difficulty of the data set and the need for future advances in event recognition in photo collections.</p><p>3 0.66438377 <a title="191-lsi-3" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>Author: Arash Vahdat, Kevin Cannons, Greg Mori, Sangmin Oh, Ilseo Kim</p><p>Abstract: We present a compositional model for video event detection. A video is modeled using a collection of both global and segment-level features and kernel functions are employed for similarity comparisons. The locations of salient, discriminative video segments are treated as a latent variable, allowing the model to explicitly ignore portions of the video that are unimportant for classification. A novel, multiple kernel learning (MKL) latent support vector machine (SVM) is defined, that is used to combine and re-weight multiple feature types in a principled fashion while simultaneously operating within the latent variable framework. The compositional nature of the proposed model allows it to respond directly to the challenges of temporal clutter and intra-class variation, which are prevalent in unconstrained internet videos. Experimental results on the TRECVID Multimedia Event Detection 2011 (MED11) dataset demonstrate the efficacy of the method.</p><p>4 0.66262144 <a title="191-lsi-4" href="./iccv-2013-Learning_to_Rank_Using_Privileged_Information.html">248 iccv-2013-Learning to Rank Using Privileged Information</a></p>
<p>Author: Viktoriia Sharmanska, Novi Quadrianto, Christoph H. Lampert</p><p>Abstract: Many computer visionproblems have an asymmetric distribution of information between training and test time. In this work, we study the case where we are given additional information about the training data, which however will not be available at test time. This situation is called learning using privileged information (LUPI). We introduce two maximum-margin techniques that are able to make use of this additional source of information, and we show that the framework is applicable to several scenarios that have been studied in computer vision before. Experiments with attributes, bounding boxes, image tags and rationales as additional information in object classification show promising results.</p><p>5 0.65597749 <a title="191-lsi-5" href="./iccv-2013-ACTIVE%3A_Activity_Concept_Transitions_in_Video_Event_Classification.html">4 iccv-2013-ACTIVE: Activity Concept Transitions in Video Event Classification</a></p>
<p>Author: Chen Sun, Ram Nevatia</p><p>Abstract: The goal of high level event classification from videos is to assign a single, high level event label to each query video. Traditional approaches represent each video as a set of low level features and encode it into a fixed length feature vector (e.g. Bag-of-Words), which leave a big gap between low level visual features and high level events. Our paper tries to address this problem by exploiting activity concept transitions in video events (ACTIVE). A video is treated as a sequence of short clips, all of which are observations corresponding to latent activity concept variables in a Hidden Markov Model (HMM). We propose to apply Fisher Kernel techniques so that the concept transitions over time can be encoded into a compact and fixed length feature vector very efficiently. Our approach can utilize concept annotations from independent datasets, and works well even with a very small number of training samples. Experiments on the challenging NIST TRECVID Multimedia Event Detection (MED) dataset shows our approach performs favorably over the state-of-the-art.</p><p>6 0.64195508 <a title="191-lsi-6" href="./iccv-2013-Feature_Weighting_via_Optimal_Thresholding_for_Video_Analysis.html">163 iccv-2013-Feature Weighting via Optimal Thresholding for Video Analysis</a></p>
<p>7 0.63340366 <a title="191-lsi-7" href="./iccv-2013-From_Large_Scale_Image_Categorization_to_Entry-Level_Categories.html">176 iccv-2013-From Large Scale Image Categorization to Entry-Level Categories</a></p>
<p>8 0.63251841 <a title="191-lsi-8" href="./iccv-2013-YouTube2Text%3A_Recognizing_and_Describing_Arbitrary_Activities_Using_Semantic_Hierarchies_and_Zero-Shot_Recognition.html">452 iccv-2013-YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition</a></p>
<p>9 0.63018763 <a title="191-lsi-9" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<p>10 0.61702275 <a title="191-lsi-10" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>11 0.61256975 <a title="191-lsi-11" href="./iccv-2013-How_Related_Exemplars_Help_Complex_Event_Detection_in_Web_Videos%3F.html">203 iccv-2013-How Related Exemplars Help Complex Event Detection in Web Videos?</a></p>
<p>12 0.61002141 <a title="191-lsi-12" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>13 0.60741127 <a title="191-lsi-13" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>14 0.56094462 <a title="191-lsi-14" href="./iccv-2013-NEIL%3A_Extracting_Visual_Knowledge_from_Web_Data.html">285 iccv-2013-NEIL: Extracting Visual Knowledge from Web Data</a></p>
<p>15 0.56025368 <a title="191-lsi-15" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>16 0.55986291 <a title="191-lsi-16" href="./iccv-2013-Learning_CRFs_for_Image_Parsing_with_Adaptive_Subgradient_Descent.html">234 iccv-2013-Learning CRFs for Image Parsing with Adaptive Subgradient Descent</a></p>
<p>17 0.54593885 <a title="191-lsi-17" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>18 0.54312527 <a title="191-lsi-18" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>19 0.54208028 <a title="191-lsi-19" href="./iccv-2013-Translating_Video_Content_to_Natural_Language_Descriptions.html">428 iccv-2013-Translating Video Content to Natural Language Descriptions</a></p>
<p>20 0.53659493 <a title="191-lsi-20" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.39), (4, 0.012), (8, 0.013), (12, 0.017), (26, 0.064), (31, 0.022), (35, 0.012), (42, 0.084), (64, 0.035), (73, 0.019), (77, 0.045), (78, 0.013), (89, 0.153), (98, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95888621 <a title="191-lda-1" href="./iccv-2013-A_General_Two-Step_Approach_to_Learning-Based_Hashing.html">13 iccv-2013-A General Two-Step Approach to Learning-Based Hashing</a></p>
<p>Author: Guosheng Lin, Chunhua Shen, David Suter, Anton van_den_Hengel</p><p>Abstract: Most existing approaches to hashing apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to respond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. This framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problemspecific hashing methods. Our framework decomposes the hashing learning problem into two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically be formulated as binary quadratic problems, and the second step can be accomplished by training standard binary classifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate that the proposed framework is effective, flexible and outperforms the state-of-the-art.</p><p>2 0.95235825 <a title="191-lda-2" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>Author: Jayaguru Panda, Michael S. Brown, C.V. Jawahar</p><p>Abstract: Existing mobile image instance retrieval applications assume a network-based usage where image features are sent to a server to query an online visual database. In this scenario, there are no restrictions on the size of the visual database. This paper, however, examines how to perform this same task offline, where the entire visual index must reside on the mobile device itself within a small memory footprint. Such solutions have applications on location recognition and product recognition. Mobile instance retrieval requires a significant reduction in the visual index size. To achieve this, we describe a set of strategies that can reduce the visual index up to 60-80 compared to a scatannd raerddu iens tthaen vceis rueatrli ienvdaelx xim upple tom 6en0t-8at0io ×n found on ddte osk atops or servers. While our proposed reduction steps affect the overall mean Average Precision (mAP), they are able to maintain a good Precision for the top K results (PK). We argue that for such offline application, maintaining a good PK is sufficient. The effectiveness of this approach is demonstrated on several standard databases. A working application designed for a remote historical site is also presented. This application is able to reduce an 50,000 image index structure to 25 MBs while providing a precision of 97% for P10 and 100% for P1.</p><p>3 0.92822927 <a title="191-lda-3" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<p>Author: Shi Qiu, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: This paper proposes modeling the complex web image collections with an automatically generated graph structure called visual semantic complex network (VSCN). The nodes on this complex network are clusters of images with both visual and semantic consistency, called semantic concepts. These nodes are connected based on the visual and semantic correlations. Our VSCN with 33, 240 concepts is generated from a collection of 10 million web images. 1 A great deal of valuable information on the structures of the web image collections can be revealed by exploring the VSCN, such as the small-world behavior, concept community, indegree distribution, hubs, and isolated concepts. It not only helps us better understand the web image collections at a macroscopic level, but also has many important practical applications. This paper presents two application examples: content-based image retrieval and image browsing. Experimental results show that the VSCN leads to significant improvement on both the precision of image retrieval (over 200%) and user experience for image browsing.</p><p>4 0.92043841 <a title="191-lda-4" href="./iccv-2013-Revisiting_Example_Dependent_Cost-Sensitive_Learning_with_Decision_Trees.html">352 iccv-2013-Revisiting Example Dependent Cost-Sensitive Learning with Decision Trees</a></p>
<p>Author: Oisin Mac Aodha, Gabriel J. Brostow</p><p>Abstract: Typical approaches to classification treat class labels as disjoint. For each training example, it is assumed that there is only one class label that correctly describes it, and that all other labels are equally bad. We know however, that good and bad labels are too simplistic in many scenarios, hurting accuracy. In the realm of example dependent costsensitive learning, each label is instead a vector representing a data point’s affinity for each of the classes. At test time, our goal is not to minimize the misclassification rate, but to maximize that affinity. We propose a novel example dependent cost-sensitive impurity measure for decision trees. Our experiments show that this new impurity measure improves test performance while still retaining the fast test times of standard classification trees. We compare our approach to classification trees and other cost-sensitive methods on three computer vision problems, tracking, descriptor matching, and optical flow, and show improvements in all three domains.</p><p>same-paper 5 0.91849101 <a title="191-lda-5" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>Author: Arash Vahdat, Greg Mori</p><p>Abstract: Gathering accurate training data for recognizing a set of attributes or tags on images or videos is a challenge. Obtaining labels via manual effort or from weakly-supervised data typically results in noisy training labels. We develop the FlipSVM, a novel algorithm for handling these noisy, structured labels. The FlipSVM models label noise by “flipping ” labels on training examples. We show empirically that the FlipSVM is effective on images-and-attributes and video tagging datasets.</p><p>6 0.90130746 <a title="191-lda-6" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>7 0.89858377 <a title="191-lda-7" href="./iccv-2013-Improving_Graph_Matching_via_Density_Maximization.html">214 iccv-2013-Improving Graph Matching via Density Maximization</a></p>
<p>8 0.87628102 <a title="191-lda-8" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>9 0.83071166 <a title="191-lda-9" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>10 0.81780022 <a title="191-lda-10" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>11 0.81443644 <a title="191-lda-11" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>12 0.80385464 <a title="191-lda-12" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>13 0.79450184 <a title="191-lda-13" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>14 0.78505325 <a title="191-lda-14" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>15 0.75624734 <a title="191-lda-15" href="./iccv-2013-Learning_to_Rank_Using_Privileged_Information.html">248 iccv-2013-Learning to Rank Using Privileged Information</a></p>
<p>16 0.75430334 <a title="191-lda-16" href="./iccv-2013-Person_Re-identification_by_Salience_Matching.html">313 iccv-2013-Person Re-identification by Salience Matching</a></p>
<p>17 0.74669081 <a title="191-lda-17" href="./iccv-2013-SYM-FISH%3A_A_Symmetry-Aware_Flip_Invariant_Sketch_Histogram_Shape_Descriptor.html">368 iccv-2013-SYM-FISH: A Symmetry-Aware Flip Invariant Sketch Histogram Shape Descriptor</a></p>
<p>18 0.73956919 <a title="191-lda-18" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>19 0.73933369 <a title="191-lda-19" href="./iccv-2013-Class-Specific_Simplex-Latent_Dirichlet_Allocation_for_Image_Classification.html">73 iccv-2013-Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification</a></p>
<p>20 0.7333166 <a title="191-lda-20" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
