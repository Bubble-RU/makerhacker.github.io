<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>125 iccv-2013-Drosophila Embryo Stage Annotation Using Label Propagation</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-125" href="#">iccv2013-125</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>125 iccv-2013-Drosophila Embryo Stage Annotation Using Label Propagation</h1>
<br/><p>Source: <a title="iccv-2013-125-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Kazmar_Drosophila_Embryo_Stage_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Tomáš Kazmar, Evgeny Z. Kvon, Alexander Stark, Christoph H. Lampert</p><p>Abstract: In this work we propose a system for automatic classification of Drosophila embryos into developmental stages. While the system is designed to solve an actual problem in biological research, we believe that the principle underlying it is interesting not only for biologists, but also for researchers in computer vision. The main idea is to combine two orthogonal sources of information: one is a classifier trained on strongly invariant features, which makes it applicable to images of very different conditions, but also leads to rather noisy predictions. The other is a label propagation step based on a more powerful similarity measure that however is only consistent within specific subsets of the data at a time. In our biological setup, the information sources are the shape and the staining patterns of embryo images. We show experimentally that while neither of the methods can be used by itself to achieve satisfactory results, their combination achieves prediction quality comparable to human per- formance.</p><p>Reference: <a title="iccv-2013-125-reference" href="../iccv2013_reference/iccv-2013-Drosophila_Embryo_Stage_Annotation_Using_Label_Propagation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 at Abstract  In this work we propose a system for automatic classification of Drosophila embryos into developmental stages. [sent-6, score-0.722]
</p><p>2 The other is a label propagation step based on a more powerful similarity measure that however is only consistent within specific subsets of the data at a time. [sent-9, score-0.243]
</p><p>3 In our biological setup, the information sources are the shape and the staining patterns of embryo images. [sent-10, score-0.813]
</p><p>4 at st Line 1  Line 2  Line 3  Stage  interval  4-6  Stage  interval  7-8  Stage interval 9-10 Stage  interval  1 12 1-  Stage interval 13-14 Stage interval  15-16  In biological research, high-throughput screening has become a common technique for producing microscopic data. [sent-15, score-0.368]
</p><p>5 In this work we tackle the following problem: given an image containing many embryos of the fruit fly Drosophila melanogaster, identify for each embryo which out of six developmental stage groups it is in, see Figure 1. [sent-19, score-1.51]
</p><p>6 Examples of embryo images of three differently genetically modified Drosophila lines (left, middle, right). [sent-21, score-0.697]
</p><p>7 The modifications cause different appearances, which change as the embryos develop (top-to-bottom). [sent-22, score-0.527]
</p><p>8 The pattern is often indicative of stage  inside the line, even though it can vary due to different pose of the embryo (stages 7–8 and 11–12 of Line 1). [sent-23, score-0.821]
</p><p>9 Across different lines the pattern for the same stage usually differs significantly (compare embryos of stage 15–16). [sent-24, score-0.905]
</p><p>10 Images of stage 4–6 show no activity; in other stages empty space means no activity pattern. [sent-25, score-0.31]
</p><p>11 Subsequently, the resulting embryo is allowed to develop up to a duration of 24 hours. [sent-32, score-0.611]
</p><p>12 If the fragment has a regulatory function and becomes active during development, the reporter gene is co-activated, and can be visualized by in-situ hybridization [13] giving rise to a visible pattern inside the otherwise transparent embryo. [sent-33, score-0.275]
</p><p>13 Therefore, hundreds of genetically identical embryos are created at the same time, mounted on a microscopic slide and imaged simultaneously using a large whole-slide microscopic setup (for a sample image, see the supplemental material). [sent-37, score-0.806]
</p><p>14 An advantage of this process is that embryos of all developmental stages occur within a single image, making it unnecessary to repeat the time-consuming and costly step of embryo preparation and image acquisition multiple times for different stages. [sent-38, score-1.39]
</p><p>15 A disadvantage is that in order to interpret the patterns and identify the function of the respective DNA fragment, the hundreds of embryos need to be separated into their respective developmental stages before a further analysis can be performed. [sent-39, score-0.835]
</p><p>16 This is necessary for every of the thousands of DNA fragments to be tested, resulting in the need to classify several million embryo images. [sent-40, score-0.644]
</p><p>17 Consequently, a computer vision system that can automatically or semi-automatically provide stage annotation for the embryo images is a crucial element for the success of the project. [sent-43, score-0.871]
</p><p>18 With the advancement of computer vision techniques for biological applications, also the task of analyzing Drosophila embryo images has received a certain amount of attention. [sent-45, score-0.694]
</p><p>19 This database consists of manually selected embryos with a clearly visible pattern and per-embryo pose annotation. [sent-55, score-0.527]
</p><p>20 In this work, we introduce a system for automatic staging of Drosophila embryos that leverages the information in expression patterns without requiring training data specific to any of these patterns. [sent-61, score-0.736]
</p><p>21 This requires only modest amounts of training data, since the embryo contour is not affected by the genetic modifications and therefore the same classifier can be applied to all embryo images in all slides at test time. [sent-64, score-1.527]
</p><p>22 Within each slide, the resulting predictions are improved and robustified using label propagation, with a similarity graph that is obtained from the similarity between expression patterns (appearance). [sent-65, score-0.309]
</p><p>23 It is only between different slides that they can change arbitrarily as embryos in different slides have different mutations. [sent-69, score-0.788]
</p><p>24 A base classifier uses a general source of information (embryo shape) and therefore it is trained only once, whereas the label propagation works always with a set of embryos sharing the same genotype and can take advantage of the genotype-specific features (embryo appearance). [sent-74, score-0.83]
</p><p>25 Shape-based stage classification One way to distinguish between different developmental stages is based on the embryos’ shape [3], in particular their contour (Figure 3). [sent-82, score-0.551]
</p><p>26 Therefore, using a shape feature we can build a classifier that is trained just once off-line and then applied to every later embryo image, regardless of which DNA fragment had been inserted into the genome. [sent-84, score-0.732]
</p><p>27 On a global scale, the embryo contour is always close to elliptical. [sent-85, score-0.658]
</p><p>28 We make use of this fact by automatically extracting individual embryos from the slide image and rotating them such that their main axis is aligned horizontally. [sent-86, score-0.678]
</p><p>29 On finer scales, embryos of different stages differ in short characteristic regions that occur in different locations and  at different times during the development. [sent-87, score-0.627]
</p><p>30 To capture these in a feature vector, we first represent the embryo outline by a chaincode [5], such that any short substring of the code  4-6  7-8  9- 10  11-12  13- 14  15- 16  Figure 3. [sent-88, score-0.611]
</p><p>31 Contours of some of the stages (4–6, 9–10) are very smooth, whereas for other stages there are easily recognizable markers reflecting the changes of embryo morphology, like the appearing segments for stages 11–12, and 13–14. [sent-90, score-1.01]
</p><p>32 From a training set of embryo contours we first perform term frequency/ inverse document frequency (tfidf) weighting of the exacted substrings, with developmental stages taking the role of different documents. [sent-94, score-0.933]
</p><p>33 To represent an embryo image, we extract contour segments on all scales, assigning each segment the cluster ID of its nearest codebook entry, as measured by Hamming distance. [sent-113, score-0.694]
</p><p>34 Since the shape-based classifier relies on features that are not affected by the tested genetic modification, we need only a single training set to get a classifier that can be applied to all future embryo images. [sent-120, score-0.755]
</p><p>35 We can take any normal (wild-type) Drosophila line and take images of its embryos to form a training set. [sent-122, score-0.543]
</p><p>36 Most importantly, we can make use of the fact that the target classes we want to predict are developmental stages, which themselves are defined by certain intervals of how many hours the embryo has been developing. [sent-123, score-0.763]
</p><p>37 All we have  to do is to precisely time the interval between when the eggs start developing, and when the embryos are prepared for imaging. [sent-125, score-0.548]
</p><p>38 Note that this trick of breeding a training set works most efficiently for the genetically unmodified Drosophila, which can be bred in large quantities and create a large amount of healthy embryos per generation. [sent-127, score-0.661]
</p><p>39 We cannot use the same trick to get stage specific embryo images for the genetically modified organisms, since this would require many repetitions of breeding, staining and imaging for each stage of each DNA segment to be tested. [sent-128, score-1.097]
</p><p>40 In addition to the training data obtained by breeding we also create a smaller amount of annotated images of genetically modified embryos with consistent pattern in the traditional, manual way. [sent-129, score-0.702]
</p><p>41 Pattern similarity A good similarity graph is a key element for the label propagation method we want to apply. [sent-133, score-0.278]
</p><p>42 Typical measures of pattern similarity divide the embryo area into a grid or a triangulation of cells and compare the average intensities within each cell, for example by the sum of squared differences (SSD) [6]. [sent-135, score-0.667]
</p><p>43 A stage 15–16 embryo and its 7 closest neighbors according to our measure of similarity of expression patterns. [sent-145, score-0.921]
</p><p>44 The similarity is invariant to rotation around the embryo’s main axis and to in-plane rotation as well leading to all relevant embryos that should share the same stage classification being close to each other. [sent-146, score-0.777]
</p><p>45 situations when embryo images are registered, but they are very sensitive to out-of-plane rotation, which can drastically change the appearance between embryos of the same stage, see Figure 4. [sent-147, score-1.117]
</p><p>46 Finally, we get a similarity between two embryo images that is also invariant to in-plane rotation by comparing the images always in two ways, once directly as described above, and once after flipping one of the images along the vertical axis, keeping the larger of the two similarity values. [sent-150, score-0.681]
</p><p>47 The reason is likely that we only measure the similarity between embryo images within the same slide, where typically only few and characteristically different patterns occur. [sent-153, score-0.702]
</p><p>48 Label propagation At test time, given a slide with n embryo images, we form a symmetric k-NN graph of n nodes. [sent-156, score-0.889]
</p><p>49 Since ordinary label propagation handles only binary classification, we use a one-vs-rest approach, solving one 11009922  label propagation step for each of the six stage groups we  are interested in. [sent-158, score-0.623]
</p><p>50 In each case, each node iis assigned a label preference score yil, which is the output of the shapebased classifier for this stage on the corresponding embryo image. [sent-159, score-0.955]
</p><p>51 The idea of label propagation is that the decisions of images, for which the classifier is confident (yli close to 1), will influence the decisions of their neighbors to also prefer this label. [sent-160, score-0.256]
</p><p>52 We can use these to rank embryo images by their label confidence, for example in an interactive labeling tool, or make a hard assignment to the label of maximum response: l∗i= arglmaxfil thereby achieving a fully automatic setup. [sent-173, score-0.806]
</p><p>53 Experiments  We performed experiments on synthetic data and on Drosophila embryo gene expression images. [sent-175, score-0.839]
</p><p>54 We pick the values for two hyper-parameters of label propagation (bandwidth, and regularization weight) on the validation set in an analogous way. [sent-178, score-0.229]
</p><p>55 Illustrative toy example We first show the power of label propagation with two orthogonal sources ofinformation in the following synthetic toy example. [sent-181, score-0.246]
</p><p>56 Synthetic test data (left) and corresponding results from the base classifier (middle) and label propagation (right). [sent-190, score-0.281]
</p><p>57 Drosophila embryos stage annotation Next, we describe the actual Drosophila stage annotation system. [sent-199, score-0.986]
</p><p>58 We describe how to obtain the data, and we evaluate the performance of stage annotation by per-embryo shape-based classification and show how it improves by label propagation. [sent-200, score-0.341]
</p><p>59 1  Training data collection  We perform stage-specific collections of embryos as described in Section 2. [sent-203, score-0.506]
</p><p>60 We exclude abnormal embryos (blurred, overlapping, broken, incorrect stage) yielding a total of 6810 embryos along with their contours from 47 slides. [sent-205, score-1.043]
</p><p>61 We explicitly balance this dataset resulting in 1014 embryos per stage group. [sent-206, score-0.695]
</p><p>62 2  Validation and test data collection  We collected expert annotations for 193 slides from the described biological study (embryos show various expression patterns). [sent-209, score-0.336]
</p><p>63 Two annotators were given a set of automatically preselected embryos for each of the slides separately. [sent-210, score-0.647]
</p><p>64 At least a hundred embryos were selected with emphasis on non-blurred, non-overlapping embryos showing any active pattern. [sent-211, score-1.012]
</p><p>65 If there were not sufficiently many such automatically selected embryos the set was completed by random sampling. [sent-212, score-0.506]
</p><p>66 The annotators were free in which embryos to label, with the instruction to try to annotate at least one embryo of each stage that shows any active pattern. [sent-213, score-1.306]
</p><p>67 11009933  The first expert annotated 49 slides with 102 embryos an-  ×  notated on average, the second expert annotated 150 slides with 27 embryos per slide, in total we obtained 9006 annotated embryos. [sent-214, score-1.406]
</p><p>68 As there are usually more embryos of later stages and the activity is more frequent in later stages as well, the dataset is significantly unbalanced in favor of later stages. [sent-215, score-0.817]
</p><p>69 We randomly select 20 slides as a validation set for tuning the parameters of label propagation. [sent-217, score-0.24]
</p><p>70 3  Embryo stage annotation  We choose the parameters of the shape features based on prior experience on a smaller dataset. [sent-225, score-0.259]
</p><p>71 A late stage 14 embryo incorrectly predicted by a classifier as 11–12 gets the right label upon label propagation thanks to some of its nearest neighbors being confidently and correctly predicted (top right, top left). [sent-248, score-1.173]
</p><p>72 In Figure 7, we show one particular example where the contour of an embryo does not reflect well the correct stage and the embryo is labelled incorrectly by the SVM classifier. [sent-253, score-1.478]
</p><p>73 This error is corrected by label propagation since there are enough embryos which are confidently predicted to be of the correct stage and which have very similar pattern. [sent-254, score-0.922]
</p><p>74 The improvement of label propagation as measured by average slide accuracy is largely due to later stages (11 12, 13–14, 15–16) which contribute to this improvement the most, see Figure 6 (right). [sent-255, score-0.494]
</p><p>75 the fraction of embryos correctly classified across all slides, and label accuracy, i. [sent-259, score-0.584]
</p><p>76 the fraction of correctly classified embryos within each stage, averaged over all stages. [sent-261, score-0.506]
</p><p>77 Overall, slide accuracy and sample accuracy have higher values compared to label accuracy because the majority of the embryos belongs to the more improved later stages. [sent-263, score-0.749]
</p><p>78 The numbers  indicate embryo counts, the colors encode per-class fractions. [sent-266, score-0.611]
</p><p>79 4  Discussion  Overall, label propagation improves over the classification in all sample, label, and slide accuracies. [sent-282, score-0.379]
</p><p>80 First, stage group 9–10 is difficult to classify based on contour and leads to common mistakes, label propagation then corrects these mistakes only sporadically. [sent-284, score-0.444]
</p><p>81 Second, stage group 7–8 is the only label for which label propagation leads to a decrease in accuracy. [sent-286, score-0.475]
</p><p>82 This stage group is unique in the sense that it lasts only 35 minutes (compared to 90–260 minutes for the other five stage groups) which results in very few embryos collected and mounted on one particular slide. [sent-287, score-0.884]
</p><p>83 Stage group 7–8 is also the point when the embryo undergoes highly dynamic changes (final gastrulation and rapid germ band elongation [3]) which result in diverse expression patterns. [sent-288, score-0.697]
</p><p>84 For this stage one cannot expect a significant improvement from the label propagation. [sent-289, score-0.267]
</p><p>85 If a 7–8 embryo is labeled incorrectly by  the SVM classifier, its decision will likely not be corrected, because none of the other embryos of this stage have a similar enough pattern to play a role during propagation. [sent-290, score-1.347]
</p><p>86 Conclusion We proposed a system for Drosophila embryo stage classification that combines a classifier and graph-based label propagation. [sent-292, score-0.969]
</p><p>87 The classifier is rather weak but can be learned from general shape features, whereas the graph captures specific appearance of each of the stages for one particular genotype and enables us to correct for the errors of the classifier. [sent-293, score-0.231]
</p><p>88 A further advantage of the semi-automatic scenario is that the user can change the parameters of label propagation on the fly, thereby avoiding the need for model selection on a validation set. [sent-299, score-0.247]
</p><p>89 Also, rare classes can get suppressed by frequent ones in the label propagation step as we saw with the early developmental stages which are very short. [sent-303, score-0.515]
</p><p>90 We plan to use the system to automatically prefilter the millions of individual embryo images and identify a small set of embryo images per stage, from which a human picks the visually most suitable one for biological interpretation. [sent-307, score-1.325]
</p><p>91 Acknowledgement We would like to thank Katharina Schernhuber and Michaela Pagani for their help with the embryo collections. [sent-308, score-0.611]
</p><p>92 Joint stage recognition and anatomical annotation of Drosophila gene expression patterns. [sent-322, score-0.49]
</p><p>93 Drosophila gene expression pattern annotation using sparse features and term-term interactions. [sent-359, score-0.3]
</p><p>94 BEST: a novel computational approach for comparing gene expression patterns from early stages of Drosophila melanogaster development. [sent-369, score-0.434]
</p><p>95 Drosophila gene expression pattern annotation through multi-instance multi-label learning. [sent-398, score-0.3]
</p><p>96 SPEX2: automated concise extraction of spatial gene expression patterns from fly embryo ISH images. [sent-443, score-0.929]
</p><p>97 Systematic determination of patterns of gene expression during Drosophila embryogenesis. [sent-467, score-0.284]
</p><p>98 Global analysis of patterns of gene expression during Drosophila embryogenesis. [sent-481, score-0.284]
</p><p>99 Classification of Drosophila embryonic developmental stage range based on gene expression pattern images. [sent-488, score-0.623]
</p><p>100 Automatic recognition and annotation of gene expression patterns of fly embryos. [sent-493, score-0.369]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('embryo', 0.611), ('embryos', 0.506), ('drosophila', 0.264), ('stage', 0.189), ('developmental', 0.152), ('slide', 0.148), ('gene', 0.142), ('slides', 0.141), ('propagation', 0.13), ('stages', 0.121), ('expression', 0.086), ('genetically', 0.086), ('dna', 0.084), ('biological', 0.083), ('label', 0.078), ('substrings', 0.057), ('patterns', 0.056), ('annotation', 0.051), ('breeding', 0.051), ('genome', 0.051), ('classifier', 0.048), ('contour', 0.047), ('genotype', 0.043), ('kvq', 0.043), ('interval', 0.042), ('imp', 0.038), ('celniker', 0.038), ('fragment', 0.037), ('bioinformatics', 0.036), ('biology', 0.035), ('similarity', 0.035), ('fly', 0.034), ('fragments', 0.033), ('embryonic', 0.033), ('fil', 0.033), ('microscopic', 0.033), ('molecular', 0.032), ('contours', 0.031), ('austria', 0.03), ('genetic', 0.03), ('hammonds', 0.029), ('kazmar', 0.029), ('kvon', 0.029), ('kwan', 0.029), ('melanogaster', 0.029), ('pagani', 0.029), ('puniyani', 0.029), ('regulatory', 0.029), ('reporter', 0.029), ('schernhuber', 0.029), ('shapebased', 0.029), ('staging', 0.029), ('expert', 0.026), ('beaton', 0.025), ('biologists', 0.025), ('hartenstein', 0.025), ('organism', 0.025), ('tomancak', 0.025), ('weiszmann', 0.025), ('yil', 0.025), ('base', 0.025), ('hamming', 0.025), ('development', 0.024), ('axis', 0.024), ('aij', 0.023), ('classification', 0.023), ('sources', 0.022), ('anatomical', 0.022), ('staining', 0.022), ('ish', 0.022), ('pattern', 0.021), ('validation', 0.021), ('bandwidth', 0.021), ('modifications', 0.021), ('automatic', 0.021), ('svm', 0.02), ('system', 0.02), ('situation', 0.02), ('annotated', 0.02), ('incorrectly', 0.02), ('predictions', 0.019), ('segments', 0.019), ('confidently', 0.019), ('line', 0.019), ('shape', 0.019), ('thereby', 0.018), ('groups', 0.018), ('confusion', 0.018), ('chain', 0.018), ('frequent', 0.018), ('training', 0.018), ('misclassified', 0.017), ('codebook', 0.017), ('transparent', 0.017), ('later', 0.017), ('markers', 0.017), ('orthogonal', 0.016), ('suppressed', 0.016), ('international', 0.016), ('invariance', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="125-tfidf-1" href="./iccv-2013-Drosophila_Embryo_Stage_Annotation_Using_Label_Propagation.html">125 iccv-2013-Drosophila Embryo Stage Annotation Using Label Propagation</a></p>
<p>Author: Tomáš Kazmar, Evgeny Z. Kvon, Alexander Stark, Christoph H. Lampert</p><p>Abstract: In this work we propose a system for automatic classification of Drosophila embryos into developmental stages. While the system is designed to solve an actual problem in biological research, we believe that the principle underlying it is interesting not only for biologists, but also for researchers in computer vision. The main idea is to combine two orthogonal sources of information: one is a classifier trained on strongly invariant features, which makes it applicable to images of very different conditions, but also leads to rather noisy predictions. The other is a label propagation step based on a more powerful similarity measure that however is only consistent within specific subsets of the data at a time. In our biological setup, the information sources are the shape and the staining patterns of embryo images. We show experimentally that while neither of the methods can be used by itself to achieve satisfactory results, their combination achieves prediction quality comparable to human per- formance.</p><p>2 0.16479449 <a title="125-tfidf-2" href="./iccv-2013-Volumetric_Semantic_Segmentation_Using_Pyramid_Context_Features.html">447 iccv-2013-Volumetric Semantic Segmentation Using Pyramid Context Features</a></p>
<p>Author: Jonathan T. Barron, Mark D. Biggin, Pablo Arbeláez, David W. Knowles, Soile V.E. Keranen, Jitendra Malik</p><p>Abstract: We present an algorithm for the per-voxel semantic segmentation of a three-dimensional volume. At the core of our algorithm is a novel “pyramid context” feature, a descriptive representation designed such that exact per-voxel linear classification can be made extremely efficient. This feature not only allows for efficient semantic segmentation but enables other aspects of our algorithm, such as novel learned features and a stacked architecture that can reason about self-consistency. We demonstrate our technique on 3Dfluorescence microscopy data ofDrosophila embryosfor which we are able to produce extremely accurate semantic segmentations in a matter of minutes, and for which other algorithms fail due to the size and high-dimensionality of the data, or due to the difficulty of the task.</p><p>3 0.082914792 <a title="125-tfidf-3" href="./iccv-2013-Conservation_Tracking.html">87 iccv-2013-Conservation Tracking</a></p>
<p>Author: Martin Schiegg, Philipp Hanslovsky, Bernhard X. Kausler, Lars Hufnagel, Fred A. Hamprecht</p><p>Abstract: The quality of any tracking-by-assignment hinges on the accuracy of the foregoing target detection / segmentation step. In many kinds of images, errors in this first stage are unavoidable. These errors then propagate to, and corrupt, the tracking result. Our main contribution is the first probabilistic graphical model that can explicitly account for over- and undersegmentation errors even when the number of tracking targets is unknown and when they may divide, as in cell cultures. The tracking model we present implements global consistency constraints for the number of targets comprised by each detection and is solved to global optimality on reasonably large 2D+t and 3D+t datasets. In addition, we empirically demonstrate the effectiveness of a postprocessing that allows to establish target identity even across occlusion / undersegmentation. The usefulness and efficiency of this new tracking method is demonstrated on three different and challenging 2D+t and 3D+t datasets from developmental biology.</p><p>4 0.067538217 <a title="125-tfidf-4" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>Author: Bo Wang, Zhuowen Tu, John K. Tsotsos</p><p>Abstract: In graph-based semi-supervised learning approaches, the classification rate is highly dependent on the size of the availabel labeled data, as well as the accuracy of the similarity measures. Here, we propose a semi-supervised multi-class/multi-label classification scheme, dynamic label propagation (DLP), which performs transductive learning through propagation in a dynamic process. Existing semi-supervised classification methods often have difficulty in dealing with multi-class/multi-label problems due to the lack in consideration of label correlation; our algorithm instead emphasizes dynamic metric fusion with label information. Significant improvement over the state-of-the-art methods is observed on benchmark datasets for both multiclass and multi-label tasks.</p><p>5 0.066749357 <a title="125-tfidf-5" href="./iccv-2013-Multi-stage_Contextual_Deep_Learning_for_Pedestrian_Detection.html">279 iccv-2013-Multi-stage Contextual Deep Learning for Pedestrian Detection</a></p>
<p>Author: Xingyu Zeng, Wanli Ouyang, Xiaogang Wang</p><p>Abstract: Cascaded classifiers1 have been widely used in pedestrian detection and achieved great success. These classifiers are trained sequentially without joint optimization. In this paper, we propose a new deep model that can jointly train multi-stage classifiers through several stages of backpropagation. It keeps the score map output by a classifier within a local region and uses it as contextual information to support the decision at the next stage. Through a specific design of the training strategy, this deep architecture is able to simulate the cascaded classifiers by mining hard samples to train the network stage-by-stage. Each classifier handles samples at a different difficulty level. Unsupervised pre-training and specifically designed stage-wise supervised training are used to regularize the optimization problem. Both theoretical analysis and experimental results show that the training strategy helps to avoid overfitting. Experimental results on three datasets (Caltech, ETH and TUD-Brussels) show that our approach outperforms the state-of-the-art approaches.</p><p>6 0.065795392 <a title="125-tfidf-6" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>7 0.056123912 <a title="125-tfidf-7" href="./iccv-2013-Image_Segmentation_with_Cascaded_Hierarchical_Models_and_Logistic_Disjunctive_Normal_Networks.html">211 iccv-2013-Image Segmentation with Cascaded Hierarchical Models and Logistic Disjunctive Normal Networks</a></p>
<p>8 0.052183628 <a title="125-tfidf-8" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<p>9 0.046865944 <a title="125-tfidf-9" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>10 0.046273354 <a title="125-tfidf-10" href="./iccv-2013-Stacked_Predictive_Sparse_Coding_for_Classification_of_Distinct_Regions_in_Tumor_Histopathology.html">401 iccv-2013-Stacked Predictive Sparse Coding for Classification of Distinct Regions in Tumor Histopathology</a></p>
<p>11 0.045049593 <a title="125-tfidf-11" href="./iccv-2013-Handling_Occlusions_with_Franken-Classifiers.html">190 iccv-2013-Handling Occlusions with Franken-Classifiers</a></p>
<p>12 0.043366935 <a title="125-tfidf-12" href="./iccv-2013-A_Convex_Optimization_Framework_for_Active_Learning.html">6 iccv-2013-A Convex Optimization Framework for Active Learning</a></p>
<p>13 0.042652972 <a title="125-tfidf-13" href="./iccv-2013-New_Graph_Structured_Sparsity_Model_for_Multi-label_Image_Annotations.html">290 iccv-2013-New Graph Structured Sparsity Model for Multi-label Image Annotations</a></p>
<p>14 0.040819373 <a title="125-tfidf-14" href="./iccv-2013-Quantize_and_Conquer%3A_A_Dimensionality-Recursive_Solution_to_Clustering%2C_Vector_Quantization%2C_and_Image_Retrieval.html">333 iccv-2013-Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval</a></p>
<p>15 0.039885182 <a title="125-tfidf-15" href="./iccv-2013-Discriminative_Label_Propagation_for_Multi-object_Tracking_with_Sporadic_Appearance_Features.html">120 iccv-2013-Discriminative Label Propagation for Multi-object Tracking with Sporadic Appearance Features</a></p>
<p>16 0.039702591 <a title="125-tfidf-16" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>17 0.03898938 <a title="125-tfidf-17" href="./iccv-2013-Joint_Noise_Level_Estimation_from_Personal_Photo_Collections.html">223 iccv-2013-Joint Noise Level Estimation from Personal Photo Collections</a></p>
<p>18 0.038484339 <a title="125-tfidf-18" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>19 0.03785482 <a title="125-tfidf-19" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>20 0.037718177 <a title="125-tfidf-20" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.106), (1, 0.011), (2, -0.012), (3, -0.021), (4, 0.022), (5, 0.007), (6, -0.003), (7, 0.012), (8, -0.009), (9, -0.038), (10, -0.014), (11, -0.02), (12, -0.002), (13, -0.009), (14, 0.024), (15, -0.023), (16, -0.029), (17, 0.005), (18, -0.004), (19, 0.004), (20, -0.025), (21, -0.022), (22, -0.045), (23, 0.039), (24, -0.061), (25, 0.002), (26, -0.0), (27, 0.026), (28, 0.014), (29, 0.016), (30, 0.002), (31, 0.0), (32, 0.006), (33, 0.01), (34, -0.046), (35, 0.008), (36, 0.005), (37, -0.03), (38, 0.028), (39, 0.029), (40, 0.009), (41, 0.025), (42, -0.054), (43, -0.001), (44, -0.015), (45, -0.073), (46, -0.056), (47, 0.0), (48, -0.123), (49, 0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85078204 <a title="125-lsi-1" href="./iccv-2013-Drosophila_Embryo_Stage_Annotation_Using_Label_Propagation.html">125 iccv-2013-Drosophila Embryo Stage Annotation Using Label Propagation</a></p>
<p>Author: Tomáš Kazmar, Evgeny Z. Kvon, Alexander Stark, Christoph H. Lampert</p><p>Abstract: In this work we propose a system for automatic classification of Drosophila embryos into developmental stages. While the system is designed to solve an actual problem in biological research, we believe that the principle underlying it is interesting not only for biologists, but also for researchers in computer vision. The main idea is to combine two orthogonal sources of information: one is a classifier trained on strongly invariant features, which makes it applicable to images of very different conditions, but also leads to rather noisy predictions. The other is a label propagation step based on a more powerful similarity measure that however is only consistent within specific subsets of the data at a time. In our biological setup, the information sources are the shape and the staining patterns of embryo images. We show experimentally that while neither of the methods can be used by itself to achieve satisfactory results, their combination achieves prediction quality comparable to human per- formance.</p><p>2 0.80020398 <a title="125-lsi-2" href="./iccv-2013-Volumetric_Semantic_Segmentation_Using_Pyramid_Context_Features.html">447 iccv-2013-Volumetric Semantic Segmentation Using Pyramid Context Features</a></p>
<p>Author: Jonathan T. Barron, Mark D. Biggin, Pablo Arbeláez, David W. Knowles, Soile V.E. Keranen, Jitendra Malik</p><p>Abstract: We present an algorithm for the per-voxel semantic segmentation of a three-dimensional volume. At the core of our algorithm is a novel “pyramid context” feature, a descriptive representation designed such that exact per-voxel linear classification can be made extremely efficient. This feature not only allows for efficient semantic segmentation but enables other aspects of our algorithm, such as novel learned features and a stacked architecture that can reason about self-consistency. We demonstrate our technique on 3Dfluorescence microscopy data ofDrosophila embryosfor which we are able to produce extremely accurate semantic segmentations in a matter of minutes, and for which other algorithms fail due to the size and high-dimensionality of the data, or due to the difficulty of the task.</p><p>3 0.71420085 <a title="125-lsi-3" href="./iccv-2013-Stacked_Predictive_Sparse_Coding_for_Classification_of_Distinct_Regions_in_Tumor_Histopathology.html">401 iccv-2013-Stacked Predictive Sparse Coding for Classification of Distinct Regions in Tumor Histopathology</a></p>
<p>Author: Hang Chang, Yin Zhou, Paul Spellman, Bahram Parvin</p><p>Abstract: Image-based classification ofhistology sections, in terms of distinct components (e.g., tumor, stroma, normal), provides a series of indices for tumor composition. Furthermore, aggregation of these indices, from each whole slide image (WSI) in a large cohort, can provide predictive models of the clinical outcome. However, performance of the existing techniques is hindered as a result of large technical variations and biological heterogeneities that are always present in a large cohort. We propose a system that automatically learns a series of basis functions for representing the underlying spatial distribution using stacked predictive sparse decomposition (PSD). The learned representation is then fed into the spatial pyramid matching framework (SPM) with a linear SVM classifier. The system has been evaluated for classification of (a) distinct histological components for two cohorts of tumor types, and (b) colony organization of normal and malignant cell lines in 3D cell culture models. Throughput has been increased through the utility of graphical processing unit (GPU), and evalu- ation indicates a superior performance results, compared with previous research.</p><p>4 0.64642811 <a title="125-lsi-4" href="./iccv-2013-Shape_Index_Descriptors_Applied_to_Texture-Based_Galaxy_Analysis.html">388 iccv-2013-Shape Index Descriptors Applied to Texture-Based Galaxy Analysis</a></p>
<p>Author: Kim Steenstrup Pedersen, Kristoffer Stensbo-Smidt, Andrew Zirm, Christian Igel</p><p>Abstract: A texture descriptor based on the shape index and the accompanying curvedness measure is proposed, and it is evaluated for the automated analysis of astronomical image data. A representative sample of images of low-redshift galaxies from the Sloan Digital Sky Survey (SDSS) serves as a testbed. The goal of applying texture descriptors to these data is to extract novel information about galaxies; information which is often lost in more traditional analysis. In this study, we build a regression model for predicting a spectroscopic quantity, the specific star-formation rate (sSFR). As texture features we consider multi-scale gradient orientation histograms as well as multi-scale shape index histograms, which lead to a new descriptor. Our results show that we can successfully predict spectroscopic quantities from the texture in optical multi-band images. We successfully recover the observed bi-modal distribution of galaxies into quiescent and star-forming. The state-ofthe-art for predicting the sSFR is a color-based physical model. We significantly improve its accuracy by augmenting the model with texture information. This study is thefirst step towards enabling the quantification of physical galaxy properties from imaging data alone.</p><p>5 0.63321453 <a title="125-lsi-5" href="./iccv-2013-Pyramid_Coding_for_Functional_Scene_Element_Recognition_in_Video_Scenes.html">331 iccv-2013-Pyramid Coding for Functional Scene Element Recognition in Video Scenes</a></p>
<p>Author: Eran Swears, Anthony Hoogs, Kim Boyer</p><p>Abstract: Recognizing functional scene elemeents in video scenes based on the behaviors of moving objects that interact with them is an emerging problem ooff interest. Existing approaches have a limited ability to chharacterize elements such as cross-walks, intersections, andd buildings that have low activity, are multi-modal, or havee indirect evidence. Our approach recognizes the low activvity and multi-model elements (crosswalks/intersections) by introducing a hierarchy of descriptive clusters to fform a pyramid of codebooks that is sparse in the numbber of clusters and dense in content. The incorporation oof local behavioral context such as person-enter-building aand vehicle-parking nearby enables the detection of elemennts that do not have direct motion-based evidence, e.g. buuildings. These two contributions significantly improvee scene element recognition when compared against thhree state-of-the-art approaches. Results are shown on tyypical ground level surveillance video and for the first time on the more complex Wide Area Motion Imagery.</p><p>6 0.63210434 <a title="125-lsi-6" href="./iccv-2013-Image_Segmentation_with_Cascaded_Hierarchical_Models_and_Logistic_Disjunctive_Normal_Networks.html">211 iccv-2013-Image Segmentation with Cascaded Hierarchical Models and Logistic Disjunctive Normal Networks</a></p>
<p>7 0.62538332 <a title="125-lsi-7" href="./iccv-2013-Detecting_Irregular_Curvilinear_Structures_in_Gray_Scale_and_Color_Imagery_Using_Multi-directional_Oriented_Flux.html">112 iccv-2013-Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-directional Oriented Flux</a></p>
<p>8 0.58578646 <a title="125-lsi-8" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>9 0.56953716 <a title="125-lsi-9" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<p>10 0.56899858 <a title="125-lsi-10" href="./iccv-2013-Heterogeneous_Auto-similarities_of_Characteristics_%28HASC%29%3A_Exploiting_Relational_Information_for_Classification.html">193 iccv-2013-Heterogeneous Auto-similarities of Characteristics (HASC): Exploiting Relational Information for Classification</a></p>
<p>11 0.55445468 <a title="125-lsi-11" href="./iccv-2013-Sequential_Bayesian_Model_Update_under_Structured_Scene_Prior_for_Semantic_Road_Scenes_Labeling.html">386 iccv-2013-Sequential Bayesian Model Update under Structured Scene Prior for Semantic Road Scenes Labeling</a></p>
<p>12 0.55253482 <a title="125-lsi-12" href="./iccv-2013-Synergistic_Clustering_of_Image_and_Segment_Descriptors_for_Unsupervised_Scene_Understanding.html">412 iccv-2013-Synergistic Clustering of Image and Segment Descriptors for Unsupervised Scene Understanding</a></p>
<p>13 0.55183822 <a title="125-lsi-13" href="./iccv-2013-The_Interestingness_of_Images.html">416 iccv-2013-The Interestingness of Images</a></p>
<p>14 0.5298515 <a title="125-lsi-14" href="./iccv-2013-Beyond_Hard_Negative_Mining%3A_Efficient_Detector_Learning_via_Block-Circulant_Decomposition.html">61 iccv-2013-Beyond Hard Negative Mining: Efficient Detector Learning via Block-Circulant Decomposition</a></p>
<p>15 0.52774751 <a title="125-lsi-15" href="./iccv-2013-Random_Forests_of_Local_Experts_for_Pedestrian_Detection.html">336 iccv-2013-Random Forests of Local Experts for Pedestrian Detection</a></p>
<p>16 0.52667087 <a title="125-lsi-16" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>17 0.52660745 <a title="125-lsi-17" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>18 0.52083641 <a title="125-lsi-18" href="./iccv-2013-An_Adaptive_Descriptor_Design_for_Object_Recognition_in_the_Wild.html">48 iccv-2013-An Adaptive Descriptor Design for Object Recognition in the Wild</a></p>
<p>19 0.51749974 <a title="125-lsi-19" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>20 0.51532245 <a title="125-lsi-20" href="./iccv-2013-Fast_High_Dimensional_Vector_Multiplication_Face_Recognition.html">158 iccv-2013-Fast High Dimensional Vector Multiplication Face Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.052), (7, 0.018), (12, 0.018), (26, 0.507), (31, 0.026), (42, 0.071), (48, 0.011), (64, 0.023), (73, 0.019), (84, 0.012), (89, 0.117), (98, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93229955 <a title="125-lda-1" href="./iccv-2013-Structured_Light_in_Sunlight.html">405 iccv-2013-Structured Light in Sunlight</a></p>
<p>Author: Mohit Gupta, Qi Yin, Shree K. Nayar</p><p>Abstract: Strong ambient illumination severely degrades the performance of structured light based techniques. This is especially true in outdoor scenarios, where the structured light sources have to compete with sunlight, whose power is often 2-5 orders of magnitude larger than the projected light. In this paper, we propose the concept of light-concentration to overcome strong ambient illumination. Our key observation is that given a fixed light (power) budget, it is always better to allocate it sequentially in several portions of the scene, as compared to spreading it over the entire scene at once. For a desired level of accuracy, we show that by distributing light appropriately, the proposed approach requires 1-2 orders lower acquisition time than existing approaches. Our approach is illumination-adaptive as the optimal light distribution is determined based on a measurement of the ambient illumination level. Since current light sources have a fixed light distribution, we have built a prototype light source that supports flexible light distribution by controlling the scanning speed of a laser scanner. We show several high quality 3D scanning results in a wide range of outdoor scenarios. The proposed approach will benefit 3D vision systems that need to operate outdoors under extreme ambient illumination levels on a limited time and power budget.</p><p>2 0.89236891 <a title="125-lda-2" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>Author: Radu Timofte, Vincent De_Smet, Luc Van_Gool</p><p>Abstract: Recently there have been significant advances in image upscaling or image super-resolution based on a dictionary of low and high resolution exemplars. The running time of the methods is often ignored despite the fact that it is a critical factor for real applications. This paper proposes fast super-resolution methods while making no compromise on quality. First, we support the use of sparse learned dictionaries in combination with neighbor embedding methods. In this case, the nearest neighbors are computed using the correlation with the dictionary atoms rather than the Euclidean distance. Moreover, we show that most of the current approaches reach top performance for the right parameters. Second, we show that using global collaborative coding has considerable speed advantages, reducing the super-resolution mapping to a precomputed projective matrix. Third, we propose the anchored neighborhood regression. That is to anchor the neighborhood embedding of a low resolution patch to the nearest atom in the dictionary and to precompute the corresponding embedding matrix. These proposals are contrasted with current state-of- the-art methods on standard images. We obtain similar or improved quality and one or two orders of magnitude speed improvements.</p><p>3 0.89090961 <a title="125-lda-3" href="./iccv-2013-Slice_Sampling_Particle_Belief_Propagation.html">395 iccv-2013-Slice Sampling Particle Belief Propagation</a></p>
<p>Author: Oliver Müller, Michael Ying Yang, Bodo Rosenhahn</p><p>Abstract: Inference in continuous label Markov random fields is a challenging task. We use particle belief propagation (PBP) for solving the inference problem in continuous label space. Sampling particles from the belief distribution is typically done by using Metropolis-Hastings (MH) Markov chain Monte Carlo (MCMC) methods which involves sampling from a proposal distribution. This proposal distribution has to be carefully designed depending on the particular model and input data to achieve fast convergence. We propose to avoid dependence on a proposal distribution by introducing a slice sampling based PBP algorithm. The proposed approach shows superior convergence performance on an image denoising toy example. Our findings are validated on a challenging relational 2D feature tracking application.</p><p>same-paper 4 0.87995744 <a title="125-lda-4" href="./iccv-2013-Drosophila_Embryo_Stage_Annotation_Using_Label_Propagation.html">125 iccv-2013-Drosophila Embryo Stage Annotation Using Label Propagation</a></p>
<p>Author: Tomáš Kazmar, Evgeny Z. Kvon, Alexander Stark, Christoph H. Lampert</p><p>Abstract: In this work we propose a system for automatic classification of Drosophila embryos into developmental stages. While the system is designed to solve an actual problem in biological research, we believe that the principle underlying it is interesting not only for biologists, but also for researchers in computer vision. The main idea is to combine two orthogonal sources of information: one is a classifier trained on strongly invariant features, which makes it applicable to images of very different conditions, but also leads to rather noisy predictions. The other is a label propagation step based on a more powerful similarity measure that however is only consistent within specific subsets of the data at a time. In our biological setup, the information sources are the shape and the staining patterns of embryo images. We show experimentally that while neither of the methods can be used by itself to achieve satisfactory results, their combination achieves prediction quality comparable to human per- formance.</p><p>5 0.85956728 <a title="125-lda-5" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>Author: Lingxi Xie, Qi Tian, Richang Hong, Shuicheng Yan, Bo Zhang</p><p>Abstract: As a special topic in computer vision, , fine-grained visual categorization (FGVC) has been attracting growing attention these years. Different with traditional image classification tasks in which objects have large inter-class variation, the visual concepts in the fine-grained datasets, such as hundreds of bird species, often have very similar semantics. Due to the large inter-class similarity, it is very difficult to classify the objects without locating really discriminative features, therefore it becomes more important for the algorithm to make full use of the part information in order to train a robust model. In this paper, we propose a powerful flowchart named Hierarchical Part Matching (HPM) to cope with finegrained classification tasks. We extend the Bag-of-Features (BoF) model by introducing several novel modules to integrate into image representation, including foreground inference and segmentation, Hierarchical Structure Learn- ing (HSL), and Geometric Phrase Pooling (GPP). We verify in experiments that our algorithm achieves the state-ofthe-art classification accuracy in the Caltech-UCSD-Birds200-2011 dataset by making full use of the ground-truth part annotations.</p><p>6 0.85838789 <a title="125-lda-6" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>7 0.85653543 <a title="125-lda-7" href="./iccv-2013-Refractive_Structure-from-Motion_on_Underwater_Images.html">348 iccv-2013-Refractive Structure-from-Motion on Underwater Images</a></p>
<p>8 0.7882461 <a title="125-lda-8" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>9 0.76247561 <a title="125-lda-9" href="./iccv-2013-Data-Driven_3D_Primitives_for_Single_Image_Understanding.html">102 iccv-2013-Data-Driven 3D Primitives for Single Image Understanding</a></p>
<p>10 0.76196015 <a title="125-lda-10" href="./iccv-2013-A_Deformable_Mixture_Parsing_Model_with_Parselets.html">8 iccv-2013-A Deformable Mixture Parsing Model with Parselets</a></p>
<p>11 0.67199957 <a title="125-lda-11" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>12 0.668006 <a title="125-lda-12" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>13 0.64886588 <a title="125-lda-13" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>14 0.62985694 <a title="125-lda-14" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>15 0.62885535 <a title="125-lda-15" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>16 0.61369115 <a title="125-lda-16" href="./iccv-2013-Uncertainty-Driven_Efficiently-Sampled_Sparse_Graphical_Models_for_Concurrent_Tumor_Segmentation_and_Atlas_Registration.html">432 iccv-2013-Uncertainty-Driven Efficiently-Sampled Sparse Graphical Models for Concurrent Tumor Segmentation and Atlas Registration</a></p>
<p>17 0.61357731 <a title="125-lda-17" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>18 0.60950804 <a title="125-lda-18" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>19 0.60910338 <a title="125-lda-19" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<p>20 0.60632008 <a title="125-lda-20" href="./iccv-2013-Bounded_Labeling_Function_for_Global_Segmentation_of_Multi-part_Objects_with_Geometric_Constraints.html">63 iccv-2013-Bounded Labeling Function for Global Segmentation of Multi-part Objects with Geometric Constraints</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
