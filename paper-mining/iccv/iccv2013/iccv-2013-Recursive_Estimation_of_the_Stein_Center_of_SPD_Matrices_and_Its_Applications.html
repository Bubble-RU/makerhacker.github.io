<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-347" href="#">iccv2013-347</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</h1>
<br/><p>Source: <a title="iccv-2013-347-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Salehian_Recursive_Estimation_of_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Hesamoddin Salehian, Guang Cheng, Baba C. Vemuri, Jeffrey Ho</p><p>Abstract: Symmetric positive-definite (SPD) matrices are ubiquitous in Computer Vision, Machine Learning and Medical Image Analysis. Finding the center/average of a population of such matrices is a common theme in many algorithms such as clustering, segmentation, principal geodesic analysis, etc. The center of a population of such matrices can be defined using a variety of distance/divergence measures as the minimizer of the sum of squared distances/divergences from the unknown center to the members of the population. It is well known that the computation of the Karcher mean for the space of SPD matrices which is a negativelycurved Riemannian manifold is computationally expensive. Recently, the LogDet divergence-based center was shown to be a computationally attractive alternative. However, the LogDet-based mean of more than two matrices can not be computed in closed form, which makes it computationally less attractive for large populations. In this paper we present a novel recursive estimator for center based on the Stein distance which is the square root of the LogDet di– vergence that is significantly faster than the batch mode computation of this center. The key theoretical contribution is a closed-form solution for the weighted Stein center of two SPD matrices, which is used in the recursive computation of the Stein center for a population of SPD matrices. Additionally, we show experimental evidence of the convergence of our recursive Stein center estimator to the batch mode Stein center. We present applications of our recursive estimator to K-means clustering and image indexing depicting significant time gains over corresponding algorithms that use the batch mode computations. For the latter application, we develop novel hashing functions using the Stein distance and apply it to publicly available data sets, and experimental results have shown favorable com– ∗This research was funded in part by the NIH grant NS066340 to BCV. †Corresponding author parisons to other competing methods.</p><p>Reference: <a title="iccv-2013-347-reference" href="../iccv2013_reference/iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Department of CISE, University of Florida Gainesville, FL 3261 1, USA  Abstract Symmetric positive-definite (SPD) matrices are ubiquitous in Computer Vision, Machine Learning and Medical Image Analysis. [sent-10, score-0.101]
</p><p>2 Finding the center/average of a population of such matrices is a common theme in many algorithms such as clustering, segmentation, principal geodesic analysis, etc. [sent-11, score-0.197]
</p><p>3 The center of a population of such matrices can be defined using a variety of distance/divergence measures as the minimizer of the sum of squared distances/divergences from the unknown center to the members of the population. [sent-12, score-0.23]
</p><p>4 It is well known that the computation of the Karcher mean for the space of SPD matrices which is a negativelycurved Riemannian manifold is computationally expensive. [sent-13, score-0.236]
</p><p>5 However, the LogDet-based mean of more than two matrices can not be computed in closed form, which makes it computationally less attractive for large populations. [sent-15, score-0.253]
</p><p>6 In this paper we present a novel recursive estimator for center based on the Stein distance which is the square root of the LogDet di–  vergence that is significantly faster than the batch mode computation of this center. [sent-16, score-0.635]
</p><p>7 The key theoretical contribution is a closed-form solution for the weighted Stein center of two SPD matrices, which is used in the recursive computation of the Stein center for a population of SPD matrices. [sent-17, score-0.512]
</p><p>8 Additionally, we show experimental evidence of the convergence of our recursive Stein center estimator to the batch mode Stein center. [sent-18, score-0.559]
</p><p>9 We present applications of our recursive estimator to K-means clustering and image indexing depicting significant time gains over corresponding algorithms that use the batch mode computations. [sent-19, score-0.575]
</p><p>10 Introduction Symmetric Positive-Definite (SPD) matrices are commonly encountered in many fields of Science and Engineering. [sent-23, score-0.115]
</p><p>11 For instance, as covariance descriptors in Computer Vision, diffusion tensors in Medical Imaging, Cauchy-Green tensors in Mechanics, metric tensors in numerous fields of Science and Technology. [sent-24, score-0.25]
</p><p>12 Finding the mean of a population of such matrices as a representative of the population is also  ×  a commonly addressed problem in numerous fields. [sent-25, score-0.308]
</p><p>13 Over the past several years, there has been a flurry of activity in finding the means of a population of such matrices due to the abundant availability of matrix-valued data in various domains e. [sent-26, score-0.17]
</p><p>14 , diffusion tensor imaging [1] and Elastography [16] in medical image analysis, covariance descriptors in computer vision [14, 4], dictionary learning on Riemannian manifolds [17, 7, 22] in machine learning, etc. [sent-28, score-0.149]
</p><p>15 It is well known that the space of n n SPD matrices equipped with the GL(n)-invariant metric is a Riemannian symmetric space [8] with negative sectional curvature [19], which we will henceforth denote by Pn. [sent-29, score-0.273]
</p><p>16 In [20], symmetrized Kullback-Liebler divergence was used to measure the similarities between SPD matrices, and the mean was computed in closed-form and applied to texture and diffusion tensor image (DTI) segmentation. [sent-36, score-0.222]
</p><p>17 More recently, in [5] the LogDet divergence was introduced and applied for tensor clustering and covariance tracking. [sent-41, score-0.176]
</p><p>18 For instance, the natural geodesic distance derived from the GL-invariant metric is GL-invariant. [sent-43, score-0.121]
</p><p>19 Among these distances/divergences, the LogDet divergence was shown to posses interesting bounding properties with regards to the natural Riemannian distance in [5] and much more computationally attractive for computing the mean. [sent-45, score-0.133]
</p><p>20 When the number of samples in the population is large and the size of SPD matrices is larger, it would be desirable to have a computationally more attractive algorithm for computing the mean using this divergence. [sent-47, score-0.309]
</p><p>21 Recursive formulation leads to considerable efficiency in mean computation, because for each new sample, all one needs to do is to update the old. [sent-49, score-0.105]
</p><p>22 Consequently, the algorithm only needs to keep track of the most recently computed mean, while computing the mean in a batch mode requires one to store all previously given samples. [sent-50, score-0.141]
</p><p>23 Thus, by using a recursive formula we can significantly reduce the time and space complexity. [sent-52, score-0.378]
</p><p>24 Recently, in [3] recursive algorithms to estimate the mean SPD matrix based on the natural GL-invariant Riemannian metric and symmetrized KL-divergence were proposed and applied to DTI segmentation. [sent-53, score-0.54]
</p><p>25 Also in [21] a recursive form of LogEuclidean based mean was introduced. [sent-54, score-0.426]
</p><p>26 In this paper we present a novel recursive algorithm for computing the mean of a set of SPD matrices, using the Stein metric. [sent-55, score-0.426]
</p><p>27 Unfortunately, the mean of SPD matrices based on the Stein metric can not be computed in a closed form, for more than two matrices [2, 5]. [sent-61, score-0.374]
</p><p>28 The computational efficiency of these iterative schemes is effected considerably especially when the number of samples and size of matrices is large. [sent-63, score-0.138]
</p><p>29 In this paper, we introduce an efficient recursive formula to compute the Stein mean. [sent-65, score-0.378]
</p><p>30 The key contributions of this paper are: (i) derivation of a closed form solution to the weighted Stein center of two matrices which is then used in the formulation of the recursive form for the Stein center estimation of more than two SPD matrices. [sent-68, score-0.55]
</p><p>31 (ii) Empirical evidence ofconvergence ofthe recursive estimator of Stein mean to the batch mode Stein mean is shown. [sent-69, score-0.667]
</p><p>32 (iii) A new hashing technique for image indexing and retrieval using covariance descriptors. [sent-70, score-0.181]
</p><p>33 (iv) Synthetic and real data experiments depicting significant gains in computation time for SPD matrix clustering and image retrieval (using covariance descriptor features), using our recursive Stein center estimator. [sent-71, score-0.552]
</p><p>34 The rest of paper is organized as follows: in Section 2 we present the recursive algorithm to find the Stein distancebased mean of a set of SPD matrices. [sent-72, score-0.426]
</p><p>35 Section 3 presents the empirical evidences of the convergence of recursive Stein mean estimator to the Stein expectation. [sent-73, score-0.526]
</p><p>36 Recursive Stein Mean Computation  The action of the general linear group of n n invertible matrices (denoted by GL(n)) on Pn defines the natural group action and is defined as follows: ∀g ∈ GL(n) ,∀X ∈ Pn, X[g] = gXgT, where T denotes the matrix transpose operation. [sent-77, score-0.136]
</p><p>37 The mean of a set of N SPD matrices based on the above Riemannian metric is called the Karcher mean, and is defined as N  X∗  = argminX? [sent-80, score-0.241]
</p><p>38 However, computation of the distance using (1), requires eigen decomposition of the matrix, which for large matrices slows down the computation considerably. [sent-84, score-0.198]
</p><p>39 This new metric is called Stein metric and is defined by, dS(A, B) =  ? [sent-93, score-0.142]
</p><p>40 Accordingly, the mean of a set of SPD tensors, based on Stein metric is defined by N  X∗  = argminX? [sent-96, score-0.14]
</p><p>41 Before turning to the recursive algorithm for computing Stein expectation, we briefly remark on the metric geometry of Pn equipped with the Stein metric. [sent-102, score-0.468]
</p><p>42 Both the Stein metric dS and the GL(n)-invariant Riemannian metric dR are GL(n)-invariant. [sent-103, score-0.142]
</p><p>43 In particular, the Stein metric is not a Riemannian metric, and more precisely, we have the following two important features of the Stein metric: •  Pn equipped with the Stein metric is not a length space, i. [sent-105, score-0.185]
</p><p>44 •  Pn equipped with the Stein metric satisfies the Reshetnyak inequality presented in [18]. [sent-113, score-0.129]
</p><p>45 However, the second feature shows that the Stein geometry has some characteristics of a negatively-curved metric space since Reshetnyak inequality is one of a few important properties satisfied by all metric (length) spaces with non-positive curvature (e. [sent-116, score-0.217]
</p><p>46 We remark that for metric spaces with non-positive curvature, there are existence and uniqueness results on the geometrically-defined expectations (similar to the Stein expectation above) [18]. [sent-119, score-0.184]
</p><p>47 Unfortunately, none of these known results are applicable to Pn endowed with the Stein metric because it is not a length space. [sent-120, score-0.102]
</p><p>48 Recursive Algorithm for Stein Expectation Having established the existence and uniqueness of the  Stein expectation, we now present a recursive algorithm for computing the same. [sent-133, score-0.39]
</p><p>49 The recursive Stein mean can be defined as M1 = X1 Mk+1 (wk+1 ) = argminM (1 wk+1 )d2S (Mk , M) +wk+1d2S(Xk+1, M)  (6)  −  (7)  where wk+1 = k+11, Mk is the old mean of k SPD matrices, Xk+1 is the new incoming sample and Mk+1 is the updated mean for k + 1matrices. [sent-137, score-0.564]
</p><p>50 Note that (7) can be thought of as a weighted Stein mean between the old mean and the new sample point, with the weight being set to be the same as in Euclidean mean update. [sent-138, score-0.207]
</p><p>51 The weighted mean of A and B, denoted by C, with the weights being wa and wb such that wa + wb = 1, should minimize (7). [sent-141, score-0.353]
</p><p>52 (7) for recursive Stein mean estimation can be rewritten as Mk+1 =  Mk[? [sent-148, score-0.426]
</p><p>53 If Pn equipped with the Stein metric were a global NonPositive Curvature (NPC) space [18], Sturm shows that Mk converges to the unique Stein expectation as k → ∞ [18]. [sent-150, score-0.158]
</p><p>54 Therefore, a proof of convergence for the recursive estimator for Stein metric-based center would be considerably more delicate and involved. [sent-152, score-0.487]
</p><p>55 However, we present empirical evidence for 100 SPD matrices randomly drawn from a log-Normal distribution to indicate that the recursive estimates of the Stein mean converge to the batch mode Stein mean (see Fig. [sent-153, score-0.668]
</p><p>56 Performance of the Recursive Stein Center To illustrate the performance of the proposed recursive algorithm, we generate 100 i. [sent-161, score-0.357]
</p><p>57 Then, we input these random samples to the recursive Stein mean estimator (RSM) and its non-recursive counterpart (SM). [sent-165, score-0.545]
</p><p>58 Error comparison for the recursive (red) versus non-  recursive (blue) Stein mean computation for data on P3. [sent-167, score-0.809]
</p><p>59 Running time comparison for the recursive (red) versus non-recursive (blue) Stein mean computation for data on P3. [sent-169, score-0.452]
</p><p>60 It can be seen that for the given 100 samples, as desired, the accuracy of the recursive and non-recursive algorithms are almost the same. [sent-175, score-0.357]
</p><p>61 This means that the recursive Stein-based mean is computationally far more efficient, especially when the  number of samples is very large and the samples are input incrementally, for example as in clustering and some segmentation algorithms. [sent-181, score-0.519]
</p><p>62 Application to K-means Clustering In this section we evaluate the performance of the proposed recursive algorithm applied to K-means clustering. [sent-184, score-0.357]
</p><p>63 The two fundamental components of the K-means algorithm at each step are: (i) distance computation and (ii) the mean update. [sent-185, score-0.118]
</p><p>64 However, due to the lack of a closed form formula for computing the Stein mean, the cluster center update is more time consuming, and to tackle this problem, we employ our recursive Stein mean estimator. [sent-187, score-0.545]
</p><p>65 More specifically, at the end of each K-means iteration, only the matrices that change their cluster memberships in previous iteration are considered. [sent-188, score-0.119]
</p><p>66 Then, each cluster center is updated only by applying the changes incurred by the matrices that most recently changed cluster memberships. [sent-189, score-0.167]
</p><p>67 There are two reasons that support the time efficiency of RSM: (i) recursive update of the Stein mean, which is achieved via the closed form expression in Eq. [sent-204, score-0.425]
</p><p>68 3 that for large datasets, the recursive Log-Euclidean, Karcher and KLs-mean methods are as slow as their non-recursive counterparts, since a substantial portion of the running time is consumed in the distance computation involved in the algorithm. [sent-209, score-0.429]
</p><p>69 It can be seen that, in all the cases, the accuracy of the RSM estimator is very close to the other competing methods, and in particular to the non-recursive Stein mean (SM) and Karcher mean (KM). [sent-212, score-0.256]
</p><p>70 Thus, in terms of accuracy, the proposed RSM estimator is as good as the best in the class but far more computationally efficient. [sent-213, score-0.126]
</p><p>71 These experiments verify that the proposed recursive method is a computationally attractive candidate for the task of Kmeans clustering in the space of SPD matrices. [sent-214, score-0.437]
</p><p>72 Application to Image Retrieval In this section, we present results of applying our recursive Stein mean estimator to the image hashing and retrieval problem. [sent-217, score-0.641]
</p><p>73 To this end, we present a novel hashing function which is a generalization of the spherical hashing applied to SPD matrices. [sent-218, score-0.18]
</p><p>74 The spherical hashing was introduced in [9] for binary encoding of large scale image databases. [sent-219, score-0.106]
</p><p>75 In this section we describe our extension of the spherical hashing technique in order to deal with SPD matrices (which are elements of a Riemannian manifold with negative sectional curvature). [sent-221, score-0.241]
</p><p>76 Given a population of SPD matrices, our hashing function is based on the distances to a set of fixed pivot points. [sent-222, score-0.195]
</p><p>77 In order to locate the pivot points we have employed the K-means clustering based on the Stein mean as discussed in Section 3. [sent-237, score-0.15]
</p><p>78 Furthermore, the radius ri is determined so that the hashing function, hi satisfies Pr[hi (X) = 1] = 21, which guarantees that each geodesic ball contains half of the samples. [sent-239, score-0.137]
</p><p>79 Based on this framework, each member of a set of (n n) SPD matrices is mapped to a binary code with length k. [sent-240, score-0.117]
</p><p>80 In order to evaluate the performance of the proposed recursive Stein mean algorithm in this image hashing context, we compare the performance for locating the pivot points by four of the K-means clustering techniques discussed in Section 3. [sent-242, score-0.581]
</p><p>81 ore, from this dataset, ten thousand 16 16 covariance matrices were extracted. [sent-256, score-0.15]
</p><p>82 As expected, it can be observed that the recursive Stein mean estimator significantly outperforms other methods, especially for longer binary codes. [sent-263, score-0.526]
</p><p>83 The recursive framework provides an efficient way to update the mean covariance matrix. [sent-264, score-0.493]
</p><p>84 Further, RKEE which is based on the GL-invariant Riemannian metric is much more computationally expensive than the recursive Stein method. [sent-265, score-0.454]
</p><p>85 It can be seen that the recursive Stein mean estimator provides almost the same accuracy as the nonrecursive Stein as well as the RKEE. [sent-268, score-0.526]
</p><p>86 Running time comparison for the K-means clustering using non-recursive Stein, Karcher, Log-Euclidean, KLs-mean denoted by SM, KM, LEM and KLsM, respectively, as well as their recursive counterparts denoted by RSM, RKEE, RLEM and RKLsM . [sent-275, score-0.386]
</p><p>87 The times taken by KM for (6 6) and (8 8) matrices were much larger than other methods (211and 332 seconds, respectively) and well beyond the range used in the plot. [sent-279, score-0.101]
</p><p>88 Running time comparison for the initialization ofhashing functions, for recursive Stein mean (RSM), non-recursive Stein mean (SM), recursive LogEuclidean mean (RLEM) and recursive Karcher expectation estimator (RKEE), over increasing binary code lengths. [sent-285, score-1.438]
</p><p>89 Finally, we combined these matrices into a single block diagonal matrix, resulting in an 8 8 covariance descriptor. [sent-293, score-0.15]
</p><p>90 Sample results returned by the proposed retrieval system based on the recursive Stein mean using 640-bits binary codes. [sent-297, score-0.48]
</p><p>91 Conclusions In this paper, we have presented a novel recursive estimator for computing the Stein center/mean for a population of SPD matrices. [sent-314, score-0.526]
</p><p>92 The key contribution here is the derivation of a closed form solution for the computation of a weighted Stein mean for two SPD matrices which is then used in developing a recursive algorithm for computing the Stein mean of a population of SPD matrices. [sent-315, score-0.723]
</p><p>93 In the absence of a proven convergence, we presented compelling empirical results demonstrating the convergence of the recursive Stein  mean estimator to the batch-mode Stein mean. [sent-316, score-0.526]
</p><p>94 Several experiments were presented showing superior performance of the recursive Stein estimator over the non-recursive counterpart as well as the recursive Karcher expectation estimator in K-means clustering and image retrieval. [sent-317, score-1.003]
</p><p>95 Another key contribution of this work is the design of hashing functions for the image retrieval application using covariance descriptors as features. [sent-318, score-0.164]
</p><p>96 Our future work will be focused on several new theoretical and practical aspects of the recursive estimator presented here. [sent-319, score-0.457]
</p><p>97 Means of hermitian positivedefinite matrices based on the log-determinant divergence function. [sent-329, score-0.145]
</p><p>98 Efficient recursive algorithms for computing the mean diffusion tensor and applications to DTI segmentation. [sent-337, score-0.512]
</p><p>99 A novel dynamic system in the space of SPD matrices with applications to appearance tracking. [sent-343, score-0.101]
</p><p>100 Efficient similarity search for covariance matrices via the JB LogDet divergence. [sent-348, score-0.15]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stein', 0.724), ('spd', 0.367), ('recursive', 0.357), ('rsm', 0.17), ('pn', 0.109), ('karcher', 0.102), ('matrices', 0.101), ('estimator', 0.1), ('jbld', 0.09), ('riemannian', 0.087), ('wb', 0.085), ('logdet', 0.083), ('mk', 0.081), ('hashing', 0.074), ('metric', 0.071), ('mean', 0.069), ('population', 0.069), ('rkee', 0.068), ('expectation', 0.06), ('wa', 0.057), ('tensor', 0.054), ('pivot', 0.052), ('gl', 0.051), ('covariance', 0.049), ('divergence', 0.044), ('retrieval', 0.041), ('batch', 0.04), ('ufl', 0.04), ('logeuclidean', 0.037), ('dti', 0.036), ('sm', 0.036), ('dld', 0.034), ('rlem', 0.034), ('spherical', 0.032), ('mode', 0.032), ('diffusion', 0.032), ('closed', 0.032), ('wk', 0.031), ('center', 0.03), ('clustering', 0.029), ('symmetric', 0.028), ('tensors', 0.028), ('vemuri', 0.028), ('geodesic', 0.027), ('equipped', 0.027), ('curvature', 0.026), ('argminx', 0.026), ('computationally', 0.026), ('computation', 0.026), ('attractive', 0.025), ('km', 0.024), ('running', 0.023), ('distance', 0.023), ('elastography', 0.023), ('klsm', 0.023), ('reshetnyak', 0.023), ('rklsm', 0.023), ('salehian', 0.023), ('symmetrized', 0.023), ('eigen', 0.022), ('formula', 0.021), ('ds', 0.021), ('hi', 0.021), ('triangle', 0.02), ('nonpositive', 0.02), ('sectional', 0.02), ('spaces', 0.02), ('uniqueness', 0.02), ('matrix', 0.02), ('hamming', 0.019), ('samples', 0.019), ('npc', 0.019), ('estimators', 0.019), ('update', 0.018), ('efficiency', 0.018), ('competing', 0.018), ('cluster', 0.018), ('indexing', 0.017), ('aforementioned', 0.017), ('shortest', 0.017), ('retrieved', 0.016), ('length', 0.016), ('inequality', 0.016), ('invertible', 0.015), ('endowed', 0.015), ('clusters', 0.015), ('regards', 0.015), ('radii', 0.015), ('xk', 0.015), ('satisfies', 0.015), ('manifold', 0.014), ('medical', 0.014), ('fields', 0.014), ('square', 0.014), ('cheng', 0.014), ('returned', 0.013), ('root', 0.013), ('geometry', 0.013), ('existence', 0.013), ('lem', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="347-tfidf-1" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>Author: Hesamoddin Salehian, Guang Cheng, Baba C. Vemuri, Jeffrey Ho</p><p>Abstract: Symmetric positive-definite (SPD) matrices are ubiquitous in Computer Vision, Machine Learning and Medical Image Analysis. Finding the center/average of a population of such matrices is a common theme in many algorithms such as clustering, segmentation, principal geodesic analysis, etc. The center of a population of such matrices can be defined using a variety of distance/divergence measures as the minimizer of the sum of squared distances/divergences from the unknown center to the members of the population. It is well known that the computation of the Karcher mean for the space of SPD matrices which is a negativelycurved Riemannian manifold is computationally expensive. Recently, the LogDet divergence-based center was shown to be a computationally attractive alternative. However, the LogDet-based mean of more than two matrices can not be computed in closed form, which makes it computationally less attractive for large populations. In this paper we present a novel recursive estimator for center based on the Stein distance which is the square root of the LogDet di– vergence that is significantly faster than the batch mode computation of this center. The key theoretical contribution is a closed-form solution for the weighted Stein center of two SPD matrices, which is used in the recursive computation of the Stein center for a population of SPD matrices. Additionally, we show experimental evidence of the convergence of our recursive Stein center estimator to the batch mode Stein center. We present applications of our recursive estimator to K-means clustering and image indexing depicting significant time gains over corresponding algorithms that use the batch mode computations. For the latter application, we develop novel hashing functions using the Stein distance and apply it to publicly available data sets, and experimental results have shown favorable com– ∗This research was funded in part by the NIH grant NS066340 to BCV. †Corresponding author parisons to other competing methods.</p><p>2 0.37045458 <a title="347-tfidf-2" href="./iccv-2013-Log-Euclidean_Kernels_for_Sparse_Representation_and_Dictionary_Learning.html">257 iccv-2013-Log-Euclidean Kernels for Sparse Representation and Dictionary Learning</a></p>
<p>Author: Peihua Li, Qilong Wang, Wangmeng Zuo, Lei Zhang</p><p>Abstract: The symmetric positive de?nite (SPD) matrices have been widely used in image and vision problems. Recently there are growing interests in studying sparse representation (SR) of SPD matrices, motivated by the great success of SR for vector data. Though the space of SPD matrices is well-known to form a Lie group that is a Riemannian manifold, existing work fails to take full advantage of its geometric structure. This paper attempts to tackle this problem by proposing a kernel based method for SR and dictionary learning (DL) of SPD matrices. We disclose that the space of SPD matrices, with the operations of logarithmic multiplication and scalar logarithmic multiplication de?ned in the Log-Euclidean framework, is a complete inner product space. We can thus develop a broad family of kernels that satis?es Mercer’s condition. These kernels characterize the geodesic distance and can be computed ef?ciently. We also consider the geometric structure in the DL process by updating atom matrices in the Riemannian space instead of in the Euclidean space. The proposed method is evaluated with various vision problems and shows notable per- formance gains over state-of-the-arts.</p><p>3 0.073534898 <a title="347-tfidf-3" href="./iccv-2013-A_Novel_Earth_Mover%27s_Distance_Methodology_for_Image_Matching_with_Gaussian_Mixture_Models.html">25 iccv-2013-A Novel Earth Mover's Distance Methodology for Image Matching with Gaussian Mixture Models</a></p>
<p>Author: Peihua Li, Qilong Wang, Lei Zhang</p><p>Abstract: The similarity or distance measure between Gaussian mixture models (GMMs) plays a crucial role in contentbased image matching. Though the Earth Mover’s Distance (EMD) has shown its advantages in matching histogram features, its potentials in matching GMMs remain unclear and are not fully explored. To address this problem, we propose a novel EMD methodology for GMM matching. We ?rst present a sparse representation based EMD called SR-EMD by exploiting the sparse property of the underlying problem. SR-EMD is more ef?cient and robust than the conventional EMD. Second, we present two novel ground distances between component Gaussians based on the information geometry. The perspective from the Riemannian geometry distinguishes the proposed ground distances from the classical entropy- or divergence-based ones. Furthermore, motivated by the success of distance metric learning of vector data, we make the ?rst attempt to learn the EMD distance metrics between GMMs by using a simple yet effective supervised pair-wise based method. It can adapt the distance metrics between GMMs to speci?c classi?ca- tion tasks. The proposed method is evaluated on both simulated data and benchmark real databases and achieves very promising performance.</p><p>4 0.069203094 <a title="347-tfidf-4" href="./iccv-2013-Dictionary_Learning_and_Sparse_Coding_on_Grassmann_Manifolds%3A_An_Extrinsic_Solution.html">114 iccv-2013-Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution</a></p>
<p>Author: Mehrtash Harandi, Conrad Sanderson, Chunhua Shen, Brian Lovell</p><p>Abstract: Recent advances in computer vision and machine learning suggest that a wide range of problems can be addressed more appropriately by considering non-Euclidean geometry. In this paper we explore sparse dictionary learning over the space of linear subspaces, which form Riemannian structures known as Grassmann manifolds. To this end, we propose to embed Grassmann manifolds into the space of symmetric matrices by an isometric mapping, which enables us to devise a closed-form solution for updating a Grassmann dictionary, atom by atom. Furthermore, to handle non-linearity in data, we propose a kernelised version of the dictionary learning algorithm. Experiments on several classification tasks (face recognition, action recognition, dynamic texture classification) show that the proposed approach achieves considerable improvements in discrimination accuracy, in comparison to state-of-the-art methods such as kernelised Affine Hull Method and graphembedding Grassmann discriminant analysis.</p><p>5 0.060906913 <a title="347-tfidf-5" href="./iccv-2013-Quantize_and_Conquer%3A_A_Dimensionality-Recursive_Solution_to_Clustering%2C_Vector_Quantization%2C_and_Image_Retrieval.html">333 iccv-2013-Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval</a></p>
<p>Author: Yannis Avrithis</p><p>Abstract: Inspired by the close relation between nearest neighbor search and clustering in high-dimensional spaces as well as the success of one helping to solve the other, we introduce a new paradigm where both problems are solved simultaneously. Our solution is recursive, not in the size of input data but in the number of dimensions. One result is a clustering algorithm that is tuned to small codebooks but does not need all data in memory at the same time and is practically constant in the data size. As a by-product, a tree structure performs either exact or approximate quantization on trained centroids, the latter being not very precise but extremely fast. A lesser contribution is a new indexing scheme for image retrieval that exploits multiple small codebooks to provide an arbitrarily fine partition of the descriptor space. Large scale experiments on public datasets exhibit state of the art performance and remarkable generalization.</p><p>6 0.060744751 <a title="347-tfidf-6" href="./iccv-2013-Discriminant_Tracking_Using_Tensor_Representation_with_Semi-supervised_Improvement.html">119 iccv-2013-Discriminant Tracking Using Tensor Representation with Semi-supervised Improvement</a></p>
<p>7 0.059210677 <a title="347-tfidf-7" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>8 0.057509106 <a title="347-tfidf-8" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>9 0.051796541 <a title="347-tfidf-9" href="./iccv-2013-Automatic_Kronecker_Product_Model_Based_Detection_of_Repeated_Patterns_in_2D_Urban_Images.html">55 iccv-2013-Automatic Kronecker Product Model Based Detection of Repeated Patterns in 2D Urban Images</a></p>
<p>10 0.049338404 <a title="347-tfidf-10" href="./iccv-2013-Curvature-Aware_Regularization_on_Riemannian_Submanifolds.html">100 iccv-2013-Curvature-Aware Regularization on Riemannian Submanifolds</a></p>
<p>11 0.045925021 <a title="347-tfidf-11" href="./iccv-2013-Parallel_Transport_of_Deformations_in_Shape_Space_of_Elastic_Surfaces.html">307 iccv-2013-Parallel Transport of Deformations in Shape Space of Elastic Surfaces</a></p>
<p>12 0.04427249 <a title="347-tfidf-12" href="./iccv-2013-A_General_Two-Step_Approach_to_Learning-Based_Hashing.html">13 iccv-2013-A General Two-Step Approach to Learning-Based Hashing</a></p>
<p>13 0.041132983 <a title="347-tfidf-13" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>14 0.038946606 <a title="347-tfidf-14" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>15 0.038048778 <a title="347-tfidf-15" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>16 0.035699047 <a title="347-tfidf-16" href="./iccv-2013-Total_Variation_Regularization_for_Functions_with_Values_in_a_Manifold.html">421 iccv-2013-Total Variation Regularization for Functions with Values in a Manifold</a></p>
<p>17 0.03477975 <a title="347-tfidf-17" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>18 0.034708258 <a title="347-tfidf-18" href="./iccv-2013-Robust_Tucker_Tensor_Decomposition_for_Effective_Image_Representation.html">362 iccv-2013-Robust Tucker Tensor Decomposition for Effective Image Representation</a></p>
<p>19 0.034378748 <a title="347-tfidf-19" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>20 0.033580326 <a title="347-tfidf-20" href="./iccv-2013-On_the_Mean_Curvature_Flow_on_Graphs_with_Applications_in_Image_and_Manifold_Processing.html">296 iccv-2013-On the Mean Curvature Flow on Graphs with Applications in Image and Manifold Processing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.081), (1, 0.007), (2, -0.034), (3, -0.032), (4, -0.054), (5, 0.065), (6, -0.006), (7, 0.005), (8, -0.01), (9, -0.014), (10, -0.031), (11, -0.008), (12, -0.012), (13, -0.019), (14, 0.056), (15, 0.02), (16, 0.012), (17, -0.01), (18, 0.006), (19, -0.007), (20, 0.031), (21, 0.044), (22, 0.05), (23, -0.015), (24, -0.042), (25, 0.061), (26, 0.041), (27, 0.046), (28, -0.064), (29, 0.126), (30, -0.088), (31, -0.05), (32, -0.079), (33, 0.033), (34, 0.106), (35, 0.168), (36, 0.103), (37, 0.055), (38, -0.141), (39, 0.057), (40, 0.02), (41, 0.033), (42, 0.063), (43, 0.069), (44, -0.016), (45, -0.025), (46, -0.115), (47, -0.018), (48, 0.007), (49, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9288162 <a title="347-lsi-1" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>Author: Hesamoddin Salehian, Guang Cheng, Baba C. Vemuri, Jeffrey Ho</p><p>Abstract: Symmetric positive-definite (SPD) matrices are ubiquitous in Computer Vision, Machine Learning and Medical Image Analysis. Finding the center/average of a population of such matrices is a common theme in many algorithms such as clustering, segmentation, principal geodesic analysis, etc. The center of a population of such matrices can be defined using a variety of distance/divergence measures as the minimizer of the sum of squared distances/divergences from the unknown center to the members of the population. It is well known that the computation of the Karcher mean for the space of SPD matrices which is a negativelycurved Riemannian manifold is computationally expensive. Recently, the LogDet divergence-based center was shown to be a computationally attractive alternative. However, the LogDet-based mean of more than two matrices can not be computed in closed form, which makes it computationally less attractive for large populations. In this paper we present a novel recursive estimator for center based on the Stein distance which is the square root of the LogDet di– vergence that is significantly faster than the batch mode computation of this center. The key theoretical contribution is a closed-form solution for the weighted Stein center of two SPD matrices, which is used in the recursive computation of the Stein center for a population of SPD matrices. Additionally, we show experimental evidence of the convergence of our recursive Stein center estimator to the batch mode Stein center. We present applications of our recursive estimator to K-means clustering and image indexing depicting significant time gains over corresponding algorithms that use the batch mode computations. For the latter application, we develop novel hashing functions using the Stein distance and apply it to publicly available data sets, and experimental results have shown favorable com– ∗This research was funded in part by the NIH grant NS066340 to BCV. †Corresponding author parisons to other competing methods.</p><p>2 0.81988645 <a title="347-lsi-2" href="./iccv-2013-Log-Euclidean_Kernels_for_Sparse_Representation_and_Dictionary_Learning.html">257 iccv-2013-Log-Euclidean Kernels for Sparse Representation and Dictionary Learning</a></p>
<p>Author: Peihua Li, Qilong Wang, Wangmeng Zuo, Lei Zhang</p><p>Abstract: The symmetric positive de?nite (SPD) matrices have been widely used in image and vision problems. Recently there are growing interests in studying sparse representation (SR) of SPD matrices, motivated by the great success of SR for vector data. Though the space of SPD matrices is well-known to form a Lie group that is a Riemannian manifold, existing work fails to take full advantage of its geometric structure. This paper attempts to tackle this problem by proposing a kernel based method for SR and dictionary learning (DL) of SPD matrices. We disclose that the space of SPD matrices, with the operations of logarithmic multiplication and scalar logarithmic multiplication de?ned in the Log-Euclidean framework, is a complete inner product space. We can thus develop a broad family of kernels that satis?es Mercer’s condition. These kernels characterize the geodesic distance and can be computed ef?ciently. We also consider the geometric structure in the DL process by updating atom matrices in the Riemannian space instead of in the Euclidean space. The proposed method is evaluated with various vision problems and shows notable per- formance gains over state-of-the-arts.</p><p>3 0.7246061 <a title="347-lsi-3" href="./iccv-2013-A_Novel_Earth_Mover%27s_Distance_Methodology_for_Image_Matching_with_Gaussian_Mixture_Models.html">25 iccv-2013-A Novel Earth Mover's Distance Methodology for Image Matching with Gaussian Mixture Models</a></p>
<p>Author: Peihua Li, Qilong Wang, Lei Zhang</p><p>Abstract: The similarity or distance measure between Gaussian mixture models (GMMs) plays a crucial role in contentbased image matching. Though the Earth Mover’s Distance (EMD) has shown its advantages in matching histogram features, its potentials in matching GMMs remain unclear and are not fully explored. To address this problem, we propose a novel EMD methodology for GMM matching. We ?rst present a sparse representation based EMD called SR-EMD by exploiting the sparse property of the underlying problem. SR-EMD is more ef?cient and robust than the conventional EMD. Second, we present two novel ground distances between component Gaussians based on the information geometry. The perspective from the Riemannian geometry distinguishes the proposed ground distances from the classical entropy- or divergence-based ones. Furthermore, motivated by the success of distance metric learning of vector data, we make the ?rst attempt to learn the EMD distance metrics between GMMs by using a simple yet effective supervised pair-wise based method. It can adapt the distance metrics between GMMs to speci?c classi?ca- tion tasks. The proposed method is evaluated on both simulated data and benchmark real databases and achieves very promising performance.</p><p>4 0.60992014 <a title="347-lsi-4" href="./iccv-2013-Discriminant_Tracking_Using_Tensor_Representation_with_Semi-supervised_Improvement.html">119 iccv-2013-Discriminant Tracking Using Tensor Representation with Semi-supervised Improvement</a></p>
<p>Author: Jin Gao, Junliang Xing, Weiming Hu, Steve Maybank</p><p>Abstract: Visual tracking has witnessed growing methods in object representation, which is crucial to robust tracking. The dominant mechanism in object representation is using image features encoded in a vector as observations to perform tracking, without considering that an image is intrinsically a matrix, or a 2nd-order tensor. Thus approaches following this mechanism inevitably lose a lot of useful information, and therefore cannot fully exploit the spatial correlations within the 2D image ensembles. In this paper, we address an image as a 2nd-order tensor in its original form, and find a discriminative linear embedding space approximation to the original nonlinear submanifold embedded in the tensor space based on the graph embedding framework. We specially design two graphs for characterizing the intrinsic local geometrical structure of the tensor space, so as to retain more discriminant information when reducing the dimension along certain tensor dimensions. However, spatial correlations within a tensor are not limited to the elements along these dimensions. This means that some part of the discriminant information may not be encoded in the embedding space. We introduce a novel technique called semi-supervised improvement to iteratively adjust the embedding space to compensate for the loss of discriminant information, hence improving the performance of our tracker. Experimental results on challenging videos demonstrate the effectiveness and robustness of the proposed tracker.</p><p>5 0.56568408 <a title="347-lsi-5" href="./iccv-2013-Robust_Tucker_Tensor_Decomposition_for_Effective_Image_Representation.html">362 iccv-2013-Robust Tucker Tensor Decomposition for Effective Image Representation</a></p>
<p>Author: Miao Zhang, Chris Ding</p><p>Abstract: Many tensor based algorithms have been proposed for the study of high dimensional data in a large variety ofcomputer vision and machine learning applications. However, most of the existing tensor analysis approaches are based on Frobenius norm, which makes them sensitive to outliers, because they minimize the sum of squared errors and enlarge the influence of both outliers and large feature noises. In this paper, we propose a robust Tucker tensor decomposition model (RTD) to suppress the influence of outliers, which uses L1-norm loss function. Yet, the optimization on L1-norm based tensor analysis is much harder than standard tensor decomposition. In this paper, we propose a simple and efficient algorithm to solve our RTD model. Moreover, tensor factorization-based image storage needs much less space than PCA based methods. We carry out extensive experiments to evaluate the proposed algorithm, and verify the robustness against image occlusions. Both numerical and visual results show that our RTD model is consistently better against the existence of outliers than previous tensor and PCA methods.</p><p>6 0.46288407 <a title="347-lsi-6" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>7 0.44975331 <a title="347-lsi-7" href="./iccv-2013-Automatic_Kronecker_Product_Model_Based_Detection_of_Repeated_Patterns_in_2D_Urban_Images.html">55 iccv-2013-Automatic Kronecker Product Model Based Detection of Repeated Patterns in 2D Urban Images</a></p>
<p>8 0.44940382 <a title="347-lsi-8" href="./iccv-2013-Parallel_Transport_of_Deformations_in_Shape_Space_of_Elastic_Surfaces.html">307 iccv-2013-Parallel Transport of Deformations in Shape Space of Elastic Surfaces</a></p>
<p>9 0.42253482 <a title="347-lsi-9" href="./iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning.html">212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</a></p>
<p>10 0.41599685 <a title="347-lsi-10" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>11 0.40809661 <a title="347-lsi-11" href="./iccv-2013-Dictionary_Learning_and_Sparse_Coding_on_Grassmann_Manifolds%3A_An_Extrinsic_Solution.html">114 iccv-2013-Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution</a></p>
<p>12 0.39841411 <a title="347-lsi-12" href="./iccv-2013-From_Point_to_Set%3A_Extend_the_Learning_of_Distance_Metrics.html">177 iccv-2013-From Point to Set: Extend the Learning of Distance Metrics</a></p>
<p>13 0.3900117 <a title="347-lsi-13" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>14 0.38454804 <a title="347-lsi-14" href="./iccv-2013-Efficient_Higher-Order_Clustering_on_the_Grassmann_Manifold.html">134 iccv-2013-Efficient Higher-Order Clustering on the Grassmann Manifold</a></p>
<p>15 0.38350046 <a title="347-lsi-15" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>16 0.37468222 <a title="347-lsi-16" href="./iccv-2013-Total_Variation_Regularization_for_Functions_with_Values_in_a_Manifold.html">421 iccv-2013-Total Variation Regularization for Functions with Values in a Manifold</a></p>
<p>17 0.371829 <a title="347-lsi-17" href="./iccv-2013-Curvature-Aware_Regularization_on_Riemannian_Submanifolds.html">100 iccv-2013-Curvature-Aware Regularization on Riemannian Submanifolds</a></p>
<p>18 0.33287847 <a title="347-lsi-18" href="./iccv-2013-Joint_Learning_of_Discriminative_Prototypes_and_Large_Margin_Nearest_Neighbor_Classifiers.html">222 iccv-2013-Joint Learning of Discriminative Prototypes and Large Margin Nearest Neighbor Classifiers</a></p>
<p>19 0.33044285 <a title="347-lsi-19" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<p>20 0.3301523 <a title="347-lsi-20" href="./iccv-2013-Quantize_and_Conquer%3A_A_Dimensionality-Recursive_Solution_to_Clustering%2C_Vector_Quantization%2C_and_Image_Retrieval.html">333 iccv-2013-Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.08), (7, 0.039), (13, 0.016), (26, 0.053), (27, 0.013), (31, 0.029), (42, 0.094), (64, 0.015), (73, 0.034), (89, 0.116), (95, 0.036), (97, 0.337)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80526054 <a title="347-lda-1" href="./iccv-2013-Saliency_and_Human_Fixations%3A_State-of-the-Art_and_Study_of_Comparison_Metrics.html">373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</a></p>
<p>Author: Nicolas Riche, Matthieu Duvinage, Matei Mancas, Bernard Gosselin, Thierry Dutoit</p><p>Abstract: Visual saliency has been an increasingly active research area in the last ten years with dozens of saliency models recently published. Nowadays, one of the big challenges in the field is to find a way to fairly evaluate all of these models. In this paper, on human eye fixations ,we compare the ranking of 12 state-of-the art saliency models using 12 similarity metrics. The comparison is done on Jian Li ’s database containing several hundreds of natural images. Based on Kendall concordance coefficient, it is shown that some of the metrics are strongly correlated leading to a redundancy in the performance metrics reported in the available benchmarks. On the other hand, other metrics provide a more diverse picture of models ’ overall performance. As a recommendation, three similarity metrics should be used to obtain a complete point of view of saliency model performance.</p><p>same-paper 2 0.72745419 <a title="347-lda-2" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>Author: Hesamoddin Salehian, Guang Cheng, Baba C. Vemuri, Jeffrey Ho</p><p>Abstract: Symmetric positive-definite (SPD) matrices are ubiquitous in Computer Vision, Machine Learning and Medical Image Analysis. Finding the center/average of a population of such matrices is a common theme in many algorithms such as clustering, segmentation, principal geodesic analysis, etc. The center of a population of such matrices can be defined using a variety of distance/divergence measures as the minimizer of the sum of squared distances/divergences from the unknown center to the members of the population. It is well known that the computation of the Karcher mean for the space of SPD matrices which is a negativelycurved Riemannian manifold is computationally expensive. Recently, the LogDet divergence-based center was shown to be a computationally attractive alternative. However, the LogDet-based mean of more than two matrices can not be computed in closed form, which makes it computationally less attractive for large populations. In this paper we present a novel recursive estimator for center based on the Stein distance which is the square root of the LogDet di– vergence that is significantly faster than the batch mode computation of this center. The key theoretical contribution is a closed-form solution for the weighted Stein center of two SPD matrices, which is used in the recursive computation of the Stein center for a population of SPD matrices. Additionally, we show experimental evidence of the convergence of our recursive Stein center estimator to the batch mode Stein center. We present applications of our recursive estimator to K-means clustering and image indexing depicting significant time gains over corresponding algorithms that use the batch mode computations. For the latter application, we develop novel hashing functions using the Stein distance and apply it to publicly available data sets, and experimental results have shown favorable com– ∗This research was funded in part by the NIH grant NS066340 to BCV. †Corresponding author parisons to other competing methods.</p><p>3 0.65706259 <a title="347-lda-3" href="./iccv-2013-Synergistic_Clustering_of_Image_and_Segment_Descriptors_for_Unsupervised_Scene_Understanding.html">412 iccv-2013-Synergistic Clustering of Image and Segment Descriptors for Unsupervised Scene Understanding</a></p>
<p>Author: Daniel M. Steinberg, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: With the advent of cheap, high fidelity, digital imaging systems, the quantity and rate of generation of visual data can dramatically outpace a humans ability to label or annotate it. In these situations there is scope for the use of unsupervised approaches that can model these datasets and automatically summarise their content. To this end, we present a totally unsupervised, and annotation-less, model for scene understanding. This model can simultaneously cluster whole-image and segment descriptors, therebyforming an unsupervised model of scenes and objects. We show that this model outperforms other unsupervised models that can only cluster one source of information (image or segment) at once. We are able to compare unsupervised and supervised techniques using standard measures derived from confusion matrices and contingency tables. This shows that our unsupervised model is competitive with current supervised and weakly-supervised models for scene understanding on standard datasets. We also demonstrate our model operating on a dataset with more than 100,000 images col- lected by an autonomous underwater vehicle.</p><p>4 0.64062166 <a title="347-lda-4" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>Author: Zheyun Feng, Rong Jin, Anil Jain</p><p>Abstract: One of the key challenges in search-based image annotation models is to define an appropriate similarity measure between images. Many kernel distance metric learning (KML) algorithms have been developed in order to capture the nonlinear relationships between visual features and semantics ofthe images. Onefundamental limitation in applying KML to image annotation is that it requires converting image annotations into binary constraints, leading to a significant information loss. In addition, most KML algorithms suffer from high computational cost due to the requirement that the learned matrix has to be positive semi-definitive (PSD). In this paper, we propose a robust kernel metric learning (RKML) algorithm based on the regression technique that is able to directly utilize image annotations. The proposed method is also computationally more efficient because PSD property is automatically ensured by regression. We provide the theoretical guarantee for the proposed algorithm, and verify its efficiency and effectiveness for image annotation by comparing it to state-of-the-art approaches for both distance metric learning and image annotation. ,</p><p>5 0.61823606 <a title="347-lda-5" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>Author: Zhaowen Wang, Jianchao Yang, Nasser Nasrabadi, Thomas Huang</p><p>Abstract: Sparse Representation-based Classification (SRC) is a powerful tool in distinguishing signal categories which lie on different subspaces. Despite its wide application to visual recognition tasks, current understanding of SRC is solely based on a reconstructive perspective, which neither offers any guarantee on its classification performance nor provides any insight on how to design a discriminative dictionary for SRC. In this paper, we present a novel perspective towards SRC and interpret it as a margin classifier. The decision boundary and margin of SRC are analyzed in local regions where the support of sparse code is stable. Based on the derived margin, we propose a hinge loss function as the gauge for the classification performance of SRC. A stochastic gradient descent algorithm is implemented to maximize the margin of SRC and obtain more discriminative dictionaries. Experiments validate the effectiveness of the proposed approach in predicting classification performance and improving dictionary quality over reconstructive ones. Classification results competitive with other state-ofthe-art sparse coding methods are reported on several data sets.</p><p>6 0.61253709 <a title="347-lda-6" href="./iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction.html">50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</a></p>
<p>7 0.61198878 <a title="347-lda-7" href="./iccv-2013-Saliency_Detection_via_Dense_and_Sparse_Reconstruction.html">372 iccv-2013-Saliency Detection via Dense and Sparse Reconstruction</a></p>
<p>8 0.58215576 <a title="347-lda-8" href="./iccv-2013-Saliency_Detection%3A_A_Boolean_Map_Approach.html">369 iccv-2013-Saliency Detection: A Boolean Map Approach</a></p>
<p>9 0.5800854 <a title="347-lda-9" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>10 0.55448419 <a title="347-lda-10" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>11 0.54521751 <a title="347-lda-11" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>12 0.54397392 <a title="347-lda-12" href="./iccv-2013-Saliency_Detection_via_Absorbing_Markov_Chain.html">371 iccv-2013-Saliency Detection via Absorbing Markov Chain</a></p>
<p>13 0.49824232 <a title="347-lda-13" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>14 0.49717933 <a title="347-lda-14" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>15 0.49682003 <a title="347-lda-15" href="./iccv-2013-Log-Euclidean_Kernels_for_Sparse_Representation_and_Dictionary_Learning.html">257 iccv-2013-Log-Euclidean Kernels for Sparse Representation and Dictionary Learning</a></p>
<p>16 0.49658459 <a title="347-lda-16" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>17 0.48501521 <a title="347-lda-17" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>18 0.48485795 <a title="347-lda-18" href="./iccv-2013-Semantically-Based_Human_Scanpath_Estimation_with_HMMs.html">381 iccv-2013-Semantically-Based Human Scanpath Estimation with HMMs</a></p>
<p>19 0.48245323 <a title="347-lda-19" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>20 0.48189002 <a title="347-lda-20" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
