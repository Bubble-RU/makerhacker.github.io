<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-266" href="#">iccv2013-266</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</h1>
<br/><p>Source: <a title="iccv-2013-266-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Fernando_Mining_Multiple_Queries_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Basura Fernando, Tinne Tuytelaars</p><p>Abstract: In this paper we present a new method for object retrieval starting from multiple query images. The use of multiple queries allows for a more expressive formulation of the query object including, e.g., different viewpoints and/or viewing conditions. This, in turn, leads to more diverse and more accurate retrieval results. When no query images are available to the user, they can easily be retrieved from the internet using a standard image search engine. In particular, we propose a new method based on pattern mining. Using the minimal description length principle, we derive the most suitable set of patterns to describe the query object, with patterns corresponding to local feature configurations. This results in apowerful object-specific mid-level image representation. The archive can then be searched efficiently for similar images based on this representation, using a combination of two inverted file systems. Since the patterns already encode local spatial information, good results on several standard image retrieval datasets are obtained even without costly re-ranking based on geometric verification.</p><p>Reference: <a title="iccv-2013-266-reference" href="../iccv2013_reference/iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 be  Abstract In this paper we present a new method for object retrieval starting from multiple query images. [sent-4, score-0.872]
</p><p>2 The use of multiple queries allows for a more expressive formulation of the query object including, e. [sent-5, score-0.77]
</p><p>3 When no query images are available to the user, they can easily be retrieved from the internet using a standard image search engine. [sent-9, score-0.754]
</p><p>4 Using the minimal description length principle, we derive the most suitable set of patterns to describe the query object, with patterns corresponding to local feature configurations. [sent-11, score-1.151]
</p><p>5 In this context, good results have been reported using bag-of-visual-words based methods [3, 5, 16] following the query by example paradigm. [sent-20, score-0.652]
</p><p>6 Multi-query examples with different viewing conditions  for query Eiffel tower and London bridge. [sent-22, score-0.751]
</p><p>7 The result is a ranked list, with images ranked according to their similarity to the given query image. [sent-24, score-0.823]
</p><p>8 The question then arises: How to select a good query image? [sent-25, score-0.652]
</p><p>9 Which images are retrieved then depends, to a large extent, on the selected query image. [sent-29, score-0.727]
</p><p>10 The image retrieval and computer vision communities have gained considerable progress in this direction with the help of affine invariant representations, domain adaptation techniques, and ‘tricks’ such as query expansion [3, 5, 6, 18]. [sent-31, score-1.02]
</p><p>11 By specifying multiple query images, the user can provide more information to the system about the specific object he wants to find without the need to select the best one. [sent-37, score-0.732]
</p><p>12 Indeed, by 22554444  leveraging the power of internet image search engines such as Google Images, a textual query can be used to retrieve a set of images from the internet. [sent-39, score-0.799]
</p><p>13 Given the variety in viewing conditions, different aspects, as well as image clutter, the challenge then is for the system to figure out, purely based on the provided query images, What exactly is the object of interest? [sent-45, score-0.738]
</p><p>14 [2, 12]) do not try to answer this question, yet simply perform a search based on one query image at a time and then fuse the results, or fall back to rather simple schemes to merge the features from the different query images (see also section 2 on related work). [sent-49, score-1.333]
</p><p>15 In contrast, we rely on an unsupervised pattern mining method that builds an object-specific mid-level representation that best models the query images based on the minimal description length principle (MDL) [10]. [sent-50, score-1.158]
</p><p>16 Using these local his-  tograms, we mine the query images to discover a set of midlevel patterns that best explain the query images. [sent-52, score-1.687]
</p><p>17 (2) Our method is robust to noisy images as it explores the regularity in the query images using the MDL principle. [sent-63, score-0.732]
</p><p>18 Our major contributions are as follows: (1) We present a multiple query based image retrieval method that systematically outperforms other MQIR methods even without costly spatial verification. [sent-66, score-0.927]
</p><p>19 (2) We adapt a minimum description length based pattern mining algorithm to discover local –  structural patterns for specific object retrieval. [sent-67, score-0.691]
</p><p>20 An example query from our new dataset is shown in the first row and retrieved results using our method are shown in the next two rows. [sent-69, score-0.698]
</p><p>21 (3) We present a novel query expansion method based on pattern mining which we call pattern based query expansion (PQE). [sent-71, score-2.077]
</p><p>22 Then we present our pattern based multiple query retrieval method in section 3. [sent-75, score-0.936]
</p><p>23 Related Work Single query methods Several methods for large scale image retrieval starting from a single query have been pro-  posed [7, 13, 15, 17, 23]. [sent-78, score-1.486]
</p><p>24 They almost invariably consist of the following components: bag-of-words creation, indexing, query expansion, and geometric verification. [sent-79, score-0.67]
</p><p>25 In query expansion [3, 5, 6] (QE), the original query image is replaced with a more representative set of images constructed based on the top ranked images. [sent-80, score-1.587]
</p><p>26 This, in effect, turns the single query retrieval problem into a multiple query retrieval one. [sent-81, score-1.637]
</p><p>27 Multiple query methods and textual queries In [27] a query specific feature fusion method is proposed. [sent-91, score-1.461]
</p><p>28 [12], a textual query based image retrieval method is 22554455  presented that uses labeled positive/negative images down-  loaded from the internet to train a classifier. [sent-95, score-0.918]
</p><p>29 In contrast with all these methods, to the best of our knowledge we are the first to mine query images to discover object-specific patterns to build mid-level features to retrieve images. [sent-101, score-1.005]
</p><p>30 We mine for patterns specific for an object using small number of query images. [sent-105, score-0.94]
</p><p>31 To this end, we use the top-k patterns that best represent the query object where the best patterns are selected using a crude MDL principle [10]. [sent-112, score-1.093]
</p><p>32 Online Process : Given a set of query images and the inverted file systems 퐼퐹푆1 and 퐼퐹푆2, we retrieve similar images using the algorithm presented in Algorithm 1. [sent-166, score-0.924]
</p><p>33 First we extract descriptors for all query images; we assign them to visual words and create transactions as explained in section 3. [sent-167, score-0.811]
</p><p>34 22554466  key-points that can be found in more than one query image (see section 3. [sent-177, score-0.652]
</p><p>35 Then we mine for patterns using the MDL principle and obtain a set of pat-  terns 퐻 that best explains the query image visual transactions. [sent-180, score-0.992]
</p><p>36 Finally, this ranked list is refined based on query expansion to obtain the final ranking. [sent-187, score-0.93]
</p><p>37 3 Data: Set of query images, 퐼퐹푆1, 퐼퐹푆2 Result: Ranked list 1. [sent-188, score-0.676]
</p><p>38 Pattern mining Now that we have created a database of transactions 퐷 based on the query images representing the object of interest, we need to explain pattern mining techniques to discover relevant patterns, that can serve as an object-specific mid-level representation for better image retrieval. [sent-218, score-1.416]
</p><p>39 At this point it’s worth noting that a single pattern 푥 only describes a part of the query object. [sent-219, score-0.761]
</p><p>40 A set of patterns that together describe the query object is called a model denoted by 퐻. [sent-220, score-0.858]
</p><p>41 With FIM, all closed frequent patterns are used to represent the query object. [sent-222, score-0.891]
</p><p>42 Instead, what we need is a model that best explains the query image data (i. [sent-228, score-0.675]
</p><p>43 It exploits the regularity in the query image data to discover relevant patterns to represent the intended object. [sent-234, score-0.902]
</p><p>44 This approach finds a balance between complexity of the model (number ofpatterns and complexity ofthe patterns) and representation of the query data. [sent-235, score-0.652]
</p><p>45 The usage of a pattern 푥 ∈ 퐻 by the query image based tranTshaecti uosanagle d oafta ab pasaett i s푥 computed as qfouellroyw ism:  e퐷rn  푢푠푎푔푒(푥∣퐷) = ∣ {푡 ∈  퐷:  푥  ∈ 푐표푣푒푟(퐻, 푡)} ∣ . [sent-242, score-0.761]
</p><p>46 In words, usage measures how many times the pattern used to encode the query images. [sent-243, score-0.761]
</p><p>47 Given a set of models ℍ, the best model (퐻∗) is the one that minimizes 퐻∗ = 푎푟푔푚푖푛퐻∈ℍ퐿(퐻) + 퐿(퐷∣퐻) (2) in which 퐿(퐻) is the length of the model in bits and 퐿(퐷∣퐻) is the length of the query image data once it is e퐿n(c퐷od∣퐻ed) w isith th teh lee mngothdel o Hf . [sent-247, score-0.764]
</p><p>48 php 22554477  model, we use Shannon entropy: 퐿(푥∣퐻) = −푙표푔(푃(푥∣퐷)),  (4)  The quantity 푃(푥∣퐷) is computed using the query image iTnhfeorqm uaatinotnity as fo(푥ll∣o퐷w)s: i  푃(푥∣퐷) =∑∀푦푢∈푠퐻푎푔푢푒푠(푎푥푔∣푒퐷(푦)∣퐷). [sent-253, score-0.652]
</p><p>49 The length of the entire query based transactional database 퐷 when encoded by the model 퐻, 퐿(퐷∣퐻), is the summation of all tchoed lengths hoef tmraondsealc 퐻tio,ns 퐿 (in퐷 퐷∣퐻 once e tnhceosd uemd by ttihoen m ofod aelll 퐻: 퐿(퐷∣퐻) = (6)  ∑퐿(푡∣퐻). [sent-256, score-0.788]
</p><p>50 v By optimizing eth feu nabctoiovne objective function we find the best model 퐻∗ that best explains the image query data 퐷 with a minimal model complexity. [sent-261, score-0.704]
</p><p>51 Given a set of multiple query images, we use KRIMP to discover the patterns, followed by Algorithm 1presented in section 3. [sent-268, score-0.712]
</p><p>52 The final score is obtained by summing the square-root histogram intersection similarity over all query images, i. [sent-276, score-0.669]
</p><p>53 for a database image 퐼푑 we get  푠푐표푟푒(퐼푑) =∑푞∑푖푚푖푛(√푤푑푖,√푤푞푖)  (8)  where 푤푑푖 is the tf-idf weighted L2 normalized 푖푡ℎ bin of the histogram (bag-of-patterns) from database image 푑 and 푤푞푖 is the same for the 푞푡ℎ query image. [sent-278, score-0.755]
</p><p>54 Selecting consistent keypoints: Multiple query basic matching (MQBM) We have now explained most steps of Algorithm 1, ex-  cept for steps 2 and 7, which we describe in this and next section. [sent-281, score-0.691]
</p><p>55 This preprocessing step identifies a set of key points that are consistent across multiple query images. [sent-283, score-0.715]
</p><p>56 Inconsistent key points, that are found only in a single query image, are filtered out. [sent-284, score-0.676]
</p><p>57 We call this step Multiple query basic matching or MQBM. [sent-285, score-0.669]
</p><p>58 This is done efficiently using quantized SIFT features, with processing time linear in the number of query images. [sent-286, score-0.652]
</p><p>59 By summing all binary-bag-of-words of query images we find visual words that appear at-least among two or more images. [sent-289, score-0.738]
</p><p>60 This initial multiple query matching step removes unstable and noisy key points, significantly reduces the size of the transactional database and allows to explore larger spatial patterns. [sent-292, score-0.802]
</p><p>61 Pattern based query expansion (PQE) Query expansion is widely used [3, 5] to obtain better query image representations. [sent-295, score-1.67]
</p><p>62 For instance, the most widely used average query expansion takes the average of the top-k ranked image histograms. [sent-296, score-0.906]
</p><p>63 Instead, in our novel pattern based query expansion (PQE), we combine the topk ranked images along with the query images to find a bet-  ter model (a set of patterns). [sent-297, score-1.725]
</p><p>64 For example, for FIM, we select the most frequent set of patterns from both the query images and the top-k ranked images. [sent-300, score-0.991]
</p><p>65 When KRIMP is used, we re-discover patterns using the query and top-k images using the same MDL objective function (equation 2). [sent-301, score-0.866]
</p><p>66 Additionally, we introduce a new challenging dataset to evaluate the performance of multiple query based image retrieval methods. [sent-304, score-0.827]
</p><p>67 Each query consists of five images of a specific place or object from different time periods, viewing conditions and viewpoints. [sent-308, score-0.798]
</p><p>68 The objective is to retrieve more images of the same place or object using the five query images  given. [sent-309, score-0.77]
</p><p>69 An example query and the results we obtain using our method is shown in Figure 2. [sent-311, score-0.652]
</p><p>70 Altogether there are 11 multiple query sets for each dataset. [sent-315, score-0.669]
</p><p>71 We also evaluate all methods using query images obtained using Google image search. [sent-316, score-0.681]
</p><p>72 We use the top 8 Google images returned for a textual query such as “Eiffel Tower Paris” from the Google image search API5. [sent-317, score-0.762]
</p><p>73 Comparing several multi-query approaches In this experiment by default we use 10 spatial neighbours of a key-point to construct the LBOWs, KRIMP based pattern discovery and PQE for query expansion using the top-5 ranked images (see detailed analysis in section 4. [sent-321, score-1.104]
</p><p>74 We use only 300 patterns to represent an query object. [sent-323, score-0.837]
</p><p>75 In Table 1 we compare our method against several multiple query based methods introduced in [2]. [sent-324, score-0.669]
</p><p>76 : a joint average query method that takes the average of all query histograms; ii) MQ. [sent-326, score-1.304]
</p><p>77 : a method that takes the maximum instead of the mean of the scores returned by each single image in the query; and iv) Joint SVM: a method that learns a SVM  over all query images using a fixed pool of negatives. [sent-330, score-0.71]
</p><p>78 For the spatial verification we use a maximum of 200 top ranked images per query image as in [2]. [sent-336, score-0.827]
</p><p>79 This suggests that, for practical applications and given the ease of collecting query images from the web, multiple queries should be used whenever possible. [sent-346, score-0.795]
</p><p>80 Especially when the number of query images increases, our method outperforms the other methods. [sent-348, score-0.681]
</p><p>81 Moreover, it also shows less variance than the other methods as the number of images in the query grows. [sent-349, score-0.681]
</p><p>82 Comparing query expansion methods: First, we compare several query expansion methods that can be used in combination with our pattern based approach. [sent-354, score-1.779]
</p><p>83 In this experiment, we use the FIM pattern mining method to build the query object model and a maximum of 3 neighbors to create LBOWs/transactions. [sent-355, score-1.008]
</p><p>84 We compare pattern based query expansion (PQE) with the most commonly used average query expansion (AQE) and discriminative query expansion (DQE) introduced in [3]. [sent-356, score-2.614]
</p><p>85 For all these methods we use the query images along with the top 5 retrieved images for query expansion (in the case of DQE, 5 query images + 5 top ranked images are used as positives and a fixed set of 200 negatives). [sent-358, score-2.372]
</p><p>86 From this result we conclude that for our pattern based approach, pattern based query expansion (PQE) is a better choice compared to DQE or AQE. [sent-360, score-1.053]
</p><p>87 Left: The effect of multiple query basic SIFT matching, Right: Comparison of pattern mining methods: FIM vs. [sent-399, score-0.984]
</p><p>88 Note that in [6] it is proposed to use the top at most 50 spatially verified results in query expansion and low tf-idf scores provide the negative training data for DQE [3]. [sent-402, score-0.835]
</p><p>89 Effect of initial basic query matching: In this experiment we show the effect of query side basic SIFT matching  (MQBM). [sent-405, score-1.338]
</p><p>90 Execution times and complexity analysis: Our MQIR method consists of two major steps: (1) processing the query images (SIFT feature extraction, MQBM, transaction creation, pattern mining), and (2) pattern based image retrieval using inverted file systems. [sent-425, score-1.332]
</p><p>91 The execution time for step (1) only depends on the number of query images. [sent-426, score-0.673]
</p><p>92 The first step takes about 2-3 seconds for 8 query images  while the second step takes less than 0. [sent-428, score-0.681]
</p><p>93 Discussion and Conclusion In this paper we present a new method for image retrieval starting from multiple query images. [sent-439, score-0.851]
</p><p>94 Pattern based retrieval time (excluding the processing time of the query images which is independent of the database size) for 300 patterns by varying the number of images in the image archive. [sent-443, score-1.096]
</p><p>95 query object model on-the-fly using a minimal description length principle based pattern mining approach. [sent-444, score-1.15]
</p><p>96 This way we construct a new mid-level feature representation for the query object. [sent-445, score-0.683]
</p><p>97 We also introduced pattern based query expansion which is suitable for pattern based image retrieval methods. [sent-447, score-1.211]
</p><p>98 Compared to other methods the proposed method shows steady performance improvement as the number of images in a query increases. [sent-449, score-0.681]
</p><p>99 We claim that using multiple queries is a much better choice for image retrieval whenever it is possible to collect several query images. [sent-453, score-0.924]
</p><p>100 Total recall: Automatic query expansion with a generative feature model for object retrieval. [sent-490, score-0.856]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('query', 0.652), ('krimp', 0.387), ('mining', 0.189), ('patterns', 0.185), ('expansion', 0.183), ('mqir', 0.166), ('retrieval', 0.158), ('pqe', 0.147), ('mdl', 0.136), ('fim', 0.131), ('pattern', 0.109), ('transaction', 0.1), ('dqe', 0.098), ('file', 0.089), ('inverted', 0.086), ('archive', 0.085), ('queries', 0.08), ('lbows', 0.074), ('mqbm', 0.074), ('ranked', 0.071), ('chum', 0.066), ('transactions', 0.065), ('mine', 0.057), ('length', 0.056), ('frequent', 0.054), ('textual', 0.052), ('principle', 0.05), ('costly', 0.05), ('viewing', 0.048), ('retrieved', 0.046), ('verification', 0.046), ('google', 0.045), ('description', 0.044), ('discover', 0.043), ('database', 0.043), ('philbin', 0.04), ('retrieve', 0.039), ('explain', 0.038), ('create', 0.037), ('eiffel', 0.037), ('lcm', 0.037), ('sgvle', 0.037), ('transactional', 0.037), ('terminology', 0.036), ('itemset', 0.033), ('cover', 0.032), ('words', 0.032), ('midlevel', 0.031), ('construct', 0.031), ('audiovisual', 0.03), ('minimal', 0.029), ('returned', 0.029), ('word', 0.029), ('images', 0.029), ('spatial', 0.029), ('archives', 0.028), ('tower', 0.028), ('arandjelovi', 0.028), ('affine', 0.027), ('fernando', 0.027), ('quack', 0.027), ('internet', 0.027), ('isard', 0.026), ('mir', 0.025), ('visual', 0.025), ('specific', 0.025), ('key', 0.024), ('starting', 0.024), ('sivic', 0.024), ('iminds', 0.024), ('list', 0.024), ('ifs', 0.024), ('explains', 0.023), ('belgium', 0.023), ('conditions', 0.023), ('regularity', 0.022), ('consistent', 0.022), ('material', 0.021), ('item', 0.021), ('created', 0.021), ('object', 0.021), ('indexing', 0.021), ('execution', 0.021), ('neighbourhood', 0.021), ('systematically', 0.021), ('discovers', 0.02), ('mined', 0.02), ('items', 0.019), ('creation', 0.019), ('structural', 0.019), ('vocabulary', 0.019), ('sift', 0.018), ('geometric', 0.018), ('hundred', 0.018), ('serve', 0.017), ('whenever', 0.017), ('multiple', 0.017), ('system', 0.017), ('histogram', 0.017), ('basic', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="266-tfidf-1" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>Author: Basura Fernando, Tinne Tuytelaars</p><p>Abstract: In this paper we present a new method for object retrieval starting from multiple query images. The use of multiple queries allows for a more expressive formulation of the query object including, e.g., different viewpoints and/or viewing conditions. This, in turn, leads to more diverse and more accurate retrieval results. When no query images are available to the user, they can easily be retrieved from the internet using a standard image search engine. In particular, we propose a new method based on pattern mining. Using the minimal description length principle, we derive the most suitable set of patterns to describe the query object, with patterns corresponding to local feature configurations. This results in apowerful object-specific mid-level image representation. The archive can then be searched efficiently for similar images based on this representation, using a combination of two inverted file systems. Since the patterns already encode local spatial information, good results on several standard image retrieval datasets are obtained even without costly re-ranking based on geometric verification.</p><p>2 0.37478748 <a title="266-tfidf-2" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>Author: Xu Wang, Stefan Atev, John Wright, Gilad Lerman</p><p>Abstract: The problem of efficiently deciding which of a database of models is most similar to a given input query arises throughout modern computer vision. Motivated by applications in recognition, image retrieval and optimization, there has been significant recent interest in the variant of this problem in which the database models are linear subspaces and the input is either a point or a subspace. Current approaches to this problem have poor scaling in high dimensions, and may not guarantee sublinear query complexity. We present a new approach to approximate nearest subspace search, based on a simple, new locality sensitive hash for subspaces. Our approach allows point-tosubspace query for a database of subspaces of arbitrary dimension d, in a time that depends sublinearly on the number of subspaces in the database. The query complexity of our algorithm is linear in the ambient dimension D, allow- ing it to be directly applied to high-dimensional imagery data. Numerical experiments on model problems in image repatching and automatic face recognition confirm the advantages of our algorithm in terms of both speed and accuracy.</p><p>3 0.35145667 <a title="266-tfidf-3" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>Author: Dror Aiger, Efi Kokiopoulou, Ehud Rivlin</p><p>Abstract: We propose two solutions for both nearest neighbors and range search problems. For the nearest neighbors problem, we propose a c-approximate solutionfor the restricted version ofthe decisionproblem with bounded radius which is then reduced to the nearest neighbors by a known reduction. For range searching we propose a scheme that learns the parameters in a learning stage adopting them to the case of a set of points with low intrinsic dimension that are embedded in high dimensional space (common scenario for image point descriptors). We compare our algorithms to the best known methods for these problems, i.e. LSH, ANN and FLANN. We show analytically and experimentally that we can do better for moderate approximation factor. Our algorithms are trivial to parallelize. In the experiments conducted, running on couple of million im- ages, our algorithms show meaningful speed-ups when compared with the above mentioned methods.</p><p>4 0.29957166 <a title="266-tfidf-4" href="./iccv-2013-Query-Adaptive_Asymmetrical_Dissimilarities_for_Visual_Object_Retrieval.html">334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</a></p>
<p>Author: Cai-Zhi Zhu, Hervé Jégou, Shin'Ichi Satoh</p><p>Abstract: Visual object retrieval aims at retrieving, from a collection of images, all those in which a given query object appears. It is inherently asymmetric: the query object is mostly included in the database image, while the converse is not necessarily true. However, existing approaches mostly compare the images with symmetrical measures, without considering the different roles of query and database. This paper first measure the extent of asymmetry on large-scale public datasets reflecting this task. Considering the standard bag-of-words representation, we then propose new asymmetrical dissimilarities accounting for the different inlier ratios associated with query and database images. These asymmetrical measures depend on the query, yet they are compatible with an inverted file structure, without noticeably impacting search efficiency. Our experiments show the benefit of our approach, and show that the visual object retrieval task is better treated asymmetrically, in the spirit of state-of-the-art text retrieval.</p><p>5 0.25715134 <a title="266-tfidf-5" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<p>Author: Anand Mishra, Karteek Alahari, C.V. Jawahar</p><p>Abstract: We present an approach for the text-to-image retrieval problem based on textual content present in images. Given the recent developments in understanding text in images, an appealing approach to address this problem is to localize and recognize the text, and then query the database, as in a text retrieval problem. We show that such an approach, despite being based on state-of-the-artmethods, is insufficient, and propose a method, where we do not rely on an exact localization and recognition pipeline. We take a query-driven search approach, where we find approximate locations of characters in the text query, and then impose spatial constraints to generate a ranked list of images in the database. The retrieval performance is evaluated on public scene text datasets as well as three large datasets, namely IIIT scene text retrieval, Sports-10K and TV series-1M, we introduce.</p><p>6 0.2509582 <a title="266-tfidf-6" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>7 0.23511782 <a title="266-tfidf-7" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>8 0.21510637 <a title="266-tfidf-8" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>9 0.21299453 <a title="266-tfidf-9" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>10 0.18153667 <a title="266-tfidf-10" href="./iccv-2013-3D_Sub-query_Expansion_for_Improving_Sketch-Based_Multi-view_Image_Retrieval.html">3 iccv-2013-3D Sub-query Expansion for Improving Sketch-Based Multi-view Image Retrieval</a></p>
<p>11 0.17469431 <a title="266-tfidf-11" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>12 0.1604052 <a title="266-tfidf-12" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>13 0.15020338 <a title="266-tfidf-13" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<p>14 0.13557719 <a title="266-tfidf-14" href="./iccv-2013-Quantize_and_Conquer%3A_A_Dimensionality-Recursive_Solution_to_Clustering%2C_Vector_Quantization%2C_and_Image_Retrieval.html">333 iccv-2013-Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval</a></p>
<p>15 0.13168237 <a title="266-tfidf-15" href="./iccv-2013-To_Aggregate_or_Not_to_aggregate%3A_Selective_Match_Kernels_for_Image_Search.html">419 iccv-2013-To Aggregate or Not to aggregate: Selective Match Kernels for Image Search</a></p>
<p>16 0.1278218 <a title="266-tfidf-16" href="./iccv-2013-Learning_Coupled_Feature_Spaces_for_Cross-Modal_Matching.html">235 iccv-2013-Learning Coupled Feature Spaces for Cross-Modal Matching</a></p>
<p>17 0.12759382 <a title="266-tfidf-17" href="./iccv-2013-Fast_Neighborhood_Graph_Search_Using_Cartesian_Concatenation.html">159 iccv-2013-Fast Neighborhood Graph Search Using Cartesian Concatenation</a></p>
<p>18 0.12632854 <a title="266-tfidf-18" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>19 0.1075435 <a title="266-tfidf-19" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>20 0.10632291 <a title="266-tfidf-20" href="./iccv-2013-EVSAC%3A_Accelerating_Hypotheses_Generation_by_Modeling_Matching_Scores_with_Extreme_Value_Theory.html">131 iccv-2013-EVSAC: Accelerating Hypotheses Generation by Modeling Matching Scores with Extreme Value Theory</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.159), (1, 0.082), (2, -0.081), (3, -0.139), (4, 0.025), (5, 0.277), (6, 0.048), (7, -0.065), (8, -0.182), (9, 0.135), (10, 0.212), (11, -0.003), (12, 0.114), (13, 0.116), (14, -0.011), (15, -0.02), (16, 0.175), (17, -0.233), (18, 0.234), (19, -0.13), (20, -0.052), (21, -0.128), (22, -0.048), (23, -0.053), (24, -0.051), (25, 0.015), (26, 0.038), (27, -0.094), (28, 0.048), (29, -0.05), (30, -0.049), (31, 0.051), (32, 0.022), (33, -0.046), (34, 0.064), (35, -0.029), (36, -0.039), (37, 0.108), (38, -0.028), (39, -0.003), (40, 0.075), (41, -0.008), (42, -0.018), (43, 0.014), (44, -0.005), (45, 0.072), (46, -0.008), (47, 0.04), (48, 0.012), (49, -0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98308337 <a title="266-lsi-1" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>Author: Basura Fernando, Tinne Tuytelaars</p><p>Abstract: In this paper we present a new method for object retrieval starting from multiple query images. The use of multiple queries allows for a more expressive formulation of the query object including, e.g., different viewpoints and/or viewing conditions. This, in turn, leads to more diverse and more accurate retrieval results. When no query images are available to the user, they can easily be retrieved from the internet using a standard image search engine. In particular, we propose a new method based on pattern mining. Using the minimal description length principle, we derive the most suitable set of patterns to describe the query object, with patterns corresponding to local feature configurations. This results in apowerful object-specific mid-level image representation. The archive can then be searched efficiently for similar images based on this representation, using a combination of two inverted file systems. Since the patterns already encode local spatial information, good results on several standard image retrieval datasets are obtained even without costly re-ranking based on geometric verification.</p><p>2 0.93556935 <a title="266-lsi-2" href="./iccv-2013-Query-Adaptive_Asymmetrical_Dissimilarities_for_Visual_Object_Retrieval.html">334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</a></p>
<p>Author: Cai-Zhi Zhu, Hervé Jégou, Shin'Ichi Satoh</p><p>Abstract: Visual object retrieval aims at retrieving, from a collection of images, all those in which a given query object appears. It is inherently asymmetric: the query object is mostly included in the database image, while the converse is not necessarily true. However, existing approaches mostly compare the images with symmetrical measures, without considering the different roles of query and database. This paper first measure the extent of asymmetry on large-scale public datasets reflecting this task. Considering the standard bag-of-words representation, we then propose new asymmetrical dissimilarities accounting for the different inlier ratios associated with query and database images. These asymmetrical measures depend on the query, yet they are compatible with an inverted file structure, without noticeably impacting search efficiency. Our experiments show the benefit of our approach, and show that the visual object retrieval task is better treated asymmetrically, in the spirit of state-of-the-art text retrieval.</p><p>3 0.86935812 <a title="266-lsi-3" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>Author: Xu Wang, Stefan Atev, John Wright, Gilad Lerman</p><p>Abstract: The problem of efficiently deciding which of a database of models is most similar to a given input query arises throughout modern computer vision. Motivated by applications in recognition, image retrieval and optimization, there has been significant recent interest in the variant of this problem in which the database models are linear subspaces and the input is either a point or a subspace. Current approaches to this problem have poor scaling in high dimensions, and may not guarantee sublinear query complexity. We present a new approach to approximate nearest subspace search, based on a simple, new locality sensitive hash for subspaces. Our approach allows point-tosubspace query for a database of subspaces of arbitrary dimension d, in a time that depends sublinearly on the number of subspaces in the database. The query complexity of our algorithm is linear in the ambient dimension D, allow- ing it to be directly applied to high-dimensional imagery data. Numerical experiments on model problems in image repatching and automatic face recognition confirm the advantages of our algorithm in terms of both speed and accuracy.</p><p>4 0.86916399 <a title="266-lsi-4" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>Author: Dror Aiger, Efi Kokiopoulou, Ehud Rivlin</p><p>Abstract: We propose two solutions for both nearest neighbors and range search problems. For the nearest neighbors problem, we propose a c-approximate solutionfor the restricted version ofthe decisionproblem with bounded radius which is then reduced to the nearest neighbors by a known reduction. For range searching we propose a scheme that learns the parameters in a learning stage adopting them to the case of a set of points with low intrinsic dimension that are embedded in high dimensional space (common scenario for image point descriptors). We compare our algorithms to the best known methods for these problems, i.e. LSH, ANN and FLANN. We show analytically and experimentally that we can do better for moderate approximation factor. Our algorithms are trivial to parallelize. In the experiments conducted, running on couple of million im- ages, our algorithms show meaningful speed-ups when compared with the above mentioned methods.</p><p>5 0.74057162 <a title="266-lsi-5" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>Author: Shiliang Zhang, Ming Yang, Xiaoyu Wang, Yuanqing Lin, Qi Tian</p><p>Abstract: Inverted indexes in image retrieval not only allow fast access to database images but also summarize all knowledge about the database, so that their discriminative capacity largely determines the retrieval performance. In this paper, for vocabulary tree based image retrieval, we propose a semantic-aware co-indexing algorithm to jointly San Antonio, TX 78249 . j dl@gmai l com qit ian@cs .ut sa . edu . The query embed two strong cues into the inverted indexes: 1) local invariant features that are robust to delineate low-level image contents, and 2) semantic attributes from large-scale object recognition that may reveal image semantic meanings. For an initial set of inverted indexes of local features, we utilize 1000 semantic attributes to filter out isolated images and insert semantically similar images to the initial set. Encoding these two distinct cues together effectively enhances the discriminative capability of inverted indexes. Such co-indexing operations are totally off-line and introduce small computation overhead to online query cause only local features but no semantic attributes are used for query. Experiments and comparisons with recent retrieval methods on 3 datasets, i.e., UKbench, Holidays, Oxford5K, and 1.3 million images from Flickr as distractors, manifest the competitive performance of our method 1.</p><p>6 0.72360373 <a title="266-lsi-6" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<p>7 0.72197574 <a title="266-lsi-7" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>8 0.72161341 <a title="266-lsi-8" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>9 0.71745408 <a title="266-lsi-9" href="./iccv-2013-Joint_Inverted_Indexing.html">221 iccv-2013-Joint Inverted Indexing</a></p>
<p>10 0.70890832 <a title="266-lsi-10" href="./iccv-2013-3D_Sub-query_Expansion_for_Improving_Sketch-Based_Multi-view_Image_Retrieval.html">3 iccv-2013-3D Sub-query Expansion for Improving Sketch-Based Multi-view Image Retrieval</a></p>
<p>11 0.69333893 <a title="266-lsi-11" href="./iccv-2013-Fast_Neighborhood_Graph_Search_Using_Cartesian_Concatenation.html">159 iccv-2013-Fast Neighborhood Graph Search Using Cartesian Concatenation</a></p>
<p>12 0.67839068 <a title="266-lsi-12" href="./iccv-2013-To_Aggregate_or_Not_to_aggregate%3A_Selective_Match_Kernels_for_Image_Search.html">419 iccv-2013-To Aggregate or Not to aggregate: Selective Match Kernels for Image Search</a></p>
<p>13 0.64561129 <a title="266-lsi-13" href="./iccv-2013-Quantize_and_Conquer%3A_A_Dimensionality-Recursive_Solution_to_Clustering%2C_Vector_Quantization%2C_and_Image_Retrieval.html">333 iccv-2013-Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval</a></p>
<p>14 0.62567782 <a title="266-lsi-14" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>15 0.55733752 <a title="266-lsi-15" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<p>16 0.52704674 <a title="266-lsi-16" href="./iccv-2013-SYM-FISH%3A_A_Symmetry-Aware_Flip_Invariant_Sketch_Histogram_Shape_Descriptor.html">368 iccv-2013-SYM-FISH: A Symmetry-Aware Flip Invariant Sketch Histogram Shape Descriptor</a></p>
<p>17 0.51976788 <a title="266-lsi-17" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>18 0.51817757 <a title="266-lsi-18" href="./iccv-2013-Paper_Doll_Parsing%3A_Retrieving_Similar_Styles_to_Parse_Clothing_Items.html">306 iccv-2013-Paper Doll Parsing: Retrieving Similar Styles to Parse Clothing Items</a></p>
<p>19 0.47769701 <a title="266-lsi-19" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>20 0.46720445 <a title="266-lsi-20" href="./iccv-2013-Learning_Coupled_Feature_Spaces_for_Cross-Modal_Matching.html">235 iccv-2013-Learning Coupled Feature Spaces for Cross-Modal Matching</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.119), (7, 0.027), (12, 0.026), (13, 0.011), (26, 0.066), (27, 0.015), (30, 0.191), (31, 0.035), (35, 0.012), (42, 0.098), (64, 0.079), (73, 0.019), (89, 0.193)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87926602 <a title="266-lda-1" href="./iccv-2013-Active_Visual_Recognition_with_Expertise_Estimation_in_Crowdsourcing.html">43 iccv-2013-Active Visual Recognition with Expertise Estimation in Crowdsourcing</a></p>
<p>Author: Chengjiang Long, Gang Hua, Ashish Kapoor</p><p>Abstract: We present a noise resilient probabilistic model for active learning of a Gaussian process classifier from crowds, i.e., a set of noisy labelers. It explicitly models both the overall label noises and the expertise level of each individual labeler in two levels of flip models. Expectation propagation is adopted for efficient approximate Bayesian inference of our probabilistic model for classification, based on which, a generalized EM algorithm is derived to estimate both the global label noise and the expertise of each individual labeler. The probabilistic nature of our model immediately allows the adoption of the prediction entropy and estimated expertise for active selection of data sample to be labeled, and active selection of high quality labelers to label the data, respectively. We apply the proposed model for three visual recognition tasks, i.e, object category recognition, gender recognition, and multi-modal activity recognition, on three datasets with real crowd-sourced labels from Amazon Mechanical Turk. The experiments clearly demonstrated the efficacy of the proposed model.</p><p>2 0.86801535 <a title="266-lda-2" href="./iccv-2013-Query-Adaptive_Asymmetrical_Dissimilarities_for_Visual_Object_Retrieval.html">334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</a></p>
<p>Author: Cai-Zhi Zhu, Hervé Jégou, Shin'Ichi Satoh</p><p>Abstract: Visual object retrieval aims at retrieving, from a collection of images, all those in which a given query object appears. It is inherently asymmetric: the query object is mostly included in the database image, while the converse is not necessarily true. However, existing approaches mostly compare the images with symmetrical measures, without considering the different roles of query and database. This paper first measure the extent of asymmetry on large-scale public datasets reflecting this task. Considering the standard bag-of-words representation, we then propose new asymmetrical dissimilarities accounting for the different inlier ratios associated with query and database images. These asymmetrical measures depend on the query, yet they are compatible with an inverted file structure, without noticeably impacting search efficiency. Our experiments show the benefit of our approach, and show that the visual object retrieval task is better treated asymmetrically, in the spirit of state-of-the-art text retrieval.</p><p>3 0.84141135 <a title="266-lda-3" href="./iccv-2013-Discriminative_Label_Propagation_for_Multi-object_Tracking_with_Sporadic_Appearance_Features.html">120 iccv-2013-Discriminative Label Propagation for Multi-object Tracking with Sporadic Appearance Features</a></p>
<p>Author: K.C. Amit Kumar, Christophe De_Vleeschouwer</p><p>Abstract: Given a set of plausible detections, detected at each time instant independently, we investigate how to associate them across time. This is done by propagating labels on a set of graphs that capture how the spatio-temporal and the appearance cues promote the assignment of identical or distinct labels to a pair of nodes. The graph construction is driven by the locally linear embedding (LLE) of either the spatio-temporal or the appearance features associated to the detections. Interestingly, the neighborhood of a node in each appearance graph is defined to include all nodes for which the appearance feature is available (except the ones that coexist at the same time). This allows to connect the nodes that share the same appearance even if they are temporally distant, which gives our framework the uncommon ability to exploit the appearance features that are available only sporadically along the sequence of detections. Once the graphs have been defined, the multi-object tracking is formulated as the problem of finding a label assignment that is consistent with the constraints captured by each of the graphs. This results into a difference of convex program that can be efficiently solved. Experiments are performed on a basketball and several well-known pedestrian datasets in order to validate the effectiveness of the proposed solution.</p><p>4 0.83487457 <a title="266-lda-4" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>Author: Gang Hua, Chengjiang Long, Ming Yang, Yan Gao</p><p>Abstract: Active learning is an effective way of engaging users to interactively train models for visual recognition. The vast majority of previous works, if not all of them, focused on active learning with a single human oracle. The problem of active learning with multiple oracles in a collaborative setting has not been well explored. Moreover, most of the previous works assume that the labels provided by the human oracles are noise free, which may often be violated in reality. We present a collaborative computational model for active learning with multiple human oracles. It leads to not only an ensemble kernel machine that is robust to label noises, but also a principled label quality measure to online detect irresponsible labelers. Instead of running independent active learning processes for each individual human oracle, our model captures the inherent correlations among the labelers through shared data among them. Our simulation experiments and experiments with real crowd-sourced noisy labels demonstrated the efficacy of our model.</p><p>same-paper 5 0.83414257 <a title="266-lda-5" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>Author: Basura Fernando, Tinne Tuytelaars</p><p>Abstract: In this paper we present a new method for object retrieval starting from multiple query images. The use of multiple queries allows for a more expressive formulation of the query object including, e.g., different viewpoints and/or viewing conditions. This, in turn, leads to more diverse and more accurate retrieval results. When no query images are available to the user, they can easily be retrieved from the internet using a standard image search engine. In particular, we propose a new method based on pattern mining. Using the minimal description length principle, we derive the most suitable set of patterns to describe the query object, with patterns corresponding to local feature configurations. This results in apowerful object-specific mid-level image representation. The archive can then be searched efficiently for similar images based on this representation, using a combination of two inverted file systems. Since the patterns already encode local spatial information, good results on several standard image retrieval datasets are obtained even without costly re-ranking based on geometric verification.</p><p>6 0.79960561 <a title="266-lda-6" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>7 0.79738456 <a title="266-lda-7" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>8 0.79687071 <a title="266-lda-8" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>9 0.79648602 <a title="266-lda-9" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>10 0.79553211 <a title="266-lda-10" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>11 0.79539371 <a title="266-lda-11" href="./iccv-2013-Linear_Sequence_Discriminant_Analysis%3A_A_Model-Based_Dimensionality_Reduction_Method_for_Vector_Sequences.html">253 iccv-2013-Linear Sequence Discriminant Analysis: A Model-Based Dimensionality Reduction Method for Vector Sequences</a></p>
<p>12 0.7940613 <a title="266-lda-12" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>13 0.79385763 <a title="266-lda-13" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<p>14 0.79370725 <a title="266-lda-14" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>15 0.79338157 <a title="266-lda-15" href="./iccv-2013-Learning_to_Rank_Using_Privileged_Information.html">248 iccv-2013-Learning to Rank Using Privileged Information</a></p>
<p>16 0.79328269 <a title="266-lda-16" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>17 0.79293722 <a title="266-lda-17" href="./iccv-2013-Real-Time_Articulated_Hand_Pose_Estimation_Using_Semi-supervised_Transductive_Regression_Forests.html">340 iccv-2013-Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests</a></p>
<p>18 0.79277289 <a title="266-lda-18" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>19 0.79145575 <a title="266-lda-19" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>20 0.79086912 <a title="266-lda-20" href="./iccv-2013-Person_Re-identification_by_Salience_Matching.html">313 iccv-2013-Person Re-identification by Salience Matching</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
