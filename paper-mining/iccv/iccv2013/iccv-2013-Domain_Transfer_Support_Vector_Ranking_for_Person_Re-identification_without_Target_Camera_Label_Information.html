<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-124" href="#">iccv2013-124</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</h1>
<br/><p>Source: <a title="iccv-2013-124-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Ma_Domain_Transfer_Support_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Andy J. Ma, Pong C. Yuen, Jiawei Li</p><p>Abstract: This paper addresses a new person re-identification problem without the label information of persons under non-overlapping target cameras. Given the matched (positive) and unmatched (negative) image pairs from source domain cameras, as well as unmatched (negative) image pairs which can be easily generated from target domain cameras, we propose a Domain Transfer Ranked Support Vector Machines (DTRSVM) method for re-identification under target domain cameras. To overcome the problems introduced due to the absence of matched (positive) image pairs in target domain, we relax the discriminative constraint to a necessary condition only relying on the positive mean in target domain. By estimating the target positive mean using source and target domain data, a new discriminative model with high confidence in target positive mean and low confidence in target negative image pairs is developed. Since the necessary condition may not truly preserve the discriminability, multi-task support vector ranking is proposed to incorporate the training data from source domain with label information. Experimental results show that the proposed DTRSVM outperforms existing methods without using label information in target cameras. And the top 30 rank accuracy can be improved by the proposed method upto 9.40% on publicly available person re-identification datasets.</p><p>Reference: <a title="iccv-2013-124-reference" href="../iccv2013_reference/iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 hk }  Abstract This paper addresses a new person re-identification problem without the label information of persons under non-overlapping target cameras. [sent-4, score-0.72]
</p><p>2 To overcome the problems introduced due to the absence of matched (positive) image pairs in target domain, we relax the discriminative constraint to a necessary condition only relying on the positive mean in target domain. [sent-6, score-1.081]
</p><p>3 By estimating the target positive mean using source and target domain data, a new discriminative model with high confidence in target positive mean and low confidence in target negative image pairs is developed. [sent-7, score-2.29]
</p><p>4 Since the necessary condition may not truly preserve the discriminability, multi-task support vector ranking is proposed to incorporate the training data from source domain with label  information. [sent-8, score-0.725]
</p><p>5 Experimental results show that the proposed DTRSVM outperforms existing methods without using label information in target cameras. [sent-9, score-0.369]
</p><p>6 Background and Motivation In recent years, person re-identification across a camera network comprising multiple cameras with non-overlapping views has become an active research topic due to its importance in many camera-network-based computer vision applications. [sent-15, score-0.344]
</p><p>7 The goal of person re-identification is to reidentify a person when he/she disappears from the fieldof-view of a camera and appears in another. [sent-16, score-0.433]
</p><p>8 With the person labels, matched (positive) and unmatched (negative) image pairs are generated to train the discriminative distance model. [sent-21, score-0.469]
</p><p>9 1 would also lead to dramatic performance deterioration, when the distance model learnt from other camera set with label information is directly applied to the cameras missing person labels. [sent-27, score-0.45]
</p><p>10 Problem Definition Motivated by domain transfer learning approach (see [25] for a review), we consider data from the camera set with label information as the source domain; while data from camera set missing label information as the tar-  33556670  (a) PRID (b) VIPeR Figure 1. [sent-31, score-0.726]
</p><p>11 Comparison between person images from different datasets (better viewed in color): matched image pairs in (a) PRID [16] dataset and (b) VIPeR [14] dataset. [sent-32, score-0.336]
</p><p>12 Here, we denote the source and target domains as s and t, respectively. [sent-35, score-0.578]
</p><p>13 1, the source and target joint distributions of the positive or negative tag y and feature vector z for an image pair are supposed to be different, i. [sent-37, score-0.766]
</p><p>14 Prs (Φ(z)) ≈ Prt (Φ(z)), can be learnt via domain adaptation techniques [13] [24]. [sent-42, score-0.347]
</p><p>15 (z S))can be interpreted as classification score, zt)h)e ocro Pndri(tyio|nΦ t(hz)a)t  =  Prs (y|Φ(z)) ≈ Prt (y|Φ(z)) implies an equivalence of the dist(ayn|cΦe( zm))ode ≈ls Pinr source )a)n dim target adonm eaqiunivsa. [sent-47, score-0.528]
</p><p>16 [1] [15] [17] [26] [30], can be employed to learn the distance model in the source domain (consisting of projected data with positive and negative image pairs generated by the label information), which can be applied to the target domain without significant performance degradation. [sent-50, score-1.403]
</p><p>17 model learnt from the projected data in source domain is equivalent to the target one. [sent-53, score-0.825]
</p><p>18 Thus, we propose to learn the target distance model using data from both source and target domains. [sent-54, score-0.816]
</p><p>19 Therefore, the key problem is how to define the learning task in target domain and incorporate the data from source domain for training. [sent-56, score-1.051]
</p><p>20 • We develop a new method to train the target discriminative model based on the negative image pairs generat-  ed from non-overlapping target cameras. [sent-60, score-0.886]
</p><p>21 Without positive image pairs generated by the label information of persons, we propose to relax the discriminative constraint into a necessary condition to it, which only relies on the mean of positive pairs. [sent-61, score-0.606]
</p><p>22 Since source and target domains must be related, we estimate the positive mean in the target domain by assuming that the difference between the positive and negative means in the source domain is close to that in the target domain. [sent-62, score-2.327]
</p><p>23 With the estimated mean of positive pairs in the target domain, the target learning problem is defined by maximizing the difference between the estimated mean and the negative image pairs, similar to RankSVM [26]. [sent-63, score-1.096]
</p><p>24 • We propose a novel multi-task support vector ranking met•ho Wde t op roapnoks eth ae ninodvievli mduualltsi ftoasrk person rrte v-iedcetnotrifi rcaantkiionng. [sent-64, score-0.313]
</p><p>25 Since the learning task in the target domain depends on a necessary condition to the discriminative constraint, the learnt classification model may not be discriminative enough for classifying the detected persons in target cameras. [sent-65, score-1.287]
</p><p>26 Therefore, we propose to incorporate the data in the source domain to learn the classification model for the target domain. [sent-67, score-0.778]
</p><p>27 Inspired by multi-task SVM [10], we propose to learn the optimal models for the source and target tasks, simultaneously. [sent-68, score-0.505]
</p><p>28 In [26], person re-identification was formulated as a ranking problem and the RankSVM model is learnt by assigning higher confidence to the positive image pairs and vice versa. [sent-79, score-0.55]
</p><p>29 Denote xj as the feature vector for person j, xj+i for i = 1, · · · , nj+ as feature vectors of its matched observations, and xj−i for i= 1, · · · , nj− as feature vectors of its unmatched observations, where nj+ (nj−) is the number of the matched (unmatched) observations. [sent-80, score-0.493]
</p><p>30 [29] addressed a watch list (set) based verification problem and proposed to transfer the information from nontarget person data to mine the discriminative information for the target people in the watch list. [sent-101, score-0.681]
</p><p>31 Domain Transfer Support Vector Ranking As indicated in [30], the absolute difference space shows some advantages over the common difference space, so we follow [30] to use the absolute difference vector as the feature representation method for both positive and negative image pairs. [sent-103, score-0.417]
</p><p>32 Similar to the symbol definition as presented in Section 2, denote zsjs+i and zsj−si as the difference vectors of positive and negative image pairs in the source domain, respectively. [sent-104, score-0.601]
</p><p>33 For the target domain, the label information of persons is not available, so positive image pairs cannot be generated. [sent-105, score-0.755]
</p><p>34 However, negative image pairs can be easily generated, because same person cannot be presented at the same instant under different non-overlapping cameras. [sent-106, score-0.418]
</p><p>35 With zsjs+i and zsj−si in the source domain and ztj−ti in the target domain, we first propose a new method to learn  the target distance model in Section 3. [sent-108, score-1.064]
</p><p>36 Leaning without Positive Image Pairs in Target Cameras Since feature entries give different importance in identifying a person, we follow [26] to use the weighted summation of the absolute difference vector to calculate the confident score for the image pairs in the target domain, i. [sent-114, score-0.508]
</p><p>37 If positive image pairs are available, the scores of positive image pairs must be larger than those of the negative ones for a discriminative weight vector wt, i. [sent-117, score-0.682]
</p><p>38 (2)  However, positive image pairs are not available in the target domain, so we cannot obtain the absolute difference vectors ztjt+i in practice. [sent-123, score-0.634]
</p><p>39 One way to solve this problem is to determine the weight vector wt by assigning smaller values to the difference vectors ztjt−i of negative image pairs using one-class SVM [27]. [sent-124, score-0.433]
</p><p>40 Nevertheless, it is possible that the scores of positive image pairs also decrease when minimizing those ofthe negative ones. [sent-125, score-0.344]
</p><p>41 Taking the summation of constraint (2) over the difference vectors ztjt+i of positive image pairs for all jt and i, it has wtTmt+ > wtTzjt−t? [sent-128, score-0.413]
</p><p>42 (3)  where mt+ denote the mean of positive image pairs in the target domain. [sent-132, score-0.59]
</p><p>43 Therefore, a necessary condition to constraint (2) is given by equation (3) such that the score of the positive mean is larger than those of the negative image pairs. [sent-133, score-0.457]
</p><p>44 Denote the mean calculated by the difference vectors of all the image pairs in the target domain as mt and the mean of negative image pairs estimated by the available data ztjt−i of unmatched pairs as m? [sent-134, score-1.348]
</p><p>45 However, Nt+ and Nt−are difficult to be computed, if target positive samples are not available. [sent-142, score-0.432]
</p><p>46 On the other hand, the estimation error for the positive mean with equation (4) can be 33556692  very large, since the number of negative pairs is much larger than that of positive pairs, i. [sent-143, score-0.58]
</p><p>47 Denote the genuine means of the positive and negative image pairs  as mt+ and mt−, respectively. [sent-147, score-0.344]
</p><p>48 efor Ne, according to equation (5), the estimation error for the positive mean is very large, even though the error for the negative mean is small. [sent-158, score-0.399]
</p><p>49 In order to solve this problem, we propose to incorporate the data with label information of persons in the source domain. [sent-159, score-0.433]
</p><p>50 With the label information, the true means of positive and negative image pairs in the source domain can be calculated and denoted as ms+ and ms−, respectively. [sent-160, score-0.844]
</p><p>51 Since the source and target domains are related, the positive and negative distributions in the source domain must be related to those in the target domain. [sent-161, score-1.566]
</p><p>52 We suppose the relationship can be modeled in a way that the difference between the positive and negative means in the source domain is close to that in the target domain, i. [sent-162, score-1.02]
</p><p>53 mt+  −  mt−  ≈  ms+  −  ms−  (6)  With equation (6), the positive mean in the target domain can be estimated by the following equation, m? [sent-164, score-0.795]
</p><p>54 (8)  Since lots of negative image pairs can be obtained from the non-overlapping target cameras, the estimated mean of negative pairs is close to the true one. [sent-176, score-0.806]
</p><p>55 Under the assumption given by equation (6), the upper bound of the estimation error for the positive mean in the target domain is small. [sent-177, score-0.795]
</p><p>56 t+ in the target domaiWn atnhd t thee e snteimceastesadry p ocosintidvietio mne agniv me? [sent-180, score-0.311]
</p><p>57 e the learning task in the target domain similar to the optimization problem (1) in RankSVM [26] as the following equation,  mwitn21? [sent-182, score-0.584]
</p><p>58 0, jt 1, · · · Jt, i 1, · · · nj−t where nj−t denotes the number of negative image pairs for person jt and Jt is the number of detected persons in the target domain. [sent-190, score-1.075]
</p><p>59 Multi-Task Support Vector Ranking Since equation (3) is not a sufficient condition to the dis-  criminative constraint (2), the weight vector wt learnt by the derived optimization problem (9) may not be discriminative enough for classifying the detected persons in target cameras. [sent-193, score-0.87]
</p><p>60 On the other hand, the performance may deteriorate by directly using the weight vector learnt from optimization problem (9), since the assumption on the relation between the source and target domains introduces the estimation error to some extent. [sent-194, score-0.739]
</p><p>61 Therefore, we employ the concept of multi-task learning to incorporate the source domain data with label information of persons for the determination of the weight vector wt. [sent-195, score-0.748]
</p><p>62 The learning task in the source domain is defined by employing the learning problem (1) in RankSVM [26]. [sent-197, score-0.467]
</p><p>63 Combining the source task similar to (1) and target task (9), all the vectors w0, vs and vt can be estimated simultaneously by solving the following optimization problem,  w0m,vsin,vt21? [sent-198, score-0.744]
</p><p>64 t+ − zjtt−it) ≥ 1− ξjtit , ξjtit ≥ 0, jt( m= 1, · · ·  ,  Jt, it  = 1, · · ·  ,  nj−t  where nj+s (nj−s) denotes the number of positive (negative) image pairs for person js and Js is the number of detected persons in the source domain. [sent-212, score-0.839]
</p><p>65 In optimization problem (11), the positive regularization parameter μ controls the difference between the weight vector ws (wt) in the source (target) domain and the common model w0. [sent-213, score-0.687]
</p><p>66 Intuitively, for a fixed value of C, if μ → ∞, vs and vt tend to zero, which means tahluate tohfe C source a→nd ∞ target models become the same. [sent-214, score-0.663]
</p><p>67 fI nμ t →his case, the common model w0 33556703  takes little effect on the source and target models, which implies that the source task and target task are unrelated. [sent-216, score-1.083]
</p><p>68 lt1,·ξlt·),Ls, wTblt  ≥ 1− ξlt,ξlt ≥ 0,lt  = 1, ··· ,  (15) Lt  where Ls and Lt represent the number of inequality constraints in source and target domains, respectively. [sent-238, score-0.505]
</p><p>69 Then, the target weight vector wt is calculated by equations (10) and (12). [sent-245, score-0.489]
</p><p>70 Algorithm  1  Training  DTRSVM  Algorithm 1Training DTRSVM Input: Difference vectors zsjs+i and zsjs−i in source domain, ztj−ti in target domain, parameters C and μ; 1: Compute the means ms+ by zsjs+i, ms−by zsjs−i, and m? [sent-251, score-0.536]
</p><p>71 t  by zjt−ti; 2: Estimate the relevant mean in the target doma,in a nm? [sent-252, score-0.36]
</p><p>72 VIPeR is a re-identification dataset containing 632 person image pairs captured by two cameras outdoor. [sent-258, score-0.386]
</p><p>73 Total 385 persons were captured by camera A, while 749 persons captured by camera B. [sent-260, score-0.398]
</p><p>74 In re-identification application, total 476 person images from 119 persons are used for experiments as in [30]. [sent-264, score-0.351]
</p><p>75 In our experiments, we use VIPeR or PRID as the target domain. [sent-266, score-0.311]
</p><p>76 Without the time acquisition information in the PRID and VIPeR datasets, negative image pairs from non-overlapping cameras are generated by simulating the synchronization using label information. [sent-267, score-0.363]
</p><p>77 Since the i-LIDS dataset does not provide the camera information, negative image pairs from non-overlapping cameras cannot be generated to simulate the real situation. [sent-268, score-0.348]
</p><p>78 Fixing the target domain dataset as VIPeR or PRID, one of the other two datasets is used as the source domain to train the proposed DTRSVM. [sent-270, score-1.001]
</p><p>79 Therefore, experiments of four transfer scenarios with different source or target domain are performed. [sent-271, score-0.835]
</p><p>80 If VIPeR is used as the the target dataset, 632 image pairs are randomly separated into half for training and the other half for testing. [sent-272, score-0.42]
</p><p>81 When PRID is used as the the target dataset, 100 out of the 200 image pairs are randomly selected as the training set, and the others for testing set. [sent-273, score-0.42]
</p><p>82 Following [30], one positive and one negative image pair for each person in the source dataset are used for training, while the training data in the target domain contains only one negative image pair for each person. [sent-281, score-1.297]
</p><p>83 Since the label information of persons is supposed to be not available in the target dataset, crossvalidation cannot be performed to select the best parameters. [sent-282, score-0.525]
</p><p>84 From Table 1, we can see that the estimation error introduced by equation (4) does not change with different source domains, since the estimated mean using equation (4) is only based on the information in the target domain. [sent-290, score-0.686]
</p><p>85 We also plot the CMC curves ofthe learning based methods training with the label information in the target domain as the baseline of the upper bound performance. [sent-295, score-0.617]
</p><p>86 2(a), when the source domain is i-LIDS  and the target domain is PRID, the proposed DTRSVM  achieves convincing performance closed to that of the upper bound using the label information in the target domain for training. [sent-299, score-1.618]
</p><p>87 For the transfer scenario from VIPeR to PRID, DTRSVM also clearly outperforms the other methods without using the label information in the target domain as indicated in Fig. [sent-300, score-0.699]
</p><p>88 Although the performance of DTRSVM for VIPeR as target domain is close to RankSVM, RDC, and the non-learning based methods when using PRID as the source domain, DTRSVM achieves obvious improvement when using i-LIDS as the source domain. [sent-302, score-0.947]
</p><p>89 In order to further compare the performance without label information of persons in the target domain, we summarize the top r ranked matching accuracies (%) of different methods and different source domains for PRID in Table 2 and VIPeR in Table 3. [sent-304, score-0.836]
</p><p>90 This convinces that the unmatched image pairs generated from non-overlapping cameras help to improve the re-identification performance, when the label information is not available. [sent-308, score-0.375]
</p><p>91 CMC curves of all the four source and target domain combinations (better viewed in color)  Performance influence of parameter μ: As mentioned in Section 3. [sent-310, score-0.753]
</p><p>92 2, the regularization parameter μ measures the degree of relevance of the source and target domains. [sent-311, score-0.528]
</p><p>93 This implies that the degree of relevance differs with different source or target domain. [sent-316, score-0.551]
</p><p>94 Therefore, if the degree of relevance between source and target domains can be discovered, the parameter μ in the proposed method can be de-  termined more accurately to further improve the proposed re-identification performance. [sent-317, score-0.601]
</p><p>95 Conclusions In this paper, we propose a novel Domain Transfer Ranked Support Vector Machines (DTRSVM) method to task in the target domain is defined by a necessary condition to the discriminative constraint, which only relies on the mean of positive pairs. [sent-319, score-0.877]
</p><p>96 In order to estimate the positive mean in the target domain, we assume that the difference between the positive and negative means in source domain is close to the one in the target domain. [sent-320, score-1.501]
</p><p>97 After defining the learning problem in the target domain, a multi-task support vector ranking method is developed to incorporate the data in source domain with label information to train the classification model for the target domain. [sent-321, score-1.265]
</p><p>98 Experimental results show that the estimation error ofthe positive mean is small, which indicates that the proposed assumption is suitable in person re-identification applications. [sent-322, score-0.365]
</p><p>99 Comparing state-of-the-art discriminative learning methods using the source and target domain data for training, respectively, it is shown that the performance deteriorates dramatically when using the learnt model trained on source domain to target domain. [sent-323, score-1.619]
</p><p>100 Appearancebased person reidentification in camera networks: problem overview and current approaches. [sent-395, score-0.328]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('prid', 0.425), ('dtrsvm', 0.328), ('target', 0.311), ('viper', 0.285), ('domain', 0.248), ('person', 0.195), ('source', 0.194), ('ranksvm', 0.164), ('persons', 0.156), ('zsjs', 0.135), ('positive', 0.121), ('negative', 0.114), ('pairs', 0.109), ('nj', 0.107), ('ztjt', 0.097), ('jt', 0.095), ('mt', 0.095), ('unmatched', 0.092), ('reidentification', 0.09), ('prs', 0.086), ('prt', 0.086), ('vt', 0.083), ('cameras', 0.082), ('transfer', 0.082), ('wt', 0.08), ('rdc', 0.077), ('wttzjt', 0.077), ('vs', 0.075), ('domains', 0.073), ('learnt', 0.072), ('ms', 0.067), ('equation', 0.066), ('js', 0.064), ('label', 0.058), ('jti', 0.058), ('jtit', 0.058), ('rpml', 0.058), ('wtblt', 0.058), ('zjss', 0.058), ('xj', 0.054), ('ranking', 0.053), ('condition', 0.052), ('nt', 0.05), ('mean', 0.049), ('bazzani', 0.045), ('ranked', 0.044), ('zheng', 0.043), ('camera', 0.043), ('cmc', 0.041), ('discriminative', 0.041), ('zj', 0.041), ('weight', 0.041), ('avss', 0.04), ('support', 0.039), ('baptist', 0.039), ('corv', 0.039), ('emond', 0.039), ('jsis', 0.039), ('wtals', 0.039), ('zjt', 0.039), ('zjtt', 0.039), ('zsj', 0.039), ('ztj', 0.039), ('lt', 0.038), ('gong', 0.034), ('convinces', 0.034), ('wtt', 0.034), ('machines', 0.033), ('matched', 0.032), ('difference', 0.032), ('vectors', 0.031), ('equations', 0.031), ('deterioration', 0.03), ('perina', 0.03), ('absolute', 0.03), ('necessary', 0.03), ('kong', 0.029), ('hirzer', 0.028), ('kuo', 0.027), ('ji', 0.027), ('adaptation', 0.027), ('vector', 0.026), ('ak', 0.026), ('ti', 0.026), ('watch', 0.026), ('cristani', 0.026), ('task', 0.025), ('ws', 0.025), ('constraint', 0.025), ('sebastian', 0.025), ('incorporate', 0.025), ('network', 0.024), ('disjoint', 0.024), ('mthe', 0.023), ('relevance', 0.023), ('implies', 0.023), ('ls', 0.023), ('hong', 0.023), ('br', 0.022), ('deteriorate', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="124-tfidf-1" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>Author: Andy J. Ma, Pong C. Yuen, Jiawei Li</p><p>Abstract: This paper addresses a new person re-identification problem without the label information of persons under non-overlapping target cameras. Given the matched (positive) and unmatched (negative) image pairs from source domain cameras, as well as unmatched (negative) image pairs which can be easily generated from target domain cameras, we propose a Domain Transfer Ranked Support Vector Machines (DTRSVM) method for re-identification under target domain cameras. To overcome the problems introduced due to the absence of matched (positive) image pairs in target domain, we relax the discriminative constraint to a necessary condition only relying on the positive mean in target domain. By estimating the target positive mean using source and target domain data, a new discriminative model with high confidence in target positive mean and low confidence in target negative image pairs is developed. Since the necessary condition may not truly preserve the discriminability, multi-task support vector ranking is proposed to incorporate the training data from source domain with label information. Experimental results show that the proposed DTRSVM outperforms existing methods without using label information in target cameras. And the top 30 rank accuracy can be improved by the proposed method upto 9.40% on publicly available person re-identification datasets.</p><p>2 0.27535284 <a title="124-tfidf-2" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>Author: Fatemeh Mirrashed, Mohammad Rastegari</p><p>Abstract: We propose an unsupervised domain adaptation method that exploits intrinsic compact structures of categories across different domains using binary attributes. Our method directly optimizes for classification in the target domain. The key insight is finding attributes that are discriminative across categories and predictable across domains. We achieve a performance that significantly exceeds the state-of-the-art results on standard benchmarks. In fact, in many cases, our method reaches the same-domain performance, the upper bound, in unsupervised domain adaptation scenarios.</p><p>3 0.24053077 <a title="124-tfidf-3" href="./iccv-2013-Unsupervised_Visual_Domain_Adaptation_Using_Subspace_Alignment.html">438 iccv-2013-Unsupervised Visual Domain Adaptation Using Subspace Alignment</a></p>
<p>Author: Basura Fernando, Amaury Habrard, Marc Sebban, Tinne Tuytelaars</p><p>Abstract: In this paper, we introduce a new domain adaptation (DA) algorithm where the source and target domains are represented by subspaces described by eigenvectors. In this context, our method seeks a domain adaptation solution by learning a mapping function which aligns the source subspace with the target one. We show that the solution of the corresponding optimization problem can be obtained in a simple closed form, leading to an extremely fast algorithm. We use a theoretical result to tune the unique hyperparameter corresponding to the size of the subspaces. We run our method on various datasets and show that, despite its intrinsic simplicity, it outperforms state of the art DA methods.</p><p>4 0.22173578 <a title="124-tfidf-4" href="./iccv-2013-Unsupervised_Domain_Adaptation_by_Domain_Invariant_Projection.html">435 iccv-2013-Unsupervised Domain Adaptation by Domain Invariant Projection</a></p>
<p>Author: Mahsa Baktashmotlagh, Mehrtash T. Harandi, Brian C. Lovell, Mathieu Salzmann</p><p>Abstract: Domain-invariant representations are key to addressing the domain shift problem where the training and test examples follow different distributions. Existing techniques that have attempted to match the distributions of the source and target domains typically compare these distributions in the original feature space. This space, however, may not be directly suitable for such a comparison, since some of the features may have been distorted by the domain shift, or may be domain specific. In this paper, we introduce a Domain Invariant Projection approach: An unsupervised domain adaptation method that overcomes this issue by extracting the information that is invariant across the source and target domains. More specifically, we learn a projection of the data to a low-dimensional latent space where the distance between the empirical distributions of the source and target examples is minimized. We demonstrate the effectiveness of our approach on the task of visual object recognition and show that it outperforms state-of-the-art methods on a standard domain adaptation benchmark dataset.</p><p>5 0.17598447 <a title="124-tfidf-5" href="./iccv-2013-Cross-View_Action_Recognition_over_Heterogeneous_Feature_Spaces.html">99 iccv-2013-Cross-View Action Recognition over Heterogeneous Feature Spaces</a></p>
<p>Author: Xinxiao Wu, Han Wang, Cuiwei Liu, Yunde Jia</p><p>Abstract: In cross-view action recognition, “what you saw” in one view is different from “what you recognize ” in another view. The data distribution even the feature space can change from one view to another due to the appearance and motion of actions drastically vary across different views. In this paper, we address the problem of transferring action models learned in one view (source view) to another different view (target view), where action instances from these two views are represented by heterogeneous features. A novel learning method, called Heterogeneous Transfer Discriminantanalysis of Canonical Correlations (HTDCC), is proposed to learn a discriminative common feature space for linking source and target views to transfer knowledge between them. Two projection matrices that respectively map data from source and target views into the common space are optimized via simultaneously minimizing the canonical correlations of inter-class samples and maximizing the intraclass canonical correlations. Our model is neither restricted to corresponding action instances in the two views nor restricted to the same type of feature, and can handle only a few or even no labeled samples available in the target view. To reduce the data distribution mismatch between the source and target views in the commonfeature space, a nonparametric criterion is included in the objective function. We additionally propose a joint weight learning method to fuse multiple source-view action classifiers for recognition in the target view. Different combination weights are assigned to different source views, with each weight presenting how contributive the corresponding source view is to the target view. The proposed method is evaluated on the IXMAS multi-view dataset and achieves promising results.</p><p>6 0.16935372 <a title="124-tfidf-6" href="./iccv-2013-POP%3A_Person_Re-identification_Post-rank_Optimisation.html">305 iccv-2013-POP: Person Re-identification Post-rank Optimisation</a></p>
<p>7 0.13413016 <a title="124-tfidf-7" href="./iccv-2013-Human_Re-identification_by_Matching_Compositional_Template_with_Cluster_Sampling.html">205 iccv-2013-Human Re-identification by Matching Compositional Template with Cluster Sampling</a></p>
<p>8 0.12500125 <a title="124-tfidf-8" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<p>9 0.12058202 <a title="124-tfidf-9" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>10 0.11640569 <a title="124-tfidf-10" href="./iccv-2013-Person_Re-identification_by_Salience_Matching.html">313 iccv-2013-Person Re-identification by Salience Matching</a></p>
<p>11 0.10963121 <a title="124-tfidf-11" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>12 0.093559861 <a title="124-tfidf-12" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>13 0.089859247 <a title="124-tfidf-13" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>14 0.077954151 <a title="124-tfidf-14" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>15 0.077441186 <a title="124-tfidf-15" href="./iccv-2013-No_Matter_Where_You_Are%3A_Flexible_Graph-Guided_Multi-task_Learning_for_Multi-view_Head_Pose_Classification_under_Target_Motion.html">291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</a></p>
<p>16 0.077239402 <a title="124-tfidf-16" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>17 0.072669357 <a title="124-tfidf-17" href="./iccv-2013-Implied_Feedback%3A_Learning_Nuances_of_User_Behavior_in_Image_Search.html">213 iccv-2013-Implied Feedback: Learning Nuances of User Behavior in Image Search</a></p>
<p>18 0.070991613 <a title="124-tfidf-18" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>19 0.07002116 <a title="124-tfidf-19" href="./iccv-2013-Finding_Actors_and_Actions_in_Movies.html">166 iccv-2013-Finding Actors and Actions in Movies</a></p>
<p>20 0.069974795 <a title="124-tfidf-20" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.162), (1, 0.041), (2, -0.05), (3, -0.051), (4, -0.004), (5, -0.012), (6, -0.003), (7, 0.035), (8, 0.056), (9, 0.059), (10, -0.044), (11, -0.161), (12, -0.056), (13, -0.07), (14, 0.176), (15, -0.192), (16, -0.069), (17, -0.022), (18, 0.007), (19, -0.038), (20, 0.192), (21, -0.149), (22, 0.068), (23, 0.07), (24, 0.09), (25, -0.006), (26, -0.144), (27, -0.074), (28, -0.056), (29, -0.045), (30, 0.06), (31, -0.009), (32, 0.045), (33, 0.022), (34, 0.054), (35, -0.001), (36, -0.039), (37, 0.026), (38, 0.079), (39, 0.084), (40, 0.005), (41, -0.045), (42, 0.027), (43, -0.055), (44, -0.003), (45, 0.025), (46, -0.017), (47, 0.016), (48, 0.021), (49, 0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97864276 <a title="124-lsi-1" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>Author: Andy J. Ma, Pong C. Yuen, Jiawei Li</p><p>Abstract: This paper addresses a new person re-identification problem without the label information of persons under non-overlapping target cameras. Given the matched (positive) and unmatched (negative) image pairs from source domain cameras, as well as unmatched (negative) image pairs which can be easily generated from target domain cameras, we propose a Domain Transfer Ranked Support Vector Machines (DTRSVM) method for re-identification under target domain cameras. To overcome the problems introduced due to the absence of matched (positive) image pairs in target domain, we relax the discriminative constraint to a necessary condition only relying on the positive mean in target domain. By estimating the target positive mean using source and target domain data, a new discriminative model with high confidence in target positive mean and low confidence in target negative image pairs is developed. Since the necessary condition may not truly preserve the discriminability, multi-task support vector ranking is proposed to incorporate the training data from source domain with label information. Experimental results show that the proposed DTRSVM outperforms existing methods without using label information in target cameras. And the top 30 rank accuracy can be improved by the proposed method upto 9.40% on publicly available person re-identification datasets.</p><p>2 0.8839972 <a title="124-lsi-2" href="./iccv-2013-Unsupervised_Visual_Domain_Adaptation_Using_Subspace_Alignment.html">438 iccv-2013-Unsupervised Visual Domain Adaptation Using Subspace Alignment</a></p>
<p>Author: Basura Fernando, Amaury Habrard, Marc Sebban, Tinne Tuytelaars</p><p>Abstract: In this paper, we introduce a new domain adaptation (DA) algorithm where the source and target domains are represented by subspaces described by eigenvectors. In this context, our method seeks a domain adaptation solution by learning a mapping function which aligns the source subspace with the target one. We show that the solution of the corresponding optimization problem can be obtained in a simple closed form, leading to an extremely fast algorithm. We use a theoretical result to tune the unique hyperparameter corresponding to the size of the subspaces. We run our method on various datasets and show that, despite its intrinsic simplicity, it outperforms state of the art DA methods.</p><p>3 0.8835339 <a title="124-lsi-3" href="./iccv-2013-Unsupervised_Domain_Adaptation_by_Domain_Invariant_Projection.html">435 iccv-2013-Unsupervised Domain Adaptation by Domain Invariant Projection</a></p>
<p>Author: Mahsa Baktashmotlagh, Mehrtash T. Harandi, Brian C. Lovell, Mathieu Salzmann</p><p>Abstract: Domain-invariant representations are key to addressing the domain shift problem where the training and test examples follow different distributions. Existing techniques that have attempted to match the distributions of the source and target domains typically compare these distributions in the original feature space. This space, however, may not be directly suitable for such a comparison, since some of the features may have been distorted by the domain shift, or may be domain specific. In this paper, we introduce a Domain Invariant Projection approach: An unsupervised domain adaptation method that overcomes this issue by extracting the information that is invariant across the source and target domains. More specifically, we learn a projection of the data to a low-dimensional latent space where the distance between the empirical distributions of the source and target examples is minimized. We demonstrate the effectiveness of our approach on the task of visual object recognition and show that it outperforms state-of-the-art methods on a standard domain adaptation benchmark dataset.</p><p>4 0.86945981 <a title="124-lsi-4" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>Author: Fatemeh Mirrashed, Mohammad Rastegari</p><p>Abstract: We propose an unsupervised domain adaptation method that exploits intrinsic compact structures of categories across different domains using binary attributes. Our method directly optimizes for classification in the target domain. The key insight is finding attributes that are discriminative across categories and predictable across domains. We achieve a performance that significantly exceeds the state-of-the-art results on standard benchmarks. In fact, in many cases, our method reaches the same-domain performance, the upper bound, in unsupervised domain adaptation scenarios.</p><p>5 0.83738565 <a title="124-lsi-5" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>Author: Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, Philip S. Yu</p><p>Abstract: Transfer learning is established as an effective technology in computer visionfor leveraging rich labeled data in the source domain to build an accurate classifier for the target domain. However, most prior methods have not simultaneously reduced the difference in both the marginal distribution and conditional distribution between domains. In this paper, we put forward a novel transfer learning approach, referred to as Joint Distribution Adaptation (JDA). Specifically, JDA aims to jointly adapt both the marginal distribution and conditional distribution in a principled dimensionality reduction procedure, and construct new feature representation that is effective and robustfor substantial distribution difference. Extensive experiments verify that JDA can significantly outperform several state-of-the-art methods on four types of cross-domain image classification problems.</p><p>6 0.81389034 <a title="124-lsi-6" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<p>7 0.76452482 <a title="124-lsi-7" href="./iccv-2013-Cross-View_Action_Recognition_over_Heterogeneous_Feature_Spaces.html">99 iccv-2013-Cross-View Action Recognition over Heterogeneous Feature Spaces</a></p>
<p>8 0.6360935 <a title="124-lsi-8" href="./iccv-2013-Target-Driven_Moire_Pattern_Synthesis_by_Phase_Modulation.html">413 iccv-2013-Target-Driven Moire Pattern Synthesis by Phase Modulation</a></p>
<p>9 0.557298 <a title="124-lsi-9" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>10 0.51415938 <a title="124-lsi-10" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>11 0.4896135 <a title="124-lsi-11" href="./iccv-2013-From_Semi-supervised_to_Transfer_Counting_of_Crowds.html">178 iccv-2013-From Semi-supervised to Transfer Counting of Crowds</a></p>
<p>12 0.46356389 <a title="124-lsi-12" href="./iccv-2013-An_Adaptive_Descriptor_Design_for_Object_Recognition_in_the_Wild.html">48 iccv-2013-An Adaptive Descriptor Design for Object Recognition in the Wild</a></p>
<p>13 0.45989287 <a title="124-lsi-13" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>14 0.43966013 <a title="124-lsi-14" href="./iccv-2013-Unbiased_Metric_Learning%3A_On_the_Utilization_of_Multiple_Datasets_and_Web_Images_for_Softening_Bias.html">431 iccv-2013-Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias</a></p>
<p>15 0.42394063 <a title="124-lsi-15" href="./iccv-2013-Person_Re-identification_by_Salience_Matching.html">313 iccv-2013-Person Re-identification by Salience Matching</a></p>
<p>16 0.40315527 <a title="124-lsi-16" href="./iccv-2013-Implied_Feedback%3A_Learning_Nuances_of_User_Behavior_in_Image_Search.html">213 iccv-2013-Implied Feedback: Learning Nuances of User Behavior in Image Search</a></p>
<p>17 0.40169516 <a title="124-lsi-17" href="./iccv-2013-Coupled_Dictionary_and_Feature_Space_Learning_with_Applications_to_Cross-Domain_Image_Synthesis_and_Recognition.html">96 iccv-2013-Coupled Dictionary and Feature Space Learning with Applications to Cross-Domain Image Synthesis and Recognition</a></p>
<p>18 0.39570898 <a title="124-lsi-18" href="./iccv-2013-POP%3A_Person_Re-identification_Post-rank_Optimisation.html">305 iccv-2013-POP: Person Re-identification Post-rank Optimisation</a></p>
<p>19 0.39284435 <a title="124-lsi-19" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>20 0.38229495 <a title="124-lsi-20" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.055), (7, 0.025), (12, 0.031), (26, 0.058), (31, 0.039), (42, 0.127), (48, 0.011), (64, 0.106), (66, 0.232), (69, 0.032), (73, 0.03), (89, 0.128), (98, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75062925 <a title="124-lda-1" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>Author: Andy J. Ma, Pong C. Yuen, Jiawei Li</p><p>Abstract: This paper addresses a new person re-identification problem without the label information of persons under non-overlapping target cameras. Given the matched (positive) and unmatched (negative) image pairs from source domain cameras, as well as unmatched (negative) image pairs which can be easily generated from target domain cameras, we propose a Domain Transfer Ranked Support Vector Machines (DTRSVM) method for re-identification under target domain cameras. To overcome the problems introduced due to the absence of matched (positive) image pairs in target domain, we relax the discriminative constraint to a necessary condition only relying on the positive mean in target domain. By estimating the target positive mean using source and target domain data, a new discriminative model with high confidence in target positive mean and low confidence in target negative image pairs is developed. Since the necessary condition may not truly preserve the discriminability, multi-task support vector ranking is proposed to incorporate the training data from source domain with label information. Experimental results show that the proposed DTRSVM outperforms existing methods without using label information in target cameras. And the top 30 rank accuracy can be improved by the proposed method upto 9.40% on publicly available person re-identification datasets.</p><p>2 0.73521417 <a title="124-lda-2" href="./iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding.html">14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</a></p>
<p>Author: Wangmeng Zuo, Deyu Meng, Lei Zhang, Xiangchu Feng, David Zhang</p><p>Abstract: In many sparse coding based image restoration and image classification problems, using non-convex ?p-norm minimization (0 ≤ p < 1) can often obtain better results than timhei convex 0?1 -norm m 1)ini camniza otfiteonn. Ab naiunm bbeetrt of algorithms, e.g., iteratively reweighted least squares (IRLS), iteratively thresholding method (ITM-?p), and look-up table (LUT), have been proposed for non-convex ?p-norm sparse coding, while some analytic solutions have been suggested for some specific values of p. In this paper, by extending the popular soft-thresholding operator, we propose a generalized iterated shrinkage algorithm (GISA) for ?p-norm non-convex sparse coding. Unlike the analytic solutions, the proposed GISA algorithm is easy to implement, and can be adopted for solving non-convex sparse coding problems with arbitrary p values. Compared with LUT, GISA is more general and does not need to compute and store the look-up tables. Compared with IRLS and ITM-?p, GISA is theoretically more solid and can achieve more accurate solutions. Experiments on image restoration and sparse coding based face recognition are conducted to validate the performance of GISA. ××</p><p>3 0.67324018 <a title="124-lda-3" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>Author: Sukrit Shankar, Joan Lasenby, Roberto Cipolla</p><p>Abstract: Relative (comparative) attributes are promising for thematic ranking of visual entities, which also aids in recognition tasks [19, 23]. However, attribute rank learning often requires a substantial amount of relational supervision, which is highly tedious, and apparently impracticalfor realworld applications. In this paper, we introduce the Semantic Transform, which under minimal supervision, adaptively finds a semantic feature space along with a class ordering that is related in the best possible way. Such a semantic space is found for every attribute category. To relate the classes under weak supervision, the class ordering needs to be refined according to a cost function in an iterative procedure. This problem is ideally NP-hard, and we thus propose a constrained search tree formulation for the same. Driven by the adaptive semantic feature space representation, our model achieves the best results to date for all of the tasks of relative, absolute and zero-shot classification on two popular datasets.</p><p>4 0.67291325 <a title="124-lda-4" href="./iccv-2013-Incorporating_Cloud_Distribution_in_Sky_Representation.html">215 iccv-2013-Incorporating Cloud Distribution in Sky Representation</a></p>
<p>Author: Kuan-Chuan Peng, Tsuhan Chen</p><p>Abstract: Most sky models only describe the cloudiness ofthe overall sky by a single category or parameter such as sky index, which does not account for the distribution of the clouds across the sky. To capture variable cloudiness, we extend the concept of sky index to a random field indicating the level of cloudiness of each sky pixel in our proposed sky representation based on the Igawa sky model. We formulate the problem of solving the sky index of every sky pixel as a labeling problem, where an approximate solution can be efficiently found. Experimental results show that our proposed sky model has better expressiveness, stability with respect to variation in camera parameters, and geo-location estimation in outdoor images compared to the uniform sky index model. Potential applications of our proposed sky model include sky image rendering, where sky images can be generated with an arbitrary cloud distribution at any time and any location, previously impossible with traditional sky models.</p><p>5 0.67176771 <a title="124-lda-5" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>Author: Qinxun Bai, Zheng Wu, Stan Sclaroff, Margrit Betke, Camille Monnier</p><p>Abstract: We propose a randomized ensemble algorithm to model the time-varying appearance of an object for visual tracking. In contrast with previous online methods for updating classifier ensembles in tracking-by-detection, the weight vector that combines weak classifiers is treated as a random variable and the posterior distribution for the weight vector is estimated in a Bayesian manner. In essence, the weight vector is treated as a distribution that reflects the confidence among the weak classifiers used to construct and adapt the classifier ensemble. The resulting formulation models the time-varying discriminative ability among weak classifiers so that the ensembled strong classifier can adapt to the varying appearance, backgrounds, and occlusions. The formulation is tested in a tracking-by-detection implementation. Experiments on 28 challenging benchmark videos demonstrate that the proposed method can achieve results comparable to and often better than those of stateof-the-art approaches.</p><p>6 0.67032862 <a title="124-lda-6" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>7 0.66870862 <a title="124-lda-7" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>8 0.66417569 <a title="124-lda-8" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>9 0.66375601 <a title="124-lda-9" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>10 0.66329545 <a title="124-lda-10" href="./iccv-2013-Orderless_Tracking_through_Model-Averaged_Posterior_Estimation.html">303 iccv-2013-Orderless Tracking through Model-Averaged Posterior Estimation</a></p>
<p>11 0.66239464 <a title="124-lda-11" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>12 0.66104108 <a title="124-lda-12" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>13 0.6600284 <a title="124-lda-13" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>14 0.65983915 <a title="124-lda-14" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>15 0.65899324 <a title="124-lda-15" href="./iccv-2013-Finding_Actors_and_Actions_in_Movies.html">166 iccv-2013-Finding Actors and Actions in Movies</a></p>
<p>16 0.65885752 <a title="124-lda-16" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>17 0.65645528 <a title="124-lda-17" href="./iccv-2013-Bayesian_Joint_Topic_Modelling_for_Weakly_Supervised_Object_Localisation.html">59 iccv-2013-Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation</a></p>
<p>18 0.65584755 <a title="124-lda-18" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>19 0.65563965 <a title="124-lda-19" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>20 0.65473229 <a title="124-lda-20" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
