<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>86 iccv-2013-Concurrent Action Detection with Structural Prediction</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-86" href="#">iccv2013-86</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>86 iccv-2013-Concurrent Action Detection with Structural Prediction</h1>
<br/><p>Source: <a title="iccv-2013-86-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Wei_Concurrent_Action_Detection_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Ping Wei, Nanning Zheng, Yibiao Zhao, Song-Chun Zhu</p><p>Abstract: Action recognition has often been posed as a classification problem, which assumes that a video sequence only have one action class label and different actions are independent. However, a single human body can perform multiple concurrent actions at the same time, and different actions interact with each other. This paper proposes a concurrent action detection model where the action detection is formulated as a structural prediction problem. In this model, an interval in a video sequence can be described by multiple action labels. An detected action interval is determined both by the unary local detector and the relations with other actions. We use a wavelet feature to represent the action sequence, and design a composite temporal logic descriptor to describe the action relations. The model parameters are trained by structural SVM learning. Given a long video sequence, a sequential decision window search algorithm is designed to detect the actions. Experiments on our new collected concurrent action dataset demonstrate the strength of our method.</p><p>Reference: <a title="iccv-2013-86-reference" href="../iccv2013_reference/iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('trash', 0.38), ('interv', 0.334), ('act', 0.247), ('throw', 0.221), ('coa', 0.211), ('temp', 0.201), ('wavelet', 0.193), ('sequ', 0.172), ('dj', 0.161), ('ds', 0.148), ('yi', 0.142), ('mcol', 0.136), ('keyboard', 0.135), ('rij', 0.129), ('body', 0.12), ('bend', 0.113), ('rics', 0.109), ('ries', 0.109), ('structural', 0.097), ('fyi', 0.097)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="86-tfidf-1" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>Author: Ping Wei, Nanning Zheng, Yibiao Zhao, Song-Chun Zhu</p><p>Abstract: Action recognition has often been posed as a classification problem, which assumes that a video sequence only have one action class label and different actions are independent. However, a single human body can perform multiple concurrent actions at the same time, and different actions interact with each other. This paper proposes a concurrent action detection model where the action detection is formulated as a structural prediction problem. In this model, an interval in a video sequence can be described by multiple action labels. An detected action interval is determined both by the unary local detector and the relations with other actions. We use a wavelet feature to represent the action sequence, and design a composite temporal logic descriptor to describe the action relations. The model parameters are trained by structural SVM learning. Given a long video sequence, a sequential decision window search algorithm is designed to detect the actions. Experiments on our new collected concurrent action dataset demonstrate the strength of our method.</p><p>2 0.2690523 <a title="86-tfidf-2" href="./iccv-2013-Event_Detection_in_Complex_Scenes_Using_Interval_Temporal_Constraints.html">146 iccv-2013-Event Detection in Complex Scenes Using Interval Temporal Constraints</a></p>
<p>Author: Yifan Zhang, Qiang Ji, Hanqing Lu</p><p>Abstract: In complex scenes with multiple atomic events happening sequentially or in parallel, detecting each individual event separately may not always obtain robust and reliable result. It is essential to detect them in a holistic way which incorporates the causality and temporal dependency among them to compensate the limitation of current computer vision techniques. In this paper, we propose an interval temporal constrained dynamic Bayesian network to extendAllen ’s interval algebra network (IAN) [2]from a deterministic static model to a probabilistic dynamic system, which can not only capture the complex interval temporal relationships, but also model the evolution dynamics and handle the uncertainty from the noisy visual observation. In the model, the topology of the IAN on each time slice and the interlinks between the time slices are discovered by an advanced structure learning method. The duration of the event and the unsynchronized time lags between two correlated event intervals are captured by a duration model, so that we can better determine the temporal boundary of the event. Empirical results on two real world datasets show the power of the proposed interval temporal constrained model.</p><p>3 0.23010458 <a title="86-tfidf-3" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>Author: Mihai Zanfir, Marius Leordeanu, Cristian Sminchisescu</p><p>Abstract: Human action recognition under low observational latency is receiving a growing interest in computer vision due to rapidly developing technologies in human-robot interaction, computer gaming and surveillance. In this paper we propose a fast, simple, yet powerful non-parametric Moving Pose (MP)frameworkfor low-latency human action and activity recognition. Central to our methodology is a moving pose descriptor that considers both pose information as well as differential quantities (speed and acceleration) of the human body joints within a short time window around the current frame. The proposed descriptor is used in conjunction with a modified kNN classifier that considers both the temporal location of a particular frame within the action sequence as well as the discrimination power of its moving pose descriptor compared to other frames in the training set. The resulting method is non-parametric and enables low-latency recognition, one-shot learning, and action detection in difficult unsegmented sequences. Moreover, the framework is real-time, scalable, and outperforms more sophisticated approaches on challenging benchmarks like MSR-Action3D or MSR-DailyActivities3D.</p><p>4 0.20786402 <a title="86-tfidf-4" href="./iccv-2013-Modeling_4D_Human-Object_Interactions_for_Event_and_Object_Recognition.html">268 iccv-2013-Modeling 4D Human-Object Interactions for Event and Object Recognition</a></p>
<p>Author: Ping Wei, Yibiao Zhao, Nanning Zheng, Song-Chun Zhu</p><p>Abstract: Recognizing the events and objects in the video sequence are two challenging tasks due to the complex temporal structures and the large appearance variations. In this paper, we propose a 4D human-object interaction model, where the two tasks jointly boost each other. Our human-object interaction is defined in 4D space: i) the cooccurrence and geometric constraints of human pose and object in 3D space; ii) the sub-events transition and objects coherence in 1D temporal dimension. We represent the structure of events, sub-events and objects in a hierarchical graph. For an input RGB-depth video, we design a dynamic programming beam search algorithm to: i) segment the video, ii) recognize the events, and iii) detect the objects simultaneously. For evaluation, we built a large-scale multiview 3D event dataset which contains 3815 video sequences and 383,036 RGBD frames captured by the Kinect cameras. The experiment results on this dataset show the effectiveness of our method.</p><p>5 0.19573604 <a title="86-tfidf-5" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>Author: Sunil Bandla, Kristen Grauman</p><p>Abstract: Collecting and annotating videos of realistic human actions is tedious, yet critical for training action recognition systems. We propose a method to actively request the most useful video annotations among a large set of unlabeled videos. Predicting the utility of annotating unlabeled video is not trivial, since any given clip may contain multiple actions of interest, and it need not be trimmed to temporal regions of interest. To deal with this problem, we propose a detection-based active learner to train action category models. We develop a voting-based framework to localize likely intervals of interest in an unlabeled clip, and use them to estimate the total reduction in uncertainty that annotating that clip would yield. On three datasets, we show our approach can learn accurate action detectors more efficiently than alternative active learning strategies that fail to accommodate the “untrimmed” nature of real video data.</p><p>6 0.17139921 <a title="86-tfidf-6" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>7 0.16073374 <a title="86-tfidf-7" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>8 0.1476251 <a title="86-tfidf-8" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>9 0.13708135 <a title="86-tfidf-9" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>10 0.12224338 <a title="86-tfidf-10" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>11 0.12084582 <a title="86-tfidf-11" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>12 0.11938119 <a title="86-tfidf-12" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>13 0.11646704 <a title="86-tfidf-13" href="./iccv-2013-Space-Time_Tradeoffs_in_Photo_Sequencing.html">397 iccv-2013-Space-Time Tradeoffs in Photo Sequencing</a></p>
<p>14 0.10293862 <a title="86-tfidf-14" href="./iccv-2013-Strong_Appearance_and_Expressive_Spatial_Models_for_Human_Pose_Estimation.html">403 iccv-2013-Strong Appearance and Expressive Spatial Models for Human Pose Estimation</a></p>
<p>15 0.10106877 <a title="86-tfidf-15" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>16 0.1007178 <a title="86-tfidf-16" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>17 0.095584936 <a title="86-tfidf-17" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>18 0.09442991 <a title="86-tfidf-18" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>19 0.093602799 <a title="86-tfidf-19" href="./iccv-2013-Active_Visual_Recognition_with_Expertise_Estimation_in_Crowdsourcing.html">43 iccv-2013-Active Visual Recognition with Expertise Estimation in Crowdsourcing</a></p>
<p>20 0.090078123 <a title="86-tfidf-20" href="./iccv-2013-Real-Time_Body_Tracking_with_One_Depth_Camera_and_Inertial_Sensors.html">341 iccv-2013-Real-Time Body Tracking with One Depth Camera and Inertial Sensors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.221), (1, -0.026), (2, 0.005), (3, -0.103), (4, 0.032), (5, -0.021), (6, 0.08), (7, 0.005), (8, -0.011), (9, -0.184), (10, 0.073), (11, -0.093), (12, -0.103), (13, -0.054), (14, 0.013), (15, 0.021), (16, -0.193), (17, -0.121), (18, 0.038), (19, 0.072), (20, -0.032), (21, 0.049), (22, 0.052), (23, 0.009), (24, -0.035), (25, -0.038), (26, 0.081), (27, -0.071), (28, 0.034), (29, 0.031), (30, 0.041), (31, 0.046), (32, 0.052), (33, 0.051), (34, 0.027), (35, 0.06), (36, -0.004), (37, -0.026), (38, -0.049), (39, -0.013), (40, -0.045), (41, -0.069), (42, -0.009), (43, -0.027), (44, -0.005), (45, -0.014), (46, -0.069), (47, -0.074), (48, 0.047), (49, 0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95893961 <a title="86-lsi-1" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>Author: Ping Wei, Nanning Zheng, Yibiao Zhao, Song-Chun Zhu</p><p>Abstract: Action recognition has often been posed as a classification problem, which assumes that a video sequence only have one action class label and different actions are independent. However, a single human body can perform multiple concurrent actions at the same time, and different actions interact with each other. This paper proposes a concurrent action detection model where the action detection is formulated as a structural prediction problem. In this model, an interval in a video sequence can be described by multiple action labels. An detected action interval is determined both by the unary local detector and the relations with other actions. We use a wavelet feature to represent the action sequence, and design a composite temporal logic descriptor to describe the action relations. The model parameters are trained by structural SVM learning. Given a long video sequence, a sequential decision window search algorithm is designed to detect the actions. Experiments on our new collected concurrent action dataset demonstrate the strength of our method.</p><p>2 0.88951719 <a title="86-lsi-2" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>Author: Jiang Wang, Ying Wu</p><p>Abstract: Temporal misalignment and duration variation in video actions largely influence the performance of action recognition, but it is very difficult to specify effective temporal alignment on action sequences. To address this challenge, this paper proposes a novel discriminative learning-based temporal alignment method, called maximum margin temporal warping (MMTW), to align two action sequences and measure their matching score. Based on the latent structure SVM formulation, the proposed MMTW method is able to learn a phantom action template to represent an action class for maximum discrimination against other classes. The recognition of this action class is based on the associated learned alignment of the input action. Extensive experiments on five benchmark datasets have demonstrated that this MMTW model is able to significantly promote the accuracy and robustness of action recognition under temporal misalignment and variations.</p><p>3 0.85828775 <a title="86-lsi-3" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>Author: Mihai Zanfir, Marius Leordeanu, Cristian Sminchisescu</p><p>Abstract: Human action recognition under low observational latency is receiving a growing interest in computer vision due to rapidly developing technologies in human-robot interaction, computer gaming and surveillance. In this paper we propose a fast, simple, yet powerful non-parametric Moving Pose (MP)frameworkfor low-latency human action and activity recognition. Central to our methodology is a moving pose descriptor that considers both pose information as well as differential quantities (speed and acceleration) of the human body joints within a short time window around the current frame. The proposed descriptor is used in conjunction with a modified kNN classifier that considers both the temporal location of a particular frame within the action sequence as well as the discrimination power of its moving pose descriptor compared to other frames in the training set. The resulting method is non-parametric and enables low-latency recognition, one-shot learning, and action detection in difficult unsegmented sequences. Moreover, the framework is real-time, scalable, and outperforms more sophisticated approaches on challenging benchmarks like MSR-Action3D or MSR-DailyActivities3D.</p><p>4 0.82628518 <a title="86-lsi-4" href="./iccv-2013-Event_Detection_in_Complex_Scenes_Using_Interval_Temporal_Constraints.html">146 iccv-2013-Event Detection in Complex Scenes Using Interval Temporal Constraints</a></p>
<p>Author: Yifan Zhang, Qiang Ji, Hanqing Lu</p><p>Abstract: In complex scenes with multiple atomic events happening sequentially or in parallel, detecting each individual event separately may not always obtain robust and reliable result. It is essential to detect them in a holistic way which incorporates the causality and temporal dependency among them to compensate the limitation of current computer vision techniques. In this paper, we propose an interval temporal constrained dynamic Bayesian network to extendAllen ’s interval algebra network (IAN) [2]from a deterministic static model to a probabilistic dynamic system, which can not only capture the complex interval temporal relationships, but also model the evolution dynamics and handle the uncertainty from the noisy visual observation. In the model, the topology of the IAN on each time slice and the interlinks between the time slices are discovered by an advanced structure learning method. The duration of the event and the unsynchronized time lags between two correlated event intervals are captured by a duration model, so that we can better determine the temporal boundary of the event. Empirical results on two real world datasets show the power of the proposed interval temporal constrained model.</p><p>5 0.82094008 <a title="86-lsi-5" href="./iccv-2013-Modeling_4D_Human-Object_Interactions_for_Event_and_Object_Recognition.html">268 iccv-2013-Modeling 4D Human-Object Interactions for Event and Object Recognition</a></p>
<p>Author: Ping Wei, Yibiao Zhao, Nanning Zheng, Song-Chun Zhu</p><p>Abstract: Recognizing the events and objects in the video sequence are two challenging tasks due to the complex temporal structures and the large appearance variations. In this paper, we propose a 4D human-object interaction model, where the two tasks jointly boost each other. Our human-object interaction is defined in 4D space: i) the cooccurrence and geometric constraints of human pose and object in 3D space; ii) the sub-events transition and objects coherence in 1D temporal dimension. We represent the structure of events, sub-events and objects in a hierarchical graph. For an input RGB-depth video, we design a dynamic programming beam search algorithm to: i) segment the video, ii) recognize the events, and iii) detect the objects simultaneously. For evaluation, we built a large-scale multiview 3D event dataset which contains 3815 video sequences and 383,036 RGBD frames captured by the Kinect cameras. The experiment results on this dataset show the effectiveness of our method.</p><p>6 0.8128463 <a title="86-lsi-6" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>7 0.8002466 <a title="86-lsi-7" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>8 0.7887724 <a title="86-lsi-8" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>9 0.78017235 <a title="86-lsi-9" href="./iccv-2013-From_Actemes_to_Action%3A_A_Strongly-Supervised_Representation_for_Detailed_Action_Understanding.html">175 iccv-2013-From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding</a></p>
<p>10 0.74908119 <a title="86-lsi-10" href="./iccv-2013-Latent_Multitask_Learning_for_View-Invariant_Action_Recognition.html">231 iccv-2013-Latent Multitask Learning for View-Invariant Action Recognition</a></p>
<p>11 0.73756051 <a title="86-lsi-11" href="./iccv-2013-ACTIVE%3A_Activity_Concept_Transitions_in_Video_Event_Classification.html">4 iccv-2013-ACTIVE: Activity Concept Transitions in Video Event Classification</a></p>
<p>12 0.7359699 <a title="86-lsi-12" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>13 0.72700417 <a title="86-lsi-13" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>14 0.7133857 <a title="86-lsi-14" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>15 0.70935452 <a title="86-lsi-15" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>16 0.70874089 <a title="86-lsi-16" href="./iccv-2013-Learning_Slow_Features_for_Behaviour_Analysis.html">243 iccv-2013-Learning Slow Features for Behaviour Analysis</a></p>
<p>17 0.69507539 <a title="86-lsi-17" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>18 0.67593974 <a title="86-lsi-18" href="./iccv-2013-Action_Recognition_with_Actons.html">38 iccv-2013-Action Recognition with Actons</a></p>
<p>19 0.65823644 <a title="86-lsi-19" href="./iccv-2013-Learning_to_Share_Latent_Tasks_for_Action_Recognition.html">249 iccv-2013-Learning to Share Latent Tasks for Action Recognition</a></p>
<p>20 0.65201986 <a title="86-lsi-20" href="./iccv-2013-Space-Time_Tradeoffs_in_Photo_Sequencing.html">397 iccv-2013-Space-Time Tradeoffs in Photo Sequencing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(12, 0.01), (13, 0.073), (20, 0.065), (25, 0.032), (42, 0.154), (48, 0.141), (54, 0.251), (60, 0.017), (77, 0.167)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.76983261 <a title="86-lda-1" href="./iccv-2013-Conservation_Tracking.html">87 iccv-2013-Conservation Tracking</a></p>
<p>Author: Martin Schiegg, Philipp Hanslovsky, Bernhard X. Kausler, Lars Hufnagel, Fred A. Hamprecht</p><p>Abstract: The quality of any tracking-by-assignment hinges on the accuracy of the foregoing target detection / segmentation step. In many kinds of images, errors in this first stage are unavoidable. These errors then propagate to, and corrupt, the tracking result. Our main contribution is the first probabilistic graphical model that can explicitly account for over- and undersegmentation errors even when the number of tracking targets is unknown and when they may divide, as in cell cultures. The tracking model we present implements global consistency constraints for the number of targets comprised by each detection and is solved to global optimality on reasonably large 2D+t and 3D+t datasets. In addition, we empirically demonstrate the effectiveness of a postprocessing that allows to establish target identity even across occlusion / undersegmentation. The usefulness and efficiency of this new tracking method is demonstrated on three different and challenging 2D+t and 3D+t datasets from developmental biology.</p><p>2 0.76632136 <a title="86-lda-2" href="./iccv-2013-Unsupervised_Intrinsic_Calibration_from_a_Single_Frame_Using_a_%22Plumb-Line%22_Approach.html">436 iccv-2013-Unsupervised Intrinsic Calibration from a Single Frame Using a "Plumb-Line" Approach</a></p>
<p>Author: R. Melo, M. Antunes, J.P. Barreto, G. Falcão, N. Gonçalves</p><p>Abstract: Estimating the amount and center ofdistortionfrom lines in the scene has been addressed in the literature by the socalled “plumb-line ” approach. In this paper we propose a new geometric method to estimate not only the distortion parameters but the entire camera calibration (up to an “angular” scale factor) using a minimum of 3 lines. We propose a new framework for the unsupervised simultaneous detection of natural image of lines and camera parameters estimation, enabling a robust calibration from a single image. Comparative experiments with existing automatic approaches for the distortion estimation and with ground truth data are presented.</p><p>same-paper 3 0.74401635 <a title="86-lda-3" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>Author: Ping Wei, Nanning Zheng, Yibiao Zhao, Song-Chun Zhu</p><p>Abstract: Action recognition has often been posed as a classification problem, which assumes that a video sequence only have one action class label and different actions are independent. However, a single human body can perform multiple concurrent actions at the same time, and different actions interact with each other. This paper proposes a concurrent action detection model where the action detection is formulated as a structural prediction problem. In this model, an interval in a video sequence can be described by multiple action labels. An detected action interval is determined both by the unary local detector and the relations with other actions. We use a wavelet feature to represent the action sequence, and design a composite temporal logic descriptor to describe the action relations. The model parameters are trained by structural SVM learning. Given a long video sequence, a sequential decision window search algorithm is designed to detect the actions. Experiments on our new collected concurrent action dataset demonstrate the strength of our method.</p><p>4 0.73771906 <a title="86-lda-4" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>Author: Olaf Kähler, Ian Reid</p><p>Abstract: We address the problem of 3D scene labeling in a structured learning framework. Unlike previous work which uses structured Support VectorMachines, we employ the recently described Decision Tree Field and Regression Tree Field frameworks, which learn the unary and binary terms of a Conditional Random Field from training data. We show this has significant advantages in terms of inference speed, while maintaining similar accuracy. We also demonstrate empirically the importance for overall labeling accuracy of features that make use of prior knowledge about the coarse scene layout such as the location of the ground plane. We show how this coarse layout can be estimated by our framework automatically, and that this information can be used to bootstrap improved accuracy in the detailed labeling.</p><p>5 0.73414981 <a title="86-lda-5" href="./iccv-2013-Event_Detection_in_Complex_Scenes_Using_Interval_Temporal_Constraints.html">146 iccv-2013-Event Detection in Complex Scenes Using Interval Temporal Constraints</a></p>
<p>Author: Yifan Zhang, Qiang Ji, Hanqing Lu</p><p>Abstract: In complex scenes with multiple atomic events happening sequentially or in parallel, detecting each individual event separately may not always obtain robust and reliable result. It is essential to detect them in a holistic way which incorporates the causality and temporal dependency among them to compensate the limitation of current computer vision techniques. In this paper, we propose an interval temporal constrained dynamic Bayesian network to extendAllen ’s interval algebra network (IAN) [2]from a deterministic static model to a probabilistic dynamic system, which can not only capture the complex interval temporal relationships, but also model the evolution dynamics and handle the uncertainty from the noisy visual observation. In the model, the topology of the IAN on each time slice and the interlinks between the time slices are discovered by an advanced structure learning method. The duration of the event and the unsynchronized time lags between two correlated event intervals are captured by a duration model, so that we can better determine the temporal boundary of the event. Empirical results on two real world datasets show the power of the proposed interval temporal constrained model.</p><p>6 0.73400152 <a title="86-lda-6" href="./iccv-2013-Dynamic_Structured_Model_Selection.html">130 iccv-2013-Dynamic Structured Model Selection</a></p>
<p>7 0.73327017 <a title="86-lda-7" href="./iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding.html">14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</a></p>
<p>8 0.73268104 <a title="86-lda-8" href="./iccv-2013-Tree_Shape_Priors_with_Connectivity_Constraints_Using_Convex_Relaxation_on_General_Graphs.html">429 iccv-2013-Tree Shape Priors with Connectivity Constraints Using Convex Relaxation on General Graphs</a></p>
<p>9 0.73031896 <a title="86-lda-9" href="./iccv-2013-Learning_CRFs_for_Image_Parsing_with_Adaptive_Subgradient_Descent.html">234 iccv-2013-Learning CRFs for Image Parsing with Adaptive Subgradient Descent</a></p>
<p>10 0.72982836 <a title="86-lda-10" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>11 0.72919524 <a title="86-lda-11" href="./iccv-2013-Robust_Non-parametric_Data_Fitting_for_Correspondence_Modeling.html">358 iccv-2013-Robust Non-parametric Data Fitting for Correspondence Modeling</a></p>
<p>12 0.72565138 <a title="86-lda-12" href="./iccv-2013-Modeling_4D_Human-Object_Interactions_for_Event_and_Object_Recognition.html">268 iccv-2013-Modeling 4D Human-Object Interactions for Event and Object Recognition</a></p>
<p>13 0.72089881 <a title="86-lda-13" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>14 0.72081405 <a title="86-lda-14" href="./iccv-2013-Finding_Causal_Interactions_in_Video_Sequences.html">167 iccv-2013-Finding Causal Interactions in Video Sequences</a></p>
<p>15 0.71856534 <a title="86-lda-15" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>16 0.71708208 <a title="86-lda-16" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>17 0.71571207 <a title="86-lda-17" href="./iccv-2013-Lifting_3D_Manhattan_Lines_from_a_Single_Image.html">250 iccv-2013-Lifting 3D Manhattan Lines from a Single Image</a></p>
<p>18 0.71567065 <a title="86-lda-18" href="./iccv-2013-Fix_Structured_Learning_of_2013_ICCV_paper_k2opt.pdf.html">171 iccv-2013-Fix Structured Learning of 2013 ICCV paper k2opt.pdf</a></p>
<p>19 0.71390289 <a title="86-lda-19" href="./iccv-2013-Coarse-to-Fine_Semantic_Video_Segmentation_Using_Supervoxel_Trees.html">76 iccv-2013-Coarse-to-Fine Semantic Video Segmentation Using Supervoxel Trees</a></p>
<p>20 0.7138474 <a title="86-lda-20" href="./iccv-2013-Complex_3D_General_Object_Reconstruction_from_Line_Drawings.html">84 iccv-2013-Complex 3D General Object Reconstruction from Line Drawings</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
