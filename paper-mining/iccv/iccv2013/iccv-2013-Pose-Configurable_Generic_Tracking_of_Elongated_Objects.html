<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-320" href="#">iccv2013-320</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</h1>
<br/><p>Source: <a title="iccv-2013-320-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Wesierski_Pose-Configurable_Generic_Tracking_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Daniel Wesierski, Patrick Horain</p><p>Abstract: Elongated objects have various shapes and can shift, rotate, change scale, and be rigid or deform by flexing, articulating, and vibrating, with examples as varied as a glass bottle, a robotic arm, a surgical suture, a finger pair, a tram, and a guitar string. This generally makes tracking of poses of elongated objects very challenging. We describe a unified, configurable framework for tracking the pose of elongated objects, which move in the image plane and extend over the image region. Our method strives for simplicity, versatility, and efficiency. The object is decomposed into a chained assembly of segments of multiple parts that are arranged under a hierarchy of tailored spatio-temporal constraints. In this hierarchy, segments can rescale independently while their elasticity is controlled with global orientations and local distances. While the trend in tracking is to design complex, structure-free algorithms that update object appearance on- line, we show that our tracker, with the novel but remarkably simple, structured organization of parts with constant appearance, reaches or improves state-of-the-art performance. Most importantly, our model can be easily configured to track exact pose of arbitrary, elongated objects in the image plane. The tracker can run up to 100 fps on a desktop PC, yet the computation time scales linearly with the number of object parts. To our knowledge, this is the first approach to generic tracking of elongated objects.</p><p>Reference: <a title="iccv-2013-320-reference" href="../iccv2013_reference/iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 we Abstract Elongated objects have various shapes and can shift, rotate, change scale, and be rigid or deform by flexing, articulating, and vibrating, with examples as varied as a glass bottle, a robotic arm, a surgical suture, a finger pair, a tram, and a guitar string. [sent-4, score-0.667]
</p><p>2 This generally makes tracking of poses of elongated objects very challenging. [sent-5, score-0.738]
</p><p>3 We describe a unified, configurable framework for tracking the pose of elongated objects, which move in the image plane and extend over the image region. [sent-6, score-0.765]
</p><p>4 The object is decomposed into a chained assembly of segments of multiple parts that are arranged under a hierarchy of tailored spatio-temporal constraints. [sent-8, score-0.706]
</p><p>5 In this hierarchy, segments can rescale independently while their elasticity is controlled with global orientations and local distances. [sent-9, score-0.213]
</p><p>6 While the trend in tracking is to design complex, structure-free algorithms that update object appearance on-  line, we show that our tracker, with the novel but remarkably simple, structured organization of parts with constant appearance, reaches or improves state-of-the-art performance. [sent-10, score-0.482]
</p><p>7 Most importantly, our model can be easily configured to track exact pose of arbitrary, elongated objects in the image plane. [sent-11, score-0.754]
</p><p>8 The tracker can run up to 100 fps on a desktop PC, yet the computation time scales linearly with the number of object parts. [sent-12, score-0.337]
</p><p>9 To our knowledge, this is the first approach to generic tracking of elongated objects. [sent-13, score-0.721]
</p><p>10 They can move fast under varying illumination and occlusions, in clutter, and deform in the camera projective space due to relaxed rigidity or change in viewpoint. [sent-16, score-0.312]
</p><p>11 Yet, applications requiring pose tracking ofelongated objects are various and span, e. [sent-17, score-0.248]
</p><p>12 Hence, tracking elongated objects is a challenging but important task. [sent-20, score-0.738]
</p><p>13 Our goal is to track with one algorithm poses of plethora of elongated objects varying in shape, motion, and rigidity. [sent-24, score-0.688]
</p><p>14 Our approach decomposes an elongated object into a chained assembly of segments ofmultiple parts that are arranged under a hierarchy of tailored spatio-temporal constraints leveraging local rigidity over object segments. [sent-25, score-1.433]
</p><p>15 As a result, we efficiently track elongated objects that can shift, rotate, change scale, and be rigid or deform by flexing, articulating, and vibrating. [sent-26, score-0.885]
</p><p>16 However, an algorithm that tracks precisely, robustly, and  rapidly a plethora of elongated objects varying in shape, motion, and rigidity has not been proposed thus far. [sent-27, score-0.773]
</p><p>17 , surface deformations of human face [36], articulating tree-based human pose [28]). [sent-30, score-0.3]
</p><p>18 In contrast, structure-free, generic approaches, which are initialized simply by a single boundingbox, can localize arbitrary objects that are rigid [19, 34], deform less [5, 22, 33, 41], or more [6, 12, 23]. [sent-32, score-0.282]
</p><p>19 They build object appearance on-line but strive to be robust against 2920  object deformations and thus neglect or filter out its pose. [sent-33, score-0.241]
</p><p>20 Arguably, the single bounding-box annotation scenario currently limits their applicability to elongated objects that occupy rather expanded image regions. [sent-34, score-0.595]
</p><p>21 In view of this, the paper addresses a new problem of developing a generic system for pose-based tracking of elongated objects, which we conformably define as chain-like image structures. [sent-35, score-0.721]
</p><p>22 We position our approach between the structured and structure-free trackers by treating elongated objects as a structure of chained segments of parts with fixed appearance. [sent-36, score-1.133]
</p><p>23 Notably, we introduce a generic, model-based tracker that admits a simple, oneshot configuration from annotated object parts in the first frame. [sent-38, score-0.409]
</p><p>24 Apart from its computational efficiency, it also tracks objects robustly against partial occlusions and local appearance changes due to spatial support through partbased structure and re-detects them after full occlusions due to temporal support through fixed appearance. [sent-39, score-0.228]
</p><p>25 We achieve this within a MAP-MRF setting of pictorial structures [10, 11] by developing a deformable model of chained parts that efficiently leverages object local rigidity over spatio-temporal domain. [sent-42, score-0.711]
</p><p>26 This means the pixels can evolve freely within object parts during tracking, so achieving robustness to rotation and to local deformations caused by moderate change in viewpoint. [sent-44, score-0.358]
</p><p>27 We then maintain spatial appearance of the whole object by decomposing it into a chained assembly of segments of multiple parts that are arranged under a hierarchy of tailored spatiotemporal constraints. [sent-45, score-0.776]
</p><p>28 We reference each segment of parts  with an oriented polar coordinate system, effectively enforcing the spatial coherency of parts by promoting these part configurations that conform to the preferred relative angular deviations and distances over time. [sent-46, score-0.56]
</p><p>29 Our other contribution is to devise the new task of generic tracking of elongated objects having arbitrary shapes and motions. [sent-48, score-0.781]
</p><p>30 We also contribute by demonstrating that even though pictorial structures are usually considered slow [17], we integrate them into a hierarchical model that can register object pose up to speeds far exceeding real-time. [sent-49, score-0.294]
</p><p>31 Related work We review related work on region and part-based trackers of object poses, and other chain-based assemblies representing elongated image structures. [sent-51, score-0.671]
</p><p>32 The tracker determined object location in real-time by mean-shifting the kernel in the gradient-ascending direction of the differentiated objective function. [sent-53, score-0.259]
</p><p>33 Also, they use a holistic appearance template that loses spatial information, reduces their robustness to occlusions [1], and renders them infeasible to track objects that deform heavily. [sent-57, score-0.392]
</p><p>34 For instance, parts described by fixed, gray histograms voted for object location in [1]. [sent-62, score-0.226]
</p><p>35 Kernels of parts were jointly mean-shifted in [9] to follow object deformations but required precomputing the subspace over their possible displacements on initial series of images to guide their joint convergence. [sent-64, score-0.275]
</p><p>36 On the other hand, the prominent pictorial structures [10, 11] have been used extensively in object tracking by approximating complete graphs with star graphs [3, 3 1] and with other tree graph extensions [42, 44]. [sent-69, score-0.361]
</p><p>37 2921  The graphs are trained off-line for specific objects, but can explain heavy foreshortening [35] and scale linearly with object parts. [sent-70, score-0.221]
</p><p>38 We aim at an efficient and precise framework to track elongated objects that can vary in number of parts by several orders of magnitude. [sent-71, score-0.806]
</p><p>39 In our setting, the primary advantage over particle filter and other pictorial structure trackers is that our tracker can render the global solution without approximative inference nor approximative object structure and its joint inference scales linearly with the number of parts. [sent-72, score-0.682]
</p><p>40 A chain-based pictorial structure thus appears natural to track elongated regions, and our approach generalizes to such structures of arbitrary rigidity in a computationally efficient manner. [sent-73, score-0.914]
</p><p>41 Our work is also related in approach to [17, 40] that use a chained pictorial structure, and loosely related to [20] that iteratively infers on a dense graph by evaluating an ensemble of chains. [sent-78, score-0.279]
</p><p>42 However, [40] tracks non-deformable objects that shift and rotate, [20] requires a large set of training examples, and [17, 20] track object keypoints by filtering out object pose. [sent-79, score-0.283]
</p><p>43 Approach We develop a model-based approach that can track the motion of the pose of an arbitrary, elongated object in the image plane. [sent-85, score-0.687]
</p><p>44 We first partition an elongated object Oe into K segments Oe = {Oi}iK=1, as depicted in Fig. [sent-86, score-0.732]
</p><p>45 Then, each segment i =is partitioned further into ki parts Oi = specified by square-like windows. [sent-88, score-0.335]
</p><p>46 We link the the parts with a chain graph Gc = (V, E), where Wnoed elisn Vk are haess poacritaste wdi wthi ath c thhaei parts ahn Gd edges ,EE are asso-  {pi,j}kj=i1  ncioadteeds Vwi athre eth aes sloinckiast ebdetw weithen t choen psaerctust ainvde parts sin E th aree ecah sasino-. [sent-90, score-0.52]
</p><p>47 Model hierarchy, with an example of a deformable, elongated object, decomposed into K=3 segments that are referenced with planar coordinate systems. [sent-96, score-0.72]
</p><p>48 Two segments share a part, which is anchored at their hinge, denoting heavy deformation (e. [sent-98, score-0.238]
</p><p>49 The orientation of the coordinate system of each segment is estimated based on the tracked locations of the centers of the parts. [sent-101, score-0.249]
</p><p>50 the last part of each segment is the first part of the next segment in the chain, so denoting a hinge. [sent-127, score-0.224]
</p><p>51 As we update the orientation of segments during tracking, orientation variant features (e. [sent-135, score-0.317]
</p><p>52 The elongated segments Oi extend over rigid or elastic  current  regions. [sent-138, score-0.749]
</p><p>53 Pictorial structures whether model whole segments 2922  and search exhaustively for their orientations [10, 3 1], or split segments further into parts and model their constraints locally [42]. [sent-139, score-0.59]
</p><p>54 We also split segments into parts but model them hierarchically with spatio-temporal constraints, i. [sent-140, score-0.337]
</p><p>55 with local distances between parts and global orientations over segments to control their linear and angular deformations, respectively. [sent-142, score-0.483]
</p><p>56 Constraining each segment in a chain with global orientation allows to control its local rigidity without the need for higher order cliques in the graph, which is the key to fast inference. [sent-143, score-0.43]
</p><p>57 In this way, such a general, inertial temporal prior regularizes the dynamics of an object by favoring shift motion that is common during tracking [43]. [sent-145, score-0.259]
</p><p>58 ject affects the distances, so we obtain:  P(pti,j,pit,j+1) = P(lit,j,lit,j+1|sit,j,sit,j+1)P(sit,j,sit,j+1)  (4)  For simplicity, we model the joint scale prior P(sit,j , sit,j+1) for each pair of parts in the chain as a uniform distribution. [sent-154, score-0.275]
</p><p>59 The bending of all the parts in the segment is then co−ntrlolled during tracking with the temporal term as:  P(pit,j,pit,j+1|Oit−1) = M(θti,j;i,j+1;θi,j;i,j+1+Θit−1,κi)  (6)  where M denotes the von Mises distribution and κi denotes angular st idfefnneotses. [sent-163, score-0.53]
</p><p>60 Therefore, our model favors such arrangements of parts of the segment, which maintain predefined geometrical configuration, presuming that the orientation Θit−1 does not change much between successive frames. [sent-165, score-0.266]
</p><p>61 Configuration: Our system admits a simple, intuitive procedure for configuring the pose of an elongated object Oe in the initial frame I0. [sent-167, score-0.682]
</p><p>62 We: (1) split Oe into |V| parts pi0,j by specifying their locations and sizes, (2i)n olin |kV neighbor parts with a chain Gc, (3) and specify K segments of parts awrtisth w thitheir a corresponding orientations Θi0. [sent-168, score-0.806]
</p><p>63 Inference: We match our model (1) to each frame It by inferring on its negative log-posterior log(P(Oet |It, Oet−1)) with dynamic programming to −obltoaign( t(heO |MIAP configuration of the elongated object Oet,MAP. [sent-170, score-0.637]
</p><p>64 The inference is fast and its complexity scales linearly with the number of object parts |V| . [sent-171, score-0.274]
</p><p>65 e object is computed as the average over scales of all windows of parts and −  +  passed through the I filter as st = (1 − r)st−1 IR rst with the forgetting factor r. [sent-173, score-0.241]
</p><p>66 Synthetic example of i-th segment of heavily deformable object, whose scale st increases. [sent-194, score-0.217]
</p><p>67 The corresponding locations of parts between frames, translated back to the origin of the 2D CS, allow for recovering segment’s rotation Rit despite its incident deformation. [sent-196, score-0.22]
</p><p>68 We show that our pose-configurable system can be used successfully to track elongated objects in the image plane, which can shift, rotate, change scale, be rigid and deform by flexing, articulating, and vibrating. [sent-200, score-0.885]
</p><p>69 We also quantitatively evaluate our tracker on PROST dataset [34] with challenges of fast viewpoint changes, motion blur, heavy scale and illumination changes, and fre-  ×  quent occlusions. [sent-201, score-0.355]
</p><p>70 The tracker is compared against state-ofthe-art trackers on PROST that learn their appearance online. [sent-202, score-0.373]
</p><p>71 We demonstrate that our spatio-temporal model with remarkably simple, fixed appearance term leads to competitive or better tracking performance. [sent-203, score-0.252]
</p><p>72 As the occlusion event is not modeled explicitly, we enforce constant appearance so that the tracker is robust against occlusions and thus can recover easily by redetecting the object. [sent-204, score-0.332]
</p><p>73 The frame processing speed scales linearly with the number of parts but also depends on their window sizes (optionally, the latter could be factored out with [30]). [sent-232, score-0.284]
</p><p>74 Qualitative evaluation: We demonstrate that our method applies to tracking elongated objects of various shapes, which are rigid or deform by flexing, articulating, −  2,  and vibrating in the image plane. [sent-233, score-1.012]
</p><p>75 In Liquor, the tracker is very successful despite multiple and heavy occlusions of the glass bottle and is not confused by another bottle, which is fairly similar in color. [sent-236, score-0.479]
</p><p>76 In Robotic arm, the tracker follows the 2D pose of the articulating robotic manipulator composed of two segments. [sent-237, score-0.517]
</p><p>77 In Surgical suture, the suture is a very long object, which is thin and deforms heavily and unsystematically. [sent-238, score-0.221]
</p><p>78 By splitting the suture into piece-wise linear segments, our poseconfigurable system can follow it very precisely. [sent-239, score-0.238]
</p><p>79 Despite no constraints at the ends of the suture, the tracker stabilized both ends correctly, which is a challenging task [15]. [sent-240, score-0.213]
</p><p>80 We posit this satisfactory behavior owes to the fact that, while some segments rotate, others only shift, and thus our hierarchical, spatio-temporal model renders the tracker stable. [sent-241, score-0.4]
</p><p>81 In Toy tram, our model can explain the bending and scale change of the tram and is robust against moderate out-ofplane rotations affecting its appearance. [sent-242, score-0.263]
</p><p>82 In Guitar string, the tracker is able to precisely register intricate deformations of the string with very little information available. [sent-243, score-0.399]
</p><p>83 In this case though, the tracker ran with fixed scale to prevent the model from shrinking on the textureless, string region. [sent-245, score-0.405]
</p><p>84 We can easily configure our  region-based model to rigid objects with K=1 segment at initial orientation Θ10, and partition it evenly into k1 = 3 parts, i. [sent-249, score-0.364]
</p><p>85 such that the parts span the segment with no (or very small) overlap (see, e. [sent-251, score-0.262]
</p><p>86 To make the comparison fair, we fix the scale of our tracker and always output the same size of the ground truth bounding-box. [sent-260, score-0.268]
</p><p>87 Note that in the first frame of each sequence, our tracker outputs center location of the whole object that is slightly misaligned (by several pixels) from the center of 2Last 3 video  sequences were  collected from YouTube. [sent-261, score-0.315]
</p><p>88 The left column shows initialized layouts of chained segments of evenly annotated parts. [sent-265, score-0.298]
</p><p>89 (i) The glass bottle is configured with K = 1 segment of k1 = 3 parts. [sent-267, score-0.295]
</p><p>90 (ii) Articulating robotic arm is split into K = 2 segments of k1 = 6 and k2 = 5 parts. [sent-268, score-0.324]
</p><p>91 (iii) We split surgical suture into K = 6 segments of ki = 11 parts. [sent-269, score-0.555]
</p><p>92 (iv) The tram only bends so we configure it with K = 1 segment of k1 = 5 parts. [sent-270, score-0.292]
</p><p>93 (v) One can expect the vibrating string to deform only slightly, so we configure it with K = 1segment, as well. [sent-271, score-0.381]
</p><p>94 Our tracker with constant appearance yields competitive performance with respect to TLD [19] and GD [22], while outperforming others, and processes videos at ∼ 100 fps. [sent-277, score-0.283]
</p><p>95 Top: Since we integrate color histograms into our appearance term, the tracker struggles with heavy illumination changes, present in the Box sequence (e. [sent-296, score-0.442]
</p><p>96 Bottom: Unlike snakes models, the tracker is confused on textureless regions and shrinks when it updates scale. [sent-299, score-0.324]
</p><p>97 In Guitar string, it cannot discern between the correct and smaller scale of the parts of the guitar string (with the same configuration as in Fig. [sent-300, score-0.413]
</p><p>98 Complementary to on-line appearance update algorithms, our future work will pursue development of online reconfiguration update mechanisms for updating object rigidity constraints over time. [sent-305, score-0.33]
</p><p>99 Since the proposed generic tracker allows for attributing local rigidity constraints over the spatio-temporal space occupied by various elongated objects, it thus opens opportunities to investigate dynamic adaptation of rigidity constraints for more robust tracking. [sent-306, score-1.083]
</p><p>100 Object tracking by asymmetric kernel mean shift with automatic scale and orientation selection. [sent-568, score-0.334]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('elongated', 0.535), ('tracker', 0.213), ('suture', 0.19), ('articulating', 0.176), ('segments', 0.151), ('parts', 0.15), ('chained', 0.147), ('rigidity', 0.146), ('tracking', 0.143), ('pictorial', 0.132), ('flexing', 0.119), ('tram', 0.117), ('deform', 0.116), ('segment', 0.112), ('string', 0.107), ('surgical', 0.105), ('guitar', 0.101), ('oet', 0.098), ('vibrating', 0.095), ('trackers', 0.09), ('heavy', 0.087), ('angular', 0.084), ('rotate', 0.084), ('robotic', 0.083), ('snakes', 0.081), ('deformations', 0.079), ('international', 0.074), ('hierarchy', 0.073), ('ki', 0.073), ('bottle', 0.071), ('prost', 0.07), ('appearance', 0.07), ('chain', 0.07), ('shift', 0.07), ('conference', 0.067), ('orientation', 0.066), ('pages', 0.065), ('configure', 0.063), ('rigid', 0.063), ('orientations', 0.062), ('track', 0.061), ('objects', 0.06), ('pattern', 0.06), ('glass', 0.059), ('liquor', 0.059), ('lit', 0.056), ('frame', 0.056), ('scale', 0.055), ('arm', 0.054), ('configured', 0.053), ('change', 0.05), ('rit', 0.05), ('assembly', 0.05), ('deformable', 0.05), ('occlusions', 0.049), ('tailored', 0.049), ('oe', 0.048), ('horain', 0.048), ('poseconfigurable', 0.048), ('wesierski', 0.048), ('gd', 0.047), ('object', 0.046), ('pose', 0.045), ('scales', 0.045), ('generic', 0.043), ('configurable', 0.042), ('approximative', 0.042), ('struggles', 0.042), ('bending', 0.041), ('structures', 0.04), ('arranged', 0.04), ('remarkably', 0.039), ('particle', 0.039), ('locations', 0.037), ('versatility', 0.037), ('stiffness', 0.037), ('renders', 0.036), ('control', 0.036), ('split', 0.036), ('oi', 0.035), ('blob', 0.035), ('coordinate', 0.034), ('update', 0.034), ('rotation', 0.033), ('linearly', 0.033), ('vision', 0.033), ('plethora', 0.032), ('santner', 0.032), ('deforms', 0.031), ('godec', 0.031), ('demonstrating', 0.031), ('updated', 0.031), ('coherency', 0.03), ('rescaling', 0.03), ('textureless', 0.03), ('finger', 0.03), ('histogram', 0.03), ('histograms', 0.03), ('ran', 0.03), ('cs', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999917 <a title="320-tfidf-1" href="./iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects.html">320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</a></p>
<p>Author: Daniel Wesierski, Patrick Horain</p><p>Abstract: Elongated objects have various shapes and can shift, rotate, change scale, and be rigid or deform by flexing, articulating, and vibrating, with examples as varied as a glass bottle, a robotic arm, a surgical suture, a finger pair, a tram, and a guitar string. This generally makes tracking of poses of elongated objects very challenging. We describe a unified, configurable framework for tracking the pose of elongated objects, which move in the image plane and extend over the image region. Our method strives for simplicity, versatility, and efficiency. The object is decomposed into a chained assembly of segments of multiple parts that are arranged under a hierarchy of tailored spatio-temporal constraints. In this hierarchy, segments can rescale independently while their elasticity is controlled with global orientations and local distances. While the trend in tracking is to design complex, structure-free algorithms that update object appearance on- line, we show that our tracker, with the novel but remarkably simple, structured organization of parts with constant appearance, reaches or improves state-of-the-art performance. Most importantly, our model can be easily configured to track exact pose of arbitrary, elongated objects in the image plane. The tracker can run up to 100 fps on a desktop PC, yet the computation time scales linearly with the number of object parts. To our knowledge, this is the first approach to generic tracking of elongated objects.</p><p>2 0.18239543 <a title="320-tfidf-2" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>Author: Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, James M. Rehg</p><p>Abstract: We propose an unsupervised video segmentation approach by simultaneously tracking multiple holistic figureground segments. Segment tracks are initialized from a pool of segment proposals generated from a figure-ground segmentation algorithm. Then, online non-local appearance models are trained incrementally for each track using a multi-output regularized least squares formulation. By using the same set of training examples for all segment tracks, a computational trick allows us to track hundreds of segment tracks efficiently, as well as perform optimal online updates in closed-form. Besides, a new composite statistical inference approach is proposed for refining the obtained segment tracks, which breaks down the initial segment proposals and recombines for better ones by utilizing highorder statistic estimates from the appearance model and enforcing temporal consistency. For evaluating the algorithm, a dataset, SegTrack v2, is collected with about 1,000 frames with pixel-level annotations. The proposed framework outperforms state-of-the-art approaches in the dataset, show- ing its efficiency and robustness to challenges in different video sequences.</p><p>3 0.17606057 <a title="320-tfidf-3" href="./iccv-2013-Online_Robust_Non-negative_Dictionary_Learning_for_Visual_Tracking.html">298 iccv-2013-Online Robust Non-negative Dictionary Learning for Visual Tracking</a></p>
<p>Author: Naiyan Wang, Jingdong Wang, Dit-Yan Yeung</p><p>Abstract: This paper studies the visual tracking problem in video sequences and presents a novel robust sparse tracker under the particle filter framework. In particular, we propose an online robust non-negative dictionary learning algorithm for updating the object templates so that each learned template can capture a distinctive aspect of the tracked object. Another appealing property of this approach is that it can automatically detect and reject the occlusion and cluttered background in a principled way. In addition, we propose a new particle representation formulation using the Huber loss function. The advantage is that it can yield robust estimation without using trivial templates adopted by previous sparse trackers, leading to faster computation. We also reveal the equivalence between this new formulation and the previous one which uses trivial templates. The proposed tracker is empirically compared with state-of-the-art trackers on some challenging video sequences. Both quantitative and qualitative comparisons show that our proposed tracker is superior and more stable.</p><p>4 0.16856328 <a title="320-tfidf-4" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>Author: Stefan Duffner, Christophe Garcia</p><p>Abstract: In this paper, we present a novel algorithm for fast tracking of generic objects in videos. The algorithm uses two components: a detector that makes use of the generalised Hough transform with pixel-based descriptors, and a probabilistic segmentation method based on global models for foreground and background. These components are used for tracking in a combined way, and they adapt each other in a co-training manner. Through effective model adaptation and segmentation, the algorithm is able to track objects that undergo rigid and non-rigid deformations and considerable shape and appearance variations. The proposed tracking method has been thoroughly evaluated on challenging standard videos, and outperforms state-of-theart tracking methods designed for the same task. Finally, the proposed models allow for an extremely efficient implementation, and thus tracking is very fast.</p><p>5 0.15430838 <a title="320-tfidf-5" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>Author: Shugao Ma, Jianming Zhang, Nazli Ikizler-Cinbis, Stan Sclaroff</p><p>Abstract: We propose Hierarchical Space-Time Segments as a new representation for action recognition and localization. This representation has a two-level hierarchy. The first level comprises the root space-time segments that may contain a human body. The second level comprises multi-grained space-time segments that contain parts of the root. We present an unsupervised method to generate this representation from video, which extracts both static and non-static relevant space-time segments, and also preserves their hierarchical and temporal relationships. Using simple linear SVM on the resultant bag of hierarchical space-time segments representation, we attain better than, or comparable to, state-of-the-art action recognition performance on two challenging benchmark datasets and at the same time produce good action localization results.</p><p>6 0.15210696 <a title="320-tfidf-6" href="./iccv-2013-Finding_the_Best_from_the_Second_Bests_-_Inhibiting_Subjective_Bias_in_Evaluation_of_Visual_Tracking_Algorithms.html">168 iccv-2013-Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms</a></p>
<p>7 0.14967906 <a title="320-tfidf-7" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>8 0.14956039 <a title="320-tfidf-8" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>9 0.14423452 <a title="320-tfidf-9" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>10 0.13992234 <a title="320-tfidf-10" href="./iccv-2013-Real-Time_Body_Tracking_with_One_Depth_Camera_and_Inertial_Sensors.html">341 iccv-2013-Real-Time Body Tracking with One Depth Camera and Inertial Sensors</a></p>
<p>11 0.13184665 <a title="320-tfidf-11" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>12 0.12545137 <a title="320-tfidf-12" href="./iccv-2013-Strong_Appearance_and_Expressive_Spatial_Models_for_Human_Pose_Estimation.html">403 iccv-2013-Strong Appearance and Expressive Spatial Models for Human Pose Estimation</a></p>
<p>13 0.12284242 <a title="320-tfidf-13" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>14 0.11854736 <a title="320-tfidf-14" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<p>15 0.11526615 <a title="320-tfidf-15" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>16 0.11035921 <a title="320-tfidf-16" href="./iccv-2013-Tree_Shape_Priors_with_Connectivity_Constraints_Using_Convex_Relaxation_on_General_Graphs.html">429 iccv-2013-Tree Shape Priors with Connectivity Constraints Using Convex Relaxation on General Graphs</a></p>
<p>17 0.10873231 <a title="320-tfidf-17" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>18 0.10540982 <a title="320-tfidf-18" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>19 0.1049969 <a title="320-tfidf-19" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>20 0.10046367 <a title="320-tfidf-20" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.234), (1, -0.08), (2, 0.033), (3, 0.062), (4, 0.065), (5, -0.07), (6, -0.09), (7, 0.128), (8, -0.075), (9, 0.102), (10, -0.051), (11, -0.084), (12, 0.026), (13, 0.024), (14, -0.02), (15, 0.046), (16, 0.094), (17, 0.012), (18, -0.041), (19, -0.063), (20, 0.029), (21, 0.024), (22, -0.024), (23, -0.07), (24, -0.029), (25, -0.005), (26, -0.007), (27, 0.03), (28, -0.005), (29, -0.013), (30, -0.022), (31, -0.046), (32, -0.085), (33, -0.062), (34, -0.043), (35, 0.068), (36, -0.017), (37, -0.089), (38, 0.008), (39, -0.066), (40, 0.058), (41, 0.087), (42, 0.009), (43, -0.007), (44, 0.026), (45, 0.091), (46, -0.018), (47, 0.073), (48, -0.051), (49, 0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95541543 <a title="320-lsi-1" href="./iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects.html">320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</a></p>
<p>Author: Daniel Wesierski, Patrick Horain</p><p>Abstract: Elongated objects have various shapes and can shift, rotate, change scale, and be rigid or deform by flexing, articulating, and vibrating, with examples as varied as a glass bottle, a robotic arm, a surgical suture, a finger pair, a tram, and a guitar string. This generally makes tracking of poses of elongated objects very challenging. We describe a unified, configurable framework for tracking the pose of elongated objects, which move in the image plane and extend over the image region. Our method strives for simplicity, versatility, and efficiency. The object is decomposed into a chained assembly of segments of multiple parts that are arranged under a hierarchy of tailored spatio-temporal constraints. In this hierarchy, segments can rescale independently while their elasticity is controlled with global orientations and local distances. While the trend in tracking is to design complex, structure-free algorithms that update object appearance on- line, we show that our tracker, with the novel but remarkably simple, structured organization of parts with constant appearance, reaches or improves state-of-the-art performance. Most importantly, our model can be easily configured to track exact pose of arbitrary, elongated objects in the image plane. The tracker can run up to 100 fps on a desktop PC, yet the computation time scales linearly with the number of object parts. To our knowledge, this is the first approach to generic tracking of elongated objects.</p><p>2 0.73802906 <a title="320-lsi-2" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>Author: Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, James M. Rehg</p><p>Abstract: We propose an unsupervised video segmentation approach by simultaneously tracking multiple holistic figureground segments. Segment tracks are initialized from a pool of segment proposals generated from a figure-ground segmentation algorithm. Then, online non-local appearance models are trained incrementally for each track using a multi-output regularized least squares formulation. By using the same set of training examples for all segment tracks, a computational trick allows us to track hundreds of segment tracks efficiently, as well as perform optimal online updates in closed-form. Besides, a new composite statistical inference approach is proposed for refining the obtained segment tracks, which breaks down the initial segment proposals and recombines for better ones by utilizing highorder statistic estimates from the appearance model and enforcing temporal consistency. For evaluating the algorithm, a dataset, SegTrack v2, is collected with about 1,000 frames with pixel-level annotations. The proposed framework outperforms state-of-the-art approaches in the dataset, show- ing its efficiency and robustness to challenges in different video sequences.</p><p>3 0.71604466 <a title="320-lsi-3" href="./iccv-2013-Finding_the_Best_from_the_Second_Bests_-_Inhibiting_Subjective_Bias_in_Evaluation_of_Visual_Tracking_Algorithms.html">168 iccv-2013-Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms</a></p>
<p>Author: Yu Pang, Haibin Ling</p><p>Abstract: Evaluating visual tracking algorithms, or “trackers ” for short, is of great importance in computer vision. However, it is hard to “fairly” compare trackers due to many parameters need to be tuned in the experimental configurations. On the other hand, when introducing a new tracker, a recent trend is to validate it by comparing it with several existing ones. Such an evaluation may have subjective biases towards the new tracker which typically performs the best. This is mainly due to the difficulty to optimally tune all its competitors and sometimes the selected testing sequences. By contrast, little subjective bias exists towards the “second best” ones1 in the contest. This observation inspires us with a novel perspective towards inhibiting subjective bias in evaluating trackers by analyzing the results between the second bests. In particular, we first collect all tracking papers published in major computer vision venues in recent years. From these papers, after filtering out potential biases in various aspects, we create a dataset containing many records of comparison results between various visual trackers. Using these records, we derive performance rank- ings of the involved trackers by four different methods. The first two methods model the dataset as a graph and then derive the rankings over the graph, one by a rank aggregation algorithm and the other by a PageRank-like solution. The other two methods take the records as generated from sports contests and adopt widely used Elo’s and Glicko ’s rating systems to derive the rankings. The experimental results are presented and may serve as a reference for related research.</p><p>4 0.70747024 <a title="320-lsi-4" href="./iccv-2013-Orderless_Tracking_through_Model-Averaged_Posterior_Estimation.html">303 iccv-2013-Orderless Tracking through Model-Averaged Posterior Estimation</a></p>
<p>Author: Seunghoon Hong, Suha Kwak, Bohyung Han</p><p>Abstract: We propose a novel offline tracking algorithm based on model-averaged posterior estimation through patch matching across frames. Contrary to existing online and offline tracking methods, our algorithm is not based on temporallyordered estimates of target state but attempts to select easyto-track frames first out of the remaining ones without exploiting temporal coherency of target. The posterior of the selected frame is estimated by propagating densities from the already tracked frames in a recursive manner. The density propagation across frames is implemented by an efficient patch matching technique, which is useful for our algorithm since it does not require motion smoothness assumption. Also, we present a hierarchical approach, where a small set of key frames are tracked first and non-key frames are handled by local key frames. Our tracking algorithm is conceptually well-suited for the sequences with abrupt motion, shot changes, and occlusion. We compare our tracking algorithm with existing techniques in real videos with such challenges and illustrate its superior performance qualitatively and quantitatively.</p><p>5 0.69977331 <a title="320-lsi-5" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>Author: Stefan Duffner, Christophe Garcia</p><p>Abstract: In this paper, we present a novel algorithm for fast tracking of generic objects in videos. The algorithm uses two components: a detector that makes use of the generalised Hough transform with pixel-based descriptors, and a probabilistic segmentation method based on global models for foreground and background. These components are used for tracking in a combined way, and they adapt each other in a co-training manner. Through effective model adaptation and segmentation, the algorithm is able to track objects that undergo rigid and non-rigid deformations and considerable shape and appearance variations. The proposed tracking method has been thoroughly evaluated on challenging standard videos, and outperforms state-of-theart tracking methods designed for the same task. Finally, the proposed models allow for an extremely efficient implementation, and thus tracking is very fast.</p><p>6 0.67292601 <a title="320-lsi-6" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>7 0.66969204 <a title="320-lsi-7" href="./iccv-2013-Online_Robust_Non-negative_Dictionary_Learning_for_Visual_Tracking.html">298 iccv-2013-Online Robust Non-negative Dictionary Learning for Visual Tracking</a></p>
<p>8 0.65927005 <a title="320-lsi-8" href="./iccv-2013-Slice_Sampling_Particle_Belief_Propagation.html">395 iccv-2013-Slice Sampling Particle Belief Propagation</a></p>
<p>9 0.65726393 <a title="320-lsi-9" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<p>10 0.64501548 <a title="320-lsi-10" href="./iccv-2013-A_New_Adaptive_Segmental_Matching_Measure_for_Human_Activity_Recognition.html">22 iccv-2013-A New Adaptive Segmental Matching Measure for Human Activity Recognition</a></p>
<p>11 0.63597381 <a title="320-lsi-11" href="./iccv-2013-Conservation_Tracking.html">87 iccv-2013-Conservation Tracking</a></p>
<p>12 0.62697548 <a title="320-lsi-12" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>13 0.61597073 <a title="320-lsi-13" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>14 0.61335135 <a title="320-lsi-14" href="./iccv-2013-Dynamic_Probabilistic_Volumetric_Models.html">128 iccv-2013-Dynamic Probabilistic Volumetric Models</a></p>
<p>15 0.60766065 <a title="320-lsi-15" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>16 0.59615165 <a title="320-lsi-16" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>17 0.58173704 <a title="320-lsi-17" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>18 0.57668549 <a title="320-lsi-18" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>19 0.57267338 <a title="320-lsi-19" href="./iccv-2013-Modeling_Self-Occlusions_in_Dynamic_Shape_and_Appearance_Tracking.html">270 iccv-2013-Modeling Self-Occlusions in Dynamic Shape and Appearance Tracking</a></p>
<p>20 0.57122475 <a title="320-lsi-20" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.05), (6, 0.012), (7, 0.022), (26, 0.075), (31, 0.044), (35, 0.022), (40, 0.03), (42, 0.096), (48, 0.268), (64, 0.093), (73, 0.034), (89, 0.142)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88993794 <a title="320-lda-1" href="./iccv-2013-Pyramid_Coding_for_Functional_Scene_Element_Recognition_in_Video_Scenes.html">331 iccv-2013-Pyramid Coding for Functional Scene Element Recognition in Video Scenes</a></p>
<p>Author: Eran Swears, Anthony Hoogs, Kim Boyer</p><p>Abstract: Recognizing functional scene elemeents in video scenes based on the behaviors of moving objects that interact with them is an emerging problem ooff interest. Existing approaches have a limited ability to chharacterize elements such as cross-walks, intersections, andd buildings that have low activity, are multi-modal, or havee indirect evidence. Our approach recognizes the low activvity and multi-model elements (crosswalks/intersections) by introducing a hierarchy of descriptive clusters to fform a pyramid of codebooks that is sparse in the numbber of clusters and dense in content. The incorporation oof local behavioral context such as person-enter-building aand vehicle-parking nearby enables the detection of elemennts that do not have direct motion-based evidence, e.g. buuildings. These two contributions significantly improvee scene element recognition when compared against thhree state-of-the-art approaches. Results are shown on tyypical ground level surveillance video and for the first time on the more complex Wide Area Motion Imagery.</p><p>2 0.87553394 <a title="320-lda-2" href="./iccv-2013-Pedestrian_Parsing_via_Deep_Decompositional_Network.html">311 iccv-2013-Pedestrian Parsing via Deep Decompositional Network</a></p>
<p>Author: Ping Luo, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: We propose a new Deep Decompositional Network (DDN) for parsing pedestrian images into semantic regions, such as hair, head, body, arms, and legs, where the pedestrians can be heavily occluded. Unlike existing methods based on template matching or Bayesian inference, our approach directly maps low-level visual features to the label maps of body parts with DDN, which is able to accurately estimate complex pose variations with good robustness to occlusions and background clutters. DDN jointly estimates occluded regions and segments body parts by stacking three types of hidden layers: occlusion estimation layers, completion layers, and decomposition layers. The occlusion estimation layers estimate a binary mask, indicating which part of a pedestrian is invisible. The completion layers synthesize low-level features of the invisible part from the original features and the occlusion mask. The decomposition layers directly transform the synthesized visual features to label maps. We devise a new strategy to pre-train these hidden layers, and then fine-tune the entire network using the stochastic gradient descent. Experimental results show that our approach achieves better segmentation accuracy than the state-of-the-art methods on pedestrian images with or without occlusions. Another important contribution of this paper is that it provides a large scale benchmark human parsing dataset1 that includes 3, 673 annotated samples collected from 171 surveillance videos. It is 20 times larger than existing public datasets.</p><p>3 0.84730792 <a title="320-lda-3" href="./iccv-2013-Bounded_Labeling_Function_for_Global_Segmentation_of_Multi-part_Objects_with_Geometric_Constraints.html">63 iccv-2013-Bounded Labeling Function for Global Segmentation of Multi-part Objects with Geometric Constraints</a></p>
<p>Author: Masoud S. Nosrati, Shawn Andrews, Ghassan Hamarneh</p><p>Abstract: The inclusion of shape and appearance priors have proven useful for obtaining more accurate and plausible segmentations, especially for complex objects with multiple parts. In this paper, we augment the popular MumfordShah model to incorporate two important geometrical constraints, termed containment and detachment, between different regions with a specified minimum distance between their boundaries. Our method is able to handle multiple instances of multi-part objects defined by these geometrical hamarneh} @ s fu . ca (a)Standar laΩb ehlingΩfuhnctionseting(Ωb)hΩOuirseΩtijng Figure 1: The inside vs. outside ambiguity in (a) is resolved by our containment constraint in (b). constraints using a single labeling function while maintaining global optimality. We demonstrate the utility and advantages of these two constraints and show that the proposed convex continuous method is superior to other state-of-theart methods, including its discrete counterpart, in terms of memory usage, and metrication errors.</p><p>same-paper 4 0.82215756 <a title="320-lda-4" href="./iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects.html">320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</a></p>
<p>Author: Daniel Wesierski, Patrick Horain</p><p>Abstract: Elongated objects have various shapes and can shift, rotate, change scale, and be rigid or deform by flexing, articulating, and vibrating, with examples as varied as a glass bottle, a robotic arm, a surgical suture, a finger pair, a tram, and a guitar string. This generally makes tracking of poses of elongated objects very challenging. We describe a unified, configurable framework for tracking the pose of elongated objects, which move in the image plane and extend over the image region. Our method strives for simplicity, versatility, and efficiency. The object is decomposed into a chained assembly of segments of multiple parts that are arranged under a hierarchy of tailored spatio-temporal constraints. In this hierarchy, segments can rescale independently while their elasticity is controlled with global orientations and local distances. While the trend in tracking is to design complex, structure-free algorithms that update object appearance on- line, we show that our tracker, with the novel but remarkably simple, structured organization of parts with constant appearance, reaches or improves state-of-the-art performance. Most importantly, our model can be easily configured to track exact pose of arbitrary, elongated objects in the image plane. The tracker can run up to 100 fps on a desktop PC, yet the computation time scales linearly with the number of object parts. To our knowledge, this is the first approach to generic tracking of elongated objects.</p><p>5 0.78281033 <a title="320-lda-5" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>Author: Zhuoyuan Chen, Ying Wu</p><p>Abstract: Sparsity models have recently shown great promise in many vision tasks. Using a learned dictionary in sparsity models can in general outperform predefined bases in clean data. In practice, both training and testing data may be corrupted and contain noises and outliers. Although recent studies attempted to cope with corrupted data and achieved encouraging results in testing phase, how to handle corruption in training phase still remains a very difficult problem. In contrast to most existing methods that learn the dictionaryfrom clean data, this paper is targeted at handling corruptions and outliers in training data for dictionary learning. We propose a general method to decompose the reconstructive residual into two components: a non-sparse component for small universal noises and a sparse component for large outliers, respectively. In addition, , further analysis reveals the connection between our approach and the “partial” dictionary learning approach, updating only part of the prototypes (or informative codewords) with remaining (or noisy codewords) fixed. Experiments on synthetic data as well as real applications have shown satisfactory per- formance of this new robust dictionary learning approach.</p><p>6 0.76648158 <a title="320-lda-6" href="./iccv-2013-Illuminant_Chromaticity_from_Image_Sequences.html">207 iccv-2013-Illuminant Chromaticity from Image Sequences</a></p>
<p>7 0.69342887 <a title="320-lda-7" href="./iccv-2013-Joint_Deep_Learning_for_Pedestrian_Detection.html">220 iccv-2013-Joint Deep Learning for Pedestrian Detection</a></p>
<p>8 0.66949844 <a title="320-lda-8" href="./iccv-2013-Multi-stage_Contextual_Deep_Learning_for_Pedestrian_Detection.html">279 iccv-2013-Multi-stage Contextual Deep Learning for Pedestrian Detection</a></p>
<p>9 0.6676631 <a title="320-lda-9" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>10 0.65495729 <a title="320-lda-10" href="./iccv-2013-Hybrid_Deep_Learning_for_Face_Verification.html">206 iccv-2013-Hybrid Deep Learning for Face Verification</a></p>
<p>11 0.64741635 <a title="320-lda-11" href="./iccv-2013-Deep_Learning_Identity-Preserving_Face_Space.html">106 iccv-2013-Deep Learning Identity-Preserving Face Space</a></p>
<p>12 0.63910186 <a title="320-lda-12" href="./iccv-2013-Incorporating_Cloud_Distribution_in_Sky_Representation.html">215 iccv-2013-Incorporating Cloud Distribution in Sky Representation</a></p>
<p>13 0.63866937 <a title="320-lda-13" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<p>14 0.63326633 <a title="320-lda-14" href="./iccv-2013-Modeling_Self-Occlusions_in_Dynamic_Shape_and_Appearance_Tracking.html">270 iccv-2013-Modeling Self-Occlusions in Dynamic Shape and Appearance Tracking</a></p>
<p>15 0.63088465 <a title="320-lda-15" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>16 0.62871337 <a title="320-lda-16" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>17 0.62752134 <a title="320-lda-17" href="./iccv-2013-Beyond_Hard_Negative_Mining%3A_Efficient_Detector_Learning_via_Block-Circulant_Decomposition.html">61 iccv-2013-Beyond Hard Negative Mining: Efficient Detector Learning via Block-Circulant Decomposition</a></p>
<p>18 0.6271472 <a title="320-lda-18" href="./iccv-2013-A_Color_Constancy_Model_with_Double-Opponency_Mechanisms.html">5 iccv-2013-A Color Constancy Model with Double-Opponency Mechanisms</a></p>
<p>19 0.62631142 <a title="320-lda-19" href="./iccv-2013-Exploiting_Reflection_Change_for_Automatic_Reflection_Removal.html">151 iccv-2013-Exploiting Reflection Change for Automatic Reflection Removal</a></p>
<p>20 0.62567699 <a title="320-lda-20" href="./iccv-2013-Perceptual_Fidelity_Aware_Mean_Squared_Error.html">312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
