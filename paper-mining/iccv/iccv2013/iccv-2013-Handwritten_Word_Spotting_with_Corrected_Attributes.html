<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-192" href="#">iccv2013-192</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</h1>
<br/><p>Source: <a title="iccv-2013-192-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Almazan_Handwritten_Word_Spotting_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Jon Almazán, Albert Gordo, Alicia Fornés, Ernest Valveny</p><p>Abstract: We propose an approach to multi-writer word spotting, where the goal is to find a query word in a dataset comprised of document images. We propose an attributes-based approach that leads to a low-dimensional, fixed-length representation of the word images that is fast to compute and, especially, fast to compare. This approach naturally leads to an unified representation of word images and strings, which seamlessly allows one to indistinctly perform queryby-example, where the query is an image, and query-bystring, where the query is a string. We also propose a calibration scheme to correct the attributes scores based on Canonical Correlation Analysis that greatly improves the results on a challenging dataset. We test our approach on two public datasets showing state-of-the-art results.</p><p>Reference: <a title="iccv-2013-192-reference" href="../iccv2013_reference/iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We propose an attributes-based approach that leads to a low-dimensional, fixed-length representation of the word images that is fast to compute and, especially, fast to compare. [sent-2, score-0.474]
</p><p>2 This approach naturally leads to an unified representation of word images and strings, which seamlessly allows one to indistinctly perform queryby-example, where the query is an image, and query-bystring, where the query is a string. [sent-3, score-0.62]
</p><p>3 We also propose a calibration scheme to correct the attributes scores based on Canonical Correlation Analysis that greatly improves the results on a challenging dataset. [sent-4, score-0.383]
</p><p>4 Introduction This paper addresses the problem of multi-writer word spotting. [sent-7, score-0.437]
</p><p>5 The objective of word spotting is to find all instances of a given word in a potentially large dataset of doc-  ument images. [sent-8, score-1.12]
</p><p>6 This is typically done in a query-by-example (QBE) scenario, where the query is an image of a handwritten word and it is assumed that the transcription of the dataset and the query word is not available. [sent-9, score-1.252]
</p><p>7 In a multi-writer setting, the writers of the dataset documents may have completely different writing styles than the writer of the query. [sent-10, score-0.367]
</p><p>8 a very large intra-class variability different writers may have completely different writing styles, making the same word look completely different (cf. [sent-18, score-0.634]
</p><p>9 This huge variability in styles makes this a much more difficult problem than typeset or single-writer handwritten word spotting. [sent-21, score-0.636]
</p><p>10 Because of this complexity, most popular techniques are based on describing word images as sequences of features of variable length and using techniques such as Dynamic Time Warping (DTW) or Hidden Markov Models (HMM) to classify them. [sent-22, score-0.437]
</p><p>11 Variable-length features are more flexible than feature vectors and have been known to lead to superior –  results in difficult word-spotting tasks since they can adapt better to the different variations of style and word length [7, 9, 22, 24, 25]. [sent-23, score-0.476]
</p><p>12 Indeed, with the steady increase of datasets size there has been a renewed interest in compact, fast-to-compare word representations. [sent-31, score-0.437]
</p><p>13 [28], where word images are represented with SIFT descriptors aggregated using the bag of visual words framework [4], or the work of Almaz ´an et al. [sent-33, score-0.619]
</p><p>14 In particular, we adopt the Fisher vector (FV) [29] representation computed over SIFT descriptors extracted densely from the word image. [sent-38, score-0.504]
</p><p>15 In this paper we propose to use labeled training data to learn how to embed our fixed-length descriptor in a more discriminative, low-dimensional space, where similarities between words are preserved independently of the writing style. [sent-42, score-0.268]
</p><p>16 Indeed, we believe that learning robust models at the word level is an extremely difficult task due to the intrinsic variation of writing styles, and its adaptation to new, unseen words at test time usually yields poor results. [sent-44, score-0.656]
</p><p>17 The use of attributes is, arguably, the most popular approach to achieve these goals. [sent-46, score-0.182]
</p><p>18 As our first contribution, we propose an embedding approach that encodes word strings as a pyramidal histogram of characters which we dubbed PHOC inspired by the bag of characters string kernels used for example in the machine learning and biocomputing communities [16, 17]. [sent-47, score-0.934]
</p><p>19 In a nutshell, this binary histogram encodes whether a particular character appears in the represented word or not. [sent-48, score-0.603]
</p><p>20 , this character appears on the first half of the word, or this character appears in the last quarter of the word (see Fig. [sent-51, score-0.713]
</p><p>21 During the learning of these attributes we use a wide variety of writers and characters, and so the adaptation to new, unseen words is almost seamless. [sent-56, score-0.374]
</p><p>22 A naive implementation of this attributes representation greatly outperforms the direct use of FVs. [sent-57, score-0.219]
</p><p>23 We found that accurately calibrating the attribute scores can have a large impact in the accuracy of the method. [sent-63, score-0.261]
</p><p>24 We believe that calibrating the scores jointly can lead to large improvements, since the information of different attributes is shared. [sent-65, score-0.353]
</p><p>25 This is particularly true in the case of pyramidal histograms, where the same character may be simultaneously represented by various attributes depending on its position inside the word. [sent-66, score-0.358]
</p><p>26 This motivates our second contribution, a scheme to calibrate all the attribute scores jointly by means of Canonical Correlation Analysis (CCA) and its kernelized  version (KCCA), where the main idea is to correlate the predicted attribute scores with their ground truth values. [sent-67, score-0.442]
</p><p>27 This calibration method can noticeably outperform the standard Platts scaling while, at the same time, perform a dimensionality reduction of the attribute space. [sent-68, score-0.232]
</p><p>28 We believe that the uses of this calibration scheme are not limited to word image representation and can also be used in other attributebased tasks. [sent-69, score-0.616]
</p><p>29 Like in [23], we learn a joint representation for word images and text. [sent-71, score-0.505]
</p><p>30 In Section 2 we review the literature on fixed-length representations –  –  110011 88  for word images and describe our baseline FV representation as well as the proposed attributes-based representation. [sent-74, score-0.528]
</p><p>31 Word Representation In this section we describe how we obtain the representation of a word image. [sent-79, score-0.474]
</p><p>32 First we review fixed-length word image representations and introduce the FV as our reference representation. [sent-80, score-0.491]
</p><p>33 In [18], a distance between binary word images is defined based on the result of XORing the images. [sent-87, score-0.437]
</p><p>34 This representation has a fixed length and can be used for efficient spotting tasks, although the paper focuses on only 10 different keywords. [sent-91, score-0.283]
</p><p>35 These fast-to-compare representations allow them to perform word spotting using a sliding window over the whole document without segmenting it into individual words. [sent-97, score-0.773]
</p><p>36 Here we adopt a similar approach and represent word images using the FV framework. [sent-98, score-0.437]
</p><p>37 To (weakly) capture the structure of the word image, we use a spatial pyramid of 2 6 leading to a final descriptor of approximately 25, 0o0f 02 d ×im 6e lnesaiodninsg. [sent-101, score-0.493]
</p><p>38 Supervised Word Representation with PHOC Attributes One of the most popular approaches to perform supervised learning for word spotting is to learn models for particular keywords. [sent-104, score-0.714]
</p><p>39 At test time, it is possible to compute the probability of a given word being generated by that keyword model, and that can be used as a score. [sent-106, score-0.489]
</p><p>40 One disadvantage of these approaches that learn at the word level is that information is not shared between similar words. [sent-113, score-0.468]
</p><p>41 We believe that sharing information between words is extremely important to learn good discriminative representations, and that the use of attributes is one way to achieve this goal. [sent-115, score-0.338]
</p><p>42 The selection of these attributes is commonly a task-dependent process, so for their application to word spotting we should define them as worddiscriminative and writer-independent properties. [sent-118, score-0.899]
</p><p>43 We define attributes such as “word contains an a” or “word contains a k”, leading to a histogram of 26 dimensions when using the English alphabet1 . [sent-120, score-0.238]
</p><p>44 Then, at train-  ing time, we learn models for each of the attributes using the image representation of the words (FVs in our case) as data, and set their labels as positive or negative according to whether those images contain that particular character or not (see Figure 2). [sent-121, score-0.477]
</p><p>45 Then, at testing time, given the FV of a word, we can compute its attribute representation simply by concatenating the scores that those models yield on that particular sample. [sent-123, score-0.258]
</p><p>46 , Platts scaling), these attribute representations can be compared using measures such as the Euclidean distance or the cosine similarity. [sent-126, score-0.18]
</p><p>47 11001199 Labeled word images  the PHOC representation as label. [sent-131, score-0.474]
</p><p>48 At level 2, we define attributes such as “word contains character x on the first half of the word” and “word contains character x on the second half of the word”. [sent-135, score-0.458]
</p><p>49 Level 3 splits the word in 3 parts, level 4 in 4, etc. [sent-136, score-0.437]
</p><p>50 Finally, we also add t(h2e + +75 3 m +o s4)t common English bigrams Fati nleavlelyl, 2 w, leading dtod 150 extra attributes for a total of 384 attributes. [sent-138, score-0.279]
</p><p>51 In the context of an attributes-based representations, the spatially-aware attributes allow one to ask more precise questions about the location of the characters, while the spatial pyramid on the image representation allows one to answer those questions. [sent-140, score-0.247]
</p><p>52 Given a transcription of a word we need to determine the regions of the pyramid where we assign each character. [sent-141, score-0.564]
</p><p>53 For that, we first define the normalized occupancy of the k-th character of a word of length n as the interval Occ(k, n) = [nk , k+n1], where the position k is zero-based. [sent-142, score-0.605]
</p><p>54 Note that this information is extracted from the word transcription, not from the word image. [sent-143, score-0.874]
</p><p>55 Indeed, assuming perfect attribute models and calibration, both representations would be identical. [sent-157, score-0.18]
</p><p>56 This leads to a very clean model to perform query-by-string (QBS, sometimes referred to as query-by-text or QBT), where, instead of having a word image as a query, we have its transcription. [sent-158, score-0.437]
</p><p>57 Since attribute scores and PHOCs lie in the same space, we can simply compute the PHOC representation of the text and directly compare it against the dataset word images represented with attribute scores. [sent-159, score-0.851]
</p><p>58 To  the best of our knowledge, we are the first to provide an unified framework where we can perform OOV QBE and QBS, as well as to be able to query text datasets using word images without an OCR transcription of the query word. [sent-161, score-0.712]
</p><p>59 Calibration of scores Through the previous section we presented an attributesbased representation of the word images. [sent-163, score-0.569]
</p><p>60 Although this representation is writer-independent, special care has to be put when comparing different words, since the scores of one attribute may dominate over the scores of other attributes. [sent-164, score-0.353]
</p><p>61 Therefore, some calibration of the attribute scores is necessary. [sent-165, score-0.327]
</p><p>62 This is particularly true when performing QBS, since otherwise attribute scores are not comparable to the binary PHOC representations. [sent-166, score-0.221]
</p><p>63 Although the similarity measure involves all the attributes, the calibration of each attribute is done individually. [sent-170, score-0.232]
</p><p>64 Here we propose to perform the calibration of the scores 11002200  Attribute scores  PHOC  [ ·  ]  a1 a2 a3 a4 a5  ad  [ a1  a2  · a3  a4  a5  ] ad  WaProjected sub[ s· ·p ·a ]ceWb p1  p2  p3  p4  p5  pk  Figure 3. [sent-171, score-0.296]
</p><p>65 Projection of predicted attribute scores and attributes ground truth into a more correlated subspace with CCA. [sent-172, score-0.403]
</p><p>66 To achieve this goal, we make use of Canonical Correlation Analysis to embed the attribute scores and the binary attributes in a common subspace where they are maximally correlated (Fig. [sent-174, score-0.457]
</p><p>67 , wbk} projection vectors that project the attributes B into the k-dimensional common subspace. [sent-208, score-0.182]
</p><p>68 This CCA embedding can be seen as a way to exploit the correlation between different attributes to correct the scores predicted by the model. [sent-214, score-0.338]
</p><p>69 Furthermore, after CCA the attribute scores and binary attributes lie in a more correlated space, which makes the comparison between the scores and the PHOCs for our QBS problem more principled. [sent-215, score-0.498]
</p><p>70 One may also note that the relation between the attribute scores and the binary attributes may not be linear, and that a kernelized CCA could yield larger improvements. [sent-218, score-0.403]
</p><p>71 The document images are annotated at word level and contain the transcriptions of more than 115, 000 words. [sent-224, score-0.473]
</p><p>72 Images are also annotated at word level and contain approximately 5, 000 words. [sent-227, score-0.437]
</p><p>73 When computing the attribute representation, we use levels 2, 3, and 4, as well as 75 common bigrams at level 2, leading to 384 dimensions. [sent-245, score-0.223]
</p><p>74 When learning and projecting with CCA and KCCA, the representations (both score attributes and PHOCs) are first L2-normalized and mean centered. [sent-246, score-0.262]
</p><p>75 We used the first partition to learn the attributes representation, the second partition to learn the calibration as well as for validation purposes, and the third partition for testing purposes. [sent-253, score-0.428]
</p><p>76 We use the “calibration” partition to validate the parameters of the attribute classifiers, and a small subset of it to validate the calibration (the regularization ρ for CCA, plus the bandwidth γ and the number of random projections for KCCA). [sent-254, score-0.307]
</p><p>77 To train the attributes we use a one-versus-rest linear SVM with a SGD solver inspired in the implementation of L. [sent-256, score-0.182]
</p><p>78 At testing time, we use each word of the test dataset as a query and use it to rank the rest of the dataset using the cosine similarity between representations. [sent-258, score-0.51]
</p><p>79 Since there is no clear writer separation, we split it at word level. [sent-265, score-0.498]
</p><p>80 First, due to the simplicity of GW, the attribute scores are already very good (notice the 70% QBE map with no calibration at all compared to the 34% on IAM), and so they may not require a complex calibration. [sent-282, score-0.327]
</p><p>81 It is also interesting to check how the learning performed on the IAM dataset (where all the writers had a “modern” writing style) adapts to a dataset with a very different (250 years old) calligraphic style. [sent-284, score-0.197]
</p><p>82 We learn the attributes and the Platts weights and CCA and KCCA projections on the IAM dataset as before, and apply it directly to the FV extracted from the GW dataset. [sent-285, score-0.262]
</p><p>83 This is not surprising due to the large differences in  style, but also because the attributes learned on the GW are specialized to that particular writing style and so perform better when only that style is present at test time. [sent-288, score-0.354]
</p><p>84 The results on QBS show that indeed we are learning attributes correctly and not simply projecting on a different space completely uncorrelated with the transcription of the words. [sent-290, score-0.307]
</p><p>85 The character HMM seems to perform well on the IAM dataset, precisely because it exploits the relations between characters of different words during training. [sent-319, score-0.305]
</p><p>86 By contrast, comparing the same queries using our attributes embedded with CCA took less than 3 seconds on the same machine. [sent-324, score-0.261]
</p><p>87 Note however that comparison should be exercised with caution: although we use similar set partitions for both GW and IAM datasets, [9] does not perform word spotting but line spotting, i. [sent-327, score-0.729]
</p><p>88 On IAM, Frinken [9] reports a 79% map using as queries the most common nonstop words appearing in the training set, while we obtain a 71% using all the non-stop words appearing in the test set,  whether they appear on the training set or not. [sent-330, score-0.232]
</p><p>89 Finally, on Figure 4 we show some qualitative results on the IAM dataset, where we observe how the retrieved words can have very different styles from the query and still be retrieved successfully. [sent-333, score-0.228]
</p><p>90 Conclusions This paper proposes a method for multi-writer word spotting in handwritten documents. [sent-335, score-0.816]
</p><p>91 We show how an attributes-based approach based on a pyramidal histogram of characters can be used to learn how to embed the word images in a more discriminative space, where the similarity between words is independent of the writing style. [sent-336, score-0.849]
</p><p>92 This attributes representation leads to an unified representation of word images and strings, resulting in a method that allows one to perform either query-by-example or query-by-string searches. [sent-337, score-0.693]
</p><p>93 We show how to jointly calibrate all the attributes scores by means of CCA and KCCA, outperforming standard calibration methods. [sent-338, score-0.383]
</p><p>94 We compare our method in two public datasets, outperforming state-of-the-art approaches and showing that the proposed attribute-based representation is well-suited for word searches, whether they are images or strings, in handwritten documents. [sent-339, score-0.607]
</p><p>95 In the future we plan to explore the use of these approaches in a segmentation-free context, where the word images are not segmented. [sent-341, score-0.437]
</p><p>96 HMMbased word spotting in handwritten documents using subword models. [sent-390, score-0.859]
</p><p>97 A novel word spotting method based on recurrent neural networks. [sent-404, score-0.683]
</p><p>98 Local gradient histogram features for word spotting in unconstrained handwritten documents. [sent-493, score-0.844]
</p><p>99 Browsing heterogeneous document collections by a segmentation-free word spotting method. [sent-515, score-0.719]
</p><p>100 Offline cursive word recognition using continuous density hidden markov models trained with PCA or ICA features. [sent-551, score-0.471]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('word', 0.437), ('iam', 0.396), ('spotting', 0.246), ('cca', 0.225), ('gw', 0.19), ('attributes', 0.182), ('qbs', 0.172), ('fv', 0.161), ('kcca', 0.153), ('character', 0.138), ('phoc', 0.138), ('platts', 0.138), ('handwritten', 0.133), ('attribute', 0.126), ('qbe', 0.12), ('calibration', 0.106), ('frinken', 0.103), ('writers', 0.103), ('transcription', 0.099), ('scores', 0.095), ('writing', 0.094), ('words', 0.089), ('hmm', 0.089), ('string', 0.088), ('almaz', 0.086), ('phocs', 0.086), ('characters', 0.078), ('query', 0.073), ('bigrams', 0.069), ('rusi', 0.069), ('rodr', 0.067), ('styles', 0.066), ('writer', 0.061), ('queries', 0.054), ('representations', 0.054), ('embed', 0.054), ('strings', 0.053), ('manmatha', 0.053), ('keyword', 0.052), ('forn', 0.052), ('keywords', 0.05), ('projections', 0.049), ('dtw', 0.047), ('partitions', 0.046), ('fisher', 0.046), ('oov', 0.046), ('documents', 0.043), ('fischer', 0.042), ('english', 0.042), ('bag', 0.04), ('calibrating', 0.04), ('style', 0.039), ('pyramidal', 0.038), ('representation', 0.037), ('gmm', 0.036), ('document', 0.036), ('believe', 0.036), ('wb', 0.035), ('historical', 0.035), ('biocomputing', 0.034), ('cursive', 0.034), ('gatos', 0.034), ('ijdar', 0.034), ('keaton', 0.034), ('listen', 0.034), ('rath', 0.034), ('silent', 0.034), ('stopwords', 0.034), ('wak', 0.034), ('worddiscriminative', 0.034), ('retrieval', 0.033), ('ol', 0.033), ('embedding', 0.033), ('icdar', 0.032), ('learn', 0.031), ('rff', 0.031), ('vinciarelli', 0.031), ('descriptors', 0.03), ('occupancy', 0.03), ('text', 0.03), ('pyramid', 0.028), ('histogram', 0.028), ('xyz', 0.028), ('slant', 0.028), ('occ', 0.028), ('correlation', 0.028), ('leading', 0.028), ('dubbed', 0.027), ('gordo', 0.027), ('fvs', 0.027), ('george', 0.027), ('partition', 0.026), ('projecting', 0.026), ('keller', 0.025), ('embedded', 0.025), ('barcelona', 0.024), ('canonical', 0.024), ('pca', 0.024), ('wa', 0.024), ('aggregated', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="192-tfidf-1" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<p>Author: Jon Almazán, Albert Gordo, Alicia Fornés, Ernest Valveny</p><p>Abstract: We propose an approach to multi-writer word spotting, where the goal is to find a query word in a dataset comprised of document images. We propose an attributes-based approach that leads to a low-dimensional, fixed-length representation of the word images that is fast to compute and, especially, fast to compare. This approach naturally leads to an unified representation of word images and strings, which seamlessly allows one to indistinctly perform queryby-example, where the query is an image, and query-bystring, where the query is a string. We also propose a calibration scheme to correct the attributes scores based on Canonical Correlation Analysis that greatly improves the results on a challenging dataset. We test our approach on two public datasets showing state-of-the-art results.</p><p>2 0.24440426 <a title="192-tfidf-2" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<p>Author: Anand Mishra, Karteek Alahari, C.V. Jawahar</p><p>Abstract: We present an approach for the text-to-image retrieval problem based on textual content present in images. Given the recent developments in understanding text in images, an appealing approach to address this problem is to localize and recognize the text, and then query the database, as in a text retrieval problem. We show that such an approach, despite being based on state-of-the-artmethods, is insufficient, and propose a method, where we do not rely on an exact localization and recognition pipeline. We take a query-driven search approach, where we find approximate locations of characters in the text query, and then impose spatial constraints to generate a ranked list of images in the database. The retrieval performance is evaluated on public scene text datasets as well as three large datasets, namely IIIT scene text retrieval, Sports-10K and TV series-1M, we introduce.</p><p>3 0.22991341 <a title="192-tfidf-3" href="./iccv-2013-Recognizing_Text_with_Perspective_Distortion_in_Natural_Scenes.html">345 iccv-2013-Recognizing Text with Perspective Distortion in Natural Scenes</a></p>
<p>Author: Trung Quy Phan, Palaiahnakote Shivakumara, Shangxuan Tian, Chew Lim Tan</p><p>Abstract: This paper presents an approach to text recognition in natural scene images. Unlike most existing works which assume that texts are horizontal and frontal parallel to the image plane, our method is able to recognize perspective texts of arbitrary orientations. For individual character recognition, we adopt a bag-of-keypoints approach, in which Scale Invariant Feature Transform (SIFT) descriptors are extracted densely and quantized using a pre-trained vocabulary. Following [1, 2], the context information is utilized through lexicons. We formulate word recognition as finding the optimal alignment between the set of characters and the list of lexicon words. Furthermore, we introduce a new dataset called StreetViewText-Perspective, which contains texts in street images with a great variety of viewpoints. Experimental results on public datasets and the proposed dataset show that our method significantly outperforms the state-of-the-art on perspective texts of arbitrary orientations.</p><p>4 0.17677322 <a title="192-tfidf-4" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>Author: Xiaoyang Wang, Qiang Ji</p><p>Abstract: This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. As a list of semantically meaningful properties of objects, attributes generally relate to each other statistically. In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and objectindependent attribute relationships. The model utilizes the captured relationships to benefit both attribute prediction and object recognition. Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases.</p><p>5 0.16526024 <a title="192-tfidf-5" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>Author: Adriana Kovashka, Kristen Grauman</p><p>Abstract: Current methods learn monolithic attribute predictors, with the assumption that a single model is sufficient to reflect human understanding of a visual attribute. However, in reality, humans vary in how they perceive the association between a named property and image content. For example, two people may have slightly different internal models for what makes a shoe look “formal”, or they may disagree on which of two scenes looks “more cluttered”. Rather than discount these differences as noise, we propose to learn user-specific attribute models. We adapt a generic model trained with annotations from multiple users, tailoring it to satisfy user-specific labels. Furthermore, we propose novel techniques to infer user-specific labels based on transitivity and contradictions in the user’s search history. We demonstrate that adapted attributes improve accuracy over both existing monolithic models as well as models that learn from scratch with user-specific data alone. In addition, we show how adapted attributes are useful to personalize image search, whether with binary or relative attributes.</p><p>6 0.15329808 <a title="192-tfidf-6" href="./iccv-2013-PhotoOCR%3A_Reading_Text_in_Uncontrolled_Conditions.html">315 iccv-2013-PhotoOCR: Reading Text in Uncontrolled Conditions</a></p>
<p>7 0.12537447 <a title="192-tfidf-7" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>8 0.12511985 <a title="192-tfidf-8" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>9 0.11749323 <a title="192-tfidf-9" href="./iccv-2013-Linear_Sequence_Discriminant_Analysis%3A_A_Model-Based_Dimensionality_Reduction_Method_for_Vector_Sequences.html">253 iccv-2013-Linear Sequence Discriminant Analysis: A Model-Based Dimensionality Reduction Method for Vector Sequences</a></p>
<p>10 0.11736801 <a title="192-tfidf-10" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>11 0.11525264 <a title="192-tfidf-11" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>12 0.11428449 <a title="192-tfidf-12" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>13 0.10753414 <a title="192-tfidf-13" href="./iccv-2013-Learning_Coupled_Feature_Spaces_for_Cross-Modal_Matching.html">235 iccv-2013-Learning Coupled Feature Spaces for Cross-Modal Matching</a></p>
<p>14 0.10561946 <a title="192-tfidf-14" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>15 0.097600006 <a title="192-tfidf-15" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>16 0.095019951 <a title="192-tfidf-16" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>17 0.090601876 <a title="192-tfidf-17" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>18 0.090390407 <a title="192-tfidf-18" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>19 0.089151762 <a title="192-tfidf-19" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>20 0.088085338 <a title="192-tfidf-20" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.153), (1, 0.13), (2, -0.048), (3, -0.127), (4, 0.088), (5, 0.073), (6, -0.031), (7, -0.117), (8, 0.062), (9, 0.073), (10, 0.205), (11, -0.064), (12, 0.101), (13, 0.072), (14, -0.019), (15, 0.03), (16, 0.003), (17, 0.065), (18, -0.066), (19, -0.001), (20, 0.055), (21, 0.052), (22, 0.012), (23, 0.001), (24, -0.013), (25, 0.001), (26, -0.009), (27, 0.029), (28, 0.029), (29, 0.078), (30, 0.017), (31, 0.015), (32, -0.026), (33, 0.015), (34, -0.03), (35, -0.019), (36, -0.003), (37, -0.081), (38, 0.02), (39, 0.015), (40, 0.024), (41, -0.011), (42, -0.044), (43, 0.063), (44, -0.028), (45, -0.01), (46, -0.002), (47, -0.001), (48, 0.046), (49, -0.083)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94741172 <a title="192-lsi-1" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<p>Author: Jon Almazán, Albert Gordo, Alicia Fornés, Ernest Valveny</p><p>Abstract: We propose an approach to multi-writer word spotting, where the goal is to find a query word in a dataset comprised of document images. We propose an attributes-based approach that leads to a low-dimensional, fixed-length representation of the word images that is fast to compute and, especially, fast to compare. This approach naturally leads to an unified representation of word images and strings, which seamlessly allows one to indistinctly perform queryby-example, where the query is an image, and query-bystring, where the query is a string. We also propose a calibration scheme to correct the attributes scores based on Canonical Correlation Analysis that greatly improves the results on a challenging dataset. We test our approach on two public datasets showing state-of-the-art results.</p><p>2 0.74842674 <a title="192-lsi-2" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<p>Author: Anand Mishra, Karteek Alahari, C.V. Jawahar</p><p>Abstract: We present an approach for the text-to-image retrieval problem based on textual content present in images. Given the recent developments in understanding text in images, an appealing approach to address this problem is to localize and recognize the text, and then query the database, as in a text retrieval problem. We show that such an approach, despite being based on state-of-the-artmethods, is insufficient, and propose a method, where we do not rely on an exact localization and recognition pipeline. We take a query-driven search approach, where we find approximate locations of characters in the text query, and then impose spatial constraints to generate a ranked list of images in the database. The retrieval performance is evaluated on public scene text datasets as well as three large datasets, namely IIIT scene text retrieval, Sports-10K and TV series-1M, we introduce.</p><p>3 0.74272293 <a title="192-lsi-3" href="./iccv-2013-Recognizing_Text_with_Perspective_Distortion_in_Natural_Scenes.html">345 iccv-2013-Recognizing Text with Perspective Distortion in Natural Scenes</a></p>
<p>Author: Trung Quy Phan, Palaiahnakote Shivakumara, Shangxuan Tian, Chew Lim Tan</p><p>Abstract: This paper presents an approach to text recognition in natural scene images. Unlike most existing works which assume that texts are horizontal and frontal parallel to the image plane, our method is able to recognize perspective texts of arbitrary orientations. For individual character recognition, we adopt a bag-of-keypoints approach, in which Scale Invariant Feature Transform (SIFT) descriptors are extracted densely and quantized using a pre-trained vocabulary. Following [1, 2], the context information is utilized through lexicons. We formulate word recognition as finding the optimal alignment between the set of characters and the list of lexicon words. Furthermore, we introduce a new dataset called StreetViewText-Perspective, which contains texts in street images with a great variety of viewpoints. Experimental results on public datasets and the proposed dataset show that our method significantly outperforms the state-of-the-art on perspective texts of arbitrary orientations.</p><p>4 0.67689681 <a title="192-lsi-4" href="./iccv-2013-PhotoOCR%3A_Reading_Text_in_Uncontrolled_Conditions.html">315 iccv-2013-PhotoOCR: Reading Text in Uncontrolled Conditions</a></p>
<p>Author: Alessandro Bissacco, Mark Cummins, Yuval Netzer, Hartmut Neven</p><p>Abstract: We describe PhotoOCR, a system for text extraction from images. Our particular focus is reliable text extraction from smartphone imagery, with the goal of text recognition as a user input modality similar to speech recognition. Commercially available OCR performs poorly on this task. Recent progress in machine learning has substantially improved isolated character classification; we build on this progress by demonstrating a complete OCR system using these techniques. We also incorporate modern datacenter-scale distributed language modelling. Our approach is capable of recognizing text in a variety of challenging imaging conditions where traditional OCR systems fail, notably in the presence of substantial blur, low resolution, low contrast, high image noise and other distortions. It also operates with low latency; mean processing time is 600 ms per image. We evaluate our system on public benchmark datasets for text extraction and outperform all previously reported results, more than halving the error rate on multiple benchmarks. The system is currently in use in many applications at Google, and is available as a user input modality in Google Translate for Android.</p><p>5 0.66514772 <a title="192-lsi-5" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>Author: Lukáš Neumann, Jiri Matas</p><p>Abstract: An unconstrained end-to-end text localization and recognition method is presented. The method introduces a novel approach for character detection and recognition which combines the advantages of sliding-window and connected component methods. Characters are detected and recognized as image regions which contain strokes of specific orientations in a specific relative position, where the strokes are efficiently detected by convolving the image gradient field with a set of oriented bar filters. Additionally, a novel character representation efficiently calculated from the values obtained in the stroke detection phase is introduced. The representation is robust to shift at the stroke level, which makes it less sensitive to intra-class variations and the noise induced by normalizing character size and positioning. The effectiveness of the representation is demonstrated by the results achieved in the classification of real-world characters using an euclidian nearestneighbor classifier trained on synthetic data in a plain form. The method was evaluated on a standard dataset, where it achieves state-of-the-art results in both text localization and recognition.</p><p>6 0.62417305 <a title="192-lsi-6" href="./iccv-2013-Text_Localization_in_Natural_Images_Using_Stroke_Feature_Transform_and_Text_Covariance_Descriptors.html">415 iccv-2013-Text Localization in Natural Images Using Stroke Feature Transform and Text Covariance Descriptors</a></p>
<p>7 0.61793691 <a title="192-lsi-7" href="./iccv-2013-Linear_Sequence_Discriminant_Analysis%3A_A_Model-Based_Dimensionality_Reduction_Method_for_Vector_Sequences.html">253 iccv-2013-Linear Sequence Discriminant Analysis: A Model-Based Dimensionality Reduction Method for Vector Sequences</a></p>
<p>8 0.61756194 <a title="192-lsi-8" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>9 0.60511053 <a title="192-lsi-9" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>10 0.5967958 <a title="192-lsi-10" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>11 0.58361095 <a title="192-lsi-11" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>12 0.57739091 <a title="192-lsi-12" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>13 0.57330704 <a title="192-lsi-13" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>14 0.55287981 <a title="192-lsi-14" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>15 0.55173624 <a title="192-lsi-15" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>16 0.5286026 <a title="192-lsi-16" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>17 0.51438981 <a title="192-lsi-17" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>18 0.49803722 <a title="192-lsi-18" href="./iccv-2013-Learning_Coupled_Feature_Spaces_for_Cross-Modal_Matching.html">235 iccv-2013-Learning Coupled Feature Spaces for Cross-Modal Matching</a></p>
<p>19 0.48550707 <a title="192-lsi-19" href="./iccv-2013-To_Aggregate_or_Not_to_aggregate%3A_Selective_Match_Kernels_for_Image_Search.html">419 iccv-2013-To Aggregate or Not to aggregate: Selective Match Kernels for Image Search</a></p>
<p>20 0.48360041 <a title="192-lsi-20" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.078), (7, 0.028), (12, 0.017), (13, 0.025), (26, 0.057), (31, 0.09), (34, 0.029), (35, 0.012), (40, 0.016), (42, 0.071), (64, 0.041), (72, 0.282), (73, 0.017), (89, 0.136), (98, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70942354 <a title="192-lda-1" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<p>Author: Jon Almazán, Albert Gordo, Alicia Fornés, Ernest Valveny</p><p>Abstract: We propose an approach to multi-writer word spotting, where the goal is to find a query word in a dataset comprised of document images. We propose an attributes-based approach that leads to a low-dimensional, fixed-length representation of the word images that is fast to compute and, especially, fast to compare. This approach naturally leads to an unified representation of word images and strings, which seamlessly allows one to indistinctly perform queryby-example, where the query is an image, and query-bystring, where the query is a string. We also propose a calibration scheme to correct the attributes scores based on Canonical Correlation Analysis that greatly improves the results on a challenging dataset. We test our approach on two public datasets showing state-of-the-art results.</p><p>2 0.62253106 <a title="192-lda-2" href="./iccv-2013-A_Joint_Intensity_and_Depth_Co-sparse_Analysis_Model_for_Depth_Map_Super-resolution.html">18 iccv-2013-A Joint Intensity and Depth Co-sparse Analysis Model for Depth Map Super-resolution</a></p>
<p>Author: Martin Kiechle, Simon Hawe, Martin Kleinsteuber</p><p>Abstract: High-resolution depth maps can be inferred from lowresolution depth measurements and an additional highresolution intensity image of the same scene. To that end, we introduce a bimodal co-sparse analysis model, which is able to capture the interdependency of registered intensity . go l e i um . de . .t ities together with the knowledge of the relative positions between all views. Despite very active research in this area and significant improvements over the past years, stereo methods still struggle with noise, texture-less regions, repetitive texture, and occluded areas. For an overview of stereo methods, the reader is referred to [25]. and depth information. This model is based on the assumption that the co-supports of corresponding bimodal image structures are aligned when computed by a suitable pair of analysis operators. No analytic form of such operators ex- ist and we propose a method for learning them from a set of registered training signals. This learning process is done offline and returns a bimodal analysis operator that is universally applicable to natural scenes. We use this to exploit the bimodal co-sparse analysis model as a prior for solving inverse problems, which leads to an efficient algorithm for depth map super-resolution.</p><p>3 0.57704163 <a title="192-lda-3" href="./iccv-2013-Action_Recognition_with_Actons.html">38 iccv-2013-Action Recognition with Actons</a></p>
<p>Author: Jun Zhu, Baoyuan Wang, Xiaokang Yang, Wenjun Zhang, Zhuowen Tu</p><p>Abstract: With the improved accessibility to an exploding amount of video data and growing demands in a wide range of video analysis applications, video-based action recognition/classification becomes an increasingly important task in computer vision. In this paper, we propose a two-layer structure for action recognition to automatically exploit a mid-level “acton ” representation. The weakly-supervised actons are learned via a new max-margin multi-channel multiple instance learning framework, which can capture multiple mid-level action concepts simultaneously. The learned actons (with no requirement for detailed manual annotations) observe theproperties ofbeing compact, informative, discriminative, and easy to scale. The experimental results demonstrate the effectiveness ofapplying the learned actons in our two-layer structure, and show the state-ofthe-art recognition performance on two challenging action datasets, i.e., Youtube and HMDB51.</p><p>4 0.57573283 <a title="192-lda-4" href="./iccv-2013-Class-Specific_Simplex-Latent_Dirichlet_Allocation_for_Image_Classification.html">73 iccv-2013-Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification</a></p>
<p>Author: Mandar Dixit, Nikhil Rasiwasia, Nuno Vasconcelos</p><p>Abstract: An extension of the latent Dirichlet allocation (LDA), denoted class-specific-simplex LDA (css-LDA), is proposed for image classification. An analysis of the supervised LDA models currently used for this task shows that the impact of class information on the topics discovered by these models is very weak in general. This implies that the discovered topics are driven by general image regularities, rather than the semantic regularities of interest for classification. To address this, we introduce a model that induces supervision in topic discovery, while retaining the original flexibility of LDA to account for unanticipated structures of interest. The proposed css-LDA is an LDA model with class supervision at the level of image features. In css-LDA topics are discovered per class, i.e. a single set of topics shared across classes is replaced by multiple class-specific topic sets. This model can be used for generative classification using the Bayes decision rule or even extended to discriminative classification with support vector machines (SVMs). A css-LDA model can endow an image with a vector of class and topic specific count statistics that are similar to the Bag-of-words (BoW) histogram. SVM-based discriminants can be learned for classes in the space of these histograms. The effectiveness of css-LDA model in both generative and discriminative classification frameworks is demonstrated through an extensive experimental evaluation, involving multiple benchmark datasets, where it is shown to outperform all existing LDA based image classification approaches.</p><p>5 0.57527721 <a title="192-lda-5" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>Author: Ming-Ming Cheng, Jonathan Warrell, Wen-Yan Lin, Shuai Zheng, Vibhav Vineet, Nigel Crook</p><p>Abstract: Detecting visually salient regions in images is one of the fundamental problems in computer vision. We propose a novel method to decompose an image into large scale perceptually homogeneous elements for efficient salient region detection, using a soft image abstraction representation. By considering both appearance similarity and spatial distribution of image pixels, the proposed representation abstracts out unnecessary image details, allowing the assignment of comparable saliency values across similar regions, and producing perceptually accurate salient region detection. We evaluate our salient region detection approach on the largest publicly available dataset with pixel accurate annotations. The experimental results show that the proposed method outperforms 18 alternate methods, reducing the mean absolute error by 25.2% compared to the previous best result, while being computationally more efficient.</p><p>6 0.57296622 <a title="192-lda-6" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>7 0.5713197 <a title="192-lda-7" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>8 0.56980437 <a title="192-lda-8" href="./iccv-2013-Motion-Aware_KNN_Laplacian_for_Video_Matting.html">275 iccv-2013-Motion-Aware KNN Laplacian for Video Matting</a></p>
<p>9 0.56864172 <a title="192-lda-9" href="./iccv-2013-PhotoOCR%3A_Reading_Text_in_Uncontrolled_Conditions.html">315 iccv-2013-PhotoOCR: Reading Text in Uncontrolled Conditions</a></p>
<p>10 0.56790459 <a title="192-lda-10" href="./iccv-2013-Characterizing_Layouts_of_Outdoor_Scenes_Using_Spatial_Topic_Processes.html">72 iccv-2013-Characterizing Layouts of Outdoor Scenes Using Spatial Topic Processes</a></p>
<p>11 0.56658757 <a title="192-lda-11" href="./iccv-2013-Image_Retrieval_Using_Textual_Cues.html">210 iccv-2013-Image Retrieval Using Textual Cues</a></p>
<p>12 0.56557441 <a title="192-lda-12" href="./iccv-2013-Modeling_Occlusion_by_Discriminative_AND-OR_Structures.html">269 iccv-2013-Modeling Occlusion by Discriminative AND-OR Structures</a></p>
<p>13 0.56277359 <a title="192-lda-13" href="./iccv-2013-Robust_Matrix_Factorization_with_Unknown_Noise.html">357 iccv-2013-Robust Matrix Factorization with Unknown Noise</a></p>
<p>14 0.55878174 <a title="192-lda-14" href="./iccv-2013-Text_Localization_in_Natural_Images_Using_Stroke_Feature_Transform_and_Text_Covariance_Descriptors.html">415 iccv-2013-Text Localization in Natural Images Using Stroke Feature Transform and Text Covariance Descriptors</a></p>
<p>15 0.55773163 <a title="192-lda-15" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>16 0.55713183 <a title="192-lda-16" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>17 0.55660045 <a title="192-lda-17" href="./iccv-2013-Linear_Sequence_Discriminant_Analysis%3A_A_Model-Based_Dimensionality_Reduction_Method_for_Vector_Sequences.html">253 iccv-2013-Linear Sequence Discriminant Analysis: A Model-Based Dimensionality Reduction Method for Vector Sequences</a></p>
<p>18 0.55556405 <a title="192-lda-18" href="./iccv-2013-Real-Time_Articulated_Hand_Pose_Estimation_Using_Semi-supervised_Transductive_Regression_Forests.html">340 iccv-2013-Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests</a></p>
<p>19 0.55554152 <a title="192-lda-19" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<p>20 0.55539334 <a title="192-lda-20" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
