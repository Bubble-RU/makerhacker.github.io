<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>5 iccv-2013-A Color Constancy Model with Double-Opponency Mechanisms</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-5" href="#">iccv2013-5</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>5 iccv-2013-A Color Constancy Model with Double-Opponency Mechanisms</h1>
<br/><p>Source: <a title="iccv-2013-5-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Gao_A_Color_Constancy_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Shaobing Gao, Kaifu Yang, Chaoyi Li, Yongjie Li</p><p>Abstract: The double-opponent color-sensitive cells in the primary visual cortex (V1) of the human visual system (HVS) have long been recognized as the physiological basis of color constancy. We introduce a new color constancy model by imitating the functional properties of the HVS from the retina to the double-opponent cells in V1. The idea behind the model originates from the observation that the color distribution of the responses of double-opponent cells to the input color-biased images coincides well with the light source direction. Then the true illuminant color of a scene is easily estimated by searching for the maxima of the separate RGB channels of the responses of double-opponent cells in the RGB space. Our systematical experimental evaluations on two commonly used image datasets show that the proposed model can produce competitive results in comparison to the complex state-of-the-art approaches, but with a simple implementation and without the need for training.</p><p>Reference: <a title="iccv-2013-5-reference" href="../iccv2013_reference/iccv-2013-A_Color_Constancy_Model_with_Double-Opponency_Mechanisms_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com, Abstract The double-opponent color-sensitive cells in the primary visual cortex (V1) of the human visual system (HVS) have long been recognized as the physiological basis of color constancy. [sent-2, score-0.914]
</p><p>2 We introduce a new color constancy model by imitating the functional properties of the HVS from the retina to the double-opponent cells in V1. [sent-3, score-1.169]
</p><p>3 The idea behind the model originates from the observation that the color distribution of the responses of double-opponent cells to the input color-biased images coincides well with the light source direction. [sent-4, score-0.89]
</p><p>4 Then the true illuminant color of a scene is easily estimated by searching for the maxima of the separate RGB channels of the responses of double-opponent cells in the RGB space. [sent-5, score-1.088]
</p><p>5 Introduction  Color constancy is one of the amazing abilities of perceptual constancy of the human visual system, which enables the perceived color of objects largely constant as the light source color changes [7]. [sent-8, score-1.39]
</p><p>6 In contrast, captured with regular digital cameras or videos, the physical color of scenes may be shifted by the varying external illuminant. [sent-9, score-0.214]
</p><p>7 Figure 1 shows the shift of color distribution between the canonical and color-biased images of the same scene. [sent-10, score-0.253]
</p><p>8 One of the important problems in computer vision, especially for the robust color-based systems, is to extract reliable color features that are invariant to the changes in external lighting. [sent-11, score-0.214]
</p><p>9 A general solution is to estimate the scene illuminant and then remove the illuminant from color-biased images to get the so-called canonical images under a white light source [17]. [sent-12, score-0.787]
</p><p>10 The blue and green crosses in the scatter plot denote the color distributions of the color-biased and canonical images[24], respectively. [sent-19, score-0.29]
</p><p>11 The red crosses in the scatter plot show the color distribution of the responses of the double-opponent cells in our model (i. [sent-20, score-0.772]
</p><p>12 The true illuminant is shown as a black solid line. [sent-23, score-0.312]
</p><p>13 mally achromatic, based on which the illuminant components could be estimated simply by computing the mean in each color channel (i. [sent-24, score-0.526]
</p><p>14 In order to reinforce the simple assumption about the reflectance distribution, a group of learning-based color constancy models introduce priori information about illuminant and employ statistical computation to estimate the illuminant. [sent-30, score-1.019]
</p><p>15 Typical examples include the gamut mapping [11, 16], 992299  Type II cells in LGN with color-opponent centre-only RF (a) and Type Icells with color-opponent centre-surround RF (b). [sent-31, score-0.52]
</p><p>16 The RF of red-green double-opponent cells in the primary visual cortex (V1) (c) can be computationally constructed using two singleopponent Type IIcells with different RF scales and opposite signs (d). [sent-32, score-0.562]
</p><p>17 Bayesian color constancy [14], regression based [12], statistics based [15, 6], and high-level visual information based [28, 3]. [sent-35, score-0.659]
</p><p>18 However, most learning-based models need to be correctly trained before the illuminant can be estimated, which is a crucial distinction that may partially determine the suitability of a color constancy model for applicability to real-world systems [17]. [sent-36, score-0.945]
</p><p>19 These physics-based methods assume non-Lambertian reflectance and explicitly use the presence of specular highlights, and the estimate of the illuminant color can be simply obtained as a solution to a set of equations. [sent-38, score-0.634]
</p><p>20 In this paper, we propose a visual system based color constancy model, and the idea behind the model originates from the computational discoveries that the responses of double-opponent (DO) cells to the color-biased images provide clear information about the scene illuminant. [sent-40, score-1.212]
</p><p>21 From Figure 1 we can find that the color distribution of the responses of DO cells to a color-biased image coincides well with the direction of light source color (see the supplemental materials for more examples1). [sent-41, score-1.111]
</p><p>22 Based on this substantial observation, we directly use the MAX mechanism to extract the true illuminant from the responses of a group of DO cells. [sent-42, score-0.391]
</p><p>23 Evaluation of our model on two typical dataset-  s commonly used in the field of computational color constancy demonstrate our model’s competitive results to the state-of-the-art approaches. [sent-43, score-0.633]
</p><p>24 We begin with an overview of the color processing mechanisms in the visual system in Section 2. [sent-45, score-0.307]
</p><p>25 In the following we give a summary to the color processing mechanisms in the early stages of the visual system. [sent-56, score-0.28]
</p><p>26 Cone photoreceptor layer of retina: The first stage of color processing of our visual system takes place in the photoreceptor layer of the retina. [sent-57, score-0.453]
</p><p>27 There are two types of photoreceptors: rods and cones, and cones are responsible for  color vision. [sent-58, score-0.269]
</p><p>28 Based on the spectral sensitivities, cones can be classified into short-wavelength cone (S-cone), mediumwavelength cone (M-cone) and long-wavelength cone (Lcone), which respond preferably to the blue (B), green (G) and red (R) colors, respectively. [sent-59, score-0.618]
</p><p>29 The color information into the eyes is first coded in a trichromatic way via L-, Mand S-cones in the retina, and then propagated in the way of color opponency via single-opponent and double-opponent neurons at the levels of retinal ganglion layer, LGN and V1 (and the higher cortical areas). [sent-60, score-0.835]
</p><p>30 Single-opponent cells: Most color-sensitive cells at the levels of ganglion layer and LGN are single-opponent cells that code the color information within their receptive fields (RFs) in the ways of red-green, blue-yellow, and blackwhite opponency. [sent-61, score-1.369]
</p><p>31 Figure 2(a) and (b) show the RF structures of the red-on/green-off single-opponent cells of type II and I, respectively. [sent-62, score-0.491]
</p><p>32 Type II cells (Figure 2(a)) have centeronly color opponent RF and respond well to uniformly colored areas [23]. [sent-63, score-0.748]
</p><p>33 In contrast, type I cells (Figure 2(b)) have center-surround RF structure with color opponency and respond well to color contrast [23]. [sent-64, score-1.122]
</p><p>34 Double-opponent cells: Many experiments [21, 25, 13] have revealed that the double-opponent (DO) cells exist widely in V1. [sent-65, score-0.442]
</p><p>35 Based on the spatial structure of RF, DO cells could be classified into two types: DO cells with concentric RFs (Figure 2(c)) and DO cells with oriented RFs (not shown here). [sent-66, score-1.36]
</p><p>36 These properties of such type of DO cells make them the reasonable physiological building blocks of color constancy as well as the phenomenon of color contrast [7, 13]. [sent-68, score-1.449]
</p><p>37 In this study we use the DO cells with concentric RFs to build color constancy model. [sent-69, score-1.109]
</p><p>38 It has been physiologically found that the majority of DO cells, of the both types, receive unbalanced 993300  cone input [23]. [sent-70, score-0.341]
</p><p>39 Hence, the DO cells used in this work have unbalanced center-surround RF structure, and DO cells with such RF property may respond to both the spatial color contrasts (i. [sent-71, score-1.275]
</p><p>40 Double-opponency based color constancy Figure 3 shows the flowchart of our double-opponency based color constancy model. [sent-75, score-1.293]
</p><p>41 The different layers in this bottom-up hierarchical framework mainly correspond to the color processing strategies involved in the HVS from the retina to V1. [sent-76, score-0.308]
</p><p>42 The key idea behind our model is to estimate the illuminant from the responses of the double-opponent cells. [sent-77, score-0.391]
</p><p>43 Cone layer: The input color image is separated into three channels: red (R), green (G), blue (B), which are denoted by r(x, y), g(x, y), and b(x, y) and sent into the L-, M-, and S-cones, respectively. [sent-78, score-0.214]
</p><p>44 Retinal ganglion/LGN layer: The retinal ganglion cells receive the outputs from the cones via horizontal cells and bipolar cells, and then send signals to LGN. [sent-83, score-1.213]
</p><p>45 Generally, ganglion cells and LGN cells have similar RF properties. [sent-84, score-1.011]
</p><p>46 Here we implement the processing of ganglion layer and LGN into a single step for simplicity. [sent-85, score-0.194]
</p><p>47 In this study we only consider the type II single-opponent (SO) cells (Figure 2(a)), which are used to computationally construct the RFs of doubleopponent (DO) cells (Figure 2(c) and (d)). [sent-86, score-0.933]
</p><p>48 Taking a SO cell of type II (Figure 2(a)) with red-on/green-off (R+G-, or L+M-) opponency as example, its response is computed as SOr+g− (x, y; σ) = Org (x, y)∗RF(x, y; σ) (3) where ∗ denotes the convolution. [sent-90, score-0.249]
</p><p>49 V1 layer: It is not yet fully understood how LGN projects to V1 to form the concentrically organized receptive fields (RFs) of DO cells that are both chromatically and spatially opponent. [sent-94, score-0.563]
</p><p>50 , the RF of a DO cell of L+M/M+L- shown in Figure 2(c) could be constructed using the outputs from two SO cells of type II with different scales: one red-on/green-off SO cell with smaller RF scale and another green-on/red-off SO cell with larger RF scale (Figure 2(d)). [sent-97, score-0.656]
</p><p>51 We set λ = 3 based on the physiological finding that the size of receptive field (RF) surround is roughly 3 times (in diameter) larger than that of RF center [22]. [sent-99, score-0.217]
</p><p>52 k 1 implies that tDroOls sc tehlels c roencetriivbeu tuionbna olfan RceFd s cone inputs, =and 1 ihmenpcleie, sh tahvaet unbalanced center-surround structures [23]. [sent-101, score-0.242]
</p><p>53 Higher visual cortex: It is as yet unknown which level of the human visual system finally realizes color constancy. [sent-102, score-0.241]
</p><p>54 What is known, however, is that color constant cells have been found at the level of visual area V4 [7, 13]. [sent-103, score-0.656]
</p><p>55 The cells of V4 normally have a very large receptive field, which may endow the V4 cells with ability to extract light source color based on global statistics. [sent-104, score-1.335]
</p><p>56 Taking a color-biased image as shown in Figure 1 as input, we compute the output of DO cells using Equations (1)∼(5). [sent-106, score-0.442]
</p><p>57 We can find from the scatter plot of Figure 1 that the distribution of the responses of DO cells (i. [sent-107, score-0.558]
</p><p>58 The flowchart of our double-opponency based color constancy model. [sent-110, score-0.66]
</p><p>59 Based on this substantial phenomenon, we speculate that V4 cells might adopt certain mechanisms to compute an accurate illuminant estimate of the scene using the color distribution of the DO cells’ output in the RGB space. [sent-115, score-1.034]
</p><p>60 We assume that the scene is illuminated by a single light source, and the color of the illuminant, = (er, eg, eb), is estimated using a canonical neural computation mechanism of MAX [5], according to  E? [sent-116, score-0.366]
</p><p>61 median angular error, we also reported the measures o? [sent-166, score-0.161]
</p><p>62 (b) Mondrian images with less (first row) and more (second row) color blocks. [sent-173, score-0.214]
</p><p>63 The mondrian images are generated using the surface reflectance spectra combined with illuminant spectra from [1]. [sent-178, score-0.491]
</p><p>64 , the scale of receptive field (σ) (in Equations (2)∼(4)) and the cone weight (k) (in Equation (4)). [sent-182, score-0.234]
</p><p>65 Figure 5 shows the influ-  ences of different parameter values on the model’s performance in terms of median angular error, according to which we set σ = 2. [sent-183, score-0.161]
</p><p>66 Figure 4(a) shows that DO cells with balanced cone inputs (i. [sent-187, score-0.631]
</p><p>67 , k = 1) only respond to the color contrast, and in contrast, DO cells with unbalanced cone inputs (i. [sent-189, score-1.022]
</p><p>68 regions [7], and with a higher k, the color edges are enhanced much more than the color regions. [sent-193, score-0.455]
</p><p>69 Figure 4(b) indicates that if fewer objects and hence fewer edges are available in an image, enhancing edges would contribute much more than enhancing color regions to the illuminant estimation. [sent-194, score-0.58]
</p><p>70 Real-World Image Set Gehler-Shi dataset [24] contains 568 high dynamic range linear images, including a variety of indoor and outdoor scenes, captured using a high-quality digital SLR camera in RAW format and therefore free of any color correction. [sent-202, score-0.32]
</p><p>71 In this study, the color-checker patch in each image used for computing ground truth illuminant was masked out in  order to fully evaluate the performance of a specific model. [sent-203, score-0.312]
</p><p>72 It can be seen from Table 1that the performance of the proposed method almost arrives at or beyond the best performance (in terms of median and mean angular errors) of the state-of-the-art learningbased algorithms (e. [sent-205, score-0.161]
</p><p>73 Although the median angular error of the best algorithm (GM(pixel)) is slightly lower than our method, the robustness (indicated by the measure of worst-25%) of our method is much better than that of GM(pixel). [sent-208, score-0.161]
</p><p>74 Figure 7 shows the median angular errors over all 568 images of the dataset for different methods, the error bars in which indicate a confidence interval of 95% [15]. [sent-211, score-0.161]
</p><p>75 Figure 6 show examples of both indoor and outdoor images corrected with the illuminant estimates of various methods. [sent-213, score-0.45]
</p><p>76 The reason may be that the learning-based algorithms are seriously dependent on the illuminant priori used for training. [sent-219, score-0.312]
</p><p>77 However, both the two indoor images in Figure 6 contain self-emitting light source (e. [sent-220, score-0.185]
</p><p>78 , the LCD and halogen lamp), which may not satisfy the illuminant priori assumed by these methods. [sent-222, score-0.312]
</p><p>79 The influence of receptive field size (σ) and the cone weight (k) of our model on the measure of median angular error. [sent-234, score-0.395]
</p><p>80 Conclusion and Future Work We proposed a physiologically based color constancy model, which is inspired by the physiological research of color constancy in the human and primate visual system. [sent-250, score-1.479]
</p><p>81 The hierarchical steps of the proposed model correspond to the color processing mechanisms involved in the HVS from the retina to the primary visual cortex (V1). [sent-251, score-0.494]
</p><p>82 We found that the responses of double-opponent (DO) cells in V1 to the color-biased images contain the exact information about the scene illuminant; especially, the max responses of the DO  cells could be utilized as the estimated illuminant. [sent-252, score-1.042]
</p><p>83 Though DO cells have been modeled by many researchers, even for the purpose of color constancy [9], our model differs entirely from them in how to utilize the output of DO cells. [sent-254, score-1.075]
</p><p>84 network based on DO cells for color constancy [8]. [sent-491, score-1.075]
</p><p>85 In their model, color constancy is achieved by using the output from the DO cells as input for a neural network of four neurons in V4. [sent-492, score-1.137]
</p><p>86 As for the popular max-RGB, the light source color is estimated from the maximum response of the separate color channels [19]. [sent-493, score-0.627]
</p><p>87 Different from the above-mentioned typical methods, of which the assumptions are based on the color distribution (e. [sent-495, score-0.214]
</p><p>88 , the max or average pixel values) in the original (or preprocessed) RGB images, our model computes the light source color by searching the maximum from the sepa-  rate RGB channels, which are transformed from the DO responses obtained in the double-opponent space. [sent-497, score-0.417]
</p><p>89 In addition, our DO cells receive unbalanced cone inputs, which provides a flexible way to utilize the information of scenes. [sent-498, score-0.717]
</p><p>90 Because of the imbalance, these DO cells would respond best to (and enhance greatly) the edges defined by the chromatic and luminance differences [7, 13]. [sent-499, score-0.673]
</p><p>91 As most edges in the real world do combine luminance and chromatic differences [29], such cells would be extremely useful for the analysis of natural scenes. [sent-500, score-0.581]
</p><p>92 These features endow our DO-based model with larger chance to get an accurate illuminant estimate by utilizing more effective local and global information of scenes. [sent-502, score-0.348]
</p><p>93 For example, the non-linear color-luminance interactions between the blue-yellow and luminance channels, which may reveal a possible neural correlate of stable perception of color constancy [18]. [sent-505, score-0.747]
</p><p>94 Color categorization and color constancy in a neural network model of v4. [sent-562, score-0.666]
</p><p>95 Color constancy using natural image statistics and scene semantics. [sent-601, score-0.445]
</p><p>96 Generalized gamut mapping using image derivative structures for color constancy. [sent-607, score-0.292]
</p><p>97 Anatomy and physiology of a color system in the primate visual cortex. [sent-638, score-0.277]
</p><p>98 Quantitative analysis of cat retinal ganglion cell response to visual stimuli. [sent-643, score-0.297]
</p><p>99 Re-processed version of the gehler color constancy dataset of 568 images. [sent-654, score-0.633]
</p><p>100 Using high-  level visual information for color constancy. [sent-682, score-0.214]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cells', 0.442), ('constancy', 0.419), ('illuminant', 0.312), ('rf', 0.218), ('color', 0.214), ('cone', 0.157), ('lgn', 0.137), ('rfs', 0.137), ('ganglion', 0.127), ('neurosci', 0.111), ('opponency', 0.111), ('physiological', 0.111), ('angular', 0.104), ('retina', 0.094), ('gm', 0.093), ('respond', 0.092), ('sfu', 0.09), ('cortex', 0.088), ('grey', 0.086), ('unbalanced', 0.085), ('retinal', 0.081), ('luminance', 0.081), ('light', 0.08), ('responses', 0.079), ('gamut', 0.078), ('receptive', 0.077), ('reflectance', 0.074), ('hvs', 0.068), ('layer', 0.067), ('physiologically', 0.066), ('rgb', 0.066), ('mechanisms', 0.066), ('chromaticity', 0.065), ('indoor', 0.061), ('cortical', 0.059), ('gijsenij', 0.059), ('rev', 0.059), ('sol', 0.058), ('median', 0.057), ('cones', 0.055), ('cell', 0.055), ('type', 0.049), ('lab', 0.048), ('colour', 0.047), ('nat', 0.047), ('outdoor', 0.045), ('source', 0.044), ('achromatic', 0.044), ('bianco', 0.044), ('ccnis', 0.044), ('cic', 0.044), ('concentrically', 0.044), ('dufort', 0.044), ('horwitz', 0.044), ('photoreceptors', 0.044), ('shaobing', 0.044), ('systematical', 0.044), ('uestc', 0.044), ('org', 0.041), ('channels', 0.041), ('dichromatic', 0.039), ('excitation', 0.039), ('mondrian', 0.039), ('photoreceptor', 0.039), ('sog', 0.039), ('soy', 0.039), ('canonical', 0.039), ('supplemental', 0.038), ('scatter', 0.037), ('endow', 0.036), ('funt', 0.036), ('primate', 0.036), ('dt', 0.035), ('gevers', 0.034), ('specular', 0.034), ('concentric', 0.034), ('inhibition', 0.034), ('response', 0.034), ('neural', 0.033), ('receive', 0.033), ('signals', 0.033), ('shades', 0.033), ('spectra', 0.033), ('inputs', 0.032), ('primary', 0.032), ('corrected', 0.032), ('ol', 0.032), ('chromatic', 0.031), ('originates', 0.031), ('sor', 0.031), ('ii', 0.03), ('neurons', 0.029), ('surround', 0.029), ('dti', 0.028), ('josa', 0.028), ('system', 0.027), ('edges', 0.027), ('weijer', 0.027), ('flowchart', 0.027), ('statistics', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999905 <a title="5-tfidf-1" href="./iccv-2013-A_Color_Constancy_Model_with_Double-Opponency_Mechanisms.html">5 iccv-2013-A Color Constancy Model with Double-Opponency Mechanisms</a></p>
<p>Author: Shaobing Gao, Kaifu Yang, Chaoyi Li, Yongjie Li</p><p>Abstract: The double-opponent color-sensitive cells in the primary visual cortex (V1) of the human visual system (HVS) have long been recognized as the physiological basis of color constancy. We introduce a new color constancy model by imitating the functional properties of the HVS from the retina to the double-opponent cells in V1. The idea behind the model originates from the observation that the color distribution of the responses of double-opponent cells to the input color-biased images coincides well with the light source direction. Then the true illuminant color of a scene is easily estimated by searching for the maxima of the separate RGB channels of the responses of double-opponent cells in the RGB space. Our systematical experimental evaluations on two commonly used image datasets show that the proposed model can produce competitive results in comparison to the complex state-of-the-art approaches, but with a simple implementation and without the need for training.</p><p>2 0.37892234 <a title="5-tfidf-2" href="./iccv-2013-Illuminant_Chromaticity_from_Image_Sequences.html">207 iccv-2013-Illuminant Chromaticity from Image Sequences</a></p>
<p>Author: Veronique Prinet, Dani Lischinski, Michael Werman</p><p>Abstract: We estimate illuminant chromaticity from temporal sequences, for scenes illuminated by either one or two dominant illuminants. While there are many methods for illuminant estimation from a single image, few works so far have focused on videos, and even fewer on multiple light sources. Our aim is to leverage information provided by the temporal acquisition, where either the objects or the camera or the light source are/is in motion in order to estimate illuminant color without the need for user interaction or using strong assumptions and heuristics. We introduce a simple physically-based formulation based on the assumption that the incident light chromaticity is constant over a short space-time domain. We show that a deterministic approach is not sufficient for accurate and robust estimation: however, a probabilistic formulation makes it possible to implicitly integrate away hidden factors that have been ignored by the physical model. Experimental results are reported on a dataset of natural video sequences and on the GrayBall benchmark, indicating that we compare favorably with the state-of-the-art.</p><p>3 0.27197167 <a title="5-tfidf-3" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>Author: Dapeng Chen, Zejian Yuan, Yang Wu, Geng Zhang, Nanning Zheng</p><p>Abstract: Representation is a fundamental problem in object tracking. Conventional methods track the target by describing its local or global appearance. In this paper we present that, besides the two paradigms, the composition of local region histograms can also provide diverse and important object cues. We use cells to extract local appearance, and construct complex cells to integrate the information from cells. With different spatial arrangements of cells, complex cells can explore various contextual information at multiple scales, which is important to improve the tracking performance. We also develop a novel template-matching algorithm for object tracking, where the template is composed of temporal varying cells and has two layers to capture the target and background appearance respectively. An adaptive weight is associated with each complex cell to cope with occlusion as well as appearance variation. A fusion weight is associated with each complex cell type to preserve the global distinctiveness. Our algorithm is evaluated on 25 challenging sequences, and the results not only confirm the contribution of each component in our tracking system, but also outperform other competing trackers.</p><p>4 0.12693265 <a title="5-tfidf-4" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<p>Author: Tae Hyun Kim, Hee Seok Lee, Kyoung Mu Lee</p><p>Abstract: Many state-of-the-art optical flow estimation algorithms optimize the data and regularization terms to solve ill-posed problems. In this paper, in contrast to the conventional optical flow framework that uses a single or fixed data model, we study a novel framework that employs locally varying data term that adaptively combines different multiple types of data models. The locally adaptive data term greatly reduces the matching ambiguity due to the complementary nature of the multiple data models. The optimal number of complementary data models is learnt by minimizing the redundancy among them under the minimum description length constraint (MDL). From these chosen data models, a new optical flow estimation energy model is designed with the weighted sum of the multiple data models, and a convex optimization-based highly effective and practical solution thatfinds the opticalflow, as well as the weights isproposed. Comparative experimental results on the Middlebury optical flow benchmark show that the proposed method using the complementary data models outperforms the state-ofthe art methods.</p><p>5 0.12266467 <a title="5-tfidf-5" href="./iccv-2013-Toward_Guaranteed_Illumination_Models_for_Non-convex_Objects.html">422 iccv-2013-Toward Guaranteed Illumination Models for Non-convex Objects</a></p>
<p>Author: Yuqian Zhang, Cun Mu, Han-Wen Kuo, John Wright</p><p>Abstract: Illumination variation remains a central challenge in object detection and recognition. Existing analyses of illumination variation typically pertain to convex, Lambertian objects, and guarantee quality of approximation in an average case sense. We show that it is possible to build models for the set of images across illumination variation with worstcase performance guarantees, for nonconvex Lambertian objects. Namely, a natural verification test based on the distance to the model guarantees to accept any image which can be sufficiently well-approximated by an image of the object under some admissible lighting condition, and guarantees to reject any image that does not have a sufficiently good approximation. These models are generated by sampling illumination directions with sufficient density, which follows from a new perturbation bound for directional illuminated images in the Lambertian model. As the number of such images required for guaranteed verification may be large, we introduce a new formulation for cone preserving dimensionality reduction, which leverages tools from sparse and low-rank decomposition to reduce the complexity, while controlling the approximation error with respect to the original model. 1</p><p>6 0.1110967 <a title="5-tfidf-6" href="./iccv-2013-Decomposing_Bag_of_Words_Histograms.html">104 iccv-2013-Decomposing Bag of Words Histograms</a></p>
<p>7 0.070754781 <a title="5-tfidf-7" href="./iccv-2013-A_Simple_Model_for_Intrinsic_Image_Decomposition_with_Depth_Cues.html">30 iccv-2013-A Simple Model for Intrinsic Image Decomposition with Depth Cues</a></p>
<p>8 0.067235097 <a title="5-tfidf-8" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>9 0.064934731 <a title="5-tfidf-9" href="./iccv-2013-Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illumination_in_the_Spectral_Domain.html">385 iccv-2013-Separating Reflective and Fluorescent Components Using High Frequency Illumination in the Spectral Domain</a></p>
<p>10 0.063120656 <a title="5-tfidf-10" href="./iccv-2013-Alternating_Regression_Forests_for_Object_Detection_and_Pose_Estimation.html">47 iccv-2013-Alternating Regression Forests for Object Detection and Pose Estimation</a></p>
<p>11 0.060621537 <a title="5-tfidf-11" href="./iccv-2013-Dynamic_Probabilistic_Volumetric_Models.html">128 iccv-2013-Dynamic Probabilistic Volumetric Models</a></p>
<p>12 0.05979785 <a title="5-tfidf-12" href="./iccv-2013-Conservation_Tracking.html">87 iccv-2013-Conservation Tracking</a></p>
<p>13 0.058331188 <a title="5-tfidf-13" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>14 0.058242019 <a title="5-tfidf-14" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>15 0.05701777 <a title="5-tfidf-15" href="./iccv-2013-Random_Forests_of_Local_Experts_for_Pedestrian_Detection.html">336 iccv-2013-Random Forests of Local Experts for Pedestrian Detection</a></p>
<p>16 0.054327957 <a title="5-tfidf-16" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>17 0.053899001 <a title="5-tfidf-17" href="./iccv-2013-Quantize_and_Conquer%3A_A_Dimensionality-Recursive_Solution_to_Clustering%2C_Vector_Quantization%2C_and_Image_Retrieval.html">333 iccv-2013-Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval</a></p>
<p>18 0.053271558 <a title="5-tfidf-18" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>19 0.052701894 <a title="5-tfidf-19" href="./iccv-2013-Exploiting_Reflection_Change_for_Automatic_Reflection_Removal.html">151 iccv-2013-Exploiting Reflection Change for Automatic Reflection Removal</a></p>
<p>20 0.051531866 <a title="5-tfidf-20" href="./iccv-2013-Structured_Light_in_Sunlight.html">405 iccv-2013-Structured Light in Sunlight</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.123), (1, -0.052), (2, -0.004), (3, -0.008), (4, 0.008), (5, 0.002), (6, -0.019), (7, -0.015), (8, -0.014), (9, -0.028), (10, -0.014), (11, -0.024), (12, 0.062), (13, 0.004), (14, 0.024), (15, -0.064), (16, -0.02), (17, 0.017), (18, 0.066), (19, 0.032), (20, 0.018), (21, 0.0), (22, 0.041), (23, -0.116), (24, -0.171), (25, 0.094), (26, 0.036), (27, -0.021), (28, 0.087), (29, -0.084), (30, 0.124), (31, 0.025), (32, 0.087), (33, 0.157), (34, 0.021), (35, 0.104), (36, -0.106), (37, -0.212), (38, -0.109), (39, 0.155), (40, -0.038), (41, 0.036), (42, 0.099), (43, 0.012), (44, 0.04), (45, -0.053), (46, 0.025), (47, -0.084), (48, -0.17), (49, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95764494 <a title="5-lsi-1" href="./iccv-2013-A_Color_Constancy_Model_with_Double-Opponency_Mechanisms.html">5 iccv-2013-A Color Constancy Model with Double-Opponency Mechanisms</a></p>
<p>Author: Shaobing Gao, Kaifu Yang, Chaoyi Li, Yongjie Li</p><p>Abstract: The double-opponent color-sensitive cells in the primary visual cortex (V1) of the human visual system (HVS) have long been recognized as the physiological basis of color constancy. We introduce a new color constancy model by imitating the functional properties of the HVS from the retina to the double-opponent cells in V1. The idea behind the model originates from the observation that the color distribution of the responses of double-opponent cells to the input color-biased images coincides well with the light source direction. Then the true illuminant color of a scene is easily estimated by searching for the maxima of the separate RGB channels of the responses of double-opponent cells in the RGB space. Our systematical experimental evaluations on two commonly used image datasets show that the proposed model can produce competitive results in comparison to the complex state-of-the-art approaches, but with a simple implementation and without the need for training.</p><p>2 0.82536566 <a title="5-lsi-2" href="./iccv-2013-Illuminant_Chromaticity_from_Image_Sequences.html">207 iccv-2013-Illuminant Chromaticity from Image Sequences</a></p>
<p>Author: Veronique Prinet, Dani Lischinski, Michael Werman</p><p>Abstract: We estimate illuminant chromaticity from temporal sequences, for scenes illuminated by either one or two dominant illuminants. While there are many methods for illuminant estimation from a single image, few works so far have focused on videos, and even fewer on multiple light sources. Our aim is to leverage information provided by the temporal acquisition, where either the objects or the camera or the light source are/is in motion in order to estimate illuminant color without the need for user interaction or using strong assumptions and heuristics. We introduce a simple physically-based formulation based on the assumption that the incident light chromaticity is constant over a short space-time domain. We show that a deterministic approach is not sufficient for accurate and robust estimation: however, a probabilistic formulation makes it possible to implicitly integrate away hidden factors that have been ignored by the physical model. Experimental results are reported on a dataset of natural video sequences and on the GrayBall benchmark, indicating that we compare favorably with the state-of-the-art.</p><p>3 0.78326738 <a title="5-lsi-3" href="./iccv-2013-Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illumination_in_the_Spectral_Domain.html">385 iccv-2013-Separating Reflective and Fluorescent Components Using High Frequency Illumination in the Spectral Domain</a></p>
<p>Author: Ying Fu, Antony Lam, Imari Sato, Takahiro Okabe, Yoichi Sato</p><p>Abstract: Hyperspectral imaging is beneficial to many applications but current methods do not consider fluorescent effects which are present in everyday items ranging from paper, to clothing, to even our food. Furthermore, everyday fluorescent items exhibit a mix of reflectance and fluorescence. So proper separation of these components is necessary for analyzing them. In this paper, we demonstrate efficient separation and recovery of reflective and fluorescent emission spectra through the use of high frequency illumination in the spectral domain. With the obtained fluorescent emission spectra from our high frequency illuminants, we then present to our knowledge, the first method for estimating the fluorescent absorption spectrum of a material given its emission spectrum. Conventional bispectral measurement of absorption and emission spectra needs to examine all combinations of incident and observed light wavelengths. In contrast, our method requires only two hyperspectral images. The effectiveness of our proposed methods are then evaluated through a combination of simulation and real experiments. We also demonstrate an application of our method to synthetic relighting of real scenes.</p><p>4 0.68651682 <a title="5-lsi-4" href="./iccv-2013-Structured_Light_in_Sunlight.html">405 iccv-2013-Structured Light in Sunlight</a></p>
<p>Author: Mohit Gupta, Qi Yin, Shree K. Nayar</p><p>Abstract: Strong ambient illumination severely degrades the performance of structured light based techniques. This is especially true in outdoor scenarios, where the structured light sources have to compete with sunlight, whose power is often 2-5 orders of magnitude larger than the projected light. In this paper, we propose the concept of light-concentration to overcome strong ambient illumination. Our key observation is that given a fixed light (power) budget, it is always better to allocate it sequentially in several portions of the scene, as compared to spreading it over the entire scene at once. For a desired level of accuracy, we show that by distributing light appropriately, the proposed approach requires 1-2 orders lower acquisition time than existing approaches. Our approach is illumination-adaptive as the optimal light distribution is determined based on a measurement of the ambient illumination level. Since current light sources have a fixed light distribution, we have built a prototype light source that supports flexible light distribution by controlling the scanning speed of a laser scanner. We show several high quality 3D scanning results in a wide range of outdoor scenarios. The proposed approach will benefit 3D vision systems that need to operate outdoors under extreme ambient illumination levels on a limited time and power budget.</p><p>5 0.6380887 <a title="5-lsi-5" href="./iccv-2013-A_Simple_Model_for_Intrinsic_Image_Decomposition_with_Depth_Cues.html">30 iccv-2013-A Simple Model for Intrinsic Image Decomposition with Depth Cues</a></p>
<p>Author: Qifeng Chen, Vladlen Koltun</p><p>Abstract: We present a model for intrinsic decomposition of RGB-D images. Our approach analyzes a single RGB-D image and estimates albedo and shading fields that explain the input. To disambiguate the problem, our model estimates a number of components that jointly account for the reconstructed shading. By decomposing the shading field, we can build in assumptions about image formation that help distinguish reflectance variation from shading. These assumptions are expressed as simple nonlocal regularizers. We evaluate the model on real-world images and on a challenging synthetic dataset. The experimental results demonstrate that the presented approach outperforms prior models for intrinsic decomposition of RGB-D images.</p><p>6 0.59160268 <a title="5-lsi-6" href="./iccv-2013-Matching_Dry_to_Wet_Materials.html">262 iccv-2013-Matching Dry to Wet Materials</a></p>
<p>7 0.5595184 <a title="5-lsi-7" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>8 0.55247891 <a title="5-lsi-8" href="./iccv-2013-Toward_Guaranteed_Illumination_Models_for_Non-convex_Objects.html">422 iccv-2013-Toward Guaranteed Illumination Models for Non-convex Objects</a></p>
<p>9 0.49250096 <a title="5-lsi-9" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>10 0.48255616 <a title="5-lsi-10" href="./iccv-2013-Shape_Index_Descriptors_Applied_to_Texture-Based_Galaxy_Analysis.html">388 iccv-2013-Shape Index Descriptors Applied to Texture-Based Galaxy Analysis</a></p>
<p>11 0.47549453 <a title="5-lsi-11" href="./iccv-2013-Modeling_the_Calibration_Pipeline_of_the_Lytro_Camera_for_High_Quality_Light-Field_Image_Reconstruction.html">271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</a></p>
<p>12 0.45153496 <a title="5-lsi-12" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<p>13 0.43822864 <a title="5-lsi-13" href="./iccv-2013-Efficient_Image_Dehazing_with_Boundary_Constraint_and_Contextual_Regularization.html">135 iccv-2013-Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</a></p>
<p>14 0.41972214 <a title="5-lsi-14" href="./iccv-2013-Detecting_Irregular_Curvilinear_Structures_in_Gray_Scale_and_Color_Imagery_Using_Multi-directional_Oriented_Flux.html">112 iccv-2013-Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-directional Oriented Flux</a></p>
<p>15 0.40148726 <a title="5-lsi-15" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>16 0.39804146 <a title="5-lsi-16" href="./iccv-2013-High_Quality_Shape_from_a_Single_RGB-D_Image_under_Uncalibrated_Natural_Illumination.html">199 iccv-2013-High Quality Shape from a Single RGB-D Image under Uncalibrated Natural Illumination</a></p>
<p>17 0.39004907 <a title="5-lsi-17" href="./iccv-2013-Exploiting_Reflection_Change_for_Automatic_Reflection_Removal.html">151 iccv-2013-Exploiting Reflection Change for Automatic Reflection Removal</a></p>
<p>18 0.37277189 <a title="5-lsi-18" href="./iccv-2013-Decomposing_Bag_of_Words_Histograms.html">104 iccv-2013-Decomposing Bag of Words Histograms</a></p>
<p>19 0.36720613 <a title="5-lsi-19" href="./iccv-2013-Stacked_Predictive_Sparse_Coding_for_Classification_of_Distinct_Regions_in_Tumor_Histopathology.html">401 iccv-2013-Stacked Predictive Sparse Coding for Classification of Distinct Regions in Tumor Histopathology</a></p>
<p>20 0.36457047 <a title="5-lsi-20" href="./iccv-2013-The_Interestingness_of_Images.html">416 iccv-2013-The Interestingness of Images</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.049), (7, 0.023), (21, 0.282), (26, 0.088), (31, 0.045), (40, 0.018), (42, 0.079), (48, 0.063), (64, 0.032), (73, 0.036), (89, 0.169), (97, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75827587 <a title="5-lda-1" href="./iccv-2013-A_Color_Constancy_Model_with_Double-Opponency_Mechanisms.html">5 iccv-2013-A Color Constancy Model with Double-Opponency Mechanisms</a></p>
<p>Author: Shaobing Gao, Kaifu Yang, Chaoyi Li, Yongjie Li</p><p>Abstract: The double-opponent color-sensitive cells in the primary visual cortex (V1) of the human visual system (HVS) have long been recognized as the physiological basis of color constancy. We introduce a new color constancy model by imitating the functional properties of the HVS from the retina to the double-opponent cells in V1. The idea behind the model originates from the observation that the color distribution of the responses of double-opponent cells to the input color-biased images coincides well with the light source direction. Then the true illuminant color of a scene is easily estimated by searching for the maxima of the separate RGB channels of the responses of double-opponent cells in the RGB space. Our systematical experimental evaluations on two commonly used image datasets show that the proposed model can produce competitive results in comparison to the complex state-of-the-art approaches, but with a simple implementation and without the need for training.</p><p>2 0.71926963 <a title="5-lda-2" href="./iccv-2013-Person_Re-identification_by_Salience_Matching.html">313 iccv-2013-Person Re-identification by Salience Matching</a></p>
<p>Author: Rui Zhao, Wanli Ouyang, Xiaogang Wang</p><p>Abstract: Human salience is distinctive and reliable information in matching pedestrians across disjoint camera views. In this paper, we exploit the pairwise salience distribution relationship between pedestrian images, and solve the person re-identification problem by proposing a salience matching strategy. To handle the misalignment problem in pedestrian images, patch matching is adopted and patch salience is estimated. Matching patches with inconsistent salience brings penalty. Images of the same person are recognized by minimizing the salience matching cost. Furthermore, our salience matching is tightly integrated with patch matching in a unified structural RankSVM learning framework. The effectiveness of our approach is validated on the VIPeR dataset and the CUHK Campus dataset. It outperforms the state-of-the-art methods on both datasets.</p><p>3 0.68956387 <a title="5-lda-3" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>Author: Xi Li, Yao Li, Chunhua Shen, Anthony Dick, Anton Van_Den_Hengel</p><p>Abstract: Salient object detection aims to locate objects that capture human attention within images. Previous approaches often pose this as a problem of image contrast analysis. In this work, we model an image as a hypergraph that utilizes a set of hyperedges to capture the contextual properties of image pixels or regions. As a result, the problem of salient object detection becomes one of finding salient vertices and hyperedges in the hypergraph. The main advantage of hypergraph modeling is that it takes into account each pixel’s (or region ’s) affinity with its neighborhood as well as its separation from image background. Furthermore, we propose an alternative approach based on centerversus-surround contextual contrast analysis, which performs salient object detection by optimizing a cost-sensitive support vector machine (SVM) objective function. Experimental results on four challenging datasets demonstrate the effectiveness of the proposed approaches against the stateof-the-art approaches to salient object detection.</p><p>4 0.66685253 <a title="5-lda-4" href="./iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction.html">50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</a></p>
<p>Author: Ali Borji, Hamed R. Tavakoli, Dicky N. Sihite, Laurent Itti</p><p>Abstract: Significant recent progress has been made in developing high-quality saliency models. However, less effort has been undertaken on fair assessment of these models, over large standardized datasets and correctly addressing confounding factors. In this study, we pursue a critical and quantitative look at challenges (e.g., center-bias, map smoothing) in saliency modeling and the way they affect model accuracy. We quantitatively compare 32 state-of-the-art models (using the shuffled AUC score to discount center-bias) on 4 benchmark eye movement datasets, for prediction of human fixation locations and scanpath sequence. We also account for the role of map smoothing. We find that, although model rankings vary, some (e.g., AWS, LG, AIM, and HouNIPS) consistently outperform other models over all datasets. Some models work well for prediction of both fixation locations and scanpath sequence (e.g., Judd, GBVS). Our results show low prediction accuracy for models over emotional stimuli from the NUSEF dataset. Our last benchmark, for the first time, gauges the ability of models to decode the stimulus category from statistics of fixations, saccades, and model saliency values at fixated locations. In this test, ITTI and AIM models win over other models. Our benchmark provides a comprehensive high-level picture of the strengths and weaknesses of many popular models, and suggests future research directions in saliency modeling.</p><p>5 0.65499794 <a title="5-lda-5" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>Author: Silvia Zuffi, Javier Romero, Cordelia Schmid, Michael J. Black</p><p>Abstract: We address the problem of upper-body human pose estimation in uncontrolled monocular video sequences, without manual initialization. Most current methods focus on isolated video frames and often fail to correctly localize arms and hands. Inferring pose over a video sequence is advantageous because poses of people in adjacent frames exhibit properties of smooth variation due to the nature of human and camera motion. To exploit this, previous methods have used prior knowledge about distinctive actions or generic temporal priors combined with static image likelihoods to track people in motion. Here we take a different approach based on a simple observation: Information about how a person moves from frame to frame is present in the optical flow field. We develop an approach for tracking articulated motions that “links” articulated shape models of peo- ple in adjacent frames through the dense optical flow. Key to this approach is a 2D shape model of the body that we use to compute how the body moves over time. The resulting “flowing puppets ” provide a way of integrating image evidence across frames to improve pose inference. We apply our method on a challenging dataset of TV video sequences and show state-of-the-art performance.</p><p>6 0.62868881 <a title="5-lda-6" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>7 0.62124431 <a title="5-lda-7" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>8 0.62104869 <a title="5-lda-8" href="./iccv-2013-Illuminant_Chromaticity_from_Image_Sequences.html">207 iccv-2013-Illuminant Chromaticity from Image Sequences</a></p>
<p>9 0.61908877 <a title="5-lda-9" href="./iccv-2013-Bounded_Labeling_Function_for_Global_Segmentation_of_Multi-part_Objects_with_Geometric_Constraints.html">63 iccv-2013-Bounded Labeling Function for Global Segmentation of Multi-part Objects with Geometric Constraints</a></p>
<p>10 0.61816114 <a title="5-lda-10" href="./iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects.html">320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</a></p>
<p>11 0.61125815 <a title="5-lda-11" href="./iccv-2013-Joint_Deep_Learning_for_Pedestrian_Detection.html">220 iccv-2013-Joint Deep Learning for Pedestrian Detection</a></p>
<p>12 0.61008722 <a title="5-lda-12" href="./iccv-2013-Pedestrian_Parsing_via_Deep_Decompositional_Network.html">311 iccv-2013-Pedestrian Parsing via Deep Decompositional Network</a></p>
<p>13 0.60623294 <a title="5-lda-13" href="./iccv-2013-Multi-stage_Contextual_Deep_Learning_for_Pedestrian_Detection.html">279 iccv-2013-Multi-stage Contextual Deep Learning for Pedestrian Detection</a></p>
<p>14 0.60529029 <a title="5-lda-14" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>15 0.60485822 <a title="5-lda-15" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>16 0.60460538 <a title="5-lda-16" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>17 0.60333145 <a title="5-lda-17" href="./iccv-2013-Exploiting_Reflection_Change_for_Automatic_Reflection_Removal.html">151 iccv-2013-Exploiting Reflection Change for Automatic Reflection Removal</a></p>
<p>18 0.60254532 <a title="5-lda-18" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>19 0.60226977 <a title="5-lda-19" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>20 0.60126305 <a title="5-lda-20" href="./iccv-2013-Saliency_Detection_via_Dense_and_Sparse_Reconstruction.html">372 iccv-2013-Saliency Detection via Dense and Sparse Reconstruction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
