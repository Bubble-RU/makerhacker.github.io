<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>405 iccv-2013-Structured Light in Sunlight</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-405" href="#">iccv2013-405</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>405 iccv-2013-Structured Light in Sunlight</h1>
<br/><p>Source: <a title="iccv-2013-405-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Gupta_Structured_Light_in_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Mohit Gupta, Qi Yin, Shree K. Nayar</p><p>Abstract: Strong ambient illumination severely degrades the performance of structured light based techniques. This is especially true in outdoor scenarios, where the structured light sources have to compete with sunlight, whose power is often 2-5 orders of magnitude larger than the projected light. In this paper, we propose the concept of light-concentration to overcome strong ambient illumination. Our key observation is that given a fixed light (power) budget, it is always better to allocate it sequentially in several portions of the scene, as compared to spreading it over the entire scene at once. For a desired level of accuracy, we show that by distributing light appropriately, the proposed approach requires 1-2 orders lower acquisition time than existing approaches. Our approach is illumination-adaptive as the optimal light distribution is determined based on a measurement of the ambient illumination level. Since current light sources have a fixed light distribution, we have built a prototype light source that supports flexible light distribution by controlling the scanning speed of a laser scanner. We show several high quality 3D scanning results in a wide range of outdoor scenarios. The proposed approach will benefit 3D vision systems that need to operate outdoors under extreme ambient illumination levels on a limited time and power budget.</p><p>Reference: <a title="iccv-2013-405-reference" href="../iccv2013_reference/iccv-2013-Structured_Light_in_Sunlight_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Strong ambient illumination severely degrades the performance of structured light based techniques. [sent-8, score-1.122]
</p><p>2 This is especially true in outdoor scenarios, where the structured light sources have to compete with sunlight, whose power is often 2-5 orders of magnitude larger than the projected light. [sent-9, score-0.779]
</p><p>3 For a desired level of accuracy, we show that by distributing light appropriately, the proposed approach requires 1-2 orders lower acquisition time than existing approaches. [sent-12, score-0.569]
</p><p>4 Our approach is illumination-adaptive as the optimal light distribution is determined based on a measurement of the ambient illumination level. [sent-13, score-1.0]
</p><p>5 Since current light sources have a fixed light distribution, we have built a prototype light source that supports flexible light distribution by controlling the scanning speed of a laser scanner. [sent-14, score-2.092]
</p><p>6 The proposed approach will benefit 3D vision  systems that need to operate outdoors under extreme ambient illumination levels on a limited time and power budget. [sent-16, score-0.81]
</p><p>7 In many real-world settings, structured light sources have to compete with strong ambient illumination. [sent-19, score-1.057]
</p><p>8 For instance, it is known that Kinect, a popular structured light device, cannot recover 3D shape in strong sunlight [1]. [sent-22, score-0.673]
</p><p>9 While several optical techniques for ambient light reduction have been proposed [10], they achieve only moderate success. [sent-23, score-0.922]
</p><p>10 Under strong ambient illumination, the reconstruction quality of an object placed outdoors degrades, even when spectral filtering is used. [sent-25, score-0.637]
</p><p>11 One obvious solution to the ambient light problem is  ? [sent-26, score-0.902]
</p><p>12 Effect of ambient illumination on structured light 3D scanning. [sent-37, score-1.122]
</p><p>13 (a) An object placed outdoors on a clear day receives strong ambient illuminance Ra from the sun and the sky. [sent-38, score-0.94]
</p><p>14 , pico projectors) are increasingly becoming popular as structured light sources. [sent-47, score-0.574]
</p><p>15 For these low-power devices to be useful outdoors, it is important to be able to handle strong ambient illumination on a tight power budget. [sent-48, score-0.709]
</p><p>16 In this paper, we introduce the concept of light concentration in order to deal with strong ambient illumination. [sent-49, score-0.935]
</p><p>17 The key idea is that even with a small light budget, signal level can be increased by concentrating the available projector light on a small portion of the scene. [sent-50, score-1.21]
</p><p>18 At first glance, it may appear that concentrating the light will require more measurements, as only a fraction of the scene is illuminated and encoded at a time. [sent-52, score-0.624]
</p><p>19 However, we show that, it is possible to achieve significantly lower acquisition times by concentrating light as compared to existing approaches that spread the available light over the entire projector image plane, and then reduce image noise by frame-averaging. [sent-53, score-1.356]
</p><p>20 We consider different light distributions for designing structured light systems that perform under strong ambient illumination. [sent-200, score-1.481]
</p><p>21 Given a fixed light budget, as the light spread decreases (from left to right), the intensity of  each projected stripe increases. [sent-201, score-0.983]
</p><p>22 Existing structured light techniques lie at the two extremes of the power distribution scale. [sent-202, score-0.646]
</p><p>23 (Left) Systems  where light is distributed over the entire projector image plane yield low signal strength and hence poor reconstruction quality. [sent-203, score-0.75]
</p><p>24 (Right)  (Center) We show that by  concentrating the light appropriately, it is possible to achieve fast and high-quality 3D scanning even in strong ambient illumination. [sent-205, score-1.174]
</p><p>25 We show that for the same accuracy (SNR) level, while the number of measurements required by existing approaches is linear in the ambient illuminance lev? [sent-208, score-0.877]
</p><p>26 Scope and contributions: This paper introduces light redistribution as a new dimension in the design of structured light systems. [sent-219, score-1.012]
</p><p>27 We do not introduce a new structured light coding scheme. [sent-220, score-0.589]
</p><p>28 Instead, we show that by managing the light budget appropriately, it is possible to perform fast and accurate 3D scanning outdoors on a limited power budget. [sent-221, score-0.895]
</p><p>29 After determining the optimal light distribution based on the ambient illuminance level, any one of the existing highSNR structured light coding scheme [14, 4, 6] can be used. [sent-222, score-1.769]
</p><p>30 The proposed approach can adapt to the ambient light level. [sent-223, score-0.902]
</p><p>31 For instance, as ambient illuminance decreases, the acquisition time required by our approach decreases. [sent-224, score-0.874]
</p><p>32 The proposed techniques are not restricted only to ambient illumination due to sunlight. [sent-225, score-0.576]
</p><p>33 Hardware prototype and practical implications: Exist-  ing projectors distribute light over the entire image plane; they do not have the ability to distribute light in a flexible manner. [sent-227, score-1.067]
</p><p>34 We have developed a prototype projector with flexible light distribution ability by using an off-the-shelf laser scanner. [sent-228, score-0.823]
</p><p>35 Different light distributions over the projector image plane are achieved by varying the scanning speed of the scanner. [sent-229, score-0.864]
</p><p>36 These features make our approach especially suitable for moving platforms such as autonomous cars which need to operate outdoors under varying ambient illuminance levels on a limited power budget. [sent-231, score-0.957]
</p><p>37 Related Work Structured light 3D scanning: Structured light techniques are classified based on the coded patterns that they project on the scene. [sent-233, score-0.882]
</p><p>38 Significant work has been done towards designing high SNR structured light coding schemes [14, 4, 6]. [sent-236, score-0.608]
</p><p>39 It has been shown that in scenarios with extremely low SNR (such as strong ambient illumination), optimal SNR is achieved by using patterns with the fewest possible intensity levels (binary patterns with two intensity levels) [6]. [sent-237, score-0.67]
</p><p>40 In Figure 1, despite binary Gray code patterns being used, result quality degrades as ambient illumination increases. [sent-238, score-0.61]
</p><p>41 This is because  using high SNR patterns without considering light redistribution is not sufficient to achieve high-quality results under strong ambient illumination. [sent-239, score-1.011]
</p><p>42 Optical methods for suppressing ambient illumination: Examples of such methods include using a narrow spectral bandwidth laser (sunlight has broad bandwidth) with a narrow-band spectral filter [10] and a polarized light source (sunlight is unpolarized) with a polarization filter [10]. [sent-240, score-1.154]
</p><p>43 Given a fixed level of ambient illuminance (after optical suppression), we 546  determine the optimal distribution of the light (of the structured light source) in order to maximize the SNR. [sent-242, score-1.746]
</p><p>44 In order to deal with extreme ambient illumination scenarios, optical suppression techniques can be used in a complementary manner to our method. [sent-244, score-0.629]
</p><p>45 Recently, a pulsed light source with a fast shutter [8] was used to suppress ambient illumination. [sent-245, score-0.979]
</p><p>46 Given the same power budget, our method, by distributing the available light efficiently, requires 10-100 times fewer images in most outdoor scenarios. [sent-250, score-0.617]
</p><p>47 Structured Light In Ambient Illumination We model the structured light source L as a projector that has an image plane with C columns. [sent-252, score-0.903]
</p><p>48 The power of the light source is fixed at P Watts. [sent-254, score-0.601]
</p><p>49 The intensity of a scene point S in a captured image is: I Il + Ia + η, = (1) where Il and Ia are intensities corresponding to the light source L and ambient illumination A, respectively. [sent-257, score-1.146]
</p><p>50 In all our experiments, we used a narrow-band laser light source and spectral filter in front of our camera. [sent-273, score-0.633]
</p><p>51 This suppresses ambient illumination by a factor of about 20. [sent-274, score-0.595]
</p><p>52 strong ambient illumination, Ra >> Rl, and the dominant source of noise is the signal-dependent photon noise, i. [sent-275, score-0.588]
</p><p>53 (6)  Let NC be the number of images required by the particular structured light coding scheme used to encode all the projector columns uniquely, and f, as defined above, is the number of frames to be averaged per image. [sent-303, score-0.903]
</p><p>54 6 and 7, we arrive at the following result: Result 1(Acquisition time for frame averaging) Given a fixed power budget P, the number of measurements M (and hence the acquisition time) using frame-averaging is linear in the ambient illuminance level Ra, i. [sent-306, score-1.181]
</p><p>55 Thus, while frame-averaging can be an effective method for increasing SNR in weak ambient illumination (e. [sent-309, score-0.576]
</p><p>56 , indoors), the acquisition time is prohibitively large for out-  door ambient illumination levels that are 102 − 103 times the typical indoor illumination. [sent-311, score-0.706]
</p><p>57 In view of this tradeoffbetween desired accuracy and acquisition time, we ask the following question: Is it possible 3The threshold τ depends on the structured light coding and decoding algorithms. [sent-312, score-0.675]
</p><p>58 547  to achieve high depth accuracy while also requiring a small number of measurements, even in extremely strong ambient light conditions and with a limited power budget? [sent-319, score-1.035]
</p><p>59 The total light budget remains the same - it is just concentrated into a smaller region. [sent-324, score-0.658]
</p><p>60 Suppose all the available light is concentrated into a single block at a time, and each block is encoded independently. [sent-327, score-0.717]
</p><p>61 We call this the  concentrate-and-scan strategy, as light is concentrated in a selected region of the scene, and then the illuminated region is scanned over the entire scene. [sent-328, score-0.584]
</p><p>62 The averaging strategy defined in the previous section is called spread-and-average, as it includes spreading all the light over the entire projector image plane, and then averaging frames. [sent-329, score-0.721]
</p><p>63 As we show later, as a consequence of the LCA, concentrateand-scan requires a much lower acquisition time (1-2 orders of magnitude smaller), as compared to spread-and-average in extreme ambient illumination conditions. [sent-335, score-0.755]
</p><p>64 Concentrate-and-scan structured light  Suppose we could concentrate all the light into any block of size K columns, where K (1 ≤ K ≤ C) could be chosen oafrbsiitzrear Kily. [sent-339, score-1.064]
</p><p>65 c Tluhmenn,s g,iwvhener ae Kfix (e1d ≤ bl Kock ≤ ≤si Cze) Kco,u cldobnececnhtorasteenand-scan structured light consists of dividing the projector image plane into ? [sent-340, score-0.826]
</p><p>66 To fhe Kn, for each block Bi, only the columns within Bi are encoded (using any existing coding scheme) while spreading light only within that block. [sent-351, score-0.657]
</p><p>67 This step is repeated sequentially for all the blocks by concentrating light in a single block at a time. [sent-352, score-0.638]
</p><p>68 (a) Variation of Kopt with ambient illuminance level, for different light source powers P (resulting in illuminance of 25, 50 and 100 lux, respectively at a normally facing scene point 1meter away). [sent-402, score-1.612]
</p><p>69 Let Rl be the source illuminance when light is spread over the entire image plane. [sent-553, score-0.836]
</p><p>70 Then, the illuminance when light is concentrated into K columns is RlCK. [sent-554, score-0.828]
</p><p>71 This ensures that the available light is concentrated into a smaller region so that the decodability condition is satisfied. [sent-561, score-0.691]
</p><p>72 The three sources correspond approximately to a small laser pointer, a desktop laser scanner and a pico projector (resulting in illuminance of 25 lux, 50 lux and 100 lux respectively at a normally facing scene  ×  point 1meter away). [sent-563, score-1.131]
</p><p>73 0 was calculated assuming binary structured light coding 5, and the accuracy level is 0. [sent-568, score-0.589]
</p><p>74 Figure 4 (a) shows the number of measurements required by the concentrateand-scan and spread-and-average techniques for a wide range of ambient illumination levels. [sent-589, score-0.697]
</p><p>75 5Similar analysis can be performed for other structured light schemes such as phase-shifting [13] and N-ary coding [6]. [sent-593, score-0.608]
</p><p>76 We also plot the number of images required for single linestriping, where all the light is concentrated into a single column (as illustrated in Figure 2 (right)). [sent-635, score-0.593]
</p><p>77 The scan-only technique requires Ms = C images, irrespective of the ambient illumination levels. [sent-637, score-0.576]
</p><p>78 Implications (from Figure 4 (a)): For typical low power projectors, the concentrate-and-scan approach requires 1-2 orders of magnitude (10-100 times) lower acquisition time than the existing schemes, for all outdoor ambient illuminance levels (Ra > 104). [sent-638, score-1.069]
</p><p>79 Again, the number of required images is relatively small even for the most extreme cases (direct sunlight, low-powered light source and large dss). [sent-641, score-0.566]
</p><p>80 Hardware Prototype In order to implement the concentrate-and-scan approach, we need a projector whose light could be distributed programmatically into any contiguous subset of K columns on the image plane. [sent-643, score-0.706]
</p><p>81 ×  (a) Objects placed outdoors in two different ambient illumination conditions  - 9am on a  cloudy day (top row) and 1pm on a bright sunny day (bottom row). [sent-655, score-0.774]
</p><p>82 The key observation is that it is possible to implement different light distributions by changing the speed of the scanning mechanism (in our case, the rotation velocity of the polygonal mirror) Let the total power of the source be P. [sent-674, score-0.801]
</p><p>83 Concentrate-and-scan structured light implementation: Let the optimal block size be Kopt; the image plane is diuTrhee  6Different laser scanning speeds have been used for generating different camera exposures in a structured light setup [7]. [sent-682, score-1.497]
</p><p>84 Results Figure 6 shows 3D scanning results for objects placed outdoors under different ambient illuminance levels. [sent-699, score-0.999]
</p><p>85 3D scanning results for two outdoor scenes with strong ambient light. [sent-707, score-0.698]
</p><p>86 Binary Gray code patterns were used as the structured light encoding scheme. [sent-710, score-0.58]
</p><p>87 Structured Light in the Wild: Figure 7 shows 3D scanning results for two outdoor scenes with strong ambient light (90,000 and 22,000 lux). [sent-720, score-1.122]
</p><p>88 As the day progresses, ambient illuminance increases, and the number of measurements increases accordingly (10, 18, 18, 32 and 56). [sent-729, score-0.926]
</p><p>89 As ambient illuminance increases, the result quality of the spread-and-average scheme deteriorates. [sent-734, score-0.756]
</p><p>90 Discussion Contributions: This paper proposes light distribution as a new dimension in the design of structured light systems. [sent-737, score-0.97]
</p><p>91 Limitations: Our approach assumes that the power of the light source, when completely concentrated into a single line, is sufficient for the decodability condition to be satisfied. [sent-739, score-0.791]
</p><p>92 While this is true in most settings even for a lowpower light source, for parts of a highly specular objects, the image component due to ambient illumination may be too strong. [sent-740, score-1.028]
</p><p>93 In this case, even concentrating all the light into a single column fails to overcome ambient illumination. [sent-741, score-1.036]
</p><p>94 For low ambient illuminance (left), both concentrate-and-scan and spread-and-average methods produce good results. [sent-765, score-0.756]
</p><p>95 As the day progresses, concentrate-and-scan method adapts to the ambient illuminance  the  level (increasing from left to right) by choosing the appropriate block size, and achieves results of much higher quality. [sent-766, score-0.917]
</p><p>96 A lowpower structured light sensor for outdoor scene reconstruction and dominant material identification. [sent-823, score-0.649]
</p><p>97 (b) Inside the highlight, even concentrating all the projector light into a single column fails to overcome ambient illumination, resulting in a large hole in the reconstructed shape. [sent-840, score-1.276]
</p><p>98 A state of  the art in structured light patterns for surface profilometry. [sent-853, score-0.58]
</p><p>99 Maximum snr pattern strategy for phase shifting methods in structured light illumination. [sent-867, score-0.835]
</p><p>100 Rapid shape acquisition using color structured light and multi-pass dynamic programming. [sent-873, score-0.632]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ambient', 0.478), ('light', 0.424), ('snr', 0.289), ('illuminance', 0.278), ('projector', 0.24), ('kopt', 0.198), ('decodability', 0.156), ('budget', 0.15), ('scanning', 0.139), ('lux', 0.138), ('structured', 0.122), ('laser', 0.11), ('concentrating', 0.1), ('power', 0.1), ('projectors', 0.098), ('illumination', 0.098), ('ra', 0.094), ('block', 0.094), ('sunlight', 0.094), ('measurements', 0.089), ('acquisition', 0.086), ('concentrated', 0.084), ('outdoors', 0.082), ('source', 0.077), ('kcopt', 0.071), ('mcs', 0.071), ('scanner', 0.062), ('rl', 0.058), ('illuminated', 0.052), ('powers', 0.05), ('outdoor', 0.048), ('day', 0.047), ('nk', 0.046), ('coding', 0.043), ('redistribution', 0.042), ('columns', 0.042), ('polygonal', 0.04), ('plane', 0.04), ('orders', 0.039), ('hardware', 0.036), ('patterns', 0.034), ('column', 0.034), ('increases', 0.034), ('extreme', 0.033), ('strong', 0.033), ('spreading', 0.033), ('il', 0.033), ('spread', 0.033), ('mirror', 0.032), ('columbia', 0.032), ('required', 0.032), ('progresses', 0.029), ('prototype', 0.029), ('scenarios', 0.028), ('lowpower', 0.028), ('pico', 0.028), ('radiated', 0.028), ('rrla', 0.028), ('snrav', 0.028), ('ticat', 0.028), ('stripe', 0.028), ('tij', 0.028), ('beam', 0.028), ('decreases', 0.027), ('scene', 0.027), ('ia', 0.027), ('condition', 0.027), ('lca', 0.025), ('projected', 0.025), ('times', 0.025), ('holes', 0.025), ('kc', 0.025), ('entire', 0.024), ('distribute', 0.024), ('msa', 0.023), ('nayar', 0.022), ('placed', 0.022), ('spectral', 0.022), ('camera', 0.022), ('intensity', 0.022), ('signal', 0.022), ('appropriately', 0.022), ('speed', 0.021), ('iij', 0.021), ('lumbi', 0.021), ('polarization', 0.021), ('encoded', 0.021), ('rotating', 0.021), ('magnitude', 0.021), ('captured', 0.02), ('flexible', 0.02), ('achieves', 0.02), ('optical', 0.02), ('distributing', 0.02), ('blocks', 0.02), ('proportional', 0.02), ('schemes', 0.019), ('factor', 0.019), ('levels', 0.019), ('illustrated', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="405-tfidf-1" href="./iccv-2013-Structured_Light_in_Sunlight.html">405 iccv-2013-Structured Light in Sunlight</a></p>
<p>Author: Mohit Gupta, Qi Yin, Shree K. Nayar</p><p>Abstract: Strong ambient illumination severely degrades the performance of structured light based techniques. This is especially true in outdoor scenarios, where the structured light sources have to compete with sunlight, whose power is often 2-5 orders of magnitude larger than the projected light. In this paper, we propose the concept of light-concentration to overcome strong ambient illumination. Our key observation is that given a fixed light (power) budget, it is always better to allocate it sequentially in several portions of the scene, as compared to spreading it over the entire scene at once. For a desired level of accuracy, we show that by distributing light appropriately, the proposed approach requires 1-2 orders lower acquisition time than existing approaches. Our approach is illumination-adaptive as the optimal light distribution is determined based on a measurement of the ambient illumination level. Since current light sources have a fixed light distribution, we have built a prototype light source that supports flexible light distribution by controlling the scanning speed of a laser scanner. We show several high quality 3D scanning results in a wide range of outdoor scenarios. The proposed approach will benefit 3D vision systems that need to operate outdoors under extreme ambient illumination levels on a limited time and power budget.</p><p>2 0.22365488 <a title="405-tfidf-2" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>Author: Supreeth Achar, Stephen T. Nuske, Srinivasa G. Narasimhan</p><p>Abstract: Separating the direct and global components of radiance can aid shape recovery algorithms and can provide useful information about materials in a scene. Practical methods for finding the direct and global components use multiple images captured under varying illumination patterns and require the scene, light source and camera to remain stationary during the image acquisition process. In this paper, we develop a motion compensation method that relaxes this condition and allows direct-global separation to beperformed on video sequences of dynamic scenes captured by moving projector-camera systems. Key to our method is being able to register frames in a video sequence to each other in the presence of time varying, high frequency active illumination patterns. We compare our motion compensated method to alternatives such as single shot separation and frame interleaving as well as ground truth. We present results on challenging video sequences that include various types of motions and deformations in scenes that contain complex materials like fabric, skin, leaves and wax.</p><p>3 0.17512536 <a title="405-tfidf-3" href="./iccv-2013-Toward_Guaranteed_Illumination_Models_for_Non-convex_Objects.html">422 iccv-2013-Toward Guaranteed Illumination Models for Non-convex Objects</a></p>
<p>Author: Yuqian Zhang, Cun Mu, Han-Wen Kuo, John Wright</p><p>Abstract: Illumination variation remains a central challenge in object detection and recognition. Existing analyses of illumination variation typically pertain to convex, Lambertian objects, and guarantee quality of approximation in an average case sense. We show that it is possible to build models for the set of images across illumination variation with worstcase performance guarantees, for nonconvex Lambertian objects. Namely, a natural verification test based on the distance to the model guarantees to accept any image which can be sufficiently well-approximated by an image of the object under some admissible lighting condition, and guarantees to reject any image that does not have a sufficiently good approximation. These models are generated by sampling illumination directions with sufficient density, which follows from a new perturbation bound for directional illuminated images in the Lambertian model. As the number of such images required for guaranteed verification may be large, we introduce a new formulation for cone preserving dimensionality reduction, which leverages tools from sparse and low-rank decomposition to reduce the complexity, while controlling the approximation error with respect to the original model. 1</p><p>4 0.15953961 <a title="405-tfidf-4" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<p>Author: Nicolas Martin, Vincent Couture, Sébastien Roy</p><p>Abstract: We present a scanning method that recovers dense subpixel camera-projector correspondence without requiring any photometric calibration nor preliminary knowledge of their relative geometry. Subpixel accuracy is achieved by considering several zero-crossings defined by the difference between pairs of unstructured patterns. We use gray-level band-pass white noise patterns that increase robustness to indirect lighting and scene discontinuities. Simulated and experimental results show that our method recovers scene geometry with high subpixel precision, and that it can handle many challenges of active reconstruction systems. We compare our results to state of the art methods such as micro phase shifting and modulated phase shifting.</p><p>5 0.1270688 <a title="405-tfidf-5" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>Author: Michael Weinmann, Aljosa Osep, Roland Ruiters, Reinhard Klein</p><p>Abstract: In this paper, we present a novel, robust multi-view normal field integration technique for reconstructing the full 3D shape of mirroring objects. We employ a turntablebased setup with several cameras and displays. These are used to display illumination patterns which are reflected by the object surface. The pattern information observed in the cameras enables the calculation of individual volumetric normal fields for each combination of camera, display and turntable angle. As the pattern information might be blurred depending on the surface curvature or due to nonperfect mirroring surface characteristics, we locally adapt the decoding to the finest still resolvable pattern resolution. In complex real-world scenarios, the normal fields contain regions without observations due to occlusions and outliers due to interreflections and noise. Therefore, a robust reconstruction using only normal information is challenging. Via a non-parametric clustering of normal hypotheses derived for each point in the scene, we obtain both the most likely local surface normal and a local surface consistency estimate. This information is utilized in an iterative mincut based variational approach to reconstruct the surface geometry.</p><p>6 0.12444385 <a title="405-tfidf-6" href="./iccv-2013-Modeling_the_Calibration_Pipeline_of_the_Lytro_Camera_for_High_Quality_Light-Field_Image_Reconstruction.html">271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</a></p>
<p>7 0.089294754 <a title="405-tfidf-7" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<p>8 0.084366053 <a title="405-tfidf-8" href="./iccv-2013-Illuminant_Chromaticity_from_Image_Sequences.html">207 iccv-2013-Illuminant Chromaticity from Image Sequences</a></p>
<p>9 0.076365218 <a title="405-tfidf-9" href="./iccv-2013-A_Joint_Intensity_and_Depth_Co-sparse_Analysis_Model_for_Depth_Map_Super-resolution.html">18 iccv-2013-A Joint Intensity and Depth Co-sparse Analysis Model for Depth Map Super-resolution</a></p>
<p>10 0.071031153 <a title="405-tfidf-10" href="./iccv-2013-High_Quality_Shape_from_a_Single_RGB-D_Image_under_Uncalibrated_Natural_Illumination.html">199 iccv-2013-High Quality Shape from a Single RGB-D Image under Uncalibrated Natural Illumination</a></p>
<p>11 0.067792132 <a title="405-tfidf-11" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>12 0.063466445 <a title="405-tfidf-12" href="./iccv-2013-Curvature-Aware_Regularization_on_Riemannian_Submanifolds.html">100 iccv-2013-Curvature-Aware Regularization on Riemannian Submanifolds</a></p>
<p>13 0.063141592 <a title="405-tfidf-13" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>14 0.062727563 <a title="405-tfidf-14" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>15 0.057793088 <a title="405-tfidf-15" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>16 0.055747215 <a title="405-tfidf-16" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>17 0.053798195 <a title="405-tfidf-17" href="./iccv-2013-Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illumination_in_the_Spectral_Domain.html">385 iccv-2013-Separating Reflective and Fluorescent Components Using High Frequency Illumination in the Spectral Domain</a></p>
<p>18 0.051764369 <a title="405-tfidf-18" href="./iccv-2013-Image_Guided_Depth_Upsampling_Using_Anisotropic_Total_Generalized_Variation.html">209 iccv-2013-Image Guided Depth Upsampling Using Anisotropic Total Generalized Variation</a></p>
<p>19 0.051531866 <a title="405-tfidf-19" href="./iccv-2013-A_Color_Constancy_Model_with_Double-Opponency_Mechanisms.html">5 iccv-2013-A Color Constancy Model with Double-Opponency Mechanisms</a></p>
<p>20 0.050094187 <a title="405-tfidf-20" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.107), (1, -0.076), (2, -0.036), (3, 0.003), (4, -0.038), (5, 0.009), (6, 0.01), (7, -0.076), (8, 0.007), (9, -0.02), (10, -0.003), (11, -0.015), (12, -0.002), (13, 0.012), (14, -0.005), (15, -0.074), (16, -0.057), (17, 0.014), (18, -0.013), (19, 0.024), (20, 0.007), (21, -0.008), (22, -0.015), (23, -0.05), (24, -0.138), (25, 0.058), (26, -0.02), (27, -0.036), (28, 0.105), (29, -0.14), (30, 0.127), (31, 0.052), (32, 0.071), (33, 0.035), (34, 0.031), (35, 0.096), (36, -0.022), (37, -0.077), (38, -0.141), (39, 0.097), (40, 0.085), (41, 0.043), (42, 0.108), (43, 0.037), (44, -0.002), (45, -0.087), (46, 0.029), (47, 0.141), (48, -0.032), (49, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97357589 <a title="405-lsi-1" href="./iccv-2013-Structured_Light_in_Sunlight.html">405 iccv-2013-Structured Light in Sunlight</a></p>
<p>Author: Mohit Gupta, Qi Yin, Shree K. Nayar</p><p>Abstract: Strong ambient illumination severely degrades the performance of structured light based techniques. This is especially true in outdoor scenarios, where the structured light sources have to compete with sunlight, whose power is often 2-5 orders of magnitude larger than the projected light. In this paper, we propose the concept of light-concentration to overcome strong ambient illumination. Our key observation is that given a fixed light (power) budget, it is always better to allocate it sequentially in several portions of the scene, as compared to spreading it over the entire scene at once. For a desired level of accuracy, we show that by distributing light appropriately, the proposed approach requires 1-2 orders lower acquisition time than existing approaches. Our approach is illumination-adaptive as the optimal light distribution is determined based on a measurement of the ambient illumination level. Since current light sources have a fixed light distribution, we have built a prototype light source that supports flexible light distribution by controlling the scanning speed of a laser scanner. We show several high quality 3D scanning results in a wide range of outdoor scenarios. The proposed approach will benefit 3D vision systems that need to operate outdoors under extreme ambient illumination levels on a limited time and power budget.</p><p>2 0.83270085 <a title="405-lsi-2" href="./iccv-2013-Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illumination_in_the_Spectral_Domain.html">385 iccv-2013-Separating Reflective and Fluorescent Components Using High Frequency Illumination in the Spectral Domain</a></p>
<p>Author: Ying Fu, Antony Lam, Imari Sato, Takahiro Okabe, Yoichi Sato</p><p>Abstract: Hyperspectral imaging is beneficial to many applications but current methods do not consider fluorescent effects which are present in everyday items ranging from paper, to clothing, to even our food. Furthermore, everyday fluorescent items exhibit a mix of reflectance and fluorescence. So proper separation of these components is necessary for analyzing them. In this paper, we demonstrate efficient separation and recovery of reflective and fluorescent emission spectra through the use of high frequency illumination in the spectral domain. With the obtained fluorescent emission spectra from our high frequency illuminants, we then present to our knowledge, the first method for estimating the fluorescent absorption spectrum of a material given its emission spectrum. Conventional bispectral measurement of absorption and emission spectra needs to examine all combinations of incident and observed light wavelengths. In contrast, our method requires only two hyperspectral images. The effectiveness of our proposed methods are then evaluated through a combination of simulation and real experiments. We also demonstrate an application of our method to synthetic relighting of real scenes.</p><p>3 0.79232967 <a title="405-lsi-3" href="./iccv-2013-Illuminant_Chromaticity_from_Image_Sequences.html">207 iccv-2013-Illuminant Chromaticity from Image Sequences</a></p>
<p>Author: Veronique Prinet, Dani Lischinski, Michael Werman</p><p>Abstract: We estimate illuminant chromaticity from temporal sequences, for scenes illuminated by either one or two dominant illuminants. While there are many methods for illuminant estimation from a single image, few works so far have focused on videos, and even fewer on multiple light sources. Our aim is to leverage information provided by the temporal acquisition, where either the objects or the camera or the light source are/is in motion in order to estimate illuminant color without the need for user interaction or using strong assumptions and heuristics. We introduce a simple physically-based formulation based on the assumption that the incident light chromaticity is constant over a short space-time domain. We show that a deterministic approach is not sufficient for accurate and robust estimation: however, a probabilistic formulation makes it possible to implicitly integrate away hidden factors that have been ignored by the physical model. Experimental results are reported on a dataset of natural video sequences and on the GrayBall benchmark, indicating that we compare favorably with the state-of-the-art.</p><p>4 0.75785232 <a title="405-lsi-4" href="./iccv-2013-Modeling_the_Calibration_Pipeline_of_the_Lytro_Camera_for_High_Quality_Light-Field_Image_Reconstruction.html">271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</a></p>
<p>Author: Donghyeon Cho, Minhaeng Lee, Sunyeong Kim, Yu-Wing Tai</p><p>Abstract: Light-field imaging systems have got much attention recently as the next generation camera model. A light-field imaging system consists of three parts: data acquisition, manipulation, and application. Given an acquisition system, it is important to understand how a light-field camera converts from its raw image to its resulting refocused image. In this paper, using the Lytro camera as an example, we describe step-by-step procedures to calibrate a raw light-field image. In particular, we are interested in knowing the spatial and angular coordinates of the micro lens array and the resampling process for image reconstruction. Since Lytro uses a hexagonal arrangement of a micro lens image, additional treatments in calibration are required. After calibration, we analyze and compare the performances of several resampling methods for image reconstruction with and without calibration. Finally, a learning based interpolation method is proposed which demonstrates a higher quality image reconstruction than previous interpolation methods including a method used in Lytro software.</p><p>5 0.70709091 <a title="405-lsi-5" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<p>Author: Nicolas Martin, Vincent Couture, Sébastien Roy</p><p>Abstract: We present a scanning method that recovers dense subpixel camera-projector correspondence without requiring any photometric calibration nor preliminary knowledge of their relative geometry. Subpixel accuracy is achieved by considering several zero-crossings defined by the difference between pairs of unstructured patterns. We use gray-level band-pass white noise patterns that increase robustness to indirect lighting and scene discontinuities. Simulated and experimental results show that our method recovers scene geometry with high subpixel precision, and that it can handle many challenges of active reconstruction systems. We compare our results to state of the art methods such as micro phase shifting and modulated phase shifting.</p><p>6 0.69554478 <a title="405-lsi-6" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>7 0.68495077 <a title="405-lsi-7" href="./iccv-2013-A_Color_Constancy_Model_with_Double-Opponency_Mechanisms.html">5 iccv-2013-A Color Constancy Model with Double-Opponency Mechanisms</a></p>
<p>8 0.64702928 <a title="405-lsi-8" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<p>9 0.63586849 <a title="405-lsi-9" href="./iccv-2013-A_Simple_Model_for_Intrinsic_Image_Decomposition_with_Depth_Cues.html">30 iccv-2013-A Simple Model for Intrinsic Image Decomposition with Depth Cues</a></p>
<p>10 0.59482414 <a title="405-lsi-10" href="./iccv-2013-Toward_Guaranteed_Illumination_Models_for_Non-convex_Objects.html">422 iccv-2013-Toward Guaranteed Illumination Models for Non-convex Objects</a></p>
<p>11 0.58827084 <a title="405-lsi-11" href="./iccv-2013-Matching_Dry_to_Wet_Materials.html">262 iccv-2013-Matching Dry to Wet Materials</a></p>
<p>12 0.50122207 <a title="405-lsi-12" href="./iccv-2013-High_Quality_Shape_from_a_Single_RGB-D_Image_under_Uncalibrated_Natural_Illumination.html">199 iccv-2013-High Quality Shape from a Single RGB-D Image under Uncalibrated Natural Illumination</a></p>
<p>13 0.48611799 <a title="405-lsi-13" href="./iccv-2013-Efficient_Image_Dehazing_with_Boundary_Constraint_and_Contextual_Regularization.html">135 iccv-2013-Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</a></p>
<p>14 0.43116519 <a title="405-lsi-14" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>15 0.43021443 <a title="405-lsi-15" href="./iccv-2013-Target-Driven_Moire_Pattern_Synthesis_by_Phase_Modulation.html">413 iccv-2013-Target-Driven Moire Pattern Synthesis by Phase Modulation</a></p>
<p>16 0.39631563 <a title="405-lsi-16" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>17 0.38574249 <a title="405-lsi-17" href="./iccv-2013-Exploiting_Reflection_Change_for_Automatic_Reflection_Removal.html">151 iccv-2013-Exploiting Reflection Change for Automatic Reflection Removal</a></p>
<p>18 0.38323632 <a title="405-lsi-18" href="./iccv-2013-Fibonacci_Exposure_Bracketing_for_High_Dynamic_Range_Imaging.html">164 iccv-2013-Fibonacci Exposure Bracketing for High Dynamic Range Imaging</a></p>
<p>19 0.37983218 <a title="405-lsi-19" href="./iccv-2013-Fluttering_Pattern_Generation_Using_Modified_Legendre_Sequence_for_Coded_Exposure_Imaging.html">173 iccv-2013-Fluttering Pattern Generation Using Modified Legendre Sequence for Coded Exposure Imaging</a></p>
<p>20 0.36726338 <a title="405-lsi-20" href="./iccv-2013-Estimating_the_Material_Properties_of_Fabric_from_Video.html">145 iccv-2013-Estimating the Material Properties of Fabric from Video</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.034), (12, 0.011), (26, 0.542), (31, 0.026), (40, 0.01), (42, 0.066), (64, 0.034), (73, 0.021), (74, 0.01), (89, 0.124)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92987382 <a title="405-lda-1" href="./iccv-2013-Structured_Light_in_Sunlight.html">405 iccv-2013-Structured Light in Sunlight</a></p>
<p>Author: Mohit Gupta, Qi Yin, Shree K. Nayar</p><p>Abstract: Strong ambient illumination severely degrades the performance of structured light based techniques. This is especially true in outdoor scenarios, where the structured light sources have to compete with sunlight, whose power is often 2-5 orders of magnitude larger than the projected light. In this paper, we propose the concept of light-concentration to overcome strong ambient illumination. Our key observation is that given a fixed light (power) budget, it is always better to allocate it sequentially in several portions of the scene, as compared to spreading it over the entire scene at once. For a desired level of accuracy, we show that by distributing light appropriately, the proposed approach requires 1-2 orders lower acquisition time than existing approaches. Our approach is illumination-adaptive as the optimal light distribution is determined based on a measurement of the ambient illumination level. Since current light sources have a fixed light distribution, we have built a prototype light source that supports flexible light distribution by controlling the scanning speed of a laser scanner. We show several high quality 3D scanning results in a wide range of outdoor scenarios. The proposed approach will benefit 3D vision systems that need to operate outdoors under extreme ambient illumination levels on a limited time and power budget.</p><p>2 0.88677651 <a title="405-lda-2" href="./iccv-2013-Slice_Sampling_Particle_Belief_Propagation.html">395 iccv-2013-Slice Sampling Particle Belief Propagation</a></p>
<p>Author: Oliver Müller, Michael Ying Yang, Bodo Rosenhahn</p><p>Abstract: Inference in continuous label Markov random fields is a challenging task. We use particle belief propagation (PBP) for solving the inference problem in continuous label space. Sampling particles from the belief distribution is typically done by using Metropolis-Hastings (MH) Markov chain Monte Carlo (MCMC) methods which involves sampling from a proposal distribution. This proposal distribution has to be carefully designed depending on the particular model and input data to achieve fast convergence. We propose to avoid dependence on a proposal distribution by introducing a slice sampling based PBP algorithm. The proposed approach shows superior convergence performance on an image denoising toy example. Our findings are validated on a challenging relational 2D feature tracking application.</p><p>3 0.88653696 <a title="405-lda-3" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>Author: Radu Timofte, Vincent De_Smet, Luc Van_Gool</p><p>Abstract: Recently there have been significant advances in image upscaling or image super-resolution based on a dictionary of low and high resolution exemplars. The running time of the methods is often ignored despite the fact that it is a critical factor for real applications. This paper proposes fast super-resolution methods while making no compromise on quality. First, we support the use of sparse learned dictionaries in combination with neighbor embedding methods. In this case, the nearest neighbors are computed using the correlation with the dictionary atoms rather than the Euclidean distance. Moreover, we show that most of the current approaches reach top performance for the right parameters. Second, we show that using global collaborative coding has considerable speed advantages, reducing the super-resolution mapping to a precomputed projective matrix. Third, we propose the anchored neighborhood regression. That is to anchor the neighborhood embedding of a low resolution patch to the nearest atom in the dictionary and to precompute the corresponding embedding matrix. These proposals are contrasted with current state-of- the-art methods on standard images. We obtain similar or improved quality and one or two orders of magnitude speed improvements.</p><p>4 0.86900049 <a title="405-lda-4" href="./iccv-2013-Drosophila_Embryo_Stage_Annotation_Using_Label_Propagation.html">125 iccv-2013-Drosophila Embryo Stage Annotation Using Label Propagation</a></p>
<p>Author: Tomáš Kazmar, Evgeny Z. Kvon, Alexander Stark, Christoph H. Lampert</p><p>Abstract: In this work we propose a system for automatic classification of Drosophila embryos into developmental stages. While the system is designed to solve an actual problem in biological research, we believe that the principle underlying it is interesting not only for biologists, but also for researchers in computer vision. The main idea is to combine two orthogonal sources of information: one is a classifier trained on strongly invariant features, which makes it applicable to images of very different conditions, but also leads to rather noisy predictions. The other is a label propagation step based on a more powerful similarity measure that however is only consistent within specific subsets of the data at a time. In our biological setup, the information sources are the shape and the staining patterns of embryo images. We show experimentally that while neither of the methods can be used by itself to achieve satisfactory results, their combination achieves prediction quality comparable to human per- formance.</p><p>5 0.85300237 <a title="405-lda-5" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>Author: Abdelaziz Djelouah, Jean-Sébastien Franco, Edmond Boyer, François Le_Clerc, Patrick Pérez</p><p>Abstract: In this paper, we address the problem of object segmentation in multiple views or videos when two or more viewpoints of the same scene are available. We propose a new approach that propagates segmentation coherence information in both space and time, hence allowing evidences in one image to be shared over the complete set. To this aim the segmentation is cast as a single efficient labeling problem over space and time with graph cuts. In contrast to most existing multi-view segmentation methods that rely on some form of dense reconstruction, ours only requires a sparse 3D sampling to propagate information between viewpoints. The approach is thoroughly evaluated on standard multiview datasets, as well as on videos. With static views, results compete with state of the art methods but they are achieved with significantly fewer viewpoints. With multiple videos, we report results that demonstrate the benefit of segmentation propagation through temporal cues.</p><p>6 0.85078311 <a title="405-lda-6" href="./iccv-2013-Refractive_Structure-from-Motion_on_Underwater_Images.html">348 iccv-2013-Refractive Structure-from-Motion on Underwater Images</a></p>
<p>7 0.85040367 <a title="405-lda-7" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>8 0.77977026 <a title="405-lda-8" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>9 0.75409847 <a title="405-lda-9" href="./iccv-2013-Data-Driven_3D_Primitives_for_Single_Image_Understanding.html">102 iccv-2013-Data-Driven 3D Primitives for Single Image Understanding</a></p>
<p>10 0.75383162 <a title="405-lda-10" href="./iccv-2013-A_Deformable_Mixture_Parsing_Model_with_Parselets.html">8 iccv-2013-A Deformable Mixture Parsing Model with Parselets</a></p>
<p>11 0.65766895 <a title="405-lda-11" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>12 0.65707576 <a title="405-lda-12" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>13 0.63270843 <a title="405-lda-13" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>14 0.61416817 <a title="405-lda-14" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>15 0.6135509 <a title="405-lda-15" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>16 0.60324407 <a title="405-lda-16" href="./iccv-2013-Uncertainty-Driven_Efficiently-Sampled_Sparse_Graphical_Models_for_Concurrent_Tumor_Segmentation_and_Atlas_Registration.html">432 iccv-2013-Uncertainty-Driven Efficiently-Sampled Sparse Graphical Models for Concurrent Tumor Segmentation and Atlas Registration</a></p>
<p>17 0.59925395 <a title="405-lda-17" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>18 0.59771734 <a title="405-lda-18" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<p>19 0.59677196 <a title="405-lda-19" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>20 0.58933097 <a title="405-lda-20" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
