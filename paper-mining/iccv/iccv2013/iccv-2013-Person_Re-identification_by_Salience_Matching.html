<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>313 iccv-2013-Person Re-identification by Salience Matching</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-313" href="#">iccv2013-313</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>313 iccv-2013-Person Re-identification by Salience Matching</h1>
<br/><p>Source: <a title="iccv-2013-313-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Zhao_Person_Re-identification_by_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Rui Zhao, Wanli Ouyang, Xiaogang Wang</p><p>Abstract: Human salience is distinctive and reliable information in matching pedestrians across disjoint camera views. In this paper, we exploit the pairwise salience distribution relationship between pedestrian images, and solve the person re-identification problem by proposing a salience matching strategy. To handle the misalignment problem in pedestrian images, patch matching is adopted and patch salience is estimated. Matching patches with inconsistent salience brings penalty. Images of the same person are recognized by minimizing the salience matching cost. Furthermore, our salience matching is tightly integrated with patch matching in a unified structural RankSVM learning framework. The effectiveness of our approach is validated on the VIPeR dataset and the CUHK Campus dataset. It outperforms the state-of-the-art methods on both datasets.</p><p>Reference: <a title="iccv-2013-313-reference" href="../iccv2013_reference/iccv-2013-Person_Re-identification_by_Salience_Matching_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 hk  Abstract Human salience is distinctive and reliable information in matching pedestrians across disjoint camera views. [sent-4, score-0.997]
</p><p>2 In this paper, we exploit the pairwise salience distribution relationship between pedestrian images, and solve the person re-identification problem by proposing a salience matching strategy. [sent-5, score-2.044]
</p><p>3 To handle the misalignment problem in pedestrian images, patch matching is adopted and patch salience is estimated. [sent-6, score-1.261]
</p><p>4 Images of the same person are recognized by minimizing the salience matching cost. [sent-8, score-1.104]
</p><p>5 Furthermore, our salience matching is tightly integrated with patch matching in a unified structural RankSVM learning framework. [sent-9, score-1.199]
</p><p>6 Introduction Person re-identification is a task of matching persons observed from non-overlapping camera views based on image appearance. [sent-13, score-0.157]
</p><p>7 It saves a lot of human efforts on exhaustively searching for a person from large amounts of video sequences. [sent-15, score-0.169]
</p><p>8 A person observed in different camera views often undergoes significant variations on viewpoints, poses, appearance and illumination, which make intra-personal variations even larger than inter-personal variations. [sent-17, score-0.193]
</p><p>9 Misalignments are caused by variations of viewpoints and poses, which are commonly exist in person reidentification. [sent-20, score-0.14]
</p><p>10 In our approach, salience matching is integrated with patch matching, and both show robustness to spatial  probe in(p c1am)eraA (p2)gal eryi n cam(pe3r)a B(p4)  q(aue1)rycor e(cat2m)atch(a3)(ian4c)or ectmat(ach5)(a6) (b1)(b2)(b3)(b4)(b5)(b6) Figure 1. [sent-23, score-1.068]
</p><p>11 Illustration of human salience and salience matching with examples. [sent-24, score-1.838]
</p><p>12 The second row and the third row show examples of salience matching. [sent-26, score-0.861]
</p><p>13 The salience map of each pedestrian image is shown. [sent-27, score-0.934]
</p><p>14 Some local patches are more distinctive and reliable when matching two persons. [sent-30, score-0.14]
</p><p>15 Some examples are shown in the first row of Figure 1, person (p1) carries a red hand bag, (p2) has an orange cap and a yellow horizontal stripe on his jacket, (p3) wears a white dress, and (p4) is dressed in red sweater with floral texture. [sent-31, score-0.14]
</p><p>16 If a body part is salient in one camera view, it usually remains salient in another view. [sent-34, score-0.12]
</p><p>17 However, most existing approaches only consider clothes and trousers  as the most important regions for person re-identification. [sent-35, score-0.14]
</p><p>18 If global features are adopted by existing approaches, those small regions have little effect on person matching. [sent-38, score-0.14]
</p><p>19 Patches with high salience values gain large weights in person reidentification, because such patches not only have good discriminative power but also can be reliably detected during patch matching across camera views. [sent-40, score-1.29]
</p><p>20 We observe that images of the same person captured from different camera views have some invariance property on their spatial distributions on salience, like pair (a1, a2) in Figure 1. [sent-41, score-0.209]
</p><p>21 Since the person in image (a1) shows salience in her dress while others in (a3)-(a6) have salient blouses. [sent-42, score-1.047]
</p><p>22 Therefore, human salience distributions provide useful information in person re-identification. [sent-44, score-1.03]
</p><p>23 If two patches from two images of the same person are matched, they are expected to have the same salience value; otherwise such matching brings salience matching penalty. [sent-46, score-2.11]
</p><p>24 In the second row in Figure 1, the query image (b1) shows  a similar salience distribution as those of gallery images. [sent-47, score-0.922]
</p><p>25 This motivates us to relate salience matching penalty to the visual similarity of two matched patches. [sent-49, score-0.966]
</p><p>26 Based on above considerations, a new person reidentification approach by salience matching is proposed. [sent-50, score-1.138]
</p><p>27 First, a probabilistic distribution of salience is reliably estimated with our approach. [sent-52, score-0.879]
</p><p>28 Different from general salience detection [6], our salience is especially designed for person re-identification. [sent-53, score-1.862]
</p><p>29 The estimated human salience is robust across disjoint camera views and is used as a meaningful representation of human appearance in recognition. [sent-54, score-0.972]
</p><p>30 Second, we formulate person re-identification as a salience matching problem. [sent-55, score-1.088]
</p><p>31 Images of the same person are recognized by minimizing the salience matching cost, which not only depends on the locations of patches but also the visual similarity of matched patches. [sent-58, score-1.175]
</p><p>32 Third, salience matching and patch matching are tightly integrated into a unified structural RankSVM learning framework. [sent-59, score-1.199]
</p><p>33 Structural RankSVM has good training efficiency given a very large number of rank constraints in person reidentification. [sent-60, score-0.161]
</p><p>34 Moreover, our approach has transformed the original high-dimensional visual feature space to a much lower dimensional salience feature space (80 times lower  in this work) to further improve the training efficiency and also avoid overfitting. [sent-61, score-0.861]
</p><p>35 Related Works Existing methods on person re-identification generally fall into two categories: unsupervised and supervised. [sent-65, score-0.165]
</p><p>36 [22] proposed an unsupervised salience learning method to exploit discriminative features, but they did not consider salience itself as an important feature for patch matching and person re-identification. [sent-78, score-2.073]
</p><p>37 Distance metric learning has been widely used in person re-identification [23, 4, 12, 13, 18, 24]. [sent-80, score-0.16]
</p><p>38 In contrast, our approach handles the problem of feature misalignment through patch matching. [sent-84, score-0.141]
</p><p>39 [8] used boosting to select a subset of optimal features for matching pedestrian images. [sent-89, score-0.16]
</p><p>40 [19] formulated person re-identification as a ranking problem, and learned global feature weights based on an ensemble of RankSVM. [sent-91, score-0.191]
</p><p>41 In this paper, we employ structural RankSVM [10], which considers the ranking difference rather than pairwise difference. [sent-93, score-0.119]
</p><p>42 In the context of person re-identification, human salience is different than general image salience in the way of drawing visual attentions. [sent-95, score-1.891]
</p><p>43 Human Salience  × ××  We compute the salience probability map based on dense correspondence with a K-nearest neighbors (KNN) method. [sent-97, score-0.923]
</p><p>44 , nNdr} a Outp{uxt: salience probability map P(lmA,,un = 1 | xAm,,un) 1: for each patch xAm,,un ∈ X do 2: compute XNN (xAm,,un) with Eq. [sent-103, score-0.977]
</p><p>45 Dense local features for an image are denoted by xA,u = {xAm,,un}, and xAm,,un represents the feature of a local patch =a t{ xthe m}-,th a row and n-th column in the u-th image from camera view A. [sent-120, score-0.131]
</p><p>46 When patch xAm,,un searches for its corresponding patch in the vth image from camera view B, i. [sent-121, score-0.23]
</p><p>47 For each patch xAm,,un, a nearest neighbor is sought from its search set in every image within a reference set. [sent-140, score-0.123]
</p><p>48 Gre nre-  gion represents the adjacency constrained search set of the patch in yellow box. [sent-144, score-0.144]
</p><p>49 Unsupervised Salience Learning Human salience is computed based on previously-built dense correspondence. [sent-148, score-0.888]
</p><p>50 In the application of person re-identification, we find salient patches that possess property of uniqueness among a reference set R. [sent-152, score-0.261]
</p><p>51 , Nr}, where S (xAm,,un, xB,v) is the adjacency search set of patch xAm,,un, and function d(·) computes the Euclidean distance betwe,e ann tdw fou patch nfe da(tu·)re cso. [sent-158, score-0.258]
</p><p>52 Our goal of computing human salience is to identify patches with special appearance. [sent-159, score-0.943]
</p><p>53 We set k = Nr/2 in the salience learning scheme with an empirical assumption that a patch is considered to have special appearance such that more than half of the people in the reference set do not share similar patch with it. [sent-161, score-1.083]
</p><p>54 Enlarging the reference dataset will not deteriorate salience detection, because the salience is defined in the statistical sense. [sent-163, score-1.766]
</p><p>55 Our human salience learning method is summarized in algorithm 1. [sent-165, score-0.89]
</p><p>56 Supervised Salience Matching One of the main contributions of this work is to match pedestrian images based on the salience probability map. [sent-167, score-0.966]
</p><p>57 In 22553300  contrast with most of the works on person re-identification,  which focus on feature selection, feature weighting, or distance metric learning, we instead exploit the consistence property of human salience and incorporate it in person matching. [sent-168, score-1.247]
</p><p>58 This is based on our observation that person in different camera views shows consistence in the salience probability map, as shown in Figure 1. [sent-169, score-1.097]
</p><p>59 i is the corresponding matched patch index in image xB produced by previously built dense correspondence. [sent-179, score-0.144]
</p><p>60 To incorporate the salience into matching, we introduce lA = {lApi | lpAi ∈ {0, 1}} and lB = {lBpi? [sent-180, score-0.861]
</p><p>61 ∈ {0, 1}} as salience labels for all the patches in image xA and xB respectively. [sent-182, score-0.914]
</p><p>62 If all the salience labels are known, we can perform person matching by computing salience matching score as follows: fz(xA, xB, lA, lB; p, z) =  (4)  ? [sent-183, score-2.054]
</p><p>63 )} are dense correspondence patch index wpahiersr,e a pnd = =z { (=p {zpi),}k} arke= 1d,e2,n3s,4e caorrer tehsep omnadtecnhcieng p astccohre ins dfeoxr fpoauirrs ,dia fnfedr zent = sa {lzience} matching results at one local patch. [sent-191, score-0.231]
</p><p>64 For example, the score of matching patches on the background should be different than those on legs. [sent-194, score-0.158]
</p><p>65 zpi,k also depends on the visual similarity between patch xpAi and patch xpBi? [sent-195, score-0.198]
</p><p>66 Therefore, we define the matching score zpi ,k as a linear function of the similarity as follows, zpi,k  ,  = αpi,k · s(xpAi xpBi? [sent-203, score-0.131]
</p><p>67 (4) are hidden variables, they can be marginalized by computing the expectation of the salience matching score as  f∗(xA, xB; p, z) = ? [sent-209, score-0.982]
</p><p>68 ) is the probabilistic salience matching cost depending on salience probabilities P(lpAi = 1 | xpAi ) and P(lpBi? [sent-220, score-1.809]
</p><p>69 ) =  (11)  Φ(xA, xB ; p) combines the salience probability map with appearance matching similarities. [sent-243, score-0.965]
</p><p>70 For each query image xA, the images in the gallery are ranked according to the expectations of salience matching scores in Eq. [sent-244, score-1.026]
</p><p>71 We will present the details in next section by formulating the person re-identification problem with Φ(xA, xB ; p) in structural RankSVM framework, and the effectiveness of salience matching will be shown in experimental results. [sent-247, score-1.134]
</p><p>72 Ranking by Partial Order We cast person re-identification as a ranking problem for training. [sent-250, score-0.191]
</p><p>73 , we rank the relevant images before irrelevant ones, but no information of the orders within relevant images or irrelevant ones is provided in groundtruth. [sent-259, score-0.119]
</p><p>74 The partial order feature [9, 17] is appropriate for our goal and can well encode the difference between relevant pairs and irrelevant pairs with only partial orders. [sent-271, score-0.123]
</p><p>75 , [2, 19]), structural SVM optimizes over ranking differences and it can incorporate non-linear multivariate loss functions directly into global optimization in SVM training. [sent-292, score-0.113]
</p><p>76 (1 1) are heavily weighted in the central part of human body which implies the importance of salience matching based on visual similarity. [sent-343, score-1.009]
</p><p>77 It means that non-salient patches on query images have little effect on person re-identification if the contribution of visual similarity is not considered. [sent-353, score-0.21]
</p><p>78 The VIPeR dataset is the mostly used person reidentification dataset for evaluation, and the recently published CUHK Campus dataset contains more images than VIPeR (3884 vs. [sent-358, score-0.25]
</p><p>79 Both are very challenging datasets for person re-identification because they contain significant variations on viewpoints, poses, and illuminations, and their images are in low resolutions, with occlusions and background clutters. [sent-360, score-0.14]
</p><p>80 we randomly partition the dataset into two even parts, 50% for training and 50% for testing, without overlap on person identities. [sent-365, score-0.16]
</p><p>81 To validate the usefulness of salience matching, we repeat all the training and testing evaluation on our approach, but without using salience. [sent-371, score-0.861]
</p><p>82 It contains 632 pedestrian pairs, and each pair contains two images of the same person observed from different camera views. [sent-375, score-0.245]
</p><p>83 16%  and outperforms  The control experiment  PatMatch  which shows the effectiveness of integrat-  ing salience matching  into patch matching. [sent-388, score-1.047]
</p><p>84 metric learning methods,  For distance  they ignore the domain knowl-  edge of person re-identification that pedestrian images suf1The VIPeR dataset is available to download at: http : / /vi s ion . [sent-389, score-0.284]
</p><p>85 Some interesting examples of salience matching in our experiments. [sent-394, score-0.948]
</p><p>86 This figure shows four categories of salience probability types: salience in upper body (in blue dashed box), salience of taking bags (in green dashed box), salience of lower body (in orange dashed box), and salience of stripes on human body (in red dashed box). [sent-395, score-4.579]
</p><p>87 It did not consider the consistency of salience distribution as a cue  or matching pedestrian images. [sent-405, score-1.021]
</p><p>88 The RankSVM also formulate person re-identification as a ranking problem, but ours shows much better performance because it adopts discriminative salience matching strategy for pairwise matching, and the structural SVM incorporates ranking loss in global optimization. [sent-407, score-1.274]
</p><p>89 This implies the impor22553344  tance of exploiting human salience matching and the effectiveness of structural SVM training. [sent-408, score-1.023]
</p><p>90 The CUHK Campus dataset  2 is also  captured with two camera views in a campus envi-  ×  ronment. [sent-410, score-0.205]
</p><p>91 Different than the VIPeR dataset, images in this dataset are of higher resolution and are more suitable to show the effectiveness of salience matching. [sent-411, score-0.881]
</p><p>92 The CUHK Campus dataset contains 971 persons, and each person has two images in each camera view. [sent-412, score-0.192]
</p><p>93 Features of all local patches are directly concatenated regardless of spatial misalignment problem (therefore, patch matching is not used), and the pairwise distance is simply computed by L1-norm and L2-norm. [sent-417, score-0.318]
</p><p>94 Apparently, our salience matching approach outperforms the others methods, and similar conclusions as in the VIPeR dataset can be drawn from the comparisons. [sent-427, score-0.968]
</p><p>95 Conclusion In this paper, we formulate person re-identification as a salience matching problem. [sent-429, score-1.088]
</p><p>96 The dense correspondences of local patches are established by patch matching. [sent-430, score-0.179]
</p><p>97 Images of the same person are recognized by minimizing the salience matching cost. [sent-433, score-1.104]
</p><p>98 We tightly integrate patch matching and salience matching in the partial order feature and feed them into a unified structural RankSVM learning framework. [sent-434, score-1.236]
</p><p>99 Experimental results show our salience matching approach greatly improved the performance of person reidentification. [sent-435, score-1.088]
</p><p>100 Bicov: a novel image representation for person re-identification and face verification. [sent-555, score-0.14]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('salience', 0.861), ('person', 0.14), ('campus', 0.132), ('viper', 0.123), ('cuhk', 0.121), ('ranksvm', 0.109), ('vv', 0.101), ('xa', 0.1), ('patch', 0.099), ('xb', 0.093), ('lpai', 0.09), ('xpai', 0.09), ('matching', 0.087), ('lpbi', 0.077), ('xpbi', 0.077), ('pedestrian', 0.073), ('sdalf', 0.068), ('sx', 0.057), ('patches', 0.053), ('aprdc', 0.051), ('patmatch', 0.051), ('salmatch', 0.051), ('ranking', 0.051), ('reidentification', 0.05), ('structural', 0.046), ('adjacency', 0.045), ('pi', 0.044), ('gallery', 0.044), ('po', 0.043), ('misalignment', 0.042), ('lb', 0.04), ('partial', 0.037), ('prdc', 0.034), ('xnn', 0.034), ('dashed', 0.033), ('camera', 0.032), ('body', 0.032), ('elf', 0.032), ('pcca', 0.032), ('irrelevant', 0.029), ('human', 0.029), ('salient', 0.028), ('cmc', 0.027), ('dense', 0.027), ('wt', 0.027), ('bicov', 0.026), ('consistence', 0.026), ('ebicov', 0.026), ('esdc', 0.026), ('nswap', 0.026), ('zpi', 0.026), ('unsupervised', 0.025), ('reference', 0.024), ('nr', 0.024), ('gong', 0.023), ('pairwise', 0.022), ('views', 0.021), ('rank', 0.021), ('probe', 0.021), ('prosser', 0.021), ('incorrect', 0.021), ('benchmarking', 0.021), ('brings', 0.021), ('relevant', 0.02), ('cps', 0.02), ('bazzani', 0.02), ('cumulated', 0.02), ('dataset', 0.02), ('metric', 0.02), ('farenzena', 0.019), ('tightly', 0.019), ('knn', 0.018), ('score', 0.018), ('la', 0.018), ('itml', 0.018), ('fz', 0.018), ('correspondence', 0.018), ('zhao', 0.018), ('reliably', 0.018), ('matched', 0.018), ('ouyang', 0.018), ('dress', 0.018), ('persons', 0.017), ('probability', 0.017), ('weighting', 0.017), ('pedestrians', 0.017), ('query', 0.017), ('lmnn', 0.017), ('ranked', 0.017), ('misaligned', 0.016), ('property', 0.016), ('loss', 0.016), ('finley', 0.016), ('loy', 0.016), ('download', 0.016), ('inconsistent', 0.016), ('expectation', 0.016), ('recognized', 0.016), ('distance', 0.015), ('match', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="313-tfidf-1" href="./iccv-2013-Person_Re-identification_by_Salience_Matching.html">313 iccv-2013-Person Re-identification by Salience Matching</a></p>
<p>Author: Rui Zhao, Wanli Ouyang, Xiaogang Wang</p><p>Abstract: Human salience is distinctive and reliable information in matching pedestrians across disjoint camera views. In this paper, we exploit the pairwise salience distribution relationship between pedestrian images, and solve the person re-identification problem by proposing a salience matching strategy. To handle the misalignment problem in pedestrian images, patch matching is adopted and patch salience is estimated. Matching patches with inconsistent salience brings penalty. Images of the same person are recognized by minimizing the salience matching cost. Furthermore, our salience matching is tightly integrated with patch matching in a unified structural RankSVM learning framework. The effectiveness of our approach is validated on the VIPeR dataset and the CUHK Campus dataset. It outperforms the state-of-the-art methods on both datasets.</p><p>2 0.12902546 <a title="313-tfidf-2" href="./iccv-2013-POP%3A_Person_Re-identification_Post-rank_Optimisation.html">305 iccv-2013-POP: Person Re-identification Post-rank Optimisation</a></p>
<p>Author: Chunxiao Liu, Chen Change Loy, Shaogang Gong, Guijin Wang</p><p>Abstract: Owing to visual ambiguities and disparities, person reidentification methods inevitably produce suboptimal ranklist, which still requires exhaustive human eyeballing to identify the correct target from hundreds of different likelycandidates. Existing re-identification studies focus on improving the ranking performance, but rarely look into the critical problem of optimising the time-consuming and error-prone post-rank visual search at the user end. In this study, we present a novel one-shot Post-rank OPtimisation (POP) method, which allows a user to quickly refine their search by either “one-shot” or a couple of sparse negative selections during a re-identification process. We conduct systematic behavioural studies to understand user’s searching behaviour and show that the proposed method allows correct re-identification to converge 2.6 times faster than the conventional exhaustive search. Importantly, through extensive evaluations we demonstrate that the method is capable of achieving significant improvement over the stateof-the-art distance metric learning based ranking models, even with just “one shot” feedback optimisation, by as much as over 30% performance improvement for rank 1reidentification on the VIPeR and i-LIDS datasets.</p><p>3 0.11640569 <a title="313-tfidf-3" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>Author: Andy J. Ma, Pong C. Yuen, Jiawei Li</p><p>Abstract: This paper addresses a new person re-identification problem without the label information of persons under non-overlapping target cameras. Given the matched (positive) and unmatched (negative) image pairs from source domain cameras, as well as unmatched (negative) image pairs which can be easily generated from target domain cameras, we propose a Domain Transfer Ranked Support Vector Machines (DTRSVM) method for re-identification under target domain cameras. To overcome the problems introduced due to the absence of matched (positive) image pairs in target domain, we relax the discriminative constraint to a necessary condition only relying on the positive mean in target domain. By estimating the target positive mean using source and target domain data, a new discriminative model with high confidence in target positive mean and low confidence in target negative image pairs is developed. Since the necessary condition may not truly preserve the discriminability, multi-task support vector ranking is proposed to incorporate the training data from source domain with label information. Experimental results show that the proposed DTRSVM outperforms existing methods without using label information in target cameras. And the top 30 rank accuracy can be improved by the proposed method upto 9.40% on publicly available person re-identification datasets.</p><p>4 0.084970273 <a title="313-tfidf-4" href="./iccv-2013-Human_Re-identification_by_Matching_Compositional_Template_with_Cluster_Sampling.html">205 iccv-2013-Human Re-identification by Matching Compositional Template with Cluster Sampling</a></p>
<p>Author: Yuanlu Xu, Liang Lin, Wei-Shi Zheng, Xiaobai Liu</p><p>Abstract: This paper aims at a newly raising task in visual surveillance: re-identifying people at a distance by matching body information, given several reference examples. Most of existing works solve this task by matching a reference template with the target individual, but often suffer from large human appearance variability (e.g. different poses/views, illumination) and high false positives in matching caused by conjunctions, occlusions or surrounding clutters. Addressing these problems, we construct a simple yet expressive template from a few reference images of a certain individual, which represents the body as an articulated assembly of compositional and alternative parts, and propose an effective matching algorithm with cluster sampling. This algorithm is designed within a candidacy graph whose vertices are matching candidates (i.e. a pair of source and target body parts), and iterates in two steps for convergence. (i) It generates possible partial matches based on compatible and competitive relations among body parts. (ii) It con- firms the partial matches to generate a new matching solution, which is accepted by the Markov Chain Monte Carlo (MCMC) mechanism. In the experiments, we demonstrate the superior performance of our approach on three public databases compared to existing methods.</p><p>5 0.061600581 <a title="313-tfidf-5" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>Author: Renliang Weng, Jiwen Lu, Junlin Hu, Gao Yang, Yap-Peng Tan</p><p>Abstract: Over the past two decades, a number of face recognition methods have been proposed in the literature. Most of them use holistic face images to recognize people. However, human faces are easily occluded by other objects in many real-world scenarios and we have to recognize the person of interest from his/her partial faces. In this paper, we propose a new partial face recognition approach by using feature set matching, which is able to align partial face patches to holistic gallery faces automatically and is robust to occlusions and illumination changes. Given each gallery image and probe face patch, we first detect keypoints and extract their local features. Then, we propose a Metric Learned ExtendedRobust PointMatching (MLERPM) method to discriminatively match local feature sets of a pair of gallery and probe samples. Lastly, the similarity of two faces is converted as the distance between two feature sets. Experimental results on three public face databases are presented to show the effectiveness of the proposed approach.</p><p>6 0.060507979 <a title="313-tfidf-6" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>7 0.055745691 <a title="313-tfidf-7" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>8 0.054997656 <a title="313-tfidf-8" href="./iccv-2013-DCSH_-_Matching_Patches_in_RGBD_Images.html">101 iccv-2013-DCSH - Matching Patches in RGBD Images</a></p>
<p>9 0.051619202 <a title="313-tfidf-9" href="./iccv-2013-CoDeL%3A_A_Human_Co-detection_and_Labeling_Framework.html">75 iccv-2013-CoDeL: A Human Co-detection and Labeling Framework</a></p>
<p>10 0.049183626 <a title="313-tfidf-10" href="./iccv-2013-Shape_Anchors_for_Data-Driven_Multi-view_Reconstruction.html">387 iccv-2013-Shape Anchors for Data-Driven Multi-view Reconstruction</a></p>
<p>11 0.048770644 <a title="313-tfidf-11" href="./iccv-2013-Multi-stage_Contextual_Deep_Learning_for_Pedestrian_Detection.html">279 iccv-2013-Multi-stage Contextual Deep Learning for Pedestrian Detection</a></p>
<p>12 0.048669241 <a title="313-tfidf-12" href="./iccv-2013-Learning_Graph_Matching%3A_Oriented_to_Category_Modeling_from_Cluttered_Scenes.html">237 iccv-2013-Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes</a></p>
<p>13 0.047950637 <a title="313-tfidf-13" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>14 0.046035811 <a title="313-tfidf-14" href="./iccv-2013-Single-Patch_Low-Rank_Prior_for_Non-pointwise_Impulse_Noise_Removal.html">394 iccv-2013-Single-Patch Low-Rank Prior for Non-pointwise Impulse Noise Removal</a></p>
<p>15 0.045896724 <a title="313-tfidf-15" href="./iccv-2013-Random_Forests_of_Local_Experts_for_Pedestrian_Detection.html">336 iccv-2013-Random Forests of Local Experts for Pedestrian Detection</a></p>
<p>16 0.044447668 <a title="313-tfidf-16" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>17 0.043717194 <a title="313-tfidf-17" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>18 0.041196246 <a title="313-tfidf-18" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<p>19 0.039618876 <a title="313-tfidf-19" href="./iccv-2013-Finding_Actors_and_Actions_in_Movies.html">166 iccv-2013-Finding Actors and Actions in Movies</a></p>
<p>20 0.039475787 <a title="313-tfidf-20" href="./iccv-2013-Joint_Deep_Learning_for_Pedestrian_Detection.html">220 iccv-2013-Joint Deep Learning for Pedestrian Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.104), (1, -0.001), (2, -0.0), (3, -0.037), (4, 0.006), (5, -0.008), (6, 0.021), (7, -0.001), (8, -0.003), (9, 0.011), (10, -0.011), (11, -0.002), (12, 0.015), (13, -0.025), (14, 0.043), (15, -0.001), (16, 0.003), (17, -0.009), (18, 0.058), (19, 0.057), (20, -0.0), (21, -0.047), (22, -0.001), (23, -0.005), (24, 0.066), (25, 0.04), (26, -0.068), (27, 0.036), (28, -0.077), (29, -0.016), (30, 0.008), (31, -0.037), (32, 0.049), (33, 0.026), (34, 0.063), (35, -0.003), (36, 0.001), (37, -0.014), (38, 0.039), (39, -0.007), (40, 0.015), (41, 0.015), (42, 0.008), (43, -0.06), (44, 0.03), (45, -0.032), (46, 0.046), (47, 0.015), (48, 0.072), (49, -0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89960253 <a title="313-lsi-1" href="./iccv-2013-Person_Re-identification_by_Salience_Matching.html">313 iccv-2013-Person Re-identification by Salience Matching</a></p>
<p>Author: Rui Zhao, Wanli Ouyang, Xiaogang Wang</p><p>Abstract: Human salience is distinctive and reliable information in matching pedestrians across disjoint camera views. In this paper, we exploit the pairwise salience distribution relationship between pedestrian images, and solve the person re-identification problem by proposing a salience matching strategy. To handle the misalignment problem in pedestrian images, patch matching is adopted and patch salience is estimated. Matching patches with inconsistent salience brings penalty. Images of the same person are recognized by minimizing the salience matching cost. Furthermore, our salience matching is tightly integrated with patch matching in a unified structural RankSVM learning framework. The effectiveness of our approach is validated on the VIPeR dataset and the CUHK Campus dataset. It outperforms the state-of-the-art methods on both datasets.</p><p>2 0.64382982 <a title="313-lsi-2" href="./iccv-2013-POP%3A_Person_Re-identification_Post-rank_Optimisation.html">305 iccv-2013-POP: Person Re-identification Post-rank Optimisation</a></p>
<p>Author: Chunxiao Liu, Chen Change Loy, Shaogang Gong, Guijin Wang</p><p>Abstract: Owing to visual ambiguities and disparities, person reidentification methods inevitably produce suboptimal ranklist, which still requires exhaustive human eyeballing to identify the correct target from hundreds of different likelycandidates. Existing re-identification studies focus on improving the ranking performance, but rarely look into the critical problem of optimising the time-consuming and error-prone post-rank visual search at the user end. In this study, we present a novel one-shot Post-rank OPtimisation (POP) method, which allows a user to quickly refine their search by either “one-shot” or a couple of sparse negative selections during a re-identification process. We conduct systematic behavioural studies to understand user’s searching behaviour and show that the proposed method allows correct re-identification to converge 2.6 times faster than the conventional exhaustive search. Importantly, through extensive evaluations we demonstrate that the method is capable of achieving significant improvement over the stateof-the-art distance metric learning based ranking models, even with just “one shot” feedback optimisation, by as much as over 30% performance improvement for rank 1reidentification on the VIPeR and i-LIDS datasets.</p><p>3 0.55652976 <a title="313-lsi-3" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>Author: Renliang Weng, Jiwen Lu, Junlin Hu, Gao Yang, Yap-Peng Tan</p><p>Abstract: Over the past two decades, a number of face recognition methods have been proposed in the literature. Most of them use holistic face images to recognize people. However, human faces are easily occluded by other objects in many real-world scenarios and we have to recognize the person of interest from his/her partial faces. In this paper, we propose a new partial face recognition approach by using feature set matching, which is able to align partial face patches to holistic gallery faces automatically and is robust to occlusions and illumination changes. Given each gallery image and probe face patch, we first detect keypoints and extract their local features. Then, we propose a Metric Learned ExtendedRobust PointMatching (MLERPM) method to discriminatively match local feature sets of a pair of gallery and probe samples. Lastly, the similarity of two faces is converted as the distance between two feature sets. Experimental results on three public face databases are presented to show the effectiveness of the proposed approach.</p><p>4 0.55238044 <a title="313-lsi-4" href="./iccv-2013-Model_Recommendation_with_Virtual_Probes_for_Egocentric_Hand_Detection.html">267 iccv-2013-Model Recommendation with Virtual Probes for Egocentric Hand Detection</a></p>
<p>Author: Cheng Li, Kris M. Kitani</p><p>Abstract: Egocentric cameras can be used to benefit such tasks as analyzing fine motor skills, recognizing gestures and learning about hand-object manipulation. To enable such technology, we believe that the hands must detected on thepixellevel to gain important information about the shape of the hands and fingers. We show that the problem of pixel-wise hand detection can be effectively solved, by posing the problem as a model recommendation task. As such, the goal of a recommendation system is to recommend the n-best hand detectors based on the probe set a small amount of labeled data from the test distribution. This requirement of a probe set is a serious limitation in many applications, such as ego-centric hand detection, where the test distribution may be continually changing. To address this limitation, we propose the use of virtual probes which can be automatically extracted from the test distribution. The key idea is – that many features, such as the color distribution or relative performance between two detectors, can be used as a proxy to the probe set. In our experiments we show that the recommendation paradigm is well-equipped to handle complex changes in the appearance of the hands in firstperson vision. In particular, we show how our system is able to generalize to new scenarios by testing our model across multiple users.</p><p>5 0.53283662 <a title="313-lsi-5" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>Author: Yuandong Tian, Srinivasa G. Narasimhan</p><p>Abstract: Real-world surfaces such as clothing, water and human body deform in complex ways. The image distortions observed are high-dimensional and non-linear, making it hard to estimate these deformations accurately. The recent datadriven descent approach [17] applies Nearest Neighbor estimators iteratively on a particular distribution of training samples to obtain a globally optimal and dense deformation field between a template and a distorted image. In this work, we develop a hierarchical structure for the Nearest Neighbor estimators, each of which can have only a local image support. We demonstrate in both theory and practice that this algorithm has several advantages over the nonhierarchical version: it guarantees global optimality with significantly fewer training samples, is several orders faster, provides a metric to decide whether a given image is “hard” (or “easy ”) requiring more (or less) samples, and can handle more complex scenes that include both global motion and local deformation. The proposed algorithm successfully tracks a broad range of non-rigid scenes including water, clothing, and medical images, and compares favorably against several other deformation estimation and tracking approaches that do not provide optimality guarantees.</p><p>6 0.51371998 <a title="313-lsi-6" href="./iccv-2013-DCSH_-_Matching_Patches_in_RGBD_Images.html">101 iccv-2013-DCSH - Matching Patches in RGBD Images</a></p>
<p>7 0.50570703 <a title="313-lsi-7" href="./iccv-2013-Human_Re-identification_by_Matching_Compositional_Template_with_Cluster_Sampling.html">205 iccv-2013-Human Re-identification by Matching Compositional Template with Cluster Sampling</a></p>
<p>8 0.50122261 <a title="313-lsi-8" href="./iccv-2013-Local_Signal_Equalization_for_Correspondence_Matching.html">255 iccv-2013-Local Signal Equalization for Correspondence Matching</a></p>
<p>9 0.49515817 <a title="313-lsi-9" href="./iccv-2013-Markov_Network-Based_Unified_Classifier_for_Face_Identification.html">261 iccv-2013-Markov Network-Based Unified Classifier for Face Identification</a></p>
<p>10 0.49085262 <a title="313-lsi-10" href="./iccv-2013-CoDeL%3A_A_Human_Co-detection_and_Labeling_Framework.html">75 iccv-2013-CoDeL: A Human Co-detection and Labeling Framework</a></p>
<p>11 0.47609127 <a title="313-lsi-11" href="./iccv-2013-Multiple_Non-rigid_Surface_Detection_and_Registration.html">283 iccv-2013-Multiple Non-rigid Surface Detection and Registration</a></p>
<p>12 0.4705933 <a title="313-lsi-12" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>13 0.46345726 <a title="313-lsi-13" href="./iccv-2013-EVSAC%3A_Accelerating_Hypotheses_Generation_by_Modeling_Matching_Scores_with_Extreme_Value_Theory.html">131 iccv-2013-EVSAC: Accelerating Hypotheses Generation by Modeling Matching Scores with Extreme Value Theory</a></p>
<p>14 0.44824916 <a title="313-lsi-14" href="./iccv-2013-Parsing_IKEA_Objects%3A_Fine_Pose_Estimation.html">308 iccv-2013-Parsing IKEA Objects: Fine Pose Estimation</a></p>
<p>15 0.44696474 <a title="313-lsi-15" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>16 0.44101819 <a title="313-lsi-16" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>17 0.43567041 <a title="313-lsi-17" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>18 0.43157953 <a title="313-lsi-18" href="./iccv-2013-Single-Patch_Low-Rank_Prior_for_Non-pointwise_Impulse_Noise_Removal.html">394 iccv-2013-Single-Patch Low-Rank Prior for Non-pointwise Impulse Noise Removal</a></p>
<p>19 0.43041512 <a title="313-lsi-19" href="./iccv-2013-SIFTpack%3A_A_Compact_Representation_for_Efficient_SIFT_Matching.html">365 iccv-2013-SIFTpack: A Compact Representation for Efficient SIFT Matching</a></p>
<p>20 0.4250356 <a title="313-lsi-20" href="./iccv-2013-Heterogeneous_Auto-similarities_of_Characteristics_%28HASC%29%3A_Exploiting_Relational_Information_for_Classification.html">193 iccv-2013-Heterogeneous Auto-similarities of Characteristics (HASC): Exploiting Relational Information for Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.093), (7, 0.021), (12, 0.046), (21, 0.245), (26, 0.061), (31, 0.039), (42, 0.066), (48, 0.042), (64, 0.051), (69, 0.024), (73, 0.02), (78, 0.011), (89, 0.129), (98, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73390913 <a title="313-lda-1" href="./iccv-2013-Person_Re-identification_by_Salience_Matching.html">313 iccv-2013-Person Re-identification by Salience Matching</a></p>
<p>Author: Rui Zhao, Wanli Ouyang, Xiaogang Wang</p><p>Abstract: Human salience is distinctive and reliable information in matching pedestrians across disjoint camera views. In this paper, we exploit the pairwise salience distribution relationship between pedestrian images, and solve the person re-identification problem by proposing a salience matching strategy. To handle the misalignment problem in pedestrian images, patch matching is adopted and patch salience is estimated. Matching patches with inconsistent salience brings penalty. Images of the same person are recognized by minimizing the salience matching cost. Furthermore, our salience matching is tightly integrated with patch matching in a unified structural RankSVM learning framework. The effectiveness of our approach is validated on the VIPeR dataset and the CUHK Campus dataset. It outperforms the state-of-the-art methods on both datasets.</p><p>2 0.72464752 <a title="313-lda-2" href="./iccv-2013-A_Color_Constancy_Model_with_Double-Opponency_Mechanisms.html">5 iccv-2013-A Color Constancy Model with Double-Opponency Mechanisms</a></p>
<p>Author: Shaobing Gao, Kaifu Yang, Chaoyi Li, Yongjie Li</p><p>Abstract: The double-opponent color-sensitive cells in the primary visual cortex (V1) of the human visual system (HVS) have long been recognized as the physiological basis of color constancy. We introduce a new color constancy model by imitating the functional properties of the HVS from the retina to the double-opponent cells in V1. The idea behind the model originates from the observation that the color distribution of the responses of double-opponent cells to the input color-biased images coincides well with the light source direction. Then the true illuminant color of a scene is easily estimated by searching for the maxima of the separate RGB channels of the responses of double-opponent cells in the RGB space. Our systematical experimental evaluations on two commonly used image datasets show that the proposed model can produce competitive results in comparison to the complex state-of-the-art approaches, but with a simple implementation and without the need for training.</p><p>3 0.67571598 <a title="313-lda-3" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>Author: Xi Li, Yao Li, Chunhua Shen, Anthony Dick, Anton Van_Den_Hengel</p><p>Abstract: Salient object detection aims to locate objects that capture human attention within images. Previous approaches often pose this as a problem of image contrast analysis. In this work, we model an image as a hypergraph that utilizes a set of hyperedges to capture the contextual properties of image pixels or regions. As a result, the problem of salient object detection becomes one of finding salient vertices and hyperedges in the hypergraph. The main advantage of hypergraph modeling is that it takes into account each pixel’s (or region ’s) affinity with its neighborhood as well as its separation from image background. Furthermore, we propose an alternative approach based on centerversus-surround contextual contrast analysis, which performs salient object detection by optimizing a cost-sensitive support vector machine (SVM) objective function. Experimental results on four challenging datasets demonstrate the effectiveness of the proposed approaches against the stateof-the-art approaches to salient object detection.</p><p>4 0.65359652 <a title="313-lda-4" href="./iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction.html">50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</a></p>
<p>Author: Ali Borji, Hamed R. Tavakoli, Dicky N. Sihite, Laurent Itti</p><p>Abstract: Significant recent progress has been made in developing high-quality saliency models. However, less effort has been undertaken on fair assessment of these models, over large standardized datasets and correctly addressing confounding factors. In this study, we pursue a critical and quantitative look at challenges (e.g., center-bias, map smoothing) in saliency modeling and the way they affect model accuracy. We quantitatively compare 32 state-of-the-art models (using the shuffled AUC score to discount center-bias) on 4 benchmark eye movement datasets, for prediction of human fixation locations and scanpath sequence. We also account for the role of map smoothing. We find that, although model rankings vary, some (e.g., AWS, LG, AIM, and HouNIPS) consistently outperform other models over all datasets. Some models work well for prediction of both fixation locations and scanpath sequence (e.g., Judd, GBVS). Our results show low prediction accuracy for models over emotional stimuli from the NUSEF dataset. Our last benchmark, for the first time, gauges the ability of models to decode the stimulus category from statistics of fixations, saccades, and model saliency values at fixated locations. In this test, ITTI and AIM models win over other models. Our benchmark provides a comprehensive high-level picture of the strengths and weaknesses of many popular models, and suggests future research directions in saliency modeling.</p><p>5 0.63451242 <a title="313-lda-5" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>Author: Silvia Zuffi, Javier Romero, Cordelia Schmid, Michael J. Black</p><p>Abstract: We address the problem of upper-body human pose estimation in uncontrolled monocular video sequences, without manual initialization. Most current methods focus on isolated video frames and often fail to correctly localize arms and hands. Inferring pose over a video sequence is advantageous because poses of people in adjacent frames exhibit properties of smooth variation due to the nature of human and camera motion. To exploit this, previous methods have used prior knowledge about distinctive actions or generic temporal priors combined with static image likelihoods to track people in motion. Here we take a different approach based on a simple observation: Information about how a person moves from frame to frame is present in the optical flow field. We develop an approach for tracking articulated motions that “links” articulated shape models of peo- ple in adjacent frames through the dense optical flow. Key to this approach is a 2D shape model of the body that we use to compute how the body moves over time. The resulting “flowing puppets ” provide a way of integrating image evidence across frames to improve pose inference. We apply our method on a challenging dataset of TV video sequences and show state-of-the-art performance.</p><p>6 0.62885952 <a title="313-lda-6" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>7 0.60094321 <a title="313-lda-7" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>8 0.59912336 <a title="313-lda-8" href="./iccv-2013-Class-Specific_Simplex-Latent_Dirichlet_Allocation_for_Image_Classification.html">73 iccv-2013-Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification</a></p>
<p>9 0.59810346 <a title="313-lda-9" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>10 0.59743661 <a title="313-lda-10" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>11 0.59563315 <a title="313-lda-11" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>12 0.59469014 <a title="313-lda-12" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>13 0.59367979 <a title="313-lda-13" href="./iccv-2013-Bounded_Labeling_Function_for_Global_Segmentation_of_Multi-part_Objects_with_Geometric_Constraints.html">63 iccv-2013-Bounded Labeling Function for Global Segmentation of Multi-part Objects with Geometric Constraints</a></p>
<p>14 0.59284449 <a title="313-lda-14" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>15 0.59264123 <a title="313-lda-15" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>16 0.59152007 <a title="313-lda-16" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>17 0.59109694 <a title="313-lda-17" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>18 0.59041166 <a title="313-lda-18" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<p>19 0.5903545 <a title="313-lda-19" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>20 0.59029502 <a title="313-lda-20" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
