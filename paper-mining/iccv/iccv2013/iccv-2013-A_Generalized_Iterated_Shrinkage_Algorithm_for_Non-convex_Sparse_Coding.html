<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-14" href="#">iccv2013-14</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</h1>
<br/><p>Source: <a title="iccv-2013-14-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Zuo_A_Generalized_Iterated_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Wangmeng Zuo, Deyu Meng, Lei Zhang, Xiangchu Feng, David Zhang</p><p>Abstract: In many sparse coding based image restoration and image classification problems, using non-convex ?p-norm minimization (0 ≤ p < 1) can often obtain better results than timhei convex 0?1 -norm m 1)ini camniza otfiteonn. Ab naiunm bbeetrt of algorithms, e.g., iteratively reweighted least squares (IRLS), iteratively thresholding method (ITM-?p), and look-up table (LUT), have been proposed for non-convex ?p-norm sparse coding, while some analytic solutions have been suggested for some specific values of p. In this paper, by extending the popular soft-thresholding operator, we propose a generalized iterated shrinkage algorithm (GISA) for ?p-norm non-convex sparse coding. Unlike the analytic solutions, the proposed GISA algorithm is easy to implement, and can be adopted for solving non-convex sparse coding problems with arbitrary p values. Compared with LUT, GISA is more general and does not need to compute and store the look-up tables. Compared with IRLS and ITM-?p, GISA is theoretically more solid and can achieve more accurate solutions. Experiments on image restoration and sparse coding based face recognition are conducted to validate the performance of GISA. ××</p><p>Reference: <a title="iccv-2013-14-reference" href="../iccv2013_reference/iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn  Abstract In many sparse coding based image restoration and image classification problems, using non-convex ? [sent-11, score-0.21]
</p><p>2 , iteratively reweighted least squares (IRLS), iteratively thresholding method (ITM-? [sent-16, score-0.256]
</p><p>3 p-norm sparse coding, while some analytic solutions have been suggested for some specific values of p. [sent-18, score-0.134]
</p><p>4 In this paper, by extending the popular soft-thresholding operator, we propose a generalized iterated shrinkage algorithm (GISA) for ? [sent-19, score-0.196]
</p><p>5 Unlike the analytic solutions, the proposed GISA algorithm is easy to implement, and can be adopted for solving non-convex sparse coding problems with arbitrary p values. [sent-21, score-0.251]
</p><p>6 Experiments on image restoration and sparse coding based face recognition are conducted to validate the performance  of GISA. [sent-25, score-0.291]
</p><p>7 Introduction  Sparse coding [7, 18, 3 1] is an effective tool in a myriad of applications such as compressed sensing [11], image restoration [24, 25], face recognition [38], etc. [sent-27, score-0.271]
</p><p>8 Originally, it aims to solve the following minimization problem: mxin21 ? [sent-28, score-0.06]
</p><p>9 p-norm non-convex sparse coding problems, and they have been applied to various vision and learning tasks, e. [sent-71, score-0.151]
</p><p>10 , compressed sensing [10], image restoration [25], face recognition [29], and variable selection [33]. [sent-73, score-0.186]
</p><p>11 Several typical algorithms include iteratively reweighted least squares (IRLS) [12, 14, 23, 24, 28], iteratively reweighted ? [sent-74, score-0.138]
</p><p>12 LUT uses look-up tables to store the solutions w. [sent-81, score-0.053]
</p><p>13 Other algorithms, such as the analytic solutions in [25, 39], can only be used for some specific values of p. [sent-88, score-0.068]
</p><p>14 217  Inspired by the great success of soft thresholding [16] and iterative shrinkage/thresholding (IST) [15] methods, in this paper, we propose a generalized iterated shrinkage algorithm (GISA) for ? [sent-89, score-0.391]
</p><p>15 p-norm sparse coding problems with arbitrary p, λ and y values. [sent-92, score-0.174]
</p><p>16 It is easy to implement and can be readily used to solve the many ? [sent-95, score-0.052]
</p><p>17 (4), IRLS and IRL1 sometimes cannot converge to the desired solutions. [sent-156, score-0.092]
</p><p>18 3, by initializing x(0) = y, IRLS and IRL1 would converge to the same local minimum. [sent-160, score-0.092]
</p><p>19 (4) is for 1D optimization, one can define a proper thresholding function [33] or construct look-up tables (LUTs) [25] in advance. [sent-162, score-0.162]
</p><p>20 p converge to the same local minimum, but GISA can converge to a better solution. [sent-170, score-0.184]
</p><p>21 , 1/2 or 2/3, the analytic solutions can be derived [25, 39]. [sent-173, score-0.068]
</p><p>22 (11) cannot always guarantee to converge to the global solution. [sent-180, score-0.092]
</p><p>23 p-norm non-convex sparse coding problems where the values of x, λ and p are unconstrained, LUT will not be an effective and efficient solution. [sent-185, score-0.174]
</p><p>24 (13)  Generally, if |y| ≤ λ, the soft-thresholding operator uses the thresholding r|yu|le ≤ t oλ assign T1(y; λ) otol d0in; goth oeprewraistoer, uses tthhee shrinkage rule to assign T1(y; λ) to sgn(y)(|y| − λ). [sent-202, score-0.349]
</p><p>25 Generalization of soft-thresholding Inspired by soft-thresholding, we proposed a generalized shrankage/thresholding operator to solve the ? [sent-205, score-0.108]
</p><p>26 (4) by modifying the thresholding and the shrinkage rules. [sent-207, score-0.275]
</p><p>27 Thus, to generalize soft thresholding for solving the problem in Eq. [sent-219, score-0.203]
</p><p>28 (19)  In ITM, She [33] extended the soft-thresholding with the thresholding function in Eq. [sent-235, score-0.162]
</p><p>29 (11) actually is not a good generalization of the soft-thresholding operator for ? [sent-242, score-0.061]
</p><p>30 Thus, to generalize softthresholding, we should solve the following nonlinear equation system to determine a correct thresholding value τpGST(λ) and its corresponding x∗p:  (x0(λ,p),  21? [sent-246, score-0.207]
</p><p>31 e range of (x(0λ,p), +∞) can be obtained as xp∗ = (2λ(1  −  p))2−1p,  (24)  and the thresholding value τpGST(λ) is τpGST(λ) = (2λ(1  −  p))2−1p  + λp(2λ(1  −  p))p2−−1p. [sent-270, score-0.162]
</p><p>32 Theorem 1 For any y ∈ (τpGST(λ), +∞), f(x) has one unique minimum SpGST(y; λ) in the range of (xp∗, +∞), which can be obtained by solving the following equation: SGpST(y;λ)  −  y+  λp? [sent-272, score-0.066]
</p><p>33 In Algorithm 1, the output would converge to the correct solution when J → ∞. [sent-298, score-0.113]
</p><p>34 s Finally, we propose a generalized soft-thresholding (GST) function for solving the ? [sent-301, score-0.083]
</p><p>35 (28)  Like the soft-thresholding function, the GST function also involves a thresholding rule TpGST(y; λ) = 0 when |y| ≤ τpGST(λ) and a shrinkage rule TpGST(y; λ) = sgn(y)SpGST(y; λ) when |y| > τpGST(λ). [sent-305, score-0.339]
</p><p>36 Compared with the thresholding functwiohne nin | [|3 >3], τ in GST we adopt a different thresholding val-  ue τpGST(λ), and propose an algorithm, i. [sent-306, score-0.324]
</p><p>37 When p = 1, GST will converge after one iteration. [sent-320, score-0.092]
</p><p>38 Since pl→im1τpGST(λ) = λp l→im1(1 − p)p−1 = λ, (29) the thresholding value of GST will become λ, and the GST function becomes  T1GST(y;λ) =? [sent-321, score-0.162]
</p><p>39 When p = 0, GST will also converge after one iteration. [sent-324, score-0.092]
</p><p>40 Generalized iterated shrinkage algorithm With the proposed GST in Eq. [sent-330, score-0.154]
</p><p>41 (28), we can readily have a generalized iterated shrinkage algorithm (GISA) for solving the ? [sent-331, score-0.237]
</p><p>42 GISA The proposed GISA is an iterative algorithm, and in each iteration it involves a gradient descent step based on A or y, followed by a generalized shrinkage/thresholding step: x(k+1) = TGpST(x(k)  −  ? [sent-336, score-0.097]
</p><p>43 Output: x  Actually, GISA is a generalization of the iterative shrinkage/thresholding (IST) method [15], and an example of the iterative thresholding method (ITM) [33]. [sent-360, score-0.247]
</p><p>44 In [33], She proved that, for any thresholding function Θ (y; λ) defined for −∞ < y < +∞ and 0 ≤ λ < +∞, if Θ (y; λ) satisfies the following properties: i) Θ(−y; λ) = −Θ(y; λ), ii) Θ(y; λ) ≤ −Θ(y? [sent-361, score-0.162]
</p><p>45 , iii) limy→∞Θ(y; λ) =; ∞, iv) 0 ≤ Θ(y; λ) ≤λ y =fo ∞r 0, ≤ y < ∞, the ITM0 ≤m eΘth(yo;dλ )w ≤ou yld f converge t o∞ a, stationary point. [sent-363, score-0.092]
</p><p>46 2-norm, GISA can al-  so converge to the optimal solution. [sent-369, score-0.092]
</p><p>47 Moreover, if p = 1, GISA would degenerate to IST, and would converge to the global minimum. [sent-370, score-0.092]
</p><p>48 Sparse GST  gradient  based deconvolution  using  One important application of sparse coding is image restoration. [sent-378, score-0.342]
</p><p>49 n A typical image deconvolution model usually includes a fidelity term and a regularization term, where the fidelity term is modeled based on the degradation process, and the regularization term is modeled based on image priors. [sent-382, score-0.245]
</p><p>50 Recent studies on natural image statistics have shown that the marginal distributions of filtering responses can be modeled as hyper-Laplacian with 0 < p < 1 [25, 28, 35], which had been adopted in many low level vision problems [13, 36]. [sent-383, score-0.062]
</p><p>51 By using the sparse gradient based image prior, the image deconvolution model can be formulated as mxin21 ? [sent-384, score-0.257]
</p><p>52 pp, (37) where λ is the regularization parameter, D = [Dh, Dv] denotes the gradient operator, and Dh and Dv are the horizontal and vertical gradient operators, respectively. [sent-388, score-0.063]
</p><p>53 We adopt an alternating minimization strategy to solve the problem in Eq. [sent-399, score-0.06]
</p><p>54 In each iteration, given a fixed d, x can be obtained by solving the following subproblem  η2λ  mxin21 ? [sent-401, score-0.068]
</p><p>55 Given a fixed x, let dref = Dx, and d can be obtained by solving the following subproblem:  mdin 2η ? [sent-413, score-0.108]
</p><p>56 (42)  Finally, we summarize the GST based image deconvolution algorithm in Algorithm 3. [sent-429, score-0.169]
</p><p>57 Algorithm 3 is similar to the algorithms in [25, 37], but Wang and Yin [37] only studied the Laplacian prior (p = 1), and Krishnan and Fergus [25] used look-up table (LUT) to solve the subproblem in Eq. [sent-430, score-0.051]
</p><p>58 Here we empirically choose J = 1, making our algorithm very efficient for sparse gradient based image deconvolution. [sent-432, score-0.088]
</p><p>59 Experimental results In this section, we evaluate the proposed GISA on two representative vision applications: image deconvolution and face recognition. [sent-434, score-0.23]
</p><p>60 In image deconvolution experiments, we compare GISA with four state-of-the-art algorithms of ? [sent-435, score-0.169]
</p><p>61 In face recognition, we use GISA to solve the sparse representation-based classification (SRC) model [38], and show that the performance of SRC can be improved by using GISA with p < 1. [sent-456, score-0.151]
</p><p>62 Image deconvolution In image deconvolution, we followed the experiment setting in [25]. [sent-465, score-0.169]
</p><p>63 5 shows the deconvolution results of GISA on a test image by using p = 1and p = 0. [sent-485, score-0.169]
</p><p>64 7 is much better than that with p = 1in terms of suppressing noise and ring effects and preserving edge details, which indicates that non-convex image deconvolution can much improve the deconvolution performance. [sent-488, score-0.338]
</p><p>65 Image deconvolution with GISA: (a) original image, (b) blurry image, (c) deconvolution result (PSNR: 27. [sent-509, score-0.369]
</p><p>66 17) of GISA with p = 1, and (d) deconvolution result (PSNR: 28. [sent-510, score-0.169]
</p><p>67 Face recognition via sparse coding Given a test sample y and the training data matrix X = [X1,X2, ,XK], where Xk, k = 1, 2, ,K, is the sample matrix of class k, Wright et al. [sent-536, score-0.171]
</p><p>68 [38] proposed a sparse representation based classification (SRC) method for face recognition (FR). [sent-537, score-0.147]
</p><p>69 SRC first seeks the solution of the following sparse coding problem:  ×  αˆ = argmαin? [sent-538, score-0.172]
</p><p>70 Then, by simply replacing the soft-thresholding operator in ALM by the proposed GST operator, we can embed the proposed GISA algorithm into the ALM method for solving the SRC model with arbitrary values of p and q. [sent-553, score-0.1]
</p><p>71 We use the principal component analysis (PCA) to reduce the dimensionality of face images, and test the algorithms in both the original image space (1, 024 dimensions) and the PCA subspace with feature dimension 500, 300, and 100, respectively. [sent-559, score-0.061]
</p><p>72 6 we show the recognition rates of SRC versus different p values. [sent-562, score-0.048]
</p><p>73 Table 2 lists the recognition rates of SRC and SRC-p (p = 0. [sent-568, score-0.068]
</p><p>74 One can see that, GISA based SRC-p can always achieve higher recognition rates than the original SRC with p = 1. [sent-570, score-0.048]
</p><p>75 This is because when the face feature dimension is small, the matrix X tends to be redundant. [sent-572, score-0.061]
</p><p>76 Thus, the coding solution should be sparser, and GISA is more probable to obtain the correct solution. [sent-573, score-0.106]
</p><p>77 By choosing q = p = 1, the SRC method would become robust to face corruption/occlusion [38]. [sent-574, score-0.061]
</p><p>78 The face recognition rate of SRC by varying the value of p with GISA. [sent-590, score-0.081]
</p><p>79 09 275  0 < q = p < 1, and embed the proposed GISA into ALM to implement SRC-p, q for robust face recognition. [sent-600, score-0.106]
</p><p>80 Two types of face image corruption are considered: random pixel corruption and random block occlusion. [sent-601, score-0.181]
</p><p>81 Table 3 lists the recognition rates of SRC and SRC-p, q under different ratios of random corruption. [sent-605, score-0.068]
</p><p>82 One can see that SRC-p, q can always outperform SRC for recognizing face images with random corruption. [sent-606, score-0.061]
</p><p>83 Table 4 lists the recognition rates of SRC and SRC-p, q under different ratios of block occlusion. [sent-608, score-0.096]
</p><p>84 Again, SRC-p, q can obtain better recognition rates than SRC for face recognition with random block occlusion. [sent-609, score-0.157]
</p><p>85 Recognition rate (%) on face images with random corruption. [sent-611, score-0.061]
</p><p>86 Recognition rate (%) on face images with block occlusion. [sent-623, score-0.089]
</p><p>87 We proposed a generalized shrinkage/thresholding (GST) function and the associated gen-  eralized iterated shrinkage algorithm (GISA) for ? [sent-629, score-0.196]
</p><p>88 Compared with the state-of-theart methods, GISA is theoretically more solid, easier to understand and more efficient to implement, and it can converge to a more accurate solution. [sent-631, score-0.114]
</p><p>89 Our experimental results on image deconvolution verify the effectiveness and efficiency of GISA, and our experiments on sparse coding based face recognition showed that ? [sent-632, score-0.401]
</p><p>90 A fast iterative shrinkage / thresholding algorithm for linear inverse problems. [sent-658, score-0.308]
</p><p>91 [6]  [7]  [8]  [9]  [10] [11] [12] [13]  [14] [15]  [16] [17]  [18] [19] [20]  [21]  thresholding algorithms for image restoration. [sent-668, score-0.162]
</p><p>92 An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. [sent-742, score-0.218]
</p><p>93 For most large underdetermined systems of linear equations the minimal l1-norm solution is also the sparsest solution. [sent-753, score-0.074]
</p><p>94 Recovering sparse signals with a certain family of nonconvex penalties and DC programming. [sent-774, score-0.108]
</p><p>95 From few to many: Illumination cone models for face recognition under variable lighting and pose. [sent-780, score-0.081]
</p><p>96 q minimization with 0 < q < 1 for sparse solution of under-determined linear systems. [sent-808, score-0.123]
</p><p>97 Acquiring linear subspaces for face recognition under variable lighting. [sent-814, score-0.081]
</p><p>98 An iterative algorithm for fitting nonconvex penalized generalized linear models with grouped predictors. [sent-857, score-0.117]
</p><p>99 1/2 regularization: A thresholding rep-  resentation theory and a fast solver. [sent-897, score-0.162]
</p><p>100 A generalized accelerated proximal gradient approach for total-variation-based image restoration. [sent-912, score-0.064]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gisa', 0.621), ('gst', 0.432), ('pgst', 0.286), ('src', 0.197), ('irls', 0.183), ('deconvolution', 0.169), ('lut', 0.165), ('thresholding', 0.162), ('shrinkage', 0.113), ('tpgst', 0.101), ('converge', 0.092), ('coding', 0.085), ('psnr', 0.075), ('dref', 0.067), ('sgpst', 0.067), ('sparse', 0.066), ('xp', 0.066), ('face', 0.061), ('restoration', 0.059), ('ax', 0.058), ('spgst', 0.05), ('tgpst', 0.05), ('corruption', 0.046), ('softthresholding', 0.045), ('reweighted', 0.044), ('dx', 0.042), ('generalized', 0.042), ('operator', 0.042), ('pp', 0.042), ('nonconvex', 0.042), ('iterated', 0.041), ('solving', 0.041), ('sgn', 0.04), ('alm', 0.038), ('minimization', 0.036), ('analytic', 0.036), ('gp', 0.035), ('deconv', 0.034), ('italian', 0.034), ('mail', 0.034), ('marjanovic', 0.034), ('pitm', 0.034), ('spitm', 0.034), ('iterative', 0.033), ('ist', 0.033), ('solutions', 0.032), ('rule', 0.032), ('blurry', 0.031), ('yale', 0.031), ('underdetermined', 0.03), ('candes', 0.029), ('joshi', 0.029), ('krishnan', 0.029), ('tes', 0.029), ('rates', 0.028), ('deblurring', 0.028), ('block', 0.028), ('mathematics', 0.028), ('implement', 0.028), ('chartrand', 0.028), ('subproblem', 0.027), ('zitnick', 0.027), ('minimum', 0.025), ('siam', 0.025), ('ieee', 0.025), ('iteratively', 0.025), ('szeliski', 0.025), ('solve', 0.024), ('beck', 0.024), ('deno', 0.024), ('iter', 0.024), ('itm', 0.024), ('twist', 0.024), ('problems', 0.023), ('compressed', 0.023), ('theorem', 0.023), ('sparsest', 0.023), ('sensing', 0.023), ('theoretically', 0.022), ('theorems', 0.022), ('gradient', 0.022), ('wavelets', 0.022), ('pure', 0.021), ('yin', 0.021), ('equation', 0.021), ('mxin', 0.021), ('solution', 0.021), ('store', 0.021), ('lists', 0.02), ('fr', 0.02), ('recognition', 0.02), ('statistics', 0.02), ('fergus', 0.02), ('generalization', 0.019), ('modeled', 0.019), ('kong', 0.019), ('regularization', 0.019), ('compressive', 0.019), ('sastry', 0.017), ('embed', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="14-tfidf-1" href="./iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding.html">14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</a></p>
<p>Author: Wangmeng Zuo, Deyu Meng, Lei Zhang, Xiangchu Feng, David Zhang</p><p>Abstract: In many sparse coding based image restoration and image classification problems, using non-convex ?p-norm minimization (0 ≤ p < 1) can often obtain better results than timhei convex 0?1 -norm m 1)ini camniza otfiteonn. Ab naiunm bbeetrt of algorithms, e.g., iteratively reweighted least squares (IRLS), iteratively thresholding method (ITM-?p), and look-up table (LUT), have been proposed for non-convex ?p-norm sparse coding, while some analytic solutions have been suggested for some specific values of p. In this paper, by extending the popular soft-thresholding operator, we propose a generalized iterated shrinkage algorithm (GISA) for ?p-norm non-convex sparse coding. Unlike the analytic solutions, the proposed GISA algorithm is easy to implement, and can be adopted for solving non-convex sparse coding problems with arbitrary p values. Compared with LUT, GISA is more general and does not need to compute and store the look-up tables. Compared with IRLS and ITM-?p, GISA is theoretically more solid and can achieve more accurate solutions. Experiments on image restoration and sparse coding based face recognition are conducted to validate the performance of GISA. ××</p><p>2 0.15459621 <a title="14-tfidf-2" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>Author: Zhaowen Wang, Jianchao Yang, Nasser Nasrabadi, Thomas Huang</p><p>Abstract: Sparse Representation-based Classification (SRC) is a powerful tool in distinguishing signal categories which lie on different subspaces. Despite its wide application to visual recognition tasks, current understanding of SRC is solely based on a reconstructive perspective, which neither offers any guarantee on its classification performance nor provides any insight on how to design a discriminative dictionary for SRC. In this paper, we present a novel perspective towards SRC and interpret it as a margin classifier. The decision boundary and margin of SRC are analyzed in local regions where the support of sparse code is stable. Based on the derived margin, we propose a hinge loss function as the gauge for the classification performance of SRC. A stochastic gradient descent algorithm is implemented to maximize the margin of SRC and obtain more discriminative dictionaries. Experiments validate the effectiveness of the proposed approach in predicting classification performance and improving dictionary quality over reconstructive ones. Classification results competitive with other state-ofthe-art sparse coding methods are reported on several data sets.</p><p>3 0.1321547 <a title="14-tfidf-3" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>Author: Yu-Tseh Chi, Mohsen Ali, Muhammad Rushdi, Jeffrey Ho</p><p>Abstract: This paper proposes a novel approach for sparse coding that further improves upon the sparse representation-based classification (SRC) framework. The proposed framework, Affine-Constrained Group Sparse Coding (ACGSC), extends the current SRC framework to classification problems with multiple input samples. Geometrically, the affineconstrained group sparse coding essentially searches for the vector in the convex hull spanned by the input vectors that can best be sparse coded using the given dictionary. The resulting objectivefunction is still convex and can be efficiently optimized using iterative block-coordinate descent scheme that is guaranteed to converge. Furthermore, we provide a form of sparse recovery result that guarantees, at least theoretically, that the classification performance of the constrained group sparse coding should be at least as good as the group sparse coding. We have evaluated the proposed approach using three different recognition experiments that involve illumination variation of faces and textures, and face recognition under occlusions. Prelimi- nary experiments have demonstrated the effectiveness of the proposed approach, and in particular, the results from the recognition/occlusion experiment are surprisingly accurate and robust.</p><p>4 0.096916795 <a title="14-tfidf-4" href="./iccv-2013-Non-convex_P-Norm_Projection_for_Robust_Sparsity.html">292 iccv-2013-Non-convex P-Norm Projection for Robust Sparsity</a></p>
<p>Author: Mithun Das Gupta, Sanjeev Kumar</p><p>Abstract: In this paper, we investigate the properties of Lp norm (p ≤ 1) within a projection framework. We start with the (KpK T≤ equations of the neoctni-olnin efraarm optimization problem a thnde then use its key properties to arrive at an algorithm for Lp norm projection on the non-negative simplex. We compare with L1projection which needs prior knowledge of the true norm, as well as hard thresholding based sparsificationproposed in recent compressed sensing literature. We show performance improvements compared to these techniques across different vision applications.</p><p>5 0.088891044 <a title="14-tfidf-5" href="./iccv-2013-Deblurring_by_Example_Using_Dense_Correspondence.html">103 iccv-2013-Deblurring by Example Using Dense Correspondence</a></p>
<p>Author: Yoav Hacohen, Eli Shechtman, Dani Lischinski</p><p>Abstract: This paper presents a new method for deblurring photos using a sharp reference example that contains some shared content with the blurry photo. Most previous deblurring methods that exploit information from other photos require an accurately registered photo of the same static scene. In contrast, our method aims to exploit reference images where the shared content may have undergone substantial photometric and non-rigid geometric transformations, as these are the kind of reference images most likely to be found in personal photo albums. Our approach builds upon a recent method for examplebased deblurring using non-rigid dense correspondence (NRDC) [11] and extends it in two ways. First, we suggest exploiting information from the reference image not only for blur kernel estimation, but also as a powerful local prior for the non-blind deconvolution step. Second, we introduce a simple yet robust technique for spatially varying blur estimation, rather than assuming spatially uniform blur. Unlike the aboveprevious method, which hasproven successful only with simple deblurring scenarios, we demonstrate that our method succeeds on a variety of real-world examples. We provide quantitative and qualitative evaluation of our method and show that it outperforms the state-of-the-art.</p><p>6 0.061694976 <a title="14-tfidf-6" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>7 0.061543528 <a title="14-tfidf-7" href="./iccv-2013-Coupled_Dictionary_and_Feature_Space_Learning_with_Applications_to_Cross-Domain_Image_Synthesis_and_Recognition.html">96 iccv-2013-Coupled Dictionary and Feature Space Learning with Applications to Cross-Domain Image Synthesis and Recognition</a></p>
<p>8 0.059135158 <a title="14-tfidf-8" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>9 0.058144733 <a title="14-tfidf-9" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>10 0.055817392 <a title="14-tfidf-10" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>11 0.054205682 <a title="14-tfidf-11" href="./iccv-2013-Coupling_Alignments_with_Recognition_for_Still-to-Video_Face_Recognition.html">97 iccv-2013-Coupling Alignments with Recognition for Still-to-Video Face Recognition</a></p>
<p>12 0.052504126 <a title="14-tfidf-12" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>13 0.052326187 <a title="14-tfidf-13" href="./iccv-2013-Low-Rank_Sparse_Coding_for_Image_Classification.html">258 iccv-2013-Low-Rank Sparse Coding for Image Classification</a></p>
<p>14 0.050909013 <a title="14-tfidf-14" href="./iccv-2013-Efficient_and_Robust_Large-Scale_Rotation_Averaging.html">138 iccv-2013-Efficient and Robust Large-Scale Rotation Averaging</a></p>
<p>15 0.047644999 <a title="14-tfidf-15" href="./iccv-2013-A_New_Image_Quality_Metric_for_Image_Auto-denoising.html">23 iccv-2013-A New Image Quality Metric for Image Auto-denoising</a></p>
<p>16 0.045900065 <a title="14-tfidf-16" href="./iccv-2013-Robust_Matrix_Factorization_with_Unknown_Noise.html">357 iccv-2013-Robust Matrix Factorization with Unknown Noise</a></p>
<p>17 0.045134328 <a title="14-tfidf-17" href="./iccv-2013-Partial_Sum_Minimization_of_Singular_Values_in_RPCA_for_Low-Level_Vision.html">310 iccv-2013-Partial Sum Minimization of Singular Values in RPCA for Low-Level Vision</a></p>
<p>18 0.04425453 <a title="14-tfidf-18" href="./iccv-2013-Unsupervised_Domain_Adaptation_by_Domain_Invariant_Projection.html">435 iccv-2013-Unsupervised Domain Adaptation by Domain Invariant Projection</a></p>
<p>19 0.0417739 <a title="14-tfidf-19" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>20 0.041523661 <a title="14-tfidf-20" href="./iccv-2013-Toward_Guaranteed_Illumination_Models_for_Non-convex_Objects.html">422 iccv-2013-Toward Guaranteed Illumination Models for Non-convex Objects</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.094), (1, 0.007), (2, -0.057), (3, -0.026), (4, -0.111), (5, -0.004), (6, 0.001), (7, 0.005), (8, 0.02), (9, -0.046), (10, -0.015), (11, -0.034), (12, 0.016), (13, -0.037), (14, -0.027), (15, 0.036), (16, -0.014), (17, 0.003), (18, -0.028), (19, 0.007), (20, 0.0), (21, -0.032), (22, -0.0), (23, -0.066), (24, 0.022), (25, -0.01), (26, 0.041), (27, -0.019), (28, 0.047), (29, -0.011), (30, -0.001), (31, -0.018), (32, -0.051), (33, 0.008), (34, 0.033), (35, -0.005), (36, -0.008), (37, 0.005), (38, 0.052), (39, 0.033), (40, -0.003), (41, 0.018), (42, -0.003), (43, 0.086), (44, -0.04), (45, 0.066), (46, 0.02), (47, -0.035), (48, -0.014), (49, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90180695 <a title="14-lsi-1" href="./iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding.html">14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</a></p>
<p>Author: Wangmeng Zuo, Deyu Meng, Lei Zhang, Xiangchu Feng, David Zhang</p><p>Abstract: In many sparse coding based image restoration and image classification problems, using non-convex ?p-norm minimization (0 ≤ p < 1) can often obtain better results than timhei convex 0?1 -norm m 1)ini camniza otfiteonn. Ab naiunm bbeetrt of algorithms, e.g., iteratively reweighted least squares (IRLS), iteratively thresholding method (ITM-?p), and look-up table (LUT), have been proposed for non-convex ?p-norm sparse coding, while some analytic solutions have been suggested for some specific values of p. In this paper, by extending the popular soft-thresholding operator, we propose a generalized iterated shrinkage algorithm (GISA) for ?p-norm non-convex sparse coding. Unlike the analytic solutions, the proposed GISA algorithm is easy to implement, and can be adopted for solving non-convex sparse coding problems with arbitrary p values. Compared with LUT, GISA is more general and does not need to compute and store the look-up tables. Compared with IRLS and ITM-?p, GISA is theoretically more solid and can achieve more accurate solutions. Experiments on image restoration and sparse coding based face recognition are conducted to validate the performance of GISA. ××</p><p>2 0.71912205 <a title="14-lsi-2" href="./iccv-2013-Non-convex_P-Norm_Projection_for_Robust_Sparsity.html">292 iccv-2013-Non-convex P-Norm Projection for Robust Sparsity</a></p>
<p>Author: Mithun Das Gupta, Sanjeev Kumar</p><p>Abstract: In this paper, we investigate the properties of Lp norm (p ≤ 1) within a projection framework. We start with the (KpK T≤ equations of the neoctni-olnin efraarm optimization problem a thnde then use its key properties to arrive at an algorithm for Lp norm projection on the non-negative simplex. We compare with L1projection which needs prior knowledge of the true norm, as well as hard thresholding based sparsificationproposed in recent compressed sensing literature. We show performance improvements compared to these techniques across different vision applications.</p><p>3 0.71273226 <a title="14-lsi-3" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>Author: Yu-Tseh Chi, Mohsen Ali, Muhammad Rushdi, Jeffrey Ho</p><p>Abstract: This paper proposes a novel approach for sparse coding that further improves upon the sparse representation-based classification (SRC) framework. The proposed framework, Affine-Constrained Group Sparse Coding (ACGSC), extends the current SRC framework to classification problems with multiple input samples. Geometrically, the affineconstrained group sparse coding essentially searches for the vector in the convex hull spanned by the input vectors that can best be sparse coded using the given dictionary. The resulting objectivefunction is still convex and can be efficiently optimized using iterative block-coordinate descent scheme that is guaranteed to converge. Furthermore, we provide a form of sparse recovery result that guarantees, at least theoretically, that the classification performance of the constrained group sparse coding should be at least as good as the group sparse coding. We have evaluated the proposed approach using three different recognition experiments that involve illumination variation of faces and textures, and face recognition under occlusions. Prelimi- nary experiments have demonstrated the effectiveness of the proposed approach, and in particular, the results from the recognition/occlusion experiment are surprisingly accurate and robust.</p><p>4 0.66293442 <a title="14-lsi-4" href="./iccv-2013-Partial_Sum_Minimization_of_Singular_Values_in_RPCA_for_Low-Level_Vision.html">310 iccv-2013-Partial Sum Minimization of Singular Values in RPCA for Low-Level Vision</a></p>
<p>Author: Tae-Hyun Oh, Hyeongwoo Kim, Yu-Wing Tai, Jean-Charles Bazin, In So Kweon</p><p>Abstract: Robust Principal Component Analysis (RPCA) via rank minimization is a powerful tool for recovering underlying low-rank structure of clean data corrupted with sparse noise/outliers. In many low-level vision problems, not only it is known that the underlying structure of clean data is low-rank, but the exact rank of clean data is also known. Yet, when applying conventional rank minimization for those problems, the objective function is formulated in a way that does not fully utilize a priori target rank information about the problems. This observation motivates us to investigate whether there is a better alternative solution when using rank minimization. In this paper, instead of minimizing the nuclear norm, we propose to minimize the partial sum of singular values. The proposed objective function implicitly encourages the target rank constraint in rank minimization. Our experimental analyses show that our approach performs better than conventional rank minimization when the number of samples is deficient, while the solutions obtained by the two approaches are almost identical when the number of samples is more than sufficient. We apply our approach to various low-level vision problems, e.g. high dynamic range imaging, photometric stereo and image alignment, and show that our results outperform those obtained by the conventional nuclear norm rank minimization method.</p><p>5 0.62368268 <a title="14-lsi-5" href="./iccv-2013-Unifying_Nuclear_Norm_and_Bilinear_Factorization_Approaches_for_Low-Rank_Matrix_Decomposition.html">434 iccv-2013-Unifying Nuclear Norm and Bilinear Factorization Approaches for Low-Rank Matrix Decomposition</a></p>
<p>Author: Ricardo Cabral, Fernando De_La_Torre, João P. Costeira, Alexandre Bernardino</p><p>Abstract: Low rank models have been widely usedfor the representation of shape, appearance or motion in computer vision problems. Traditional approaches to fit low rank models make use of an explicit bilinear factorization. These approaches benefit from fast numerical methods for optimization and easy kernelization. However, they suffer from serious local minima problems depending on the loss function and the amount/type of missing data. Recently, these lowrank models have alternatively been formulated as convex problems using the nuclear norm regularizer; unlike factorization methods, their numerical solvers are slow and it is unclear how to kernelize them or to impose a rank a priori. This paper proposes a unified approach to bilinear factorization and nuclear norm regularization, that inherits the benefits of both. We analyze the conditions under which these approaches are equivalent. Moreover, based on this analysis, we propose a new optimization algorithm and a “rank continuation ” strategy that outperform state-of-theart approaches for Robust PCA, Structure from Motion and Photometric Stereo with outliers and missing data.</p><p>6 0.60143918 <a title="14-lsi-6" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>7 0.596883 <a title="14-lsi-7" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>8 0.59383619 <a title="14-lsi-8" href="./iccv-2013-Robust_Matrix_Factorization_with_Unknown_Noise.html">357 iccv-2013-Robust Matrix Factorization with Unknown Noise</a></p>
<p>9 0.57942778 <a title="14-lsi-9" href="./iccv-2013-Finding_Causal_Interactions_in_Video_Sequences.html">167 iccv-2013-Finding Causal Interactions in Video Sequences</a></p>
<p>10 0.57244062 <a title="14-lsi-10" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>11 0.57239956 <a title="14-lsi-11" href="./iccv-2013-Low-Rank_Sparse_Coding_for_Image_Classification.html">258 iccv-2013-Low-Rank Sparse Coding for Image Classification</a></p>
<p>12 0.57046849 <a title="14-lsi-12" href="./iccv-2013-Toward_Guaranteed_Illumination_Models_for_Non-convex_Objects.html">422 iccv-2013-Toward Guaranteed Illumination Models for Non-convex Objects</a></p>
<p>13 0.56530148 <a title="14-lsi-13" href="./iccv-2013-Bayesian_Robust_Matrix_Factorization_for_Image_and_Video_Processing.html">60 iccv-2013-Bayesian Robust Matrix Factorization for Image and Video Processing</a></p>
<p>14 0.55431354 <a title="14-lsi-14" href="./iccv-2013-SGTD%3A_Structure_Gradient_and_Texture_Decorrelating_Regularization_for_Image_Decomposition.html">364 iccv-2013-SGTD: Structure Gradient and Texture Decorrelating Regularization for Image Decomposition</a></p>
<p>15 0.52701956 <a title="14-lsi-15" href="./iccv-2013-Abnormal_Event_Detection_at_150_FPS_in_MATLAB.html">34 iccv-2013-Abnormal Event Detection at 150 FPS in MATLAB</a></p>
<p>16 0.5195353 <a title="14-lsi-16" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>17 0.51559222 <a title="14-lsi-17" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>18 0.50737613 <a title="14-lsi-18" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>19 0.5022909 <a title="14-lsi-19" href="./iccv-2013-Fluttering_Pattern_Generation_Using_Modified_Legendre_Sequence_for_Coded_Exposure_Imaging.html">173 iccv-2013-Fluttering Pattern Generation Using Modified Legendre Sequence for Coded Exposure Imaging</a></p>
<p>20 0.50141156 <a title="14-lsi-20" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.054), (7, 0.019), (26, 0.058), (27, 0.035), (31, 0.06), (42, 0.166), (48, 0.015), (64, 0.025), (66, 0.23), (73, 0.043), (78, 0.016), (89, 0.129), (95, 0.011), (97, 0.027), (98, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80928671 <a title="14-lda-1" href="./iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding.html">14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</a></p>
<p>Author: Wangmeng Zuo, Deyu Meng, Lei Zhang, Xiangchu Feng, David Zhang</p><p>Abstract: In many sparse coding based image restoration and image classification problems, using non-convex ?p-norm minimization (0 ≤ p < 1) can often obtain better results than timhei convex 0?1 -norm m 1)ini camniza otfiteonn. Ab naiunm bbeetrt of algorithms, e.g., iteratively reweighted least squares (IRLS), iteratively thresholding method (ITM-?p), and look-up table (LUT), have been proposed for non-convex ?p-norm sparse coding, while some analytic solutions have been suggested for some specific values of p. In this paper, by extending the popular soft-thresholding operator, we propose a generalized iterated shrinkage algorithm (GISA) for ?p-norm non-convex sparse coding. Unlike the analytic solutions, the proposed GISA algorithm is easy to implement, and can be adopted for solving non-convex sparse coding problems with arbitrary p values. Compared with LUT, GISA is more general and does not need to compute and store the look-up tables. Compared with IRLS and ITM-?p, GISA is theoretically more solid and can achieve more accurate solutions. Experiments on image restoration and sparse coding based face recognition are conducted to validate the performance of GISA. ××</p><p>2 0.74862123 <a title="14-lda-2" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>Author: Andy J. Ma, Pong C. Yuen, Jiawei Li</p><p>Abstract: This paper addresses a new person re-identification problem without the label information of persons under non-overlapping target cameras. Given the matched (positive) and unmatched (negative) image pairs from source domain cameras, as well as unmatched (negative) image pairs which can be easily generated from target domain cameras, we propose a Domain Transfer Ranked Support Vector Machines (DTRSVM) method for re-identification under target domain cameras. To overcome the problems introduced due to the absence of matched (positive) image pairs in target domain, we relax the discriminative constraint to a necessary condition only relying on the positive mean in target domain. By estimating the target positive mean using source and target domain data, a new discriminative model with high confidence in target positive mean and low confidence in target negative image pairs is developed. Since the necessary condition may not truly preserve the discriminability, multi-task support vector ranking is proposed to incorporate the training data from source domain with label information. Experimental results show that the proposed DTRSVM outperforms existing methods without using label information in target cameras. And the top 30 rank accuracy can be improved by the proposed method upto 9.40% on publicly available person re-identification datasets.</p><p>3 0.69584614 <a title="14-lda-3" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>Author: Hongteng Xu, Hongyuan Zha</p><p>Abstract: Data sparsity has been a thorny issuefor manifold-based image synthesis, and in this paper we address this critical problem by leveraging ideas from transfer learning. Specifically, we propose methods based on generating auxiliary data in the form of synthetic samples using transformations of the original sparse samples. To incorporate the auxiliary data, we propose a weighted data synthesis method, which adaptively selects from the generated samples for inclusion during the manifold learning process via a weighted iterative algorithm. To demonstrate the feasibility of the proposed method, we apply it to the problem of face image synthesis from sparse samples. Compared with existing methods, the proposed method shows encouraging results with good performance improvements.</p><p>4 0.69384062 <a title="14-lda-4" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>Author: Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, Philip S. Yu</p><p>Abstract: Transfer learning is established as an effective technology in computer visionfor leveraging rich labeled data in the source domain to build an accurate classifier for the target domain. However, most prior methods have not simultaneously reduced the difference in both the marginal distribution and conditional distribution between domains. In this paper, we put forward a novel transfer learning approach, referred to as Joint Distribution Adaptation (JDA). Specifically, JDA aims to jointly adapt both the marginal distribution and conditional distribution in a principled dimensionality reduction procedure, and construct new feature representation that is effective and robustfor substantial distribution difference. Extensive experiments verify that JDA can significantly outperform several state-of-the-art methods on four types of cross-domain image classification problems.</p><p>5 0.69172299 <a title="14-lda-5" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>Author: Chenglong Bao, Jian-Feng Cai, Hui Ji</p><p>Abstract: In recent years, how to learn a dictionary from input images for sparse modelling has been one very active topic in image processing and recognition. Most existing dictionary learning methods consider an over-complete dictionary, e.g. the K-SVD method. Often they require solving some minimization problem that is very challenging in terms of computational feasibility and efficiency. However, if the correlations among dictionary atoms are not well constrained, the redundancy of the dictionary does not necessarily improve the performance of sparse coding. This paper proposed a fast orthogonal dictionary learning method for sparse image representation. With comparable performance on several image restoration tasks, the proposed method is much more computationally efficient than the over-complete dictionary based learning methods.</p><p>6 0.68871844 <a title="14-lda-6" href="./iccv-2013-Correlation_Adaptive_Subspace_Segmentation_by_Trace_Lasso.html">93 iccv-2013-Correlation Adaptive Subspace Segmentation by Trace Lasso</a></p>
<p>7 0.68768853 <a title="14-lda-7" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>8 0.68739438 <a title="14-lda-8" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>9 0.68687373 <a title="14-lda-9" href="./iccv-2013-Robust_Tucker_Tensor_Decomposition_for_Effective_Image_Representation.html">362 iccv-2013-Robust Tucker Tensor Decomposition for Effective Image Representation</a></p>
<p>10 0.68682534 <a title="14-lda-10" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>11 0.68506539 <a title="14-lda-11" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>12 0.68367171 <a title="14-lda-12" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>13 0.68263149 <a title="14-lda-13" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>14 0.68244851 <a title="14-lda-14" href="./iccv-2013-Deep_Learning_Identity-Preserving_Face_Space.html">106 iccv-2013-Deep Learning Identity-Preserving Face Space</a></p>
<p>15 0.68231165 <a title="14-lda-15" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>16 0.6811952 <a title="14-lda-16" href="./iccv-2013-Latent_Multitask_Learning_for_View-Invariant_Action_Recognition.html">231 iccv-2013-Latent Multitask Learning for View-Invariant Action Recognition</a></p>
<p>17 0.68116766 <a title="14-lda-17" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>18 0.68065631 <a title="14-lda-18" href="./iccv-2013-Exemplar-Based_Graph_Matching_for_Robust_Facial_Landmark_Localization.html">149 iccv-2013-Exemplar-Based Graph Matching for Robust Facial Landmark Localization</a></p>
<p>19 0.68063426 <a title="14-lda-19" href="./iccv-2013-Global_Fusion_of_Relative_Motions_for_Robust%2C_Accurate_and_Scalable_Structure_from_Motion.html">184 iccv-2013-Global Fusion of Relative Motions for Robust, Accurate and Scalable Structure from Motion</a></p>
<p>20 0.68056977 <a title="14-lda-20" href="./iccv-2013-Latent_Space_Sparse_Subspace_Clustering.html">232 iccv-2013-Latent Space Sparse Subspace Clustering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
