<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>452 iccv-2013-YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-452" href="#">iccv2013-452</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>452 iccv-2013-YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition</h1>
<br/><p>Source: <a title="iccv-2013-452-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Guadarrama_YouTube2Text_Recognizing_and_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Sergio Guadarrama, Niveda Krishnamoorthy, Girish Malkarnenkar, Subhashini Venugopalan, Raymond Mooney, Trevor Darrell, Kate Saenko</p><p>Abstract: Despite a recent push towards large-scale object recognition, activity recognition remains limited to narrow domains and small vocabularies of actions. In this paper, we tackle the challenge of recognizing and describing activities “in-the-wild”. We present a solution that takes a short video clip and outputs a brief sentence that sums up the main activity in the video, such as the actor, the action and its object. Unlike previous work, our approach works on out-of-domain actions: it does not require training videos of the exact activity. If it cannot find an accurate prediction for a pre-trained model, it finds a less specific answer that is also plausible from a pragmatic standpoint. We use semantic hierarchies learned from the data to help to choose an appropriate level of generalization, and priors learned from web-scale natural language corpora to penalize unlikely combinations of actors/actions/objects; we also use a web-scale language model to “fill in ” novel verbs, i.e. when the verb does not appear in the training set. We evaluate our method on a large YouTube corpus and demonstrate it is able to generate short sentence descriptions of video clips better than baseline approaches.</p><p>Reference: <a title="iccv-2013-452-reference" href="../iccv2013_reference/iccv-2013-YouTube2Text%3A_Recognizing_and_Describing_Arbitrary_Activities_Using_Semantic_Hierarchies_and_Zero-Shot_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We present a solution that takes a short video clip and outputs a brief sentence that sums up the main activity in the video, such as the actor, the action and its object. [sent-3, score-0.494]
</p><p>2 We use semantic hierarchies learned from the data to help to choose an appropriate level of generalization, and priors learned from web-scale natural language corpora to penalize unlikely combinations of actors/actions/objects; we also use a web-scale language model to “fill in ” novel verbs, i. [sent-6, score-0.631]
</p><p>3 when the verb does not appear in the training set. [sent-8, score-0.275]
</p><p>4 We evaluate our method on a large YouTube corpus and demonstrate it  is able to generate short sentence descriptions of video clips better than baseline approaches. [sent-9, score-0.464]
</p><p>5 Introduction Despite recent advances in activity recognition [27, 24, 2, 5] automatic understanding and description of activities “in-the-wild” remains challenging. [sent-11, score-0.384]
</p><p>6 While object recognition methods have gone large-scale, with datasets such as ImageNet[7] and LabelMe [25], activity datasets have lagged behind. [sent-12, score-0.229]
</p><p>7 Most activity recognition methods focus on narrow domains with a handful of actions (e. [sent-13, score-0.295]
</p><p>8 , Figure 1(ab)), in which labelled example videos of all actions (as well as actors and objects) are available for training. [sent-15, score-0.17]
</p><p>9 Contrast these with YouTube videos (Figure 1(c)), where the range of both activities and objects is broad and training data for each label is scarce or unavailable. [sent-16, score-0.257]
</p><p>10 *’  Figure 1: Unlike conventional approaches which recognize a limited set of actions in datasets like (a-b), we recognize a broad range of activities in short YouTube video clips, and generate brief text summaries similar to the humangenerated ones shown in (c). [sent-58, score-0.347]
</p><p>11 Furthermore, methods for generating humanunderstandable natural language descriptions, such as those shown in Figure 1(c), have yet to scale to such broad domains. [sent-59, score-0.168]
</p><p>12 Recent results on activity description in video have been restricted to a small set of actions and objects [12, 2]. [sent-61, score-0.44]
</p><p>13 Large-vocabulary video activity description present unique challenges, including modeling dynamics and actor-action-object relationships from limited training data, as well as dealing with polysemy and ambiguity. [sent-63, score-0.406]
</p><p>14 In this paper, we take steps towards scalable “in-the-  wild” description of short videos by making two observations: 1) that there are several valid ways to describe the same activity, and 2) the description does not need to be 2712  very specific to be useful. [sent-64, score-0.324]
</p><p>15 This is illustrated by Figure 1(c), where the top-left video was described by human annotators (who were asked to describe the main activity) as either “a person cooking ”, “a woman chopping vegetables ”, or “a cook preparing a meal”, etc. [sent-65, score-0.26]
</p><p>16 We see that not only do people use the different verbs (“chop”, “slice”) and nouns (“woman”, “cook”) interchangeably for descriptions, but also use words with varying specificity, i. [sent-66, score-0.548]
</p><p>17 First, we mine the natural sentence descriptions provided by humans to learn semantic relationships. [sent-70, score-0.363]
</p><p>18 Then, based on these relationships, we build semantic hierarchies and develop a model that is able to trade off between more or less general descriptions. [sent-72, score-0.278]
</p><p>19 when the verb does not appear in the training set. [sent-78, score-0.275]
</p><p>20 , given the subject “person”, object “car” and the model prediction “move”, the most likely verb is “drive”. [sent-82, score-0.296]
</p><p>21 proposed a system [10] that maps images and the corresponding textual descriptions to a “meaning” space with an object, action and scene triplet, but deal with a fixed small set of training triplets. [sent-91, score-0.161]
</p><p>22 in [29] used text-mined knowledge to generate descriptions of static images after performing object and scene detection, but do not perform activity recognition. [sent-94, score-0.358]
</p><p>23 The existing work on describing videos with sentences  [12, 17, 13, 8, 14, 5] deals with constrained domains with a limited set of actions or objects, and does not exploit text  ? [sent-95, score-0.362]
</p><p>24 Figure 2: Conventional methods try to predict a caption composed of the most visually likely objects and actions (leaf nodes), whereas our method can predict a less specific phrase that is nonetheless visually plausible and informative. [sent-151, score-0.186]
</p><p>25 The bars inside nodes indicate the posterior probability of the node given the input video (more red and taller indicates higher probability). [sent-152, score-0.196]
</p><p>26 Motwani in [20] ex-  plored how object detection and text mining can aid activity recognition in videos; however, they do not determine a complete SVO triple for describing a video nor generate a full sentential description. [sent-156, score-0.4]
</p><p>27 Our work also differs from all previous description work in that we reason over hierarchical phrases, allowing us to predict more general but more semantically correct sentences when the visual detectors are unsure. [sent-157, score-0.207]
</p><p>28 Overview of our Approach Our overall activity description approach consists of the following main steps. [sent-164, score-0.301]
</p><p>29 We first mine (S,V,O) triplets from the natural language descriptions of the videos, and build a separate semantic hierarchy for each part of the triplet (HS, HV, and HO). [sent-165, score-0.814]
</p><p>30 Third, we learn to predict node triplets over the learned hierarchies by maximizing the semantic similarity to the training data. [sent-167, score-0.622]
</p><p>31 Fourth, we learn a language model from web-scale text corpora and use it as a prior on triplets, to infer verbs missing from our vocabulary. [sent-169, score-0.74]
</p><p>32 We demonstrate our method on a diverse activity dataset. [sent-172, score-0.229]
</p><p>33 Previously published activity recognition methods that work on datasets such as KTH [26], Drinking and Smoking [16] and UCF50 [24] have a very limited recognition vocabulary of activity classes, ranging from 6 to 12 action classes. [sent-173, score-0.458]
</p><p>34 Our dataset on the contrary contains more than 218 different verbs in the human descriptions (see Section 4), and over 241 different objects. [sent-174, score-0.599]
</p><p>35 [4], consisting of 1,970 short video clips, each with an average of 16 natural-language descriptions provided by Amazon Mechanical Turk workers. [sent-176, score-0.242]
</p><p>36 Hierarchical Semantic Model Building the Semantic Hierarchies: We capitalize on the rich linguistic variation in the corpus to learn semantic hierarchies suitable for video activity description. [sent-179, score-0.648]
</p><p>37 We followed Motwani and Mooney’s [20] approach to automatically extract semantic SVO triplets from the human generated sentences. [sent-180, score-0.249]
</p><p>38 We tested different ways to build the hierarchies for subjects, verbs and objects using the idea of distributional clustering [22] and co-occurrence of the labels. [sent-188, score-0.697]
</p><p>39 As can be seen in Figure 3, the learned hierarchies group labels that would be separated in the WordNet hierarchy. [sent-190, score-0.229]
</p><p>40 Thus our learned hierarchies capture better the similarity of nouns and verbs in terms of how they are used to describe activities. [sent-192, score-0.762]
</p><p>41 Defining Semantic Accuracy over Hierarchies: We construct a hierarchy H = (V, E) of labels for each category  ? [sent-193, score-0.19]
</p><p>42 vW e∈ use tewproe measures oeft accuracy, utnhed frirs itt (isse a 0-1 loss defined over the leaf or internal nodes (similar to the accuracy defined in [6]) and the second is based on a similarity function between pairs of nodes in the hierarchy as described below. [sent-228, score-0.454]
</p><p>43 A matching function μLt : V → [0, 1] is then defined over a hierarchy H with respect t Vo a ground truth set leaf nodes Lt ⊂ L (L is the set of all leafs in the  tree) as:  μLt(v) = ml∈Laxt{st(v,l)}  (1)  where st is the similarity between any two nodes in the hierarchy. [sent-229, score-0.454]
</p><p>44 We define a binary accuracy s01 at the leaf nodes of the hierarchy as the 0-1 loss: s01  (v, l) = I[v==l]  (2)  The 0-1 loss can be unduly harsh, for instance if we incorrectly choose “pasta” instead of a “spaghetti” as the object, it should be considered better than choosing “guitar”. [sent-230, score-0.326]
</p><p>45 Then we combine activity and object descriptors using a multi-channel approach [3 1] and pass it to a non-linear SVM [3] (see Section 7). [sent-240, score-0.229]
</p><p>46 As in [18], we use the outputs of the object detections as features and learn leaf node classifiers for each individual Subject, Verb, and Object. [sent-242, score-0.225]
</p><p>47 Optimizing Phrase Specificity for Maximum Semantic Similarity: Once the leaf visual classifiers are trained, we use the hierarchies constructed in Section 4 to predict nodes by trading off specificity with semantic similarity, i. [sent-243, score-0.663]
</p><p>48 The specificity ψH (v) of node in a hierarchy is defined by the decrease in entropy:  ψH(v) = log2|L|  − log2? [sent-246, score-0.336]
</p><p>49 The difference in our case is that, instead of using the WordNet hierarchy, we use the hierarchy learned from the data (as explained in section 4). [sent-249, score-0.156]
</p><p>50 This allows our model to trade off specificity by exploiting the relationships between valid combinations of subjects, verbs and objects, whereas simply fixing a high accuracy can lead to over-generalization (see results in Section 7). [sent-251, score-0.636]
</p><p>51 Since our internal nodes are set of labels, to generate a sentence we must pick a representative word for them. [sent-252, score-0.191]
</p><p>52 Zero-shot Language Model For zero-shot activity recognition, knowledge mined from web-scale textual corpora can help determine unseen verbs for describing the video. [sent-257, score-0.89]
</p><p>53 In order to discover activities that were unseen during training, we expand the top detected verbs with their most similar verbs to generate a larger set of potential verbs for describing the action. [sent-258, score-1.636]
</p><p>54 Textmined likelihoods are then used to determine the activity that best fits the detected objects. [sent-259, score-0.229]
</p><p>55 For example, if “person” and “car” are the top subject and object detections and “move” is the top verb detection, we can expand “move” with similar verbs like “ride” and “drive” to describe the video as “A person is driving a car” without needing any training videos for “ride” or “drive”. [sent-260, score-1.13]
</p><p>56 This idea can be used to expand “coarse” activity detections, obtained by training classifiers on available (possibly limited) activity training data, with “finer” activities unseen at training time. [sent-261, score-0.773]
</p><p>57 We employ language models trained on four large text corpora (English Gigaword - 1200 million words, British National Corpus 100 million words, ukWac 2000 million words, and WaCkypedia EN 800 million words) for obtaining S-V-O triplet likelihoods. [sent-262, score-0.387]
</p><p>58 For zero-shot detection we follow the method suggested –  –  –  in [14] and expand the detections of observed verbs with their most similar verbs from the set of unseen verbs. [sent-263, score-1.032]
</p><p>59 When computing the overall vision score, we make a conditional independence assumption and multiply the probabilities of the subject, activity and object. [sent-265, score-0.229]
</p><p>60 The resulting SVO triplets are then scored using Equation 4 to select the best triplet. [sent-267, score-0.166]
</p><p>61 score = P(S|vid) ∗  ∗  P(O|vid)  P(Vexp|vid) ∗  ∗  Sim(Vexp, Vorig)  (4)  svo likelihood  A template-based approach is utilized for surface realization such that each sentence is of the form: “Determiner (A,An,The) - Subject - Verb (Present, Present Continuous) - Preposition - Determiner - Object. [sent-268, score-0.248]
</p><p>62 ” where the subject, verb and object are obtained from the 2715  content planning stage. [sent-269, score-0.243]
</p><p>63 To get a list of appropriate prepositions, we mine the text corpora for “prep ” dependencies and for every verb-object combination we find the most frequently occurring prepositions. [sent-271, score-0.173]
</p><p>64 The candidate sentences generated using the template above, are ranked for plausi-  bilty using a language model trained on the GoogleNgram corpus and the top ranked sentence is used to describe the video. [sent-272, score-0.41]
</p><p>65 Experimental Setup We split the 1,970 videos in the YouTube corpus into two: (1,300) for training and validation, and (670) for test; splits were contiguous groups of videos by index number to reduce any locality effect in the dataset. [sent-274, score-0.308]
</p><p>66 Multi-channel SVM: For classification we use a non-linear SVM [3] and combine the information from both object and activity features using a multi-channel approach as proposed in [3 1], with a RBF-kernel over the pairwise distances:  K(xi,xj) = exp? [sent-284, score-0.229]
</p><p>67 We use χ2 distance for the activity descriptors Trajectory, HoG, HoF, MBH, and Correlation distance for the objects descriptors DPM, ObjectBank. [sent-288, score-0.229]
</p><p>68 First we evaluate our learned hierarchy on predicting subject, verb and object labels in terms of binary (0-1 loss) accuracy. [sent-295, score-0.433]
</p><p>69 Since the words at higherlevel nodes of our trees do not tend to appear in the human descriptions, here we use the first level of our hierarchies, which group the flat raw labels (see Section 4). [sent-296, score-0.177]
</p><p>70 We see that visual classifiers do significantly better than a triplet-prior baseline (except for subject, for which simply guessing person does very well), and semantic grouping improves performance. [sent-299, score-0.237]
</p><p>71 The flat (FL) baseline predicts the most confident output for each SVM trained over the whole set of labels without any hierarchy or grouping. [sent-303, score-0.224]
</p><p>72 08214 Table 1: Binary 0-1 accuracy of predicting subject, verb and object labels with Prior:most frequent triplet, FL:flat visual classifiers, OU:first level of our semantic hierarchies. [sent-308, score-0.36]
</p><p>73 494151 Table 2: Comparison of WUP Similarity which combines the outputs of the SVMs according to the WordNet hierarchy and chooses the appropriate level of generalization by setting the accuracy to some prespecified value (0. [sent-315, score-0.223]
</p><p>74 Our method (OU) computes a probability distribution over the learned semantic hierarchies and chooses the appropriate level of generalization by optimizing the WUP similarity of the predictions using cross-validation on the training set. [sent-317, score-0.398]
</p><p>75 The table measures the similarity to both the ’Most Common’ gold-standard answer (single triplet for each video), and the ’Valid Answer’ (any of the Subjects, Verbs and Objects mentioned by humans). [sent-318, score-0.216]
</p><p>76 Since the WUP similarity depends on the hierarchy used, to do a fair comparison in Table 2 we use the WUP similarity defined over the WordNet hierarchy for all the methods. [sent-319, score-0.422]
</p><p>77 Our approach predicts words that are more similar on average to the human triplets than either baseline, especially for the most common answer. [sent-320, score-0.236]
</p><p>78 Table 4 shows example sentences generated based on the triplets predicted by the three methods, as well  as (the most common) human annotation. [sent-322, score-0.26]
</p><p>79 We see that HE tends to predict very general words most of the time, whereas FL predicts specific nodes but makes a lot of mistakes. [sent-324, score-0.184]
</p><p>80 In contrast, OU method outputs more general descriptions that are nonetheless informative about the content of the video, what make it more suitable for video retrieval. [sent-325, score-0.236]
</p><p>81 We also conducted an experiment to see if our method can use a language model to learn to describe activities involving verbs for which no training videos are provided. [sent-327, score-0.819]
</p><p>82 We held out a random fraction of verbs during training and judged the system’s ability to still predict them during testing based on subject and object context. [sent-328, score-0.596]
</p><p>83 Figure 4 reports the percentage of unseen verbs  Figure 4: Zero-shot Activity Recognition correctly predicted (0-1 loss) using our model. [sent-329, score-0.517]
</p><p>84 We measure the accuracy over all videos that contain unseen verbs as well as the subset of these videos where the subject and object were correctly identified. [sent-330, score-0.778]
</p><p>85 The results in Figure 4 were averaged across 8 runs, removing a different random set of verbs for zero-shot recognition in each run. [sent-331, score-0.47]
</p><p>86 Since we have no information on the test verbs during training, we cannot assume any priors about their likelihoods. [sent-332, score-0.47]
</p><p>87 So, we compare to a baseline where the system picks a verb uniformly at random (0. [sent-333, score-0.243]
</p><p>88 Even with a large portion of verb models missing, in a reasonable fraction of cases, our language model is still able to “fill in” the correct verb from context. [sent-335, score-0.616]
</p><p>89 The FL, HE and OU algorithms are used to predict SVO triplets for all 670 test videos. [sent-341, score-0.207]
</p><p>90 For each test video, we measure the similarity of its SVO triplet with the triplets of all the test videos to retrieve the 3 most similar videos. [sent-342, score-0.442]
</p><p>91 90 Table 3: Amazon mechanical turker ratings for videos retrieved by FL, HE, OU and ground truth triplets. [sent-350, score-0.148]
</p><p>92 Conclusions Broad-coverage activity recognition has wide application in surveillance and retrieval applications, yet few existing methods work outside limited verb vocabularies. [sent-361, score-0.508]
</p><p>93 We presented a system that takes a short video clip and outputs a brief sentence that sums up the main activity in the video, such as the actor, the action and its object. [sent-362, score-0.494]
</p><p>94 Unlike previous work, our approach has broad verb and object coverage and works on out-of-domain actions: it does not require training videos of the exact activity. [sent-364, score-0.417]
</p><p>95 The semantic hierarchies learned from the data help to choose an appropriate level of generalization, and a prior learned from web-scale natural language corpora penalizes unlikely combinations of actors/actions/objects and allows zero-shot activity recognition. [sent-366, score-0.73]
</p><p>96 We evaluated our method on a large YouTube corpus and demonstrated it was able to generate short sentence descriptions of video clips better than baseline approaches. [sent-367, score-0.464]
</p><p>97 A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object  stitching. [sent-412, score-0.176]
</p><p>98 Natural language description of human activities from video images based on concept hierarchy of actions. [sent-486, score-0.514]
</p><p>99 Generating natural-language video descriptions using text-mined knowl-  edge. [sent-495, score-0.202]
</p><p>100 Improving video activity recognition using object  recognition and text mining. [sent-546, score-0.349]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('verbs', 0.47), ('wup', 0.273), ('verb', 0.243), ('activity', 0.229), ('hierarchies', 0.195), ('triplets', 0.166), ('hierarchy', 0.156), ('language', 0.13), ('svo', 0.13), ('specificity', 0.13), ('descriptions', 0.129), ('sentence', 0.118), ('triplet', 0.117), ('person', 0.11), ('swup', 0.105), ('videos', 0.104), ('youtube', 0.1), ('leaf', 0.097), ('sentences', 0.094), ('wordnet', 0.094), ('corpora', 0.093), ('fl', 0.092), ('linguistics', 0.086), ('dough', 0.084), ('semantic', 0.083), ('activities', 0.083), ('mix', 0.082), ('ou', 0.08), ('chop', 0.074), ('nodes', 0.073), ('video', 0.073), ('description', 0.072), ('corpus', 0.068), ('actions', 0.066), ('egg', 0.065), ('flour', 0.063), ('pasta', 0.063), ('spaghetti', 0.063), ('yolk', 0.063), ('slice', 0.06), ('austin', 0.059), ('vexp', 0.056), ('objectbank', 0.056), ('similarity', 0.055), ('subject', 0.053), ('cut', 0.052), ('pages', 0.052), ('vid', 0.052), ('describing', 0.051), ('node', 0.05), ('motwani', 0.049), ('mooney', 0.049), ('association', 0.048), ('unseen', 0.047), ('text', 0.047), ('expand', 0.045), ('classifiers', 0.044), ('ratings', 0.044), ('answer', 0.044), ('woman', 0.043), ('prepare', 0.043), ('determiner', 0.042), ('dish', 0.042), ('noodle', 0.042), ('ride', 0.042), ('vorig', 0.042), ('subjects', 0.042), ('nouns', 0.042), ('predict', 0.041), ('proceedings', 0.04), ('short', 0.04), ('pascal', 0.039), ('phrase', 0.038), ('english', 0.038), ('ut', 0.038), ('broad', 0.038), ('mbh', 0.037), ('rocks', 0.037), ('words', 0.036), ('valid', 0.036), ('retrieval', 0.036), ('clips', 0.036), ('higherlevel', 0.034), ('krishnamoorthy', 0.034), ('malkarnenkar', 0.034), ('labels', 0.034), ('predicts', 0.034), ('outputs', 0.034), ('cook', 0.034), ('car', 0.033), ('international', 0.033), ('mine', 0.033), ('drive', 0.033), ('chooses', 0.033), ('distributional', 0.032), ('dice', 0.032), ('hv', 0.032), ('hof', 0.032), ('conference', 0.032), ('training', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="452-tfidf-1" href="./iccv-2013-YouTube2Text%3A_Recognizing_and_Describing_Arbitrary_Activities_Using_Semantic_Hierarchies_and_Zero-Shot_Recognition.html">452 iccv-2013-YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition</a></p>
<p>Author: Sergio Guadarrama, Niveda Krishnamoorthy, Girish Malkarnenkar, Subhashini Venugopalan, Raymond Mooney, Trevor Darrell, Kate Saenko</p><p>Abstract: Despite a recent push towards large-scale object recognition, activity recognition remains limited to narrow domains and small vocabularies of actions. In this paper, we tackle the challenge of recognizing and describing activities “in-the-wild”. We present a solution that takes a short video clip and outputs a brief sentence that sums up the main activity in the video, such as the actor, the action and its object. Unlike previous work, our approach works on out-of-domain actions: it does not require training videos of the exact activity. If it cannot find an accurate prediction for a pre-trained model, it finds a less specific answer that is also plausible from a pragmatic standpoint. We use semantic hierarchies learned from the data to help to choose an appropriate level of generalization, and priors learned from web-scale natural language corpora to penalize unlikely combinations of actors/actions/objects; we also use a web-scale language model to “fill in ” novel verbs, i.e. when the verb does not appear in the training set. We evaluate our method on a large YouTube corpus and demonstrate it is able to generate short sentence descriptions of video clips better than baseline approaches.</p><p>2 0.2647281 <a title="452-tfidf-2" href="./iccv-2013-Translating_Video_Content_to_Natural_Language_Descriptions.html">428 iccv-2013-Translating Video Content to Natural Language Descriptions</a></p>
<p>Author: Marcus Rohrbach, Wei Qiu, Ivan Titov, Stefan Thater, Manfred Pinkal, Bernt Schiele</p><p>Abstract: Humans use rich natural language to describe and communicate visual perceptions. In order to provide natural language descriptions for visual content, this paper combines two important ingredients. First, we generate a rich semantic representation of the visual content including e.g. object and activity labels. To predict the semantic representation we learn a CRF to model the relationships between different components of the visual input. And second, we propose to formulate the generation of natural language as a machine translation problem using the semantic representation as source language and the generated sentences as target language. For this we exploit the power of a parallel corpus of videos and textual descriptions and adapt statistical machine translation to translate between our two languages. We evaluate our video descriptions on the TACoS dataset [23], which contains video snippets aligned with sentence descriptions. Using automatic evaluation and human judgments we show significant improvements over several baseline approaches, motivated by prior work. Our translation approach also shows improvements over related work on an image description task.</p><p>3 0.21363805 <a title="452-tfidf-3" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>Author: C. Lawrence Zitnick, Devi Parikh, Lucy Vanderwende</p><p>Abstract: Sentences that describe visual scenes contain a wide variety of information pertaining to the presence of objects, their attributes and their spatial relations. In this paper we learn the visual features that correspond to semantic phrases derived from sentences. Specifically, we extract predicate tuples that contain two nouns and a relation. The relation may take several forms, such as a verb, preposition, adjective or their combination. We model a scene using a Conditional Random Field (CRF) formulation where each node corresponds to an object, and the edges to their relations. We determine the potentials of the CRF using the tuples extracted from the sentences. We generate novel scenes depicting the sentences’ visual meaning by sampling from the CRF. The CRF is also used to score a set of scenes for a text-based image retrieval task. Our results show we can generate (retrieve) scenes that convey the desired semantic meaning, even when scenes (queries) are described by multiple sentences. Significant improvement is found over several baseline approaches.</p><p>4 0.17203102 <a title="452-tfidf-4" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>Author: Vignesh Ramanathan, Percy Liang, Li Fei-Fei</p><p>Abstract: Human action and role recognition play an important part in complex event understanding. State-of-the-art methods learn action and role models from detailed spatio temporal annotations, which requires extensive human effort. In this work, we propose a method to learn such models based on natural language descriptions of the training videos, which are easier to collect and scale with the number of actions and roles. There are two challenges with using this form of weak supervision: First, these descriptions only provide a high-level summary and often do not directly mention the actions and roles occurring in a video. Second, natural language descriptions do not provide spatio temporal annotations of actions and roles. To tackle these challenges, we introduce a topic-based semantic relatedness (SR) measure between a video description and an action and role label, and incorporate it into a posterior regularization objective. Our event recognition system based on these action and role models matches the state-ofthe-art method on the TRECVID-MED11 event kit, despite weaker supervision.</p><p>5 0.16608199 <a title="452-tfidf-5" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>Author: Mohamed R. Amer, Sinisa Todorovic, Alan Fern, Song-Chun Zhu</p><p>Abstract: This paper presents an efficient approach to video parsing. Our videos show a number of co-occurring individual and group activities. To address challenges of the domain, we use an expressive spatiotemporal AND-OR graph (ST-AOG) that jointly models activity parts, their spatiotemporal relations, and context, as well as enables multitarget tracking. The standard ST-AOG inference is prohibitively expensive in our setting, since it would require running a multitude of detectors, and tracking their detections in a long video footage. This problem is addressed by formulating a cost-sensitive inference of ST-AOG as Monte Carlo Tree Search (MCTS). For querying an activity in the video, MCTS optimally schedules a sequence of detectors and trackers to be run, and where they should be applied in the space-time volume. Evaluation on the benchmark datasets demonstrates that MCTS enables two-magnitude speed-ups without compromising accuracy relative to the standard cost-insensitive inference.</p><p>6 0.16249232 <a title="452-tfidf-6" href="./iccv-2013-Find_the_Best_Path%3A_An_Efficient_and_Accurate_Classifier_for_Image_Hierarchies.html">165 iccv-2013-Find the Best Path: An Efficient and Accurate Classifier for Image Hierarchies</a></p>
<p>7 0.16069153 <a title="452-tfidf-7" href="./iccv-2013-From_Large_Scale_Image_Categorization_to_Entry-Level_Categories.html">176 iccv-2013-From Large Scale Image Categorization to Entry-Level Categories</a></p>
<p>8 0.13005291 <a title="452-tfidf-8" href="./iccv-2013-ACTIVE%3A_Activity_Concept_Transitions_in_Video_Event_Classification.html">4 iccv-2013-ACTIVE: Activity Concept Transitions in Video Event Classification</a></p>
<p>9 0.12898374 <a title="452-tfidf-9" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>10 0.11888801 <a title="452-tfidf-10" href="./iccv-2013-Finding_Actors_and_Actions_in_Movies.html">166 iccv-2013-Finding Actors and Actions in Movies</a></p>
<p>11 0.11786854 <a title="452-tfidf-11" href="./iccv-2013-Flattening_Supervoxel_Hierarchies_by_the_Uniform_Entropy_Slice.html">172 iccv-2013-Flattening Supervoxel Hierarchies by the Uniform Entropy Slice</a></p>
<p>12 0.095363215 <a title="452-tfidf-12" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>13 0.094941251 <a title="452-tfidf-13" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>14 0.088982105 <a title="452-tfidf-14" href="./iccv-2013-PhotoOCR%3A_Reading_Text_in_Uncontrolled_Conditions.html">315 iccv-2013-PhotoOCR: Reading Text in Uncontrolled Conditions</a></p>
<p>15 0.087480865 <a title="452-tfidf-15" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>16 0.084969252 <a title="452-tfidf-16" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>17 0.083623871 <a title="452-tfidf-17" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>18 0.083426587 <a title="452-tfidf-18" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>19 0.079341806 <a title="452-tfidf-19" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>20 0.076396689 <a title="452-tfidf-20" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.19), (1, 0.139), (2, 0.03), (3, 0.042), (4, 0.096), (5, 0.048), (6, 0.008), (7, -0.041), (8, 0.012), (9, -0.026), (10, 0.045), (11, -0.056), (12, 0.013), (13, 0.044), (14, 0.035), (15, 0.027), (16, -0.053), (17, -0.018), (18, -0.056), (19, 0.045), (20, -0.065), (21, -0.005), (22, 0.006), (23, 0.076), (24, -0.032), (25, -0.021), (26, 0.066), (27, -0.097), (28, -0.108), (29, -0.069), (30, 0.127), (31, -0.279), (32, -0.032), (33, -0.138), (34, -0.049), (35, -0.123), (36, -0.089), (37, 0.077), (38, -0.175), (39, -0.05), (40, -0.065), (41, 0.053), (42, -0.055), (43, -0.036), (44, -0.0), (45, -0.018), (46, 0.083), (47, -0.045), (48, -0.011), (49, 0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92906702 <a title="452-lsi-1" href="./iccv-2013-YouTube2Text%3A_Recognizing_and_Describing_Arbitrary_Activities_Using_Semantic_Hierarchies_and_Zero-Shot_Recognition.html">452 iccv-2013-YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition</a></p>
<p>Author: Sergio Guadarrama, Niveda Krishnamoorthy, Girish Malkarnenkar, Subhashini Venugopalan, Raymond Mooney, Trevor Darrell, Kate Saenko</p><p>Abstract: Despite a recent push towards large-scale object recognition, activity recognition remains limited to narrow domains and small vocabularies of actions. In this paper, we tackle the challenge of recognizing and describing activities “in-the-wild”. We present a solution that takes a short video clip and outputs a brief sentence that sums up the main activity in the video, such as the actor, the action and its object. Unlike previous work, our approach works on out-of-domain actions: it does not require training videos of the exact activity. If it cannot find an accurate prediction for a pre-trained model, it finds a less specific answer that is also plausible from a pragmatic standpoint. We use semantic hierarchies learned from the data to help to choose an appropriate level of generalization, and priors learned from web-scale natural language corpora to penalize unlikely combinations of actors/actions/objects; we also use a web-scale language model to “fill in ” novel verbs, i.e. when the verb does not appear in the training set. We evaluate our method on a large YouTube corpus and demonstrate it is able to generate short sentence descriptions of video clips better than baseline approaches.</p><p>2 0.88587463 <a title="452-lsi-2" href="./iccv-2013-Translating_Video_Content_to_Natural_Language_Descriptions.html">428 iccv-2013-Translating Video Content to Natural Language Descriptions</a></p>
<p>Author: Marcus Rohrbach, Wei Qiu, Ivan Titov, Stefan Thater, Manfred Pinkal, Bernt Schiele</p><p>Abstract: Humans use rich natural language to describe and communicate visual perceptions. In order to provide natural language descriptions for visual content, this paper combines two important ingredients. First, we generate a rich semantic representation of the visual content including e.g. object and activity labels. To predict the semantic representation we learn a CRF to model the relationships between different components of the visual input. And second, we propose to formulate the generation of natural language as a machine translation problem using the semantic representation as source language and the generated sentences as target language. For this we exploit the power of a parallel corpus of videos and textual descriptions and adapt statistical machine translation to translate between our two languages. We evaluate our video descriptions on the TACoS dataset [23], which contains video snippets aligned with sentence descriptions. Using automatic evaluation and human judgments we show significant improvements over several baseline approaches, motivated by prior work. Our translation approach also shows improvements over related work on an image description task.</p><p>3 0.73853815 <a title="452-lsi-3" href="./iccv-2013-From_Large_Scale_Image_Categorization_to_Entry-Level_Categories.html">176 iccv-2013-From Large Scale Image Categorization to Entry-Level Categories</a></p>
<p>Author: Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C. Berg, Tamara L. Berg</p><p>Abstract: Entry level categories the labels people will use to name an object were originally defined and studied by psychologists in the 1980s. In this paper we study entrylevel categories at a large scale and learn the first models for predicting entry-level categories for images. Our models combine visual recognition predictions with proxies for word “naturalness ” mined from the enormous amounts of text on the web. We demonstrate the usefulness of our models for predicting nouns (entry-level words) associated with images by people. We also learn mappings between concepts predicted by existing visual recognition systems and entry-level concepts that could be useful for improving human-focused applications such as natural language image description or retrieval. – –</p><p>4 0.67468297 <a title="452-lsi-4" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>Author: C. Lawrence Zitnick, Devi Parikh, Lucy Vanderwende</p><p>Abstract: Sentences that describe visual scenes contain a wide variety of information pertaining to the presence of objects, their attributes and their spatial relations. In this paper we learn the visual features that correspond to semantic phrases derived from sentences. Specifically, we extract predicate tuples that contain two nouns and a relation. The relation may take several forms, such as a verb, preposition, adjective or their combination. We model a scene using a Conditional Random Field (CRF) formulation where each node corresponds to an object, and the edges to their relations. We determine the potentials of the CRF using the tuples extracted from the sentences. We generate novel scenes depicting the sentences’ visual meaning by sampling from the CRF. The CRF is also used to score a set of scenes for a text-based image retrieval task. Our results show we can generate (retrieve) scenes that convey the desired semantic meaning, even when scenes (queries) are described by multiple sentences. Significant improvement is found over several baseline approaches.</p><p>5 0.60973471 <a title="452-lsi-5" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>Author: Mohamed Elhoseiny, Babak Saleh, Ahmed Elgammal</p><p>Abstract: The main question we address in this paper is how to use purely textual description of categories with no training images to learn visual classifiers for these categories. We propose an approach for zero-shot learning of object categories where the description of unseen categories comes in the form of typical text such as an encyclopedia entry, without the need to explicitly defined attributes. We propose and investigate two baseline formulations, based on regression and domain adaptation. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the classifier parameters for new classes. We applied the proposed approach on two fine-grained categorization datasets, and the results indicate successful classifier prediction.</p><p>6 0.59961534 <a title="452-lsi-6" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>7 0.55178815 <a title="452-lsi-7" href="./iccv-2013-Fingerspelling_Recognition_with_Semi-Markov_Conditional_Random_Fields.html">170 iccv-2013-Fingerspelling Recognition with Semi-Markov Conditional Random Fields</a></p>
<p>8 0.5486176 <a title="452-lsi-8" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>9 0.49675828 <a title="452-lsi-9" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>10 0.48998955 <a title="452-lsi-10" href="./iccv-2013-Estimating_the_Material_Properties_of_Fabric_from_Video.html">145 iccv-2013-Estimating the Material Properties of Fabric from Video</a></p>
<p>11 0.48035187 <a title="452-lsi-11" href="./iccv-2013-ACTIVE%3A_Activity_Concept_Transitions_in_Video_Event_Classification.html">4 iccv-2013-ACTIVE: Activity Concept Transitions in Video Event Classification</a></p>
<p>12 0.47667149 <a title="452-lsi-12" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<p>13 0.46600682 <a title="452-lsi-13" href="./iccv-2013-Find_the_Best_Path%3A_An_Efficient_and_Accurate_Classifier_for_Image_Hierarchies.html">165 iccv-2013-Find the Best Path: An Efficient and Accurate Classifier for Image Hierarchies</a></p>
<p>14 0.46257409 <a title="452-lsi-14" href="./iccv-2013-Finding_Actors_and_Actions_in_Movies.html">166 iccv-2013-Finding Actors and Actions in Movies</a></p>
<p>15 0.43614742 <a title="452-lsi-15" href="./iccv-2013-Flattening_Supervoxel_Hierarchies_by_the_Uniform_Entropy_Slice.html">172 iccv-2013-Flattening Supervoxel Hierarchies by the Uniform Entropy Slice</a></p>
<p>16 0.4271349 <a title="452-lsi-16" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<p>17 0.4094134 <a title="452-lsi-17" href="./iccv-2013-How_Do_You_Tell_a_Blackbird_from_a_Crow%3F.html">202 iccv-2013-How Do You Tell a Blackbird from a Crow?</a></p>
<p>18 0.40362543 <a title="452-lsi-18" href="./iccv-2013-Learning_to_Rank_Using_Privileged_Information.html">248 iccv-2013-Learning to Rank Using Privileged Information</a></p>
<p>19 0.3608169 <a title="452-lsi-19" href="./iccv-2013-Manipulation_Pattern_Discovery%3A_A_Nonparametric_Bayesian_Approach.html">260 iccv-2013-Manipulation Pattern Discovery: A Nonparametric Bayesian Approach</a></p>
<p>20 0.35997349 <a title="452-lsi-20" href="./iccv-2013-Coarse-to-Fine_Semantic_Video_Segmentation_Using_Supervoxel_Trees.html">76 iccv-2013-Coarse-to-Fine Semantic Video Segmentation Using Supervoxel Trees</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.062), (7, 0.01), (8, 0.031), (12, 0.064), (26, 0.076), (31, 0.037), (34, 0.026), (42, 0.092), (64, 0.047), (73, 0.028), (78, 0.01), (86, 0.248), (89, 0.156), (98, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77710229 <a title="452-lda-1" href="./iccv-2013-YouTube2Text%3A_Recognizing_and_Describing_Arbitrary_Activities_Using_Semantic_Hierarchies_and_Zero-Shot_Recognition.html">452 iccv-2013-YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition</a></p>
<p>Author: Sergio Guadarrama, Niveda Krishnamoorthy, Girish Malkarnenkar, Subhashini Venugopalan, Raymond Mooney, Trevor Darrell, Kate Saenko</p><p>Abstract: Despite a recent push towards large-scale object recognition, activity recognition remains limited to narrow domains and small vocabularies of actions. In this paper, we tackle the challenge of recognizing and describing activities “in-the-wild”. We present a solution that takes a short video clip and outputs a brief sentence that sums up the main activity in the video, such as the actor, the action and its object. Unlike previous work, our approach works on out-of-domain actions: it does not require training videos of the exact activity. If it cannot find an accurate prediction for a pre-trained model, it finds a less specific answer that is also plausible from a pragmatic standpoint. We use semantic hierarchies learned from the data to help to choose an appropriate level of generalization, and priors learned from web-scale natural language corpora to penalize unlikely combinations of actors/actions/objects; we also use a web-scale language model to “fill in ” novel verbs, i.e. when the verb does not appear in the training set. We evaluate our method on a large YouTube corpus and demonstrate it is able to generate short sentence descriptions of video clips better than baseline approaches.</p><p>2 0.73293972 <a title="452-lda-2" href="./iccv-2013-Coarse-to-Fine_Semantic_Video_Segmentation_Using_Supervoxel_Trees.html">76 iccv-2013-Coarse-to-Fine Semantic Video Segmentation Using Supervoxel Trees</a></p>
<p>Author: Aastha Jain, Shuanak Chatterjee, René Vidal</p><p>Abstract: We propose an exact, general and efficient coarse-to-fine energy minimization strategy for semantic video segmentation. Our strategy is based on a hierarchical abstraction of the supervoxel graph that allows us to minimize an energy defined at the finest level of the hierarchy by minimizing a series of simpler energies defined over coarser graphs. The strategy is exact, i.e., it produces the same solution as minimizing over the finest graph. It is general, i.e., it can be used to minimize any energy function (e.g., unary, pairwise, and higher-order terms) with any existing energy minimization algorithm (e.g., graph cuts and belief propagation). It also gives significant speedups in inference for several datasets with varying degrees of spatio-temporal continuity. We also discuss the strengths and weaknesses of our strategy relative to existing hierarchical approaches, and the kinds of image and video data that provide the best speedups.</p><p>3 0.65497482 <a title="452-lda-3" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>Author: Mohamed Elhoseiny, Babak Saleh, Ahmed Elgammal</p><p>Abstract: The main question we address in this paper is how to use purely textual description of categories with no training images to learn visual classifiers for these categories. We propose an approach for zero-shot learning of object categories where the description of unseen categories comes in the form of typical text such as an encyclopedia entry, without the need to explicitly defined attributes. We propose and investigate two baseline formulations, based on regression and domain adaptation. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the classifier parameters for new classes. We applied the proposed approach on two fine-grained categorization datasets, and the results indicate successful classifier prediction.</p><p>4 0.65419304 <a title="452-lda-4" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>Author: Michael Van_Den_Bergh, Gemma Roig, Xavier Boix, Santiago Manen, Luc Van_Gool</p><p>Abstract: Superpixel and objectness algorithms are broadly used as a pre-processing step to generate support regions and to speed-up further computations. Recently, many algorithms have been extended to video in order to exploit the temporal consistency between frames. However, most methods are computationally too expensive for real-time applications. We introduce an online, real-time video superpixel algorithm based on the recently proposed SEEDS superpixels. A new capability is incorporated which delivers multiple diverse samples (hypotheses) of superpixels in the same image or video sequence. The multiple samples are shown to provide a strong cue to efficiently measure the objectness of image windows, and we introduce the novel concept of objectness in temporal windows. Experiments show that the video superpixels achieve comparable performance to state-of-the-art offline methods while running at 30 fps on a single 2.8 GHz i7 CPU. State-of-the-art performance on objectness is also demonstrated, yet orders of magnitude faster and extended to temporal windows in video.</p><p>5 0.65174663 <a title="452-lda-5" href="./iccv-2013-Flattening_Supervoxel_Hierarchies_by_the_Uniform_Entropy_Slice.html">172 iccv-2013-Flattening Supervoxel Hierarchies by the Uniform Entropy Slice</a></p>
<p>Author: Chenliang Xu, Spencer Whitt, Jason J. Corso</p><p>Abstract: Supervoxel hierarchies provide a rich multiscale decomposition of a given video suitable for subsequent processing in video analysis. The hierarchies are typically computed by an unsupervised process that is susceptible to undersegmentation at coarse levels and over-segmentation at fine levels, which make it a challenge to adopt the hierarchies for later use. In this paper, we propose the first method to overcome this limitation and flatten the hierarchy into a single segmentation. Our method, called the uniform entropy slice, seeks a selection of supervoxels that balances the relative level of information in the selected supervoxels based on some post hoc feature criterion such as objectness. For example, with this criterion, in regions nearby objects, our method prefers finer supervoxels to capture the local details, but in regions away from any objects we prefer coarser supervoxels. We formulate the uniform entropy slice as a binary quadratic program and implement four different feature criteria, both unsupervised and supervised, to drive the flattening. Although we apply it only to supervoxel hierarchies in this paper, our method is generally applicable to segmentation tree hierarchies. Our experiments demonstrate both strong qualitative performance and superior quantitative performance to state of the art baselines on benchmark internet videos.</p><p>6 0.65134978 <a title="452-lda-6" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>7 0.64630437 <a title="452-lda-7" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>8 0.64544696 <a title="452-lda-8" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>9 0.64272243 <a title="452-lda-9" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>10 0.64254951 <a title="452-lda-10" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>11 0.64135462 <a title="452-lda-11" href="./iccv-2013-POP%3A_Person_Re-identification_Post-rank_Optimisation.html">305 iccv-2013-POP: Person Re-identification Post-rank Optimisation</a></p>
<p>12 0.64085925 <a title="452-lda-12" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>13 0.63974518 <a title="452-lda-13" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>14 0.63932776 <a title="452-lda-14" href="./iccv-2013-Efficient_Pedestrian_Detection_by_Directly_Optimizing_the_Partial_Area_under_the_ROC_Curve.html">136 iccv-2013-Efficient Pedestrian Detection by Directly Optimizing the Partial Area under the ROC Curve</a></p>
<p>15 0.63873762 <a title="452-lda-15" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>16 0.63863254 <a title="452-lda-16" href="./iccv-2013-3D_Sub-query_Expansion_for_Improving_Sketch-Based_Multi-view_Image_Retrieval.html">3 iccv-2013-3D Sub-query Expansion for Improving Sketch-Based Multi-view Image Retrieval</a></p>
<p>17 0.63763869 <a title="452-lda-17" href="./iccv-2013-Target-Driven_Moire_Pattern_Synthesis_by_Phase_Modulation.html">413 iccv-2013-Target-Driven Moire Pattern Synthesis by Phase Modulation</a></p>
<p>18 0.63751572 <a title="452-lda-18" href="./iccv-2013-Beyond_Hard_Negative_Mining%3A_Efficient_Detector_Learning_via_Block-Circulant_Decomposition.html">61 iccv-2013-Beyond Hard Negative Mining: Efficient Detector Learning via Block-Circulant Decomposition</a></p>
<p>19 0.63585329 <a title="452-lda-19" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>20 0.63584667 <a title="452-lda-20" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
