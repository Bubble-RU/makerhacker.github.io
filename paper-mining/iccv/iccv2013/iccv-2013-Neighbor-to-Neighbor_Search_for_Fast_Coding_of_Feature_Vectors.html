<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-287" href="#">iccv2013-287</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</h1>
<br/><p>Source: <a title="iccv-2013-287-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Inoue_Neighbor-to-Neighbor_Search_for_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Nakamasa Inoue, Koichi Shinoda</p><p>Abstract: Assigning a visual code to a low-level image descriptor, which we call code assignment, is the most computationally expensive part of image classification algorithms based on the bag of visual word (BoW) framework. This paper proposes a fast computation method, Neighbor-toNeighbor (NTN) search, for this code assignment. Based on the fact that image features from an adjacent region are usually similar to each other, this algorithm effectively reduces the cost of calculating the distance between a codeword and a feature vector. This method can be applied not only to a hard codebook constructed by vector quantization (NTN-VQ), but also to a soft codebook, a Gaussian mixture model (NTN-GMM). We evaluated this method on the PASCAL VOC 2007 classification challenge task. NTN-VQ reduced the assignment cost by 77.4% in super-vector coding, and NTN-GMM reduced it by 89.3% in Fisher-vector coding, without any significant degradation in classification performance.</p><p>Reference: <a title="iccv-2013-287-reference" href="../iccv2013_reference/iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Based on the fact that image features from an adjacent region are usually similar to each other, this algorithm effectively reduces the cost of calculating the distance between a codeword and a feature vector. [sent-8, score-0.216]
</p><p>2 This method can be applied not only to a hard codebook constructed by vector quantization (NTN-VQ), but also to a soft codebook, a Gaussian mixture model (NTN-GMM). [sent-9, score-0.399]
</p><p>3 Examples of these algorithms include assigning visual words to low-level image descriptors [1], finding the closest matches for image mosaicing [2], or searching for nearest neighbor shapes to 3D shape models [3]. [sent-16, score-0.213]
</p><p>4 In hard VQ, each input vector is assigned to its closest codeword. [sent-18, score-0.138]
</p><p>5 In soft VQ, each input vector is assigned to more than one codewords in a soft weighting manner typically depending on distance between the input vector and a codeword. [sent-19, score-0.473]
</p><p>6 A typical example is a Gaussian mixture model (GMM) in which each codeword has a covariance matrix and a weighting coefficient. [sent-21, score-0.191]
</p><p>7 One of the advantages of soft VQ is that it reduces quantization errors. [sent-22, score-0.147]
</p><p>8 However, there is a trade-off between speed and accuracy: soft VQ is more accurate but slower than hard VQ. [sent-23, score-0.171]
</p><p>9 NTN search assigns a code to an input vector from a neighbor vector to a neighbor vector. [sent-29, score-0.455]
</p><p>10 A typical example of a neighbor vector is a descriptor xj adjacent to a descriptor xj−1 where image descriptors are densely sampled from an image. [sent-30, score-0.499]
</p><p>11 For hard VQ, approximate nearest neighbor (ANN) algorithms such as the best bin first search [4], randomized kd-trees [5], hierarchical k-means tree [6] are known to provide speed-ups with only minor loss in accuracy. [sent-34, score-0.327]
</p><p>12 In dense SIFT, two adjacent descriptors are often assigned to the same codeword since they are similar to each other. [sent-40, score-0.208]
</p><p>13 This algorithm, assuming that 1) a set of neighbor vectors of each input vector are defined and 2) an input vector and its neighbor vector are similar, skips some distance calculations between a neighbor vector and a codeword. [sent-43, score-0.708]
</p><p>14 This algorithm effectively utilizes a triangle inequal1233  ity for the distances between neighbor vectors. [sent-44, score-0.151]
</p><p>15 We apply NTN search to hard VQ (NTN-VQ) and a GMM based soft VQ (NTN-GMM). [sent-45, score-0.192]
</p><p>16 A codebook is typically trained by using kmeans algorithm. [sent-54, score-0.146]
</p><p>17 Hard VQ costs O(K) to assign one of K codewords to an input vector in a straightforward way. [sent-55, score-0.151]
</p><p>18 Lowe [2] uses a kd-tree for nearest neighbor search of SIFT descriptors. [sent-59, score-0.213]
</p><p>19 Muja and Lowe [6] have proposed an automatic selection method from the recent two approximate nearest neighbor (ANN) algorithms: randomized kd-trees [5], and hierarchical k-means tree [6]. [sent-60, score-0.221]
</p><p>20 Soft VQ, which assigns more than one codewords to an input vector, has been proposed to reduce quantization errors in hard VQ. [sent-63, score-0.255]
</p><p>21 For example, a Gaussian mixture model (GMM) [12] provides soft weighting based on the ratios of Gaussian probabilities. [sent-64, score-0.162]
</p><p>22 Sparse coding [13] assigns  several tens of codewords to an input vector by solving a constrained least square fitting problem. [sent-66, score-0.302]
</p><p>23 [14] introduced k-nearest neighbor (k-NN) search as the preprocessing to the sparse coding. [sent-69, score-0.182]
</p><p>24 [15] compared recent image representations using these hard/soft VQ algorithms and reported that the Fisher-vector (FV) coding [9] is the best and the super-vector (SV) coding [8] is the second in terms of image classification accuracy. [sent-72, score-0.287]
</p><p>25 FV and SV use a GMM based soft VQ and hard VQ to assign codewords to image descriptors, respectively. [sent-73, score-0.218]
</p><p>26 The speed of these methods is faster than that of the other methods since their codebook size is relatively small (256 ≤ K ≤ 1024). [sent-74, score-0.153]
</p><p>27 However, the coding step itisv setillyl tsmhea ltlim (2e5-b6ot ≤tle Knec ≤k 1of0 a pipeline efvoerr extracting image representations. [sent-75, score-0.152]
</p><p>28 Red bars: descriptors that have the same visual word as a neighbor descriptor. [sent-78, score-0.182]
</p><p>29 4% of computational time in FV coding and SV coding, respectively (see Section 5, Figure 11). [sent-86, score-0.156]
</p><p>30 Let X be a set of input vectors and B(x) be a set of neighbor vectors for an input vector x ∈ X. [sent-91, score-0.31]
</p><p>31 The NTN search assumes that a neighbor vveeccttoorr xin ∈B( Xx). [sent-92, score-0.182]
</p><p>32 i sT hseim NilaTrN Nto s x, acnhd a tshsuatm tehse number of neighbor vectors is smaller than the codebook size. [sent-93, score-0.281]
</p><p>33 Here, B(x) is a set of the four descriptors adjacent to a descriptor x (Figure 1 and Figure 2) or a set of descriptors in the same pre-segmented region. [sent-95, score-0.175]
</p><p>34 In NTN search, input vectors are ordered from a neighbor vector to a neighbor vector to skip distance calculations for some input vectors based on a triangle inequality. [sent-96, score-0.651]
</p><p>35 In the initialization step  for j  =t {  1μ, xj is randomly selected from X. [sent-101, score-0.202]
</p><p>36 Its code vj is  determined as  vj = argmkindjk,  (1)  where distance djk is calculated for each k  = ? [sent-102, score-0.413]
</p><p>37 (a) Initialization step: distance from an input vector x1 to each codeword is calculated. [sent-109, score-0.205]
</p><p>38 (b) STEP 1: the next input vector x2 which minimizes Δ12 is selected from neighbor vectors. [sent-110, score-0.195]
</p><p>39 (d) STEP 2-2: a lower bound d21 = d11 − δΔ12 is calculated where is a parameter, calculation of d21 is skipped if d21 ≥ d2k∗ . [sent-112, score-0.185]
</p><p>40 (e),(f),(g): STEP 1, 2-1, and 2-2 for xj (j > 2), respectively. [sent-113, score-0.202]
</p><p>41 In (g), accumulated distance Δij between xi and xj is used to obtain≥ a ldower  δ  bound djk = dik  −  δΔij in Eq. [sent-114, score-0.813]
</p><p>42 , and  xj  =  argmin Δ(x) , x∈B(xj−1 )∩X¯  (3)  where X¯ = X \ {x1, · · · , xj−1 } is a set of remaining input vectorsX. [sent-118, score-0.27]
</p><p>43 I f= B(xj−1 ) ,∩· ·X·¯ , =x t}h iesn a xj ti so randomly picked from X¯. [sent-119, score-0.202]
</p><p>44 2-2) For k = 1, 2, · · · , k∗ − 1, k∗ + 1, · · · , K, calculate a l2o-2w)er F obrou knd = djk 2fo,·r· djk as −fol 1lo,kws. [sent-122, score-0.491]
</p><p>45 +  ∅  =  djk dik − δΔij, (4) where iis the index of the input vector whose distance dik has been calculated, δ is a parameter, and Δij is an accumulated distance from xi to xj . [sent-123, score-0.973]
</p><p>46 If djk ≥ djk∗ then skip calculation of djk, otherwise calculate djk. [sent-125, score-0.376]
</p><p>47 (STEP 3: Output a code) Calculate vj  = argmindjk,  (5)  where E is a set of indices of codewords whose distance to xj is calculated in STEP 2. [sent-126, score-0.404]
</p><p>48 For a given xj, let’s go back to the previous input vector xi (i < j) whose distance dik has been calculated (Figure 3 (g)). [sent-129, score-0.353]
</p><p>49 Take the maximum such index iand let Δij be an accumulated distance  between  xi  and  xj  given by  ? [sent-130, score-0.406]
</p><p>50 i+ 1  The triangle inequality gives dik − Δij ≤ djk ≤ dik  + Δij. [sent-135, score-0.481]
</p><p>51 δN ≥ote δ that the result of coding by this algorithm is exactly the same as that by the original hard VQ in this case. [sent-141, score-0.18]
</p><p>52 Then, the lower bound is efficiently updated from the previous lower bound by  δ  djk  = dj−1,k  − δ? [sent-145, score-0.358]
</p><p>53 (9)  The lower bound is obtained by only one distance calculation from xj−1 to xj, which is already calculated in STEP 1. [sent-148, score-0.199]
</p><p>54 1 summarizes the neighbor-to-neighbor (NTN) search for hard VQ which outputs assigned codes for each input vector quickly. [sent-151, score-0.194]
</p><p>55 NTN Search for Gaussian Mixture Models  A Gaussian mixture model (GMM) is an extension of hard VQ to a probabilistic framework since it provides a soft assignment of codewords to an input vector. [sent-168, score-0.383]
</p><p>56 The algorithm structure of NTN-GMM is the same as NTNVQ, but instead of a lower bound of distance for NTN-VQ, an upper bound of a Gaussian probability is calculated for NTN-GMM. [sent-170, score-0.233]
</p><p>57 A code cjk for an input vector xj (j = 1, 2, · · · , N) to the k-th codeword is given by  cjk=? [sent-172, score-0.443]
</p><p>58 Our empirical observation shows that the distribution of pjk over all the codewords is peaky, i. [sent-185, score-0.392]
</p><p>59 , for each input vector xj, a few pjk ’s have a large value and the others do not. [sent-187, score-0.379]
</p><p>60 If xj is similar to xj−1 , the “peak” is shifting gradually as j increments. [sent-188, score-0.202]
</p><p>61 For a given xj, let xi (i < j) be the previous input vector whose Gaussian probability pik has been calculated. [sent-191, score-0.236]
</p><p>62 The idea is to ignore the difference between pjk and pik and assume pjk = pik for k ∈ ∩ to skip calculation  Gi(b) Gj(b)  Gj(b)  of pjk. [sent-192, score-0.914]
</p><p>63 Here, is a set of mixture components in the “bottom” of the distribution, which we call a bottom set, given by  Gj(b)  = {k : pjk < pth}. [sent-193, score-0.384]
</p><p>64 Thus, we introduce an upper bound pjk of a probability pjk (see Appendix for details) given by  Gj(b)  pjk = pik exp (δikΔij) . [sent-196, score-1.12]
</p><p>65 Note that this upper bound is obtained efficiently from a previous upper bound by  pjk = pj−1,k exp (δik ? [sent-203, score-0.503]
</p><p>66 Finally, instead of the intersection of bottom sets  Gj(b) , its subset  (15)  Gi(b)  ∩  Uij given by Uij = {k : pjk < pth},  (16)  is used for determining mixture components to skip calculation of pjk (Figure 4). [sent-206, score-0.807]
</p><p>67 2 summarizes the NTN search for a GMM which outputs soft codes for each input vector quickly. [sent-214, score-0.211]
</p><p>68 0% of the computational cost in coding using 1236  Figure 4. [sent-216, score-0.197]
</p><p>69 Calculation of a Gaussian probability pjk is skipped for k ∈ Uik . [sent-218, score-0.338]
</p><p>70 The ANN-VQ uses a fast library for approximate nearest neighbor (ANN) search in [6, 15] for SV coding. [sent-235, score-0.231]
</p><p>71 Wdees csreitp a rsset a roef neighbor vectors B(x) to a set of the four SIFT descriptors adjacent to a descriptor x. [sent-239, score-0.282]
</p><p>72 A codebook is trained on randomly sampled 1 million descriptors by using k-means algorithm for VQ or EM algorithm for a GMM. [sent-242, score-0.174]
</p><p>73 e3,ve thr,e more itchtianon n9 o0f% δ o ≥f djk  gives a correct lower bound when δ = 0. [sent-269, score-0.293]
</p><p>74 VQ: standard hard vector quantization (VQ), ANN-VQ: approximate nearest neighbor search [6], NTN-VQ: our neighbor-to-neighbor (NTN) search for VQ (Alg. [sent-285, score-0.426]
</p><p>75 1), GMM: standard Gaussian mixture model (GMM), Tree-GMM: an extension of the hierarchical k-means to a GMM framework in [7] NTN-GMM: our NTN search for a GMM (Alg. [sent-286, score-0.136]
</p><p>76 |E| : the number of distance or probability calculations per input vector, T animde t:h assignment t siemte o ifn t sec, ORCedu 20ct0i7on c l raastsei:f craetdiuocnti ochna rlaleten goef. [sent-289, score-0.208]
</p><p>77 This is because NTN-LMGMM has the advantages of both of NTN-VQ and NTNGMM: it only requires distance calculations without using an exp operator as NTN-VQ, but it has a weight coefficient and a covariance matrix for each codeword as NTN-GMM. [sent-295, score-0.239]
</p><p>78 Compared with tree-based methods, a disadvantage of NTN methods is that they are not very effective if neighbor vectors are not similar to each other. [sent-296, score-0.163]
</p><p>79 We confirm this in Figure 7: a tree-based ANN-VQ performs better than RAND-VQ which replaces a neighbor vector by a randomly sampled vector in each iteration of NTN-VQ. [sent-297, score-0.208]
</p><p>80 Notably, the average distance between two neighbor descriptors (xj−1 and xj) is 132. [sent-298, score-0.224]
</p><p>81 This confirms that the assumption that neighbor vectors are similar to each other, is necessary for NTN methods. [sent-301, score-0.19]
</p><p>82 In addition, we confirm the necessity of the accumulated distance in Eq. [sent-302, score-0.144]
</p><p>83 case w ighneorere a odmisptauntactei mn taitmrixe on input vectors is pre-computed in some way, the direct distance is better than the accumulated distance. [sent-309, score-0.196]
</p><p>84 In general, the accumulated distance is computationally effective since it derives efficient update rules of a lower/upper bound in Eq. [sent-310, score-0.183]
</p><p>85 The reduction rate of the assignment cost by NTN-VQ is 84. [sent-315, score-0.137]
</p><p>86 VQ: standard phalordts avercetfo orr quantization (VQ), NTN-VQ: neighbor-to-neighbor (NTN) search for VQ, GMM: standard Gaussian mixture model (GMM), NTN-GMM: NTN search for a GMM, NTN-LM-GMM: NTN-GMM with the log-max approximation. [sent-329, score-0.229]
</p><p>87 3  Codebook size  The simplest idea to reduce the assignment cost is to re-  duce the codebook size. [sent-333, score-0.227]
</p><p>88 In Figure 10, which shows the speed-accuracy trade-off for different codebook sizes, we confirm that using NTN methods is better than reducing the codebook size. [sent-334, score-0.262]
</p><p>89 NTN-VQ: neighbor-toneighbor (NTN) search for VQ, this is the same plot as Figure 6, ANN-VQ: approximate nearest neighbor search [6], RAND-VQ: NTN-VQ in which a neighbor vector is replaced by a randomly sampled vector. [sent-340, score-0.441]
</p><p>90 4  Relative computational time in a pipeline  Figure 11 shows the relative cost of coding with respect to the cost of the other steps of processing pipeline for extracting SV and FV representations. [sent-366, score-0.282]
</p><p>91 4% of computational time are occupied from it in  FV coding and SV coding, respectively. [sent-369, score-0.176]
</p><p>92 Note that the cost of pooling in FV coding, which generate a final FV representation, is also reduced by the LM approximation since we can skip some summations in pooling if cik is equal to zero. [sent-374, score-0.189]
</p><p>93 The reduction rate of the assignment cost by NTN-VQ and ANN-VQ for each image is reported. [sent-378, score-0.137]
</p><p>94 Trade-off between assignment time and Mean AP for codebook sizes of K = 2048, 1024, 512, · · · , 16. [sent-381, score-0.186]
</p><p>95 Conclusion We have proposed a fast computation method for searching for the matches, neighbor-to-neighbor (NTN) search, and its applications to vector quantization (VQ) and a Gaussian mixture model (GMM). [sent-390, score-0.145]
</p><p>96 Our experiments on the PASCAL VOC 2007 classification challenge showed that NTN-VQ, NTNGMM, and NTN-LM-GMM reduced the assignment cost by 77. [sent-392, score-0.155]
</p><p>97 Image classification using super-vector coding of local image descriptors. [sent-457, score-0.157]
</p><p>98 Computational cost for each step of super-vector (SV) coding and Fisher-vector (FV) coding is reported. [sent-490, score-0.301]
</p><p>99 2% of computational time is occupied from coding by VQ, NTN-VQ, GMM, NTN-GMM, and NTN-LM-GMM, respectively. [sent-498, score-0.176]
</p><p>100 5 Appendix The upper bound pjk of the probability (Eq. [sent-537, score-0.394]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ntn', 0.533), ('vq', 0.426), ('pjk', 0.31), ('gmm', 0.267), ('djk', 0.228), ('xj', 0.202), ('coding', 0.13), ('neighbor', 0.126), ('codebook', 0.118), ('dik', 0.114), ('fv', 0.094), ('codeword', 0.094), ('ij', 0.092), ('xi', 0.086), ('soft', 0.086), ('codewords', 0.082), ('pik', 0.081), ('dk', 0.077), ('accumulated', 0.076), ('sv', 0.074), ('assignment', 0.068), ('bound', 0.065), ('skip', 0.063), ('quantization', 0.061), ('calculations', 0.057), ('descriptors', 0.056), ('mixture', 0.056), ('search', 0.056), ('ntngmm', 0.055), ('voc', 0.052), ('hard', 0.05), ('gj', 0.05), ('pth', 0.05), ('calculation', 0.05), ('cjk', 0.049), ('inoue', 0.049), ('gaussian', 0.046), ('pascal', 0.045), ('pk', 0.043), ('calculated', 0.042), ('ik', 0.042), ('distance', 0.042), ('input', 0.041), ('cost', 0.041), ('ap', 0.041), ('adjacent', 0.039), ('shinoda', 0.037), ('tpu', 0.037), ('vectors', 0.037), ('sift', 0.036), ('vj', 0.036), ('probabilities', 0.036), ('calculate', 0.035), ('speed', 0.035), ('kk', 0.034), ('sk', 0.033), ('nearest', 0.031), ('code', 0.029), ('cik', 0.028), ('urt', 0.028), ('uij', 0.028), ('skipped', 0.028), ('kmeans', 0.028), ('lm', 0.028), ('reduction', 0.028), ('vector', 0.028), ('classification', 0.027), ('confirms', 0.027), ('argmin', 0.027), ('confirm', 0.026), ('computational', 0.026), ('lowe', 0.025), ('tokyo', 0.025), ('exp', 0.025), ('triangle', 0.025), ('ann', 0.024), ('descriptor', 0.024), ('hierarchical', 0.024), ('chatfield', 0.023), ('csurka', 0.022), ('pipeline', 0.022), ('tree', 0.022), ('pages', 0.022), ('covariance', 0.021), ('degradation', 0.021), ('studies', 0.021), ('assigns', 0.021), ('dance', 0.021), ('occupied', 0.02), ('appendix', 0.02), ('wk', 0.02), ('weighting', 0.02), ('ignore', 0.019), ('pooling', 0.019), ('reduced', 0.019), ('upper', 0.019), ('assigned', 0.019), ('perronnin', 0.018), ('approximate', 0.018), ('bottom', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="287-tfidf-1" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<p>Author: Nakamasa Inoue, Koichi Shinoda</p><p>Abstract: Assigning a visual code to a low-level image descriptor, which we call code assignment, is the most computationally expensive part of image classification algorithms based on the bag of visual word (BoW) framework. This paper proposes a fast computation method, Neighbor-toNeighbor (NTN) search, for this code assignment. Based on the fact that image features from an adjacent region are usually similar to each other, this algorithm effectively reduces the cost of calculating the distance between a codeword and a feature vector. This method can be applied not only to a hard codebook constructed by vector quantization (NTN-VQ), but also to a soft codebook, a Gaussian mixture model (NTN-GMM). We evaluated this method on the PASCAL VOC 2007 classification challenge task. NTN-VQ reduced the assignment cost by 77.4% in super-vector coding, and NTN-GMM reduced it by 89.3% in Fisher-vector coding, without any significant degradation in classification performance.</p><p>2 0.1972187 <a title="287-tfidf-2" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>Author: Chen-Kuo Chiang, Te-Feng Su, Chih Yen, Shang-Hong Lai</p><p>Abstract: We present a multi-attributed dictionary learning algorithm for sparse coding. Considering training samples with multiple attributes, a new distance matrix is proposed by jointly incorporating data and attribute similarities. Then, an objective function is presented to learn categorydependent dictionaries that are compact (closeness of dictionary atoms based on data distance and attribute similarity), reconstructive (low reconstruction error with correct dictionary) and label-consistent (encouraging the labels of dictionary atoms to be similar). We have demonstrated our algorithm on action classification and face recognition tasks on several publicly available datasets. Experimental results with improved performance over previous dictionary learning methods are shown to validate the effectiveness of the proposed algorithm.</p><p>3 0.15141842 <a title="287-tfidf-3" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>Author: Ming-Ming Cheng, Jonathan Warrell, Wen-Yan Lin, Shuai Zheng, Vibhav Vineet, Nigel Crook</p><p>Abstract: Detecting visually salient regions in images is one of the fundamental problems in computer vision. We propose a novel method to decompose an image into large scale perceptually homogeneous elements for efficient salient region detection, using a soft image abstraction representation. By considering both appearance similarity and spatial distribution of image pixels, the proposed representation abstracts out unnecessary image details, allowing the assignment of comparable saliency values across similar regions, and producing perceptually accurate salient region detection. We evaluate our salient region detection approach on the largest publicly available dataset with pixel accurate annotations. The experimental results show that the proposed method outperforms 18 alternate methods, reducing the mean absolute error by 25.2% compared to the previous best result, while being computationally more efficient.</p><p>4 0.10645352 <a title="287-tfidf-4" href="./iccv-2013-Quantize_and_Conquer%3A_A_Dimensionality-Recursive_Solution_to_Clustering%2C_Vector_Quantization%2C_and_Image_Retrieval.html">333 iccv-2013-Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval</a></p>
<p>Author: Yannis Avrithis</p><p>Abstract: Inspired by the close relation between nearest neighbor search and clustering in high-dimensional spaces as well as the success of one helping to solve the other, we introduce a new paradigm where both problems are solved simultaneously. Our solution is recursive, not in the size of input data but in the number of dimensions. One result is a clustering algorithm that is tuned to small codebooks but does not need all data in memory at the same time and is practically constant in the data size. As a by-product, a tree structure performs either exact or approximate quantization on trained centroids, the latter being not very precise but extremely fast. A lesser contribution is a new indexing scheme for image retrieval that exploits multiple small codebooks to provide an arbitrarily fine partition of the descriptor space. Large scale experiments on public datasets exhibit state of the art performance and remarkable generalization.</p><p>5 0.10151697 <a title="287-tfidf-5" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>Author: Masakazu Iwamura, Tomokazu Sato, Koichi Kise</p><p>Abstract: Approximate nearest neighbor search (ANNS) is a basic and important technique used in many tasks such as object recognition. It involves two processes: selecting nearest neighbor candidates and performing a brute-force search of these candidates. Only the former though has scope for improvement. In most existing methods, it approximates the space by quantization. It then calculates all the distances between the query and all the quantized values (e.g., clusters or bit sequences), and selects a fixed number of candidates close to the query. The performance of the method is evaluated based on accuracy as a function of the number of candidates. This evaluation seems rational but poses a serious problem; it ignores the computational cost of the process of selection. In this paper, we propose a new ANNS method that takes into account costs in the selection process. Whereas existing methods employ computationally expensive techniques such as comparative sort and heap, the proposed method does not. This realizes a significantly more efficient search. We have succeeded in reducing computation times by one-third compared with the state-of-the- art on an experiment using 100 million SIFT features.</p><p>6 0.09657263 <a title="287-tfidf-6" href="./iccv-2013-Low-Rank_Sparse_Coding_for_Image_Classification.html">258 iccv-2013-Low-Rank Sparse Coding for Image Classification</a></p>
<p>7 0.090013683 <a title="287-tfidf-7" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>8 0.074791797 <a title="287-tfidf-8" href="./iccv-2013-Joint_Inverted_Indexing.html">221 iccv-2013-Joint Inverted Indexing</a></p>
<p>9 0.071379274 <a title="287-tfidf-9" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>10 0.068227775 <a title="287-tfidf-10" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>11 0.067452244 <a title="287-tfidf-11" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>12 0.063852087 <a title="287-tfidf-12" href="./iccv-2013-Abnormal_Event_Detection_at_150_FPS_in_MATLAB.html">34 iccv-2013-Abnormal Event Detection at 150 FPS in MATLAB</a></p>
<p>13 0.059339561 <a title="287-tfidf-13" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>14 0.05801452 <a title="287-tfidf-14" href="./iccv-2013-Fast_Neighborhood_Graph_Search_Using_Cartesian_Concatenation.html">159 iccv-2013-Fast Neighborhood Graph Search Using Cartesian Concatenation</a></p>
<p>15 0.055070288 <a title="287-tfidf-15" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>16 0.054893978 <a title="287-tfidf-16" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>17 0.053581189 <a title="287-tfidf-17" href="./iccv-2013-Decomposing_Bag_of_Words_Histograms.html">104 iccv-2013-Decomposing Bag of Words Histograms</a></p>
<p>18 0.052615367 <a title="287-tfidf-18" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>19 0.052284244 <a title="287-tfidf-19" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>20 0.051680688 <a title="287-tfidf-20" href="./iccv-2013-A_Novel_Earth_Mover%27s_Distance_Methodology_for_Image_Matching_with_Gaussian_Mixture_Models.html">25 iccv-2013-A Novel Earth Mover's Distance Methodology for Image Matching with Gaussian Mixture Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.118), (1, 0.053), (2, -0.019), (3, -0.05), (4, -0.046), (5, 0.056), (6, -0.035), (7, -0.003), (8, -0.039), (9, -0.022), (10, 0.003), (11, 0.021), (12, -0.003), (13, 0.003), (14, -0.005), (15, 0.047), (16, 0.038), (17, -0.013), (18, 0.04), (19, 0.01), (20, 0.014), (21, -0.046), (22, 0.017), (23, 0.034), (24, -0.01), (25, -0.0), (26, 0.019), (27, 0.002), (28, 0.019), (29, 0.025), (30, 0.026), (31, -0.02), (32, -0.017), (33, 0.033), (34, 0.017), (35, 0.021), (36, 0.06), (37, -0.066), (38, 0.058), (39, 0.018), (40, -0.026), (41, 0.114), (42, -0.007), (43, 0.014), (44, -0.062), (45, -0.082), (46, -0.018), (47, -0.111), (48, 0.062), (49, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92576015 <a title="287-lsi-1" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<p>Author: Nakamasa Inoue, Koichi Shinoda</p><p>Abstract: Assigning a visual code to a low-level image descriptor, which we call code assignment, is the most computationally expensive part of image classification algorithms based on the bag of visual word (BoW) framework. This paper proposes a fast computation method, Neighbor-toNeighbor (NTN) search, for this code assignment. Based on the fact that image features from an adjacent region are usually similar to each other, this algorithm effectively reduces the cost of calculating the distance between a codeword and a feature vector. This method can be applied not only to a hard codebook constructed by vector quantization (NTN-VQ), but also to a soft codebook, a Gaussian mixture model (NTN-GMM). We evaluated this method on the PASCAL VOC 2007 classification challenge task. NTN-VQ reduced the assignment cost by 77.4% in super-vector coding, and NTN-GMM reduced it by 89.3% in Fisher-vector coding, without any significant degradation in classification performance.</p><p>2 0.72157425 <a title="287-lsi-2" href="./iccv-2013-Low-Rank_Sparse_Coding_for_Image_Classification.html">258 iccv-2013-Low-Rank Sparse Coding for Image Classification</a></p>
<p>Author: Tianzhu Zhang, Bernard Ghanem, Si Liu, Changsheng Xu, Narendra Ahuja</p><p>Abstract: In this paper, we propose a low-rank sparse coding (LRSC) method that exploits local structure information among features in an image for the purpose of image-level classification. LRSC represents densely sampled SIFT descriptors, in a spatial neighborhood, collectively as lowrank, sparse linear combinations of codewords. As such, it casts the feature coding problem as a low-rank matrix learning problem, which is different from previous methods that encode features independently. This LRSC has a number of attractive properties. (1) It encourages sparsity in feature codes, locality in codebook construction, and low-rankness for spatial consistency. (2) LRSC encodes local features jointly by considering their low-rank structure information, and is computationally attractive. We evaluate the LRSC by comparing its performance on a set of challenging benchmarks with that of 7 popular coding and other state-of-theart methods. Our experiments show that by representing local features jointly, LRSC not only outperforms the state-ofthe-art in classification accuracy but also improves the time complexity of methods that use a similar sparse linear repre- sentation model for feature coding [36].</p><p>3 0.68668878 <a title="287-lsi-3" href="./iccv-2013-SIFTpack%3A_A_Compact_Representation_for_Efficient_SIFT_Matching.html">365 iccv-2013-SIFTpack: A Compact Representation for Efficient SIFT Matching</a></p>
<p>Author: Alexandra Gilinsky, Lihi Zelnik Manor</p><p>Abstract: Computing distances between large sets of SIFT descriptors is a basic step in numerous algorithms in computer vision. When the number of descriptors is large, as is often the case, computing these distances can be extremely time consuming. In this paper we propose the SIFTpack: a compact way of storing SIFT descriptors, which enables significantly faster calculations between sets of SIFTs than the current solutions. SIFTpack can be used to represent SIFTs densely extracted from a single image or sparsely from multiple different images. We show that the SIFTpack representation saves both storage space and run time, for both finding nearest neighbors and for computing all distances between all descriptors. The usefulness of SIFTpack is also demonstrated as an alternative implementation for K-means dictionaries of visual words.</p><p>4 0.67654955 <a title="287-lsi-4" href="./iccv-2013-Joint_Inverted_Indexing.html">221 iccv-2013-Joint Inverted Indexing</a></p>
<p>Author: Yan Xia, Kaiming He, Fang Wen, Jian Sun</p><p>Abstract: Inverted indexing is a popular non-exhaustive solution to large scale search. An inverted file is built by a quantizer such as k-means or a tree structure. It has been found that multiple inverted files, obtained by multiple independent random quantizers, are able to achieve practically good recall and speed. Instead of computing the multiple quantizers independently, we present a method that creates them jointly. Our method jointly optimizes all codewords in all quantizers. Then it assigns these codewords to the quantizers. In experiments this method shows significant improvement over various existing methods that use multiple independent quantizers. On the one-billion set of SIFT vectors, our method is faster and more accurate than a recent state-of-the-art inverted indexing method.</p><p>5 0.6698612 <a title="287-lsi-5" href="./iccv-2013-Quantize_and_Conquer%3A_A_Dimensionality-Recursive_Solution_to_Clustering%2C_Vector_Quantization%2C_and_Image_Retrieval.html">333 iccv-2013-Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval</a></p>
<p>Author: Yannis Avrithis</p><p>Abstract: Inspired by the close relation between nearest neighbor search and clustering in high-dimensional spaces as well as the success of one helping to solve the other, we introduce a new paradigm where both problems are solved simultaneously. Our solution is recursive, not in the size of input data but in the number of dimensions. One result is a clustering algorithm that is tuned to small codebooks but does not need all data in memory at the same time and is practically constant in the data size. As a by-product, a tree structure performs either exact or approximate quantization on trained centroids, the latter being not very precise but extremely fast. A lesser contribution is a new indexing scheme for image retrieval that exploits multiple small codebooks to provide an arbitrarily fine partition of the descriptor space. Large scale experiments on public datasets exhibit state of the art performance and remarkable generalization.</p><p>6 0.60308689 <a title="287-lsi-6" href="./iccv-2013-Fast_Neighborhood_Graph_Search_Using_Cartesian_Concatenation.html">159 iccv-2013-Fast Neighborhood Graph Search Using Cartesian Concatenation</a></p>
<p>7 0.59392411 <a title="287-lsi-7" href="./iccv-2013-To_Aggregate_or_Not_to_aggregate%3A_Selective_Match_Kernels_for_Image_Search.html">419 iccv-2013-To Aggregate or Not to aggregate: Selective Match Kernels for Image Search</a></p>
<p>8 0.59278905 <a title="287-lsi-8" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>9 0.58839583 <a title="287-lsi-9" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>10 0.57265168 <a title="287-lsi-10" href="./iccv-2013-Nested_Shape_Descriptors.html">288 iccv-2013-Nested Shape Descriptors</a></p>
<p>11 0.5342375 <a title="287-lsi-11" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>12 0.52977258 <a title="287-lsi-12" href="./iccv-2013-Stacked_Predictive_Sparse_Coding_for_Classification_of_Distinct_Regions_in_Tumor_Histopathology.html">401 iccv-2013-Stacked Predictive Sparse Coding for Classification of Distinct Regions in Tumor Histopathology</a></p>
<p>13 0.52922028 <a title="287-lsi-13" href="./iccv-2013-Pyramid_Coding_for_Functional_Scene_Element_Recognition_in_Video_Scenes.html">331 iccv-2013-Pyramid Coding for Functional Scene Element Recognition in Video Scenes</a></p>
<p>14 0.52210313 <a title="287-lsi-14" href="./iccv-2013-A_Novel_Earth_Mover%27s_Distance_Methodology_for_Image_Matching_with_Gaussian_Mixture_Models.html">25 iccv-2013-A Novel Earth Mover's Distance Methodology for Image Matching with Gaussian Mixture Models</a></p>
<p>15 0.49419999 <a title="287-lsi-15" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>16 0.48422667 <a title="287-lsi-16" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>17 0.48312232 <a title="287-lsi-17" href="./iccv-2013-EVSAC%3A_Accelerating_Hypotheses_Generation_by_Modeling_Matching_Scores_with_Extreme_Value_Theory.html">131 iccv-2013-EVSAC: Accelerating Hypotheses Generation by Modeling Matching Scores with Extreme Value Theory</a></p>
<p>18 0.48276907 <a title="287-lsi-18" href="./iccv-2013-Heterogeneous_Auto-similarities_of_Characteristics_%28HASC%29%3A_Exploiting_Relational_Information_for_Classification.html">193 iccv-2013-Heterogeneous Auto-similarities of Characteristics (HASC): Exploiting Relational Information for Classification</a></p>
<p>19 0.48226988 <a title="287-lsi-19" href="./iccv-2013-The_Interestingness_of_Images.html">416 iccv-2013-The Interestingness of Images</a></p>
<p>20 0.47959098 <a title="287-lsi-20" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.078), (7, 0.016), (13, 0.032), (24, 0.275), (26, 0.09), (31, 0.071), (35, 0.014), (42, 0.087), (64, 0.026), (73, 0.019), (78, 0.012), (89, 0.162)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75466818 <a title="287-lda-1" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<p>Author: Nakamasa Inoue, Koichi Shinoda</p><p>Abstract: Assigning a visual code to a low-level image descriptor, which we call code assignment, is the most computationally expensive part of image classification algorithms based on the bag of visual word (BoW) framework. This paper proposes a fast computation method, Neighbor-toNeighbor (NTN) search, for this code assignment. Based on the fact that image features from an adjacent region are usually similar to each other, this algorithm effectively reduces the cost of calculating the distance between a codeword and a feature vector. This method can be applied not only to a hard codebook constructed by vector quantization (NTN-VQ), but also to a soft codebook, a Gaussian mixture model (NTN-GMM). We evaluated this method on the PASCAL VOC 2007 classification challenge task. NTN-VQ reduced the assignment cost by 77.4% in super-vector coding, and NTN-GMM reduced it by 89.3% in Fisher-vector coding, without any significant degradation in classification performance.</p><p>2 0.75121391 <a title="287-lda-2" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>Author: Chaochao Lu, Deli Zhao, Xiaoou Tang</p><p>Abstract: When face images are taken in the wild, the large variations in facial pose, illumination, and expression make face recognition challenging. The most fundamental problem for face recognition is to measure the similarity between faces. The traditional measurements such as various mathematical norms, Hausdorff distance, and approximate geodesic distance cannot accurately capture the structural information between faces in such complex circumstances. To address this issue, we develop a novel face patch network, based on which we define a new similarity measure called the random path (RP) measure. The RP measure is derivedfrom the collective similarity ofpaths by performing random walks in the network. It can globally characterize the contextual and curved structures of the face space. To apply the RP measure, we construct two kinds of networks: . cuhk . edu . hk the in-face network and the out-face network. The in-face network is drawn from any two face images and captures the local structural information. The out-face network is constructed from all the training face patches, thereby modeling the global structures of face space. The two face networks are structurally complementary and can be combined together to improve the recognition performance. Experiments on the Multi-PIE and LFW benchmarks show that the RP measure outperforms most of the state-of-art algorithms for face recognition.</p><p>3 0.71808022 <a title="287-lda-3" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<p>Author: Tian Lan, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: The appearance of an object changes profoundly with pose, camera view and interactions of the object with other objects in the scene. This makes it challenging to learn detectors based on an object-level label (e.g., “car”). We postulate that having a richer set oflabelings (at different levels of granularity) for an object, including finer-grained subcategories, consistent in appearance and view, and higherorder composites – contextual groupings of objects consistent in their spatial layout and appearance, can significantly alleviate these problems. However, obtaining such a rich set of annotations, including annotation of an exponentially growing set of object groupings, is simply not feasible. We propose a weakly-supervised framework for object detection where we discover subcategories and the composites automatically with only traditional object-level category labels as input. To this end, we first propose an exemplar-SVM-based clustering approach, with latent SVM refinement, that discovers a variable length set of discriminative subcategories for each object class. We then develop a structured model for object detection that captures interactions among object subcategories and automatically discovers semantically meaningful and discriminatively relevant visual composites. We show that this model produces state-of-the-art performance on UIUC phrase object detection benchmark.</p><p>4 0.69894385 <a title="287-lda-4" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<p>Author: Federico Tombari, Alessandro Franchi, Luigi Di_Stefano</p><p>Abstract: Object detection in images withstanding significant clutter and occlusion is still a challenging task whenever the object surface is characterized by poor informative content. We propose to tackle this problem by a compact and distinctive representation of groups of neighboring line segments aggregated over limited spatial supports and invariant to rotation, translation and scale changes. Peculiarly, our proposal allows for leveraging on the inherent strengths of descriptor-based approaches, i.e. robustness to occlusion and clutter and scalability with respect to the size of the model library, also when dealing with scarcely textured objects.</p><p>5 0.63621008 <a title="287-lda-5" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>Author: Ming-Ming Cheng, Jonathan Warrell, Wen-Yan Lin, Shuai Zheng, Vibhav Vineet, Nigel Crook</p><p>Abstract: Detecting visually salient regions in images is one of the fundamental problems in computer vision. We propose a novel method to decompose an image into large scale perceptually homogeneous elements for efficient salient region detection, using a soft image abstraction representation. By considering both appearance similarity and spatial distribution of image pixels, the proposed representation abstracts out unnecessary image details, allowing the assignment of comparable saliency values across similar regions, and producing perceptually accurate salient region detection. We evaluate our salient region detection approach on the largest publicly available dataset with pixel accurate annotations. The experimental results show that the proposed method outperforms 18 alternate methods, reducing the mean absolute error by 25.2% compared to the previous best result, while being computationally more efficient.</p><p>6 0.63170666 <a title="287-lda-6" href="./iccv-2013-Class-Specific_Simplex-Latent_Dirichlet_Allocation_for_Image_Classification.html">73 iccv-2013-Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification</a></p>
<p>7 0.63054961 <a title="287-lda-7" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>8 0.6284169 <a title="287-lda-8" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>9 0.6280303 <a title="287-lda-9" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>10 0.62798715 <a title="287-lda-10" href="./iccv-2013-PhotoOCR%3A_Reading_Text_in_Uncontrolled_Conditions.html">315 iccv-2013-PhotoOCR: Reading Text in Uncontrolled Conditions</a></p>
<p>11 0.62722868 <a title="287-lda-11" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>12 0.62582135 <a title="287-lda-12" href="./iccv-2013-Low-Rank_Sparse_Coding_for_Image_Classification.html">258 iccv-2013-Low-Rank Sparse Coding for Image Classification</a></p>
<p>13 0.62561411 <a title="287-lda-13" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>14 0.62463617 <a title="287-lda-14" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>15 0.62400484 <a title="287-lda-15" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>16 0.62296569 <a title="287-lda-16" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>17 0.62285566 <a title="287-lda-17" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>18 0.62237859 <a title="287-lda-18" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>19 0.62175488 <a title="287-lda-19" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>20 0.62173748 <a title="287-lda-20" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
