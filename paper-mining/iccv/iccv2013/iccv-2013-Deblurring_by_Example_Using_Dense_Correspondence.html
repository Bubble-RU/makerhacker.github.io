<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>103 iccv-2013-Deblurring by Example Using Dense Correspondence</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-103" href="#">iccv2013-103</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>103 iccv-2013-Deblurring by Example Using Dense Correspondence</h1>
<br/><p>Source: <a title="iccv-2013-103-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Hacohen_Deblurring_by_Example_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Yoav Hacohen, Eli Shechtman, Dani Lischinski</p><p>Abstract: This paper presents a new method for deblurring photos using a sharp reference example that contains some shared content with the blurry photo. Most previous deblurring methods that exploit information from other photos require an accurately registered photo of the same static scene. In contrast, our method aims to exploit reference images where the shared content may have undergone substantial photometric and non-rigid geometric transformations, as these are the kind of reference images most likely to be found in personal photo albums. Our approach builds upon a recent method for examplebased deblurring using non-rigid dense correspondence (NRDC) [11] and extends it in two ways. First, we suggest exploiting information from the reference image not only for blur kernel estimation, but also as a powerful local prior for the non-blind deconvolution step. Second, we introduce a simple yet robust technique for spatially varying blur estimation, rather than assuming spatially uniform blur. Unlike the aboveprevious method, which hasproven successful only with simple deblurring scenarios, we demonstrate that our method succeeds on a variety of real-world examples. We provide quantitative and qualitative evaluation of our method and show that it outperforms the state-of-the-art.</p><p>Reference: <a title="iccv-2013-103-reference" href="../iccv2013_reference/iccv-2013-Deblurring_by_Example_Using_Dense_Correspondence_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 l  Abstract This paper presents a new method for deblurring photos using a sharp reference example that contains some shared content with the blurry photo. [sent-9, score-1.238]
</p><p>2 Most previous deblurring methods that exploit information from other photos require an accurately registered photo of the same static scene. [sent-10, score-0.794]
</p><p>3 In contrast, our method aims to exploit reference images where the shared content may have undergone substantial photometric and non-rigid geometric transformations, as these are the kind of reference images most likely to be found in personal photo albums. [sent-11, score-0.513]
</p><p>4 Our approach builds upon a recent method for examplebased deblurring using non-rigid dense correspondence (NRDC) [11] and extends it in two ways. [sent-12, score-0.642]
</p><p>5 First, we suggest exploiting information from the reference image not only for blur kernel estimation, but also as a powerful local prior for the non-blind deconvolution step. [sent-13, score-0.932]
</p><p>6 Second, we introduce a simple yet robust technique for spatially varying blur estimation, rather than assuming spatially uniform blur. [sent-14, score-0.52]
</p><p>7 Unlike the aboveprevious method, which hasproven successful only with simple deblurring scenarios, we demonstrate that  our method succeeds on a variety of real-world examples. [sent-15, score-0.506]
</p><p>8 Introduction Photographs often exhibit blur caused by camera defocus, camera motion, or motion in the scene. [sent-18, score-0.504]
</p><p>9 Blind deblurring, also known as blind deconvolution, refers to the problem of recovering a sharp image from a blurry one when the exact parameters of the blur operator are not known. [sent-19, score-0.993]
</p><p>10 Without any prior or additional information, this problem is inherently ill-posed, as there are many possible combinations of blur kernels and sharp images that can explain a given blurry image. [sent-20, score-1.027]
</p><p>11 In this paper we address a challenging variant of blind deblurring with unknown spatially varying camera motion blur, while assuming the availability of a sharp reference image containing some shared content under unknown geometric and photometric transformations. [sent-21, score-1.266]
</p><p>12 While recent single-image approaches based on general priors or edge-based techniques [4, 6, 18, 21, 17] have shown significant progress, blind deblurring remains a very hard and ill-posed problem [20]. [sent-22, score-0.673]
</p><p>13 General assumptions may result in an inaccurate blur kernel estimation and/or an incorrect deblurred result. [sent-23, score-0.685]
</p><p>14 [19] is often used in the (non-blind) deconvolution step to overcome artifacts due to an inaccurately estimated kernel. [sent-25, score-0.256]
</p><p>15 Several approaches perform deblurring by leveraging multiple photos of the scene [22, 27, 14]. [sent-27, score-0.586]
</p><p>16 While these methods demonstrate the benefit of having additional data, they require accurately registered photos that simultaneously capture exactly the same static scene. [sent-28, score-0.22]
</p><p>17 Although people often shoot several photos in a succession, unless the photos were taken intentionally to meet the requirements of these methods, suitable input image sets are unlikely to be found in personal photo albums. [sent-29, score-0.317]
</p><p>18 [11] presented the Non-Rigid Dense Correspondence (NRDC) method for simultaneously recovering a partial dense correspondence and a color transformation between pairs of images with shared content. [sent-31, score-0.202]
</p><p>19 NRDC was shown to be highly effective for finding large matching regions in typical personal photo collections. [sent-32, score-0.157]
</p><p>20 One of the applications demonstrated in [11] was deblurring by example: given a pair of images, where one image is sharp while another is blurry, NRDC was used to estimate a blur kernel simultaneously with the correspondence and the color transformation. [sent-33, score-1.342]
</p><p>21 However, this approach has been successfully demonstrated only on simple synthetic deblurring scenarios, involving simple blur kernels and no added noise. [sent-34, score-0.963]
</p><p>22 Our method extends the above approach to more realistic scenarios, including real-world blur kernels, noise,  and real-world sharp/blurry image pairs with spatially vary22338844  ing blur. [sent-35, score-0.423]
</p><p>23 Similarly to [11], we employ an iterative optimization scheme that alternates between finding correspondences, estimating the blur kernel, and recovering the sharp image with non-blind deconvolution. [sent-36, score-0.648]
</p><p>24 However, in order to increase the accuracy and robustness of our method, we introduce two key modifications: First, rather than exploiting the sharp reference only for kernel estimation, we also use this reference as a strong local prior for the non-blind deconvolution step. [sent-37, score-0.894]
</p><p>25 Second, we suggest a new fast and robust non-uniform blur kernel estimation method, which reduces the effect of outliers resulting from inaccuracies at the other steps. [sent-38, score-0.589]
</p><p>26 After discussing relevant previous work (Section 2) and presenting our deblurring algorithm (Section 3), we evaluate our method and compare it to the state of the art deblurring methods (Section 4). [sent-39, score-1.012]
</p><p>27 In this paper, we also assume a spatially non-uniform blur model. [sent-49, score-0.423]
</p><p>28 Recent single-image approaches for blind deblurring have shown significant progress by using general image priors [9, 21, 18] and edge-based techniques [16, 4, 6]. [sent-51, score-0.673]
</p><p>29 In order to recover both x and k most of these methods alternate between two main steps: First, updating the estimated sharp latent image x (non-blind deconvolution), and second, updating the blur kernel k. [sent-52, score-0.811]
</p><p>30 id=1  where Ak = kiPi is the blur matrix, Bx is a matrix whose i-th co? [sent-58, score-0.374]
</p><p>31 [27] and Cho and Lee [4] use Tikhonov regularization on the kernel (ρK = ? [sent-62, score-0.154]
</p><p>32 s both on x and k) and a sparsity prior on the kernel (ρK = ? [sent-73, score-0.241]
</p><p>33 age prior ρL, researchers commonly use a Gaussian prior [23, 4], or a sparsity prior [21, 18]. [sent-79, score-0.261]
</p><p>34 Recently, priors based on patch-banks have been proven effective in reducing ringing artifacts [28, 24], but they might still lose high frequency texture details due to the limited number of example patches. [sent-83, score-0.171]
</p><p>35 Other approaches assume the existence of another accurately registered image of the same static scene, but blurred by a different kernel [22, 3, 1, 14] or containing noise [27], for estimating a blur kernel. [sent-87, score-0.762]
</p><p>36 [27] use a noisy image taken from the same viewpoint as a prior for the non-blind deconvolution step, but only for recovering the low-frequencies of the latent image, leaving the highfrequencies prone to ringing and noise. [sent-89, score-0.454]
</p><p>37 Rav-Acha and Peleg [22] simultaneously deconvolve two registered images of the same static scene, each blurred by a different directional blur kernel, assuming the results should be equal (thus each image serves as a prior for the other). [sent-90, score-0.655]
</p><p>38 However, they require accurately aligned input images which are not typically available in personal photo collections. [sent-92, score-0.185]
</p><p>39 [2] use SIFT features to match between a blurry and a sharp image of a static scene. [sent-95, score-0.496]
</p><p>40 [11] simultaneously deblurs and computes a partial dense correspondence between the blurry input image and a sharp reference, where the resulting correspondence is more dense and robust than previous methods. [sent-99, score-0.727]
</p><p>41 [5] removes blur 22338855  in video frames due to camera shake using patches sam-  pled from nearby sharp frames. [sent-101, score-0.678]
</p><p>42 While our local prior is also based on reconstruction using sharp patches, our method interleaves finding correspondence, local prior and kernel estimation in an optimization framework and thus can handle more complex motions and blur kernels. [sent-102, score-0.942]
</p><p>43 As already explained in Section 1, we introduce two important improvements to the approach described in [11], making it applicable to a much wider variety of deblurring scenarios and real-world image pairs. [sent-103, score-0.506]
</p><p>44 In particular, to the best of our knowledge, our method is the first to use an additional image as a local prior when estimating the latent sharp image, without requiring an accurate full registration between the images. [sent-104, score-0.403]
</p><p>45 [11], we iteratively alternate between computing a dense correspondence, estimating the kernel, and estimating the latent image, while proceeding in a coarse-to-fine manner. [sent-107, score-0.177]
</p><p>46 However, there two crucial differences with respect to [11]: (i) we use the sharp reference image not only for blur kernel estimation, but also as a local prior for latent image recovery (Section 3. [sent-108, score-1.024]
</p><p>47 1); (ii) we robustly estimate a spatially varying blur kernel instead of a uniform one (Section 3. [sent-109, score-0.625]
</p><p>48 , finding an approximate nearest neighbor field followed by aggregation of coherent regions, to obtain a dense correspondence between the sharp refer-  ence image r and the current latent image estimate x. [sent-113, score-0.419]
</p><p>49 Next, we estimate the blur kernel k from the pair of sharp and blurry images given by rM and C(y) (section 3. [sent-115, score-0.983]
</p><p>50 Finally, we use the estimated kernel k, and a local prior given by the partial reconstruction of the latent image, C−1 (rM), to update our latent image estimate x (section 3. [sent-117, score-0.387]
</p><p>51 Note that in order to generate the kernel estimation equation (3) we need an entire neighborhood around each pixel of rM. [sent-124, score-0.184]
</p><p>52 Note also that unlike many other coarse-to-fine deblurring methods (e. [sent-135, score-0.506]
</p><p>53 , [11, 4, 21, 18, 26]), we do not upscale the kernel k when switching from a coarser scale to a finer one. [sent-137, score-0.272]
</p><p>54 We found that any small interpolation/upsampling error in the kernel might result in large deconvolution artifacts. [sent-138, score-0.345]
</p><p>55 Latent image estimation Given the blur kernel k, a na¨ ıve way of obtaining the deblurred image would be to invert the kernel by solving Eq. [sent-142, score-0.839]
</p><p>56 This often results in severe artifacts, such as ringing, because Ak is usually not well-conditioned; other sources for such artifacts might be inaccuracies in the estimate of k, presence of noise, or other violations of the blurry image formation model. [sent-144, score-0.341]
</p><p>57 To reduce these artifacts, the popular deconvolution approach of Levin et al. [sent-145, score-0.218]
</p><p>58 However, while being commonly used, the sparse prior is still too generic and often overcomes ringing artifacts only 22338866  (a) blur y input(b) reference(c) with holes(d) after hole fil ing(e) final result Figure 1. [sent-154, score-0.648]
</p><p>59 The confidence map w  (not shown) has high values where the colors in (d) are reconstructed and zero values otherwise; (e) is our final deblurred result. [sent-156, score-0.175]
</p><p>60 Our ap-  proach is to leverage the existence ofthe sharp reference image r, and the availability of a dense mapping M between r and the blurry image y, to augment the generic sparse prior with a local non-parametric one, which we refer to as the reconstruction prior. [sent-158, score-0.759]
</p><p>61 The confidence map w associated with the mapping M is determined by the consistency of the matching, but it does not guarantee that the reconstruction rM is consistent with the blurry input image (i. [sent-181, score-0.318]
</p><p>62 1  Kernel estimation  If the estimate of the latent image x is correct up to some additive Gaussian noise, the optimal kernel k can be recovered by minimizing an objective function similar to Eq. [sent-205, score-0.299]
</p><p>63 In single image deblurring methods, x is initially unknown so it is often approximated using edge prediction techniques ([4, 16, 6]) that rely on the existence of strong edges at multiple orientations. [sent-207, score-0.562]
</p><p>64 ρK (k, x) is a prior on the kernel values and may be a function of the image x. [sent-208, score-0.241]
</p><p>65 When a sharp reference image r exists and a correspondence M is established, we can replace x with rM at regions of high correspondence confidence as follows:  k = argkmin? [sent-209, score-0.58]
</p><p>66 2+ ρK(k,x)  (7)  where we also replace y by C(y) (to compensate for the photometric differences between r and y), and weight the differences with a diagonal matrix W whose main diagonal is the correspondence confidence w. [sent-211, score-0.179]
</p><p>67 We will now describe our blur model ({Pi}) and the prior ρK W(·)e twhailtl we use stoc regularize rthme okedernle(l{ kP. [sent-212, score-0.461]
</p><p>68 0] showed that general blur caused by 6D camera motion cannot be accurately represented by a translation invariant kernel and proposed spatially varying approaches. [sent-224, score-0.687]
</p><p>69 However, a recent review of deblurring algorithms [17] showed that the uniform translation-invariant blur model performs generally better than the spatiallyvarying models for large kernels, and not much worse when strong camera rotation was involved. [sent-225, score-0.976]
</p><p>70 Motivated by this review and the approximation in [13, 12], we suggest a simpler model where each block has its own translation-invariant kernel but this kernel must not be  too different from those of the adjacent blocks. [sent-227, score-0.379]
</p><p>71 model the blur as a set of convolution kernels, each estimated inside a different block (tile) of the image, while corresponding coefficients of kernels from adjacent blocks are regularized to be similar. [sent-233, score-0.615]
</p><p>72 tsh the ree ismt aofg eth bel image (ycx ( ia −nd c cy are dcefined as half of the kernel dimensions). [sent-237, score-0.182]
</p><p>73 Inspired by [12], the similarity between adjacent kernels is achieved by using the following prior in Eq. [sent-238, score-0.214]
</p><p>74 The entire deblurring process (including the non-blind deconvolution step) takes about 1–2 minutes for 1024 768 images on a 2. [sent-251, score-0.697]
</p><p>75 The metric assumes that the ground-truth blur kernel kgt and sharp image xgt are both known, and computes the error between the deblurring result xout and xgt. [sent-257, score-1.38]
</p><p>76 This error is normalized by the error between the deconvolution with the ground-truth kernel xkgt and xgt, resulting in the  error ratio: ? [sent-258, score-0.394]
</p><p>77 These pairs were collected from personal photo albums and were not deliberately captured with our method or experiment in mind. [sent-275, score-0.157]
</p><p>78 One image from each pair was blurred with each of the 8 kernels, resulting in 40 test images, while the remaining 5 images serve as the sharp references. [sent-276, score-0.243]
</p><p>79 We added Gaussian noise with σ = 1% to each of the blurry images. [sent-277, score-0.245]
</p><p>80 Figures 3 and 4 compare our method with three state-ofthe-art single-image deblurring methods. [sent-278, score-0.506]
</p><p>81 The two-image deblurring methods of [22, 27] were not tested as they both require a pair of registered images of the same static scene, and [22] was designed for 1D motion blur only. [sent-279, score-1.026]
</p><p>82 However, it failed to find any correspondences on almost all of the 40 blurry test inputs, which were generated using complex real-world kernels, and therefore its results are not included in Figures 3 and 4. [sent-282, score-0.245]
</p><p>83 Note that for most of the tested images the error ratio of our deblurring results is below 2. [sent-285, score-0.506]
</p><p>84 A quantitative comparison with several state-of-the-art single-image deblurring methods: Levin et al. [sent-288, score-0.533]
</p><p>85 22338888  reference  blurry input  Cho & Lee ’09  Krishnan ’ 11  Levin ’ 11  our result  posed); Cho and Lee [4]; Krishnan et al. [sent-292, score-0.398]
</p><p>86 The odd rows show a deblurred result (with the recovered blur kernel) and the even rows show an enlarged portion of the deblurred results. [sent-295, score-0.765]
</p><p>87 one  a  pairs, where image exhibits significant blur, but sharp reference is available that shares a significant portion of the  perimpose the four kernels recovered near the corners of the image. [sent-296, score-0.493]
</p><p>88 Figure  compares  amples and them to three state-of-the-art single image deblurring methods. [sent-299, score-0.506]
</p><p>89 We also show the blur kernels recovered by each of the methods superimposed over the  qualitative advantages over additional methods is shown in  dete bal u. [sent-300, score-0.499]
</p><p>90 22338899 reference  blurry input  Whyte ’ 11  Levin ’ 11  Sun ’ 13  our result  Sun et al. [sent-304, score-0.398]
</p><p>91 The odd rows show a deblurred result with the recovered blur kernel(s) and the even rows show an enlarged portion of the deblurred results. [sent-306, score-0.765]
</p><p>92 Another limitation is that a good kernel estimation at one scale is required for correspondence at the next scale. [sent-312, score-0.282]
</p><p>93 A possible  workaround is to initialize the algorithm with a single image deblurring result at a finer scale. [sent-314, score-0.534]
</p><p>94 A third limitation is that the local prior can be used only for regions that are available in the reference image. [sent-315, score-0.213]
</p><p>95 The sparse prior itself could be replaced with a more advanced image prior (e. [sent-317, score-0.174]
</p><p>96 Conclusions and Future Work We have presented a new method for deblurring photos using a sharp reference image that may often be found in personal photo collections. [sent-321, score-1.079]
</p><p>97 We have demonstrated that when a suitable reference exists, our method outperforms the state-of-the-art single image deblurring methods, while no other method can generally exploit such different examples for deblurring. [sent-322, score-0.632]
</p><p>98 Promising future research directions may be to further extend our method to more general blur models, (e. [sent-323, score-0.374]
</p><p>99 object motion blur), and using similar content from example images as a prior for non-shared regions. [sent-325, score-0.157]
</p><p>100 Recording and playback of camera shake: benchmarking blind deconvolution with a real-world database. [sent-485, score-0.372]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('deblurring', 0.506), ('blur', 0.374), ('blurry', 0.245), ('sharp', 0.21), ('deconvolution', 0.191), ('levin', 0.165), ('kernel', 0.154), ('nrdc', 0.152), ('cho', 0.138), ('blind', 0.133), ('hacohen', 0.127), ('deblurred', 0.127), ('reference', 0.126), ('rm', 0.108), ('correspondence', 0.098), ('personal', 0.089), ('xgt', 0.087), ('prior', 0.087), ('kernels', 0.083), ('photos', 0.08), ('whyte', 0.076), ('latent', 0.073), ('ringing', 0.072), ('hirsch', 0.072), ('registered', 0.071), ('joshi', 0.07), ('krishnan', 0.07), ('photo', 0.068), ('upscale', 0.065), ('ancuti', 0.065), ('artifacts', 0.065), ('convolution', 0.053), ('hole', 0.05), ('akc', 0.049), ('akx', 0.049), ('deconvolve', 0.049), ('kipi', 0.049), ('matchnrdc', 0.049), ('xkgt', 0.049), ('xout', 0.049), ('yoav', 0.049), ('spatially', 0.049), ('uniform', 0.048), ('confidence', 0.048), ('camera', 0.048), ('tf', 0.048), ('israel', 0.047), ('july', 0.047), ('shake', 0.046), ('ak', 0.045), ('adjacent', 0.044), ('argkmin', 0.043), ('jerusalem', 0.043), ('recovered', 0.042), ('static', 0.041), ('dense', 0.038), ('shaken', 0.038), ('harmeling', 0.038), ('hebrew', 0.038), ('yuan', 0.037), ('odd', 0.036), ('content', 0.036), ('shared', 0.035), ('pages', 0.035), ('motion', 0.034), ('durand', 0.034), ('blocks', 0.034), ('priors', 0.034), ('acm', 0.034), ('sch', 0.033), ('photometric', 0.033), ('blurred', 0.033), ('estimating', 0.033), ('portion', 0.032), ('irls', 0.031), ('pix', 0.031), ('inaccuracies', 0.031), ('recovering', 0.031), ('adobe', 0.03), ('estimation', 0.03), ('smooth', 0.029), ('holes', 0.028), ('filling', 0.028), ('finer', 0.028), ('piecewise', 0.028), ('accurately', 0.028), ('inherently', 0.028), ('existence', 0.028), ('translations', 0.028), ('cy', 0.028), ('reliability', 0.028), ('unknown', 0.028), ('enlarged', 0.027), ('block', 0.027), ('matched', 0.027), ('et', 0.027), ('zitnick', 0.026), ('mapping', 0.025), ('coarser', 0.025), ('reliably', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="103-tfidf-1" href="./iccv-2013-Deblurring_by_Example_Using_Dense_Correspondence.html">103 iccv-2013-Deblurring by Example Using Dense Correspondence</a></p>
<p>Author: Yoav Hacohen, Eli Shechtman, Dani Lischinski</p><p>Abstract: This paper presents a new method for deblurring photos using a sharp reference example that contains some shared content with the blurry photo. Most previous deblurring methods that exploit information from other photos require an accurately registered photo of the same static scene. In contrast, our method aims to exploit reference images where the shared content may have undergone substantial photometric and non-rigid geometric transformations, as these are the kind of reference images most likely to be found in personal photo albums. Our approach builds upon a recent method for examplebased deblurring using non-rigid dense correspondence (NRDC) [11] and extends it in two ways. First, we suggest exploiting information from the reference image not only for blur kernel estimation, but also as a powerful local prior for the non-blind deconvolution step. Second, we introduce a simple yet robust technique for spatially varying blur estimation, rather than assuming spatially uniform blur. Unlike the aboveprevious method, which hasproven successful only with simple deblurring scenarios, we demonstrate that our method succeeds on a variety of real-world examples. We provide quantitative and qualitative evaluation of our method and show that it outperforms the state-of-the-art.</p><p>2 0.67208874 <a title="103-tfidf-2" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>Author: Tae Hyun Kim, Byeongjoo Ahn, Kyoung Mu Lee</p><p>Abstract: Most conventional single image deblurring methods assume that the underlying scene is static and the blur is caused by only camera shake. In this paper, in contrast to this restrictive assumption, we address the deblurring problem of general dynamic scenes which contain multiple moving objects as well as camera shake. In case of dynamic scenes, moving objects and background have different blur motions, so the segmentation of the motion blur is required for deblurring each distinct blur motion accurately. Thus, we propose a novel energy model designed with the weighted sum of multiple blur data models, which estimates different motion blurs and their associated pixelwise weights, and resulting sharp image. In this framework, the local weights are determined adaptively and get high values when the corresponding data models have high data fidelity. And, the weight information is used for the segmentation of the motion blur. Non-local regularization of weights are also incorporated to produce more reliable segmentation results. A convex optimization-based method is used for the solution of the proposed energy model. Exper- imental results demonstrate that our method outperforms conventional approaches in deblurring both dynamic scenes and static scenes.</p><p>3 0.56487459 <a title="103-tfidf-3" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>Author: Shicheng Zheng, Li Xu, Jiaya Jia</p><p>Abstract: We handle a special type of motion blur considering that cameras move primarily forward or backward. Solving this type of blur is of unique practical importance since nearly all car, traffic and bike-mounted cameras follow out-ofplane translational motion. We start with the study of geometric models and analyze the difficulty of existing methods to deal with them. We also propose a solution accounting for depth variation. Homographies associated with different 3D planes are considered and solved for in an optimization framework. Our method is verified on several natural image examples that cannot be satisfyingly dealt with by previous methods.</p><p>4 0.23106012 <a title="103-tfidf-4" href="./iccv-2013-Nonparametric_Blind_Super-resolution.html">293 iccv-2013-Nonparametric Blind Super-resolution</a></p>
<p>Author: Tomer Michaeli, Michal Irani</p><p>Abstract: Super resolution (SR) algorithms typically assume that the blur kernel is known (either the Point Spread Function ‘PSF’ of the camera, or some default low-pass filter, e.g. a Gaussian). However, the performance of SR methods significantly deteriorates when the assumed blur kernel deviates from the true one. We propose a general framework for “blind” super resolution. In particular, we show that: (i) Unlike the common belief, the PSF of the camera is the wrong blur kernel to use in SR algorithms. (ii) We show how the correct SR blur kernel can be recovered directly from the low-resolution image. This is done by exploiting the inherent recurrence property of small natural image patches (either internally within the same image, or externally in a collection of other natural images). In particular, we show that recurrence of small patches across scales of the low-res image (which forms the basis for single-image SR), can also be used for estimating the optimal blur kernel. This leads to significant improvement in SR results.</p><p>5 0.20810306 <a title="103-tfidf-5" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>Author: Netalee Efrat, Daniel Glasner, Alexander Apartsin, Boaz Nadler, Anat Levin</p><p>Abstract: Over the past decade, single image Super-Resolution (SR) research has focused on developing sophisticated image priors, leading to significant advances. Estimating and incorporating the blur model, that relates the high-res and low-res images, has received much less attention, however. In particular, the reconstruction constraint, namely that the blurred and downsampled high-res output should approximately equal the low-res input image, has been either ignored or applied with default fixed blur models. In this work, we examine the relative importance ofthe imageprior and the reconstruction constraint. First, we show that an accurate reconstruction constraint combined with a simple gradient regularization achieves SR results almost as good as those of state-of-the-art algorithms with sophisticated image priors. Second, we study both empirically and theoretically the sensitivity of SR algorithms to the blur model assumed in the reconstruction constraint. We find that an accurate blur model is more important than a sophisticated image prior. Finally, using real camera data, we demonstrate that the default blur models of various SR algorithms may differ from the camera blur, typically leading to over- smoothed results. Our findings highlight the importance of accurately estimating camera blur in reconstructing raw low- res images acquired by an actual camera.</p><p>6 0.18290216 <a title="103-tfidf-6" href="./iccv-2013-Fluttering_Pattern_Generation_Using_Modified_Legendre_Sequence_for_Coded_Exposure_Imaging.html">173 iccv-2013-Fluttering Pattern Generation Using Modified Legendre Sequence for Coded Exposure Imaging</a></p>
<p>7 0.14533356 <a title="103-tfidf-7" href="./iccv-2013-A_Unified_Rolling_Shutter_and_Motion_Blur_Model_for_3D_Visual_Registration.html">32 iccv-2013-A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration</a></p>
<p>8 0.1196638 <a title="103-tfidf-8" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>9 0.11577915 <a title="103-tfidf-9" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>10 0.094406866 <a title="103-tfidf-10" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>11 0.09255448 <a title="103-tfidf-11" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>12 0.091293074 <a title="103-tfidf-12" href="./iccv-2013-Joint_Noise_Level_Estimation_from_Personal_Photo_Collections.html">223 iccv-2013-Joint Noise Level Estimation from Personal Photo Collections</a></p>
<p>13 0.088891044 <a title="103-tfidf-13" href="./iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding.html">14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</a></p>
<p>14 0.083621867 <a title="103-tfidf-14" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>15 0.080249205 <a title="103-tfidf-15" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>16 0.078021415 <a title="103-tfidf-16" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>17 0.075533636 <a title="103-tfidf-17" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>18 0.074219897 <a title="103-tfidf-18" href="./iccv-2013-Local_Signal_Equalization_for_Correspondence_Matching.html">255 iccv-2013-Local Signal Equalization for Correspondence Matching</a></p>
<p>19 0.07257814 <a title="103-tfidf-19" href="./iccv-2013-Depth_from_Combining_Defocus_and_Correspondence_Using_Light-Field_Cameras.html">108 iccv-2013-Depth from Combining Defocus and Correspondence Using Light-Field Cameras</a></p>
<p>20 0.068401083 <a title="103-tfidf-20" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.173), (1, -0.114), (2, -0.037), (3, 0.027), (4, -0.074), (5, 0.057), (6, 0.045), (7, -0.142), (8, 0.093), (9, -0.147), (10, -0.08), (11, -0.305), (12, 0.237), (13, -0.382), (14, -0.2), (15, 0.136), (16, 0.101), (17, -0.101), (18, -0.083), (19, -0.199), (20, -0.111), (21, 0.021), (22, 0.012), (23, -0.108), (24, 0.045), (25, -0.092), (26, -0.086), (27, 0.069), (28, 0.065), (29, -0.066), (30, 0.038), (31, 0.142), (32, -0.053), (33, -0.083), (34, 0.048), (35, -0.092), (36, 0.015), (37, 0.067), (38, 0.054), (39, 0.057), (40, -0.082), (41, 0.07), (42, -0.014), (43, 0.03), (44, -0.011), (45, -0.024), (46, 0.027), (47, -0.01), (48, 0.015), (49, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96404743 <a title="103-lsi-1" href="./iccv-2013-Deblurring_by_Example_Using_Dense_Correspondence.html">103 iccv-2013-Deblurring by Example Using Dense Correspondence</a></p>
<p>Author: Yoav Hacohen, Eli Shechtman, Dani Lischinski</p><p>Abstract: This paper presents a new method for deblurring photos using a sharp reference example that contains some shared content with the blurry photo. Most previous deblurring methods that exploit information from other photos require an accurately registered photo of the same static scene. In contrast, our method aims to exploit reference images where the shared content may have undergone substantial photometric and non-rigid geometric transformations, as these are the kind of reference images most likely to be found in personal photo albums. Our approach builds upon a recent method for examplebased deblurring using non-rigid dense correspondence (NRDC) [11] and extends it in two ways. First, we suggest exploiting information from the reference image not only for blur kernel estimation, but also as a powerful local prior for the non-blind deconvolution step. Second, we introduce a simple yet robust technique for spatially varying blur estimation, rather than assuming spatially uniform blur. Unlike the aboveprevious method, which hasproven successful only with simple deblurring scenarios, we demonstrate that our method succeeds on a variety of real-world examples. We provide quantitative and qualitative evaluation of our method and show that it outperforms the state-of-the-art.</p><p>2 0.96180928 <a title="103-lsi-2" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>Author: Tae Hyun Kim, Byeongjoo Ahn, Kyoung Mu Lee</p><p>Abstract: Most conventional single image deblurring methods assume that the underlying scene is static and the blur is caused by only camera shake. In this paper, in contrast to this restrictive assumption, we address the deblurring problem of general dynamic scenes which contain multiple moving objects as well as camera shake. In case of dynamic scenes, moving objects and background have different blur motions, so the segmentation of the motion blur is required for deblurring each distinct blur motion accurately. Thus, we propose a novel energy model designed with the weighted sum of multiple blur data models, which estimates different motion blurs and their associated pixelwise weights, and resulting sharp image. In this framework, the local weights are determined adaptively and get high values when the corresponding data models have high data fidelity. And, the weight information is used for the segmentation of the motion blur. Non-local regularization of weights are also incorporated to produce more reliable segmentation results. A convex optimization-based method is used for the solution of the proposed energy model. Exper- imental results demonstrate that our method outperforms conventional approaches in deblurring both dynamic scenes and static scenes.</p><p>3 0.90396512 <a title="103-lsi-3" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>Author: Shicheng Zheng, Li Xu, Jiaya Jia</p><p>Abstract: We handle a special type of motion blur considering that cameras move primarily forward or backward. Solving this type of blur is of unique practical importance since nearly all car, traffic and bike-mounted cameras follow out-ofplane translational motion. We start with the study of geometric models and analyze the difficulty of existing methods to deal with them. We also propose a solution accounting for depth variation. Homographies associated with different 3D planes are considered and solved for in an optimization framework. Our method is verified on several natural image examples that cannot be satisfyingly dealt with by previous methods.</p><p>4 0.69549656 <a title="103-lsi-4" href="./iccv-2013-Nonparametric_Blind_Super-resolution.html">293 iccv-2013-Nonparametric Blind Super-resolution</a></p>
<p>Author: Tomer Michaeli, Michal Irani</p><p>Abstract: Super resolution (SR) algorithms typically assume that the blur kernel is known (either the Point Spread Function ‘PSF’ of the camera, or some default low-pass filter, e.g. a Gaussian). However, the performance of SR methods significantly deteriorates when the assumed blur kernel deviates from the true one. We propose a general framework for “blind” super resolution. In particular, we show that: (i) Unlike the common belief, the PSF of the camera is the wrong blur kernel to use in SR algorithms. (ii) We show how the correct SR blur kernel can be recovered directly from the low-resolution image. This is done by exploiting the inherent recurrence property of small natural image patches (either internally within the same image, or externally in a collection of other natural images). In particular, we show that recurrence of small patches across scales of the low-res image (which forms the basis for single-image SR), can also be used for estimating the optimal blur kernel. This leads to significant improvement in SR results.</p><p>5 0.67102724 <a title="103-lsi-5" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>Author: Netalee Efrat, Daniel Glasner, Alexander Apartsin, Boaz Nadler, Anat Levin</p><p>Abstract: Over the past decade, single image Super-Resolution (SR) research has focused on developing sophisticated image priors, leading to significant advances. Estimating and incorporating the blur model, that relates the high-res and low-res images, has received much less attention, however. In particular, the reconstruction constraint, namely that the blurred and downsampled high-res output should approximately equal the low-res input image, has been either ignored or applied with default fixed blur models. In this work, we examine the relative importance ofthe imageprior and the reconstruction constraint. First, we show that an accurate reconstruction constraint combined with a simple gradient regularization achieves SR results almost as good as those of state-of-the-art algorithms with sophisticated image priors. Second, we study both empirically and theoretically the sensitivity of SR algorithms to the blur model assumed in the reconstruction constraint. We find that an accurate blur model is more important than a sophisticated image prior. Finally, using real camera data, we demonstrate that the default blur models of various SR algorithms may differ from the camera blur, typically leading to over- smoothed results. Our findings highlight the importance of accurately estimating camera blur in reconstructing raw low- res images acquired by an actual camera.</p><p>6 0.64336848 <a title="103-lsi-6" href="./iccv-2013-Fluttering_Pattern_Generation_Using_Modified_Legendre_Sequence_for_Coded_Exposure_Imaging.html">173 iccv-2013-Fluttering Pattern Generation Using Modified Legendre Sequence for Coded Exposure Imaging</a></p>
<p>7 0.43040717 <a title="103-lsi-7" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>8 0.39623883 <a title="103-lsi-8" href="./iccv-2013-A_Unified_Rolling_Shutter_and_Motion_Blur_Model_for_3D_Visual_Registration.html">32 iccv-2013-A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration</a></p>
<p>9 0.38997024 <a title="103-lsi-9" href="./iccv-2013-An_Adaptive_Descriptor_Design_for_Object_Recognition_in_the_Wild.html">48 iccv-2013-An Adaptive Descriptor Design for Object Recognition in the Wild</a></p>
<p>10 0.37686738 <a title="103-lsi-10" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>11 0.36569199 <a title="103-lsi-11" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>12 0.35216737 <a title="103-lsi-12" href="./iccv-2013-Fibonacci_Exposure_Bracketing_for_High_Dynamic_Range_Imaging.html">164 iccv-2013-Fibonacci Exposure Bracketing for High Dynamic Range Imaging</a></p>
<p>13 0.34565273 <a title="103-lsi-13" href="./iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding.html">14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</a></p>
<p>14 0.32806805 <a title="103-lsi-14" href="./iccv-2013-Discovering_Details_and_Scene_Structure_with_Hierarchical_Iconoid_Shift.html">117 iccv-2013-Discovering Details and Scene Structure with Hierarchical Iconoid Shift</a></p>
<p>15 0.32399598 <a title="103-lsi-15" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>16 0.3222689 <a title="103-lsi-16" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<p>17 0.30680025 <a title="103-lsi-17" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>18 0.28005213 <a title="103-lsi-18" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>19 0.27995545 <a title="103-lsi-19" href="./iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning.html">212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</a></p>
<p>20 0.27788484 <a title="103-lsi-20" href="./iccv-2013-Efficient_Image_Dehazing_with_Boundary_Constraint_and_Contextual_Regularization.html">135 iccv-2013-Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.04), (7, 0.011), (26, 0.048), (31, 0.027), (42, 0.061), (64, 0.02), (73, 0.031), (81, 0.018), (89, 0.651)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99753791 <a title="103-lda-1" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>Author: Qian-Yi Zhou, Stephen Miller, Vladlen Koltun</p><p>Abstract: We present an approach to reconstruction of detailed scene geometry from range video. Range data produced by commodity handheld cameras suffers from high-frequency errors and low-frequency distortion. Our approach deals with both sources of error by reconstructing locally smooth scene fragments and letting these fragments deform in order to align to each other. We develop a volumetric registration formulation that leverages the smoothness of the deformation to make optimization practical for large scenes. Experimental results demonstrate that our approach substantially increases the fidelity of complex scene geometry reconstructed with commodity handheld cameras.</p><p>2 0.99751639 <a title="103-lda-2" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>Author: Kevin Tang, Bangpeng Yao, Li Fei-Fei, Daphne Koller</p><p>Abstract: In this paper, we tackle the problem of combining features extracted from video for complex event recognition. Feature combination is an especially relevant task in video data, as there are many features we can extract, ranging from image features computed from individual frames to video features that take temporal information into account. To combine features effectively, we propose a method that is able to be selective of different subsets of features, as some features or feature combinations may be uninformative for certain classes. We introduce a hierarchical method for combining features based on the AND/OR graph structure, where nodes in the graph represent combinations of different sets of features. Our method automatically learns the structure of the AND/OR graph using score-based structure learning, and we introduce an inference procedure that is able to efficiently compute structure scores. We present promising results and analysis on the difficult and large-scale 2011 TRECVID Multimedia Event Detection dataset [17].</p><p>3 0.99509215 <a title="103-lda-3" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>Author: Heng Wang, Cordelia Schmid</p><p>Abstract: Recently dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets. This paper improves their performance by taking into account camera motion to correct them. To estimate camera motion, we match feature points between frames using SURF descriptors and dense optical flow, which are shown to be complementary. These matches are, then, used to robustly estimate a homography with RANSAC. Human motion is in general different from camera motion and generates inconsistent matches. To improve the estimation, a human detector is employed to remove these matches. Given the estimated camera motion, we remove trajectories consistent with it. We also use this estimation to cancel out camera motion from the optical flow. This significantly improves motion-based descriptors, such as HOF and MBH. Experimental results onfour challenging action datasets (i.e., Hollywood2, HMDB51, Olympic Sports and UCF50) significantly outperform the current state of the art.</p><p>4 0.99408787 <a title="103-lda-4" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>Author: Dan Xie, Sinisa Todorovic, Song-Chun Zhu</p><p>Abstract: This paper presents an approach to localizing functional objects in surveillance videos without domain knowledge about semantic object classes that may appear in the scene. Functional objects do not have discriminative appearance and shape, but they affect behavior of people in the scene. For example, they “attract” people to approach them for satisfying certain needs (e.g., vending machines could quench thirst), or “repel” people to avoid them (e.g., grass lawns). Therefore, functional objects can be viewed as “dark matter”, emanating “dark energy ” that affects people ’s trajectories in the video. To detect “dark matter” and infer their “dark energy ” field, we extend the Lagrangian mechanics. People are treated as particle-agents with latent intents to approach “dark matter” and thus satisfy their needs, where their motions are subject to a composite “dark energy ” field of all functional objects in the scene. We make the assumption that people take globally optimal paths toward the intended “dark matter” while avoiding latent obstacles. A Bayesian framework is used to probabilistically model: people ’s trajectories and intents, constraint map of the scene, and locations of functional objects. A data-driven Markov Chain Monte Carlo (MCMC) process is used for inference. Our evaluation on videos of public squares and courtyards demonstrates our effectiveness in localizing functional objects and predicting people ’s trajectories in unobserved parts of the video footage.</p><p>same-paper 5 0.9927994 <a title="103-lda-5" href="./iccv-2013-Deblurring_by_Example_Using_Dense_Correspondence.html">103 iccv-2013-Deblurring by Example Using Dense Correspondence</a></p>
<p>Author: Yoav Hacohen, Eli Shechtman, Dani Lischinski</p><p>Abstract: This paper presents a new method for deblurring photos using a sharp reference example that contains some shared content with the blurry photo. Most previous deblurring methods that exploit information from other photos require an accurately registered photo of the same static scene. In contrast, our method aims to exploit reference images where the shared content may have undergone substantial photometric and non-rigid geometric transformations, as these are the kind of reference images most likely to be found in personal photo albums. Our approach builds upon a recent method for examplebased deblurring using non-rigid dense correspondence (NRDC) [11] and extends it in two ways. First, we suggest exploiting information from the reference image not only for blur kernel estimation, but also as a powerful local prior for the non-blind deconvolution step. Second, we introduce a simple yet robust technique for spatially varying blur estimation, rather than assuming spatially uniform blur. Unlike the aboveprevious method, which hasproven successful only with simple deblurring scenarios, we demonstrate that our method succeeds on a variety of real-world examples. We provide quantitative and qualitative evaluation of our method and show that it outperforms the state-of-the-art.</p><p>6 0.98969197 <a title="103-lda-6" href="./iccv-2013-Optimization_Problems_for_Fast_AAM_Fitting_in-the-Wild.html">302 iccv-2013-Optimization Problems for Fast AAM Fitting in-the-Wild</a></p>
<p>7 0.98889416 <a title="103-lda-7" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>8 0.98785031 <a title="103-lda-8" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>9 0.98039347 <a title="103-lda-9" href="./iccv-2013-3D_Scene_Understanding_by_Voxel-CRF.html">2 iccv-2013-3D Scene Understanding by Voxel-CRF</a></p>
<p>10 0.97611898 <a title="103-lda-10" href="./iccv-2013-Shufflets%3A_Shared_Mid-level_Parts_for_Fast_Object_Detection.html">390 iccv-2013-Shufflets: Shared Mid-level Parts for Fast Object Detection</a></p>
<p>11 0.97111392 <a title="103-lda-11" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>12 0.95604497 <a title="103-lda-12" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>13 0.95507061 <a title="103-lda-13" href="./iccv-2013-Large-Scale_Multi-resolution_Surface_Reconstruction_from_RGB-D_Sequences.html">228 iccv-2013-Large-Scale Multi-resolution Surface Reconstruction from RGB-D Sequences</a></p>
<p>14 0.95319474 <a title="103-lda-14" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>15 0.95168996 <a title="103-lda-15" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>16 0.95008361 <a title="103-lda-16" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>17 0.94915617 <a title="103-lda-17" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>18 0.94788229 <a title="103-lda-18" href="./iccv-2013-Active_MAP_Inference_in_CRFs_for_Efficient_Semantic_Segmentation.html">42 iccv-2013-Active MAP Inference in CRFs for Efficient Semantic Segmentation</a></p>
<p>19 0.94765872 <a title="103-lda-19" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>20 0.94738519 <a title="103-lda-20" href="./iccv-2013-Saliency_Detection_in_Large_Point_Sets.html">370 iccv-2013-Saliency Detection in Large Point Sets</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
