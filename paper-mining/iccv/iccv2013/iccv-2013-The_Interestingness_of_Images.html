<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>416 iccv-2013-The Interestingness of Images</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-416" href="#">iccv2013-416</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>416 iccv-2013-The Interestingness of Images</h1>
<br/><p>Source: <a title="iccv-2013-416-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Gygli_The_Interestingness_of_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Michael Gygli, Helmut Grabner, Hayko Riemenschneider, Fabian Nater, Luc Van_Gool</p><p>Abstract: We investigate human interest in photos. Based on our own and others ’psychological experiments, we identify various cues for “interestingness ”, namely aesthetics, unusualness and general preferences. For the ranking of retrieved images, interestingness is more appropriate than cues proposed earlier. Interestingness is, for example, correlated with what people believe they will remember. This is opposed to actual memorability, which is uncorrelated to both of them. We introduce a set of features computationally capturing the three main aspects of visual interestingness that we propose and build an interestingness predictor from them. Its performance is shown on three datasets with varying context, reflecting diverse levels of prior knowledge of the viewers.</p><p>Reference: <a title="iccv-2013-416-reference" href="../iccv2013_reference/iccv-2013-The_Interestingness_of_Images_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Based on our own and others ’psychological experiments, we identify various cues for “interestingness ”, namely aesthetics, unusualness and general preferences. [sent-4, score-0.207]
</p><p>2 For the ranking of retrieved images, interestingness is more appropriate than cues proposed earlier. [sent-5, score-0.826]
</p><p>3 We introduce a set of features computationally capturing the three main aspects of visual interestingness that we propose and build an interestingness predictor from them. [sent-8, score-1.615]
</p><p>4 These include image quality [15],  memorability [14] and aesthetics [10, 11]. [sent-12, score-0.456]
</p><p>5 Yet, a measure that would seem more relevant to automatically quantify is how interesting people find an image and this “interestingness” has hardly been studied so far. [sent-13, score-0.095]
</p><p>6 [11], who used their high-level aesthetics features to train a classifier on Flickr’s interestingness. [sent-16, score-0.288]
</p><p>7 It can predict the interestingness ofthese images well, but it is questionable that these results can be generalized to other image datasets. [sent-17, score-0.796]
</p><p>8 Flickr’s interestingness [7] is based on social behavior, i. [sent-18, score-0.796]
</p><p>9 This measure has not been shown to relate to what people find interesting in images. [sent-21, score-0.117]
</p><p>10 17 ↑mem↓int↓mem↑int Figure 1: Interestingness compared  to  aesthetics and memorability. [sent-31, score-0.265]
</p><p>11 In our own series of psychological experiments we analyze “interestingness” and how it relates to measures such as aesthetics and memorability (Fig. [sent-33, score-0.513]
</p><p>12 There exists indeed a strong correlation between aesthetics and interestingness (Fig. [sent-35, score-1.107]
</p><p>13 However, what is interesting does not necessarily need to be aesthetically pleasing, e. [sent-37, score-0.099]
</p><p>14 While one would also expect a high correlation of memorability and interestingness, our experiments indicate the contrary (Fig. [sent-40, score-0.237]
</p><p>15 2) and show that it is fundamentally different from other properties such as memorability; (ii) propose a set of features able to computationally capture the most important aspects of interestingness (Sec. [sent-44, score-0.849]
</p><p>16 3); (iii) show the performance of these features and an interestingness predictor built from them, on three datasets with varying levels of context (Sec. [sent-45, score-0.838]
</p><p>17 4); (iv) show that the con-  text within which an image is viewed is crucial for the appraisal of interestingness. [sent-46, score-0.066]
</p><p>18 Given the lack of clear-cut and quantifiable psychological findings, we investigate the correlation of interestingness with an extensive list of image attributes, including emotional, aesthetic and content related aspects. [sent-60, score-0.952]
</p><p>19 2a we relate the provided image at-  tributes to the interestingness ground truth we collected (c. [sent-64, score-0.818]
</p><p>20 This figure shows the Spearman rank correlation of all attributes and highlights several with high correlations (either positive or negative). [sent-69, score-0.095]
</p><p>21 When comparing the data of [13] with our own we find that people agree, to a large extent, on which images are interesting, despite personal preferences (c. [sent-78, score-0.169]
</p><p>22 The cues that we implemented were selected on the basis of their experimentally verified correlation with interestingness. [sent-84, score-0.076]
</p><p>23 Interestingness, on the other hand, has its highest correlation with this assumed memorability. [sent-90, score-0.072]
</p><p>24 What a human observer finds interesting is what he wants to remember and believes he  (a) Interestingness correlated with an extensive set of image attributes, based on the data of [13]. [sent-91, score-0.129]
</p><p>25 We compare the attributes to our interestingness score, collected as described in Sec. [sent-92, score-0.845]
</p><p>26 6081  (c) Correlation of scene categories and interest on the dataset of [18], interestingness scores obtained as described in Sec. [sent-98, score-0.917]
</p><p>27 Additionally, we investigated the preference for certain scene types (Fig. [sent-104, score-0.131]
</p><p>28 2c) and found, in agreement with [3], that people prefer natural outdoor scenes rather than man-made scenes. [sent-105, score-0.067]
</p><p>29 While interestingness is higher for images containing sky, actual memorability decreases if sky is present. [sent-106, score-0.987]
</p><p>30 Indeed, when comparing actual memorability and interestingness, we find them to be negatively correlated1 . [sent-107, score-0.214]
</p><p>31 [13]), it makes more sense to select an interesting image, than a memorable but dull one. [sent-109, score-0.11]
</p><p>32 Computational approach for interestingness prediction In this section we propose features that computationally capture the aspects/cues of interestingness which we found most important (c. [sent-112, score-1.645]
</p><p>33 2) and are implementable: unusualness, aesthetics and general preferences. [sent-115, score-0.265]
</p><p>34 Then, we use these to predict the interestingness of images. [sent-116, score-0.796]
</p><p>35 Formally, given an image I are looking for an interwe estingness score s. [sent-118, score-0.066]
</p><p>36 Our pipeline to achieve this task consists of two stages: (i) exploring various features to capture each of the above cues for interestingness and (ii) combining these individual features. [sent-119, score-0.879]
</p><p>37 Instead, we want to capture unusualness in single images from arbitrary scenes. [sent-124, score-0.207]
</p><p>38 Interestingly they found this feature to play a crucial role in the classification of image aesthetics (c. [sent-135, score-0.265]
</p><p>39 1, correlation of interestingness and aesthetics ρ = 0. [sent-138, score-1.107]
</p><p>40 The graph’s energy determines how unusual the configuration of patches is:  =  E(L) ? [sent-148, score-0.083]
</p><p>41 Tashee unary c|o isst e qDui (alli )to oi tsh tehe n uEmu-clidean distance in the descriptor space of a superpixel i to the nearest-neighboring superpixel in the database with label l. [sent-158, score-0.066]
</p><p>42 With L being that optimal labeling, the unusualness by composition is defined as scuonmupsuosael := E(L)/|S| , i. [sent-167, score-0.199]
</p><p>43 Aesthetics To capture the aesthetics of an image, we propose several features that are rather simple in comparison to other, more extensive works in the area. [sent-173, score-0.318]
</p><p>44 For example [11] uses content preferences, such as the presence of people and animals or the preference for certain scene types to classify aesthetically pleasing images. [sent-174, score-0.222]
</p><p>45 We capture such general preferences with global scene descriptors in Sec. [sent-175, score-0.183]
</p><p>46 Machadjik and Hanbury [17] extracted emotion scores from raw pixels. [sent-185, score-0.072]
</p><p>47 Their features are based on the 11663355  empirical findings of [25], which characterized emotions that are caused by color using the space of arousal, pleasure and dominance. [sent-186, score-0.113]
</p><p>48 General preferences Following the observation that certain scene types tend to be more interesting than others (c. [sent-213, score-0.261]
</p><p>49 of SIFT histograms [16] sppryref, and color histograms Spatial pyramids and GIST are known to capture scene categories well. [sent-218, score-0.112]
</p><p>50 As we use a linear model that assumes uncorrelated features, we also applied whitening to decorrelate the features before training the model. [sent-237, score-0.066]
</p><p>51 Experiments In this section we discuss the performance of the different interestingness features. [sent-244, score-0.796]
</p><p>52 As we will see, the strength of the contextual cues that are relevant in the tested setting  ×  determines – in part – which types of features are most effective in capturing interestingness. [sent-245, score-0.075]
</p><p>53 Tshofis3 agrees with [23], where it was shown sufficient to capture scene types and important objects. [sent-250, score-0.088]
</p><p>54 As for the general preference features, we trained the ν-SVR on the training set and optimized the hyperparameters using grid search on the validation set. [sent-255, score-0.082]
</p><p>55 Images with in-between scores are excluded in  the computation of RP, as there is no clear agreement between individuals. [sent-265, score-0.07]
</p><p>56 Suppose that si∗ is the human interestingness score of image Ii, then TopN where PN is the set of N images ranked highest by? [sent-267, score-0.888]
</p><p>57 Since the presented webcam sequential evolving, there is a strong context viewer rates interestingness. [sent-273, score-0.087]
</p><p>58 Secondly, we use  of webcam images are in which a the 8 scene  11663366  GT score: 0. [sent-274, score-0.083]
</p><p>59 Figure 3: An example (Sequence 1) out of the 20 webcam sequences [12] (GT: ground truth, Est: the estimated scores from our method). [sent-320, score-0.098]
</p><p>60 Last, we use the memorability dataset [14], which contains arbitrary photographs and offers practically no context. [sent-322, score-0.191]
</p><p>61 It is annotated with interestingness ground truth, acquired in a psychological study [12]. [sent-328, score-0.853]
</p><p>62 The interestingness score of an image is calculated as the frac-  tion of people who considered it interesting. [sent-329, score-0.912]
</p><p>63 There are only a few interesting events in these streams (mean interestingness score of 0. [sent-330, score-0.928]
</p><p>64 Interestingness is highly subjective and there are individuals who did not consider any image interesting in some sequences. [sent-332, score-0.066]
</p><p>65 We tested each sequence separately and split the remaining sequences randomly into training and validation sets (80% for training / 20% for validation) to train the SVRs and the combination of the features. [sent-342, score-0.067]
</p><p>66 the unusualness scores are computed with respect to the previous frames only (while [12] uses the whole sequence). [sent-346, score-0.209]
</p><p>67 3b shows the correlation of predicted interestingness and ground truth score and Fig. [sent-356, score-0.936]
</p><p>68 3d plots the RecallPrecision curve for the combination of features along with  the five single features having the highest weights. [sent-357, score-0.091]
</p><p>69 Yet, not everything predicted as unusual is rated as interesting by humans, e. [sent-359, score-0.177]
</p><p>70 This is not unusual at the semantic level and therefore not considered interesting by humans. [sent-363, score-0.149]
</p><p>71 Aesthetics and general preference features show a lower performance. [sent-365, score-0.076]
</p><p>72 Figure 4: The 8 scene category dataset (GT: ground truth, Est: the estimated scores from our method). [sent-413, score-0.068]
</p><p>73 Weak context: Scene categories dataset  The 8 scene categories dataset of Oliva and Torralba [18] consists of 2’688 images with a fixed size of 256 256 pixeclosn. [sent-416, score-0.076]
</p><p>74 Tisthse o images are ganenso wtaittehd a w fixitehd th siezier scene categories, which allows us to investigate the correlation between scene types and interestingness. [sent-417, score-0.164]
</p><p>75 We extended this dataset with an interestingness score  by setting up a simple binary task on Amazon Mechanical Turk. [sent-421, score-0.862]
</p><p>76 The interestingness score of an image was calculated as the fraction of selections over views. [sent-424, score-0.883]
</p><p>77 The scene categories provide a weak context, given by the prior on the scene type, which allows to capture novelty/unusualness, as outliers to what are typical images of a certain scene category. [sent-434, score-0.178]
</p><p>78 The algorithm can only capture unusualness with respect to the training images (the prior knowledge of our algorithms), not the observer’s prior experience. [sent-437, score-0.207]
</p><p>79 Therefore a viewer mainly rates the images in this dataset according to aesthetics and general preferences, which transpires from the performance of the individual features. [sent-439, score-0.286]
</p><p>80 ×  General preference features yield the highest performance, as they are able to capture scene type and illumination effects (sphriesft), such as the color of a sunset. [sent-440, score-0.168]
</p><p>81 The features learn the preference for certain scene types (c. [sent-441, score-0.154]
</p><p>82 Arbitrary photos: Memorability dataset The memorability dataset consists of 2’222 images with a fixed size of 256 256 pixels. [sent-448, score-0.191]
</p><p>83 d I itn w [a1s3 ]i ttor investigate 1th4e] memorability of images (see examples in Fig. [sent-451, score-0.215]
</p><p>84 Figure 5: The memorability dataset (GT: ground truth, Est: the estimated scores from our method). [sent-494, score-0.223]
</p><p>85 Despite the different experimental setting, the scores obtained show a strong correlation (ρ = 0. [sent-501, score-0.078]
</p><p>86 Unfortunately, we are not able to capture it for two reasons: (i) What is unusual or novel, in this unconstrained setting, depends on the prior knowledge of the observers, which is unknown to the algorithm. [sent-513, score-0.113]
</p><p>87 (ii) Semantics are crucial in the appraisal of what is unusual in this dataset. [sent-514, score-0.149]
</p><p>88 To predict the interestingness of such an image correctly, we need to understand such semantics. [sent-518, score-0.796]
</p><p>89 It is clearly subjective and depends, to a certain degree, on personal preferences and prior knowledge. [sent-521, score-0.16]
</p><p>90 We proposed a set of features able to capture interestingness in varying contexts. [sent-524, score-0.849]
</p><p>91 With strong context, such as for static webcams, unusualness is the most important cue for interestingness. [sent-525, score-0.177]
</p><p>92 In single, context-free images, general preferences for certain scene types are more important. [sent-526, score-0.195]
</p><p>93 6 illustrates the importance of the different interestingness cues as con-  text gets weaker. [sent-528, score-0.826]
</p><p>94 To overcome the current limitations of interestingness prediction, one would need: (i) an extensive knowledge of what is known to most people, (ii) algorithms able to capture unusualness at the semantic level and (iii) knowledge about personal preferences of the observer. [sent-530, score-1.143]
</p><p>95 11663399  ContextCueFeatureρAPTop5  Table 1: The interestingness cues and their performance on the 3 datasets. [sent-538, score-0.826]
</p><p>96 portance of unusualness features decreases, as the context becomes weak. [sent-569, score-0.219]
</p><p>97 Studying aesthetics in photographic images using a computational approach. [sent-608, score-0.265]
</p><p>98 High level describable attributes for predicting aesthetics and interestingness. [sent-616, score-0.314]
</p><p>99 Affective image classification using features inspired by psychology and art theory. [sent-655, score-0.068]
</p><p>100 Coherence progress: A measure of interestingness based on fixed compressors. [sent-668, score-0.796]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('interestingness', 0.796), ('aesthetics', 0.265), ('gt', 0.233), ('memorability', 0.191), ('unusualness', 0.177), ('preferences', 0.117), ('unusual', 0.083), ('isola', 0.07), ('appraisal', 0.066), ('arousal', 0.066), ('score', 0.066), ('interesting', 0.066), ('psychological', 0.057), ('preference', 0.053), ('attributes', 0.049), ('webcam', 0.047), ('correlation', 0.046), ('psychology', 0.045), ('pleasure', 0.044), ('memorable', 0.044), ('datta', 0.044), ('topn', 0.041), ('lof', 0.039), ('agreement', 0.038), ('scene', 0.036), ('spearman', 0.034), ('superpixel', 0.033), ('interest', 0.033), ('aesth', 0.033), ('aesthetically', 0.033), ('berlyne', 0.033), ('druey', 0.033), ('gtscore', 0.033), ('gygl', 0.033), ('leastinter', 0.033), ('mem', 0.033), ('mostinter', 0.033), ('saceosmthplex', 0.033), ('sppirxefel', 0.033), ('ssel', 0.033), ('stingbot', 0.033), ('outlier', 0.032), ('scores', 0.032), ('lj', 0.032), ('rp', 0.032), ('jpeg', 0.031), ('est', 0.031), ('int', 0.03), ('cues', 0.03), ('capture', 0.03), ('hayko', 0.029), ('irregularities', 0.029), ('aesthetic', 0.029), ('pleasing', 0.029), ('people', 0.029), ('validation', 0.029), ('novelty', 0.028), ('predicted', 0.028), ('biederman', 0.027), ('pleasant', 0.027), ('vessel', 0.027), ('compression', 0.027), ('highest', 0.026), ('pyramids', 0.026), ('superpixels', 0.025), ('conflict', 0.024), ('emotions', 0.024), ('investigate', 0.024), ('boiman', 0.023), ('colorful', 0.023), ('features', 0.023), ('smith', 0.023), ('personal', 0.023), ('negatively', 0.023), ('ii', 0.022), ('findings', 0.022), ('types', 0.022), ('relate', 0.022), ('correlated', 0.022), ('whitening', 0.022), ('flickr', 0.022), ('composition', 0.022), ('calculated', 0.021), ('cor', 0.021), ('emotion', 0.021), ('observer', 0.021), ('viewer', 0.021), ('dhar', 0.021), ('uncorrelated', 0.021), ('gist', 0.021), ('zurich', 0.02), ('workers', 0.02), ('certain', 0.02), ('remember', 0.02), ('categories', 0.02), ('combination', 0.019), ('raw', 0.019), ('sequences', 0.019), ('context', 0.019), ('slic', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="416-tfidf-1" href="./iccv-2013-The_Interestingness_of_Images.html">416 iccv-2013-The Interestingness of Images</a></p>
<p>Author: Michael Gygli, Helmut Grabner, Hayko Riemenschneider, Fabian Nater, Luc Van_Gool</p><p>Abstract: We investigate human interest in photos. Based on our own and others ’psychological experiments, we identify various cues for “interestingness ”, namely aesthetics, unusualness and general preferences. For the ranking of retrieved images, interestingness is more appropriate than cues proposed earlier. Interestingness is, for example, correlated with what people believe they will remember. This is opposed to actual memorability, which is uncorrelated to both of them. We introduce a set of features computationally capturing the three main aspects of visual interestingness that we propose and build an interestingness predictor from them. Its performance is shown on three datasets with varying context, reflecting diverse levels of prior knowledge of the viewers.</p><p>2 0.20463938 <a title="416-tfidf-2" href="./iccv-2013-Modifying_the_Memorability_of_Face_Photographs.html">272 iccv-2013-Modifying the Memorability of Face Photographs</a></p>
<p>Author: Aditya Khosla, Wilma A. Bainbridge, Antonio Torralba, Aude Oliva</p><p>Abstract: Contemporary life bombards us with many new images of faces every day, which poses non-trivial constraints on human memory. The vast majority of face photographs are intended to be remembered, either because of personal relevance, commercial interests or because the pictures were deliberately designed to be memorable. Can we make aportrait more memorable or more forgettable automatically? Here, we provide a method to modify the memorability of individual face photographs, while keeping the identity and other facial traits (e.g. age, attractiveness, and emotional magnitude) of the individual fixed. We show that face photographs manipulated to be more memorable (or more forgettable) are indeed more often remembered (or forgotten) in a crowd-sourcing experiment with an accuracy of 74%. Quantifying and modifying the ‘memorability ’ of a face lends itself to many useful applications in computer vision and graphics, such as mnemonic aids for learning, photo editing applications for social networks and tools for designing memorable advertisements.</p><p>3 0.053241558 <a title="416-tfidf-3" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>Author: Xiao Cai, Feiping Nie, Weidong Cai, Heng Huang</p><p>Abstract: Automatic image categorization has become increasingly important with the development of Internet and the growth in the size of image databases. Although the image categorization can be formulated as a typical multiclass classification problem, two major challenges have been raised by the real-world images. On one hand, though using more labeled training data may improve the prediction performance, obtaining the image labels is a time consuming as well as biased process. On the other hand, more and more visual descriptors have been proposed to describe objects and scenes appearing in images and different features describe different aspects of the visual characteristics. Therefore, how to integrate heterogeneous visual features to do the semi-supervised learning is crucial for categorizing large-scale image data. In this paper, we propose a novel approach to integrate heterogeneous features by performing multi-modal semi-supervised classification on unlabeled as well as unsegmented images. Considering each type of feature as one modality, taking advantage of the large amoun- t of unlabeled data information, our new adaptive multimodal semi-supervised classification (AMMSS) algorithm learns a commonly shared class indicator matrix and the weights for different modalities (image features) simultaneously.</p><p>4 0.047006063 <a title="416-tfidf-4" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>Author: Adriana Kovashka, Kristen Grauman</p><p>Abstract: Current methods learn monolithic attribute predictors, with the assumption that a single model is sufficient to reflect human understanding of a visual attribute. However, in reality, humans vary in how they perceive the association between a named property and image content. For example, two people may have slightly different internal models for what makes a shoe look “formal”, or they may disagree on which of two scenes looks “more cluttered”. Rather than discount these differences as noise, we propose to learn user-specific attribute models. We adapt a generic model trained with annotations from multiple users, tailoring it to satisfy user-specific labels. Furthermore, we propose novel techniques to infer user-specific labels based on transitivity and contradictions in the user’s search history. We demonstrate that adapted attributes improve accuracy over both existing monolithic models as well as models that learn from scratch with user-specific data alone. In addition, we show how adapted attributes are useful to personalize image search, whether with binary or relative attributes.</p><p>5 0.045061667 <a title="416-tfidf-5" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>Author: Abdelaziz Djelouah, Jean-Sébastien Franco, Edmond Boyer, François Le_Clerc, Patrick Pérez</p><p>Abstract: In this paper, we address the problem of object segmentation in multiple views or videos when two or more viewpoints of the same scene are available. We propose a new approach that propagates segmentation coherence information in both space and time, hence allowing evidences in one image to be shared over the complete set. To this aim the segmentation is cast as a single efficient labeling problem over space and time with graph cuts. In contrast to most existing multi-view segmentation methods that rely on some form of dense reconstruction, ours only requires a sparse 3D sampling to propagate information between viewpoints. The approach is thoroughly evaluated on standard multiview datasets, as well as on videos. With static views, results compete with state of the art methods but they are achieved with significantly fewer viewpoints. With multiple videos, we report results that demonstrate the benefit of segmentation propagation through temporal cues.</p><p>6 0.041542098 <a title="416-tfidf-6" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>7 0.039867751 <a title="416-tfidf-7" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>8 0.039674122 <a title="416-tfidf-8" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>9 0.038291622 <a title="416-tfidf-9" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>10 0.037973374 <a title="416-tfidf-10" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>11 0.037614692 <a title="416-tfidf-11" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>12 0.037547626 <a title="416-tfidf-12" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>13 0.03750838 <a title="416-tfidf-13" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>14 0.036925524 <a title="416-tfidf-14" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>15 0.036790743 <a title="416-tfidf-15" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>16 0.036124401 <a title="416-tfidf-16" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>17 0.035824697 <a title="416-tfidf-17" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>18 0.035133187 <a title="416-tfidf-18" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>19 0.035039235 <a title="416-tfidf-19" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>20 0.03498508 <a title="416-tfidf-20" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.098), (1, 0.018), (2, 0.005), (3, -0.042), (4, 0.037), (5, 0.001), (6, -0.014), (7, -0.023), (8, 0.023), (9, -0.01), (10, 0.005), (11, 0.011), (12, 0.008), (13, 0.008), (14, -0.014), (15, -0.005), (16, -0.03), (17, 0.005), (18, 0.009), (19, -0.009), (20, -0.008), (21, -0.02), (22, -0.012), (23, -0.016), (24, -0.005), (25, -0.003), (26, 0.018), (27, -0.001), (28, -0.007), (29, 0.005), (30, 0.008), (31, 0.015), (32, 0.002), (33, 0.022), (34, 0.029), (35, -0.013), (36, 0.03), (37, -0.039), (38, -0.024), (39, 0.024), (40, -0.01), (41, 0.039), (42, -0.064), (43, 0.009), (44, 0.004), (45, -0.048), (46, 0.056), (47, -0.049), (48, -0.021), (49, 0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85859787 <a title="416-lsi-1" href="./iccv-2013-The_Interestingness_of_Images.html">416 iccv-2013-The Interestingness of Images</a></p>
<p>Author: Michael Gygli, Helmut Grabner, Hayko Riemenschneider, Fabian Nater, Luc Van_Gool</p><p>Abstract: We investigate human interest in photos. Based on our own and others ’psychological experiments, we identify various cues for “interestingness ”, namely aesthetics, unusualness and general preferences. For the ranking of retrieved images, interestingness is more appropriate than cues proposed earlier. Interestingness is, for example, correlated with what people believe they will remember. This is opposed to actual memorability, which is uncorrelated to both of them. We introduce a set of features computationally capturing the three main aspects of visual interestingness that we propose and build an interestingness predictor from them. Its performance is shown on three datasets with varying context, reflecting diverse levels of prior knowledge of the viewers.</p><p>2 0.67303193 <a title="416-lsi-2" href="./iccv-2013-Relative_Attributes_for_Large-Scale_Abandoned_Object_Detection.html">350 iccv-2013-Relative Attributes for Large-Scale Abandoned Object Detection</a></p>
<p>Author: Quanfu Fan, Prasad Gabbur, Sharath Pankanti</p><p>Abstract: Effective reduction of false alarms in large-scale video surveillance is rather challenging, especially for applications where abnormal events of interest rarely occur, such as abandoned object detection. We develop an approach to prioritize alerts by ranking them, and demonstrate its great effectiveness in reducing false positives while keeping good detection accuracy. Our approach benefits from a novel representation of abandoned object alerts by relative attributes, namely staticness, foregroundness and abandonment. The relative strengths of these attributes are quantified using a ranking function[19] learnt on suitably designed low-level spatial and temporal features.These attributes of varying strengths are not only powerful in distinguishing abandoned objects from false alarms such as people and light artifacts, but also computationally efficient for large-scale deployment. With these features, we apply a linear ranking algorithm to sort alerts according to their relevance to the end-user. We test the effectiveness of our approach on both public data sets and large ones collected from the real world.</p><p>3 0.62203538 <a title="416-lsi-3" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>Author: Ming Shao, Liangyue Li, Yun Fu</p><p>Abstract: In this paper, we investigate the problem of recognizing occupations of multiple people with arbitrary poses in a photo. Previous work utilizing single person ’s nearly frontal clothing information and fore/background context preliminarily proves that occupation recognition is computationally feasible in computer vision. However, in practice, multiple people with arbitrary poses are common in a photo, and recognizing their occupations is even more challenging. We argue that with appropriately built visual attributes, co-occurrence, and spatial configuration model that is learned through structure SVM, we can recognize multiple people ’s occupations in a photo simultaneously. To evaluate our method’s performance, we conduct extensive experiments on a new well-labeled occupation database with 14 representative occupations and over 7K images. Results on this database validate our method’s effectiveness and show that occupation recognition is solvable in a more general case.</p><p>4 0.60059565 <a title="416-lsi-4" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>Author: C. Lawrence Zitnick, Devi Parikh, Lucy Vanderwende</p><p>Abstract: Sentences that describe visual scenes contain a wide variety of information pertaining to the presence of objects, their attributes and their spatial relations. In this paper we learn the visual features that correspond to semantic phrases derived from sentences. Specifically, we extract predicate tuples that contain two nouns and a relation. The relation may take several forms, such as a verb, preposition, adjective or their combination. We model a scene using a Conditional Random Field (CRF) formulation where each node corresponds to an object, and the edges to their relations. We determine the potentials of the CRF using the tuples extracted from the sentences. We generate novel scenes depicting the sentences’ visual meaning by sampling from the CRF. The CRF is also used to score a set of scenes for a text-based image retrieval task. Our results show we can generate (retrieve) scenes that convey the desired semantic meaning, even when scenes (queries) are described by multiple sentences. Significant improvement is found over several baseline approaches.</p><p>5 0.58886266 <a title="416-lsi-5" href="./iccv-2013-Quadruplet-Wise_Image_Similarity_Learning.html">332 iccv-2013-Quadruplet-Wise Image Similarity Learning</a></p>
<p>Author: Marc T. Law, Nicolas Thome, Matthieu Cord</p><p>Abstract: This paper introduces a novel similarity learning framework. Working with inequality constraints involving quadruplets of images, our approach aims at efficiently modeling similarity from rich or complex semantic label relationships. From these quadruplet-wise constraints, we propose a similarity learning framework relying on a convex optimization scheme. We then study how our metric learning scheme can exploit specific class relationships, such as class ranking (relative attributes), and class taxonomy. We show that classification using the learned metrics gets improved performance over state-of-the-art methods on several datasets. We also evaluate our approach in a new application to learn similarities between webpage screenshots in a fully unsupervised way.</p><p>6 0.5774985 <a title="416-lsi-6" href="./iccv-2013-SIFTpack%3A_A_Compact_Representation_for_Efficient_SIFT_Matching.html">365 iccv-2013-SIFTpack: A Compact Representation for Efficient SIFT Matching</a></p>
<p>7 0.56740093 <a title="416-lsi-7" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>8 0.56516683 <a title="416-lsi-8" href="./iccv-2013-Drosophila_Embryo_Stage_Annotation_Using_Label_Propagation.html">125 iccv-2013-Drosophila Embryo Stage Annotation Using Label Propagation</a></p>
<p>9 0.56376868 <a title="416-lsi-9" href="./iccv-2013-Heterogeneous_Auto-similarities_of_Characteristics_%28HASC%29%3A_Exploiting_Relational_Information_for_Classification.html">193 iccv-2013-Heterogeneous Auto-similarities of Characteristics (HASC): Exploiting Relational Information for Classification</a></p>
<p>10 0.56086653 <a title="416-lsi-10" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<p>11 0.56026798 <a title="416-lsi-11" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>12 0.55859458 <a title="416-lsi-12" href="./iccv-2013-Decomposing_Bag_of_Words_Histograms.html">104 iccv-2013-Decomposing Bag of Words Histograms</a></p>
<p>13 0.55741322 <a title="416-lsi-13" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>14 0.55693185 <a title="416-lsi-14" href="./iccv-2013-Shape_Index_Descriptors_Applied_to_Texture-Based_Galaxy_Analysis.html">388 iccv-2013-Shape Index Descriptors Applied to Texture-Based Galaxy Analysis</a></p>
<p>15 0.55280656 <a title="416-lsi-15" href="./iccv-2013-Learning_to_Rank_Using_Privileged_Information.html">248 iccv-2013-Learning to Rank Using Privileged Information</a></p>
<p>16 0.54385775 <a title="416-lsi-16" href="./iccv-2013-NEIL%3A_Extracting_Visual_Knowledge_from_Web_Data.html">285 iccv-2013-NEIL: Extracting Visual Knowledge from Web Data</a></p>
<p>17 0.5413326 <a title="416-lsi-17" href="./iccv-2013-Unbiased_Metric_Learning%3A_On_the_Utilization_of_Multiple_Datasets_and_Web_Images_for_Softening_Bias.html">431 iccv-2013-Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias</a></p>
<p>18 0.5391649 <a title="416-lsi-18" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>19 0.5270862 <a title="416-lsi-19" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>20 0.51991671 <a title="416-lsi-20" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.057), (7, 0.012), (8, 0.032), (12, 0.023), (26, 0.078), (31, 0.033), (35, 0.015), (42, 0.09), (44, 0.294), (64, 0.036), (73, 0.031), (89, 0.151), (98, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71062738 <a title="416-lda-1" href="./iccv-2013-The_Interestingness_of_Images.html">416 iccv-2013-The Interestingness of Images</a></p>
<p>Author: Michael Gygli, Helmut Grabner, Hayko Riemenschneider, Fabian Nater, Luc Van_Gool</p><p>Abstract: We investigate human interest in photos. Based on our own and others ’psychological experiments, we identify various cues for “interestingness ”, namely aesthetics, unusualness and general preferences. For the ranking of retrieved images, interestingness is more appropriate than cues proposed earlier. Interestingness is, for example, correlated with what people believe they will remember. This is opposed to actual memorability, which is uncorrelated to both of them. We introduce a set of features computationally capturing the three main aspects of visual interestingness that we propose and build an interestingness predictor from them. Its performance is shown on three datasets with varying context, reflecting diverse levels of prior knowledge of the viewers.</p><p>2 0.70610034 <a title="416-lda-2" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>Author: Taegyu Lim, Seunghoon Hong, Bohyung Han, Joon Hee Han</p><p>Abstract: We propose an on-line algorithm to extract a human by foreground/background segmentation and estimate pose of the human from the videos captured by moving cameras. We claim that a virtuous cycle can be created by appropriate interactions between the two modules to solve individual problems. This joint estimation problem is divided into two subproblems, , foreground/background segmentation and pose tracking, which alternate iteratively for optimization; segmentation step generates foreground mask for human pose tracking, and human pose tracking step provides foreground response map for segmentation. The final solution is obtained when the iterative procedure converges. We evaluate our algorithm quantitatively and qualitatively in real videos involving various challenges, and present its outstandingperformance compared to the state-of-the-art techniques for segmentation and pose estimation.</p><p>3 0.70259768 <a title="416-lda-3" href="./iccv-2013-Orderless_Tracking_through_Model-Averaged_Posterior_Estimation.html">303 iccv-2013-Orderless Tracking through Model-Averaged Posterior Estimation</a></p>
<p>Author: Seunghoon Hong, Suha Kwak, Bohyung Han</p><p>Abstract: We propose a novel offline tracking algorithm based on model-averaged posterior estimation through patch matching across frames. Contrary to existing online and offline tracking methods, our algorithm is not based on temporallyordered estimates of target state but attempts to select easyto-track frames first out of the remaining ones without exploiting temporal coherency of target. The posterior of the selected frame is estimated by propagating densities from the already tracked frames in a recursive manner. The density propagation across frames is implemented by an efficient patch matching technique, which is useful for our algorithm since it does not require motion smoothness assumption. Also, we present a hierarchical approach, where a small set of key frames are tracked first and non-key frames are handled by local key frames. Our tracking algorithm is conceptually well-suited for the sequences with abrupt motion, shot changes, and occlusion. We compare our tracking algorithm with existing techniques in real videos with such challenges and illustrate its superior performance qualitatively and quantitatively.</p><p>4 0.66754031 <a title="416-lda-4" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>Author: Ping Wei, Nanning Zheng, Yibiao Zhao, Song-Chun Zhu</p><p>Abstract: Action recognition has often been posed as a classification problem, which assumes that a video sequence only have one action class label and different actions are independent. However, a single human body can perform multiple concurrent actions at the same time, and different actions interact with each other. This paper proposes a concurrent action detection model where the action detection is formulated as a structural prediction problem. In this model, an interval in a video sequence can be described by multiple action labels. An detected action interval is determined both by the unary local detector and the relations with other actions. We use a wavelet feature to represent the action sequence, and design a composite temporal logic descriptor to describe the action relations. The model parameters are trained by structural SVM learning. Given a long video sequence, a sequential decision window search algorithm is designed to detect the actions. Experiments on our new collected concurrent action dataset demonstrate the strength of our method.</p><p>5 0.63566917 <a title="416-lda-5" href="./iccv-2013-Volumetric_Semantic_Segmentation_Using_Pyramid_Context_Features.html">447 iccv-2013-Volumetric Semantic Segmentation Using Pyramid Context Features</a></p>
<p>Author: Jonathan T. Barron, Mark D. Biggin, Pablo Arbeláez, David W. Knowles, Soile V.E. Keranen, Jitendra Malik</p><p>Abstract: We present an algorithm for the per-voxel semantic segmentation of a three-dimensional volume. At the core of our algorithm is a novel “pyramid context” feature, a descriptive representation designed such that exact per-voxel linear classification can be made extremely efficient. This feature not only allows for efficient semantic segmentation but enables other aspects of our algorithm, such as novel learned features and a stacked architecture that can reason about self-consistency. We demonstrate our technique on 3Dfluorescence microscopy data ofDrosophila embryosfor which we are able to produce extremely accurate semantic segmentations in a matter of minutes, and for which other algorithms fail due to the size and high-dimensionality of the data, or due to the difficulty of the task.</p><p>6 0.62286317 <a title="416-lda-6" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>7 0.58122158 <a title="416-lda-7" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>8 0.5738523 <a title="416-lda-8" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>9 0.57071209 <a title="416-lda-9" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>10 0.57057035 <a title="416-lda-10" href="./iccv-2013-3D_Sub-query_Expansion_for_Improving_Sketch-Based_Multi-view_Image_Retrieval.html">3 iccv-2013-3D Sub-query Expansion for Improving Sketch-Based Multi-view Image Retrieval</a></p>
<p>11 0.57026821 <a title="416-lda-11" href="./iccv-2013-A_Method_of_Perceptual-Based_Shape_Decomposition.html">21 iccv-2013-A Method of Perceptual-Based Shape Decomposition</a></p>
<p>12 0.56922656 <a title="416-lda-12" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>13 0.56901443 <a title="416-lda-13" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>14 0.56882608 <a title="416-lda-14" href="./iccv-2013-Beyond_Hard_Negative_Mining%3A_Efficient_Detector_Learning_via_Block-Circulant_Decomposition.html">61 iccv-2013-Beyond Hard Negative Mining: Efficient Detector Learning via Block-Circulant Decomposition</a></p>
<p>15 0.56843746 <a title="416-lda-15" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>16 0.5681209 <a title="416-lda-16" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>17 0.5680148 <a title="416-lda-17" href="./iccv-2013-A_Convex_Optimization_Framework_for_Active_Learning.html">6 iccv-2013-A Convex Optimization Framework for Active Learning</a></p>
<p>18 0.56748307 <a title="416-lda-18" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>19 0.56710052 <a title="416-lda-19" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<p>20 0.56687856 <a title="416-lda-20" href="./iccv-2013-Modifying_the_Memorability_of_Face_Photographs.html">272 iccv-2013-Modifying the Memorability of Face Photographs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
