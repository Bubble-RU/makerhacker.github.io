<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-380" href="#">iccv2013-380</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</h1>
<br/><p>Source: <a title="iccv-2013-380-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Shankar_Semantic_Transform_Weakly_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Sukrit Shankar, Joan Lasenby, Roberto Cipolla</p><p>Abstract: Relative (comparative) attributes are promising for thematic ranking of visual entities, which also aids in recognition tasks [19, 23]. However, attribute rank learning often requires a substantial amount of relational supervision, which is highly tedious, and apparently impracticalfor realworld applications. In this paper, we introduce the Semantic Transform, which under minimal supervision, adaptively finds a semantic feature space along with a class ordering that is related in the best possible way. Such a semantic space is found for every attribute category. To relate the classes under weak supervision, the class ordering needs to be refined according to a cost function in an iterative procedure. This problem is ideally NP-hard, and we thus propose a constrained search tree formulation for the same. Driven by the adaptive semantic feature space representation, our model achieves the best results to date for all of the tasks of relative, absolute and zero-shot classification on two popular datasets.</p><p>Reference: <a title="iccv-2013-380-reference" href="../iccv2013_reference/iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk 0  Abstract Relative (comparative) attributes are promising for thematic ranking of visual entities, which also aids in recognition tasks [19, 23]. [sent-8, score-0.386]
</p><p>2 However, attribute rank learning often requires a substantial amount of relational supervision, which is highly tedious, and apparently impracticalfor realworld applications. [sent-9, score-0.45]
</p><p>3 In this paper, we introduce the Semantic Transform, which under minimal supervision, adaptively finds a semantic feature space along with a class ordering that is related in the best possible way. [sent-10, score-0.726]
</p><p>4 Such a semantic space is found for every attribute category. [sent-11, score-0.502]
</p><p>5 To relate the classes under weak supervision, the class ordering needs to be refined according to a cost function in an iterative procedure. [sent-12, score-0.677]
</p><p>6 Driven by the adaptive semantic feature space representation, our model achieves the best results to date for all of the tasks of relative, absolute and zero-shot classification on two popular datasets. [sent-14, score-0.338]
</p><p>7 For example, images of natural scenes can have attributes such as ‘open ’ and ‘depth-close ’; while images of faces can have the attributes like ‘smiling’ and ‘sad’. [sent-17, score-0.36]
</p><p>8 Inspired by web page ranking formulations, some researchers [19, 23, 14] have recently used relative attributes for visual recognition tasks with considerable success, i. [sent-19, score-0.469]
</p><p>9 instead of the binary knowledge of whether an attribute is present/absent in an image, a realvalued attribute score is considered. [sent-21, score-0.685]
</p><p>10 For example, instead of an image having a binary attribute of ‘smiling’ or ‘not smiling ’, the image has a real-valued attribute score which depicts notions like ‘more smiling than ’ (Fig 1). [sent-22, score-0.809]
</p><p>11 Such relations between the attributes provide semantically richer image descriptions, and have been shown to be useful for relative, absolute and zero-shot classification tasks [19]1 . [sent-23, score-0.395]
</p><p>12 The process typically requires a tedious supervision step, where all the given classes need to be related for every attribute category. [sent-24, score-0.775]
</p><p>13 Using this relational supervision and the training images belonging to each class, a latent feature  space is learnt, on which the projection of (visual) feature descriptors (of images) produces non-binary attribute scores. [sent-25, score-0.822]
</p><p>14 For example, if an image I1 is known to be ‘more open ’ than an image I2, for the attribute ‘open ’, and the latent feature space is modelled as a 1D weighting curve; the weighting curve should guarantee that I1has a greater score than I2, say 0. [sent-26, score-0.722]
</p><p>15 Given a test image, its visual feature descriptor is multiplied by the learnt weighting curve to give an attribute score, which then aids in its ranking and classification. [sent-29, score-0.761]
</p><p>16 Thus, it is important that the class relations are learnt automatically (or with minimum supervision) along with the associated latent feature spaces (per-attribute). [sent-31, score-0.446]
</p><p>17 The accuracy of the attribute scores so that they correctly represent the underlying conceptual space, often depends on the model of latent feature (to be learnt) and/or on the availability of a sufficiently diverse set of training data. [sent-32, score-0.565]
</p><p>18 The attribute score is proportional to the strength of attribute presence. [sent-36, score-0.685]
</p><p>19 ) belonging to a new class (unseen in the training data) 2, which we need to rank relative to the known classes. [sent-40, score-0.347]
</p><p>20 Ranking the data points of this new class is subject to the assumption that the learnt latent feature space encompasses the inherent diversity of the possible set of visual entities that will ever need to be ranked. [sent-41, score-0.557]
</p><p>21 With a large amount of complex data, the learnt model is often not able to relate the new classes in an acceptable way. [sent-43, score-0.428]
</p><p>22 Ranking entities may also involve some sort of semantic supervision in addition to relational supervision, i. [sent-44, score-0.558]
</p><p>23 instead of only giving relations between all given classes, a rough attribute score (reflecting the underlying theme) on a pre-  defined scale (such as 0 to 1) is also given for the known classes. [sent-46, score-0.419]
</p><p>24 In such a case, a model is learnt that conforms to the given attribute scores, which then automatically ranks the classes. [sent-47, score-0.53]
</p><p>25 However, for new classes, the attribute scores again depend on the assumption that the learnt model has encompassed the semantics of all possible data. [sent-48, score-0.589]
</p><p>26 Proposal and Contributions: While many types of models have been proposed for learning the latent feature space under relational and semantic supervision (e. [sent-50, score-0.605]
</p><p>27 ), we aim to answer a more generic question here: Given a set of training images and a latent feature space model, can we learn this latent space (with minimal supervision) so as to relate all types of possible classes in a semantically best possible way  ? [sent-53, score-0.641]
</p><p>28 3  2Note that in real-world situations, broader classes of visual entities are almost always known due to the presence of textual tags. [sent-54, score-0.341]
</p><p>29 However, attribute-specific information is often not present in the form oftextual data with the visual entities, and hence, ranking multimedia entities according to various attributes is largely an unsolved and to an extent a non-targeted problem. [sent-55, score-0.382]
</p><p>30 3Note that the main requirement is that the classes are ordered so as to correctly reflect the underlying attribute-specific themes and the learnt model maximally separates the classes while conforming to the (minimal) supervision that we have. [sent-56, score-0.872]
</p><p>31 We introduce the Semantic Transform, which under minimal supervision, adaptively finds a semantic feature space along with a class ordering that is related in the best possible way. [sent-58, score-0.726]
</p><p>32 To relate the classes under weak supervision, the class ordering needs to be refined according to a cost function in an iterative procedure. [sent-60, score-0.677]
</p><p>33 Our approach is weakly supervised since we assume the semantic attribute score of at least two classes (out of a possible 8 for our datasets) to be known per-attribute. [sent-62, score-0.812]
</p><p>34 This minimal supervision helps us to find an initial feature space (according to a chosen model), which we then keep adapting in an alternating framework, until the best (maximally separated) class ordering is found out. [sent-64, score-0.75]
</p><p>35 To learn the relative ordering, we also present an efficient algorithm using a constrained search tree structure, which makes the generic problem of relating classes tractable while achieving acceptable results. [sent-65, score-0.481]
</p><p>36 Related Work  Binary and Relative Attributes: Learning attribute categories has been shown to be useful to provide cues for object/face recognition [12, 11], zero-shot transfer [12, 21] and part localization [6, 26]. [sent-67, score-0.322]
</p><p>37 While authors in [19, 23] assume the availability of relative ordering of classes on a per-attribute basis, [15] assumes the absence of such information. [sent-71, score-0.706]
</p><p>38 However, the unsupervised method of [15] achieves marginally better results than the supervised counterparts, because of their bottom-up learning structure which is directed more towards selecting some key attributes for classes, rather than ordering classes based on all given attributes. [sent-72, score-0.782]
</p><p>39 random forests, kernel max margin learning) [14, 19] in an attempt to learn feature spaces that produce minimally confusing attribute scores. [sent-80, score-0.36]
</p><p>40 One of the most famous methodologies for ranking entities based on semantic supervision is the supervised topic model [2]. [sent-82, score-0.658]
</p><p>41 Our approach of learning relative ordering (Semantic Transform) is in contrast different from that of the aforementioned approaches. [sent-86, score-0.526]
</p><p>42 We make the use of minimal semantic supervision (rough attribute scores) per-attribute, and then adaptively learn a semantic feature space along with the most plausible class ordering. [sent-87, score-1.129]
</p><p>43 We are thus able to counter the problem of tedious relational supervision while also adaptively learning a feature space that can exude attribute-specific themes for possibly all types of classes. [sent-88, score-0.518]
</p><p>44 a tStrinibcuete w am, an ordering of only two class labels is given (cpm ≺ cqm ) along with the corresponding response variables (rpm < rqm ). [sent-93, score-0.87]
</p><p>45 With the minimal supervision that we consider, our aim  is to learn a latent feature space (which conforms to the semantic response variables) along with the best possible class ordering. [sent-103, score-0.761]
</p><p>46 Learning For each attribute am, we need to learn a latent feature space bm and a relative ordering of the classes cs ; s = 1, . [sent-106, score-1.388]
</p><p>47 Thus, given that cpm ≺ cqm and corresponding response variables rpm < rqm , we require that ∀i ∈ cs1 j ∈ cs2 cs1 ? [sent-110, score-0.799]
</p><p>48 ) produces a score for the attribute am, given the feature space bm and input feature vector xis of an image belonging to class cs. [sent-112, score-0.714]
</p><p>49 Given a test image with its feature vector xt, its score for attribute am is given by ϕ(bTm, xt). [sent-115, score-0.401]
</p><p>50 i,jξi2j⎠⎞  ϕ(bTm, xsi1) − ϕ(bTm,xjs2) > 1− ξij,  (3)  ξij ≥ 0 (4)  Since we have the semantic response variables (rpm , rqm), we make an initial estimate of the latent feature space bm by solving the following optimization problem:  argbm min  ? [sent-123, score-0.611]
</p><p>51 Once we have the initial estimate of our latent feature space, we perturb it to a limited extent so as to approximately conform to the semantic scores (obtained from supervision), while also estimating the most plausible relative ordering of the classes. [sent-165, score-0.892]
</p><p>52 Perturbation not only helps to make the initially learnt feature space adaptive, but also takes into account the inaccuracy inherent in the response variables. [sent-167, score-0.328]
</p><p>53 For a given relative ordering of classes and a perturbation model Δm, the equations (3) and (4) are modified as follows (∀i ∈ cs1 ,j ∈ cs2 , cs1 ? [sent-168, score-0.779]
</p><p>54 Now, we require that along with the weighting curve bm and the perturbation model Δm, the relative ordering constrained by cpm ≺ cqm is also learnt. [sent-192, score-1.301]
</p><p>55 We follow an alternating optimization approach, where we fix a class ordering, and adapt the feature space, and then with this adapted feature space, we refine the relative ordering of the classes. [sent-193, score-0.695]
</p><p>56 The alternating procedure stated above is feasible only when there is some efficient algorithm for refining the relative ordering of classes. [sent-196, score-0.526]
</p><p>57 This problem in itself is NP-hard since the class ordering needs to be refined subject to a cost function. [sent-197, score-0.425]
</p><p>58 r of relative orders, since all of 2, 3 will change their order relative to 1 and 45. [sent-203, score-0.388]
</p><p>59 Each swapping of the classes affects the loss function (encoding maximal separation between classes for a relative ordering) of Eqn (8). [sent-209, score-0.777]
</p><p>60 Since, we are given a prior ordering as cpm ≺ cqm , we constrain the aforementioned swappings such tha≺t cpm ≺ cqm is always satisfied. [sent-210, score-1.216]
</p><p>61 Initialize relative ordering for S classes csusing bmestimated from equation (5). [sent-224, score-0.706]
</p><p>62 This will automatically satisfy the constraint cpm ≺ cqm . [sent-225, score-0.425]
</p><p>63 The last ordering learnt is the final class ordering, and bm + Δm is the adaptively learnt feature space. [sent-232, score-0.998]
</p><p>64 For given Δmand bm, and initial relative ordering of S classes cs1 ≺ cs2 ≺ cs3 ··· ≺ csS, form a search tree with the root nod≺e as s? [sent-234, score-0.759]
</p><p>65 Given a test image It and its associated score for a given attribute am, we randomly select one image Il from the training set whose attribute score is less than that of It, and an image Iu whose attribute score is greater than that of It. [sent-274, score-1.12]
</p><p>66 For this we form a generative model for each class cs in RM similar to [19], once the attribute scores for the training images are obtained and the best relative ordering of classes are learnt. [sent-286, score-1.334]
</p><p>67 Zero-shot Classification For zero-shot classification [19], we have Ss number of seen classes and Su number of unseen classes from a total of S classes. [sent-296, score-0.576]
</p><p>68 For all the seen classes Ss, we learn the relative ordering as specified in Sec 3. [sent-297, score-0.74]
</p><p>69 Similar to [19], we assume that the relative ordering of the unseen class is  known with respect to the seen classes. [sent-299, score-0.703]
</p><p>70 Then, a generative model for the unseen classes can be built using that of the seen classes [19]. [sent-300, score-0.559]
</p><p>71 If an attribute am is not used to define an unseen cl? [sent-307, score-0.399]
</p><p>72 ass cu, μum is taken to be the mean of the scores of all training images for attribute am, and the diagonal entry of Σu((Σu)m,m) to be the variance of the same. [sent-308, score-0.444]
</p><p>73 For relative and absolute classification tasks, all of the classes are seen, i. [sent-320, score-0.494]
</p><p>74 the relative ordering of the classes for each attribute are not known, but instead need to be learnt using minimal prior relative information. [sent-325, score-1.447]
</p><p>75 We compare our automatically learnt class relations with the human annotated ones of [19] in Fig 3(a). [sent-326, score-0.325]
</p><p>76 It can be seen that our learnt class relations are normally the same as that of the human-annotated ones, except for closely related or similarly related classes. [sent-327, score-0.402]
</p><p>77 Also, since our learning approach uses semantic supervision instead of just the relational supervision, our learnt weighting curve is semantically more correct than the one learnt with [19]. [sent-329, score-0.97]
</p><p>78 The weighting curve of [19] does not know what should roughly be the semantic score of a class for a given attribute, unlike  ours where we know for atleast 2 classes per-attribute. [sent-330, score-0.616]
</p><p>79 For relational supervision as considered in [19], the number of times the relations are satisfied by the training images are considerably lesser than what we get with our approach. [sent-333, score-0.364]
</p><p>80 Fig 5(a), 5(b) shows how this affects the relative separation of the classes, and the variance of the attribute scores of the training images. [sent-336, score-0.72]
</p><p>81 Moreover, the classes were so chosen that for any attribute, they were neither the same, nor closely related, in order to have meaningful and discriminative semantic response variables rpm , rqm . [sent-340, score-0.728]
</p><p>82 The response variables are set on a scale of (0, 1) as follows: For 8 classes, in each dataset, we assume equally spaced attribute scores according to the Superlative Relative Ordering. [sent-341, score-0.533]
</p><p>83 These are rough scores that we set for our experiments, but note that we only use response variables for cpm , cqm . [sent-342, score-0.636]
</p><p>84 The yellow shaded areas represent differences in our learnt ordering as compared to [19]. [sent-346, score-0.515]
</p><p>85 The red shaded areas are the classes cpm , cqm (per-attribute) that were considered for supervision. [sent-347, score-0.666]
</p><p>86 (c) Mean (taken across attributes) percentage of the number of training images conforming to the supervised/learnt class ordering for RA and ST. [sent-350, score-0.465]
</p><p>87 We compare the  366  results of our ST approach outlined in Sec 3 with the supervised relative attribute learning (RA) approach of [19] and supervised relative attribute forest (RF) learning approach of [14]. [sent-355, score-1.248]
</p><p>88 Note that unlike [19, 14], we  also learn the relative ordering of classes in conjunction with the weighting curves. [sent-358, score-0.783]
</p><p>89 Typically, it is sug-  gested that one should choose Il and Iu such that the attribute scores for them are not too far nor too close to that of It. [sent-361, score-0.413]
</p><p>90 The previous works of [19, 14] do not give any account of the relative separation of the attribute scores while doing classification on a relative basis. [sent-362, score-0.934]
</p><p>91 75 if the attribute scores for all of the training images (corresponding to the seen classes) are within a normalized range of 0 to 1. [sent-364, score-0.478]
</p><p>92 75 is somewhat large for relative classification, since a lot of attribute scores tend to fall within this scale. [sent-369, score-0.607]
</p><p>93 The mean relative classification values for both the OSR and the PubFig datasets are shown for varying separation scales in Fig 4(d). [sent-371, score-0.327]
</p><p>94 This can be attributed to the fact that weighting curves learnt from ST have semantic information embedded in them. [sent-373, score-0.4]
</p><p>95 The mean recognition results are presented in Fig 4(b) for our ST ap-  proach, for the approach of [19] (RA), the unsupervised relative attribute (UR) learning approach of [15] and the approach of [19] (RF). [sent-377, score-0.543]
</p><p>96 For zero-shot learning, out of 8 classes per dataset, we normally choose 2 unseen classes and consider the remaining 6 classes as seen. [sent-380, score-0.741]
</p><p>97 During testing, we use all of the images of the unseen  (a)  Figure 5: (a) The attribute scores (for training images) with per-class means using [19] for a human-annotated relative order S ≺ M ≺ Z ≺ mVe a≺n J u i≺n gA [1 9≺] Hfor ≺a h uCm. [sent-383, score-0.715]
</p><p>98 Conclusions and Future Work We have introduced the Semantic Transform, which under minimal supervision, can adaptively find a semantic feature space along with a class ordering that is related in the best possible way. [sent-400, score-0.726]
</p><p>99 To relate the classes under weak supervision, the class ordering needs to be refined according to a cost function in an iterative procedure, for which we have proposed a constrained search tree formulation. [sent-401, score-0.757]
</p><p>100 Learning to detect unseen object classes by between-class attribute transfer. [sent-490, score-0.606]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('attribute', 0.322), ('ordering', 0.305), ('btm', 0.266), ('cqm', 0.225), ('classes', 0.207), ('supervision', 0.205), ('cpm', 0.2), ('relative', 0.194), ('attributes', 0.18), ('learnt', 0.176), ('fig', 0.153), ('bm', 0.149), ('semantic', 0.147), ('osr', 0.134), ('entities', 0.134), ('rpm', 0.127), ('rqm', 0.127), ('pubfig', 0.126), ('class', 0.093), ('scores', 0.091), ('swapping', 0.087), ('latent', 0.083), ('separation', 0.082), ('response', 0.081), ('weighting', 0.077), ('unseen', 0.077), ('perturbation', 0.073), ('relational', 0.072), ('tm', 0.071), ('evolutionary', 0.07), ('ranking', 0.068), ('pak', 0.067), ('sec', 0.064), ('supervised', 0.063), ('smiling', 0.062), ('chrk', 0.061), ('lchek', 0.061), ('swappings', 0.061), ('adaptively', 0.061), ('cu', 0.061), ('st', 0.059), ('transform', 0.059), ('cs', 0.057), ('relations', 0.056), ('thematic', 0.054), ('aesthetic', 0.054), ('tree', 0.053), ('classification', 0.051), ('children', 0.051), ('curve', 0.051), ('ur', 0.05), ('minimal', 0.049), ('rf', 0.046), ('relate', 0.045), ('genetic', 0.043), ('normally', 0.043), ('absolute', 0.042), ('ra', 0.041), ('topic', 0.041), ('alterations', 0.041), ('argbm', 0.041), ('penultimate', 0.041), ('themes', 0.041), ('score', 0.041), ('tedious', 0.041), ('iu', 0.04), ('variables', 0.039), ('semantically', 0.039), ('feature', 0.038), ('cl', 0.037), ('outlined', 0.036), ('equ', 0.036), ('slog', 0.036), ('conforming', 0.036), ('node', 0.034), ('shaded', 0.034), ('seen', 0.034), ('generative', 0.034), ('encompass', 0.034), ('plausible', 0.034), ('space', 0.033), ('weakly', 0.032), ('categorizing', 0.032), ('swapped', 0.032), ('conforms', 0.032), ('training', 0.031), ('parent', 0.03), ('eqn', 0.03), ('rank', 0.029), ('ratings', 0.029), ('aids', 0.029), ('uk', 0.028), ('ss', 0.028), ('constrained', 0.027), ('alternating', 0.027), ('learning', 0.027), ('il', 0.027), ('sad', 0.027), ('refined', 0.027), ('tasks', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999917 <a title="380-tfidf-1" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>Author: Sukrit Shankar, Joan Lasenby, Roberto Cipolla</p><p>Abstract: Relative (comparative) attributes are promising for thematic ranking of visual entities, which also aids in recognition tasks [19, 23]. However, attribute rank learning often requires a substantial amount of relational supervision, which is highly tedious, and apparently impracticalfor realworld applications. In this paper, we introduce the Semantic Transform, which under minimal supervision, adaptively finds a semantic feature space along with a class ordering that is related in the best possible way. Such a semantic space is found for every attribute category. To relate the classes under weak supervision, the class ordering needs to be refined according to a cost function in an iterative procedure. This problem is ideally NP-hard, and we thus propose a constrained search tree formulation for the same. Driven by the adaptive semantic feature space representation, our model achieves the best results to date for all of the tasks of relative, absolute and zero-shot classification on two popular datasets.</p><p>2 0.35674411 <a title="380-tfidf-2" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>Author: Xiaoyang Wang, Qiang Ji</p><p>Abstract: This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. As a list of semantically meaningful properties of objects, attributes generally relate to each other statistically. In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and objectindependent attribute relationships. The model utilizes the captured relationships to benefit both attribute prediction and object recognition. Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases.</p><p>3 0.27620551 <a title="380-tfidf-3" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>Author: Adriana Kovashka, Kristen Grauman</p><p>Abstract: Current methods learn monolithic attribute predictors, with the assumption that a single model is sufficient to reflect human understanding of a visual attribute. However, in reality, humans vary in how they perceive the association between a named property and image content. For example, two people may have slightly different internal models for what makes a shoe look “formal”, or they may disagree on which of two scenes looks “more cluttered”. Rather than discount these differences as noise, we propose to learn user-specific attribute models. We adapt a generic model trained with annotations from multiple users, tailoring it to satisfy user-specific labels. Furthermore, we propose novel techniques to infer user-specific labels based on transitivity and contradictions in the user’s search history. We demonstrate that adapted attributes improve accuracy over both existing monolithic models as well as models that learn from scratch with user-specific data alone. In addition, we show how adapted attributes are useful to personalize image search, whether with binary or relative attributes.</p><p>4 0.25530031 <a title="380-tfidf-4" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Devi Parikh, Tsuhan Chen</p><p>Abstract: In recent years, there has been a great deal of progress in describing objects with attributes. Attributes have proven useful for object recognition, image search, face verification, image description, and zero-shot learning. Typically, attributes are either binary or relative: they describe either the presence or absence of a descriptive characteristic, or the relative magnitude of the characteristic when comparing two exemplars. However, prior work fails to model the actual way in which humans use these attributes in descriptive statements of images. Specifically, it does not address the important interactions between the binary and relative aspects of an attribute. In this work we propose a spoken attribute classifier which models a more natural way of using an attribute in a description. For each attribute we train a classifier which captures the specific way this attribute should be used. We show that as a result of using this model, we produce descriptions about images of people that are more natural and specific than past systems.</p><p>5 0.22086805 <a title="380-tfidf-5" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>Author: Naman Turakhia, Devi Parikh</p><p>Abstract: When we look at an image, some properties or attributes of the image stand out more than others. When describing an image, people are likely to describe these dominant attributes first. Attribute dominance is a result of a complex interplay between the various properties present or absent in the image. Which attributes in an image are more dominant than others reveals rich information about the content of the image. In this paper we tap into this information by modeling attribute dominance. We show that this helps improve the performance of vision systems on a variety of human-centric applications such as zero-shot learning, image search and generating textual descriptions of images.</p><p>6 0.21525489 <a title="380-tfidf-6" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>7 0.21084669 <a title="380-tfidf-7" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>8 0.15325181 <a title="380-tfidf-8" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>9 0.15155016 <a title="380-tfidf-9" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>10 0.13298358 <a title="380-tfidf-10" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>11 0.13214689 <a title="380-tfidf-11" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>12 0.1275969 <a title="380-tfidf-12" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>13 0.12684953 <a title="380-tfidf-13" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>14 0.11998735 <a title="380-tfidf-14" href="./iccv-2013-Quadruplet-Wise_Image_Similarity_Learning.html">332 iccv-2013-Quadruplet-Wise Image Similarity Learning</a></p>
<p>15 0.11154144 <a title="380-tfidf-15" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>16 0.11075121 <a title="380-tfidf-16" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>17 0.11031047 <a title="380-tfidf-17" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>18 0.10742519 <a title="380-tfidf-18" href="./iccv-2013-Learning_Graph_Matching%3A_Oriented_to_Category_Modeling_from_Cluttered_Scenes.html">237 iccv-2013-Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes</a></p>
<p>19 0.10587525 <a title="380-tfidf-19" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>20 0.10561946 <a title="380-tfidf-20" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.203), (1, 0.173), (2, -0.064), (3, -0.163), (4, 0.163), (5, -0.028), (6, -0.106), (7, -0.171), (8, 0.223), (9, 0.119), (10, -0.069), (11, 0.023), (12, -0.0), (13, -0.013), (14, -0.05), (15, 0.005), (16, -0.017), (17, 0.051), (18, -0.028), (19, 0.011), (20, -0.009), (21, 0.046), (22, 0.008), (23, -0.048), (24, -0.046), (25, -0.036), (26, 0.096), (27, 0.008), (28, 0.027), (29, 0.085), (30, 0.031), (31, 0.02), (32, -0.075), (33, 0.03), (34, -0.043), (35, -0.032), (36, -0.036), (37, -0.023), (38, -0.01), (39, -0.047), (40, 0.005), (41, 0.011), (42, -0.021), (43, -0.017), (44, -0.003), (45, -0.027), (46, 0.009), (47, -0.001), (48, 0.001), (49, -0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96651298 <a title="380-lsi-1" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>Author: Sukrit Shankar, Joan Lasenby, Roberto Cipolla</p><p>Abstract: Relative (comparative) attributes are promising for thematic ranking of visual entities, which also aids in recognition tasks [19, 23]. However, attribute rank learning often requires a substantial amount of relational supervision, which is highly tedious, and apparently impracticalfor realworld applications. In this paper, we introduce the Semantic Transform, which under minimal supervision, adaptively finds a semantic feature space along with a class ordering that is related in the best possible way. Such a semantic space is found for every attribute category. To relate the classes under weak supervision, the class ordering needs to be refined according to a cost function in an iterative procedure. This problem is ideally NP-hard, and we thus propose a constrained search tree formulation for the same. Driven by the adaptive semantic feature space representation, our model achieves the best results to date for all of the tasks of relative, absolute and zero-shot classification on two popular datasets.</p><p>2 0.94330627 <a title="380-lsi-2" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>Author: Xiaoyang Wang, Qiang Ji</p><p>Abstract: This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. As a list of semantically meaningful properties of objects, attributes generally relate to each other statistically. In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and objectindependent attribute relationships. The model utilizes the captured relationships to benefit both attribute prediction and object recognition. Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases.</p><p>3 0.92616314 <a title="380-lsi-3" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Devi Parikh, Tsuhan Chen</p><p>Abstract: In recent years, there has been a great deal of progress in describing objects with attributes. Attributes have proven useful for object recognition, image search, face verification, image description, and zero-shot learning. Typically, attributes are either binary or relative: they describe either the presence or absence of a descriptive characteristic, or the relative magnitude of the characteristic when comparing two exemplars. However, prior work fails to model the actual way in which humans use these attributes in descriptive statements of images. Specifically, it does not address the important interactions between the binary and relative aspects of an attribute. In this work we propose a spoken attribute classifier which models a more natural way of using an attribute in a description. For each attribute we train a classifier which captures the specific way this attribute should be used. We show that as a result of using this model, we produce descriptions about images of people that are more natural and specific than past systems.</p><p>4 0.92446506 <a title="380-lsi-4" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>Author: Naman Turakhia, Devi Parikh</p><p>Abstract: When we look at an image, some properties or attributes of the image stand out more than others. When describing an image, people are likely to describe these dominant attributes first. Attribute dominance is a result of a complex interplay between the various properties present or absent in the image. Which attributes in an image are more dominant than others reveals rich information about the content of the image. In this paper we tap into this information by modeling attribute dominance. We show that this helps improve the performance of vision systems on a variety of human-centric applications such as zero-shot learning, image search and generating textual descriptions of images.</p><p>5 0.88948965 <a title="380-lsi-5" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>Author: Adriana Kovashka, Kristen Grauman</p><p>Abstract: Current methods learn monolithic attribute predictors, with the assumption that a single model is sufficient to reflect human understanding of a visual attribute. However, in reality, humans vary in how they perceive the association between a named property and image content. For example, two people may have slightly different internal models for what makes a shoe look “formal”, or they may disagree on which of two scenes looks “more cluttered”. Rather than discount these differences as noise, we propose to learn user-specific attribute models. We adapt a generic model trained with annotations from multiple users, tailoring it to satisfy user-specific labels. Furthermore, we propose novel techniques to infer user-specific labels based on transitivity and contradictions in the user’s search history. We demonstrate that adapted attributes improve accuracy over both existing monolithic models as well as models that learn from scratch with user-specific data alone. In addition, we show how adapted attributes are useful to personalize image search, whether with binary or relative attributes.</p><p>6 0.79840893 <a title="380-lsi-6" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>7 0.78525436 <a title="380-lsi-7" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>8 0.71455884 <a title="380-lsi-8" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>9 0.67219239 <a title="380-lsi-9" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<p>10 0.63719988 <a title="380-lsi-10" href="./iccv-2013-Relative_Attributes_for_Large-Scale_Abandoned_Object_Detection.html">350 iccv-2013-Relative Attributes for Large-Scale Abandoned Object Detection</a></p>
<p>11 0.62737137 <a title="380-lsi-11" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>12 0.61325294 <a title="380-lsi-12" href="./iccv-2013-NEIL%3A_Extracting_Visual_Knowledge_from_Web_Data.html">285 iccv-2013-NEIL: Extracting Visual Knowledge from Web Data</a></p>
<p>13 0.6106537 <a title="380-lsi-13" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>14 0.6017794 <a title="380-lsi-14" href="./iccv-2013-Quadruplet-Wise_Image_Similarity_Learning.html">332 iccv-2013-Quadruplet-Wise Image Similarity Learning</a></p>
<p>15 0.59367549 <a title="380-lsi-15" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>16 0.5933724 <a title="380-lsi-16" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>17 0.57899249 <a title="380-lsi-17" href="./iccv-2013-Learning_to_Rank_Using_Privileged_Information.html">248 iccv-2013-Learning to Rank Using Privileged Information</a></p>
<p>18 0.52737659 <a title="380-lsi-18" href="./iccv-2013-The_Interestingness_of_Images.html">416 iccv-2013-The Interestingness of Images</a></p>
<p>19 0.52154052 <a title="380-lsi-19" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>20 0.51726478 <a title="380-lsi-20" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.063), (7, 0.011), (12, 0.025), (26, 0.057), (31, 0.036), (34, 0.042), (40, 0.014), (42, 0.105), (48, 0.012), (64, 0.348), (73, 0.031), (89, 0.144), (98, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97054815 <a title="380-lda-1" href="./iccv-2013-Cross-View_Action_Recognition_over_Heterogeneous_Feature_Spaces.html">99 iccv-2013-Cross-View Action Recognition over Heterogeneous Feature Spaces</a></p>
<p>Author: Xinxiao Wu, Han Wang, Cuiwei Liu, Yunde Jia</p><p>Abstract: In cross-view action recognition, “what you saw” in one view is different from “what you recognize ” in another view. The data distribution even the feature space can change from one view to another due to the appearance and motion of actions drastically vary across different views. In this paper, we address the problem of transferring action models learned in one view (source view) to another different view (target view), where action instances from these two views are represented by heterogeneous features. A novel learning method, called Heterogeneous Transfer Discriminantanalysis of Canonical Correlations (HTDCC), is proposed to learn a discriminative common feature space for linking source and target views to transfer knowledge between them. Two projection matrices that respectively map data from source and target views into the common space are optimized via simultaneously minimizing the canonical correlations of inter-class samples and maximizing the intraclass canonical correlations. Our model is neither restricted to corresponding action instances in the two views nor restricted to the same type of feature, and can handle only a few or even no labeled samples available in the target view. To reduce the data distribution mismatch between the source and target views in the commonfeature space, a nonparametric criterion is included in the objective function. We additionally propose a joint weight learning method to fuse multiple source-view action classifiers for recognition in the target view. Different combination weights are assigned to different source views, with each weight presenting how contributive the corresponding source view is to the target view. The proposed method is evaluated on the IXMAS multi-view dataset and achieves promising results.</p><p>2 0.95666307 <a title="380-lda-2" href="./iccv-2013-Online_Robust_Non-negative_Dictionary_Learning_for_Visual_Tracking.html">298 iccv-2013-Online Robust Non-negative Dictionary Learning for Visual Tracking</a></p>
<p>Author: Naiyan Wang, Jingdong Wang, Dit-Yan Yeung</p><p>Abstract: This paper studies the visual tracking problem in video sequences and presents a novel robust sparse tracker under the particle filter framework. In particular, we propose an online robust non-negative dictionary learning algorithm for updating the object templates so that each learned template can capture a distinctive aspect of the tracked object. Another appealing property of this approach is that it can automatically detect and reject the occlusion and cluttered background in a principled way. In addition, we propose a new particle representation formulation using the Huber loss function. The advantage is that it can yield robust estimation without using trivial templates adopted by previous sparse trackers, leading to faster computation. We also reveal the equivalence between this new formulation and the previous one which uses trivial templates. The proposed tracker is empirically compared with state-of-the-art trackers on some challenging video sequences. Both quantitative and qualitative comparisons show that our proposed tracker is superior and more stable.</p><p>3 0.94120145 <a title="380-lda-3" href="./iccv-2013-Constant_Time_Weighted_Median_Filtering_for_Stereo_Matching_and_Beyond.html">88 iccv-2013-Constant Time Weighted Median Filtering for Stereo Matching and Beyond</a></p>
<p>Author: Ziyang Ma, Kaiming He, Yichen Wei, Jian Sun, Enhua Wu</p><p>Abstract: Despite the continuous advances in local stereo matching for years, most efforts are on developing robust cost computation and aggregation methods. Little attention has been seriously paid to the disparity refinement. In this work, we study weighted median filtering for disparity refinement. We discover that with this refinement, even the simple box filter aggregation achieves comparable accuracy with various sophisticated aggregation methods (with the same refinement). This is due to the nice weighted median filtering properties of removing outlier error while respecting edges/structures. This reveals that the previously overlooked refinement can be at least as crucial as aggregation. We also develop the first constant time algorithmfor the previously time-consuming weighted median filter. This makes the simple combination “box aggregation + weighted median ” an attractive solution in practice for both speed and accuracy. As a byproduct, the fast weighted median filtering unleashes its potential in other applications that were hampered by high complexities. We show its superiority in various applications such as depth upsampling, clip-art JPEG artifact removal, and image stylization.</p><p>4 0.92740357 <a title="380-lda-4" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>Author: Siyu Tang, Mykhaylo Andriluka, Anton Milan, Konrad Schindler, Stefan Roth, Bernt Schiele</p><p>Abstract: People tracking in crowded real-world scenes is challenging due to frequent and long-term occlusions. Recent tracking methods obtain the image evidence from object (people) detectors, but typically use off-the-shelf detectors and treat them as black box components. In this paper we argue that for best performance one should explicitly train people detectors on failure cases of the overall tracker instead. To that end, we first propose a novel joint people detector that combines a state-of-the-art single person detector with a detector for pairs of people, which explicitly exploits common patterns of person-person occlusions across multiple viewpoints that are a frequent failure case for tracking in crowded scenes. To explicitly address remaining failure modes of the tracker we explore two methods. First, we analyze typical failures of trackers and train a detector explicitly on these cases. And second, we train the detector with the people tracker in the loop, focusing on the most common tracker failures. We show that our joint multi-person detector significantly improves both de- tection accuracy as well as tracker performance, improving the state-of-the-art on standard benchmarks.</p><p>5 0.91267383 <a title="380-lda-5" href="./iccv-2013-Finding_Actors_and_Actions_in_Movies.html">166 iccv-2013-Finding Actors and Actions in Movies</a></p>
<p>Author: P. Bojanowski, F. Bach, I. Laptev, J. Ponce, C. Schmid, J. Sivic</p><p>Abstract: We address the problem of learning a joint model of actors and actions in movies using weak supervision provided by scripts. Specifically, we extract actor/action pairs from the script and use them as constraints in a discriminative clustering framework. The corresponding optimization problem is formulated as a quadratic program under linear constraints. People in video are represented by automatically extracted and tracked faces together with corresponding motion features. First, we apply the proposed framework to the task of learning names of characters in the movie and demonstrate significant improvements over previous methods used for this task. Second, we explore the joint actor/action constraint and show its advantage for weakly supervised action learning. We validate our method in the challenging setting of localizing and recognizing characters and their actions in feature length movies Casablanca and American Beauty.</p><p>6 0.89523882 <a title="380-lda-6" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>7 0.87506229 <a title="380-lda-7" href="./iccv-2013-Video_Motion_for_Every_Visible_Point.html">441 iccv-2013-Video Motion for Every Visible Point</a></p>
<p>8 0.87469929 <a title="380-lda-8" href="./iccv-2013-Incorporating_Cloud_Distribution_in_Sky_Representation.html">215 iccv-2013-Incorporating Cloud Distribution in Sky Representation</a></p>
<p>same-paper 9 0.84719384 <a title="380-lda-9" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>10 0.82211071 <a title="380-lda-10" href="./iccv-2013-Orderless_Tracking_through_Model-Averaged_Posterior_Estimation.html">303 iccv-2013-Orderless Tracking through Model-Averaged Posterior Estimation</a></p>
<p>11 0.7968846 <a title="380-lda-11" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>12 0.7844553 <a title="380-lda-12" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>13 0.77769792 <a title="380-lda-13" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>14 0.76582068 <a title="380-lda-14" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>15 0.75668216 <a title="380-lda-15" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>16 0.75055146 <a title="380-lda-16" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>17 0.74772894 <a title="380-lda-17" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>18 0.72097576 <a title="380-lda-18" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>19 0.72075462 <a title="380-lda-19" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>20 0.70912188 <a title="380-lda-20" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
