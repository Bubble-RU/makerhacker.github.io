<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-208" href="#">iccv2013-208</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</h1>
<br/><p>Source: <a title="iccv-2013-208-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Wang_Image_Co-segmentation_via_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Fan Wang, Qixing Huang, Leonidas J. Guibas</p><p>Abstract: Joint segmentation of image sets has great importance for object recognition, image classification, and image retrieval. In this paper, we aim to jointly segment a set of images starting from a small number of labeled images or none at all. To allow the images to share segmentation information with each other, we build a network that contains segmented as well as unsegmented images, and extract functional maps between connected image pairs based on image appearance features. These functional maps act as general property transporters between the images and, in particular, are used to transfer segmentations. We define and operate in a reduced functional space optimized so that the functional maps approximately satisfy cycle-consistency under composition in the network. A joint optimization framework is proposed to simultaneously generate all segmentation functions over the images so that they both align with local segmentation cues in each particular image, and agree with each other under network transportation. This formulation allows us to extract segmentations even with no training data, but can also exploit such data when available. The collective effect of the joint processing using functional maps leads to accurate information sharing among images and yields superior segmentation results, as shown on the iCoseg, MSRC, and PASCAL data sets.</p><p>Reference: <a title="iccv-2013-208-reference" href="../iccv2013_reference/iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Joint segmentation of image sets has great importance for object recognition, image classification, and image retrieval. [sent-6, score-0.217]
</p><p>2 In this paper, we aim to jointly segment a set of images starting from a small number of labeled images or none at all. [sent-7, score-0.212]
</p><p>3 To allow the images to share segmentation information with each other, we build a network that contains segmented as well as unsegmented images, and extract functional maps between connected image pairs based on image appearance features. [sent-8, score-1.237]
</p><p>4 These functional maps act as general property transporters between the images and, in particular, are used to transfer segmentations. [sent-9, score-0.931]
</p><p>5 We define and operate in a reduced functional space optimized so that the functional maps approximately satisfy cycle-consistency under composition in the network. [sent-10, score-1.561]
</p><p>6 A joint optimization framework is proposed to simultaneously generate all segmentation functions over the images so that they both align with local segmentation cues in each particular image, and agree with each other under network transportation. [sent-11, score-0.718]
</p><p>7 The collective effect of the joint processing using functional  maps leads to accurate information sharing among images and yields superior segmentation results, as shown on the iCoseg, MSRC, and PASCAL data sets. [sent-13, score-1.152]
</p><p>8 Compared with single image segmentation, co-segmentation has the potential of aggregating information from multiple images to improve the segmentation of individual images. [sent-18, score-0.265]
</p><p>9 So far this has been approached by computing point-based maps between pairs of images using local descriptors such as SIFT. [sent-20, score-0.293]
</p><p>10 It alleviates the imperfections of the individual maps and the difficulty of making path  —  choices when transferring information between images. [sent-24, score-0.278]
</p><p>11 In this paper we present a novel framework, called consistent functional maps, for representing and computing consistent appearance relations among a collection of images. [sent-25, score-0.839]
</p><p>12 The proposed framework modifies the functional map framework [16] to use it on pairs of images instead of shapes, and further extends it to handle multiple images under consistency constraints. [sent-26, score-0.902]
</p><p>13 The basic idea of functional maps is to equip each image with a linear functional space, and represent relations between images as linear maps between these functional spaces. [sent-27, score-2.423]
</p><p>14 This functional representation is powerful because image descriptors and segmentations can be considered as functions on images, and their relations can thus be encoded as linear constraints on the linear map between the two spaces. [sent-28, score-0.902]
</p><p>15 In particular, with a properly chosen basis for each functional space, optimizing functional maps between images becomes optimization of the familiar transformation matrices. [sent-29, score-1.669]
</p><p>16 Most importantly, for our purposes, in a network of images connected by such functional maps, the functional setting admits of an easy approach for enforcing the consistency of these maps. [sent-31, score-1.518]
</p><p>17 By introducing a latent basis for the functional space associated with each image, the consistency of the functional maps is equivalent to the fact that each functional map transforms the source image latent basis to the target image latent basis. [sent-32, score-2.618]
</p><p>18 This leads to a simple formulation of the cycle-consistency constraint, enabling us to compute consistent functional maps among multiple  images by solving a tractable optimization problem. [sent-33, score-1.002]
</p><p>19 Given the consistent functional maps computed between 884499  pairs of images, we jointly optimize segmentations of all images so that they (i) are consistent with each other when transported via the functional maps, and (ii) agree with segmentation boundary clues presented on each image (e. [sent-34, score-2.235]
</p><p>20 We note that this optimization procedure is easily modified to incorporate labeled images as input, in which case we simply let the labeled images provide additional clues for segmentation. [sent-37, score-0.296]
</p><p>21 Related Work Earlier work on joint segmentation mainly compared the visual features of image pairs, such as foreground color histogram [18], SIFT [14], saliency [5], and Gabor features [9]. [sent-42, score-0.333]
</p><p>22 In the supervised setting, a pool of object-like candidate segmentations were generated and a random forest regressor was trained to score each pair of segmentations [24]. [sent-47, score-0.23]
</p><p>23 Recently, segmentation masks were transferred from the training windows to similar windows in test images [11], and images were jointly segmented in a energy minimization framework with multiple unary potentials [12]. [sent-50, score-0.499]
</p><p>24 Functional maps are related to graph matching for feature correspondences in object categorization and image matching [4, 13, 7, 23]. [sent-51, score-0.328]
</p><p>25 The edges of the graph reflect the underlying spatial structure of the image, such as region proximity, and are used to guarantee the geometric consistency of nearby regions during matching. [sent-53, score-0.175]
</p><p>26 However, our framework solves the graph matching problem in a functional setting, which is fundamentally different from point-wise correspondences and leads to a linear system with an easilyobtained optimal solution for each pairwise functional map. [sent-57, score-1.387]
</p><p>27 These approaches are typically formulated as solving constrained optimization problems, where the objective functions encode the score of maps, and the constraints enforce the consistency of maps along cycles. [sent-59, score-0.441]
</p><p>28 However, these approaches assume that correct maps are dominant in the graph so that the small number of bad maps can be identified through their participation in many bad cycles. [sent-60, score-0.496]
</p><p>29 The consistency property of functional maps is also related to diffusion maps [6] and vector diffusion maps [22]. [sent-62, score-1.491]
</p><p>30 Building blocks To better explain the proposed approach, we present a brief introduction to functional maps adapted from [16] for mapping meshed 3D shapes to our super-pixel setting, and formulate the cycle-consistency constraint. [sent-76, score-0.875]
</p><p>31 We formulate image segmentation as computing an indicator functions on the super885500  Figure 1: Overview  of the proposed framework. [sent-79, score-0.402]
</p><p>32 Any segmentation Oi ∈ Pi corresponds otof a binary oinnd Kica. [sent-87, score-0.217]
</p><p>33 On the o(tph)er = = ha 1n,d∀,p any Ofunction f ∈( p F)i = =ind 0u,c∀pes a segmentation Othei o=t h{epr|f ha(pn)d > a tyi} f,u given a properly chosen threshold ti. [sent-89, score-0.217]
</p><p>34 To improve efficiency, we reduce the search space of segmentation indicator functions to a subspaceFi ⊂ Fi ofdimension M < K foreachimage tIoi, spanned by a ⊂b as Fis Bi = (bi1 , · · · , biM). [sent-91, score-0.373]
</p><p>35 Relations between images can be easily described as linear functional maps in the functional setting. [sent-97, score-1.529]
</p><p>36 Specifically, a functional map from Fi to Fj is given by a pmecatirfiicxa Xij a∈ f RnMcti×onMa,l mwhapere f Xij maps a function f ∈ Fi with coeffi∈cieRn t vector f to the fumncatpiosn a af f? [sent-98, score-0.882]
</p><p>37 In the functional setting, the cycleconsistency constraint can be described as the fact that a transported function along any loop should be identical to the original function. [sent-104, score-0.779]
</p><p>38 Suppose we are given a connected directed graph G that connects some pairs of images in I. [sent-105, score-0.185]
</p><p>39 L Fet C denote the space of all cycles in wG,i tthhe end gthee ( cycle consistency ecnoontset trahient s can eb oef d aellsc cryibceleds as  X∀(iIki 00,·I·i 1,X·i 1·i2 ,XIi k0)i1 ∈f C =, f f ∈ Fi0. [sent-108, score-0.267]
</p><p>40 This latent space is expected ,to· ·in· c,ylude functions that are consistent across multiple images, e. [sent-112, score-0.25]
</p><p>41 The first stage computes a reduced functional space on each image. [sent-125, score-0.71]
</p><p>42 The second stage optimizes consistent functional maps between pairs of images. [sent-126, score-0.984]
</p><p>43 The objective function combines a term that quantifies the quality ofpair-wise functional maps, and another term that enforces the consistency among all functional maps. [sent-127, score-1.431]
</p><p>44 Given these consistent functional maps, the final stage generates the segmentations by jointly optimizing segmentation functions that (i) align with the segmentation clues on each image and (ii) are consistent with neighboring image segmentations after transportation by functional maps. [sent-128, score-2.344]
</p><p>45 Reduced Functional Spaces We choose Fi as the eigen-space spanned by the first M eigenvectors of the normalized graph Laplacian Li ∈ RK×K, motivated by some practical success of using thes∈e eigenvectors or combinations of them as segmentation indicator functions [20]. [sent-131, score-0.632]
</p><p>46 An important distinction of the proposed approach from previous methods is that we do not commit to any segmentation indicator function at this stage. [sent-132, score-0.274]
</p><p>47 Instead, these are jointly selected later over all input images using optimized functional maps. [sent-133, score-0.786]
</p><p>48 The segmentation functions can be approximated well in the reduced eigen-space. [sent-134, score-0.365]
</p><p>49 2 shows that when M = 8855 11  Approximation of (a) with 30 basis functions (18. [sent-136, score-0.172]
</p><p>50 2% error); (c) Binary version of (b) by thresholding (3% error); (d) The gray lines are reconstruction errors of typical segmentation functions; the red line shows the case in (b) and (c); the blue line is the average of all cases. [sent-137, score-0.217]
</p><p>51 30, the normalized error between the original segmentation function and its projection to the reduced space is usually less than 20%. [sent-138, score-0.295]
</p><p>52 e2r) w, othrdes, a optimizing othre i segmentation cfeudnc ttioo n3s% i. [sent-142, score-0.254]
</p><p>53 1), and how to compute consistent functional maps faogre se (a§ch5. [sent-146, score-0.924]
</p><p>54 Similarity graph We compute a sparse similarity graph G for the input image ccoomllepcuttieon a I s,p arsned only compute f Gun fctoiron tahle maps bimetawgeeen c pairs oofn images specified by Gute. [sent-152, score-0.441]
</p><p>55 Aligning image features When computing the functional map Xij from image Ii to image Ij, it is natural to enforce that Xij agrees with the features computed from the images. [sent-164, score-0.671]
</p><p>56 In the functional setting, this is equivalent to the constraint that Xijdi ≈ dj, where di and dj are corresponding descriptor fun≈ctio dns on the two images represented in the reduced functional space. [sent-165, score-1.457]
</p><p>57 Figure 3: Visualization of some probe functions that are put in correspondence by the functional map. [sent-178, score-0.763]
</p><p>58 4 shows an example of functional map with and without the regularization term — the one with regularization is closer to a diagonal matrix, meaning that eigenvectors are transported only to their counterparts with similar frequencies. [sent-206, score-0.84]
</p><p>59 2, we formulate the cycleconsistency constraint of functional maps by introducing a latent basis Yi for each Fi, and force each functional map Xij to transformfo rY eia icnhto F Yj . [sent-210, score-1.775]
</p><p>60 We choose m = 20 for 885522  (a)(b) Figure 4: The functional map (a) with and (b) without commutativity regularization. [sent-213, score-0.671]
</p><p>61 With this setup, we formulate the map consistency term as  fcons  =  ? [sent-215, score-0.196]
</p><p>62 We thus impose an additional constraint YTY = Im, where the latent basis matrix Y is simply (Y1T, ·· · , YNT)T. [sent-228, score-0.176]
</p><p>63 4-6, we arrive at the following optimization problem for computing consistent functional maps: min  ? [sent-233, score-0.798]
</p><p>64 5 and a segmentation function transferred along a cycle is illustrated in Fig. [sent-242, score-0.378]
</p><p>65 When the latent basis matrix Y is fixed, Xij can be optimized independently, i. [sent-247, score-0.177]
</p><p>66 7 for solving the latent basis matrix Y becomes min  trace(YTWY )  s. [sent-260, score-0.176]
</p><p>67 ×  Figure 5: (a) Training image with ground truth segmentation; (b) test images; segmentation results transferred from (a) through the maps obtained by Eq. [sent-326, score-0.492]
</p><p>68 7 without the consistency term in (c) and with the consistency term in (d). [sent-327, score-0.262]
</p><p>69 Figure 6: Given a cycle of 3 images in (a), the segmentation function of the first image is transferred along the cycle. [sent-348, score-0.426]
</p><p>70 The final function transferred back looks like the original one more in (c) when the maps are consistent than that in (b) when map consistency is not enforced. [sent-349, score-0.49]
</p><p>71 Figure 7: Generated segmentation function (c) compared with normalized cut results (b). [sent-382, score-0.274]
</p><p>72 Generating Consistent Segmentations Given the consistent functional maps {Xij }, the final stage voefn nth teh proposed approach jointly optimizes an approximate segmentation indicator function fi ∈ Fi for each image. [sent-386, score-1.408]
</p><p>73 We then generate the final segmentation by rounding/binarizing fi into a segmentation indicator function. [sent-387, score-0.603]
</p><p>74 Joint optimization of segmentation functions To optimize the coefficient vectors fi of segmentation functions fi = Bifi, we minimize an objective function which consists of a map consistency term fmap and a segmentation term fseg. [sent-390, score-1.431]
</p><p>75 The map consistency term fmap ensures the segmentation functions are consistent with the optimized functional maps: fmap =  ? [sent-391, score-1.489]
</p><p>76 (11)  The segmentation term sums the alignment score between each segmentation function and segmentation clues provided on each image. [sent-396, score-0.763]
</p><p>77 For labeled images, we re-define Li as the normalized graph Laplacian of the graph that only connects super-pixels within the foreground or background segment. [sent-399, score-0.291]
</p><p>78 In the same spirit as optimizing the latent basis Y in Eq. [sent-416, score-0.183]
</p><p>79 The corresponding segmentation f ∈un Rction of each image Ii is then obtained by si = Bifi? [sent-433, score-0.217]
</p><p>80 Rounding segmentation functions The continuous functions si generated above already delineate the objects in the images well. [sent-437, score-0.463]
</p><p>81 To convert them into binary indicator functions, we simply sample 30 thresholds within the interval [min(si) , max(si)] uniformly, and choose the threshold whose corresponding segmentation has the smallest normalized cut score [20]. [sent-438, score-0.331]
</p><p>82 Table 1 shows the accuracy of our unsupervised joint segmentation method, two other state-of-the-art unsupervised co-segmentation algorithms [10, 19], and a supervised method [24]. [sent-445, score-0.486]
</p><p>83 Additionally, we also show the comparison of average accuracy with the segmentation transfer method [12] in Table 2. [sent-449, score-0.254]
</p><p>84 The accuracy of our unsupervised joint segmentation method is shown in Fig. [sent-453, score-0.341]
</p><p>85 Semi-supervised joint segmentation accuracy of our method is compared with [24] and [12] in Fig. [sent-459, score-0.258]
</p><p>86 Discussion The functional maps are surprisingly effective in building natural correspondences between images. [sent-536, score-0.889]
</p><p>87 The superior performance shows the effectiveness of the consistent functional maps and the joint optimization framework in generating robust results from the image network, despite the imperfections of individual maps. [sent-537, score-1.038]
</p><p>88 Although exact pixel or region correspondences may exist, in the more general  functional formulation cycle consistency can be easily enforced, yielding improved results. [sent-538, score-0.876]
</p><p>89 Our framework is compared with another state-of-the-art segmentation transfer method [12], and the results are in Fig. [sent-545, score-0.254]
</p><p>90 In the last column, we also include the performance of our unsupervised framework with all images in each class jointly segmented without any label information. [sent-548, score-0.254]
</p><p>91 087695 0Nu1mbero20fTain3gSCBRImoahlvnp4oWdge0r−mFsG50 (a) Unsupervised  (b) Supervised  Figure 9: Segmentation accuracy as a function of (a) unlabeled images in the unsupervised setting and (b) labeled images in the supervised setting. [sent-552, score-0.354]
</p><p>92 This suggests that, because the image set does not have much variation in object appearance, adding labeled images does not provide much additional information beyond the clues contained in the unlabeled images. [sent-554, score-0.21]
</p><p>93 This improves accuracy  in general because added images provide more segmentation cues to each other (Fig. [sent-558, score-0.265]
</p><p>94 In the supervised case, we vary the number of training images selected from the “training set”, and evaluate the performance on a separate “validation set” of 90 images (Fig. [sent-560, score-0.182]
</p><p>95 As the number of training images increases, the segmentation accuracy improves rapidly, then gradually saturates. [sent-563, score-0.289]
</p><p>96 To further confirm this observation, we pretend that the foregrounds of the test images are known, and select training images based on similarities in foreground color histogram (Color-FG) and foreground shape histogram (Shape-FG [2]). [sent-566, score-0.27]
</p><p>97 Conclusion We have proposed a framework for joint image segmentation, in which functional between images are jointly esti885555  classNL[12]FMaps-sFMaps-uns  cpcflaoa csnwese3 3N0 07[8 13140. [sent-569, score-0.796]
</p><p>98 Using the obtained functional maps, segmentation functions of all images are jointly optimized so that they are consistent under functional transport and well-aligned with each image’s own segmentation cues. [sent-592, score-2.067]
</p><p>99 We believe that this approach, focussing on establishing transport mechanisms for image properties in a network setting and then using global analysis tools over the entire network, can be beneficial to other vision problems on joint analysis of image collections. [sent-597, score-0.202]
</p><p>100 Functional maps: A flexible representation of maps between shapes. [sent-703, score-0.211]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('functional', 0.635), ('xij', 0.245), ('segmentation', 0.217), ('maps', 0.211), ('icoseg', 0.161), ('fmap', 0.131), ('msrc', 0.118), ('fi', 0.112), ('consistency', 0.101), ('functions', 0.099), ('cycle', 0.097), ('segmentations', 0.084), ('unsupervised', 0.083), ('clues', 0.082), ('firjeg', 0.079), ('xijyi', 0.079), ('yty', 0.079), ('eigenvectors', 0.078), ('consistent', 0.078), ('graph', 0.074), ('latent', 0.073), ('basis', 0.073), ('pascal', 0.073), ('jointly', 0.072), ('cosegmentation', 0.072), ('cycles', 0.069), ('wij', 0.068), ('network', 0.066), ('transferred', 0.064), ('supervised', 0.062), ('diffusion', 0.061), ('transported', 0.061), ('dj', 0.06), ('bovw', 0.058), ('fj', 0.057), ('indicator', 0.057), ('bifi', 0.053), ('cycleconsistency', 0.053), ('ficjons', 0.053), ('fifjeature', 0.053), ('xijdi', 0.053), ('foi', 0.052), ('oi', 0.052), ('reduced', 0.049), ('images', 0.048), ('joulin', 0.048), ('relations', 0.048), ('kite', 0.047), ('fseg', 0.047), ('labeled', 0.044), ('correspondences', 0.043), ('imperfections', 0.043), ('yj', 0.042), ('joint', 0.041), ('rnm', 0.041), ('foreground', 0.041), ('laplacian', 0.04), ('gi', 0.039), ('stanford', 0.038), ('optimizing', 0.037), ('transfer', 0.037), ('voc', 0.037), ('unlabeled', 0.036), ('map', 0.036), ('compositions', 0.036), ('ii', 0.035), ('ford', 0.035), ('transport', 0.035), ('supplemental', 0.034), ('histogram', 0.034), ('pairs', 0.034), ('setting', 0.033), ('kuettel', 0.032), ('optimized', 0.031), ('optimization', 0.03), ('yi', 0.03), ('pi', 0.03), ('constraint', 0.03), ('bear', 0.03), ('min', 0.03), ('rother', 0.03), ('term', 0.03), ('connects', 0.029), ('probe', 0.029), ('gj', 0.029), ('formulate', 0.029), ('normalized', 0.029), ('orthonormal', 0.028), ('cut', 0.028), ('establishing', 0.027), ('fg', 0.027), ('jk', 0.026), ('proposition', 0.026), ('stage', 0.026), ('segmented', 0.026), ('class', 0.025), ('frequencies', 0.025), ('arrive', 0.025), ('training', 0.024), ('transferring', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999923 <a title="208-tfidf-1" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<p>Author: Fan Wang, Qixing Huang, Leonidas J. Guibas</p><p>Abstract: Joint segmentation of image sets has great importance for object recognition, image classification, and image retrieval. In this paper, we aim to jointly segment a set of images starting from a small number of labeled images or none at all. To allow the images to share segmentation information with each other, we build a network that contains segmented as well as unsegmented images, and extract functional maps between connected image pairs based on image appearance features. These functional maps act as general property transporters between the images and, in particular, are used to transfer segmentations. We define and operate in a reduced functional space optimized so that the functional maps approximately satisfy cycle-consistency under composition in the network. A joint optimization framework is proposed to simultaneously generate all segmentation functions over the images so that they both align with local segmentation cues in each particular image, and agree with each other under network transportation. This formulation allows us to extract segmentations even with no training data, but can also exploit such data when available. The collective effect of the joint processing using functional maps leads to accurate information sharing among images and yields superior segmentation results, as shown on the iCoseg, MSRC, and PASCAL data sets.</p><p>2 0.17253469 <a title="208-tfidf-2" href="./iccv-2013-Pyramid_Coding_for_Functional_Scene_Element_Recognition_in_Video_Scenes.html">331 iccv-2013-Pyramid Coding for Functional Scene Element Recognition in Video Scenes</a></p>
<p>Author: Eran Swears, Anthony Hoogs, Kim Boyer</p><p>Abstract: Recognizing functional scene elemeents in video scenes based on the behaviors of moving objects that interact with them is an emerging problem ooff interest. Existing approaches have a limited ability to chharacterize elements such as cross-walks, intersections, andd buildings that have low activity, are multi-modal, or havee indirect evidence. Our approach recognizes the low activvity and multi-model elements (crosswalks/intersections) by introducing a hierarchy of descriptive clusters to fform a pyramid of codebooks that is sparse in the numbber of clusters and dense in content. The incorporation oof local behavioral context such as person-enter-building aand vehicle-parking nearby enables the detection of elemennts that do not have direct motion-based evidence, e.g. buuildings. These two contributions significantly improvee scene element recognition when compared against thhree state-of-the-art approaches. Results are shown on tyypical ground level surveillance video and for the first time on the more complex Wide Area Motion Imagery.</p><p>3 0.16831523 <a title="208-tfidf-3" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>Author: Alon Faktor, Michal Irani</p><p>Abstract: Given a set of images which share an object from the same semantic category, we would like to co-segment the shared object. We define ‘good’ co-segments to be ones which can be easily composed (like a puzzle) from large pieces of other co-segments, yet are difficult to compose from remaining image parts. These pieces must not only match well but also be statistically significant (hard to compose at random). This gives rise to co-segmentation of objects in very challenging scenarios with large variations in appearance, shape and large amounts of clutter. We further show how multiple images can collaborate and “score each others ’ co-segments to improve the overall fidelity and accuracy of the co-segmentation. Our co-segmentation can be applied both to large image collections, as well as to very few images (where there is too little data for unsupervised learning). At the extreme, it can be applied even to a single image, to extract its co-occurring objects. Our approach obtains state-of-the-art results on benchmark datasets. We further show very encouraging co-segmentation results on the challenging PASCAL-VOC dataset. ”</p><p>4 0.16232693 <a title="208-tfidf-4" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>Author: Dan Xie, Sinisa Todorovic, Song-Chun Zhu</p><p>Abstract: This paper presents an approach to localizing functional objects in surveillance videos without domain knowledge about semantic object classes that may appear in the scene. Functional objects do not have discriminative appearance and shape, but they affect behavior of people in the scene. For example, they “attract” people to approach them for satisfying certain needs (e.g., vending machines could quench thirst), or “repel” people to avoid them (e.g., grass lawns). Therefore, functional objects can be viewed as “dark matter”, emanating “dark energy ” that affects people ’s trajectories in the video. To detect “dark matter” and infer their “dark energy ” field, we extend the Lagrangian mechanics. People are treated as particle-agents with latent intents to approach “dark matter” and thus satisfy their needs, where their motions are subject to a composite “dark energy ” field of all functional objects in the scene. We make the assumption that people take globally optimal paths toward the intended “dark matter” while avoiding latent obstacles. A Bayesian framework is used to probabilistically model: people ’s trajectories and intents, constraint map of the scene, and locations of functional objects. A data-driven Markov Chain Monte Carlo (MCMC) process is used for inference. Our evaluation on videos of public squares and courtyards demonstrates our effectiveness in localizing functional objects and predicting people ’s trajectories in unobserved parts of the video footage.</p><p>5 0.13779767 <a title="208-tfidf-5" href="./iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation.html">383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</a></p>
<p>Author: Zhengxiang Wang, Rujie Liu</p><p>Abstract: This paper introduces to use semi-supervised learning for large scale image cosegmentation. Different from traditional unsupervised cosegmentation that does not use any segmentation groundtruth, semi-supervised cosegmentation exploits the similarity from both the very limited training image foregrounds, as well as the common object shared between the large number of unsegmented images. This would be a much practical way to effectively cosegment a large number of related images simultaneously, where previous unsupervised cosegmentation work poorly due to the large variances in appearance between different images and the lack ofsegmentation groundtruthfor guidance in cosegmentation. For semi-supervised cosegmentation in large scale, we propose an effective method by minimizing an energy function, which consists of the inter-image distance, the intraimage distance and the balance term. We also propose an iterative updating algorithm to efficiently solve this energy function, which decomposes the original energy minimization problem into sub-problems, and updates each image alternatively to reduce the number of variables in each subproblem for computation efficiency. Experiment results on iCoseg and Pascal VOC datasets show that the proposed cosegmentation method can effectively cosegment hundreds of images in less than one minute. And our semi-supervised cosegmentation is able to outperform both unsupervised cosegmentation as well asfully supervised single image segmentation, especially when the training data is limited.</p><p>6 0.13201392 <a title="208-tfidf-6" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>7 0.12907504 <a title="208-tfidf-7" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>8 0.12848043 <a title="208-tfidf-8" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>9 0.12735835 <a title="208-tfidf-9" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>10 0.11759643 <a title="208-tfidf-10" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>11 0.10721019 <a title="208-tfidf-11" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<p>12 0.10632694 <a title="208-tfidf-12" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>13 0.10465506 <a title="208-tfidf-13" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>14 0.10268199 <a title="208-tfidf-14" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>15 0.10244347 <a title="208-tfidf-15" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>16 0.10218141 <a title="208-tfidf-16" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>17 0.097591586 <a title="208-tfidf-17" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>18 0.096612029 <a title="208-tfidf-18" href="./iccv-2013-Bounded_Labeling_Function_for_Global_Segmentation_of_Multi-part_Objects_with_Geometric_Constraints.html">63 iccv-2013-Bounded Labeling Function for Global Segmentation of Multi-part Objects with Geometric Constraints</a></p>
<p>19 0.095237762 <a title="208-tfidf-19" href="./iccv-2013-A_Unified_Video_Segmentation_Benchmark%3A_Annotation%2C_Metrics_and_Analysis.html">33 iccv-2013-A Unified Video Segmentation Benchmark: Annotation, Metrics and Analysis</a></p>
<p>20 0.090045139 <a title="208-tfidf-20" href="./iccv-2013-Tree_Shape_Priors_with_Connectivity_Constraints_Using_Convex_Relaxation_on_General_Graphs.html">429 iccv-2013-Tree Shape Priors with Connectivity Constraints Using Convex Relaxation on General Graphs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.223), (1, -0.004), (2, 0.023), (3, -0.03), (4, 0.023), (5, 0.05), (6, -0.076), (7, 0.087), (8, 0.045), (9, -0.135), (10, -0.017), (11, 0.081), (12, -0.045), (13, -0.028), (14, -0.015), (15, 0.029), (16, -0.025), (17, -0.019), (18, -0.054), (19, -0.038), (20, 0.065), (21, -0.052), (22, -0.047), (23, 0.003), (24, 0.031), (25, 0.094), (26, 0.009), (27, 0.013), (28, 0.02), (29, 0.097), (30, 0.086), (31, 0.075), (32, 0.084), (33, 0.022), (34, -0.044), (35, 0.013), (36, -0.109), (37, 0.085), (38, 0.012), (39, -0.012), (40, 0.024), (41, 0.043), (42, 0.012), (43, 0.005), (44, -0.087), (45, -0.033), (46, -0.037), (47, -0.07), (48, -0.027), (49, -0.092)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95749605 <a title="208-lsi-1" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<p>Author: Fan Wang, Qixing Huang, Leonidas J. Guibas</p><p>Abstract: Joint segmentation of image sets has great importance for object recognition, image classification, and image retrieval. In this paper, we aim to jointly segment a set of images starting from a small number of labeled images or none at all. To allow the images to share segmentation information with each other, we build a network that contains segmented as well as unsegmented images, and extract functional maps between connected image pairs based on image appearance features. These functional maps act as general property transporters between the images and, in particular, are used to transfer segmentations. We define and operate in a reduced functional space optimized so that the functional maps approximately satisfy cycle-consistency under composition in the network. A joint optimization framework is proposed to simultaneously generate all segmentation functions over the images so that they both align with local segmentation cues in each particular image, and agree with each other under network transportation. This formulation allows us to extract segmentations even with no training data, but can also exploit such data when available. The collective effect of the joint processing using functional maps leads to accurate information sharing among images and yields superior segmentation results, as shown on the iCoseg, MSRC, and PASCAL data sets.</p><p>2 0.81035048 <a title="208-lsi-2" href="./iccv-2013-Bounded_Labeling_Function_for_Global_Segmentation_of_Multi-part_Objects_with_Geometric_Constraints.html">63 iccv-2013-Bounded Labeling Function for Global Segmentation of Multi-part Objects with Geometric Constraints</a></p>
<p>Author: Masoud S. Nosrati, Shawn Andrews, Ghassan Hamarneh</p><p>Abstract: The inclusion of shape and appearance priors have proven useful for obtaining more accurate and plausible segmentations, especially for complex objects with multiple parts. In this paper, we augment the popular MumfordShah model to incorporate two important geometrical constraints, termed containment and detachment, between different regions with a specified minimum distance between their boundaries. Our method is able to handle multiple instances of multi-part objects defined by these geometrical hamarneh} @ s fu . ca (a)Standar laΩb ehlingΩfuhnctionseting(Ωb)hΩOuirseΩtijng Figure 1: The inside vs. outside ambiguity in (a) is resolved by our containment constraint in (b). constraints using a single labeling function while maintaining global optimality. We demonstrate the utility and advantages of these two constraints and show that the proposed convex continuous method is superior to other state-of-theart methods, including its discrete counterpart, in terms of memory usage, and metrication errors.</p><p>3 0.76336503 <a title="208-lsi-3" href="./iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation.html">383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</a></p>
<p>Author: Zhengxiang Wang, Rujie Liu</p><p>Abstract: This paper introduces to use semi-supervised learning for large scale image cosegmentation. Different from traditional unsupervised cosegmentation that does not use any segmentation groundtruth, semi-supervised cosegmentation exploits the similarity from both the very limited training image foregrounds, as well as the common object shared between the large number of unsegmented images. This would be a much practical way to effectively cosegment a large number of related images simultaneously, where previous unsupervised cosegmentation work poorly due to the large variances in appearance between different images and the lack ofsegmentation groundtruthfor guidance in cosegmentation. For semi-supervised cosegmentation in large scale, we propose an effective method by minimizing an energy function, which consists of the inter-image distance, the intraimage distance and the balance term. We also propose an iterative updating algorithm to efficiently solve this energy function, which decomposes the original energy minimization problem into sub-problems, and updates each image alternatively to reduce the number of variables in each subproblem for computation efficiency. Experiment results on iCoseg and Pascal VOC datasets show that the proposed cosegmentation method can effectively cosegment hundreds of images in less than one minute. And our semi-supervised cosegmentation is able to outperform both unsupervised cosegmentation as well asfully supervised single image segmentation, especially when the training data is limited.</p><p>4 0.75152797 <a title="208-lsi-4" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>Author: Jifeng Dai, Ying Nian Wu, Jie Zhou, Song-Chun Zhu</p><p>Abstract: Cosegmentation refers to theproblem ofsegmenting multiple images simultaneously by exploiting the similarities between the foreground and background regions in these images. The key issue in cosegmentation is to align common objects between these images. To address this issue, we propose an unsupervised learning framework for cosegmentation, by coupling cosegmentation with what we call “cosketch ”. The goal of cosketch is to automatically discover a codebook of deformable shape templates shared by the input images. These shape templates capture distinct image patterns and each template is matched to similar image patches in different images. Thus the cosketch of the images helps to align foreground objects, thereby providing crucial information for cosegmentation. We present a statistical model whose energy function couples cosketch and cosegmentation. We then present an unsupervised learning algorithm that performs cosketch and cosegmentation by energy minimization. Experiments show that our method outperforms state of the art methods for cosegmentation on the challenging MSRC and iCoseg datasets. We also illustrate our method on a new dataset called Coseg-Rep where cosegmentation can be performed within a single image with repetitive patterns.</p><p>5 0.74238223 <a title="208-lsi-5" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<p>Author: Meng Tang, Lena Gorelick, Olga Veksler, Yuri Boykov</p><p>Abstract: Among image segmentation algorithms there are two major groups: (a) methods assuming known appearance models and (b) methods estimating appearance models jointly with segmentation. Typically, the first group optimizes appearance log-likelihoods in combination with some spacial regularization. This problem is relatively simple and many methods guarantee globally optimal results. The second group treats model parameters as additional variables transforming simple segmentation energies into highorder NP-hard functionals (Zhu-Yuille, Chan-Vese, GrabCut, etc). It is known that such methods indirectly minimize the appearance overlap between the segments. We propose a new energy term explicitly measuring L1 distance between the object and background appearance models that can be globally maximized in one graph cut. We show that in many applications our simple term makes NP-hard segmentation functionals unnecessary. Our one cut algorithm effectively replaces approximate iterative optimization techniques based on block coordinate descent.</p><p>6 0.72634798 <a title="208-lsi-6" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>7 0.69685364 <a title="208-lsi-7" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>8 0.67832005 <a title="208-lsi-8" href="./iccv-2013-Progressive_Multigrid_Eigensolvers_for_Multiscale_Spectral_Segmentation.html">329 iccv-2013-Progressive Multigrid Eigensolvers for Multiscale Spectral Segmentation</a></p>
<p>9 0.67660534 <a title="208-lsi-9" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>10 0.6695478 <a title="208-lsi-10" href="./iccv-2013-Tree_Shape_Priors_with_Connectivity_Constraints_Using_Convex_Relaxation_on_General_Graphs.html">429 iccv-2013-Tree Shape Priors with Connectivity Constraints Using Convex Relaxation on General Graphs</a></p>
<p>11 0.6430952 <a title="208-lsi-11" href="./iccv-2013-Fix_Structured_Learning_of_2013_ICCV_paper_k2opt.pdf.html">171 iccv-2013-Fix Structured Learning of 2013 ICCV paper k2opt.pdf</a></p>
<p>12 0.63821858 <a title="208-lsi-12" href="./iccv-2013-Uncertainty-Driven_Efficiently-Sampled_Sparse_Graphical_Models_for_Concurrent_Tumor_Segmentation_and_Atlas_Registration.html">432 iccv-2013-Uncertainty-Driven Efficiently-Sampled Sparse Graphical Models for Concurrent Tumor Segmentation and Atlas Registration</a></p>
<p>13 0.63544166 <a title="208-lsi-13" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>14 0.63461125 <a title="208-lsi-14" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>15 0.6335122 <a title="208-lsi-15" href="./iccv-2013-Bayesian_Joint_Topic_Modelling_for_Weakly_Supervised_Object_Localisation.html">59 iccv-2013-Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation</a></p>
<p>16 0.62430054 <a title="208-lsi-16" href="./iccv-2013-Learning_a_Dictionary_of_Shape_Epitomes_with_Applications_to_Image_Labeling.html">245 iccv-2013-Learning a Dictionary of Shape Epitomes with Applications to Image Labeling</a></p>
<p>17 0.61933959 <a title="208-lsi-17" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>18 0.60549098 <a title="208-lsi-18" href="./iccv-2013-Learning_CRFs_for_Image_Parsing_with_Adaptive_Subgradient_Descent.html">234 iccv-2013-Learning CRFs for Image Parsing with Adaptive Subgradient Descent</a></p>
<p>19 0.60019332 <a title="208-lsi-19" href="./iccv-2013-Potts_Model%2C_Parametric_Maxflow_and_K-Submodular_Functions.html">324 iccv-2013-Potts Model, Parametric Maxflow and K-Submodular Functions</a></p>
<p>20 0.57901633 <a title="208-lsi-20" href="./iccv-2013-Active_MAP_Inference_in_CRFs_for_Efficient_Semantic_Segmentation.html">42 iccv-2013-Active MAP Inference in CRFs for Efficient Semantic Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.068), (4, 0.017), (7, 0.018), (12, 0.01), (26, 0.094), (31, 0.062), (40, 0.011), (42, 0.128), (48, 0.014), (64, 0.049), (68, 0.155), (73, 0.041), (78, 0.014), (89, 0.19), (95, 0.013), (98, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91159856 <a title="208-lda-1" href="./iccv-2013-How_Related_Exemplars_Help_Complex_Event_Detection_in_Web_Videos%3F.html">203 iccv-2013-How Related Exemplars Help Complex Event Detection in Web Videos?</a></p>
<p>Author: Yi Yang, Zhigang Ma, Zhongwen Xu, Shuicheng Yan, Alexander G. Hauptmann</p><p>Abstract: Compared to visual concepts such as actions, scenes and objects, complex event is a higher level abstraction of longer video sequences. For example, a “marriage proposal” event is described by multiple objects (e.g., ring, faces), scenes (e.g., in a restaurant, outdoor) and actions (e.g., kneeling down). The positive exemplars which exactly convey the precise semantic of an event are hard to obtain. It would be beneficial to utilize the related exemplars for complex event detection. However, the semantic correlations between related exemplars and the target event vary substantially as relatedness assessment is subjective. Two related exemplars can be about completely different events, e.g., in the TRECVID MED dataset, both bicycle riding and equestrianism are labeled as related to “attempting a bike trick” event. To tackle the subjectiveness of human assessment, our algorithm automatically evaluates how positive the related exemplars are for the detection of an event and uses them on an exemplar-specific basis. Experiments demonstrate that our algorithm is able to utilize related exemplars adaptively, and the algorithm gains good perform- z. ance for complex event detection.</p><p>2 0.89599133 <a title="208-lda-2" href="./iccv-2013-Learning_Coupled_Feature_Spaces_for_Cross-Modal_Matching.html">235 iccv-2013-Learning Coupled Feature Spaces for Cross-Modal Matching</a></p>
<p>Author: Kaiye Wang, Ran He, Wei Wang, Liang Wang, Tieniu Tan</p><p>Abstract: Cross-modal matching has recently drawn much attention due to the widespread existence of multimodal data. It aims to match data from different modalities, and generally involves two basic problems: the measure of relevance and coupled feature selection. Most previous works mainly focus on solving the first problem. In this paper, we propose a novel coupled linear regression framework to deal with both problems. Our method learns two projection matrices to map multimodal data into a common feature space, in which cross-modal data matching can be performed. And in the learning procedure, the ?21-norm penalties are imposed on the two projection matrices separately, which leads to select relevant and discriminative features from coupled feature spaces simultaneously. A trace norm is further imposed on the projected data as a low-rank constraint, which enhances the relevance of different modal data with connections. We also present an iterative algorithm based on halfquadratic minimization to solve the proposed regularized linear regression problem. The experimental results on two challenging cross-modal datasets demonstrate that the proposed method outperforms the state-of-the-art approaches.</p><p>same-paper 3 0.88623428 <a title="208-lda-3" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<p>Author: Fan Wang, Qixing Huang, Leonidas J. Guibas</p><p>Abstract: Joint segmentation of image sets has great importance for object recognition, image classification, and image retrieval. In this paper, we aim to jointly segment a set of images starting from a small number of labeled images or none at all. To allow the images to share segmentation information with each other, we build a network that contains segmented as well as unsegmented images, and extract functional maps between connected image pairs based on image appearance features. These functional maps act as general property transporters between the images and, in particular, are used to transfer segmentations. We define and operate in a reduced functional space optimized so that the functional maps approximately satisfy cycle-consistency under composition in the network. A joint optimization framework is proposed to simultaneously generate all segmentation functions over the images so that they both align with local segmentation cues in each particular image, and agree with each other under network transportation. This formulation allows us to extract segmentations even with no training data, but can also exploit such data when available. The collective effect of the joint processing using functional maps leads to accurate information sharing among images and yields superior segmentation results, as shown on the iCoseg, MSRC, and PASCAL data sets.</p><p>4 0.87187284 <a title="208-lda-4" href="./iccv-2013-Unsupervised_Intrinsic_Calibration_from_a_Single_Frame_Using_a_%22Plumb-Line%22_Approach.html">436 iccv-2013-Unsupervised Intrinsic Calibration from a Single Frame Using a "Plumb-Line" Approach</a></p>
<p>Author: R. Melo, M. Antunes, J.P. Barreto, G. Falcão, N. Gonçalves</p><p>Abstract: Estimating the amount and center ofdistortionfrom lines in the scene has been addressed in the literature by the socalled “plumb-line ” approach. In this paper we propose a new geometric method to estimate not only the distortion parameters but the entire camera calibration (up to an “angular” scale factor) using a minimum of 3 lines. We propose a new framework for the unsupervised simultaneous detection of natural image of lines and camera parameters estimation, enabling a robust calibration from a single image. Comparative experiments with existing automatic approaches for the distortion estimation and with ground truth data are presented.</p><p>5 0.86681789 <a title="208-lda-5" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<p>Author: Jiyan Pan, Takeo Kanade</p><p>Abstract: Objects in a real world image cannot have arbitrary appearance, sizes and locations due to geometric constraints in 3D space. Such a 3D geometric context plays an important role in resolving visual ambiguities and achieving coherent object detection. In this paper, we develop a RANSAC-CRF framework to detect objects that are geometrically coherent in the 3D world. Different from existing methods, we propose a novel generalized RANSAC algorithm to generate global 3D geometry hypothesesfrom local entities such that outlier suppression and noise reduction is achieved simultaneously. In addition, we evaluate those hypotheses using a CRF which considers both the compatibility of individual objects under global 3D geometric context and the compatibility between adjacent objects under local 3D geometric context. Experiment results show that our approach compares favorably with the state of the art.</p><p>6 0.86010647 <a title="208-lda-6" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>7 0.8555029 <a title="208-lda-7" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>8 0.85330999 <a title="208-lda-8" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>9 0.85289121 <a title="208-lda-9" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>10 0.85198092 <a title="208-lda-10" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>11 0.85027885 <a title="208-lda-11" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>12 0.84963179 <a title="208-lda-12" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>13 0.84855688 <a title="208-lda-13" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>14 0.84843576 <a title="208-lda-14" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>15 0.84827733 <a title="208-lda-15" href="./iccv-2013-Bayesian_Joint_Topic_Modelling_for_Weakly_Supervised_Object_Localisation.html">59 iccv-2013-Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation</a></p>
<p>16 0.84801316 <a title="208-lda-16" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>17 0.84790051 <a title="208-lda-17" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>18 0.84728336 <a title="208-lda-18" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>19 0.84667242 <a title="208-lda-19" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>20 0.84665304 <a title="208-lda-20" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
