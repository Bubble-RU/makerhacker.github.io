<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-328" href="#">iccv2013-328</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</h1>
<br/><p>Source: <a title="iccv-2013-328-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Li_Probabilistic_Elastic_Part_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Haoxiang Li, Gang Hua, Zhe Lin, Jonathan Brandt, Jianchao Yang</p><p>Abstract: We propose an unsupervised detector adaptation algorithm to adapt any offline trained face detector to a specific collection of images, and hence achieve better accuracy. The core of our detector adaptation algorithm is a probabilistic elastic part (PEP) model, which is offline trained with a set of face examples. It produces a statisticallyaligned part based face representation, namely the PEP representation. To adapt a general face detector to a collection of images, we compute the PEP representations of the candidate detections from the general face detector, and then train a discriminative classifier with the top positives and negatives. Then we re-rank all the candidate detections with this classifier. This way, a face detector tailored to the statistics of the specific image collection is adapted from the original detector. We present extensive results on three datasets with two state-of-the-art face detectors. The significant improvement of detection accuracy over these state- of-the-art face detectors strongly demonstrates the efficacy of the proposed face detector adaptation algorithm.</p><p>Reference: <a title="iccv-2013-328-reference" href="../iccv2013_reference/iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu 18 Abstract We propose an unsupervised detector adaptation algorithm to adapt any offline trained face detector to a specific collection of images, and hence achieve better accuracy. [sent-2, score-1.094]
</p><p>2 The core of our detector adaptation algorithm is a probabilistic elastic part (PEP) model, which is offline trained with a set of face examples. [sent-3, score-0.893]
</p><p>3 It produces a statisticallyaligned part based face representation, namely the PEP representation. [sent-4, score-0.28]
</p><p>4 To adapt a general face detector to a collection of images, we compute the PEP representations of the candidate detections from the general face detector, and then train a discriminative classifier with the top positives and negatives. [sent-5, score-1.22]
</p><p>5 This way, a face detector tailored to the statistics of the specific image collection is adapted from the original detector. [sent-7, score-0.527]
</p><p>6 We present extensive results on three datasets with two state-of-the-art face detectors. [sent-8, score-0.28]
</p><p>7 The significant improvement of detection accuracy over these state-  of-the-art face detectors strongly demonstrates the efficacy of the proposed face detector adaptation algorithm. [sent-9, score-1.078]
</p><p>8 Introduction There are now a number of practical face detection solutions as well as publicly available face detectors [25, 26, 18, 20]. [sent-11, score-0.669]
</p><p>9 In the appearance-based face detector, models are usually learned from a large set of positive and negative training images to capture the variations of faces while keeping the model discriminative. [sent-13, score-0.435]
</p><p>10 As a result, stateof-the-art face detectors are practical in a number of general scenarios. [sent-14, score-0.356]
</p><p>11 Since factors affecting face detection such as scale, location, pose, face expression, lighting conditions, etc. [sent-16, score-0.611]
</p><p>12 vary from task to task, it requires non-trivial engineering efforts to make a face detector work for extensive scenarios even with the cutting-edge algorithms. [sent-17, score-0.483]
</p><p>13 While the design of a general face Zhe Lin, Jonathan Brandt, Jianchao Yang Adobe Systems Inc. [sent-18, score-0.298]
</p><p>14 com  detector is an important and attractive problem, to the end  users how a face detector performs on their photos matters most. [sent-20, score-0.747]
</p><p>15 In this sense, how to adapt a general face detector to a specific task (e. [sent-21, score-0.52]
</p><p>16 It can be straightforward to fit this detector adaptation problem into a domain transfer learning framework. [sent-24, score-0.449]
</p><p>17 However, for the face detector adaptation task it may not be possible to access the training data since state-of-the-art detectors are usually trained with massive data that cannot be easily transferred, or is not publicly available. [sent-26, score-0.819]
</p><p>18 In this paper, we treat the general face detector as a black box without access to its training data and approach to this problem with an online trained classifier with a novel probabilistic elastic part (PEP) representation. [sent-27, score-0.749]
</p><p>19 In the offline stage, we train a PEP model, which is a local spatial-appearance feature based Gaussian mixture model, from a set of face examples. [sent-29, score-0.509]
</p><p>20 Then we utilize the PEP model to build the PEP representation for every candidate detection extracted by the face detector. [sent-30, score-0.465]
</p><p>21 We argue that the PEP representation statistically aligns faces and confines the comparison between two faces within locally corresponding face structures. [sent-31, score-0.58]
</p><p>22 Using the detection confidence scores from the general face detector, we pick up the detections with  high confidence scores as positive candidates and regard detections with low confidence scores as negative candidates. [sent-32, score-0.886]
</p><p>23 Finally, over these positive and negative examples, with the PEP representation, we train a discriminative classifier online to induce a probability output to predict how likely a candidate detection is right. [sent-33, score-0.342]
</p><p>24 Under this framework, we turn the detector adaptation problem into a binary classification problem. [sent-35, score-0.409]
</p><p>25 , we anticipate that an aligned visual representation for face images would be robust to these visual variations, as it enables local comparison within the same face structures. [sent-38, score-0.631]
</p><p>26 We approach this by building the PEP representation as a statistically aligned visual representation for face image with the help of an offline trained PEP model. [sent-39, score-0.536]
</p><p>27 In our framework, we augment every local feature extracted from face image with its spatial location in the image. [sent-40, score-0.319]
</p><p>28 The PEP model formulated as a special Gaussian mixture model with spherical Gaussian components is trained over the spatial-augmented local features from the offline training faces. [sent-41, score-0.292]
</p><p>29 As a result, a weighted mixture component in the PEP model captures the appearance of a face structure (e. [sent-42, score-0.415]
</p><p>30 the nose) as well as the location of the structure  on the face (e. [sent-44, score-0.3]
</p><p>31 In this sense, given a face image as a set of spatial-augmented local features, to align the features to the PEP model is to align features to the face structures described by the mixture components. [sent-47, score-0.743]
</p><p>32 In brief, given a face image as a set of spatial-augmented local features, one weighted Gaussian mixture component picks one feature which activates it. [sent-50, score-0.446]
</p><p>33 And we concatenate all the activation features to build PEP representation of the face image. [sent-51, score-0.51]
</p><p>34 In Section 3, we explain the unsupervised detector adaptation work-flow and we introduce the probabilistic elastic part model in Section 4. [sent-56, score-0.537]
</p><p>35 Specially, we can formulate the target problem as one of adapting the general face detector pre-trained on a source domain to the testing photos which are from a different distribution as the target domain. [sent-60, score-0.707]
</p><p>36 One common approach to handle unlabeled data in unsupervised domain adaptation is to utilize the training labels in the source domain [19, 6]. [sent-61, score-0.37]
</p><p>37 It can be done by exploiting the spatio-temporal information in the video detector adaptation setting[15, 14]. [sent-67, score-0.409]
</p><p>38 These approaches are not applicable to our scenario because the pre-trained detector is treated as a black box without access to its massive training data and we aim at adapting it to a specific collection of photos, which do not have dense temporal correlation as in videos. [sent-69, score-0.36]
</p><p>39 [21] approach the pedestrian detector adaptation with an online non-parametric classifier on binary representations of candidate detections. [sent-77, score-0.615]
</p><p>40 On one hand their method is proposed for pedestrian detector adaptation while ours are for face detector adaptation. [sent-79, score-0.892]
</p><p>41 Similarly, in this paper a set of faces is used to learn the PEP model, which is subsequently leveraged to build a new representation, the PEP representation, for candidate detections and ultimately improve the final classification accuracy. [sent-84, score-0.307]
</p><p>42 Unsupervised Detector Adaptation  General face detectors are typically not perfect, so they may have false positives and/or missed detections. [sent-86, score-0.512]
</p><p>43 Our unsupervised detector adaptation work-flow  tections with high recall and precision. [sent-90, score-0.472]
</p><p>44 Formally, given a set of test images, we denote the extracted candidate detections from a general face detector as T = {t1, t2, . [sent-91, score-0.684]
</p><p>45 We assume the face detector generates confidence scores for candidate detections, otherwise we will resort to the activation score (See Section 4. [sent-96, score-0.834]
</p><p>46 3) of the candidate detections over the probabilistic model trained offline as the detection confidence scores. [sent-97, score-0.465]
</p><p>47 λh denotes the threshold that detections with confidence score higher then λh is expected to be faces with high confidence (top positives). [sent-116, score-0.431]
</p><p>48 The better the face detector is the more reliable is this assumption. [sent-137, score-0.483]
</p><p>49 In our scenario, the face detector is treated as a black-box. [sent-138, score-0.483]
</p><p>50 In order to improve the performance of detection, we re-evaluate the detections by assigning a new confidence score to each candidate detection = { sˆi}, i = 1, . [sent-139, score-0.365]
</p><p>51 fin Iend t as t sheen probability ntehawt ti is a face (positive) and we have H and N as estimated  Sˆ  positive and negative examples. [sent-145, score-0.392]
</p><p>52 We use a set of general faces to train the offline probabilistic model a Gaussian Mixture Model (GMM) with spatial information encoded. [sent-153, score-0.294]
</p><p>53 The training corpus here can be any set of general faces which are very easy to collect from the Internet or existing face datasets. [sent-154, score-0.404]
</p><p>54 Then face images are processed into spatial-augmented features through a feature extraction pipeline. [sent-156, score-0.3]
</p><p>55 In this step, we extract dense patches over multiple scales and augment the local image patch descriptor with the spatial location of the image patch to build a set of spatial-appearance features for every face image as in [ 13]. [sent-157, score-0.357]
</p><p>56 Since the features have spatial 779955  information encoded, the GMM captures both the appearance and spatial distribution of face structures such as nose, left eye corner, right eye corner, etc. [sent-168, score-0.338]
</p><p>57 Intuitively, a weighted Gaussian mixture component describes the average appearance of a certain face structure and its location jointly. [sent-169, score-0.435]
</p><p>58 PEP representation As in the classical image classification pipeline, we first build a more descriptive and structured PEP representation for each face image. [sent-172, score-0.386]
</p><p>59 In our framework, the PEP representation statistically aligns a face image to the mixture components. [sent-173, score-0.477]
</p><p>60 Each local comparison is conducted between features aligned to a mix-  ture component, or in other words, between features that describe the same face structure (See Figure 3). [sent-175, score-0.347]
</p><p>61 In our framework, we define the response of a feature over a statistical receptive field as the probability of the feature over the mixture component since this value measures how likely the feature describes the expected face structure. [sent-178, score-0.571]
</p><p>62 We treat the one draws the highest probability over a mixture component as the activation feature of the component and say the feature activates the component. [sent-180, score-0.365]
</p><p>63 To align a face image (a feature set F) to the GMM, we make each Gaussian mixture componFe)n tt ofi tnhde ei tGs MacMtiv,a wtioen m feaakteur eae cfhro Gma Fuss. [sent-181, score-0.406]
</p><p>64 The effect of the spatial constraint is that face structures similar in appearance but different in location can be differentiated from each other. [sent-185, score-0.319]
</p><p>65 Furthermore, details of faces that describe the same face structure with a slight offset can be captured, as depicted in row (3) and (4). [sent-187, score-0.386]
</p><p>66 Following Equation 2, K Gaussian components pick K activation features given a face image. [sent-188, score-0.476]
</p><p>67 As shown in Figure 3, the appearance part of the K activation features is concatenated in the sequence of Gaussian mixture model as the PEP representation of the face image F, i. [sent-189, score-0.619]
</p><p>68 Each column is the PEP representation for a face (showing the appearance part), each row shows the activation features. [sent-210, score-0.472]
</p><p>69 We can see (across the row) element-wise comparison between PEP representations actually consist of local comparison within the same face structure. [sent-211, score-0.303]
</p><p>70 Applying the trained classifier to every candidate detection, a new detection confidence score si is computed for ti. [sent-219, score-0.36]
</p><p>71 Activation Score  An optional step in the proposed framework is to calculate the activation score as the initial confidence score in case the general face detector does not provide a detection confidence score. [sent-222, score-0.962]
</p><p>72 We re-use the offline trained GMM in a generative manner to calculate the activation score Sa (ti) for a candidate detection ti, which is defined as the total response of all the activation features, i. [sent-223, score-0.618]
</p><p>73 Experiments We verified the proposed method with the Viola-Jones (VJ) detector [20] and the XZJY detector [18], over the EAlbum [3], G-Album[4] and FDDB face detection benchmarks [10]. [sent-229, score-0.737]
</p><p>74 [21] detector adaptation algorithm over the albums for comparison. [sent-231, score-0.49]
</p><p>75 To demonstrate the statistical alignment introduced by PEP representation contributes to performance improvement, we design a baseline experiment by concatenating the local features directly  as the face representation. [sent-234, score-0.344]
</p><p>76 The experimental results strongly support the proposed method and suggest that the PEP representation is a very effective visual representation for face image. [sent-235, score-0.368]
</p><p>77 One of our motivations is to exploit within dataset correlation to help improve face detection. [sent-236, score-0.302]
</p><p>78 In this sense, our method is more suitable for detector adaptation on personal albums, since the same person might appear in many photos within an album. [sent-237, score-0.511]
</p><p>79 E-Album has stronger in-dataset correlation since the number of different individuals in this album is limited, in other words face of the same person appear many times in E-Album. [sent-238, score-0.326]
</p><p>80 However we observe that we can still boost the face detector performance by a large margin since the PEP representation enlarges the gap between faces and non-faces. [sent-240, score-0.633]
</p><p>81 Comparing to the detection time of state-of-the-art detectors such as the XZJY detector, which can take 10 seconds per image on the albums, the time taken by our adaptation process is minor. [sent-242, score-0.341]
</p><p>82 Settings We use the OpenCV implementation of Viola-Jones face detector 2 and the XZJY face detector [18] which achieved the state ofthe art results on the FDDB and UCI datasets. [sent-252, score-0.966]
</p><p>83 We train the PEP offline with face images from the face dataset Labeled Face in the Wild [8], in which faces are roughly aligned with the funneling method. [sent-253, score-0.85]
</p><p>84 Results on person photo albums As shown in Figure 5 the proposed method improves the performance of the Viola-Jones (VJ) detector and the XZJY detector by a large margin and outperforms the Wang et al. [sent-263, score-0.53]
</p><p>85 The lower boundary of the Viola-Jones detector confidence score in our case is fixed as 1by design while the upper boundary can be very large, we fix λl = 3 and alter λh to explore how our method performs with respect to different high thresholds λh 4. [sent-281, score-0.39]
</p><p>86 For each face image, the baseline representation is built by concatenating all SIFT features from left to right, top to bottom, while in the PEP representation a set of features (possibly with duplicates) is selected by mixture components and concatenated. [sent-291, score-0.545]
</p><p>87 As shown in Figure 8, the contribution of PEP representation in our detector adaptation framework is significant. [sent-292, score-0.453]
</p><p>88 Results on FDDB dataset FDDB is a dataset designed as a benchmark for face detection algorithms [10]. [sent-295, score-0.348]
</p><p>89 Several existing face detection algorithms are tested on the FDDB dataset and the results are published on the website. [sent-297, score-0.331]
</p><p>90 The first one is the all-folds adaptation where all images in the 10 folds are put together and we train only one online classifier for all 10 folds. [sent-301, score-0.351]
</p><p>91 As shown in Figure 9, the result from our method adapting the OpenCV’s frontal face detector achieved very competitive results to the state-the-art detectors 5. [sent-310, score-0.591]
</p><p>92 Performance comparison on FDDB by adapting ViolaJones face detector in the continuous score evaluation. [sent-313, score-0.581]
</p><p>93 Another interesting question could be whether we could boost a very strong face detector on FDDB which is of less intra-class correlation and larger inter-class variance. [sent-314, score-0.505]
</p><p>94 Conclusion In this paper, we propose an unsupervised detector adaptation framework with an effective PEP representation for face images. [sent-317, score-0.774]
</p><p>95 We verified the proposed framework with two state-of-the-art face detectors on two person albums and a challenging unconstrained face detection benchmark, and demonstrate that the proposed method can improve the two general face detectors on a specific task by a significant margin. [sent-318, score-1.17]
</p><p>96 Performance comparison on FDDB by adapting the XZJY face detector ence Foundation Grant 61228303, GH’s start-up funds from Stevens Institute of Technology, a Collaboration Research Gift Grant from Adobe System Incorporated, a gift grant from NEC Labs American, and a Google Research Faculty Award. [sent-320, score-0.558]
</p><p>97 Easyalbum: an interactive photo annotation system based on face clustering and re-ranking. [sent-342, score-0.299]
</p><p>98 Labeled faces in the wild: a database for studying face recog-  [9] [10]  [11] [12]  [13]  [14]  [15]  [16]  [17] [18] [19]  nition in unconstrained environments. [sent-375, score-0.407]
</p><p>99 Fddb: A benchmark for face detection in unconstrained settings. [sent-385, score-0.369]
</p><p>100 Detection by detections: Nonparametric detector adaptation for a video. [sent-463, score-0.409]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pep', 0.728), ('face', 0.28), ('fddb', 0.23), ('adaptation', 0.206), ('detector', 0.203), ('activation', 0.148), ('xzjy', 0.112), ('detections', 0.111), ('mixture', 0.109), ('faces', 0.106), ('positives', 0.105), ('offline', 0.086), ('confidence', 0.083), ('albums', 0.081), ('receptive', 0.076), ('candidate', 0.072), ('false', 0.069), ('gmm', 0.068), ('photos', 0.061), ('online', 0.06), ('detectors', 0.058), ('elastic', 0.056), ('thresholds', 0.056), ('classifier', 0.051), ('brandt', 0.051), ('detection', 0.051), ('adapting', 0.05), ('score', 0.048), ('stevens', 0.046), ('representation', 0.044), ('roc', 0.042), ('kg', 0.041), ('unsupervised', 0.041), ('domain', 0.04), ('jain', 0.038), ('ti', 0.038), ('baypwt', 0.037), ('fgk', 0.037), ('funneling', 0.037), ('gaussian', 0.036), ('response', 0.034), ('adobe', 0.034), ('train', 0.034), ('tm', 0.032), ('opencv', 0.032), ('probabilistic', 0.031), ('trained', 0.031), ('jh', 0.031), ('activates', 0.031), ('libsvm', 0.029), ('components', 0.028), ('negatives', 0.028), ('aligned', 0.027), ('component', 0.026), ('seconds', 0.026), ('unlabeled', 0.026), ('gift', 0.025), ('positive', 0.025), ('collection', 0.025), ('probability', 0.025), ('hua', 0.025), ('negative', 0.024), ('si', 0.024), ('statistically', 0.024), ('person', 0.024), ('sense', 0.023), ('representations', 0.023), ('biology', 0.023), ('gopalan', 0.023), ('cascade', 0.022), ('massive', 0.022), ('hit', 0.022), ('recall', 0.022), ('correlation', 0.022), ('alarms', 0.022), ('curves', 0.022), ('field', 0.021), ('unconstrained', 0.021), ('location', 0.02), ('aligns', 0.02), ('features', 0.02), ('target', 0.019), ('photo', 0.019), ('access', 0.019), ('spatial', 0.019), ('specific', 0.019), ('nose', 0.019), ('fi', 0.019), ('concatenated', 0.018), ('build', 0.018), ('vj', 0.018), ('general', 0.018), ('biological', 0.018), ('spherical', 0.018), ('candidates', 0.017), ('american', 0.017), ('source', 0.017), ('personal', 0.017), ('align', 0.017), ('benchmark', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="328-tfidf-1" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>Author: Haoxiang Li, Gang Hua, Zhe Lin, Jonathan Brandt, Jianchao Yang</p><p>Abstract: We propose an unsupervised detector adaptation algorithm to adapt any offline trained face detector to a specific collection of images, and hence achieve better accuracy. The core of our detector adaptation algorithm is a probabilistic elastic part (PEP) model, which is offline trained with a set of face examples. It produces a statisticallyaligned part based face representation, namely the PEP representation. To adapt a general face detector to a collection of images, we compute the PEP representations of the candidate detections from the general face detector, and then train a discriminative classifier with the top positives and negatives. Then we re-rank all the candidate detections with this classifier. This way, a face detector tailored to the statistics of the specific image collection is adapted from the original detector. We present extensive results on three datasets with two state-of-the-art face detectors. The significant improvement of detection accuracy over these state- of-the-art face detectors strongly demonstrates the efficacy of the proposed face detector adaptation algorithm.</p><p>2 0.23112671 <a title="328-tfidf-2" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>Author: Kristina Scherbaum, James Petterson, Rogerio S. Feris, Volker Blanz, Hans-Peter Seidel</p><p>Abstract: Face detection is an important task in computer vision and often serves as the first step for a variety of applications. State-of-the-art approaches use efficient learning algorithms and train on large amounts of manually labeled imagery. Acquiring appropriate training images, however, is very time-consuming and does not guarantee that the collected training data is representative in terms of data variability. Moreover, available data sets are often acquired under controlled settings, restricting, for example, scene illumination or 3D head pose to a narrow range. This paper takes a look into the automated generation of adaptive training samples from a 3D morphable face model. Using statistical insights, the tailored training data guarantees full data variability and is enriched by arbitrary facial attributes such as age or body weight. Moreover, it can automatically adapt to environmental constraints, such as illumination or viewing angle of recorded video footage from surveillance cameras. We use the tailored imagery to train a new many-core imple- mentation of Viola Jones ’ AdaBoost object detection framework. The new implementation is not only faster but also enables the use of multiple feature channels such as color features at training time. In our experiments we trained seven view-dependent face detectors and evaluate these on the Face Detection Data Set and Benchmark (FDDB). Our experiments show that the use of tailored training imagery outperforms state-of-the-art approaches on this challenging dataset.</p><p>3 0.1621161 <a title="328-tfidf-3" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>Author: Vidit Jain, Sachin Sudhakar Farfade</p><p>Abstract: Classification cascades have been very effective for object detection. Such a cascade fails to perform well in data domains with variations in appearances that may not be captured in the training examples. This limited generalization severely restricts the domains for which they can be used effectively. A common approach to address this limitation is to train a new cascade of classifiers from scratch for each of the new domains. Building separate detectors for each of the different domains requires huge annotation and computational effort, making it not scalable to a large number of data domains. Here we present an algorithm for quickly adapting a pre-trained cascade of classifiers using a small number oflabeledpositive instancesfrom a different yet similar data domain. In our experiments with images of human babies and human-like characters from movies, we demonstrate that the adapted cascade significantly outperforms both of the original cascade and the one trained from scratch using the given training examples. –</p><p>4 0.16075143 <a title="328-tfidf-4" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>Author: Yizhe Zhang, Ming Shao, Edward K. Wong, Yun Fu</p><p>Abstract: One of the most challenging task in face recognition is to identify people with varied poses. Namely, the test faces have significantly different poses compared with the registered faces. In this paper, we propose a high-level feature learning scheme to extract pose-invariant identity feature for face recognition. First, we build a single-hiddenlayer neural network with sparse constraint, to extractposeinvariant feature in a supervised fashion. Second, we further enhance the discriminative capability of the proposed feature by using multiple random faces as the target values for multiple encoders. By enforcing the target values to be uniquefor inputfaces over differentposes, the learned highlevel feature that is represented by the neurons in the hidden layer is pose free and only relevant to the identity information. Finally, we conduct face identification on CMU MultiPIE, and verification on Labeled Faces in the Wild (LFW) databases, where identification rank-1 accuracy and face verification accuracy with ROC curve are reported. These experiments demonstrate that our model is superior to oth- er state-of-the-art approaches on handling pose variations.</p><p>5 0.13299151 <a title="328-tfidf-5" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>Author: Tal Hassner</p><p>Abstract: We present a data-driven method for estimating the 3D shapes of faces viewed in single, unconstrained photos (aka “in-the-wild”). Our method was designed with an emphasis on robustness and efficiency with the explicit goal of deployment in real-world applications which reconstruct and display faces in 3D. Our key observation is that for many practical applications, warping the shape of a reference face to match the appearance of a query, is enough to produce realistic impressions of the query ’s 3D shape. Doing so, however, requires matching visual features between the (possibly very different) query and reference images, while ensuring that a plausible face shape is produced. To this end, we describe an optimization process which seeks to maximize the similarity of appearances and depths, jointly, to those of a reference model. We describe our system for monocular face shape reconstruction and present both qualitative and quantitative experiments, comparing our method against alternative systems, and demonstrating its capabilities. Finally, as a testament to its suitability for real-world applications, we offer an open, online implementation of our system, providing unique means – of instant 3D viewing of faces appearing in web photos.</p><p>6 0.11921249 <a title="328-tfidf-6" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>7 0.11850389 <a title="328-tfidf-7" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>8 0.11356188 <a title="328-tfidf-8" href="./iccv-2013-Coupling_Alignments_with_Recognition_for_Still-to-Video_Face_Recognition.html">97 iccv-2013-Coupling Alignments with Recognition for Still-to-Video Face Recognition</a></p>
<p>9 0.10873905 <a title="328-tfidf-9" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>10 0.10404132 <a title="328-tfidf-10" href="./iccv-2013-CoDeL%3A_A_Human_Co-detection_and_Labeling_Framework.html">75 iccv-2013-CoDeL: A Human Co-detection and Labeling Framework</a></p>
<p>11 0.10402588 <a title="328-tfidf-11" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>12 0.10227272 <a title="328-tfidf-12" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>13 0.099706367 <a title="328-tfidf-13" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>14 0.098204508 <a title="328-tfidf-14" href="./iccv-2013-Cascaded_Shape_Space_Pruning_for_Robust_Facial_Landmark_Detection.html">70 iccv-2013-Cascaded Shape Space Pruning for Robust Facial Landmark Detection</a></p>
<p>15 0.094040081 <a title="328-tfidf-15" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>16 0.093858153 <a title="328-tfidf-16" href="./iccv-2013-Hidden_Factor_Analysis_for_Age_Invariant_Face_Recognition.html">195 iccv-2013-Hidden Factor Analysis for Age Invariant Face Recognition</a></p>
<p>17 0.09322492 <a title="328-tfidf-17" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>18 0.092041157 <a title="328-tfidf-18" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>19 0.091959842 <a title="328-tfidf-19" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>20 0.091698781 <a title="328-tfidf-20" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.179), (1, 0.059), (2, -0.06), (3, -0.103), (4, 0.025), (5, -0.113), (6, 0.161), (7, 0.112), (8, -0.018), (9, -0.004), (10, 0.027), (11, -0.022), (12, 0.036), (13, -0.056), (14, -0.012), (15, -0.091), (16, -0.049), (17, 0.032), (18, -0.008), (19, 0.028), (20, -0.021), (21, -0.107), (22, 0.036), (23, 0.023), (24, 0.042), (25, 0.036), (26, -0.101), (27, -0.062), (28, -0.006), (29, -0.036), (30, -0.031), (31, 0.043), (32, 0.056), (33, -0.031), (34, -0.05), (35, -0.075), (36, 0.088), (37, -0.01), (38, -0.044), (39, 0.009), (40, -0.008), (41, 0.135), (42, -0.016), (43, 0.055), (44, -0.02), (45, -0.038), (46, -0.064), (47, -0.068), (48, -0.051), (49, -0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95711315 <a title="328-lsi-1" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>Author: Haoxiang Li, Gang Hua, Zhe Lin, Jonathan Brandt, Jianchao Yang</p><p>Abstract: We propose an unsupervised detector adaptation algorithm to adapt any offline trained face detector to a specific collection of images, and hence achieve better accuracy. The core of our detector adaptation algorithm is a probabilistic elastic part (PEP) model, which is offline trained with a set of face examples. It produces a statisticallyaligned part based face representation, namely the PEP representation. To adapt a general face detector to a collection of images, we compute the PEP representations of the candidate detections from the general face detector, and then train a discriminative classifier with the top positives and negatives. Then we re-rank all the candidate detections with this classifier. This way, a face detector tailored to the statistics of the specific image collection is adapted from the original detector. We present extensive results on three datasets with two state-of-the-art face detectors. The significant improvement of detection accuracy over these state- of-the-art face detectors strongly demonstrates the efficacy of the proposed face detector adaptation algorithm.</p><p>2 0.81587404 <a title="328-lsi-2" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>Author: Kristina Scherbaum, James Petterson, Rogerio S. Feris, Volker Blanz, Hans-Peter Seidel</p><p>Abstract: Face detection is an important task in computer vision and often serves as the first step for a variety of applications. State-of-the-art approaches use efficient learning algorithms and train on large amounts of manually labeled imagery. Acquiring appropriate training images, however, is very time-consuming and does not guarantee that the collected training data is representative in terms of data variability. Moreover, available data sets are often acquired under controlled settings, restricting, for example, scene illumination or 3D head pose to a narrow range. This paper takes a look into the automated generation of adaptive training samples from a 3D morphable face model. Using statistical insights, the tailored training data guarantees full data variability and is enriched by arbitrary facial attributes such as age or body weight. Moreover, it can automatically adapt to environmental constraints, such as illumination or viewing angle of recorded video footage from surveillance cameras. We use the tailored imagery to train a new many-core imple- mentation of Viola Jones ’ AdaBoost object detection framework. The new implementation is not only faster but also enables the use of multiple feature channels such as color features at training time. In our experiments we trained seven view-dependent face detectors and evaluate these on the Face Detection Data Set and Benchmark (FDDB). Our experiments show that the use of tailored training imagery outperforms state-of-the-art approaches on this challenging dataset.</p><p>3 0.78414464 <a title="328-lsi-3" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>Author: Vidit Jain, Sachin Sudhakar Farfade</p><p>Abstract: Classification cascades have been very effective for object detection. Such a cascade fails to perform well in data domains with variations in appearances that may not be captured in the training examples. This limited generalization severely restricts the domains for which they can be used effectively. A common approach to address this limitation is to train a new cascade of classifiers from scratch for each of the new domains. Building separate detectors for each of the different domains requires huge annotation and computational effort, making it not scalable to a large number of data domains. Here we present an algorithm for quickly adapting a pre-trained cascade of classifiers using a small number oflabeledpositive instancesfrom a different yet similar data domain. In our experiments with images of human babies and human-like characters from movies, we demonstrate that the adapted cascade significantly outperforms both of the original cascade and the one trained from scratch using the given training examples. –</p><p>4 0.72043318 <a title="328-lsi-4" href="./iccv-2013-Modifying_the_Memorability_of_Face_Photographs.html">272 iccv-2013-Modifying the Memorability of Face Photographs</a></p>
<p>Author: Aditya Khosla, Wilma A. Bainbridge, Antonio Torralba, Aude Oliva</p><p>Abstract: Contemporary life bombards us with many new images of faces every day, which poses non-trivial constraints on human memory. The vast majority of face photographs are intended to be remembered, either because of personal relevance, commercial interests or because the pictures were deliberately designed to be memorable. Can we make aportrait more memorable or more forgettable automatically? Here, we provide a method to modify the memorability of individual face photographs, while keeping the identity and other facial traits (e.g. age, attractiveness, and emotional magnitude) of the individual fixed. We show that face photographs manipulated to be more memorable (or more forgettable) are indeed more often remembered (or forgotten) in a crowd-sourcing experiment with an accuracy of 74%. Quantifying and modifying the ‘memorability ’ of a face lends itself to many useful applications in computer vision and graphics, such as mnemonic aids for learning, photo editing applications for social networks and tools for designing memorable advertisements.</p><p>5 0.71251696 <a title="328-lsi-5" href="./iccv-2013-Hidden_Factor_Analysis_for_Age_Invariant_Face_Recognition.html">195 iccv-2013-Hidden Factor Analysis for Age Invariant Face Recognition</a></p>
<p>Author: Dihong Gong, Zhifeng Li, Dahua Lin, Jianzhuang Liu, Xiaoou Tang</p><p>Abstract: Age invariant face recognition has received increasing attention due to its great potential in real world applications. In spite of the great progress in face recognition techniques, reliably recognizingfaces across ages remains a difficult task. The facial appearance of a person changes substantially over time, resulting in significant intra-class variations. Hence, the key to tackle this problem is to separate the variation caused by aging from the person-specific features that are stable. Specifically, we propose a new method, calledHidden FactorAnalysis (HFA). This methodcaptures the intuition above through a probabilistic model with two latent factors: an identity factor that is age-invariant and an age factor affected by the aging process. Then, the observed appearance can be modeled as a combination of the components generated based on these factors. We also develop a learning algorithm that jointly estimates the latent factors and the model parameters using an EM procedure. Extensive experiments on two well-known public domain face aging datasets: MORPH (the largest public face aging database) and FGNET, clearly show that the proposed method achieves notable improvement over state-of-the-art algorithms.</p><p>6 0.68053472 <a title="328-lsi-6" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>7 0.6558308 <a title="328-lsi-7" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>8 0.63746232 <a title="328-lsi-8" href="./iccv-2013-CoDeL%3A_A_Human_Co-detection_and_Labeling_Framework.html">75 iccv-2013-CoDeL: A Human Co-detection and Labeling Framework</a></p>
<p>9 0.63431311 <a title="328-lsi-9" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>10 0.62260509 <a title="328-lsi-10" href="./iccv-2013-Fast_High_Dimensional_Vector_Multiplication_Face_Recognition.html">158 iccv-2013-Fast High Dimensional Vector Multiplication Face Recognition</a></p>
<p>11 0.60795659 <a title="328-lsi-11" href="./iccv-2013-Face_Recognition_via_Archetype_Hull_Ranking.html">154 iccv-2013-Face Recognition via Archetype Hull Ranking</a></p>
<p>12 0.5949744 <a title="328-lsi-12" href="./iccv-2013-Hybrid_Deep_Learning_for_Face_Verification.html">206 iccv-2013-Hybrid Deep Learning for Face Verification</a></p>
<p>13 0.57514977 <a title="328-lsi-13" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>14 0.57331896 <a title="328-lsi-14" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<p>15 0.57239699 <a title="328-lsi-15" href="./iccv-2013-Markov_Network-Based_Unified_Classifier_for_Face_Identification.html">261 iccv-2013-Markov Network-Based Unified Classifier for Face Identification</a></p>
<p>16 0.57065696 <a title="328-lsi-16" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>17 0.56818843 <a title="328-lsi-17" href="./iccv-2013-Deep_Learning_Identity-Preserving_Face_Space.html">106 iccv-2013-Deep Learning Identity-Preserving Face Space</a></p>
<p>18 0.56542081 <a title="328-lsi-18" href="./iccv-2013-Robust_Face_Landmark_Estimation_under_Occlusion.html">355 iccv-2013-Robust Face Landmark Estimation under Occlusion</a></p>
<p>19 0.55615425 <a title="328-lsi-19" href="./iccv-2013-Coupling_Alignments_with_Recognition_for_Still-to-Video_Face_Recognition.html">97 iccv-2013-Coupling Alignments with Recognition for Still-to-Video Face Recognition</a></p>
<p>20 0.55460304 <a title="328-lsi-20" href="./iccv-2013-Efficient_Pedestrian_Detection_by_Directly_Optimizing_the_Partial_Area_under_the_ROC_Curve.html">136 iccv-2013-Efficient Pedestrian Detection by Directly Optimizing the Partial Area under the ROC Curve</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.063), (4, 0.014), (7, 0.021), (12, 0.021), (26, 0.083), (31, 0.063), (42, 0.151), (64, 0.071), (70, 0.171), (73, 0.03), (77, 0.014), (89, 0.143), (98, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88893676 <a title="328-lda-1" href="./iccv-2013-Predicting_Primary_Gaze_Behavior_Using_Social_Saliency_Fields.html">325 iccv-2013-Predicting Primary Gaze Behavior Using Social Saliency Fields</a></p>
<p>Author: Hyun Soo Park, Eakta Jain, Yaser Sheikh</p><p>Abstract: We present a method to predict primary gaze behavior in a social scene. Inspired by the study of electric fields, we posit “social charges ”—latent quantities that drive the primary gaze behavior of members of a social group. These charges induce a gradient field that defines the relationship between the social charges and the primary gaze direction of members in the scene. This field model is used to predict primary gaze behavior at any location or time in the scene. We present an algorithm to estimate the time-varying behavior of these charges from the primary gaze behavior of measured observers in the scene. We validate the model by evaluating its predictive precision via cross-validation in a variety of social scenes.</p><p>same-paper 2 0.85027659 <a title="328-lda-2" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>Author: Haoxiang Li, Gang Hua, Zhe Lin, Jonathan Brandt, Jianchao Yang</p><p>Abstract: We propose an unsupervised detector adaptation algorithm to adapt any offline trained face detector to a specific collection of images, and hence achieve better accuracy. The core of our detector adaptation algorithm is a probabilistic elastic part (PEP) model, which is offline trained with a set of face examples. It produces a statisticallyaligned part based face representation, namely the PEP representation. To adapt a general face detector to a collection of images, we compute the PEP representations of the candidate detections from the general face detector, and then train a discriminative classifier with the top positives and negatives. Then we re-rank all the candidate detections with this classifier. This way, a face detector tailored to the statistics of the specific image collection is adapted from the original detector. We present extensive results on three datasets with two state-of-the-art face detectors. The significant improvement of detection accuracy over these state- of-the-art face detectors strongly demonstrates the efficacy of the proposed face detector adaptation algorithm.</p><p>3 0.83689052 <a title="328-lda-3" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>Author: Suyog Dutt Jain, Kristen Grauman</p><p>Abstract: The mode of manual annotation used in an interactive segmentation algorithm affects both its accuracy and easeof-use. For example, bounding boxes are fast to supply, yet may be too coarse to get good results on difficult images; freehand outlines are slower to supply and more specific, yet they may be overkill for simple images. Whereas existing methods assume a fixed form of input no matter the image, we propose to predict the tradeoff between accuracy and effort. Our approach learns whether a graph cuts segmentation will succeed if initialized with a given annotation mode, based on the image ’s visual separability and foreground uncertainty. Using these predictions, we optimize the mode of input requested on new images a user wants segmented. Whether given a single image that should be segmented as quickly as possible, or a batch of images that must be segmented within a specified time budget, we show how to select the easiest modality that will be sufficiently strong to yield high quality segmentations. Extensive results with real users and three datasets demonstrate the impact.</p><p>4 0.80848688 <a title="328-lda-4" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>Author: Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, Philip S. Yu</p><p>Abstract: Transfer learning is established as an effective technology in computer visionfor leveraging rich labeled data in the source domain to build an accurate classifier for the target domain. However, most prior methods have not simultaneously reduced the difference in both the marginal distribution and conditional distribution between domains. In this paper, we put forward a novel transfer learning approach, referred to as Joint Distribution Adaptation (JDA). Specifically, JDA aims to jointly adapt both the marginal distribution and conditional distribution in a principled dimensionality reduction procedure, and construct new feature representation that is effective and robustfor substantial distribution difference. Extensive experiments verify that JDA can significantly outperform several state-of-the-art methods on four types of cross-domain image classification problems.</p><p>5 0.80601662 <a title="328-lda-5" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>Author: S. Karthikeyan, Vignesh Jagadeesh, Renuka Shenoy, Miguel Ecksteinz, B.S. Manjunath</p><p>Abstract: Eye movement studies have confirmed that overt attention is highly biased towards faces and text regions in images. In this paper we explore a novel problem of predicting face and text regions in images using eye tracking data from multiple subjects. The problem is challenging as we aim to predict the semantics (face/text/background) only from eye tracking data without utilizing any image information. The proposed algorithm spatially clusters eye tracking data obtained in an image into different coherent groups and subsequently models the likelihood of the clusters containing faces and text using afully connectedMarkov Random Field (MRF). Given the eye tracking datafrom a test image, itpredicts potential face/head (humans, dogs and cats) and text locations reliably. Furthermore, the approach can be used to select regions of interest for further analysis by object detectors for faces and text. The hybrid eye position/object detector approach achieves better detection performance and reduced computation time compared to using only the object detection algorithm. We also present a new eye tracking dataset on 300 images selected from ICDAR, Street-view, Flickr and Oxford-IIIT Pet Dataset from 15 subjects.</p><p>6 0.80301386 <a title="328-lda-6" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>7 0.79784673 <a title="328-lda-7" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>8 0.7971034 <a title="328-lda-8" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>9 0.79495323 <a title="328-lda-9" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>10 0.79425722 <a title="328-lda-10" href="./iccv-2013-Bayesian_Joint_Topic_Modelling_for_Weakly_Supervised_Object_Localisation.html">59 iccv-2013-Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation</a></p>
<p>11 0.79403937 <a title="328-lda-11" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>12 0.79380631 <a title="328-lda-12" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>13 0.79308391 <a title="328-lda-13" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>14 0.79186285 <a title="328-lda-14" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>15 0.79146749 <a title="328-lda-15" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>16 0.79062539 <a title="328-lda-16" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>17 0.79051256 <a title="328-lda-17" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>18 0.78984129 <a title="328-lda-18" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>19 0.78957033 <a title="328-lda-19" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>20 0.78898108 <a title="328-lda-20" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
