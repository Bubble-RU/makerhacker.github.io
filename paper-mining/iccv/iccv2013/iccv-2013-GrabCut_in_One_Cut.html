<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>186 iccv-2013-GrabCut in One Cut</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-186" href="#">iccv2013-186</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>186 iccv-2013-GrabCut in One Cut</h1>
<br/><p>Source: <a title="iccv-2013-186-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Tang_GrabCut_in_One_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Meng Tang, Lena Gorelick, Olga Veksler, Yuri Boykov</p><p>Abstract: Among image segmentation algorithms there are two major groups: (a) methods assuming known appearance models and (b) methods estimating appearance models jointly with segmentation. Typically, the first group optimizes appearance log-likelihoods in combination with some spacial regularization. This problem is relatively simple and many methods guarantee globally optimal results. The second group treats model parameters as additional variables transforming simple segmentation energies into highorder NP-hard functionals (Zhu-Yuille, Chan-Vese, GrabCut, etc). It is known that such methods indirectly minimize the appearance overlap between the segments. We propose a new energy term explicitly measuring L1 distance between the object and background appearance models that can be globally maximized in one graph cut. We show that in many applications our simple term makes NP-hard segmentation functionals unnecessary. Our one cut algorithm effectively replaces approximate iterative optimization techniques based on block coordinate descent.</p><p>Reference: <a title="iccv-2013-186-reference" href="../iccv2013_reference/iccv-2013-GrabCut_in_One_Cut_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 GrabCut in One Cut Meng Tang  Lena Gorelick Olga Veksler University of Western Ontario Canada  Yuri Boykov  Abstract Among image segmentation algorithms there are two major groups: (a) methods assuming known appearance models and (b) methods estimating appearance models jointly with segmentation. [sent-1, score-0.496]
</p><p>2 The second group treats model parameters as additional variables transforming simple segmentation energies into highorder NP-hard functionals (Zhu-Yuille, Chan-Vese, GrabCut, etc). [sent-4, score-0.296]
</p><p>3 It is known that such methods indirectly minimize the appearance overlap between the segments. [sent-5, score-0.428]
</p><p>4 We propose a new energy term explicitly measuring L1 distance between the object and background appearance models that can be globally maximized in one graph cut. [sent-6, score-0.615]
</p><p>5 We show that in many applications our simple term makes NP-hard segmentation functionals unnecessary. [sent-7, score-0.36]
</p><p>6 The most basic object segmentation energy [3, 20] combines boundary regularization with loglikelihood ratios for fixed foreground and background appearance models, e. [sent-11, score-0.663]
</p><p>7 {p,q}∈N ωpq |sp − sq | where sp ∈ {0, 1} are binary indic? [sent-18, score-0.201]
</p><p>8 n tO Sne ⊂ important practical advantage o off tnheisig hbbasoicri energy is that there are efficient methods for their global minimization using graph cuts [4] or continuous relaxations [5, 18]. [sent-20, score-0.221]
</p><p>9 Some well-known approaches to segmentation [25, 19, 6] consider model parameters as extra optimization variables in their segmentation energies. [sent-22, score-0.372]
</p><p>10 p∈Ω  which is known to be NP-hard [22], is used for interactive segmentation in GrabCut [ 19] where initial appearance models θ1, θ0 are computed from a given bounding box. [sent-27, score-0.493]
</p><p>11 using a graph cut algorithm for energy (1) as in [3]. [sent-31, score-0.351]
</p><p>12 Second, they fix segmentation S and then optimize over model parameters θ1 and θ0. [sent-32, score-0.226]
</p><p>13 Two wellknown alternatives, dual decomposition [22] and branchand-mincut [14], can find a global minimum of energy (2), but these methods are too slow in practice. [sent-33, score-0.29]
</p><p>14 We observe that when appearance models θ1, θ0 are represented by (non-parametric) color histograms, minimization of (2) is equivalent to minimization of energy  · H(θS¯) + |∂S| on S only. [sent-34, score-0.389]
</p><p>15 HT(h·i)s sfo trhme of energy (2) can be obtained by replacing the sum over pixels in (2) by the sum over color bins. [sent-36, score-0.337]
</p><p>16 Interestingly, the global minimum|θ of) segmentation energy (3) does not depend on the initial color models provided by the user. [sent-40, score-0.42]
</p><p>17 The entropy terms of this energy prefer segments with more peaked color distributions. [sent-43, score-0.378]
</p><p>18 In general, the color separation bias in energy (3) is shown by equivalently rewriting its two entropy terms as  hΩ(S) −? [sent-49, score-0.518]
</p><p>19 Energy (3): volum−eh balancing (a) and −Jen|Ssen−-Shan|non color separation terms (b). [sent-85, score-0.349]
</p><p>20 plicitly bias image segmentation to two parts of equal size, see Fig. [sent-87, score-0.251]
</p><p>21 The remaining terms in (4) show bias to color separation between the segments, see Fig. [sent-89, score-0.299]
</p><p>22 Note that a similar analysis in [22] is used to motivate their convexconcave approximation algorithm for energy (2). [sent-91, score-0.237]
</p><p>23 Volume balancing hΩ (S) is the only term in (4) and (2) that is not submodular and makes optimization difficult. [sent-92, score-0.247]
</p><p>24 Our observation is that in many applications this volume balancing term is simply unnecessary, see Sections 3. [sent-93, score-0.207]
</p><p>25 Moreover, while it is known that JS color separation term −hΩi (Si) is submodular1 and can be optimized by graph c−uhts [11, 12, 22], we propose to replace it with a more basic L1 separation term in Fig. [sent-99, score-0.744]
</p><p>26 We show that it corresponds to a simpler construction with much fewer auxiliary nodes leading to higher efficiency. [sent-101, score-0.338]
</p><p>27 Interestingly, it also gives better color separation effect, see Section 3. [sent-102, score-0.231]
</p><p>28 We also observe one practical limitation of blockcoordinate approach to (2), as in GrabCut [ 19], could be due to deteriorating sensitivity to local minima when the number of color bins for models θS and is increased, see Table 2 and Fig. [sent-105, score-0.415]
</p><p>29 In practice, however, finer bins better capture the information contained in the full dynamic range of color images (8-bit per channel or more). [sent-107, score-0.35]
</p><p>30 This applies to JS, χ2, Bhattacharyya, and our L1 color separation terms in Figs. [sent-109, score-0.266]
</p><p>31 The corresponding ROC curves show that the color separation between the object and background increases for finer bins. [sent-113, score-0.305]
</p><p>32 curves show that even a difficult camouflage image in Figure 2 has a good separation of intensities between the object and background if larger number of bins is used. [sent-114, score-0.485]
</p><p>33 With bins, however, the overlap between the “fish” and the background is too strong making it hard to segment. [sent-115, score-0.347]
</p><p>34 Our contributions are summarized below:  163  •  •  •  We propose a simple energy term penalizing L1 measure orfo appearance overlap ybe tetwrmee pne segments. [sent-118, score-0.701]
</p><p>35 Unlike NP-hard multi-label problems discussed in [ 11, 12], we focus on binary segmentation where such high-order constraints can be globally minimized. [sent-121, score-0.293]
</p><p>36 Moreover, we show that our L1 term works better for separating colors than other concave separators (including JS, Bhattacharyya, and χ2). [sent-122, score-0.204]
</p><p>37 We are first to demonstrate fast globally optimal binary segmentation technique explicitly minimizing overlap between object/background color distributions. [sent-123, score-0.651]
</p><p>38 In one graph cut we get similar or better results at faster running times w. [sent-124, score-0.248]
</p><p>39 We show general usefulness of the proposed appearance overlap penalty by showing dthifefe prreonpto practical applications: binary segmentation, shape matching, etc. [sent-130, score-0.619]
</p><p>40 Minimizing appearance overlap in One-Cut Let S ⊂ Ω be a segment and denote by θS and the Luentno Srm ⊂aliz Ωed b eco alor s histograms f dore thoete foreground and background appearance respectively. [sent-132, score-0.827]
</p><p>41 vnkcor-  responding to the pixels in bin k are connected to the auxiliary node Ak using undirected links. [sent-135, score-0.434]
</p><p>42 The capacity of these links is the weight of appearance overlap term β > 0. [sent-136, score-0.659]
</p><p>43 of pixels in the image the belong to bin k and let nkS and be the according number of the foreground and background pixels in bin k. [sent-137, score-0.566]
</p><p>44 Our appearance overlap term penalizes the intersection between the foreground and background bin counts by incorporating the simple yet effective high-order L1 term into the energy function:  nSk¯  EL1(θS,  θS¯)  = −? [sent-138, score-1.107]
</p><p>45 (5)  Below we explain how to incorporate and optimize the term EL1 (θS, θS¯) using one graph cut. [sent-141, score-0.236]
</p><p>46 The details of the graph construction for the above term are  shown in Fig. [sent-145, score-0.289]
</p><p>47 , AK into the graph and connect kth auxiliary node to all the pixels that belong to the kth bin. [sent-150, score-0.364]
</p><p>48 Assume that bin k is split into foreground and background, resulting in nkS and nSk¯ pixels accordingly. [sent-153, score-0.279]
</p><p>49 Then any cut separating the foreground and background pixels must either cut nkS or nSk¯ number of links that connect the pixels in bin k to the auxiliary node Ak. [sent-154, score-0.971]
</p><p>50 A similar graph construction with auxiliary nodes is proposed in [11, 12] to minimize higher order pseudo-boolean functions of the following form: f(Xc) = min{θ0+? [sent-156, score-0.443]
</p><p>51 The construction in [11, 12] can be used to minimize EL1 as follows: consider each color bin as a clique and set parameters wi0 = 1, wi1 =  Figure4. [sent-166, score-0.357]
</p><p>52 1, θ0 = 0, θ1 = 0 and θmax = nk/2, where nk is the number of pixels in bin k. [sent-168, score-0.247]
</p><p>53 e Oedn one auxiliary fn ooudre g froarp hea ccohn bsitrnu icncontrast to two auxiliary nodes in [11, 12]. [sent-171, score-0.468]
</p><p>54 To see how it works we consider four possible label assignments for the auxiliary nodes Ak1 and Ak0. [sent-174, score-0.245]
</p><p>55 ments to the binary auxiliary nodes Ak1 and Ak0 . [sent-177, score-0.311]
</p><p>56 Applications In this section we apply our appearance overlap penalty term in a number of different practical applications including interactive binary segmentation in Sec. [sent-189, score-0.941]
</p><p>57 Interactive segmentation First, we discuss interactive segmentation with several standard user interfaces: bounding box [ 19] in Section 3. [sent-198, score-0.608]
</p><p>58 We compare different color separation terms in Section 3. [sent-203, score-0.266]
</p><p>59 1  Binary segmentation with bounding box  First, we use appearance overlap penalty in a binary segmentation experiment a la GrabCut [19]. [sent-234, score-1.144]
</p><p>60 A user provides a bounding box around an object of interest and the goal is to perform binary image segmentation within the box. [sent-235, score-0.424]
</p><p>61 The pixels outside the bounding box are assigned to the background using hard constraints. [sent-236, score-0.317]
</p><p>62 e bounding Ωbo dxe,n oStGeT t ⊆e Ωibe the ground truth segmentation and S ⊆ Ω be a segment. [sent-238, score-0.274]
</p><p>63 The segmentation energy }fu tnhceti cohna rEac(tSer) istsi given by E(S) = |S¯ ∩ R|  − β? [sent-241, score-0.335]
</p><p>64 L1 + λ|∂S|,  (9)  where the first term is a standard ballooning inside the bounding box R, the second term is the appearance overlap penalty as in (5), and the last term is a contrast-sensitive smoothness term. [sent-243, score-1.11]
</p><p>65 It is common to tune the relative weight of each energy term for a given dataset [22]. [sent-251, score-0.337]
</p><p>66 We use the measure of appearance overlap between the box R and its background R¯ to automatically find image specific relative weight β of the appearance overlap term w. [sent-253, score-1.138]
</p><p>67 The error rate is defined as the number of misclassified pixels within the bounding box R divided by the size of the box |R| . [sent-270, score-0.494]
</p><p>68 We test with different number of color boifn sth aend b provide quantitative comparison with the grab-cut method [ 19] (our implementation, modified to work with histograms as in [22]) and the dual decomposition method [22] (results reported by the authors). [sent-271, score-0.231]
</p><p>69 As we increase the number of color bins, the  error rate for the GrabCut method increases, while the error rate of One-Cut decreases. [sent-277, score-0.285]
</p><p>70 When using 1283 bins One-Cut runs twice as fast, while obtaining much lower error rate. [sent-278, score-0.31]
</p><p>71 This is because with more bins, more auxiliary nodes are used, but each auxiliary node is connected to less pixels. [sent-279, score-0.466]
</p><p>72 Finally, Figures 6-7 show examples of input bounding boxes and segmentation results with the GrabCut [ 19], Dual Decomposition [22] and our One-Cut method. [sent-283, score-0.274]
</p><p>73 2  Comparison of Appearance Overlap Terms  Below we discus additional variants for appearance overlap penalty term. [sent-286, score-0.501]
</p><p>74 4 and compare their performance in the task of binary segmentation applied to the GrabCut dataset [19]. [sent-288, score-0.252]
</p><p>75 We consider four appearance overlap terms based on the L1 norm, χ2 distance, Bhattacharyya coefficient and Jensen-Shannon divergence 3There are 50 images in the dataset, but we exclude the cross image for the sake of comparison with [22]. [sent-289, score-0.463]
</p><p>76 All four terms above are concave functions of nks attaining maximum at nk/2. [sent-313, score-0.407]
</p><p>77 These truncated components can be incorporated into our graph using the construction shown in Fig. [sent-319, score-0.234]
</p><p>78 All three appearance overlap terms above can be optimized with one graph cut. [sent-322, score-0.535]
</p><p>79 We wish to find out which term and what level of approximation accuracy are optimal for the task of binary segmentation. [sent-323, score-0.246]
</p><p>80 In the first experiment, we use an adaptive image specific weight βImg for the appearance overlap term as in (10) and set β? [sent-331, score-0.552]
</p><p>81 In the second experiment, we choose the optimal βImg separately for each appearance overlap term by replacing the denominator of (10) with either Dχ2 (θR, θR¯),  θR¯)  θR¯)  DBha(θR, or DJS(θR, according to the appearance overlap term used. [sent-336, score-1.154]
</p><p>82 Comparison of appearance overlap terms: (top-left) shows the functions plotted for one bin k, (top-right) shows segmentation error rates using the same βImg as in (10) for all overlap terms and (bottom-left) shows segmentation results when using a term-specific βImg . [sent-343, score-1.369]
</p><p>83 10 reports the error  (θR, θR¯)  rates of the segmentation as a function of the parameter t. [sent-355, score-0.272]
</p><p>84 This further proves the benefit of our proposed DL1 appearance overlap term. [sent-357, score-0.428]
</p><p>85 3  Interactive Segmentation with Seeds  Unlike interactive segmentation with abounding box, using seeds a la Boykov-Jolly [3] makes volumetric balancing unnecessary due to hard constraints enforced by the user. [sent-371, score-0.432]
</p><p>86 Therefore, the segmentation energy is quite simple: Eseeds(S) = −β? [sent-372, score-0.335]
</p><p>87 Template Shape Matching Below we discuss how appearance overlap penalty term can be used for template shape matching. [sent-378, score-0.784]
</p><p>88 Several prior methods rely on graph-cut based segmentation with shape prior [2 1, 8, 14, 23]. [sent-379, score-0.238]
</p><p>89 Most commonly, these methods use a binary template mask M and combine the shape matching cue a contrast sensitive smoothness term via energy  E1(S) = mρ∈iΦnEShape(S,Mρ) + λ|∂S|. [sent-380, score-0.539]
</p><p>90 We further incorporate the appearance overlap into the energy: E2(S) = E1(S) − β? [sent-384, score-0.428]
</p><p>91 Without the appearance overlap term shape matching yields erroneous segmentation due to the clutter edges in the background. [sent-389, score-0.831]
</p><p>92 We down-scaled all images to 320 240 and used 1283 color bins per image. [sent-392, score-0.35]
</p><p>93 For the template matching, we scan the image in a sliding-window fashion and compute maxflow/mincut with dynamic graph cut [13]. [sent-394, score-0.309]
</p><p>94 Template shape matching examples: ages, (b) Contrast sensitive edge weights, (c-d) with and without the appearance overlap penalty. [sent-400, score-0.521]
</p><p>95 Table 3 provides quantitative comparison of the results obtained with and without incorporating the appearance overlap term, namely using E2 (S) and E1(S). [sent-404, score-0.428]
</p><p>96 The results are reported with respect to manually outlined ground truth segmentations and clearly point to the benefit of incorporating the overlap term EL1 into the segmentation energy. [sent-405, score-0.583]
</p><p>97 Template shape matching: comparing E1(S) and E2 (S) in terms of TP, FP, misclassified pixels, and mean running time. [sent-418, score-0.2]
</p><p>98 Salient object segmentation Salient region detection and segmentation is an important preprocessing step for object recognition and adaptive compression. [sent-423, score-0.372]
</p><p>99 Salient objects usually have an appearance that is distinct from the background [1, 7, 17]. [sent-424, score-0.229]
</p><p>100 Below we show how our appearance overlap penalty term can be used for the task of salient object segmentation. [sent-425, score-0.67]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('grabcut', 0.348), ('overlap', 0.273), ('bins', 0.265), ('nks', 0.259), ('auxiliary', 0.186), ('segmentation', 0.186), ('img', 0.18), ('appearance', 0.155), ('energy', 0.149), ('dbha', 0.146), ('nsk', 0.146), ('separation', 0.146), ('bin', 0.142), ('cut', 0.13), ('djs', 0.13), ('term', 0.124), ('template', 0.107), ('construction', 0.093), ('bhattacharyya', 0.09), ('bounding', 0.088), ('sp', 0.085), ('color', 0.085), ('box', 0.084), ('balancing', 0.083), ('concave', 0.08), ('background', 0.074), ('penalty', 0.073), ('lnpr', 0.073), ('js', 0.072), ('xc', 0.072), ('graph', 0.072), ('pixels', 0.071), ('entropy', 0.07), ('truncated', 0.069), ('misclassified', 0.067), ('binary', 0.066), ('foreground', 0.066), ('links', 0.066), ('seeds', 0.065), ('ballooning', 0.065), ('tune', 0.064), ('max', 0.064), ('interactive', 0.064), ('dual', 0.062), ('energies', 0.06), ('nodes', 0.059), ('dd', 0.057), ('cated', 0.056), ('approximation', 0.056), ('pq', 0.055), ('rate', 0.055), ('shape', 0.052), ('kk', 0.051), ('functionals', 0.05), ('denominator', 0.05), ('sq', 0.05), ('replace', 0.047), ('ln', 0.046), ('running', 0.046), ('error', 0.045), ('salient', 0.045), ('decomposition', 0.044), ('min', 0.041), ('matching', 0.041), ('saliency', 0.041), ('rates', 0.041), ('capacity', 0.041), ('globally', 0.041), ('submodular', 0.04), ('si', 0.04), ('histograms', 0.04), ('optimize', 0.04), ('segments', 0.039), ('clique', 0.037), ('fn', 0.037), ('terms', 0.035), ('node', 0.035), ('minimum', 0.035), ('unnecessary', 0.034), ('nk', 0.034), ('bias', 0.033), ('minima', 0.033), ('ratios', 0.033), ('ak', 0.033), ('functions', 0.033), ('roc', 0.033), ('experiment', 0.033), ('blockcoordinate', 0.032), ('plicitly', 0.032), ('nmda', 0.032), ('trhme', 0.032), ('olga', 0.032), ('dbee', 0.032), ('rrte', 0.032), ('jen', 0.032), ('royf', 0.032), ('thoete', 0.032), ('bce', 0.032), ('convexconcave', 0.032), ('dore', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="186-tfidf-1" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<p>Author: Meng Tang, Lena Gorelick, Olga Veksler, Yuri Boykov</p><p>Abstract: Among image segmentation algorithms there are two major groups: (a) methods assuming known appearance models and (b) methods estimating appearance models jointly with segmentation. Typically, the first group optimizes appearance log-likelihoods in combination with some spacial regularization. This problem is relatively simple and many methods guarantee globally optimal results. The second group treats model parameters as additional variables transforming simple segmentation energies into highorder NP-hard functionals (Zhu-Yuille, Chan-Vese, GrabCut, etc). It is known that such methods indirectly minimize the appearance overlap between the segments. We propose a new energy term explicitly measuring L1 distance between the object and background appearance models that can be globally maximized in one graph cut. We show that in many applications our simple term makes NP-hard segmentation functionals unnecessary. Our one cut algorithm effectively replaces approximate iterative optimization techniques based on block coordinate descent.</p><p>2 0.20026901 <a title="186-tfidf-2" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>Author: Wei Xia, Csaba Domokos, Jian Dong, Loong-Fah Cheong, Shuicheng Yan</p><p>Abstract: Numerous existing object segmentation frameworks commonly utilize the object bounding box as a prior. In this paper, we address semantic segmentation assuming that object bounding boxes are provided by object detectors, but no training data with annotated segments are available. Based on a set of segment hypotheses, we introduce a simple voting scheme to estimate shape guidance for each bounding box. The derived shape guidance is used in the subsequent graph-cut-based figure-ground segmentation. The final segmentation result is obtained by merging the segmentation results in the bounding boxes. We conduct an extensive analysis of the effect of object bounding box accuracy. Comprehensive experiments on both the challenging PASCAL VOC object segmentation dataset and GrabCut50 image segmentation dataset show that the proposed approach achieves competitive results compared to previous detection or bounding box prior based methods, as well as other state-of-the-art semantic segmentation methods.</p><p>3 0.17914358 <a title="186-tfidf-3" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>Author: Yuning Chai, Victor Lempitsky, Andrew Zisserman</p><p>Abstract: We propose a new method for the task of fine-grained visual categorization. The method builds a model of the baselevel category that can be fitted to images, producing highquality foreground segmentation and mid-level part localizations. The model can be learnt from the typical datasets available for fine-grained categorization, where the only annotation provided is a loose bounding box around the instance (e.g. bird) in each image. Both segmentation and part localizations are then used to encode the image content into a highly-discriminative visual signature. The model is symbiotic in that part discovery/localization is helped by segmentation and, conversely, the segmentation is helped by the detection (e.g. part layout). Our model builds on top of the part-based object category detector of Felzenszwalb et al., and also on the powerful GrabCut segmentation algorithm of Rother et al., and adds a simple spatial saliency coupling between them. In our evaluation, the model improves the categorization accuracy over the state-of-the-art. It also improves over what can be achieved with an analogous system that runs segmentation and part-localization independently.</p><p>4 0.15468848 <a title="186-tfidf-4" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>Author: Suyog Dutt Jain, Kristen Grauman</p><p>Abstract: The mode of manual annotation used in an interactive segmentation algorithm affects both its accuracy and easeof-use. For example, bounding boxes are fast to supply, yet may be too coarse to get good results on difficult images; freehand outlines are slower to supply and more specific, yet they may be overkill for simple images. Whereas existing methods assume a fixed form of input no matter the image, we propose to predict the tradeoff between accuracy and effort. Our approach learns whether a graph cuts segmentation will succeed if initialized with a given annotation mode, based on the image ’s visual separability and foreground uncertainty. Using these predictions, we optimize the mode of input requested on new images a user wants segmented. Whether given a single image that should be segmented as quickly as possible, or a batch of images that must be segmented within a specified time budget, we show how to select the easiest modality that will be sufficiently strong to yield high quality segmentations. Extensive results with real users and three datasets demonstrate the impact.</p><p>5 0.14716443 <a title="186-tfidf-5" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>Author: Abdelaziz Djelouah, Jean-Sébastien Franco, Edmond Boyer, François Le_Clerc, Patrick Pérez</p><p>Abstract: In this paper, we address the problem of object segmentation in multiple views or videos when two or more viewpoints of the same scene are available. We propose a new approach that propagates segmentation coherence information in both space and time, hence allowing evidences in one image to be shared over the complete set. To this aim the segmentation is cast as a single efficient labeling problem over space and time with graph cuts. In contrast to most existing multi-view segmentation methods that rely on some form of dense reconstruction, ours only requires a sparse 3D sampling to propagate information between viewpoints. The approach is thoroughly evaluated on standard multiview datasets, as well as on videos. With static views, results compete with state of the art methods but they are achieved with significantly fewer viewpoints. With multiple videos, we report results that demonstrate the benefit of segmentation propagation through temporal cues.</p><p>6 0.13924639 <a title="186-tfidf-6" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>7 0.12406189 <a title="186-tfidf-7" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>8 0.12276065 <a title="186-tfidf-8" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>9 0.12254714 <a title="186-tfidf-9" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>10 0.11387544 <a title="186-tfidf-10" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>11 0.11352478 <a title="186-tfidf-11" href="./iccv-2013-Discriminative_Label_Propagation_for_Multi-object_Tracking_with_Sporadic_Appearance_Features.html">120 iccv-2013-Discriminative Label Propagation for Multi-object Tracking with Sporadic Appearance Features</a></p>
<p>12 0.11345831 <a title="186-tfidf-12" href="./iccv-2013-Tree_Shape_Priors_with_Connectivity_Constraints_Using_Convex_Relaxation_on_General_Graphs.html">429 iccv-2013-Tree Shape Priors with Connectivity Constraints Using Convex Relaxation on General Graphs</a></p>
<p>13 0.11287697 <a title="186-tfidf-13" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>14 0.10721019 <a title="186-tfidf-14" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<p>15 0.10410839 <a title="186-tfidf-15" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>16 0.10297545 <a title="186-tfidf-16" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>17 0.10208889 <a title="186-tfidf-17" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>18 0.10087863 <a title="186-tfidf-18" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>19 0.10079422 <a title="186-tfidf-19" href="./iccv-2013-Fix_Structured_Learning_of_2013_ICCV_paper_k2opt.pdf.html">171 iccv-2013-Fix Structured Learning of 2013 ICCV paper k2opt.pdf</a></p>
<p>20 0.096594572 <a title="186-tfidf-20" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.224), (1, -0.053), (2, 0.092), (3, -0.034), (4, 0.057), (5, 0.028), (6, -0.12), (7, 0.111), (8, 0.026), (9, -0.102), (10, -0.019), (11, 0.13), (12, 0.028), (13, -0.002), (14, -0.012), (15, 0.037), (16, 0.043), (17, -0.008), (18, -0.068), (19, -0.067), (20, 0.098), (21, 0.021), (22, -0.073), (23, 0.008), (24, 0.001), (25, 0.072), (26, -0.071), (27, -0.009), (28, -0.003), (29, 0.002), (30, 0.008), (31, 0.033), (32, 0.042), (33, 0.018), (34, -0.025), (35, 0.046), (36, -0.059), (37, 0.102), (38, -0.008), (39, 0.062), (40, 0.008), (41, 0.049), (42, 0.004), (43, 0.033), (44, 0.044), (45, 0.061), (46, 0.049), (47, -0.043), (48, -0.078), (49, -0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.981893 <a title="186-lsi-1" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<p>Author: Meng Tang, Lena Gorelick, Olga Veksler, Yuri Boykov</p><p>Abstract: Among image segmentation algorithms there are two major groups: (a) methods assuming known appearance models and (b) methods estimating appearance models jointly with segmentation. Typically, the first group optimizes appearance log-likelihoods in combination with some spacial regularization. This problem is relatively simple and many methods guarantee globally optimal results. The second group treats model parameters as additional variables transforming simple segmentation energies into highorder NP-hard functionals (Zhu-Yuille, Chan-Vese, GrabCut, etc). It is known that such methods indirectly minimize the appearance overlap between the segments. We propose a new energy term explicitly measuring L1 distance between the object and background appearance models that can be globally maximized in one graph cut. We show that in many applications our simple term makes NP-hard segmentation functionals unnecessary. Our one cut algorithm effectively replaces approximate iterative optimization techniques based on block coordinate descent.</p><p>2 0.87664235 <a title="186-lsi-2" href="./iccv-2013-Bounded_Labeling_Function_for_Global_Segmentation_of_Multi-part_Objects_with_Geometric_Constraints.html">63 iccv-2013-Bounded Labeling Function for Global Segmentation of Multi-part Objects with Geometric Constraints</a></p>
<p>Author: Masoud S. Nosrati, Shawn Andrews, Ghassan Hamarneh</p><p>Abstract: The inclusion of shape and appearance priors have proven useful for obtaining more accurate and plausible segmentations, especially for complex objects with multiple parts. In this paper, we augment the popular MumfordShah model to incorporate two important geometrical constraints, termed containment and detachment, between different regions with a specified minimum distance between their boundaries. Our method is able to handle multiple instances of multi-part objects defined by these geometrical hamarneh} @ s fu . ca (a)Standar laΩb ehlingΩfuhnctionseting(Ωb)hΩOuirseΩtijng Figure 1: The inside vs. outside ambiguity in (a) is resolved by our containment constraint in (b). constraints using a single labeling function while maintaining global optimality. We demonstrate the utility and advantages of these two constraints and show that the proposed convex continuous method is superior to other state-of-theart methods, including its discrete counterpart, in terms of memory usage, and metrication errors.</p><p>3 0.84860617 <a title="186-lsi-3" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>Author: Jimei Yang, Yi-Hsuan Tsai, Ming-Hsuan Yang</p><p>Abstract: We present a hybrid parametric and nonparametric algorithm, exemplar cut, for generating class-specific object segmentation hypotheses. For the parametric part, we train a pylon model on a hierarchical region tree as the energy function for segmentation. For the nonparametric part, we match the input image with each exemplar by using regions to obtain a score which augments the energy function from the pylon model. Our method thus generates a set of highly plausible segmentation hypotheses by solving a series of exemplar augmented graph cuts. Experimental results on the Graz and PASCAL datasets show that the proposed algorithm achievesfavorable segmentationperformance against the state-of-the-art methods in terms of visual quality and accuracy.</p><p>4 0.82664138 <a title="186-lsi-4" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>Author: Wei Xia, Csaba Domokos, Jian Dong, Loong-Fah Cheong, Shuicheng Yan</p><p>Abstract: Numerous existing object segmentation frameworks commonly utilize the object bounding box as a prior. In this paper, we address semantic segmentation assuming that object bounding boxes are provided by object detectors, but no training data with annotated segments are available. Based on a set of segment hypotheses, we introduce a simple voting scheme to estimate shape guidance for each bounding box. The derived shape guidance is used in the subsequent graph-cut-based figure-ground segmentation. The final segmentation result is obtained by merging the segmentation results in the bounding boxes. We conduct an extensive analysis of the effect of object bounding box accuracy. Comprehensive experiments on both the challenging PASCAL VOC object segmentation dataset and GrabCut50 image segmentation dataset show that the proposed approach achieves competitive results compared to previous detection or bounding box prior based methods, as well as other state-of-the-art semantic segmentation methods.</p><p>5 0.81007254 <a title="186-lsi-5" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<p>Author: Fan Wang, Qixing Huang, Leonidas J. Guibas</p><p>Abstract: Joint segmentation of image sets has great importance for object recognition, image classification, and image retrieval. In this paper, we aim to jointly segment a set of images starting from a small number of labeled images or none at all. To allow the images to share segmentation information with each other, we build a network that contains segmented as well as unsegmented images, and extract functional maps between connected image pairs based on image appearance features. These functional maps act as general property transporters between the images and, in particular, are used to transfer segmentations. We define and operate in a reduced functional space optimized so that the functional maps approximately satisfy cycle-consistency under composition in the network. A joint optimization framework is proposed to simultaneously generate all segmentation functions over the images so that they both align with local segmentation cues in each particular image, and agree with each other under network transportation. This formulation allows us to extract segmentations even with no training data, but can also exploit such data when available. The collective effect of the joint processing using functional maps leads to accurate information sharing among images and yields superior segmentation results, as shown on the iCoseg, MSRC, and PASCAL data sets.</p><p>6 0.80392814 <a title="186-lsi-6" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>7 0.78093928 <a title="186-lsi-7" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>8 0.74388427 <a title="186-lsi-8" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>9 0.72800905 <a title="186-lsi-9" href="./iccv-2013-Tree_Shape_Priors_with_Connectivity_Constraints_Using_Convex_Relaxation_on_General_Graphs.html">429 iccv-2013-Tree Shape Priors with Connectivity Constraints Using Convex Relaxation on General Graphs</a></p>
<p>10 0.71006387 <a title="186-lsi-10" href="./iccv-2013-Uncertainty-Driven_Efficiently-Sampled_Sparse_Graphical_Models_for_Concurrent_Tumor_Segmentation_and_Atlas_Registration.html">432 iccv-2013-Uncertainty-Driven Efficiently-Sampled Sparse Graphical Models for Concurrent Tumor Segmentation and Atlas Registration</a></p>
<p>11 0.69833517 <a title="186-lsi-11" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>12 0.68726456 <a title="186-lsi-12" href="./iccv-2013-Learning_a_Dictionary_of_Shape_Epitomes_with_Applications_to_Image_Labeling.html">245 iccv-2013-Learning a Dictionary of Shape Epitomes with Applications to Image Labeling</a></p>
<p>13 0.68627656 <a title="186-lsi-13" href="./iccv-2013-A_Unified_Video_Segmentation_Benchmark%3A_Annotation%2C_Metrics_and_Analysis.html">33 iccv-2013-A Unified Video Segmentation Benchmark: Annotation, Metrics and Analysis</a></p>
<p>14 0.67344701 <a title="186-lsi-14" href="./iccv-2013-A_Deformable_Mixture_Parsing_Model_with_Parselets.html">8 iccv-2013-A Deformable Mixture Parsing Model with Parselets</a></p>
<p>15 0.66649276 <a title="186-lsi-15" href="./iccv-2013-Progressive_Multigrid_Eigensolvers_for_Multiscale_Spectral_Segmentation.html">329 iccv-2013-Progressive Multigrid Eigensolvers for Multiscale Spectral Segmentation</a></p>
<p>16 0.65778959 <a title="186-lsi-16" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>17 0.65036005 <a title="186-lsi-17" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>18 0.62369668 <a title="186-lsi-18" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>19 0.62135482 <a title="186-lsi-19" href="./iccv-2013-PixelTrack%3A_A_Fast_Adaptive_Algorithm_for_Tracking_Non-rigid_Objects.html">318 iccv-2013-PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects</a></p>
<p>20 0.61837792 <a title="186-lsi-20" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.071), (7, 0.013), (8, 0.19), (12, 0.013), (13, 0.011), (26, 0.103), (27, 0.017), (31, 0.051), (40, 0.013), (42, 0.094), (48, 0.017), (64, 0.072), (73, 0.054), (89, 0.193), (95, 0.011), (98, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86812621 <a title="186-lda-1" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>Author: C. Lawrence Zitnick, Devi Parikh, Lucy Vanderwende</p><p>Abstract: Sentences that describe visual scenes contain a wide variety of information pertaining to the presence of objects, their attributes and their spatial relations. In this paper we learn the visual features that correspond to semantic phrases derived from sentences. Specifically, we extract predicate tuples that contain two nouns and a relation. The relation may take several forms, such as a verb, preposition, adjective or their combination. We model a scene using a Conditional Random Field (CRF) formulation where each node corresponds to an object, and the edges to their relations. We determine the potentials of the CRF using the tuples extracted from the sentences. We generate novel scenes depicting the sentences’ visual meaning by sampling from the CRF. The CRF is also used to score a set of scenes for a text-based image retrieval task. Our results show we can generate (retrieve) scenes that convey the desired semantic meaning, even when scenes (queries) are described by multiple sentences. Significant improvement is found over several baseline approaches.</p><p>2 0.85851306 <a title="186-lda-2" href="./iccv-2013-3D_Sub-query_Expansion_for_Improving_Sketch-Based_Multi-view_Image_Retrieval.html">3 iccv-2013-3D Sub-query Expansion for Improving Sketch-Based Multi-view Image Retrieval</a></p>
<p>Author: Yen-Liang Lin, Cheng-Yu Huang, Hao-Jeng Wang, Winston Hsu</p><p>Abstract: We propose a 3D sub-query expansion approach for boosting sketch-based multi-view image retrieval. The core idea of our method is to automatically convert two (guided) 2D sketches into an approximated 3D sketch model, and then generate multi-view sketches as expanded sub-queries to improve the retrieval performance. To learn the weights among synthesized views (sub-queries), we present a new multi-query feature to model the similarity between subqueries and dataset images, and formulate it into a convex optimization problem. Our approach shows superior performance compared with the state-of-the-art approach on a public multi-view image dataset. Moreover, we also conduct sensitivity tests to analyze the parameters of our approach based on the gathered user sketches.</p><p>same-paper 3 0.85599905 <a title="186-lda-3" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<p>Author: Meng Tang, Lena Gorelick, Olga Veksler, Yuri Boykov</p><p>Abstract: Among image segmentation algorithms there are two major groups: (a) methods assuming known appearance models and (b) methods estimating appearance models jointly with segmentation. Typically, the first group optimizes appearance log-likelihoods in combination with some spacial regularization. This problem is relatively simple and many methods guarantee globally optimal results. The second group treats model parameters as additional variables transforming simple segmentation energies into highorder NP-hard functionals (Zhu-Yuille, Chan-Vese, GrabCut, etc). It is known that such methods indirectly minimize the appearance overlap between the segments. We propose a new energy term explicitly measuring L1 distance between the object and background appearance models that can be globally maximized in one graph cut. We show that in many applications our simple term makes NP-hard segmentation functionals unnecessary. Our one cut algorithm effectively replaces approximate iterative optimization techniques based on block coordinate descent.</p><p>4 0.85227889 <a title="186-lda-4" href="./iccv-2013-Modifying_the_Memorability_of_Face_Photographs.html">272 iccv-2013-Modifying the Memorability of Face Photographs</a></p>
<p>Author: Aditya Khosla, Wilma A. Bainbridge, Antonio Torralba, Aude Oliva</p><p>Abstract: Contemporary life bombards us with many new images of faces every day, which poses non-trivial constraints on human memory. The vast majority of face photographs are intended to be remembered, either because of personal relevance, commercial interests or because the pictures were deliberately designed to be memorable. Can we make aportrait more memorable or more forgettable automatically? Here, we provide a method to modify the memorability of individual face photographs, while keeping the identity and other facial traits (e.g. age, attractiveness, and emotional magnitude) of the individual fixed. We show that face photographs manipulated to be more memorable (or more forgettable) are indeed more often remembered (or forgotten) in a crowd-sourcing experiment with an accuracy of 74%. Quantifying and modifying the ‘memorability ’ of a face lends itself to many useful applications in computer vision and graphics, such as mnemonic aids for learning, photo editing applications for social networks and tools for designing memorable advertisements.</p><p>5 0.81203544 <a title="186-lda-5" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>Author: Jiongxin Liu, Peter N. Belhumeur</p><p>Abstract: In this paper, we propose a novel approach for bird part localization, targeting fine-grained categories with wide variations in appearance due to different poses (including aspect and orientation) and subcategories. As it is challenging to represent such variations across a large set of diverse samples with tractable parametric models, we turn to individual exemplars. Specifically, we extend the exemplarbased models in [4] by enforcing pose and subcategory consistency at the parts. During training, we build posespecific detectors scoring part poses across subcategories, and subcategory-specific detectors scoring part appearance across poses. At the testing stage, likely exemplars are matched to the image, suggesting part locations whose pose and subcategory consistency are well-supported by the image cues. From these hypotheses, part configuration can be predicted with very high accuracy. Experimental results demonstrate significantperformance gainsfrom our method on an extensive dataset: CUB-200-2011 [30], for both localization and classification tasks.</p><p>6 0.80136389 <a title="186-lda-6" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>7 0.79896766 <a title="186-lda-7" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>8 0.79882312 <a title="186-lda-8" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>9 0.79716206 <a title="186-lda-9" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>10 0.79554343 <a title="186-lda-10" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>11 0.79431498 <a title="186-lda-11" href="./iccv-2013-Translating_Video_Content_to_Natural_Language_Descriptions.html">428 iccv-2013-Translating Video Content to Natural Language Descriptions</a></p>
<p>12 0.79418921 <a title="186-lda-12" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>13 0.79372847 <a title="186-lda-13" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<p>14 0.79185212 <a title="186-lda-14" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>15 0.79152811 <a title="186-lda-15" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>16 0.79134798 <a title="186-lda-16" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>17 0.79111689 <a title="186-lda-17" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>18 0.78975797 <a title="186-lda-18" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>19 0.78967369 <a title="186-lda-19" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>20 0.78963125 <a title="186-lda-20" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
