<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-80" href="#">iccv2013-80</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</h1>
<br/><p>Source: <a title="iccv-2013-80-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Hua_Collaborative_Active_Learning_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Gang Hua, Chengjiang Long, Ming Yang, Yan Gao</p><p>Abstract: Active learning is an effective way of engaging users to interactively train models for visual recognition. The vast majority of previous works, if not all of them, focused on active learning with a single human oracle. The problem of active learning with multiple oracles in a collaborative setting has not been well explored. Moreover, most of the previous works assume that the labels provided by the human oracles are noise free, which may often be violated in reality. We present a collaborative computational model for active learning with multiple human oracles. It leads to not only an ensemble kernel machine that is robust to label noises, but also a principled label quality measure to online detect irresponsible labelers. Instead of running independent active learning processes for each individual human oracle, our model captures the inherent correlations among the labelers through shared data among them. Our simulation experiments and experiments with real crowd-sourced noisy labels demonstrated the efficacy of our model.</p><p>Reference: <a title="iccv-2013-80-reference" href="../iccv2013_reference/iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The vast majority of previous works, if not all of them, focused on active learning with a single human oracle. [sent-8, score-0.386]
</p><p>2 The problem of active learning with multiple oracles in a collaborative setting has not been well explored. [sent-9, score-0.625]
</p><p>3 We present a collaborative computational model for active learning with multiple human oracles. [sent-11, score-0.597]
</p><p>4 It leads to not only an ensemble kernel machine that is robust to label noises, but also a principled label quality measure to online detect irresponsible labelers. [sent-12, score-0.734]
</p><p>5 Instead of running independent active learning processes for each individual human oracle, our model captures the inherent correlations among the labelers through shared data among them. [sent-13, score-0.996]
</p><p>6 To facilitate more efficient data labeling, some previous works have explored the use of active learning [11, 22, 12, 21, 16, 8], where the learning machine guides the labeler to label the most informative visual examples. [sent-20, score-0.895]
</p><p>7 The problem of active learning with multiple collaborative labelers has largely remained unexplored. [sent-22, score-1.176]
</p><p>8 Moreover, most previous active learning algorithms assume that labels provided by the human oracle are noise free. [sent-23, score-0.482]
</p><p>9 Hence, the problem of active learning under the condition that the human oracle may provide somewhat noisy labels is largely neglected. [sent-24, score-0.482]
</p><p>10 First of all, there is no active guidance from the system to enable the labelers to more efficiently label the data. [sent-27, score-0.954]
</p><p>11 Last but not least, several studies have shown that the label information collected from Mechanical Turk could be very noisy, either due to irresponsible behaviors from some of the labelers, or due to the inherent ambiguities of the target semantics. [sent-29, score-0.432]
</p><p>12 We propose a computational model for collaborative active learning with multiple labelers to address all the above issues, which learns an ensemble kernel machine for classification problems. [sent-30, score-1.329]
</p><p>13 In our framework, each labeler is running an individual active learning process, where the system naturally guides the labelers to label different images more efficiently towards learning the classifier. [sent-31, score-1.452]
</p><p>14 These active learning processes are not independent of one another. [sent-32, score-0.386]
</p><p>15 Our unified discriminative formulation explicitly models the consistencies among all the different active learning processes through the shared data among them. [sent-33, score-0.445]
</p><p>16 By doing so, not only can we make our active learning model to be more robust to label noises, but also we can derive principled measures to detect irresponsible labelers who are careless about their label quality earlier in the visual labeling process. [sent-34, score-1.572]
</p><p>17 There have been some previous works which attempted to use active learning to facilitate crowd-sourced human labeling [23, 1, 22] in various tasks including machine translation [1], named entity extraction and sentiment detection [14], and visual object detection [22]. [sent-35, score-0.434]
</p><p>18 [7] proposed a majority voting based con1209  fidence interval method to determine the labeling quality of each annotator, which is assumed to be stationary, and used it to select a subset of annotators to query in the active learning process. [sent-39, score-0.534]
</p><p>19 [26] proposed an incremental relabeling mechanism which exploits active learning to not only select the unlabeled data to be labeled by the crowds, but also select already labeled data samples to be relabeled until sufficient confidence is built. [sent-43, score-0.486]
</p><p>20 Yan [24, 25] presented a probabilistic multi-labeler model to learn from the crowds, where the quality of each labeler is modeled by a logistic regression function. [sent-54, score-0.404]
</p><p>21 Previous study has demonstrated that an ensemble classifier or multiple classifier system, such as those using bagging, tend to be more resilient to label noises, which partly motivated us to design such a collaborative active learning algorithm to learn an ensemble kernel machine. [sent-57, score-0.99]
</p><p>22 We apply the proposed collaborative active learning framework for training classifiers for visual recognition. [sent-58, score-0.646]
</p><p>23 Our extensive empirical evaluations clearly show that our collaborative active learning algorithm is more robust to label noises when compared with multiple independent active learners, and the learned ensemble kernel classifier can often generalize better to new data. [sent-60, score-1.216]
</p><p>24 We also show that conducting collaborative active learning naturally leads to more efficient labeling than random learning (i. [sent-61, score-0.728]
</p><p>25 When there are irresponsible labelers, our experiments also manifested that the measure we derived from our model show a very strong signal to detect these  irresponsible labelers earlier in the active learning process, which is desired as we want to ban them as early as possible. [sent-64, score-1.611]
</p><p>26 Our main contributions are hence four-fold: (1) we propose a unified and distributed discriminative learning model for collaborative active learning among a set of labelers to induce an ensemble kernel machine classifier. [sent-65, score-1.434]
</p><p>27 (2) From our proposed computational model, we are able to derive principled criterion which presents strong signal to identify irresponsible labelers online. [sent-66, score-0.966]
</p><p>28 (3) We demonstrate that through explicit modeling of the label consistency in the active learning model, our collaborative active learning process is robust to label noises and label errors from irresponsible labelers. [sent-67, score-1.647]
</p><p>29 (4) We apply the proposed collaborative active learning framework to learn classifiers for visual recognition, which produced models that can often generalize better to new data than other competing methods. [sent-68, score-0.685]
</p><p>30 4, we derive a principled measure from our computational model to detect irresponsible labelers for label quality control. [sent-74, score-1.066]
</p><p>31 Since our goal is to design a collaborative active learning strategy across all the K labelers, we further assume that each subset Di is composed of two subsets: the labeled stheta tL eia, cahn dsu tbhsee utn Dlabeled set Ui such that Di = Li ∪ Ui and Lseit ∩ L U,i a n=d t∅h. [sent-103, score-0.651]
</p><p>32 If the two labels are inconsistent, then it could either be the case that this example caused confusion among the different labelers, or some labelers are not doing a good job. [sent-146, score-0.646]
</p><p>33 In the third case, xk is only labeled by labeler j, then this label information will need to be leveraged to benefit the learning of fi(x). [sent-148, score-0.701]
</p><p>34 sWee d aaltaso s ademnpoltees t ihnat U ∀i∩, yUi be the label vectors of the set of lWabee laeldso od datean soatem thplaets ∀ ∀iin, Lyi from labeler i, and yilj be the olafb lealb veelecdto dra toaf stahmospel ssa imnp Lles in Di ∩ Lj from labeler j. [sent-174, score-0.832]
</p><p>35 It can be clearly observed that the learning of classifier fi will take into consideration of labels from other labelers with shared data. [sent-206, score-0.886]
</p><p>36 This is by design from our collaboration formulation, as labels are naturally shared across different labelers on the shared data. [sent-207, score-0.708]
</p><p>37 Collaborative active learning We design a collaborative active learning strategy based on the collaborative discriminative kernel machine proposed in Sec. [sent-222, score-1.257]
</p><p>38 Recall that for each single labeler i, the task of active learning is to select the most informative example xk ∈ Ui to be labeled by the labeler, such that the performanc∈e oUf the learning machine can be improved the most. [sent-225, score-0.985]
</p><p>39 Hence, it is natural for us to define our active learning criterion for labeler ito be Ai (xk) = |fi (xk) |. [sent-231, score-0.73]
</p><p>40 (7) At each round of the active learning step, we choose  xi∗ = argx mk∈inUiAi(xk)  (8)  for labeler ito label. [sent-232, score-0.704]
</p><p>41 Certainly, more complicated active learning criterion can be adopted at the expense of more computational cost. [sent-234, score-0.382]
</p><p>42 We also would like to emphasize that although our active learning criterion Ai (x) for labeler iis derived from tthivee c lleaassrnifiincgati cornit efurinocnti Aon fi(x) only, it does not mean that the active example selection is independent of each labeler. [sent-237, score-1.011]
</p><p>43 Therefore the dependent information from other labelers have been carried over into the active selection criterion. [sent-241, score-0.86]
</p><p>44 Hence the active sample selection processes of all the K labelers are indeed coupled with one another in our formulation. [sent-243, score-0.89]
</p><p>45 Each time a new image or several new images are labeled by the labelers, the fi(x) for each specific labeler ineeds to be updated. [sent-244, score-0.402]
</p><p>46 Labeling quality control Most previous collaborative tagging systems such as Amazon Mechanical Turk can only rely on post check of label consistency to filter out noisy labels. [sent-248, score-0.393]
</p><p>47 1, when the labels from two labelers i and j on an example xk are conflicting with each other, our joint formulation will encourage the classifier fi(xk) and fj (xk) to have low confidence predictions on xk. [sent-253, score-0.912]
</p><p>48 Although the Q score of the other labelers will also be degraded by labeler i’s irresponsible behavior, they will be degraded less than the Q score of labeler i. [sent-259, score-1.613]
</p><p>49 Nevertheless, we are still assuming that the majority of the labelers will behave honestly–as is the case in real-world. [sent-260, score-0.609]
</p><p>50 The number of images per category for these 10 categories used for collaborative active learning ranges from 2125 to 3047. [sent-269, score-0.652]
</p><p>51 We hold out 2000 of face images which had all 5 copies of labels in consensus for testing purpose and the rest of the face images with different percentage of label inconsistency are used for collaborative active learning. [sent-289, score-0.759]
</p><p>52 We conduct experiments on these datasets to measure how our proposed method and other competing methods are performing with the progress of the active learning process. [sent-294, score-0.395]
</p><p>53 Experiments with synthetic label noise Efficacy of collaborative active learning. [sent-304, score-0.642]
</p><p>54 We distributed the training data evenly to 20 labelers to ensure that roughly 1000 images are allocated to each labeler. [sent-307, score-0.579]
</p><p>55 We run simulation experiments with the proposed collaborative active learning algorithm and compare it with five baseline algorithms. [sent-308, score-0.65]
</p><p>56 The second baseline algorithm is to run multiple independent active learning process with the proposed kernel  ltmha becemlheinrdledolisnestSfeurencm. [sent-312, score-0.423]
</p><p>57 In addition, we also run multiple independent active learning SVM and multiple independent random learning SVM, respectively, which is similar to the previous two baseline algorithms using hedge loss instead of logistic regression loss. [sent-321, score-0.527]
</p><p>58 For notation simplification, we denote our proposed collaborative active learning algorithm to be CAL. [sent-322, score-0.597]
</p><p>59 The results on the active learning pool and the hold-out test dataset are presented in Fig. [sent-326, score-0.441]
</p><p>60 1a, the vertical axis represents the mean average precision (mAP) (the mean is taken over all the runs of 10 categories from all labelers) of the learned classifiers over the examples in the active learning pool. [sent-330, score-0.436]
</p><p>61 Therefore, at each step, each labeler is providing label for one data sample. [sent-333, score-0.442]
</p><p>62 We only show the average results across all labelers over all image categories due to the space limit. [sent-339, score-0.61]
</p><p>63 By ensuring the consistencies among the classification models through the shared data, our collaborative discriminative learning paradigm allows the label information to be shared among labelers and hence better utilize them to train better classifiers. [sent-343, score-1.051]
</p><p>64 Specifically, in our collaborative formulation, if labeler A labeled a data sample shared by labeler B. [sent-347, score-1.022]
</p><p>65 That label is immediately factored into the learning of the classifier of labeler B even if B has not labeled it yet. [sent-348, score-0.61]
</p><p>66 For MIAL, there is no cross labeler cost so the label is not shared. [sent-349, score-0.442]
</p><p>67 We also want to point out that the ensemble classifier produced by CAL always achieved better accuracy in the held-out testing dataset, which implies that our collaborative formulation can help learn classifiers that can generalize well. [sent-350, score-0.482]
</p><p>68 1213  (a) Active learning pool  (a) Active learning pool  (b) Hold-out testing dataset Figure 1: Recognition performance  (b) Hold-out testing dataset with Figure 2: Recognition performance on the ac-  nreMpaoigvcs0. [sent-352, score-0.39]
</p><p>69 The vertical bar tive learning pool with different levels of la- crowd-sourced labels on five ImageNet catindicates the standard deviation of mAP val- bel noises, and the hold-out testing dataset, egories in active learning pool and hold-out  ues on the curve. [sent-357, score-0.761]
</p><p>70 We simulate the case that the labelers have a chance to generate noisy labels, ranging from 5%, 15%, to 25%, meaning that the labeler has such a probability to label the image incorrectly. [sent-361, score-1.047]
</p><p>71 We run the experiments with different level of label noises for all 20 labelers on all the 10 image classes. [sent-362, score-0.747]
</p><p>72 However, at all noise levels, our proposed CAL algorithm always achieves better mAP scores on both the active learning pool (Fig. [sent-368, score-0.467]
</p><p>73 4a, we simulate the case that there are 5 irresponsible labelers and the rest are responsible latesting dataset. [sent-381, score-0.962]
</p><p>74 It is clearly observed that the label quality of the irresponsible labelers are consistently and significantly lower than those of the responsible labelers across the collaborative active learning process. [sent-383, score-2.264]
</p><p>75 This is a very strong and consistent signal that enables us to capture irresponsible labelers from the very beginning of the collaborative active labeling process. [sent-385, score-1.495]
</p><p>76 This validated our hypothesis that the average of signed classification scores on the labeled images for each labeler  (as defined in Eq. [sent-386, score-0.402]
</p><p>77 9) naturally serves as a label quality measure to detect irresponsible labelers. [sent-387, score-0.464]
</p><p>78 4b and 4c, we compare the AP scores of our CAL algorithm with those of SVM-MIAL, SVM-MIRL, MIAL, and MIRL under the presence of five irresponsible labelers on the active learning pool and hold-out testing dataset, respectively. [sent-389, score-1.42]
</p><p>79 The AP of MIAL and MIRL on the active learning pool actually dropped when more labels are added due to the bad performance of the classifiers from those 5 irresponsible labelers. [sent-391, score-0.895]
</p><p>80 However, the SVMMIAL and SVM-MIRL do not suffer from this in the active learning pool, suggesting that the hedge loss is more robust. [sent-392, score-0.402]
</p><p>81 We have also run extensive experiments on all 10 image categories with different number irresponsible labelers (upto half), and the observations are consistent with what we show in Fig. [sent-394, score-0.948]
</p><p>82 67849023405670SV8M- 0CIRA L 901 Number  (a) Label quality with 5% label noise for responsible labelers and 5 irresponsible labelers. [sent-397, score-1.114]
</p><p>83 Figure 4:  The label  quality  of label s  (b) AP with 5% label noise and five irresponsible labelers on active learning pool. [sent-398, score-1.64]
</p><p>84 and recognition accuracy  of responsible  (c) AP with 5% label noise and five irresponsible labelers on hold-out testing dataset. [sent-399, score-1.144]
</p><p>85 In addition to comparing with the original 5 baseline algorithms, we add two new baseline algorithms and also compare with two version of the active learning algorithms presented in Yan et al. [sent-405, score-0.408]
</p><p>86 The two new baseline algorithms adopt an online majority voting strategy in the active learning process to induce a single kernel classifier using either the logistic regression loss (as in our formulation) or hedge loss (as in SVM). [sent-407, score-0.592]
</p><p>87 Specifically, at each  round of the active learning step, each data sample is labeled by 7 or 5 labelers, and we utilize the majority voted label as the label for this data sample and re-train the classifier. [sent-408, score-0.628]
</p><p>88 We want to clarify that for MVAL and SVM-MVAL, the active learning pool contains all images, so it is a larger pool than the pool of examples handled by each individual labeler in our CAL formulation. [sent-412, score-0.959]
</p><p>89 We followed the same data split for active learning and hold-out testing as in the experiments with synthetic noisy label. [sent-415, score-0.417]
</p><p>90 Each image is assigned to m = 7 labelers as we have seven copies of crowdsourced labels per image. [sent-416, score-0.768]
</p><p>91 3 presents the mAP curves on the active learning pool and the hold-out testing dataset. [sent-419, score-0.476]
</p><p>92 Our proposed CAL outperformed all the other 9 competing algorithms in both the active learning pool and the hold-out testing datasets. [sent-420, score-0.515]
</p><p>93 The two methods proposed  Number  of  labe l s  (a) Active learning pool  Number  of  labe l s  (b) Hold-out testing dataset Figure 5: Recognition performance on real crowd-sourced labels on a face gender image dataset. [sent-423, score-0.395]
</p><p>94 Each data sample is assigned to m = 5 labelers to label and around thirty labelers in total are used. [sent-431, score-1.252]
</p><p>95 Again, it is clear that our proposed CAL algorithm outperformed all other 9 competing methods in both the active learning pool and the holdout testing datasets. [sent-432, score-0.515]
</p><p>96 The results also demonstrated the efficacy of our collaborative model formulation, as the second 1215  best algorithm is CRL while the other algorithms are either running multiple independent processes for model learning or just inducing a single classifier using active learning. [sent-433, score-0.704]
</p><p>97 Again, the AP on the active learning pool is the mean across all labelers, while the AP on the hold-out testing dataset is computed using the resulting ensemble kernel classifier. [sent-434, score-0.607]
</p><p>98 As verified by our experiments, our approach enables more efficient model learning from multiple labelers, is robust to label noise and irresponsible labelers, and can readily detect irresponsible labelers online. [sent-438, score-1.45]
</p><p>99 Ralf: A reinforced active learning formulation for object class recognition. [sent-509, score-0.384]
</p><p>100 Incremental relabeling for active learning with noisy crowdsourced annotations. [sent-651, score-0.485]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('labelers', 0.579), ('labeler', 0.348), ('irresponsible', 0.338), ('active', 0.281), ('collaborative', 0.241), ('xk', 0.13), ('mial', 0.113), ('fi', 0.095), ('label', 0.094), ('cal', 0.092), ('ensemble', 0.09), ('meerkat', 0.088), ('pool', 0.085), ('crowdsourced', 0.081), ('xnew', 0.075), ('learning', 0.075), ('noises', 0.074), ('labels', 0.067), ('mechanical', 0.066), ('imagenet', 0.066), ('amazon', 0.06), ('annotators', 0.06), ('crl', 0.057), ('lilj', 0.057), ('labeling', 0.056), ('conference', 0.055), ('labeled', 0.054), ('xj', 0.052), ('ki', 0.05), ('di', 0.049), ('classifiers', 0.049), ('june', 0.047), ('dekel', 0.046), ('hedge', 0.046), ('kil', 0.046), ('ui', 0.046), ('responsible', 0.045), ('gender', 0.045), ('labe', 0.044), ('mirl', 0.042), ('mval', 0.042), ('yilj', 0.042), ('copies', 0.041), ('kernel', 0.041), ('competing', 0.039), ('classifier', 0.039), ('fj', 0.039), ('turk', 0.038), ('efficacy', 0.038), ('international', 0.038), ('donmez', 0.038), ('raykar', 0.035), ('testing', 0.035), ('crowdsourcing', 0.034), ('yan', 0.033), ('oracle', 0.033), ('quality', 0.032), ('bar', 0.031), ('ap', 0.031), ('pages', 0.031), ('categories', 0.031), ('shared', 0.031), ('processes', 0.03), ('conflicting', 0.03), ('induce', 0.03), ('majority', 0.03), ('kilj', 0.028), ('monarch', 0.028), ('nil', 0.028), ('oracles', 0.028), ('pilj', 0.028), ('ridgeback', 0.028), ('svmmial', 0.028), ('vizsla', 0.028), ('formulation', 0.028), ('proceedings', 0.027), ('five', 0.027), ('annotator', 0.026), ('noisy', 0.026), ('noise', 0.026), ('baseline', 0.026), ('workers', 0.026), ('criterion', 0.026), ('fung', 0.025), ('rosales', 0.025), ('yorkshire', 0.025), ('resources', 0.025), ('category', 0.024), ('logistic', 0.024), ('malicious', 0.023), ('setter', 0.023), ('stevens', 0.023), ('terrier', 0.023), ('butterfly', 0.023), ('principled', 0.023), ('yk', 0.023), ('machine', 0.022), ('relabeling', 0.022), ('pointer', 0.022), ('annual', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999785 <a title="80-tfidf-1" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>Author: Gang Hua, Chengjiang Long, Ming Yang, Yan Gao</p><p>Abstract: Active learning is an effective way of engaging users to interactively train models for visual recognition. The vast majority of previous works, if not all of them, focused on active learning with a single human oracle. The problem of active learning with multiple oracles in a collaborative setting has not been well explored. Moreover, most of the previous works assume that the labels provided by the human oracles are noise free, which may often be violated in reality. We present a collaborative computational model for active learning with multiple human oracles. It leads to not only an ensemble kernel machine that is robust to label noises, but also a principled label quality measure to online detect irresponsible labelers. Instead of running independent active learning processes for each individual human oracle, our model captures the inherent correlations among the labelers through shared data among them. Our simulation experiments and experiments with real crowd-sourced noisy labels demonstrated the efficacy of our model.</p><p>2 0.73101473 <a title="80-tfidf-2" href="./iccv-2013-Active_Visual_Recognition_with_Expertise_Estimation_in_Crowdsourcing.html">43 iccv-2013-Active Visual Recognition with Expertise Estimation in Crowdsourcing</a></p>
<p>Author: Chengjiang Long, Gang Hua, Ashish Kapoor</p><p>Abstract: We present a noise resilient probabilistic model for active learning of a Gaussian process classifier from crowds, i.e., a set of noisy labelers. It explicitly models both the overall label noises and the expertise level of each individual labeler in two levels of flip models. Expectation propagation is adopted for efficient approximate Bayesian inference of our probabilistic model for classification, based on which, a generalized EM algorithm is derived to estimate both the global label noise and the expertise of each individual labeler. The probabilistic nature of our model immediately allows the adoption of the prediction entropy and estimated expertise for active selection of data sample to be labeled, and active selection of high quality labelers to label the data, respectively. We apply the proposed model for three visual recognition tasks, i.e, object category recognition, gender recognition, and multi-modal activity recognition, on three datasets with real crowd-sourced labels from Amazon Mechanical Turk. The experiments clearly demonstrated the efficacy of the proposed model.</p><p>3 0.17361559 <a title="80-tfidf-3" href="./iccv-2013-A_Convex_Optimization_Framework_for_Active_Learning.html">6 iccv-2013-A Convex Optimization Framework for Active Learning</a></p>
<p>Author: Ehsan Elhamifar, Guillermo Sapiro, Allen Yang, S. Shankar Sasrty</p><p>Abstract: In many image/video/web classification problems, we have access to a large number of unlabeled samples. However, it is typically expensive and time consuming to obtain labels for the samples. Active learning is the problem of progressively selecting and annotating the most informative unlabeled samples, in order to obtain a high classification performance. Most existing active learning algorithms select only one sample at a time prior to retraining the classifier. Hence, they are computationally expensive and cannot take advantage of parallel labeling systems such as Mechanical Turk. On the other hand, algorithms that allow the selection of multiple samples prior to retraining the classifier, may select samples that have significant information overlap or they involve solving a non-convex optimization. More importantly, the majority of active learning algorithms are developed for a certain classifier type such as SVM. In this paper, we develop an efficient active learning framework based on convex programming, which can select multiple samples at a time for annotation. Unlike the state of the art, our algorithm can be used in conjunction with any type of classifiers, including those of the fam- ily of the recently proposed Sparse Representation-based Classification (SRC). We use the two principles of classifier uncertainty and sample diversity in order to guide the optimization program towards selecting the most informative unlabeled samples, which have the least information overlap. Our method can incorporate the data distribution in the selection process by using the appropriate dissimilarity between pairs of samples. We show the effectiveness of our framework in person detection, scene categorization and face recognition on real-world datasets.</p><p>4 0.12837604 <a title="80-tfidf-4" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>Author: Sunil Bandla, Kristen Grauman</p><p>Abstract: Collecting and annotating videos of realistic human actions is tedious, yet critical for training action recognition systems. We propose a method to actively request the most useful video annotations among a large set of unlabeled videos. Predicting the utility of annotating unlabeled video is not trivial, since any given clip may contain multiple actions of interest, and it need not be trimmed to temporal regions of interest. To deal with this problem, we propose a detection-based active learner to train action category models. We develop a voting-based framework to localize likely intervals of interest in an unlabeled clip, and use them to estimate the total reduction in uncertainty that annotating that clip would yield. On three datasets, we show our approach can learn accurate action detectors more efficiently than alternative active learning strategies that fail to accommodate the “untrimmed” nature of real video data.</p><p>5 0.073725209 <a title="80-tfidf-5" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>Author: Qinxun Bai, Zheng Wu, Stan Sclaroff, Margrit Betke, Camille Monnier</p><p>Abstract: We propose a randomized ensemble algorithm to model the time-varying appearance of an object for visual tracking. In contrast with previous online methods for updating classifier ensembles in tracking-by-detection, the weight vector that combines weak classifiers is treated as a random variable and the posterior distribution for the weight vector is estimated in a Bayesian manner. In essence, the weight vector is treated as a distribution that reflects the confidence among the weak classifiers used to construct and adapt the classifier ensemble. The resulting formulation models the time-varying discriminative ability among weak classifiers so that the ensembled strong classifier can adapt to the varying appearance, backgrounds, and occlusions. The formulation is tested in a tracking-by-detection implementation. Experiments on 28 challenging benchmark videos demonstrate that the proposed method can achieve results comparable to and often better than those of stateof-the-art approaches.</p><p>6 0.069826208 <a title="80-tfidf-6" href="./iccv-2013-Active_MAP_Inference_in_CRFs_for_Efficient_Semantic_Segmentation.html">42 iccv-2013-Active MAP Inference in CRFs for Efficient Semantic Segmentation</a></p>
<p>7 0.06923268 <a title="80-tfidf-7" href="./iccv-2013-Ensemble_Projection_for_Semi-supervised_Image_Classification.html">142 iccv-2013-Ensemble Projection for Semi-supervised Image Classification</a></p>
<p>8 0.068205304 <a title="80-tfidf-8" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>9 0.059881158 <a title="80-tfidf-9" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>10 0.058895268 <a title="80-tfidf-10" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>11 0.05726869 <a title="80-tfidf-11" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>12 0.056595307 <a title="80-tfidf-12" href="./iccv-2013-Enhanced_Continuous_Tabu_Search_for_Parameter_Estimation_in_Multiview_Geometry.html">141 iccv-2013-Enhanced Continuous Tabu Search for Parameter Estimation in Multiview Geometry</a></p>
<p>13 0.05438716 <a title="80-tfidf-13" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>14 0.054126184 <a title="80-tfidf-14" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>15 0.052346408 <a title="80-tfidf-15" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>16 0.048806585 <a title="80-tfidf-16" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>17 0.047772203 <a title="80-tfidf-17" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>18 0.047434151 <a title="80-tfidf-18" href="./iccv-2013-Multi-stage_Contextual_Deep_Learning_for_Pedestrian_Detection.html">279 iccv-2013-Multi-stage Contextual Deep Learning for Pedestrian Detection</a></p>
<p>19 0.04736327 <a title="80-tfidf-19" href="./iccv-2013-Handling_Occlusions_with_Franken-Classifiers.html">190 iccv-2013-Handling Occlusions with Franken-Classifiers</a></p>
<p>20 0.047056515 <a title="80-tfidf-20" href="./iccv-2013-From_Large_Scale_Image_Categorization_to_Entry-Level_Categories.html">176 iccv-2013-From Large Scale Image Categorization to Entry-Level Categories</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.139), (1, 0.071), (2, -0.032), (3, -0.042), (4, 0.012), (5, -0.01), (6, -0.004), (7, 0.028), (8, 0.018), (9, -0.058), (10, -0.027), (11, -0.049), (12, -0.024), (13, -0.054), (14, 0.098), (15, -0.054), (16, -0.093), (17, -0.045), (18, -0.078), (19, -0.007), (20, -0.062), (21, -0.072), (22, -0.225), (23, 0.087), (24, 0.006), (25, -0.022), (26, 0.297), (27, 0.204), (28, 0.218), (29, -0.12), (30, -0.22), (31, -0.037), (32, 0.279), (33, -0.309), (34, -0.018), (35, 0.003), (36, 0.145), (37, -0.148), (38, 0.004), (39, -0.002), (40, -0.099), (41, 0.016), (42, 0.118), (43, -0.063), (44, -0.183), (45, 0.213), (46, -0.018), (47, 0.033), (48, 0.05), (49, -0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94495988 <a title="80-lsi-1" href="./iccv-2013-Active_Visual_Recognition_with_Expertise_Estimation_in_Crowdsourcing.html">43 iccv-2013-Active Visual Recognition with Expertise Estimation in Crowdsourcing</a></p>
<p>Author: Chengjiang Long, Gang Hua, Ashish Kapoor</p><p>Abstract: We present a noise resilient probabilistic model for active learning of a Gaussian process classifier from crowds, i.e., a set of noisy labelers. It explicitly models both the overall label noises and the expertise level of each individual labeler in two levels of flip models. Expectation propagation is adopted for efficient approximate Bayesian inference of our probabilistic model for classification, based on which, a generalized EM algorithm is derived to estimate both the global label noise and the expertise of each individual labeler. The probabilistic nature of our model immediately allows the adoption of the prediction entropy and estimated expertise for active selection of data sample to be labeled, and active selection of high quality labelers to label the data, respectively. We apply the proposed model for three visual recognition tasks, i.e, object category recognition, gender recognition, and multi-modal activity recognition, on three datasets with real crowd-sourced labels from Amazon Mechanical Turk. The experiments clearly demonstrated the efficacy of the proposed model.</p><p>same-paper 2 0.93517399 <a title="80-lsi-2" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>Author: Gang Hua, Chengjiang Long, Ming Yang, Yan Gao</p><p>Abstract: Active learning is an effective way of engaging users to interactively train models for visual recognition. The vast majority of previous works, if not all of them, focused on active learning with a single human oracle. The problem of active learning with multiple oracles in a collaborative setting has not been well explored. Moreover, most of the previous works assume that the labels provided by the human oracles are noise free, which may often be violated in reality. We present a collaborative computational model for active learning with multiple human oracles. It leads to not only an ensemble kernel machine that is robust to label noises, but also a principled label quality measure to online detect irresponsible labelers. Instead of running independent active learning processes for each individual human oracle, our model captures the inherent correlations among the labelers through shared data among them. Our simulation experiments and experiments with real crowd-sourced noisy labels demonstrated the efficacy of our model.</p><p>3 0.685175 <a title="80-lsi-3" href="./iccv-2013-A_Convex_Optimization_Framework_for_Active_Learning.html">6 iccv-2013-A Convex Optimization Framework for Active Learning</a></p>
<p>Author: Ehsan Elhamifar, Guillermo Sapiro, Allen Yang, S. Shankar Sasrty</p><p>Abstract: In many image/video/web classification problems, we have access to a large number of unlabeled samples. However, it is typically expensive and time consuming to obtain labels for the samples. Active learning is the problem of progressively selecting and annotating the most informative unlabeled samples, in order to obtain a high classification performance. Most existing active learning algorithms select only one sample at a time prior to retraining the classifier. Hence, they are computationally expensive and cannot take advantage of parallel labeling systems such as Mechanical Turk. On the other hand, algorithms that allow the selection of multiple samples prior to retraining the classifier, may select samples that have significant information overlap or they involve solving a non-convex optimization. More importantly, the majority of active learning algorithms are developed for a certain classifier type such as SVM. In this paper, we develop an efficient active learning framework based on convex programming, which can select multiple samples at a time for annotation. Unlike the state of the art, our algorithm can be used in conjunction with any type of classifiers, including those of the fam- ily of the recently proposed Sparse Representation-based Classification (SRC). We use the two principles of classifier uncertainty and sample diversity in order to guide the optimization program towards selecting the most informative unlabeled samples, which have the least information overlap. Our method can incorporate the data distribution in the selection process by using the appropriate dissimilarity between pairs of samples. We show the effectiveness of our framework in person detection, scene categorization and face recognition on real-world datasets.</p><p>4 0.4235965 <a title="80-lsi-4" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>Author: Sunil Bandla, Kristen Grauman</p><p>Abstract: Collecting and annotating videos of realistic human actions is tedious, yet critical for training action recognition systems. We propose a method to actively request the most useful video annotations among a large set of unlabeled videos. Predicting the utility of annotating unlabeled video is not trivial, since any given clip may contain multiple actions of interest, and it need not be trimmed to temporal regions of interest. To deal with this problem, we propose a detection-based active learner to train action category models. We develop a voting-based framework to localize likely intervals of interest in an unlabeled clip, and use them to estimate the total reduction in uncertainty that annotating that clip would yield. On three datasets, we show our approach can learn accurate action detectors more efficiently than alternative active learning strategies that fail to accommodate the “untrimmed” nature of real video data.</p><p>5 0.37476882 <a title="80-lsi-5" href="./iccv-2013-Active_MAP_Inference_in_CRFs_for_Efficient_Semantic_Segmentation.html">42 iccv-2013-Active MAP Inference in CRFs for Efficient Semantic Segmentation</a></p>
<p>Author: Gemma Roig, Xavier Boix, Roderick De_Nijs, Sebastian Ramos, Koljia Kuhnlenz, Luc Van_Gool</p><p>Abstract: Most MAP inference algorithms for CRFs optimize an energy function knowing all the potentials. In this paper, we focus on CRFs where the computational cost of instantiating the potentials is orders of magnitude higher than MAP inference. This is often the case in semantic image segmentation, where most potentials are instantiated by slow classifiers fed with costly features. We introduce Active MAP inference 1) to on-the-fly select a subset of potentials to be instantiated in the energy function, leaving the rest of the parameters of the potentials unknown, and 2) to estimate the MAP labeling from such incomplete energy function. Results for semantic segmentation benchmarks, namely PASCAL VOC 2010 [5] and MSRC-21 [19], show that Active MAP inference achieves similar levels of accuracy but with major efficiency gains.</p><p>6 0.35740075 <a title="80-lsi-6" href="./iccv-2013-Learning_CRFs_for_Image_Parsing_with_Adaptive_Subgradient_Descent.html">234 iccv-2013-Learning CRFs for Image Parsing with Adaptive Subgradient Descent</a></p>
<p>7 0.3421163 <a title="80-lsi-7" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>8 0.31654692 <a title="80-lsi-8" href="./iccv-2013-From_Semi-supervised_to_Transfer_Counting_of_Crowds.html">178 iccv-2013-From Semi-supervised to Transfer Counting of Crowds</a></p>
<p>9 0.28785393 <a title="80-lsi-9" href="./iccv-2013-Drosophila_Embryo_Stage_Annotation_Using_Label_Propagation.html">125 iccv-2013-Drosophila Embryo Stage Annotation Using Label Propagation</a></p>
<p>10 0.28772071 <a title="80-lsi-10" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>11 0.2868633 <a title="80-lsi-11" href="./iccv-2013-Ensemble_Projection_for_Semi-supervised_Image_Classification.html">142 iccv-2013-Ensemble Projection for Semi-supervised Image Classification</a></p>
<p>12 0.28583717 <a title="80-lsi-12" href="./iccv-2013-Sequential_Bayesian_Model_Update_under_Structured_Scene_Prior_for_Semantic_Road_Scenes_Labeling.html">386 iccv-2013-Sequential Bayesian Model Update under Structured Scene Prior for Semantic Road Scenes Labeling</a></p>
<p>13 0.27904668 <a title="80-lsi-13" href="./iccv-2013-Enhanced_Continuous_Tabu_Search_for_Parameter_Estimation_in_Multiview_Geometry.html">141 iccv-2013-Enhanced Continuous Tabu Search for Parameter Estimation in Multiview Geometry</a></p>
<p>14 0.26740375 <a title="80-lsi-14" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>15 0.2531012 <a title="80-lsi-15" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>16 0.25131229 <a title="80-lsi-16" href="./iccv-2013-Image_Segmentation_with_Cascaded_Hierarchical_Models_and_Logistic_Disjunctive_Normal_Networks.html">211 iccv-2013-Image Segmentation with Cascaded Hierarchical Models and Logistic Disjunctive Normal Networks</a></p>
<p>17 0.23577635 <a title="80-lsi-17" href="./iccv-2013-CoDeL%3A_A_Human_Co-detection_and_Labeling_Framework.html">75 iccv-2013-CoDeL: A Human Co-detection and Labeling Framework</a></p>
<p>18 0.22218588 <a title="80-lsi-18" href="./iccv-2013-Potts_Model%2C_Parametric_Maxflow_and_K-Submodular_Functions.html">324 iccv-2013-Potts Model, Parametric Maxflow and K-Submodular Functions</a></p>
<p>19 0.22205859 <a title="80-lsi-19" href="./iccv-2013-NEIL%3A_Extracting_Visual_Knowledge_from_Web_Data.html">285 iccv-2013-NEIL: Extracting Visual Knowledge from Web Data</a></p>
<p>20 0.21820043 <a title="80-lsi-20" href="./iccv-2013-From_Point_to_Set%3A_Extend_the_Learning_of_Distance_Metrics.html">177 iccv-2013-From Point to Set: Extend the Learning of Distance Metrics</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.072), (7, 0.018), (12, 0.018), (26, 0.066), (30, 0.126), (31, 0.052), (33, 0.16), (34, 0.021), (42, 0.093), (48, 0.014), (64, 0.032), (73, 0.038), (77, 0.024), (78, 0.016), (89, 0.125), (98, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7873261 <a title="80-lda-1" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>Author: Gang Hua, Chengjiang Long, Ming Yang, Yan Gao</p><p>Abstract: Active learning is an effective way of engaging users to interactively train models for visual recognition. The vast majority of previous works, if not all of them, focused on active learning with a single human oracle. The problem of active learning with multiple oracles in a collaborative setting has not been well explored. Moreover, most of the previous works assume that the labels provided by the human oracles are noise free, which may often be violated in reality. We present a collaborative computational model for active learning with multiple human oracles. It leads to not only an ensemble kernel machine that is robust to label noises, but also a principled label quality measure to online detect irresponsible labelers. Instead of running independent active learning processes for each individual human oracle, our model captures the inherent correlations among the labelers through shared data among them. Our simulation experiments and experiments with real crowd-sourced noisy labels demonstrated the efficacy of our model.</p><p>2 0.75779402 <a title="80-lda-2" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>Author: Ping Luo, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: Recent works have shown that facial attributes are useful in a number of applications such as face recognition and retrieval. However, estimating attributes in images with large variations remains a big challenge. This challenge is addressed in this paper. Unlike existing methods that assume the independence of attributes during their estimation, our approach captures the interdependencies of local regions for each attribute, as well as the high-order correlations between different attributes, which makes it more robust to occlusions and misdetection of face regions. First, we have modeled region interdependencies with a discriminative decision tree, where each node consists of a detector and a classifier trained on a local region. The detector allows us to locate the region, while the classifier determines the presence or absence of an attribute. Second, correlations of attributes and attribute predictors are modeled by organizing all of the decision trees into a large sum-product network (SPN), which is learned by the EM algorithm and yields the most probable explanation (MPE) of the facial attributes in terms of the region ’s localization and classification. Experimental results on a large data set with 22, 400 images show the effectiveness of the proposed approach.</p><p>3 0.7499463 <a title="80-lda-3" href="./iccv-2013-Active_Visual_Recognition_with_Expertise_Estimation_in_Crowdsourcing.html">43 iccv-2013-Active Visual Recognition with Expertise Estimation in Crowdsourcing</a></p>
<p>Author: Chengjiang Long, Gang Hua, Ashish Kapoor</p><p>Abstract: We present a noise resilient probabilistic model for active learning of a Gaussian process classifier from crowds, i.e., a set of noisy labelers. It explicitly models both the overall label noises and the expertise level of each individual labeler in two levels of flip models. Expectation propagation is adopted for efficient approximate Bayesian inference of our probabilistic model for classification, based on which, a generalized EM algorithm is derived to estimate both the global label noise and the expertise of each individual labeler. The probabilistic nature of our model immediately allows the adoption of the prediction entropy and estimated expertise for active selection of data sample to be labeled, and active selection of high quality labelers to label the data, respectively. We apply the proposed model for three visual recognition tasks, i.e, object category recognition, gender recognition, and multi-modal activity recognition, on three datasets with real crowd-sourced labels from Amazon Mechanical Turk. The experiments clearly demonstrated the efficacy of the proposed model.</p><p>4 0.72064966 <a title="80-lda-4" href="./iccv-2013-Query-Adaptive_Asymmetrical_Dissimilarities_for_Visual_Object_Retrieval.html">334 iccv-2013-Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval</a></p>
<p>Author: Cai-Zhi Zhu, Hervé Jégou, Shin'Ichi Satoh</p><p>Abstract: Visual object retrieval aims at retrieving, from a collection of images, all those in which a given query object appears. It is inherently asymmetric: the query object is mostly included in the database image, while the converse is not necessarily true. However, existing approaches mostly compare the images with symmetrical measures, without considering the different roles of query and database. This paper first measure the extent of asymmetry on large-scale public datasets reflecting this task. Considering the standard bag-of-words representation, we then propose new asymmetrical dissimilarities accounting for the different inlier ratios associated with query and database images. These asymmetrical measures depend on the query, yet they are compatible with an inverted file structure, without noticeably impacting search efficiency. Our experiments show the benefit of our approach, and show that the visual object retrieval task is better treated asymmetrically, in the spirit of state-of-the-art text retrieval.</p><p>5 0.7012791 <a title="80-lda-5" href="./iccv-2013-A_Convex_Optimization_Framework_for_Active_Learning.html">6 iccv-2013-A Convex Optimization Framework for Active Learning</a></p>
<p>Author: Ehsan Elhamifar, Guillermo Sapiro, Allen Yang, S. Shankar Sasrty</p><p>Abstract: In many image/video/web classification problems, we have access to a large number of unlabeled samples. However, it is typically expensive and time consuming to obtain labels for the samples. Active learning is the problem of progressively selecting and annotating the most informative unlabeled samples, in order to obtain a high classification performance. Most existing active learning algorithms select only one sample at a time prior to retraining the classifier. Hence, they are computationally expensive and cannot take advantage of parallel labeling systems such as Mechanical Turk. On the other hand, algorithms that allow the selection of multiple samples prior to retraining the classifier, may select samples that have significant information overlap or they involve solving a non-convex optimization. More importantly, the majority of active learning algorithms are developed for a certain classifier type such as SVM. In this paper, we develop an efficient active learning framework based on convex programming, which can select multiple samples at a time for annotation. Unlike the state of the art, our algorithm can be used in conjunction with any type of classifiers, including those of the fam- ily of the recently proposed Sparse Representation-based Classification (SRC). We use the two principles of classifier uncertainty and sample diversity in order to guide the optimization program towards selecting the most informative unlabeled samples, which have the least information overlap. Our method can incorporate the data distribution in the selection process by using the appropriate dissimilarity between pairs of samples. We show the effectiveness of our framework in person detection, scene categorization and face recognition on real-world datasets.</p><p>6 0.6888535 <a title="80-lda-6" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>7 0.68396944 <a title="80-lda-7" href="./iccv-2013-Discriminative_Label_Propagation_for_Multi-object_Tracking_with_Sporadic_Appearance_Features.html">120 iccv-2013-Discriminative Label Propagation for Multi-object Tracking with Sporadic Appearance Features</a></p>
<p>8 0.68114805 <a title="80-lda-8" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>9 0.67710066 <a title="80-lda-9" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<p>10 0.67306662 <a title="80-lda-10" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>11 0.67301023 <a title="80-lda-11" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>12 0.6726737 <a title="80-lda-12" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>13 0.67231786 <a title="80-lda-13" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>14 0.67215466 <a title="80-lda-14" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>15 0.67093521 <a title="80-lda-15" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>16 0.67040241 <a title="80-lda-16" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>17 0.66865748 <a title="80-lda-17" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>18 0.66839349 <a title="80-lda-18" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>19 0.66795635 <a title="80-lda-19" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>20 0.6671294 <a title="80-lda-20" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
