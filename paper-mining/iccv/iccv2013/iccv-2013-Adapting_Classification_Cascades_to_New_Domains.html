<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>44 iccv-2013-Adapting Classification Cascades to New Domains</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-44" href="#">iccv2013-44</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>44 iccv-2013-Adapting Classification Cascades to New Domains</h1>
<br/><p>Source: <a title="iccv-2013-44-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Jain_Adapting_Classification_Cascades_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Vidit Jain, Sachin Sudhakar Farfade</p><p>Abstract: Classification cascades have been very effective for object detection. Such a cascade fails to perform well in data domains with variations in appearances that may not be captured in the training examples. This limited generalization severely restricts the domains for which they can be used effectively. A common approach to address this limitation is to train a new cascade of classifiers from scratch for each of the new domains. Building separate detectors for each of the different domains requires huge annotation and computational effort, making it not scalable to a large number of data domains. Here we present an algorithm for quickly adapting a pre-trained cascade of classifiers using a small number oflabeledpositive instancesfrom a different yet similar data domain. In our experiments with images of human babies and human-like characters from movies, we demonstrate that the adapted cascade significantly outperforms both of the original cascade and the one trained from scratch using the given training examples. –</p><p>Reference: <a title="iccv-2013-44-reference" href="../iccv2013_reference/iccv-2013-Adapting_Classification_Cascades_to_New_Domains_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Such a cascade fails to perform well in data domains with variations in appearances that may not be captured in the training examples. [sent-4, score-0.935]
</p><p>2 A common approach to address this limitation is to train a new cascade of classifiers from scratch for each of the new domains. [sent-6, score-1.014]
</p><p>3 Here we present an algorithm for quickly adapting a pre-trained cascade of classifiers using a small number oflabeledpositive instancesfrom a different yet similar data domain. [sent-8, score-0.983]
</p><p>4 In our experiments with images of human babies and human-like characters from movies, we demonstrate that the adapted cascade significantly outperforms both of the original cascade and the one trained from scratch using the given training examples. [sent-9, score-2.006]
</p><p>5 Therefore, instead of learning a single complex classifier, a cascade of classifiers with increasing complexity is often used. [sent-13, score-0.876]
</p><p>6 This cascade may employ several simple binary classifiers and accept a candidate image region as detection if and only if all of these binary classifiers accept it. [sent-14, score-1.173]
</p><p>7 Once trained, a cascade classifier is often used in different, unconstrained data domains (or acquisition settings) with variations in appearances that may not be captured in the training examples. [sent-18, score-0.989]
</p><p>8 In both of these images, a standard face detector correctly identified the adult faces but failed to detect the faces of the babies. [sent-23, score-0.356]
</p><p>9 For instance, a cascade trained on the images of human faces only from a particular age group (e. [sent-25, score-0.852]
</p><p>10 This limited generalization of the cascade classifiers severely restrict the data domains for which they can be used effectively. [sent-30, score-0.993]
</p><p>11 A common approach to address this limitation is to train a new cascade of classifiers from scratch for each of the new domains. [sent-31, score-1.014]
</p><p>12 Instead, we need an approach that can quickly adapt a pre-trained cascade to perform well on a new domain. [sent-35, score-0.732]
</p><p>13 We consider the problem of domain adaptation for cascade classifiers when the positive examples available from the target class are not sufficient to train the cascade from –  –  scratch. [sent-36, score-2.033]
</p><p>14 Furthermore, we assume that only the pre-trained cascade is available, and not the data used for training it. [sent-37, score-0.749]
</p><p>15 1 This setting of limited availability of training data in a new domain arises not only for object detection but also for several other rare-event classification problems such as medical diagnosis and intrusion detection. [sent-38, score-0.278]
</p><p>16 For some of these problems, domain adaptation and transductive learning of general classifiers have been explored, but adaptation techniques specific to cascade classifiers have not been studied. [sent-39, score-1.51]
</p><p>17 1While it is common to make a pre-trained classification cascade available, it is sometimes not feasible to retain the examples used for training it due to operational and copyright issues. [sent-40, score-0.837]
</p><p>18 105 We observe that the lack of robustness in the cascade classifiers is primarily due to their over-fitting to training examples. [sent-41, score-0.921]
</p><p>19 To address this issue, we split the trained cascade into three functional components, and devise appropriate adaptation techniques for these components. [sent-42, score-0.89]
</p><p>20 There are two main contributions in this paper: (a) a mathematical model that systematically identifies and removes the classifiers in a cascade that contribute little to detection in the new domain; and (b) an efficient generative model for an in-domain verification of the detected regions. [sent-43, score-0.974]
</p><p>21 These two models are used to adapt a pre-trained (base) classification cascade to a new domain with a few training examples. [sent-44, score-0.907]
</p><p>22 In our experiments, we consider cascade adaptation for the problem of face detection. [sent-45, score-0.951]
</p><p>23 Here the different data domains arise from the appearances diversity across age  groups, race, acquisition settings, and human-like characters in virtual environments and sci-fi or fantasy movies. [sent-46, score-0.352]
</p><p>24 , after the release of a new sci-fi movie or a popular video game) to allow for a cascade to be trained from scratch. [sent-52, score-0.832]
</p><p>25 A system that can quickly build a face detector for a new target domain from a pre-trained face detector is useful for these new domains. [sent-53, score-0.5]
</p><p>26 In this work, we consider the set up where it is feasible to obtain only a few (hundred) positive examples of the target class, which are not sufficient to train an effective cascade classifier from scratch. [sent-54, score-0.946]
</p><p>27 The image collections comprising faces of human babies and human-like characters from movies are presented in Section 5; the related improvement in detection performance are shown in Section 6. [sent-56, score-0.566]
</p><p>28 Related Work We first distinguish our work from the relevant work from the domain adaptation and transfer learning literature. [sent-58, score-0.309]
</p><p>29 Then we discuss some of the key research related to cascade classifiers and face detection. [sent-59, score-0.977]
</p><p>30 In domain adaptation, labeled data from one or multiple “source” domains is used to train models to perform well on a different yet related “target” domain. [sent-62, score-0.266]
</p><p>31 Another approach to the domain adaptation problem employs models trained on the data from the source domain to label a subset of the unlabeled data from the unlabeled target domain, and re-trains the classifier on the combined labeled data set [4]. [sent-65, score-0.575]
</p><p>32 Most of the work in domain adaptation (including the above two) suggests minimizing a convex combination of source and target empirical risk [10]. [sent-66, score-0.362]
</p><p>33 In this paper, we consider the problem of domain adaptation without an access to the original training data. [sent-69, score-0.343]
</p><p>34 Cascade classifiers are commonly used for anomaly detection [8] and one-class classification [18]. [sent-80, score-0.247]
</p><p>35 The cascade classifier by Viola and Jones [18] is arguably the most popular solution for face detection. [sent-81, score-0.859]
</p><p>36 ’s soft cascade [3] reduces the over-fitting issue by allowing the cascade to make decisions based on cumulative performance. [sent-85, score-1.408]
</p><p>37 Similar to previous cascade classifiers, their model also does not consider the generalizability of the trained classifier to other domains. [sent-89, score-0.798]
</p><p>38 [12] suggested the adaptation of a pre-trained classifier to a single image, and reported significant improvement 106  in face detection performance on the FDDB data set [11]. [sent-91, score-0.337]
</p><p>39 Their algorithm adapts a cascade classifier to a new data domain, but considers the same classification task, i. [sent-92, score-0.797]
</p><p>40 There have been other similar studies that address different aspects of cascade classifiers (e. [sent-95, score-0.876]
</p><p>41 To our knowledge, none of them focuses on our set up of adapting a cascade classifier to a different but related classification problem. [sent-98, score-0.876]
</p><p>42 Cascade adaptation A cascade of classifiers F is a classifier that is composed of m stage ec loafs cslifaisesrisf e{rfs1 F , . [sent-100, score-1.231]
</p><p>43 rFso r{ computational efficiency, a rejection cascade is typically employed in rare-class classification tasks where the input is instantaneously rejected if it is rejected by any of these m classifiers. [sent-104, score-0.881]
</p><p>44 In face detection, we are given a candidate image patch x and it is classified as a face region if and only if it is accepted by all of these stage classifiers in the cascade. [sent-105, score-0.529]
</p><p>45 Functionally, this cascade can usually be split into two phases: rejection of false positives and validation of true positives. [sent-108, score-0.892]
</p><p>46 The first phase corresponds to the early stages of the cascade that are designed to perform easy rejection and the subsequent stages of increasing complexity. [sent-109, score-1.132]
</p><p>47 In the second phase, the stage classifiers are very detailed and typically use several hundred features. [sent-112, score-0.327]
</p><p>48 These classifiers capture most of the structure in a face and can be considered similar to a descriptive model of face appearances. [sent-113, score-0.374]
</p><p>49 This interpretation of cascade classifiers is illustrated in Figure 2. [sent-114, score-0.876]
</p><p>50 Training new stage classifiers  Compared to the later stages of the cascade, the first few stages {f1, . [sent-119, score-0.593]
</p><p>51 Since these early stages are expected to eliminate only the easy-to-reject instances, they can be trained effectively from scratch even with a few training examples. [sent-124, score-0.326]
</p><p>52 To this end, we train a short cascade with very few stages using the positive examples from the target class. [sent-125, score-1.025]
</p><p>53 The stage classifiers from this new cascade will become candidate replacements to the stage classifiers in an existing (generic) cascade classifier. [sent-126, score-2.106]
</p><p>54 To maintain the computational efficiency of the original cascade, we consider the same family of classification functions to learn stage classifiers for the new cascade. [sent-127, score-0.399]
</p><p>55 Similar to Viola and Jones [18], a variant of AdaBoost learning algorithm is employed to train the individual stage classifiers from the few training examples from the target domain. [sent-128, score-0.516]
</p><p>56 Here we use Haar-like rectangular features to form the pool of weak classifiers and use the desired rates for hit rate and false alarm as the stopping criteria for the learning algorithm. [sent-130, score-0.238]
</p><p>57 Each of the stage classifiers (blue) has a binary selection variable (red) associated with it. [sent-137, score-0.362]
</p><p>58 The second step of the rejection phase is composed of an ordered set of stage classifiers {fh+1 , . [sent-139, score-0.46]
</p><p>59 Iff s any o cfl athsseisfeie stage classifiers rejects a given image patch, the patch is immediately discarded, otherwise it is evaluated by the next stage classifier. [sent-143, score-0.482]
</p><p>60 The increase in complexity of the subsequent classifiers is because the acceptable false-alarm and hit-rate for the trained classifier becomes stricter for subsequent stages. [sent-144, score-0.324]
</p><p>61 By  selecting only the stage classifiers that capture these shared structures, we can construct a new classification cascade for the target domain. [sent-147, score-1.135]
</p><p>62 To this end, we modify the pre-trained cascade (for the source domain) as follows. [sent-148, score-0.736]
</p><p>63 For each stage 107  in this cascade, we introduce a binary selection variable θ that specifies if the evaluation of this stage is useful for the target domain. [sent-149, score-0.41]
</p><p>64 Note that since we are removing some intermediate stages from the given cascade, it is possible that the subsequent, expensive stages are evaluated for more candidate windows, thereby leading to a decrease in the processing time. [sent-151, score-0.266]
</p><p>65 Our observations validate the existence of stage classifiers in the pre-trained cascade that are ineffectual for the target domain. [sent-153, score-1.096]
</p><p>66 Our modified cascade use binary variables {θ1 , . [sent-175, score-0.704]
</p><p>67 Generative validation Now we discuss our proposal for adapting the second phase of the cascade i. [sent-217, score-0.888]
</p><p>68 Since the configuration of facial features for babies is different from normal adults [15], the problem of detecting baby face images is an example of adaptation to a similar class. [sent-257, score-0.515]
</p><p>69 A collection of 764 images of babies is annotated with face regions, which is referred to as BabyFaces data set. [sent-258, score-0.278]
</p><p>70 Each image in this collection is annotated with the position and size of the faces of babies (and infants) appearing in them. [sent-262, score-0.254]
</p><p>71 For instance, when the Avengers movie was released, at least 66K queries for characters from this movie (e. [sent-266, score-0.364]
</p><p>72 Similar was the case with the Na’vi and Star Wars characters for the Avatar movie and the Star Wars 1313 video game, respectively. [sent-269, score-0.25]
</p><p>73 On the other hand, it is infeasible to employ separate detectors for individual characters due to the large number of labeled examples and the large computation time required to train these detectors. [sent-273, score-0.278]
</p><p>74 To assess the applicability of our algorithm to image search, we collected images for four different movie characters that are “human like” (see Figure 5). [sent-274, score-0.25]
</p><p>75 As a result, we have less than 800 positive examples to train a pose-invariant face detector in each of our experiments. [sent-280, score-0.267]
</p><p>76 3 A careful selection of 800 examples to 2In fact, the main characters in six of the top-10 highest-grossing hollywood movies of the year 2012 are non-human characters that have appearances with strong similarity to humans. [sent-281, score-0.503]
</p><p>77 , bootstrapping, selection of negative examples) to achieve a trained cascade from scratch that comprises the first few stages of the final cascade. [sent-295, score-1.049]
</p><p>78 Our approach for cascade adaptation is a supervised approach i. [sent-299, score-0.85]
</p><p>79 (a) Three types of nonface image regions are selected as negative examples for training new stage classifiers: regions from non-face images, regions from outside the face, and small regions inside the face. [sent-307, score-0.39]
</p><p>80 Using these labeled examples, we trained new in-domain stage classifiers as candidate replacements for the start of the frontal face detection cascade (hereafter referred to as the original cascade) available with the OpenCV distribution. [sent-318, score-1.285]
</p><p>81 Figure 6(b) shows the improvement in performance for the different number of stage classifiers replaced in the original cascade. [sent-319, score-0.36]
</p><p>82 Based on these observations, we chose to replace the first eight stages of the original cascade with the in-domain stage classifiers. [sent-320, score-1.025]
</p><p>83 The stage selection algorithm converged to recommend the rejection of 13th, 14th and 17th stage of the cascade. [sent-327, score-0.417]
</p><p>84 To obtain similar computation cost, we drop the last few stages of the cascade that are computationally equivalent to the trained kernel density estimator. [sent-340, score-0.91]
</p><p>85 For a test image region, the final output of the adapted cascade is computed as a linear combination4 of the score from the cascade and the validation score from the generative model. [sent-341, score-1.547]
</p><p>86 In all ofthe image collections, the original cascade is significantly outperformed by the adapted cascade. [sent-343, score-0.77]
</p><p>87 A cascade trained from scratch using the training examples correspond to the first few stages of the cascade (Section 3. [sent-344, score-1.783]
</p><p>88 Since these stages only serve the purpose of easy-rejection, we observe a large number (tens of thousands) of false positives for this cascade; the true positive rates are close to zero for < 5K false positives. [sent-346, score-0.315]
</p><p>89 As shown in Figure 9, the detections from the adapted cascade are expected to improve more than those from the original cascade. [sent-349, score-0.797]
</p><p>90 We also experimented with characters from animation movies such as Anton Ego from the movie Ratatouille (see Figure 10). [sent-352, score-0.341]
</p><p>91 The faces of such characters have very few edge and gradient features. [sent-353, score-0.239]
</p><p>92 So the Haar-like features employed in our cascade are not very effective for detecting these face regions. [sent-354, score-0.805]
</p><p>93 For these characters, cascade adaptation showed no improvement over the original face detector. [sent-356, score-0.984]
</p><p>94 Performing at the same false positive rate, the original cascade did not detect any of these face regions. [sent-375, score-0.948]
</p><p>95 (Col 3) Performance curves using the FDDB discrete matching score [11]: original cascade (black), original + validation (magenta), original + adaptation (blue), and original + adaptation + validation (green). [sent-376, score-1.216]
</p><p>96 Adaptive cascade (bottom row) obtains more robust detections than the original cascade (top row). [sent-392, score-1.468]
</p><p>97 Discussion We presented an approach for adapting a cascade of classifiers to perform classification in a similar domain for which only a few positive examples are available. [sent-403, score-1.206]
</p><p>98 Using this approach, we demonstrated huge gains in performance in detecting faces of human babies and human-like characters from movies. [sent-404, score-0.416]
</p><p>99 Given a few labeled examples of a target domain, this approach constructed an effective detector for this domain within a day. [sent-406, score-0.276]
</p><p>100 Online domain adaptation of a pre-trained cascade of classifiers. [sent-473, score-0.969]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cascade', 0.704), ('nmf', 0.18), ('babies', 0.177), ('classifiers', 0.172), ('characters', 0.162), ('stage', 0.155), ('adaptation', 0.146), ('stages', 0.133), ('domain', 0.119), ('domains', 0.117), ('scratch', 0.108), ('face', 0.101), ('wars', 0.089), ('movie', 0.088), ('adapting', 0.079), ('faces', 0.077), ('rejection', 0.072), ('babyfaces', 0.067), ('nmfsc', 0.067), ('yoda', 0.067), ('target', 0.065), ('generative', 0.062), ('collections', 0.061), ('phase', 0.061), ('bane', 0.059), ('humanlike', 0.059), ('snmf', 0.059), ('fk', 0.057), ('fddb', 0.055), ('classifier', 0.054), ('movies', 0.053), ('cascades', 0.052), ('transductive', 0.051), ('adults', 0.049), ('examples', 0.049), ('jain', 0.046), ('training', 0.045), ('replacements', 0.044), ('spiderman', 0.044), ('vidit', 0.044), ('basis', 0.044), ('validation', 0.044), ('transfer', 0.044), ('positive', 0.044), ('detector', 0.043), ('masked', 0.043), ('appearances', 0.042), ('baby', 0.042), ('false', 0.041), ('star', 0.04), ('trained', 0.04), ('estimator', 0.039), ('abundance', 0.039), ('avatar', 0.039), ('functionally', 0.039), ('intrusion', 0.039), ('oca', 0.039), ('offensive', 0.039), ('saberian', 0.039), ('classification', 0.039), ('animation', 0.038), ('employ', 0.037), ('bangalore', 0.036), ('yahoo', 0.036), ('fj', 0.036), ('detection', 0.036), ('game', 0.035), ('relaxation', 0.035), ('selection', 0.035), ('sparseness', 0.034), ('fh', 0.034), ('jones', 0.034), ('projection', 0.034), ('rejected', 0.033), ('density', 0.033), ('adapted', 0.033), ('adult', 0.033), ('yif', 0.033), ('original', 0.033), ('source', 0.032), ('positives', 0.031), ('age', 0.031), ('ak', 0.03), ('train', 0.03), ('negative', 0.029), ('subsequent', 0.029), ('quickly', 0.028), ('regions', 0.028), ('viola', 0.028), ('daum', 0.028), ('photos', 0.027), ('variations', 0.027), ('detections', 0.027), ('accept', 0.026), ('labs', 0.026), ('queries', 0.026), ('factorization', 0.026), ('websites', 0.026), ('detect', 0.025), ('rates', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="44-tfidf-1" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>Author: Vidit Jain, Sachin Sudhakar Farfade</p><p>Abstract: Classification cascades have been very effective for object detection. Such a cascade fails to perform well in data domains with variations in appearances that may not be captured in the training examples. This limited generalization severely restricts the domains for which they can be used effectively. A common approach to address this limitation is to train a new cascade of classifiers from scratch for each of the new domains. Building separate detectors for each of the different domains requires huge annotation and computational effort, making it not scalable to a large number of data domains. Here we present an algorithm for quickly adapting a pre-trained cascade of classifiers using a small number oflabeledpositive instancesfrom a different yet similar data domain. In our experiments with images of human babies and human-like characters from movies, we demonstrate that the adapted cascade significantly outperforms both of the original cascade and the one trained from scratch using the given training examples. –</p><p>2 0.21703985 <a title="44-tfidf-2" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<p>Author: Tianfu Wu, Song-Chun Zhu</p><p>Abstract: Many object detectors, such as AdaBoost, SVM and deformable part-based models (DPM), compute additive scoring functions at a large number of windows scanned over image pyramid, thus computational efficiency is an important consideration beside accuracy performance. In this paper, we present a framework of learning cost-sensitive decision policy which is a sequence of two-sided thresholds to execute early rejection or early acceptance based on the accumulative scores at each step. A decision policy is said to be optimal if it minimizes an empirical global risk function that sums over the loss of false negatives (FN) and false positives (FP), and the cost of computation. While the risk function is very complex due to high-order connections among the two-sided thresholds, we find its upper bound can be optimized by dynamic programming (DP) efficiently and thus say the learned policy is near-optimal. Given the loss of FN and FP and the cost in three numbers, our method can produce a policy on-the-fly for Adaboost, SVM and DPM. In experiments, we show that our decision policy outperforms state-of-the-art cascade methods significantly in terms of speed with similar accuracy performance.</p><p>3 0.17889552 <a title="44-tfidf-3" href="./iccv-2013-Multi-stage_Contextual_Deep_Learning_for_Pedestrian_Detection.html">279 iccv-2013-Multi-stage Contextual Deep Learning for Pedestrian Detection</a></p>
<p>Author: Xingyu Zeng, Wanli Ouyang, Xiaogang Wang</p><p>Abstract: Cascaded classifiers1 have been widely used in pedestrian detection and achieved great success. These classifiers are trained sequentially without joint optimization. In this paper, we propose a new deep model that can jointly train multi-stage classifiers through several stages of backpropagation. It keeps the score map output by a classifier within a local region and uses it as contextual information to support the decision at the next stage. Through a specific design of the training strategy, this deep architecture is able to simulate the cascaded classifiers by mining hard samples to train the network stage-by-stage. Each classifier handles samples at a different difficulty level. Unsupervised pre-training and specifically designed stage-wise supervised training are used to regularize the optimization problem. Both theoretical analysis and experimental results show that the training strategy helps to avoid overfitting. Experimental results on three datasets (Caltech, ETH and TUD-Brussels) show that our approach outperforms the state-of-the-art approaches.</p><p>4 0.178892 <a title="44-tfidf-4" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>Author: Fatemeh Mirrashed, Mohammad Rastegari</p><p>Abstract: We propose an unsupervised domain adaptation method that exploits intrinsic compact structures of categories across different domains using binary attributes. Our method directly optimizes for classification in the target domain. The key insight is finding attributes that are discriminative across categories and predictable across domains. We achieve a performance that significantly exceeds the state-of-the-art results on standard benchmarks. In fact, in many cases, our method reaches the same-domain performance, the upper bound, in unsupervised domain adaptation scenarios.</p><p>5 0.17293294 <a title="44-tfidf-5" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>Author: Kristina Scherbaum, James Petterson, Rogerio S. Feris, Volker Blanz, Hans-Peter Seidel</p><p>Abstract: Face detection is an important task in computer vision and often serves as the first step for a variety of applications. State-of-the-art approaches use efficient learning algorithms and train on large amounts of manually labeled imagery. Acquiring appropriate training images, however, is very time-consuming and does not guarantee that the collected training data is representative in terms of data variability. Moreover, available data sets are often acquired under controlled settings, restricting, for example, scene illumination or 3D head pose to a narrow range. This paper takes a look into the automated generation of adaptive training samples from a 3D morphable face model. Using statistical insights, the tailored training data guarantees full data variability and is enriched by arbitrary facial attributes such as age or body weight. Moreover, it can automatically adapt to environmental constraints, such as illumination or viewing angle of recorded video footage from surveillance cameras. We use the tailored imagery to train a new many-core imple- mentation of Viola Jones ’ AdaBoost object detection framework. The new implementation is not only faster but also enables the use of multiple feature channels such as color features at training time. In our experiments we trained seven view-dependent face detectors and evaluate these on the Face Detection Data Set and Benchmark (FDDB). Our experiments show that the use of tailored training imagery outperforms state-of-the-art approaches on this challenging dataset.</p><p>6 0.1621161 <a title="44-tfidf-6" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>7 0.14687772 <a title="44-tfidf-7" href="./iccv-2013-Handling_Occlusions_with_Franken-Classifiers.html">190 iccv-2013-Handling Occlusions with Franken-Classifiers</a></p>
<p>8 0.14252676 <a title="44-tfidf-8" href="./iccv-2013-Unsupervised_Domain_Adaptation_by_Domain_Invariant_Projection.html">435 iccv-2013-Unsupervised Domain Adaptation by Domain Invariant Projection</a></p>
<p>9 0.14179294 <a title="44-tfidf-9" href="./iccv-2013-Unsupervised_Visual_Domain_Adaptation_Using_Subspace_Alignment.html">438 iccv-2013-Unsupervised Visual Domain Adaptation Using Subspace Alignment</a></p>
<p>10 0.13098809 <a title="44-tfidf-10" href="./iccv-2013-Discriminatively_Trained_Templates_for_3D_Object_Detection%3A_A_Real_Time_Scalable_Approach.html">121 iccv-2013-Discriminatively Trained Templates for 3D Object Detection: A Real Time Scalable Approach</a></p>
<p>11 0.12648235 <a title="44-tfidf-11" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>12 0.11911778 <a title="44-tfidf-12" href="./iccv-2013-Efficient_Pedestrian_Detection_by_Directly_Optimizing_the_Partial_Area_under_the_ROC_Curve.html">136 iccv-2013-Efficient Pedestrian Detection by Directly Optimizing the Partial Area under the ROC Curve</a></p>
<p>13 0.10375664 <a title="44-tfidf-13" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<p>14 0.097500049 <a title="44-tfidf-14" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>15 0.093559861 <a title="44-tfidf-15" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>16 0.093557619 <a title="44-tfidf-16" href="./iccv-2013-Random_Forests_of_Local_Experts_for_Pedestrian_Detection.html">336 iccv-2013-Random Forests of Local Experts for Pedestrian Detection</a></p>
<p>17 0.092711262 <a title="44-tfidf-17" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>18 0.09257789 <a title="44-tfidf-18" href="./iccv-2013-Finding_Actors_and_Actions_in_Movies.html">166 iccv-2013-Finding Actors and Actions in Movies</a></p>
<p>19 0.092465699 <a title="44-tfidf-19" href="./iccv-2013-Cascaded_Shape_Space_Pruning_for_Robust_Facial_Landmark_Detection.html">70 iccv-2013-Cascaded Shape Space Pruning for Robust Facial Landmark Detection</a></p>
<p>20 0.091551915 <a title="44-tfidf-20" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.197), (1, 0.076), (2, -0.071), (3, -0.108), (4, 0.03), (5, -0.06), (6, 0.093), (7, 0.091), (8, -0.016), (9, -0.025), (10, 0.037), (11, -0.112), (12, 0.041), (13, -0.054), (14, 0.103), (15, -0.143), (16, -0.09), (17, 0.066), (18, -0.027), (19, 0.071), (20, 0.04), (21, -0.09), (22, 0.003), (23, 0.073), (24, -0.044), (25, -0.046), (26, -0.119), (27, -0.03), (28, -0.008), (29, -0.072), (30, -0.019), (31, 0.122), (32, -0.01), (33, 0.022), (34, -0.044), (35, -0.127), (36, -0.019), (37, 0.059), (38, -0.005), (39, -0.029), (40, 0.005), (41, 0.084), (42, 0.01), (43, 0.111), (44, -0.009), (45, 0.021), (46, -0.051), (47, -0.044), (48, -0.048), (49, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96200216 <a title="44-lsi-1" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>Author: Vidit Jain, Sachin Sudhakar Farfade</p><p>Abstract: Classification cascades have been very effective for object detection. Such a cascade fails to perform well in data domains with variations in appearances that may not be captured in the training examples. This limited generalization severely restricts the domains for which they can be used effectively. A common approach to address this limitation is to train a new cascade of classifiers from scratch for each of the new domains. Building separate detectors for each of the different domains requires huge annotation and computational effort, making it not scalable to a large number of data domains. Here we present an algorithm for quickly adapting a pre-trained cascade of classifiers using a small number oflabeledpositive instancesfrom a different yet similar data domain. In our experiments with images of human babies and human-like characters from movies, we demonstrate that the adapted cascade significantly outperforms both of the original cascade and the one trained from scratch using the given training examples. –</p><p>2 0.75586849 <a title="44-lsi-2" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<p>Author: Tianfu Wu, Song-Chun Zhu</p><p>Abstract: Many object detectors, such as AdaBoost, SVM and deformable part-based models (DPM), compute additive scoring functions at a large number of windows scanned over image pyramid, thus computational efficiency is an important consideration beside accuracy performance. In this paper, we present a framework of learning cost-sensitive decision policy which is a sequence of two-sided thresholds to execute early rejection or early acceptance based on the accumulative scores at each step. A decision policy is said to be optimal if it minimizes an empirical global risk function that sums over the loss of false negatives (FN) and false positives (FP), and the cost of computation. While the risk function is very complex due to high-order connections among the two-sided thresholds, we find its upper bound can be optimized by dynamic programming (DP) efficiently and thus say the learned policy is near-optimal. Given the loss of FN and FP and the cost in three numbers, our method can produce a policy on-the-fly for Adaboost, SVM and DPM. In experiments, we show that our decision policy outperforms state-of-the-art cascade methods significantly in terms of speed with similar accuracy performance.</p><p>3 0.72907519 <a title="44-lsi-3" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>Author: Haoxiang Li, Gang Hua, Zhe Lin, Jonathan Brandt, Jianchao Yang</p><p>Abstract: We propose an unsupervised detector adaptation algorithm to adapt any offline trained face detector to a specific collection of images, and hence achieve better accuracy. The core of our detector adaptation algorithm is a probabilistic elastic part (PEP) model, which is offline trained with a set of face examples. It produces a statisticallyaligned part based face representation, namely the PEP representation. To adapt a general face detector to a collection of images, we compute the PEP representations of the candidate detections from the general face detector, and then train a discriminative classifier with the top positives and negatives. Then we re-rank all the candidate detections with this classifier. This way, a face detector tailored to the statistics of the specific image collection is adapted from the original detector. We present extensive results on three datasets with two state-of-the-art face detectors. The significant improvement of detection accuracy over these state- of-the-art face detectors strongly demonstrates the efficacy of the proposed face detector adaptation algorithm.</p><p>4 0.71974123 <a title="44-lsi-4" href="./iccv-2013-Efficient_Pedestrian_Detection_by_Directly_Optimizing_the_Partial_Area_under_the_ROC_Curve.html">136 iccv-2013-Efficient Pedestrian Detection by Directly Optimizing the Partial Area under the ROC Curve</a></p>
<p>Author: Sakrapee Paisitkriangkrai, Chunhua Shen, Anton Van Den Hengel</p><p>Abstract: Many typical applications of object detection operate within a prescribed false-positive range. In this situation the performance of a detector should be assessed on the basis of the area under the ROC curve over that range, rather than over the full curve, as the performance outside the range is irrelevant. This measure is labelled as the partial area under the ROC curve (pAUC). Effective cascade-based classification, for example, depends on training node classifiers that achieve the maximal detection rate at a moderate false positive rate, e.g., around 40% to 50%. We propose a novel ensemble learning method which achieves a maximal detection rate at a user-defined range of false positive rates by directly optimizing the partial AUC using structured learning. By optimizing for different ranges of false positive rates, the proposed method can be used to train either a single strong classifier or a node classifier forming part of a cascade classifier. Experimental results on both synthetic and real-world data sets demonstrate the effectiveness of our approach, and we show that it is possible to train state-of-the-art pedestrian detectors using the pro- posed structured ensemble learning method.</p><p>5 0.68108141 <a title="44-lsi-5" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<p>Author: Tatiana Tommasi, Barbara Caputo</p><p>Abstract: Over the last years, several authors have signaled that state of the art categorization methods fail to perform well when trained and tested on data from different databases. The general consensus in the literature is that this issue, known as domain adaptation and/or dataset bias, is due to a distribution mismatch between data collections. Methods addressing it go from max-margin classifiers to learning how to modify the features and obtain a more robust representation. The large majority of these works use BOW feature descriptors, and learning methods based on imageto-image distance functions. Following the seminal work of [6], in this paper we challenge these two assumptions. We experimentally show that using the NBNN classifier over existing domain adaptation databases achieves always very strong performances. We build on this result, and present an NBNN-based domain adaptation algorithm that learns iteratively a class metric while inducing, for each sample, a large margin separation among classes. To the best of our knowledge, this is the first work casting the domain adaptation problem within the NBNN framework. Experiments show that our method achieves the state of the art, both in the unsupervised and semi-supervised settings.</p><p>6 0.65997994 <a title="44-lsi-6" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>7 0.63984621 <a title="44-lsi-7" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>8 0.63558674 <a title="44-lsi-8" href="./iccv-2013-Multi-stage_Contextual_Deep_Learning_for_Pedestrian_Detection.html">279 iccv-2013-Multi-stage Contextual Deep Learning for Pedestrian Detection</a></p>
<p>9 0.63014996 <a title="44-lsi-9" href="./iccv-2013-Handling_Occlusions_with_Franken-Classifiers.html">190 iccv-2013-Handling Occlusions with Franken-Classifiers</a></p>
<p>10 0.62259066 <a title="44-lsi-10" href="./iccv-2013-Image_Segmentation_with_Cascaded_Hierarchical_Models_and_Logistic_Disjunctive_Normal_Networks.html">211 iccv-2013-Image Segmentation with Cascaded Hierarchical Models and Logistic Disjunctive Normal Networks</a></p>
<p>11 0.61389601 <a title="44-lsi-11" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>12 0.6138292 <a title="44-lsi-12" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>13 0.60107458 <a title="44-lsi-13" href="./iccv-2013-Unsupervised_Visual_Domain_Adaptation_Using_Subspace_Alignment.html">438 iccv-2013-Unsupervised Visual Domain Adaptation Using Subspace Alignment</a></p>
<p>14 0.59238058 <a title="44-lsi-14" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>15 0.58566618 <a title="44-lsi-15" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>16 0.58510578 <a title="44-lsi-16" href="./iccv-2013-Unsupervised_Domain_Adaptation_by_Domain_Invariant_Projection.html">435 iccv-2013-Unsupervised Domain Adaptation by Domain Invariant Projection</a></p>
<p>17 0.58019114 <a title="44-lsi-17" href="./iccv-2013-CoDeL%3A_A_Human_Co-detection_and_Labeling_Framework.html">75 iccv-2013-CoDeL: A Human Co-detection and Labeling Framework</a></p>
<p>18 0.57418352 <a title="44-lsi-18" href="./iccv-2013-Beyond_Hard_Negative_Mining%3A_Efficient_Detector_Learning_via_Block-Circulant_Decomposition.html">61 iccv-2013-Beyond Hard Negative Mining: Efficient Detector Learning via Block-Circulant Decomposition</a></p>
<p>19 0.54908711 <a title="44-lsi-19" href="./iccv-2013-Unbiased_Metric_Learning%3A_On_the_Utilization_of_Multiple_Datasets_and_Web_Images_for_Softening_Bias.html">431 iccv-2013-Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias</a></p>
<p>20 0.54770643 <a title="44-lsi-20" href="./iccv-2013-Random_Forests_of_Local_Experts_for_Pedestrian_Detection.html">336 iccv-2013-Random Forests of Local Experts for Pedestrian Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.077), (7, 0.028), (12, 0.026), (26, 0.097), (31, 0.043), (40, 0.012), (42, 0.166), (64, 0.079), (73, 0.028), (77, 0.014), (85, 0.138), (89, 0.128), (98, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87938553 <a title="44-lda-1" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>Author: Vidit Jain, Sachin Sudhakar Farfade</p><p>Abstract: Classification cascades have been very effective for object detection. Such a cascade fails to perform well in data domains with variations in appearances that may not be captured in the training examples. This limited generalization severely restricts the domains for which they can be used effectively. A common approach to address this limitation is to train a new cascade of classifiers from scratch for each of the new domains. Building separate detectors for each of the different domains requires huge annotation and computational effort, making it not scalable to a large number of data domains. Here we present an algorithm for quickly adapting a pre-trained cascade of classifiers using a small number oflabeledpositive instancesfrom a different yet similar data domain. In our experiments with images of human babies and human-like characters from movies, we demonstrate that the adapted cascade significantly outperforms both of the original cascade and the one trained from scratch using the given training examples. –</p><p>2 0.85925728 <a title="44-lda-2" href="./iccv-2013-Potts_Model%2C_Parametric_Maxflow_and_K-Submodular_Functions.html">324 iccv-2013-Potts Model, Parametric Maxflow and K-Submodular Functions</a></p>
<p>Author: Igor Gridchyn, Vladimir Kolmogorov</p><p>Abstract: The problem of minimizing the Potts energy function frequently occurs in computer vision applications. One way to tackle this NP-hard problem was proposed by Kovtun [20, 21]. It identifies a part of an optimal solution by running k maxflow computations, where k is the number of labels. The number of “labeled” pixels can be significant in some applications, e.g. 50-93% in our tests for stereo. We show how to reduce the runtime to O(log k) maxflow computations (or one parametric maxflow computation). Furthermore, the output of our algorithm allows to speed-up the subsequent alpha expansion for the unlabeled part, or can be used as it is for time-critical applications. To derive our technique, we generalize the algorithm of Felzenszwalb et al. [7] for Tree Metrics. We also show a connection to k-submodular functions from combinatorial optimization, and discuss k-submodular relaxations for general energy functions.</p><p>3 0.8507449 <a title="44-lda-3" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>Author: Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, Philip S. Yu</p><p>Abstract: Transfer learning is established as an effective technology in computer visionfor leveraging rich labeled data in the source domain to build an accurate classifier for the target domain. However, most prior methods have not simultaneously reduced the difference in both the marginal distribution and conditional distribution between domains. In this paper, we put forward a novel transfer learning approach, referred to as Joint Distribution Adaptation (JDA). Specifically, JDA aims to jointly adapt both the marginal distribution and conditional distribution in a principled dimensionality reduction procedure, and construct new feature representation that is effective and robustfor substantial distribution difference. Extensive experiments verify that JDA can significantly outperform several state-of-the-art methods on four types of cross-domain image classification problems.</p><p>4 0.83812666 <a title="44-lda-4" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>Author: S. Karthikeyan, Vignesh Jagadeesh, Renuka Shenoy, Miguel Ecksteinz, B.S. Manjunath</p><p>Abstract: Eye movement studies have confirmed that overt attention is highly biased towards faces and text regions in images. In this paper we explore a novel problem of predicting face and text regions in images using eye tracking data from multiple subjects. The problem is challenging as we aim to predict the semantics (face/text/background) only from eye tracking data without utilizing any image information. The proposed algorithm spatially clusters eye tracking data obtained in an image into different coherent groups and subsequently models the likelihood of the clusters containing faces and text using afully connectedMarkov Random Field (MRF). Given the eye tracking datafrom a test image, itpredicts potential face/head (humans, dogs and cats) and text locations reliably. Furthermore, the approach can be used to select regions of interest for further analysis by object detectors for faces and text. The hybrid eye position/object detector approach achieves better detection performance and reduced computation time compared to using only the object detection algorithm. We also present a new eye tracking dataset on 300 images selected from ICDAR, Street-view, Flickr and Oxford-IIIT Pet Dataset from 15 subjects.</p><p>5 0.83618474 <a title="44-lda-5" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>Author: Chenglong Bao, Jian-Feng Cai, Hui Ji</p><p>Abstract: In recent years, how to learn a dictionary from input images for sparse modelling has been one very active topic in image processing and recognition. Most existing dictionary learning methods consider an over-complete dictionary, e.g. the K-SVD method. Often they require solving some minimization problem that is very challenging in terms of computational feasibility and efficiency. However, if the correlations among dictionary atoms are not well constrained, the redundancy of the dictionary does not necessarily improve the performance of sparse coding. This paper proposed a fast orthogonal dictionary learning method for sparse image representation. With comparable performance on several image restoration tasks, the proposed method is much more computationally efficient than the over-complete dictionary based learning methods.</p><p>6 0.83439171 <a title="44-lda-6" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>7 0.8334384 <a title="44-lda-7" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>8 0.83330756 <a title="44-lda-8" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>9 0.83201593 <a title="44-lda-9" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>10 0.83184755 <a title="44-lda-10" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>11 0.83156967 <a title="44-lda-11" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>12 0.83154422 <a title="44-lda-12" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>13 0.82962716 <a title="44-lda-13" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>14 0.82947731 <a title="44-lda-14" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>15 0.8290481 <a title="44-lda-15" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>16 0.82811648 <a title="44-lda-16" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>17 0.82579088 <a title="44-lda-17" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<p>18 0.82558054 <a title="44-lda-18" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>19 0.82429546 <a title="44-lda-19" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>20 0.82418346 <a title="44-lda-20" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
