<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>277 iccv-2013-Multi-channel Correlation Filters</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-277" href="#">iccv2013-277</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>277 iccv-2013-Multi-channel Correlation Filters</h1>
<br/><p>Source: <a title="iccv-2013-277-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Galoogahi_Multi-channel_Correlation_Filters_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Hamed Kiani Galoogahi, Terence Sim, Simon Lucey</p><p>Abstract: Modern descriptors like HOG and SIFT are now commonly used in vision for pattern detection within image and video. From a signal processing perspective, this detection process can be efficiently posed as a correlation/convolution between a multi-channel image and a multi-channel detector/filter which results in a singlechannel response map indicating where the pattern (e.g. object) has occurred. In this paper, we propose a novel framework for learning a multi-channel detector/filter efficiently in the frequency domain, both in terms of training time and memory footprint, which we refer to as a multichannel correlation filter. To demonstrate the effectiveness of our strategy, we evaluate it across a number of visual detection/localization tasks where we: (i) exhibit superiorperformance to current state of the art correlation filters, and (ii) superior computational and memory efficiencies compared to state of the art spatial detectors.</p><p>Reference: <a title="iccv-2013-277-reference" href="../iccv2013_reference/iccv-2013-Multi-channel_Correlation_Filters_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 From a signal processing perspective, this detection process can be efficiently posed as a correlation/convolution between a multi-channel image and a multi-channel detector/filter which results in a singlechannel response map indicating where the pattern (e. [sent-10, score-0.342]
</p><p>2 In this paper, we propose a novel framework for learning a multi-channel detector/filter efficiently in the frequency domain, both in terms of training time and memory footprint, which we refer to as a multichannel correlation filter. [sent-13, score-0.785]
</p><p>3 Introduction  In computer vision it is now rare for tasks like convolution/correlation to be performed on single channel image signals (e. [sent-16, score-0.204]
</p><p>4 With the advent of advanced descriptors like HOG [5] and SIFT [13] convolution/correlation across multi-channel signals has become the norm rather than the exception in most visual detection tasks. [sent-19, score-0.268]
</p><p>5 An example of multi-channel correlation can be seen in Figure 1 where a multi-channel image is convolved/correlated with a multi-channel filter/detector in order to obtain a single-channel response. [sent-22, score-0.343]
</p><p>6 The peak of the response (in white) indicating where the pattern of interest is located. [sent-23, score-0.204]
</p><p>7 Like single channel signals, correlation between two multi-channel signals is rarely performed naively in the spa-  x? [sent-24, score-0.547]
</p><p>8 An example of multi-channel correlation/convolution  where one has a multi-channel image x correlated/convolved with a multi-channel filter h to give a single-channel response y. [sent-26, score-0.276]
</p><p>9 By posing this objective in the frequency domain, our multi-channel correlation filter approach attempts to give a computational & memory efficient strategy for estimating h given x and y. [sent-27, score-0.97]
</p><p>10 detection/tracking through convolution) are performed in the spatial domain [5]. [sent-32, score-0.183]
</p><p>11 Correlation filters, developed initially in the seminal work of Hester and Casasent [8], are a method for learning a template/filter in the frequency domain that rose to some prominence in the 80s and 90s. [sent-35, score-0.291]
</p><p>12 Like correlation itself, one of the central advantages of the single channel approach is that it attempts to learn the filter in the frequency domain due to the efficiency of correlation/convolution in that domain. [sent-37, score-0.95]
</p><p>13 Learning multi-channel filters in the frequency domain, however, comes at the high cost of computation and memory usage. [sent-38, score-0.636]
</p><p>14 33006725  Contributions: In this paper we make the following contributions •  •  •  We propose an extension to canonical correlation filter theory tphoaste i sa na ebxlete ntos efficiently hnaicnadll ceo rmreullattii-cohna fnilnteelr signals. [sent-40, score-0.543]
</p><p>15 Specifically, we show how when posed in the frequency domain the task ofmulti-channel correlation filter estimation forms a sparse banded linear system. [sent-41, score-0.934]
</p><p>16 Further, we demonstrate how our system can be solved much more efficiently than spatial domain methods. [sent-42, score-0.183]
</p><p>17 We characterize theoretically and demonstrate empirically hhaorwac our em tuhletoi-rcehtaicnanlleyl canordr deleamtioonn approach iarif-fords substantial memory savings when learning on multi-channel signals. [sent-43, score-0.249]
</p><p>18 Specifically, we demonstrate how our approach does not have a memory cost that is linear in the number of samples, allowing for substantial savings when learning detectors across large amounts of data. [sent-44, score-0.415]
</p><p>19 We apply our approach across a myriad of detectWioen aapndp yloc oaulriza atipopnro otaacshks including: eye ldoca olfiz daetitoenc,car detection and pedestrian detection. [sent-45, score-0.278]
</p><p>20 We demonstrate: (i) superior performance to current state of the art single-channel correlation filters, and (ii) superior computational and memory efficiency in comparison to spatial detectors (e. [sent-46, score-0.893]
</p><p>21 All M-mode array signals shall be expressed in vectorized form a. [sent-57, score-0.21]
</p><p>22 The role of filter or signal d ˆai can bioen interchanged watritixh . [sent-75, score-0.237]
</p><p>23 The operator conj ( aˆ) applies the complex conjugate to the complex vector aˆ. [sent-78, score-0.194]
</p><p>24 Related Work  Multi-Channel Detectors: The most notable approach to multi-channel detection in computer vision can be found in the seminal work of Dalal & Triggs [5] where the authors employ a HOG descriptor in conjunction with a linear SVM to learn a detector for pedestrian detection. [sent-80, score-0.309]
</p><p>25 This same multi-channel detection pipeline has gone on to be employed in a myriad of other detection tasks in vision ranging from facial landmark localization/detection [19] to general object detection [7]. [sent-81, score-0.587]
</p><p>26 Computational and memory efficiency, however, are issues for Dalal & Triggs style multi-channel detectors. [sent-82, score-0.212]
</p><p>27 A central advantage ofusing a linear SVM, over kernel SVMs, for learning a multi-channel detector is the ability to treat that detector as a multi-channel linear filter during evaluation. [sent-83, score-0.341]
</p><p>28 Instead of inefficiently moving the detector spatially across a multi-channel image, one can take advantage of the fast Fourier transform (FFT) for the efficient application of correlating a desired template/filter with a signal. [sent-84, score-0.205]
</p><p>29 We argue in this paper, that this is a real strength of correlation filters as the objective provides a way for naturally modeling shifted versions of an image without the burden of explicitly storing all the shifted image patches. [sent-89, score-0.794]
</p><p>30 Canonical features such as HOG and SIFT employ filter banks with strong selectivity to spatial frequency, orientation and scale (e. [sent-97, score-0.229]
</p><p>31 [3] recently proposed an extension to traditional correlation filters referred to as Minimum Output Sum of Squared Error (MOSSE) filters. [sent-102, score-0.605]
</p><p>32 A full discussion on other variants of correlation filters such as Optimal Tradeoff Filters (OTF) [15], Unconstrained MACE (UMACE) [17] filters, etc. [sent-106, score-0.605]
</p><p>33 [2] introduced vector correlation filter to train multi-channel descriptors in the Fourier domain for car landmark detection and alignment. [sent-110, score-1.017]
</p><p>34 This approach, however, suffered from huge amount of memory usage and computational complexity, since this approach required to solve a KD KD linear system, where K is the number osofl cvhea nan KelDs a ×nd K KDD Dis tinhee length mof, ,v wechtoerreiz Ked signals. [sent-111, score-0.244]
</p><p>35 Correlation Filters  Due to the efficiency of correlation in the frequency domain, correlation filters have canonically been posed in the frequency domain. [sent-113, score-1.34]
</p><p>36 There is nothing, however, stopping one (other than computational expense) from expressing a correlation filter in the spatial domain. [sent-114, score-0.572]
</p><p>37 In fact, we argue that viewing a correlation filter in the spatial domain can give: (i) important links to existing spatial methods for learning templates/detectors, and (ii) crucial insights into fundamental problems in current correlation filter methods. [sent-115, score-1.268]
</p><p>38 al’s [3] MOSSE correlation filter can be expressed in the spatial domain as solving the following ridge regression problem,  E(h) =12i? [sent-117, score-0.772]
</p><p>39 jD=1||yi(j) − hTxi[Δτj]| 22+2λ||h| 22(1) where yi ∈ RD is the desired response for the i-th observation xi ∈R RD and is a regularization term. [sent-119, score-0.213]
</p><p>40 1  Solving a correlation filter in the spatial domain quickly becomes intractable as a function of the signal length D, as the cost of solving Equation 2 becomes O(D3 + ND2). [sent-145, score-0.83]
</p><p>41 Efficiency in the Frequency Domain: It is well understood in the signal processing community that circular convolution in the spatial domain can be expressed as a Hadamard product in the frequency domain. [sent-146, score-0.46]
</p><p>42 The complex conjugate of is employed to ensure the operation is correlation not convolution. [sent-150, score-0.424]
</p><p>43 1  =  sxy ◦−1  ( sˆxx + λ1)  where ◦−1 denotes element-wise division, and  sxx  ? [sent-156, score-0.232]
</p><p>44 The solution for hˆ in Equations 1and 4 are identical (other than that one is posed in the spatial domain, and the other is in the frequency domain). [sent-165, score-0.273]
</p><p>45 In the frequency domain a solution to hˆ can be 33006747  found with a cost of O(ND log D). [sent-167, score-0.286]
</p><p>46 em Tbhlee oprfi training signals {xi}iN=1 and desired responses {yi}iN=1. [sent-169, score-0.234]
</p><p>47 Memory Efficiency: Inspecting Equation 7 one can see an additional advantage of correlation filters when posed in the frequency domain. [sent-170, score-0.819]
</p><p>48 One  does not need to store the training examples in memory before learning. [sent-172, score-0.271]
</p><p>49 This is a powerful result not often discussed in correlation filter literature as unlike other spatial strategies for learning detectors (e. [sent-174, score-0.604]
</p><p>50 linear SVM) whose memory usage grows as a function of the number of training examples O(ND), correlation filters have fixed memory oevxaermhpealedss OO((NDD) irrespective onf f ithlteer nsu hmavbeer f ioxfe training examples. [sent-176, score-1.179]
</p><p>51 Our Approach Inspired by single-channel correlation filters we shall explore a multi-channel strategy for learning a correlation filter. [sent-178, score-1.041]
</p><p>52 We can express the multi-channel objective in the spatial domain as  E(h)  = 12? [sent-179, score-0.285]
</p><p>53 1  where x(k) and h(k) refers to the kth channel of the vectorized image and filter respectively where K represents the number of filters. [sent-190, score-0.31]
</p><p>54 As with a canonical filter the desired response is single channel y = [y(1), . [sent-191, score-0.469]
</p><p>55 , y(D)]T  ×  even though both the filter and the signal are multi-channel. [sent-194, score-0.237]
</p><p>56 Solving this multi-channel form in the spatial domain is even more intractable than the single channel form with a cost of O(D3K3 + ND2K2) since we now have to solve a oKstD of × O K(DD linear system. [sent-195, score-0.347]
</p><p>57 Fourier Efficiency: Inspired by the efficiencies of posing single channel correlation filters in the Fourier domain we can express Equation 8 equivalently and more succintly  = 12? [sent-196, score-1.01]
</p><p>58 where = At first glance the cost of solving this linear system looks no different to the spatial domain as one still has to solve a KD eKntD t olitn heear s system:  hˆ∗  =  ? [sent-213, score-0.282]
</p><p>59 1  This results in a substantially smaller computational cost of O(DK3 + NDK2) than solving this objective in the spatial DdoKmain O(D3K3 + ND2K2). [sent-239, score-0.203]
</p><p>60 Memory Efficiency: As outlined in Section 3 an additional strength of single channel correlation filters are their memory efficiency. [sent-240, score-0.913]
</p><p>61 Instead, they need to just 33006758  sxx  sxy  compute the auto-spectral and cross-spectral energies respectively of the training observations (see Equation 7). [sent-242, score-0.338]
</p><p>62 The memory saving become sizable as the number of training examples increase as the memory overhead remains constant O(D) instead of O(ND) if one was thoe employ a spatial objective. [sent-243, score-0.614]
</p><p>63 tAea dsim ofil Oar( strategy can baes taken advantage of in our multi-channel correlation form. [sent-244, score-0.378]
</p><p>64 For multi-channel correlation filters this saving becomes even more dramatic as the memory overhead remains constant O(K2D) as opposed to O(NDK). [sent-245, score-0.889]
</p><p>65 f multi-channel correlation filters such that the problem can be posed as D independent K K linear systems. [sent-247, score-0.725]
</p><p>66 Experiments We evaluated our method across a number of challenging localization and detection tasks: facial landmark localization, car detection, and pedestrian detection. [sent-249, score-0.702]
</p><p>67 For all our experiments we used the same parametric form for the desired correlation response, which we defined as a 2D Gaussian function with a spatial variance of two pixels whose the peak is centered at the location of the target of interest (facial landmarks, cars, pedestrians, etc. [sent-250, score-0.567]
</p><p>68 All correlation filters, both single-channel and multi-channel, employed in this paper used a 2D cosine window (as suggested by Bolme et al. [sent-253, score-0.382]
</p><p>69 Facial Landmark Localization We evaluated our method for facial landmark localization on the Labeled Faces in the Wild (LFW) database1 , including 13,233 face images stemming from 5749 subjects. [sent-257, score-0.363]
</p><p>70 edu/lfw peak response location was used as the predicted landmark location. [sent-270, score-0.361]
</p><p>71 The facial landmark localization was evaluated using normalized distance between the desired location and the predicted coordinate of the landmarks:  d =? [sent-271, score-0.43]
</p><p>72 Results and Analysis: Inspecting Figure 2 one can see the superiority of our multi-channel approach compared to state of the art single-channel correlation filter methods MOSSE  ××  and ASEF. [sent-277, score-0.588]
</p><p>73 Some visual examples of the output from our approach employed for facial landmark localization can be seen in Figure 3. [sent-281, score-0.402]
</p><p>74 It should be noted that this approach to landmark localization employs no shape prior, relying instead solely on the landmark detectors making a fair comparison with more recent methods in facial landmark localization such as Zhu and Ramanan [19] difficult. [sent-282, score-0.806]
</p><p>75 Car Detection  The objective of this experiment is to evaluate our proposed multi-channel correlation filter (MCCF) strategy for car localization in street scene images. [sent-285, score-0.792]
</p><p>76 nOdu thr eMnC poCwF was trmraainleizde adn tdo ehavavleu zaeterdo- imn etahen same manner to the previous experiment using 100 180 car patches cropped pfrroemvi training images (excluding ×st 1re8e0t scenes). [sent-288, score-0.201]
</p><p>77 The peak of the Gaussian desired responses was located at the center of the car patches. [sent-289, score-0.265]
</p><p>78 We selected the peak of the correlation output as the predicted location of a car in street scene of the testing images. [sent-290, score-0.541]
</p><p>79 2 depicts our localization performance in comparison to leading single-channel correlation filters MOSSE and ASEF where  we obtain superior performance across all thresholds. [sent-292, score-0.799]
</p><p>80 Pedestrian Detection We evaluated our method for pedestrian detection using Daimler pedestrian dataset [14] containing five disjoint im2http : / / cbcl . [sent-296, score-0.323]
</p><p>81 The performance of facial features localization: localization rate versus threshold (best viewed in color). [sent-307, score-0.272]
</p><p>82 Inspecting Figure 6 (a)  one can see our MCCF obtains similar detection results to linear SVM in terms of detection performance as a function of different false positive rates. [sent-322, score-0.241]
</p><p>83 This result is not that surprising as the linear SVM objective is quite similar to the MCCF objective (which can be interpreted as a ridge regression when posed in the spatial domain). [sent-323, score-0.318]
</p><p>84 3Peak-to-Sidelobe Ratio (PSR) is a common metric used in correlation filter literature for detection/verification tasks. [sent-325, score-0.513]
</p><p>85 It is the ratio of the peak response to the local surrounding response, more details on this measure can be found in [12]. [sent-326, score-0.204]
</p><p>86 Comparing minimum required  memory (MB)  of our method with SVM  as a  function of number of training images. [sent-348, score-0.271]
</p><p>87 It is interesting to note that our MCCF objective can achieve good detection performance with substantially smaller amounts of training data when compared to linear SVM. [sent-350, score-0.278]
</p><p>88 This superior performance can be attributed to how correlation filters implicitly use synthetic circular shifted versions of images within the learning process without having to explicitly create the images. [sent-351, score-0.836]
</p><p>89 As a result our MCCF objective can do “more with less” by achieving good detection performance with substantially less training data. [sent-352, score-0.217]
</p><p>90 One can see how training time starts to increase dramatically for linear SVM4 where as our training time only increases modestly as a function of training set size. [sent-354, score-0.209]
</p><p>91 The central advantage of our proposed approach here is that the solving of the multi-channel linear system in the frequency domain is independent to the number of images. [sent-355, score-0.352]
</p><p>92 o Finally, inspecting Table 1 one can see the superior nature of our MCCF approach in comparison to linear SVM with respect to memory usage. [sent-362, score-0.418]
</p><p>93 As discussed in Section 4 our proposed MCCF approach has a modest fixed memory requirement independent of the training set size, whereas the amount of memory used by the linear SVM approach is a linear function of the number of training examples. [sent-363, score-0.606]
</p><p>94 Conclusion In this paper, we propose a novel extension to correlation filter theory which allows for the employment of multichannel signals with the efficient use of memory and computations. [sent-365, score-0.878]
</p><p>95 We demonstrate the advantages of our new approach across a variety of detection and localization tasks. [sent-366, score-0.215]
</p><p>96 Comparing our method with SVM + HOG (a) ROC curve of detection rate as a function of false positive rate (8000 training images), (b) pedestrian detection rate at FPR = 0. [sent-374, score-0.476]
</p><p>97 Some samples of (top)  true  detection of pedestrian (true positive), (middle) false detection of non-pedestrian (false negative), and  (bottom) false detection of pedestrian (false positive). [sent-377, score-0.579]
</p><p>98 Optimal trade-off filters for noise robustness, sharpness of the correlation peak, and horner efficiency. [sent-493, score-0.605]
</p><p>99 Efficient design of advanced correlation filters for robust distortion-tolerant face recognition. [sent-508, score-0.605]
</p><p>100 Face detection, pose estimation, and landmark localization in the wild. [sent-520, score-0.254]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mccf', 0.376), ('correlation', 0.343), ('filters', 0.262), ('memory', 0.212), ('bolme', 0.179), ('mosse', 0.174), ('filter', 0.17), ('landmark', 0.157), ('fourier', 0.129), ('frequency', 0.126), ('domain', 0.124), ('pedestrian', 0.121), ('conj', 0.116), ('sxx', 0.116), ('sxy', 0.116), ('inspecting', 0.114), ('facial', 0.109), ('signals', 0.108), ('response', 0.106), ('car', 0.1), ('peak', 0.098), ('localization', 0.097), ('channel', 0.096), ('posed', 0.088), ('asef', 0.087), ('detection', 0.081), ('svm', 0.069), ('desired', 0.067), ('correlating', 0.067), ('signal', 0.067), ('equation', 0.062), ('kd', 0.062), ('superior', 0.06), ('fft', 0.059), ('training', 0.059), ('spatial', 0.059), ('shall', 0.058), ('efficiencies', 0.058), ('hester', 0.058), ('diag', 0.057), ('shifted', 0.056), ('express', 0.055), ('circular', 0.055), ('efficiency', 0.052), ('singapore', 0.052), ('boddeti', 0.051), ('draper', 0.051), ('banded', 0.051), ('hadamard', 0.048), ('jarrett', 0.047), ('interocular', 0.047), ('energies', 0.047), ('objective', 0.047), ('false', 0.047), ('triggs', 0.046), ('ridge', 0.045), ('art', 0.045), ('multichannel', 0.045), ('vectorized', 0.044), ('hog', 0.044), ('saving', 0.043), ('descriptors', 0.042), ('conjugate', 0.042), ('cropped', 0.042), ('seminal', 0.041), ('psr', 0.041), ('yi', 0.04), ('myriad', 0.039), ('arrays', 0.039), ('central', 0.039), ('employed', 0.039), ('valstar', 0.038), ('landmarks', 0.038), ('nd', 0.037), ('threshold', 0.037), ('savings', 0.037), ('posing', 0.037), ('across', 0.037), ('cost', 0.036), ('operator', 0.036), ('strategy', 0.035), ('tial', 0.035), ('equivalently', 0.035), ('inefficient', 0.034), ('detector', 0.034), ('dalal', 0.033), ('lucey', 0.032), ('detectors', 0.032), ('linear', 0.032), ('solving', 0.031), ('comp', 0.031), ('substantially', 0.03), ('canonical', 0.03), ('synthetic', 0.03), ('versions', 0.03), ('state', 0.03), ('rate', 0.029), ('amounts', 0.029), ('understood', 0.029), ('overhead', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999869 <a title="277-tfidf-1" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>Author: Hamed Kiani Galoogahi, Terence Sim, Simon Lucey</p><p>Abstract: Modern descriptors like HOG and SIFT are now commonly used in vision for pattern detection within image and video. From a signal processing perspective, this detection process can be efficiently posed as a correlation/convolution between a multi-channel image and a multi-channel detector/filter which results in a singlechannel response map indicating where the pattern (e.g. object) has occurred. In this paper, we propose a novel framework for learning a multi-channel detector/filter efficiently in the frequency domain, both in terms of training time and memory footprint, which we refer to as a multichannel correlation filter. To demonstrate the effectiveness of our strategy, we evaluate it across a number of visual detection/localization tasks where we: (i) exhibit superiorperformance to current state of the art correlation filters, and (ii) superior computational and memory efficiencies compared to state of the art spatial detectors.</p><p>2 0.1721485 <a title="277-tfidf-2" href="./iccv-2013-Cascaded_Shape_Space_Pruning_for_Robust_Facial_Landmark_Detection.html">70 iccv-2013-Cascaded Shape Space Pruning for Robust Facial Landmark Detection</a></p>
<p>Author: Xiaowei Zhao, Shiguang Shan, Xiujuan Chai, Xilin Chen</p><p>Abstract: In this paper, we propose a novel cascaded face shape space pruning algorithm for robust facial landmark detection. Through progressively excluding the incorrect candidate shapes, our algorithm can accurately and efficiently achieve the globally optimal shape configuration. Specifically, individual landmark detectors are firstly applied to eliminate wrong candidates for each landmark. Then, the candidate shape space is further pruned by jointly removing incorrect shape configurations. To achieve this purpose, a discriminative structure classifier is designed to assess the candidate shape configurations. Based on the learned discriminative structure classifier, an efficient shape space pruning strategy is proposed to quickly reject most incorrect candidate shapes while preserve the true shape. The proposed algorithm is carefully evaluated on a large set of real world face images. In addition, comparison results on the publicly available BioID and LFW face databases demonstrate that our algorithm outperforms some state-of-the-art algorithms.</p><p>3 0.1475873 <a title="277-tfidf-3" href="./iccv-2013-Pose-Free_Facial_Landmark_Fitting_via_Optimized_Part_Mixtures_and_Cascaded_Deformable_Shape_Model.html">321 iccv-2013-Pose-Free Facial Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Shape Model</a></p>
<p>Author: Xiang Yu, Junzhou Huang, Shaoting Zhang, Wang Yan, Dimitris N. Metaxas</p><p>Abstract: This paper addresses the problem of facial landmark localization and tracking from a single camera. We present a two-stage cascaded deformable shape model to effectively and efficiently localize facial landmarks with large head pose variations. For face detection, we propose a group sparse learning method to automatically select the most salient facial landmarks. By introducing 3D face shape model, we use procrustes analysis to achieve pose-free facial landmark initialization. For deformation, the first step uses mean-shift local search with constrained local model to rapidly approach the global optimum. The second step uses component-wise active contours to discriminatively refine the subtle shape variation. Our framework can simultaneously handle face detection, pose-free landmark localization and tracking in real time. Extensive experiments are conducted on both laboratory environmental face databases and face-in-the-wild databases. All results demonstrate that our approach has certain advantages over state-of-theart methods in handling pose variations1.</p><p>4 0.14203726 <a title="277-tfidf-4" href="./iccv-2013-Beyond_Hard_Negative_Mining%3A_Efficient_Detector_Learning_via_Block-Circulant_Decomposition.html">61 iccv-2013-Beyond Hard Negative Mining: Efficient Detector Learning via Block-Circulant Decomposition</a></p>
<p>Author: João F. Henriques, João Carreira, Rui Caseiro, Jorge Batista</p><p>Abstract: Competitive sliding window detectors require vast training sets. Since a pool of natural images provides a nearly endless supply of negative samples, in the form of patches at different scales and locations, training with all the available data is considered impractical. A staple of current approaches is hard negative mining, a method of selecting relevant samples, which is nevertheless expensive. Given that samples at slightly different locations have overlapping support, there seems to be an enormous amount of duplicated work. It is natural, then, to ask whether these redundancies can be eliminated. In this paper, we show that the Gram matrix describing such data is block-circulant. We derive a transformation based on the Fourier transform that block-diagonalizes the Gram matrix, at once eliminating redundancies and partitioning the learning problem. This decomposition is valid for any dense features and several learning algorithms, and takes full advantage of modern parallel architectures. Surprisingly, it allows training with all the potential samples in sets of thousands of images. By considering the full set, we generate in a single shot the optimal solution, which is usually obtained only after several rounds of hard negative mining. We report speed gains on Caltech Pedestrians and INRIA Pedestrians of over an order of magnitude, allowing training on a desktop computer in a couple of minutes.</p><p>5 0.1348725 <a title="277-tfidf-5" href="./iccv-2013-Local_Signal_Equalization_for_Correspondence_Matching.html">255 iccv-2013-Local Signal Equalization for Correspondence Matching</a></p>
<p>Author: Derek Bradley, Thabo Beeler</p><p>Abstract: Correspondence matching is one of the most common problems in computer vision, and it is often solved using photo-consistency of local regions. These approaches typically assume that the frequency content in the local region is consistent in the image pair, such that matching is performed on similar signals. However, in many practical situations this is not the case, for example with low depth of field cameras a scene point may be out of focus in one view and in-focus in the other, causing a mismatch of frequency signals. Furthermore, this mismatch can vary spatially over the entire image. In this paper we propose a local signal equalization approach for correspondence matching. Using a measure of local image frequency, we equalize local signals using an efficient scale-space image representation such that their frequency contents are optimally suited for matching. Our approach allows better correspondence matching, which we demonstrate with a number of stereo reconstruction examples on synthetic and real datasets.</p><p>6 0.10301634 <a title="277-tfidf-6" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>7 0.10261318 <a title="277-tfidf-7" href="./iccv-2013-Exemplar-Based_Graph_Matching_for_Robust_Facial_Landmark_Localization.html">149 iccv-2013-Exemplar-Based Graph Matching for Robust Facial Landmark Localization</a></p>
<p>8 0.097712062 <a title="277-tfidf-8" href="./iccv-2013-Multi-stage_Contextual_Deep_Learning_for_Pedestrian_Detection.html">279 iccv-2013-Multi-stage Contextual Deep Learning for Pedestrian Detection</a></p>
<p>9 0.09523984 <a title="277-tfidf-9" href="./iccv-2013-Accurate_and_Robust_3D_Facial_Capture_Using_a_Single_RGBD_Camera.html">36 iccv-2013-Accurate and Robust 3D Facial Capture Using a Single RGBD Camera</a></p>
<p>10 0.092600584 <a title="277-tfidf-10" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>11 0.092104308 <a title="277-tfidf-11" href="./iccv-2013-Constant_Time_Weighted_Median_Filtering_for_Stereo_Matching_and_Beyond.html">88 iccv-2013-Constant Time Weighted Median Filtering for Stereo Matching and Beyond</a></p>
<p>12 0.088146836 <a title="277-tfidf-12" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>13 0.083763726 <a title="277-tfidf-13" href="./iccv-2013-Random_Forests_of_Local_Experts_for_Pedestrian_Detection.html">336 iccv-2013-Random Forests of Local Experts for Pedestrian Detection</a></p>
<p>14 0.080893487 <a title="277-tfidf-14" href="./iccv-2013-HOGgles%3A_Visualizing_Object_Detection_Features.html">189 iccv-2013-HOGgles: Visualizing Object Detection Features</a></p>
<p>15 0.077603735 <a title="277-tfidf-15" href="./iccv-2013-Shufflets%3A_Shared_Mid-level_Parts_for_Fast_Object_Detection.html">390 iccv-2013-Shufflets: Shared Mid-level Parts for Fast Object Detection</a></p>
<p>16 0.075258136 <a title="277-tfidf-16" href="./iccv-2013-Volumetric_Semantic_Segmentation_Using_Pyramid_Context_Features.html">447 iccv-2013-Volumetric Semantic Segmentation Using Pyramid Context Features</a></p>
<p>17 0.07365986 <a title="277-tfidf-17" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>18 0.073107906 <a title="277-tfidf-18" href="./iccv-2013-Modeling_Occlusion_by_Discriminative_AND-OR_Structures.html">269 iccv-2013-Modeling Occlusion by Discriminative AND-OR Structures</a></p>
<p>19 0.072912097 <a title="277-tfidf-19" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>20 0.072414137 <a title="277-tfidf-20" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.197), (1, 0.015), (2, -0.05), (3, -0.078), (4, 0.017), (5, -0.065), (6, 0.095), (7, 0.054), (8, -0.04), (9, -0.064), (10, 0.01), (11, -0.023), (12, 0.042), (13, -0.049), (14, 0.039), (15, -0.018), (16, 0.017), (17, 0.059), (18, 0.033), (19, -0.006), (20, 0.022), (21, 0.073), (22, -0.052), (23, 0.085), (24, -0.039), (25, 0.021), (26, -0.006), (27, -0.118), (28, -0.031), (29, 0.002), (30, -0.012), (31, 0.044), (32, 0.017), (33, 0.093), (34, 0.035), (35, 0.021), (36, 0.004), (37, -0.035), (38, 0.07), (39, 0.075), (40, -0.014), (41, -0.071), (42, -0.048), (43, -0.055), (44, -0.042), (45, -0.042), (46, -0.051), (47, -0.05), (48, 0.006), (49, 0.069)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94309419 <a title="277-lsi-1" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>Author: Hamed Kiani Galoogahi, Terence Sim, Simon Lucey</p><p>Abstract: Modern descriptors like HOG and SIFT are now commonly used in vision for pattern detection within image and video. From a signal processing perspective, this detection process can be efficiently posed as a correlation/convolution between a multi-channel image and a multi-channel detector/filter which results in a singlechannel response map indicating where the pattern (e.g. object) has occurred. In this paper, we propose a novel framework for learning a multi-channel detector/filter efficiently in the frequency domain, both in terms of training time and memory footprint, which we refer to as a multichannel correlation filter. To demonstrate the effectiveness of our strategy, we evaluate it across a number of visual detection/localization tasks where we: (i) exhibit superiorperformance to current state of the art correlation filters, and (ii) superior computational and memory efficiencies compared to state of the art spatial detectors.</p><p>2 0.65775412 <a title="277-lsi-2" href="./iccv-2013-Exemplar-Based_Graph_Matching_for_Robust_Facial_Landmark_Localization.html">149 iccv-2013-Exemplar-Based Graph Matching for Robust Facial Landmark Localization</a></p>
<p>Author: Feng Zhou, Jonathan Brandt, Zhe Lin</p><p>Abstract: Localizing facial landmarks is a fundamental step in facial image analysis. However, the problem is still challenging due to the large variability in pose and appearance, and the existence ofocclusions in real-worldface images. In this paper, we present exemplar-based graph matching (EGM), a robust framework for facial landmark localization. Compared to conventional algorithms, EGM has three advantages: (1) an affine-invariant shape constraint is learned online from similar exemplars to better adapt to the test face; (2) the optimal landmark configuration can be directly obtained by solving a graph matching problem with the learned shape constraint; (3) the graph matching problem can be optimized efficiently by linear programming. To our best knowledge, this is the first attempt to apply a graph matching technique for facial landmark localization. Experiments on several challenging datasets demonstrate the advantages of EGM over state-of-the-art methods.</p><p>3 0.6395216 <a title="277-lsi-3" href="./iccv-2013-Cascaded_Shape_Space_Pruning_for_Robust_Facial_Landmark_Detection.html">70 iccv-2013-Cascaded Shape Space Pruning for Robust Facial Landmark Detection</a></p>
<p>Author: Xiaowei Zhao, Shiguang Shan, Xiujuan Chai, Xilin Chen</p><p>Abstract: In this paper, we propose a novel cascaded face shape space pruning algorithm for robust facial landmark detection. Through progressively excluding the incorrect candidate shapes, our algorithm can accurately and efficiently achieve the globally optimal shape configuration. Specifically, individual landmark detectors are firstly applied to eliminate wrong candidates for each landmark. Then, the candidate shape space is further pruned by jointly removing incorrect shape configurations. To achieve this purpose, a discriminative structure classifier is designed to assess the candidate shape configurations. Based on the learned discriminative structure classifier, an efficient shape space pruning strategy is proposed to quickly reject most incorrect candidate shapes while preserve the true shape. The proposed algorithm is carefully evaluated on a large set of real world face images. In addition, comparison results on the publicly available BioID and LFW face databases demonstrate that our algorithm outperforms some state-of-the-art algorithms.</p><p>4 0.6387195 <a title="277-lsi-4" href="./iccv-2013-Pose-Free_Facial_Landmark_Fitting_via_Optimized_Part_Mixtures_and_Cascaded_Deformable_Shape_Model.html">321 iccv-2013-Pose-Free Facial Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Shape Model</a></p>
<p>Author: Xiang Yu, Junzhou Huang, Shaoting Zhang, Wang Yan, Dimitris N. Metaxas</p><p>Abstract: This paper addresses the problem of facial landmark localization and tracking from a single camera. We present a two-stage cascaded deformable shape model to effectively and efficiently localize facial landmarks with large head pose variations. For face detection, we propose a group sparse learning method to automatically select the most salient facial landmarks. By introducing 3D face shape model, we use procrustes analysis to achieve pose-free facial landmark initialization. For deformation, the first step uses mean-shift local search with constrained local model to rapidly approach the global optimum. The second step uses component-wise active contours to discriminatively refine the subtle shape variation. Our framework can simultaneously handle face detection, pose-free landmark localization and tracking in real time. Extensive experiments are conducted on both laboratory environmental face databases and face-in-the-wild databases. All results demonstrate that our approach has certain advantages over state-of-theart methods in handling pose variations1.</p><p>5 0.6311965 <a title="277-lsi-5" href="./iccv-2013-Beyond_Hard_Negative_Mining%3A_Efficient_Detector_Learning_via_Block-Circulant_Decomposition.html">61 iccv-2013-Beyond Hard Negative Mining: Efficient Detector Learning via Block-Circulant Decomposition</a></p>
<p>Author: João F. Henriques, João Carreira, Rui Caseiro, Jorge Batista</p><p>Abstract: Competitive sliding window detectors require vast training sets. Since a pool of natural images provides a nearly endless supply of negative samples, in the form of patches at different scales and locations, training with all the available data is considered impractical. A staple of current approaches is hard negative mining, a method of selecting relevant samples, which is nevertheless expensive. Given that samples at slightly different locations have overlapping support, there seems to be an enormous amount of duplicated work. It is natural, then, to ask whether these redundancies can be eliminated. In this paper, we show that the Gram matrix describing such data is block-circulant. We derive a transformation based on the Fourier transform that block-diagonalizes the Gram matrix, at once eliminating redundancies and partitioning the learning problem. This decomposition is valid for any dense features and several learning algorithms, and takes full advantage of modern parallel architectures. Surprisingly, it allows training with all the potential samples in sets of thousands of images. By considering the full set, we generate in a single shot the optimal solution, which is usually obtained only after several rounds of hard negative mining. We report speed gains on Caltech Pedestrians and INRIA Pedestrians of over an order of magnitude, allowing training on a desktop computer in a couple of minutes.</p><p>6 0.60635281 <a title="277-lsi-6" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>7 0.59800249 <a title="277-lsi-7" href="./iccv-2013-Shape_Index_Descriptors_Applied_to_Texture-Based_Galaxy_Analysis.html">388 iccv-2013-Shape Index Descriptors Applied to Texture-Based Galaxy Analysis</a></p>
<p>8 0.59087145 <a title="277-lsi-8" href="./iccv-2013-Like_Father%2C_Like_Son%3A_Facial_Expression_Dynamics_for_Kinship_Verification.html">251 iccv-2013-Like Father, Like Son: Facial Expression Dynamics for Kinship Verification</a></p>
<p>9 0.58273882 <a title="277-lsi-9" href="./iccv-2013-Robust_Face_Landmark_Estimation_under_Occlusion.html">355 iccv-2013-Robust Face Landmark Estimation under Occlusion</a></p>
<p>10 0.57985413 <a title="277-lsi-10" href="./iccv-2013-Heterogeneous_Auto-similarities_of_Characteristics_%28HASC%29%3A_Exploiting_Relational_Information_for_Classification.html">193 iccv-2013-Heterogeneous Auto-similarities of Characteristics (HASC): Exploiting Relational Information for Classification</a></p>
<p>11 0.57456964 <a title="277-lsi-11" href="./iccv-2013-HOGgles%3A_Visualizing_Object_Detection_Features.html">189 iccv-2013-HOGgles: Visualizing Object Detection Features</a></p>
<p>12 0.5717259 <a title="277-lsi-12" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>13 0.56551582 <a title="277-lsi-13" href="./iccv-2013-Shufflets%3A_Shared_Mid-level_Parts_for_Fast_Object_Detection.html">390 iccv-2013-Shufflets: Shared Mid-level Parts for Fast Object Detection</a></p>
<p>14 0.54075426 <a title="277-lsi-14" href="./iccv-2013-Efficient_Pedestrian_Detection_by_Directly_Optimizing_the_Partial_Area_under_the_ROC_Curve.html">136 iccv-2013-Efficient Pedestrian Detection by Directly Optimizing the Partial Area under the ROC Curve</a></p>
<p>15 0.53552544 <a title="277-lsi-15" href="./iccv-2013-Facial_Action_Unit_Event_Detection_by_Cascade_of_Tasks.html">155 iccv-2013-Facial Action Unit Event Detection by Cascade of Tasks</a></p>
<p>16 0.53228801 <a title="277-lsi-16" href="./iccv-2013-Automatic_Kronecker_Product_Model_Based_Detection_of_Repeated_Patterns_in_2D_Urban_Images.html">55 iccv-2013-Automatic Kronecker Product Model Based Detection of Repeated Patterns in 2D Urban Images</a></p>
<p>17 0.52899241 <a title="277-lsi-17" href="./iccv-2013-Constant_Time_Weighted_Median_Filtering_for_Stereo_Matching_and_Beyond.html">88 iccv-2013-Constant Time Weighted Median Filtering for Stereo Matching and Beyond</a></p>
<p>18 0.52782643 <a title="277-lsi-18" href="./iccv-2013-Discovering_Details_and_Scene_Structure_with_Hierarchical_Iconoid_Shift.html">117 iccv-2013-Discovering Details and Scene Structure with Hierarchical Iconoid Shift</a></p>
<p>19 0.52618921 <a title="277-lsi-19" href="./iccv-2013-Random_Forests_of_Local_Experts_for_Pedestrian_Detection.html">336 iccv-2013-Random Forests of Local Experts for Pedestrian Detection</a></p>
<p>20 0.52587044 <a title="277-lsi-20" href="./iccv-2013-Multi-stage_Contextual_Deep_Learning_for_Pedestrian_Detection.html">279 iccv-2013-Multi-stage Contextual Deep Learning for Pedestrian Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.086), (7, 0.015), (12, 0.012), (13, 0.01), (26, 0.079), (27, 0.024), (31, 0.064), (42, 0.145), (48, 0.011), (64, 0.067), (73, 0.035), (75, 0.198), (78, 0.022), (89, 0.122), (98, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82047141 <a title="277-lda-1" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>Author: Hamed Kiani Galoogahi, Terence Sim, Simon Lucey</p><p>Abstract: Modern descriptors like HOG and SIFT are now commonly used in vision for pattern detection within image and video. From a signal processing perspective, this detection process can be efficiently posed as a correlation/convolution between a multi-channel image and a multi-channel detector/filter which results in a singlechannel response map indicating where the pattern (e.g. object) has occurred. In this paper, we propose a novel framework for learning a multi-channel detector/filter efficiently in the frequency domain, both in terms of training time and memory footprint, which we refer to as a multichannel correlation filter. To demonstrate the effectiveness of our strategy, we evaluate it across a number of visual detection/localization tasks where we: (i) exhibit superiorperformance to current state of the art correlation filters, and (ii) superior computational and memory efficiencies compared to state of the art spatial detectors.</p><p>2 0.81903684 <a title="277-lda-2" href="./iccv-2013-Learning_to_Predict_Gaze_in_Egocentric_Video.html">247 iccv-2013-Learning to Predict Gaze in Egocentric Video</a></p>
<p>Author: Yin Li, Alireza Fathi, James M. Rehg</p><p>Abstract: We present a model for gaze prediction in egocentric video by leveraging the implicit cues that exist in camera wearer’s behaviors. Specifically, we compute the camera wearer’s head motion and hand location from the video and combine them to estimate where the eyes look. We further model the dynamic behavior of the gaze, in particular fixations, as latent variables to improve the gaze prediction. Our gaze prediction results outperform the state-of-the-art algorithms by a large margin on publicly available egocentric vision datasets. In addition, we demonstrate that we get a significant performance boost in recognizing daily actions and segmenting foreground objects by plugging in our gaze predictions into state-of-the-art methods.</p><p>3 0.7669943 <a title="277-lda-3" href="./iccv-2013-Efficient_Pedestrian_Detection_by_Directly_Optimizing_the_Partial_Area_under_the_ROC_Curve.html">136 iccv-2013-Efficient Pedestrian Detection by Directly Optimizing the Partial Area under the ROC Curve</a></p>
<p>Author: Sakrapee Paisitkriangkrai, Chunhua Shen, Anton Van Den Hengel</p><p>Abstract: Many typical applications of object detection operate within a prescribed false-positive range. In this situation the performance of a detector should be assessed on the basis of the area under the ROC curve over that range, rather than over the full curve, as the performance outside the range is irrelevant. This measure is labelled as the partial area under the ROC curve (pAUC). Effective cascade-based classification, for example, depends on training node classifiers that achieve the maximal detection rate at a moderate false positive rate, e.g., around 40% to 50%. We propose a novel ensemble learning method which achieves a maximal detection rate at a user-defined range of false positive rates by directly optimizing the partial AUC using structured learning. By optimizing for different ranges of false positive rates, the proposed method can be used to train either a single strong classifier or a node classifier forming part of a cascade classifier. Experimental results on both synthetic and real-world data sets demonstrate the effectiveness of our approach, and we show that it is possible to train state-of-the-art pedestrian detectors using the pro- posed structured ensemble learning method.</p><p>4 0.74893081 <a title="277-lda-4" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>Author: Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, Philip S. Yu</p><p>Abstract: Transfer learning is established as an effective technology in computer visionfor leveraging rich labeled data in the source domain to build an accurate classifier for the target domain. However, most prior methods have not simultaneously reduced the difference in both the marginal distribution and conditional distribution between domains. In this paper, we put forward a novel transfer learning approach, referred to as Joint Distribution Adaptation (JDA). Specifically, JDA aims to jointly adapt both the marginal distribution and conditional distribution in a principled dimensionality reduction procedure, and construct new feature representation that is effective and robustfor substantial distribution difference. Extensive experiments verify that JDA can significantly outperform several state-of-the-art methods on four types of cross-domain image classification problems.</p><p>5 0.74630022 <a title="277-lda-5" href="./iccv-2013-Higher_Order_Matching_for_Consistent_Multiple_Target_Tracking.html">200 iccv-2013-Higher Order Matching for Consistent Multiple Target Tracking</a></p>
<p>Author: Chetan Arora, Amir Globerson</p><p>Abstract: This paper addresses the data assignment problem in multi frame multi object tracking in video sequences. Traditional methods employing maximum weight bipartite matching offer limited temporal modeling. It has recently been shown [6, 8, 24] that incorporating higher order temporal constraints improves the assignment solution. Finding maximum weight matching with higher order constraints is however NP-hard and the solutions proposed until now have either been greedy [8] or rely on greedy rounding of the solution obtained from spectral techniques [15]. We propose a novel algorithm to find the approximate solution to data assignment problem with higher order temporal constraints using the method of dual decomposition and the MPLP message passing algorithm [21]. We compare the proposed algorithm with an implementation of [8] and [15] and show that proposed technique provides better solution with a bound on approximation factor for each inferred solution.</p><p>6 0.74481654 <a title="277-lda-6" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>7 0.7386924 <a title="277-lda-7" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>8 0.73575079 <a title="277-lda-8" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>9 0.73483407 <a title="277-lda-9" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>10 0.73282182 <a title="277-lda-10" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>11 0.73251247 <a title="277-lda-11" href="./iccv-2013-Bayesian_Joint_Topic_Modelling_for_Weakly_Supervised_Object_Localisation.html">59 iccv-2013-Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation</a></p>
<p>12 0.73231488 <a title="277-lda-12" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>13 0.72939831 <a title="277-lda-13" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>14 0.72924447 <a title="277-lda-14" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>15 0.72918761 <a title="277-lda-15" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<p>16 0.72871357 <a title="277-lda-16" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>17 0.7282027 <a title="277-lda-17" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>18 0.72710967 <a title="277-lda-18" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>19 0.7262131 <a title="277-lda-19" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>20 0.72574741 <a title="277-lda-20" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
