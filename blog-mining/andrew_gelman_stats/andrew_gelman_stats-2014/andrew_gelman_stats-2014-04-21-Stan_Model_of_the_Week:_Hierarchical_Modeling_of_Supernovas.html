<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2299 andrew gelman stats-2014-04-21-Stan Model of the Week: Hierarchical Modeling of Supernovas</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2014" href="../home/andrew_gelman_stats-2014_home.html">andrew_gelman_stats-2014</a> <a title="andrew_gelman_stats-2014-2299" href="#">andrew_gelman_stats-2014-2299</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2299 andrew gelman stats-2014-04-21-Stan Model of the Week: Hierarchical Modeling of Supernovas</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2014-2299-html" href="http://andrewgelman.com/2014/04/21/stan-model-week-hierarchical-modeling-supernovas/">html</a></p><p>Introduction: The Stan Model of the Week showcases research using Stan to push the limits of applied statistics.  If you have a model that you would like to submit for a future post then send us an  email . 
 
Our inaugural post comes from Nathan Sanders, a graduate student finishing up his thesis on astrophysics at Harvard. Nathan writes,
  
“Core-collapse supernovae, the luminous explosions of massive stars, exhibit an expansive and meaningful diversity of behavior in their brightness evolution over time (their “light curves”). Our group discovers and monitors these events using the Pan-STARRS1 telescope in Hawaii, and we’ve collected a dataset of about 20,000 individual photometric observations of about 80 Type IIP supernovae, the class my work has focused on. While this dataset provides one of the best available tools to infer the explosion properties of these supernovae, due to the nature of extragalactic astronomy (observing from distances 
  1 billion light years), these light curves typicall</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 If you have a model that you would like to submit for a future post then send us an  email . [sent-2, score-0.22]
</p><p>2 Our group discovers and monitors these events using the Pan-STARRS1 telescope in Hawaii, and we’ve collected a dataset of about 20,000 individual photometric observations of about 80 Type IIP supernovae, the class my work has focused on. [sent-5, score-0.252]
</p><p>3 My goal has been to develop a light curve model, with a physically interpretable parameterization, robust enough to fit the diversity of observed behavior and to extract the most information possible from every light curve in the sample, regardless of data quality or completeness. [sent-7, score-1.499]
</p><p>4 Because light curve parameters of individual objects are often not identified by the data, we have adopted a hierarchical model structure. [sent-8, score-1.177]
</p><p>5 The intention is to capitalize on partial pooling of information to simultaneously regularize the fits of individual light curves and constrain the population level properties of the light curve sample. [sent-9, score-1.466]
</p><p>6 The highly non-linear character of the light curves motivates a full Bayes approach to explore the complex joint structure of the posterior. [sent-10, score-0.713]
</p><p>7 Sampling from a ~  dimensional, highly correlated joint posterior seemed intimidating to me, but I’m fortunate to have been empowered by having taken Andrew’s course at Harvard, by befriending expert practitioners in this field like Kaisey Mandel and Michael Betancourt, and by using Stan! [sent-11, score-0.208]
</p><p>8 It has allowed us to rapidly develop and test a variety of functional forms for the light curve model and strategies for optimization and regularization of the hierarchical structure. [sent-13, score-1.113]
</p><p>9 Over the course of the project, I learned to pay increasingly close attention to the stepsize, n_treedepth and n_divergent NUTS parameters, and other diagnostic information provided by Stan in order to help debug sampling issues. [sent-15, score-0.306]
</p><p>10 Encountering saturation of the treedepth and/or extremely small stepsizes often motivated simplifications of the hierarchical structure in order to reduce the curvature in the posterior. [sent-16, score-0.518]
</p><p>11 Divergences during sampling led us to apply stronger prior information on key parameters (particularly those that are exponentiated in the light curve model) in order to avoid numerical overflow on samples drawn from the tails. [sent-17, score-1.342]
</p><p>12 ”         By modeling the hierarchical structure of the supernova measurements Nathan was able to significantly improve the utilization of the data. [sent-19, score-0.341]
</p><p>13 Building and fitting this model proved to be a tremendous learning experience for both Nathan any myself. [sent-21, score-0.236]
</p><p>14 We haven’t really seen Stan applied to such deep hierarchical models before, and our first naive implementations proved to be vulnerable to all kinds of pathologies. [sent-22, score-0.339]
</p><p>15 A problem early on came in how to model hierarchical dependences  between constrained parameters. [sent-23, score-0.472]
</p><p>16 As has become a common theme,  the most successful computational strategy is to model the hierarchical dependencies on the unconstrained latent space and transform to the constrained space only when necessary. [sent-24, score-0.588]
</p><p>17 With multiple layers the parameter variances increase exponentially, and the naive generalization of a one-layer prior induces huge variances on the top-level parameters. [sent-26, score-0.426]
</p><p>18 This became especially pathological when those top-level parameters are constrained — the exponential function is very easy to overflow in floating point. [sent-27, score-0.376]
</p><p>19 Ultimately we established the desired variance on the top-level parameters and worked backwards, scaling the deeper priors by the number of groups in the next layer to ensure the desired behavior. [sent-28, score-0.338]
</p><p>20 Nathan was able to include the full model as an appendix to his paper, which you can find on the  arXiv . [sent-30, score-0.158]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('light', 0.38), ('curve', 0.261), ('stan', 0.215), ('nathan', 0.211), ('supernovae', 0.203), ('hierarchical', 0.186), ('curves', 0.172), ('model', 0.158), ('constrained', 0.128), ('parameters', 0.124), ('overflow', 0.124), ('saturation', 0.111), ('sampling', 0.107), ('structure', 0.091), ('diversity', 0.086), ('variances', 0.086), ('nuts', 0.085), ('numerical', 0.084), ('desired', 0.078), ('proved', 0.078), ('properties', 0.078), ('posterior', 0.076), ('naive', 0.075), ('failure', 0.073), ('order', 0.072), ('joint', 0.07), ('individual', 0.068), ('feature', 0.066), ('develop', 0.066), ('information', 0.065), ('modeling', 0.064), ('dataset', 0.064), ('prior', 0.063), ('us', 0.062), ('debug', 0.062), ('mandel', 0.062), ('discovers', 0.062), ('empowered', 0.062), ('inaugural', 0.062), ('capitalize', 0.062), ('pathologies', 0.062), ('induces', 0.058), ('layers', 0.058), ('telescope', 0.058), ('explosions', 0.058), ('showcases', 0.058), ('inadequate', 0.058), ('treedepth', 0.058), ('space', 0.058), ('priors', 0.058)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="2299-tfidf-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-Stan_Model_of_the_Week%3A_Hierarchical_Modeling_of_Supernovas.html">2299 andrew gelman stats-2014-04-21-Stan Model of the Week: Hierarchical Modeling of Supernovas</a></p>
<p>Introduction: The Stan Model of the Week showcases research using Stan to push the limits of applied statistics.  If you have a model that you would like to submit for a future post then send us an  email . 
 
Our inaugural post comes from Nathan Sanders, a graduate student finishing up his thesis on astrophysics at Harvard. Nathan writes,
  
“Core-collapse supernovae, the luminous explosions of massive stars, exhibit an expansive and meaningful diversity of behavior in their brightness evolution over time (their “light curves”). Our group discovers and monitors these events using the Pan-STARRS1 telescope in Hawaii, and we’ve collected a dataset of about 20,000 individual photometric observations of about 80 Type IIP supernovae, the class my work has focused on. While this dataset provides one of the best available tools to infer the explosion properties of these supernovae, due to the nature of extragalactic astronomy (observing from distances 
  1 billion light years), these light curves typicall</p><p>2 0.18932134 <a title="2299-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-21-Model_complexity_as_a_function_of_sample_size.html">1543 andrew gelman stats-2012-10-21-Model complexity as a function of sample size</a></p>
<p>Introduction: As we get more data, we can fit more model.  But at some point we become so overwhelmed by data that, for computational reasons, we can barely do anything at all.  Thus, the curve above could be thought of as the product of two curves:  a steadily increasing curve showing the  statistical  ability to fit more complex models with more data, and a steadily decreasing curve showing the  computational  feasibility of doing so.</p><p>3 0.17947638 <a title="2299-tfidf-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-14-Transitioning_to_Stan.html">2291 andrew gelman stats-2014-04-14-Transitioning to Stan</a></p>
<p>Introduction: Kevin Cartier writes:
  
I’ve been happily using R for a number of years now and recently came across Stan. Looks big and powerful, so I’d like to pick an appropriate project and try it out. I wondered if you could point me to a link or document that goes into the motivation for this tool (aside from the Stan user doc)?  What I’d like to understand is, at what point might you look at an emergent R project and advise, “You know, that thing you’re trying to do would be a whole lot easier/simpler/more straightforward to implement with Stan.” (or words to that effect).
  
My reply:  For my collaborators in political science, Stan has been most useful for models where the data set is not huge (e.g., we might have 10,000 data points or 50,000 data points but not 10 million) but where the model is somewhat complex (for example, a model with latent time series structure).  The point is that the model has enough parameters and uncertainty that you’ll want to do full Bayes (rather than some sort</p><p>4 0.15917888 <a title="2299-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-22-My_talks_that_were_scheduled_for_Tues_at_the_Data_Skeptics_meetup_and_Wed_at_the_Open_Statistical_Programming_meetup.html">1950 andrew gelman stats-2013-07-22-My talks that were scheduled for Tues at the Data Skeptics meetup and Wed at the Open Statistical Programming meetup</a></p>
<p>Introduction: Statistical Methods and Data Skepticism 
  
Data analysis today is dominated by three paradigms:  null hypothesis significance testing, Bayesian inference, and exploratory data analysis.  There is concern that all these methods lead to overconfidence on the part of researchers and the general public, and this concern has led to the new “data skepticism” movement.


But the history of statistics is already in some sense a history of data skepticism.  Concepts of bias, variance, sampling and measurement error, least-squares regression, and statistical significance can all be viewed as formalizations of data skepticism.  All these methods address the concern that patterns in observed data might not generalize to the population of interest.


We discuss the challenge of attaining data skepticism while avoiding data nihilism, and consider some proposed future directions.
  
 Stan 
  
Stan (mc-stan.org) is an open-source package for obtaining Bayesian inference using the No-U-Turn sampler, a</p><p>5 0.15843734 <a title="2299-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-07-Robust_logistic_regression.html">1886 andrew gelman stats-2013-06-07-Robust logistic regression</a></p>
<p>Introduction: Corey Yanofsky writes:
  
In your work, you’ve robustificated logistic regression by having the logit function saturate at, e.g., 0.01 and 0.99, instead of  0 and 1. Do you have any thoughts on a sensible setting for the saturation values? My intuition suggests that it has something to do with proportion of outliers expected in the data (assuming a reasonable model fit). 


It would be desirable to have them fit in the model, but my intuition is that integrability of the posterior distribution might become an issue.
  
My reply:  it should be no problem to put these saturation values in the model, I bet it would work fine in Stan if you give them uniform (0,.1) priors or something like that.  Or you could just fit the robit model.
 
And this reminds me . . . I’ve been told that when Stan’s on its optimization setting, it fits generalized linear models just about as fast as regular glm or bayesglm in R.  This suggests to me that we should have some precompiled regression models in Stan,</p><p>6 0.15241009 <a title="2299-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-19-Prior_distributions_on_derived_quantities_rather_than_on_parameters_themselves.html">1946 andrew gelman stats-2013-07-19-Prior distributions on derived quantities rather than on parameters themselves</a></p>
<p>7 0.14952128 <a title="2299-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-A_Stan_is_Born.html">1475 andrew gelman stats-2012-08-30-A Stan is Born</a></p>
<p>8 0.1426509 <a title="2299-tfidf-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-07-My_recent_debugging_experience.html">2161 andrew gelman stats-2014-01-07-My recent debugging experience</a></p>
<p>9 0.14237456 <a title="2299-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-16-Stantastic%21.html">1580 andrew gelman stats-2012-11-16-Stantastic!</a></p>
<p>10 0.14136562 <a title="2299-tfidf-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>11 0.13841948 <a title="2299-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>12 0.13721505 <a title="2299-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-23-Examples_of_the_use_of_hierarchical_modeling_to_generalize_to_new_settings.html">1425 andrew gelman stats-2012-07-23-Examples of the use of hierarchical modeling to generalize to new settings</a></p>
<p>13 0.13716824 <a title="2299-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>14 0.13225941 <a title="2299-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-27-%28R-Py-Cmd%29Stan_2.1.0.html">2150 andrew gelman stats-2013-12-27-(R-Py-Cmd)Stan 2.1.0</a></p>
<p>15 0.12760609 <a title="2299-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-04-PyStan%21.html">1748 andrew gelman stats-2013-03-04-PyStan!</a></p>
<p>16 0.12647304 <a title="2299-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>17 0.12450086 <a title="2299-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-23-Scalable_Stan.html">2035 andrew gelman stats-2013-09-23-Scalable Stan</a></p>
<p>18 0.12343058 <a title="2299-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>19 0.12265173 <a title="2299-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-29-How_many_parameters_are_in_a_multilevel_model%3F.html">1144 andrew gelman stats-2012-01-29-How many parameters are in a multilevel model?</a></p>
<p>20 0.1215027 <a title="2299-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.198), (1, 0.178), (2, -0.006), (3, 0.069), (4, 0.086), (5, 0.072), (6, 0.019), (7, -0.152), (8, -0.104), (9, 0.008), (10, -0.058), (11, 0.018), (12, -0.083), (13, -0.022), (14, -0.006), (15, -0.037), (16, 0.002), (17, 0.006), (18, 0.006), (19, 0.001), (20, -0.016), (21, -0.05), (22, -0.056), (23, -0.023), (24, -0.003), (25, 0.012), (26, -0.006), (27, -0.007), (28, 0.012), (29, 0.003), (30, -0.012), (31, -0.033), (32, -0.019), (33, -0.001), (34, -0.033), (35, 0.044), (36, 0.032), (37, -0.014), (38, 0.02), (39, -0.012), (40, 0.009), (41, -0.007), (42, 0.004), (43, -0.033), (44, 0.005), (45, -0.048), (46, -0.015), (47, -0.016), (48, 0.005), (49, -0.012)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96078908 <a title="2299-lsi-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-Stan_Model_of_the_Week%3A_Hierarchical_Modeling_of_Supernovas.html">2299 andrew gelman stats-2014-04-21-Stan Model of the Week: Hierarchical Modeling of Supernovas</a></p>
<p>Introduction: The Stan Model of the Week showcases research using Stan to push the limits of applied statistics.  If you have a model that you would like to submit for a future post then send us an  email . 
 
Our inaugural post comes from Nathan Sanders, a graduate student finishing up his thesis on astrophysics at Harvard. Nathan writes,
  
“Core-collapse supernovae, the luminous explosions of massive stars, exhibit an expansive and meaningful diversity of behavior in their brightness evolution over time (their “light curves”). Our group discovers and monitors these events using the Pan-STARRS1 telescope in Hawaii, and we’ve collected a dataset of about 20,000 individual photometric observations of about 80 Type IIP supernovae, the class my work has focused on. While this dataset provides one of the best available tools to infer the explosion properties of these supernovae, due to the nature of extragalactic astronomy (observing from distances 
  1 billion light years), these light curves typicall</p><p>2 0.88565576 <a title="2299-lsi-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-10-Stan_Model_of_the_Week%3A__PK_Calculation_of_IV_and_Oral_Dosing.html">2242 andrew gelman stats-2014-03-10-Stan Model of the Week:  PK Calculation of IV and Oral Dosing</a></p>
<p>Introduction: [Update: Revised given comments from Wingfeet, Andrew and germo.  Thanks!  I'd mistakenly translated the dlnorm priors in the first version --- amazing what a difference the priors make.  I also escaped the less-than and greater-than signs in the constraints in the model so they're visible.  I also updated to match the thin=2 output of JAGS.]
 
We’re going to be starting a Stan “model of the P” (for some time period P) column, so I thought I’d kick things off with one of my own.  I’ve been following the  Wingvoet blog , the author of which is identified only by the Blogger handle  Wingfeet ;  a couple of days ago this lovely post came out:
  
  PK calculation of IV and oral dosing in JAGS 
   
Wingfeet’s post implemented an answer to question 6 from chapter 6 of problem from Rowland and Tozer’s 2010 book,   Clinical Pharmacokinetics and Pharmacodynamics  , Fourth edition, Lippincott, Williams & Wilkins.   
 
So in the grand tradition of using this blog to procrastinate, I thought I’d t</p><p>3 0.83703244 <a title="2299-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-30-Stan_Project%3A__Continuous_Relaxations_for_Discrete_MRFs.html">2003 andrew gelman stats-2013-08-30-Stan Project:  Continuous Relaxations for Discrete MRFs</a></p>
<p>Introduction: Hamiltonian Monte Carlo (HMC), as used by  Stan , is only defined for continuous parameters.  We’d love to be able to do discrete sampling.  So I was excited when I saw this:
  

Yichuan Zhang, Charles Sutton, Amos J Storkey, and Zoubin Ghahramani.  2012.  Continuous Relaxations for Discrete Hamiltonian Monte Carlo .   NIPS  25.


 Abstract:  Continuous relaxations play an important role in discrete optimization, but have not seen much use in approximate probabilistic inference. Here we show that a general form of the Gaussian Integral Trick makes it possible to transform a wide class of discrete variable undirected models into fully continuous systems. The continuous representation allows the use of gradient-based Hamiltonian Monte Carlo for inference, results in new ways of estimating normalization constants (partition functions), and in general opens up a number of new avenues for inference in difficult discrete systems. We demonstrate some of these continuous relaxation inference a</p><p>4 0.83491629 <a title="2299-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-16-Stantastic%21.html">1580 andrew gelman stats-2012-11-16-Stantastic!</a></p>
<p>Introduction: Richard McElreath writes:
  
I’ve been translating a few ongoing data analysis projects into  Stan  code, mostly with success. The most important for me right now has been a hierarchical zero-inflated gamma problem. This a “hurdle” model, in which a bernoulli GLM produces zeros/nonzeros, and then a gamma GLM produces the nonzero values, using varying effects correlated with those in the bernoulli process.


The data are 20 years of human foraging returns from a subsistence hunting population in Paraguay (the Ache), comprising about 15k hunts in total (Hill & Kintigh. 2009. Current Anthropology 50:369-377). Observed values are kilograms of meat returned to camp. The more complex models contain a 147-by-9 matrix of varying effects (147 unique hunters), as well as imputation of missing values.


Originally, I had written the sampler myself in raw R code. It was very slow, but I knew what it was doing at least. Just before Stan version 1.0 was released, I had managed to get JAGS to do it a</p><p>5 0.83046108 <a title="2299-lsi-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-14-Transitioning_to_Stan.html">2291 andrew gelman stats-2014-04-14-Transitioning to Stan</a></p>
<p>Introduction: Kevin Cartier writes:
  
I’ve been happily using R for a number of years now and recently came across Stan. Looks big and powerful, so I’d like to pick an appropriate project and try it out. I wondered if you could point me to a link or document that goes into the motivation for this tool (aside from the Stan user doc)?  What I’d like to understand is, at what point might you look at an emergent R project and advise, “You know, that thing you’re trying to do would be a whole lot easier/simpler/more straightforward to implement with Stan.” (or words to that effect).
  
My reply:  For my collaborators in political science, Stan has been most useful for models where the data set is not huge (e.g., we might have 10,000 data points or 50,000 data points but not 10 million) but where the model is somewhat complex (for example, a model with latent time series structure).  The point is that the model has enough parameters and uncertainty that you’ll want to do full Bayes (rather than some sort</p><p>6 0.8085115 <a title="2299-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-07-Robust_logistic_regression.html">1886 andrew gelman stats-2013-06-07-Robust logistic regression</a></p>
<p>7 0.7990548 <a title="2299-lsi-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-07-My_recent_debugging_experience.html">2161 andrew gelman stats-2014-01-07-My recent debugging experience</a></p>
<p>8 0.7858777 <a title="2299-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-30-Stan_uses_Nuts%21.html">1036 andrew gelman stats-2011-11-30-Stan uses Nuts!</a></p>
<p>9 0.78450525 <a title="2299-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-12-Samplers_for_Big_Science%3A__emcee_and_BAT.html">2020 andrew gelman stats-2013-09-12-Samplers for Big Science:  emcee and BAT</a></p>
<p>10 0.77386296 <a title="2299-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-06-Stan_1.2.0_and_RStan_1.2.0.html">1753 andrew gelman stats-2013-03-06-Stan 1.2.0 and RStan 1.2.0</a></p>
<p>11 0.77023518 <a title="2299-lsi-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>12 0.76775479 <a title="2299-lsi-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-Running_into_a_Stan_Reference_by_Accident.html">2231 andrew gelman stats-2014-03-03-Running into a Stan Reference by Accident</a></p>
<p>13 0.75071359 <a title="2299-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-23-Scalable_Stan.html">2035 andrew gelman stats-2013-09-23-Scalable Stan</a></p>
<p>14 0.73968947 <a title="2299-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-Stan_is_fast.html">1476 andrew gelman stats-2012-08-30-Stan is fast</a></p>
<p>15 0.72997272 <a title="2299-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-22-A_Bayesian_model_for_an_increasing_function%2C_in_Stan%21.html">2110 andrew gelman stats-2013-11-22-A Bayesian model for an increasing function, in Stan!</a></p>
<p>16 0.72934014 <a title="2299-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-06-The_new_Stan_1.1.1%2C_featuring_Gaussian_processes%21.html">1710 andrew gelman stats-2013-02-06-The new Stan 1.1.1, featuring Gaussian processes!</a></p>
<p>17 0.72718525 <a title="2299-lsi-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-04-Stan_%28%26_JAGS%29_Tutorial_on_Linear_Mixed_Models.html">2318 andrew gelman stats-2014-05-04-Stan (& JAGS) Tutorial on Linear Mixed Models</a></p>
<p>18 0.72404599 <a title="2299-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-27-%28R-Py-Cmd%29Stan_2.1.0.html">2150 andrew gelman stats-2013-12-27-(R-Py-Cmd)Stan 2.1.0</a></p>
<p>19 0.71429282 <a title="2299-lsi-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-20-Mailing_List_Degree-of-Difficulty_Difficulty.html">2178 andrew gelman stats-2014-01-20-Mailing List Degree-of-Difficulty Difficulty</a></p>
<p>20 0.71160215 <a title="2299-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-21-BDA3_table_of_contents_%28also_a_new_paper_on_visualization%29.html">1991 andrew gelman stats-2013-08-21-BDA3 table of contents (also a new paper on visualization)</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.019), (6, 0.029), (9, 0.018), (15, 0.047), (16, 0.079), (21, 0.016), (24, 0.162), (41, 0.02), (43, 0.011), (51, 0.023), (52, 0.014), (57, 0.031), (58, 0.012), (66, 0.011), (77, 0.02), (82, 0.031), (84, 0.052), (86, 0.066), (98, 0.018), (99, 0.217)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97518051 <a title="2299-lda-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-Stan_Model_of_the_Week%3A_Hierarchical_Modeling_of_Supernovas.html">2299 andrew gelman stats-2014-04-21-Stan Model of the Week: Hierarchical Modeling of Supernovas</a></p>
<p>Introduction: The Stan Model of the Week showcases research using Stan to push the limits of applied statistics.  If you have a model that you would like to submit for a future post then send us an  email . 
 
Our inaugural post comes from Nathan Sanders, a graduate student finishing up his thesis on astrophysics at Harvard. Nathan writes,
  
“Core-collapse supernovae, the luminous explosions of massive stars, exhibit an expansive and meaningful diversity of behavior in their brightness evolution over time (their “light curves”). Our group discovers and monitors these events using the Pan-STARRS1 telescope in Hawaii, and we’ve collected a dataset of about 20,000 individual photometric observations of about 80 Type IIP supernovae, the class my work has focused on. While this dataset provides one of the best available tools to infer the explosion properties of these supernovae, due to the nature of extragalactic astronomy (observing from distances 
  1 billion light years), these light curves typicall</p><p>2 0.94547784 <a title="2299-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>Introduction: This article  is a discussion of a  paper  by Greg Francis for a special issue, edited by E. J. Wagenmakers, of the Journal of Mathematical Psychology.  Here’s what I wrote:
  
Much of statistical practice is an effort to reduce or deny variation and uncertainty. The reduction is done through standardization, replication, and other practices of experimental design, with the idea being to isolate and stabilize the quantity being estimated and then average over many cases. Even so, however, uncertainty persists, and statistical hypothesis testing is in many ways an endeavor to deny this, by reporting binary accept/reject decisions.


Classical statistical methods produce binary statements, but there is no reason to assume that the world works that way. Expressions such as Type 1 error, Type 2 error, false positive, and so on, are based on a model in which the world is divided into real and non-real effects. To put it another way, I understand the general scientific distinction of real vs</p><p>3 0.94228154 <a title="2299-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-01-Post-publication_peer_review%3A__How_it_%28sometimes%29_really_works.html">2004 andrew gelman stats-2013-09-01-Post-publication peer review:  How it (sometimes) really works</a></p>
<p>Introduction: In an ideal world, research articles would be open to criticism and discussion in the same place where they are published, in a sort of non-corrupt version of Yelp.  What is happening now is that the occasional paper or research area gets lots of press coverage, and this inspires reactions on science-focused blogs.  The trouble here is that it’s easier to give off-the-cuff comments than detailed criticisms.
 
Here’s an example.  It starts a couple years ago with  this article  by Ryota Kanai, Tom Feilden, Colin Firth, and Geraint Rees, on brain size and political orientation:
  
In a large sample of young adults, we related self-reported political attitudes to gray matter volume using structural MRI. We found that greater liberalism was associated with increased gray matter volume in the anterior cingulate cortex, whereas greater conservatism was associated with increased volume of the right amygdala. These results were replicated in an independent sample of additional participants. Ou</p><p>4 0.94183058 <a title="2299-lda-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-19-Updated_solutions_to_Bayesian_Data_Analysis_homeworks.html">42 andrew gelman stats-2010-05-19-Updated solutions to Bayesian Data Analysis homeworks</a></p>
<p>Introduction: Here are  solutions to about 50 of the exercises from Bayesian Data Analysis.  The solutions themselves haven’t been updated; I just cleaned up the file:  some change in Latex had resulted in much of the computer code running off the page, so I went in and cleaned up the files.
 
I wrote most of these in 1996, and I like them a lot.  I think several of them would’ve made good journal articles, and in retrospect I wish I’d published them as such.  Original material that appears first in a book (or, even worse, in homework solutions) can easily be overlooked.</p><p>5 0.93827665 <a title="2299-lda-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-25-Basketball_Stats%3A__Don%E2%80%99t_model_the_probability_of_win%2C_model_the_expected_score_differential..html">2224 andrew gelman stats-2014-02-25-Basketball Stats:  Don’t model the probability of win, model the expected score differential.</a></p>
<p>Introduction: Someone who wants to remain anonymous writes:
  
I am working to create a more accurate in-game win probability model for basketball games. My idea is for each timestep in a game (a second, 5 seconds, etc), use the Vegas line, the current score differential, who has the ball, and the number of possessions played already (to account for differences in pace) to create a point estimate probability of the home team winning.


This problem would seem to fit a multi-level model structure well. It seems silly to estimate 2,000 regressions (one for each timestep), but the coefficients should vary at each timestep. Do you have suggestions for what type of model this could/would be? Additionally, I believe this needs to be some form of logit/probit given the binary dependent variable (win or loss).


Finally, do you have suggestions for what package could accomplish this in Stata or R?
  
To answer the questions in reverse order: 
3.  I’d hope this could be done in Stan (which can be run from R)</p><p>6 0.93818951 <a title="2299-lda-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-06-Bootstrap_averaging%3A_Examples_where_it_works_and_where_it_doesn%E2%80%99t_work.html">2201 andrew gelman stats-2014-02-06-Bootstrap averaging: Examples where it works and where it doesn’t work</a></p>
<p>7 0.93693185 <a title="2299-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>8 0.93589157 <a title="2299-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-13-Test_scores_and_grades_predict_job_performance_%28but_maybe_not_at_Google%29.html">1980 andrew gelman stats-2013-08-13-Test scores and grades predict job performance (but maybe not at Google)</a></p>
<p>9 0.93587804 <a title="2299-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-31-Type_S_error_rates_for_classical_and_Bayesian_single_and_multiple_comparison_procedures.html">494 andrew gelman stats-2010-12-31-Type S error rates for classical and Bayesian single and multiple comparison procedures</a></p>
<p>10 0.93438792 <a title="2299-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-24-Textbook_for_data_visualization%3F.html">1637 andrew gelman stats-2012-12-24-Textbook for data visualization?</a></p>
<p>11 0.93396449 <a title="2299-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-05-Question_26_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1367 andrew gelman stats-2012-06-05-Question 26 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>12 0.93386602 <a title="2299-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-06-Josh_Tenenbaum_presents_._._._a_model_of_folk_physics%21.html">994 andrew gelman stats-2011-11-06-Josh Tenenbaum presents . . . a model of folk physics!</a></p>
<p>13 0.93351769 <a title="2299-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-19-Validation_of_Software_for_Bayesian_Models_Using_Posterior_Quantiles.html">1019 andrew gelman stats-2011-11-19-Validation of Software for Bayesian Models Using Posterior Quantiles</a></p>
<p>14 0.93308204 <a title="2299-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>15 0.93307906 <a title="2299-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-The_statistical_significance_filter.html">899 andrew gelman stats-2011-09-10-The statistical significance filter</a></p>
<p>16 0.93304467 <a title="2299-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>17 0.93274355 <a title="2299-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-05-Update_on_state_size_and_governors%E2%80%99_popularity.html">187 andrew gelman stats-2010-08-05-Update on state size and governors’ popularity</a></p>
<p>18 0.9326247 <a title="2299-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-30-Stan_uses_Nuts%21.html">1036 andrew gelman stats-2011-11-30-Stan uses Nuts!</a></p>
<p>19 0.93247843 <a title="2299-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-08-A_Bayesian_approach_for_peer-review_panels%3F__and_a_speculation_about_Bruno_Frey.html">2055 andrew gelman stats-2013-10-08-A Bayesian approach for peer-review panels?  and a speculation about Bruno Frey</a></p>
<p>20 0.93135792 <a title="2299-lda-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-31-What_is_a_Bayesian%3F.html">1438 andrew gelman stats-2012-07-31-What is a Bayesian?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
