<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2305 andrew gelman stats-2014-04-25-Revised statistical standards for evidence (comments to Val Johnson’s comments on our comments on Val’s comments on p-values)</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2014" href="../home/andrew_gelman_stats-2014_home.html">andrew_gelman_stats-2014</a> <a title="andrew_gelman_stats-2014-2305" href="#">andrew_gelman_stats-2014-2305</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2305 andrew gelman stats-2014-04-25-Revised statistical standards for evidence (comments to Val Johnson’s comments on our comments on Val’s comments on p-values)</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2014-2305-html" href="http://andrewgelman.com/2014/04/25/revised-statistical-standards-evidence-comments-val-johnsons-comments-comments-vals-comments-p-values/">html</a></p><p>Introduction: As regular readers of this blog are aware, a few months ago Val Johnson  published  an article, “Revised standards for statistical evidence,” making a Bayesian argument that researchers and journals should use a p=0.005 publication threshold rather than the usual p=0.05.
 
Christian Robert and I were unconvinced by Val’s reasoning and wrote a  response , “Revised evidence for statistical standards,” in which we wrote:
  
Johnson’s minimax prior is not intended to correspond to any distribution of effect sizes; rather, it represents a worst case scenario under some mathematical assumptions. Minimax and tradeoffs do well together, and it is hard for us to see how any worst case procedure can supply much guidance on how to balance between two different losses. . . .


We would argue that the appropriate significance level depends on the scenario and that what worked well for agricultural experiments in the 1920s might not be so appropriate for many applications in modern biosciences . . .</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 As regular readers of this blog are aware, a few months ago Val Johnson  published  an article, “Revised standards for statistical evidence,” making a Bayesian argument that researchers and journals should use a p=0. [sent-1, score-0.291]
</p><p>2 We would argue that the appropriate significance level depends on the scenario and that what worked well for agricultural experiments in the 1920s might not be so appropriate for many applications in modern biosciences . [sent-9, score-0.257]
</p><p>3 “) and Luis Pericchi, Carlos Pereira, and María-Eglée Pérez (“ Adaptive revised standards for statistical evidence “), along with  Johnson’s reply  to all of us. [sent-13, score-0.385]
</p><p>4 We both feel strongly dissatisfied with the existing default approach of scientific publication in which (a) statistical significance at the p=0. [sent-15, score-0.401]
</p><p>5 Val’s approach is to apply a minimax argument leading to a more stringent p-value threshold, whereas I’d be more interested in not using p-values (or related quantities such as Bayes factors) as publication thresholds at all. [sent-18, score-0.727]
</p><p>6 And I think we also agree that thresholds for evidence should depend on scientific context. [sent-20, score-0.441]
</p><p>7 ”  Again, I don’t like using any p-value threshold but I agree with Val that the current p=0. [sent-23, score-0.233]
</p><p>8 05 statistical significance was a joke, but (b) maybe their claims should still be published, despite their inconclusive nature, because of the importance of the topic). [sent-28, score-0.159]
</p><p>9 In most hypothesis tests, unique and well-defined loss functions and prior densities do not exist. [sent-34, score-0.481]
</p><p>10 Instead, a plethora of vaguely defined loss functions and prior densities exist. [sent-35, score-0.549]
</p><p>11 Thousands of scientific manuscripts are written each year, and eliciting these distinct loss functions and priors on a case-by-case basis, and determining how to combine them, is simply not feasible. [sent-39, score-0.611]
</p><p>12 Just to be clear here:  Christian and I nowhere used the term “subjective” in our letter, and indeed I do not consider our reference to decision analysis to be subjective, at least not any more subjective than the choice of a probability of 1/20 that drives Val’s calculations. [sent-43, score-0.26]
</p><p>13 The 1/20 level is objective only in the sociological sense that it represents a scientific tradition. [sent-44, score-0.21]
</p><p>14 Val’s second point is that well-defined loss functions are difficult to achieve. [sent-45, score-0.347]
</p><p>15 I agree, and indeed in my own work I have rarely worked with formal loss functions or performed formal decision analyses. [sent-46, score-0.564]
</p><p>16 He is trying to set a universal threshold for statistical significance. [sent-49, score-0.285]
</p><p>17 Finally, Val writes of the difficulty of eliciting loss functions and priors for the “thousands of scientific manuscripts [that] are written each year. [sent-51, score-0.611]
</p><p>18 Val also replies to one of our specific comments in this way:    The characterization of uniformly most powerful Bayesian tests (UMPBTs) as minimax procedures is inaccurate. [sent-54, score-0.347]
</p><p>19 Minimax procedures are defined by minimizing the maximum loss that a decision maker can suffer. [sent-55, score-0.371]
</p><p>20 In any case, I stand by what Christian and I wrote earlier (setting aside the particular word “minimax”) that we can’t see it making sense to work with a worst-case probability that, in this case, does not correspond to any sensible prior distribution. [sent-58, score-0.253]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('val', 0.633), ('minimax', 0.294), ('loss', 0.188), ('functions', 0.159), ('thresholds', 0.152), ('johnson', 0.15), ('threshold', 0.148), ('christian', 0.146), ('evidence', 0.108), ('standards', 0.107), ('subjective', 0.105), ('scientific', 0.096), ('revised', 0.096), ('umpbts', 0.096), ('correspond', 0.094), ('eliciting', 0.087), ('significance', 0.085), ('agree', 0.085), ('publication', 0.085), ('stringent', 0.081), ('manuscripts', 0.081), ('statistical', 0.074), ('densities', 0.069), ('defined', 0.068), ('prior', 0.065), ('universal', 0.063), ('level', 0.063), ('decision', 0.062), ('scenario', 0.061), ('approach', 0.061), ('published', 0.056), ('worst', 0.055), ('argument', 0.054), ('formal', 0.054), ('letter', 0.053), ('procedures', 0.053), ('thousands', 0.051), ('represents', 0.051), ('bayesian', 0.05), ('wrote', 0.048), ('biosciences', 0.048), ('coal', 0.048), ('carlos', 0.048), ('settings', 0.048), ('indeed', 0.047), ('robert', 0.047), ('probability', 0.046), ('disagree', 0.046), ('luis', 0.045), ('elicit', 0.045)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="2305-tfidf-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-25-Revised_statistical_standards_for_evidence_%28comments_to_Val_Johnson%E2%80%99s_comments_on_our_comments_on_Val%E2%80%99s_comments_on_p-values%29.html">2305 andrew gelman stats-2014-04-25-Revised statistical standards for evidence (comments to Val Johnson’s comments on our comments on Val’s comments on p-values)</a></p>
<p>Introduction: As regular readers of this blog are aware, a few months ago Val Johnson  published  an article, “Revised standards for statistical evidence,” making a Bayesian argument that researchers and journals should use a p=0.005 publication threshold rather than the usual p=0.05.
 
Christian Robert and I were unconvinced by Val’s reasoning and wrote a  response , “Revised evidence for statistical standards,” in which we wrote:
  
Johnson’s minimax prior is not intended to correspond to any distribution of effect sizes; rather, it represents a worst case scenario under some mathematical assumptions. Minimax and tradeoffs do well together, and it is hard for us to see how any worst case procedure can supply much guidance on how to balance between two different losses. . . .


We would argue that the appropriate significance level depends on the scenario and that what worked well for agricultural experiments in the 1920s might not be so appropriate for many applications in modern biosciences . . .</p><p>2 0.53213775 <a title="2305-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>Introduction: In response to the  discussion  of X and me of his recent  paper , Val Johnson writes:
  
I would like to thank Andrew for forwarding his comments on uniformly most powerful Bayesian tests (UMPBTs) to me and his invitation to respond to them.  I think he  (and also Christian Robert) raise a number of interesting points concerning this new class of Bayesian tests, but I think that they may have confounded several issues that might more usefully be examined separately.


The first issue involves the choice of the Bayesian evidence threshold, gamma, used in rejecting a null hypothesis in favor of an alternative hypothesis.  Andrew objects to the higher values of gamma proposed in my recent PNAS article on grounds that too many important scientific effects would be missed if thresholds of 25-50 were routinely used.  These evidence thresholds correspond roughly to p-values of 0.005; Andrew suggests that evidence thresholds around 5 should continue to be used (gamma=5 corresponds approximate</p><p>3 0.49864289 <a title="2305-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-19-Revised_evidence_for_statistical_standards.html">2140 andrew gelman stats-2013-12-19-Revised evidence for statistical standards</a></p>
<p>Introduction: X  and I heard about  this  much-publicized recent paper by Val Johnson, who suggests changing the default level of statistical significance from z=2 to z=3 (or, as he puts it, going from p=.05 to p=.005 or .001).  Val argues that you need to go out to 3 standard errors to get a Bayes factor of 25 or 50 in favor of the alternative hypothesis.  I don’t really buy this, first because Val’s model is a weird (to me) mixture of two point masses, which he creates in order to make a minimax argument, and second because I don’t see why you need a Bayes factor of 25 to 50 in order to make a claim.  I’d think that a factor of 5:1, say, provides strong information already—if you really believe those odds.  The real issue, as I see it, is that we’re getting Bayes factors and posterior probabilities we don’t believe, because we’re assuming flat priors that don’t really make sense.  This is a topic that’s come up over and over in recent months on this blog, for example in this discussion of why I  d</p><p>4 0.3315959 <a title="2305-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-15-Wacky_priors_can_work_well%3F.html">1723 andrew gelman stats-2013-02-15-Wacky priors can work well?</a></p>
<p>Introduction: Dave Judkins writes:
  
I would love to see a blog entry on  this article , Bayesian Model Selection in High-Dimensional Settings, by Valen Johnson and David Rossell.  The simulation results are very encouraging although the choice of colors for some of the graphics is unfortunate.  Unless I am colorblind in some way that I am unaware of, they have two thin charcoal lines that are indistinguishable.
  
When Dave Judkins puts in a request, I’ll respond.  Also, I’m always happy to see a new Val Johnson paper.  Val and I are contemporaries—he and I got our PhD’s at around the same time, with both of us working on Bayesian image reconstruction, then in the early 1990s Val was part of the legendary group at Duke’s Institute of Statistics and Decision Sciences—a veritable ’27 Yankees featuring Mike West, Merlise Clyde, Michael Lavine, Dave Higdon, Peter Mueller, Val, and a bunch of others.  I always thought it was too bad they all had to go their separate ways.
 
Val also wrote two classic p</p><p>5 0.18177341 <a title="2305-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-21-More_on_Bayesian_model_selection_in_high-dimensional_settings.html">1817 andrew gelman stats-2013-04-21-More on Bayesian model selection in high-dimensional settings</a></p>
<p>Introduction: David Rossell writes:
  
A friend pointed out that you were having an interesting philosophical  discussion  on my paper with Val Johnson [on Bayesian model selection in high-dimensional settings].


I agree with the view that in almost all practical situations the true model is not in the set under consideration. Still, asking a model choice procedure to be able to pick up the correct model when it is in the set under consideration seems a minimal requirement (though perhaps not sufficient). In other words, if a procedure is unable to pick the data-generating model even when it is one of the models under consideration, I don’t have high hopes for it working well in more realistic scenarios either. 


Most results in the history in statistics seem to have been obtained under an assumed model, e.g. why even do MLE or penalized-likelihood if we don’t trust the model. While unrealistic, these results were useful to help understand important basic principles. In our case Val and I are defe</p><p>6 0.14446148 <a title="2305-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-18-What_is_this%2C_a_statistics_class_or_a_dentist%E2%80%99s_office%3F%3F.html">579 andrew gelman stats-2011-02-18-What is this, a statistics class or a dentist’s office??</a></p>
<p>7 0.13918489 <a title="2305-tfidf-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Ken_Rice_presents_a_unifying_approach_to_statistical_inference_and_hypothesis_testing.html">2312 andrew gelman stats-2014-04-29-Ken Rice presents a unifying approach to statistical inference and hypothesis testing</a></p>
<p>8 0.13101578 <a title="2305-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>9 0.12263363 <a title="2305-tfidf-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-09-Using_ranks_as_numbers.html">136 andrew gelman stats-2010-07-09-Using ranks as numbers</a></p>
<p>10 0.12198181 <a title="2305-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>11 0.12129852 <a title="2305-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-08-Is_linear_regression_unethical_in_that_it_gives_more_weight_to_cases_that_are_far_from_the_average%3F.html">1409 andrew gelman stats-2012-07-08-Is linear regression unethical in that it gives more weight to cases that are far from the average?</a></p>
<p>12 0.12065599 <a title="2305-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>13 0.11912242 <a title="2305-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-11-Hey%2C_good_news%21__Your_p-value_just_passed_the_0.05_threshold%21.html">758 andrew gelman stats-2011-06-11-Hey, good news!  Your p-value just passed the 0.05 threshold!</a></p>
<p>14 0.11903699 <a title="2305-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-29-More_by_Berger_and_me_on_weakly_informative_priors.html">1092 andrew gelman stats-2011-12-29-More by Berger and me on weakly informative priors</a></p>
<p>15 0.11641996 <a title="2305-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-22-Grade_inflation%3A_why_weren%E2%80%99t_the_instructors_all_giving_all_A%E2%80%99s_already%3F%3F.html">1507 andrew gelman stats-2012-09-22-Grade inflation: why weren’t the instructors all giving all A’s already??</a></p>
<p>16 0.11381974 <a title="2305-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-03-Statistical_methods_that_work_in_some_settings_but_not_others.html">1560 andrew gelman stats-2012-11-03-Statistical methods that work in some settings but not others</a></p>
<p>17 0.11236472 <a title="2305-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>18 0.10959546 <a title="2305-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-27-Grade_inflation%3A__why_weren%E2%80%99t_the_instructors_all_giving_all_A%E2%80%99s_already%3F%3F.html">825 andrew gelman stats-2011-07-27-Grade inflation:  why weren’t the instructors all giving all A’s already??</a></p>
<p>19 0.10851053 <a title="2305-tfidf-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-25-Classics_of_statistics.html">109 andrew gelman stats-2010-06-25-Classics of statistics</a></p>
<p>20 0.10777583 <a title="2305-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-08-Statistical_significance_and_the_dangerous_lure_of_certainty.html">1974 andrew gelman stats-2013-08-08-Statistical significance and the dangerous lure of certainty</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.189), (1, 0.096), (2, -0.033), (3, -0.052), (4, -0.103), (5, -0.074), (6, 0.031), (7, 0.049), (8, -0.077), (9, -0.042), (10, -0.055), (11, -0.003), (12, 0.004), (13, -0.033), (14, 0.013), (15, -0.023), (16, -0.005), (17, 0.012), (18, -0.017), (19, 0.008), (20, 0.021), (21, 0.038), (22, -0.058), (23, 0.002), (24, -0.05), (25, -0.006), (26, 0.075), (27, 0.004), (28, 0.03), (29, 0.004), (30, -0.01), (31, 0.034), (32, 0.04), (33, 0.053), (34, -0.05), (35, -0.047), (36, 0.048), (37, 0.001), (38, 0.052), (39, 0.027), (40, -0.101), (41, -0.08), (42, -0.004), (43, 0.067), (44, 0.008), (45, 0.016), (46, -0.054), (47, -0.033), (48, -0.008), (49, 0.017)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9472937 <a title="2305-lsi-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-25-Revised_statistical_standards_for_evidence_%28comments_to_Val_Johnson%E2%80%99s_comments_on_our_comments_on_Val%E2%80%99s_comments_on_p-values%29.html">2305 andrew gelman stats-2014-04-25-Revised statistical standards for evidence (comments to Val Johnson’s comments on our comments on Val’s comments on p-values)</a></p>
<p>Introduction: As regular readers of this blog are aware, a few months ago Val Johnson  published  an article, “Revised standards for statistical evidence,” making a Bayesian argument that researchers and journals should use a p=0.005 publication threshold rather than the usual p=0.05.
 
Christian Robert and I were unconvinced by Val’s reasoning and wrote a  response , “Revised evidence for statistical standards,” in which we wrote:
  
Johnson’s minimax prior is not intended to correspond to any distribution of effect sizes; rather, it represents a worst case scenario under some mathematical assumptions. Minimax and tradeoffs do well together, and it is hard for us to see how any worst case procedure can supply much guidance on how to balance between two different losses. . . .


We would argue that the appropriate significance level depends on the scenario and that what worked well for agricultural experiments in the 1920s might not be so appropriate for many applications in modern biosciences . . .</p><p>2 0.89380229 <a title="2305-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>Introduction: In response to the  discussion  of X and me of his recent  paper , Val Johnson writes:
  
I would like to thank Andrew for forwarding his comments on uniformly most powerful Bayesian tests (UMPBTs) to me and his invitation to respond to them.  I think he  (and also Christian Robert) raise a number of interesting points concerning this new class of Bayesian tests, but I think that they may have confounded several issues that might more usefully be examined separately.


The first issue involves the choice of the Bayesian evidence threshold, gamma, used in rejecting a null hypothesis in favor of an alternative hypothesis.  Andrew objects to the higher values of gamma proposed in my recent PNAS article on grounds that too many important scientific effects would be missed if thresholds of 25-50 were routinely used.  These evidence thresholds correspond roughly to p-values of 0.005; Andrew suggests that evidence thresholds around 5 should continue to be used (gamma=5 corresponds approximate</p><p>3 0.81922531 <a title="2305-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-19-Revised_evidence_for_statistical_standards.html">2140 andrew gelman stats-2013-12-19-Revised evidence for statistical standards</a></p>
<p>Introduction: X  and I heard about  this  much-publicized recent paper by Val Johnson, who suggests changing the default level of statistical significance from z=2 to z=3 (or, as he puts it, going from p=.05 to p=.005 or .001).  Val argues that you need to go out to 3 standard errors to get a Bayes factor of 25 or 50 in favor of the alternative hypothesis.  I don’t really buy this, first because Val’s model is a weird (to me) mixture of two point masses, which he creates in order to make a minimax argument, and second because I don’t see why you need a Bayes factor of 25 to 50 in order to make a claim.  I’d think that a factor of 5:1, say, provides strong information already—if you really believe those odds.  The real issue, as I see it, is that we’re getting Bayes factors and posterior probabilities we don’t believe, because we’re assuming flat priors that don’t really make sense.  This is a topic that’s come up over and over in recent months on this blog, for example in this discussion of why I  d</p><p>4 0.74187237 <a title="2305-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-31-Lindley%E2%80%99s_paradox.html">1355 andrew gelman stats-2012-05-31-Lindley’s paradox</a></p>
<p>Introduction: Sam Seaver writes:
  
I [Seaver] happened to be reading an ironic  article  by Karl Friston when I learned something new about frequentist vs bayesian, namely Lindley’s paradox, on page 12.  The text is as follows:

 
So why are we worried about trivial effects? They are important because the probability that the true effect size is exactly zero is itself zero and could cause us to reject the null hypothesis inappropriately. This is a fallacy of classical inference and is not unrelated to Lindley’s paradox (Lindley 1957). Lindley’s paradox describes a counterintuitive situation in which Bayesian and frequentist approaches to hypothesis testing give opposite results. It occurs when; (i) a result is significant by a frequentist test, indicating sufficient evidence to reject the null hypothesis d=0 and (ii) priors render the posterior probability of d=0 high, indicating strong evidence that the null hypothesis is true. In his original 
treatment, Lindley (1957) showed that – under a parti</p><p>5 0.73761678 <a title="2305-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-08-P-values_and_statistical_practice.html">1713 andrew gelman stats-2013-02-08-P-values and statistical practice</a></p>
<p>Introduction: From  my new article  in the journal Epidemiology:
  
Sander Greenland and Charles Poole accept that P values are here to stay but recognize that some of their most common interpretations have problems. The casual view of the P value as posterior probability of the truth of the null hypothesis is false and not even close to valid under any reasonable model, yet this misunderstanding persists even in high-stakes settings (as discussed, for example, by Greenland in 2011). The formal view of the P value as a probability conditional on the null is mathematically correct but typically irrelevant to research goals (hence, the popularity of alternative—if wrong—interpretations). A Bayesian interpretation based on a spike-and-slab model makes little sense in applied contexts in epidemiology, political science, and other fields in which true effects are typically nonzero and bounded (thus violating both the “spike” and the “slab” parts of the model).


I find Greenland and Poole’s perspective t</p><p>6 0.73347443 <a title="2305-lsi-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Ken_Rice_presents_a_unifying_approach_to_statistical_inference_and_hypothesis_testing.html">2312 andrew gelman stats-2014-04-29-Ken Rice presents a unifying approach to statistical inference and hypothesis testing</a></p>
<p>7 0.71673447 <a title="2305-lsi-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-10-Bayes_jumps_the_shark.html">331 andrew gelman stats-2010-10-10-Bayes jumps the shark</a></p>
<p>8 0.69662559 <a title="2305-lsi-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-03-Disconnect_between_drug_and_medical_device_approval.html">314 andrew gelman stats-2010-10-03-Disconnect between drug and medical device approval</a></p>
<p>9 0.69366974 <a title="2305-lsi-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>10 0.67455351 <a title="2305-lsi-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-18-One-tailed_or_two-tailed%3F.html">2295 andrew gelman stats-2014-04-18-One-tailed or two-tailed?</a></p>
<p>11 0.67190629 <a title="2305-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>12 0.65605623 <a title="2305-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-26-%E2%80%9CA_Vast_Graveyard_of_Undead_Theories%3A__Publication_Bias_and_Psychological_Science%E2%80%99s_Aversion_to_the_Null%E2%80%9D.html">1826 andrew gelman stats-2013-04-26-“A Vast Graveyard of Undead Theories:  Publication Bias and Psychological Science’s Aversion to the Null”</a></p>
<p>13 0.65275377 <a title="2305-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-02-So-called_Bayesian_hypothesis_testing_is_just_as_bad_as_regular_hypothesis_testing.html">643 andrew gelman stats-2011-04-02-So-called Bayesian hypothesis testing is just as bad as regular hypothesis testing</a></p>
<p>14 0.64004922 <a title="2305-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-26-%E2%80%9CThe_Bayesian_approach_to_forensic_evidence%E2%80%9D.html">2078 andrew gelman stats-2013-10-26-“The Bayesian approach to forensic evidence”</a></p>
<p>15 0.63162559 <a title="2305-lsi-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-23-Discussion_on_preregistration_of_research_studies.html">2183 andrew gelman stats-2014-01-23-Discussion on preregistration of research studies</a></p>
<p>16 0.60624105 <a title="2305-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-08-The_virtues_of_incoherence%3F.html">792 andrew gelman stats-2011-07-08-The virtues of incoherence?</a></p>
<p>17 0.60598189 <a title="2305-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-15-%E2%80%9CAre_all_significant_p-values_created_equal%3F%E2%80%9D.html">2102 andrew gelman stats-2013-11-15-“Are all significant p-values created equal?”</a></p>
<p>18 0.60344476 <a title="2305-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-20-Non-statistical_thinking_in_the_US_foreign_policy_establishment.html">721 andrew gelman stats-2011-05-20-Non-statistical thinking in the US foreign policy establishment</a></p>
<p>19 0.60259545 <a title="2305-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-12-Thinking_like_a_statistician_%28continuously%29_rather_than_like_a_civilian_%28discretely%29.html">1575 andrew gelman stats-2012-11-12-Thinking like a statistician (continuously) rather than like a civilian (discretely)</a></p>
<p>20 0.60251701 <a title="2305-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-10-I_don%E2%80%99t_like_this_cartoon.html">1572 andrew gelman stats-2012-11-10-I don’t like this cartoon</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.011), (9, 0.029), (15, 0.025), (16, 0.079), (21, 0.026), (24, 0.203), (34, 0.073), (49, 0.03), (53, 0.063), (55, 0.018), (82, 0.014), (84, 0.02), (86, 0.023), (93, 0.025), (99, 0.227)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97065544 <a title="2305-lda-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-25-Revised_statistical_standards_for_evidence_%28comments_to_Val_Johnson%E2%80%99s_comments_on_our_comments_on_Val%E2%80%99s_comments_on_p-values%29.html">2305 andrew gelman stats-2014-04-25-Revised statistical standards for evidence (comments to Val Johnson’s comments on our comments on Val’s comments on p-values)</a></p>
<p>Introduction: As regular readers of this blog are aware, a few months ago Val Johnson  published  an article, “Revised standards for statistical evidence,” making a Bayesian argument that researchers and journals should use a p=0.005 publication threshold rather than the usual p=0.05.
 
Christian Robert and I were unconvinced by Val’s reasoning and wrote a  response , “Revised evidence for statistical standards,” in which we wrote:
  
Johnson’s minimax prior is not intended to correspond to any distribution of effect sizes; rather, it represents a worst case scenario under some mathematical assumptions. Minimax and tradeoffs do well together, and it is hard for us to see how any worst case procedure can supply much guidance on how to balance between two different losses. . . .


We would argue that the appropriate significance level depends on the scenario and that what worked well for agricultural experiments in the 1920s might not be so appropriate for many applications in modern biosciences . . .</p><p>2 0.95778346 <a title="2305-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-02-Covariate_Adjustment_in_RCT_-_Model_Overfitting_in_Multilevel_Regression.html">936 andrew gelman stats-2011-10-02-Covariate Adjustment in RCT - Model Overfitting in Multilevel Regression</a></p>
<p>Introduction: Makoto Hanita writes:
  
We have been discussing the following two issues amongst ourselves, then with our methodological consultant for several days. However, we have not been able to arrive at a consensus. Consequently, we decided to seek an opinion from nationally known experts. FYI, we sent a similar inquiry to Larry Hedges and David Rogosa . . .  


1)      We are wondering if a post-hoc covariate adjustment is a good practice in the context of RCTs [randomized clinical trials]. We have a situation where we found a significant baseline difference between the treatment and the control groups in 3 variables. Some of us argue that adding those three variables to the original impact analysis model is a good idea, as that would remove the confound from the impact estimate. Others among us, on the other hand, argue that a post-hoc covariate adjustment should never be done, on the ground that those covariates are correlated with the treatment, which makes the analysis model that of quasi</p><p>3 0.9542594 <a title="2305-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>Introduction: In response to the  discussion  of X and me of his recent  paper , Val Johnson writes:
  
I would like to thank Andrew for forwarding his comments on uniformly most powerful Bayesian tests (UMPBTs) to me and his invitation to respond to them.  I think he  (and also Christian Robert) raise a number of interesting points concerning this new class of Bayesian tests, but I think that they may have confounded several issues that might more usefully be examined separately.


The first issue involves the choice of the Bayesian evidence threshold, gamma, used in rejecting a null hypothesis in favor of an alternative hypothesis.  Andrew objects to the higher values of gamma proposed in my recent PNAS article on grounds that too many important scientific effects would be missed if thresholds of 25-50 were routinely used.  These evidence thresholds correspond roughly to p-values of 0.005; Andrew suggests that evidence thresholds around 5 should continue to be used (gamma=5 corresponds approximate</p><p>4 0.95062524 <a title="2305-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-31-Using_sample_size_in_the_prior_distribution.html">547 andrew gelman stats-2011-01-31-Using sample size in the prior distribution</a></p>
<p>Introduction: Mike McLaughlin writes:
  
Consider the Seeds example in vol. 1 of the BUGS examples.  There, a binomial likelihood has a p parameter constructed, via logit, from two covariates.  What I am wondering is: Would it be legitimate, in a binomial + logit problem like this, to allow binomial p[i] to be a function of the corresponding n[i] or would that amount to using the data in the prior?  In other words, in the context of the Seeds example, is r[] the only data or is n[] data as well and therefore not permissible in a prior formulation?


I [McLaughlin] currently have a model with a common beta prior for all p[i] but would like to mitigate this commonality (a kind of James-Stein effect) when there are lots of observations for some i.  But this seems to feed the data back into the prior.  Does it really?


It also occurs to me [McLaughlin] that, perhaps, a binomial likelihood is not the one to use here (not flexible enough).
  
My reply:
 
Strictly speaking, “n” is data, and so what you wa</p><p>5 0.95062494 <a title="2305-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-09-Rasmussen_sez%3A__%E2%80%9C108%25_of_Respondents_Say_._._.%E2%80%9D.html">135 andrew gelman stats-2010-07-09-Rasmussen sez:  “108% of Respondents Say . . .”</a></p>
<p>Introduction: The recent discussion of pollsters reminded me of  a story  from a couple years ago that perhaps is still relevant . . .
 
I was looking up the governors’ popularity numbers on the web, and came across  this page from Rasmussen Reports  which shows Sarah Palin as the 3rd-most-popular governor. But then I looked more carefully. Janet Napolitano of Arizona was viewed as Excellent by 28% of respondents, Good by 27%, Fair by 26%, and Poor by 27%. That adds up to 108%! What’s going on?
 
I’d think they would have a computer program to pipe the survey results directly into the spreadsheet. But I guess not, someone must be typing in these numbers one at a time.  Another possibility is that they are altering their numbers by hand, and someone made a mistake with the Napolitano numbers, adding a few percent in one place and forgetting to subtract elsewhere.  Or maybe there’s another explanation?
  

 
P.S.  Here  are some thoughts from Mark Blumenthal
 
P.P.S.  I checked the Rasmussen link toda</p><p>6 0.94065344 <a title="2305-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-29-Zero_is_zero.html">687 andrew gelman stats-2011-04-29-Zero is zero</a></p>
<p>7 0.93955511 <a title="2305-lda-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-19-Revised_evidence_for_statistical_standards.html">2140 andrew gelman stats-2013-12-19-Revised evidence for statistical standards</a></p>
<p>8 0.93779469 <a title="2305-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-29-How_many_parameters_are_in_a_multilevel_model%3F.html">1144 andrew gelman stats-2012-01-29-How many parameters are in a multilevel model?</a></p>
<p>9 0.93702459 <a title="2305-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-02-Blogads_update.html">1240 andrew gelman stats-2012-04-02-Blogads update</a></p>
<p>10 0.93634093 <a title="2305-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-15-Reputations_changeable%2C_situations_tolerable.html">1858 andrew gelman stats-2013-05-15-Reputations changeable, situations tolerable</a></p>
<p>11 0.93542433 <a title="2305-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>12 0.93505752 <a title="2305-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-27-Graph_of_the_year.html">488 andrew gelman stats-2010-12-27-Graph of the year</a></p>
<p>13 0.93497241 <a title="2305-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>14 0.93479109 <a title="2305-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-Fourteen_magic_words%3A_an_update.html">898 andrew gelman stats-2011-09-10-Fourteen magic words: an update</a></p>
<p>15 0.93455845 <a title="2305-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-19-Updated_solutions_to_Bayesian_Data_Analysis_homeworks.html">42 andrew gelman stats-2010-05-19-Updated solutions to Bayesian Data Analysis homeworks</a></p>
<p>16 0.93455172 <a title="2305-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-24-Latest_in_blog_advertising.html">1080 andrew gelman stats-2011-12-24-Latest in blog advertising</a></p>
<p>17 0.93291581 <a title="2305-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-18-More_studies_on_the_economic_effects_of_climate_change.html">1501 andrew gelman stats-2012-09-18-More studies on the economic effects of climate change</a></p>
<p>18 0.9321903 <a title="2305-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-18-Understanding_posterior_p-values.html">2029 andrew gelman stats-2013-09-18-Understanding posterior p-values</a></p>
<p>19 0.93088037 <a title="2305-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-03-Setting_aside_the_politics%2C_the_debate_over_the_new_health-care_study_reveals_that_we%E2%80%99re_moving_to_a_new_high_standard_of_statistical_journalism.html">1838 andrew gelman stats-2013-05-03-Setting aside the politics, the debate over the new health-care study reveals that we’re moving to a new high standard of statistical journalism</a></p>
<p>20 0.93045008 <a title="2305-lda-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-Stan_Model_of_the_Week%3A_Hierarchical_Modeling_of_Supernovas.html">2299 andrew gelman stats-2014-04-21-Stan Model of the Week: Hierarchical Modeling of Supernovas</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
