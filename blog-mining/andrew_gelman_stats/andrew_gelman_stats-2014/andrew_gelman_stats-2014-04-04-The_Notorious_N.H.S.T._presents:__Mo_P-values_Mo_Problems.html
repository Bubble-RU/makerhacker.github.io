<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2014" href="../home/andrew_gelman_stats-2014_home.html">andrew_gelman_stats-2014</a> <a title="andrew_gelman_stats-2014-2281" href="#">andrew_gelman_stats-2014-2281</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2014-2281-html" href="http://andrewgelman.com/2014/04/04/notorious-n-h-s-t-presents-mo-p-values-mo-problems/">html</a></p><p>Introduction: A recent  discussion  between commenters Question and Fernando captured one of the recurrent themes here from the past year.
 
 Question:   The problem is simple, the researchers are disproving always false null hypotheses and taking this disproof as near proof that their theory is correct.
 
 Fernando:   Whereas it is probably true that researchers misuse NHT, the problem with tabloid science is broader and deeper. It is systemic.
 
 Question:   I do not see how anything can be deeper than replacing careful description, prediction, falsification, and independent replication with dynamite plots, p-values, affirming the consequent, and peer review. From my own experience I am confident in saying that confusion caused by NHST is at the root of this problem.
 
 Fernando:   Incentives? Impact factors? Publish or die? “Interesting” and “new” above quality and reliability, or actually answering a research question, and a silly and unbecoming obsession with being quoted in NYT, etc. . . . Giv</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A recent  discussion  between commenters Question and Fernando captured one of the recurrent themes here from the past year. [sent-1, score-0.156]
</p><p>2 Question:   The problem is simple, the researchers are disproving always false null hypotheses and taking this disproof as near proof that their theory is correct. [sent-2, score-1.153]
</p><p>3 Fernando:   Whereas it is probably true that researchers misuse NHT, the problem with tabloid science is broader and deeper. [sent-3, score-0.148]
</p><p>4 Question:   I do not see how anything can be deeper than replacing careful description, prediction, falsification, and independent replication with dynamite plots, p-values, affirming the consequent, and peer review. [sent-5, score-0.15]
</p><p>5 From my own experience I am confident in saying that confusion caused by NHST is at the root of this problem. [sent-6, score-0.172]
</p><p>6 “Interesting” and “new” above quality and reliability, or actually answering a research question, and a silly and unbecoming obsession with being quoted in NYT, etc. [sent-10, score-0.231]
</p><p>7 Given the incentives something silly is bound to happen. [sent-14, score-0.185]
</p><p>8 At this point I was going to respond in the comments, but I decided to make this a separate post (at the cost of pre-empting yet another scheduled item on the queue), for two reasons:   1. [sent-16, score-0.134]
</p><p>9 I think Bayesian methods are great, don’t get me wrong, but the discussion here has little to do with Bayes. [sent-22, score-0.066]
</p><p>10 Null hypothesis significance testing can be done in a non-Bayesian way (of course, just see all sorts of theoretical-statistics textbooks) but some Bayesians like to do it too, using Bayes factors and all the rest of that crap to decide whether to accept models of the theta=0 variety. [sent-23, score-0.588]
</p><p>11 Do it using p-values or Bayes factors, either way it’s significance testing with the goal of rejecting models. [sent-24, score-0.355]
</p><p>12 as an enabler    I agree with the now-conventional wisdom expressed by the original commenter, that null hypothesis significance testing is generally inappropriate. [sent-29, score-0.728]
</p><p>13 But I also agree with Fernando’s comment that the pressures of publication would be leading to the aggressive dissemination of noise, in any case. [sent-30, score-0.456]
</p><p>14 This relates to my recent discussion with Steven Pinker (not published on blog yet, it’s on the queue, you’ll see it in a month or so). [sent-37, score-0.066]
</p><p>15 To say it another way, the reason why I go on and on about multiple comparisons is not that I think it’s so important to get correct p-values, but rather that these p-values are being used as the statistical justification for otherwise laughable claims. [sent-38, score-0.155]
</p><p>16 , some other tool would be used to give the stamp of approval on data-based speculations. [sent-43, score-0.134]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fernando', 0.426), ('null', 0.254), ('hypotheses', 0.207), ('disproving', 0.17), ('testing', 0.164), ('dissemination', 0.163), ('disproof', 0.163), ('bayes', 0.159), ('noise', 0.15), ('queue', 0.149), ('aggressive', 0.128), ('factors', 0.123), ('significance', 0.12), ('proof', 0.117), ('hypothesis', 0.11), ('confusion', 0.106), ('smart', 0.105), ('incentives', 0.104), ('commenter', 0.097), ('notorious', 0.094), ('near', 0.094), ('recurrent', 0.09), ('laughable', 0.09), ('question', 0.085), ('pressures', 0.085), ('obsession', 0.085), ('dynamite', 0.085), ('overarching', 0.085), ('false', 0.083), ('nhst', 0.081), ('nht', 0.081), ('silly', 0.081), ('agree', 0.08), ('tabloid', 0.074), ('misuse', 0.074), ('skeptics', 0.071), ('wary', 0.071), ('rejecting', 0.071), ('sorts', 0.071), ('stamp', 0.069), ('yet', 0.068), ('scheduled', 0.066), ('root', 0.066), ('discussion', 0.066), ('falsification', 0.065), ('pinker', 0.065), ('used', 0.065), ('taking', 0.065), ('answering', 0.065), ('replacing', 0.065)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="2281-tfidf-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>Introduction: A recent  discussion  between commenters Question and Fernando captured one of the recurrent themes here from the past year.
 
 Question:   The problem is simple, the researchers are disproving always false null hypotheses and taking this disproof as near proof that their theory is correct.
 
 Fernando:   Whereas it is probably true that researchers misuse NHT, the problem with tabloid science is broader and deeper. It is systemic.
 
 Question:   I do not see how anything can be deeper than replacing careful description, prediction, falsification, and independent replication with dynamite plots, p-values, affirming the consequent, and peer review. From my own experience I am confident in saying that confusion caused by NHST is at the root of this problem.
 
 Fernando:   Incentives? Impact factors? Publish or die? “Interesting” and “new” above quality and reliability, or actually answering a research question, and a silly and unbecoming obsession with being quoted in NYT, etc. . . . Giv</p><p>2 0.39817345 <a title="2281-tfidf-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-29-I_agree_with_this_comment.html">2272 andrew gelman stats-2014-03-29-I agree with this comment</a></p>
<p>Introduction: The anonymous commenter  puts it well :
  
The problem is simple, the researchers are disproving always false null hypotheses and taking this disproof as near proof that their theory is correct.</p><p>3 0.20960732 <a title="2281-tfidf-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21.html">256 andrew gelman stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</a></p>
<p>Introduction: Masanao sends  this one  in, under the heading, “another incident of misunderstood p-value”:
  
Warren Davies, a positive psychology MSc student at UEL, provides the latest in our ongoing series of guest features for students. Warren has just released a Psychology Study Guide, which covers information on statistics, research methods and study skills for psychology students.

 
Despite the myriad rules and procedures of science, some research findings are pure flukes. Perhaps you’re testing a new drug, and by chance alone, a large number of people spontaneously get better. The better your study is conducted, the lower the chance that your result was a fluke – but still, there is always a certain probability that it was.


Statistical significance testing gives you an idea of what this probability is.


In science we’re always testing hypotheses. We never conduct a study to ‘see what happens’, because there’s always at least one way to make any useless set of data look important. We take</p><p>4 0.20325148 <a title="2281-tfidf-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-24-Empirical_implications_of_Empirical_Implications_of_Theoretical_Models.html">2263 andrew gelman stats-2014-03-24-Empirical implications of Empirical Implications of Theoretical Models</a></p>
<p>Introduction: Robert Bloomfield writes:
  
Most of the people in my field (accounting, which is basically applied economics and finance, leavened with psychology and organizational behavior) use ‘positive research methods’, which are typically described as coming to the data with a predefined theory, and using hypothesis testing to accept or reject the theory’s predictions.  But a substantial minority use ‘interpretive research methods’ (sometimes called qualitative methods, for those that call positive research ‘quantitative’).  No one seems entirely happy with the definition of this method, but I’ve found it useful to think of it as an attempt to see the world through the eyes of your subjects, much as Jane Goodall lived with gorillas and tried to see the world through their eyes.)


Interpretive researchers often criticize positive researchers by noting that the latter don’t make the best use of their data, because they come to the data with a predetermined theory, and only test a narrow set of h</p><p>5 0.17467009 <a title="2281-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-26-%E2%80%9CA_Vast_Graveyard_of_Undead_Theories%3A__Publication_Bias_and_Psychological_Science%E2%80%99s_Aversion_to_the_Null%E2%80%9D.html">1826 andrew gelman stats-2013-04-26-“A Vast Graveyard of Undead Theories:  Publication Bias and Psychological Science’s Aversion to the Null”</a></p>
<p>Introduction: Erin Jonaitis points us to  this article  by Christopher Ferguson and Moritz Heene, who write:
  
Publication bias remains a controversial issue in psychological science. . . . that the field often constructs arguments to block the publication and interpretation of null results and that null results may be further extinguished through questionable researcher practices. Given that science is dependent on the process of falsification, we argue that these problems reduce psychological science’s capability to have a proper mechanism for theory falsification, thus resulting in the promulgation of numerous “undead” theories that are ideologically popular but have little basis in fact.
  
They mention the infamous Daryl Bem article.  It is pretty much only because Bem’s claims are (presumably) false that they got published in a major research journal.  Had the claims been true—that is, had Bem run identical experiments, analyzed his data more carefully and objectively, and reported that the r</p><p>6 0.17400081 <a title="2281-tfidf-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Ken_Rice_presents_a_unifying_approach_to_statistical_inference_and_hypothesis_testing.html">2312 andrew gelman stats-2014-04-29-Ken Rice presents a unifying approach to statistical inference and hypothesis testing</a></p>
<p>7 0.16472875 <a title="2281-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>8 0.14959571 <a title="2281-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-31-Lindley%E2%80%99s_paradox.html">1355 andrew gelman stats-2012-05-31-Lindley’s paradox</a></p>
<p>9 0.14460802 <a title="2281-tfidf-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-18-One-tailed_or_two-tailed%3F.html">2295 andrew gelman stats-2014-04-18-One-tailed or two-tailed?</a></p>
<p>10 0.14410596 <a title="2281-tfidf-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-23-Discussion_on_preregistration_of_research_studies.html">2183 andrew gelman stats-2014-01-23-Discussion on preregistration of research studies</a></p>
<p>11 0.14199632 <a title="2281-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-20-More_proposals_to_reform_the_peer-review_system.html">1272 andrew gelman stats-2012-04-20-More proposals to reform the peer-review system</a></p>
<p>12 0.13933481 <a title="2281-tfidf-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-08-Discussion_with_Steven_Pinker_on_research_that_is_attached_to_data_that_are_so_noisy_as_to_be_essentially_uninformative.html">2326 andrew gelman stats-2014-05-08-Discussion with Steven Pinker on research that is attached to data that are so noisy as to be essentially uninformative</a></p>
<p>13 0.13477957 <a title="2281-tfidf-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-28-More_on_Bayesian_deduction-induction.html">114 andrew gelman stats-2010-06-28-More on Bayesian deduction-induction</a></p>
<p>14 0.13465583 <a title="2281-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-24-In_which_I_side_with_Neyman_over_Fisher.html">1869 andrew gelman stats-2013-05-24-In which I side with Neyman over Fisher</a></p>
<p>15 0.13201401 <a title="2281-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-06-That_silly_ESP_paper_and_some_silliness_in_a_rebuttal_as_well.html">506 andrew gelman stats-2011-01-06-That silly ESP paper and some silliness in a rebuttal as well</a></p>
<p>16 0.12953053 <a title="2281-tfidf-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-23-Of_hypothesis_tests_and_Unitarians.html">1024 andrew gelman stats-2011-11-23-Of hypothesis tests and Unitarians</a></p>
<p>17 0.12753037 <a title="2281-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-28-Escalatingly_uncomfortable.html">1873 andrew gelman stats-2013-05-28-Escalatingly uncomfortable</a></p>
<p>18 0.12668926 <a title="2281-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-19-Data_exploration_and_multiple_comparisons.html">524 andrew gelman stats-2011-01-19-Data exploration and multiple comparisons</a></p>
<p>19 0.12402948 <a title="2281-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>20 0.12074599 <a title="2281-tfidf-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-20-How_to_schedule_projects_in_an_introductory_statistics_course%3F.html">423 andrew gelman stats-2010-11-20-How to schedule projects in an introductory statistics course?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.204), (1, 0.021), (2, -0.045), (3, -0.081), (4, -0.093), (5, -0.069), (6, -0.014), (7, 0.034), (8, 0.051), (9, -0.109), (10, -0.068), (11, 0.028), (12, 0.036), (13, -0.078), (14, 0.015), (15, -0.005), (16, -0.046), (17, -0.066), (18, -0.025), (19, -0.042), (20, 0.051), (21, -0.003), (22, -0.062), (23, 0.004), (24, -0.092), (25, -0.067), (26, 0.078), (27, 0.034), (28, 0.034), (29, -0.012), (30, 0.027), (31, 0.002), (32, 0.077), (33, 0.001), (34, -0.101), (35, -0.056), (36, 0.081), (37, -0.028), (38, 0.035), (39, -0.017), (40, -0.08), (41, 0.031), (42, 0.013), (43, 0.006), (44, 0.008), (45, 0.022), (46, 0.003), (47, -0.051), (48, 0.031), (49, -0.034)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95342743 <a title="2281-lsi-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>Introduction: A recent  discussion  between commenters Question and Fernando captured one of the recurrent themes here from the past year.
 
 Question:   The problem is simple, the researchers are disproving always false null hypotheses and taking this disproof as near proof that their theory is correct.
 
 Fernando:   Whereas it is probably true that researchers misuse NHT, the problem with tabloid science is broader and deeper. It is systemic.
 
 Question:   I do not see how anything can be deeper than replacing careful description, prediction, falsification, and independent replication with dynamite plots, p-values, affirming the consequent, and peer review. From my own experience I am confident in saying that confusion caused by NHST is at the root of this problem.
 
 Fernando:   Incentives? Impact factors? Publish or die? “Interesting” and “new” above quality and reliability, or actually answering a research question, and a silly and unbecoming obsession with being quoted in NYT, etc. . . . Giv</p><p>2 0.89923137 <a title="2281-lsi-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-29-I_agree_with_this_comment.html">2272 andrew gelman stats-2014-03-29-I agree with this comment</a></p>
<p>Introduction: The anonymous commenter  puts it well :
  
The problem is simple, the researchers are disproving always false null hypotheses and taking this disproof as near proof that their theory is correct.</p><p>3 0.84129077 <a title="2281-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-31-Lindley%E2%80%99s_paradox.html">1355 andrew gelman stats-2012-05-31-Lindley’s paradox</a></p>
<p>Introduction: Sam Seaver writes:
  
I [Seaver] happened to be reading an ironic  article  by Karl Friston when I learned something new about frequentist vs bayesian, namely Lindley’s paradox, on page 12.  The text is as follows:

 
So why are we worried about trivial effects? They are important because the probability that the true effect size is exactly zero is itself zero and could cause us to reject the null hypothesis inappropriately. This is a fallacy of classical inference and is not unrelated to Lindley’s paradox (Lindley 1957). Lindley’s paradox describes a counterintuitive situation in which Bayesian and frequentist approaches to hypothesis testing give opposite results. It occurs when; (i) a result is significant by a frequentist test, indicating sufficient evidence to reject the null hypothesis d=0 and (ii) priors render the posterior probability of d=0 high, indicating strong evidence that the null hypothesis is true. In his original 
treatment, Lindley (1957) showed that – under a parti</p><p>4 0.83668691 <a title="2281-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-23-Of_hypothesis_tests_and_Unitarians.html">1024 andrew gelman stats-2011-11-23-Of hypothesis tests and Unitarians</a></p>
<p>Introduction: Xian, Judith, and I read this line in a book by statistician Murray Aitkin in which he considered the following hypothetical example:
  
A survey of 100 individuals expressing support (Yes/No) for the president, before and after a presidential address . . . The question of interest is whether there has been a change in support between the surveys . . . We want to assess the evidence for the hypothesis of equality H1 against the alternative hypothesis H2 of a change.
  
Here is  our response :
  
Based on our experience in public opinion research, this is not a real question. Support for any political position is always changing. The real question is how much the support has changed, or perhaps how this change is distributed across the population.


A defender of Aitkin (and of classical hypothesis testing) might respond at this point that, yes, everybody knows that changes are never exactly zero and that we should take a more “grown-up” view of the null hypothesis, not that the change</p><p>5 0.82041794 <a title="2281-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>Introduction: In response to the  discussion  of X and me of his recent  paper , Val Johnson writes:
  
I would like to thank Andrew for forwarding his comments on uniformly most powerful Bayesian tests (UMPBTs) to me and his invitation to respond to them.  I think he  (and also Christian Robert) raise a number of interesting points concerning this new class of Bayesian tests, but I think that they may have confounded several issues that might more usefully be examined separately.


The first issue involves the choice of the Bayesian evidence threshold, gamma, used in rejecting a null hypothesis in favor of an alternative hypothesis.  Andrew objects to the higher values of gamma proposed in my recent PNAS article on grounds that too many important scientific effects would be missed if thresholds of 25-50 were routinely used.  These evidence thresholds correspond roughly to p-values of 0.005; Andrew suggests that evidence thresholds around 5 should continue to be used (gamma=5 corresponds approximate</p><p>6 0.81972909 <a title="2281-lsi-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-18-One-tailed_or_two-tailed%3F.html">2295 andrew gelman stats-2014-04-18-One-tailed or two-tailed?</a></p>
<p>7 0.81737244 <a title="2281-lsi-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21.html">256 andrew gelman stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</a></p>
<p>8 0.80941427 <a title="2281-lsi-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Ken_Rice_presents_a_unifying_approach_to_statistical_inference_and_hypothesis_testing.html">2312 andrew gelman stats-2014-04-29-Ken Rice presents a unifying approach to statistical inference and hypothesis testing</a></p>
<p>9 0.79901534 <a title="2281-lsi-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-24-Empirical_implications_of_Empirical_Implications_of_Theoretical_Models.html">2263 andrew gelman stats-2014-03-24-Empirical implications of Empirical Implications of Theoretical Models</a></p>
<p>10 0.76541746 <a title="2281-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-26-%E2%80%9CA_Vast_Graveyard_of_Undead_Theories%3A__Publication_Bias_and_Psychological_Science%E2%80%99s_Aversion_to_the_Null%E2%80%9D.html">1826 andrew gelman stats-2013-04-26-“A Vast Graveyard of Undead Theories:  Publication Bias and Psychological Science’s Aversion to the Null”</a></p>
<p>11 0.76209652 <a title="2281-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>12 0.75864673 <a title="2281-lsi-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-23-Discussion_on_preregistration_of_research_studies.html">2183 andrew gelman stats-2014-01-23-Discussion on preregistration of research studies</a></p>
<p>13 0.75150353 <a title="2281-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-24-In_which_I_side_with_Neyman_over_Fisher.html">1869 andrew gelman stats-2013-05-24-In which I side with Neyman over Fisher</a></p>
<p>14 0.74520272 <a title="2281-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-15-%E2%80%9CAre_all_significant_p-values_created_equal%3F%E2%80%9D.html">2102 andrew gelman stats-2013-11-15-“Are all significant p-values created equal?”</a></p>
<p>15 0.72872347 <a title="2281-lsi-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-25-Revised_statistical_standards_for_evidence_%28comments_to_Val_Johnson%E2%80%99s_comments_on_our_comments_on_Val%E2%80%99s_comments_on_p-values%29.html">2305 andrew gelman stats-2014-04-25-Revised statistical standards for evidence (comments to Val Johnson’s comments on our comments on Val’s comments on p-values)</a></p>
<p>16 0.72664583 <a title="2281-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-12-Misunderstanding_the_p-value.html">1760 andrew gelman stats-2013-03-12-Misunderstanding the p-value</a></p>
<p>17 0.72247702 <a title="2281-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-08-The_never-ending_%28and_often_productive%29_race_between_theory_and_practice.html">2127 andrew gelman stats-2013-12-08-The never-ending (and often productive) race between theory and practice</a></p>
<p>18 0.71658528 <a title="2281-lsi-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-10-Bayes_jumps_the_shark.html">331 andrew gelman stats-2010-10-10-Bayes jumps the shark</a></p>
<p>19 0.70458651 <a title="2281-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-06-That_silly_ESP_paper_and_some_silliness_in_a_rebuttal_as_well.html">506 andrew gelman stats-2011-01-06-That silly ESP paper and some silliness in a rebuttal as well</a></p>
<p>20 0.68937367 <a title="2281-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-17-Where_do_theories_come_from%3F.html">1861 andrew gelman stats-2013-05-17-Where do theories come from?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.025), (9, 0.031), (16, 0.114), (18, 0.012), (21, 0.098), (24, 0.164), (43, 0.027), (45, 0.02), (63, 0.025), (84, 0.011), (85, 0.052), (86, 0.06), (96, 0.011), (99, 0.228)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95074743 <a title="2281-lda-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>Introduction: A recent  discussion  between commenters Question and Fernando captured one of the recurrent themes here from the past year.
 
 Question:   The problem is simple, the researchers are disproving always false null hypotheses and taking this disproof as near proof that their theory is correct.
 
 Fernando:   Whereas it is probably true that researchers misuse NHT, the problem with tabloid science is broader and deeper. It is systemic.
 
 Question:   I do not see how anything can be deeper than replacing careful description, prediction, falsification, and independent replication with dynamite plots, p-values, affirming the consequent, and peer review. From my own experience I am confident in saying that confusion caused by NHST is at the root of this problem.
 
 Fernando:   Incentives? Impact factors? Publish or die? “Interesting” and “new” above quality and reliability, or actually answering a research question, and a silly and unbecoming obsession with being quoted in NYT, etc. . . . Giv</p><p>2 0.93798196 <a title="2281-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-26-%E2%80%9CA_Vast_Graveyard_of_Undead_Theories%3A__Publication_Bias_and_Psychological_Science%E2%80%99s_Aversion_to_the_Null%E2%80%9D.html">1826 andrew gelman stats-2013-04-26-“A Vast Graveyard of Undead Theories:  Publication Bias and Psychological Science’s Aversion to the Null”</a></p>
<p>Introduction: Erin Jonaitis points us to  this article  by Christopher Ferguson and Moritz Heene, who write:
  
Publication bias remains a controversial issue in psychological science. . . . that the field often constructs arguments to block the publication and interpretation of null results and that null results may be further extinguished through questionable researcher practices. Given that science is dependent on the process of falsification, we argue that these problems reduce psychological science’s capability to have a proper mechanism for theory falsification, thus resulting in the promulgation of numerous “undead” theories that are ideologically popular but have little basis in fact.
  
They mention the infamous Daryl Bem article.  It is pretty much only because Bem’s claims are (presumably) false that they got published in a major research journal.  Had the claims been true—that is, had Bem run identical experiments, analyzed his data more carefully and objectively, and reported that the r</p><p>3 0.93588299 <a title="2281-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<p>Introduction: Becky Passonneau and colleagues at the Center for Computational Learning Systems (CCLS) at Columbia have been working on a project for ConEd (New York’s major electric utility) to  rank structures based on vulnerability to secondary events  (e.g., transformer explosions, cable meltdowns, electrical fires).  They’ve been using the R implementation  BayesTree  of Chipman, George and McCulloch’s  Bayesian Additive Regression Trees  (BART).
 
BART is a Bayesian non-parametric method that is non-identifiable in two ways.  Firstly, it is an additive tree model with a fixed number of trees, the indexes of which aren’t identified (you get the same predictions in a model swapping the order of the trees).  This is the same kind of non-identifiability you get with any mixture model (additive or interpolated) with an exchangeable prior on the mixture components.  Secondly, the trees themselves have varying structure over samples in terms of number of nodes and their topology (depth, branching, etc</p><p>4 0.93492788 <a title="2281-lda-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-26-Sleazy_sock_puppet_can%E2%80%99t_stop_spamming_our_discussion_of_compressed_sensing_and_promoting_the_work_of_Xiteng_Liu.html">2306 andrew gelman stats-2014-04-26-Sleazy sock puppet can’t stop spamming our discussion of compressed sensing and promoting the work of Xiteng Liu</a></p>
<p>Introduction: Some asshole who has a bug up his ass about compressed sensing is spamming our  comments  with a bunch of sock puppets.  All from the same IP address:  “George Stoneriver,” Scott Wolfe,” and just plain “Paul,”  all saying pretty much the same thing in the same sort of broken English (except for Paul, whose post was too short to do a dialect analysis). “Scott Wolfe” is a generic sort of name, but a quick google search reveals nothing related to this topic. “George Stoneriver” seems to have no internet presence at all (besides the comments at this blog).  As for “Paul,” I don’t know, maybe the spammer was too lazy to invent a last name?
 
Our spammer spends about half his time slamming the field of compressed sensing and the other half pumping up the work of someone named Xiteng Liu.  There’s no excuse for this behavior.  It’s horrible, a true abuse of our scholarly community.
 
If Scott Adams wants to use a sock puppet, fine, the guy’s an artist and we should cut him some slack.  If tha</p><p>5 0.93293208 <a title="2281-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-25-An_interesting_but_flawed_attempt_to_apply_general_forecasting_principles_to_contextualize_attitudes_toward_risks_of_global_warming.html">2112 andrew gelman stats-2013-11-25-An interesting but flawed attempt to apply general forecasting principles to contextualize attitudes toward risks of global warming</a></p>
<p>Introduction: I came across  a document  [updated link  here ], “Applying structured analogies to the global warming alarm movement,” by Kesten Green and Scott Armstrong.  The general approach is appealing to me, but the execution seemed disturbingly flawed.
 
Here’s how they introduce the project:
  
The structured analogies procedure we [Green and Armstrong] used for this study was as follows:


1. Identify possible analogies by searching the literature and by asking experts with different viewpoints to nominate analogies to the target situation: alarm over dangerous manmade global warming. 
2. Screen the possible analogies to ensure they meet the stated criteria and that the outcomes are known. 
3. Code the relevant characteristics of the analogous situations. 
4. Forecast target situation outcomes by using a predetermined mechanical rule to select the outcomes of the analogies. 
Here is how we posed the question to the experts:

 
The Intergovernmental Panel on Climate Change and other organizat</p><p>6 0.9316209 <a title="2281-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-19-Validation_of_Software_for_Bayesian_Models_Using_Posterior_Quantiles.html">1019 andrew gelman stats-2011-11-19-Validation of Software for Bayesian Models Using Posterior Quantiles</a></p>
<p>7 0.93125808 <a title="2281-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-23-Combining_survey_data_obtained_using_different_modes_of_sampling.html">777 andrew gelman stats-2011-06-23-Combining survey data obtained using different modes of sampling</a></p>
<p>8 0.92830515 <a title="2281-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-10-A_defense_of_Tom_Wolfe_based_on_the_impossibility_of_the_law_of_small_numbers_in_network_structure.html">1615 andrew gelman stats-2012-12-10-A defense of Tom Wolfe based on the impossibility of the law of small numbers in network structure</a></p>
<p>9 0.92760062 <a title="2281-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-23-A_statistical_version_of_Arrow%E2%80%99s_paradox.html">586 andrew gelman stats-2011-02-23-A statistical version of Arrow’s paradox</a></p>
<p>10 0.92682397 <a title="2281-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-28-50_shades_of_gray%3A__A_research_story.html">1959 andrew gelman stats-2013-07-28-50 shades of gray:  A research story</a></p>
<p>11 0.9267056 <a title="2281-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-18-Understanding_posterior_p-values.html">2029 andrew gelman stats-2013-09-18-Understanding posterior p-values</a></p>
<p>12 0.92623889 <a title="2281-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-25-Classical_probability_does_not_apply_to_quantum_systems_%28causal_inference_edition%29.html">2037 andrew gelman stats-2013-09-25-Classical probability does not apply to quantum systems (causal inference edition)</a></p>
<p>13 0.92408538 <a title="2281-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-25-Fascinating_graphs_from_facebook_data.html">1824 andrew gelman stats-2013-04-25-Fascinating graphs from facebook data</a></p>
<p>14 0.92394841 <a title="2281-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-13-News_coverage_of_statistical_issues%E2%80%A6how_did_I_do%3F.html">514 andrew gelman stats-2011-01-13-News coverage of statistical issues…how did I do?</a></p>
<p>15 0.9231689 <a title="2281-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-25-Postdoc_Position_%231%3A__Missing-Data_Imputation%2C_Diagnostics%2C_and_Applications.html">537 andrew gelman stats-2011-01-25-Postdoc Position #1:  Missing-Data Imputation, Diagnostics, and Applications</a></p>
<p>16 0.92310083 <a title="2281-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-07-Descriptive_statistics%2C_causal_inference%2C_and_story_time.html">789 andrew gelman stats-2011-07-07-Descriptive statistics, causal inference, and story time</a></p>
<p>17 0.92224991 <a title="2281-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-17-Macro_causality.html">807 andrew gelman stats-2011-07-17-Macro causality</a></p>
<p>18 0.92168254 <a title="2281-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-13-Jim_Campbell_argues_that_Larry_Bartels%E2%80%99s_%E2%80%9CUnequal_Democracy%E2%80%9D_findings_are_not_robust.html">659 andrew gelman stats-2011-04-13-Jim Campbell argues that Larry Bartels’s “Unequal Democracy” findings are not robust</a></p>
<p>19 0.92165291 <a title="2281-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Adding_more_information_can_make_the_variance_go_up_%28depending_on_your_model%29.html">810 andrew gelman stats-2011-07-20-Adding more information can make the variance go up (depending on your model)</a></p>
<p>20 0.92085356 <a title="2281-lda-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-27-Graph_of_the_year.html">488 andrew gelman stats-2010-12-27-Graph of the year</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
