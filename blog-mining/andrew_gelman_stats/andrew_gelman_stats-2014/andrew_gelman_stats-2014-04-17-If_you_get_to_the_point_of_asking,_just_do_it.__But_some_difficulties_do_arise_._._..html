<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2294 andrew gelman stats-2014-04-17-If you get to the point of asking, just do it.  But some difficulties do arise . . .</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2014" href="../home/andrew_gelman_stats-2014_home.html">andrew_gelman_stats-2014</a> <a title="andrew_gelman_stats-2014-2294" href="#">andrew_gelman_stats-2014-2294</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2294 andrew gelman stats-2014-04-17-If you get to the point of asking, just do it.  But some difficulties do arise . . .</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2014-2294-html" href="http://andrewgelman.com/2014/04/17/get-point-asking-just-difficulties-arise/">html</a></p><p>Introduction: Nelson Villoria writes:
  
I find the multilevel approach very useful for a problem I am dealing with, and I was wondering whether you could point me to some references about poolability tests for multilevel models. I am working with time series of cross sectional data and I want to test whether the data supports cross sectional and/or time pooling. In a standard panel data setting I do this with Chow tests and/or CUSUM. Are these ideas directly transferable to the multilevel setting?
  
My reply:  I think you should do partial pooling.  Once the question arises, just do it.  Other models are just special cases.  I donâ&euro;&trade;t see the need for any test.
 
That said, if you do a group-level model, you need to consider including group-level averages of individual predictors (see  here ).  And if the number of groups is small, there can be real gains from using an informative prior distribution on the hierarchical variance parameters.  This is something that Jennifer and I do not discuss in our</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Nelson Villoria writes:    I find the multilevel approach very useful for a problem I am dealing with, and I was wondering whether you could point me to some references about poolability tests for multilevel models. [sent-1, score-1.411]
</p><p>2 I am working with time series of cross sectional data and I want to test whether the data supports cross sectional and/or time pooling. [sent-2, score-2.345]
</p><p>3 In a standard panel data setting I do this with Chow tests and/or CUSUM. [sent-3, score-0.705]
</p><p>4 Are these ideas directly transferable to the multilevel setting? [sent-4, score-0.659]
</p><p>5 My reply:  I think you should do partial pooling. [sent-5, score-0.131]
</p><p>6 That said, if you do a group-level model, you need to consider including group-level averages of individual predictors (see  here ). [sent-9, score-0.587]
</p><p>7 And if the number of groups is small, there can be real gains from using an informative prior distribution on the hierarchical variance parameters. [sent-10, score-0.851]
</p><p>8 This is something that Jennifer and I do not discuss in our book, unfortunately. [sent-11, score-0.087]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sectional', 0.429), ('cross', 0.31), ('multilevel', 0.284), ('transferable', 0.214), ('chow', 0.214), ('tests', 0.199), ('setting', 0.198), ('nelson', 0.167), ('gains', 0.161), ('supports', 0.155), ('panel', 0.144), ('dealing', 0.14), ('averages', 0.139), ('arises', 0.131), ('partial', 0.131), ('whether', 0.13), ('references', 0.123), ('jennifer', 0.121), ('need', 0.115), ('predictors', 0.107), ('wondering', 0.107), ('informative', 0.104), ('variance', 0.103), ('unfortunately', 0.101), ('groups', 0.098), ('special', 0.098), ('hierarchical', 0.098), ('series', 0.096), ('data', 0.093), ('discuss', 0.087), ('directly', 0.086), ('test', 0.084), ('individual', 0.084), ('distribution', 0.079), ('prior', 0.077), ('ideas', 0.075), ('time', 0.074), ('approach', 0.072), ('useful', 0.072), ('including', 0.072), ('standard', 0.071), ('consider', 0.07), ('working', 0.068), ('small', 0.068), ('number', 0.067), ('reply', 0.066), ('said', 0.065), ('real', 0.064), ('see', 0.06), ('book', 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="2294-tfidf-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-17-If_you_get_to_the_point_of_asking%2C_just_do_it.__But_some_difficulties_do_arise_._._..html">2294 andrew gelman stats-2014-04-17-If you get to the point of asking, just do it.  But some difficulties do arise . . .</a></p>
<p>Introduction: Nelson Villoria writes:
  
I find the multilevel approach very useful for a problem I am dealing with, and I was wondering whether you could point me to some references about poolability tests for multilevel models. I am working with time series of cross sectional data and I want to test whether the data supports cross sectional and/or time pooling. In a standard panel data setting I do this with Chow tests and/or CUSUM. Are these ideas directly transferable to the multilevel setting?
  
My reply:  I think you should do partial pooling.  Once the question arises, just do it.  Other models are just special cases.  I donâ&euro;&trade;t see the need for any test.
 
That said, if you do a group-level model, you need to consider including group-level averages of individual predictors (see  here ).  And if the number of groups is small, there can be real gains from using an informative prior distribution on the hierarchical variance parameters.  This is something that Jennifer and I do not discuss in our</p><p>2 0.18718986 <a title="2294-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-06-17_groups%2C_6_group-level_predictors%3A__What_to_do%3F.html">1248 andrew gelman stats-2012-04-06-17 groups, 6 group-level predictors:  What to do?</a></p>
<p>Introduction: Yi-Chun Ou writes: 
  
  
I am using a multilevel model with three levels. I read that you wrote a book about multilevel models, and wonder if you can solve the following question.  


The data structure is like this: 


Level one: customer (8444 customers) 
Level two: companys (90 companies) 
Level three: industry (17 industries) 


I use 6 level-three variables (i.e. industry characteristics) to explain the variance of the level-one effect across industries. The question here is whether there is an over-fitting problem since there are only 17 industries. I understand that this must be a problem for non-multilevel models, but is it also a problem for multilevel models?
  
My reply:  Yes, this could be a problem.  I’d suggest combining some of your variables into a common score, or using only some of the variables, or using strong priors to control the inferences.  This is an interesting and important area of statistics research, to do this sort of thing systematically.  There’s lots o</p><p>3 0.16760331 <a title="2294-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-11-Understanding_how_estimates_change_when_you_move_to_a_multilevel_model.html">850 andrew gelman stats-2011-08-11-Understanding how estimates change when you move to a multilevel model</a></p>
<p>Introduction: Ramu Sudhagoni writes:
  
 
I am working on combining three longitudinal studies using Bayesian hierarchical technique.  In each study, I have at least 70 subjects follow up on 5 different visit months. My model consists of 10 different covariates including longitudinal and cross-sectional effects. Mixed models are used to fit the three studies individually using Bayesian approach and I noticed that few covariates were significant. When I combined using three level hierarchical approach, all the covariates became non-significant at the population level,  and large estimates were found for variance parameters at the population level. I am struggling to understand why I am getting large variances at population level and wider credible intervals. I assumed non-informative normal priors for all my cross sectional and longitudinal effects, and non-informative inverse-gamma priors for variance parameters. I followed the approach explained by Inoue et al. (Title: Combining Longitudinal Studie</p><p>4 0.16409512 <a title="2294-tfidf-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-25-Clusters_with_very_small_numbers_of_observations.html">295 andrew gelman stats-2010-09-25-Clusters with very small numbers of observations</a></p>
<p>Introduction: James O’Brien writes:
  
How would you explain, to a “classically-trained” hypothesis-tester, that “It’s OK to fit a multilevel model even if some groups have only one observation each”?


I [O'Brien] think I understand the logic and the statistical principles at work in this, but I’ve having trouble being clear and persuasive. I also feel like I’m contending with some methodological conventional wisdom here. 
  
My reply:  I’m so used to this idea that I find it difficult to defend it in some sort of general conceptual way.  So let me retreat to a more functional defense, which is that multilevel modeling gives good estimates,  especially  when the number of observations per group is small.
 
One way to see this in any particular example in through cross-validation.  Another way is to consider the alternatives.   If you try really hard you can come up with a “classical hypothesis testing” approach which will do as well as the multilevel model.  It would just take a lot of work.  I’d r</p><p>5 0.15423618 <a title="2294-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-25-Correlation_of_1_._._._too_good_to_be_true%3F.html">1737 andrew gelman stats-2013-02-25-Correlation of 1 . . . too good to be true?</a></p>
<p>Introduction: Alex Hoffman points me to  this interview  by Dylan Matthews of education researcher Thomas Kane, who at one point says,
  
Once you corrected for measurement error, a teacher’s score on their chosen videos and on their unchosen videos were correlated at 1. They were perfectly correlated.
  
Hoffman asks, “What do you think? Do you think that just maybe, perhaps, it’s possible we aught to consider, I’m just throwing out the possibility that it might be that the procedure for correcting measurement error might, you now, be a little too strong?”
 
I don’t know exactly what’s happening here, but it might be something that I’ve seen on occasion when fitting multilevel models using a point estimate for the group-level variance.  It goes like this:  measurement-error models are multilevel models, they involve the estimation of a distribution of a latent variable.  When fitting multilevel models, it is possible to estimate the group-level variance to be zero, even though the group-level varia</p><p>6 0.14831406 <a title="2294-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-17-Graphical_tools_for_understanding_multilevel_models.html">772 andrew gelman stats-2011-06-17-Graphical tools for understanding multilevel models</a></p>
<p>7 0.14228885 <a title="2294-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-08-Multilevel_regression_with_shrinkage_for_%E2%80%9Cfixed%E2%80%9D_effects.html">653 andrew gelman stats-2011-04-08-Multilevel regression with shrinkage for “fixed” effects</a></p>
<p>8 0.1349038 <a title="2294-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-11-Yes%2C_worry_about_generalizing_from_data_to_population.__But_multilevel_modeling_is_the_solution%2C_not_the_problem.html">1934 andrew gelman stats-2013-07-11-Yes, worry about generalizing from data to population.  But multilevel modeling is the solution, not the problem</a></p>
<p>9 0.12933406 <a title="2294-tfidf-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-How_does_multilevel_modeling_affect_the_estimate_of_the_grand_mean%3F.html">255 andrew gelman stats-2010-09-04-How does multilevel modeling affect the estimate of the grand mean?</a></p>
<p>10 0.12447056 <a title="2294-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-23-More_on_Bayesian_methods_and_multilevel_modeling.html">2033 andrew gelman stats-2013-09-23-More on Bayesian methods and multilevel modeling</a></p>
<p>11 0.1238957 <a title="2294-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-03-Uncertainty_in_parameter_estimates_using_multilevel_models.html">1966 andrew gelman stats-2013-08-03-Uncertainty in parameter estimates using multilevel models</a></p>
<p>12 0.12119388 <a title="2294-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>13 0.11851668 <a title="2294-tfidf-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-06-Multilevel_quantile_regression.html">397 andrew gelman stats-2010-11-06-Multilevel quantile regression</a></p>
<p>14 0.11771498 <a title="2294-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-06-Slow_progress.html">1445 andrew gelman stats-2012-08-06-Slow progress</a></p>
<p>15 0.11618447 <a title="2294-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-23-Examples_of_the_use_of_hierarchical_modeling_to_generalize_to_new_settings.html">1425 andrew gelman stats-2012-07-23-Examples of the use of hierarchical modeling to generalize to new settings</a></p>
<p>16 0.1156669 <a title="2294-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-24-Estimating_and_summarizing_inference_for_hierarchical_variance_parameters_when_the_number_of_groups_is_small.html">2145 andrew gelman stats-2013-12-24-Estimating and summarizing inference for hierarchical variance parameters when the number of groups is small</a></p>
<p>17 0.11496706 <a title="2294-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-10-Multiple_imputation_and_multilevel_analysis.html">704 andrew gelman stats-2011-05-10-Multiple imputation and multilevel analysis</a></p>
<p>18 0.11495634 <a title="2294-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-15-Exploratory_multilevel_analysis_when_group-level_variables_are_of_importance.html">1900 andrew gelman stats-2013-06-15-Exploratory multilevel analysis when group-level variables are of importance</a></p>
<p>19 0.11227782 <a title="2294-tfidf-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-09-Sof%5Bt%5D.html">77 andrew gelman stats-2010-06-09-Sof[t]</a></p>
<p>20 0.11141174 <a title="2294-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.172), (1, 0.149), (2, 0.046), (3, 0.002), (4, 0.087), (5, 0.023), (6, 0.025), (7, -0.031), (8, 0.03), (9, 0.127), (10, 0.027), (11, 0.004), (12, 0.063), (13, -0.022), (14, 0.064), (15, -0.012), (16, -0.079), (17, -0.021), (18, 0.033), (19, -0.011), (20, 0.003), (21, 0.023), (22, -0.004), (23, 0.027), (24, -0.046), (25, -0.127), (26, -0.021), (27, -0.033), (28, -0.023), (29, 0.021), (30, -0.017), (31, -0.03), (32, 0.022), (33, 0.009), (34, 0.001), (35, 0.023), (36, -0.009), (37, 0.023), (38, 0.048), (39, 0.044), (40, -0.002), (41, -0.001), (42, 0.021), (43, -0.055), (44, -0.064), (45, 0.042), (46, 0.023), (47, -0.007), (48, -0.04), (49, -0.058)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95719367 <a title="2294-lsi-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-17-If_you_get_to_the_point_of_asking%2C_just_do_it.__But_some_difficulties_do_arise_._._..html">2294 andrew gelman stats-2014-04-17-If you get to the point of asking, just do it.  But some difficulties do arise . . .</a></p>
<p>Introduction: Nelson Villoria writes:
  
I find the multilevel approach very useful for a problem I am dealing with, and I was wondering whether you could point me to some references about poolability tests for multilevel models. I am working with time series of cross sectional data and I want to test whether the data supports cross sectional and/or time pooling. In a standard panel data setting I do this with Chow tests and/or CUSUM. Are these ideas directly transferable to the multilevel setting?
  
My reply:  I think you should do partial pooling.  Once the question arises, just do it.  Other models are just special cases.  I donâ&euro;&trade;t see the need for any test.
 
That said, if you do a group-level model, you need to consider including group-level averages of individual predictors (see  here ).  And if the number of groups is small, there can be real gains from using an informative prior distribution on the hierarchical variance parameters.  This is something that Jennifer and I do not discuss in our</p><p>2 0.87213677 <a title="2294-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-06-17_groups%2C_6_group-level_predictors%3A__What_to_do%3F.html">1248 andrew gelman stats-2012-04-06-17 groups, 6 group-level predictors:  What to do?</a></p>
<p>Introduction: Yi-Chun Ou writes: 
  
  
I am using a multilevel model with three levels. I read that you wrote a book about multilevel models, and wonder if you can solve the following question.  


The data structure is like this: 


Level one: customer (8444 customers) 
Level two: companys (90 companies) 
Level three: industry (17 industries) 


I use 6 level-three variables (i.e. industry characteristics) to explain the variance of the level-one effect across industries. The question here is whether there is an over-fitting problem since there are only 17 industries. I understand that this must be a problem for non-multilevel models, but is it also a problem for multilevel models?
  
My reply:  Yes, this could be a problem.  I’d suggest combining some of your variables into a common score, or using only some of the variables, or using strong priors to control the inferences.  This is an interesting and important area of statistics research, to do this sort of thing systematically.  There’s lots o</p><p>3 0.82624578 <a title="2294-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-10-Multiple_imputation_and_multilevel_analysis.html">704 andrew gelman stats-2011-05-10-Multiple imputation and multilevel analysis</a></p>
<p>Introduction: Robert Birkelbach:
  
I am writing my Bachelor Thesis in which I want to assess the reading competencies of German elementary school children using the PIRLS2006 data. My levels are classrooms and the individuals. However, my dependent variable is a multiple imputed (m=5) reading test. The problem I have is, that I do not know, whether I can just calculate 5 linear multilevel models and then average all the results (the coefficients, standard deviation, bic, intra class correlation, R2, t-statistics, p-values etc) or if I need different formulas for integrating the results of the five models into one because it is a multilevel analysis? Do you think there’s a better way in solving my problem? I would greatly appreciate if you could help me with a problem regarding my analysis — I am quite a newbie to multilevel modeling and especially to multiple imputation. Also: Is it okay to use frequentist models when the multiple imputation was done bayesian? Would the different philosophies of sc</p><p>4 0.80623788 <a title="2294-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-08-Multilevel_regression_with_shrinkage_for_%E2%80%9Cfixed%E2%80%9D_effects.html">653 andrew gelman stats-2011-04-08-Multilevel regression with shrinkage for “fixed” effects</a></p>
<p>Introduction: Dean Eckles writes:
  
I remember reading on your blog that you were working on some tools to fit multilevel models that also include “fixed” effects — such as continuous predictors — that are also estimated with shrinkage (for example, an L1 or L2 penalty). Any new developments on this front?


I often find myself wanting to fit a multilevel model to some data, but also needing to include a number of “fixed” effects, mainly continuous variables. This makes me wary of overfitting to these predictors, so then I’d want to use some kind of shrinkage.


As far as I can tell, the main options for doing this now is by going fully Bayesian and using a Gibbs sampler. With MCMCglmm or BUGS/JAGS I could just specify a prior on the fixed effects that corresponds to a desired penalty. However, this is pretty slow, especially with a large data set and because I’d like to select the penalty parameter by cross-validation (which is where this isn’t very Bayesian I guess?).
  
My reply:
 
We allow info</p><p>5 0.7663362 <a title="2294-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-10-Combining_data_from_many_sources.html">948 andrew gelman stats-2011-10-10-Combining data from many sources</a></p>
<p>Introduction: Mark Grote writes: 
  
  
I’d like to request general feedback and references for a problem of combining disparate data sources in a regression model. We’d like to model log crop yield as a function of environmental predictors, but the observations come from many data sources and are peculiarly structured. Among the issues are:


1. Measurement precision in predictors and outcome varies widely with data sources. Some observations are in very coarse units of measurement, due to rounding or even observer guesswork. 


2. There are obvious clusters of observations arising from studies in which crop yields were monitored over successive years in spatially proximate communities. Thus some variables may be constant within clusters–this is true even for log yield, probably due to rounding of similar yields. 


3. Cluster size and intra-cluster association structure (temporal, spatial or both) vary widely across the dataset. 


My [Grote's] intuition is that we can learn about central tendency</p><p>6 0.76629251 <a title="2294-lsi-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-25-Clusters_with_very_small_numbers_of_observations.html">295 andrew gelman stats-2010-09-25-Clusters with very small numbers of observations</a></p>
<p>7 0.7660858 <a title="2294-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Allowing_interaction_terms_to_vary.html">753 andrew gelman stats-2011-06-09-Allowing interaction terms to vary</a></p>
<p>8 0.75884986 <a title="2294-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-Multilevel_modeling_even_when_you%E2%80%99re_not_interested_in_predictions_for_new_groups.html">1194 andrew gelman stats-2012-03-04-Multilevel modeling even when you’re not interested in predictions for new groups</a></p>
<p>9 0.75301093 <a title="2294-lsi-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-19-Index_or_indicator_variables.html">2296 andrew gelman stats-2014-04-19-Index or indicator variables</a></p>
<p>10 0.74477226 <a title="2294-lsi-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-06-Multilevel_quantile_regression.html">397 andrew gelman stats-2010-11-06-Multilevel quantile regression</a></p>
<p>11 0.74224293 <a title="2294-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-11-Yes%2C_worry_about_generalizing_from_data_to_population.__But_multilevel_modeling_is_the_solution%2C_not_the_problem.html">1934 andrew gelman stats-2013-07-11-Yes, worry about generalizing from data to population.  But multilevel modeling is the solution, not the problem</a></p>
<p>12 0.7388258 <a title="2294-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-03-Uncertainty_in_parameter_estimates_using_multilevel_models.html">1966 andrew gelman stats-2013-08-03-Uncertainty in parameter estimates using multilevel models</a></p>
<p>13 0.73304921 <a title="2294-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-03-How_best_to_compare_effects_measured_in_two_different_time_periods%3F.html">2086 andrew gelman stats-2013-11-03-How best to compare effects measured in two different time periods?</a></p>
<p>14 0.7290315 <a title="2294-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-25-Correlation_of_1_._._._too_good_to_be_true%3F.html">1737 andrew gelman stats-2013-02-25-Correlation of 1 . . . too good to be true?</a></p>
<p>15 0.72499001 <a title="2294-lsi-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-17-Graphical_tools_for_understanding_multilevel_models.html">772 andrew gelman stats-2011-06-17-Graphical tools for understanding multilevel models</a></p>
<p>16 0.72369224 <a title="2294-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-24-Estimating_and_summarizing_inference_for_hierarchical_variance_parameters_when_the_number_of_groups_is_small.html">2145 andrew gelman stats-2013-12-24-Estimating and summarizing inference for hierarchical variance parameters when the number of groups is small</a></p>
<p>17 0.7150386 <a title="2294-lsi-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-31-Analyzing_the_entire_population_rather_than_a_sample.html">383 andrew gelman stats-2010-10-31-Analyzing the entire population rather than a sample</a></p>
<p>18 0.71327364 <a title="2294-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-23-More_on_Bayesian_methods_and_multilevel_modeling.html">2033 andrew gelman stats-2013-09-23-More on Bayesian methods and multilevel modeling</a></p>
<p>19 0.71311438 <a title="2294-lsi-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-10-Two_great_tastes_that_taste_great_together.html">25 andrew gelman stats-2010-05-10-Two great tastes that taste great together</a></p>
<p>20 0.7119416 <a title="2294-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-11-Understanding_how_estimates_change_when_you_move_to_a_multilevel_model.html">850 andrew gelman stats-2011-08-11-Understanding how estimates change when you move to a multilevel model</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.085), (3, 0.03), (21, 0.023), (24, 0.146), (63, 0.076), (89, 0.118), (99, 0.399)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97575796 <a title="2294-lda-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-17-If_you_get_to_the_point_of_asking%2C_just_do_it.__But_some_difficulties_do_arise_._._..html">2294 andrew gelman stats-2014-04-17-If you get to the point of asking, just do it.  But some difficulties do arise . . .</a></p>
<p>Introduction: Nelson Villoria writes:
  
I find the multilevel approach very useful for a problem I am dealing with, and I was wondering whether you could point me to some references about poolability tests for multilevel models. I am working with time series of cross sectional data and I want to test whether the data supports cross sectional and/or time pooling. In a standard panel data setting I do this with Chow tests and/or CUSUM. Are these ideas directly transferable to the multilevel setting?
  
My reply:  I think you should do partial pooling.  Once the question arises, just do it.  Other models are just special cases.  I donâ&euro;&trade;t see the need for any test.
 
That said, if you do a group-level model, you need to consider including group-level averages of individual predictors (see  here ).  And if the number of groups is small, there can be real gains from using an informative prior distribution on the hierarchical variance parameters.  This is something that Jennifer and I do not discuss in our</p><p>2 0.96308434 <a title="2294-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-11-Understanding_how_estimates_change_when_you_move_to_a_multilevel_model.html">850 andrew gelman stats-2011-08-11-Understanding how estimates change when you move to a multilevel model</a></p>
<p>Introduction: Ramu Sudhagoni writes:
  
 
I am working on combining three longitudinal studies using Bayesian hierarchical technique.  In each study, I have at least 70 subjects follow up on 5 different visit months. My model consists of 10 different covariates including longitudinal and cross-sectional effects. Mixed models are used to fit the three studies individually using Bayesian approach and I noticed that few covariates were significant. When I combined using three level hierarchical approach, all the covariates became non-significant at the population level,  and large estimates were found for variance parameters at the population level. I am struggling to understand why I am getting large variances at population level and wider credible intervals. I assumed non-informative normal priors for all my cross sectional and longitudinal effects, and non-informative inverse-gamma priors for variance parameters. I followed the approach explained by Inoue et al. (Title: Combining Longitudinal Studie</p><p>3 0.9607383 <a title="2294-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-17-Statistics_in_a_world_where_nothing_is_random.html">1628 andrew gelman stats-2012-12-17-Statistics in a world where nothing is random</a></p>
<p>Introduction: Rama Ganesan writes:
  
I think I am having an existential crisis.


I used to work with animals (rats, mice, gerbils etc.) Then I started to work in marketing research where we did have some kind of random sampling procedure. So up until a few years ago, I was sort of okay.


Now I am teaching marketing research, and I feel like there is no real random sampling anymore. I take pains to get students to understand what random means, and then the whole lot of inferential statistics.  Then almost anything they do – the sample is not random. They think I am contradicting myself.  They use convenience samples at every turn – for their school work, and the enormous amount on online surveying that gets done. Do you have any suggestions for me?


Other than say, something like  this .
  
My reply:
 
Statistics does not require randomness.  The three essential elements of statistics are measurement, comparison, and variation.  Randomness is one way to supply variation, and it’s one way to model</p><p>4 0.95941222 <a title="2294-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-09-The_boxer%2C_the_wrestler%2C_and_the_coin_flip%2C_again.html">566 andrew gelman stats-2011-02-09-The boxer, the wrestler, and the coin flip, again</a></p>
<p>Introduction: Mike Grosskopf writes:
  
 
I came across your blog the other day and noticed  your paper  about “The Boxer, the Wrestler, and the Coin Flip” . . . I do not understand the objection to the robust Bayesian inference for conditioning on X=Y in the problem as you describe in the paper.  The paper talks about how using Robust Bayes when conditioning on X=Y “degrades our inference about the coin flip” and “has led us to the claim that we can say nothing at all about the coin ﬂip”. Does that have to be the case however, because while conditioning on X=Y does mean that p({X=1}|{X=Y}I) =  p({Y=1}|{X=Y}I), I don’t see why it has to mean that both have the same π-distribution where Pr(Y = 1) = π.


Which type of inference is being done about Y in the problem?


If you are trying to make an inference on the results of the fight between the boxer and the wrestler that has already happened, in which your friend tells you that either the boxer won and he flipped heads with a coin or the boxer lost a</p><p>5 0.95190907 <a title="2294-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-24-Yet_another_Bayesian_job_opportunity.html">231 andrew gelman stats-2010-08-24-Yet another Bayesian job opportunity</a></p>
<p>Introduction: Steve Cohen writes:
  
My [Cohen's] firm is looking for strong candidates to help us in developing software and analyzing data using Bayesian methods.


We have been developing a suite of programs in C++ which allow us to do Bayesian hierarchical regression and logit/probit models on marketing data. These efforts have included the use of high performance computing tools like nVidia’s CUDA and the new OpenCL standard, which allow parallel processing of Bayesian models. Our software is very, very fast – even on databases that are ½ terabyte in size. The software still needs many additions and improvements and a person with the right skill set will have the chance to make a significant contribution.
  
Here’s the job description he sent:
  
 
Bayesian statistician and C++ programmer


The company


In4mation Insights is a marketing research, analytics, and consulting firm which operates on the leading-edge of our industry.  Our clients are Fortune 500 companies and major management consul</p><p>6 0.95170784 <a title="2294-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-31-He%E2%80%99s_getting_ready_to_write_a_book.html">1783 andrew gelman stats-2013-03-31-He’s getting ready to write a book</a></p>
<p>7 0.95005202 <a title="2294-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-Piss-poor_monocausal_social_science.html">1196 andrew gelman stats-2012-03-04-Piss-poor monocausal social science</a></p>
<p>8 0.94979584 <a title="2294-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-30-I_suppose_it%E2%80%99s_too_late_to_add_Turing%E2%80%99s_run-around-the-house-chess_to_the_2012_London_Olympics%3F.html">1290 andrew gelman stats-2012-04-30-I suppose it’s too late to add Turing’s run-around-the-house-chess to the 2012 London Olympics?</a></p>
<p>9 0.9467178 <a title="2294-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-17-Sports_examples_in_class.html">1173 andrew gelman stats-2012-02-17-Sports examples in class</a></p>
<p>10 0.94270861 <a title="2294-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-21-Building_a_regression_model_._._._with_only_27_data_points.html">1506 andrew gelman stats-2012-09-21-Building a regression model . . . with only 27 data points</a></p>
<p>11 0.94257635 <a title="2294-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-14-Question_4_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1320 andrew gelman stats-2012-05-14-Question 4 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>12 0.94242442 <a title="2294-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-11-Update_on_the_spam_email_study.html">27 andrew gelman stats-2010-05-11-Update on the spam email study</a></p>
<p>13 0.94110167 <a title="2294-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-04-One_more_thought_on_Hoover_historian_Niall_Ferguson%E2%80%99s_thing_about_Keynes_being_gay_and_marrying_a_ballerina_and_talking_about_poetry.html">1840 andrew gelman stats-2013-05-04-One more thought on Hoover historian Niall Ferguson’s thing about Keynes being gay and marrying a ballerina and talking about poetry</a></p>
<p>14 0.94033277 <a title="2294-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-22-A_Bayesian_model_for_an_increasing_function%2C_in_Stan%21.html">2110 andrew gelman stats-2013-11-22-A Bayesian model for an increasing function, in Stan!</a></p>
<p>15 0.94029003 <a title="2294-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-21-Baseball%E2%80%99s_greatest_fielders.html">623 andrew gelman stats-2011-03-21-Baseball’s greatest fielders</a></p>
<p>16 0.94027972 <a title="2294-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-04-Recently_in_the_sister_blog.html">2088 andrew gelman stats-2013-11-04-Recently in the sister blog</a></p>
<p>17 0.93939221 <a title="2294-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-29-What_is_expected_of_a_consultant.html">1597 andrew gelman stats-2012-11-29-What is expected of a consultant</a></p>
<p>18 0.93929648 <a title="2294-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-13-Stan%21.html">1855 andrew gelman stats-2013-05-13-Stan!</a></p>
<p>19 0.9389838 <a title="2294-lda-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-31-Into_the_thicket_of_variation%3A__More_on_the_political_orientations_of_parents_of_sons_and_daughters%2C_and_a_return_to_the_tradeoff_between_internal_and_external_validity_in_design_and_interpretation_of_research_studies.html">2193 andrew gelman stats-2014-01-31-Into the thicket of variation:  More on the political orientations of parents of sons and daughters, and a return to the tradeoff between internal and external validity in design and interpretation of research studies</a></p>
<p>20 0.93841314 <a title="2294-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-23-When_are_complicated_models_helpful_in_psychology_research_and_when_are_they_overkill%3F.html">1690 andrew gelman stats-2013-01-23-When are complicated models helpful in psychology research and when are they overkill?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
