<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2349 andrew gelman stats-2014-05-26-WAIC and cross-validation in Stan!</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2014" href="../home/andrew_gelman_stats-2014_home.html">andrew_gelman_stats-2014</a> <a title="andrew_gelman_stats-2014-2349" href="#">andrew_gelman_stats-2014-2349</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2349 andrew gelman stats-2014-05-26-WAIC and cross-validation in Stan!</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2014-2349-html" href="http://andrewgelman.com/2014/05/26/waic-cross-validation-stan/">html</a></p><p>Introduction: Aki and I  write :
  
The Watanabe-Akaike information criterion (WAIC) and cross-validation are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model. WAIC is based on the series expansion of leave-one-out cross-validation (LOO), and asymptotically they are equal. With finite data, WAIC and cross-validation address different predictive questions and thus it is useful to be able to compute both. WAIC and an importance-sampling approximated LOO can be estimated directly using the log-likelihood evaluated at the posterior simulations of the parameter values. We show how to compute WAIC, IS-LOO, K-fold cross-validation, and related diagnostic quantities in the Bayesian inference package Stan as called from R.
  
This is important, I think.  One reason the deviance information criterion (DIC) has been so popular is its implementation in Bugs.  We think WAIC and cross-validation make more sense than DIC, especially from a Bayesian perspective in whic</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Aki and I  write :    The Watanabe-Akaike information criterion (WAIC) and cross-validation are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model. [sent-1, score-0.659]
</p><p>2 WAIC is based on the series expansion of leave-one-out cross-validation (LOO), and asymptotically they are equal. [sent-2, score-0.246]
</p><p>3 With finite data, WAIC and cross-validation address different predictive questions and thus it is useful to be able to compute both. [sent-3, score-0.361]
</p><p>4 WAIC and an importance-sampling approximated LOO can be estimated directly using the log-likelihood evaluated at the posterior simulations of the parameter values. [sent-4, score-0.564]
</p><p>5 We show how to compute WAIC, IS-LOO, K-fold cross-validation, and related diagnostic quantities in the Bayesian inference package Stan as called from R. [sent-5, score-0.496]
</p><p>6 One reason the deviance information criterion (DIC) has been so popular is its implementation in Bugs. [sent-7, score-0.574]
</p><p>7 We think WAIC and cross-validation make more sense than DIC, especially from a Bayesian perspective in which inference comes as a posterior distribution rather than a point estimate, and we hope that this and future Stan implementations will allow users to become more familiar with these tools. [sent-8, score-0.716]
</p><p>8 In addition to the implementation, the paper discusses some challenges of interpretation with hierarchical models, demonstrating with the canonical 8 schools example. [sent-9, score-0.602]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('waic', 0.676), ('loo', 0.293), ('dic', 0.22), ('criterion', 0.184), ('implementation', 0.159), ('compute', 0.149), ('pointwise', 0.133), ('approximated', 0.126), ('stan', 0.116), ('canonical', 0.113), ('implementations', 0.11), ('posterior', 0.11), ('diagnostic', 0.105), ('deviance', 0.105), ('bayesian', 0.104), ('asymptotically', 0.103), ('demonstrating', 0.093), ('aki', 0.093), ('expansion', 0.087), ('quantities', 0.087), ('evaluated', 0.085), ('inference', 0.085), ('finite', 0.082), ('simulations', 0.076), ('challenges', 0.075), ('fitted', 0.075), ('discusses', 0.072), ('users', 0.072), ('accuracy', 0.072), ('information', 0.07), ('package', 0.07), ('address', 0.069), ('schools', 0.067), ('familiar', 0.065), ('prediction', 0.064), ('interpretation', 0.063), ('allow', 0.062), ('addition', 0.062), ('estimating', 0.061), ('predictive', 0.061), ('estimated', 0.059), ('parameter', 0.058), ('hierarchical', 0.057), ('series', 0.056), ('popular', 0.056), ('future', 0.055), ('become', 0.054), ('hope', 0.052), ('perspective', 0.051), ('directly', 0.05)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="2349-tfidf-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-26-WAIC_and_cross-validation_in_Stan%21.html">2349 andrew gelman stats-2014-05-26-WAIC and cross-validation in Stan!</a></p>
<p>Introduction: Aki and I  write :
  
The Watanabe-Akaike information criterion (WAIC) and cross-validation are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model. WAIC is based on the series expansion of leave-one-out cross-validation (LOO), and asymptotically they are equal. With finite data, WAIC and cross-validation address different predictive questions and thus it is useful to be able to compute both. WAIC and an importance-sampling approximated LOO can be estimated directly using the log-likelihood evaluated at the posterior simulations of the parameter values. We show how to compute WAIC, IS-LOO, K-fold cross-validation, and related diagnostic quantities in the Bayesian inference package Stan as called from R.
  
This is important, I think.  One reason the deviance information criterion (DIC) has been so popular is its implementation in Bugs.  We think WAIC and cross-validation make more sense than DIC, especially from a Bayesian perspective in whic</p><p>2 0.42952365 <a title="2349-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-09-Understanding_predictive_information_criteria_for_Bayesian_models.html">1975 andrew gelman stats-2013-08-09-Understanding predictive information criteria for Bayesian models</a></p>
<p>Introduction: Jessy, Aki, and I  write :
  
We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a bias-corrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this review is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.
  
I like this paper.  It came about as a result of preparing Chapter 7 for the  new BDA .  I had difficulty understanding AIC, DIC, WAIC, etc., but I recognized that these methods served a need.  My first plan was to just apply DIC and WAIC on a couple of simple examples (a linear regression and the 8 schools) and leave it at that.  But when I did the calculations, I couldnâ&euro;&trade;t understand the resu</p><p>3 0.23865055 <a title="2349-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-22-Deviance%2C_DIC%2C_AIC%2C_cross-validation%2C_etc.html">776 andrew gelman stats-2011-06-22-Deviance, DIC, AIC, cross-validation, etc</a></p>
<p>Introduction: The deviance information criterion (or DIC) is an idea of Brad Carlin and others for comparing the fits of models estimated using Bayesian simulation (for more information, see  this article  by Angelika van der Linde).
 
I don’t really ever know what to make of DIC.  On one hand, it seems sensible, it handles uncertainty in inferences within each model, and it does not depend on aspects of the models that don’t affect inferences within each model (unlike Bayes factors; see discussion  here ).  On the other hand, I don’t really have any idea what I would do with DIC in any real example.  In our book we included an example of DIC–people use it and we don’t have any great alternatives–but I had to be pretty careful that the example made sense.  Unlike the usual setting where we use a method and that gives us insight into a problem, here we used our insight into the problem to make sure that in this particular case the method gave a reasonable answer.
 
One of my practical problems with D</p><p>4 0.20228778 <a title="2349-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-24-New_ideas_on_DIC_from_Martyn_Plummer_and_Sumio_Watanabe.html">778 andrew gelman stats-2011-06-24-New ideas on DIC from Martyn Plummer and Sumio Watanabe</a></p>
<p>Introduction: Martyn Plummer  replied  to my recent  blog  on DIC with information that was important enough that I thought it deserved its own blog entry.  Martyn wrote:
  
DIC has been around for 10 years now and despite being immensely popular with applied statisticians it has generated very little theoretical interest. In fact, the silence has been deafening. I [Martyn] hope my paper added some clarity.


As you say, DIC is (an approximation to) a theoretical out-of-sample predictive error. When I finished the paper I was a little embarrassed to see that I had almost perfectly reconstructed the justification of AIC as approximate cross-validation measure by Stone (1977), with a Bayesian spin of course.


But even this insight leaves a lot of choices open. You need to choose the right loss function and also which level of the model you want to replicate from. David Spiegelhalter and colleagues called this the “focus”. In practice the focus is limited to the lowest level of the model. You generall</p><p>5 0.19520542 <a title="2349-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-02-A_important_new_survey_of_Bayesian_predictive_methods_for_model_assessment%2C_selection_and_comparison.html">1648 andrew gelman stats-2013-01-02-A important new survey of Bayesian predictive methods for model assessment, selection and comparison</a></p>
<p>Introduction: Aki Vehtari and Janne Ojanen just published a  long paper  that begins:
  
To date, several methods exist in the statistical literature for model assessment, which purport themselves specifically as Bayesian predictive methods. The decision theoretic assumptions on which these methods are based are not always clearly stated in the original articles, however. The aim of this survey is to provide a unified review of Bayesian predictive model assessment and selection methods, and of methods closely related to them. We review the various assumptions that are made in this context and discuss the connections between different approaches, with an emphasis on how each method approximates the expected utility of using a Bayesian model for the purpose of predicting future data.
  
AIC (which Akaike called “An Information Criterion”) is the starting point for all these methods.  More recently, Watanabe came up with WAIC (which he called the “Widely Available Information Criterion”).  In between t</p><p>6 0.17956018 <a title="2349-tfidf-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-26-On_deck_this_week.html">2348 andrew gelman stats-2014-05-26-On deck this week</a></p>
<p>7 0.1684521 <a title="2349-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-15-More_on_AIC%2C_WAIC%2C_etc.html">1983 andrew gelman stats-2013-08-15-More on AIC, WAIC, etc</a></p>
<p>8 0.10801937 <a title="2349-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-22-My_talks_that_were_scheduled_for_Tues_at_the_Data_Skeptics_meetup_and_Wed_at_the_Open_Statistical_Programming_meetup.html">1950 andrew gelman stats-2013-07-22-My talks that were scheduled for Tues at the Data Skeptics meetup and Wed at the Open Statistical Programming meetup</a></p>
<p>9 0.10204703 <a title="2349-tfidf-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>10 0.10165203 <a title="2349-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>11 0.093414083 <a title="2349-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>12 0.091319382 <a title="2349-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-04-PyStan%21.html">1748 andrew gelman stats-2013-03-04-PyStan!</a></p>
<p>13 0.090316951 <a title="2349-tfidf-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-11-Bayes_in_the_research_conversation.html">2368 andrew gelman stats-2014-06-11-Bayes in the research conversation</a></p>
<p>14 0.089181095 <a title="2349-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-19-Whassup_with_deviance_having_a_high_posterior_correlation_with_a_parameter_in_the_model%3F.html">1221 andrew gelman stats-2012-03-19-Whassup with deviance having a high posterior correlation with a parameter in the model?</a></p>
<p>15 0.086108953 <a title="2349-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-A_Stan_is_Born.html">1475 andrew gelman stats-2012-08-30-A Stan is Born</a></p>
<p>16 0.085191339 <a title="2349-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<p>17 0.084385172 <a title="2349-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-05-Stan_%28quietly%29_passes_512_people_on_the_users_list.html">2124 andrew gelman stats-2013-12-05-Stan (quietly) passes 512 people on the users list</a></p>
<p>18 0.080970526 <a title="2349-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-24-Estimating_and_summarizing_inference_for_hierarchical_variance_parameters_when_the_number_of_groups_is_small.html">2145 andrew gelman stats-2013-12-24-Estimating and summarizing inference for hierarchical variance parameters when the number of groups is small</a></p>
<p>19 0.080345549 <a title="2349-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-13-A_question_about_AIC.html">1377 andrew gelman stats-2012-06-13-A question about AIC</a></p>
<p>20 0.079576656 <a title="2349-tfidf-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-07-My_recent_debugging_experience.html">2161 andrew gelman stats-2014-01-07-My recent debugging experience</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.11), (1, 0.137), (2, -0.028), (3, 0.053), (4, -0.008), (5, 0.031), (6, -0.032), (7, -0.066), (8, -0.042), (9, -0.072), (10, -0.045), (11, -0.007), (12, -0.054), (13, 0.016), (14, 0.02), (15, 0.002), (16, 0.009), (17, 0.023), (18, -0.013), (19, 0.033), (20, -0.002), (21, 0.029), (22, 0.031), (23, -0.0), (24, 0.07), (25, 0.009), (26, 0.006), (27, 0.016), (28, -0.001), (29, 0.021), (30, -0.007), (31, 0.038), (32, 0.052), (33, -0.018), (34, 0.01), (35, 0.016), (36, 0.016), (37, -0.038), (38, 0.02), (39, -0.019), (40, -0.029), (41, 0.036), (42, -0.054), (43, -0.022), (44, 0.002), (45, -0.02), (46, 0.028), (47, 0.002), (48, -0.021), (49, -0.004)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95993054 <a title="2349-lsi-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-26-WAIC_and_cross-validation_in_Stan%21.html">2349 andrew gelman stats-2014-05-26-WAIC and cross-validation in Stan!</a></p>
<p>Introduction: Aki and I  write :
  
The Watanabe-Akaike information criterion (WAIC) and cross-validation are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model. WAIC is based on the series expansion of leave-one-out cross-validation (LOO), and asymptotically they are equal. With finite data, WAIC and cross-validation address different predictive questions and thus it is useful to be able to compute both. WAIC and an importance-sampling approximated LOO can be estimated directly using the log-likelihood evaluated at the posterior simulations of the parameter values. We show how to compute WAIC, IS-LOO, K-fold cross-validation, and related diagnostic quantities in the Bayesian inference package Stan as called from R.
  
This is important, I think.  One reason the deviance information criterion (DIC) has been so popular is its implementation in Bugs.  We think WAIC and cross-validation make more sense than DIC, especially from a Bayesian perspective in whic</p><p>2 0.80274391 <a title="2349-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-04-Stan_in_L.A._this_Wed_3%3A30pm.html">1749 andrew gelman stats-2013-03-04-Stan in L.A. this Wed 3:30pm</a></p>
<p>Introduction: Michael Betancourt  will be speaking at UCLA:
  
The location for refreshment is in room 51-254 CHS at 3:00 PM.


The place for the seminar is at CHS 33-105A  at 3:30pm – 4:30pm, Wed 6 Mar.


["CHS" stands for Center for Health Sciences, the building of the UCLA schools of medicine and public health.   Here's a map with directions .]


Title of talk:  Stan : Practical Bayesian Inference with Hamiltonian Monte Carlo


Abstract: Practical implementations of Bayesian inference are often limited to approximation methods that only slowly explore the posterior distribution.  By taking advantage of the curvature of the posterior, however, Hamiltonian Monte Carlo (HMC) efficiently explores even the most highly contorted distributions.  In this talk I will review the foundations of and recent developments within HMC, concluding with a discussion of Stan, a powerful inference engine that utilizes HMC, automatic differentiation, and adaptive methods to minimize user input.
  
This is cool stuff.</p><p>3 0.74578154 <a title="2349-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-19-Validation_of_Software_for_Bayesian_Models_Using_Posterior_Quantiles.html">1019 andrew gelman stats-2011-11-19-Validation of Software for Bayesian Models Using Posterior Quantiles</a></p>
<p>Introduction: I love  this stuff :
  
This article presents a simulation-based method designed to establish the computational correctness of software developed to fit a specific Bayesian model, capitalizing on properties of Bayesian posterior distributions. We illustrate the validation technique with two examples. The validation method is shown to find errors in software when they exist and, moreover, the validation output can be informative about the nature and location of such errors. We also compare our method with that of an earlier approach.
  
   
 
   
 
   
 
I hope we can put it into Stan.</p><p>4 0.72305501 <a title="2349-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-20-Stan_at_Google_this_Thurs_and_at_Berkeley_this_Fri_noon.html">1772 andrew gelman stats-2013-03-20-Stan at Google this Thurs and at Berkeley this Fri noon</a></p>
<p>Introduction: Michael Betancourt  will be speaking at Google and at the University of California, Berkeley.  The Google talk is closed to outsiders (but if you work at Google, you should go!); the Berkeley talk is open to all:
  
Friday March 22, 12:10 pm, Evans Hall 1011.


Title of talk:  Stan : Practical Bayesian Inference with Hamiltonian Monte Carlo


Abstract: Practical implementations of Bayesian inference are often limited to approximation methods that only slowly explore the posterior distribution.  By taking advantage of the curvature of the posterior, however, Hamiltonian Monte Carlo (HMC) efficiently explores even the most highly contorted distributions.  In this talk I will review the foundations of and recent developments within HMC, concluding with a discussion of Stan, a powerful inference engine that utilizes HMC, automatic differentiation, and adaptive methods to minimize user input.
  
This is cool stuff.  And heâ&euro;&trade;ll be showing the whirlpool movie!</p><p>5 0.71697938 <a title="2349-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-My_talk_at_MIT_on_Thurs_11_Oct.html">1528 andrew gelman stats-2012-10-10-My talk at MIT on Thurs 11 Oct</a></p>
<p>Introduction: Stan: open-source Bayesian inference 
 
Speaker: Andrew Gelman, Columbia University 
Date: Thursday, October 11 2012 
Time: 4:00PM to 5:00PM 
Location: 32-D507 
Host: Polina Golland, CSAIL 
Contact: Polina Golland, 6172538005, polina@csail.mit.edu
 
Stan ( mc-stan.org ) is an open-source package for obtaining Bayesian inference using the No-U-Turn sampler, a variant of Hamiltonian Monte Carlo. We discuss how Stan works and what it can do, the problems that motivated us to write Stan, current challenges, and areas of planned development, including tools for improved generality and usability, more efficient sampling algorithms, and fuller integration of model building, model checking, and model understanding in Bayesian data analysis.
 
P.S.   Hereâ&euro;&trade;s the talk .</p><p>6 0.70034724 <a title="2349-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-09-Understanding_predictive_information_criteria_for_Bayesian_models.html">1975 andrew gelman stats-2013-08-09-Understanding predictive information criteria for Bayesian models</a></p>
<p>7 0.69239789 <a title="2349-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-30-Stan_Project%3A__Continuous_Relaxations_for_Discrete_MRFs.html">2003 andrew gelman stats-2013-08-30-Stan Project:  Continuous Relaxations for Discrete MRFs</a></p>
<p>8 0.67649347 <a title="2349-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-02-A_important_new_survey_of_Bayesian_predictive_methods_for_model_assessment%2C_selection_and_comparison.html">1648 andrew gelman stats-2013-01-02-A important new survey of Bayesian predictive methods for model assessment, selection and comparison</a></p>
<p>9 0.67048413 <a title="2349-lsi-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-Running_into_a_Stan_Reference_by_Accident.html">2231 andrew gelman stats-2014-03-03-Running into a Stan Reference by Accident</a></p>
<p>10 0.66988903 <a title="2349-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-12-Samplers_for_Big_Science%3A__emcee_and_BAT.html">2020 andrew gelman stats-2013-09-12-Samplers for Big Science:  emcee and BAT</a></p>
<p>11 0.66916066 <a title="2349-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-24-New_ideas_on_DIC_from_Martyn_Plummer_and_Sumio_Watanabe.html">778 andrew gelman stats-2011-06-24-New ideas on DIC from Martyn Plummer and Sumio Watanabe</a></p>
<p>12 0.66722542 <a title="2349-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-30-Stan_uses_Nuts%21.html">1036 andrew gelman stats-2011-11-30-Stan uses Nuts!</a></p>
<p>13 0.65774173 <a title="2349-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-22-My_talks_that_were_scheduled_for_Tues_at_the_Data_Skeptics_meetup_and_Wed_at_the_Open_Statistical_Programming_meetup.html">1950 andrew gelman stats-2013-07-22-My talks that were scheduled for Tues at the Data Skeptics meetup and Wed at the Open Statistical Programming meetup</a></p>
<p>14 0.63251501 <a title="2349-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-17-Stan_and_RStan_1.1.0.html">1627 andrew gelman stats-2012-12-17-Stan and RStan 1.1.0</a></p>
<p>15 0.62374032 <a title="2349-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-06-The_new_Stan_1.1.1%2C_featuring_Gaussian_processes%21.html">1710 andrew gelman stats-2013-02-06-The new Stan 1.1.1, featuring Gaussian processes!</a></p>
<p>16 0.62344062 <a title="2349-lsi-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-05-Identifying_pathways_for_managing_multiple_disturbances_to_limit_plant_invasions.html">2360 andrew gelman stats-2014-06-05-Identifying pathways for managing multiple disturbances to limit plant invasions</a></p>
<p>17 0.62261993 <a title="2349-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-23-Validation_of_Software_for_Bayesian_Models_Using_Posterior_Quantiles.html">1868 andrew gelman stats-2013-05-23-Validation of Software for Bayesian Models Using Posterior Quantiles</a></p>
<p>18 0.62036431 <a title="2349-lsi-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-04-Generalized_Method_of_Moments%2C_whatever_that_is.html">449 andrew gelman stats-2010-12-04-Generalized Method of Moments, whatever that is</a></p>
<p>19 0.61901873 <a title="2349-lsi-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>20 0.61388034 <a title="2349-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-27-%28R-Py-Cmd%29Stan_2.1.0.html">2150 andrew gelman stats-2013-12-27-(R-Py-Cmd)Stan 2.1.0</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(13, 0.015), (16, 0.056), (20, 0.074), (24, 0.11), (37, 0.019), (59, 0.016), (61, 0.235), (82, 0.046), (86, 0.041), (99, 0.262)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93548036 <a title="2349-lda-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-26-WAIC_and_cross-validation_in_Stan%21.html">2349 andrew gelman stats-2014-05-26-WAIC and cross-validation in Stan!</a></p>
<p>Introduction: Aki and I  write :
  
The Watanabe-Akaike information criterion (WAIC) and cross-validation are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model. WAIC is based on the series expansion of leave-one-out cross-validation (LOO), and asymptotically they are equal. With finite data, WAIC and cross-validation address different predictive questions and thus it is useful to be able to compute both. WAIC and an importance-sampling approximated LOO can be estimated directly using the log-likelihood evaluated at the posterior simulations of the parameter values. We show how to compute WAIC, IS-LOO, K-fold cross-validation, and related diagnostic quantities in the Bayesian inference package Stan as called from R.
  
This is important, I think.  One reason the deviance information criterion (DIC) has been so popular is its implementation in Bugs.  We think WAIC and cross-validation make more sense than DIC, especially from a Bayesian perspective in whic</p><p>2 0.93074107 <a title="2349-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-02-Not_so_fast_on_levees_and_seawalls_for_NY_harbor%3F.html">1558 andrew gelman stats-2012-11-02-Not so fast on levees and seawalls for NY harbor?</a></p>
<p>Introduction: I was talking with  June Williamson  and mentioned offhand that I’d seen  something in the paper  saying that if only we’d invested a few billion dollars in levees we would’ve saved zillions in economic damage from the flood.  (A quick search also revealed  this  eerily prescient article from last month and, more recently,  this  online discussion.)
 
June said, No, no, no:  levees are not the way to go:
  
 Here  and  here  are the articles on “soft infrastructure” for the New York-New Jersey Harbor I was mentioning, summarizing work that is more extensively published in two books, “Rising Currents” and “On the Water: Palisade Bay”:

 
The hazards posed by climate change, sea level rise, and severe storm surges make this the time to transform our coastal cities through adaptive design. The conventional response to flooding, in recent history, has been hard engineering — fortifying the coastal infrastructure with seawalls and bulkheads to protect real estate at the expense of natural t</p><p>3 0.91106045 <a title="2349-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-04-Burgess_on_Kipling.html">16 andrew gelman stats-2010-05-04-Burgess on Kipling</a></p>
<p>Introduction: This is my last entry derived from  Anthony Burgess’s book reviews , and it’ll be short.  His review of Angus Wilson’s “The Strange Ride of Rudyard Kipling:  His Life and Works” is a wonderfully balanced little thing.  Nothing incredibly deep–like most items in the collection, the review is only two pages long–but I give it credit for being a rare piece of Kipling criticism I’ve seen that (a) seriously engages with the politics, without (b) congratulating itself on bravely going against the fashions of the politically incorrect chattering classes by celebrating Kipling’s magnificent achievement blah blah blah.  Instead, Burgess shows respect for Kipling’s work and puts it in historical, biographical, and literary context.
 
Burgess concludes that Wilson’s book “reminds us, in John Gross’s words, that Kipling ‘remains a haunting, unsettling presence, with whom we still have to come to terms.’  Still.”  Well put, and generous of Burgess to end his review with another’s quote.
 
Other cri</p><p>4 0.90071541 <a title="2349-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-26-Tenure_lets_you_handle_students_who_cheat.html">1028 andrew gelman stats-2011-11-26-Tenure lets you handle students who cheat</a></p>
<p>Introduction: The other day, a friend of mine who is an untenured professor (not in statistics or political science) was telling me about a class where many of the students seemed to be resubmitting papers that they had already written for previous classes.  (The supposition was based on internal evidence of the topics of the submitted papers.)  It would be possible to check this and then kick the cheating students out of the program—but why do it?  It would be a lot of work, also some of the students who are caught might complain, then word would get around that my friend is a troublemaker.  And nobody likes a troublemaker.
 
Once my friend has tenure it would be possible to do the right thing.  But . . . here’s the hitch:  most college instructors do  not  have tenure, and one result, I suspect, is a decline in ethical standards.
 
This is something I hadn’t thought of in our  earlier discussion  of job security for teachers:  tenure gives you the freedom to kick out cheating students.</p><p>5 0.89791203 <a title="2349-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-09-Understanding_predictive_information_criteria_for_Bayesian_models.html">1975 andrew gelman stats-2013-08-09-Understanding predictive information criteria for Bayesian models</a></p>
<p>Introduction: Jessy, Aki, and I  write :
  
We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a bias-corrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this review is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.
  
I like this paper.  It came about as a result of preparing Chapter 7 for the  new BDA .  I had difficulty understanding AIC, DIC, WAIC, etc., but I recognized that these methods served a need.  My first plan was to just apply DIC and WAIC on a couple of simple examples (a linear regression and the 8 schools) and leave it at that.  But when I did the calculations, I couldnâ&euro;&trade;t understand the resu</p><p>6 0.88938332 <a title="2349-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-07-Duncan_Watts_and_the_Titanic.html">1370 andrew gelman stats-2012-06-07-Duncan Watts and the Titanic</a></p>
<p>7 0.88744444 <a title="2349-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-04-28-But_it_all_goes_to_pay_for_gas%2C_car_insurance%2C_and_tolls_on_the_turnpike.html">9 andrew gelman stats-2010-04-28-But it all goes to pay for gas, car insurance, and tolls on the turnpike</a></p>
<p>8 0.88082051 <a title="2349-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-09-The_difference_between_%E2%80%9Csignificant%E2%80%9D_and_%E2%80%9Cnon-significant%E2%80%9D_is_not_itself_statistically_significant.html">1662 andrew gelman stats-2013-01-09-The difference between “significant” and “non-significant” is not itself statistically significant</a></p>
<p>9 0.8780452 <a title="2349-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-16-NYT_Labs_releases_Openpaths%2C_a_utility_for_saving_your_iphone_data.html">714 andrew gelman stats-2011-05-16-NYT Labs releases Openpaths, a utility for saving your iphone data</a></p>
<p>10 0.86806208 <a title="2349-lda-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-07-Environmentally_induced_cancer_%E2%80%9Cgrossly_underestimated%E2%80%9D%3F__Doubtful..html">21 andrew gelman stats-2010-05-07-Environmentally induced cancer “grossly underestimated”?  Doubtful.</a></p>
<p>11 0.86401927 <a title="2349-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-28-Amusing_case_of_self-defeating_science_writing.html">827 andrew gelman stats-2011-07-28-Amusing case of self-defeating science writing</a></p>
<p>12 0.85414439 <a title="2349-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-09-Partial_least_squares_path_analysis.html">1714 andrew gelman stats-2013-02-09-Partial least squares path analysis</a></p>
<p>13 0.84272981 <a title="2349-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-28-LOL_without_the_CATS.html">1433 andrew gelman stats-2012-07-28-LOL without the CATS</a></p>
<p>14 0.83579624 <a title="2349-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-22-Deviance%2C_DIC%2C_AIC%2C_cross-validation%2C_etc.html">776 andrew gelman stats-2011-06-22-Deviance, DIC, AIC, cross-validation, etc</a></p>
<p>15 0.82958555 <a title="2349-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-06-Poverty%2C_educational_performance_%E2%80%93_and_can_be_done_about_it.html">561 andrew gelman stats-2011-02-06-Poverty, educational performance – and can be done about it</a></p>
<p>16 0.82653177 <a title="2349-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-14-Oswald_evidence.html">2134 andrew gelman stats-2013-12-14-Oswald evidence</a></p>
<p>17 0.82421017 <a title="2349-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>18 0.81719667 <a title="2349-lda-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-01-%E2%80%9CThough_They_May_Be_Unaware%2C_Newlyweds_Implicitly_Know_Whether_Their_Marriage_Will_Be_Satisfying%E2%80%9D.html">2156 andrew gelman stats-2014-01-01-“Though They May Be Unaware, Newlyweds Implicitly Know Whether Their Marriage Will Be Satisfying”</a></p>
<p>19 0.81676334 <a title="2349-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-23-More_on_Bayesian_methods_and_multilevel_modeling.html">2033 andrew gelman stats-2013-09-23-More on Bayesian methods and multilevel modeling</a></p>
<p>20 0.80745769 <a title="2349-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-24-Deviance_as_a_difference.html">729 andrew gelman stats-2011-05-24-Deviance as a difference</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
