<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2176 andrew gelman stats-2014-01-19-Transformations for non-normal data</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2014" href="../home/andrew_gelman_stats-2014_home.html">andrew_gelman_stats-2014</a> <a title="andrew_gelman_stats-2014-2176" href="#">andrew_gelman_stats-2014-2176</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2176 andrew gelman stats-2014-01-19-Transformations for non-normal data</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2014-2176-html" href="http://andrewgelman.com/2014/01/19/transformations-for-non-normal-data/">html</a></p><p>Introduction: Steve Peterson writes:
  
I recently submitted a proposal on applying a Bayesian analysis to gender comparisons on motivational constructs. I had an idea on how to improve the model I used and was hoping you could give me some feedback.


The data come from a survey based on 5-point Likert scales. Different constructs are measured for each student as scores derived from averaging a student’s responses on particular subsets of survey questions. (I suppose it is not uncontroversial to treat these scores as interval measures and would be interested to hear if you have any objections.) I am comparing genders on each construct. Researchers typically use t-tests to do so.


To use a Bayesian approach I applied the programs written in R and JAGS by John Kruschke for estimating the difference of means:


http://www.indiana.edu/~kruschke/BEST/


An issue in that analysis is that the distributions of student scores are not normal. There was skewness in some of the distributions and not always in</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Different constructs are measured for each student as scores derived from averaging a student’s responses on particular subsets of survey questions. [sent-4, score-0.502]
</p><p>2 (I suppose it is not uncontroversial to treat these scores as interval measures and would be interested to hear if you have any objections. [sent-5, score-0.451]
</p><p>3 To use a Bayesian approach I applied the programs written in R and JAGS by John Kruschke for estimating the difference of means:   http://www. [sent-8, score-0.221]
</p><p>4 edu/~kruschke/BEST/   An issue in that analysis is that the distributions of student scores are not normal. [sent-10, score-0.455]
</p><p>5 There was skewness in some of the distributions and not always in the same direction. [sent-11, score-0.245]
</p><p>6 I wanted to find a distribution I could use for the likelihood that could better accommodate this sort of data. [sent-12, score-0.503]
</p><p>7 What seems nice about betas for this type of data is that they can be skewed in either direction and can even be bimodal. [sent-14, score-0.47]
</p><p>8 I get very similar results as to those using t distributions when using data that is fairly normal, but I also get post predictive graphs that suggest that the betas are much better when I use skewed data. [sent-16, score-0.823]
</p><p>9 It seems to me that a good model for this sort of data (scores constructed as averages of responses on Likert scales) would be of interest to social science researchers. [sent-19, score-0.277]
</p><p>10 If you are rescaling, I think it might make more sense to just rank all the data and then transform to z-scores, as everyone knows how to think about the normal distribution. [sent-21, score-0.259]
</p><p>11 Finally, I’d be very wary of trying to use the beta distribution to capture bimodality. [sent-22, score-0.575]
</p><p>12 The trouble is that the bimodality that you find can be very sensitive to the parametric form that you are assuming. [sent-24, score-0.306]
</p><p>13 To which Steve responded:    I guess I was hung up on having a very accurate model for the data. [sent-26, score-0.251]
</p><p>14 In frequentist statistics we rely on the CLT to give us approximate normality in the sampling distribution almost regardless of the shape of the population distributions when working with very large samples. [sent-27, score-0.68]
</p><p>15 I suppose what you are saying is that Bayesian estimation with a normal likelihood is also robust to departures from normality, so I shouldn’t worry. [sent-29, score-0.324]
</p><p>16 I do think an accurate model for the data can be a good thing. [sent-30, score-0.273]
</p><p>17 I just think that the best way to get there is with regression models or mixture models rather than trying to get cute by, for example, using the parameters of the beta distribution as a way to catch multi modality. [sent-31, score-0.538]
</p><p>18 I have the same feelings about this, as I do about the use of fifth-degree polynomial regression models to capture nonlinear patterns. [sent-32, score-0.227]
</p><p>19 In either case, I’d rather fit the relevant aspect of the data directly, rather than trying to get lucky and hope it happens to fit some family of curves that happens to be sitting around. [sent-33, score-0.616]
</p><p>20 It’s just been my experience that if you’re interested in multimodality, an explicit mixture model is the way to go. [sent-35, score-0.267]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('betas', 0.238), ('scores', 0.19), ('likert', 0.176), ('bimodality', 0.176), ('normal', 0.163), ('kruschke', 0.159), ('distributions', 0.157), ('normality', 0.153), ('distribution', 0.152), ('skewed', 0.136), ('parametric', 0.13), ('jags', 0.128), ('curves', 0.123), ('deviations', 0.121), ('use', 0.119), ('shape', 0.119), ('beta', 0.119), ('averaging', 0.11), ('capture', 0.108), ('student', 0.108), ('programs', 0.102), ('mixture', 0.102), ('interval', 0.1), ('steve', 0.1), ('frequentist', 0.099), ('data', 0.096), ('means', 0.095), ('responses', 0.094), ('bayesian', 0.091), ('accurate', 0.09), ('multimodality', 0.088), ('multi', 0.088), ('skewness', 0.088), ('model', 0.087), ('fit', 0.085), ('comparing', 0.083), ('uncontroversial', 0.083), ('departures', 0.083), ('motivational', 0.083), ('genders', 0.083), ('peterson', 0.079), ('disdain', 0.079), ('interested', 0.078), ('likelihood', 0.078), ('accommodate', 0.077), ('bimodal', 0.077), ('trying', 0.077), ('better', 0.077), ('happens', 0.075), ('hung', 0.074)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="2176-tfidf-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-Transformations_for_non-normal_data.html">2176 andrew gelman stats-2014-01-19-Transformations for non-normal data</a></p>
<p>Introduction: Steve Peterson writes:
  
I recently submitted a proposal on applying a Bayesian analysis to gender comparisons on motivational constructs. I had an idea on how to improve the model I used and was hoping you could give me some feedback.


The data come from a survey based on 5-point Likert scales. Different constructs are measured for each student as scores derived from averaging a student’s responses on particular subsets of survey questions. (I suppose it is not uncontroversial to treat these scores as interval measures and would be interested to hear if you have any objections.) I am comparing genders on each construct. Researchers typically use t-tests to do so.


To use a Bayesian approach I applied the programs written in R and JAGS by John Kruschke for estimating the difference of means:


http://www.indiana.edu/~kruschke/BEST/


An issue in that analysis is that the distributions of student scores are not normal. There was skewness in some of the distributions and not always in</p><p>2 0.20369205 <a title="2176-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-Another_reason_why_you_can_get_good_inferences_from_a_bad_model.html">1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</a></p>
<p>Introduction: John Cook  considers  how people justify probability distribution assumptions:
  
Sometimes distribution assumptions are not justified.


Sometimes distributions can be derived from fundamental principles [or] . . . on theoretical grounds. For example, large samples and the central limit theorem together may justify assuming that something is normally distributed.


Often the choice of distribution is somewhat arbitrary, chosen by intuition or for convenience, and then empirically shown to work well enough.


Sometimes a distribution can be a bad fit and still work well, depending on what you’re asking of it.
  
Cook continues:
  
The last point is particularly interesting. It’s not hard to imagine that a poor fit would produce poor results. It’s surprising when a poor fit produces good results.
  
And then he gives an example of an effective but inaccurate model used to model survival times in a clinical trial.  Cook explains:
  
The [poorly-fitting] method works well because of the q</p><p>3 0.1930868 <a title="2176-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>Introduction: My (coauthored) books on Bayesian data analysis and applied regression are like almost all the other statistics textbooks out there, in that we spend most of our time on the basic distributions such as normal and logistic and then, only as an aside, discuss robust models such as t and robit.
 
Why aren’t the t and robit front and center?  Sure, I can see starting with the normal (at least in the Bayesian book, where we actually work out all the algebra), but then why don’t we move on immediately to the real stuff?
 
This isn’t just (or mainly) a question of textbooks or teaching; I’m really thinking here about statistical practice.  My statistical practice.  Should t and robit be the default?  If not, why not?
 
Some possible answers:
  
10.  Estimating the degrees of freedom in the error distribution isn’t so easy, and throwing this extra parameter into the model could make inference unstable.


9.  Real data usually don’t have outliers.  In practice, fitting a robust model costs you</p><p>4 0.1770519 <a title="2176-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>Introduction: Lots of good statistical methods make use of two models.  For example:
 
- Classical statistics:  estimates and standard errors using the likelihood function; tests and p-values using the sampling distribution.  (The sampling distribution is  not  equivalent to the likelihood, as has been much discussed, for example in sequential stopping problems.)
 
- Bayesian data analysis:  inference using the posterior distribution; model checking using the predictive distribution (which, again, depends on the data-generating process in a way that the likelihood does not).
 
- Machine learning:  estimation using the data; evaluation using cross-validation (which requires some rule for partitioning the data, a rule that stands outside of the data themselves).
 
- Bootstrap, jackknife, etc:  estimation using an “estimator” (which, I would argue, is based in some sense on a model for the data), uncertainties using resampling (which, I would argue, is close to the idea of a “sampling distribution” in</p><p>5 0.16982818 <a title="2176-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>Introduction: Larry Wasserman  refers  to finite mixture models as “beasts” and  writes  jokes that they “should be avoided at all costs.”
 
I’ve thought a lot about mixture models, ever since using them in an  analysis  of voting patterns that was published in 1990.  First off, I’d like to say that our model was useful so I’d prefer not to pay the cost of avoiding it.  For a quick description of our mixture model and its context, see pp. 379-380 of my  article  in the Jim Berger volume).  Actually, our case was particularly difficult because we were not even fitting a mixture model to data, we were fitting it to latent data and using the model to perform partial pooling.  My difficulties in trying to fit this model inspired our discussion of mixture models in Bayesian Data Analysis (page 109 in the second edition, in the section on “Counterexamples to the theorems”).
 
I agree with Larry that if you’re fitting a mixture model, it’s good to be aware of the problems that arise if you try to estimate</p><p>6 0.16978431 <a title="2176-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>7 0.15999138 <a title="2176-tfidf-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-22-Philosophy_of_Bayes_and_non-Bayes%3A__A_dialogue_with_Deborah_Mayo.html">291 andrew gelman stats-2010-09-22-Philosophy of Bayes and non-Bayes:  A dialogue with Deborah Mayo</a></p>
<p>8 0.15718329 <a title="2176-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-18-Multimodality_in_hierarchical_models.html">916 andrew gelman stats-2011-09-18-Multimodality in hierarchical models</a></p>
<p>9 0.15452147 <a title="2176-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-01-I%E2%80%99ll_say_it_again.html">2046 andrew gelman stats-2013-10-01-I’ll say it again</a></p>
<p>10 0.15086481 <a title="2176-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>11 0.147461 <a title="2176-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-09-How_to_model_distributions_that_have_outliers_in_one_direction.html">2128 andrew gelman stats-2013-12-09-How to model distributions that have outliers in one direction</a></p>
<p>12 0.14202549 <a title="2176-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>13 0.13629068 <a title="2176-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-21-The_future_%28and_past%29_of_statistical_sciences.html">2072 andrew gelman stats-2013-10-21-The future (and past) of statistical sciences</a></p>
<p>14 0.13494943 <a title="2176-tfidf-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-01-How_does_Bayes_do_it%3F.html">247 andrew gelman stats-2010-09-01-How does Bayes do it?</a></p>
<p>15 0.1348286 <a title="2176-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>16 0.13237709 <a title="2176-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>17 0.13103049 <a title="2176-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-14-Progress%21__%28on_the_understanding_of_the_role_of_randomization_in_Bayesian_inference%29.html">1898 andrew gelman stats-2013-06-14-Progress!  (on the understanding of the role of randomization in Bayesian inference)</a></p>
<p>18 0.12975787 <a title="2176-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-16-%E2%80%9CReal_data_can_be_a_pain%E2%80%9D.html">1460 andrew gelman stats-2012-08-16-“Real data can be a pain”</a></p>
<p>19 0.12741609 <a title="2176-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-11-Gelman_on_Hennig_on_Gelman_on_Bayes.html">1208 andrew gelman stats-2012-03-11-Gelman on Hennig on Gelman on Bayes</a></p>
<p>20 0.12725724 <a title="2176-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-14-Everyone%E2%80%99s_trading_bias_for_variance_at_some_point%2C_it%E2%80%99s_just_done_at_different_places_in_the_analyses.html">1763 andrew gelman stats-2013-03-14-Everyone’s trading bias for variance at some point, it’s just done at different places in the analyses</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.261), (1, 0.19), (2, 0.029), (3, 0.021), (4, 0.043), (5, 0.032), (6, 0.007), (7, 0.035), (8, 0.017), (9, -0.013), (10, 0.023), (11, -0.023), (12, -0.042), (13, -0.024), (14, -0.038), (15, 0.005), (16, 0.032), (17, -0.014), (18, -0.003), (19, -0.019), (20, 0.046), (21, 0.022), (22, 0.002), (23, -0.039), (24, 0.033), (25, -0.009), (26, -0.016), (27, -0.016), (28, 0.029), (29, 0.055), (30, -0.018), (31, 0.003), (32, 0.008), (33, 0.019), (34, 0.01), (35, 0.045), (36, -0.044), (37, 0.021), (38, -0.057), (39, 0.025), (40, 0.078), (41, 0.009), (42, 0.021), (43, -0.008), (44, -0.01), (45, -0.022), (46, 0.029), (47, 0.018), (48, 0.046), (49, -0.006)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96187407 <a title="2176-lsi-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-Transformations_for_non-normal_data.html">2176 andrew gelman stats-2014-01-19-Transformations for non-normal data</a></p>
<p>Introduction: Steve Peterson writes:
  
I recently submitted a proposal on applying a Bayesian analysis to gender comparisons on motivational constructs. I had an idea on how to improve the model I used and was hoping you could give me some feedback.


The data come from a survey based on 5-point Likert scales. Different constructs are measured for each student as scores derived from averaging a student’s responses on particular subsets of survey questions. (I suppose it is not uncontroversial to treat these scores as interval measures and would be interested to hear if you have any objections.) I am comparing genders on each construct. Researchers typically use t-tests to do so.


To use a Bayesian approach I applied the programs written in R and JAGS by John Kruschke for estimating the difference of means:


http://www.indiana.edu/~kruschke/BEST/


An issue in that analysis is that the distributions of student scores are not normal. There was skewness in some of the distributions and not always in</p><p>2 0.89326549 <a title="2176-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-Another_reason_why_you_can_get_good_inferences_from_a_bad_model.html">1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</a></p>
<p>Introduction: John Cook  considers  how people justify probability distribution assumptions:
  
Sometimes distribution assumptions are not justified.


Sometimes distributions can be derived from fundamental principles [or] . . . on theoretical grounds. For example, large samples and the central limit theorem together may justify assuming that something is normally distributed.


Often the choice of distribution is somewhat arbitrary, chosen by intuition or for convenience, and then empirically shown to work well enough.


Sometimes a distribution can be a bad fit and still work well, depending on what you’re asking of it.
  
Cook continues:
  
The last point is particularly interesting. It’s not hard to imagine that a poor fit would produce poor results. It’s surprising when a poor fit produces good results.
  
And then he gives an example of an effective but inaccurate model used to model survival times in a clinical trial.  Cook explains:
  
The [poorly-fitting] method works well because of the q</p><p>3 0.87344921 <a title="2176-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-16-%E2%80%9CReal_data_can_be_a_pain%E2%80%9D.html">1460 andrew gelman stats-2012-08-16-“Real data can be a pain”</a></p>
<p>Introduction: Michael McLaughlin sent me the following query with the above title.
  
Some time ago, I [McLaughlin] was handed a dataset that needed to be modeled.  It was generated as follows:


1. Random navigation errors, historically a binary mixture of normal and Laplace with a common mean, were collected by observation.


2. Sadly, these data were recorded with too few decimal places so that the resulting quantization is clearly visible in a scatterplot.


3. The quantized data were then interpolated (to an unobserved location).


The final result looks like fuzzy points (small scale jitter) at quantized intervals spanning a much larger scale (the parent mixture distribution).  This fuzziness, likely ~normal or ~Laplace, results from the interpolation.  Otherwise, the data would look like a discrete analogue of the normal/Laplace mixture.


I would like to characterize the latent normal/Laplace mixture distribution but the quantization is “getting in the way”.  When I tried MCMC on this proble</p><p>4 0.85770482 <a title="2176-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-27-Average_predictive_comparisons_when_changing_a_pair_of_variables.html">1346 andrew gelman stats-2012-05-27-Average predictive comparisons when changing a pair of variables</a></p>
<p>Introduction: Jay Jones writes:
  
I recently came across your paper on average predictive comparisons ( Gelman and Pardoe, 2007 ) and can see many applications for this in my work (I’m an applied statistician working for Weyerhaeuser Company at our R&D; center near Seattle).  At the moment, I am using APC’s to help describe the results of a hierarchical multi-species model we fit to bird occupancy (presence/absence) data collected in the Oregon Coast Range.
  
  
  
A question that came up in our study led me to consider whether the APC framework can be used for post-hoc combinations of inputs.  For example, let’s say that after calculating the APC for each individual input in our model, we would like to look at some linear function f of two inputs of interest, u1 and u2.  Naively, I would like to be able to plug this into the APC framework.  For example, equation 5 in your paper might look something like this (for brevity, I’m omitting the summations):


Numerator:  w_ij * (E(y|u1_j, u2_j, v_i, the</p><p>5 0.83753592 <a title="2176-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-24-Analyzing_photon_counts.html">1509 andrew gelman stats-2012-09-24-Analyzing photon counts</a></p>
<p>Introduction: Via Tom LaGatta, Boris Glebov writes:
  
My labmates have statistics problem. We are all experimentalists, but need an input on a fine statistics point.


The problem is as follows. The data set consists of photon counts measured at a series of coordinates. The number of input photons is known, but the system transmission (T) is not known and needs to be estimated. The number of transmitted photons at each coordinate follows a binomial distribution, not a Gaussian one.


The spatial distribution of T values it then fit using a Levenberg-Marquart method modified to use weights for each data point.


At present, my labmates are not sure how to properly calculate and use the weights. The equations are designed for Gaussian distributions, not binomial ones, and this is a problem because in many cases the photon counts are near the edge (say, zero), where a Gaussian width is nonsensical.


Could you recommend a source they could use to guide their calculations?
  
My reply:
 
I donâ&euro;&trade;t know a</p><p>6 0.82815528 <a title="2176-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>7 0.82230741 <a title="2176-lsi-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-31-Somewhat_Bayesian_multilevel_modeling.html">246 andrew gelman stats-2010-08-31-Somewhat Bayesian multilevel modeling</a></p>
<p>8 0.81649667 <a title="2176-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-09-How_to_model_distributions_that_have_outliers_in_one_direction.html">2128 andrew gelman stats-2013-12-09-How to model distributions that have outliers in one direction</a></p>
<p>9 0.81508785 <a title="2176-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-03-Boot.html">1881 andrew gelman stats-2013-06-03-Boot</a></p>
<p>10 0.80760098 <a title="2176-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>11 0.80103606 <a title="2176-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-07-Chi-square_FAIL_when_many_cells_have_small_expected_values.html">996 andrew gelman stats-2011-11-07-Chi-square FAIL when many cells have small expected values</a></p>
<p>12 0.79833454 <a title="2176-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-25-Bayes-respecting_experimental_design_and_other_things.html">1955 andrew gelman stats-2013-07-25-Bayes-respecting experimental design and other things</a></p>
<p>13 0.79340965 <a title="2176-lsi-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-07-Bayesian_hierarchical_model_for_the_prediction_of_soccer_results.html">20 andrew gelman stats-2010-05-07-Bayesian hierarchical model for the prediction of soccer results</a></p>
<p>14 0.79290277 <a title="2176-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-11-Gelman_on_Hennig_on_Gelman_on_Bayes.html">1208 andrew gelman stats-2012-03-11-Gelman on Hennig on Gelman on Bayes</a></p>
<p>15 0.79226494 <a title="2176-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<p>16 0.78751153 <a title="2176-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-08-I_Am_Too_Absolutely_Heteroskedastic_for_This_Probit_Model.html">1047 andrew gelman stats-2011-12-08-I Am Too Absolutely Heteroskedastic for This Probit Model</a></p>
<p>17 0.78416342 <a title="2176-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-06-Bayesian_model-building_by_pure_thought%3A__Some_principles_and_examples.html">1156 andrew gelman stats-2012-02-06-Bayesian model-building by pure thought:  Some principles and examples</a></p>
<p>18 0.7838465 <a title="2176-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>19 0.78183872 <a title="2176-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-03-Comparing_prediction_errors.html">938 andrew gelman stats-2011-10-03-Comparing prediction errors</a></p>
<p>20 0.77826303 <a title="2176-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.02), (8, 0.03), (15, 0.02), (16, 0.048), (18, 0.012), (21, 0.046), (24, 0.17), (36, 0.03), (37, 0.027), (43, 0.044), (63, 0.025), (86, 0.034), (88, 0.022), (89, 0.018), (95, 0.018), (99, 0.324)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98278946 <a title="2176-lda-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-Transformations_for_non-normal_data.html">2176 andrew gelman stats-2014-01-19-Transformations for non-normal data</a></p>
<p>Introduction: Steve Peterson writes:
  
I recently submitted a proposal on applying a Bayesian analysis to gender comparisons on motivational constructs. I had an idea on how to improve the model I used and was hoping you could give me some feedback.


The data come from a survey based on 5-point Likert scales. Different constructs are measured for each student as scores derived from averaging a student’s responses on particular subsets of survey questions. (I suppose it is not uncontroversial to treat these scores as interval measures and would be interested to hear if you have any objections.) I am comparing genders on each construct. Researchers typically use t-tests to do so.


To use a Bayesian approach I applied the programs written in R and JAGS by John Kruschke for estimating the difference of means:


http://www.indiana.edu/~kruschke/BEST/


An issue in that analysis is that the distributions of student scores are not normal. There was skewness in some of the distributions and not always in</p><p>2 0.98110807 <a title="2176-lda-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-12-Historical_Arc_of_Universities.html">2330 andrew gelman stats-2014-05-12-Historical Arc of Universities</a></p>
<p>Introduction: This post is by  David K. Park 
 
Even though I’m an engineer with a PhD in political science, I tend to gravitate toward history to anchor my contextual lens. (If fact, if I were pressed to put a methodological stake in the ground, I would say I’m a historical comparative institutional ecologist.) In that regard, it may be helpful to situate this discussion within the broader historical arc of intellectual pursuits at universities. As we know with the birth of universities, we had scholars who embodied so many disciplines such as mathematics, philosophy, religion law, etc into one individual. Then in the 50′s and 60′s we started going into hyper-specialization mode, and it was necessary because we needed to better understand our specific domains. In the 80s and 90s, certain disciplines started to recognize the importance of other disciplines on their work but the tendency was to bring those skills into a single individual. So we produced, by way of e.g., law professors who could do ga</p><p>3 0.97891474 <a title="2176-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-30-%E2%80%9CNon-statistical%E2%80%9D_statistics_tools.html">1920 andrew gelman stats-2013-06-30-“Non-statistical” statistics tools</a></p>
<p>Introduction: Ulrich Atz writes:
  
I regard myself fairly familiar with modern “big data” tools and models such as random forests, SVM etc. However,  HyperCube  is something I haven’t come across yet (met the marketing guy last week) and they  advertise  it as “disruptive”, “unique”, “best performing data analysis tool available”. 


Have you seen it in action? Perhaps performing in any data science style competition?


On a side note, they claim it is “non-statistical” which I find absurd. A marketing ploy, but sounds like physics without math.


Hence, my question: 


Do you think there is such a thing as a (1) non-statistical data analysis and (2) non-statistical data set?
  
Here’s what’s on the webpage:
  
The technology is non-statistical, meaning it does not take a sample and use algorithms in order to validate a hypothesis. Instead, it takes input from a large volume of data and outputs the results from the data alone. This means that all the available data is taken into account.
  
I’m not</p><p>4 0.97745049 <a title="2176-lda-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-03-The_statistical_properties_of_smart_chains_%28and_referral_chains_more_generally%29.html">1882 andrew gelman stats-2013-06-03-The statistical properties of smart chains (and referral chains more generally)</a></p>
<p>Introduction: Louis Mittel writes:
  
The premise of  the column  this guy is starting is interesting:  Noah Davis interviews a smart person and then interviews the smartest person that smart person knows and so on. 


It reminded me of you mentioning survey design strategy of asking people about other people, like “How many people do you know named Stuart?” or “How many people do you know that have had an abortion?”


Ignoring the interview aspect of what this guy is doing, I think there’s some cool questions about the distribution/path behavior of smartest-person-I-know chains (say, seeded at random).  Do they loop?  If so, how long do they run before looping, how large are the loops?  What parts of the population do the explore?  Do you know of anything that’s been done on something like this?
  
My reply:  Interesting question.  It could be asked of any referral chain, for example asking a sequence of people, “Who’s the tallest person you know?” or “Who’s the best piano player you know” or “Who’</p><p>5 0.97587872 <a title="2176-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-15-Still_more_Mr._P_in_public_health.html">770 andrew gelman stats-2011-06-15-Still more Mr. P in public health</a></p>
<p>Introduction: When it  rains  it pours . . .
 
John Transue writes:
  
I saw  a post  on Andrew Sullivan’s blog today about life expectancy in different US counties. With a bunch of the worst counties being in Mississippi, I thought that it might be another case of analysts getting extreme values from small counties.


However, the paper (see  here ) includes a pretty interesting methods section. This is from page 5, “Specifically, we used a mixed-effects Poisson regression with time, geospatial, and covariate components. Poisson regression fits count outcome variables, e.g., death counts, and is preferable to a logistic model because the latter is biased when an outcome is rare (occurring in less than 1% of observations).”


They have downloadable data. I believe that the data are predicted values from the model. A web appendix also gives 90% CIs for their estimates.


Do you think they solved the small county problem and that the worst counties really are where their spreadsheet suggests?
  
My re</p><p>6 0.97510809 <a title="2176-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-02-Moving_beyond_hopeless_graphics.html">1403 andrew gelman stats-2012-07-02-Moving beyond hopeless graphics</a></p>
<p>7 0.97441608 <a title="2176-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-08-Poli_sci_plagiarism_update%2C_and_a_note_about_the_benefits_of_not_caring.html">400 andrew gelman stats-2010-11-08-Poli sci plagiarism update, and a note about the benefits of not caring</a></p>
<p>8 0.97421056 <a title="2176-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-17-How_can_statisticians_help_psychologists_do_their_research_better%3F.html">1860 andrew gelman stats-2013-05-17-How can statisticians help psychologists do their research better?</a></p>
<p>9 0.97388709 <a title="2176-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-11-Adding_an_error_model_to_a_deterministic_model.html">1162 andrew gelman stats-2012-02-11-Adding an error model to a deterministic model</a></p>
<p>10 0.97373492 <a title="2176-lda-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-08-Here%E2%80%99s_how_rumors_get_started%3A__Lineplots%2C_dotplots%2C_and_nonfunctional_modernist_architecture.html">262 andrew gelman stats-2010-09-08-Here’s how rumors get started:  Lineplots, dotplots, and nonfunctional modernist architecture</a></p>
<p>11 0.97338438 <a title="2176-lda-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-23-Scientists_can_read_your_mind_._._._as_long_as_the%E2%80%99re_allowed_to_look_at_more_than_one_place_in_your_brain_and_then_make_a_prediction_after_seeing_what_you_actually_did.html">106 andrew gelman stats-2010-06-23-Scientists can read your mind . . . as long as the’re allowed to look at more than one place in your brain and then make a prediction after seeing what you actually did</a></p>
<p>12 0.97324818 <a title="2176-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-07-Jenny_Davidson_wins_Mark_Van_Doren_Award%2C_also_some_reflections_on_the_continuity_of_work_within_literary_criticism_or_statistics.html">22 andrew gelman stats-2010-05-07-Jenny Davidson wins Mark Van Doren Award, also some reflections on the continuity of work within literary criticism or statistics</a></p>
<p>13 0.97270215 <a title="2176-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-29-Infovis%2C_infographics%2C_and_data_visualization%3A__Where_I%E2%80%99m_coming_from%2C_and_where_I%E2%80%99d_like_to_go.html">878 andrew gelman stats-2011-08-29-Infovis, infographics, and data visualization:  Where I’m coming from, and where I’d like to go</a></p>
<p>14 0.97257388 <a title="2176-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>15 0.97237682 <a title="2176-lda-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>16 0.97210789 <a title="2176-lda-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-26-Age_and_happiness%3A__The_pattern_isn%E2%80%99t_as_clear_as_you_might_think.html">486 andrew gelman stats-2010-12-26-Age and happiness:  The pattern isn’t as clear as you might think</a></p>
<p>17 0.97172034 <a title="2176-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-04-Write_This_Book.html">1605 andrew gelman stats-2012-12-04-Write This Book</a></p>
<p>18 0.97170365 <a title="2176-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>19 0.97128719 <a title="2176-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-15-Bayesian_statistical_pragmatism.html">662 andrew gelman stats-2011-04-15-Bayesian statistical pragmatism</a></p>
<p>20 0.97100097 <a title="2176-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-11-One_more_time_on_that_ESP_study%3A__The_problem_of_overestimates_and_the_shrinkage_solution.html">511 andrew gelman stats-2011-01-11-One more time on that ESP study:  The problem of overestimates and the shrinkage solution</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
