<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2315 andrew gelman stats-2014-05-02-Discovering general multidimensional associations</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2014" href="../home/andrew_gelman_stats-2014_home.html">andrew_gelman_stats-2014</a> <a title="andrew_gelman_stats-2014-2315" href="#">andrew_gelman_stats-2014-2315</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2315 andrew gelman stats-2014-05-02-Discovering general multidimensional associations</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2014-2315-html" href="http://andrewgelman.com/2014/05/02/discovering-general-multidimensional-associations/">html</a></p><p>Introduction: Continuing our discussion of general measures of correlations, Ben Murrell sends along  this paper  (with  corresponding  R package), which begins:
  
When two variables are related by a known function, the coefficient of determination (denoted R-squared) measures the proportion of the total variance in the observations that is explained by that function. This quantifies the strength of the relationship between variables by describing what proportion of the variance is signal as opposed to noise. For linear relationships, this is equal to the square of the correlation coefficient, ρ. When the parametric form of the relationship is unknown, however, it is unclear how to estimate the proportion of explained variance equitably – assigning similar values to equally noisy relationships. Here we demonstrate how to directly estimate a generalized R-squared when the form of the relationship is unknown, and we question the performance of the Maximal Information Coefficient (MIC) – a recently pr</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This quantifies the strength of the relationship between variables by describing what proportion of the variance is signal as opposed to noise. [sent-2, score-1.235]
</p><p>2 For linear relationships, this is equal to the square of the correlation coefficient, ρ. [sent-3, score-0.166]
</p><p>3 When the parametric form of the relationship is unknown, however, it is unclear how to estimate the proportion of explained variance equitably – assigning similar values to equally noisy relationships. [sent-4, score-1.833]
</p><p>4 Here we demonstrate how to directly estimate a generalized R-squared when the form of the relationship is unknown, and we question the performance of the Maximal Information Coefficient (MIC) – a recently proposed information theoretic measure of dependence. [sent-5, score-0.838]
</p><p>5 We show that our approach behaves equitably, has more power than MIC to detect association between variables, and converges faster with increasing sample size. [sent-6, score-0.515]
</p><p>6 Most importantly, our approach generalizes to higher dimensions, which allows us to estimate the strength of multivariate relationships (Y against A,B,…) and to measure association while controlling for covariates (Y against X controlling for C). [sent-7, score-1.403]
</p><p>7 And, since we’re talking about R-squared, let me point you to my 2006 paper with Iain Pardoe,  Bayesian measures of explained variance and pooling in multilevel (hierarchical) models . [sent-8, score-0.73]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('equitably', 0.269), ('variance', 0.243), ('mic', 0.227), ('relationship', 0.219), ('coefficient', 0.217), ('proportion', 0.216), ('explained', 0.2), ('measures', 0.2), ('unknown', 0.181), ('strength', 0.176), ('relationships', 0.17), ('controlling', 0.158), ('variables', 0.147), ('iain', 0.134), ('association', 0.13), ('determination', 0.127), ('theoretic', 0.127), ('converges', 0.127), ('estimate', 0.126), ('pardoe', 0.121), ('generalizes', 0.121), ('murrell', 0.114), ('measure', 0.111), ('maximal', 0.104), ('unclear', 0.1), ('form', 0.1), ('parametric', 0.099), ('assigning', 0.099), ('importantly', 0.093), ('square', 0.093), ('detect', 0.089), ('covariates', 0.087), ('pooling', 0.087), ('approach', 0.086), ('equally', 0.084), ('ben', 0.084), ('faster', 0.083), ('generalized', 0.082), ('dimensions', 0.081), ('signal', 0.081), ('multivariate', 0.08), ('opposed', 0.079), ('noisy', 0.078), ('continuing', 0.077), ('corresponding', 0.076), ('begins', 0.074), ('describing', 0.074), ('observations', 0.074), ('proposed', 0.073), ('equal', 0.073)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="2315-tfidf-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-02-Discovering_general_multidimensional_associations.html">2315 andrew gelman stats-2014-05-02-Discovering general multidimensional associations</a></p>
<p>Introduction: Continuing our discussion of general measures of correlations, Ben Murrell sends along  this paper  (with  corresponding  R package), which begins:
  
When two variables are related by a known function, the coefficient of determination (denoted R-squared) measures the proportion of the total variance in the observations that is explained by that function. This quantifies the strength of the relationship between variables by describing what proportion of the variance is signal as opposed to noise. For linear relationships, this is equal to the square of the correlation coefficient, ρ. When the parametric form of the relationship is unknown, however, it is unclear how to estimate the proportion of explained variance equitably – assigning similar values to equally noisy relationships. Here we demonstrate how to directly estimate a generalized R-squared when the form of the relationship is unknown, and we question the performance of the Maximal Information Coefficient (MIC) – a recently pr</p><p>2 0.23081538 <a title="2315-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-26-Further_thoughts_on_nonparametric_correlation_measures.html">1230 andrew gelman stats-2012-03-26-Further thoughts on nonparametric correlation measures</a></p>
<p>Introduction: Malka Gorfine, Ruth Heller, and Yair Heller write a comment on the paper of Reshef et al. that we  discussed  a few months ago.
 
Just to remind you what’s going on here, here’s my quick summary from December:
  
Reshef et al. propose a new nonlinear R-squared-like measure.


Unlike R-squared, this new method depends on a tuning parameter that controls the level of discretization, in a “How long is the coast of Britain” sort of way. The dependence on scale is inevitable for such a general method. Just consider: if you sample 1000 points from the unit bivariate normal distribution, (x,y) ~ N(0,I), you’ll be able to fit them perfectly by a 999-degree polynomial fit to the data. So the scale of the fit matters.


The clever idea of the paper is that, instead of going for an absolute measure (which, as we’ve seen, will be scale-dependent), they focus on the problem of summarizing the grid of pairwise dependences in a large set of variables. As they put it: “Imagine a data set with hundreds</p><p>3 0.1636501 <a title="2315-tfidf-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-01-Heller%2C_Heller%2C_and_Gorfine_on_univariate_and_multivariate_information_measures.html">2314 andrew gelman stats-2014-05-01-Heller, Heller, and Gorfine on univariate and multivariate information measures</a></p>
<p>Introduction: Malka Gorfine writes:
  
We noticed that the important topic of association measures and tests  came up again  in your blog, and we have few comments in this regard.


It is useful to distinguish between the univariate and multivariate methods. A consistent multivariate method can recognise dependence between two vectors of random variables, while a univariate method can only loop over pairs of components and check for dependency between them.


There are very few consistent multivariate methods. To the best of our  knowledge there are three practical methods:


1) HSIC by Gretton et al. (http://www.gatsby.ucl.ac.uk/~gretton/papers/GreBouSmoSch05.pdf)


2) dcov by Szekely et al. (http://projecteuclid.org/euclid.aoas/1267453933)


3) A method we introduced in Heller et al (Biometrika, 2013, 503—510, http://biomet.oxfordjournals.org/content/early/2012/12/04/biomet.ass070.full.pdf+html, and an R package, HHG, is available as well http://cran.r-project.org/web/packages/HHG/index.html).


A</p><p>4 0.15862453 <a title="2315-tfidf-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-14-The_maximal_information_coefficient.html">2247 andrew gelman stats-2014-03-14-The maximal information coefficient</a></p>
<p>Introduction: Justin Kinney writes:
  
I wanted to let you know that the critique Mickey Atwal and I wrote regarding equitability and the maximal information coefficient has just been  published .
  
We discussed this paper last year, under the heading,  Too many MC’s not enough MIC’s, or What principles should govern attempts to summarize bivariate associations in large multivariate datasets? 
 
Kinney and Atwal’s paper is interesting, with my only criticism being that in some places they seem to aim for what might not be possible.  For example, they write that “mutual information is already widely believed to quantify dependencies without bias for relationships of one type or another,” which seems a bit vague to me.  And later they write, “How to compute such an estimate that does not bias the resulting mutual information value remains an open problem,” which seems to me to miss the point in that unbiased statistical estimates are not generally possible and indeed are often not desirable.
 
Their</p><p>5 0.13997418 <a title="2315-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-04-Too_many_MC%E2%80%99s_not_enough_MIC%E2%80%99s%2C_or_What_principles_should_govern_attempts_to_summarize_bivariate_associations_in_large_multivariate_datasets%3F.html">1706 andrew gelman stats-2013-02-04-Too many MC’s not enough MIC’s, or What principles should govern attempts to summarize bivariate associations in large multivariate datasets?</a></p>
<p>Introduction: Justin Kinney writes:
  
Since your blog has discussed the “maximal information coefficient” (MIC) of Reshef et al., I figured you might want to see  the critique  that Gurinder Atwal and I have posted.


In short,  Reshef et al.’s central claim that MIC is “equitable” is incorrect.  


We [Kinney and Atwal] offer mathematical proof that the definition of “equitability” Reshef et al. propose is unsatisfiable—no nontrivial dependence measure, including MIC, has this property. Replicating the simulations in their paper with modestly larger data sets validates this finding. 


The heuristic notion of equitability, however, can be formalized instead as a self-consistency condition closely related to the Data Processing Inequality. Mutual information satisfies this new definition of equitability but MIC does not.  We therefore propose that simply estimating mutual information will, in many cases, provide the sort of dependence measure Reshef et al. seek.
  
For background, here are my two p</p><p>6 0.13860169 <a title="2315-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Adding_more_information_can_make_the_variance_go_up_%28depending_on_your_model%29.html">810 andrew gelman stats-2011-07-20-Adding more information can make the variance go up (depending on your model)</a></p>
<p>7 0.13673051 <a title="2315-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-25-Correlation_of_1_._._._too_good_to_be_true%3F.html">1737 andrew gelman stats-2013-02-25-Correlation of 1 . . . too good to be true?</a></p>
<p>8 0.12625396 <a title="2315-tfidf-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-07-Once_more_on_nonparametric_measures_of_mutual_information.html">2324 andrew gelman stats-2014-05-07-Once more on nonparametric measures of mutual information</a></p>
<p>9 0.12349215 <a title="2315-tfidf-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-Correlation%2C_prediction%2C_variation%2C_etc..html">301 andrew gelman stats-2010-09-28-Correlation, prediction, variation, etc.</a></p>
<p>10 0.12301692 <a title="2315-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>11 0.12276942 <a title="2315-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-04-What_are_the_key_assumptions_of_linear_regression%3F.html">1967 andrew gelman stats-2013-08-04-What are the key assumptions of linear regression?</a></p>
<p>12 0.11935133 <a title="2315-tfidf-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Question_about_standard_range_for_social_science_correlations.html">257 andrew gelman stats-2010-09-04-Question about standard range for social science correlations</a></p>
<p>13 0.11797613 <a title="2315-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-23-Question_13_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1340 andrew gelman stats-2012-05-23-Question 13 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>14 0.11720035 <a title="2315-tfidf-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-02-The_problem_of_overestimation_of_group-level_variance_parameters.html">63 andrew gelman stats-2010-06-02-The problem of overestimation of group-level variance parameters</a></p>
<p>15 0.11134394 <a title="2315-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-26-How_to_understand_coefficients_that_reverse_sign_when_you_start_controlling_for_things%3F.html">1870 andrew gelman stats-2013-05-26-How to understand coefficients that reverse sign when you start controlling for things?</a></p>
<p>16 0.10716553 <a title="2315-tfidf-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-11-Understanding_how_estimates_change_when_you_move_to_a_multilevel_model.html">850 andrew gelman stats-2011-08-11-Understanding how estimates change when you move to a multilevel model</a></p>
<p>17 0.10539529 <a title="2315-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-15-The_bias-variance_tradeoff.html">960 andrew gelman stats-2011-10-15-The bias-variance tradeoff</a></p>
<p>18 0.1050252 <a title="2315-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-02-%E2%80%9CBased_on_my_experiences%2C_I_think_you_could_make_general_progress_by_constructing_a_solution_to_your_specific_problem.%E2%80%9D.html">1441 andrew gelman stats-2012-08-02-“Based on my experiences, I think you could make general progress by constructing a solution to your specific problem.”</a></p>
<p>19 0.10275246 <a title="2315-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>20 0.097468667 <a title="2315-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-06-17_groups%2C_6_group-level_predictors%3A__What_to_do%3F.html">1248 andrew gelman stats-2012-04-06-17 groups, 6 group-level predictors:  What to do?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.121), (1, 0.11), (2, 0.094), (3, -0.069), (4, 0.073), (5, 0.015), (6, -0.003), (7, -0.047), (8, -0.0), (9, 0.066), (10, 0.028), (11, -0.0), (12, 0.017), (13, 0.055), (14, 0.035), (15, 0.013), (16, -0.018), (17, 0.022), (18, -0.021), (19, -0.033), (20, 0.017), (21, 0.022), (22, 0.092), (23, 0.018), (24, 0.085), (25, 0.016), (26, 0.034), (27, 0.037), (28, 0.06), (29, -0.025), (30, 0.064), (31, 0.093), (32, 0.093), (33, -0.064), (34, 0.081), (35, 0.005), (36, 0.034), (37, -0.007), (38, 0.03), (39, -0.021), (40, 0.017), (41, -0.019), (42, 0.046), (43, 0.035), (44, -0.053), (45, -0.027), (46, 0.058), (47, -0.037), (48, -0.051), (49, -0.047)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98641157 <a title="2315-lsi-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-02-Discovering_general_multidimensional_associations.html">2315 andrew gelman stats-2014-05-02-Discovering general multidimensional associations</a></p>
<p>Introduction: Continuing our discussion of general measures of correlations, Ben Murrell sends along  this paper  (with  corresponding  R package), which begins:
  
When two variables are related by a known function, the coefficient of determination (denoted R-squared) measures the proportion of the total variance in the observations that is explained by that function. This quantifies the strength of the relationship between variables by describing what proportion of the variance is signal as opposed to noise. For linear relationships, this is equal to the square of the correlation coefficient, ρ. When the parametric form of the relationship is unknown, however, it is unclear how to estimate the proportion of explained variance equitably – assigning similar values to equally noisy relationships. Here we demonstrate how to directly estimate a generalized R-squared when the form of the relationship is unknown, and we question the performance of the Maximal Information Coefficient (MIC) – a recently pr</p><p>2 0.74359512 <a title="2315-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-26-Further_thoughts_on_nonparametric_correlation_measures.html">1230 andrew gelman stats-2012-03-26-Further thoughts on nonparametric correlation measures</a></p>
<p>Introduction: Malka Gorfine, Ruth Heller, and Yair Heller write a comment on the paper of Reshef et al. that we  discussed  a few months ago.
 
Just to remind you what’s going on here, here’s my quick summary from December:
  
Reshef et al. propose a new nonlinear R-squared-like measure.


Unlike R-squared, this new method depends on a tuning parameter that controls the level of discretization, in a “How long is the coast of Britain” sort of way. The dependence on scale is inevitable for such a general method. Just consider: if you sample 1000 points from the unit bivariate normal distribution, (x,y) ~ N(0,I), you’ll be able to fit them perfectly by a 999-degree polynomial fit to the data. So the scale of the fit matters.


The clever idea of the paper is that, instead of going for an absolute measure (which, as we’ve seen, will be scale-dependent), they focus on the problem of summarizing the grid of pairwise dependences in a large set of variables. As they put it: “Imagine a data set with hundreds</p><p>3 0.72630841 <a title="2315-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-Correlation%2C_prediction%2C_variation%2C_etc..html">301 andrew gelman stats-2010-09-28-Correlation, prediction, variation, etc.</a></p>
<p>Introduction: Hamdan Azhar writes:
  
 
I [Azhar] write with a question about language in the context of statistics. Consider the three statements below.


a) Y is significantly associated (correlated) with X;


b) knowledge of X allows us to account for __% of the variance in Y;


c) Y can be predicted to a significant extent given knowledge of X.


To what extent are these statements equivalent? Much of the (non-statistical) scientific literature doesn’t seem to distinguish between these notions. Is this just about semantics — or are there meaningful differences here, particularly between b and c?


Consider a framework where X constitutes a predictor space of p variables (x1,…,xp). We wish to generate a linear combination of these variables to yield a score that optimally correlates with Y. Can we substitute the word “predicts” for “optimally correlates with” in this context?


One can argue that “correlating” or “accounting for variance” suggests that we are trying to maximize goodness-of-fit (i</p><p>4 0.72084785 <a title="2315-lsi-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-14-The_maximal_information_coefficient.html">2247 andrew gelman stats-2014-03-14-The maximal information coefficient</a></p>
<p>Introduction: Justin Kinney writes:
  
I wanted to let you know that the critique Mickey Atwal and I wrote regarding equitability and the maximal information coefficient has just been  published .
  
We discussed this paper last year, under the heading,  Too many MC’s not enough MIC’s, or What principles should govern attempts to summarize bivariate associations in large multivariate datasets? 
 
Kinney and Atwal’s paper is interesting, with my only criticism being that in some places they seem to aim for what might not be possible.  For example, they write that “mutual information is already widely believed to quantify dependencies without bias for relationships of one type or another,” which seems a bit vague to me.  And later they write, “How to compute such an estimate that does not bias the resulting mutual information value remains an open problem,” which seems to me to miss the point in that unbiased statistical estimates are not generally possible and indeed are often not desirable.
 
Their</p><p>5 0.715074 <a title="2315-lsi-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-01-Heller%2C_Heller%2C_and_Gorfine_on_univariate_and_multivariate_information_measures.html">2314 andrew gelman stats-2014-05-01-Heller, Heller, and Gorfine on univariate and multivariate information measures</a></p>
<p>Introduction: Malka Gorfine writes:
  
We noticed that the important topic of association measures and tests  came up again  in your blog, and we have few comments in this regard.


It is useful to distinguish between the univariate and multivariate methods. A consistent multivariate method can recognise dependence between two vectors of random variables, while a univariate method can only loop over pairs of components and check for dependency between them.


There are very few consistent multivariate methods. To the best of our  knowledge there are three practical methods:


1) HSIC by Gretton et al. (http://www.gatsby.ucl.ac.uk/~gretton/papers/GreBouSmoSch05.pdf)


2) dcov by Szekely et al. (http://projecteuclid.org/euclid.aoas/1267453933)


3) A method we introduced in Heller et al (Biometrika, 2013, 503—510, http://biomet.oxfordjournals.org/content/early/2012/12/04/biomet.ass070.full.pdf+html, and an R package, HHG, is available as well http://cran.r-project.org/web/packages/HHG/index.html).


A</p><p>6 0.66725582 <a title="2315-lsi-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-07-Once_more_on_nonparametric_measures_of_mutual_information.html">2324 andrew gelman stats-2014-05-07-Once more on nonparametric measures of mutual information</a></p>
<p>7 0.63204575 <a title="2315-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-16-Mr._Pearson%2C_meet_Mr._Mandelbrot%3A__Detecting_Novel_Associations_in_Large_Data_Sets.html">1062 andrew gelman stats-2011-12-16-Mr. Pearson, meet Mr. Mandelbrot:  Detecting Novel Associations in Large Data Sets</a></p>
<p>8 0.63000089 <a title="2315-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-04-Too_many_MC%E2%80%99s_not_enough_MIC%E2%80%99s%2C_or_What_principles_should_govern_attempts_to_summarize_bivariate_associations_in_large_multivariate_datasets%3F.html">1706 andrew gelman stats-2013-02-04-Too many MC’s not enough MIC’s, or What principles should govern attempts to summarize bivariate associations in large multivariate datasets?</a></p>
<p>9 0.60027999 <a title="2315-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-09-The_effects_of_fiscal_consolidation.html">1663 andrew gelman stats-2013-01-09-The effects of fiscal consolidation</a></p>
<p>10 0.59305882 <a title="2315-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-24-New_ideas_on_DIC_from_Martyn_Plummer_and_Sumio_Watanabe.html">778 andrew gelman stats-2011-06-24-New ideas on DIC from Martyn Plummer and Sumio Watanabe</a></p>
<p>11 0.58424169 <a title="2315-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-06-Bayesian_Anova_found_useful_in_ecology.html">1102 andrew gelman stats-2012-01-06-Bayesian Anova found useful in ecology</a></p>
<p>12 0.58005822 <a title="2315-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-21-Avoiding_boundary_estimates_in_linear_mixed_models.html">918 andrew gelman stats-2011-09-21-Avoiding boundary estimates in linear mixed models</a></p>
<p>13 0.57571101 <a title="2315-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-25-It%E2%80%99s_binless%21__A_program_for_computing_normalizing_functions.html">1825 andrew gelman stats-2013-04-25-It’s binless!  A program for computing normalizing functions</a></p>
<p>14 0.57197791 <a title="2315-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-03-Uncertainty_in_parameter_estimates_using_multilevel_models.html">1966 andrew gelman stats-2013-08-03-Uncertainty in parameter estimates using multilevel models</a></p>
<p>15 0.57152879 <a title="2315-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Interpreting_interactions_in_discrete-data_regression.html">1908 andrew gelman stats-2013-06-21-Interpreting interactions in discrete-data regression</a></p>
<p>16 0.56751382 <a title="2315-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Adding_more_information_can_make_the_variance_go_up_%28depending_on_your_model%29.html">810 andrew gelman stats-2011-07-20-Adding more information can make the variance go up (depending on your model)</a></p>
<p>17 0.55521035 <a title="2315-lsi-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Question_about_standard_range_for_social_science_correlations.html">257 andrew gelman stats-2010-09-04-Question about standard range for social science correlations</a></p>
<p>18 0.55385506 <a title="2315-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-25-Correlation_of_1_._._._too_good_to_be_true%3F.html">1737 andrew gelman stats-2013-02-25-Correlation of 1 . . . too good to be true?</a></p>
<p>19 0.53950959 <a title="2315-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-02-%E2%80%9CBased_on_my_experiences%2C_I_think_you_could_make_general_progress_by_constructing_a_solution_to_your_specific_problem.%E2%80%9D.html">1441 andrew gelman stats-2012-08-02-“Based on my experiences, I think you could make general progress by constructing a solution to your specific problem.”</a></p>
<p>20 0.52593291 <a title="2315-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-21-Finite-population_Anova_calculations_for_models_with_interactions.html">1686 andrew gelman stats-2013-01-21-Finite-population Anova calculations for models with interactions</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(4, 0.014), (10, 0.028), (13, 0.015), (15, 0.04), (17, 0.073), (24, 0.194), (52, 0.014), (55, 0.023), (57, 0.056), (84, 0.011), (85, 0.014), (86, 0.036), (88, 0.012), (89, 0.011), (90, 0.021), (98, 0.046), (99, 0.292)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98730874 <a title="2315-lda-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-02-Discovering_general_multidimensional_associations.html">2315 andrew gelman stats-2014-05-02-Discovering general multidimensional associations</a></p>
<p>Introduction: Continuing our discussion of general measures of correlations, Ben Murrell sends along  this paper  (with  corresponding  R package), which begins:
  
When two variables are related by a known function, the coefficient of determination (denoted R-squared) measures the proportion of the total variance in the observations that is explained by that function. This quantifies the strength of the relationship between variables by describing what proportion of the variance is signal as opposed to noise. For linear relationships, this is equal to the square of the correlation coefficient, ρ. When the parametric form of the relationship is unknown, however, it is unclear how to estimate the proportion of explained variance equitably – assigning similar values to equally noisy relationships. Here we demonstrate how to directly estimate a generalized R-squared when the form of the relationship is unknown, and we question the performance of the Maximal Information Coefficient (MIC) – a recently pr</p><p>2 0.95442665 <a title="2315-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-26-Further_thoughts_on_nonparametric_correlation_measures.html">1230 andrew gelman stats-2012-03-26-Further thoughts on nonparametric correlation measures</a></p>
<p>Introduction: Malka Gorfine, Ruth Heller, and Yair Heller write a comment on the paper of Reshef et al. that we  discussed  a few months ago.
 
Just to remind you what’s going on here, here’s my quick summary from December:
  
Reshef et al. propose a new nonlinear R-squared-like measure.


Unlike R-squared, this new method depends on a tuning parameter that controls the level of discretization, in a “How long is the coast of Britain” sort of way. The dependence on scale is inevitable for such a general method. Just consider: if you sample 1000 points from the unit bivariate normal distribution, (x,y) ~ N(0,I), you’ll be able to fit them perfectly by a 999-degree polynomial fit to the data. So the scale of the fit matters.


The clever idea of the paper is that, instead of going for an absolute measure (which, as we’ve seen, will be scale-dependent), they focus on the problem of summarizing the grid of pairwise dependences in a large set of variables. As they put it: “Imagine a data set with hundreds</p><p>3 0.94442707 <a title="2315-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-10-Cross-validation_and_Bayesian_estimation_of_tuning_parameters.html">2129 andrew gelman stats-2013-12-10-Cross-validation and Bayesian estimation of tuning parameters</a></p>
<p>Introduction: Ilya Lipkovich writes:
  
I read with great interest your 2008  paper  [with Aleks Jakulin, Grazia Pittau, and Yu-Sung Su] on weakly informative priors for logistic regression and also followed an interesting discussion on your blog. This discussion was within Bayesian community in relation to the validity of priors. However i would like to approach it rather from a more broad perspective on predictive modeling bringing in the ideas from machine/statistical learning approach”.  Actually you were the first to bring it up by mentioning in your paper “borrowing ideas from computer science” on cross-validation when comparing predictive ability of your proposed priors with other choices.


However, using cross-validation for comparing method performance is not the only or primary use of CV in machine-learning. Most of machine learning methods have some “meta” or complexity parameters and use cross-validation to tune them up. For example, one of your comparison methods is BBR which actually</p><p>4 0.94418836 <a title="2315-lda-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-06-An_old_discussion_of_food_deserts.html">2283 andrew gelman stats-2014-04-06-An old discussion of food deserts</a></p>
<p>Introduction: I happened to be reading  an old comment thread  from 2012 (follow the link from  here ) and came across this amusing exchange:
 
 
 
Perhaps  this  is the paper Jonathan was talking about?
 
Here’s more from the thread:
 
 
 
 
 
Anyway, I don’t have anything to add right now, I just thought it was an interesting discussion.</p><p>5 0.94371879 <a title="2315-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-23-Scalable_Stan.html">2035 andrew gelman stats-2013-09-23-Scalable Stan</a></p>
<p>Introduction: Bob writes:
  
If you have papers that have used Stan, we’d love to hear about it.  We finally got some submissions, so we’re going to start a list on the web site for 2.0 in earnest.  You can either mail them to the list, to me directly, or just update the issue (at least until it’s closed or moved):


https://github.com/stan-dev/stan/issues/187
  
For example, Henrik Mannerstrom fit a hierarchical model the other day with 360,000 data points and 120,000 variables.  And it worked just fine in Stan.  I’ve asked him to write this up so we can post it here.
 
Here’s the famous graph Bob made showing the scalability of Stan for a series of hierarchical item-response models:</p><p>6 0.94368529 <a title="2315-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-19-Scalability_in_education.html">1502 andrew gelman stats-2012-09-19-Scalability in education</a></p>
<p>7 0.94356841 <a title="2315-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>8 0.94349647 <a title="2315-lda-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-06-Inbox_zero.__Really..html">259 andrew gelman stats-2010-09-06-Inbox zero.  Really.</a></p>
<p>9 0.94300222 <a title="2315-lda-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-01-Heller%2C_Heller%2C_and_Gorfine_on_univariate_and_multivariate_information_measures.html">2314 andrew gelman stats-2014-05-01-Heller, Heller, and Gorfine on univariate and multivariate information measures</a></p>
<p>10 0.94299519 <a title="2315-lda-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-09-Sof%5Bt%5D.html">77 andrew gelman stats-2010-06-09-Sof[t]</a></p>
<p>11 0.93997335 <a title="2315-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>12 0.93979156 <a title="2315-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-31-Type_S_error_rates_for_classical_and_Bayesian_single_and_multiple_comparison_procedures.html">494 andrew gelman stats-2010-12-31-Type S error rates for classical and Bayesian single and multiple comparison procedures</a></p>
<p>13 0.93968022 <a title="2315-lda-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-04-All_the_Assumptions_That_Are_My_Life.html">2359 andrew gelman stats-2014-06-04-All the Assumptions That Are My Life</a></p>
<p>14 0.93939024 <a title="2315-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-16-A_previous_discussion_with_Charles_Murray_about_liberals%2C_conservatives%2C_and_social_class.html">1170 andrew gelman stats-2012-02-16-A previous discussion with Charles Murray about liberals, conservatives, and social class</a></p>
<p>15 0.93916523 <a title="2315-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-24-Bell_Labs.html">970 andrew gelman stats-2011-10-24-Bell Labs</a></p>
<p>16 0.93893248 <a title="2315-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-18-Question_on_Type_M_errors.html">963 andrew gelman stats-2011-10-18-Question on Type M errors</a></p>
<p>17 0.93862903 <a title="2315-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>18 0.93848908 <a title="2315-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-22-Krugman_sets_the_bar_too_high.html">1733 andrew gelman stats-2013-02-22-Krugman sets the bar too high</a></p>
<p>19 0.93831444 <a title="2315-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-02-The_inevitable_problems_with_statistical_significance_and_95%25_intervals.html">1150 andrew gelman stats-2012-02-02-The inevitable problems with statistical significance and 95% intervals</a></p>
<p>20 0.93821388 <a title="2315-lda-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-20-Thermodynamic_Monte_Carlo%3A__Michael_Betancourt%E2%80%99s_new_method_for_simulating_from_difficult_distributions_and_evaluating_normalizing_constants.html">2340 andrew gelman stats-2014-05-20-Thermodynamic Monte Carlo:  Michael Betancourt’s new method for simulating from difficult distributions and evaluating normalizing constants</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
