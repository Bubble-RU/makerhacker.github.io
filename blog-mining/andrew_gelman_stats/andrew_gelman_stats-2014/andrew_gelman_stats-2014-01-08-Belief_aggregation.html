<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2162 andrew gelman stats-2014-01-08-Belief aggregation</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2014" href="../home/andrew_gelman_stats-2014_home.html">andrew_gelman_stats-2014</a> <a title="andrew_gelman_stats-2014-2162" href="#">andrew_gelman_stats-2014-2162</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2162 andrew gelman stats-2014-01-08-Belief aggregation</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2014-2162-html" href="http://andrewgelman.com/2014/01/08/belief-aggregation/">html</a></p><p>Introduction: Johannes Castner writes:
  
Suppose there are k scientists, each with her own model (Bayesian Net) over m random variables.  Then, because the space of Bayesian Nets over these m variables, with the square-root of the Jensen-Shannon Divergence as a distance metric is a closed and bounded space, there exists one unique Bayes Net that is a mixture of the k model joint-distributions which is at equal distance to each of the k models and may be called a “consensus graph.”  This consensus graph is in turn a Bayes Net, which can be updated with evidence.  The first question is: What are the conditions for which, given a new bit of evidence, the updated consensus graph is exactly the same graph as the consensus graph of the updated k Bayes Nets? In other words, if we arrive at a synthetic model from k models and then update this synthetic model, under what conditions is this the same thing as if we had first updated all k models and then build a synthesis.  The second question is: If these ar</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Johannes Castner writes:    Suppose there are k scientists, each with her own model (Bayesian Net) over m random variables. [sent-1, score-0.178]
</p><p>2 ”  This consensus graph is in turn a Bayes Net, which can be updated with evidence. [sent-3, score-1.058]
</p><p>3 The first question is: What are the conditions for which, given a new bit of evidence, the updated consensus graph is exactly the same graph as the consensus graph of the updated k Bayes Nets? [sent-4, score-2.601]
</p><p>4 In other words, if we arrive at a synthetic model from k models and then update this synthetic model, under what conditions is this the same thing as if we had first updated all k models and then build a synthesis. [sent-5, score-1.898]
</p><p>5 The second question is: If these are not the same, then which of the two would be better and under what conditions, from the perspective of collective learning? [sent-6, score-0.288]
</p><p>6 It all seems related to various topics of interest to me (see, for example,  this presentation  from 2003) but I don’t know anything about what he is talking about. [sent-8, score-0.405]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('updated', 0.399), ('consensus', 0.379), ('net', 0.302), ('synthetic', 0.287), ('nets', 0.24), ('conditions', 0.225), ('graph', 0.21), ('bayes', 0.201), ('distance', 0.181), ('johannes', 0.152), ('space', 0.143), ('divergence', 0.137), ('arrive', 0.125), ('model', 0.12), ('metric', 0.12), ('models', 0.114), ('collective', 0.11), ('bounded', 0.11), ('closed', 0.105), ('exists', 0.102), ('mixture', 0.088), ('build', 0.085), ('equal', 0.083), ('update', 0.083), ('unique', 0.081), ('presentation', 0.08), ('bayesian', 0.079), ('question', 0.072), ('topics', 0.07), ('turn', 0.07), ('learning', 0.065), ('first', 0.059), ('exactly', 0.059), ('thoughts', 0.059), ('anyone', 0.059), ('words', 0.058), ('perspective', 0.058), ('random', 0.058), ('scientists', 0.057), ('related', 0.057), ('suppose', 0.056), ('talking', 0.056), ('variables', 0.056), ('called', 0.054), ('interest', 0.052), ('evidence', 0.049), ('second', 0.048), ('various', 0.047), ('anything', 0.043), ('may', 0.04)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="2162-tfidf-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-08-Belief_aggregation.html">2162 andrew gelman stats-2014-01-08-Belief aggregation</a></p>
<p>Introduction: Johannes Castner writes:
  
Suppose there are k scientists, each with her own model (Bayesian Net) over m random variables.  Then, because the space of Bayesian Nets over these m variables, with the square-root of the Jensen-Shannon Divergence as a distance metric is a closed and bounded space, there exists one unique Bayes Net that is a mixture of the k model joint-distributions which is at equal distance to each of the k models and may be called a “consensus graph.”  This consensus graph is in turn a Bayes Net, which can be updated with evidence.  The first question is: What are the conditions for which, given a new bit of evidence, the updated consensus graph is exactly the same graph as the consensus graph of the updated k Bayes Nets? In other words, if we arrive at a synthetic model from k models and then update this synthetic model, under what conditions is this the same thing as if we had first updated all k models and then build a synthesis.  The second question is: If these ar</p><p>2 0.13829552 <a title="2162-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-05-Deadwood_in_the_math_curriculum.html">992 andrew gelman stats-2011-11-05-Deadwood in the math curriculum</a></p>
<p>Introduction: Mark Palko  asks :  What are the worst examples of curriculum dead wood?
 
Here’s the background:
  
One of the first things that hit me [Palko] when I started teaching high school math was how much material there was to cover. . . . The most annoying part, though, was the number of topics that could easily have been cut, thus giving the students the time to master the important skills and concepts.


The example that really stuck with me was synthetic division, a more concise but less intuitive way of performing polynomial long division. Both of these topics are pretty much useless in daily life but polynomial long division does, at least, give the student some insight into the relationship between polynomials and familiar base-ten numbers. Synthetic division has no such value; it’s just a faster but less interesting way of doing something you’ll never have to do.


I started asking hardcore math people — mathematicians, statisticians, physicists, rocket scientists — if they.’d ever u</p><p>3 0.12459204 <a title="2162-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>Introduction: Larry Wasserman  refers  to finite mixture models as “beasts” and  writes  jokes that they “should be avoided at all costs.”
 
I’ve thought a lot about mixture models, ever since using them in an  analysis  of voting patterns that was published in 1990.  First off, I’d like to say that our model was useful so I’d prefer not to pay the cost of avoiding it.  For a quick description of our mixture model and its context, see pp. 379-380 of my  article  in the Jim Berger volume).  Actually, our case was particularly difficult because we were not even fitting a mixture model to data, we were fitting it to latent data and using the model to perform partial pooling.  My difficulties in trying to fit this model inspired our discussion of mixture models in Bayesian Data Analysis (page 109 in the second edition, in the section on “Counterexamples to the theorems”).
 
I agree with Larry that if you’re fitting a mixture model, it’s good to be aware of the problems that arise if you try to estimate</p><p>4 0.10907472 <a title="2162-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>Introduction: In response to  this article  by Cosma Shalizi and myself on the philosophy of Bayesian statistics, David Hogg writes:
  
I [Hogg] agree–even in physics and astronomy–that the models are not “True” in the God-like sense of being absolute reality (that is, I am not a realist); and I  have argued  (a philosophically very naive 
paper, but hey, I was new to all this) that for pretty fundamental reasons we could never arrive at the True (with a capital “T”) model of the Universe.  The goal of inference is to find the “best” model, where “best” might have something to do with prediction, or explanation, or message length, or (horror!) our utility.  Needless to say, most of my physics friends *are* realists, even in the face of “effective theories” as Newtonian mechanics is an effective theory of GR and GR is an effective theory of “quantum gravity” (this plays to your point, because if you think any theory is possibly an effective theory, how could you ever find Truth?).  I also liked the i</p><p>5 0.098724663 <a title="2162-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-29-Infovis%2C_infographics%2C_and_data_visualization%3A__Where_I%E2%80%99m_coming_from%2C_and_where_I%E2%80%99d_like_to_go.html">878 andrew gelman stats-2011-08-29-Infovis, infographics, and data visualization:  Where I’m coming from, and where I’d like to go</a></p>
<p>Introduction: I continue to struggle to convey my thoughts on statistical graphics so I’ll try another approach, this time giving my own story.
 
For newcomers to this discussion:  the background is that Antony Unwin and I wrote  an article  on the different goals embodied in information visualization and statistical graphics, but I have difficulty communicating on this point with the infovis people.
 
Maybe if I tell my own story, and then they tell their stories, this will point a way forward to a more constructive discussion.
 
So here goes. 
   
I majored in physics in college and I worked in a couple of research labs during the summer. Physicists graph everything.  I did most of my plotting on graph paper–this continued through my second year of grad school–and became expert at putting points at 1/5, 2/5, 3/5, and 4/5 between the x and y grid lines.
 
In grad school in statistics, I continued my physics habits and graphed everything I could.  I did notice, though, that the faculty and the other</p><p>6 0.098638594 <a title="2162-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>7 0.095893264 <a title="2162-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-03-An_argument_that_can%E2%80%99t_possibly_make_sense.html">743 andrew gelman stats-2011-06-03-An argument that can’t possibly make sense</a></p>
<p>8 0.095520601 <a title="2162-tfidf-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-04-26-Bayes_in_the_news%E2%80%A6in_a_somewhat_frustrating_way.html">3 andrew gelman stats-2010-04-26-Bayes in the news…in a somewhat frustrating way</a></p>
<p>9 0.095434263 <a title="2162-tfidf-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Useful_models%2C_model_checking%2C_and_external_validation%3A__a_mini-discussion.html">244 andrew gelman stats-2010-08-30-Useful models, model checking, and external validation:  a mini-discussion</a></p>
<p>10 0.094835058 <a title="2162-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>11 0.094816446 <a title="2162-tfidf-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-25-A_statistical_graphics_course_and_statistical_graphics_advice.html">2266 andrew gelman stats-2014-03-25-A statistical graphics course and statistical graphics advice</a></p>
<p>12 0.094480664 <a title="2162-tfidf-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-15-Bill_Easterly_vs._Jeff_Sachs%3A_What_percentage_of_the_recipients_didn%E2%80%99t_use_the_free_malaria_bed_nets_in_Zambia%3F.html">2335 andrew gelman stats-2014-05-15-Bill Easterly vs. Jeff Sachs: What percentage of the recipients didn’t use the free malaria bed nets in Zambia?</a></p>
<p>13 0.089790016 <a title="2162-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-05-Monitor_the_efficiency_of_your_Markov_chain_sampler_using_expected_squared_jumped_distance%21.html">650 andrew gelman stats-2011-04-05-Monitor the efficiency of your Markov chain sampler using expected squared jumped distance!</a></p>
<p>14 0.088023908 <a title="2162-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-28-Understanding_simulations_in_terms_of_predictive_inference%3F.html">1287 andrew gelman stats-2012-04-28-Understanding simulations in terms of predictive inference?</a></p>
<p>15 0.08788538 <a title="2162-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-04-Model_checking_and_model_understanding_in_machine_learning.html">1482 andrew gelman stats-2012-09-04-Model checking and model understanding in machine learning</a></p>
<p>16 0.086340405 <a title="2162-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-30-Bill_Gates%E2%80%99s_favorite_graph_of_the_year.html">2154 andrew gelman stats-2013-12-30-Bill Gates’s favorite graph of the year</a></p>
<p>17 0.085726067 <a title="2162-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-12-%E2%80%9CNot_only_defended_but_also_applied%E2%80%9D%3A_The_perceived_absurdity_of_Bayesian_inference.html">1262 andrew gelman stats-2012-04-12-“Not only defended but also applied”: The perceived absurdity of Bayesian inference</a></p>
<p>18 0.081502013 <a title="2162-tfidf-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-18-Those_wacky_anti-Bayesians_used_to_be_intimidating%2C_but_now_they%E2%80%99re_just_pathetic.html">2254 andrew gelman stats-2014-03-18-Those wacky anti-Bayesians used to be intimidating, but now they’re just pathetic</a></p>
<p>19 0.081084818 <a title="2162-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-01-A_graph_at_war_with_its_caption.__Also%2C_how_to_visualize_the_same_numbers_without_giving_the_display_a_misleading_causal_feel%3F.html">1834 andrew gelman stats-2013-05-01-A graph at war with its caption.  Also, how to visualize the same numbers without giving the display a misleading causal feel?</a></p>
<p>20 0.080669485 <a title="2162-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.128), (1, 0.085), (2, -0.014), (3, 0.067), (4, 0.036), (5, -0.034), (6, -0.045), (7, 0.018), (8, 0.048), (9, -0.004), (10, 0.012), (11, 0.024), (12, -0.034), (13, 0.027), (14, -0.01), (15, -0.002), (16, 0.063), (17, 0.013), (18, -0.02), (19, 0.012), (20, 0.009), (21, 0.009), (22, -0.035), (23, -0.045), (24, 0.017), (25, -0.019), (26, -0.004), (27, 0.012), (28, -0.009), (29, -0.018), (30, -0.001), (31, -0.025), (32, -0.061), (33, -0.056), (34, -0.033), (35, -0.006), (36, -0.033), (37, -0.06), (38, 0.024), (39, 0.019), (40, -0.035), (41, -0.011), (42, 0.034), (43, 0.087), (44, -0.034), (45, 0.005), (46, 0.038), (47, 0.039), (48, -0.02), (49, -0.022)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9669584 <a title="2162-lsi-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-08-Belief_aggregation.html">2162 andrew gelman stats-2014-01-08-Belief aggregation</a></p>
<p>Introduction: Johannes Castner writes:
  
Suppose there are k scientists, each with her own model (Bayesian Net) over m random variables.  Then, because the space of Bayesian Nets over these m variables, with the square-root of the Jensen-Shannon Divergence as a distance metric is a closed and bounded space, there exists one unique Bayes Net that is a mixture of the k model joint-distributions which is at equal distance to each of the k models and may be called a “consensus graph.”  This consensus graph is in turn a Bayes Net, which can be updated with evidence.  The first question is: What are the conditions for which, given a new bit of evidence, the updated consensus graph is exactly the same graph as the consensus graph of the updated k Bayes Nets? In other words, if we arrive at a synthetic model from k models and then update this synthetic model, under what conditions is this the same thing as if we had first updated all k models and then build a synthesis.  The second question is: If these ar</p><p>2 0.72729319 <a title="2162-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-27-Visual_diagnostics_for_discrete-data_regressions.html">929 andrew gelman stats-2011-09-27-Visual diagnostics for discrete-data regressions</a></p>
<p>Introduction: Jeff asked me what I thought of  this  recent AJPS article by Brian Greenhill, Michael Ward, and Audrey Sacks, “The Separation Plot: A New Visual Method for Evaluating the Fit of Binary Models.”  It’s similar to a graph of observed vs. predicted values, but using color rather than the y-axis to display the observed values.  It seems like it could be useful, also could be applied more generally to discrete-data regressions with more than two categories.
 
When it comes to checking the model fit, I recommend binned residual plots, as discussed in  this 2000 article  with Yuri Goegebeur, Francis Tuerlinckx, and Iven Van Mechelen.</p><p>3 0.71317565 <a title="2162-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-07-Bayesian_hierarchical_model_for_the_prediction_of_soccer_results.html">20 andrew gelman stats-2010-05-07-Bayesian hierarchical model for the prediction of soccer results</a></p>
<p>Introduction: Gianluca Baio sends along  this article  (coauthored with Marta Blangiardo):
  
 
The problem of modelling football [soccer] data has become increasingly popular in the last few years and many different models have been proposed with the aim of estimating the characteristics that bring a team to lose or win a game, or to predict the score of a particular match. We propose a Bayesian hierarchical model to address both these aims and test its predictive strength on data about the Italian Serie A championship 1991-1992. To overcome the issue of overshrinkage produced by the Bayesian hierarchical model, we specify a more complex mixture model that results in better fit to the observed data. We test its performance using an example about the Italian Serie A championship 2007-2008.
 

 
I like the use of the hierarchical model and the focus on prediction.  I’m wondering, though, shouldn’t the model include a correlation between the “attack” and “defense” parameters?  Or maybe that’s in the m</p><p>4 0.68569893 <a title="2162-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-15-Wacky_priors_can_work_well%3F.html">1723 andrew gelman stats-2013-02-15-Wacky priors can work well?</a></p>
<p>Introduction: Dave Judkins writes:
  
I would love to see a blog entry on  this article , Bayesian Model Selection in High-Dimensional Settings, by Valen Johnson and David Rossell.  The simulation results are very encouraging although the choice of colors for some of the graphics is unfortunate.  Unless I am colorblind in some way that I am unaware of, they have two thin charcoal lines that are indistinguishable.
  
When Dave Judkins puts in a request, I’ll respond.  Also, I’m always happy to see a new Val Johnson paper.  Val and I are contemporaries—he and I got our PhD’s at around the same time, with both of us working on Bayesian image reconstruction, then in the early 1990s Val was part of the legendary group at Duke’s Institute of Statistics and Decision Sciences—a veritable ’27 Yankees featuring Mike West, Merlise Clyde, Michael Lavine, Dave Higdon, Peter Mueller, Val, and a bunch of others.  I always thought it was too bad they all had to go their separate ways.
 
Val also wrote two classic p</p><p>5 0.67698967 <a title="2162-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-14-Cool-ass_signal_processing_using_Gaussian_processes_%28birthdays_again%29.html">1379 andrew gelman stats-2012-06-14-Cool-ass signal processing using Gaussian processes (birthdays again)</a></p>
<p>Introduction: Aki writes:
  
Here’s my version of the  birthday frequency graph . I used Gaussian process with two slowly varying components and periodic component with decay, so that periodic form can change in time. I used Student’s t-distribution as observation model to allow exceptional dates to be outliers. I guess that periodic component due to week effect is still in the data because there is data only from twenty years. Naturally it would be better to model the whole timeseries, but it was easier to just use the cvs by Mulligan.
  
   
 
ALl I can say is . . . wow.  Bayes wins again.  Maybe Aki can supply the R or Matlab code?
 
P.S.  And let’s not forget how great the simple and clear time series plots are, compared to various fancy visualizations that people might try.
 
P.P.S.  More  here .</p><p>6 0.67436975 <a title="2162-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-07-A_compelling_reason_to_go_to_London%2C_Ontario%3F%3F.html">1104 andrew gelman stats-2012-01-07-A compelling reason to go to London, Ontario??</a></p>
<p>7 0.6678583 <a title="2162-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>8 0.65818036 <a title="2162-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-08-Technology_speedup_graph.html">1253 andrew gelman stats-2012-04-08-Technology speedup graph</a></p>
<p>9 0.65783763 <a title="2162-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-04-David_MacKay_and_Occam%E2%80%99s_Razor.html">1041 andrew gelman stats-2011-12-04-David MacKay and Occam’s Razor</a></p>
<p>10 0.64479268 <a title="2162-lsi-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Useful_models%2C_model_checking%2C_and_external_validation%3A__a_mini-discussion.html">244 andrew gelman stats-2010-08-30-Useful models, model checking, and external validation:  a mini-discussion</a></p>
<p>11 0.64399636 <a title="2162-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>12 0.64033258 <a title="2162-lsi-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-20-%E2%80%9CPeople_with_an_itch_to_scratch%E2%80%9D.html">101 andrew gelman stats-2010-06-20-“People with an itch to scratch”</a></p>
<p>13 0.63909781 <a title="2162-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Kind_of_Bayesian.html">811 andrew gelman stats-2011-07-20-Kind of Bayesian</a></p>
<p>14 0.63891667 <a title="2162-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>15 0.63826919 <a title="2162-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<p>16 0.63563186 <a title="2162-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-26-Modeling_probability_data.html">1284 andrew gelman stats-2012-04-26-Modeling probability data</a></p>
<p>17 0.63406461 <a title="2162-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-11-Adding_an_error_model_to_a_deterministic_model.html">1162 andrew gelman stats-2012-02-11-Adding an error model to a deterministic model</a></p>
<p>18 0.63083118 <a title="2162-lsi-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-07-Valencia%3A___Summer_of_1991.html">72 andrew gelman stats-2010-06-07-Valencia:   Summer of 1991</a></p>
<p>19 0.63019449 <a title="2162-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-15-Induction_within_a_model%2C_deductive_inference_for_model_evaluation.html">614 andrew gelman stats-2011-03-15-Induction within a model, deductive inference for model evaluation</a></p>
<p>20 0.62965667 <a title="2162-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-14-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">1856 andrew gelman stats-2013-05-14-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(24, 0.271), (31, 0.055), (44, 0.013), (72, 0.051), (77, 0.028), (82, 0.148), (84, 0.112), (96, 0.019), (99, 0.165)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92695618 <a title="2162-lda-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-08-Belief_aggregation.html">2162 andrew gelman stats-2014-01-08-Belief aggregation</a></p>
<p>Introduction: Johannes Castner writes:
  
Suppose there are k scientists, each with her own model (Bayesian Net) over m random variables.  Then, because the space of Bayesian Nets over these m variables, with the square-root of the Jensen-Shannon Divergence as a distance metric is a closed and bounded space, there exists one unique Bayes Net that is a mixture of the k model joint-distributions which is at equal distance to each of the k models and may be called a “consensus graph.”  This consensus graph is in turn a Bayes Net, which can be updated with evidence.  The first question is: What are the conditions for which, given a new bit of evidence, the updated consensus graph is exactly the same graph as the consensus graph of the updated k Bayes Nets? In other words, if we arrive at a synthetic model from k models and then update this synthetic model, under what conditions is this the same thing as if we had first updated all k models and then build a synthesis.  The second question is: If these ar</p><p>2 0.87724847 <a title="2162-lda-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-Running_into_a_Stan_Reference_by_Accident.html">2231 andrew gelman stats-2014-03-03-Running into a Stan Reference by Accident</a></p>
<p>Introduction: We were talking about parallelizing MCMC and I came up with what I thought was a neat idea for parallelizing MCMC (sample with fractional prior, average samples on a per-draw basis).  But then I realized this approach could get the right posterior mean or right posterior variance, but not both, depending on how the prior was divided (for a beta-binomial example).  Then  Aki  told me it had already been done in a more general form in a paper of Scott et al.,  Bayes and Big Data , which was then used as the baseline in: 
 
Willie Neiswanger, Chong Wang, and Eric Xing. 2013.   Asymptotically Exact, Embarrassingly Parallel MCMC .  arXiv  1311.4780. 
 
It’s a neat paper, which Xi’an  already blogged  about months ago.  But what really struck me was the following quote:
  

We use Stan, an automated Hamiltonian Monte Carlo (HMC) software package, to perform sampling for both the true posterior (for groundtruth and comparison methods) and for the subposteriors on each machine. One advantage o</p><p>3 0.85597491 <a title="2162-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-20-Sas_and_R.html">357 andrew gelman stats-2010-10-20-Sas and R</a></p>
<p>Introduction: Xian sends along  this link  that might be of interest to some of you.</p><p>4 0.84788364 <a title="2162-lda-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-23-Capitalism_as_a_form_of_voluntarism.html">482 andrew gelman stats-2010-12-23-Capitalism as a form of voluntarism</a></p>
<p>Introduction: Interesting discussion  by Alex Tabarrok (following up on an article by Rebecca Solnit) on the continuum between voluntarism (or, more generally, non-cash transactions) and markets with monetary exchange.  I just have a few comments of my own:
 
1.  Solnit writes of “the iceberg economy,” which she characterizes as “based on gift economies, barter, mutual aid, and giving without hope of return . . . the relations between friends, between family members, the activities of volunteers or those who have chosen their vocation on principle rather than for profit.”  I just wonder whether “barter” completely fits in here.  Maybe it depends on context.  Sometimes barter is an informal way of keeping track (you help me and I help you), but in settings of low liquidity I could imagine barter being simply an inefficient way of performing an economic transaction.
 
2.  I am no expert on capitalism but my impression is that it’s not just about “competition and selfishness” but also is related to the</p><p>5 0.84383839 <a title="2162-lda-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-17-Ma_conf%C3%A9rence_demain_%28mardi%29_%C3%A0_l%E2%80%99%C3%89cole_Polytechnique.html">2252 andrew gelman stats-2014-03-17-Ma conférence demain (mardi) à l’École Polytechnique</a></p>
<p>Introduction: À 11h15  au Centre de Mathématiques Appliquées:
 
 Peut-on utiliser les méthodes bayésiennes pour résoudre la crise des résultats de la recherche statistiquement significatifs que ne tiennent pas? 
 
It’s the usual story:  the audience will be technical but with a varying mix of interests, and so what they most wanted to hear was something general and nontechnical.  But this work does have connections to more involved research on interactions and weakly informative priors.</p><p>6 0.84368938 <a title="2162-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-12-Fixing_the_race%2C_ethnicity%2C_and_national_origin_questions_on_the_U.S._Census.html">1978 andrew gelman stats-2013-08-12-Fixing the race, ethnicity, and national origin questions on the U.S. Census</a></p>
<p>7 0.84065968 <a title="2162-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-18-Breastfeeding%2C_infant_hyperbilirubinemia%2C_statistical_graphics%2C_and_modern_medicine.html">38 andrew gelman stats-2010-05-18-Breastfeeding, infant hyperbilirubinemia, statistical graphics, and modern medicine</a></p>
<p>8 0.84025991 <a title="2162-lda-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-29-Ethics_and_statistics_in_development_research.html">241 andrew gelman stats-2010-08-29-Ethics and statistics in development research</a></p>
<p>9 0.83971387 <a title="2162-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-03-Comparing_prediction_errors.html">938 andrew gelman stats-2011-10-03-Comparing prediction errors</a></p>
<p>10 0.83909881 <a title="2162-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-09-%E2%80%9CHeterogeneity_of_variance_in_experimental_studies%3A__A_challenge_to_conventional_interpretations%E2%80%9D.html">1891 andrew gelman stats-2013-06-09-“Heterogeneity of variance in experimental studies:  A challenge to conventional interpretations”</a></p>
<p>11 0.83907825 <a title="2162-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-29-More_by_Berger_and_me_on_weakly_informative_priors.html">1092 andrew gelman stats-2011-12-29-More by Berger and me on weakly informative priors</a></p>
<p>12 0.83687854 <a title="2162-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-01-Mothers_and_Moms.html">1479 andrew gelman stats-2012-09-01-Mothers and Moms</a></p>
<p>13 0.83540225 <a title="2162-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-02-So-called_Bayesian_hypothesis_testing_is_just_as_bad_as_regular_hypothesis_testing.html">643 andrew gelman stats-2011-04-02-So-called Bayesian hypothesis testing is just as bad as regular hypothesis testing</a></p>
<p>14 0.83466864 <a title="2162-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-03-An_argument_that_can%E2%80%99t_possibly_make_sense.html">743 andrew gelman stats-2011-06-03-An argument that can’t possibly make sense</a></p>
<p>15 0.83465958 <a title="2162-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-04-Too_many_MC%E2%80%99s_not_enough_MIC%E2%80%99s%2C_or_What_principles_should_govern_attempts_to_summarize_bivariate_associations_in_large_multivariate_datasets%3F.html">1706 andrew gelman stats-2013-02-04-Too many MC’s not enough MIC’s, or What principles should govern attempts to summarize bivariate associations in large multivariate datasets?</a></p>
<p>16 0.83362854 <a title="2162-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-12-Simple_graph_WIN%3A__the_example_of_birthday_frequencies.html">1376 andrew gelman stats-2012-06-12-Simple graph WIN:  the example of birthday frequencies</a></p>
<p>17 0.82880926 <a title="2162-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-30-New_innovations_in_spam.html">545 andrew gelman stats-2011-01-30-New innovations in spam</a></p>
<p>18 0.82772833 <a title="2162-lda-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-15-Advice_that_might_make_sense_for_individuals_but_is_negative-sum_overall.html">278 andrew gelman stats-2010-09-15-Advice that might make sense for individuals but is negative-sum overall</a></p>
<p>19 0.82726431 <a title="2162-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-04-Wanna_be_the_next_Tyler_Cowen%3F__It%E2%80%99s_not_as_easy_as_you_might_think%21.html">1787 andrew gelman stats-2013-04-04-Wanna be the next Tyler Cowen?  It’s not as easy as you might think!</a></p>
<p>20 0.82714272 <a title="2162-lda-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-29-ARM_solutions.html">240 andrew gelman stats-2010-08-29-ARM solutions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
