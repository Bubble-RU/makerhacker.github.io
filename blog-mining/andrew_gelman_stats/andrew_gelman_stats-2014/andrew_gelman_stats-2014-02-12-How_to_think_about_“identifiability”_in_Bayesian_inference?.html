<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2014" href="../home/andrew_gelman_stats-2014_home.html">andrew_gelman_stats-2014</a> <a title="andrew_gelman_stats-2014-2208" href="#">andrew_gelman_stats-2014-2208</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2014-2208-html" href="http://andrewgelman.com/2014/02/12/think-identifiability-bayesian-inference/">html</a></p><p>Introduction: We had some questions on the Stan list regarding identification.  The topic arose because people were fitting models with improper posterior distributions, the kind of model where there’s a ridge in the likelihood and the parameters are not otherwise constrained.
 
I tried to help by writing something on Bayesian identifiability for the Stan list.  Then Ben Goodrich came along and cleaned up what I wrote.  I think this might be of interest to many of you so I’ll repeat the discussion here.
 
Here’s what I wrote:
  
Identification is actually a tricky concept and is not so clearly defined.  In the broadest sense, a Bayesian model is identified if the posterior distribution is proper.  Then one can do Bayesian inference and that’s that.  No need to require a finite variance or even a finite mean, all that’s needed is a finite integral of the probability distribution.


That said, there are some reasons why a stronger definition can be useful:


1.  Weak identification.  Suppose that, wit</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The topic arose because people were fitting models with improper posterior distributions, the kind of model where there’s a ridge in the likelihood and the parameters are not otherwise constrained. [sent-2, score-0.684]
</p><p>2 In the broadest sense, a Bayesian model is identified if the posterior distribution is proper. [sent-7, score-0.592]
</p><p>3 If we wanted to make this concept of “weak identification” more formal, we could stipulate that the model is expressed in terms of some hyperparameter A which is set to a large value, and that weak identifiability corresponds to nonidentifiability when A -> infinity. [sent-16, score-0.592]
</p><p>4 For example, suppose your model includes a parameter p that is defined on [0,1] and is given a Beta(2,2) prior, and suppose the data don’t tell us anything about p, so that our posterior is also Beta(2,2). [sent-18, score-0.676]
</p><p>5 Even if all priors are proper, so the full posterior is proper, it contains all these copies so this labeling is not identified in any real sense. [sent-25, score-0.475]
</p><p>6 Here, and in general, identification depends not just on the model but also on the data. [sent-26, score-0.673]
</p><p>7 First, in reaction to my definition that a Bayesian model is identified if the posterior distribution is proper, Ben said he agreed, but in that case “what good is the word ‘identified’? [sent-29, score-0.715]
</p><p>8 ”   I agree with Ben, indeed the concept of identification is less important in the Bayesian world than elsewhere. [sent-31, score-0.605]
</p><p>9 Ben continues:    I agree that a lot of people use the word identification without defining what they mean, but there are no shortage of definitions out there. [sent-34, score-0.548]
</p><p>10 However, I’m not sure that identification is that helpful a concept for the practical problems we are trying to solve here when providing recommendations on how users should write . [sent-35, score-0.781]
</p><p>11 I think many if not most people that think about identification rigorously have in mind a concept that is pre-statistical. [sent-37, score-0.605]
</p><p>12 In economics, the idea of identification of a parameter goes back at least to the Cowles Commission guys, such as in the first couple of papers  here . [sent-39, score-0.683]
</p><p>13 In causal inference, the idea of identification of an average causal effect is a property of a DAG in Pearl’s  stuff . [sent-40, score-0.493]
</p><p>14 This paper  basically is interested in situations where some parameter is not identified iff another parameter is zero. [sent-45, score-0.686]
</p><p>15 Regarding my example where the data provide no information on the parameter p defined on (0,1), Ben writes:    Do you mean that a particular sample doesn’t tell us anything about p or that data are incapable of telling us anything about p? [sent-54, score-0.488]
</p><p>16 That’s a practical problem that Stan users face, but I don’t think many people would consider it to be an identification problem. [sent-56, score-0.614]
</p><p>17 Maybe something that is somewhat unique to Stan is the idea of identified in the constrained parameter space but not identified in the unconstrained parameter space like we have with uniform sampling on the unit sphere. [sent-57, score-1.195]
</p><p>18 Regarding the question of whether identification is defined conditional on the data as well as the model, Ben writes:    Certainly, whether you have computational problems depends on the data, among other things. [sent-60, score-0.8]
</p><p>19 But to say that identification depends on the data goes against the conventional usage where identification is pre-statistical so we need to think about whether it would be more effective to try to redefine identification or to use other phrases to describe the problems we are trying to overcome. [sent-61, score-1.65]
</p><p>20 Finally, recall that the discussion all started because people were having problems running Stan with improper posteriors or with models with nearly flat priors and where certain parameters were not identified by the data alone. [sent-65, score-0.758]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('identification', 0.493), ('ben', 0.298), ('identified', 0.238), ('parameter', 0.19), ('identifiability', 0.188), ('posterior', 0.181), ('weak', 0.174), ('qoi', 0.16), ('stan', 0.156), ('improper', 0.128), ('likelihood', 0.123), ('model', 0.118), ('concept', 0.112), ('unique', 0.106), ('proper', 0.098), ('finite', 0.098), ('flat', 0.097), ('identifiable', 0.096), ('maximum', 0.091), ('estimator', 0.085), ('bayesian', 0.083), ('constrained', 0.083), ('defined', 0.076), ('space', 0.075), ('ridge', 0.07), ('situations', 0.068), ('definition', 0.068), ('posteriors', 0.066), ('users', 0.065), ('parameters', 0.064), ('inference', 0.063), ('asymptotically', 0.062), ('regarding', 0.062), ('depends', 0.062), ('infinity', 0.061), ('computational', 0.06), ('supplied', 0.058), ('reflections', 0.057), ('densities', 0.057), ('anything', 0.057), ('priors', 0.056), ('tricky', 0.056), ('practical', 0.056), ('distribution', 0.055), ('everywhere', 0.055), ('sd', 0.055), ('word', 0.055), ('problems', 0.055), ('prior', 0.054), ('data', 0.054)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="2208-tfidf-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>Introduction: We had some questions on the Stan list regarding identification.  The topic arose because people were fitting models with improper posterior distributions, the kind of model where there’s a ridge in the likelihood and the parameters are not otherwise constrained.
 
I tried to help by writing something on Bayesian identifiability for the Stan list.  Then Ben Goodrich came along and cleaned up what I wrote.  I think this might be of interest to many of you so I’ll repeat the discussion here.
 
Here’s what I wrote:
  
Identification is actually a tricky concept and is not so clearly defined.  In the broadest sense, a Bayesian model is identified if the posterior distribution is proper.  Then one can do Bayesian inference and that’s that.  No need to require a finite variance or even a finite mean, all that’s needed is a finite integral of the probability distribution.


That said, there are some reasons why a stronger definition can be useful:


1.  Weak identification.  Suppose that, wit</p><p>2 0.2045898 <a title="2208-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>Introduction: Nick Firoozye writes:
  
While I am absolutely sympathetic to the Bayesian agenda I am often troubled by the requirement of having priors. We must have priors on the parameter of an infinite number of model we have never seen before and I find this troubling. There is a similarly troubling problem in economics of utility theory. Utility is on consumables. To be complete a consumer must assign utility to all sorts of things they never would have encountered. More recent versions of utility theory instead make consumption goods a portfolio of attributes. Cadillacs are x many units of luxury y of transport etc etc. And we can automatically have personal utilities to all these attributes.  


I don’t ever see parameters. Some model have few and some have hundreds. Instead, I see data. So I don’t know how to have an opinion on parameters themselves. Rather I think it far more natural to have opinions on the behavior of models. The prior predictive density is a good and sensible notion. Also</p><p>3 0.18603991 <a title="2208-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-02-An_IV_won%E2%80%99t_save_your_life_if_the_line_is_tangled.html">550 andrew gelman stats-2011-02-02-An IV won’t save your life if the line is tangled</a></p>
<p>Introduction: Alex Tabarrok  quotes  Randall Morck and Bernard Yeung on difficulties with instrumental variables.  This reminded me of some related things I’ve written.
 
In the official story the causal question comes first and then the clever researcher comes up with an IV.  I suspect that often it’s the other way around:  you find a natural experiment and look at the consequences that flow from it.  And maybe that’s not such a bad thing.  See section 4 of  this article .
 
More generally, I think economists and political scientists are currently a bit overinvested in identification strategies.  I agree with Heckman’s point (as I understand it) that ultimately we should be building models that work for us rather than always thinking we can get causal inference on the cheap, as it were, by some trick or another.  (This is a point I briefly discuss in a couple places  here  and also in my recent paper for the causality volume that Don Green etc are involved with.)
 
I recently had this discussion wi</p><p>4 0.18394452 <a title="2208-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-29-More_by_Berger_and_me_on_weakly_informative_priors.html">1092 andrew gelman stats-2011-12-29-More by Berger and me on weakly informative priors</a></p>
<p>Introduction: A couple days ago we  discussed  some remarks by Tony O’Hagan and Jim Berger on weakly informative priors.  Jim  followed up  on Deborah Mayo’s blog with this:
  
Objective Bayesian priors are often improper (i.e., have infinite total mass), but this is not a problem when they are developed correctly. But not every improper prior is satisfactory. For instance, the constant prior is known to be unsatisfactory in many situations. The ‘solution’ pseudo-Bayesians often use is to choose a constant prior over a large but bounded set (a ‘weakly informative’ prior), saying it is now proper and so all is well. This is not true; if the constant prior on the whole parameter space is bad, so will be the constant prior over the bounded set. The problem is, in part, that some people confuse proper priors with subjective priors and, having learned that true subjective priors are fine, incorrectly presume that weakly informative proper priors are fine.
  
I have a few reactions to this:
 
1.  I agree</p><p>5 0.17822091 <a title="2208-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>Introduction: Following up on Christian’s  post  [link fixed] on the topic, I’d like to offer a few thoughts of my own.
 
In BDA, we express the idea that a noninformative prior is a placeholder:  you can use the noninformative prior to get the analysis started, then if your posterior distribution is less informative than you would like, or if it does not make sense, you can go back and add prior information.
 
Same thing for the data model (the “likelihood”), for that matter:  it often makes sense to start with something simple and conventional and then go from there.
 
So, in that sense, noninformative priors are no big deal, they’re just a way to get started.  Just don’t take them too seriously.
 
Traditionally in statistics we’ve worked with the paradigm of a single highly informative dataset with only weak external information.  But if the data are sparse and prior information is strong, we have to think differently.  And, when you increase the dimensionality of a problem, both these things hap</p><p>6 0.16408437 <a title="2208-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>7 0.15872213 <a title="2208-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>8 0.15691009 <a title="2208-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>9 0.15282683 <a title="2208-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>10 0.15257038 <a title="2208-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>11 0.15168205 <a title="2208-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-25-Bayes-respecting_experimental_design_and_other_things.html">1955 andrew gelman stats-2013-07-25-Bayes-respecting experimental design and other things</a></p>
<p>12 0.15065785 <a title="2208-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-03-Statistical_methods_that_work_in_some_settings_but_not_others.html">1560 andrew gelman stats-2012-11-03-Statistical methods that work in some settings but not others</a></p>
<p>13 0.14956646 <a title="2208-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>14 0.14941321 <a title="2208-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-19-Prior_distributions_on_derived_quantities_rather_than_on_parameters_themselves.html">1946 andrew gelman stats-2013-07-19-Prior distributions on derived quantities rather than on parameters themselves</a></p>
<p>15 0.14741234 <a title="2208-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>16 0.14136562 <a title="2208-tfidf-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-Stan_Model_of_the_Week%3A_Hierarchical_Modeling_of_Supernovas.html">2299 andrew gelman stats-2014-04-21-Stan Model of the Week: Hierarchical Modeling of Supernovas</a></p>
<p>17 0.14127333 <a title="2208-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-11-My_problem_with_the_Lindley_paradox.html">1757 andrew gelman stats-2013-03-11-My problem with the Lindley paradox</a></p>
<p>18 0.13695857 <a title="2208-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-28-Understanding_simulations_in_terms_of_predictive_inference%3F.html">1287 andrew gelman stats-2012-04-28-Understanding simulations in terms of predictive inference?</a></p>
<p>19 0.13671961 <a title="2208-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>20 0.13396838 <a title="2208-tfidf-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-23-Unhappy_with_improvement_by_a_factor_of_10%5E29.html">160 andrew gelman stats-2010-07-23-Unhappy with improvement by a factor of 10^29</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.243), (1, 0.216), (2, -0.002), (3, 0.074), (4, -0.02), (5, -0.005), (6, 0.089), (7, -0.077), (8, -0.072), (9, -0.034), (10, -0.064), (11, 0.01), (12, -0.042), (13, 0.01), (14, -0.014), (15, -0.037), (16, 0.011), (17, 0.017), (18, -0.014), (19, 0.008), (20, -0.024), (21, -0.075), (22, 0.001), (23, -0.009), (24, 0.019), (25, 0.066), (26, 0.005), (27, -0.008), (28, 0.014), (29, 0.034), (30, -0.002), (31, -0.014), (32, -0.015), (33, 0.014), (34, -0.037), (35, 0.046), (36, 0.04), (37, -0.045), (38, 0.021), (39, -0.006), (40, 0.017), (41, -0.02), (42, -0.033), (43, -0.003), (44, -0.04), (45, -0.017), (46, 0.055), (47, 0.043), (48, -0.001), (49, 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95888531 <a title="2208-lsi-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>Introduction: We had some questions on the Stan list regarding identification.  The topic arose because people were fitting models with improper posterior distributions, the kind of model where there’s a ridge in the likelihood and the parameters are not otherwise constrained.
 
I tried to help by writing something on Bayesian identifiability for the Stan list.  Then Ben Goodrich came along and cleaned up what I wrote.  I think this might be of interest to many of you so I’ll repeat the discussion here.
 
Here’s what I wrote:
  
Identification is actually a tricky concept and is not so clearly defined.  In the broadest sense, a Bayesian model is identified if the posterior distribution is proper.  Then one can do Bayesian inference and that’s that.  No need to require a finite variance or even a finite mean, all that’s needed is a finite integral of the probability distribution.


That said, there are some reasons why a stronger definition can be useful:


1.  Weak identification.  Suppose that, wit</p><p>2 0.79498738 <a title="2208-lsi-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-Stan_Model_of_the_Week%3A_Hierarchical_Modeling_of_Supernovas.html">2299 andrew gelman stats-2014-04-21-Stan Model of the Week: Hierarchical Modeling of Supernovas</a></p>
<p>Introduction: The Stan Model of the Week showcases research using Stan to push the limits of applied statistics.  If you have a model that you would like to submit for a future post then send us an  email . 
 
Our inaugural post comes from Nathan Sanders, a graduate student finishing up his thesis on astrophysics at Harvard. Nathan writes,
  
“Core-collapse supernovae, the luminous explosions of massive stars, exhibit an expansive and meaningful diversity of behavior in their brightness evolution over time (their “light curves”). Our group discovers and monitors these events using the Pan-STARRS1 telescope in Hawaii, and we’ve collected a dataset of about 20,000 individual photometric observations of about 80 Type IIP supernovae, the class my work has focused on. While this dataset provides one of the best available tools to infer the explosion properties of these supernovae, due to the nature of extragalactic astronomy (observing from distances 
  1 billion light years), these light curves typicall</p><p>3 0.78025514 <a title="2208-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>Introduction: Nick Firoozye writes:
  
While I am absolutely sympathetic to the Bayesian agenda I am often troubled by the requirement of having priors. We must have priors on the parameter of an infinite number of model we have never seen before and I find this troubling. There is a similarly troubling problem in economics of utility theory. Utility is on consumables. To be complete a consumer must assign utility to all sorts of things they never would have encountered. More recent versions of utility theory instead make consumption goods a portfolio of attributes. Cadillacs are x many units of luxury y of transport etc etc. And we can automatically have personal utilities to all these attributes.  


I don’t ever see parameters. Some model have few and some have hundreds. Instead, I see data. So I don’t know how to have an opinion on parameters themselves. Rather I think it far more natural to have opinions on the behavior of models. The prior predictive density is a good and sensible notion. Also</p><p>4 0.76972204 <a title="2208-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-30-Stan_Project%3A__Continuous_Relaxations_for_Discrete_MRFs.html">2003 andrew gelman stats-2013-08-30-Stan Project:  Continuous Relaxations for Discrete MRFs</a></p>
<p>Introduction: Hamiltonian Monte Carlo (HMC), as used by  Stan , is only defined for continuous parameters.  We’d love to be able to do discrete sampling.  So I was excited when I saw this:
  

Yichuan Zhang, Charles Sutton, Amos J Storkey, and Zoubin Ghahramani.  2012.  Continuous Relaxations for Discrete Hamiltonian Monte Carlo .   NIPS  25.


 Abstract:  Continuous relaxations play an important role in discrete optimization, but have not seen much use in approximate probabilistic inference. Here we show that a general form of the Gaussian Integral Trick makes it possible to transform a wide class of discrete variable undirected models into fully continuous systems. The continuous representation allows the use of gradient-based Hamiltonian Monte Carlo for inference, results in new ways of estimating normalization constants (partition functions), and in general opens up a number of new avenues for inference in difficult discrete systems. We demonstrate some of these continuous relaxation inference a</p><p>5 0.7654835 <a title="2208-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-19-Whassup_with_deviance_having_a_high_posterior_correlation_with_a_parameter_in_the_model%3F.html">1221 andrew gelman stats-2012-03-19-Whassup with deviance having a high posterior correlation with a parameter in the model?</a></p>
<p>Introduction: Jean Richardson writes: 
  
  
Do you know what might lead to a large negative cross-correlation (-0.95) between deviance and one of the model parameters?


Here’s the (brief) background:


I [Richardson] have written a Bayesian hierarchical site occupancy model for presence of disease on individual amphibians. The response variable is therefore binary (disease present/absent) and the probability of disease being present in an individual (psi) depends on various covariates (species of amphibian, location sampled, etc.) paramaterized using a logit link function.  Replicates are individuals sampled (tested for presence of disease) together.  The possibility of imperfect detection is included as p = (prob. disease detected given disease is present).


Posterior distributions were estimated using WinBUGS via R2WinBUGS. 
Simulated data from the model fit the real data very well and posterior distribution densities seem robust to any changes in the model (different priors, etc.)  All autocor</p><p>6 0.76203513 <a title="2208-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>7 0.75664681 <a title="2208-lsi-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-23-Unhappy_with_improvement_by_a_factor_of_10%5E29.html">160 andrew gelman stats-2010-07-23-Unhappy with improvement by a factor of 10^29</a></p>
<p>8 0.75475585 <a title="2208-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-02-Fighting_a_losing_battle.html">1518 andrew gelman stats-2012-10-02-Fighting a losing battle</a></p>
<p>9 0.74966598 <a title="2208-lsi-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-10-Stan_Model_of_the_Week%3A__PK_Calculation_of_IV_and_Oral_Dosing.html">2242 andrew gelman stats-2014-03-10-Stan Model of the Week:  PK Calculation of IV and Oral Dosing</a></p>
<p>10 0.74481839 <a title="2208-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Adding_more_information_can_make_the_variance_go_up_%28depending_on_your_model%29.html">810 andrew gelman stats-2011-07-20-Adding more information can make the variance go up (depending on your model)</a></p>
<p>11 0.7394982 <a title="2208-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-25-Bayes-respecting_experimental_design_and_other_things.html">1955 andrew gelman stats-2013-07-25-Bayes-respecting experimental design and other things</a></p>
<p>12 0.73855263 <a title="2208-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-19-Prior_distributions_on_derived_quantities_rather_than_on_parameters_themselves.html">1946 andrew gelman stats-2013-07-19-Prior distributions on derived quantities rather than on parameters themselves</a></p>
<p>13 0.73731536 <a title="2208-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>14 0.73152804 <a title="2208-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-30-Infill_asymptotics_and_sprawl_asymptotics.html">1877 andrew gelman stats-2013-05-30-Infill asymptotics and sprawl asymptotics</a></p>
<p>15 0.72925001 <a title="2208-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-10-Cross-validation_and_Bayesian_estimation_of_tuning_parameters.html">2129 andrew gelman stats-2013-12-10-Cross-validation and Bayesian estimation of tuning parameters</a></p>
<p>16 0.72780609 <a title="2208-lsi-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-26-WAIC_and_cross-validation_in_Stan%21.html">2349 andrew gelman stats-2014-05-26-WAIC and cross-validation in Stan!</a></p>
<p>17 0.72688895 <a title="2208-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-22-The_kluges_of_today_are_the_textbook_solutions_of_tomorrow..html">2143 andrew gelman stats-2013-12-22-The kluges of today are the textbook solutions of tomorrow.</a></p>
<p>18 0.72466314 <a title="2208-lsi-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-Running_into_a_Stan_Reference_by_Accident.html">2231 andrew gelman stats-2014-03-03-Running into a Stan Reference by Accident</a></p>
<p>19 0.72413331 <a title="2208-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-22-A_Bayesian_model_for_an_increasing_function%2C_in_Stan%21.html">2110 andrew gelman stats-2013-11-22-A Bayesian model for an increasing function, in Stan!</a></p>
<p>20 0.72268164 <a title="2208-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-23-Validation_of_Software_for_Bayesian_Models_Using_Posterior_Quantiles.html">1868 andrew gelman stats-2013-05-23-Validation of Software for Bayesian Models Using Posterior Quantiles</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(15, 0.05), (16, 0.085), (20, 0.035), (21, 0.047), (22, 0.013), (24, 0.247), (25, 0.012), (59, 0.016), (72, 0.063), (86, 0.042), (99, 0.27)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9787457 <a title="2208-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-23-My_new_writing_strategy.html">727 andrew gelman stats-2011-05-23-My new writing strategy</a></p>
<p>Introduction: In high school and college I would write long assignments using a series of outlines.  I’d start with a single sheet where I’d write down the key phrases, connect them with lines, and then write more and more phrases until the page was filled up.  Then I’d write a series of outlines, culminating in a sentence-level outline that was roughly one line per sentence of the paper.  Then I’d write.  It worked pretty well.  Or horribly, depending on how you look at it.  I was able to produce 10-page papers etc. on time.  But I think it crippled my writing style for years.  It’s taken me a long time to learn how to write directly–to explain clearly what I’ve done and why.  And I’m still working on the “why” part.  There’s a thin line between verbosity and terseness.
 
I went to MIT and my roommate was a computer science major.  He wrote me a word processor on his Atari 800, which did the job pretty well.  For my senior thesis I broke down and used the computers in campus.  I formatted it in tro</p><p>same-blog 2 0.97594655 <a title="2208-lda-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>Introduction: We had some questions on the Stan list regarding identification.  The topic arose because people were fitting models with improper posterior distributions, the kind of model where there’s a ridge in the likelihood and the parameters are not otherwise constrained.
 
I tried to help by writing something on Bayesian identifiability for the Stan list.  Then Ben Goodrich came along and cleaned up what I wrote.  I think this might be of interest to many of you so I’ll repeat the discussion here.
 
Here’s what I wrote:
  
Identification is actually a tricky concept and is not so clearly defined.  In the broadest sense, a Bayesian model is identified if the posterior distribution is proper.  Then one can do Bayesian inference and that’s that.  No need to require a finite variance or even a finite mean, all that’s needed is a finite integral of the probability distribution.


That said, there are some reasons why a stronger definition can be useful:


1.  Weak identification.  Suppose that, wit</p><p>3 0.96684611 <a title="2208-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-02-Blogads_update.html">1240 andrew gelman stats-2012-04-02-Blogads update</a></p>
<p>Introduction: A few months ago I  reported  on someone who wanted to insert text links into the blog.  I asked her how much they would pay and got no answer.
 
Yesterday, though, I received this reply:
  
Hello Andrew,


I am sorry for the delay in getting back to you. I’d like to make a proposal for your site. Please refer below.


We would like to place a simple text link ad on page http://andrewgelman.com/2011/07/super_sam_fuld/  to link to *** with the key phrase ***.


We will incorporate the key phrase into a sentence so it would read well. Rest assured it won’t sound obnoxious or advertorial. We will then process the final text link code as soon as you agree to our proposal. 


We can offer you $200 for this with the assumption that you will keep the link “live” on that page for 12 months or longer if you prefer.


Please get back to us with a quick reply on your thoughts on this and include your Paypal ID for payment process.  Hoping for a positive response from you.
  
I wrote back:
  
Hi,</p><p>4 0.96656811 <a title="2208-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-24-Latest_in_blog_advertising.html">1080 andrew gelman stats-2011-12-24-Latest in blog advertising</a></p>
<p>Introduction: I received the following message from “Patricia Lopez” of “Premium Link Ads”:
  
Hello,


I am interested in placing a text link on your page: http://andrewgelman.com/2011/07/super_sam_fuld/. The link would point to a page on a website that is relevant to your page and may be useful to your site visitors. We would be happy to compensate you for your time if it is something we are able to work out.


The best way to reach me is through a direct response to this email. This will help me get back to you about the right link request. Please let me know if you are interested, and if not thanks for your time. Thanks.
  
Usually I just ignore these, but after our recent  discussion  I decided to reply.  I wrote:
  
How much do you pay?
  
But no answer.  I wonder what’s going on?  I mean, why bother sending the email in the first place if you’re not going to follow up?</p><p>5 0.96636403 <a title="2208-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-18-Understanding_posterior_p-values.html">2029 andrew gelman stats-2013-09-18-Understanding posterior p-values</a></p>
<p>Introduction: David Kaplan writes:
  
I came across your  paper  “Understanding Posterior Predictive P-values”, and I have a question regarding your statement “If a posterior predictive p-value is 0.4, say, that means that, if we believe the model, we think there is a 40% chance that tomorrow’s value of T(y_rep) will exceed today’s T(y).” This is perfectly understandable to me and represents the idea of calibration.  However, I am unsure how this relates to statements about fit.  If T is the LR chi-square or Pearson chi-square, then your statement that there is a 40% chance that tomorrows value exceeds today’s value indicates bad fit, I think.  Yet, some literature indicates that high p-values suggest good fit.  Could you clarify this?
  
My reply:
 
I think that “fit” depends on the question being asked.  In this case, I’d say the model fits for this particular purpose, even though it might not fit for other purposes.
 
And here’s the abstract of the paper:
  
Posterior predictive p-values do not i</p><p>6 0.96256965 <a title="2208-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-05-Question_26_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1367 andrew gelman stats-2012-06-05-Question 26 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>7 0.96209532 <a title="2208-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-10-95%25_intervals_that_I_don%E2%80%99t_believe%2C_because_they%E2%80%99re_from_a_flat_prior_I_don%E2%80%99t_believe.html">1206 andrew gelman stats-2012-03-10-95% intervals that I don’t believe, because they’re from a flat prior I don’t believe</a></p>
<p>8 0.9616816 <a title="2208-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-03-Setting_aside_the_politics%2C_the_debate_over_the_new_health-care_study_reveals_that_we%E2%80%99re_moving_to_a_new_high_standard_of_statistical_journalism.html">1838 andrew gelman stats-2013-05-03-Setting aside the politics, the debate over the new health-care study reveals that we’re moving to a new high standard of statistical journalism</a></p>
<p>9 0.9614355 <a title="2208-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>10 0.96121788 <a title="2208-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-11-Steve_Jobs%E2%80%99s_cancer_and_science-based_medicine.html">953 andrew gelman stats-2011-10-11-Steve Jobs’s cancer and science-based medicine</a></p>
<p>11 0.96070778 <a title="2208-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-08-P-values_and_statistical_practice.html">1713 andrew gelman stats-2013-02-08-P-values and statistical practice</a></p>
<p>12 0.96049678 <a title="2208-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>13 0.96003735 <a title="2208-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-11-My_problem_with_the_Lindley_paradox.html">1757 andrew gelman stats-2013-03-11-My problem with the Lindley paradox</a></p>
<p>14 0.95981169 <a title="2208-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>15 0.95917928 <a title="2208-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>16 0.95866239 <a title="2208-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-10-Using_a_%E2%80%9Cpure_infographic%E2%80%9D_to_explore_differences_between_information_visualization_and_statistical_graphics.html">847 andrew gelman stats-2011-08-10-Using a “pure infographic” to explore differences between information visualization and statistical graphics</a></p>
<p>17 0.95856988 <a title="2208-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-11-Gelman_on_Hennig_on_Gelman_on_Bayes.html">1208 andrew gelman stats-2012-03-11-Gelman on Hennig on Gelman on Bayes</a></p>
<p>18 0.95820045 <a title="2208-lda-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Ken_Rice_presents_a_unifying_approach_to_statistical_inference_and_hypothesis_testing.html">2312 andrew gelman stats-2014-04-29-Ken Rice presents a unifying approach to statistical inference and hypothesis testing</a></p>
<p>19 0.95785511 <a title="2208-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-Fourteen_magic_words%3A_an_update.html">898 andrew gelman stats-2011-09-10-Fourteen magic words: an update</a></p>
<p>20 0.95778465 <a title="2208-lda-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-05-The_p-value_is_not_._._..html">1607 andrew gelman stats-2012-12-05-The p-value is not . . .</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
