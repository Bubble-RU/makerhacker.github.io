<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2183 andrew gelman stats-2014-01-23-Discussion on preregistration of research studies</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2014" href="../home/andrew_gelman_stats-2014_home.html">andrew_gelman_stats-2014</a> <a title="andrew_gelman_stats-2014-2183" href="#">andrew_gelman_stats-2014-2183</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2183 andrew gelman stats-2014-01-23-Discussion on preregistration of research studies</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2014-2183-html" href="http://andrewgelman.com/2014/01/23/discussion-preregistration-research-studies/">html</a></p><p>Introduction: Chris Chambers and I had an enlightening discussion the other day at the blog of Rolf Zwaan, regarding the Garden of Forking Paths ( go here  and scroll down through the comments).
 
Chris sent me the following note:
  
I’m writing a book at the moment about reforming practices in psychological research (focusing on various bad practices such as p-hacking, HARKing, low statistical power, publication bias, lack of data sharing etc. – and posing solutions such as pre-registration, Bayesian hypothesis testing, mandatory data archiving etc.) and I am arriving at rather unsettling conclusion: that null hypothesis significance testing (NHST) simply isn’t valid for observational research. If this is true then most of the psychological literature is statistically flawed.


I was wonder what your thoughts were on this, both from a statistical point of view and from your experience working in an observational field. 


We all know about the dangers of researcher degrees of freedom. We also know</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Chris sent me the following note:    I’m writing a book at the moment about reforming practices in psychological research (focusing on various bad practices such as p-hacking, HARKing, low statistical power, publication bias, lack of data sharing etc. [sent-2, score-0.39]
</p><p>2 – and posing solutions such as pre-registration, Bayesian hypothesis testing, mandatory data archiving etc. [sent-3, score-0.449]
</p><p>3 ) and I am arriving at rather unsettling conclusion: that null hypothesis significance testing (NHST) simply isn’t valid for observational research. [sent-4, score-0.776]
</p><p>4 We all know about the dangers of researcher degrees of freedom. [sent-7, score-0.286]
</p><p>5 We also know how it is easy it is to obtain significant p values in exploratory analyses that are meaningless and misleading. [sent-8, score-0.392]
</p><p>6 html   Given the threat of researcher degrees of freedom, do you feel that NHST ever an appropriate approach to exploratory (unregistered) inferential statistical analysis? [sent-13, score-0.582]
</p><p>7 So, null hypothesis significance testing is equivalent to a classical conf interval which is equivalent to Bayes with flat prior which can make sense if the effects size are large and the measurement error is strong. [sent-16, score-0.729]
</p><p>8 Chris responded:        If the best and only defence to researcher degrees of freedom is pre-registration, then how can scientists securely interpret p values in observational research? [sent-17, score-0.718]
</p><p>9 That is, doesn’t interpreting a p value carry the concrete requirement that no researcher dfs have been exploited? [sent-19, score-0.158]
</p><p>10 All pre-registration does is enable readers to distinguish confirmatory analysis from exploratory analysis – it doesn’t block exploratory analysis or hinder it in any way (that I can see). [sent-22, score-1.106]
</p><p>11 That being so, and assuming your major discoveries stemmed from exploratory analysis, why would having those same exploratory analyses form part of a pre-registered study make any difference to their interpretation or impact? [sent-23, score-0.95]
</p><p>12 I find this discussion intriguing because I’ve never seen pre-registration an an enemy of exploration, only as an aid to distinguish hypothesis testing from hypothesis generation. [sent-28, score-0.654]
</p><p>13 I replied:   I don’t think the existence of preregistration would have killed my results, and I support proposals in psychology and political science to allow preregistration to be done in an open way. [sent-29, score-0.696]
</p><p>14 I just wouldn’t want preregistration to be  required , indeed the concept of preregistration would seem to me to be just about impossible to apply in the analysis of public datasets such as we use in political science. [sent-30, score-0.854]
</p><p>15 And here’s Chris again:    I don’t think pre-registration should be mandatory either. [sent-32, score-0.2]
</p><p>16 Though I think it should be strongly encouraged in fields where undisclosed flexibility is identified as a major cause of false discoveries (which is certainly the case in psychology and cognitive neuroscience). [sent-33, score-0.443]
</p><p>17 As you say, it’s more challenging for areas that rely on analysis of existing datasets. [sent-34, score-0.216]
</p><p>18 In psychology I think the solution in that case is to consider all analyses of existing datasets as (by definition) exploratory and thus most valuable in terms of hypothesis generation and modeling. [sent-35, score-0.889]
</p><p>19 Having said that, I don’t know if you were aware of this but the revised Declaration of Helsinki (to which major psychology and neuro journals adhere) now requires mandatory pre-registration. [sent-36, score-0.408]
</p><p>20 And  here are my previously published thoughts  on preregistration in political science. [sent-50, score-0.291]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('exploratory', 0.296), ('preregistration', 0.291), ('mandatory', 0.2), ('nhst', 0.2), ('chris', 0.184), ('hypothesis', 0.179), ('discoveries', 0.168), ('researcher', 0.158), ('analysis', 0.142), ('testing', 0.134), ('datasets', 0.13), ('degrees', 0.128), ('observational', 0.125), ('null', 0.125), ('psychology', 0.114), ('equivalent', 0.113), ('analyses', 0.096), ('major', 0.094), ('practices', 0.089), ('distinguish', 0.088), ('freedom', 0.086), ('responded', 0.077), ('interpret', 0.077), ('existing', 0.074), ('unsettling', 0.074), ('arriving', 0.074), ('enlightening', 0.074), ('defence', 0.074), ('dorothy', 0.074), ('enemy', 0.074), ('mindset', 0.074), ('research', 0.073), ('psychological', 0.072), ('recruitment', 0.07), ('securely', 0.07), ('registering', 0.07), ('archiving', 0.07), ('helsinki', 0.07), ('declaration', 0.07), ('preregister', 0.07), ('replied', 0.069), ('undisclosed', 0.067), ('adhere', 0.067), ('reforming', 0.067), ('zwaan', 0.067), ('http', 0.066), ('bias', 0.066), ('significance', 0.065), ('prevented', 0.064), ('chambers', 0.064)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="2183-tfidf-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-23-Discussion_on_preregistration_of_research_studies.html">2183 andrew gelman stats-2014-01-23-Discussion on preregistration of research studies</a></p>
<p>Introduction: Chris Chambers and I had an enlightening discussion the other day at the blog of Rolf Zwaan, regarding the Garden of Forking Paths ( go here  and scroll down through the comments).
 
Chris sent me the following note:
  
I’m writing a book at the moment about reforming practices in psychological research (focusing on various bad practices such as p-hacking, HARKing, low statistical power, publication bias, lack of data sharing etc. – and posing solutions such as pre-registration, Bayesian hypothesis testing, mandatory data archiving etc.) and I am arriving at rather unsettling conclusion: that null hypothesis significance testing (NHST) simply isn’t valid for observational research. If this is true then most of the psychological literature is statistically flawed.


I was wonder what your thoughts were on this, both from a statistical point of view and from your experience working in an observational field. 


We all know about the dangers of researcher degrees of freedom. We also know</p><p>2 0.30952764 <a title="2183-tfidf-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-10-Preregistration%3A_what%E2%80%99s_in_it_for_you%3F.html">2241 andrew gelman stats-2014-03-10-Preregistration: what’s in it for you?</a></p>
<p>Introduction: Chris Chambers pointed me to a blog by someone called Neuroskeptic who  suggested  that I preregister my political science studies:
  
So when Andrew Gelman (let’s say) is going to start using a new approach, he goes on Twitter, or on his blog, and posts a bare-bones summary of what he’s going to do. Then he does it. If he finds something interesting, he writes it up as a paper, citing that tweet or post as his preregistration. . . .
  
I think this approach has some benefits but doesn’t really address the issues of preregistration that concern me—but I’d like to spend an entire blog post explaining why.  I have two key points:
 
1.  If your study is crap, preregistration might fix it.  Preregistration is fine—indeed, the wide acceptance of preregistration might well motivate researchers to not do so many crap studies—but it doesn’t solve fundamental problems of experimental design.
 
2.  “Preregistration” seems to mean different things in different scenarios:
 
A.  When the concern is</p><p>3 0.19476987 <a title="2183-tfidf-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21.html">256 andrew gelman stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</a></p>
<p>Introduction: Masanao sends  this one  in, under the heading, “another incident of misunderstood p-value”:
  
Warren Davies, a positive psychology MSc student at UEL, provides the latest in our ongoing series of guest features for students. Warren has just released a Psychology Study Guide, which covers information on statistics, research methods and study skills for psychology students.

 
Despite the myriad rules and procedures of science, some research findings are pure flukes. Perhaps you’re testing a new drug, and by chance alone, a large number of people spontaneously get better. The better your study is conducted, the lower the chance that your result was a fluke – but still, there is always a certain probability that it was.


Statistical significance testing gives you an idea of what this probability is.


In science we’re always testing hypotheses. We never conduct a study to ‘see what happens’, because there’s always at least one way to make any useless set of data look important. We take</p><p>4 0.17277032 <a title="2183-tfidf-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-24-Empirical_implications_of_Empirical_Implications_of_Theoretical_Models.html">2263 andrew gelman stats-2014-03-24-Empirical implications of Empirical Implications of Theoretical Models</a></p>
<p>Introduction: Robert Bloomfield writes:
  
Most of the people in my field (accounting, which is basically applied economics and finance, leavened with psychology and organizational behavior) use ‘positive research methods’, which are typically described as coming to the data with a predefined theory, and using hypothesis testing to accept or reject the theory’s predictions.  But a substantial minority use ‘interpretive research methods’ (sometimes called qualitative methods, for those that call positive research ‘quantitative’).  No one seems entirely happy with the definition of this method, but I’ve found it useful to think of it as an attempt to see the world through the eyes of your subjects, much as Jane Goodall lived with gorillas and tried to see the world through their eyes.)


Interpretive researchers often criticize positive researchers by noting that the latter don’t make the best use of their data, because they come to the data with a predetermined theory, and only test a narrow set of h</p><p>5 0.16813323 <a title="2183-tfidf-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-20-How_to_schedule_projects_in_an_introductory_statistics_course%3F.html">423 andrew gelman stats-2010-11-20-How to schedule projects in an introductory statistics course?</a></p>
<p>Introduction: John Haubrick writes:
  
Next semester I want to center my statistics class around independent projects that they will present at the end of the semester.   My question is, by centering around a project and teaching for the different parts that they need at the time, should topics such as hypothesis testing be moved toward the beginning of the course?  Or should I only discuss setting up a research hypothesis and discuss the actual testing later after they have the data?
  
My reply:
 
Iâ&euro;&trade;m not sure.  There always is a difficulty of what can be covered in a project.  My quick thought is that a project will perhaps work better if it is focused on data collection or exploratory data analysis rather than on estimation and hypothesis testing, which are topics that get covered pretty well in the course as a whole.</p><p>6 0.16677989 <a title="2183-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-25-The_harm_done_by_tests_of_significance.html">1776 andrew gelman stats-2013-03-25-The harm done by tests of significance</a></p>
<p>7 0.16459417 <a title="2183-tfidf-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-18-One-tailed_or_two-tailed%3F.html">2295 andrew gelman stats-2014-04-18-One-tailed or two-tailed?</a></p>
<p>8 0.16410726 <a title="2183-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-04-Write_This_Book.html">1605 andrew gelman stats-2012-12-04-Write This Book</a></p>
<p>9 0.15854912 <a title="2183-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-31-Response_by_Jessica_Tracy_and_Alec_Beall_to_my_critique_of_the_methods_in_their_paper%2C_%E2%80%9CWomen_Are_More_Likely_to_Wear_Red_or_Pink_at_Peak_Fertility%E2%80%9D.html">1963 andrew gelman stats-2013-07-31-Response by Jessica Tracy and Alec Beall to my critique of the methods in their paper, “Women Are More Likely to Wear Red or Pink at Peak Fertility”</a></p>
<p>10 0.15560976 <a title="2183-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-17-Ripley_on_model_selection%2C_and_some_links_on_exploratory_model_analysis.html">1066 andrew gelman stats-2011-12-17-Ripley on model selection, and some links on exploratory model analysis</a></p>
<p>11 0.14410596 <a title="2183-tfidf-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>12 0.14067233 <a title="2183-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>13 0.13891146 <a title="2183-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-23-Of_hypothesis_tests_and_Unitarians.html">1024 andrew gelman stats-2011-11-23-Of hypothesis tests and Unitarians</a></p>
<p>14 0.13521373 <a title="2183-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-20-More_proposals_to_reform_the_peer-review_system.html">1272 andrew gelman stats-2012-04-20-More proposals to reform the peer-review system</a></p>
<p>15 0.1303716 <a title="2183-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-26-%E2%80%9CA_Vast_Graveyard_of_Undead_Theories%3A__Publication_Bias_and_Psychological_Science%E2%80%99s_Aversion_to_the_Null%E2%80%9D.html">1826 andrew gelman stats-2013-04-26-“A Vast Graveyard of Undead Theories:  Publication Bias and Psychological Science’s Aversion to the Null”</a></p>
<p>16 0.12848289 <a title="2183-tfidf-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>17 0.12676337 <a title="2183-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-31-Lindley%E2%80%99s_paradox.html">1355 andrew gelman stats-2012-05-31-Lindley’s paradox</a></p>
<p>18 0.12564105 <a title="2183-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-19-The_scope_for_snooping.html">1070 andrew gelman stats-2011-12-19-The scope for snooping</a></p>
<p>19 0.12363222 <a title="2183-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>20 0.12261765 <a title="2183-tfidf-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-26-New_research_journal_on_observational_studies.html">2268 andrew gelman stats-2014-03-26-New research journal on observational studies</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.235), (1, 0.023), (2, -0.019), (3, -0.148), (4, -0.05), (5, -0.055), (6, -0.063), (7, 0.029), (8, -0.016), (9, -0.049), (10, -0.054), (11, 0.027), (12, 0.051), (13, -0.098), (14, 0.039), (15, -0.011), (16, -0.056), (17, -0.07), (18, 0.024), (19, -0.049), (20, 0.059), (21, -0.007), (22, -0.007), (23, -0.002), (24, -0.071), (25, -0.056), (26, 0.077), (27, -0.039), (28, 0.01), (29, -0.027), (30, -0.008), (31, -0.031), (32, 0.036), (33, 0.044), (34, -0.037), (35, -0.003), (36, 0.001), (37, -0.025), (38, 0.012), (39, 0.001), (40, -0.046), (41, -0.012), (42, 0.008), (43, 0.021), (44, 0.016), (45, 0.011), (46, 0.034), (47, -0.027), (48, -0.014), (49, 0.013)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96907687 <a title="2183-lsi-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-23-Discussion_on_preregistration_of_research_studies.html">2183 andrew gelman stats-2014-01-23-Discussion on preregistration of research studies</a></p>
<p>Introduction: Chris Chambers and I had an enlightening discussion the other day at the blog of Rolf Zwaan, regarding the Garden of Forking Paths ( go here  and scroll down through the comments).
 
Chris sent me the following note:
  
I’m writing a book at the moment about reforming practices in psychological research (focusing on various bad practices such as p-hacking, HARKing, low statistical power, publication bias, lack of data sharing etc. – and posing solutions such as pre-registration, Bayesian hypothesis testing, mandatory data archiving etc.) and I am arriving at rather unsettling conclusion: that null hypothesis significance testing (NHST) simply isn’t valid for observational research. If this is true then most of the psychological literature is statistically flawed.


I was wonder what your thoughts were on this, both from a statistical point of view and from your experience working in an observational field. 


We all know about the dangers of researcher degrees of freedom. We also know</p><p>2 0.87520415 <a title="2183-lsi-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-18-One-tailed_or_two-tailed%3F.html">2295 andrew gelman stats-2014-04-18-One-tailed or two-tailed?</a></p>
<p>Introduction: Someone writes:
  
Suppose I have two groups of people, A and B, which differ on some characteristic of interest to me;  and for each person I measure a single real-valued quantity X.  I have a theory that group A has a higher mean value of X than group B.  I test this theory by using a t-test.  Am I entitled to use a *one-tailed* t-test?  Or should I use a *two-tailed* one (thereby giving a p-value that is twice as large)?


I know you will probably answer:  Forget the t-test; you should use Bayesian methods instead.


But what is the standard frequentist answer to this question?
  
My reply:
 
The quick answer here is that different people will do different things here.  I would say the 2-tailed p-value is more standard but some people will insist on the one-tailed version, and itâ&euro;&trade;s hard to make a big stand on this one, given all the other problems with p-values in practice:
 
http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf
 
http://www.stat.columbia.edu/~gelm</p><p>3 0.87022024 <a title="2183-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>Introduction: This article  is a discussion of a  paper  by Greg Francis for a special issue, edited by E. J. Wagenmakers, of the Journal of Mathematical Psychology.  Here’s what I wrote:
  
Much of statistical practice is an effort to reduce or deny variation and uncertainty. The reduction is done through standardization, replication, and other practices of experimental design, with the idea being to isolate and stabilize the quantity being estimated and then average over many cases. Even so, however, uncertainty persists, and statistical hypothesis testing is in many ways an endeavor to deny this, by reporting binary accept/reject decisions.


Classical statistical methods produce binary statements, but there is no reason to assume that the world works that way. Expressions such as Type 1 error, Type 2 error, false positive, and so on, are based on a model in which the world is divided into real and non-real effects. To put it another way, I understand the general scientific distinction of real vs</p><p>4 0.86033499 <a title="2183-lsi-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21.html">256 andrew gelman stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</a></p>
<p>Introduction: Masanao sends  this one  in, under the heading, “another incident of misunderstood p-value”:
  
Warren Davies, a positive psychology MSc student at UEL, provides the latest in our ongoing series of guest features for students. Warren has just released a Psychology Study Guide, which covers information on statistics, research methods and study skills for psychology students.

 
Despite the myriad rules and procedures of science, some research findings are pure flukes. Perhaps you’re testing a new drug, and by chance alone, a large number of people spontaneously get better. The better your study is conducted, the lower the chance that your result was a fluke – but still, there is always a certain probability that it was.


Statistical significance testing gives you an idea of what this probability is.


In science we’re always testing hypotheses. We never conduct a study to ‘see what happens’, because there’s always at least one way to make any useless set of data look important. We take</p><p>5 0.84122324 <a title="2183-lsi-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-24-Empirical_implications_of_Empirical_Implications_of_Theoretical_Models.html">2263 andrew gelman stats-2014-03-24-Empirical implications of Empirical Implications of Theoretical Models</a></p>
<p>Introduction: Robert Bloomfield writes:
  
Most of the people in my field (accounting, which is basically applied economics and finance, leavened with psychology and organizational behavior) use ‘positive research methods’, which are typically described as coming to the data with a predefined theory, and using hypothesis testing to accept or reject the theory’s predictions.  But a substantial minority use ‘interpretive research methods’ (sometimes called qualitative methods, for those that call positive research ‘quantitative’).  No one seems entirely happy with the definition of this method, but I’ve found it useful to think of it as an attempt to see the world through the eyes of your subjects, much as Jane Goodall lived with gorillas and tried to see the world through their eyes.)


Interpretive researchers often criticize positive researchers by noting that the latter don’t make the best use of their data, because they come to the data with a predetermined theory, and only test a narrow set of h</p><p>6 0.8273561 <a title="2183-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-26-%E2%80%9CA_Vast_Graveyard_of_Undead_Theories%3A__Publication_Bias_and_Psychological_Science%E2%80%99s_Aversion_to_the_Null%E2%80%9D.html">1826 andrew gelman stats-2013-04-26-“A Vast Graveyard of Undead Theories:  Publication Bias and Psychological Science’s Aversion to the Null”</a></p>
<p>7 0.81383491 <a title="2183-lsi-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>8 0.81175226 <a title="2183-lsi-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-08-Discussion_with_Steven_Pinker_on_research_that_is_attached_to_data_that_are_so_noisy_as_to_be_essentially_uninformative.html">2326 andrew gelman stats-2014-05-08-Discussion with Steven Pinker on research that is attached to data that are so noisy as to be essentially uninformative</a></p>
<p>9 0.80331808 <a title="2183-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-25-The_harm_done_by_tests_of_significance.html">1776 andrew gelman stats-2013-03-25-The harm done by tests of significance</a></p>
<p>10 0.79439551 <a title="2183-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-31-Lindley%E2%80%99s_paradox.html">1355 andrew gelman stats-2012-05-31-Lindley’s paradox</a></p>
<p>11 0.79064739 <a title="2183-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-Fourteen_magic_words%3A_an_update.html">898 andrew gelman stats-2011-09-10-Fourteen magic words: an update</a></p>
<p>12 0.78833544 <a title="2183-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-08-Statistical_significance_and_the_dangerous_lure_of_certainty.html">1974 andrew gelman stats-2013-08-08-Statistical significance and the dangerous lure of certainty</a></p>
<p>13 0.78531778 <a title="2183-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>14 0.77937502 <a title="2183-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-16-%E2%80%9CFalse-positive_psychology%E2%80%9D.html">1171 andrew gelman stats-2012-02-16-“False-positive psychology”</a></p>
<p>15 0.77275324 <a title="2183-lsi-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-23-Of_hypothesis_tests_and_Unitarians.html">1024 andrew gelman stats-2011-11-23-Of hypothesis tests and Unitarians</a></p>
<p>16 0.76416588 <a title="2183-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-04-Write_This_Book.html">1605 andrew gelman stats-2012-12-04-Write This Book</a></p>
<p>17 0.75596607 <a title="2183-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-31-Response_by_Jessica_Tracy_and_Alec_Beall_to_my_critique_of_the_methods_in_their_paper%2C_%E2%80%9CWomen_Are_More_Likely_to_Wear_Red_or_Pink_at_Peak_Fertility%E2%80%9D.html">1963 andrew gelman stats-2013-07-31-Response by Jessica Tracy and Alec Beall to my critique of the methods in their paper, “Women Are More Likely to Wear Red or Pink at Peak Fertility”</a></p>
<p>18 0.7455197 <a title="2183-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-11-One_more_time_on_that_ESP_study%3A__The_problem_of_overestimates_and_the_shrinkage_solution.html">511 andrew gelman stats-2011-01-11-One more time on that ESP study:  The problem of overestimates and the shrinkage solution</a></p>
<p>19 0.74189478 <a title="2183-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-07-I%E2%80%99m_negative_on_the_expression_%E2%80%9Cfalse_positives%E2%80%9D.html">2093 andrew gelman stats-2013-11-07-I’m negative on the expression “false positives”</a></p>
<p>20 0.73955768 <a title="2183-lsi-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-25-Revised_statistical_standards_for_evidence_%28comments_to_Val_Johnson%E2%80%99s_comments_on_our_comments_on_Val%E2%80%99s_comments_on_p-values%29.html">2305 andrew gelman stats-2014-04-25-Revised statistical standards for evidence (comments to Val Johnson’s comments on our comments on Val’s comments on p-values)</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.018), (2, 0.027), (3, 0.01), (8, 0.025), (15, 0.029), (16, 0.088), (21, 0.031), (24, 0.165), (28, 0.01), (44, 0.014), (47, 0.067), (53, 0.038), (63, 0.013), (70, 0.013), (79, 0.019), (84, 0.044), (86, 0.025), (89, 0.011), (99, 0.25)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97125065 <a title="2183-lda-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-23-Discussion_on_preregistration_of_research_studies.html">2183 andrew gelman stats-2014-01-23-Discussion on preregistration of research studies</a></p>
<p>Introduction: Chris Chambers and I had an enlightening discussion the other day at the blog of Rolf Zwaan, regarding the Garden of Forking Paths ( go here  and scroll down through the comments).
 
Chris sent me the following note:
  
I’m writing a book at the moment about reforming practices in psychological research (focusing on various bad practices such as p-hacking, HARKing, low statistical power, publication bias, lack of data sharing etc. – and posing solutions such as pre-registration, Bayesian hypothesis testing, mandatory data archiving etc.) and I am arriving at rather unsettling conclusion: that null hypothesis significance testing (NHST) simply isn’t valid for observational research. If this is true then most of the psychological literature is statistically flawed.


I was wonder what your thoughts were on this, both from a statistical point of view and from your experience working in an observational field. 


We all know about the dangers of researcher degrees of freedom. We also know</p><p>2 0.95576733 <a title="2183-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-01-What_goes_around_._._..html">548 andrew gelman stats-2011-02-01-What goes around . . .</a></p>
<p>Introduction: A few weeks ago I delivered a 10-minute talk on statistical graphics that went so well, it was the best-received talk I’ve ever given.  The crowd was raucous.  Then some poor sap had to go on after me.  He started by saying that my talk was a hard act to follow.  And, indeed, the audience politely listened but did not really get involved in his presentation.  Boy did I feel smug.
 
More recently I gave a talk on Stan, at an entirely different venue.  And this time the story was the exact opposite.  Jim Demmel spoke first and gave a wonderful talk on optimization for linear algebra (it was an applied math conference).  Then I followed, and I never really grabbed the crowd.  My talk was not a disaster but it didn’t really work.  This was particularly frustrating because I’m really excited about Stan and this was a group of researchers I wouldn’t usually have a chance to reach.  It was the plenary session at the conference.
 
Anyway, now I know how that guy felt from last month.  My talk</p><p>3 0.95551658 <a title="2183-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>Introduction: This article  is a discussion of a  paper  by Greg Francis for a special issue, edited by E. J. Wagenmakers, of the Journal of Mathematical Psychology.  Here’s what I wrote:
  
Much of statistical practice is an effort to reduce or deny variation and uncertainty. The reduction is done through standardization, replication, and other practices of experimental design, with the idea being to isolate and stabilize the quantity being estimated and then average over many cases. Even so, however, uncertainty persists, and statistical hypothesis testing is in many ways an endeavor to deny this, by reporting binary accept/reject decisions.


Classical statistical methods produce binary statements, but there is no reason to assume that the world works that way. Expressions such as Type 1 error, Type 2 error, false positive, and so on, are based on a model in which the world is divided into real and non-real effects. To put it another way, I understand the general scientific distinction of real vs</p><p>4 0.95414239 <a title="2183-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-18-Check_your_missing-data_imputations_using_cross-validation.html">1218 andrew gelman stats-2012-03-18-Check your missing-data imputations using cross-validation</a></p>
<p>Introduction: Elena Grewal writes:
  
I am currently using the iterative regression imputation model as implemented in the Stata ICE package. I am using data from a survey of about 90,000 students in 142 schools and my variable of interest is parent level of education. I want only this variable to be imputed with as little bias as possible as I am not using any other variable. So I scoured the survey for every variable I thought could possibly predict parent education. The main variable I found is parent occupation, which explains about 35% of the variance in parent education for the students with complete data on both. I then include the 20 other variables I found in the survey in a regression predicting parent education, which explains about 40% of the variance in parent education for students with complete data on all the variables. 


My question is this: many of the other variables I found have more missing values than the parent education variable, and also, although statistically significant</p><p>5 0.95256478 <a title="2183-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>Introduction: In response to the  discussion  of X and me of his recent  paper , Val Johnson writes:
  
I would like to thank Andrew for forwarding his comments on uniformly most powerful Bayesian tests (UMPBTs) to me and his invitation to respond to them.  I think he  (and also Christian Robert) raise a number of interesting points concerning this new class of Bayesian tests, but I think that they may have confounded several issues that might more usefully be examined separately.


The first issue involves the choice of the Bayesian evidence threshold, gamma, used in rejecting a null hypothesis in favor of an alternative hypothesis.  Andrew objects to the higher values of gamma proposed in my recent PNAS article on grounds that too many important scientific effects would be missed if thresholds of 25-50 were routinely used.  These evidence thresholds correspond roughly to p-values of 0.005; Andrew suggests that evidence thresholds around 5 should continue to be used (gamma=5 corresponds approximate</p><p>6 0.94873035 <a title="2183-lda-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-Stan_Model_of_the_Week%3A_Hierarchical_Modeling_of_Supernovas.html">2299 andrew gelman stats-2014-04-21-Stan Model of the Week: Hierarchical Modeling of Supernovas</a></p>
<p>7 0.94846016 <a title="2183-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-19-Updated_solutions_to_Bayesian_Data_Analysis_homeworks.html">42 andrew gelman stats-2010-05-19-Updated solutions to Bayesian Data Analysis homeworks</a></p>
<p>8 0.94678605 <a title="2183-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>9 0.9466207 <a title="2183-lda-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-25-Revised_statistical_standards_for_evidence_%28comments_to_Val_Johnson%E2%80%99s_comments_on_our_comments_on_Val%E2%80%99s_comments_on_p-values%29.html">2305 andrew gelman stats-2014-04-25-Revised statistical standards for evidence (comments to Val Johnson’s comments on our comments on Val’s comments on p-values)</a></p>
<p>10 0.94645077 <a title="2183-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-27-%E2%80%9CHow_to_Lie_with_Statistics%E2%80%9D_guy_worked_for_the_tobacco_industry_to_mock_studies_of_the_risks_of_smoking_statistics.html">1285 andrew gelman stats-2012-04-27-“How to Lie with Statistics” guy worked for the tobacco industry to mock studies of the risks of smoking statistics</a></p>
<p>11 0.94562072 <a title="2183-lda-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-27-Graph_of_the_year.html">488 andrew gelman stats-2010-12-27-Graph of the year</a></p>
<p>12 0.94526601 <a title="2183-lda-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-06-Bootstrap_averaging%3A_Examples_where_it_works_and_where_it_doesn%E2%80%99t_work.html">2201 andrew gelman stats-2014-02-06-Bootstrap averaging: Examples where it works and where it doesn’t work</a></p>
<p>13 0.94503337 <a title="2183-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-07-Prior_distributions_for_regression_coefficients.html">1486 andrew gelman stats-2012-09-07-Prior distributions for regression coefficients</a></p>
<p>14 0.94485199 <a title="2183-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-01-Post-publication_peer_review%3A__How_it_%28sometimes%29_really_works.html">2004 andrew gelman stats-2013-09-01-Post-publication peer review:  How it (sometimes) really works</a></p>
<p>15 0.94433457 <a title="2183-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-Fourteen_magic_words%3A_an_update.html">898 andrew gelman stats-2011-09-10-Fourteen magic words: an update</a></p>
<p>16 0.94157743 <a title="2183-lda-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-04-Rob_Kass_on_statistical_pragmatism%2C_and_my_reactions.html">317 andrew gelman stats-2010-10-04-Rob Kass on statistical pragmatism, and my reactions</a></p>
<p>17 0.9413203 <a title="2183-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-23-A_statistical_version_of_Arrow%E2%80%99s_paradox.html">586 andrew gelman stats-2011-02-23-A statistical version of Arrow’s paradox</a></p>
<p>18 0.94051135 <a title="2183-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-13-What_are_the_important_issues_in_ethics_and_statistics%3F__I%E2%80%99m_looking_for_your_input%21.html">1117 andrew gelman stats-2012-01-13-What are the important issues in ethics and statistics?  I’m looking for your input!</a></p>
<p>19 0.94039345 <a title="2183-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-13-When%E2%80%99s_that_next_gamma-ray_blast_gonna_come%2C_already%3F.html">1897 andrew gelman stats-2013-06-13-When’s that next gamma-ray blast gonna come, already?</a></p>
<p>20 0.94038868 <a title="2183-lda-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-16-%E2%80%9CFalse-positive_psychology%E2%80%9D.html">1171 andrew gelman stats-2012-02-16-“False-positive psychology”</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
