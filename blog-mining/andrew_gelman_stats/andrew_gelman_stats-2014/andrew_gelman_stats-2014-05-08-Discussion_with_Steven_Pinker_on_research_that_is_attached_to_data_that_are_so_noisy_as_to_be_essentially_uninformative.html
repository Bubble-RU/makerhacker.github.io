<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2326 andrew gelman stats-2014-05-08-Discussion with Steven Pinker on research that is attached to data that are so noisy as to be essentially uninformative</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2014" href="../home/andrew_gelman_stats-2014_home.html">andrew_gelman_stats-2014</a> <a title="andrew_gelman_stats-2014-2326" href="#">andrew_gelman_stats-2014-2326</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2326 andrew gelman stats-2014-05-08-Discussion with Steven Pinker on research that is attached to data that are so noisy as to be essentially uninformative</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2014-2326-html" href="http://andrewgelman.com/2014/05/08/discussion-steven-pinker-research-attached-data-noisy-essentially-uninformative/">html</a></p><p>Introduction: I pointed Steven Pinker to my post,  How much time (if any) should we spend criticizing research that’s fraudulent, crappy, or just plain pointless? , and he responded:
  
Clearly it *is* important to call out publicized research whose conclusions are likely to be false. The only danger is that it’s so easy and fun to criticize, with all the perks of intellectual and moral superiority for so little cost, that there is a moral hazard to go overboard and become a professional slasher and snarker. (That’s a common phenomenon among literary critics, especially in the UK.) There’s also the risk of altering the incentive structure for innovative research, so that researchers stick to the safest kinds of paradigm-twiddling. I think these two considerations were what my late colleague Dan Wegner had in mind when he made the bumbler-pointer contrast — he himself was certainly a discerning critic of social science research. [Just to clarify:  Wegner is the person who talked about bumblers and po</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The only danger is that it’s so easy and fun to criticize, with all the perks of intellectual and moral superiority for so little cost, that there is a moral hazard to go overboard and become a professional slasher and snarker. [sent-3, score-0.616]
</p><p>2 ) There’s also the risk of altering the incentive structure for innovative research, so that researchers stick to the safest kinds of paradigm-twiddling. [sent-5, score-0.507]
</p><p>3 I think these two considerations were what my late colleague Dan Wegner had in mind when he made the bumbler-pointer contrast — he himself was certainly a discerning critic of social science research. [sent-6, score-0.231]
</p><p>4 [Just to clarify:  Wegner is the person who talked about bumblers and pointers but he was not the person who sent me the email characterizing these as "our only choices in life. [sent-7, score-0.252]
</p><p>5 ]   The other comment is that I don’t think that evolutionary psychology is a worse offender at noise-mining than social psychology in general. [sent-9, score-0.455]
</p><p>6 Quite the contrary, the requirement that a psychological mechanism enhance reproductive success in a pre-modern environment at least imposes a modicum of aprioricity on hypotheses, which is entirely lacking in non-evolutionary (and defiantly atheoretical) social psychology. [sent-10, score-0.43]
</p><p>7 The worry that you can spin scientifically respectable evolutionary hypotheses post hoc for any finding is, in my view, greatly exaggerated. [sent-11, score-0.497]
</p><p>8 The Griskevicius finding may be wrong, for all the usual reasons, but the hypothesis is well motivated by prior theory and research. [sent-12, score-0.408]
</p><p>9 As Lakatos and other philosophers of science have emphasized, any real scientific theory will make all sorts of predictions. [sent-15, score-0.302]
</p><p>10 The mapping of theory to prediction is a messy and necessary part of science. [sent-16, score-0.227]
</p><p>11 So a theory can be valid even if it is difficult to test, indeed part of the reason for testing a theory is often not to confirm or dispute the theory’s validity but to refine the theory. [sent-17, score-0.537]
</p><p>12 Variability is high, measurements are crude, comparisons are performed between subjects, and this is all with a background of small effects that vary in sign and magnitude. [sent-22, score-0.162]
</p><p>13 As a result, the studies provide essentially zero information about the theory. [sent-23, score-0.292]
</p><p>14 The reason that multiple comparisons come in is to explain how it is that researchers such as Bem, Griskevicius, etc. [sent-26, score-0.501]
</p><p>15 , manage to consistently find statistical significance (typically, many statistically significant comparisons in a single study) even though their noise level is so high. [sent-27, score-0.162]
</p><p>16 Multiple comparisons is the answer, and the point of our  garden of forking paths  paper is to explain how this problem can arise even for studies that are well motivated by substantive theory. [sent-28, score-0.704]
</p><p>17 Rather, my problem is that the study design is such that the data provide essentially no information about the science. [sent-31, score-0.323]
</p><p>18 I’d have no problem with the theory being presented as such; my problem is with the incorrect (in this case) claims that the data add anything to the story. [sent-32, score-0.395]
</p><p>19 Regarding incentives structure, I fear that the current  lack  of incentives to criticize serves to offer an incentive for researchers to do small noisy studies which then they can sometimes publish in places such as Psychological Science. [sent-33, score-0.816]
</p><p>20 I would love if the incentives were to change so that researchers would put more effort into careful measurement and design! [sent-34, score-0.282]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('griskevicius', 0.346), ('theory', 0.227), ('wegner', 0.21), ('incentives', 0.166), ('comparisons', 0.162), ('multiple', 0.146), ('evolutionary', 0.139), ('studies', 0.131), ('incentive', 0.123), ('researchers', 0.116), ('criticize', 0.114), ('hypotheses', 0.11), ('moral', 0.105), ('motivated', 0.096), ('atheoretical', 0.096), ('safest', 0.096), ('reproductive', 0.096), ('bumblers', 0.096), ('structure', 0.095), ('psychological', 0.094), ('perks', 0.09), ('offender', 0.086), ('hoc', 0.086), ('publicized', 0.086), ('finding', 0.085), ('essentially', 0.084), ('problem', 0.084), ('superiority', 0.083), ('overboard', 0.083), ('pointers', 0.083), ('lacking', 0.083), ('refine', 0.083), ('social', 0.082), ('forking', 0.079), ('design', 0.078), ('provide', 0.077), ('altering', 0.077), ('critic', 0.077), ('respectable', 0.077), ('explain', 0.077), ('enhance', 0.075), ('danger', 0.075), ('hazard', 0.075), ('philosophers', 0.075), ('garden', 0.075), ('fraudulent', 0.074), ('psychology', 0.074), ('characterizing', 0.073), ('lakatos', 0.072), ('considerations', 0.072)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="2326-tfidf-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-08-Discussion_with_Steven_Pinker_on_research_that_is_attached_to_data_that_are_so_noisy_as_to_be_essentially_uninformative.html">2326 andrew gelman stats-2014-05-08-Discussion with Steven Pinker on research that is attached to data that are so noisy as to be essentially uninformative</a></p>
<p>Introduction: I pointed Steven Pinker to my post,  How much time (if any) should we spend criticizing research that’s fraudulent, crappy, or just plain pointless? , and he responded:
  
Clearly it *is* important to call out publicized research whose conclusions are likely to be false. The only danger is that it’s so easy and fun to criticize, with all the perks of intellectual and moral superiority for so little cost, that there is a moral hazard to go overboard and become a professional slasher and snarker. (That’s a common phenomenon among literary critics, especially in the UK.) There’s also the risk of altering the incentive structure for innovative research, so that researchers stick to the safest kinds of paradigm-twiddling. I think these two considerations were what my late colleague Dan Wegner had in mind when he made the bumbler-pointer contrast — he himself was certainly a discerning critic of social science research. [Just to clarify:  Wegner is the person who talked about bumblers and po</p><p>2 0.18000813 <a title="2326-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-20-Correcting_for_multiple_comparisons_in_a_Bayesian_regression_model.html">1989 andrew gelman stats-2013-08-20-Correcting for multiple comparisons in a Bayesian regression model</a></p>
<p>Introduction: Joe Northrup writes:
  
I have a question about correcting for multiple comparisons in a Bayesian regression model. I believe I understand the argument in  your 2012 paper  in Journal of Research on Educational Effectiveness that when you have a hierarchical model there is shrinkage of estimates towards the group-level mean and thus there is no need to add any additional penalty to correct for multiple comparisons. In my case I do not have hierarchically structured dataâ&euro;&rdquo;i.e. I have only 1 observation per group but have a categorical variable with a large number of categories. Thus, I am fitting a simple multiple regression in a Bayesian framework. Would putting a strong, mean 0, multivariate normal prior on the betas in this model accomplish the same sort of shrinkage (it seems to me that it would) and do you believe this is a valid way to address criticism of multiple comparisons in this setting?
  
My reply:  Yes, I think this makes sense.  One way to address concerns of multiple com</p><p>3 0.16746955 <a title="2326-tfidf-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-06-How_much_time_%28if_any%29_should_we_spend_criticizing_research_that%E2%80%99s_fraudulent%2C_crappy%2C_or_just_plain_pointless%3F.html">2235 andrew gelman stats-2014-03-06-How much time (if any) should we spend criticizing research that’s fraudulent, crappy, or just plain pointless?</a></p>
<p>Introduction: I had a brief email exchange with Jeff Leek regarding our recent  discussions  of replication, criticism, and the self-correcting process of science.
 
Jeff writes:
  
(1) I can see the problem with serious, evidence-based criticisms not being published in the same journal (and linked to) studies that are shown to be incorrect. I have been mostly seeing these sorts of things show up in blogs. But I’m not sure that is a bad thing. I think people read blogs more than they read the literature. I wonder if this means that blogs will eventually be a sort of “shadow literature”? 


(2) I think there is a ton of bad literature out there, just like there is a ton of bad stuff on Google. If we focus too much on the bad stuff we will be paralyzed. I still manage to find good papers despite all the bad papers. 


(3) I think one positive solution to this problem is to incentivize/publish referee reports and give people credit for a good referee report just like they get credit for a good paper. T</p><p>4 0.16139875 <a title="2326-tfidf-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-24-Empirical_implications_of_Empirical_Implications_of_Theoretical_Models.html">2263 andrew gelman stats-2014-03-24-Empirical implications of Empirical Implications of Theoretical Models</a></p>
<p>Introduction: Robert Bloomfield writes:
  
Most of the people in my field (accounting, which is basically applied economics and finance, leavened with psychology and organizational behavior) use ‘positive research methods’, which are typically described as coming to the data with a predefined theory, and using hypothesis testing to accept or reject the theory’s predictions.  But a substantial minority use ‘interpretive research methods’ (sometimes called qualitative methods, for those that call positive research ‘quantitative’).  No one seems entirely happy with the definition of this method, but I’ve found it useful to think of it as an attempt to see the world through the eyes of your subjects, much as Jane Goodall lived with gorillas and tried to see the world through their eyes.)


Interpretive researchers often criticize positive researchers by noting that the latter don’t make the best use of their data, because they come to the data with a predetermined theory, and only test a narrow set of h</p><p>5 0.15786378 <a title="2326-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-31-How_to_fix_the_tabloids%3F__Toward_replicable_social_science_research.html">1878 andrew gelman stats-2013-05-31-How to fix the tabloids?  Toward replicable social science research</a></p>
<p>Introduction: This seems to be the topic of the week.  Yesterday I posted on the sister blog  some further thoughts  on those “Psychological Science” papers on menstrual cycles, biceps size, and political attitudes, tied to a horrible press release from the journal Psychological Science hyping the biceps and politics study.
 
Then I was pointed to these  suggestions  from Richard Lucas and M. Brent Donnellan have on improving the replicability and reproducibility of research published in the Journal of Research in Personality:
  
It goes without saying that editors of scientific journals strive to publish research that is not only theoretically interesting but also methodologically rigorous. The goal is to select papers that advance the field. Accordingly, editors want to publish findings that can be reproduced and replicated by other scientists. Unfortunately, there has been a recent “crisis in confidence” among psychologists about the quality of psychological research (Pashler & Wagenmakers, 2012)</p><p>6 0.1542061 <a title="2326-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-17-How_can_statisticians_help_psychologists_do_their_research_better%3F.html">1860 andrew gelman stats-2013-05-17-How can statisticians help psychologists do their research better?</a></p>
<p>7 0.13933481 <a title="2326-tfidf-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>8 0.13147791 <a title="2326-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>9 0.13018493 <a title="2326-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-30-Strings_Attached%3A_Untangling_the_Ethics_of_Incentives.html">1093 andrew gelman stats-2011-12-30-Strings Attached: Untangling the Ethics of Incentives</a></p>
<p>10 0.12711963 <a title="2326-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-31-Response_by_Jessica_Tracy_and_Alec_Beall_to_my_critique_of_the_methods_in_their_paper%2C_%E2%80%9CWomen_Are_More_Likely_to_Wear_Red_or_Pink_at_Peak_Fertility%E2%80%9D.html">1963 andrew gelman stats-2013-07-31-Response by Jessica Tracy and Alec Beall to my critique of the methods in their paper, “Women Are More Likely to Wear Red or Pink at Peak Fertility”</a></p>
<p>11 0.12375753 <a title="2326-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-26-Suggested_resolution_of_the_Bem_paradox.html">1139 andrew gelman stats-2012-01-26-Suggested resolution of the Bem paradox</a></p>
<p>12 0.12275236 <a title="2326-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-04-Discussion_with_Dan_Kahan_on_political_polarization%2C_partisan_information_processing.__And%2C_more_generally%2C_the_role_of_theory_in_empirical_social_science.html">2050 andrew gelman stats-2013-10-04-Discussion with Dan Kahan on political polarization, partisan information processing.  And, more generally, the role of theory in empirical social science</a></p>
<p>13 0.11656516 <a title="2326-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-17-I_got_99_comparisons_but_multiplicity_ain%E2%80%99t_one.html">1016 andrew gelman stats-2011-11-17-I got 99 comparisons but multiplicity ain’t one</a></p>
<p>14 0.11617925 <a title="2326-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-28-Difficulties_of_using_statistical_significance_%28or_lack_thereof%29_to_sift_through_and_compare_research_hypotheses.html">2042 andrew gelman stats-2013-09-28-Difficulties of using statistical significance (or lack thereof) to sift through and compare research hypotheses</a></p>
<p>15 0.11244804 <a title="2326-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-03-%E2%80%9CThe_Case_for_Inductive_Theory_Building%E2%80%9D.html">1652 andrew gelman stats-2013-01-03-“The Case for Inductive Theory Building”</a></p>
<p>16 0.11018693 <a title="2326-tfidf-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-01-Why_Development_Economics_Needs_Theory%3F.html">309 andrew gelman stats-2010-10-01-Why Development Economics Needs Theory?</a></p>
<p>17 0.10571238 <a title="2326-tfidf-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-18-One-tailed_or_two-tailed%3F.html">2295 andrew gelman stats-2014-04-18-One-tailed or two-tailed?</a></p>
<p>18 0.10452095 <a title="2326-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-30-Retracted_articles_and_unethical_behavior_in_economics_journals%3F.html">1435 andrew gelman stats-2012-07-30-Retracted articles and unethical behavior in economics journals?</a></p>
<p>19 0.10442236 <a title="2326-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-19-Data_exploration_and_multiple_comparisons.html">524 andrew gelman stats-2011-01-19-Data exploration and multiple comparisons</a></p>
<p>20 0.10398081 <a title="2326-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-08-The_never-ending_%28and_often_productive%29_race_between_theory_and_practice.html">2127 andrew gelman stats-2013-12-08-The never-ending (and often productive) race between theory and practice</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.211), (1, -0.014), (2, -0.016), (3, -0.156), (4, -0.063), (5, -0.062), (6, -0.037), (7, 0.002), (8, -0.037), (9, 0.014), (10, -0.058), (11, 0.041), (12, 0.016), (13, -0.06), (14, -0.003), (15, 0.018), (16, -0.015), (17, -0.046), (18, 0.01), (19, -0.029), (20, -0.006), (21, -0.041), (22, -0.045), (23, 0.021), (24, -0.075), (25, -0.03), (26, 0.109), (27, -0.006), (28, 0.036), (29, -0.046), (30, -0.021), (31, -0.021), (32, 0.044), (33, -0.019), (34, -0.007), (35, -0.005), (36, 0.026), (37, 0.007), (38, -0.02), (39, -0.038), (40, -0.021), (41, 0.036), (42, 0.021), (43, -0.052), (44, -0.007), (45, -0.052), (46, -0.003), (47, 0.016), (48, 0.013), (49, -0.07)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98617142 <a title="2326-lsi-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-08-Discussion_with_Steven_Pinker_on_research_that_is_attached_to_data_that_are_so_noisy_as_to_be_essentially_uninformative.html">2326 andrew gelman stats-2014-05-08-Discussion with Steven Pinker on research that is attached to data that are so noisy as to be essentially uninformative</a></p>
<p>Introduction: I pointed Steven Pinker to my post,  How much time (if any) should we spend criticizing research that’s fraudulent, crappy, or just plain pointless? , and he responded:
  
Clearly it *is* important to call out publicized research whose conclusions are likely to be false. The only danger is that it’s so easy and fun to criticize, with all the perks of intellectual and moral superiority for so little cost, that there is a moral hazard to go overboard and become a professional slasher and snarker. (That’s a common phenomenon among literary critics, especially in the UK.) There’s also the risk of altering the incentive structure for innovative research, so that researchers stick to the safest kinds of paradigm-twiddling. I think these two considerations were what my late colleague Dan Wegner had in mind when he made the bumbler-pointer contrast — he himself was certainly a discerning critic of social science research. [Just to clarify:  Wegner is the person who talked about bumblers and po</p><p>2 0.79469663 <a title="2326-lsi-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-23-Discussion_on_preregistration_of_research_studies.html">2183 andrew gelman stats-2014-01-23-Discussion on preregistration of research studies</a></p>
<p>Introduction: Chris Chambers and I had an enlightening discussion the other day at the blog of Rolf Zwaan, regarding the Garden of Forking Paths ( go here  and scroll down through the comments).
 
Chris sent me the following note:
  
I’m writing a book at the moment about reforming practices in psychological research (focusing on various bad practices such as p-hacking, HARKing, low statistical power, publication bias, lack of data sharing etc. – and posing solutions such as pre-registration, Bayesian hypothesis testing, mandatory data archiving etc.) and I am arriving at rather unsettling conclusion: that null hypothesis significance testing (NHST) simply isn’t valid for observational research. If this is true then most of the psychological literature is statistically flawed.


I was wonder what your thoughts were on this, both from a statistical point of view and from your experience working in an observational field. 


We all know about the dangers of researcher degrees of freedom. We also know</p><p>3 0.78373587 <a title="2326-lsi-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-10-Preregistration%3A_what%E2%80%99s_in_it_for_you%3F.html">2241 andrew gelman stats-2014-03-10-Preregistration: what’s in it for you?</a></p>
<p>Introduction: Chris Chambers pointed me to a blog by someone called Neuroskeptic who  suggested  that I preregister my political science studies:
  
So when Andrew Gelman (let’s say) is going to start using a new approach, he goes on Twitter, or on his blog, and posts a bare-bones summary of what he’s going to do. Then he does it. If he finds something interesting, he writes it up as a paper, citing that tweet or post as his preregistration. . . .
  
I think this approach has some benefits but doesn’t really address the issues of preregistration that concern me—but I’d like to spend an entire blog post explaining why.  I have two key points:
 
1.  If your study is crap, preregistration might fix it.  Preregistration is fine—indeed, the wide acceptance of preregistration might well motivate researchers to not do so many crap studies—but it doesn’t solve fundamental problems of experimental design.
 
2.  “Preregistration” seems to mean different things in different scenarios:
 
A.  When the concern is</p><p>4 0.7752474 <a title="2326-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-31-Response_by_Jessica_Tracy_and_Alec_Beall_to_my_critique_of_the_methods_in_their_paper%2C_%E2%80%9CWomen_Are_More_Likely_to_Wear_Red_or_Pink_at_Peak_Fertility%E2%80%9D.html">1963 andrew gelman stats-2013-07-31-Response by Jessica Tracy and Alec Beall to my critique of the methods in their paper, “Women Are More Likely to Wear Red or Pink at Peak Fertility”</a></p>
<p>Introduction: Last week I published in Slate  a critique  of a paper that appeared in the journal Psychological Science.  That paper, by Alec Beall and Jessica Tracy, found that women who were at peak fertility were three times more likely to wear red or pink shirts, compared to women at other points in their menstrual cycles.  The study was based an 100 participants on the internet and 24 college students.  In my critique, I argued that we had no reason to believe the results generalized to the larger population, because (1) the samples were not representative, (2) the measurements were noisy, (3) the researchers did not use the correct dates of peak fertility, and (4) there were many different comparisons that could have been reported in the data, so there was nothing special about a particular comparison being statistically significant.  I likened their paper to other work which I considered flawed for multiple comparisons (too many researcher degrees of freedom), including a claimed relation bet</p><p>5 0.77237117 <a title="2326-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-07-I%E2%80%99m_negative_on_the_expression_%E2%80%9Cfalse_positives%E2%80%9D.html">2093 andrew gelman stats-2013-11-07-I’m negative on the expression “false positives”</a></p>
<p>Introduction: After seeing a document sent to me and others regarding the  crisis  of spurious, statistically-significant research findings in psychology research, I had the following reaction:
  
I am unhappy with the use in the document of the phrase “false positives.”  I feel that this expression is unhelpful as it frames science in terms of “true” and “false” claims, which I don’t think is particularly accurate.  In particular, in most of the recent disputed Psych Science type studies (the ESP study excepted, perhaps), there is little doubt that there is _some_ underlying effect.  The issue, as I see it, as that the underlying effects are much smaller, and much more variable, than mainstream researchers imagine.  So what happens is that Psych Science or Nature or whatever will publish a result that is purported to be some sort of universal truth, but it is actually a pattern specific to one data set, one population, and one experimental condition.  In a sense, yes, these journals are publishing</p><p>6 0.77094197 <a title="2326-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-26-%E2%80%9CA_Vast_Graveyard_of_Undead_Theories%3A__Publication_Bias_and_Psychological_Science%E2%80%99s_Aversion_to_the_Null%E2%80%9D.html">1826 andrew gelman stats-2013-04-26-“A Vast Graveyard of Undead Theories:  Publication Bias and Psychological Science’s Aversion to the Null”</a></p>
<p>7 0.7624172 <a title="2326-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-16-%E2%80%9CFalse-positive_psychology%E2%80%9D.html">1171 andrew gelman stats-2012-02-16-“False-positive psychology”</a></p>
<p>8 0.75086761 <a title="2326-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-04-Discussion_with_Dan_Kahan_on_political_polarization%2C_partisan_information_processing.__And%2C_more_generally%2C_the_role_of_theory_in_empirical_social_science.html">2050 andrew gelman stats-2013-10-04-Discussion with Dan Kahan on political polarization, partisan information processing.  And, more generally, the role of theory in empirical social science</a></p>
<p>9 0.74818343 <a title="2326-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>10 0.74016047 <a title="2326-lsi-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-31-Jessica_Tracy_and_Alec_Beall_%28authors_of_the_fertile-women-wear-pink_study%29_comment_on_our_Garden_of_Forking_Paths_paper%2C_and_I_comment_on_their_comments.html">2355 andrew gelman stats-2014-05-31-Jessica Tracy and Alec Beall (authors of the fertile-women-wear-pink study) comment on our Garden of Forking Paths paper, and I comment on their comments</a></p>
<p>11 0.73990482 <a title="2326-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-08-Statistical_significance_and_the_dangerous_lure_of_certainty.html">1974 andrew gelman stats-2013-08-08-Statistical significance and the dangerous lure of certainty</a></p>
<p>12 0.73143047 <a title="2326-lsi-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-06-Hurricanes_vs._Himmicanes.html">2361 andrew gelman stats-2014-06-06-Hurricanes vs. Himmicanes</a></p>
<p>13 0.73120719 <a title="2326-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-31-How_to_fix_the_tabloids%3F__Toward_replicable_social_science_research.html">1878 andrew gelman stats-2013-05-31-How to fix the tabloids?  Toward replicable social science research</a></p>
<p>14 0.72997868 <a title="2326-lsi-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21.html">256 andrew gelman stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</a></p>
<p>15 0.72720808 <a title="2326-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-28-Difficulties_of_using_statistical_significance_%28or_lack_thereof%29_to_sift_through_and_compare_research_hypotheses.html">2042 andrew gelman stats-2013-09-28-Difficulties of using statistical significance (or lack thereof) to sift through and compare research hypotheses</a></p>
<p>16 0.72010851 <a title="2326-lsi-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>17 0.71860677 <a title="2326-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-17-Where_do_theories_come_from%3F.html">1861 andrew gelman stats-2013-05-17-Where do theories come from?</a></p>
<p>18 0.71753281 <a title="2326-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-Fourteen_magic_words%3A_an_update.html">898 andrew gelman stats-2011-09-10-Fourteen magic words: an update</a></p>
<p>19 0.71169281 <a title="2326-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-17-How_can_statisticians_help_psychologists_do_their_research_better%3F.html">1860 andrew gelman stats-2013-05-17-How can statisticians help psychologists do their research better?</a></p>
<p>20 0.70936447 <a title="2326-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-12-Meta-analysis%2C_game_theory%2C_and_incentives_to_do_replicable_research.html">1163 andrew gelman stats-2012-02-12-Meta-analysis, game theory, and incentives to do replicable research</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(5, 0.016), (15, 0.05), (16, 0.076), (20, 0.017), (21, 0.083), (24, 0.102), (30, 0.022), (43, 0.048), (47, 0.033), (49, 0.011), (63, 0.043), (73, 0.016), (77, 0.029), (79, 0.01), (90, 0.013), (93, 0.012), (95, 0.024), (99, 0.3)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97933531 <a title="2326-lda-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-08-Discussion_with_Steven_Pinker_on_research_that_is_attached_to_data_that_are_so_noisy_as_to_be_essentially_uninformative.html">2326 andrew gelman stats-2014-05-08-Discussion with Steven Pinker on research that is attached to data that are so noisy as to be essentially uninformative</a></p>
<p>Introduction: I pointed Steven Pinker to my post,  How much time (if any) should we spend criticizing research that’s fraudulent, crappy, or just plain pointless? , and he responded:
  
Clearly it *is* important to call out publicized research whose conclusions are likely to be false. The only danger is that it’s so easy and fun to criticize, with all the perks of intellectual and moral superiority for so little cost, that there is a moral hazard to go overboard and become a professional slasher and snarker. (That’s a common phenomenon among literary critics, especially in the UK.) There’s also the risk of altering the incentive structure for innovative research, so that researchers stick to the safest kinds of paradigm-twiddling. I think these two considerations were what my late colleague Dan Wegner had in mind when he made the bumbler-pointer contrast — he himself was certainly a discerning critic of social science research. [Just to clarify:  Wegner is the person who talked about bumblers and po</p><p>2 0.96065825 <a title="2326-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-03-Popper_and_Jaynes.html">2007 andrew gelman stats-2013-09-03-Popper and Jaynes</a></p>
<p>Introduction: Deborah Mayo quotes me as saying, “Popper has argued (convincingly, in my opinion) that scientific inference is not inductive but deductive.”  She then  follows up  with:
  
Gelman employs significance test-type reasoning to reject a model when the data sufficiently disagree.


Now, strictly speaking, a model falsification, even to inferring something as weak as “the model breaks down,” is not purely deductive, but Gelman is right to see it as about as close as one can get, in statistics, to a deductive falsification of a model. But where does that leave him as a Jaynesian?
  
My reply:
 
I was influenced by reading a toy example from Jaynes’s book where he sets up a model (for the probability of a die landing on each of its six sides) based on first principles, then presents some data that contradict the model, then expands the model.
 
I’d seen very little of this sort of this reasoning before in statistics!  In physics it’s the standard way to go:  you set up a model based on physic</p><p>3 0.95922488 <a title="2326-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-25-Postdoc_Position_%232%3A__Hierarchical_Modeling_and_Statistical_Graphics.html">538 andrew gelman stats-2011-01-25-Postdoc Position #2:  Hierarchical Modeling and Statistical Graphics</a></p>
<p>Introduction: Andrew Gelman (Columbia University) and Eric Johnson (Columbia University) seek to hire a post-doctoral fellow to work on the application of the latest methods of multilevel data analysis, visualization and regression modeling to an important commercial problem:  forecasting retail sales at the individual item level.  These forecasts are used to make ordering, pricing and promotions decisions which can have significant economic impact to the retail chain such that even modest improvements in the accuracy of predictions, across a large retailerâ&euro;&trade;s product line, can yield substantial margin improvements.
 
Activities focus on the development of iterative imputation algorithms and diagnostics for missing-data imputation.   Activities would include model-development, programming, and data analysis.  This project is to be undertaken with, and largely funded by, a firm which provides forecasting technology and services to large retail chains, and which will provide access to a unique and rich</p><p>4 0.95871502 <a title="2326-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-13-Jim_Campbell_argues_that_Larry_Bartels%E2%80%99s_%E2%80%9CUnequal_Democracy%E2%80%9D_findings_are_not_robust.html">659 andrew gelman stats-2011-04-13-Jim Campbell argues that Larry Bartels’s “Unequal Democracy” findings are not robust</a></p>
<p>Introduction: A few years ago Larry Bartels  presented  this graph, a version of which latter appeared in his book Unequal Democracy:
 
 
 
Larry looked at the data in a number of ways, and the evidence seemed convincing that, at least in the short term, the Democrats were better than Republicans for the economy. This is consistent with Democrats’ general policies of lowering unemployment, as compared to Republicans lowering inflation, and, by comparing first-term to second-term presidents, he found that the result couldn’t simply be explained as a rebound or alternation pattern.
 
The question then arose, why have the Republicans won so many elections? Why aren’t the Democrats consistently dominating? Non-economic issues are part of the story, of course, but lots of evidence shows the economy to be a key concern for voters, so it’s still hard to see how, with a pattern such as shown above, the Republicans could keep winning.
 
Larry had some explanations, largely having to do with timing:  under De</p><p>5 0.95671904 <a title="2326-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-25-Fascinating_graphs_from_facebook_data.html">1824 andrew gelman stats-2013-04-25-Fascinating graphs from facebook data</a></p>
<p>Introduction: Yair points us to  this  page full of wonderful graphs from the Stephen Wolfram blog.  Here are a few:
 
 
 
 
 
 
 
And some words:
  
People talk less about video games as they get older, and more about politics and the weather. Men typically talk more about sports and technology than women—and, somewhat surprisingly to me, they also talk more about movies, television and music. Women talk more about pets+animals, family+friends, relationships—and, at least after they reach child-bearing years, health. . . . Some of this is rather depressingly stereotypical. And most of it isn’t terribly surprising to anyone who’s known a reasonable diversity of people of different ages. But what to me is remarkable is how we can see everything laid out in such quantitative detail in the pictures above—kind of a signature of people’s thinking as they go through life. 


Of course, the pictures above are all based on aggregate data, carefully anonymized. But if we start looking at individuals, we’ll s</p><p>6 0.95648372 <a title="2326-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-03-The_statistical_properties_of_smart_chains_%28and_referral_chains_more_generally%29.html">1882 andrew gelman stats-2013-06-03-The statistical properties of smart chains (and referral chains more generally)</a></p>
<p>7 0.95626795 <a title="2326-lda-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-20-Do_differences_between_biology_and_statistics_explain_some_of_our_diverging_attitudes_regarding_criticism_and_replication_of_scientific_claims%3F.html">2218 andrew gelman stats-2014-02-20-Do differences between biology and statistics explain some of our diverging attitudes regarding criticism and replication of scientific claims?</a></p>
<p>8 0.9561013 <a title="2326-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-25-Postdoc_Position_%231%3A__Missing-Data_Imputation%2C_Diagnostics%2C_and_Applications.html">537 andrew gelman stats-2011-01-25-Postdoc Position #1:  Missing-Data Imputation, Diagnostics, and Applications</a></p>
<p>9 0.95351994 <a title="2326-lda-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-27-A_whole_fleet_of_gremlins%3A__Looking_more_carefully_at_Richard_Tol%E2%80%99s_twice-corrected_paper%2C_%E2%80%9CThe_Economic_Effects_of_Climate_Change%E2%80%9D.html">2350 andrew gelman stats-2014-05-27-A whole fleet of gremlins:  Looking more carefully at Richard Tol’s twice-corrected paper, “The Economic Effects of Climate Change”</a></p>
<p>10 0.9534722 <a title="2326-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-24-Bayesian_quality_control%3F.html">1912 andrew gelman stats-2013-06-24-Bayesian quality control?</a></p>
<p>11 0.95317739 <a title="2326-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-04-Discussion_with_Dan_Kahan_on_political_polarization%2C_partisan_information_processing.__And%2C_more_generally%2C_the_role_of_theory_in_empirical_social_science.html">2050 andrew gelman stats-2013-10-04-Discussion with Dan Kahan on political polarization, partisan information processing.  And, more generally, the role of theory in empirical social science</a></p>
<p>12 0.95310599 <a title="2326-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-08-%E2%80%9CIs_the_cyber_mob_a_threat_to_freedom%3F%E2%80%9D.html">75 andrew gelman stats-2010-06-08-“Is the cyber mob a threat to freedom?”</a></p>
<p>13 0.95253366 <a title="2326-lda-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-27-%E2%80%9CWhat_Can_we_Learn_from_the_Many_Labs_Replication_Project%3F%E2%80%9D.html">2227 andrew gelman stats-2014-02-27-“What Can we Learn from the Many Labs Replication Project?”</a></p>
<p>14 0.95219177 <a title="2326-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-20-Correcting_for_multiple_comparisons_in_a_Bayesian_regression_model.html">1989 andrew gelman stats-2013-08-20-Correcting for multiple comparisons in a Bayesian regression model</a></p>
<p>15 0.95199871 <a title="2326-lda-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-11-Bayes_in_the_research_conversation.html">2368 andrew gelman stats-2014-06-11-Bayes in the research conversation</a></p>
<p>16 0.95192873 <a title="2326-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-17-How_can_statisticians_help_psychologists_do_their_research_better%3F.html">1860 andrew gelman stats-2013-05-17-How can statisticians help psychologists do their research better?</a></p>
<p>17 0.95143926 <a title="2326-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-05-Wacky_interview_questions%3A__An_exploration_into_the_nature_of_evidence_on_the_internet.html">505 andrew gelman stats-2011-01-05-Wacky interview questions:  An exploration into the nature of evidence on the internet</a></p>
<p>18 0.95142746 <a title="2326-lda-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-20-Domain_specificity%3A__Does_being_really_really_smart_or_really_really_rich_qualify_you_to_make_economic_policy%3F.html">45 andrew gelman stats-2010-05-20-Domain specificity:  Does being really really smart or really really rich qualify you to make economic policy?</a></p>
<p>19 0.95037234 <a title="2326-lda-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>20 0.95010316 <a title="2326-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-07-Descriptive_statistics%2C_causal_inference%2C_and_story_time.html">789 andrew gelman stats-2011-07-07-Descriptive statistics, causal inference, and story time</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
