<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2258 andrew gelman stats-2014-03-21-Random matrices in the news</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2014" href="../home/andrew_gelman_stats-2014_home.html">andrew_gelman_stats-2014</a> <a title="andrew_gelman_stats-2014-2258" href="#">andrew_gelman_stats-2014-2258</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2258 andrew gelman stats-2014-03-21-Random matrices in the news</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2014-2258-html" href="http://andrewgelman.com/2014/03/21/random-matrices-news/">html</a></p><p>Introduction: From 2010 :
  
Mark Buchanan wrote  a cover article  for the New Scientist on random matrices, a heretofore obscure area of probability theory that his headline writer characterizes as “the deep law that shapes our reality.”


It’s interesting stuff, and he gets into some statistical applications at the end, so I’ll give you my take on it.


But first, some background.


About two hundred years ago, the mathematician/physicist Laplace discovered what is now called the central limit theorem, which is that, under certain conditions, the average of a large number of small random variables has an approximate normal (bell-shaped) distribution. A bit over 100 years ago, social scientists such as Galton applied this theorem to all sorts of biological and social phenomena. The central limit theorem, in its generality, is also important in the information that it indirectly conveys when it fails.


For example, the distribution of the heights of adult men or women is nicely bell-shaped, but the</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 From 2010 :    Mark Buchanan wrote  a cover article  for the New Scientist on random matrices, a heretofore obscure area of probability theory that his headline writer characterizes as “the deep law that shapes our reality. [sent-1, score-0.407]
</p><p>2 About two hundred years ago, the mathematician/physicist Laplace discovered what is now called the central limit theorem, which is that, under certain conditions, the average of a large number of small random variables has an approximate normal (bell-shaped) distribution. [sent-4, score-0.826]
</p><p>3 A bit over 100 years ago, social scientists such as Galton applied this theorem to all sorts of biological and social phenomena. [sent-5, score-0.472]
</p><p>4 For example, the distribution of the heights of adult men or women is nicely bell-shaped, but the distribution of the heights of all adults has a different, more spread-out distribution. [sent-7, score-0.731]
</p><p>5 The central limit theorem is an example of an attractor—a mathematical model that appears as a limit as sample size gets large. [sent-13, score-0.754]
</p><p>6 (Or, for other models, such as that used to describe the distribution of incomes, the attractor might be a power-law distribution. [sent-16, score-0.559]
</p><p>7 ) The beauty of an attractor is that, if you believe the model, it can be used to explain an observed pattern without needing to know the details of its components. [sent-17, score-0.416]
</p><p>8 A random matrix is an array of numbers, where each number is drawn from some specified probability distribution. [sent-20, score-0.683]
</p><p>9 You can compute the eigenvalues of a square matrix—that’s a set of numbers summarizing the structure of the matrix—and they will have a probability distribution that is induced by the probability distribution of the individual elements of the matrix. [sent-21, score-0.829]
</p><p>10 Over the past few decades, mathematicians such as Alan Edelman have performed computer simulations and proved theorems deriving the distribution of the eigenvalues of a random matrix, as the dimension of the matrix becomes large. [sent-22, score-1.16]
</p><p>11 That is, for a broad range of different input models (distributions of the random matrices), you get the same output—the same eigenvalue distribution—as the sample size becomes large. [sent-24, score-0.69]
</p><p>12 If the eigenvalue distribution is an attractor, this means that a lot of physical and social phenomena which can be modeled by eigenvalues (including, apparently, quantum energy levels and some properties of statistical tests) might have a common structure. [sent-28, score-0.917]
</p><p>13 Nevertheless, Kuemmeth’s team found that random matrix theory described the measured levels very accurately. [sent-32, score-0.749]
</p><p>14 Thus, I don’t quite understand this quote:    Random matrix theory has got mathematicians like Percy Deift of New York University imagining that there might be more general patterns there too. [sent-34, score-0.549]
</p><p>15 While random matrix theory suggests that this is a promising approach, it also points to hidden dangers. [sent-43, score-0.649]
</p><p>16 As more and more complex data is collected, the number of variables being studied grows, and the number of apparent correlations between them grows even faster. [sent-44, score-0.512]
</p><p>17 The new idea is that mathematical theory might enable the distribution of these correlations to be understood for a general range of cases. [sent-51, score-0.623]
</p><p>18 We are in fact studying the properties of hierarchical models when the number of cases and variables becomes large, and it’s a hard problem. [sent-55, score-0.419]
</p><p>19 Maybe the ideas from random matrix theory will be relevant here too. [sent-56, score-0.649]
</p><p>20 That might be an illusion, and random matrix theory could be the tool to separate what is real and what is not. [sent-59, score-0.649]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('attractor', 0.343), ('matrix', 0.308), ('random', 0.217), ('distribution', 0.216), ('buchanan', 0.208), ('eigenvalues', 0.188), ('theorem', 0.177), ('eigenvalue', 0.171), ('limit', 0.149), ('correlations', 0.139), ('theory', 0.124), ('heights', 0.123), ('mathematicians', 0.117), ('kuemmeth', 0.114), ('becomes', 0.114), ('levels', 0.1), ('variables', 0.095), ('grows', 0.094), ('central', 0.093), ('number', 0.092), ('social', 0.089), ('energy', 0.087), ('begun', 0.082), ('structure', 0.077), ('incomes', 0.074), ('range', 0.073), ('observed', 0.073), ('matrices', 0.072), ('factors', 0.071), ('mathematical', 0.071), ('probability', 0.066), ('properties', 0.066), ('size', 0.063), ('curve', 0.063), ('large', 0.062), ('small', 0.062), ('sorts', 0.061), ('measurements', 0.06), ('years', 0.056), ('men', 0.053), ('similar', 0.052), ('models', 0.052), ('edelman', 0.052), ('futures', 0.052), ('funnel', 0.052), ('electrons', 0.052), ('discernible', 0.052), ('sift', 0.052), ('fluctuating', 0.052), ('example', 0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="2258-tfidf-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-21-Random_matrices_in_the_news.html">2258 andrew gelman stats-2014-03-21-Random matrices in the news</a></p>
<p>Introduction: From 2010 :
  
Mark Buchanan wrote  a cover article  for the New Scientist on random matrices, a heretofore obscure area of probability theory that his headline writer characterizes as “the deep law that shapes our reality.”


It’s interesting stuff, and he gets into some statistical applications at the end, so I’ll give you my take on it.


But first, some background.


About two hundred years ago, the mathematician/physicist Laplace discovered what is now called the central limit theorem, which is that, under certain conditions, the average of a large number of small random variables has an approximate normal (bell-shaped) distribution. A bit over 100 years ago, social scientists such as Galton applied this theorem to all sorts of biological and social phenomena. The central limit theorem, in its generality, is also important in the information that it indirectly conveys when it fails.


For example, the distribution of the heights of adult men or women is nicely bell-shaped, but the</p><p>2 0.20107111 <a title="2258-tfidf-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-19-Index_or_indicator_variables.html">2296 andrew gelman stats-2014-04-19-Index or indicator variables</a></p>
<p>Introduction: Someone who doesn’t want his name shared (for the perhaps reasonable reason that he’ll “one day not be confused, and would rather my confusion not live on online forever”) writes:
  
I’m exploring HLMs and stan, using your book with Jennifer Hill as my field guide to this new territory. I think I have a generally clear grasp on the material, but wanted to be sure I haven’t gone astray. 


The problem in working on involves a multi-nation survey of students, and I’m especially interested in understanding the effects of country, religion, and sex, and the interactions among those factors (using IRT to estimate individual-level ability, then estimating individual, school, and country effects).


Following the basic approach laid out in chapter 13 for such interactions between levels, I think I need to create a matrix of indicator variables for religion and sex. Elsewhere in the book, you recommend against indicator variables in favor of a single index variable. 


Am I right in thinking t</p><p>3 0.1947953 <a title="2258-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-Another_reason_why_you_can_get_good_inferences_from_a_bad_model.html">1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</a></p>
<p>Introduction: John Cook  considers  how people justify probability distribution assumptions:
  
Sometimes distribution assumptions are not justified.


Sometimes distributions can be derived from fundamental principles [or] . . . on theoretical grounds. For example, large samples and the central limit theorem together may justify assuming that something is normally distributed.


Often the choice of distribution is somewhat arbitrary, chosen by intuition or for convenience, and then empirically shown to work well enough.


Sometimes a distribution can be a bad fit and still work well, depending on what you’re asking of it.
  
Cook continues:
  
The last point is particularly interesting. It’s not hard to imagine that a poor fit would produce poor results. It’s surprising when a poor fit produces good results.
  
And then he gives an example of an effective but inaccurate model used to model survival times in a clinical trial.  Cook explains:
  
The [poorly-fitting] method works well because of the q</p><p>4 0.15746218 <a title="2258-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-06-Stan_1.2.0_and_RStan_1.2.0.html">1753 andrew gelman stats-2013-03-06-Stan 1.2.0 and RStan 1.2.0</a></p>
<p>Introduction: Stan 1.2.0 and RStan 1.2.0 are now available for download. See:
  
  http://mc-stan.org/ 
   
Here are the highlights.
  Full Mass Matrix Estimation during Warmup  
Yuanjun Gao, a first-year grad student here at Columbia (!), built a regularized mass-matrix estimator.   This helps for posteriors with high correlation among parameters and varying scales.  We’re still testing this ourselves, so the estimation procedure may change in the future (don’t worry — it satisfies detailed balance as is, but we might be able to make it more computationally efficient in terms of time per effective sample).
 
It’s not the default option.  The major reason is the matrix operations required are expensive, raising the algorithm cost to    , where   is the average number of leapfrog steps,   is the number of iterations, and   is the number of parameters.
 
Yuanjun did a great job with the Cholesky factorizations and implemented this about as efficiently as is possible. (His homework for Andrew’s class w</p><p>5 0.14989704 <a title="2258-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-21-The_future_%28and_past%29_of_statistical_sciences.html">2072 andrew gelman stats-2013-10-21-The future (and past) of statistical sciences</a></p>
<p>Introduction: In connection with  this  workshop, I was asked to write a few paragraphs describing my perspective on “the current and near-term future state of the statistical sciences you are most familiar with.”  Here’s what I wrote:
  
I think that, at any given time, the field of statistics has a core, but that core changes over time.  There are different paradigmatic ways to solve problems.


100 or 150 years ago, the thing to do was to identify a phenomenon of interest with some probability distribution and then use the mathematics of that distribution to gain insight into the underlying process.  Thus, for example, if certain data looked like they came from a normal distribution, one could surmise that the values in question arose by adding many small independent pieces.  If the data looked like they came from a binomial distribution, that would imply independence and equal probabilities.  Waiting times that followed an exponential distribution could be considered as coming from a memoryless</p><p>6 0.14101654 <a title="2258-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>7 0.12831931 <a title="2258-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-17-Hierarchical-multilevel_modeling_with_%E2%80%9Cbig_data%E2%80%9D.html">1267 andrew gelman stats-2012-04-17-Hierarchical-multilevel modeling with “big data”</a></p>
<p>8 0.12712623 <a title="2258-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-21-BDA3_table_of_contents_%28also_a_new_paper_on_visualization%29.html">1991 andrew gelman stats-2013-08-21-BDA3 table of contents (also a new paper on visualization)</a></p>
<p>9 0.12637554 <a title="2258-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-09-How_to_model_distributions_that_have_outliers_in_one_direction.html">2128 andrew gelman stats-2013-12-09-How to model distributions that have outliers in one direction</a></p>
<p>10 0.12522167 <a title="2258-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-Visualizing_Distributions_of_Covariance_Matrices.html">1477 andrew gelman stats-2012-08-30-Visualizing Distributions of Covariance Matrices</a></p>
<p>11 0.12113508 <a title="2258-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>12 0.11822122 <a title="2258-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<p>13 0.11767215 <a title="2258-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-25-Bayes-respecting_experimental_design_and_other_things.html">1955 andrew gelman stats-2013-07-25-Bayes-respecting experimental design and other things</a></p>
<p>14 0.11664469 <a title="2258-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>15 0.11647694 <a title="2258-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-12-year_%2B_%281%7Cyear%29.html">851 andrew gelman stats-2011-08-12-year + (1|year)</a></p>
<p>16 0.11640628 <a title="2258-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-17-Statistics_in_a_world_where_nothing_is_random.html">1628 andrew gelman stats-2012-12-17-Statistics in a world where nothing is random</a></p>
<p>17 0.11553009 <a title="2258-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<p>18 0.11502188 <a title="2258-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-04-Handy_Matrix_Cheat_Sheet%2C_with_Gradients.html">555 andrew gelman stats-2011-02-04-Handy Matrix Cheat Sheet, with Gradients</a></p>
<p>19 0.11330538 <a title="2258-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>20 0.11122803 <a title="2258-tfidf-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-Transformations_for_non-normal_data.html">2176 andrew gelman stats-2014-01-19-Transformations for non-normal data</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.246), (1, 0.095), (2, 0.05), (3, -0.004), (4, 0.034), (5, 0.022), (6, 0.01), (7, -0.01), (8, -0.007), (9, 0.036), (10, -0.056), (11, -0.015), (12, -0.05), (13, -0.025), (14, -0.031), (15, 0.016), (16, 0.024), (17, 0.014), (18, 0.018), (19, -0.061), (20, 0.017), (21, -0.042), (22, -0.025), (23, 0.025), (24, 0.043), (25, 0.041), (26, -0.024), (27, 0.103), (28, 0.064), (29, -0.002), (30, -0.006), (31, 0.058), (32, -0.008), (33, -0.008), (34, 0.034), (35, -0.044), (36, -0.028), (37, 0.07), (38, -0.064), (39, -0.016), (40, -0.016), (41, -0.036), (42, 0.06), (43, -0.033), (44, -0.018), (45, -0.029), (46, -0.006), (47, 0.002), (48, 0.054), (49, -0.09)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96790671 <a title="2258-lsi-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-21-Random_matrices_in_the_news.html">2258 andrew gelman stats-2014-03-21-Random matrices in the news</a></p>
<p>Introduction: From 2010 :
  
Mark Buchanan wrote  a cover article  for the New Scientist on random matrices, a heretofore obscure area of probability theory that his headline writer characterizes as “the deep law that shapes our reality.”


It’s interesting stuff, and he gets into some statistical applications at the end, so I’ll give you my take on it.


But first, some background.


About two hundred years ago, the mathematician/physicist Laplace discovered what is now called the central limit theorem, which is that, under certain conditions, the average of a large number of small random variables has an approximate normal (bell-shaped) distribution. A bit over 100 years ago, social scientists such as Galton applied this theorem to all sorts of biological and social phenomena. The central limit theorem, in its generality, is also important in the information that it indirectly conveys when it fails.


For example, the distribution of the heights of adult men or women is nicely bell-shaped, but the</p><p>2 0.79979527 <a title="2258-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-24-Bleg%3A_Automatic_Differentiation_for_Log_Prob_Gradients%3F.html">535 andrew gelman stats-2011-01-24-Bleg: Automatic Differentiation for Log Prob Gradients?</a></p>
<p>Introduction: We need help picking out an automatic differentiation package for Hamiltonian Monte Carlo sampling from the posterior of a  generalized linear model with deep interactions.  Specifically, we need to compute gradients for log probability functions with thousands of parameters that involve matrix (determinants, eigenvalues, inverses), stats (distributions), and math (log gamma) functions.  Any suggestions?
    
 The Application: Hybrid Monte Carlo for Posteriors 
 
We’re getting serious about implementing posterior sampling using Hamiltonian Monte Carlo.  HMC speeds up mixing by including gradient information to help guide the Metropolis proposals toward areas  high probability.  In practice, the algorithm requires a handful or  of gradient calculations per sample, but there are many dimensions and the functions are hairy enough we don’t want to compute derivaties by hand.
 

 Auto Diff: Perhaps not What you Think 
 
It may not have been clear to readers of this blog that automatic diffe</p><p>3 0.7581026 <a title="2258-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-04-Handy_Matrix_Cheat_Sheet%2C_with_Gradients.html">555 andrew gelman stats-2011-02-04-Handy Matrix Cheat Sheet, with Gradients</a></p>
<p>Introduction: This post is an (unpaid) advertisement for the following extremely useful resource:
  
 Petersen, K. B. and M. S. Pedersen. 2008.   The Matrix Cookbook  .  Tehcnical Report, Technical University of Denmark. 
  
It contains 70+ pages of useful relations and derivations involving matrices.  What grabbed my eye was the computation of gradients for matrix operations ranging from eigenvalues and determinants to multivariate normal density functions.   I had no idea the multivariate normal had such a clean gradient (see section 8).
  

 
We’ve been playing around with  Hamiltonian (aka Hybrid) Monte Carlo  for sampling from the posterior of hierarchical generalized linear models with lots of interactions.  HMC speeds up Metropolis sampling by using the gradient of the log probability to drive samples in the direction of higher probability density, which is particularly useful for correlated parameters that mix slowly with standard Gibbs sampling.   Matt “III” Hoffman ‘s already got it workin</p><p>4 0.74693978 <a title="2258-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-21-The_future_%28and_past%29_of_statistical_sciences.html">2072 andrew gelman stats-2013-10-21-The future (and past) of statistical sciences</a></p>
<p>Introduction: In connection with  this  workshop, I was asked to write a few paragraphs describing my perspective on “the current and near-term future state of the statistical sciences you are most familiar with.”  Here’s what I wrote:
  
I think that, at any given time, the field of statistics has a core, but that core changes over time.  There are different paradigmatic ways to solve problems.


100 or 150 years ago, the thing to do was to identify a phenomenon of interest with some probability distribution and then use the mathematics of that distribution to gain insight into the underlying process.  Thus, for example, if certain data looked like they came from a normal distribution, one could surmise that the values in question arose by adding many small independent pieces.  If the data looked like they came from a binomial distribution, that would imply independence and equal probabilities.  Waiting times that followed an exponential distribution could be considered as coming from a memoryless</p><p>5 0.74442756 <a title="2258-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<p>Introduction: From August 1990.  It was in the form of a note sent to all the people in the statistics group of Bell Labs, where I’d worked that summer.
  
To all:


Here’s the abstract of the work I’ve done this summer.  It’s stored in the file, 
/fs5/gelman/abstract.bell, and copies of the Figures 1-3 are on Trevor’s desk. 
Any comments are of course appreciated; I’m at gelman@stat.berkeley.edu.


On the Routine Use of Markov Chains for Simulation


Andrew Gelman and Donald Rubin, 6 August 1990


corrected version:  8 August 1990
  
  
  
1.  Simulation


In probability and statistics we can often specify multivariate distributions 
many of whose properties we do not fully understand–perhaps, as in the 
Ising model of statistical physics, we can write the joint density function, up 
to a multiplicative constant that cannot be expressed in closed form. 
For an example in statistics, consider the Normal random 
effects model in the analysis of variance, which can be 
easily placed in a Bayesian fram</p><p>6 0.71973395 <a title="2258-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-12-Controversy_about_average_personality_differences_between_men_and_women.html">1114 andrew gelman stats-2012-01-12-Controversy about average personality differences between men and women</a></p>
<p>7 0.71781147 <a title="2258-lsi-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-12-%E2%80%9CThe_results_%28not_shown%29_._._.%E2%80%9D.html">2332 andrew gelman stats-2014-05-12-“The results (not shown) . . .”</a></p>
<p>8 0.69355845 <a title="2258-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-21-Handbook_of_Markov_Chain_Monte_Carlo.html">674 andrew gelman stats-2011-04-21-Handbook of Markov Chain Monte Carlo</a></p>
<p>9 0.68929774 <a title="2258-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<p>10 0.6875518 <a title="2258-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>11 0.67858171 <a title="2258-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-15-More_on_AIC%2C_WAIC%2C_etc.html">1983 andrew gelman stats-2013-08-15-More on AIC, WAIC, etc</a></p>
<p>12 0.67493188 <a title="2258-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-03-Boot.html">1881 andrew gelman stats-2013-06-03-Boot</a></p>
<p>13 0.67095399 <a title="2258-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-24-New_ideas_on_DIC_from_Martyn_Plummer_and_Sumio_Watanabe.html">778 andrew gelman stats-2011-06-24-New ideas on DIC from Martyn Plummer and Sumio Watanabe</a></p>
<p>14 0.66812682 <a title="2258-lsi-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-10-R_vs._Stata%2C_or%2C_Different_ways_to_estimate_multilevel_models.html">269 andrew gelman stats-2010-09-10-R vs. Stata, or, Different ways to estimate multilevel models</a></p>
<p>15 0.66479337 <a title="2258-lsi-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-07-Chi-square_FAIL_when_many_cells_have_small_expected_values.html">996 andrew gelman stats-2011-11-07-Chi-square FAIL when many cells have small expected values</a></p>
<p>16 0.65989077 <a title="2258-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-21-Going_viral_%E2%80%94_not%21.html">864 andrew gelman stats-2011-08-21-Going viral — not!</a></p>
<p>17 0.65558904 <a title="2258-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-13-Philosophy_of_Bayesian_statistics%3A__my_reactions_to_Wasserman.html">1165 andrew gelman stats-2012-02-13-Philosophy of Bayesian statistics:  my reactions to Wasserman</a></p>
<p>18 0.65540057 <a title="2258-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-04-A_new_R_package_for_fititng_multilevel_models.html">501 andrew gelman stats-2011-01-04-A new R package for fititng multilevel models</a></p>
<p>19 0.6544953 <a title="2258-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-29-Going_negative.html">1918 andrew gelman stats-2013-06-29-Going negative</a></p>
<p>20 0.65417707 <a title="2258-lsi-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Bayesian_Uncertainty_Quantification_for_Differential_Equations%21.html">2311 andrew gelman stats-2014-04-29-Bayesian Uncertainty Quantification for Differential Equations!</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(5, 0.015), (12, 0.015), (15, 0.016), (16, 0.047), (17, 0.014), (21, 0.04), (24, 0.117), (41, 0.025), (45, 0.013), (60, 0.016), (63, 0.011), (76, 0.037), (77, 0.033), (86, 0.038), (89, 0.039), (94, 0.012), (99, 0.393)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99200869 <a title="2258-lda-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-21-Random_matrices_in_the_news.html">2258 andrew gelman stats-2014-03-21-Random matrices in the news</a></p>
<p>Introduction: From 2010 :
  
Mark Buchanan wrote  a cover article  for the New Scientist on random matrices, a heretofore obscure area of probability theory that his headline writer characterizes as “the deep law that shapes our reality.”


It’s interesting stuff, and he gets into some statistical applications at the end, so I’ll give you my take on it.


But first, some background.


About two hundred years ago, the mathematician/physicist Laplace discovered what is now called the central limit theorem, which is that, under certain conditions, the average of a large number of small random variables has an approximate normal (bell-shaped) distribution. A bit over 100 years ago, social scientists such as Galton applied this theorem to all sorts of biological and social phenomena. The central limit theorem, in its generality, is also important in the information that it indirectly conveys when it fails.


For example, the distribution of the heights of adult men or women is nicely bell-shaped, but the</p><p>2 0.98404014 <a title="2258-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-29-More_consulting_experiences%2C_this_time_in_computational_linguistics.html">1596 andrew gelman stats-2012-11-29-More consulting experiences, this time in computational linguistics</a></p>
<p>Introduction: Bob wrote  this  long comment that I think is worth posting:
  
I [Bob] have done a fair bit of consulting for my small natural language processing  company  over the past ten years. Like statistics, natural language processing is something may companies think they want, but have no idea how to do themselves.


We almost always handed out “free” consulting. Usually on the phone to people who called us out of the blue. Our blog and tutorials Google ranking was pretty much our only approach to marketing other than occassionally going to business-oriented conferences.


Our goal was to sell software licenses (because consulting doesn’t scale nor does it provide continuing royalty income), but since so few people knew how to use toolkits like ours, we had to help them along the way. We even provided “free” consulting with our startup license package.


We were brutally honest with customers, both about our goals and their goals. Their goals were often incompatible with ours (use company X’</p><p>3 0.98367506 <a title="2258-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-22-That_claim_that_students_whose_parents_pay_for_more_of_college_get_worse_grades.html">1688 andrew gelman stats-2013-01-22-That claim that students whose parents pay for more of college get worse grades</a></p>
<p>Introduction: Theodore Vasiloudis writes:
  
I came upon  this article  by Laura Hamilton, an assistant professor in the University of California at Merced, that claims that “The more money that parents provide for higher education, the lower the grades their children earn.”


I can’t help but feel that there something wrong with the basis of the study or a confounding factor causing this apparent correlation, and since you often comment on studies on your blog I thought you might find this study interesting.
  
My reply:  I have to admit that the description above made me suspicious of the study before I even looked at it.  On first thought, I’d expect the effect of parent’s financial contributions to be positive (as they free the student from the need to get a job during college), but not negative.  Hamilton argues that “parental investments create a disincentive for student achievement,” which may be—but I’m generally suspicious of arguments in which the rebound is bigger than the main effect.</p><p>4 0.9836697 <a title="2258-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-29-We_go_to_war_with_the_data_we_have%2C_not_the_data_we_want.html">1289 andrew gelman stats-2012-04-29-We go to war with the data we have, not the data we want</a></p>
<p>Introduction: This post is by Phil.
 
Psychologists perform experiments on Canadian undergraduate psychology students and draws conclusions that (they believe) apply to humans in general; they publish in Science. A drug company decides to embark on additional trials that will cost tens of millions of dollars based on the results of a careful double-blind study….whose patients are all volunteers from two hospitals. A movie studio holds 9 screenings of a new movie for volunteer viewers and, based on their survey responses, decides to spend another $8 million to re-shoot the ending.  A researcher interested in the effect of ventilation on worker performance conducts a months-long study in which ventilation levels are varied and worker performance is monitored…in a single building.
 
In almost all fields of research, most studies are based on convenience samples, or on random samples from a larger population that is itself a convenience sample. The paragraph above gives just a few examples.  The benefit</p><p>5 0.9821502 <a title="2258-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-15-Induction_within_a_model%2C_deductive_inference_for_model_evaluation.html">614 andrew gelman stats-2011-03-15-Induction within a model, deductive inference for model evaluation</a></p>
<p>Introduction: Jonathan Livengood writes:
  
I have a couple of questions on your paper with Cosma Shalizi on “Philosophy and the practice of Bayesian statistics.”


First, you distinguish between inductive approaches and hypothetico-deductive approaches to inference and locate statistical practice (at least, the practice of model building and checking) on the hypothetico-deductive side.  Do you think that there are any interesting elements of statistical practice that are properly inductive?  For example, suppose someone is playing around with a system that more or less resembles a toy model, like drawing balls from an urn or some such, and where the person has some well-defined priors.  The person makes a number of draws from the urn and applies Bayes theorem to get a posterior.  On your view, is that person making an induction?  If so, how much space is there in statistical practice for genuine inductions like this?


Second, I agree with you that one ought to distinguish induction from other kind</p><p>6 0.98194444 <a title="2258-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-26-Lottery_probability_update.html">731 andrew gelman stats-2011-05-26-Lottery probability update</a></p>
<p>7 0.98179269 <a title="2258-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-10-Controversy_over_the_Christakis-Fowler_findings_on_the_contagion_of_obesity.html">757 andrew gelman stats-2011-06-10-Controversy over the Christakis-Fowler findings on the contagion of obesity</a></p>
<p>8 0.98158133 <a title="2258-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-21-Bayes_related.html">1948 andrew gelman stats-2013-07-21-Bayes related</a></p>
<p>9 0.98136073 <a title="2258-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<p>10 0.98130286 <a title="2258-lda-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-07-How_literature_is_like_statistical_reasoning%3A__Kosara_on_stories.__Gelman_and_Basb%C3%B8ll_on_stories..html">2284 andrew gelman stats-2014-04-07-How literature is like statistical reasoning:  Kosara on stories.  Gelman and Basbøll on stories.</a></p>
<p>11 0.98109066 <a title="2258-lda-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-05-Bizarre_academic_spam.html">2282 andrew gelman stats-2014-04-05-Bizarre academic spam</a></p>
<p>12 0.98106897 <a title="2258-lda-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-13-Judea_Pearl_overview_on_causal_inference%2C_and_more_general_thoughts_on_the_reexpression_of_existing_methods_by_considering_their_implicit_assumptions.html">2170 andrew gelman stats-2014-01-13-Judea Pearl overview on causal inference, and more general thoughts on the reexpression of existing methods by considering their implicit assumptions</a></p>
<p>13 0.98106325 <a title="2258-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-11-Why_waste_time_philosophizing%3F.html">1719 andrew gelman stats-2013-02-11-Why waste time philosophizing?</a></p>
<p>14 0.98088408 <a title="2258-lda-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-21-Everything_I_need_to_know_about_Bayesian_statistics%2C_I_learned_in_eight_schools..html">2180 andrew gelman stats-2014-01-21-Everything I need to know about Bayesian statistics, I learned in eight schools.</a></p>
<p>15 0.98080897 <a title="2258-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-17-Data_problems%2C_coding_errors%E2%80%A6what_can_be_done%3F.html">1807 andrew gelman stats-2013-04-17-Data problems, coding errors…what can be done?</a></p>
<p>16 0.98072082 <a title="2258-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>17 0.98019969 <a title="2258-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-01-%24241%2C364.83_%E2%80%93_%2413%2C000_%3D_%24228%2C364.83.html">1600 andrew gelman stats-2012-12-01-$241,364.83 – $13,000 = $228,364.83</a></p>
<p>18 0.9801935 <a title="2258-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-27-What_is_%E2%80%9Cexplanation%E2%80%9D%3F.html">1742 andrew gelman stats-2013-02-27-What is “explanation”?</a></p>
<p>19 0.97976977 <a title="2258-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-28-Using_randomized_incentives_as_an_instrument_for_survey_nonresponse%3F.html">2152 andrew gelman stats-2013-12-28-Using randomized incentives as an instrument for survey nonresponse?</a></p>
<p>20 0.97975397 <a title="2258-lda-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-01-Martin_and_Liu%3A__Probabilistic_inference_based_on_consistency_of_model_with_data.html">1095 andrew gelman stats-2012-01-01-Martin and Liu:  Probabilistic inference based on consistency of model with data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
