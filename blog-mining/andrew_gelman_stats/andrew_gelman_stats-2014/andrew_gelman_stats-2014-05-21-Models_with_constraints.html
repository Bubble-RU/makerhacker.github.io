<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2342 andrew gelman stats-2014-05-21-Models with constraints</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2014" href="../home/andrew_gelman_stats-2014_home.html">andrew_gelman_stats-2014</a> <a title="andrew_gelman_stats-2014-2342" href="#">andrew_gelman_stats-2014-2342</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2342 andrew gelman stats-2014-05-21-Models with constraints</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2014-2342-html" href="http://andrewgelman.com/2014/05/21/models-constraints/">html</a></p><p>Introduction: I had an interesting conversation with Aki about monotonicity constraints.  We were discussing a particular set of Gaussian processes that we were fitting to the arsenic well-switching data (the example from the logistic regression chapter in my book with Jennifer) but some more general issues arose that I thought might interest you.
 
The idea was to fit a model where the response (the logit probability of switching wells) was constrained to be monotonically increasing in your current arsenic level and monotonically decreasing in your current distance to the closest safe well.  These constraints seem reasonable enough, but when we actually fit the model we found that doing Bayesian inference with the constraint pulled the estimate, not just toward monotonicity, but to a strong increase (for the increasing relation) or a strong decrease (for the decreasing relation).  This makes sense from a statistical standpoint because if you restrict a parameter to be nonnegative, any posterior dis</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I had an interesting conversation with Aki about monotonicity constraints. [sent-1, score-0.299]
</p><p>2 We were discussing a particular set of Gaussian processes that we were fitting to the arsenic well-switching data (the example from the logistic regression chapter in my book with Jennifer) but some more general issues arose that I thought might interest you. [sent-2, score-0.67]
</p><p>3 The idea was to fit a model where the response (the logit probability of switching wells) was constrained to be monotonically increasing in your current arsenic level and monotonically decreasing in your current distance to the closest safe well. [sent-3, score-2.132]
</p><p>4 These constraints seem reasonable enough, but when we actually fit the model we found that doing Bayesian inference with the constraint pulled the estimate, not just toward monotonicity, but to a strong increase (for the increasing relation) or a strong decrease (for the decreasing relation). [sent-4, score-1.304]
</p><p>5 This makes sense from a statistical standpoint because if you restrict a parameter to be nonnegative, any posterior distribution will end up on the positive half of the line. [sent-5, score-0.139]
</p><p>6 Thinking about it more, I’m not always comfortable with any strict constraint unless there is a clear physical reason. [sent-7, score-0.499]
</p><p>7 For example, yes it seems logical that increasing arsenic would increase the probability of switching but I could imagine that in any particular dataset there could be areas of negative slope. [sent-8, score-1.273]
</p><p>8 After all, it is observational data and for example there could be a village that happens to have arsenic in a particular high range but where for cultural reasons there would be less switching. [sent-9, score-0.983]
</p><p>9 Here there’s an omitted variable (“culture” or a village indicator) but the point is that these (hypothetical) data would not really support a strictly monotonic model, and including that restriction could distort things in other ways. [sent-10, score-0.887]
</p><p>10 It does not mean we should ignore prior information, of course, but it’s a reason that I prefer soft rather than hard constraints. [sent-12, score-0.218]
</p><p>11 Alternatively in this example one could put a hard constraint on the monotonicity and then add a latent omitted variable which would have the effect of turning it into a soft constraint, but I don’t usually see people do this. [sent-13, score-1.424]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('arsenic', 0.442), ('constraint', 0.342), ('monotonicity', 0.299), ('monotonically', 0.221), ('omitted', 0.182), ('switching', 0.178), ('increasing', 0.175), ('village', 0.174), ('decreasing', 0.168), ('soft', 0.154), ('relation', 0.123), ('monotonic', 0.104), ('wells', 0.104), ('increase', 0.095), ('restriction', 0.093), ('variable', 0.09), ('distort', 0.089), ('strict', 0.087), ('particular', 0.087), ('alternatively', 0.085), ('strong', 0.084), ('pulled', 0.083), ('current', 0.082), ('could', 0.08), ('closest', 0.08), ('aki', 0.077), ('indicator', 0.076), ('constrained', 0.076), ('strictly', 0.075), ('example', 0.073), ('turning', 0.073), ('fit', 0.071), ('decrease', 0.071), ('standpoint', 0.07), ('probability', 0.07), ('comfortable', 0.07), ('logit', 0.07), ('restrict', 0.069), ('arose', 0.068), ('gaussian', 0.067), ('latent', 0.067), ('constraints', 0.066), ('logical', 0.066), ('distance', 0.066), ('safe', 0.065), ('model', 0.065), ('hypothetical', 0.065), ('cultural', 0.064), ('hard', 0.064), ('observational', 0.063)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="2342-tfidf-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-21-Models_with_constraints.html">2342 andrew gelman stats-2014-05-21-Models with constraints</a></p>
<p>Introduction: I had an interesting conversation with Aki about monotonicity constraints.  We were discussing a particular set of Gaussian processes that we were fitting to the arsenic well-switching data (the example from the logistic regression chapter in my book with Jennifer) but some more general issues arose that I thought might interest you.
 
The idea was to fit a model where the response (the logit probability of switching wells) was constrained to be monotonically increasing in your current arsenic level and monotonically decreasing in your current distance to the closest safe well.  These constraints seem reasonable enough, but when we actually fit the model we found that doing Bayesian inference with the constraint pulled the estimate, not just toward monotonicity, but to a strong increase (for the increasing relation) or a strong decrease (for the decreasing relation).  This makes sense from a statistical standpoint because if you restrict a parameter to be nonnegative, any posterior dis</p><p>2 0.13449201 <a title="2342-tfidf-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-25-Modeling_constrained_parameters.html">234 andrew gelman stats-2010-08-25-Modeling constrained parameters</a></p>
<p>Introduction: Mike McLaughlin writes:
  
In general, is there any way to do MCMC with a fixed constraint?


E.g., suppose I measure the three internal angles of a triangle with errors ~dnorm(0, tau) where tau might be different for the three measurements.  This would be an easy BUGS/WinBUGS/JAGS exercise but suppose, in addition, I wanted to include prior information to the effect that the three angles had to total 180 degrees exactly.


Is this feasible? Could you point me to any BUGS model in which a constraint of this type is implemented?


Note: Even in my own (non-hierarchical) code which tends to be component-wise, random-walk Metropolis with tuned Laplacian proposals, I cannot see how I could incorporate such a constraint.
  
My reply:  See page 508 of Bayesian Data Analysis (2nd edition).  We have an example of such a model there (from  this paper  with Bois and Jiang).</p><p>3 0.12035774 <a title="2342-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>Introduction: Following up on Christian’s  post  [link fixed] on the topic, I’d like to offer a few thoughts of my own.
 
In BDA, we express the idea that a noninformative prior is a placeholder:  you can use the noninformative prior to get the analysis started, then if your posterior distribution is less informative than you would like, or if it does not make sense, you can go back and add prior information.
 
Same thing for the data model (the “likelihood”), for that matter:  it often makes sense to start with something simple and conventional and then go from there.
 
So, in that sense, noninformative priors are no big deal, they’re just a way to get started.  Just don’t take them too seriously.
 
Traditionally in statistics we’ve worked with the paradigm of a single highly informative dataset with only weak external information.  But if the data are sparse and prior information is strong, we have to think differently.  And, when you increase the dimensionality of a problem, both these things hap</p><p>4 0.10898779 <a title="2342-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-18-Lack_of_complete_overlap.html">1017 andrew gelman stats-2011-11-18-Lack of complete overlap</a></p>
<p>Introduction: Evens Salies writes:
  
I have a question regarding a randomizing constraint in my current funded electricity experiment. 


After elimination of missing data we have 110 voluntary households from a larger population (resource constraints do not allow us to have more households!). I randomly assign them to threated and non treated where the treatment variable is some ICT that allows the treated to track their electricity consumption in real tim. The ICT is made of two devices, one that is plugged on the household’s modem and the other on the electric meter. A necessary condition for being treated is that the distance between the box and the meter be below some threshold (d), the value of which is 20 meters approximately. 


50 ICTs can be installed. 
60 households will be in the control group.


But, I can only assign 6 households in the control group for whom d is less than 20. Therefore, I have only 6 households in the control group who have a counterfactual in the group of treated.</p><p>5 0.10876469 <a title="2342-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-29-Putting_together_multinomial_discrete_regressions_by_combining_simple_logits.html">782 andrew gelman stats-2011-06-29-Putting together multinomial discrete regressions by combining simple logits</a></p>
<p>Introduction: When predicting 0/1 data we can use logit (or probit or robit or some other robust model such as invlogit (0.01 + 0.98*X*beta)).  Logit is simple enough and we can use  bayesglm  to regularize and avoid the problem of separation.
 
What if there are more than 2 categories?  If they’re ordered (1, 2, 3, etc), we can do ordered logit (and use bayespolr() to avoid separation).  If the categories are unordered (vanilla, chocolate, strawberry), there are unordered multinomial logit and probit models out there.
 
But it’s not so easy to fit these multinomial model in a multilevel setting (with coefficients that vary by group), especially if the computation is embedded in an iterative routine such as mi where you have real time constraints at each step.
 
So this got me wondering whether we could kluge it with logits.  Here’s the basic idea (in the ordered and unordered forms):
 
- If you have a variable that goes 1, 2, 3, etc., set up a series of logits:  1 vs. 2,3,…; 2 vs. 3,…; and so forth</p><p>6 0.10396129 <a title="2342-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>7 0.10222405 <a title="2342-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-21-The_future_%28and_past%29_of_statistical_sciences.html">2072 andrew gelman stats-2013-10-21-The future (and past) of statistical sciences</a></p>
<p>8 0.10193302 <a title="2342-tfidf-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>9 0.097513646 <a title="2342-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>10 0.096178904 <a title="2342-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>11 0.093851067 <a title="2342-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-27-Cross-validation_%28What_is_it_good_for%3F%29.html">1395 andrew gelman stats-2012-06-27-Cross-validation (What is it good for?)</a></p>
<p>12 0.091720991 <a title="2342-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-21-Model_complexity_as_a_function_of_sample_size.html">1543 andrew gelman stats-2012-10-21-Model complexity as a function of sample size</a></p>
<p>13 0.085266747 <a title="2342-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>14 0.084149815 <a title="2342-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>15 0.083592378 <a title="2342-tfidf-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-04-27-Should_Mister_P_be_allowed-encouraged_to_reside_in_counter-factual_populations%3F.html">7 andrew gelman stats-2010-04-27-Should Mister P be allowed-encouraged to reside in counter-factual populations?</a></p>
<p>16 0.083011553 <a title="2342-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>17 0.080166399 <a title="2342-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>18 0.07949286 <a title="2342-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-09-iPython_Notebook.html">1716 andrew gelman stats-2013-02-09-iPython Notebook</a></p>
<p>19 0.077412248 <a title="2342-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-24-F-f-f-fake_data.html">1735 andrew gelman stats-2013-02-24-F-f-f-fake data</a></p>
<p>20 0.076725177 <a title="2342-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-11-My_problem_with_the_Lindley_paradox.html">1757 andrew gelman stats-2013-03-11-My problem with the Lindley paradox</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.158), (1, 0.096), (2, 0.033), (3, 0.017), (4, 0.003), (5, -0.018), (6, 0.034), (7, 0.001), (8, 0.014), (9, 0.029), (10, -0.003), (11, 0.023), (12, -0.024), (13, -0.02), (14, 0.009), (15, 0.011), (16, 0.015), (17, 0.001), (18, 0.0), (19, -0.02), (20, 0.011), (21, 0.008), (22, -0.008), (23, -0.039), (24, -0.008), (25, 0.034), (26, 0.013), (27, -0.017), (28, 0.0), (29, -0.012), (30, -0.011), (31, 0.012), (32, -0.041), (33, 0.02), (34, -0.017), (35, -0.039), (36, -0.001), (37, -0.017), (38, -0.025), (39, 0.015), (40, 0.001), (41, -0.034), (42, -0.031), (43, -0.041), (44, 0.029), (45, 0.041), (46, 0.031), (47, 0.024), (48, -0.023), (49, 0.032)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9578737 <a title="2342-lsi-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-21-Models_with_constraints.html">2342 andrew gelman stats-2014-05-21-Models with constraints</a></p>
<p>Introduction: I had an interesting conversation with Aki about monotonicity constraints.  We were discussing a particular set of Gaussian processes that we were fitting to the arsenic well-switching data (the example from the logistic regression chapter in my book with Jennifer) but some more general issues arose that I thought might interest you.
 
The idea was to fit a model where the response (the logit probability of switching wells) was constrained to be monotonically increasing in your current arsenic level and monotonically decreasing in your current distance to the closest safe well.  These constraints seem reasonable enough, but when we actually fit the model we found that doing Bayesian inference with the constraint pulled the estimate, not just toward monotonicity, but to a strong increase (for the increasing relation) or a strong decrease (for the decreasing relation).  This makes sense from a statistical standpoint because if you restrict a parameter to be nonnegative, any posterior dis</p><p>2 0.79464626 <a title="2342-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-29-Putting_together_multinomial_discrete_regressions_by_combining_simple_logits.html">782 andrew gelman stats-2011-06-29-Putting together multinomial discrete regressions by combining simple logits</a></p>
<p>Introduction: When predicting 0/1 data we can use logit (or probit or robit or some other robust model such as invlogit (0.01 + 0.98*X*beta)).  Logit is simple enough and we can use  bayesglm  to regularize and avoid the problem of separation.
 
What if there are more than 2 categories?  If they’re ordered (1, 2, 3, etc), we can do ordered logit (and use bayespolr() to avoid separation).  If the categories are unordered (vanilla, chocolate, strawberry), there are unordered multinomial logit and probit models out there.
 
But it’s not so easy to fit these multinomial model in a multilevel setting (with coefficients that vary by group), especially if the computation is embedded in an iterative routine such as mi where you have real time constraints at each step.
 
So this got me wondering whether we could kluge it with logits.  Here’s the basic idea (in the ordered and unordered forms):
 
- If you have a variable that goes 1, 2, 3, etc., set up a series of logits:  1 vs. 2,3,…; 2 vs. 3,…; and so forth</p><p>3 0.79073489 <a title="2342-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-22-A_Bayesian_model_for_an_increasing_function%2C_in_Stan%21.html">2110 andrew gelman stats-2013-11-22-A Bayesian model for an increasing function, in Stan!</a></p>
<p>Introduction: Following up on yesterday’s post, here’s David Chudzicki’s  story  (with graphs and Stan/R code!) of how he fit a model for an increasing function (“isotonic regression”).  Chudzicki writes:
  
This post will describe a way I came up with of fitting a function that’s constrained to be increasing, using Stan. If you want practical help, standard statistical approaches, or expert research, this isn’t the place for you (look up “isotonic regression” or “Bayesian isotonic regression” or David Dunson). This is the place for you if you want to read about how I thought about setting up a model, implemented the model in Stan, and created graphics to understand what was going on.
  
The background is that a simple, natural-seeming uniform prior on the function values does not work so well—it’s a much stronger prior distribution than one might naively think, just one of those unexpected aspects of high-dimensional probability distributions.  So Chudzicki sets up a more general family with a hype</p><p>4 0.78315765 <a title="2342-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-26-Modeling_probability_data.html">1284 andrew gelman stats-2012-04-26-Modeling probability data</a></p>
<p>Introduction: Rafael Huber writes: 
  
  
I conducted an experiment in which subjects where asked to estimate the probability of a certain event given a number of information (like a wheater forecaster or a stockmarket trader). These probability estimates are the dependent variable of my experiment. My goal is to model the data with a (hierarchical) Bayesian regression. A linear equation with all the presented information (quantified as log odds) defines the mu of a normal likelihood. The tau as precision is another free parameter.


y[r] ~ dnorm( mu[r] , tau[ subj[r] ] ) 
mu[r] <- b0[ subj[r] ] + b1[ subj[r] ] * x1[r] + b2[ subj[r] ] * x2[r] + b3[ subj[r] ] * x3[r]


My problem is that I do not believe that the normal is the correct probability distribution to model probability data (â&euro;Ś because the error is limited). However, until now nobody was able to tell me how I can correctly model probability data.
  
My reply:  You can take the logit of the data before analyzing them.  That is assuming there</p><p>5 0.78300691 <a title="2342-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-19-Whassup_with_deviance_having_a_high_posterior_correlation_with_a_parameter_in_the_model%3F.html">1221 andrew gelman stats-2012-03-19-Whassup with deviance having a high posterior correlation with a parameter in the model?</a></p>
<p>Introduction: Jean Richardson writes: 
  
  
Do you know what might lead to a large negative cross-correlation (-0.95) between deviance and one of the model parameters?


Here’s the (brief) background:


I [Richardson] have written a Bayesian hierarchical site occupancy model for presence of disease on individual amphibians. The response variable is therefore binary (disease present/absent) and the probability of disease being present in an individual (psi) depends on various covariates (species of amphibian, location sampled, etc.) paramaterized using a logit link function.  Replicates are individuals sampled (tested for presence of disease) together.  The possibility of imperfect detection is included as p = (prob. disease detected given disease is present).


Posterior distributions were estimated using WinBUGS via R2WinBUGS. 
Simulated data from the model fit the real data very well and posterior distribution densities seem robust to any changes in the model (different priors, etc.)  All autocor</p><p>6 0.77284724 <a title="2342-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>7 0.76912087 <a title="2342-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-15-Wacky_priors_can_work_well%3F.html">1723 andrew gelman stats-2013-02-15-Wacky priors can work well?</a></p>
<p>8 0.76005435 <a title="2342-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>9 0.75829905 <a title="2342-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>10 0.75778574 <a title="2342-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-18-Lack_of_complete_overlap.html">1017 andrew gelman stats-2011-11-18-Lack of complete overlap</a></p>
<p>11 0.73953503 <a title="2342-lsi-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>12 0.73866183 <a title="2342-lsi-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>13 0.73549426 <a title="2342-lsi-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-25-Modeling_constrained_parameters.html">234 andrew gelman stats-2010-08-25-Modeling constrained parameters</a></p>
<p>14 0.73407745 <a title="2342-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-25-Bayes-respecting_experimental_design_and_other_things.html">1955 andrew gelman stats-2013-07-25-Bayes-respecting experimental design and other things</a></p>
<p>15 0.72950441 <a title="2342-lsi-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Interactions_of_predictors_in_a_causal_model.html">251 andrew gelman stats-2010-09-02-Interactions of predictors in a causal model</a></p>
<p>16 0.72719526 <a title="2342-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-07-Chi-square_FAIL_when_many_cells_have_small_expected_values.html">996 andrew gelman stats-2011-11-07-Chi-square FAIL when many cells have small expected values</a></p>
<p>17 0.72378898 <a title="2342-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-31-Using_sample_size_in_the_prior_distribution.html">547 andrew gelman stats-2011-01-31-Using sample size in the prior distribution</a></p>
<p>18 0.71431082 <a title="2342-lsi-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-08-Regression_and_causality_and_variable_ordering.html">2364 andrew gelman stats-2014-06-08-Regression and causality and variable ordering</a></p>
<p>19 0.71410769 <a title="2342-lsi-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Bayesian_Uncertainty_Quantification_for_Differential_Equations%21.html">2311 andrew gelman stats-2014-04-29-Bayesian Uncertainty Quantification for Differential Equations!</a></p>
<p>20 0.71230108 <a title="2342-lsi-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-28-Another_argument_in_favor_of_expressing_conditional_probability_statements_using_the_population_distribution.html">56 andrew gelman stats-2010-05-28-Another argument in favor of expressing conditional probability statements using the population distribution</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(5, 0.052), (6, 0.028), (13, 0.044), (15, 0.042), (16, 0.057), (24, 0.117), (42, 0.01), (51, 0.037), (53, 0.058), (56, 0.028), (57, 0.02), (59, 0.062), (67, 0.012), (73, 0.044), (86, 0.013), (87, 0.032), (99, 0.247)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95609879 <a title="2342-lda-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-21-Models_with_constraints.html">2342 andrew gelman stats-2014-05-21-Models with constraints</a></p>
<p>Introduction: I had an interesting conversation with Aki about monotonicity constraints.  We were discussing a particular set of Gaussian processes that we were fitting to the arsenic well-switching data (the example from the logistic regression chapter in my book with Jennifer) but some more general issues arose that I thought might interest you.
 
The idea was to fit a model where the response (the logit probability of switching wells) was constrained to be monotonically increasing in your current arsenic level and monotonically decreasing in your current distance to the closest safe well.  These constraints seem reasonable enough, but when we actually fit the model we found that doing Bayesian inference with the constraint pulled the estimate, not just toward monotonicity, but to a strong increase (for the increasing relation) or a strong decrease (for the decreasing relation).  This makes sense from a statistical standpoint because if you restrict a parameter to be nonnegative, any posterior dis</p><p>2 0.91519332 <a title="2342-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-23-Thinking_outside_the_%28graphical%29_box%3A__Instead_of_arguing_about_how_best_to_fix_a_bar_chart%2C_graph_it_as_a_time_series_lineplot_instead.html">294 andrew gelman stats-2010-09-23-Thinking outside the (graphical) box:  Instead of arguing about how best to fix a bar chart, graph it as a time series lineplot instead</a></p>
<p>Introduction: John Kastellec points me to  this blog  by Ezra Klein criticizing the following graph from a recent Republican Party report:
 
 
 
Klein (following  Alexander Hart ) slams the graph for not going all the way to zero on the y-axis, thus making the projected change seem bigger than it really is.
 
I agree with Klein and Hart that, if you’re gonna do a bar chart, you want the bars to go down to 0.  On the other hand, a projected change from 19% to 23% is actually pretty big, and I don’t see the point of using a graphical display that hides it.
 
 The solution:  Ditch the bar graph entirely and replace it by a lineplot , in particular, a time series with year-by-year data.  The time series would have several advantages:
 
1.  Data are placed in context.  You’d see every year, instead of discrete averages, and you’d get to see the changes in the context of year-to-year variation.
 
2.  With the time series, you can use whatever y-axis works with the data.  No need to go to zero.
 
P.S.  I l</p><p>3 0.91511589 <a title="2342-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-15-How_do_I_make_my_graphs%3F.html">1764 andrew gelman stats-2013-03-15-How do I make my graphs?</a></p>
<p>Introduction: Someone who wishes to remain anonymous writes: 
  
  
I’ve been following your blog a long time and enjoy your posts on visualization/statistical graphics matters.  I don’t recall however you ever describing the details of your setup for plotting.  I’m a new R user (convert from matplotlib) and would love to know your thoughts on the ideal setup: do you use mainly the R base?  Do you use lattice?  What do you think of ggplot2?  etc.  


I found ggplot2 nearly indecipherable until a recent eureka moment, and I think its default theme is a waste tremendous ink (all those silly grey backgrounds and grids are really unnecessary), but if you customize that away it can be made to look like ordinary, pretty statistical graphs.  


Feel free to respond on your blog, but if you do, please remove my name from the post (my colleagues already make fun of me for thinking about visualization too much.) 
  
I love that last bit!
 
Anyway, my response is that I do everything in base graphics (using my</p><p>4 0.91498852 <a title="2342-lda-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-08-What_we_need_here_is_some_peer_review_for_statistical_graphics.html">2013 andrew gelman stats-2013-09-08-What we need here is some peer review for statistical graphics</a></p>
<p>Introduction: Under the heading, “Bad graph candidate,” Kevin Wright points to  this article  [link fixed], writing:
  
Some of the figures use the same line type for two different series.


More egregious are the confidence intervals that are constant width instead of increasing in width into the future.
  
Indeed.  What’s even more embarrassing is that these graphs appeared in an article in the magazine Significance, sponsored by the American Statistical Association and the Royal Statistical Society.  
 
Perhaps every scientific journal could have a graphics editor whose job is to point out really horrible problems and require authors to make improvements.
 
The difficulty, as always, is that scientists write these articles for free and as a public service (publishing in Significance doesn’t pay, nor does it count as a publication in an academic record), so it might be difficult to get authors to fix their graphs.  On the other hand, if an article is worth writing at all, it’s worth trying to conv</p><p>5 0.91382593 <a title="2342-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-14-Bayes_in_China_update.html">517 andrew gelman stats-2011-01-14-Bayes in China update</a></p>
<p>Introduction: Some clarification on the Bayes-in-China issue raised  last week :
 
1.  We heard that the Chinese publisher cited the following pages that might contain politically objectionable materials:  3, 5, 21, 73, 112, 201.
 
2.  It appears that, as some commenters suggested, the objection was to some of the applications, not to the Bayesian methods.
 
3.  Our book is not censored in China.  In fact, as some commenters mentioned, it is possible to buy it there, and it is also available in university libraries there.  The edition of the book which was canceled was intended to be a low-cost reprint of the book.  The original book is still available.  I used the phrase “Banned in China” as a joke and I apologize if it was misinterpreted.
 
4.  I have no quarrel with the Chinese government or with any Chinese publishers.  They can publish whatever books they would like.  I found this episode amusing only because I do not think my book on regression and multilevel models has any strong political co</p><p>6 0.91174436 <a title="2342-lda-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-03-Is_0.05_too_strict_as_a_p-value_threshold%3F.html">446 andrew gelman stats-2010-12-03-Is 0.05 too strict as a p-value threshold?</a></p>
<p>7 0.91145486 <a title="2342-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-15-Coaching%2C_teaching%2C_and_writing.html">1380 andrew gelman stats-2012-06-15-Coaching, teaching, and writing</a></p>
<p>8 0.91036111 <a title="2342-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-29-Zero_is_zero.html">687 andrew gelman stats-2011-04-29-Zero is zero</a></p>
<p>9 0.90906215 <a title="2342-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-28-More_on_that_machine_learning_course.html">1960 andrew gelman stats-2013-07-28-More on that machine learning course</a></p>
<p>10 0.90829211 <a title="2342-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-10-Quotes_from_me%21.html">1453 andrew gelman stats-2012-08-10-Quotes from me!</a></p>
<p>11 0.90791243 <a title="2342-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-12-Sometimes_a_graph_really_is_just_ugly.html">798 andrew gelman stats-2011-07-12-Sometimes a graph really is just ugly</a></p>
<p>12 0.90749806 <a title="2342-lda-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-30-Seth_Roberts.html">2313 andrew gelman stats-2014-04-30-Seth Roberts</a></p>
<p>13 0.90699065 <a title="2342-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-18-Subtle_statistical_issues_to_be_debated_on_TV..html">350 andrew gelman stats-2010-10-18-Subtle statistical issues to be debated on TV.</a></p>
<p>14 0.9066655 <a title="2342-lda-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-01-Ratios_where_the_numerator_and_denominator_both_change_signs.html">248 andrew gelman stats-2010-09-01-Ratios where the numerator and denominator both change signs</a></p>
<p>15 0.9066022 <a title="2342-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-25-Is_there_too_much_coauthorship_in_economics_%28and_science_more_generally%29%3F__Or_too_little%3F.html">1914 andrew gelman stats-2013-06-25-Is there too much coauthorship in economics (and science more generally)?  Or too little?</a></p>
<p>16 0.90655696 <a title="2342-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-08-I_Am_Too_Absolutely_Heteroskedastic_for_This_Probit_Model.html">1047 andrew gelman stats-2011-12-08-I Am Too Absolutely Heteroskedastic for This Probit Model</a></p>
<p>17 0.90622211 <a title="2342-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-31-Social_scientists_who_use_medical_analogies_to_explain_causal_inference_are%2C_I_think%2C_implicitly_trying_to_borrow_some_of_the_scientific_and_cultural_authority_of_that_field_for_our_own_purposes.html">1555 andrew gelman stats-2012-10-31-Social scientists who use medical analogies to explain causal inference are, I think, implicitly trying to borrow some of the scientific and cultural authority of that field for our own purposes</a></p>
<p>18 0.90547663 <a title="2342-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-06-Yes%2C_checking_calibration_of_probability_forecasts_is_part_of_Bayesian_statistics.html">1610 andrew gelman stats-2012-12-06-Yes, checking calibration of probability forecasts is part of Bayesian statistics</a></p>
<p>19 0.90517128 <a title="2342-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-31-A_data_visualization_manifesto.html">61 andrew gelman stats-2010-05-31-A data visualization manifesto</a></p>
<p>20 0.90474993 <a title="2342-lda-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-04-Literal_vs._rhetorical.html">2233 andrew gelman stats-2014-03-04-Literal vs. rhetorical</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
