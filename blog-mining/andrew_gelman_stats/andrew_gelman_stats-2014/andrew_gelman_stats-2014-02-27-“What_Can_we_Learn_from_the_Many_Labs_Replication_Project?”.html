<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2227 andrew gelman stats-2014-02-27-“What Can we Learn from the Many Labs Replication Project?”</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2014" href="../home/andrew_gelman_stats-2014_home.html">andrew_gelman_stats-2014</a> <a title="andrew_gelman_stats-2014-2227" href="#">andrew_gelman_stats-2014-2227</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2227 andrew gelman stats-2014-02-27-“What Can we Learn from the Many Labs Replication Project?”</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2014-2227-html" href="http://andrewgelman.com/2014/02/27/can-learn-many-labs-replication-project/">html</a></p><p>Introduction: Aki points us to  this discussion  from Rolf Zwaan:
  
The first massive replication project in psychology has just reached completion (several others are to follow). . . . What can we learn from the ManyLabs project? The results here show the effect sizes for the replication efforts (in green and grey) as well as the original studies (in blue). The 99% confidence intervals are for the meta-analysis of the effect size (the green dots); the studies are ordered by effect size.


 


Let’s first consider what we canNOT learn from these data. Of the 13 replication attempts (when the first four are taken together), 11 succeeded and 2 did not (in fact, at some point ManyLabs suggests that a third one, Imagined Contact also doesn’t really replicate). We cannot learn from this that the vast majority of psychological findings will replicate . . .


But even if we had an accurate estimate of the percentage of findings that replicate, how useful would that be? Rather than trying to arrive at a mo</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Aki points us to  this discussion  from Rolf Zwaan:    The first massive replication project in psychology has just reached completion (several others are to follow). [sent-1, score-0.649]
</p><p>2 The results here show the effect sizes for the replication efforts (in green and grey) as well as the original studies (in blue). [sent-6, score-0.86]
</p><p>3 The 99% confidence intervals are for the meta-analysis of the effect size (the green dots); the studies are ordered by effect size. [sent-7, score-0.567]
</p><p>4 Let’s first consider what we canNOT learn from these data. [sent-8, score-0.334]
</p><p>5 Of the 13 replication attempts (when the first four are taken together), 11 succeeded and 2 did not (in fact, at some point ManyLabs suggests that a third one, Imagined Contact also doesn’t really replicate). [sent-9, score-0.454]
</p><p>6 We cannot learn from this that the vast majority of psychological findings will replicate . [sent-10, score-0.655]
</p><p>7 But even if we had an accurate estimate of the percentage of findings that replicate, how useful would that be? [sent-13, score-0.188]
</p><p>8 Rather than trying to arrive at a more precise estimate, it might be more informative to follow up the ManyLabs projects with projects that focus on a specific research area or topic . [sent-14, score-0.524]
</p><p>9 We learn that for some experiments, the replications actually yield much larger effects that the original studies, a highly intriguing findings that warrants further analysis. [sent-18, score-0.953]
</p><p>10 We also learn that the two social priming studies in the sample, dangling at the bottom of the list in the figure, were resoundingly nonreplicated. [sent-19, score-0.555]
</p><p>11 It is striking how far the effects sizes of the original studies (indicated by an x) are away from the rest of the experiments. [sent-23, score-0.527]
</p><p>12 Most importantly, we learn that several labs working together yield data that have an enormous evidentiary power. [sent-27, score-0.792]
</p><p>13 At the same time, it is clear that such large-scale replication projects will have diminishing returns . [sent-28, score-0.57]
</p><p>14 rather than using the ManyLabs approach retrospectively, we can also use it prospectively: to test novel hypotheses. [sent-31, score-0.064]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('manylabs', 0.571), ('learn', 0.274), ('replication', 0.237), ('replicate', 0.19), ('projects', 0.178), ('studies', 0.177), ('green', 0.134), ('project', 0.133), ('yield', 0.124), ('findings', 0.123), ('original', 0.113), ('sizes', 0.107), ('warrants', 0.104), ('grey', 0.104), ('evidentiary', 0.104), ('prospectively', 0.104), ('priming', 0.104), ('imagined', 0.094), ('tal', 0.094), ('yarkoni', 0.094), ('zwaan', 0.094), ('effect', 0.092), ('succeeded', 0.09), ('rolf', 0.09), ('retrospectively', 0.088), ('completion', 0.086), ('arrive', 0.086), ('diminishing', 0.086), ('together', 0.083), ('follow', 0.082), ('intriguing', 0.078), ('ordered', 0.072), ('aki', 0.072), ('enormous', 0.072), ('importantly', 0.072), ('replications', 0.072), ('indicated', 0.07), ('labs', 0.07), ('dots', 0.069), ('returns', 0.069), ('vast', 0.068), ('attempts', 0.067), ('massive', 0.067), ('reached', 0.066), ('several', 0.065), ('estimate', 0.065), ('striking', 0.065), ('effects', 0.065), ('novel', 0.064), ('first', 0.06)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="2227-tfidf-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-27-%E2%80%9CWhat_Can_we_Learn_from_the_Many_Labs_Replication_Project%3F%E2%80%9D.html">2227 andrew gelman stats-2014-02-27-“What Can we Learn from the Many Labs Replication Project?”</a></p>
<p>Introduction: Aki points us to  this discussion  from Rolf Zwaan:
  
The first massive replication project in psychology has just reached completion (several others are to follow). . . . What can we learn from the ManyLabs project? The results here show the effect sizes for the replication efforts (in green and grey) as well as the original studies (in blue). The 99% confidence intervals are for the meta-analysis of the effect size (the green dots); the studies are ordered by effect size.


 


Let’s first consider what we canNOT learn from these data. Of the 13 replication attempts (when the first four are taken together), 11 succeeded and 2 did not (in fact, at some point ManyLabs suggests that a third one, Imagined Contact also doesn’t really replicate). We cannot learn from this that the vast majority of psychological findings will replicate . . .


But even if we had an accurate estimate of the percentage of findings that replicate, how useful would that be? Rather than trying to arrive at a mo</p><p>2 0.16331507 <a title="2227-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-17-Replication_backlash.html">2137 andrew gelman stats-2013-12-17-Replication backlash</a></p>
<p>Introduction: Raghuveer Parthasarathy pointed me to an article in Nature by Mina Bissell, who  writes , “The push to replicate findings could shelve promising research and unfairly damage the reputations of careful, meticulous scientists.”
 
I can see where she’s coming from:  if you work hard day after day in the lab, it’s gotta be a bit frustrating to find all your work questioned, for the frauds of the  Dr. Anil Pottis  and Diederik Stapels to be treated as a reason for everyone else’s work to be considered guilty until proven innocent.
 
That said, I pretty much disagree with Bissell’s article, and really the best thing I can say about it is that I think it’s a good sign that the push for replication is so strong that now there’s a backlash against it.  Traditionally, leading scientists have been able to simply ignore the push for replication.  If they are feeling that the replication movement is strong enough that they need to fight it, that to me is good news.
 
I’ll explain a bit in the conte</p><p>3 0.13664505 <a title="2227-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-12-As_a_Bayesian_I_want_scientists_to_report_their_data_non-Bayesianly.html">1209 andrew gelman stats-2012-03-12-As a Bayesian I want scientists to report their data non-Bayesianly</a></p>
<p>Introduction: Philipp Doebler writes: 
  
  
I was quite happy that recently you shared some thoughts of yours and others on meta-analysis. I especially liked the  slides by Chris Schmid  that you linked from your blog. A large portion of my work deals with meta-analysis and I am also fond of using Bayesian methods (actually two of the projects I am working on are very Bayesian), though I can not say I have opinions with respect to the underlying philosophy. I would say though, that I do share your view that there are good reasons to use informative priors.


The reason I am writing to you is that this leads to the following dilemma, which is puzzling me. Say a number of scientists conduct similar studies over the years and all of them did this in a Bayesian fashion. If each of the groups used informative priors based on the research of existing groups the priors could become more and more informative over the years, since more and more is known over the subject. At least in smallish studies these p</p><p>4 0.11015852 <a title="2227-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-06-Suspicious_pattern_of_too-strong_replications_of_medical_research.html">700 andrew gelman stats-2011-05-06-Suspicious pattern of too-strong replications of medical research</a></p>
<p>Introduction: Howard Wainer  writes  in the Statistics Forum:
  
The Chinese scientific literature is rarely read or cited outside of China. But the authors of this work are usually knowledgeable of the non-Chinese literature — at least the A-list journals. And so they too try to replicate the alpha finding. But do they? One would think that they would find the same diminished effect size, but they don’t! Instead they replicate the original result, even larger.  Here’s one of the graphs:


   


How did this happen?
  
Full story  here .</p><p>5 0.10655738 <a title="2227-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-14-Type_M_errors_in_the_lab.html">908 andrew gelman stats-2011-09-14-Type M errors in the lab</a></p>
<p>Introduction: Jeff points us to this  news article  by Asher Mullard:
  
Bayer halts nearly two-thirds of its target-validation projects because in-house experimental findings fail to match up with published literature claims, finds a first-of-a-kind analysis on data irreproducibility.


An unspoken industry rule alleges that at least 50% of published studies from academic laboratories cannot be repeated in an industrial setting, wrote venture capitalist Bruce Booth in a recent blog post. A first-of-a-kind analysis of Bayer’s internal efforts to validate ‘new drug target’ claims now not only supports this view but suggests that 50% may be an underestimate; the company’s in-house experimental data do not match literature claims in 65% of target-validation projects, leading to project discontinuation. . . .


Khusru Asadullah, Head of Target Discovery at Bayer, and his colleagues looked back at 67 target-validation projects, covering the majority of Bayer’s work in oncology, women’s health and cardiov</p><p>6 0.10264248 <a title="2227-tfidf-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-20-Do_differences_between_biology_and_statistics_explain_some_of_our_diverging_attitudes_regarding_criticism_and_replication_of_scientific_claims%3F.html">2218 andrew gelman stats-2014-02-20-Do differences between biology and statistics explain some of our diverging attitudes regarding criticism and replication of scientific claims?</a></p>
<p>7 0.099504687 <a title="2227-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-29-Unfinished_business.html">637 andrew gelman stats-2011-03-29-Unfinished business</a></p>
<p>8 0.099077895 <a title="2227-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-19-%E2%80%9CConfirmation%2C_on_the_other_hand%2C_is_not_sexy%E2%80%9D.html">1683 andrew gelman stats-2013-01-19-“Confirmation, on the other hand, is not sexy”</a></p>
<p>9 0.088796511 <a title="2227-tfidf-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-24-On_deck_this_week.html">2222 andrew gelman stats-2014-02-24-On deck this week</a></p>
<p>10 0.085522406 <a title="2227-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-29-Decline_Effect_in_Linguistics%3F.html">1400 andrew gelman stats-2012-06-29-Decline Effect in Linguistics?</a></p>
<p>11 0.082938105 <a title="2227-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-31-How_to_fix_the_tabloids%3F__Toward_replicable_social_science_research.html">1878 andrew gelman stats-2013-05-31-How to fix the tabloids?  Toward replicable social science research</a></p>
<p>12 0.08244592 <a title="2227-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-28-Difficulties_of_using_statistical_significance_%28or_lack_thereof%29_to_sift_through_and_compare_research_hypotheses.html">2042 andrew gelman stats-2013-09-28-Difficulties of using statistical significance (or lack thereof) to sift through and compare research hypotheses</a></p>
<p>13 0.081000365 <a title="2227-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-13-Preregistration_of_Studies_and_Mock_Reports.html">1671 andrew gelman stats-2013-01-13-Preregistration of Studies and Mock Reports</a></p>
<p>14 0.080537483 <a title="2227-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-02-The_inevitable_problems_with_statistical_significance_and_95%25_intervals.html">1150 andrew gelman stats-2012-02-02-The inevitable problems with statistical significance and 95% intervals</a></p>
<p>15 0.075693831 <a title="2227-tfidf-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-19-The_replication_and_criticism_movement_is_not_about_suppressing_speculative_research%3B_rather%2C_it%E2%80%99s_all_about_enabling_science%E2%80%99s_fabled_self-correcting_nature.html">2217 andrew gelman stats-2014-02-19-The replication and criticism movement is not about suppressing speculative research; rather, it’s all about enabling science’s fabled self-correcting nature</a></p>
<p>16 0.075631365 <a title="2227-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>17 0.075388834 <a title="2227-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-01-Why_big_effects_are_more_important_than_small_effects.html">1744 andrew gelman stats-2013-03-01-Why big effects are more important than small effects</a></p>
<p>18 0.074555367 <a title="2227-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-20-Reading_a_research_paper_%21%3D_agreeing_with_its_claims.html">1074 andrew gelman stats-2011-12-20-Reading a research paper != agreeing with its claims</a></p>
<p>19 0.074129865 <a title="2227-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-14-Subtleties_with_measurement-error_models_for_the_evaluation_of_wacky_claims.html">803 andrew gelman stats-2011-07-14-Subtleties with measurement-error models for the evaluation of wacky claims</a></p>
<p>20 0.073263876 <a title="2227-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-Fourteen_magic_words%3A_an_update.html">898 andrew gelman stats-2011-09-10-Fourteen magic words: an update</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.131), (1, -0.01), (2, 0.018), (3, -0.114), (4, 0.017), (5, -0.011), (6, -0.018), (7, -0.005), (8, -0.043), (9, -0.006), (10, -0.01), (11, 0.025), (12, 0.032), (13, -0.034), (14, 0.028), (15, -0.002), (16, -0.018), (17, 0.009), (18, -0.002), (19, -0.015), (20, 0.0), (21, 0.02), (22, -0.001), (23, 0.012), (24, -0.013), (25, -0.022), (26, -0.003), (27, 0.012), (28, 0.001), (29, 0.029), (30, -0.037), (31, -0.072), (32, -0.025), (33, -0.03), (34, 0.034), (35, 0.016), (36, -0.045), (37, -0.002), (38, 0.024), (39, -0.024), (40, 0.011), (41, 0.007), (42, 0.043), (43, 0.002), (44, 0.038), (45, 0.002), (46, -0.038), (47, -0.002), (48, -0.022), (49, 0.028)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97826594 <a title="2227-lsi-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-27-%E2%80%9CWhat_Can_we_Learn_from_the_Many_Labs_Replication_Project%3F%E2%80%9D.html">2227 andrew gelman stats-2014-02-27-“What Can we Learn from the Many Labs Replication Project?”</a></p>
<p>Introduction: Aki points us to  this discussion  from Rolf Zwaan:
  
The first massive replication project in psychology has just reached completion (several others are to follow). . . . What can we learn from the ManyLabs project? The results here show the effect sizes for the replication efforts (in green and grey) as well as the original studies (in blue). The 99% confidence intervals are for the meta-analysis of the effect size (the green dots); the studies are ordered by effect size.


 


Let’s first consider what we canNOT learn from these data. Of the 13 replication attempts (when the first four are taken together), 11 succeeded and 2 did not (in fact, at some point ManyLabs suggests that a third one, Imagined Contact also doesn’t really replicate). We cannot learn from this that the vast majority of psychological findings will replicate . . .


But even if we had an accurate estimate of the percentage of findings that replicate, how useful would that be? Rather than trying to arrive at a mo</p><p>2 0.765297 <a title="2227-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-02-The_inevitable_problems_with_statistical_significance_and_95%25_intervals.html">1150 andrew gelman stats-2012-02-02-The inevitable problems with statistical significance and 95% intervals</a></p>
<p>Introduction: I’m thinking more and more that we have to get rid of statistical significance, 95% intervals, and all the rest, and just come to a more fundamental acceptance of uncertainty.
 
In practice, I think we use confidence intervals and hypothesis tests as a way to avoid acknowledging uncertainty. We set up some rules and then act as if we know what is real and what is not. Even in my own applied work, I’ve often enough presented 95% intervals and gone on from there. But maybe that’s just not right.
 
I was thinking about this after receiving the following email from a psychology student: 
  
  
I [the student] am trying to conceptualize the lessons in  your paper with Stern  with comparing treatment effects across studies. When trying to understand if a certain intervention works, we must look at what the literature says. However this can be complicated if the literature has divergent results. There are four situations I am thinking of. FOr each of these situations, assume the studies are r</p><p>3 0.76194108 <a title="2227-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-12-More_frustrations_trying_to_replicate_an_analysis_published_in_a_reputable_journal.html">1054 andrew gelman stats-2011-12-12-More frustrations trying to replicate an analysis published in a reputable journal</a></p>
<p>Introduction: The story starts in September, when psychology professor Fred Oswald wrote me:
  
I [Oswald] wanted to point out  this paper  in Science (Ramirez & Beilock, 2010) examining how students’ emotional writing improves their test performance in high-pressure situations.


Although replication is viewed as the hallmark of research, this paper replicates implausibly large d-values and correlations across studies, leading me to be more suspicious of the findings (not less, as is generally the case).
  
   
He also pointed me to this paper:
  
Experimental disclosure and its moderators: A meta-analysis.


Frattaroli, Joanne


Psychological Bulletin, Vol 132(6), Nov 2006, 823-865. 


Disclosing information, thoughts, and feelings about personal and meaningful topics (experimental disclosure) is purported to have various health and psychological consequences (e.g., J. W. Pennebaker, 1993). Although the results of 2 small meta-analyses (P. G. Frisina, J. C. Borod, & S. J. Lepore, 2004; J. M. Smyth</p><p>4 0.75576854 <a title="2227-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-28-Difficulties_of_using_statistical_significance_%28or_lack_thereof%29_to_sift_through_and_compare_research_hypotheses.html">2042 andrew gelman stats-2013-09-28-Difficulties of using statistical significance (or lack thereof) to sift through and compare research hypotheses</a></p>
<p>Introduction: Dean Eckles writes:
  
Thought you might be interested in an example that touches on a couple recurring topics: 
1. The  difference  between a statistically significant finding and one that is non-significant need not be itself statistically significant (thus highlighting the problems of using NHST to declare whether an effect exists or not). 
2. Continued issues with the credibility of high profile studies of “social contagion”, especially by  Christakis and Fowler .


A new paper in  Archives of Sexual Behavior  produces observational estimates of peer effects in sexual behavior and same-sex attraction. In the text, the authors (who include C&F;) make repeated comparisons of the results for peer effects in sexual intercourse and those for peer effects in same-sex attraction. However, the 95% CI for the later actually includes the point estimate for the former! This is most clear in Figure 2, as highlighted by Real Clear Science’s  blog post  about the study.  (Now because there is som</p><p>5 0.74935615 <a title="2227-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-18-Question_on_Type_M_errors.html">963 andrew gelman stats-2011-10-18-Question on Type M errors</a></p>
<p>Introduction: Inti Pedroso writes:
  
Today during the group meeting at my new job we were revising a paper whose main conclusions were sustained by an ANOVA.


One of the first observations is that the experiment had a small sample size. Interestingly (may not so), some of the reported effects (most of them interactions) were quite large. One of the experience group members said that “there is a common wisdom that one should not believe effects from small sample sizes but [he thinks] if they [the effects] are large enough to be picked on a small study they must be real large effects”. I argued that if the sample size is small one could incur on a M-type error in which the magnitude of the effect is being over-estimated and that if larger samples are evaluated the magnitude may become smaller and also the confidence intervals. The concept of M-type error is completely new to all other members of the group (on which I am in my second week) and I was given the job of finding a suitable ref to explain</p><p>6 0.74871635 <a title="2227-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-Fourteen_magic_words%3A_an_update.html">898 andrew gelman stats-2011-09-10-Fourteen magic words: an update</a></p>
<p>7 0.74307251 <a title="2227-lsi-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-15-Problematic_interpretations_of_confidence_intervals.html">2248 andrew gelman stats-2014-03-15-Problematic interpretations of confidence intervals</a></p>
<p>8 0.73832786 <a title="2227-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-28-50_shades_of_gray%3A__A_research_story.html">1959 andrew gelman stats-2013-07-28-50 shades of gray:  A research story</a></p>
<p>9 0.73722947 <a title="2227-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-14-Type_M_errors_in_the_lab.html">908 andrew gelman stats-2011-09-14-Type M errors in the lab</a></p>
<p>10 0.72497213 <a title="2227-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-13-Preregistration_of_Studies_and_Mock_Reports.html">1671 andrew gelman stats-2013-01-13-Preregistration of Studies and Mock Reports</a></p>
<p>11 0.72076082 <a title="2227-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-19-R_package_for_effect_size_calculations_for_psychology_researchers.html">2069 andrew gelman stats-2013-10-19-R package for effect size calculations for psychology researchers</a></p>
<p>12 0.71978933 <a title="2227-lsi-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-10-Preregistration%3A_what%E2%80%99s_in_it_for_you%3F.html">2241 andrew gelman stats-2014-03-10-Preregistration: what’s in it for you?</a></p>
<p>13 0.71764833 <a title="2227-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-29-Decline_Effect_in_Linguistics%3F.html">1400 andrew gelman stats-2012-06-29-Decline Effect in Linguistics?</a></p>
<p>14 0.7176218 <a title="2227-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-31-Response_by_Jessica_Tracy_and_Alec_Beall_to_my_critique_of_the_methods_in_their_paper%2C_%E2%80%9CWomen_Are_More_Likely_to_Wear_Red_or_Pink_at_Peak_Fertility%E2%80%9D.html">1963 andrew gelman stats-2013-07-31-Response by Jessica Tracy and Alec Beall to my critique of the methods in their paper, “Women Are More Likely to Wear Red or Pink at Peak Fertility”</a></p>
<p>15 0.71734273 <a title="2227-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-07-Stereotype_threat%21.html">1929 andrew gelman stats-2013-07-07-Stereotype threat!</a></p>
<p>16 0.71379834 <a title="2227-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-12-Meta-analysis%2C_game_theory%2C_and_incentives_to_do_replicable_research.html">1163 andrew gelman stats-2012-02-12-Meta-analysis, game theory, and incentives to do replicable research</a></p>
<p>17 0.69423437 <a title="2227-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-07-I%E2%80%99m_negative_on_the_expression_%E2%80%9Cfalse_positives%E2%80%9D.html">2093 andrew gelman stats-2013-11-07-I’m negative on the expression “false positives”</a></p>
<p>18 0.69222581 <a title="2227-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-09-The_difference_between_%E2%80%9Csignificant%E2%80%9D_and_%E2%80%9Cnon-significant%E2%80%9D_is_not_itself_statistically_significant.html">1662 andrew gelman stats-2013-01-09-The difference between “significant” and “non-significant” is not itself statistically significant</a></p>
<p>19 0.68855309 <a title="2227-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-09-The_difference_between_significant_and_not_significant%E2%80%A6.html">897 andrew gelman stats-2011-09-09-The difference between significant and not significant…</a></p>
<p>20 0.68741733 <a title="2227-lsi-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-24-%E2%80%9CEdlin%E2%80%99s_rule%E2%80%9D_for_routinely_scaling_down_published_estimates.html">2223 andrew gelman stats-2014-02-24-“Edlin’s rule” for routinely scaling down published estimates</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(7, 0.013), (14, 0.027), (15, 0.116), (16, 0.138), (20, 0.013), (21, 0.05), (24, 0.146), (44, 0.016), (59, 0.02), (71, 0.011), (77, 0.02), (79, 0.017), (82, 0.019), (99, 0.284)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9654882 <a title="2227-lda-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-27-%E2%80%9CWhat_Can_we_Learn_from_the_Many_Labs_Replication_Project%3F%E2%80%9D.html">2227 andrew gelman stats-2014-02-27-“What Can we Learn from the Many Labs Replication Project?”</a></p>
<p>Introduction: Aki points us to  this discussion  from Rolf Zwaan:
  
The first massive replication project in psychology has just reached completion (several others are to follow). . . . What can we learn from the ManyLabs project? The results here show the effect sizes for the replication efforts (in green and grey) as well as the original studies (in blue). The 99% confidence intervals are for the meta-analysis of the effect size (the green dots); the studies are ordered by effect size.


 


Let’s first consider what we canNOT learn from these data. Of the 13 replication attempts (when the first four are taken together), 11 succeeded and 2 did not (in fact, at some point ManyLabs suggests that a third one, Imagined Contact also doesn’t really replicate). We cannot learn from this that the vast majority of psychological findings will replicate . . .


But even if we had an accurate estimate of the percentage of findings that replicate, how useful would that be? Rather than trying to arrive at a mo</p><p>2 0.96419311 <a title="2227-lda-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-30-I_posted_this_as_a_comment_on_a_sociology_blog.html">2353 andrew gelman stats-2014-05-30-I posted this as a comment on a sociology blog</a></p>
<p>Introduction: I discussed two problems:
 
1.  An artificial scarcity applied to journal publication, a scarcity which I believe is being enforced based on a monetary principle of not wanting to reduce the value of publication.  The problem is that journals don’t just spread information and improve communication, they also represent chits for hiring and promotion.  I’d prefer to separate these two aspects of publication. To keep these functions tied together seems to me like a terrible mistake.  It would be as if, instead of using dollar bills as currency, we were to just use  paper , and then if the government kept paper artificially scarce to retain the value of money, so that we were reduced to scratching notes to each other on walls and tables.
 
2.  The discontinuous way in which unpublished papers and submissions to journals are taken as highly suspect and requiring a strong justification of all methods and assumptions, but once a paper becomes published its conclusions are taken as true unless</p><p>3 0.95297241 <a title="2227-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-12-The_importance_of_style_in_academic_writing.html">902 andrew gelman stats-2011-09-12-The importance of style in academic writing</a></p>
<p>Introduction: In my comments on  academic cheating , I briefly discussed the question of how some of these papers could’ve been published in the first place, given that they tend to be of low quality.  (It’s rare that people plagiarize the good stuff, and, when they do—for example when a senior scholar takes credit for a junior researcher’s contributions without giving proper credit—there’s not always a paper trail, and there can be legitimate differences of opinion about the relative contributions of the participants.)
 
Anyway, to get back to the cases at hand:  how did these rulebreakers get published in the first place?  The question here is not how did they get away with cheating but how is it that top journals were publishing mediocre research? 
   
In the case of the profs who falsified data (Diederik Stapel) or did not follow scientific protocol (Mark Hauser), the answer is clear:  By cheating, they were able to get the sort of too-good-to-be-true results which, if they  were  true, would be</p><p>4 0.95239377 <a title="2227-lda-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-31-How_to_fix_the_tabloids%3F__Toward_replicable_social_science_research.html">1878 andrew gelman stats-2013-05-31-How to fix the tabloids?  Toward replicable social science research</a></p>
<p>Introduction: This seems to be the topic of the week.  Yesterday I posted on the sister blog  some further thoughts  on those “Psychological Science” papers on menstrual cycles, biceps size, and political attitudes, tied to a horrible press release from the journal Psychological Science hyping the biceps and politics study.
 
Then I was pointed to these  suggestions  from Richard Lucas and M. Brent Donnellan have on improving the replicability and reproducibility of research published in the Journal of Research in Personality:
  
It goes without saying that editors of scientific journals strive to publish research that is not only theoretically interesting but also methodologically rigorous. The goal is to select papers that advance the field. Accordingly, editors want to publish findings that can be reproduced and replicated by other scientists. Unfortunately, there has been a recent “crisis in confidence” among psychologists about the quality of psychological research (Pashler & Wagenmakers, 2012)</p><p>5 0.95220017 <a title="2227-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>Introduction: Prasanta Bandyopadhyay and Gordon Brittan  write :
  
We introduce a distinction, unnoticed in the literature, between four varieties of objective Bayesianism. What we call ‘strong objective Bayesianism’ is characterized by two claims, that all scientific inference is ‘logical’ and that, given the same background information two agents will ascribe a unique probability to their priors. We think that neither of these claims can be sustained; in this sense, they are ‘dogmatic’. The first fails to recognize that some scientific inference, in particular that concerning evidential relations, is not (in the appropriate sense) logical, the second fails to provide a non-question-begging account of ‘same background information’. We urge that a suitably objective Bayesian account of scientific inference does not require either of the claims. Finally, we argue that Bayesianism needs to be fine-grained in the same way that Bayesians fine-grain their beliefs.
  
I have not read their paper in detai</p><p>6 0.95151186 <a title="2227-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-20-What_happened_that_the_journal_Psychological_Science_published_a_paper_with_no_identifiable_strengths%3F.html">1865 andrew gelman stats-2013-05-20-What happened that the journal Psychological Science published a paper with no identifiable strengths?</a></p>
<p>7 0.95131958 <a title="2227-lda-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-17-Replication_backlash.html">2137 andrew gelman stats-2013-12-17-Replication backlash</a></p>
<p>8 0.9512493 <a title="2227-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-23-A_statistical_version_of_Arrow%E2%80%99s_paradox.html">586 andrew gelman stats-2011-02-23-A statistical version of Arrow’s paradox</a></p>
<p>9 0.95115358 <a title="2227-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-06-W%E2%80%99man_%3C_W%E2%80%99pedia%2C_again.html">945 andrew gelman stats-2011-10-06-W’man < W’pedia, again</a></p>
<p>10 0.94881582 <a title="2227-lda-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-19-The_replication_and_criticism_movement_is_not_about_suppressing_speculative_research%3B_rather%2C_it%E2%80%99s_all_about_enabling_science%E2%80%99s_fabled_self-correcting_nature.html">2217 andrew gelman stats-2014-02-19-The replication and criticism movement is not about suppressing speculative research; rather, it’s all about enabling science’s fabled self-correcting nature</a></p>
<p>11 0.94873869 <a title="2227-lda-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-29-%E2%80%9CQuestioning_The_Lancet%2C_PLOS%2C_And_Other_Surveys_On_Iraqi_Deaths%2C_An_Interview_With_Univ._of_London_Professor_Michael_Spagat%E2%80%9D.html">2191 andrew gelman stats-2014-01-29-“Questioning The Lancet, PLOS, And Other Surveys On Iraqi Deaths, An Interview With Univ. of London Professor Michael Spagat”</a></p>
<p>12 0.9485625 <a title="2227-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-22-The_Jumpstart_financial_literacy_survey_and_the_different_purposes_of_tests.html">481 andrew gelman stats-2010-12-22-The Jumpstart financial literacy survey and the different purposes of tests</a></p>
<p>13 0.94808489 <a title="2227-lda-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-20-The_AAA_Tranche_of_Subprime_Science.html">2179 andrew gelman stats-2014-01-20-The AAA Tranche of Subprime Science</a></p>
<p>14 0.94729781 <a title="2227-lda-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-%E2%80%9CThe_British_amateur_who_debunked_the_mathematics_of_happiness%E2%80%9D.html">2177 andrew gelman stats-2014-01-19-“The British amateur who debunked the mathematics of happiness”</a></p>
<p>15 0.94482076 <a title="2227-lda-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-15-Problematic_interpretations_of_confidence_intervals.html">2248 andrew gelman stats-2014-03-15-Problematic interpretations of confidence intervals</a></p>
<p>16 0.94402421 <a title="2227-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-30-Retracted_articles_and_unethical_behavior_in_economics_journals%3F.html">1435 andrew gelman stats-2012-07-30-Retracted articles and unethical behavior in economics journals?</a></p>
<p>17 0.9439798 <a title="2227-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-30-You_can%E2%80%99t_put_Pandora_back_in_the_box.html">120 andrew gelman stats-2010-06-30-You can’t put Pandora back in the box</a></p>
<p>18 0.94372165 <a title="2227-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-22-Likelihood_Ratio_%E2%89%A0_1_Journal.html">1774 andrew gelman stats-2013-03-22-Likelihood Ratio ≠ 1 Journal</a></p>
<p>19 0.94318092 <a title="2227-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-14-Bayes_in_China_update.html">517 andrew gelman stats-2011-01-14-Bayes in China update</a></p>
<p>20 0.94307923 <a title="2227-lda-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-11-What_if_I_were_to_stop_publishing_in_journals%3F.html">2244 andrew gelman stats-2014-03-11-What if I were to stop publishing in journals?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
