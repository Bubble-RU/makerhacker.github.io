<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2180 andrew gelman stats-2014-01-21-Everything I need to know about Bayesian statistics, I learned in eight schools.</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2014" href="../home/andrew_gelman_stats-2014_home.html">andrew_gelman_stats-2014</a> <a title="andrew_gelman_stats-2014-2180" href="#">andrew_gelman_stats-2014-2180</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2180 andrew gelman stats-2014-01-21-Everything I need to know about Bayesian statistics, I learned in eight schools.</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2014-2180-html" href="http://andrewgelman.com/2014/01/21/everything-need-know-bayesian-statistics-learned-eight-schools/">html</a></p><p>Introduction: This post is by Phil.
 
I’m aware that there  are  some people who use a Bayesian approach largely because it allows them to provide a highly informative prior distribution based subjective judgment, but that is not the appeal of Bayesian methods for a lot of us practitioners. It’s disappointing and surprising, twenty years after my initial experiences, to still hear highly informed professional statisticians who think that what distinguishes Bayesian statistics from Frequentist statistics is “subjectivity” ( as seen in  a recent blog post and its comments ).
 
 My first encounter with Bayesian statistics was just over 20 years ago. I was a postdoc at Lawrence Berkeley National Laboratory, with a new PhD in theoretical atomic physics but working on various problems related to the geographical and statistical distribution of indoor radon (a naturally occurring radioactive gas that can be dangerous if present at high concentrations). One of the issues I ran into right at the start was th</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 One of the issues I ran into right at the start was that many researchers were trying to use data from statewide radon surveys to make maps of radon concentrations within states. [sent-6, score-0.829]
</p><p>2 The surveys had been designed to characterize the  statistical  distribution of radon in each state, not the  spatial  distribution, so sample sizes were much higher in highly populated counties than in low-population counties. [sent-7, score-0.764]
</p><p>3 Within the counties with lots of measurements, the statistical distribution of radon measurements was roughly lognormal, with a geometric standard deviation of around 3 (a dimensionless number) and a geometric mean that varied from county to county. [sent-8, score-2.39]
</p><p>4 One of the first datasets I looked at in detail was from Minnesota, where counties with more than 100 radon survey measurements had observed geometric means as low as 83 Bequerels per cubic meter and as high as 136 Bequerels per cubic meter. [sent-9, score-1.947]
</p><p>5 Many counties had a number of measurements in the single digits. [sent-12, score-0.323]
</p><p>6 I still remember, 20 years later, that the highest county geometric mean was in Lac Qui Parle County, with just two sampled homes and an observed geometric mean of 500 Bequerels per cubic meter. [sent-14, score-2.027]
</p><p>7 So it seemed to me to be extremely unlikely that the geometric mean radon concentration of all of the homes in Lac Qui Parle county was anywhere near 500 Bequerels per cubic meter. [sent-17, score-1.748]
</p><p>8 Instead, the geometric mean was probably much lower but that the survey had happened to sample two homes from the upper end of the distribution. [sent-18, score-0.719]
</p><p>9 But however certain I was, how could I come up with an estimate and an uncertainty for the county’s true geometric mean radon concentration? [sent-19, score-1.222]
</p><p>10 That professor’s attitude was:  I have an “unbiased” estimate of the geometric mean, 500 Bequerels per cubic meter, and an uncertainty in that quantity…what else could I hope for? [sent-21, score-0.881]
</p><p>11 The second argument would assume that the true effect in all schools is exactly equal, in spite of the courses being taught by different teachers to different students. [sent-38, score-0.351]
</p><p>12 One can play around with a few different prior distributions for the standard deviation of true effects and find that it doesn’t really matter, any reasonable choice that doesn’t force the standard deviation to be very small or very large turns out to give about the same statistical distribution. [sent-42, score-0.659]
</p><p>13 The thing that was so great about this example at the time is that it was almost exactly like the data and model I was already using for my indoor radon data. [sent-44, score-0.475]
</p><p>14 If I worked with the logarithms of the radon measurements, then I had data that were approximately normally distributed within each county, and I was already assuming that the county means were (log)normally distributed in my simulation program. [sent-45, score-0.707]
</p><p>15 So i mmediately after leaving Andrew’s classroom I went back to my office, coded up the calculations, and came up with estimates for the geometric mean radon concentration in every county in Minnesota. [sent-46, score-1.333]
</p><p>16 The final estimate for Lac Qui Parle County: 192 +/- 29 Bequerels per Cubic Meter, much lower than the geometric mean of its two observations, but higher than the typical Minnesota county. [sent-48, score-0.812]
</p><p>17 (In the unlikely event that  you’re  interested in the details, including model checking, you can read Price PN, Nero AV, and Gelman A, “Bayesian prediction of mean indoor radon concentrations for Minnesota counties,” Health Physics 71:922-936, 1996. [sent-49, score-0.703]
</p><p>18 At the time that we did the radon work described above, Bayesian data analysis was not at all mainstream. [sent-51, score-0.361]
</p><p>19 One of the criticisms those professors brought up was that “of course, Bayesian analysis is so subjective…”  But in fact, the normal-normal model that we used for the radon analysis was no more (or less! [sent-57, score-0.408]
</p><p>20 If there’s a Frequentist method for estimating the radon concentration in Lac Qui Parle County, Minnesota that is better — by which I mean either more accurate in its central estimate or providing more accurate uncertainty estimates, or both — then I want to use it. [sent-60, score-0.743]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('geometric', 0.455), ('radon', 0.361), ('county', 0.289), ('bequerels', 0.233), ('cubic', 0.185), ('measurements', 0.184), ('homes', 0.143), ('counties', 0.139), ('meter', 0.133), ('true', 0.131), ('mean', 0.121), ('distribution', 0.119), ('school', 0.112), ('deviation', 0.111), ('bayesian', 0.11), ('effect', 0.11), ('estimate', 0.107), ('concentration', 0.107), ('concentrations', 0.107), ('lac', 0.106), ('parle', 0.106), ('standard', 0.1), ('qui', 0.1), ('berkeley', 0.098), ('minnesota', 0.092), ('per', 0.087), ('rubin', 0.084), ('observed', 0.074), ('estimated', 0.07), ('indoor', 0.067), ('uc', 0.067), ('schools', 0.067), ('frequentist', 0.06), ('eight', 0.058), ('means', 0.057), ('statistical', 0.056), ('sampled', 0.053), ('unbiased', 0.052), ('force', 0.05), ('model', 0.047), ('designed', 0.047), ('uncertainty', 0.047), ('andrew', 0.046), ('lognormal', 0.045), ('statistics', 0.044), ('highest', 0.044), ('subjective', 0.043), ('equal', 0.043), ('taught', 0.043), ('higher', 0.042)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="2180-tfidf-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-21-Everything_I_need_to_know_about_Bayesian_statistics%2C_I_learned_in_eight_schools..html">2180 andrew gelman stats-2014-01-21-Everything I need to know about Bayesian statistics, I learned in eight schools.</a></p>
<p>Introduction: This post is by Phil.
 
I’m aware that there  are  some people who use a Bayesian approach largely because it allows them to provide a highly informative prior distribution based subjective judgment, but that is not the appeal of Bayesian methods for a lot of us practitioners. It’s disappointing and surprising, twenty years after my initial experiences, to still hear highly informed professional statisticians who think that what distinguishes Bayesian statistics from Frequentist statistics is “subjectivity” ( as seen in  a recent blog post and its comments ).
 
 My first encounter with Bayesian statistics was just over 20 years ago. I was a postdoc at Lawrence Berkeley National Laboratory, with a new PhD in theoretical atomic physics but working on various problems related to the geographical and statistical distribution of indoor radon (a naturally occurring radioactive gas that can be dangerous if present at high concentrations). One of the issues I ran into right at the start was th</p><p>2 0.23118621 <a title="2180-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-13-News_coverage_of_statistical_issues%E2%80%A6how_did_I_do%3F.html">514 andrew gelman stats-2011-01-13-News coverage of statistical issues…how did I do?</a></p>
<p>Introduction: This post is by Phil Price. 
 
A reporter once told me that the worst-kept secret of journalism is that every story has errors.  And it’s true that just about every time I know about something first-hand, the news stories about it have some mistakes. Reporters aren’t subject-matter experts, they have limited time, and they generally can’t keep revisiting the things they are saying and checking them for accuracy.  Many of us have published papers with errors — my most recent paper has an incorrect figure — and that’s after working on them carefully for weeks!
 
One way that reporters can try to get things right is by quoting experts.  Even then, there are problems with taking quotes out of context, or with making poor choices about what material to include or exclude, or, of course, with making a poor selection of experts.  
 
Yesterday, I was interviewed by an NPR reporter about the risks of breathing radon (a naturally occurring radioactive gas): who should test for it, how dangerous</p><p>3 0.22097777 <a title="2180-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-15-Still_more_Mr._P_in_public_health.html">770 andrew gelman stats-2011-06-15-Still more Mr. P in public health</a></p>
<p>Introduction: When it  rains  it pours . . .
 
John Transue writes:
  
I saw  a post  on Andrew Sullivan’s blog today about life expectancy in different US counties. With a bunch of the worst counties being in Mississippi, I thought that it might be another case of analysts getting extreme values from small counties.


However, the paper (see  here ) includes a pretty interesting methods section. This is from page 5, “Specifically, we used a mixed-effects Poisson regression with time, geospatial, and covariate components. Poisson regression fits count outcome variables, e.g., death counts, and is preferable to a logistic model because the latter is biased when an outcome is rare (occurring in less than 1% of observations).”


They have downloadable data. I believe that the data are predicted values from the model. A web appendix also gives 90% CIs for their estimates.


Do you think they solved the small county problem and that the worst counties really are where their spreadsheet suggests?
  
My re</p><p>4 0.1409509 <a title="2180-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-17-NUTS_discussed_on_Xi%E2%80%99an%E2%80%99s_Og.html">1809 andrew gelman stats-2013-04-17-NUTS discussed on Xi’an’s Og</a></p>
<p>Introduction: Xi’an’s Og  (aka Christian Robert’s blog) is featuring a very nice  presentation of NUTS  by Marco Banterle, with discussion and some suggestions.
 
I’m not even sure how they found Michael Betancourt’s paper on geometric NUTS — I don’t see it on the arXiv yet, or I’d provide a link.</p><p>5 0.13764775 <a title="2180-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-22-Evaluating_the_impacts_of_welfare_reform%3F.html">1732 andrew gelman stats-2013-02-22-Evaluating the impacts of welfare reform?</a></p>
<p>Introduction: John Pugliese writes: 
  
  
I was recently in a conversation with some colleagues regarding the evaluation of recent welfare reform in California. The discussion centered around what types of design might allow us to understand the impact the changes. Experimental designs were out, as random assignment is not feasible. Our data is pre/post, and some of my colleagues believed that the best we can do under these circumstance was a descriptive study; i.e. no causal inference. All of us were concerned with changes in economic and population changes over the pre-to-post period; i.e. over-estimating the effects in an improving economy. 


I was thought a quasi-experimental design was possible using MLM. Briefly, my suggestion was the following:


Match our post-participants to a set of pre-participants on relevant person level factors, and treat the pre/post differences as a random effect at the county level. Next, we would adjust the pre/post differences by changes in economic and populati</p><p>6 0.13619179 <a title="2180-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-25-An_interesting_but_flawed_attempt_to_apply_general_forecasting_principles_to_contextualize_attitudes_toward_risks_of_global_warming.html">2112 andrew gelman stats-2013-11-25-An interesting but flawed attempt to apply general forecasting principles to contextualize attitudes toward risks of global warming</a></p>
<p>7 0.13047419 <a title="2180-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-14-Subtleties_with_measurement-error_models_for_the_evaluation_of_wacky_claims.html">803 andrew gelman stats-2011-07-14-Subtleties with measurement-error models for the evaluation of wacky claims</a></p>
<p>8 0.12923707 <a title="2180-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-11-How_do_we_evaluate_a_new_and_wacky_claim%3F.html">797 andrew gelman stats-2011-07-11-How do we evaluate a new and wacky claim?</a></p>
<p>9 0.12902069 <a title="2180-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>10 0.12772612 <a title="2180-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-04-Write_This_Book.html">1605 andrew gelman stats-2012-12-04-Write This Book</a></p>
<p>11 0.12664121 <a title="2180-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-25-Bayes-respecting_experimental_design_and_other_things.html">1955 andrew gelman stats-2013-07-25-Bayes-respecting experimental design and other things</a></p>
<p>12 0.12546834 <a title="2180-tfidf-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-22-Philosophy_of_Bayes_and_non-Bayes%3A__A_dialogue_with_Deborah_Mayo.html">291 andrew gelman stats-2010-09-22-Philosophy of Bayes and non-Bayes:  A dialogue with Deborah Mayo</a></p>
<p>13 0.1254375 <a title="2180-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-15-The_bias-variance_tradeoff.html">960 andrew gelman stats-2011-10-15-The bias-variance tradeoff</a></p>
<p>14 0.12088068 <a title="2180-tfidf-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-03-Nebraska_never_looked_so_appealing%3A_anatomy_of_a_zombie_attack._Oops%2C_I_mean_a_recession..html">182 andrew gelman stats-2010-08-03-Nebraska never looked so appealing: anatomy of a zombie attack. Oops, I mean a recession.</a></p>
<p>15 0.11736678 <a title="2180-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>16 0.11664947 <a title="2180-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>17 0.11582729 <a title="2180-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-02-So-called_Bayesian_hypothesis_testing_is_just_as_bad_as_regular_hypothesis_testing.html">643 andrew gelman stats-2011-04-02-So-called Bayesian hypothesis testing is just as bad as regular hypothesis testing</a></p>
<p>18 0.11359529 <a title="2180-tfidf-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-01-How_does_Bayes_do_it%3F.html">247 andrew gelman stats-2010-09-01-How does Bayes do it?</a></p>
<p>19 0.11246607 <a title="2180-tfidf-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-31-Somewhat_Bayesian_multilevel_modeling.html">246 andrew gelman stats-2010-08-31-Somewhat Bayesian multilevel modeling</a></p>
<p>20 0.11125843 <a title="2180-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-01-Peter_Huber%E2%80%99s_reflections_on_data_analysis.html">690 andrew gelman stats-2011-05-01-Peter Huber’s reflections on data analysis</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.226), (1, 0.125), (2, 0.047), (3, -0.039), (4, -0.001), (5, 0.036), (6, 0.02), (7, 0.087), (8, -0.014), (9, -0.056), (10, -0.01), (11, -0.031), (12, 0.029), (13, 0.005), (14, 0.006), (15, 0.017), (16, 0.004), (17, 0.033), (18, 0.007), (19, 0.013), (20, -0.018), (21, 0.041), (22, 0.014), (23, -0.002), (24, 0.048), (25, 0.003), (26, -0.056), (27, 0.011), (28, -0.009), (29, -0.005), (30, 0.026), (31, 0.025), (32, -0.023), (33, -0.003), (34, 0.017), (35, 0.003), (36, -0.011), (37, -0.012), (38, 0.018), (39, -0.013), (40, -0.016), (41, -0.027), (42, -0.099), (43, -0.04), (44, -0.035), (45, 0.016), (46, -0.004), (47, 0.001), (48, -0.006), (49, -0.025)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97466981 <a title="2180-lsi-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-21-Everything_I_need_to_know_about_Bayesian_statistics%2C_I_learned_in_eight_schools..html">2180 andrew gelman stats-2014-01-21-Everything I need to know about Bayesian statistics, I learned in eight schools.</a></p>
<p>Introduction: This post is by Phil.
 
I’m aware that there  are  some people who use a Bayesian approach largely because it allows them to provide a highly informative prior distribution based subjective judgment, but that is not the appeal of Bayesian methods for a lot of us practitioners. It’s disappointing and surprising, twenty years after my initial experiences, to still hear highly informed professional statisticians who think that what distinguishes Bayesian statistics from Frequentist statistics is “subjectivity” ( as seen in  a recent blog post and its comments ).
 
 My first encounter with Bayesian statistics was just over 20 years ago. I was a postdoc at Lawrence Berkeley National Laboratory, with a new PhD in theoretical atomic physics but working on various problems related to the geographical and statistical distribution of indoor radon (a naturally occurring radioactive gas that can be dangerous if present at high concentrations). One of the issues I ran into right at the start was th</p><p>2 0.82009017 <a title="2180-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-15-The_bias-variance_tradeoff.html">960 andrew gelman stats-2011-10-15-The bias-variance tradeoff</a></p>
<p>Introduction: Joshua Vogelstein asks for my thoughts as  a Bayesian on the above topic.  So here they are (briefly):
 
The concept of the bias-variance tradeoff can be useful if you don’t take it too seriously.  The basic idea is as follows:  if you’re estimating something, you can slice your data finer and finer, or perform more and more adjustments, each time getting a purer—and less biased—estimate.  But each subdivision or each adjustment reduces your sample size or increases potential estimation error, hence the variance of your estimate goes up.
 
That story is real.  In lots and lots of examples, there’s a continuum between a completely unadjusted general estimate (high bias, low variance) and a specific, focused, adjusted estimate (low bias, high variance).
 
Suppose, for example, you’re using data from a large experiment to estimate the effect of a treatment on a fairly narrow group, say, white men between the ages of 45 and 50.  At one extreme, you could just take the estimated treatment e</p><p>3 0.78614467 <a title="2180-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-25-Is_instrumental_variables_analysis_particularly_susceptible_to_Type_M_errors%3F.html">368 andrew gelman stats-2010-10-25-Is instrumental variables analysis particularly susceptible to Type M errors?</a></p>
<p>Introduction: Hendrik Juerges writes:
  
I am an applied econometrician. The reason I am writing is that I am pondering a question for some time now and I am curious whether you have any views on it.


One problem the practitioner of instrumental variables estimation faces is large standard errors even with very large samples. Part of the problem is of course that one estimates a ratio. Anyhow, more often than not, I and many other researchers I know end up with large point estimates and standard errors when trying IV on a problem. Sometimes some of us are lucky and get a statistically significant result. Those estimates that make it beyond the 2 standard error threshold are often ridiculously large (one famous example in my line of research being Lleras-Muney’s estimates of the 10% effect of one year of schooling on mortality). The standard defense here is that IV estimates the complier-specific causal effect (which is mathematically correct). But still, I find many of the IV results (including my</p><p>4 0.78123951 <a title="2180-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-10-I_don%E2%80%99t_like_this_cartoon.html">1572 andrew gelman stats-2012-11-10-I don’t like this cartoon</a></p>
<p>Introduction: Some people pointed me to  this :
 
 
 
I am happy to see statistical theory and methods be a topic in popular culture, and of course I’m glad that, contra  Feller , the Bayesian is presented as the hero this time, but . . . . I think the lower-left panel of the cartoon unfairly misrepresents frequentist statisticians.
 
Frequentist statisticians recognize many statistical goals.  Point estimates trade off bias and variance.  Interval estimates have the goal of achieving nominal coverage and the goal of being informative.  Tests have the goals of calibration and power.  Frequentists know that no single principle applies in all settings, and this is a setting where this particular method is clearly inappropriate.  All statisticians use prior information in their statistical analysis.  Non-Bayesians express their prior information not through a probability distribution on parameters but rather through their choice of methods.  I think this non-Bayesian attitude is too restrictive, but in</p><p>5 0.77410519 <a title="2180-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-15-Static_sensitivity_analysis.html">804 andrew gelman stats-2011-07-15-Static sensitivity analysis</a></p>
<p>Introduction: This is one of my favorite ideas.  I used it in an application but have never formally studied it or written it up as a general method.
 
Sensitivity analysis is when you check how inferences change when you vary fit several different models or when you vary inputs within a model.  Sensitivity analysis is often recommended but is typically difficult to do, what with the hassle of carrying around all these different estimates.  In Bayesian inference, sensitivity analysis is associated with varying the prior distribution, which irritates me:  why not consider sensitivity to the likelihood, as that’s typically just as arbitrary as the prior while having a much larger effect on the inferences.
 
So we came up with  static sensitivity analysis , which is a way to assess sensitivity to assumptions while fitting only one model.  The idea is that Bayesian posterior simulation gives you a range of parameter values, and from these you can learn about sensitivity directly.
 
The  published exampl</p><p>6 0.76329535 <a title="2180-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-13-%E2%80%9CWhat_are_some_situations_in_which_the_classical_approach_%28or_a_naive_implementation_of_it%2C_based_on_cookbook_recipes%29_gives_worse_results_than_a_Bayesian_approach%2C_results_that_actually_impeded_the_science%3F%E2%80%9D.html">2099 andrew gelman stats-2013-11-13-“What are some situations in which the classical approach (or a naive implementation of it, based on cookbook recipes) gives worse results than a Bayesian approach, results that actually impeded the science?”</a></p>
<p>7 0.76075178 <a title="2180-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-08-Is_linear_regression_unethical_in_that_it_gives_more_weight_to_cases_that_are_far_from_the_average%3F.html">1409 andrew gelman stats-2012-07-08-Is linear regression unethical in that it gives more weight to cases that are far from the average?</a></p>
<p>8 0.74780375 <a title="2180-lsi-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-31-Somewhat_Bayesian_multilevel_modeling.html">246 andrew gelman stats-2010-08-31-Somewhat Bayesian multilevel modeling</a></p>
<p>9 0.73805273 <a title="2180-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-21-Fundamental_difficulty_of_inference_for_a_ratio_when_the_denominator_could_be_positive_or_negative.html">775 andrew gelman stats-2011-06-21-Fundamental difficulty of inference for a ratio when the denominator could be positive or negative</a></p>
<p>10 0.73571694 <a title="2180-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<p>11 0.73493689 <a title="2180-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-25-Bayes-respecting_experimental_design_and_other_things.html">1955 andrew gelman stats-2013-07-25-Bayes-respecting experimental design and other things</a></p>
<p>12 0.71472436 <a title="2180-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-13-A_question_about_AIC.html">1377 andrew gelman stats-2012-06-13-A question about AIC</a></p>
<p>13 0.70731169 <a title="2180-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-25-How_do_you_interpret_standard_errors_from_a_regression_fit_to_the_entire_population%3F.html">972 andrew gelman stats-2011-10-25-How do you interpret standard errors from a regression fit to the entire population?</a></p>
<p>14 0.70520836 <a title="2180-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-03-Statistical_methods_that_work_in_some_settings_but_not_others.html">1560 andrew gelman stats-2012-11-03-Statistical methods that work in some settings but not others</a></p>
<p>15 0.70483053 <a title="2180-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-27-Setting_up_Jitts_online.html">2041 andrew gelman stats-2013-09-27-Setting up Jitts online</a></p>
<p>16 0.70459831 <a title="2180-lsi-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-06-Bootstrap_averaging%3A_Examples_where_it_works_and_where_it_doesn%E2%80%99t_work.html">2201 andrew gelman stats-2014-02-06-Bootstrap averaging: Examples where it works and where it doesn’t work</a></p>
<p>17 0.6978724 <a title="2180-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-02-So-called_Bayesian_hypothesis_testing_is_just_as_bad_as_regular_hypothesis_testing.html">643 andrew gelman stats-2011-04-02-So-called Bayesian hypothesis testing is just as bad as regular hypothesis testing</a></p>
<p>18 0.69260788 <a title="2180-lsi-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-12-Finite-population_standard_deviation_in_a_hierarchical_model.html">464 andrew gelman stats-2010-12-12-Finite-population standard deviation in a hierarchical model</a></p>
<p>19 0.69009769 <a title="2180-lsi-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-23-More_on_those_L.A._Times_estimates_of_teacher_effectiveness.html">226 andrew gelman stats-2010-08-23-More on those L.A. Times estimates of teacher effectiveness</a></p>
<p>20 0.6835444 <a title="2180-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-28-Plain_old_everyday_Bayesianism%21.html">1829 andrew gelman stats-2013-04-28-Plain old everyday Bayesianism!</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.03), (9, 0.011), (14, 0.04), (16, 0.054), (21, 0.076), (24, 0.129), (28, 0.012), (45, 0.024), (47, 0.012), (60, 0.014), (76, 0.058), (84, 0.071), (86, 0.039), (99, 0.322)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97399342 <a title="2180-lda-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-21-Everything_I_need_to_know_about_Bayesian_statistics%2C_I_learned_in_eight_schools..html">2180 andrew gelman stats-2014-01-21-Everything I need to know about Bayesian statistics, I learned in eight schools.</a></p>
<p>Introduction: This post is by Phil.
 
I’m aware that there  are  some people who use a Bayesian approach largely because it allows them to provide a highly informative prior distribution based subjective judgment, but that is not the appeal of Bayesian methods for a lot of us practitioners. It’s disappointing and surprising, twenty years after my initial experiences, to still hear highly informed professional statisticians who think that what distinguishes Bayesian statistics from Frequentist statistics is “subjectivity” ( as seen in  a recent blog post and its comments ).
 
 My first encounter with Bayesian statistics was just over 20 years ago. I was a postdoc at Lawrence Berkeley National Laboratory, with a new PhD in theoretical atomic physics but working on various problems related to the geographical and statistical distribution of indoor radon (a naturally occurring radioactive gas that can be dangerous if present at high concentrations). One of the issues I ran into right at the start was th</p><p>2 0.96521342 <a title="2180-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-19-Further_thoughts_on_happiness_and_life_satisfaction_research.html">98 andrew gelman stats-2010-06-19-Further thoughts on happiness and life satisfaction research</a></p>
<p>Introduction: As part of my continuing research project with Grazia and Roberto, I’ve been reading papers on happiness and life satisfaction research.  I’ll share with you my thoughts on some of the published work in this area.
  

 
Alberto Alesina,, Rafael Di Tella, and Robert MacCulloch published  a paper  in 2004 called “Inequality and happiness: are Europeans and Americans different?”:
  
We study the effect of the level of inequality in society on individual well-being using a total of 123,668 answers to a survey question about “happiness.” We find that individuals have a lower tendency to report themselves happy when inequality is high, even after controlling for individual income, a large set of personal characteristics, and year and country (or, in the case of the US, state) dummies. The effect, however, is more precisely defined statistically in Europe than in the US. In addition, we find striking differences across groups. In Europe, the poor and those on the left of the political spectru</p><p>3 0.9616943 <a title="2180-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-04-%E2%80%9CTo_find_out_what_happens_when_you_change_something%2C_it_is_necessary_to_change_it.%E2%80%9D.html">186 andrew gelman stats-2010-08-04-“To find out what happens when you change something, it is necessary to change it.”</a></p>
<p>Introduction: From the classic Box, Hunter, and Hunter book.  The point of the saying is pretty clear, I think:  There are things you learn from perturbing a system that you’ll never find out from any amount of passive observation.  This is not always true–sometimes “nature” does the experiment for you–but I think it represents an important insight.
 
I’m currently writing (yet another) review article on causal inference and am planning use this quote.
 
P.S.  I find it helpful to write these reviews for a similar reason that I like to blog on certain topics over and over, each time going a bit further (I hope) than the time before.  Beyond the benefit of communicating my recommendations to new audiences, writing these sorts of reviews gives me an excuse to explore my thoughts in more rigor.
 
P.P.S.  In the original version of this blog entry, I correctly attributed the quote to Box but I incorrectly remembered it as “No understanding without manipulation.”  Karl Broman (see comment below) gave me</p><p>4 0.95865703 <a title="2180-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>Introduction: Larry Wasserman  refers  to finite mixture models as “beasts” and  writes  jokes that they “should be avoided at all costs.”
 
I’ve thought a lot about mixture models, ever since using them in an  analysis  of voting patterns that was published in 1990.  First off, I’d like to say that our model was useful so I’d prefer not to pay the cost of avoiding it.  For a quick description of our mixture model and its context, see pp. 379-380 of my  article  in the Jim Berger volume).  Actually, our case was particularly difficult because we were not even fitting a mixture model to data, we were fitting it to latent data and using the model to perform partial pooling.  My difficulties in trying to fit this model inspired our discussion of mixture models in Bayesian Data Analysis (page 109 in the second edition, in the section on “Counterexamples to the theorems”).
 
I agree with Larry that if you’re fitting a mixture model, it’s good to be aware of the problems that arise if you try to estimate</p><p>5 0.9585678 <a title="2180-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-31-Somewhat_Bayesian_multilevel_modeling.html">246 andrew gelman stats-2010-08-31-Somewhat Bayesian multilevel modeling</a></p>
<p>Introduction: Eric McGhee writes:
  
 
I’m trying to generate county-level estimates from a statewide survey of California using multilevel modeling.  I would love to learn the full Bayesian approach, but I’m on a tight schedule and worried about teaching myself something of that complexity in the time available.


I’m hoping I can use the classical approach and simulate standard errors using what you and Jennifer Hill call the “informal Bayesian” method.  This has raised a few questions:


First, what are the costs of using this approach as opposed to full Bayesian?


Second, when I use the predictive simulation as described on p. 149 of “Data Analysis” on a binary dependent variable and a sample of 2000, I get a 5%-95% range of simulation results so large as to be effectively useless (on the order of +/- 15 points).  This is true even for LA county, which has enough cases by itself (about 500) to get a standard error of about 2 points from simple disaggregation.  However, if I simulate only with t</p><p>6 0.95770699 <a title="2180-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-06-Ideas_that_spread_fast_and_slow.html">2053 andrew gelman stats-2013-10-06-Ideas that spread fast and slow</a></p>
<p>7 0.95649618 <a title="2180-lda-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-25-Classical_probability_does_not_apply_to_quantum_systems_%28causal_inference_edition%29.html">2037 andrew gelman stats-2013-09-25-Classical probability does not apply to quantum systems (causal inference edition)</a></p>
<p>8 0.95628864 <a title="2180-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-30-Infill_asymptotics_and_sprawl_asymptotics.html">1877 andrew gelman stats-2013-05-30-Infill asymptotics and sprawl asymptotics</a></p>
<p>9 0.95564318 <a title="2180-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Question_about_standard_range_for_social_science_correlations.html">257 andrew gelman stats-2010-09-04-Question about standard range for social science correlations</a></p>
<p>10 0.95461583 <a title="2180-lda-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-17-Vote_Buying%3A_Evidence_from_a_List_Experiment_in_Lebanon.html">283 andrew gelman stats-2010-09-17-Vote Buying: Evidence from a List Experiment in Lebanon</a></p>
<p>11 0.9538275 <a title="2180-lda-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>12 0.95364785 <a title="2180-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-23-Scientists_can_read_your_mind_._._._as_long_as_the%E2%80%99re_allowed_to_look_at_more_than_one_place_in_your_brain_and_then_make_a_prediction_after_seeing_what_you_actually_did.html">106 andrew gelman stats-2010-06-23-Scientists can read your mind . . . as long as the’re allowed to look at more than one place in your brain and then make a prediction after seeing what you actually did</a></p>
<p>13 0.95359975 <a title="2180-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-09-The_difference_between_significant_and_not_significant%E2%80%A6.html">897 andrew gelman stats-2011-09-09-The difference between significant and not significant…</a></p>
<p>14 0.95277208 <a title="2180-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-07-Descriptive_statistics%2C_causal_inference%2C_and_story_time.html">789 andrew gelman stats-2011-07-07-Descriptive statistics, causal inference, and story time</a></p>
<p>15 0.9522543 <a title="2180-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-26-Age_and_happiness%3A__The_pattern_isn%E2%80%99t_as_clear_as_you_might_think.html">486 andrew gelman stats-2010-12-26-Age and happiness:  The pattern isn’t as clear as you might think</a></p>
<p>16 0.9518699 <a title="2180-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-29-A_Ph.D._thesis_is_not_really_a_marathon.html">1351 andrew gelman stats-2012-05-29-A Ph.D. thesis is not really a marathon</a></p>
<p>17 0.9514609 <a title="2180-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-13-Philosophy_of_Bayesian_statistics%3A__my_reactions_to_Wasserman.html">1165 andrew gelman stats-2012-02-13-Philosophy of Bayesian statistics:  my reactions to Wasserman</a></p>
<p>18 0.95130086 <a title="2180-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-09-Coming_to_agreement_on_philosophy_of_statistics.html">1205 andrew gelman stats-2012-03-09-Coming to agreement on philosophy of statistics</a></p>
<p>19 0.95129144 <a title="2180-lda-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-04-%E2%80%9CDogs_are_sensitive_to_small_variations_of_the_Earth%E2%80%99s_magnetic_field%E2%80%9D.html">2159 andrew gelman stats-2014-01-04-“Dogs are sensitive to small variations of the Earth’s magnetic field”</a></p>
<p>20 0.95114076 <a title="2180-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-15-Still_more_Mr._P_in_public_health.html">770 andrew gelman stats-2011-06-15-Still more Mr. P in public health</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
