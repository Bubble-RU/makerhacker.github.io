<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2291 andrew gelman stats-2014-04-14-Transitioning to Stan</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2014" href="../home/andrew_gelman_stats-2014_home.html">andrew_gelman_stats-2014</a> <a title="andrew_gelman_stats-2014-2291" href="#">andrew_gelman_stats-2014-2291</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2291 andrew gelman stats-2014-04-14-Transitioning to Stan</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2014-2291-html" href="http://andrewgelman.com/2014/04/14/transitioning-stan/">html</a></p><p>Introduction: Kevin Cartier writes:
  
I’ve been happily using R for a number of years now and recently came across Stan. Looks big and powerful, so I’d like to pick an appropriate project and try it out. I wondered if you could point me to a link or document that goes into the motivation for this tool (aside from the Stan user doc)?  What I’d like to understand is, at what point might you look at an emergent R project and advise, “You know, that thing you’re trying to do would be a whole lot easier/simpler/more straightforward to implement with Stan.” (or words to that effect).
  
My reply:  For my collaborators in political science, Stan has been most useful for models where the data set is not huge (e.g., we might have 10,000 data points or 50,000 data points but not 10 million) but where the model is somewhat complex (for example, a model with latent time series structure).  The point is that the model has enough parameters and uncertainty that you’ll want to do full Bayes (rather than some sort</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Kevin Cartier writes:    I’ve been happily using R for a number of years now and recently came across Stan. [sent-1, score-0.147]
</p><p>2 Looks big and powerful, so I’d like to pick an appropriate project and try it out. [sent-2, score-0.324]
</p><p>3 I wondered if you could point me to a link or document that goes into the motivation for this tool (aside from the Stan user doc)? [sent-3, score-1.021]
</p><p>4 What I’d like to understand is, at what point might you look at an emergent R project and advise, “You know, that thing you’re trying to do would be a whole lot easier/simpler/more straightforward to implement with Stan. [sent-4, score-0.649]
</p><p>5 My reply:  For my collaborators in political science, Stan has been most useful for models where the data set is not huge (e. [sent-6, score-0.272]
</p><p>6 , we might have 10,000 data points or 50,000 data points but not 10 million) but where the model is somewhat complex (for example, a model with latent time series structure). [sent-8, score-0.938]
</p><p>7 The point is that the model has enough parameters and uncertainty that you’ll want to do full Bayes (rather than some sort of point estimate). [sent-9, score-0.608]
</p><p>8 At that point, Stan is a winner compared to programming one’s own Monte Carlo algorithm. [sent-10, score-0.302]
</p><p>9 We (the Stan team) should really prepare a document with a bunch of examples where Stan is a win, in one way or another. [sent-11, score-0.564]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stan', 0.488), ('document', 0.352), ('point', 0.17), ('project', 0.159), ('happily', 0.147), ('preparing', 0.144), ('advise', 0.142), ('doc', 0.142), ('prepare', 0.138), ('straightforward', 0.136), ('kevin', 0.13), ('winner', 0.125), ('wondered', 0.124), ('collaborators', 0.116), ('carlo', 0.116), ('implement', 0.116), ('latent', 0.113), ('improving', 0.111), ('monte', 0.111), ('model', 0.111), ('powerful', 0.108), ('user', 0.107), ('programming', 0.104), ('motivation', 0.101), ('points', 0.098), ('tool', 0.096), ('blogging', 0.096), ('win', 0.095), ('structure', 0.092), ('million', 0.09), ('somewhat', 0.089), ('aside', 0.087), ('pick', 0.087), ('complex', 0.087), ('team', 0.086), ('bayes', 0.082), ('uncertainty', 0.082), ('spend', 0.081), ('huge', 0.08), ('takes', 0.08), ('series', 0.079), ('appropriate', 0.078), ('rather', 0.077), ('data', 0.076), ('parameters', 0.075), ('bunch', 0.074), ('compared', 0.073), ('words', 0.072), ('goes', 0.071), ('whole', 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="2291-tfidf-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-14-Transitioning_to_Stan.html">2291 andrew gelman stats-2014-04-14-Transitioning to Stan</a></p>
<p>Introduction: Kevin Cartier writes:
  
I’ve been happily using R for a number of years now and recently came across Stan. Looks big and powerful, so I’d like to pick an appropriate project and try it out. I wondered if you could point me to a link or document that goes into the motivation for this tool (aside from the Stan user doc)?  What I’d like to understand is, at what point might you look at an emergent R project and advise, “You know, that thing you’re trying to do would be a whole lot easier/simpler/more straightforward to implement with Stan.” (or words to that effect).
  
My reply:  For my collaborators in political science, Stan has been most useful for models where the data set is not huge (e.g., we might have 10,000 data points or 50,000 data points but not 10 million) but where the model is somewhat complex (for example, a model with latent time series structure).  The point is that the model has enough parameters and uncertainty that you’ll want to do full Bayes (rather than some sort</p><p>2 0.33249018 <a title="2291-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-A_Stan_is_Born.html">1475 andrew gelman stats-2012-08-30-A Stan is Born</a></p>
<p>Introduction: Stan 1.0.0 and RStan 1.0.0 
 
It’s official.  The Stan Development Team is happy to announce the first stable versions of Stan and RStan.  
 
 What is (R)Stan? 
 
Stan is an open-source package for obtaining Bayesian inference using the No-U-Turn sampler, a variant of Hamiltonian Monte Carlo.  It’s sort of like BUGS, but with a different language for expressing models and a different sampler for sampling from their posteriors. 
 
RStan is the R interface to Stan.  
 
 Stan Home Page 
 
Stan’s home page is:     http://mc-stan.org/    
 
It links everything you need to get started running Stan from the command line, from R, or from C++, including full step-by-step install instructions, a detailed user’s guide and reference manual for the modeling language, and tested ports of most of the BUGS examples.
 
 Peruse the Manual 
 
If you’d like to learn more, the   Stan User’s Guide and Reference Manual   is the place to start.</p><p>3 0.26701081 <a title="2291-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-04-PyStan%21.html">1748 andrew gelman stats-2013-03-04-PyStan!</a></p>
<p>Introduction: Stan  is written in C++ and can be run from the command line and from R.  We’d like for  Python  users to be able to run Stan as well.  If anyone is interested in doing this, please let us know and we’d be happy to work with you on it.
 
Stan, like Python, is completely free and open-source.
 
P.S.  Because Stan is open-source, it of course would also be possible for people to translate Stan into Python, or to take whatever features they like from Stan and incorporate them into a Python package.  That’s fine too.  But we think it would make sense in addition for users to be able to run Stan directly from Python, in the same way that it can be run from R.</p><p>4 0.25857335 <a title="2291-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-16-Stantastic%21.html">1580 andrew gelman stats-2012-11-16-Stantastic!</a></p>
<p>Introduction: Richard McElreath writes:
  
I’ve been translating a few ongoing data analysis projects into  Stan  code, mostly with success. The most important for me right now has been a hierarchical zero-inflated gamma problem. This a “hurdle” model, in which a bernoulli GLM produces zeros/nonzeros, and then a gamma GLM produces the nonzero values, using varying effects correlated with those in the bernoulli process.


The data are 20 years of human foraging returns from a subsistence hunting population in Paraguay (the Ache), comprising about 15k hunts in total (Hill & Kintigh. 2009. Current Anthropology 50:369-377). Observed values are kilograms of meat returned to camp. The more complex models contain a 147-by-9 matrix of varying effects (147 unique hunters), as well as imputation of missing values.


Originally, I had written the sampler myself in raw R code. It was very slow, but I knew what it was doing at least. Just before Stan version 1.0 was released, I had managed to get JAGS to do it a</p><p>5 0.23895 <a title="2291-tfidf-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-07-My_recent_debugging_experience.html">2161 andrew gelman stats-2014-01-07-My recent debugging experience</a></p>
<p>Introduction: OK, so this sort of thing happens sometimes.  I was working on a new idea (still working on it; if it ultimately works out—or if it doesn’t—I’ll let you know) and as part of it I was fitting little models in Stan, in a loop.  I thought it would make sense to start with linear regression with normal priors and known data variance, because then the exact solution is Gaussian and I can also work with the problem analytically.  So I programmed up the algorithm and, no surprise, it didn’t work.  I went through my R code, put in print statements here and there, and cleared out bug after bug until at least it stopped crashing.  But the algorithm still wasn’t doing what it was supposed to do.
 
So I decided to do something simpler, and just check that the Stan linear regression gave the same answer as the analytic posterior distribution:  I ran Stan for tons of iterations, then computed the sample mean and variance of the simulations.  It was an example with two coefficients—I’d originally cho</p><p>6 0.2368692 <a title="2291-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-22-My_talks_that_were_scheduled_for_Tues_at_the_Data_Skeptics_meetup_and_Wed_at_the_Open_Statistical_Programming_meetup.html">1950 andrew gelman stats-2013-07-22-My talks that were scheduled for Tues at the Data Skeptics meetup and Wed at the Open Statistical Programming meetup</a></p>
<p>7 0.18942298 <a title="2291-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-27-%28R-Py-Cmd%29Stan_2.1.0.html">2150 andrew gelman stats-2013-12-27-(R-Py-Cmd)Stan 2.1.0</a></p>
<p>8 0.17947638 <a title="2291-tfidf-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-Stan_Model_of_the_Week%3A_Hierarchical_Modeling_of_Supernovas.html">2299 andrew gelman stats-2014-04-21-Stan Model of the Week: Hierarchical Modeling of Supernovas</a></p>
<p>9 0.17801523 <a title="2291-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-My_talk_at_MIT_on_Thurs_11_Oct.html">1528 andrew gelman stats-2012-10-10-My talk at MIT on Thurs 11 Oct</a></p>
<p>10 0.17336471 <a title="2291-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-10-Schiminovich_is_on_The_Simpsons.html">2096 andrew gelman stats-2013-11-10-Schiminovich is on The Simpsons</a></p>
<p>11 0.17204531 <a title="2291-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-21-Bayes_related.html">1948 andrew gelman stats-2013-07-21-Bayes related</a></p>
<p>12 0.16902037 <a title="2291-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-05-Stan_%28quietly%29_passes_512_people_on_the_users_list.html">2124 andrew gelman stats-2013-12-05-Stan (quietly) passes 512 people on the users list</a></p>
<p>13 0.16715962 <a title="2291-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-07-Robust_logistic_regression.html">1886 andrew gelman stats-2013-06-07-Robust logistic regression</a></p>
<p>14 0.16647477 <a title="2291-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-13-Stan%21.html">1855 andrew gelman stats-2013-05-13-Stan!</a></p>
<p>15 0.16604115 <a title="2291-tfidf-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-13-CmdStan%2C_RStan%2C_PyStan_v2.2.0.html">2209 andrew gelman stats-2014-02-13-CmdStan, RStan, PyStan v2.2.0</a></p>
<p>16 0.16247204 <a title="2291-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-28-Migrating_from_dot_to_underscore.html">1472 andrew gelman stats-2012-08-28-Migrating from dot to underscore</a></p>
<p>17 0.16090706 <a title="2291-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-23-Scalable_Stan.html">2035 andrew gelman stats-2013-09-23-Scalable Stan</a></p>
<p>18 0.15855095 <a title="2291-tfidf-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-07-Stan_users_meetup_next_week.html">2325 andrew gelman stats-2014-05-07-Stan users meetup next week</a></p>
<p>19 0.15694869 <a title="2291-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-17-Stan_and_RStan_1.1.0.html">1627 andrew gelman stats-2012-12-17-Stan and RStan 1.1.0</a></p>
<p>20 0.14470659 <a title="2291-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-20-Stan%3A_A_%28Bayesian%29_Directed_Graphical_Model_Compiler.html">1131 andrew gelman stats-2012-01-20-Stan: A (Bayesian) Directed Graphical Model Compiler</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.209), (1, 0.086), (2, -0.036), (3, 0.105), (4, 0.131), (5, 0.101), (6, -0.001), (7, -0.268), (8, -0.068), (9, -0.14), (10, -0.152), (11, 0.002), (12, -0.11), (13, -0.086), (14, 0.053), (15, -0.037), (16, -0.045), (17, 0.051), (18, -0.013), (19, -0.0), (20, -0.056), (21, -0.036), (22, -0.081), (23, -0.01), (24, -0.013), (25, -0.021), (26, 0.032), (27, -0.045), (28, -0.079), (29, -0.02), (30, -0.01), (31, -0.032), (32, -0.024), (33, -0.051), (34, -0.017), (35, 0.089), (36, 0.036), (37, -0.006), (38, 0.017), (39, -0.045), (40, 0.017), (41, 0.006), (42, -0.029), (43, -0.036), (44, 0.005), (45, 0.011), (46, 0.006), (47, 0.002), (48, -0.049), (49, 0.032)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9509005 <a title="2291-lsi-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-14-Transitioning_to_Stan.html">2291 andrew gelman stats-2014-04-14-Transitioning to Stan</a></p>
<p>Introduction: Kevin Cartier writes:
  
I’ve been happily using R for a number of years now and recently came across Stan. Looks big and powerful, so I’d like to pick an appropriate project and try it out. I wondered if you could point me to a link or document that goes into the motivation for this tool (aside from the Stan user doc)?  What I’d like to understand is, at what point might you look at an emergent R project and advise, “You know, that thing you’re trying to do would be a whole lot easier/simpler/more straightforward to implement with Stan.” (or words to that effect).
  
My reply:  For my collaborators in political science, Stan has been most useful for models where the data set is not huge (e.g., we might have 10,000 data points or 50,000 data points but not 10 million) but where the model is somewhat complex (for example, a model with latent time series structure).  The point is that the model has enough parameters and uncertainty that you’ll want to do full Bayes (rather than some sort</p><p>2 0.94785041 <a title="2291-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-16-Stantastic%21.html">1580 andrew gelman stats-2012-11-16-Stantastic!</a></p>
<p>Introduction: Richard McElreath writes:
  
I’ve been translating a few ongoing data analysis projects into  Stan  code, mostly with success. The most important for me right now has been a hierarchical zero-inflated gamma problem. This a “hurdle” model, in which a bernoulli GLM produces zeros/nonzeros, and then a gamma GLM produces the nonzero values, using varying effects correlated with those in the bernoulli process.


The data are 20 years of human foraging returns from a subsistence hunting population in Paraguay (the Ache), comprising about 15k hunts in total (Hill & Kintigh. 2009. Current Anthropology 50:369-377). Observed values are kilograms of meat returned to camp. The more complex models contain a 147-by-9 matrix of varying effects (147 unique hunters), as well as imputation of missing values.


Originally, I had written the sampler myself in raw R code. It was very slow, but I knew what it was doing at least. Just before Stan version 1.0 was released, I had managed to get JAGS to do it a</p><p>3 0.91911572 <a title="2291-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-27-%28R-Py-Cmd%29Stan_2.1.0.html">2150 andrew gelman stats-2013-12-27-(R-Py-Cmd)Stan 2.1.0</a></p>
<p>Introduction: We’re happy to announce the release of Stan C++, CmdStan, 
RStan, and PyStan 2.1.0.  This is a minor feature release, 
but it is also an important bug fix release.  As always, the 
place to start is the (all new) Stan web pages:
 
 http://mc-stan.org 
 
 
 
 Major Bug in 2.0.0, 2.0.1 
 
Stan 2.0.0 and Stan 2.0.1 introduced a bug in the implementation 
of the NUTS criterion that led to poor tail exploration and 
thus biased the posterior uncertainty downward.  There was no 
bug in NUTS in Stan 1.3 or earlier, and 2.1 has been extensively tested 
and tests put in place so this problem will not recur.
 
If you are using Stan 2.0.0 or 2.0.1, you should switch to 2.1.0 as 
soon as possible and rerun any models you care about.
 
 
 
 New Target Acceptance Rate Default for Stan 2.1.0 
  Another big change aimed at reducing posterior estimation bias 
was an increase in the target acceptance rate during adaptation 
from 0.65 to 0.80.  The bad news is that iterations will take 
around 50% longer</p><p>4 0.91754824 <a title="2291-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-04-PyStan%21.html">1748 andrew gelman stats-2013-03-04-PyStan!</a></p>
<p>Introduction: Stan  is written in C++ and can be run from the command line and from R.  We’d like for  Python  users to be able to run Stan as well.  If anyone is interested in doing this, please let us know and we’d be happy to work with you on it.
 
Stan, like Python, is completely free and open-source.
 
P.S.  Because Stan is open-source, it of course would also be possible for people to translate Stan into Python, or to take whatever features they like from Stan and incorporate them into a Python package.  That’s fine too.  But we think it would make sense in addition for users to be able to run Stan directly from Python, in the same way that it can be run from R.</p><p>5 0.91525447 <a title="2291-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-A_Stan_is_Born.html">1475 andrew gelman stats-2012-08-30-A Stan is Born</a></p>
<p>Introduction: Stan 1.0.0 and RStan 1.0.0 
 
It’s official.  The Stan Development Team is happy to announce the first stable versions of Stan and RStan.  
 
 What is (R)Stan? 
 
Stan is an open-source package for obtaining Bayesian inference using the No-U-Turn sampler, a variant of Hamiltonian Monte Carlo.  It’s sort of like BUGS, but with a different language for expressing models and a different sampler for sampling from their posteriors. 
 
RStan is the R interface to Stan.  
 
 Stan Home Page 
 
Stan’s home page is:     http://mc-stan.org/    
 
It links everything you need to get started running Stan from the command line, from R, or from C++, including full step-by-step install instructions, a detailed user’s guide and reference manual for the modeling language, and tested ports of most of the BUGS examples.
 
 Peruse the Manual 
 
If you’d like to learn more, the   Stan User’s Guide and Reference Manual   is the place to start.</p><p>6 0.90257663 <a title="2291-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-14-The_joys_of_working_in_the_public_domain.html">712 andrew gelman stats-2011-05-14-The joys of working in the public domain</a></p>
<p>7 0.88693446 <a title="2291-lsi-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-07-My_recent_debugging_experience.html">2161 andrew gelman stats-2014-01-07-My recent debugging experience</a></p>
<p>8 0.88199347 <a title="2291-lsi-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-07-Stan_users_meetup_next_week.html">2325 andrew gelman stats-2014-05-07-Stan users meetup next week</a></p>
<p>9 0.88073403 <a title="2291-lsi-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-04-Stan_%28%26_JAGS%29_Tutorial_on_Linear_Mixed_Models.html">2318 andrew gelman stats-2014-05-04-Stan (& JAGS) Tutorial on Linear Mixed Models</a></p>
<p>10 0.8776353 <a title="2291-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-05-Stan_%28quietly%29_passes_512_people_on_the_users_list.html">2124 andrew gelman stats-2013-12-05-Stan (quietly) passes 512 people on the users list</a></p>
<p>11 0.87488234 <a title="2291-lsi-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-13-CmdStan%2C_RStan%2C_PyStan_v2.2.0.html">2209 andrew gelman stats-2014-02-13-CmdStan, RStan, PyStan v2.2.0</a></p>
<p>12 0.85369909 <a title="2291-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-17-Stan_and_RStan_1.1.0.html">1627 andrew gelman stats-2012-12-17-Stan and RStan 1.1.0</a></p>
<p>13 0.84924382 <a title="2291-lsi-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-10-Stan_Model_of_the_Week%3A__PK_Calculation_of_IV_and_Oral_Dosing.html">2242 andrew gelman stats-2014-03-10-Stan Model of the Week:  PK Calculation of IV and Oral Dosing</a></p>
<p>14 0.8402766 <a title="2291-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-30-Stan_uses_Nuts%21.html">1036 andrew gelman stats-2011-11-30-Stan uses Nuts!</a></p>
<p>15 0.83872581 <a title="2291-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-13-Stan_at_NIPS_2012_Workshop_on_Probabilistic_Programming.html">1576 andrew gelman stats-2012-11-13-Stan at NIPS 2012 Workshop on Probabilistic Programming</a></p>
<p>16 0.8342219 <a title="2291-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-10-Schiminovich_is_on_The_Simpsons.html">2096 andrew gelman stats-2013-11-10-Schiminovich is on The Simpsons</a></p>
<p>17 0.81045657 <a title="2291-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-23-Scalable_Stan.html">2035 andrew gelman stats-2013-09-23-Scalable Stan</a></p>
<p>18 0.81032169 <a title="2291-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-13-Stan%21.html">1855 andrew gelman stats-2013-05-13-Stan!</a></p>
<p>19 0.80296987 <a title="2291-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-12-Samplers_for_Big_Science%3A__emcee_and_BAT.html">2020 andrew gelman stats-2013-09-12-Samplers for Big Science:  emcee and BAT</a></p>
<p>20 0.79864919 <a title="2291-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-28-Migrating_from_dot_to_underscore.html">1472 andrew gelman stats-2012-08-28-Migrating from dot to underscore</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(9, 0.039), (16, 0.028), (18, 0.017), (19, 0.017), (21, 0.015), (24, 0.152), (58, 0.018), (59, 0.064), (77, 0.012), (85, 0.017), (86, 0.08), (89, 0.038), (99, 0.403)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98645532 <a title="2291-lda-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-14-Transitioning_to_Stan.html">2291 andrew gelman stats-2014-04-14-Transitioning to Stan</a></p>
<p>Introduction: Kevin Cartier writes:
  
I’ve been happily using R for a number of years now and recently came across Stan. Looks big and powerful, so I’d like to pick an appropriate project and try it out. I wondered if you could point me to a link or document that goes into the motivation for this tool (aside from the Stan user doc)?  What I’d like to understand is, at what point might you look at an emergent R project and advise, “You know, that thing you’re trying to do would be a whole lot easier/simpler/more straightforward to implement with Stan.” (or words to that effect).
  
My reply:  For my collaborators in political science, Stan has been most useful for models where the data set is not huge (e.g., we might have 10,000 data points or 50,000 data points but not 10 million) but where the model is somewhat complex (for example, a model with latent time series structure).  The point is that the model has enough parameters and uncertainty that you’ll want to do full Bayes (rather than some sort</p><p>2 0.97965729 <a title="2291-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-14-Wickham_R_short_course.html">1009 andrew gelman stats-2011-11-14-Wickham R short course</a></p>
<p>Introduction: Hadley  writes:
  
I [Hadley] am going to be teaching an R development master class in New York City on Dec 
12-13. The basic idea of the class is to help you write better code, 
focused on the mantra of “do not repeat yourself”. In day one you will 
learn powerful new tools of abstraction, allowing you to solve a wider 
range of problems with fewer lines of code. Day two will teach you how 
to make packages, the fundamental unit of code distribution in R, 
allowing others to save time by allowing them to use your code.


To get the most out of this course, you should have some experience 
programming in R already: you should be familiar with writing 
functions, and the basic data structures of R: vectors, matrices, 
arrays, lists and data frames. You will find the course particularly 
useful if you’re an experienced R user looking to take the next step, 
or if you’re moving to R from other programming languages and you want 
to quickly get up to speed with R’s unique features. A coupl</p><p>3 0.97930515 <a title="2291-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-31-Analyzing_the_entire_population_rather_than_a_sample.html">383 andrew gelman stats-2010-10-31-Analyzing the entire population rather than a sample</a></p>
<p>Introduction: Lee Mobley writes:
  
I recently read what you posted on your blog  How does statistical analysis differ when analyzing the entire population rather than a sample? 


What you said in the blog accords with my training in econometrics.  However I am concerned about a new wrinkle on this problem that derives from multilevel modeling.
      
We are analyzing multilevel models of the probability of using cancer screening for the entire Medicare population. I argue that every state has different systems in place (politics, cancer control efforts, culture, insurance regulations, etc) so that essentially a different probability generating mechanism is in place for each state. Thus I estimate 50 separate regressions for the populations in each state, and then note and map the variability in the effect estimates (slope parameters) for each covariate.


Reviewers argue that I should be using random slopes modeling, pooling all individuals in all states together. I am familiar with this approach</p><p>4 0.9786033 <a title="2291-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-07-Not_much_difference_between_communicating_to_self_and_communicating_to_others.html">1408 andrew gelman stats-2012-07-07-Not much difference between communicating to self and communicating to others</a></p>
<p>Introduction: Thomas Basbøll  writes :
  
[Advertising executive] Russell Davies wrote a  blog post  called “The Tyranny of the Big Idea”. His five-point procedure begins:

 
Start doing stuff. Start executing things which seem right. Do it quickly and do it often. Don’t cling onto anything, good or bad. Don’t repeat much. Take what was good and do it differently.
 

And ends with: “And something else and something else.”
  
This inspires several thoughts, which I’ll take advantage of the blog format to present with no attempt to be cohesively organized.
 
1.  My first concern is the extent to which productivity-enhancing advice such as Davies’s (and Basbøll’s) is zero or even  negative-sum , just helping people in the rat race.  But, upon reflection, I’d rate the recommendations as positive-sum.  If people learn to write better and be more productive, that’s not (necessarily) just positional.
 
2.  Blogging fits with the “Do it quickly and do it often” advice.
 
3.  I wonder what Basbøll thinks abo</p><p>5 0.9778257 <a title="2291-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-13-A_question_about_AIC.html">1377 andrew gelman stats-2012-06-13-A question about AIC</a></p>
<p>Introduction: Jacob Oaknin asks: 
  
  
 Akaike ‘s selection criterion is often justified on the basis of the empirical risk of a ML estimate being a biased estimate of the true generalization error of a parametric family, say the family, S_m, of linear regressors on a m-dimensional variable x=(x_1,..,x_m) with gaussian noise independent of x (for instance in “Unifying the derivations for the Akaike and Corrected Akaike information criteria”, by J.E.Cavanaugh, Statistics and Probability Letters, vol. 33, 1997, pp. 201-208).


On the other hand, the family S_m is known to have finite VC-dimension (VC = m+1), and this fact should grant  that empirical risk minimizer is asymtotically consistent regardless of the underlying probability distribution, and in particular for the assumed gaussian distribution of noise(“An overview of statistical learning theory”, by V.N.Vapnik, IEEE Transactions On Neural Networks, vol. 10, No. 5, 1999, pp. 988-999)


What am I missing?
  
My reply:  I’m no expert on AIC so</p><p>6 0.97429705 <a title="2291-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-03-Double_standard%3F__Plagiarizing_journos_get_slammed%2C_plagiarizing_profs_just_shrug_it_off.html">1442 andrew gelman stats-2012-08-03-Double standard?  Plagiarizing journos get slammed, plagiarizing profs just shrug it off</a></p>
<p>7 0.97389901 <a title="2291-lda-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-11-Gladwell_and_Chabris%2C_David_and_Goliath%2C_and_science_writing_as_stone_soup.html">2058 andrew gelman stats-2013-10-11-Gladwell and Chabris, David and Goliath, and science writing as stone soup</a></p>
<p>8 0.97322124 <a title="2291-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-27-Setting_up_Jitts_online.html">2041 andrew gelman stats-2013-09-27-Setting up Jitts online</a></p>
<p>9 0.97312576 <a title="2291-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-17-Hierarchical-multilevel_modeling_with_%E2%80%9Cbig_data%E2%80%9D.html">1267 andrew gelman stats-2012-04-17-Hierarchical-multilevel modeling with “big data”</a></p>
<p>10 0.97291386 <a title="2291-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-18-Postdoc_positions_at_Microsoft_Research_%E2%80%93_NYC.html">1630 andrew gelman stats-2012-12-18-Postdoc positions at Microsoft Research – NYC</a></p>
<p>11 0.97255611 <a title="2291-lda-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-08-Regression_and_causality_and_variable_ordering.html">2364 andrew gelman stats-2014-06-08-Regression and causality and variable ordering</a></p>
<p>12 0.97224599 <a title="2291-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-02-Fishing_for_cherries.html">1746 andrew gelman stats-2013-03-02-Fishing for cherries</a></p>
<p>13 0.9721303 <a title="2291-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-25-The_problem_with_realistic_advice%3F.html">1428 andrew gelman stats-2012-07-25-The problem with realistic advice?</a></p>
<p>14 0.97123635 <a title="2291-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-26-Lottery_probability_update.html">731 andrew gelman stats-2011-05-26-Lottery probability update</a></p>
<p>15 0.97117448 <a title="2291-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>16 0.97087908 <a title="2291-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-03-Psychology_researchers_discuss_ESP.html">691 andrew gelman stats-2011-05-03-Psychology researchers discuss ESP</a></p>
<p>17 0.97078168 <a title="2291-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-25-Continuous_variables_in_Bayesian_networks.html">1228 andrew gelman stats-2012-03-25-Continuous variables in Bayesian networks</a></p>
<p>18 0.97061884 <a title="2291-lda-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-28-More_on_Bayesian_deduction-induction.html">114 andrew gelman stats-2010-06-28-More on Bayesian deduction-induction</a></p>
<p>19 0.97047961 <a title="2291-lda-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-24-Parables_vs._stories.html">2184 andrew gelman stats-2014-01-24-Parables vs. stories</a></p>
<p>20 0.9704656 <a title="2291-lda-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-21-Random_matrices_in_the_news.html">2258 andrew gelman stats-2014-03-21-Random matrices in the news</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
