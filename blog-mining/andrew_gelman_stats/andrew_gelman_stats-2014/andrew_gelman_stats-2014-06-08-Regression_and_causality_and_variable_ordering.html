<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2364 andrew gelman stats-2014-06-08-Regression and causality and variable ordering</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2014" href="../home/andrew_gelman_stats-2014_home.html">andrew_gelman_stats-2014</a> <a title="andrew_gelman_stats-2014-2364" href="#">andrew_gelman_stats-2014-2364</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2364 andrew gelman stats-2014-06-08-Regression and causality and variable ordering</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2014-2364-html" href="http://andrewgelman.com/2014/06/08/regression-causality-variable-ordering/">html</a></p><p>Introduction: Bill Harris wrote in with a question:
  
David Hogg points out in one of his general articles on data modeling that regression assumptions require one to put the variable with the highest variance in the ‘y’ position and the variable you know best (lowest variance) in the ‘x’ position.  As he points out, others speak of independent and dependent variables, as if causality determined the form of a regression formula.  In a quick scan of ARM and BDA, I don’t see clear advice, but I do see the use of ‘independent’ and ‘dependent.’


I recently did a model over data in which we know the ‘effect’ pretty well (we measure it), while we know the ’cause’ less well (it’s estimated by people who only need to get it approximately correct).  A model of the form ’cause ~ effect’ fit visually much better than one of the form ‘effect ~ cause’, but interpreting it seems challenging.  


For a simplistic example, let the effect be energy use in a building for cooling (E), and let the cause be outdoor ai</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Bill Harris wrote in with a question:    David Hogg points out in one of his general articles on data modeling that regression assumptions require one to put the variable with the highest variance in the ‘y’ position and the variable you know best (lowest variance) in the ‘x’ position. [sent-1, score-0.515]
</p><p>2 As he points out, others speak of independent and dependent variables, as if causality determined the form of a regression formula. [sent-2, score-0.429]
</p><p>3 In a quick scan of ARM and BDA, I don’t see clear advice, but I do see the use of ‘independent’ and ‘dependent. [sent-3, score-0.294]
</p><p>4 ’   I recently did a model over data in which we know the ‘effect’ pretty well (we measure it), while we know the ’cause’ less well (it’s estimated by people who only need to get it approximately correct). [sent-4, score-0.304]
</p><p>5 A model of the form ’cause ~ effect’ fit visually much better than one of the form ‘effect ~ cause’, but interpreting it seems challenging. [sent-5, score-0.323]
</p><p>6 For a simplistic example, let the effect be energy use in a building for cooling (E), and let the cause be outdoor air temperature (T). [sent-6, score-1.623]
</p><p>7 We typically get T at a “nearby” location (within 5-10 miles, perhaps), but we know microclimates cause that to be in error for what counts at the particular building. [sent-8, score-0.402]
</p><p>8 So ‘E ~ T’ makes sense, but ‘T ~ E’ may violate fewer regression assumptions. [sent-9, score-0.171]
</p><p>9 At least in the short term and over a volume that’s bigger than covered by the exhaust plume from the air conditioner, the natural interpretation of that (“the outdoor air temperature is a function of the energy you consume to cool the building”) is hard to swallow. [sent-10, score-1.49]
</p><p>10 In a complete modeling sense, I see modeling the uncertainty in x and y, but often a simpler ‘lm(y ~ x)’ suffices. [sent-12, score-0.176]
</p><p>11 I replied:   Do we really use the terms “independent” and “dependent” variables in this sense in ARM and BDA? [sent-15, score-0.255]
</p><p>12 In ARM I think we make it pretty clear that regression is about predicting y from x. [sent-19, score-0.209]
</p><p>13 Sometimes people want to predict y from x, but x is not observed, all we that is available is z which is some noisy measure of x. [sent-21, score-0.182]
</p><p>14 In this case one can fit a measurement error model. [sent-22, score-0.284]
</p><p>15 37, which seemed crystal clear until I read Hogg (below); then it wasn’t clear if the predictor on p. [sent-26, score-0.281]
</p><p>16 37 of ARM really means what I think it means (energy use doesn’t drive outside air temperature, at least on the short term, but I /could/ interpret it as energy use can be used to /predict/ outdoor air temperature more accurately than temperature can predict energy use). [sent-27, score-2.276]
</p><p>17 mention that you should regress x on y, not y on x, in those cases if you don’t model the measurement error. [sent-31, score-0.298]
</p><p>18 Perhaps that’s something to cover more fully in a new ARM: is there anything to do in particular when working up from a simple lm() to a full-blown model of measurement error (or perhaps you have and I forgot or missed it). [sent-33, score-0.571]
</p><p>19 My reply:  We’ll definitely cover this in the next edition of ARM. [sent-34, score-0.192]
</p><p>20 We’ll do it in Stan, where it’s very easy to write a measurement error model. [sent-35, score-0.284]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('arm', 0.295), ('lm', 0.277), ('temperature', 0.258), ('air', 0.246), ('hogg', 0.241), ('energy', 0.234), ('outdoor', 0.22), ('cause', 0.217), ('measurement', 0.166), ('independent', 0.121), ('error', 0.118), ('variance', 0.114), ('use', 0.114), ('regression', 0.108), ('dependent', 0.106), ('bda', 0.105), ('measure', 0.104), ('clear', 0.101), ('effect', 0.1), ('edition', 0.1), ('form', 0.094), ('cover', 0.092), ('modeling', 0.088), ('interpret', 0.087), ('building', 0.082), ('sense', 0.08), ('crystal', 0.079), ('cooling', 0.079), ('scan', 0.079), ('predict', 0.078), ('bill', 0.078), ('exhaust', 0.076), ('consume', 0.073), ('simplistic', 0.073), ('term', 0.072), ('visually', 0.069), ('variable', 0.069), ('know', 0.067), ('model', 0.066), ('regress', 0.066), ('forgot', 0.065), ('short', 0.065), ('harris', 0.064), ('perhaps', 0.064), ('violate', 0.063), ('glm', 0.063), ('variables', 0.061), ('miles', 0.061), ('means', 0.061), ('nearby', 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="2364-tfidf-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-08-Regression_and_causality_and_variable_ordering.html">2364 andrew gelman stats-2014-06-08-Regression and causality and variable ordering</a></p>
<p>Introduction: Bill Harris wrote in with a question:
  
David Hogg points out in one of his general articles on data modeling that regression assumptions require one to put the variable with the highest variance in the ‘y’ position and the variable you know best (lowest variance) in the ‘x’ position.  As he points out, others speak of independent and dependent variables, as if causality determined the form of a regression formula.  In a quick scan of ARM and BDA, I don’t see clear advice, but I do see the use of ‘independent’ and ‘dependent.’


I recently did a model over data in which we know the ‘effect’ pretty well (we measure it), while we know the ’cause’ less well (it’s estimated by people who only need to get it approximately correct).  A model of the form ’cause ~ effect’ fit visually much better than one of the form ‘effect ~ cause’, but interpreting it seems challenging.  


For a simplistic example, let the effect be energy use in a building for cooling (E), and let the cause be outdoor ai</p><p>2 0.20604686 <a title="2364-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-14-Another_day%2C_another_stats_postdoc.html">906 andrew gelman stats-2011-09-14-Another day, another stats postdoc</a></p>
<p>Introduction: This post is from Phil Price.  I work in the Environmental Energy Technologies Division at Lawrence Berkeley National Laboratory, and I am looking for a postdoc who knows substantially more than I do about time-series modeling; in practice this probably means someone whose dissertation work involved that sort of thing.  The work involves developing models to predict and/or forecast the time-dependent energy use in buildings, given historical data and some covariates such as outdoor temperature.  Simple regression approaches (e.g. using time-of-week indicator variables, plus outdoor temperature) work fine for a lot of things, but we still have a variety of problems.  To give one example, sometimes building behavior changes — due to retrofits, or a change in occupant behavior — so that a single model won’t fit well over a long time period. We want to recognize these changes automatically .  We have many other issues besides: heteroskedasticity, need for good uncertainty estimates, abilit</p><p>3 0.19273058 <a title="2364-tfidf-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-20-Thermodynamic_Monte_Carlo%3A__Michael_Betancourt%E2%80%99s_new_method_for_simulating_from_difficult_distributions_and_evaluating_normalizing_constants.html">2340 andrew gelman stats-2014-05-20-Thermodynamic Monte Carlo:  Michael Betancourt’s new method for simulating from difficult distributions and evaluating normalizing constants</a></p>
<p>Introduction: I hate to keep bumping our scheduled posts but this is just too important and too exciting to wait.  So it’s time to jump the queue.
 
 
 
The news is  a paper from Michael Betancourt  that presents a super-cool new way to compute normalizing constants:
  
A common strategy for inference in complex models is the relaxation of a simple model into the more complex target model, for example the prior into the posterior in Bayesian inference. Existing approaches that attempt to generate such transformations, however, are sensitive to the pathologies of complex distributions and can be difficult to implement in practice. Leveraging the geometry of thermodynamic processes I introduce a principled and robust approach to deforming measures that presents a powerful new tool for inference.
  
The idea is to generalize Hamiltonian Monte Carlo so that it moves through a family of distributions (that is, it transitions through an “inverse temperature” variable called beta that indexes the family) a</p><p>4 0.17355296 <a title="2364-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-14-%E2%80%9CFree_energy%E2%80%9D_and_economic_resources.html">1010 andrew gelman stats-2011-11-14-“Free energy” and economic resources</a></p>
<p>Introduction: By “free energy” I don’t mean perpetual motion machines, cars that run on water and get 200 mpg, or the latest cold-fusion hype.
 
No, I’m referring to the term from physics.  The free energy of a system is, roughly, the amount of energy that can be directly extracted from it.  For example, a rock at room temperature is just  full  of energy—not just the energy locked in its nuclei, but basic thermal energy—but at room temperature you can’t extract any of it.
 
To the physicists in the audience:  Yes, I realize that free energy has a technical meaning in statistical mechanics and that my above definition is sloppy.  Please bear with me.  And, to the non-physicists:  feel free to head to Wikipedia or a physics textbook for a more careful treatment.
 
I was thinking about free energy the other day when hearing someone on the radio say something about China bailing out the E.U.  I did a double-take.  Huh?  The E.U. is rich, China’s not so rich.  How can a middle-income country bail out a</p><p>5 0.16875075 <a title="2364-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-18-More_studies_on_the_economic_effects_of_climate_change.html">1501 andrew gelman stats-2012-09-18-More studies on the economic effects of climate change</a></p>
<p>Introduction: After writing  yesterday’s post , I was going through Solomon Hsiang’s blog and found  a post  pointing to three studies from researchers at business schools:
  
Severe Weather and Automobile Assembly Productivity


Gérard P. Cachon, Santiago Gallino and Marcelo Olivares


Abstract: It is expected that climate change could lead to an increased frequency of severe weather. In turn, severe weather intuitively should hamper the productivity of work that occurs outside. But what is the effect of rain, snow, fog, heat and wind on work that occurs indoors, such as the production of automobiles? Using weekly production data from 64 automobile plants in the United States over a ten-year period, we ﬁnd that adverse weather conditions lead to a signiﬁcant reduction in production. For example, one additional day of high wind advisory by the National Weather Service (i.e., maximum winds generally in excess of 44 miles per hour) reduces production by 26%, which is comparable in order of magnitude t</p><p>6 0.13903767 <a title="2364-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-07-Prior_distributions_for_regression_coefficients.html">1486 andrew gelman stats-2012-09-07-Prior distributions for regression coefficients</a></p>
<p>7 0.13804887 <a title="2364-tfidf-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-03-Climate_Change_News.html">180 andrew gelman stats-2010-08-03-Climate Change News</a></p>
<p>8 0.13653359 <a title="2364-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-03-Uncertainty_in_parameter_estimates_using_multilevel_models.html">1966 andrew gelman stats-2013-08-03-Uncertainty in parameter estimates using multilevel models</a></p>
<p>9 0.12066182 <a title="2364-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-20-Likelihood_thresholds_and_decisions.html">1422 andrew gelman stats-2012-07-20-Likelihood thresholds and decisions</a></p>
<p>10 0.11902675 <a title="2364-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>11 0.11881583 <a title="2364-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>12 0.11784673 <a title="2364-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>13 0.11657762 <a title="2364-tfidf-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-05-On_deck_this_week.html">2321 andrew gelman stats-2014-05-05-On deck this week</a></p>
<p>14 0.1164207 <a title="2364-tfidf-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Question_about_standard_range_for_social_science_correlations.html">257 andrew gelman stats-2010-09-04-Question about standard range for social science correlations</a></p>
<p>15 0.11159076 <a title="2364-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-11-Adding_an_error_model_to_a_deterministic_model.html">1162 andrew gelman stats-2012-02-11-Adding an error model to a deterministic model</a></p>
<p>16 0.11073975 <a title="2364-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-13-Can_you_write_a_program_to_determine_the_causal_order%3F.html">1801 andrew gelman stats-2013-04-13-Can you write a program to determine the causal order?</a></p>
<p>17 0.1087804 <a title="2364-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-04-What_are_the_key_assumptions_of_linear_regression%3F.html">1967 andrew gelman stats-2013-08-04-What are the key assumptions of linear regression?</a></p>
<p>18 0.10849468 <a title="2364-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-07-Inference_%3D_data_%2B_model.html">1201 andrew gelman stats-2012-03-07-Inference = data + model</a></p>
<p>19 0.10756249 <a title="2364-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-05-Understanding_regression_models_and_regression_coefficients.html">1656 andrew gelman stats-2013-01-05-Understanding regression models and regression coefficients</a></p>
<p>20 0.10741665 <a title="2364-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-16-Long_discussion_about_causal_inference_and_the_use_of_hierarchical_models_to_bridge_between_different_inferential_settings.html">1418 andrew gelman stats-2012-07-16-Long discussion about causal inference and the use of hierarchical models to bridge between different inferential settings</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.215), (1, 0.081), (2, 0.048), (3, -0.009), (4, 0.097), (5, 0.015), (6, 0.039), (7, -0.063), (8, 0.085), (9, 0.031), (10, -0.017), (11, 0.043), (12, 0.025), (13, -0.035), (14, -0.017), (15, 0.01), (16, 0.023), (17, 0.005), (18, -0.004), (19, -0.019), (20, -0.006), (21, 0.02), (22, 0.022), (23, 0.008), (24, 0.017), (25, 0.02), (26, 0.03), (27, -0.064), (28, 0.015), (29, 0.013), (30, 0.048), (31, 0.061), (32, -0.005), (33, -0.036), (34, -0.028), (35, -0.02), (36, -0.001), (37, -0.028), (38, -0.013), (39, -0.074), (40, 0.011), (41, 0.009), (42, -0.07), (43, -0.008), (44, 0.009), (45, 0.035), (46, -0.063), (47, -0.018), (48, -0.056), (49, 0.065)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95748365 <a title="2364-lsi-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-08-Regression_and_causality_and_variable_ordering.html">2364 andrew gelman stats-2014-06-08-Regression and causality and variable ordering</a></p>
<p>Introduction: Bill Harris wrote in with a question:
  
David Hogg points out in one of his general articles on data modeling that regression assumptions require one to put the variable with the highest variance in the ‘y’ position and the variable you know best (lowest variance) in the ‘x’ position.  As he points out, others speak of independent and dependent variables, as if causality determined the form of a regression formula.  In a quick scan of ARM and BDA, I don’t see clear advice, but I do see the use of ‘independent’ and ‘dependent.’


I recently did a model over data in which we know the ‘effect’ pretty well (we measure it), while we know the ’cause’ less well (it’s estimated by people who only need to get it approximately correct).  A model of the form ’cause ~ effect’ fit visually much better than one of the form ‘effect ~ cause’, but interpreting it seems challenging.  


For a simplistic example, let the effect be energy use in a building for cooling (E), and let the cause be outdoor ai</p><p>2 0.81235135 <a title="2364-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-14-Another_day%2C_another_stats_postdoc.html">906 andrew gelman stats-2011-09-14-Another day, another stats postdoc</a></p>
<p>Introduction: This post is from Phil Price.  I work in the Environmental Energy Technologies Division at Lawrence Berkeley National Laboratory, and I am looking for a postdoc who knows substantially more than I do about time-series modeling; in practice this probably means someone whose dissertation work involved that sort of thing.  The work involves developing models to predict and/or forecast the time-dependent energy use in buildings, given historical data and some covariates such as outdoor temperature.  Simple regression approaches (e.g. using time-of-week indicator variables, plus outdoor temperature) work fine for a lot of things, but we still have a variety of problems.  To give one example, sometimes building behavior changes — due to retrofits, or a change in occupant behavior — so that a single model won’t fit well over a long time period. We want to recognize these changes automatically .  We have many other issues besides: heteroskedasticity, need for good uncertainty estimates, abilit</p><p>3 0.76875293 <a title="2364-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-31-Using_factor_analysis_or_principal_components_analysis_or_measurement-error_models_for_biological_measurements_in_archaeology%3F.html">1094 andrew gelman stats-2011-12-31-Using factor analysis or principal components analysis or measurement-error models for biological measurements in archaeology?</a></p>
<p>Introduction: Greg Campbell writes:
  
I am a Canadian archaeologist (BSc in Chemistry) researching the past human use of European Atlantic shellfish. After two decades of practice I am finally getting a MA in archaeology at Reading. I am seeing if the habitat or size of harvested mussels (Mytilus edulis) can be reconstructed from measurements of the umbo (the pointy end, and the only bit that survives well in archaeological deposits) using log-transformed measurements (or allometry; relationships between dimensions are more likely exponential than linear). 
Of course multivariate regressions in most statistics packages (Minitab, SPSS, SAS) assume you are trying to predict one variable from all the others (a Model I regression), and use ordinary least squares to fit the regression line. For organismal dimensions this makes little sense, since all the dimensions are (at least in theory) free to change their mutual proportions during growth. So there is no predictor and predicted, mutual variation of</p><p>4 0.76264763 <a title="2364-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-15-Still_more_Mr._P_in_public_health.html">770 andrew gelman stats-2011-06-15-Still more Mr. P in public health</a></p>
<p>Introduction: When it  rains  it pours . . .
 
John Transue writes:
  
I saw  a post  on Andrew Sullivan’s blog today about life expectancy in different US counties. With a bunch of the worst counties being in Mississippi, I thought that it might be another case of analysts getting extreme values from small counties.


However, the paper (see  here ) includes a pretty interesting methods section. This is from page 5, “Specifically, we used a mixed-effects Poisson regression with time, geospatial, and covariate components. Poisson regression fits count outcome variables, e.g., death counts, and is preferable to a logistic model because the latter is biased when an outcome is rare (occurring in less than 1% of observations).”


They have downloadable data. I believe that the data are predicted values from the model. A web appendix also gives 90% CIs for their estimates.


Do you think they solved the small county problem and that the worst counties really are where their spreadsheet suggests?
  
My re</p><p>5 0.74954003 <a title="2364-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-Piss-poor_monocausal_social_science.html">1196 andrew gelman stats-2012-03-04-Piss-poor monocausal social science</a></p>
<p>Introduction: Dan Kahan writes: 
  
  
Okay, have done due diligence here & can’t find the reference. It was in recent blog — and was more or less an aside — but you ripped into researchers (pretty sure econometricians, but this could be my memory adding to your account recollections it conjured from my own experience) who purport to make estimates or predictions based on multivariate regression in which the value of particular predictor is set at some level while others “held constant” etc., on ground that variance in that particular predictor independent of covariance in other model predictors is unrealistic.  You made it sound, too, as if this were one of the pet peeves in your menagerie — leading me to think you had blasted into it before.


Know what I’m talking about?


Also — isn’t this really just a way of saying that the model is misspecified — at least if the goal is to try to make a valid & unbiased estimate of the impact of that particular predictor? The problem can’t be that one is usin</p><p>6 0.74251884 <a title="2364-lsi-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-31-Predicting_marathon_times.html">245 andrew gelman stats-2010-08-31-Predicting marathon times</a></p>
<p>7 0.73502946 <a title="2364-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-05-Understanding_regression_models_and_regression_coefficients.html">1656 andrew gelman stats-2013-01-05-Understanding regression models and regression coefficients</a></p>
<p>8 0.73201007 <a title="2364-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-13-Help_with_this_problem%2C_win_valuable_prizes.html">1164 andrew gelman stats-2012-02-13-Help with this problem, win valuable prizes</a></p>
<p>9 0.73020571 <a title="2364-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-04-What_are_the_key_assumptions_of_linear_regression%3F.html">1967 andrew gelman stats-2013-08-04-What are the key assumptions of linear regression?</a></p>
<p>10 0.72965568 <a title="2364-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-13-Lame_Statistics_Patents.html">1761 andrew gelman stats-2013-03-13-Lame Statistics Patents</a></p>
<p>11 0.72677517 <a title="2364-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-21-Fundamental_difficulty_of_inference_for_a_ratio_when_the_denominator_could_be_positive_or_negative.html">775 andrew gelman stats-2011-06-21-Fundamental difficulty of inference for a ratio when the denominator could be positive or negative</a></p>
<p>12 0.72496367 <a title="2364-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-03-Comparing_prediction_errors.html">938 andrew gelman stats-2011-10-03-Comparing prediction errors</a></p>
<p>13 0.72338831 <a title="2364-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-27-Cross-validation_%28What_is_it_good_for%3F%29.html">1395 andrew gelman stats-2012-06-27-Cross-validation (What is it good for?)</a></p>
<p>14 0.72287083 <a title="2364-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>15 0.70473552 <a title="2364-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-18-Standardizing_regression_inputs.html">1462 andrew gelman stats-2012-08-18-Standardizing regression inputs</a></p>
<p>16 0.7040624 <a title="2364-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>17 0.70044988 <a title="2364-lsi-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-09-Keli_Liu_and_Xiao-Li_Meng_on_Simpson%E2%80%99s_paradox.html">2204 andrew gelman stats-2014-02-09-Keli Liu and Xiao-Li Meng on Simpson’s paradox</a></p>
<p>18 0.69746041 <a title="2364-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-02-Interaction-based_feature_selection_and_classification_for_high-dimensional_biological_data.html">1703 andrew gelman stats-2013-02-02-Interaction-based feature selection and classification for high-dimensional biological data</a></p>
<p>19 0.69549066 <a title="2364-lsi-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Bayesian_Uncertainty_Quantification_for_Differential_Equations%21.html">2311 andrew gelman stats-2014-04-29-Bayesian Uncertainty Quantification for Differential Equations!</a></p>
<p>20 0.68964314 <a title="2364-lsi-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-28-Matching_for_preprocessing_data_for_causal_inference.html">375 andrew gelman stats-2010-10-28-Matching for preprocessing data for causal inference</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.037), (5, 0.012), (21, 0.034), (24, 0.196), (31, 0.057), (42, 0.011), (47, 0.012), (54, 0.016), (58, 0.034), (86, 0.11), (95, 0.029), (99, 0.332)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97739398 <a title="2364-lda-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-08-Regression_and_causality_and_variable_ordering.html">2364 andrew gelman stats-2014-06-08-Regression and causality and variable ordering</a></p>
<p>Introduction: Bill Harris wrote in with a question:
  
David Hogg points out in one of his general articles on data modeling that regression assumptions require one to put the variable with the highest variance in the ‘y’ position and the variable you know best (lowest variance) in the ‘x’ position.  As he points out, others speak of independent and dependent variables, as if causality determined the form of a regression formula.  In a quick scan of ARM and BDA, I don’t see clear advice, but I do see the use of ‘independent’ and ‘dependent.’


I recently did a model over data in which we know the ‘effect’ pretty well (we measure it), while we know the ’cause’ less well (it’s estimated by people who only need to get it approximately correct).  A model of the form ’cause ~ effect’ fit visually much better than one of the form ‘effect ~ cause’, but interpreting it seems challenging.  


For a simplistic example, let the effect be energy use in a building for cooling (E), and let the cause be outdoor ai</p><p>2 0.96986628 <a title="2364-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-07-I%E2%80%99m_negative_on_the_expression_%E2%80%9Cfalse_positives%E2%80%9D.html">2093 andrew gelman stats-2013-11-07-I’m negative on the expression “false positives”</a></p>
<p>Introduction: After seeing a document sent to me and others regarding the  crisis  of spurious, statistically-significant research findings in psychology research, I had the following reaction:
  
I am unhappy with the use in the document of the phrase “false positives.”  I feel that this expression is unhelpful as it frames science in terms of “true” and “false” claims, which I don’t think is particularly accurate.  In particular, in most of the recent disputed Psych Science type studies (the ESP study excepted, perhaps), there is little doubt that there is _some_ underlying effect.  The issue, as I see it, as that the underlying effects are much smaller, and much more variable, than mainstream researchers imagine.  So what happens is that Psych Science or Nature or whatever will publish a result that is purported to be some sort of universal truth, but it is actually a pattern specific to one data set, one population, and one experimental condition.  In a sense, yes, these journals are publishing</p><p>3 0.96565342 <a title="2364-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-29-Decision_science_vs._social_psychology.html">305 andrew gelman stats-2010-09-29-Decision science vs. social psychology</a></p>
<p>Introduction: Dan Goldstein sends along  this bit of research , distinguishing terms used in two different subfields of psychology.  Dan writes:
  
Intuitive calls included not listing words that don’t occur 3 or more times in both programs. I [Dan] did this because when I looked at the results, those cases tended to be proper names or arbitrary things like header or footer text.  It also narrowed down the space of words to inspect, which means I could actually get the thing done in my copious free time.
  
I think the bar graphs are kinda ugly, maybe there’s a better way to do it based on classifying the words according to content?  Also the whole exercise would gain a new dimension by comparing several areas instead of just two.  Maybe that’s coming next.</p><p>4 0.96486545 <a title="2364-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-26-Blog_on_applied_probability_modeling.html">872 andrew gelman stats-2011-08-26-Blog on applied probability modeling</a></p>
<p>Introduction: Joseph Wilson points me to  this blog  on applied probability modeling.  He sent me the link a couple months ago.  If heâ&euro;&trade;s still adding new entries, then his blog is probably already longer-lasting than most!</p><p>5 0.96422338 <a title="2364-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-15-More_on_AIC%2C_WAIC%2C_etc.html">1983 andrew gelman stats-2013-08-15-More on AIC, WAIC, etc</a></p>
<p>Introduction: Following up on our  discussion  from the other day, Angelika van der Linde sends along  this paper  from 2012 (link to journal  here ).
 
And Aki pulls out this great quote from Geisser and Eddy (1979):
  
This discussion makes clear that in the nested case this method, as Akaike’s, is not consistent; i.e., even if $M_k$ is true, it will be rejected with probability $\alpha$ as $N\to\infty$. This point is also made by Schwarz (1978).  However, from the point of view of prediction, this is of no great consequence. For large numbers of observations, a prediction based on the falsely assumed $M_k$, will not differ appreciably from one based on the true $M_k$.  For example, if we assert that two normal populations have different means when in fact they have the same mean, then the use of the group mean as opposed to the grand mean for predicting a future observation results in predictors which are asymptotically equivalent and whose predictive variances are $\sigma^2[1 + (1/2n)]$ and $\si</p><p>6 0.96196771 <a title="2364-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>7 0.96104878 <a title="2364-lda-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-I_doubt_they_cheated.html">1971 andrew gelman stats-2013-08-07-I doubt they cheated</a></p>
<p>8 0.96047783 <a title="2364-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-18-Comments_on_%E2%80%9CA_Bayesian_approach_to_complex_clinical_diagnoses%3A_a_case-study_in_child_abuse%E2%80%9D.html">1327 andrew gelman stats-2012-05-18-Comments on “A Bayesian approach to complex clinical diagnoses: a case-study in child abuse”</a></p>
<p>9 0.96047163 <a title="2364-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-29-%E2%80%9CCommunication_is_a_central_task_of_statistics%2C_and_ideally_a_state-of-the-art_data_analysis_can_have_state-of-the-art_displays_to_match%E2%80%9D.html">1552 andrew gelman stats-2012-10-29-“Communication is a central task of statistics, and ideally a state-of-the-art data analysis can have state-of-the-art displays to match”</a></p>
<p>10 0.95904446 <a title="2364-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-17-Christian_Robert_on_the_Jeffreys-Lindley_paradox%3B_more_generally%2C_it%E2%80%99s_good_news_when_philosophical_arguments_can_be_transformed_into_technical_modeling_issues.html">2027 andrew gelman stats-2013-09-17-Christian Robert on the Jeffreys-Lindley paradox; more generally, it’s good news when philosophical arguments can be transformed into technical modeling issues</a></p>
<p>11 0.9580431 <a title="2364-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-11-%E2%80%9C2_level_logit_with_2_REs_%26_large_sample._computational_nightmare_%E2%80%93_please_help%E2%80%9D.html">759 andrew gelman stats-2011-06-11-“2 level logit with 2 REs & large sample. computational nightmare – please help”</a></p>
<p>12 0.95800143 <a title="2364-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-23-Participate_in_a_research_project_on_combining_information_for_prediction.html">866 andrew gelman stats-2011-08-23-Participate in a research project on combining information for prediction</a></p>
<p>13 0.95775664 <a title="2364-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-14-%E2%80%9CToo_much_data%E2%80%9D%3F.html">86 andrew gelman stats-2010-06-14-“Too much data”?</a></p>
<p>14 0.95699441 <a title="2364-lda-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-31-Type_S_error_rates_for_classical_and_Bayesian_single_and_multiple_comparison_procedures.html">494 andrew gelman stats-2010-12-31-Type S error rates for classical and Bayesian single and multiple comparison procedures</a></p>
<p>15 0.95561051 <a title="2364-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-07-Robust_logistic_regression.html">1886 andrew gelman stats-2013-06-07-Robust logistic regression</a></p>
<p>16 0.95457131 <a title="2364-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-The_statistical_significance_filter.html">899 andrew gelman stats-2011-09-10-The statistical significance filter</a></p>
<p>17 0.95424676 <a title="2364-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-15-%E2%80%9CAre_all_significant_p-values_created_equal%3F%E2%80%9D.html">2102 andrew gelman stats-2013-11-15-“Are all significant p-values created equal?”</a></p>
<p>18 0.9541254 <a title="2364-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-22-My_talks_that_were_scheduled_for_Tues_at_the_Data_Skeptics_meetup_and_Wed_at_the_Open_Statistical_Programming_meetup.html">1950 andrew gelman stats-2013-07-22-My talks that were scheduled for Tues at the Data Skeptics meetup and Wed at the Open Statistical Programming meetup</a></p>
<p>19 0.95386934 <a title="2364-lda-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-09-I_hate_polynomials.html">2365 andrew gelman stats-2014-06-09-I hate polynomials</a></p>
<p>20 0.95371687 <a title="2364-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
