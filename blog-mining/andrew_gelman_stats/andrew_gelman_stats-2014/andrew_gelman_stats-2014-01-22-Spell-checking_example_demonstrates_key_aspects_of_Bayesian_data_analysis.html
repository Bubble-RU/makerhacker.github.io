<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2182 andrew gelman stats-2014-01-22-Spell-checking example demonstrates key aspects of Bayesian data analysis</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2014" href="../home/andrew_gelman_stats-2014_home.html">andrew_gelman_stats-2014</a> <a title="andrew_gelman_stats-2014-2182" href="#">andrew_gelman_stats-2014-2182</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2182 andrew gelman stats-2014-01-22-Spell-checking example demonstrates key aspects of Bayesian data analysis</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2014-2182-html" href="http://andrewgelman.com/2014/01/22/spell-checking-example/">html</a></p><p>Introduction: One of the new examples for the third edition of Bayesian Data Analysis is a spell-checking story.   Here it is  (just start at 2/3 down on the first page, with “Spelling correction”).
 
I like this example—it demonstrates the Bayesian algebra, also gives a sense of the way that probability models (both “likelihood” and “prior”) are constructed from existing assumptions and data.  The models aren’t just specified as a mathematical exercise, they represent some statement about reality.  And the problem is close enough to our experience that we can consider ways in which the model can be criticized and improved, all in a simple example that has only three possibilities.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 One of the new examples for the third edition of Bayesian Data Analysis is a spell-checking story. [sent-1, score-0.526]
</p><p>2 Here it is  (just start at 2/3 down on the first page, with “Spelling correction”). [sent-2, score-0.173]
</p><p>3 I like this example—it demonstrates the Bayesian algebra, also gives a sense of the way that probability models (both “likelihood” and “prior”) are constructed from existing assumptions and data. [sent-3, score-1.376]
</p><p>4 The models aren’t just specified as a mathematical exercise, they represent some statement about reality. [sent-4, score-0.831]
</p><p>5 And the problem is close enough to our experience that we can consider ways in which the model can be criticized and improved, all in a simple example that has only three possibilities. [sent-5, score-1.228]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spelling', 0.312), ('algebra', 0.231), ('constructed', 0.228), ('demonstrates', 0.228), ('possibilities', 0.215), ('criticized', 0.213), ('specified', 0.213), ('correction', 0.211), ('exercise', 0.198), ('edition', 0.196), ('improved', 0.183), ('bayesian', 0.172), ('existing', 0.166), ('models', 0.165), ('represent', 0.158), ('third', 0.158), ('mathematical', 0.15), ('likelihood', 0.146), ('statement', 0.145), ('assumptions', 0.144), ('aren', 0.137), ('experience', 0.13), ('gives', 0.13), ('close', 0.127), ('page', 0.123), ('ways', 0.117), ('examples', 0.113), ('prior', 0.112), ('example', 0.11), ('start', 0.109), ('three', 0.107), ('probability', 0.105), ('simple', 0.103), ('consider', 0.102), ('enough', 0.082), ('sense', 0.079), ('problem', 0.072), ('analysis', 0.071), ('model', 0.065), ('first', 0.064), ('new', 0.059), ('way', 0.052), ('data', 0.045), ('also', 0.043), ('like', 0.036), ('one', 0.033)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="2182-tfidf-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-22-Spell-checking_example_demonstrates_key_aspects_of_Bayesian_data_analysis.html">2182 andrew gelman stats-2014-01-22-Spell-checking example demonstrates key aspects of Bayesian data analysis</a></p>
<p>Introduction: One of the new examples for the third edition of Bayesian Data Analysis is a spell-checking story.   Here it is  (just start at 2/3 down on the first page, with “Spelling correction”).
 
I like this example—it demonstrates the Bayesian algebra, also gives a sense of the way that probability models (both “likelihood” and “prior”) are constructed from existing assumptions and data.  The models aren’t just specified as a mathematical exercise, they represent some statement about reality.  And the problem is close enough to our experience that we can consider ways in which the model can be criticized and improved, all in a simple example that has only three possibilities.</p><p>2 0.15928936 <a title="2182-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>Introduction: I’ve been writing a lot about my philosophy of Bayesian statistics and how it fits into Popper’s ideas about falsification and Kuhn’s ideas about scientific revolutions.
 
 Here’s  my long, somewhat technical paper with Cosma Shalizi. 
 Here’s  our shorter overview for the volume on the philosophy of social science. 
 Here’s  my latest try (for an online symposium), focusing on the key issues.
 
I’m pretty happy with my approach–the familiar idea that Bayesian data analysis iterates the three steps of model building, inference, and model checking–but it does have some unresolved (maybe unresolvable) problems.  Here are a couple mentioned in the third of the above links.
 
Consider a simple model with independent data y_1, y_2, .., y_10 ~ N(θ,σ^2), with a prior distribution θ ~ N(0,10^2) and σ known and taking on some value of approximately 10. Inference about μ is straightforward, as is model checking, whether based on graphs or numerical summaries such as the sample variance and skewn</p><p>3 0.15322463 <a title="2182-tfidf-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>Introduction: I received the following email:
  
I have an interesting thought on a prior for a logistic regression, and would love your input on how to make it “work.”


Some of my research, two published papers, are on mathematical models of **.  Along those lines, I’m interested in developing more models for **. . . .  Empirical studies show that the public is rather smart and that the wisdom-of-the-crowd is fairly accurate.


So, my thought would be to tread the public’s probability of the event as a prior, and then see how adding data, through a model, would change or perturb our inferred probability of **.  (Similarly, I could envision using previously published epidemiological research as a prior probability of a disease, and then seeing how the addition of new testing protocols would update that belief.)


However, everything I learned about hierarchical Bayesian models has a prior as a distribution on the coefficients.  I don’t know how to start with a prior point estimate for the probabili</p><p>4 0.14732835 <a title="2182-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-13-You_heard_it_here_first%3A_Intense_exercise_can_suppress_appetite.html">2022 andrew gelman stats-2013-09-13-You heard it here first: Intense exercise can suppress appetite</a></p>
<p>Introduction: This post is by Phil Price.
 
The New York Times recently ran an article entitled  “How Exercise Can Help Us Eat Less,”  which begins with this: “Strenuous exercise seems to dull the urge to eat afterward better than gentler workouts, several new studies show, adding to a growing body of science suggesting that intense exercise may have unique benefits.” The article is based on a couple of recent studies in which moderately overweight volunteers participated in different types of exercise, and had their food intake monitored at a subsequent meal.
 
The article also says “[The volunteers] also displayed significantly lower levels of the hormone ghrelin, which is known to stimulate appetite, and elevated levels of both blood lactate and blood sugar, which have been shown to lessen the drive to eat, after the most vigorous interval session than after the other workouts. And the appetite-suppressing effect of the highly intense intervals lingered into the next day, according to food diarie</p><p>5 0.14479467 <a title="2182-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-31-It_not_necessary_that_Bayesian_methods_conform_to_the_likelihood_principle.html">1554 andrew gelman stats-2012-10-31-It not necessary that Bayesian methods conform to the likelihood principle</a></p>
<p>Introduction: Bayesian inference, conditional on the model and data, conforms to the likelihood principle. But there is more to Bayesian methods than Bayesian inference. See chapters 6 and 7 of Bayesian Data Analysis for much discussion of this point.
 
It saddens me to see that people are still  confused  on this issue.</p><p>6 0.14391389 <a title="2182-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-11-Why_waste_time_philosophizing%3F.html">1719 andrew gelman stats-2013-02-11-Why waste time philosophizing?</a></p>
<p>7 0.1404734 <a title="2182-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<p>8 0.13674982 <a title="2182-tfidf-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-22-Philosophy_of_Bayes_and_non-Bayes%3A__A_dialogue_with_Deborah_Mayo.html">291 andrew gelman stats-2010-09-22-Philosophy of Bayes and non-Bayes:  A dialogue with Deborah Mayo</a></p>
<p>9 0.1343147 <a title="2182-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-16-Long_discussion_about_causal_inference_and_the_use_of_hierarchical_models_to_bridge_between_different_inferential_settings.html">1418 andrew gelman stats-2012-07-16-Long discussion about causal inference and the use of hierarchical models to bridge between different inferential settings</a></p>
<p>10 0.13149163 <a title="2182-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-20-Statisticians_vs._everybody_else.html">582 andrew gelman stats-2011-02-20-Statisticians vs. everybody else</a></p>
<p>11 0.12688887 <a title="2182-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>12 0.12609529 <a title="2182-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>13 0.12508696 <a title="2182-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>14 0.12323073 <a title="2182-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>15 0.11962494 <a title="2182-tfidf-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-01-bayesglm_in_Stata%3F.html">442 andrew gelman stats-2010-12-01-bayesglm in Stata?</a></p>
<p>16 0.11883818 <a title="2182-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>17 0.11883514 <a title="2182-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>18 0.11734347 <a title="2182-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>19 0.11534686 <a title="2182-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-11-Bayesian_brains%3F.html">1529 andrew gelman stats-2012-10-11-Bayesian brains?</a></p>
<p>20 0.11420569 <a title="2182-tfidf-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-11-Bayes_in_the_research_conversation.html">2368 andrew gelman stats-2014-06-11-Bayes in the research conversation</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.17), (1, 0.18), (2, -0.046), (3, 0.075), (4, -0.068), (5, -0.009), (6, 0.011), (7, 0.042), (8, 0.006), (9, -0.014), (10, -0.005), (11, -0.027), (12, -0.019), (13, -0.003), (14, 0.011), (15, 0.04), (16, 0.048), (17, 0.008), (18, 0.01), (19, 0.022), (20, -0.007), (21, 0.026), (22, 0.002), (23, -0.01), (24, -0.02), (25, 0.01), (26, 0.034), (27, -0.024), (28, 0.026), (29, -0.027), (30, -0.082), (31, 0.002), (32, -0.045), (33, 0.025), (34, 0.036), (35, -0.028), (36, -0.046), (37, -0.029), (38, -0.008), (39, 0.002), (40, 0.011), (41, 0.011), (42, 0.009), (43, 0.018), (44, 0.027), (45, 0.011), (46, -0.001), (47, 0.01), (48, -0.047), (49, -0.024)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97844464 <a title="2182-lsi-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-22-Spell-checking_example_demonstrates_key_aspects_of_Bayesian_data_analysis.html">2182 andrew gelman stats-2014-01-22-Spell-checking example demonstrates key aspects of Bayesian data analysis</a></p>
<p>Introduction: One of the new examples for the third edition of Bayesian Data Analysis is a spell-checking story.   Here it is  (just start at 2/3 down on the first page, with “Spelling correction”).
 
I like this example—it demonstrates the Bayesian algebra, also gives a sense of the way that probability models (both “likelihood” and “prior”) are constructed from existing assumptions and data.  The models aren’t just specified as a mathematical exercise, they represent some statement about reality.  And the problem is close enough to our experience that we can consider ways in which the model can be criticized and improved, all in a simple example that has only three possibilities.</p><p>2 0.84527308 <a title="2182-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-24-Untangling_the_Jeffreys-Lindley_paradox.html">1182 andrew gelman stats-2012-02-24-Untangling the Jeffreys-Lindley paradox</a></p>
<p>Introduction: Ryan Ickert writes:
  
I was wondering if you’d seen  this post , by a particle physicist with some degree of influence.  Dr. Dorigo works at CERN and Fermilab.


The penultimate paragraph is:

 
From the above expression, the Frequentist researcher concludes that the tracker is indeed biased, and rejects the null hypothesis H0, since there is a less-than-2% probability (P’<α) that a result as the one observed could arise by chance! A Frequentist thus draws, strongly, the opposite conclusion than a Bayesian from the same set of data. How to solve the riddle?
 

He goes on to not solve the riddle.  Perhaps you can?


Surely with the large sample size they have (n=10^6), the precision on the frequentist p-value is pretty good, is it not?
  
My reply:
 
The first comment on the site (by Anonymous [who, just to be clear, is not me; I have no idea who wrote that comment], 22 Feb 2012, 21:27pm) pretty much nails it:  In setting up the Bayesian model, Dorigo assumed a silly distribution on th</p><p>3 0.82355803 <a title="2182-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Kind_of_Bayesian.html">811 andrew gelman stats-2011-07-20-Kind of Bayesian</a></p>
<p>Introduction: Astrophysicist Andrew Jaffe pointed me to  this and discussion  of my  philosophy  of statistics (which is, in turn, my rational reconstruction of the statistical practice of Bayesians such as Rubin and Jaynes).  Jaffe’s summary is fair enough and I only disagree in a few points: 
   
1.  Jaffe writes:
  
Subjective probability, at least the way it is actually used by practicing scientists, is a sort of “as-if” subjectivity — how would an agent reason if her beliefs were reflected in a certain set of probability distributions? This is why when I discuss probability I try to make the pedantic point that all probabilities are conditional, at least on some background prior information or context.
  
I agree, and my problem with the usual procedures used for Bayesian model comparison and Bayesian model averaging is not that these approaches are subjective but that the particular models being considered don’t make sense.  I’m thinking of the sorts of models that say the truth is either A or</p><p>4 0.80896837 <a title="2182-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-11-Gelman_on_Hennig_on_Gelman_on_Bayes.html">1208 andrew gelman stats-2012-03-11-Gelman on Hennig on Gelman on Bayes</a></p>
<p>Introduction: Deborah Mayo pointed me to  this discussion  by Christian Hennig of my recent  article  on Induction and Deduction in Bayesian Data Analysis.
 
A couple days ago I  responded  to comments by Mayo, Stephen Senn, and Larry Wasserman.  I will respond to Hennig by pulling out paragraphs from his discussion and then replying.
 
Hennig:
  
for me the terms “frequentist” and “subjective Bayes” point to interpretations of probability, and not to specific methods of inference. The frequentist one refers to the idea that there is an underlying data generating process that repeatedly throws out data and would approximate the assumed distribution if one could only repeat it infinitely often.
  
Hennig makes the good point that, if this is the way you would define “frequentist” (it’s not how I’d define the term myself, but I’ll use Hennig’s definition here), then it makes sense to be a frequentist in some settings but not others.  Dice really can be rolled over and over again; a sample survey of 15</p><p>5 0.80703729 <a title="2182-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>Introduction: I’ve been writing a lot about my philosophy of Bayesian statistics and how it fits into Popper’s ideas about falsification and Kuhn’s ideas about scientific revolutions.
 
 Here’s  my long, somewhat technical paper with Cosma Shalizi. 
 Here’s  our shorter overview for the volume on the philosophy of social science. 
 Here’s  my latest try (for an online symposium), focusing on the key issues.
 
I’m pretty happy with my approach–the familiar idea that Bayesian data analysis iterates the three steps of model building, inference, and model checking–but it does have some unresolved (maybe unresolvable) problems.  Here are a couple mentioned in the third of the above links.
 
Consider a simple model with independent data y_1, y_2, .., y_10 ~ N(θ,σ^2), with a prior distribution θ ~ N(0,10^2) and σ known and taking on some value of approximately 10. Inference about μ is straightforward, as is model checking, whether based on graphs or numerical summaries such as the sample variance and skewn</p><p>6 0.79902947 <a title="2182-lsi-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-22-Philosophy_of_Bayes_and_non-Bayes%3A__A_dialogue_with_Deborah_Mayo.html">291 andrew gelman stats-2010-09-22-Philosophy of Bayes and non-Bayes:  A dialogue with Deborah Mayo</a></p>
<p>7 0.79689509 <a title="2182-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>8 0.79560661 <a title="2182-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-31-What_is_a_Bayesian%3F.html">1438 andrew gelman stats-2012-07-31-What is a Bayesian?</a></p>
<p>9 0.79165882 <a title="2182-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-25-Incoherence_of_Bayesian_data_analysis.html">1510 andrew gelman stats-2012-09-25-Incoherence of Bayesian data analysis</a></p>
<p>10 0.78789437 <a title="2182-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-11-Why_waste_time_philosophizing%3F.html">1719 andrew gelman stats-2013-02-11-Why waste time philosophizing?</a></p>
<p>11 0.78575671 <a title="2182-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-17-Christian_Robert_on_the_Jeffreys-Lindley_paradox%3B_more_generally%2C_it%E2%80%99s_good_news_when_philosophical_arguments_can_be_transformed_into_technical_modeling_issues.html">2027 andrew gelman stats-2013-09-17-Christian Robert on the Jeffreys-Lindley paradox; more generally, it’s good news when philosophical arguments can be transformed into technical modeling issues</a></p>
<p>12 0.78355736 <a title="2182-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-07-Philosophy_of_Bayesian_statistics%3A_my_reactions_to_Hendry.html">1157 andrew gelman stats-2012-02-07-Philosophy of Bayesian statistics: my reactions to Hendry</a></p>
<p>13 0.77842182 <a title="2182-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-14-Progress%21__%28on_the_understanding_of_the_role_of_randomization_in_Bayesian_inference%29.html">1898 andrew gelman stats-2013-06-14-Progress!  (on the understanding of the role of randomization in Bayesian inference)</a></p>
<p>14 0.77472395 <a title="2182-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-09-The_anti-Bayesian_moment_and_its_passing.html">1571 andrew gelman stats-2012-11-09-The anti-Bayesian moment and its passing</a></p>
<p>15 0.77369338 <a title="2182-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-11-Bayesian_brains%3F.html">1529 andrew gelman stats-2012-10-11-Bayesian brains?</a></p>
<p>16 0.77104628 <a title="2182-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-04-David_MacKay_and_Occam%E2%80%99s_Razor.html">1041 andrew gelman stats-2011-12-04-David MacKay and Occam’s Razor</a></p>
<p>17 0.77099371 <a title="2182-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-20-Problemen_met_het_boek.html">1332 andrew gelman stats-2012-05-20-Problemen met het boek</a></p>
<p>18 0.77064669 <a title="2182-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>19 0.76645553 <a title="2182-lsi-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-05-Does_posterior_predictive_model_checking_fit_with_the_operational_subjective_approach%3F.html">320 andrew gelman stats-2010-10-05-Does posterior predictive model checking fit with the operational subjective approach?</a></p>
<p>20 0.76620966 <a title="2182-lsi-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-14-Trying_to_be_precise_about_vagueness.html">342 andrew gelman stats-2010-10-14-Trying to be precise about vagueness</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(16, 0.163), (18, 0.051), (24, 0.119), (40, 0.034), (53, 0.047), (86, 0.139), (94, 0.045), (99, 0.273)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.95627993 <a title="2182-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-04-Why_does_anyone_support_private_macroeconomic_forecasts%3F.html">185 andrew gelman stats-2010-08-04-Why does anyone support private macroeconomic forecasts?</a></p>
<p>Introduction: Tyler Cowen  asks  the above question.  I donâ&euro;&trade;t have a full answer, but, in the Economics section of  A Quantitative Tour of the Social Sciences , Richard Clarida discusses in detail the ways that researchers have tried to estimate the extent to which government or private forecasts supply additional information.</p><p>same-blog 2 0.95082372 <a title="2182-lda-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-22-Spell-checking_example_demonstrates_key_aspects_of_Bayesian_data_analysis.html">2182 andrew gelman stats-2014-01-22-Spell-checking example demonstrates key aspects of Bayesian data analysis</a></p>
<p>Introduction: One of the new examples for the third edition of Bayesian Data Analysis is a spell-checking story.   Here it is  (just start at 2/3 down on the first page, with “Spelling correction”).
 
I like this example—it demonstrates the Bayesian algebra, also gives a sense of the way that probability models (both “likelihood” and “prior”) are constructed from existing assumptions and data.  The models aren’t just specified as a mathematical exercise, they represent some statement about reality.  And the problem is close enough to our experience that we can consider ways in which the model can be criticized and improved, all in a simple example that has only three possibilities.</p><p>3 0.92983043 <a title="2182-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-25-College_football%2C_voting%2C_and_the_law_of_large_numbers.html">1547 andrew gelman stats-2012-10-25-College football, voting, and the law of large numbers</a></p>
<p>Introduction: In an article provocatively entitled, “Will Ohio State’s football team decide who wins the White House?”, Tyler Cowen and Kevin Grier  report :
  
It is statistically possible that the outcome of a handful of college football games in the right battleground states could determine the race for the White House.


Economists Andrew Healy, Neil Malhotra, and Cecilia Mo make this argument in a fascinating article in the Proceedings of the National Academy of Science. They examined whether the outcomes of college football games on the eve of elections for presidents, senators, and governors affected the choices voters made. They found that a win by the local team, in the week before an election, raises the vote going to the incumbent by around 1.5 percentage points. When it comes to the 20 highest attendance teams—big athletic programs like the University of Michigan, Oklahoma, and Southern Cal—a victory on the eve of an election pushes the vote for the incumbent up by 3 percentage points. T</p><p>4 0.92594576 <a title="2182-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-23-%E2%80%9CAny_old_map_will_do%E2%80%9D_meets_%E2%80%9CGod_is_in_every_leaf_of_every_tree%E2%80%9D.html">1278 andrew gelman stats-2012-04-23-“Any old map will do” meets “God is in every leaf of every tree”</a></p>
<p>Introduction: As a statistician I am particularly worried about the rhetorical power of anecdotes (even though I use them in my own reasoning; see discussion below).  But much can be learned from a  true  anecdote.  The rough edges—the places where the anecdote doesn’t fit your thesis—these are where you learn.
 
We have recently had a discussion ( here  and  here ) of Karl Weick, a prominent scholar of business management who plagiarized a story and then went on to draw different lessons from the pilfered anecdote in several different publications published over many years.
 
Setting aside an issues of plagiarism and rulebreaking, I argue that, by hiding the source of the story and changing its form, Weick and his management-science audience are losing their ability to get anything out of it beyond empty confirmation.
 
A full discussion follows.
 
 1.  The lost Hungarian soldiers 
 
Thomas Basbøll (who has the unusual (to me) job of “writing consultant” at the Copenhagen Business School) has been</p><p>5 0.92368257 <a title="2182-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-30-Berri_Gladwell_Loken_football_update.html">2082 andrew gelman stats-2013-10-30-Berri Gladwell Loken football update</a></p>
<p>Introduction: Sports researcher Dave Berri had a disagreement with  a remark  in our recent discussion of Malcolm Gladwell.  Berri writes:
  
This post [from Gelman] contains the following paragraph:

 
Similarly, when Gladwell claimed that NFL quarterback performance is unrelated to the order they were drafted out of college, he appears to have been wrong. But if you take his writing as stone soup, maybe it’s valuable: just retreat to the statement that there’s only a weak relationship between draft order and NFL performance. That alone is interesting. It’s too bad that Gladwell sometimes has to make false general statements in order to get our attention, but maybe that’s what is needed to shake people out of their mental complacency.
 

The above paragraph  links  to a blog post by Eric Loken.  This is something you have linked to before.  And when you linked to it before I tried to explain why Loken’s work is not very good.  Since you still think this work shows that Gladwell – and therefore Rob</p><p>6 0.91974556 <a title="2182-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-16-Another_day%2C_another_plagiarist.html">1266 andrew gelman stats-2012-04-16-Another day, another plagiarist</a></p>
<p>7 0.91954827 <a title="2182-lda-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-11-Toward_a_framework_for_automatic_model_building.html">1718 andrew gelman stats-2013-02-11-Toward a framework for automatic model building</a></p>
<p>8 0.91919804 <a title="2182-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-20-Why_no_Wegmania%3F.html">722 andrew gelman stats-2011-05-20-Why no Wegmania?</a></p>
<p>9 0.91689312 <a title="2182-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-16-Chess_vs._checkers.html">615 andrew gelman stats-2011-03-16-Chess vs. checkers</a></p>
<p>10 0.91669816 <a title="2182-lda-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-03-Gladwell_vs_Pinker.html">253 andrew gelman stats-2010-09-03-Gladwell vs Pinker</a></p>
<p>11 0.91606557 <a title="2182-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-13-Test_scores_and_grades_predict_job_performance_%28but_maybe_not_at_Google%29.html">1980 andrew gelman stats-2013-08-13-Test scores and grades predict job performance (but maybe not at Google)</a></p>
<p>12 0.91444635 <a title="2182-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-19-Validation_of_Software_for_Bayesian_Models_Using_Posterior_Quantiles.html">1019 andrew gelman stats-2011-11-19-Validation of Software for Bayesian Models Using Posterior Quantiles</a></p>
<p>13 0.91356218 <a title="2182-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-17-I_got_99_comparisons_but_multiplicity_ain%E2%80%99t_one.html">1016 andrew gelman stats-2011-11-17-I got 99 comparisons but multiplicity ain’t one</a></p>
<p>14 0.91308916 <a title="2182-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-I_doubt_they_cheated.html">1971 andrew gelman stats-2013-08-07-I doubt they cheated</a></p>
<p>15 0.91131216 <a title="2182-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-26-Luck_or_knowledge%3F.html">873 andrew gelman stats-2011-08-26-Luck or knowledge?</a></p>
<p>16 0.91112655 <a title="2182-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-21-Readings_for_a_two-week_segment_on_Bayesian_modeling%3F.html">1586 andrew gelman stats-2012-11-21-Readings for a two-week segment on Bayesian modeling?</a></p>
<p>17 0.91040844 <a title="2182-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-11-Gladwell_and_Chabris%2C_David_and_Goliath%2C_and_science_writing_as_stone_soup.html">2058 andrew gelman stats-2013-10-11-Gladwell and Chabris, David and Goliath, and science writing as stone soup</a></p>
<p>18 0.90882945 <a title="2182-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-18-Comments_on_%E2%80%9CA_Bayesian_approach_to_complex_clinical_diagnoses%3A_a_case-study_in_child_abuse%E2%80%9D.html">1327 andrew gelman stats-2012-05-18-Comments on “A Bayesian approach to complex clinical diagnoses: a case-study in child abuse”</a></p>
<p>19 0.9085263 <a title="2182-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-18-Predictive_checks_for_hierarchical_models.html">154 andrew gelman stats-2010-07-18-Predictive checks for hierarchical models</a></p>
<p>20 0.90783042 <a title="2182-lda-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-05-Update_on_state_size_and_governors%E2%80%99_popularity.html">187 andrew gelman stats-2010-08-05-Update on state size and governors’ popularity</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
