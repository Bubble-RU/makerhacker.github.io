<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2128 andrew gelman stats-2013-12-09-How to model distributions that have outliers in one direction</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-2128" href="#">andrew_gelman_stats-2013-2128</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2128 andrew gelman stats-2013-12-09-How to model distributions that have outliers in one direction</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-2128-html" href="http://andrewgelman.com/2013/12/09/model-distributions-outliers-one-direction/">html</a></p><p>Introduction: Shravan writes:
  
I have a problem very similar to the one presented chapter 6 of BDA, the speed of light example. You use the distribution of the minimum scores from the posterior predictive distribution, show that it’s not realistic given the data, and suggest that an asymmetric contaminated normal distribution or a symmetric long-tailed distribution would be better.


How does one use such a distribution?
  
My reply:
 
You can actually use a symmetric long-tailed distribution such as t with low degrees of freedom.  One striking feature of symmetric long-tailed distributions is that a small random sample from such a distribution can have outliers on one side or the other and look asymmetric.
 
Just to see this, try the following in R: 
  
par (mfrow=c(3,3), mar=c(1,1,1,1)) 
for (i in 1:9) hist (rt (100, 2), xlab="", ylab="", main="") 
 
 
You’ll see some skewed distributions.  So that’s the message (which I learned from an offhand comment of Rubin, actually):  if you want to model</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Shravan writes:    I have a problem very similar to the one presented chapter 6 of BDA, the speed of light example. [sent-1, score-0.353]
</p><p>2 You use the distribution of the minimum scores from the posterior predictive distribution, show that it’s not realistic given the data, and suggest that an asymmetric contaminated normal distribution or a symmetric long-tailed distribution would be better. [sent-2, score-2.823]
</p><p>3 My reply:   You can actually use a symmetric long-tailed distribution such as t with low degrees of freedom. [sent-4, score-1.227]
</p><p>4 One striking feature of symmetric long-tailed distributions is that a small random sample from such a distribution can have outliers on one side or the other and look asymmetric. [sent-5, score-1.58]
</p><p>5 Just to see this, try the following in R:     par (mfrow=c(3,3), mar=c(1,1,1,1))  for (i in 1:9) hist (rt (100, 2), xlab="", ylab="", main="")      You’ll see some skewed distributions. [sent-6, score-0.761]
</p><p>6 So that’s the message (which I learned from an offhand comment of Rubin, actually):  if you want to model an asymmetric distribution with outliers, you can use a symmetric long-tailed model. [sent-7, score-1.655]
</p><p>7 Of course, if you really want to blow your mind, try this:     hist(rcauchy(10000))      If you’re like me, you’ll have to think a little bit about this before you see what’s happening. [sent-8, score-0.316]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('symmetric', 0.498), ('distribution', 0.407), ('hist', 0.323), ('asymmetric', 0.278), ('outliers', 0.214), ('mfrow', 0.139), ('ylab', 0.139), ('shravan', 0.139), ('contaminated', 0.133), ('xlab', 0.128), ('par', 0.121), ('skewed', 0.114), ('use', 0.114), ('offhand', 0.11), ('blow', 0.11), ('mar', 0.104), ('minimum', 0.096), ('realistic', 0.095), ('striking', 0.092), ('bda', 0.092), ('speed', 0.09), ('try', 0.087), ('degrees', 0.085), ('scores', 0.079), ('feature', 0.079), ('rubin', 0.078), ('happening', 0.077), ('light', 0.075), ('message', 0.069), ('normal', 0.068), ('presented', 0.068), ('learned', 0.068), ('ll', 0.067), ('predictive', 0.067), ('mind', 0.067), ('distributions', 0.065), ('actually', 0.064), ('chapter', 0.062), ('suggest', 0.062), ('want', 0.061), ('posterior', 0.061), ('side', 0.061), ('low', 0.059), ('see', 0.058), ('one', 0.058), ('main', 0.057), ('random', 0.056), ('show', 0.051), ('comment', 0.05), ('sample', 0.05)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="2128-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-09-How_to_model_distributions_that_have_outliers_in_one_direction.html">2128 andrew gelman stats-2013-12-09-How to model distributions that have outliers in one direction</a></p>
<p>Introduction: Shravan writes:
  
I have a problem very similar to the one presented chapter 6 of BDA, the speed of light example. You use the distribution of the minimum scores from the posterior predictive distribution, show that it’s not realistic given the data, and suggest that an asymmetric contaminated normal distribution or a symmetric long-tailed distribution would be better.


How does one use such a distribution?
  
My reply:
 
You can actually use a symmetric long-tailed distribution such as t with low degrees of freedom.  One striking feature of symmetric long-tailed distributions is that a small random sample from such a distribution can have outliers on one side or the other and look asymmetric.
 
Just to see this, try the following in R: 
  
par (mfrow=c(3,3), mar=c(1,1,1,1)) 
for (i in 1:9) hist (rt (100, 2), xlab="", ylab="", main="") 
 
 
You’ll see some skewed distributions.  So that’s the message (which I learned from an offhand comment of Rubin, actually):  if you want to model</p><p>2 0.1696008 <a title="2128-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-Another_reason_why_you_can_get_good_inferences_from_a_bad_model.html">1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</a></p>
<p>Introduction: John Cook  considers  how people justify probability distribution assumptions:
  
Sometimes distribution assumptions are not justified.


Sometimes distributions can be derived from fundamental principles [or] . . . on theoretical grounds. For example, large samples and the central limit theorem together may justify assuming that something is normally distributed.


Often the choice of distribution is somewhat arbitrary, chosen by intuition or for convenience, and then empirically shown to work well enough.


Sometimes a distribution can be a bad fit and still work well, depending on what you’re asking of it.
  
Cook continues:
  
The last point is particularly interesting. It’s not hard to imagine that a poor fit would produce poor results. It’s surprising when a poor fit produces good results.
  
And then he gives an example of an effective but inaccurate model used to model survival times in a clinical trial.  Cook explains:
  
The [poorly-fitting] method works well because of the q</p><p>3 0.16184424 <a title="2128-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>Introduction: Klaas Metselaar writes: 
  
  
I [Metselaar] am currently involved in a discussion about the use of the notion “predictive” as used in “posterior predictive check”. I would argue that the notion “predictive” should be reserved for posterior checks using information not used in the determination of the posterior. 
I quote from the discussion: 
“However, the predictive uncertainty in a Bayesian calculation requires sampling from all the random variables, and this includes both the model parameters and the residual error”.


My [Metselaar's] comment:


This may be exactly the point I am worried about: shouldn’t the predictive uncertainty be defined as sampling from the posterior parameter distribution +  residual error + sampling from the prediction error distribution? 
Residual error reduces to measurement error in the case of a  model which is perfect for the sample of experiments. Measurement error could be reduced to almost zero by ideal and perfect measurement instruments. 
I would h</p><p>4 0.15305972 <a title="2128-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-03-Boot.html">1881 andrew gelman stats-2013-06-03-Boot</a></p>
<p>Introduction: Joshua Hartshorne writes: 
  
  
I ran several large-N experiments (separate participants) and looked at performance against age. What we want to do is compare age-of-peak-performance across the different tasks (again, different participants).


We bootstrapped age-of-peak-performance. On each iteration, we sampled (with replacement) the X scores at each age, where X=num of participants at that age, and recorded the age at which performance peaked on that task. We then recorded the age at which performance was at peak and repeated. Once we had distributions of age-of-peak-performance, we used the means and SDs to calculate t-statistics to compare the results across different tasks. For graphical presentation, we used medians, interquartile ranges, and 95% confidence intervals (based on the distributions: the range within which 75% and 95% of the bootstrapped peaks appeared). 


While a number of people we consulted with thought this made a lot of sense, one reviewer of the paper insist</p><p>5 0.147461 <a title="2128-tfidf-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-Transformations_for_non-normal_data.html">2176 andrew gelman stats-2014-01-19-Transformations for non-normal data</a></p>
<p>Introduction: Steve Peterson writes:
  
I recently submitted a proposal on applying a Bayesian analysis to gender comparisons on motivational constructs. I had an idea on how to improve the model I used and was hoping you could give me some feedback.


The data come from a survey based on 5-point Likert scales. Different constructs are measured for each student as scores derived from averaging a student’s responses on particular subsets of survey questions. (I suppose it is not uncontroversial to treat these scores as interval measures and would be interested to hear if you have any objections.) I am comparing genders on each construct. Researchers typically use t-tests to do so.


To use a Bayesian approach I applied the programs written in R and JAGS by John Kruschke for estimating the difference of means:


http://www.indiana.edu/~kruschke/BEST/


An issue in that analysis is that the distributions of student scores are not normal. There was skewness in some of the distributions and not always in</p><p>6 0.14249313 <a title="2128-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>7 0.13697959 <a title="2128-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-21-The_future_%28and_past%29_of_statistical_sciences.html">2072 andrew gelman stats-2013-10-21-The future (and past) of statistical sciences</a></p>
<p>8 0.13069123 <a title="2128-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-16-The_%E2%80%9CWashington_read%E2%80%9D_and_the_algebra_of_conditional_distributions.html">961 andrew gelman stats-2011-10-16-The “Washington read” and the algebra of conditional distributions</a></p>
<p>9 0.12949958 <a title="2128-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-18-The_estimated_effect_size_is_implausibly_large.__Under_what_models_is_this_a_piece_of_evidence_that_the_true_effect_is_small%3F.html">808 andrew gelman stats-2011-07-18-The estimated effect size is implausibly large.  Under what models is this a piece of evidence that the true effect is small?</a></p>
<p>10 0.12637554 <a title="2128-tfidf-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-21-Random_matrices_in_the_news.html">2258 andrew gelman stats-2014-03-21-Random matrices in the news</a></p>
<p>11 0.11625019 <a title="2128-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>12 0.11271032 <a title="2128-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>13 0.1070539 <a title="2128-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<p>14 0.099649772 <a title="2128-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-17-Christian_Robert_on_the_Jeffreys-Lindley_paradox%3B_more_generally%2C_it%E2%80%99s_good_news_when_philosophical_arguments_can_be_transformed_into_technical_modeling_issues.html">2027 andrew gelman stats-2013-09-17-Christian Robert on the Jeffreys-Lindley paradox; more generally, it’s good news when philosophical arguments can be transformed into technical modeling issues</a></p>
<p>15 0.097759143 <a title="2128-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>16 0.094667166 <a title="2128-tfidf-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-29-Could_someone_please_set_this_as_the_new_R_default_in_base_graphics%3F.html">379 andrew gelman stats-2010-10-29-Could someone please set this as the new R default in base graphics?</a></p>
<p>17 0.093397625 <a title="2128-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-30-More_on_the_correlation_between_statistical_and_political_ideology.html">638 andrew gelman stats-2011-03-30-More on the correlation between statistical and political ideology</a></p>
<p>18 0.091433316 <a title="2128-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-05-How_accurate_is_your_gaydar%3F.html">944 andrew gelman stats-2011-10-05-How accurate is your gaydar?</a></p>
<p>19 0.090087071 <a title="2128-tfidf-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>20 0.090004422 <a title="2128-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-22-Extreme_events_as_evidence_for_differences_in_distributions.html">1424 andrew gelman stats-2012-07-22-Extreme events as evidence for differences in distributions</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.116), (1, 0.096), (2, 0.026), (3, 0.035), (4, 0.036), (5, -0.024), (6, 0.062), (7, 0.012), (8, -0.029), (9, -0.016), (10, 0.0), (11, -0.005), (12, 0.001), (13, -0.03), (14, -0.033), (15, -0.004), (16, -0.001), (17, -0.005), (18, 0.009), (19, -0.027), (20, 0.045), (21, -0.008), (22, -0.002), (23, -0.03), (24, 0.024), (25, 0.049), (26, -0.041), (27, 0.049), (28, 0.038), (29, 0.052), (30, 0.012), (31, 0.013), (32, -0.035), (33, 0.024), (34, 0.011), (35, 0.011), (36, -0.036), (37, 0.01), (38, -0.041), (39, 0.009), (40, 0.079), (41, -0.03), (42, 0.008), (43, -0.025), (44, -0.029), (45, 0.005), (46, 0.046), (47, 0.024), (48, 0.037), (49, -0.023)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96284658 <a title="2128-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-09-How_to_model_distributions_that_have_outliers_in_one_direction.html">2128 andrew gelman stats-2013-12-09-How to model distributions that have outliers in one direction</a></p>
<p>Introduction: Shravan writes:
  
I have a problem very similar to the one presented chapter 6 of BDA, the speed of light example. You use the distribution of the minimum scores from the posterior predictive distribution, show that it’s not realistic given the data, and suggest that an asymmetric contaminated normal distribution or a symmetric long-tailed distribution would be better.


How does one use such a distribution?
  
My reply:
 
You can actually use a symmetric long-tailed distribution such as t with low degrees of freedom.  One striking feature of symmetric long-tailed distributions is that a small random sample from such a distribution can have outliers on one side or the other and look asymmetric.
 
Just to see this, try the following in R: 
  
par (mfrow=c(3,3), mar=c(1,1,1,1)) 
for (i in 1:9) hist (rt (100, 2), xlab="", ylab="", main="") 
 
 
You’ll see some skewed distributions.  So that’s the message (which I learned from an offhand comment of Rubin, actually):  if you want to model</p><p>2 0.81689519 <a title="2128-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-17-Jumping_off_the_edge_of_the_world.html">858 andrew gelman stats-2011-08-17-Jumping off the edge of the world</a></p>
<p>Introduction: Tomas Iesmantas writes:
  
I’m facing a problem where parameter space is bounded, e.g. all parameters have to be positive. If in MCMC as proposal distribution I use normal distribution, then at some iterations I get negative proposals. So my question is: should I use recalculation of acceptance probability every time I reject the proposal (something like in delayed rejection method), or I have to use another proposal (like lognormal, truncated normal, etc.)?
  
The simplest solution is to just calculate p(theta)=0 for theta outside the legal region, thus reject those jumps.  This will work fine (just remember that when you reject, you have to stay at the last value for one more iteration), but if you’re doing these rejections all the time, you might want to reparameterize your space, for example using logs for positive parameters, logits for constrained parameters, and softmax for parameters that are constrained to sum to 1.</p><p>3 0.80100775 <a title="2128-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-28-Path_sampling_for_models_of_varying_dimension.html">1089 andrew gelman stats-2011-12-28-Path sampling for models of varying dimension</a></p>
<p>Introduction: Somebody asks: 
  
  
Iâ&euro;&trade;m reading your paper on path sampling. It essentially solves the 
problem of computing the ratio \int q0(omega)d omega/\int q1(omega) d omega. I.e the arguments in q0() and q1() are the same. But this assumption is not always true in Bayesian model selection using Bayes factor.


In general (for BF), we have this problem, t1 and t2 may have no relation at all.


\int f1(y|t1)p1(t1) d t1 / \int f2(y|t2)p2(t2) d t2


As an example, suppose that we want to compare two sets of normally distributed data with known variance whether they have the same mean (H0) or they are not necessarily have the same mean (H1). Then the dummy variable should be mu in H0 (which is the common mean of both set of samples), and should be (mu1, mu2) (which are the means for each set of samples).


One straight method to address my problem is to preform path integration for the numerate and the denominator, as both the numerate and the denominator are integrals. Each integral can be rewrit</p><p>4 0.76747543 <a title="2128-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-07-Chi-square_FAIL_when_many_cells_have_small_expected_values.html">996 andrew gelman stats-2011-11-07-Chi-square FAIL when many cells have small expected values</a></p>
<p>Introduction: William Perkins, Mark Tygert, and Rachel Ward  write :
  
If a discrete probability distribution in a model being tested for goodness-of-fit is not close to uniform, then forming the Pearson χ2 statistic can involve division by nearly zero. This often leads to serious trouble in practice — even in the absence of round-off errors . . .
  
The problem is not merely that the chi-squared  statistic  doesn’t have the advertised chi-squared  distribution —a reference distribution can always be computed via simulation, either using the posterior predictive distribution or by conditioning on a point estimate of the cell expectations and then making a degrees-of-freedom sort of adjustment.
 
Rather, the problem is that, when there are lots of cells with near-zero expectation, the chi-squared test is mostly noise.
 
And this is not merely a theoretical problem.  It comes up in real examples.
 
Here’s one, taken from the classic 1992 genetics paper of Guo and Thomspson:
 
   
 
And here are the e</p><p>5 0.7572093 <a title="2128-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-16-The_%E2%80%9CWashington_read%E2%80%9D_and_the_algebra_of_conditional_distributions.html">961 andrew gelman stats-2011-10-16-The “Washington read” and the algebra of conditional distributions</a></p>
<p>Introduction: I was trying to explain in class how a (Bayesian) statistician reads the formula for a probability distribution.  In old-fashioned statistics textbooks you’re told that if you want to compute a conditional distribution from a joint distribution you need to do some heavy math:  p(a|b) = p(a,b)/\int p(a’,b)da’.
 
When doing Bayesian statistics, though, you usually don’t have to do the integration or the division. If you have parameters theta and data y, you first write p(y,theta).  Then to get p(theta|y), you  don’t  need to integrate or divide.  All you have to do is look at p(y,theta) in a certain way:  Treat y as a constant and theta as a variable.  Similarly, if you’re doing the Gibbs sampler and want a conditional distribution, just consider the parameter you’re updating as the variable and everything else as a constant.  No need to integrate or divide, you just take the joint distribution and look at it from the right perspective.
 
Awhile ago Yair told me there’s something called</p><p>6 0.74680489 <a title="2128-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-31-Using_sample_size_in_the_prior_distribution.html">547 andrew gelman stats-2011-01-31-Using sample size in the prior distribution</a></p>
<p>7 0.73899943 <a title="2128-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>8 0.73771363 <a title="2128-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-Another_reason_why_you_can_get_good_inferences_from_a_bad_model.html">1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</a></p>
<p>9 0.73750371 <a title="2128-lsi-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-28-Another_argument_in_favor_of_expressing_conditional_probability_statements_using_the_population_distribution.html">56 andrew gelman stats-2010-05-28-Another argument in favor of expressing conditional probability statements using the population distribution</a></p>
<p>10 0.72786444 <a title="2128-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-16-%E2%80%9CReal_data_can_be_a_pain%E2%80%9D.html">1460 andrew gelman stats-2012-08-16-“Real data can be a pain”</a></p>
<p>11 0.71272188 <a title="2128-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-24-Analyzing_photon_counts.html">1509 andrew gelman stats-2012-09-24-Analyzing photon counts</a></p>
<p>12 0.70273077 <a title="2128-lsi-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-Transformations_for_non-normal_data.html">2176 andrew gelman stats-2014-01-19-Transformations for non-normal data</a></p>
<p>13 0.69918084 <a title="2128-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<p>14 0.69704628 <a title="2128-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-03-Boot.html">1881 andrew gelman stats-2013-06-03-Boot</a></p>
<p>15 0.67894244 <a title="2128-lsi-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>16 0.67306346 <a title="2128-lsi-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-21-Random_matrices_in_the_news.html">2258 andrew gelman stats-2014-03-21-Random matrices in the news</a></p>
<p>17 0.67075604 <a title="2128-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-27-Average_predictive_comparisons_when_changing_a_pair_of_variables.html">1346 andrew gelman stats-2012-05-27-Average predictive comparisons when changing a pair of variables</a></p>
<p>18 0.65983766 <a title="2128-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-16-Update_on_the_generalized_method_of_moments.html">519 andrew gelman stats-2011-01-16-Update on the generalized method of moments</a></p>
<p>19 0.6516872 <a title="2128-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-25-Bayes-respecting_experimental_design_and_other_things.html">1955 andrew gelman stats-2013-07-25-Bayes-respecting experimental design and other things</a></p>
<p>20 0.64898831 <a title="2128-lsi-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-14-Confusion_about_continuous_probability_densities.html">341 andrew gelman stats-2010-10-14-Confusion about continuous probability densities</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(16, 0.052), (21, 0.135), (24, 0.093), (28, 0.017), (42, 0.153), (44, 0.027), (56, 0.014), (73, 0.047), (78, 0.041), (80, 0.017), (99, 0.277)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91780066 <a title="2128-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-09-How_to_model_distributions_that_have_outliers_in_one_direction.html">2128 andrew gelman stats-2013-12-09-How to model distributions that have outliers in one direction</a></p>
<p>Introduction: Shravan writes:
  
I have a problem very similar to the one presented chapter 6 of BDA, the speed of light example. You use the distribution of the minimum scores from the posterior predictive distribution, show that it’s not realistic given the data, and suggest that an asymmetric contaminated normal distribution or a symmetric long-tailed distribution would be better.


How does one use such a distribution?
  
My reply:
 
You can actually use a symmetric long-tailed distribution such as t with low degrees of freedom.  One striking feature of symmetric long-tailed distributions is that a small random sample from such a distribution can have outliers on one side or the other and look asymmetric.
 
Just to see this, try the following in R: 
  
par (mfrow=c(3,3), mar=c(1,1,1,1)) 
for (i in 1:9) hist (rt (100, 2), xlab="", ylab="", main="") 
 
 
You’ll see some skewed distributions.  So that’s the message (which I learned from an offhand comment of Rubin, actually):  if you want to model</p><p>2 0.91150594 <a title="2128-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-30-What_Auteur_Theory_and_Freshwater_Economics_have_in_common.html">60 andrew gelman stats-2010-05-30-What Auteur Theory and Freshwater Economics have in common</a></p>
<p>Introduction: Mark Palko  writes :
  
 
We’ll define freshwater economics as the theory that economic behavior (and perhaps most non-economic behavior) can be explained using the concepts of rational actors and efficient markets and auteur theory as the idea that most films (particularly great films) represent the artistic vision of a single author (almost always the director) and the best way to approach one of those films is through the body of work of its author. Both of these definitions are oversimplified and a bit unfair but they will get the discussion started. . . .


Compared to their nearest neighbors, film criticism and economics (particularly macroeconomics) are both difficult, messy fields. Films are collaborative efforts where individual contributions defy attribution and creative decisions often can’t be distinguished from accidents of filming. Worse yet, most films are the product of large corporations which means that dozens of VPs and executives might have played a role (sometimes</p><p>3 0.90314507 <a title="2128-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-25-Good_introductory_book_for_statistical_computation%3F.html">590 andrew gelman stats-2011-02-25-Good introductory book for statistical computation?</a></p>
<p>Introduction: Geen Tomko asks:
  
Can you recommend a good introductory book for statistical computation? Mostly, something that would help make it easier in collecting and analyzing data from student test scores.
  
I don’t know.  Usually, when people ask for a starter statistics book, my recommendation (beyond my own books) is The Statistical Sleuth.  But that’s not really a computation book.  ARM isn’t really a statistical computation book either.  But the statistical computation books that I’ve seen don’t seems so relevant for the analyses that Tomko is looking for.  For example, the R book of Venables and Ripley focuses on nonparametric statistics, which is fine but seems a bit esoteric for these purposes.
 
Does anyone have any suggestions?</p><p>4 0.90144742 <a title="2128-lda-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-23-In_which_I_disagree_with_John_Maynard_Keynes.html">1775 andrew gelman stats-2013-03-23-In which I disagree with John Maynard Keynes</a></p>
<p>Introduction: In his  review  in 1938 of  Historical Development of the Graphical Representation of Statistical Data , by H. Gray Funkhauser, for  The Economic Journal , the great economist writes:
  
Perhaps the most striking outcome of Mr. Funkhouser’s researches is the fact of the very slow progress which graphical methods made until quite recently. . . . In the first fifty volumes of the Statistical Journal, 1837-87, only fourteen graphs are printed altogether. It is surprising to be told that Laplace never drew a graph of the normal law of error . . . Edgeworth made no use of statistical charts as distinct from mathematical diagrams.


Apart from Quetelet and Jevons, the most important influences were probably those of Galton and of Mulhall’s Dictionary, first published in 1884. Galton was indeed following his father and grandfather in this field, but his pioneer work was mainly restricted to meteorological maps, and he did not contribute to the development of the graphical representation of ec</p><p>5 0.90001798 <a title="2128-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-15-1-2_social_scientist_%2B_1-2_politician_%3D_%3F%3F%3F.html">713 andrew gelman stats-2011-05-15-1-2 social scientist + 1-2 politician = ???</a></p>
<p>Introduction: A couple things in  this interview  by Andrew Goldman of Larry Summers currently irritated me.
 
I’ll give the quotes and then explain my annoyance.
  
 
1.  Goldman: What would the economy look like now if $1.2 trillion had been spent?


Summers:  I think it’s an artificial question because there would have been all kinds of problems in actually moving $1.2 trillion dollars through the system — finding enough bridge projects that were ready to go and the like. But the recovery probably would have proceeded more rapidly if the fiscal program had been larger. . . .


2.  Goldman:  You’re aware of — and were making light of — the fact that you occasionally rub people the wrong way.


Summers:   In meetings, I’m more focused on trying to figure out what the right answer is than making everybody feel validated. In Washington and at Harvard, that sometimes rubs people the wrong way.
 

 
OK, now my reactions:
 
1.  Not enough bridge projects, huh?  I don’t believe it.  We’ve been hearing fo</p><p>6 0.89853519 <a title="2128-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-25-Freakonomics_Experiments.html">1692 andrew gelman stats-2013-01-25-Freakonomics Experiments</a></p>
<p>7 0.89574099 <a title="2128-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-18-The_estimated_effect_size_is_implausibly_large.__Under_what_models_is_this_a_piece_of_evidence_that_the_true_effect_is_small%3F.html">808 andrew gelman stats-2011-07-18-The estimated effect size is implausibly large.  Under what models is this a piece of evidence that the true effect is small?</a></p>
<p>8 0.8937543 <a title="2128-lda-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-26-Tough_love_as_a_style_of_writing.html">111 andrew gelman stats-2010-06-26-Tough love as a style of writing</a></p>
<p>9 0.89341038 <a title="2128-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-23-Science%2C_ideology%2C_and_human_origins.html">483 andrew gelman stats-2010-12-23-Science, ideology, and human origins</a></p>
<p>10 0.89049679 <a title="2128-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-15-Freakonomics%3A__What_went_wrong%3F.html">1060 andrew gelman stats-2011-12-15-Freakonomics:  What went wrong?</a></p>
<p>11 0.88280505 <a title="2128-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-07-Hipmunk_FAIL%3A__Graphics_without_content_is_not_enough.html">894 andrew gelman stats-2011-09-07-Hipmunk FAIL:  Graphics without content is not enough</a></p>
<p>12 0.88237619 <a title="2128-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-05-An_unexpected_benefit_of_Arrow%E2%80%99s_other_theorem.html">746 andrew gelman stats-2011-06-05-An unexpected benefit of Arrow’s other theorem</a></p>
<p>13 0.88234115 <a title="2128-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-13-Economic_policy_does_not_occur_in_a_political_vacuum.html">1936 andrew gelman stats-2013-07-13-Economic policy does not occur in a political vacuum</a></p>
<p>14 0.88182044 <a title="2128-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-25-Chris_Schmid_on_Evidence_Based_Medicine.html">1138 andrew gelman stats-2012-01-25-Chris Schmid on Evidence Based Medicine</a></p>
<p>15 0.88032395 <a title="2128-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-27-Neumann_update.html">432 andrew gelman stats-2010-11-27-Neumann update</a></p>
<p>16 0.8800922 <a title="2128-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-22-Please_stop_me_before_I_barf_again.html">1275 andrew gelman stats-2012-04-22-Please stop me before I barf again</a></p>
<p>17 0.88005888 <a title="2128-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-20-I_think_you_knew_this_already.html">218 andrew gelman stats-2010-08-20-I think you knew this already</a></p>
<p>18 0.87825823 <a title="2128-lda-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-30-That_puzzle-solving_feeling.html">492 andrew gelman stats-2010-12-30-That puzzle-solving feeling</a></p>
<p>19 0.87812549 <a title="2128-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-18-What_to_read_to_catch_up_on_multivariate_statistics%3F.html">1726 andrew gelman stats-2013-02-18-What to read to catch up on multivariate statistics?</a></p>
<p>20 0.87797892 <a title="2128-lda-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-02-Why_we_hate_stepwise_regression.html">2357 andrew gelman stats-2014-06-02-Why we hate stepwise regression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
