<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2086 andrew gelman stats-2013-11-03-How best to compare effects measured in two different time periods?</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-2086" href="#">andrew_gelman_stats-2013-2086</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2086 andrew gelman stats-2013-11-03-How best to compare effects measured in two different time periods?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-2086-html" href="http://andrewgelman.com/2013/11/03/how-best-to-compare-effects-measured-in-two-different-time-periods/">html</a></p><p>Introduction: I received the following email from someone who wishes to remain anonymous:
  
My colleague and I are trying to understand the best way to approach a problem involving measuring a group of individuals’ abilities across time, and are hoping you can offer some guidance.


We are trying to analyze the combined effect of two distinct groups of people (A and B, with no overlap between A and B) who collaborate to produce a binary outcome, using a mixed logistic regression along the lines of the following.


Outcome ~ (1 | A) + (1 | B) + Other variables


What we’re interested in testing was whether the observed A random effects in period  1 are predictive of the A random effects in the following period 2.  Our idea being create two models, each using a different period’s worth of data, to create two sets of A coefficients, then observe the relationship between the two.  If the A’s have a persistent ability across periods, the coefficients should be correlated or show a linear-ish relationshi</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I received the following email from someone who wishes to remain anonymous:    My colleague and I are trying to understand the best way to approach a problem involving measuring a group of individuals’ abilities across time, and are hoping you can offer some guidance. [sent-1, score-0.331]
</p><p>2 We are trying to analyze the combined effect of two distinct groups of people (A and B, with no overlap between A and B) who collaborate to produce a binary outcome, using a mixed logistic regression along the lines of the following. [sent-2, score-0.334]
</p><p>3 Outcome ~ (1 | A) + (1 | B) + Other variables   What we’re interested in testing was whether the observed A random effects in period  1 are predictive of the A random effects in the following period 2. [sent-3, score-2.168]
</p><p>4 Our idea being create two models, each using a different period’s worth of data, to create two sets of A coefficients, then observe the relationship between the two. [sent-4, score-0.419]
</p><p>5 If the A’s have a persistent ability across periods, the coefficients should be correlated or show a linear-ish relationship. [sent-5, score-0.284]
</p><p>6 There’s some difference on opinion on the best way to proceed, but neither of us are knowledgeable enough in this area. [sent-7, score-0.2]
</p><p>7 More specifically, should we compare   (a) Period 1 A random effects estimates vs. [sent-8, score-0.597]
</p><p>8 Period 2 B random effects estimates, or  (b) Period 1 A random effects estimates vs. [sent-9, score-0.982]
</p><p>9 Period 2 B fixed effects estimates   Option (a) uses the same type of model for each sample, seemingly making comparison of effects across periods more direct. [sent-10, score-1.029]
</p><p>10 And if balancing bias and variance in one sample is appropriate, it should be for the other as well. [sent-11, score-0.34]
</p><p>11 One can also look at the variance parameter for both periods to see if the population has changed characteristics. [sent-12, score-0.292]
</p><p>12 Option (b) uses an unbiased estimates for period 2, with the argument that (a) introduces bias in both samples that will artificially increase the correlation between the effects across the samples, making it appear to be a stronger relationship than truly exists. [sent-13, score-1.773]
</p><p>13 I have consulted your book, Data Analysis Using Regression and Multilevel/Hierarchical Models, but did not see this particular issue addressed (or perhaps, it is in there, but didn’t understand it for what it was). [sent-14, score-0.136]
</p><p>14 My reply:  Best would be to fit a varying-intercept, varying-slope model, where the intercept is the avg coef and the slope is the difference between time 1 and time 2. [sent-15, score-0.341]
</p><p>15 Thus,   Outcome ~ (1 + Period | A) + (1 + Period | B) + Period * Other variables,   where Period is coded as -0. [sent-16, score-0.066]
</p><p>16 Then you can look at the sizes of the slopes for Period for the different people, or for a quick summary the standard deviations of the slopes. [sent-19, score-0.128]
</p><p>17 I recommend fitting the model using blmer (better than lmer because it avoids zero group-level variance estimates and degenerate group-level covariance matrix estimates) and displaying using display(). [sent-20, score-0.863]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('period', 0.667), ('effects', 0.218), ('estimates', 0.212), ('periods', 0.173), ('random', 0.167), ('across', 0.124), ('outcome', 0.123), ('variance', 0.119), ('option', 0.099), ('relationship', 0.095), ('using', 0.092), ('samples', 0.091), ('coefficients', 0.089), ('degenerate', 0.088), ('create', 0.085), ('uses', 0.084), ('balancing', 0.083), ('blme', 0.083), ('coef', 0.079), ('artificially', 0.079), ('bias', 0.079), ('avoids', 0.077), ('consulted', 0.074), ('avg', 0.074), ('wishes', 0.074), ('best', 0.071), ('persistent', 0.071), ('knowledgeable', 0.068), ('lmer', 0.067), ('slopes', 0.067), ('introduces', 0.067), ('collaborate', 0.067), ('coded', 0.066), ('intercept', 0.065), ('variables', 0.064), ('abilities', 0.062), ('proceed', 0.062), ('addressed', 0.062), ('observe', 0.062), ('slope', 0.062), ('distinct', 0.061), ('difference', 0.061), ('deviations', 0.061), ('sample', 0.059), ('overlap', 0.058), ('covariance', 0.058), ('displaying', 0.058), ('unbiased', 0.057), ('regression', 0.056), ('anonymous', 0.056)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="2086-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-03-How_best_to_compare_effects_measured_in_two_different_time_periods%3F.html">2086 andrew gelman stats-2013-11-03-How best to compare effects measured in two different time periods?</a></p>
<p>Introduction: I received the following email from someone who wishes to remain anonymous:
  
My colleague and I are trying to understand the best way to approach a problem involving measuring a group of individuals’ abilities across time, and are hoping you can offer some guidance.


We are trying to analyze the combined effect of two distinct groups of people (A and B, with no overlap between A and B) who collaborate to produce a binary outcome, using a mixed logistic regression along the lines of the following.


Outcome ~ (1 | A) + (1 | B) + Other variables


What we’re interested in testing was whether the observed A random effects in period  1 are predictive of the A random effects in the following period 2.  Our idea being create two models, each using a different period’s worth of data, to create two sets of A coefficients, then observe the relationship between the two.  If the A’s have a persistent ability across periods, the coefficients should be correlated or show a linear-ish relationshi</p><p>2 0.17655671 <a title="2086-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-17-Hierarchical-multilevel_modeling_with_%E2%80%9Cbig_data%E2%80%9D.html">1267 andrew gelman stats-2012-04-17-Hierarchical-multilevel modeling with “big data”</a></p>
<p>Introduction: Dean Eckles writes:
  
I make extensive use of random effects models in my academic and industry research, as they are very often appropriate.


However, with very large data sets, I am not sure what to do. Say I have thousands of levels of a grouping factor, and the number of observations totals in the billions. Despite having lots of observations, I am often either dealing with (a) small effects or (b) trying to fit models with many predictors.


So I would really like to use a random effects model to borrow strength across the levels of the grouping factor, but I am not sure how to practically do this. Are you aware of any approaches to fitting random effects models (including approximations) that work for very large data sets? For example, applying a procedure to each group, and then using the results of this to shrink each fit in some appropriate way.


Just to clarify, here I am only worried about the non-crossed and in fact single-level case. I don’t see any easy route for cross</p><p>3 0.16674156 <a title="2086-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-30-Fixed_effects%2C_followed_by_Bayes_shrinkage%3F.html">1644 andrew gelman stats-2012-12-30-Fixed effects, followed by Bayes shrinkage?</a></p>
<p>Introduction: Stuart Buck writes:
  
I have a question about  fixed effects vs. random effects . Amongst economists who study teacher value-added, it has become common to see people saying that they estimated teacher fixed effects (via least squares dummy variables, so that there is a parameter for each teacher), but that they then applied empirical Bayes shrinkage so that the teacher effects are brought closer to the mean.  (See  this paper  by Jacob and Lefgren, for example.)


Can that really be what they are doing? Why wouldn’t they just run random (modeled) effects in the first place? I feel like there’s something I’m missing.
  
My reply:  I don’t know the full story here, but I’m thinking there are two goals, first to get an unbiased estimate of an overall treatment effect (and there the econometricians prefer so-called fixed effects; I disagree with them on this but I know where they’re coming from) and second to estimate individual teacher effects (and there it makes sense to use so-called</p><p>4 0.15418218 <a title="2086-tfidf-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-17-So-called_fixed_and_random_effects.html">472 andrew gelman stats-2010-12-17-So-called fixed and random effects</a></p>
<p>Introduction: Someone writes:
  
I am hoping you can give me some advice about when to use fixed and random effects model. I am currently working on a paper that examines the effect of . . . by comparing states . . .


It got reviewed . . . by three economists and all suggest that we run a fixed effects model.  We ran a hierarchial model in the paper that allow the intercept and slope to vary before and after . . . My question is which is correct? We have ran it both ways and really it makes no difference which model you run, the results are very similar. But for my own learning, I would really like to understand which to use under what circumstances.  Is the fact that we use the whole population reason enough to just run a fixed effect model?


Perhaps you can suggest a good reference to this question of when to run a fixed vs. random effects model.
  
I’m not always sure what is meant by a “fixed effects model”; see my paper on Anova for discussion of the problems with this terminology:
 
http://w</p><p>5 0.14589287 <a title="2086-tfidf-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-31-Analyzing_the_entire_population_rather_than_a_sample.html">383 andrew gelman stats-2010-10-31-Analyzing the entire population rather than a sample</a></p>
<p>Introduction: Lee Mobley writes:
  
I recently read what you posted on your blog  How does statistical analysis differ when analyzing the entire population rather than a sample? 


What you said in the blog accords with my training in econometrics.  However I am concerned about a new wrinkle on this problem that derives from multilevel modeling.
      
We are analyzing multilevel models of the probability of using cancer screening for the entire Medicare population. I argue that every state has different systems in place (politics, cancer control efforts, culture, insurance regulations, etc) so that essentially a different probability generating mechanism is in place for each state. Thus I estimate 50 separate regressions for the populations in each state, and then note and map the variability in the effect estimates (slope parameters) for each covariate.


Reviewers argue that I should be using random slopes modeling, pooling all individuals in all states together. I am familiar with this approach</p><p>6 0.14379972 <a title="2086-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>7 0.13320123 <a title="2086-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-Multilevel_modeling_even_when_you%E2%80%99re_not_interested_in_predictions_for_new_groups.html">1194 andrew gelman stats-2012-03-04-Multilevel modeling even when you’re not interested in predictions for new groups</a></p>
<p>8 0.13123092 <a title="2086-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-13-Secret_weapon_with_rare_events.html">610 andrew gelman stats-2011-03-13-Secret weapon with rare events</a></p>
<p>9 0.12981707 <a title="2086-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-02-Fixed_effects_and_identification.html">1241 andrew gelman stats-2012-04-02-Fixed effects and identification</a></p>
<p>10 0.12942354 <a title="2086-tfidf-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-12-Finite-population_standard_deviation_in_a_hierarchical_model.html">464 andrew gelman stats-2010-12-12-Finite-population standard deviation in a hierarchical model</a></p>
<p>11 0.12894456 <a title="2086-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-22-Evaluating_the_impacts_of_welfare_reform%3F.html">1732 andrew gelman stats-2013-02-22-Evaluating the impacts of welfare reform?</a></p>
<p>12 0.12869848 <a title="2086-tfidf-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-10-R_vs._Stata%2C_or%2C_Different_ways_to_estimate_multilevel_models.html">269 andrew gelman stats-2010-09-10-R vs. Stata, or, Different ways to estimate multilevel models</a></p>
<p>13 0.12777372 <a title="2086-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-09-%E2%80%9CHeterogeneity_of_variance_in_experimental_studies%3A__A_challenge_to_conventional_interpretations%E2%80%9D.html">1891 andrew gelman stats-2013-06-09-“Heterogeneity of variance in experimental studies:  A challenge to conventional interpretations”</a></p>
<p>14 0.12573448 <a title="2086-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-12-year_%2B_%281%7Cyear%29.html">851 andrew gelman stats-2011-08-12-year + (1|year)</a></p>
<p>15 0.12504771 <a title="2086-tfidf-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Blending_results_from_two_relatively_independent_multi-level_models.html">250 andrew gelman stats-2010-09-02-Blending results from two relatively independent multi-level models</a></p>
<p>16 0.12490521 <a title="2086-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-10-The_birthday_problem.html">1976 andrew gelman stats-2013-08-10-The birthday problem</a></p>
<p>17 0.12336951 <a title="2086-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-11-%E2%80%9C2_level_logit_with_2_REs_%26_large_sample._computational_nightmare_%E2%80%93_please_help%E2%80%9D.html">759 andrew gelman stats-2011-06-11-“2 level logit with 2 REs & large sample. computational nightmare – please help”</a></p>
<p>18 0.11267401 <a title="2086-tfidf-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-19-Analysis_of_survey_data%3A_Design_based_models_vs._hierarchical_modeling%3F.html">352 andrew gelman stats-2010-10-19-Analysis of survey data: Design based models vs. hierarchical modeling?</a></p>
<p>19 0.11221779 <a title="2086-tfidf-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-02-The_problem_of_overestimation_of_group-level_variance_parameters.html">63 andrew gelman stats-2010-06-02-The problem of overestimation of group-level variance parameters</a></p>
<p>20 0.11014667 <a title="2086-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-24-Estimating_and_summarizing_inference_for_hierarchical_variance_parameters_when_the_number_of_groups_is_small.html">2145 andrew gelman stats-2013-12-24-Estimating and summarizing inference for hierarchical variance parameters when the number of groups is small</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.176), (1, 0.114), (2, 0.12), (3, -0.084), (4, 0.124), (5, 0.01), (6, 0.001), (7, -0.031), (8, 0.052), (9, 0.074), (10, 0.011), (11, 0.002), (12, 0.067), (13, -0.017), (14, 0.043), (15, 0.041), (16, -0.059), (17, 0.004), (18, -0.019), (19, 0.038), (20, -0.022), (21, -0.013), (22, 0.023), (23, -0.006), (24, -0.015), (25, -0.047), (26, -0.062), (27, 0.055), (28, 0.002), (29, 0.029), (30, 0.015), (31, -0.01), (32, -0.043), (33, -0.025), (34, 0.024), (35, -0.032), (36, -0.049), (37, 0.001), (38, 0.014), (39, 0.012), (40, 0.033), (41, -0.004), (42, 0.029), (43, 0.014), (44, -0.04), (45, 0.022), (46, 0.029), (47, 0.002), (48, 0.033), (49, -0.037)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98364121 <a title="2086-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-03-How_best_to_compare_effects_measured_in_two_different_time_periods%3F.html">2086 andrew gelman stats-2013-11-03-How best to compare effects measured in two different time periods?</a></p>
<p>Introduction: I received the following email from someone who wishes to remain anonymous:
  
My colleague and I are trying to understand the best way to approach a problem involving measuring a group of individuals’ abilities across time, and are hoping you can offer some guidance.


We are trying to analyze the combined effect of two distinct groups of people (A and B, with no overlap between A and B) who collaborate to produce a binary outcome, using a mixed logistic regression along the lines of the following.


Outcome ~ (1 | A) + (1 | B) + Other variables


What we’re interested in testing was whether the observed A random effects in period  1 are predictive of the A random effects in the following period 2.  Our idea being create two models, each using a different period’s worth of data, to create two sets of A coefficients, then observe the relationship between the two.  If the A’s have a persistent ability across periods, the coefficients should be correlated or show a linear-ish relationshi</p><p>2 0.85739368 <a title="2086-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-Multilevel_modeling_even_when_you%E2%80%99re_not_interested_in_predictions_for_new_groups.html">1194 andrew gelman stats-2012-03-04-Multilevel modeling even when you’re not interested in predictions for new groups</a></p>
<p>Introduction: Fred Wu writes: 
  
  
I work at National Prescribing Services in Australia.  I have a database representing say, antidiabetic drug utilisation for the entire Australia in the past few years. I planned to do a longitudinal analysis across GP Division Network (112 divisions in AUS) using mixed-effects models (or as you called in your book varying intercept and varying slope) on this data. 


The problem here is: as data actually represent the population who use antidiabetic drugs in AUS, should I use 112 fixed dummy variables to capture the random variations or use varying intercept and varying slope for the model ? Because some one may aruge, like divisions in AUS or states in USA can hardly be considered from a “superpopulation”, then fixed dummies should be used.  What I think is the population are those who use the drugs, what will happen when the rest need to use them? In terms of exchangeability, using varying intercept and varying slopes can be justified.


Also you provided in y</p><p>3 0.84199113 <a title="2086-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-10-R_vs._Stata%2C_or%2C_Different_ways_to_estimate_multilevel_models.html">269 andrew gelman stats-2010-09-10-R vs. Stata, or, Different ways to estimate multilevel models</a></p>
<p>Introduction: Cyrus writes:
  
I [Cyrus] was teaching a class on multilevel modeling, and we were playing around with different method to fit a random effects logit model with 2 random intercepts—one corresponding to “family” and another corresponding to “community” (labeled “mom” and “cluster” in the data, respectively).  There are also a few regressors at the individual, family, and community level.  We were replicating in part some of the results from the  following paper :  Improved estimation procedures for multilevel models with binary response: a case-study, by G Rodriguez, N Goldman.


(I say “replicating in part” because we didn’t include all the regressors that they use, only a subset.)  We were looking at the performance of estimation via glmer in R’s lme4 package, glmmPQL in R’s MASS package, and Stata’s xtmelogit.  We wanted to study the performance of various estimation methods, including adaptive quadrature methods and penalized quasi-likelihood.


I was shocked to discover that glmer</p><p>4 0.82198346 <a title="2086-lsi-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-12-Finite-population_standard_deviation_in_a_hierarchical_model.html">464 andrew gelman stats-2010-12-12-Finite-population standard deviation in a hierarchical model</a></p>
<p>Introduction: Karri Seppa writes:
  
My topic is regional variation in the cause-specific survival of breast cancer patients across the 21 hospital districts in Finland, this component being modeled by random effects. I am interested mainly in the district-specific effects, and with a hierarchical model I can get reasonable estimates also for sparsely populated districts.


Based on the recommendation given in the book by yourself and Dr. Hill (2007) I tend to think that the finite-population variance would be an appropriate measure to summarize the overall variation across the 21 districts. However, I feel it is somewhat incoherent first to assume a Normal distribution for the district effects, involving a “superpopulation” variance parameter, and then to compute the finite-population variance from the estimated district-specific parameters. I wonder whether the finite-population variance were more appropriate in the context of a model with fixed district effects?
  
My reply:
  

 
I agree that th</p><p>5 0.81132281 <a title="2086-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-17-Hierarchical-multilevel_modeling_with_%E2%80%9Cbig_data%E2%80%9D.html">1267 andrew gelman stats-2012-04-17-Hierarchical-multilevel modeling with “big data”</a></p>
<p>Introduction: Dean Eckles writes:
  
I make extensive use of random effects models in my academic and industry research, as they are very often appropriate.


However, with very large data sets, I am not sure what to do. Say I have thousands of levels of a grouping factor, and the number of observations totals in the billions. Despite having lots of observations, I am often either dealing with (a) small effects or (b) trying to fit models with many predictors.


So I would really like to use a random effects model to borrow strength across the levels of the grouping factor, but I am not sure how to practically do this. Are you aware of any approaches to fitting random effects models (including approximations) that work for very large data sets? For example, applying a procedure to each group, and then using the results of this to shrink each fit in some appropriate way.


Just to clarify, here I am only worried about the non-crossed and in fact single-level case. I don’t see any easy route for cross</p><p>6 0.81115776 <a title="2086-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-30-Fixed_effects%2C_followed_by_Bayes_shrinkage%3F.html">1644 andrew gelman stats-2012-12-30-Fixed effects, followed by Bayes shrinkage?</a></p>
<p>7 0.77593905 <a title="2086-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Allowing_interaction_terms_to_vary.html">753 andrew gelman stats-2011-06-09-Allowing interaction terms to vary</a></p>
<p>8 0.77590597 <a title="2086-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-12-year_%2B_%281%7Cyear%29.html">851 andrew gelman stats-2011-08-12-year + (1|year)</a></p>
<p>9 0.76974374 <a title="2086-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>10 0.76821202 <a title="2086-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-11-%E2%80%9C2_level_logit_with_2_REs_%26_large_sample._computational_nightmare_%E2%80%93_please_help%E2%80%9D.html">759 andrew gelman stats-2011-06-11-“2 level logit with 2 REs & large sample. computational nightmare – please help”</a></p>
<p>11 0.76737231 <a title="2086-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-03-Uncertainty_in_parameter_estimates_using_multilevel_models.html">1966 andrew gelman stats-2013-08-03-Uncertainty in parameter estimates using multilevel models</a></p>
<p>12 0.75902456 <a title="2086-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-21-Finite-population_Anova_calculations_for_models_with_interactions.html">1686 andrew gelman stats-2013-01-21-Finite-population Anova calculations for models with interactions</a></p>
<p>13 0.75293231 <a title="2086-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-22-Handling_multiple_versions_of_an_outcome_variable.html">726 andrew gelman stats-2011-05-22-Handling multiple versions of an outcome variable</a></p>
<p>14 0.7510094 <a title="2086-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-02-Fixed_effects_and_identification.html">1241 andrew gelman stats-2012-04-02-Fixed effects and identification</a></p>
<p>15 0.7502358 <a title="2086-lsi-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-08-Multilevel_regression_with_shrinkage_for_%E2%80%9Cfixed%E2%80%9D_effects.html">653 andrew gelman stats-2011-04-08-Multilevel regression with shrinkage for “fixed” effects</a></p>
<p>16 0.74601829 <a title="2086-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-24-Estimating_and_summarizing_inference_for_hierarchical_variance_parameters_when_the_number_of_groups_is_small.html">2145 andrew gelman stats-2013-12-24-Estimating and summarizing inference for hierarchical variance parameters when the number of groups is small</a></p>
<p>17 0.74593836 <a title="2086-lsi-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-17-So-called_fixed_and_random_effects.html">472 andrew gelman stats-2010-12-17-So-called fixed and random effects</a></p>
<p>18 0.73761159 <a title="2086-lsi-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-31-Analyzing_the_entire_population_rather_than_a_sample.html">383 andrew gelman stats-2010-10-31-Analyzing the entire population rather than a sample</a></p>
<p>19 0.73676348 <a title="2086-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-06-17_groups%2C_6_group-level_predictors%3A__What_to_do%3F.html">1248 andrew gelman stats-2012-04-06-17 groups, 6 group-level predictors:  What to do?</a></p>
<p>20 0.73429638 <a title="2086-lsi-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-17-Clutering_and_variance_components.html">417 andrew gelman stats-2010-11-17-Clutering and variance components</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(7, 0.012), (14, 0.016), (16, 0.089), (21, 0.057), (24, 0.247), (50, 0.016), (59, 0.029), (63, 0.024), (78, 0.018), (85, 0.034), (86, 0.026), (87, 0.029), (89, 0.041), (97, 0.022), (99, 0.249)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97034109 <a title="2086-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-03-How_best_to_compare_effects_measured_in_two_different_time_periods%3F.html">2086 andrew gelman stats-2013-11-03-How best to compare effects measured in two different time periods?</a></p>
<p>Introduction: I received the following email from someone who wishes to remain anonymous:
  
My colleague and I are trying to understand the best way to approach a problem involving measuring a group of individuals’ abilities across time, and are hoping you can offer some guidance.


We are trying to analyze the combined effect of two distinct groups of people (A and B, with no overlap between A and B) who collaborate to produce a binary outcome, using a mixed logistic regression along the lines of the following.


Outcome ~ (1 | A) + (1 | B) + Other variables


What we’re interested in testing was whether the observed A random effects in period  1 are predictive of the A random effects in the following period 2.  Our idea being create two models, each using a different period’s worth of data, to create two sets of A coefficients, then observe the relationship between the two.  If the A’s have a persistent ability across periods, the coefficients should be correlated or show a linear-ish relationshi</p><p>2 0.96870589 <a title="2086-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-18-Understanding_posterior_p-values.html">2029 andrew gelman stats-2013-09-18-Understanding posterior p-values</a></p>
<p>Introduction: David Kaplan writes:
  
I came across your  paper  “Understanding Posterior Predictive P-values”, and I have a question regarding your statement “If a posterior predictive p-value is 0.4, say, that means that, if we believe the model, we think there is a 40% chance that tomorrow’s value of T(y_rep) will exceed today’s T(y).” This is perfectly understandable to me and represents the idea of calibration.  However, I am unsure how this relates to statements about fit.  If T is the LR chi-square or Pearson chi-square, then your statement that there is a 40% chance that tomorrows value exceeds today’s value indicates bad fit, I think.  Yet, some literature indicates that high p-values suggest good fit.  Could you clarify this?
  
My reply:
 
I think that “fit” depends on the question being asked.  In this case, I’d say the model fits for this particular purpose, even though it might not fit for other purposes.
 
And here’s the abstract of the paper:
  
Posterior predictive p-values do not i</p><p>3 0.96764529 <a title="2086-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-24-Latest_in_blog_advertising.html">1080 andrew gelman stats-2011-12-24-Latest in blog advertising</a></p>
<p>Introduction: I received the following message from “Patricia Lopez” of “Premium Link Ads”:
  
Hello,


I am interested in placing a text link on your page: http://andrewgelman.com/2011/07/super_sam_fuld/. The link would point to a page on a website that is relevant to your page and may be useful to your site visitors. We would be happy to compensate you for your time if it is something we are able to work out.


The best way to reach me is through a direct response to this email. This will help me get back to you about the right link request. Please let me know if you are interested, and if not thanks for your time. Thanks.
  
Usually I just ignore these, but after our recent  discussion  I decided to reply.  I wrote:
  
How much do you pay?
  
But no answer.  I wonder what’s going on?  I mean, why bother sending the email in the first place if you’re not going to follow up?</p><p>4 0.96626568 <a title="2086-lda-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-11-My_problem_with_the_Lindley_paradox.html">1757 andrew gelman stats-2013-03-11-My problem with the Lindley paradox</a></p>
<p>Introduction: From  a couple years ago but still relevant, I think:
  
To me, the Lindley paradox falls apart because of its noninformative prior distribution on the parameter of interest. If you really think there’s a high probability the parameter is nearly exactly zero, I don’t see the point of the model saying that you have no prior information at all on the parameter. In short: my criticism of so-called Bayesian hypothesis testing is that it’s insufficiently Bayesian.
  
P.S.  To clarify (in response to Bill’s comment below):  I’m speaking of all the examples I’ve ever worked on in social and environmental science, where in some settings I can imagine a parameter being very close to zero and in other settings I can imagine a parameter taking on just about any value in a wide range, but where I’ve never seen an example where a parameter could be  either  right at zero  or  taking on any possible value.  But such examples might occur in areas of application that I haven’t worked on.</p><p>5 0.9647916 <a title="2086-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-19-Tradeoffs_in_information_graphics.html">1584 andrew gelman stats-2012-11-19-Tradeoffs in information graphics</a></p>
<p>Introduction: The visual display of quantitative information (to use Edward Tufte’s wonderful term) is a diverse field or set of fields, and its practitioners have different goals. The goals of software designers, applied statisticians, biologists, graphic designers, and journalists (to list just a few of the important creators of data graphics) often overlap—but not completely. One of our aims in writing  our article  [on Infovis and Statistical Graphics] was to emphasize the diversity of graphical goals, as it seems to us that even experts tend to consider one aspect of a graph and not others.


Our main practical suggestion was that, in the internet age, we should not have to choose between attractive graphs and informational graphs: it should be possible to display both, via interactive displays. But to follow this suggestion, one must first accept that not every beautiful graph is informative, and not every informative graph is beautiful. . . .


Yes, it can sometimes be possible for a graph to</p><p>6 0.96428978 <a title="2086-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>7 0.96292567 <a title="2086-lda-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-03-Setting_aside_the_politics%2C_the_debate_over_the_new_health-care_study_reveals_that_we%E2%80%99re_moving_to_a_new_high_standard_of_statistical_journalism.html">1838 andrew gelman stats-2013-05-03-Setting aside the politics, the debate over the new health-care study reveals that we’re moving to a new high standard of statistical journalism</a></p>
<p>8 0.96104985 <a title="2086-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-11-Steve_Jobs%E2%80%99s_cancer_and_science-based_medicine.html">953 andrew gelman stats-2011-10-11-Steve Jobs’s cancer and science-based medicine</a></p>
<p>9 0.95923734 <a title="2086-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-05-The_p-value_is_not_._._..html">1607 andrew gelman stats-2012-12-05-The p-value is not . . .</a></p>
<p>10 0.95734715 <a title="2086-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>11 0.956779 <a title="2086-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-02-Blogads_update.html">1240 andrew gelman stats-2012-04-02-Blogads update</a></p>
<p>12 0.95569789 <a title="2086-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-05-Question_26_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1367 andrew gelman stats-2012-06-05-Question 26 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>13 0.95568132 <a title="2086-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-Fourteen_magic_words%3A_an_update.html">898 andrew gelman stats-2011-09-10-Fourteen magic words: an update</a></p>
<p>14 0.95559049 <a title="2086-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-12-Probabilistic_screening_to_get_an_approximate_self-weighted_sample.html">1455 andrew gelman stats-2012-08-12-Probabilistic screening to get an approximate self-weighted sample</a></p>
<p>15 0.95527303 <a title="2086-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-07-X_on_JLP.html">1792 andrew gelman stats-2013-04-07-X on JLP</a></p>
<p>16 0.95521069 <a title="2086-lda-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-10-The_last_great_essayist%3F.html">197 andrew gelman stats-2010-08-10-The last great essayist?</a></p>
<p>17 0.95510739 <a title="2086-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-10-Using_a_%E2%80%9Cpure_infographic%E2%80%9D_to_explore_differences_between_information_visualization_and_statistical_graphics.html">847 andrew gelman stats-2011-08-10-Using a “pure infographic” to explore differences between information visualization and statistical graphics</a></p>
<p>18 0.95498008 <a title="2086-lda-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-07-Challenges_of_experimental_design%3B_also_another_rant_on_the_practice_of_mentioning_the_publication_of_an_article_but_not_naming_its_author.html">399 andrew gelman stats-2010-11-07-Challenges of experimental design; also another rant on the practice of mentioning the publication of an article but not naming its author</a></p>
<p>19 0.95485675 <a title="2086-lda-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Ken_Rice_presents_a_unifying_approach_to_statistical_inference_and_hypothesis_testing.html">2312 andrew gelman stats-2014-04-29-Ken Rice presents a unifying approach to statistical inference and hypothesis testing</a></p>
<p>20 0.95449317 <a title="2086-lda-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
