<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1903 andrew gelman stats-2013-06-17-Weak identification provides partial information</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-1903" href="#">andrew_gelman_stats-2013-1903</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1903 andrew gelman stats-2013-06-17-Weak identification provides partial information</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-1903-html" href="http://andrewgelman.com/2013/06/17/weak-identification-provides-partial-information/">html</a></p><p>Introduction: Matt Selove writes: 
  
  
My question is about Bayesian analysis of the linear regression model. It seems to me that in some cases this approach throws out useful information.


As an example, imagine you have two basketball players randomly drawn from the pool of NBA players (which provides the prior). You’d like to estimate how many free throws each can make out of 100. You have two pieces of information:


- Session 1: Each player shoots 100 shots, and you learn player A’s total minus player B’s total


- Session 2: Player A does another session where he shoots 100 shots alone, and you learn his total


If we take the regression approach:


y_i = number of shots made 
beta_A = player A’s expected number out of 100 
beta_B = player B’s expected number out of 100 
x_i = vector of zeros and ones showing which player took shots


In the above example, our datapoints are:


y_1 (first number reported) = beta_A * 1 + beta_B * (-1) + epsilon_1 
y_2 (second number reported) = beta_A * 1 +</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Matt Selove writes:        My question is about Bayesian analysis of the linear regression model. [sent-1, score-0.076]
</p><p>2 It seems to me that in some cases this approach throws out useful information. [sent-2, score-0.285]
</p><p>3 As an example, imagine you have two basketball players randomly drawn from the pool of NBA players (which provides the prior). [sent-3, score-0.459]
</p><p>4 You’d like to estimate how many free throws each can make out of 100. [sent-4, score-0.127]
</p><p>5 But it seems to me the first session, providing the difference between the two scores (did player A beat player B or lose to player B), is also useful information that should be incorporated into beliefs about player A. [sent-6, score-3.486]
</p><p>6 Would the marginal posterior distribution for player A incorporate information from both sessions or just the second session? [sent-8, score-1.287]
</p><p>7 My reply:   Indeed, both data points are relevant for inference about player A. [sent-9, score-0.754]
</p><p>8 The easiest way to see this is to just write the model directly in Stan. [sent-10, score-0.058]
</p><p>9 It’s also clear if you consider extreme cases, for example if y1=100, you’ve obviously learned something from that data point. [sent-11, score-0.119]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('player', 0.754), ('session', 0.326), ('shots', 0.284), ('shoots', 0.137), ('throws', 0.127), ('number', 0.116), ('total', 0.11), ('incorporate', 0.105), ('formula', 0.103), ('players', 0.091), ('marginal', 0.086), ('information', 0.083), ('regression', 0.076), ('second', 0.074), ('reported', 0.069), ('zeros', 0.069), ('nba', 0.069), ('expected', 0.068), ('sessions', 0.065), ('posterior', 0.065), ('incorporated', 0.061), ('learn', 0.059), ('easiest', 0.058), ('cases', 0.058), ('minus', 0.056), ('distribution', 0.055), ('vector', 0.054), ('basketball', 0.054), ('beat', 0.053), ('matt', 0.052), ('pool', 0.051), ('approach', 0.05), ('useful', 0.05), ('beliefs', 0.047), ('providing', 0.046), ('textbook', 0.046), ('pieces', 0.046), ('randomly', 0.045), ('drawn', 0.045), ('two', 0.044), ('alone', 0.044), ('lose', 0.043), ('update', 0.043), ('scores', 0.043), ('obviously', 0.042), ('bayesian', 0.041), ('example', 0.039), ('provides', 0.038), ('ones', 0.038), ('extreme', 0.038)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1903-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-17-Weak_identification_provides_partial_information.html">1903 andrew gelman stats-2013-06-17-Weak identification provides partial information</a></p>
<p>Introduction: Matt Selove writes: 
  
  
My question is about Bayesian analysis of the linear regression model. It seems to me that in some cases this approach throws out useful information.


As an example, imagine you have two basketball players randomly drawn from the pool of NBA players (which provides the prior). You’d like to estimate how many free throws each can make out of 100. You have two pieces of information:


- Session 1: Each player shoots 100 shots, and you learn player A’s total minus player B’s total


- Session 2: Player A does another session where he shoots 100 shots alone, and you learn his total


If we take the regression approach:


y_i = number of shots made 
beta_A = player A’s expected number out of 100 
beta_B = player B’s expected number out of 100 
x_i = vector of zeros and ones showing which player took shots


In the above example, our datapoints are:


y_1 (first number reported) = beta_A * 1 + beta_B * (-1) + epsilon_1 
y_2 (second number reported) = beta_A * 1 +</p><p>2 0.31604007 <a title="1903-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-21-Scrabble%21.html">813 andrew gelman stats-2011-07-21-Scrabble!</a></p>
<p>Introduction: AT  writes :
  
Sitting on my [AT's] to-do list for a while now has been an exploration of Scrabble from an experimental design point of view; how to better design a tournament to make the variance as small as possible while still preserving the appearance of the home game to its players. . . . 


I’m proud (relieved?) to say that I’ve finally  finished the first draft  of this work for two-player head-to-head games, with a duplication method that ensures that if the game were repeated, each player would receive tiles from the reserve in the same sequence: think of the tiles being laid out in order (but unseen to the players), so that one player draws from the front and the other draws from the back. . . .


One goal of this was to figure out how much of the variance in score comes from the tile order and how much comes from the board, given that a tile order would be expected. It turns out to be about half-bag, half-board . . .


Some other findings from the simulations:

 
 The blank</p><p>3 0.23429631 <a title="1903-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-28-Turing_chess_run_update.html">1473 andrew gelman stats-2012-08-28-Turing chess run update</a></p>
<p>Introduction: In honor of the  Olympics , I got my butt over to the park and played run-around-the-house chess for the first time ever.
 
As was discussed in the comments thread  awhile ago , there seem to be three possible ways to play Turing chess:
 
1.  You make your move and run around the house.  The other player has to move before you return.  Once you sit down to the table, the other player runs around the house.  Then you have to move, etc.  You lose if you are checkmated or if you fail to move before your opponent returns to his chair.
 
2.  You make your move and run around the house.  The other player has to move before you return, but he does not have to wait until you return to start running.  He can start running once he’s moved.  Then when you get back you have to move before he gets back, but you can start to run once you’ve moved, etc.
 
3.  You make your move and run around the house.  The other player takes as long as he wants and makes his move, then he runs.  When you return, yo</p><p>4 0.22254184 <a title="1903-tfidf-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-26-Is_a_steal_really_worth_9_points%3F.html">2267 andrew gelman stats-2014-03-26-Is a steal really worth 9 points?</a></p>
<p>Introduction: Theodore Vasiloudis writes:
  
I’d like to bring your attention to  this article  by Benjamin Morris discussing the value of steals for the NBA. The author argues that a steal should be a highly sought after statistic as it equates to higher chances of victory and is very hard to replace when a player is injured.


I would argue that the reason behind the correlations showing this data is the fact that steals are much more rare in an NBA game than any of the other stats examined so their contribution is exaggerated.
  
I looked at Morris’s article and it looks like he’s running a regression of players’ plus-minus statistics on points, rebounds, assists, blocks, steals and turnovers.  He writes, “A marginal steal is weighted nine times more heavily when predicting a player’s impact than a marginal point.  For example, a player who averages 16 points and two steals per game is predicted (assuming all else is equal) to have a similar impact on his team’s success as one who averages 25 poi</p><p>5 0.17187169 <a title="1903-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-27-Heat_map.html">593 andrew gelman stats-2011-02-27-Heat map</a></p>
<p>Introduction: Jarad Niemi sends along this plot:
 
 
 
and writes:
  
2010-2011 Miami Heat offensive (red), defensive (blue), and combined (black) player contribution means (dots) and 95% credible intervals (lines) where zero indicates an average NBA player. Larger positive numbers for offensive and combined are better while larger negative numbers for defense are better.


In retrospect, I [Niemi] should have plotted -1*defensive_contribution so that larger was always better. The main point with this figure is that this awesome combination of James-Wade-Bosh that was discussed immediately after the LeBron trade to the Heat has a one-of-these-things-is-not-like-the-other aspect. At least according to my analysis, Bosh is hurting his team compared to the average player (although not statistically significant) due to his terrible defensive contribution (which is statistically significant).
  
All fine so far.  But the punchline comes at the end, when he writes:
  
Anyway, a reviewer said he hated the</p><p>6 0.13333485 <a title="1903-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-25-Diving_chess.html">1638 andrew gelman stats-2012-12-25-Diving chess</a></p>
<p>7 0.12712954 <a title="1903-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-16-The_%E2%80%9Chot_hand%E2%80%9D_and_problems_with_hypothesis_testing.html">1215 andrew gelman stats-2012-03-16-The “hot hand” and problems with hypothesis testing</a></p>
<p>8 0.12702292 <a title="1903-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-20-Problemen_met_het_boek.html">1332 andrew gelman stats-2012-05-20-Problemen met het boek</a></p>
<p>9 0.12338843 <a title="1903-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-18-What%E2%80%99s_my_Kasparov_number%3F.html">2105 andrew gelman stats-2013-11-18-What’s my Kasparov number?</a></p>
<p>10 0.12170991 <a title="1903-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-10-Some_interesting_unpublished_ideas_on_survey_weighting.html">705 andrew gelman stats-2011-05-10-Some interesting unpublished ideas on survey weighting</a></p>
<p>11 0.11802949 <a title="1903-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-28-Value-added_assessment%3A__What_went_wrong%3F.html">1350 andrew gelman stats-2012-05-28-Value-added assessment:  What went wrong?</a></p>
<p>12 0.11009754 <a title="1903-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-05-A_statistician_rereads_Bill_James.html">697 andrew gelman stats-2011-05-05-A statistician rereads Bill James</a></p>
<p>13 0.10313597 <a title="1903-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-24-Rcpp_class_in_Sat_9_Mar_in_NYC.html">1736 andrew gelman stats-2013-02-24-Rcpp class in Sat 9 Mar in NYC</a></p>
<p>14 0.10115559 <a title="1903-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-03-Bayes_pays%21.html">1923 andrew gelman stats-2013-07-03-Bayes pays!</a></p>
<p>15 0.095895372 <a title="1903-tfidf-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-07-QB2.html">260 andrew gelman stats-2010-09-07-QB2</a></p>
<p>16 0.095800929 <a title="1903-tfidf-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-27-Why_can%E2%80%99t_I_be_more_like_Bill_James%2C_or%2C_The_use_of_default_and_default-like_models.html">541 andrew gelman stats-2011-01-27-Why can’t I be more like Bill James, or, The use of default and default-like models</a></p>
<p>17 0.092307247 <a title="1903-tfidf-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-06-Inbox_zero.__Really..html">259 andrew gelman stats-2010-09-06-Inbox zero.  Really.</a></p>
<p>18 0.083612718 <a title="1903-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-21-Baseball%E2%80%99s_greatest_fielders.html">623 andrew gelman stats-2011-03-21-Baseball’s greatest fielders</a></p>
<p>19 0.074355774 <a title="1903-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-11-Toshiro_Kageyama_on_professionalism.html">1113 andrew gelman stats-2012-01-11-Toshiro Kageyama on professionalism</a></p>
<p>20 0.074139111 <a title="1903-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.107), (1, 0.066), (2, 0.016), (3, 0.02), (4, 0.024), (5, 0.003), (6, 0.038), (7, 0.02), (8, -0.009), (9, -0.024), (10, -0.006), (11, 0.002), (12, -0.008), (13, -0.017), (14, -0.014), (15, 0.026), (16, 0.024), (17, -0.02), (18, 0.024), (19, -0.011), (20, 0.001), (21, 0.082), (22, 0.026), (23, 0.067), (24, 0.007), (25, 0.068), (26, 0.009), (27, 0.021), (28, -0.02), (29, -0.115), (30, 0.052), (31, -0.069), (32, 0.005), (33, 0.016), (34, 0.028), (35, 0.019), (36, 0.012), (37, -0.028), (38, 0.019), (39, 0.044), (40, 0.043), (41, 0.033), (42, 0.016), (43, -0.016), (44, 0.007), (45, -0.052), (46, 0.017), (47, -0.035), (48, 0.014), (49, -0.037)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93030864 <a title="1903-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-17-Weak_identification_provides_partial_information.html">1903 andrew gelman stats-2013-06-17-Weak identification provides partial information</a></p>
<p>Introduction: Matt Selove writes: 
  
  
My question is about Bayesian analysis of the linear regression model. It seems to me that in some cases this approach throws out useful information.


As an example, imagine you have two basketball players randomly drawn from the pool of NBA players (which provides the prior). You’d like to estimate how many free throws each can make out of 100. You have two pieces of information:


- Session 1: Each player shoots 100 shots, and you learn player A’s total minus player B’s total


- Session 2: Player A does another session where he shoots 100 shots alone, and you learn his total


If we take the regression approach:


y_i = number of shots made 
beta_A = player A’s expected number out of 100 
beta_B = player B’s expected number out of 100 
x_i = vector of zeros and ones showing which player took shots


In the above example, our datapoints are:


y_1 (first number reported) = beta_A * 1 + beta_B * (-1) + epsilon_1 
y_2 (second number reported) = beta_A * 1 +</p><p>2 0.81618601 <a title="1903-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-21-Scrabble%21.html">813 andrew gelman stats-2011-07-21-Scrabble!</a></p>
<p>Introduction: AT  writes :
  
Sitting on my [AT's] to-do list for a while now has been an exploration of Scrabble from an experimental design point of view; how to better design a tournament to make the variance as small as possible while still preserving the appearance of the home game to its players. . . . 


I’m proud (relieved?) to say that I’ve finally  finished the first draft  of this work for two-player head-to-head games, with a duplication method that ensures that if the game were repeated, each player would receive tiles from the reserve in the same sequence: think of the tiles being laid out in order (but unseen to the players), so that one player draws from the front and the other draws from the back. . . .


One goal of this was to figure out how much of the variance in score comes from the tile order and how much comes from the board, given that a tile order would be expected. It turns out to be about half-bag, half-board . . .


Some other findings from the simulations:

 
 The blank</p><p>3 0.74950528 <a title="1903-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-20-I_think_you_knew_this_already.html">218 andrew gelman stats-2010-08-20-I think you knew this already</a></p>
<p>Introduction: I was playing out a chess game from the newspaper and we reminded how the best players use the entire board in their game.  In my own games (I’m not very good, I’m guessing my “rating” would be something like 1500?), the action always gets concentrated on one part of the board.  Grandmaster games do get focused on particular squares of the board, of course, but, meanwhile, there are implications in other places and the action can suddenly shift.</p><p>4 0.71057534 <a title="1903-lsi-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-26-Is_a_steal_really_worth_9_points%3F.html">2267 andrew gelman stats-2014-03-26-Is a steal really worth 9 points?</a></p>
<p>Introduction: Theodore Vasiloudis writes:
  
I’d like to bring your attention to  this article  by Benjamin Morris discussing the value of steals for the NBA. The author argues that a steal should be a highly sought after statistic as it equates to higher chances of victory and is very hard to replace when a player is injured.


I would argue that the reason behind the correlations showing this data is the fact that steals are much more rare in an NBA game than any of the other stats examined so their contribution is exaggerated.
  
I looked at Morris’s article and it looks like he’s running a regression of players’ plus-minus statistics on points, rebounds, assists, blocks, steals and turnovers.  He writes, “A marginal steal is weighted nine times more heavily when predicting a player’s impact than a marginal point.  For example, a player who averages 16 points and two steals per game is predicted (assuming all else is equal) to have a similar impact on his team’s success as one who averages 25 poi</p><p>5 0.71036518 <a title="1903-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-25-Diving_chess.html">1638 andrew gelman stats-2012-12-25-Diving chess</a></p>
<p>Introduction: Knowing of my interest in  Turing run-around-the-house chess , David Lockhart points me to  this :
  
Diving Chess is a chess variant, which is played in a swimming pool. Instead of using chess clocks, each player must submerge themselves underwater during their turn, only to resurface when they are ready to make a move. Players must make a move within 5 seconds of resurfacing (they will receive a warning if not, and three warnings will result in a forfeit). Diving Chess was invented by American Chess Master Etan Ilfeld; the very first exhibition game took place between Ilfeld and former British Chess Champion William Hartston at the Thirdspace gym in Soho on August 2nd, 2011. Hartston won the match which lasted almost two hours such that each player was underwater for an entire hour.</p><p>6 0.69149762 <a title="1903-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-06-Bidding_for_the_kickoff.html">559 andrew gelman stats-2011-02-06-Bidding for the kickoff</a></p>
<p>7 0.69052565 <a title="1903-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-23-The_pinch-hitter_syndrome_again.html">1467 andrew gelman stats-2012-08-23-The pinch-hitter syndrome again</a></p>
<p>8 0.68092245 <a title="1903-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-22-Goal%3A__Rules_for_Turing_chess.html">1818 andrew gelman stats-2013-04-22-Goal:  Rules for Turing chess</a></p>
<p>9 0.68041414 <a title="1903-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-16-Chess_vs._checkers.html">615 andrew gelman stats-2011-03-16-Chess vs. checkers</a></p>
<p>10 0.67785025 <a title="1903-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-28-Turing_chess_run_update.html">1473 andrew gelman stats-2012-08-28-Turing chess run update</a></p>
<p>11 0.66078353 <a title="1903-lsi-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-12-Probability_of_successive_wins_in_baseball.html">29 andrew gelman stats-2010-05-12-Probability of successive wins in baseball</a></p>
<p>12 0.64458662 <a title="1903-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-08-Of_parsing_and_chess.html">1847 andrew gelman stats-2013-05-08-Of parsing and chess</a></p>
<p>13 0.63015592 <a title="1903-lsi-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-03-Getting_a_job_in_pro_sports%E2%80%A6_as_a_statistician.html">445 andrew gelman stats-2010-12-03-Getting a job in pro sports… as a statistician</a></p>
<p>14 0.62556797 <a title="1903-lsi-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-30-Silly_baseball_example_illustrates_a_couple_of_key_ideas_they_don%E2%80%99t_usually_teach_you_in_statistics_class.html">171 andrew gelman stats-2010-07-30-Silly baseball example illustrates a couple of key ideas they don’t usually teach you in statistics class</a></p>
<p>15 0.61443317 <a title="1903-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-18-What%E2%80%99s_my_Kasparov_number%3F.html">2105 andrew gelman stats-2013-11-18-What’s my Kasparov number?</a></p>
<p>16 0.61199337 <a title="1903-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-20-Problemen_met_het_boek.html">1332 andrew gelman stats-2012-05-20-Problemen met het boek</a></p>
<p>17 0.61073858 <a title="1903-lsi-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-03-Gladwell_vs_Pinker.html">253 andrew gelman stats-2010-09-03-Gladwell vs Pinker</a></p>
<p>18 0.60607624 <a title="1903-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-30-Berri_Gladwell_Loken_football_update.html">2082 andrew gelman stats-2013-10-30-Berri Gladwell Loken football update</a></p>
<p>19 0.58778715 <a title="1903-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-06-Statistician_cracks_Toronto_lottery.html">562 andrew gelman stats-2011-02-06-Statistician cracks Toronto lottery</a></p>
<p>20 0.58538467 <a title="1903-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-21-Baseball%E2%80%99s_greatest_fielders.html">623 andrew gelman stats-2011-03-21-Baseball’s greatest fielders</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(9, 0.026), (16, 0.036), (17, 0.052), (24, 0.142), (30, 0.012), (54, 0.01), (65, 0.144), (81, 0.011), (82, 0.016), (86, 0.024), (89, 0.141), (99, 0.226)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92212605 <a title="1903-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-17-Weak_identification_provides_partial_information.html">1903 andrew gelman stats-2013-06-17-Weak identification provides partial information</a></p>
<p>Introduction: Matt Selove writes: 
  
  
My question is about Bayesian analysis of the linear regression model. It seems to me that in some cases this approach throws out useful information.


As an example, imagine you have two basketball players randomly drawn from the pool of NBA players (which provides the prior). You’d like to estimate how many free throws each can make out of 100. You have two pieces of information:


- Session 1: Each player shoots 100 shots, and you learn player A’s total minus player B’s total


- Session 2: Player A does another session where he shoots 100 shots alone, and you learn his total


If we take the regression approach:


y_i = number of shots made 
beta_A = player A’s expected number out of 100 
beta_B = player B’s expected number out of 100 
x_i = vector of zeros and ones showing which player took shots


In the above example, our datapoints are:


y_1 (first number reported) = beta_A * 1 + beta_B * (-1) + epsilon_1 
y_2 (second number reported) = beta_A * 1 +</p><p>2 0.88799912 <a title="1903-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-15-Last_word_on_Mister_P_%28for_now%29.html">2062 andrew gelman stats-2013-10-15-Last word on Mister P (for now)</a></p>
<p>Introduction: To recap:
 
Matt Buttice and Ben Highton recently published  an article  where they evaluated multilevel regression and poststratification (MRP) on a bunch of political examples estimating state-level attitudes.
 
My Columbia colleagues Jeff Lax, Justin Phillips, and Yair Ghitza added some  discussion , giving a bunch of practical tips and pointing to some problems with Buttice and Highton’s evaluations.
 
Buttice and Highton  replied , emphasizing the difficulties of comparing methods in the absence of a known ground truth.
 
And Jeff Lax added the following comment, which I think is a good overview of the discussion so far:
  
In the back and forth between us all on details, some points may get lost and disagreements overstated. Where are things at this point?


1.	Buttice and Highton (BH) show beyond previous work that MRP performance in making state estimates can vary to an extent that is not directly observable unless one knows the true estimates (in which case one would not be us</p><p>3 0.88347858 <a title="1903-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-11-Data_Visualization_vs._Statistical_Graphics.html">407 andrew gelman stats-2010-11-11-Data Visualization vs. Statistical Graphics</a></p>
<p>Introduction: I have this great talk on the above topic but nowhere to give it.
 
Here’s the story.  Several months ago, I was invited to speak at IEEE VisWeek.  It sounded like a great opportunity.  The organizer told me that there were typically about 700 people in the audience, and these are people in the visualization community whom I’d like to reach but normally wouldn’t have the opportunity to encounter.  It sounded great, but I didn’t want to fly most of the way across the country by myself, so I offered to give the talk by videolink.
 
I was surprised to get a No response:  I’d think that a visualization conference, of all things, would welcome a video talk.
 
In the meantime, though, I’d thought a lot about what I’d talk about and had started preparing something.  Once I found out I wouldn’t be giving the talk, I channeled the efforts into an article which, with the collaboration of Antony Unwin, was completed about a month ago.
 
It would take very little effort to adapt this graph-laden a</p><p>4 0.88173687 <a title="1903-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-31-Untunable_Metropolis.html">833 andrew gelman stats-2011-07-31-Untunable Metropolis</a></p>
<p>Introduction: Michael Margolis writes:
  
What are we to make of it when a Metropolis-Hastings step just won’t tune? That is, the acceptance rate is zero at expected-jump-size X, and way above 1/2 at X-exp(-16) (i.e.,  machine precision ).


I’ve solved my practical problem by writing that I would have liked to include results from a diffuse prior, but couldn’t. But I’m bothered by the poverty of my intuition. And since everything I’ve read says this is an issue of efficiency, rather than accuracy, I wonder if I could solve it just by running massive and heavily thinned chains.
  
My reply:
 
I can’t see how this could happen in a well-specified problem!  I suspect it’s a bug.  Otherwise try rescaling your variables so that your parameters will have values on the order of magnitude of 1.
 
To which Margolis responded:
  
I hardly wrote any of the code, so I can’t speak to the bug question — it’s binomial kriging from the R package geoRglm. And there are no covariates to scale — just the zero and one</p><p>5 0.88040555 <a title="1903-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-10-He_said_he_was_sorry.html">1756 andrew gelman stats-2013-03-10-He said he was sorry</a></p>
<p>Introduction: Yes, it can be  done :
  
Hereby I contact you to clarify the situation that occurred with the publication of the article entitled *** which was published in Volume 11, Issue 3 of *** and I made the mistake of declaring as an author.  This chapter is a plagiarism of . . .


I wish to express and acknowledge that I am solely responsible for this . . . I recognize the gravity of the offense committed, since there is no justification for so doing. Therefore, and as a sign of shame and regret I feel in this situation, I will publish this letter, in order to set an example for other researchers do not engage in a similar error.


No more, and to please accept my apologies,


Sincerely,


***
  
P.S.  Since we’re on Retraction Watch already, I’ll point you to  this unrelated story  featuring a hilarious photo of a fraudster, who in this case was a grad student in psychology who faked his data and “has agreed to submit to a three-year supervisory period for any work involving funding from the</p><p>6 0.87438297 <a title="1903-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-05-Wouldn%E2%80%99t_it_be_cool_if_Glenn_Hubbard_were_consulting_for_Herbalife_and_I_were_on_the_other_side%3F.html">1708 andrew gelman stats-2013-02-05-Wouldn’t it be cool if Glenn Hubbard were consulting for Herbalife and I were on the other side?</a></p>
<p>7 0.87105989 <a title="1903-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-20-One_more_time-use_graph.html">671 andrew gelman stats-2011-04-20-One more time-use graph</a></p>
<p>8 0.86892486 <a title="1903-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-%E2%80%9CAll_Models_are_Right%2C_Most_are_Useless%E2%80%9D.html">1197 andrew gelman stats-2012-03-04-“All Models are Right, Most are Useless”</a></p>
<p>9 0.86806244 <a title="1903-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-21-Don%E2%80%99t_judge_a_book_by_its_title.html">1021 andrew gelman stats-2011-11-21-Don’t judge a book by its title</a></p>
<p>10 0.86785829 <a title="1903-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-A_Stan_is_Born.html">1475 andrew gelman stats-2012-08-30-A Stan is Born</a></p>
<p>11 0.86766374 <a title="1903-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-16-The_%E2%80%9Chot_hand%E2%80%9D_and_problems_with_hypothesis_testing.html">1215 andrew gelman stats-2012-03-16-The “hot hand” and problems with hypothesis testing</a></p>
<p>12 0.86269838 <a title="1903-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-21-BDA3_table_of_contents_%28also_a_new_paper_on_visualization%29.html">1991 andrew gelman stats-2013-08-21-BDA3 table of contents (also a new paper on visualization)</a></p>
<p>13 0.86269164 <a title="1903-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-07-Whassup_with_phantom-limb_treatment%3F.html">457 andrew gelman stats-2010-12-07-Whassup with phantom-limb treatment?</a></p>
<p>14 0.86146927 <a title="1903-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-23-Special_effects.html">1426 andrew gelman stats-2012-07-23-Special effects</a></p>
<p>15 0.86062628 <a title="1903-lda-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-04-Question_25_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1365 andrew gelman stats-2012-06-04-Question 25 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>16 0.8580842 <a title="1903-lda-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-24-Yet_another_Bayesian_job_opportunity.html">231 andrew gelman stats-2010-08-24-Yet another Bayesian job opportunity</a></p>
<p>17 0.85713661 <a title="1903-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-10-I_don%E2%80%99t_like_this_cartoon.html">1572 andrew gelman stats-2012-11-10-I don’t like this cartoon</a></p>
<p>18 0.8534686 <a title="1903-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-09-Mister_P%3A__What%E2%80%99s_its_secret_sauce%3F.html">2056 andrew gelman stats-2013-10-09-Mister P:  What’s its secret sauce?</a></p>
<p>19 0.85342896 <a title="1903-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-09-The_boxer%2C_the_wrestler%2C_and_the_coin_flip%2C_again.html">566 andrew gelman stats-2011-02-09-The boxer, the wrestler, and the coin flip, again</a></p>
<p>20 0.85202241 <a title="1903-lda-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-09-Familial_Linkage_between_Neuropsychiatric_Disorders_and_Intellectual_Interests.html">1160 andrew gelman stats-2012-02-09-Familial Linkage between Neuropsychiatric Disorders and Intellectual Interests</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
