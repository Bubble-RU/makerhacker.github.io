<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2110 andrew gelman stats-2013-11-22-A Bayesian model for an increasing function, in Stan!</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-2110" href="#">andrew_gelman_stats-2013-2110</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2110 andrew gelman stats-2013-11-22-A Bayesian model for an increasing function, in Stan!</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-2110-html" href="http://andrewgelman.com/2013/11/22/bayesian-model-increasing-function-stan/">html</a></p><p>Introduction: Following up on yesterday’s post, here’s David Chudzicki’s  story  (with graphs and Stan/R code!) of how he fit a model for an increasing function (“isotonic regression”).  Chudzicki writes:
  
This post will describe a way I came up with of fitting a function that’s constrained to be increasing, using Stan. If you want practical help, standard statistical approaches, or expert research, this isn’t the place for you (look up “isotonic regression” or “Bayesian isotonic regression” or David Dunson). This is the place for you if you want to read about how I thought about setting up a model, implemented the model in Stan, and created graphics to understand what was going on.
  
The background is that a simple, natural-seeming uniform prior on the function values does not work so well—it’s a much stronger prior distribution than one might naively think, just one of those unexpected aspects of high-dimensional probability distributions.  So Chudzicki sets up a more general family with a hype</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Following up on yesterday’s post, here’s David Chudzicki’s  story  (with graphs and Stan/R code! [sent-1, score-0.057]
</p><p>2 ) of how he fit a model for an increasing function (“isotonic regression”). [sent-2, score-0.542]
</p><p>3 Chudzicki writes:    This post will describe a way I came up with of fitting a function that’s constrained to be increasing, using Stan. [sent-3, score-0.587]
</p><p>4 If you want practical help, standard statistical approaches, or expert research, this isn’t the place for you (look up “isotonic regression” or “Bayesian isotonic regression” or David Dunson). [sent-4, score-0.885]
</p><p>5 This is the place for you if you want to read about how I thought about setting up a model, implemented the model in Stan, and created graphics to understand what was going on. [sent-5, score-0.622]
</p><p>6 The background is that a simple, natural-seeming uniform prior on the function values does not work so well—it’s a much stronger prior distribution than one might naively think, just one of those unexpected aspects of high-dimensional probability distributions. [sent-6, score-1.179]
</p><p>7 So Chudzicki sets up a more general family with a hyper parameter. [sent-7, score-0.309]
</p><p>8 One thing I like about this example is that it’s  not  the latest research; it has a charming DIY flavor that might make you feel that you too can patch together a model in Stan to do what you need. [sent-8, score-0.698]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('chudzicki', 0.513), ('isotonic', 0.513), ('function', 0.205), ('increasing', 0.164), ('hyper', 0.156), ('regression', 0.15), ('patch', 0.141), ('stan', 0.136), ('dunson', 0.128), ('model', 0.123), ('charming', 0.123), ('naively', 0.115), ('flavor', 0.115), ('david', 0.114), ('place', 0.113), ('constrained', 0.107), ('prior', 0.106), ('unexpected', 0.101), ('implemented', 0.098), ('uniform', 0.092), ('stronger', 0.088), ('created', 0.084), ('post', 0.08), ('latest', 0.079), ('family', 0.077), ('approaches', 0.077), ('sets', 0.076), ('aspects', 0.076), ('yesterday', 0.076), ('expert', 0.074), ('describe', 0.073), ('fitting', 0.073), ('practical', 0.072), ('graphics', 0.072), ('code', 0.068), ('setting', 0.068), ('background', 0.067), ('values', 0.065), ('want', 0.064), ('together', 0.062), ('research', 0.061), ('graphs', 0.057), ('might', 0.055), ('distribution', 0.054), ('isn', 0.052), ('help', 0.051), ('fit', 0.05), ('probability', 0.049), ('standard', 0.049), ('came', 0.049)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="2110-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-22-A_Bayesian_model_for_an_increasing_function%2C_in_Stan%21.html">2110 andrew gelman stats-2013-11-22-A Bayesian model for an increasing function, in Stan!</a></p>
<p>Introduction: Following up on yesterday’s post, here’s David Chudzicki’s  story  (with graphs and Stan/R code!) of how he fit a model for an increasing function (“isotonic regression”).  Chudzicki writes:
  
This post will describe a way I came up with of fitting a function that’s constrained to be increasing, using Stan. If you want practical help, standard statistical approaches, or expert research, this isn’t the place for you (look up “isotonic regression” or “Bayesian isotonic regression” or David Dunson). This is the place for you if you want to read about how I thought about setting up a model, implemented the model in Stan, and created graphics to understand what was going on.
  
The background is that a simple, natural-seeming uniform prior on the function values does not work so well—it’s a much stronger prior distribution than one might naively think, just one of those unexpected aspects of high-dimensional probability distributions.  So Chudzicki sets up a more general family with a hype</p><p>2 0.1476185 <a title="2110-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-07-Robust_logistic_regression.html">1886 andrew gelman stats-2013-06-07-Robust logistic regression</a></p>
<p>Introduction: Corey Yanofsky writes:
  
In your work, you’ve robustificated logistic regression by having the logit function saturate at, e.g., 0.01 and 0.99, instead of  0 and 1. Do you have any thoughts on a sensible setting for the saturation values? My intuition suggests that it has something to do with proportion of outliers expected in the data (assuming a reasonable model fit). 


It would be desirable to have them fit in the model, but my intuition is that integrability of the posterior distribution might become an issue.
  
My reply:  it should be no problem to put these saturation values in the model, I bet it would work fine in Stan if you give them uniform (0,.1) priors or something like that.  Or you could just fit the robit model.
 
And this reminds me . . . I’ve been told that when Stan’s on its optimization setting, it fits generalized linear models just about as fast as regular glm or bayesglm in R.  This suggests to me that we should have some precompiled regression models in Stan,</p><p>3 0.1321024 <a title="2110-tfidf-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>Introduction: I received the following email:
  
I have an interesting thought on a prior for a logistic regression, and would love your input on how to make it “work.”


Some of my research, two published papers, are on mathematical models of **.  Along those lines, I’m interested in developing more models for **. . . .  Empirical studies show that the public is rather smart and that the wisdom-of-the-crowd is fairly accurate.


So, my thought would be to tread the public’s probability of the event as a prior, and then see how adding data, through a model, would change or perturb our inferred probability of **.  (Similarly, I could envision using previously published epidemiological research as a prior probability of a disease, and then seeing how the addition of new testing protocols would update that belief.)


However, everything I learned about hierarchical Bayesian models has a prior as a distribution on the coefficients.  I don’t know how to start with a prior point estimate for the probabili</p><p>4 0.12045957 <a title="2110-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>Introduction: Following up on Christian’s  post  [link fixed] on the topic, I’d like to offer a few thoughts of my own.
 
In BDA, we express the idea that a noninformative prior is a placeholder:  you can use the noninformative prior to get the analysis started, then if your posterior distribution is less informative than you would like, or if it does not make sense, you can go back and add prior information.
 
Same thing for the data model (the “likelihood”), for that matter:  it often makes sense to start with something simple and conventional and then go from there.
 
So, in that sense, noninformative priors are no big deal, they’re just a way to get started.  Just don’t take them too seriously.
 
Traditionally in statistics we’ve worked with the paradigm of a single highly informative dataset with only weak external information.  But if the data are sparse and prior information is strong, we have to think differently.  And, when you increase the dimensionality of a problem, both these things hap</p><p>5 0.11018896 <a title="2110-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>Introduction: Nick Firoozye writes:
  
While I am absolutely sympathetic to the Bayesian agenda I am often troubled by the requirement of having priors. We must have priors on the parameter of an infinite number of model we have never seen before and I find this troubling. There is a similarly troubling problem in economics of utility theory. Utility is on consumables. To be complete a consumer must assign utility to all sorts of things they never would have encountered. More recent versions of utility theory instead make consumption goods a portfolio of attributes. Cadillacs are x many units of luxury y of transport etc etc. And we can automatically have personal utilities to all these attributes.  


I don’t ever see parameters. Some model have few and some have hundreds. Instead, I see data. So I don’t know how to have an opinion on parameters themselves. Rather I think it far more natural to have opinions on the behavior of models. The prior predictive density is a good and sensible notion. Also</p><p>6 0.10831576 <a title="2110-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>7 0.10788775 <a title="2110-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>8 0.10482296 <a title="2110-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-06-Stan_1.2.0_and_RStan_1.2.0.html">1753 andrew gelman stats-2013-03-06-Stan 1.2.0 and RStan 1.2.0</a></p>
<p>9 0.10143528 <a title="2110-tfidf-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-14-Transitioning_to_Stan.html">2291 andrew gelman stats-2014-04-14-Transitioning to Stan</a></p>
<p>10 0.10087126 <a title="2110-tfidf-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-01-How_does_Bayes_do_it%3F.html">247 andrew gelman stats-2010-09-01-How does Bayes do it?</a></p>
<p>11 0.096519411 <a title="2110-tfidf-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-07-My_recent_debugging_experience.html">2161 andrew gelman stats-2014-01-07-My recent debugging experience</a></p>
<p>12 0.094411395 <a title="2110-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>13 0.093766429 <a title="2110-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>14 0.092419684 <a title="2110-tfidf-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-Stan_Model_of_the_Week%3A_Hierarchical_Modeling_of_Supernovas.html">2299 andrew gelman stats-2014-04-21-Stan Model of the Week: Hierarchical Modeling of Supernovas</a></p>
<p>15 0.092415594 <a title="2110-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-24-F-f-f-fake_data.html">1735 andrew gelman stats-2013-02-24-F-f-f-fake data</a></p>
<p>16 0.092098385 <a title="2110-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>17 0.091735229 <a title="2110-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-11-%E2%80%9CInformative_g-Priors_for_Logistic_Regression%E2%80%9D.html">2017 andrew gelman stats-2013-09-11-“Informative g-Priors for Logistic Regression”</a></p>
<p>18 0.091149382 <a title="2110-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-A_Stan_is_Born.html">1475 andrew gelman stats-2012-08-30-A Stan is Born</a></p>
<p>19 0.089587867 <a title="2110-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-17-Data_problems%2C_coding_errors%E2%80%A6what_can_be_done%3F.html">1807 andrew gelman stats-2013-04-17-Data problems, coding errors…what can be done?</a></p>
<p>20 0.088112712 <a title="2110-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-16-Stantastic%21.html">1580 andrew gelman stats-2012-11-16-Stantastic!</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.153), (1, 0.126), (2, -0.008), (3, 0.076), (4, 0.061), (5, -0.008), (6, 0.019), (7, -0.076), (8, -0.04), (9, -0.002), (10, -0.024), (11, 0.021), (12, -0.034), (13, 0.007), (14, -0.006), (15, -0.008), (16, -0.014), (17, -0.005), (18, 0.009), (19, 0.0), (20, 0.008), (21, -0.023), (22, -0.018), (23, -0.04), (24, -0.0), (25, 0.005), (26, 0.045), (27, -0.066), (28, -0.048), (29, -0.003), (30, 0.025), (31, 0.023), (32, -0.015), (33, 0.038), (34, -0.001), (35, -0.024), (36, 0.003), (37, 0.013), (38, -0.016), (39, -0.029), (40, 0.013), (41, 0.014), (42, -0.015), (43, -0.035), (44, 0.067), (45, 0.031), (46, 0.013), (47, 0.0), (48, -0.002), (49, 0.029)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96115756 <a title="2110-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-22-A_Bayesian_model_for_an_increasing_function%2C_in_Stan%21.html">2110 andrew gelman stats-2013-11-22-A Bayesian model for an increasing function, in Stan!</a></p>
<p>Introduction: Following up on yesterday’s post, here’s David Chudzicki’s  story  (with graphs and Stan/R code!) of how he fit a model for an increasing function (“isotonic regression”).  Chudzicki writes:
  
This post will describe a way I came up with of fitting a function that’s constrained to be increasing, using Stan. If you want practical help, standard statistical approaches, or expert research, this isn’t the place for you (look up “isotonic regression” or “Bayesian isotonic regression” or David Dunson). This is the place for you if you want to read about how I thought about setting up a model, implemented the model in Stan, and created graphics to understand what was going on.
  
The background is that a simple, natural-seeming uniform prior on the function values does not work so well—it’s a much stronger prior distribution than one might naively think, just one of those unexpected aspects of high-dimensional probability distributions.  So Chudzicki sets up a more general family with a hype</p><p>2 0.87482482 <a title="2110-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-07-Robust_logistic_regression.html">1886 andrew gelman stats-2013-06-07-Robust logistic regression</a></p>
<p>Introduction: Corey Yanofsky writes:
  
In your work, you’ve robustificated logistic regression by having the logit function saturate at, e.g., 0.01 and 0.99, instead of  0 and 1. Do you have any thoughts on a sensible setting for the saturation values? My intuition suggests that it has something to do with proportion of outliers expected in the data (assuming a reasonable model fit). 


It would be desirable to have them fit in the model, but my intuition is that integrability of the posterior distribution might become an issue.
  
My reply:  it should be no problem to put these saturation values in the model, I bet it would work fine in Stan if you give them uniform (0,.1) priors or something like that.  Or you could just fit the robit model.
 
And this reminds me . . . I’ve been told that when Stan’s on its optimization setting, it fits generalized linear models just about as fast as regular glm or bayesglm in R.  This suggests to me that we should have some precompiled regression models in Stan,</p><p>3 0.79870379 <a title="2110-lsi-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-07-My_recent_debugging_experience.html">2161 andrew gelman stats-2014-01-07-My recent debugging experience</a></p>
<p>Introduction: OK, so this sort of thing happens sometimes.  I was working on a new idea (still working on it; if it ultimately works out—or if it doesn’t—I’ll let you know) and as part of it I was fitting little models in Stan, in a loop.  I thought it would make sense to start with linear regression with normal priors and known data variance, because then the exact solution is Gaussian and I can also work with the problem analytically.  So I programmed up the algorithm and, no surprise, it didn’t work.  I went through my R code, put in print statements here and there, and cleared out bug after bug until at least it stopped crashing.  But the algorithm still wasn’t doing what it was supposed to do.
 
So I decided to do something simpler, and just check that the Stan linear regression gave the same answer as the analytic posterior distribution:  I ran Stan for tons of iterations, then computed the sample mean and variance of the simulations.  It was an example with two coefficients—I’d originally cho</p><p>4 0.74546665 <a title="2110-lsi-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-Stan_Model_of_the_Week%3A_Hierarchical_Modeling_of_Supernovas.html">2299 andrew gelman stats-2014-04-21-Stan Model of the Week: Hierarchical Modeling of Supernovas</a></p>
<p>Introduction: The Stan Model of the Week showcases research using Stan to push the limits of applied statistics.  If you have a model that you would like to submit for a future post then send us an  email . 
 
Our inaugural post comes from Nathan Sanders, a graduate student finishing up his thesis on astrophysics at Harvard. Nathan writes,
  
“Core-collapse supernovae, the luminous explosions of massive stars, exhibit an expansive and meaningful diversity of behavior in their brightness evolution over time (their “light curves”). Our group discovers and monitors these events using the Pan-STARRS1 telescope in Hawaii, and we’ve collected a dataset of about 20,000 individual photometric observations of about 80 Type IIP supernovae, the class my work has focused on. While this dataset provides one of the best available tools to infer the explosion properties of these supernovae, due to the nature of extragalactic astronomy (observing from distances 
  1 billion light years), these light curves typicall</p><p>5 0.72480685 <a title="2110-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>Introduction: My (coauthored) books on Bayesian data analysis and applied regression are like almost all the other statistics textbooks out there, in that we spend most of our time on the basic distributions such as normal and logistic and then, only as an aside, discuss robust models such as t and robit.
 
Why aren’t the t and robit front and center?  Sure, I can see starting with the normal (at least in the Bayesian book, where we actually work out all the algebra), but then why don’t we move on immediately to the real stuff?
 
This isn’t just (or mainly) a question of textbooks or teaching; I’m really thinking here about statistical practice.  My statistical practice.  Should t and robit be the default?  If not, why not?
 
Some possible answers:
  
10.  Estimating the degrees of freedom in the error distribution isn’t so easy, and throwing this extra parameter into the model could make inference unstable.


9.  Real data usually don’t have outliers.  In practice, fitting a robust model costs you</p><p>6 0.72476739 <a title="2110-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-07-Prior_distributions_for_regression_coefficients.html">1486 andrew gelman stats-2012-09-07-Prior distributions for regression coefficients</a></p>
<p>7 0.72356564 <a title="2110-lsi-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-10-Stan_Model_of_the_Week%3A__PK_Calculation_of_IV_and_Oral_Dosing.html">2242 andrew gelman stats-2014-03-10-Stan Model of the Week:  PK Calculation of IV and Oral Dosing</a></p>
<p>8 0.71583951 <a title="2110-lsi-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-14-Transitioning_to_Stan.html">2291 andrew gelman stats-2014-04-14-Transitioning to Stan</a></p>
<p>9 0.7150228 <a title="2110-lsi-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>10 0.70675755 <a title="2110-lsi-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>11 0.70435959 <a title="2110-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-09-Same_old_same_old.html">1849 andrew gelman stats-2013-05-09-Same old same old</a></p>
<p>12 0.70304626 <a title="2110-lsi-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-21-Models_with_constraints.html">2342 andrew gelman stats-2014-05-21-Models with constraints</a></p>
<p>13 0.6932463 <a title="2110-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-16-%E2%80%9CReal_data_can_be_a_pain%E2%80%9D.html">1460 andrew gelman stats-2012-08-16-“Real data can be a pain”</a></p>
<p>14 0.68691802 <a title="2110-lsi-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-07-There_are_never_70_distinct_parameters.html">327 andrew gelman stats-2010-10-07-There are never 70 distinct parameters</a></p>
<p>15 0.68676138 <a title="2110-lsi-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-18-The_1.6_rule.html">39 andrew gelman stats-2010-05-18-The 1.6 rule</a></p>
<p>16 0.683568 <a title="2110-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-12-Samplers_for_Big_Science%3A__emcee_and_BAT.html">2020 andrew gelman stats-2013-09-12-Samplers for Big Science:  emcee and BAT</a></p>
<p>17 0.68265778 <a title="2110-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-30-Computational_problems_with_glm_etc..html">1516 andrew gelman stats-2012-09-30-Computational problems with glm etc.</a></p>
<p>18 0.68168986 <a title="2110-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-30-Stan_Project%3A__Continuous_Relaxations_for_Discrete_MRFs.html">2003 andrew gelman stats-2013-08-30-Stan Project:  Continuous Relaxations for Discrete MRFs</a></p>
<p>19 0.68071657 <a title="2110-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-29-Putting_together_multinomial_discrete_regressions_by_combining_simple_logits.html">782 andrew gelman stats-2011-06-29-Putting together multinomial discrete regressions by combining simple logits</a></p>
<p>20 0.6767056 <a title="2110-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-19-Will_Stan_work_well_with_40%C3%9740_matrices%3F.html">861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(15, 0.016), (24, 0.18), (40, 0.046), (44, 0.029), (48, 0.012), (63, 0.188), (71, 0.019), (73, 0.015), (86, 0.022), (99, 0.347)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9885971 <a title="2110-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-22-Tables_as_graphs%3A_The_Ramanujan_principle.html">1078 andrew gelman stats-2011-12-22-Tables as graphs: The Ramanujan principle</a></p>
<p>Introduction: Tables are commonly read as crude graphs: what you notice in a table of numbers is (a) the minus signs, and thus which values are positive and which are negative, and (b) the length of each number, that is, its order of magnitude. The most famous example of such a read might be when the mathematician Srinivasa Ramanujan supposedly conjectured the asymptotic form of the partition function based on a look at a table of the first several partition numbers: he was essentially looking at a graph on the logarithmic scale.
 
I discuss some modern-day statistical examples in  this article for Significance magazine .
 
   
 
 
 
   
 
I had a lot of fun creating the “calculator font” for the above graph in R and then writing the article. I hope you enjoy it too!
 
P.S. Also check out  this short note  by Marcin Kozak and Wojtek Krzanowski on effective presentation of data.
 
P.P.S. I wrote this blog entry a month ago and had it in storage. Then my issue of Significance came in the mail—with my</p><p>2 0.978181 <a title="2110-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-29-Putting_together_multinomial_discrete_regressions_by_combining_simple_logits.html">782 andrew gelman stats-2011-06-29-Putting together multinomial discrete regressions by combining simple logits</a></p>
<p>Introduction: When predicting 0/1 data we can use logit (or probit or robit or some other robust model such as invlogit (0.01 + 0.98*X*beta)).  Logit is simple enough and we can use  bayesglm  to regularize and avoid the problem of separation.
 
What if there are more than 2 categories?  If they’re ordered (1, 2, 3, etc), we can do ordered logit (and use bayespolr() to avoid separation).  If the categories are unordered (vanilla, chocolate, strawberry), there are unordered multinomial logit and probit models out there.
 
But it’s not so easy to fit these multinomial model in a multilevel setting (with coefficients that vary by group), especially if the computation is embedded in an iterative routine such as mi where you have real time constraints at each step.
 
So this got me wondering whether we could kluge it with logits.  Here’s the basic idea (in the ordered and unordered forms):
 
- If you have a variable that goes 1, 2, 3, etc., set up a series of logits:  1 vs. 2,3,…; 2 vs. 3,…; and so forth</p><p>3 0.97507846 <a title="2110-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-03-A_question_for_psychometricians.html">313 andrew gelman stats-2010-10-03-A question for psychometricians</a></p>
<p>Introduction: Don Coffin writes:
  
A colleague of mine and I are doing a presentation for new faculty on a number of topics related to teaching.  Our charge is to identify interesting issues and to find research-based information for them about how to approach things.  So, what I wondered is, do you know of any published research dealing with the sort of issues about structuring a course and final exam in the ways you talk about in  this blog post ?  Some poking around in the usual places hasn’t turned anything up yet.
  
I don’t really know the psychometrics literature but I imagine that some good stuff has been written on principles of test design. There are probably some good papers from back in the 1920s.  Can anyone supply some references?</p><p>4 0.97208667 <a title="2110-lda-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-21-Why_modern_art_is_all_in_the_mind.html">102 andrew gelman stats-2010-06-21-Why modern art is all in the mind</a></p>
<p>Introduction: This  looks cool:
  
Ten years ago researchers in America took two groups of three-year-olds and showed them a blob of paint on a canvas. Children who were told that the marks were the result of an accidental spillage showed little interest. The others, who had been told that the splodge of colour had been carefully created for them, started to refer to it as “a painting”.


Now that experiment . . . has gone on to form part of the foundation of an influential new book that questions the way in which we respond to art. . . . The book, which is subtitled The New Science of Why We Like What We Like, is not an attack on modern or contemporary art and Bloom says fans of more traditional art are not capable of making purely aesthetic judgments either. “I don’t have a strong position about the art itself,” he said this weekend. “But I do have a strong position about why we actually like it.”
  
This sounds fascinating.  But I’m skeptical about this part:
  
Humans are incapable of just getti</p><p>5 0.96162039 <a title="2110-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-13-Puzzles_of_criminal_justice.html">1621 andrew gelman stats-2012-12-13-Puzzles of criminal justice</a></p>
<p>Introduction: Four recent news stories about crime and punishment made me realize, yet again, how little I understand all this.
 
1.   “HSBC to Pay $1.92 Billion to Settle Charges of Money Laundering” :
  
State and federal authorities decided against indicting HSBC in a money-laundering case over concerns that criminal charges could jeopardize one of the world’s largest banks and ultimately destabilize the global financial system.  Instead, HSBC announced on Tuesday that it had agreed to a record $1.92 billion settlement with authorities. . . .
  
I don’t understand this idea of punishing the institution.  I have the same problem when the NCAA punishes a college football program.  These are individual people breaking the law (or the rules), right?  So why not punish them directly?  Giving 40 lashes to a bunch of HSBC executives and garnisheeing their salaries for life, say, that wouldn’t destabilize the global financial system would it?  From the article:  “A money-laundering indictment, or a guilt</p><p>6 0.95535886 <a title="2110-lda-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-23-Lowess_is_great.html">293 andrew gelman stats-2010-09-23-Lowess is great</a></p>
<p>7 0.95068437 <a title="2110-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-04-High-level_intellectual_discussions_in_the_Columbia_statistics_department.html">745 andrew gelman stats-2011-06-04-High-level intellectual discussions in the Columbia statistics department</a></p>
<p>8 0.95036143 <a title="2110-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-05-Two_exciting_movie_ideas%3A__%E2%80%9CSecond_Chance_U%E2%80%9D_and_%E2%80%9CThe_New_Dirty_Dozen%E2%80%9D.html">1484 andrew gelman stats-2012-09-05-Two exciting movie ideas:  “Second Chance U” and “The New Dirty Dozen”</a></p>
<p>same-blog 9 0.95019507 <a title="2110-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-22-A_Bayesian_model_for_an_increasing_function%2C_in_Stan%21.html">2110 andrew gelman stats-2013-11-22-A Bayesian model for an increasing function, in Stan!</a></p>
<p>10 0.94911671 <a title="2110-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-02-%E2%80%9CIf_our_product_is_harmful_._._._we%E2%80%99ll_stop_making_it.%E2%80%9D.html">1480 andrew gelman stats-2012-09-02-“If our product is harmful . . . we’ll stop making it.”</a></p>
<p>11 0.94313872 <a title="2110-lda-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-14-Felix_Salmon_wins_the_American_Statistical_Association%E2%80%99s_Excellence_in_Statistical_Reporting_Award.html">33 andrew gelman stats-2010-05-14-Felix Salmon wins the American Statistical Association’s Excellence in Statistical Reporting Award</a></p>
<p>12 0.93742645 <a title="2110-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-23-When_are_complicated_models_helpful_in_psychology_research_and_when_are_they_overkill%3F.html">1690 andrew gelman stats-2013-01-23-When are complicated models helpful in psychology research and when are they overkill?</a></p>
<p>13 0.93211198 <a title="2110-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-21-Building_a_regression_model_._._._with_only_27_data_points.html">1506 andrew gelman stats-2012-09-21-Building a regression model . . . with only 27 data points</a></p>
<p>14 0.93079817 <a title="2110-lda-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-20-Are_the_Democrats_avoiding_a_national_campaign%3F.html">286 andrew gelman stats-2010-09-20-Are the Democrats avoiding a national campaign?</a></p>
<p>15 0.93036413 <a title="2110-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-29-Splitting_the_data.html">544 andrew gelman stats-2011-01-29-Splitting the data</a></p>
<p>16 0.92865419 <a title="2110-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-16-Objects_of_the_class_%E2%80%9CObjects_of_the_class%E2%80%9D.html">2103 andrew gelman stats-2013-11-16-Objects of the class “Objects of the class”</a></p>
<p>17 0.92229736 <a title="2110-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-25-100-year_floods.html">628 andrew gelman stats-2011-03-25-100-year floods</a></p>
<p>18 0.92107975 <a title="2110-lda-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-20-Mailing_List_Degree-of-Difficulty_Difficulty.html">2178 andrew gelman stats-2014-01-20-Mailing List Degree-of-Difficulty Difficulty</a></p>
<p>19 0.91873783 <a title="2110-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-07-Inference_%3D_data_%2B_model.html">1201 andrew gelman stats-2012-03-07-Inference = data + model</a></p>
<p>20 0.9181565 <a title="2110-lda-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-19-Just_chaid.html">421 andrew gelman stats-2010-11-19-Just chaid</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
