<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1723 andrew gelman stats-2013-02-15-Wacky priors can work well?</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-1723" href="#">andrew_gelman_stats-2013-1723</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1723 andrew gelman stats-2013-02-15-Wacky priors can work well?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-1723-html" href="http://andrewgelman.com/2013/02/15/wacky-priors-can-work-well/">html</a></p><p>Introduction: Dave Judkins writes:
  
I would love to see a blog entry on  this article , Bayesian Model Selection in High-Dimensional Settings, by Valen Johnson and David Rossell.  The simulation results are very encouraging although the choice of colors for some of the graphics is unfortunate.  Unless I am colorblind in some way that I am unaware of, they have two thin charcoal lines that are indistinguishable.
  
When Dave Judkins puts in a request, I’ll respond.  Also, I’m always happy to see a new Val Johnson paper.  Val and I are contemporaries—he and I got our PhD’s at around the same time, with both of us working on Bayesian image reconstruction, then in the early 1990s Val was part of the legendary group at Duke’s Institute of Statistics and Decision Sciences—a veritable ’27 Yankees featuring Mike West, Merlise Clyde, Michael Lavine, Dave Higdon, Peter Mueller, Val, and a bunch of others.  I always thought it was too bad they all had to go their separate ways.
 
Val also wrote two classic p</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The simulation results are very encouraging although the choice of colors for some of the graphics is unfortunate. [sent-2, score-0.133]
</p><p>2 Unless I am colorblind in some way that I am unaware of, they have two thin charcoal lines that are indistinguishable. [sent-3, score-0.277]
</p><p>3 Also, I’m always happy to see a new Val Johnson paper. [sent-5, score-0.075]
</p><p>4 I always thought it was too bad they all had to go their separate ways. [sent-7, score-0.075]
</p><p>5 Val also wrote two classic papers featuring multilevel modeling, one on adjustment of college grades (leading to a proposal that Duke University famously shot down), and one on primate intelligence. [sent-8, score-0.514]
</p><p>6 The true model is never in the set of models I’m fitting. [sent-14, score-0.345]
</p><p>7 Rather, the true model is always out of reach, a bit more complicated then I ever have the data and technology to fit. [sent-15, score-0.337]
</p><p>8 They also write:    In practice, it is usually important to identify not only the most probable model for a given set of data, but also the probability that the identified model is correct. [sent-16, score-0.882]
</p><p>9 I take Johnson and Rossell’s word that this describes their practice but it doesn’t describe mine. [sent-17, score-0.077]
</p><p>10 I know ahead of time that the probability is zero that the identified model is correct. [sent-18, score-0.555]
</p><p>11 The proposal is for Bayesian regression where each coefficient has a prior distribution that is a mix of a spike at zero and a funny-shaped distribution for the nonzero values. [sent-23, score-0.656]
</p><p>12 I’d be interested in comparing to a direct Bayesian approach that keeps all the coefficients in the model and just uses a hierarchical prior that partially pools everything to zero. [sent-24, score-0.416]
</p><p>13 To answer Dave’s implicit question:  I think Figure 1 would’ve worked better as three small graphs on common scale. [sent-27, score-0.071]
</p><p>14 It would be more readable and actually take up less space. [sent-28, score-0.065]
</p><p>15 Also, set the y-axis to go to zero at zero, and remove the box (in R talk, use plot(…,bty=”l”). [sent-29, score-0.238]
</p><p>16 Figures 2 through 4 would be better as denser grids of plots; that is, use more graphs and fewer lines per graph. [sent-30, score-0.286]
</p><p>17 Also label the lines directly rather than with that legend, and for chrissake don’t have a probability scale that goes below 0 and above 1. [sent-31, score-0.343]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('val', 0.374), ('dave', 0.247), ('johnson', 0.241), ('judkins', 0.213), ('rossell', 0.194), ('model', 0.172), ('duke', 0.164), ('zero', 0.155), ('featuring', 0.147), ('lines', 0.133), ('probability', 0.123), ('proposal', 0.118), ('identified', 0.105), ('bayesian', 0.101), ('prior', 0.099), ('primate', 0.097), ('glib', 0.097), ('lavine', 0.097), ('contemporaries', 0.097), ('valen', 0.097), ('true', 0.09), ('selection', 0.088), ('regularity', 0.087), ('chrissake', 0.087), ('set', 0.083), ('grids', 0.082), ('reconstruction', 0.08), ('yankees', 0.08), ('pools', 0.078), ('practice', 0.077), ('also', 0.076), ('legendary', 0.076), ('spike', 0.076), ('famously', 0.076), ('always', 0.075), ('legend', 0.075), ('probable', 0.075), ('nonzero', 0.074), ('doesn', 0.073), ('unaware', 0.072), ('thin', 0.072), ('graphs', 0.071), ('bounded', 0.07), ('densities', 0.069), ('encouraging', 0.068), ('distribution', 0.067), ('partially', 0.067), ('colors', 0.065), ('readable', 0.065), ('regularization', 0.065)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="1723-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-15-Wacky_priors_can_work_well%3F.html">1723 andrew gelman stats-2013-02-15-Wacky priors can work well?</a></p>
<p>Introduction: Dave Judkins writes:
  
I would love to see a blog entry on  this article , Bayesian Model Selection in High-Dimensional Settings, by Valen Johnson and David Rossell.  The simulation results are very encouraging although the choice of colors for some of the graphics is unfortunate.  Unless I am colorblind in some way that I am unaware of, they have two thin charcoal lines that are indistinguishable.
  
When Dave Judkins puts in a request, I’ll respond.  Also, I’m always happy to see a new Val Johnson paper.  Val and I are contemporaries—he and I got our PhD’s at around the same time, with both of us working on Bayesian image reconstruction, then in the early 1990s Val was part of the legendary group at Duke’s Institute of Statistics and Decision Sciences—a veritable ’27 Yankees featuring Mike West, Merlise Clyde, Michael Lavine, Dave Higdon, Peter Mueller, Val, and a bunch of others.  I always thought it was too bad they all had to go their separate ways.
 
Val also wrote two classic p</p><p>2 0.3315959 <a title="1723-tfidf-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-25-Revised_statistical_standards_for_evidence_%28comments_to_Val_Johnson%E2%80%99s_comments_on_our_comments_on_Val%E2%80%99s_comments_on_p-values%29.html">2305 andrew gelman stats-2014-04-25-Revised statistical standards for evidence (comments to Val Johnson’s comments on our comments on Val’s comments on p-values)</a></p>
<p>Introduction: As regular readers of this blog are aware, a few months ago Val Johnson  published  an article, “Revised standards for statistical evidence,” making a Bayesian argument that researchers and journals should use a p=0.005 publication threshold rather than the usual p=0.05.
 
Christian Robert and I were unconvinced by Val’s reasoning and wrote a  response , “Revised evidence for statistical standards,” in which we wrote:
  
Johnson’s minimax prior is not intended to correspond to any distribution of effect sizes; rather, it represents a worst case scenario under some mathematical assumptions. Minimax and tradeoffs do well together, and it is hard for us to see how any worst case procedure can supply much guidance on how to balance between two different losses. . . .


We would argue that the appropriate significance level depends on the scenario and that what worked well for agricultural experiments in the 1920s might not be so appropriate for many applications in modern biosciences . . .</p><p>3 0.25233898 <a title="1723-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-19-Revised_evidence_for_statistical_standards.html">2140 andrew gelman stats-2013-12-19-Revised evidence for statistical standards</a></p>
<p>Introduction: X  and I heard about  this  much-publicized recent paper by Val Johnson, who suggests changing the default level of statistical significance from z=2 to z=3 (or, as he puts it, going from p=.05 to p=.005 or .001).  Val argues that you need to go out to 3 standard errors to get a Bayes factor of 25 or 50 in favor of the alternative hypothesis.  I don’t really buy this, first because Val’s model is a weird (to me) mixture of two point masses, which he creates in order to make a minimax argument, and second because I don’t see why you need a Bayes factor of 25 to 50 in order to make a claim.  I’d think that a factor of 5:1, say, provides strong information already—if you really believe those odds.  The real issue, as I see it, is that we’re getting Bayes factors and posterior probabilities we don’t believe, because we’re assuming flat priors that don’t really make sense.  This is a topic that’s come up over and over in recent months on this blog, for example in this discussion of why I  d</p><p>4 0.24340385 <a title="1723-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-21-More_on_Bayesian_model_selection_in_high-dimensional_settings.html">1817 andrew gelman stats-2013-04-21-More on Bayesian model selection in high-dimensional settings</a></p>
<p>Introduction: David Rossell writes:
  
A friend pointed out that you were having an interesting philosophical  discussion  on my paper with Val Johnson [on Bayesian model selection in high-dimensional settings].


I agree with the view that in almost all practical situations the true model is not in the set under consideration. Still, asking a model choice procedure to be able to pick up the correct model when it is in the set under consideration seems a minimal requirement (though perhaps not sufficient). In other words, if a procedure is unable to pick the data-generating model even when it is one of the models under consideration, I don’t have high hopes for it working well in more realistic scenarios either. 


Most results in the history in statistics seem to have been obtained under an assumed model, e.g. why even do MLE or penalized-likelihood if we don’t trust the model. While unrealistic, these results were useful to help understand important basic principles. In our case Val and I are defe</p><p>5 0.19234157 <a title="1723-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>Introduction: In response to the  discussion  of X and me of his recent  paper , Val Johnson writes:
  
I would like to thank Andrew for forwarding his comments on uniformly most powerful Bayesian tests (UMPBTs) to me and his invitation to respond to them.  I think he  (and also Christian Robert) raise a number of interesting points concerning this new class of Bayesian tests, but I think that they may have confounded several issues that might more usefully be examined separately.


The first issue involves the choice of the Bayesian evidence threshold, gamma, used in rejecting a null hypothesis in favor of an alternative hypothesis.  Andrew objects to the higher values of gamma proposed in my recent PNAS article on grounds that too many important scientific effects would be missed if thresholds of 25-50 were routinely used.  These evidence thresholds correspond roughly to p-values of 0.005; Andrew suggests that evidence thresholds around 5 should continue to be used (gamma=5 corresponds approximate</p><p>6 0.17399794 <a title="1723-tfidf-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>7 0.15929575 <a title="1723-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-10-Bringing_Causal_Models_Into_the_Mainstream.html">703 andrew gelman stats-2011-05-10-Bringing Causal Models Into the Mainstream</a></p>
<p>8 0.1445097 <a title="1723-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>9 0.14439063 <a title="1723-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>10 0.14332947 <a title="1723-tfidf-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-09-Using_ranks_as_numbers.html">136 andrew gelman stats-2010-07-09-Using ranks as numbers</a></p>
<p>11 0.14202897 <a title="1723-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>12 0.14005519 <a title="1723-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-07-Prior_distributions_for_regression_coefficients.html">1486 andrew gelman stats-2012-09-07-Prior distributions for regression coefficients</a></p>
<p>13 0.13813265 <a title="1723-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>14 0.13673484 <a title="1723-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>15 0.13358076 <a title="1723-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Kind_of_Bayesian.html">811 andrew gelman stats-2011-07-20-Kind of Bayesian</a></p>
<p>16 0.13083422 <a title="1723-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>17 0.1281189 <a title="1723-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>18 0.12789091 <a title="1723-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>19 0.12704593 <a title="1723-tfidf-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Useful_models%2C_model_checking%2C_and_external_validation%3A__a_mini-discussion.html">244 andrew gelman stats-2010-08-30-Useful models, model checking, and external validation:  a mini-discussion</a></p>
<p>20 0.1261355 <a title="1723-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.247), (1, 0.173), (2, -0.016), (3, 0.057), (4, 0.008), (5, -0.048), (6, 0.019), (7, 0.049), (8, -0.026), (9, 0.001), (10, 0.014), (11, 0.019), (12, -0.019), (13, 0.003), (14, -0.058), (15, -0.021), (16, 0.019), (17, 0.001), (18, -0.003), (19, 0.018), (20, 0.016), (21, 0.035), (22, -0.038), (23, -0.058), (24, -0.026), (25, -0.021), (26, -0.007), (27, -0.026), (28, 0.003), (29, -0.023), (30, -0.015), (31, 0.005), (32, -0.035), (33, 0.024), (34, -0.047), (35, -0.047), (36, 0.048), (37, -0.034), (38, 0.029), (39, 0.026), (40, -0.039), (41, -0.034), (42, 0.006), (43, 0.028), (44, 0.031), (45, -0.032), (46, -0.041), (47, 0.014), (48, 0.007), (49, 0.014)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95743537 <a title="1723-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-15-Wacky_priors_can_work_well%3F.html">1723 andrew gelman stats-2013-02-15-Wacky priors can work well?</a></p>
<p>Introduction: Dave Judkins writes:
  
I would love to see a blog entry on  this article , Bayesian Model Selection in High-Dimensional Settings, by Valen Johnson and David Rossell.  The simulation results are very encouraging although the choice of colors for some of the graphics is unfortunate.  Unless I am colorblind in some way that I am unaware of, they have two thin charcoal lines that are indistinguishable.
  
When Dave Judkins puts in a request, I’ll respond.  Also, I’m always happy to see a new Val Johnson paper.  Val and I are contemporaries—he and I got our PhD’s at around the same time, with both of us working on Bayesian image reconstruction, then in the early 1990s Val was part of the legendary group at Duke’s Institute of Statistics and Decision Sciences—a veritable ’27 Yankees featuring Mike West, Merlise Clyde, Michael Lavine, Dave Higdon, Peter Mueller, Val, and a bunch of others.  I always thought it was too bad they all had to go their separate ways.
 
Val also wrote two classic p</p><p>2 0.83736581 <a title="1723-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-11-Gelman_on_Hennig_on_Gelman_on_Bayes.html">1208 andrew gelman stats-2012-03-11-Gelman on Hennig on Gelman on Bayes</a></p>
<p>Introduction: Deborah Mayo pointed me to  this discussion  by Christian Hennig of my recent  article  on Induction and Deduction in Bayesian Data Analysis.
 
A couple days ago I  responded  to comments by Mayo, Stephen Senn, and Larry Wasserman.  I will respond to Hennig by pulling out paragraphs from his discussion and then replying.
 
Hennig:
  
for me the terms “frequentist” and “subjective Bayes” point to interpretations of probability, and not to specific methods of inference. The frequentist one refers to the idea that there is an underlying data generating process that repeatedly throws out data and would approximate the assumed distribution if one could only repeat it infinitely often.
  
Hennig makes the good point that, if this is the way you would define “frequentist” (it’s not how I’d define the term myself, but I’ll use Hennig’s definition here), then it makes sense to be a frequentist in some settings but not others.  Dice really can be rolled over and over again; a sample survey of 15</p><p>3 0.82463282 <a title="1723-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-08-P-values_and_statistical_practice.html">1713 andrew gelman stats-2013-02-08-P-values and statistical practice</a></p>
<p>Introduction: From  my new article  in the journal Epidemiology:
  
Sander Greenland and Charles Poole accept that P values are here to stay but recognize that some of their most common interpretations have problems. The casual view of the P value as posterior probability of the truth of the null hypothesis is false and not even close to valid under any reasonable model, yet this misunderstanding persists even in high-stakes settings (as discussed, for example, by Greenland in 2011). The formal view of the P value as a probability conditional on the null is mathematically correct but typically irrelevant to research goals (hence, the popularity of alternative—if wrong—interpretations). A Bayesian interpretation based on a spike-and-slab model makes little sense in applied contexts in epidemiology, political science, and other fields in which true effects are typically nonzero and bounded (thus violating both the “spike” and the “slab” parts of the model).


I find Greenland and Poole’s perspective t</p><p>4 0.82264775 <a title="1723-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>Introduction: Konrad Scheffler writes:
  
I was interested by your  paper  “Induction and deduction in Bayesian data analysis” and was wondering if you would entertain a few questions:
  
  
  
 – Under the banner of objective Bayesianism, I would posit something like this as a description of Bayesian inference:


“Objective Bayesian probability is not a degree of belief (which would necessarily be subjective) but a measure of the plausibility of a hypothesis, conditional on a formally specified information state. One way of specifying a formal information state is to specify a model, which involves specifying both a prior distribution (typically for a set of unobserved variables) and a likelihood function (typically for a set of observed variables, conditioned on the values of the unobserved variables). Bayesian inference involves calculating the objective degree of plausibility of a hypothesis (typically the truth value of the hypothesis is a function of the variables mentioned above) given such a</p><p>5 0.80412412 <a title="1723-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>Introduction: Nick Firoozye writes:
  
I had a question about BMA [Bayesian model averaging] and model combinations in general, and direct it to you since they are a basic form of hierarchical model, albeit in the simplest of forms. I wanted to ask what the underlying assumptions are that could lead to BMA improving on a larger model. 


I know model combination is a topic of interest in the (frequentist) econometrics community (e.g., Bates & Granger, http://www.jstor.org/discover/10.2307/3008764?uid=3738032&uid;=2&uid;=4&sid;=21101948653381) but at the time it was considered a bit of a puzzle. Perhaps small models combined outperform a big model due to standard errors, insufficient data, etc. But I haven’t seen much in way of Bayesian justification.


In simplest terms, you might have a joint density P(Y,theta_1,theta_2) from which you could use the two marginals P(Y,theta_1) and P(Y,theta_2) to derive two separate forecasts. A BMA-er would do a weighted average of the two forecast densities, having p</p><p>6 0.80222654 <a title="1723-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-28-Plain_old_everyday_Bayesianism%21.html">1829 andrew gelman stats-2013-04-28-Plain old everyday Bayesianism!</a></p>
<p>7 0.79322934 <a title="1723-lsi-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-05-Does_posterior_predictive_model_checking_fit_with_the_operational_subjective_approach%3F.html">320 andrew gelman stats-2010-10-05-Does posterior predictive model checking fit with the operational subjective approach?</a></p>
<p>8 0.79049611 <a title="1723-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-25-Incoherence_of_Bayesian_data_analysis.html">1510 andrew gelman stats-2012-09-25-Incoherence of Bayesian data analysis</a></p>
<p>9 0.78848946 <a title="1723-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>10 0.78491551 <a title="1723-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-17-Christian_Robert_on_the_Jeffreys-Lindley_paradox%3B_more_generally%2C_it%E2%80%99s_good_news_when_philosophical_arguments_can_be_transformed_into_technical_modeling_issues.html">2027 andrew gelman stats-2013-09-17-Christian Robert on the Jeffreys-Lindley paradox; more generally, it’s good news when philosophical arguments can be transformed into technical modeling issues</a></p>
<p>11 0.78308541 <a title="1723-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-04-David_MacKay_and_Occam%E2%80%99s_Razor.html">1041 andrew gelman stats-2011-12-04-David MacKay and Occam’s Razor</a></p>
<p>12 0.78208905 <a title="1723-lsi-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-21-Models_with_constraints.html">2342 andrew gelman stats-2014-05-21-Models with constraints</a></p>
<p>13 0.77916008 <a title="1723-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-18-Understanding_posterior_p-values.html">2029 andrew gelman stats-2013-09-18-Understanding posterior p-values</a></p>
<p>14 0.77813447 <a title="1723-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>15 0.77757901 <a title="1723-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-21-More_on_Bayesian_model_selection_in_high-dimensional_settings.html">1817 andrew gelman stats-2013-04-21-More on Bayesian model selection in high-dimensional settings</a></p>
<p>16 0.77512252 <a title="1723-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Kind_of_Bayesian.html">811 andrew gelman stats-2011-07-20-Kind of Bayesian</a></p>
<p>17 0.76927865 <a title="1723-lsi-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-07-Bayesian_hierarchical_model_for_the_prediction_of_soccer_results.html">20 andrew gelman stats-2010-05-07-Bayesian hierarchical model for the prediction of soccer results</a></p>
<p>18 0.7682907 <a title="1723-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>19 0.76564932 <a title="1723-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-15-More_on_AIC%2C_WAIC%2C_etc.html">1983 andrew gelman stats-2013-08-15-More on AIC, WAIC, etc</a></p>
<p>20 0.76468158 <a title="1723-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-17-Modeling_group-level_predictors_in_a_multilevel_regression.html">1216 andrew gelman stats-2012-03-17-Modeling group-level predictors in a multilevel regression</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(5, 0.018), (15, 0.023), (16, 0.068), (21, 0.036), (24, 0.161), (34, 0.191), (47, 0.031), (66, 0.027), (77, 0.03), (82, 0.011), (89, 0.018), (99, 0.266)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96267998 <a title="1723-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-23-AI_Stats_conference_on_Stan_etc..html">1911 andrew gelman stats-2013-06-23-AI Stats conference on Stan etc.</a></p>
<p>Introduction: Jaakko Peltonen writes:
  
The Seventeenth International Conference on Artificial Intelligence and Statistics (http://www.aistats.org) will be next April in Reykjavik, Iceland. AISTATS is an interdisciplinary conference at the intersection of computer science, artificial intelligence, machine learning, statistics, and related areas.
  
  
  
============================================================================== 
AISTATS 2014 Call for Papers 
Seventeenth International Conference on Artificial Intelligence and Statistics 
April 22 – 25, 2014, Reykjavik, Iceland


http://www.aistats.org


Colocated with a MLSS Machine Learning Summer School 
==============================================================================


AISTATS is an interdisciplinary gathering of researchers at the intersection of computer science, artificial intelligence, machine learning, statistics, and related areas. Since its inception in 1985, the primary goal of AISTATS has been to broaden research in the</p><p>2 0.95510626 <a title="1723-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-29-How_many_parameters_are_in_a_multilevel_model%3F.html">1144 andrew gelman stats-2012-01-29-How many parameters are in a multilevel model?</a></p>
<p>Introduction: Stephen Collins writes:
  
I’m reading your Multilevel modeling book and am trying to apply it to my work.  I’m concerned with how to estimate a random intercept model if there are hundreds/thousands of levels.  In the Gibbs sampling, am I sampling a parameter for each level?  Or, just the hyper-parameters?  In other words, say I had 500 zipcode intercepts modeled as ~ N(m,s).  Would my posterior be two dimensional, sampling for “m” and “s,” or would it have 502 dimensions?
  
My reply:  Indeed you will have hundreds or thousands of parameters—or, in classical terms, hundreds or thousands of predictive quantities.  But that’s ok.  Even if none of those predictions is precise, you’re learning  about the model.
 
See page 526 of the book for more discussion of the number of parameters in a multilevel model.</p><p>3 0.95209825 <a title="1723-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-27-Visual_diagnostics_for_discrete-data_regressions.html">929 andrew gelman stats-2011-09-27-Visual diagnostics for discrete-data regressions</a></p>
<p>Introduction: Jeff asked me what I thought of  this  recent AJPS article by Brian Greenhill, Michael Ward, and Audrey Sacks, “The Separation Plot: A New Visual Method for Evaluating the Fit of Binary Models.”  It’s similar to a graph of observed vs. predicted values, but using color rather than the y-axis to display the observed values.  It seems like it could be useful, also could be applied more generally to discrete-data regressions with more than two categories.
 
When it comes to checking the model fit, I recommend binned residual plots, as discussed in  this 2000 article  with Yuri Goegebeur, Francis Tuerlinckx, and Iven Van Mechelen.</p><p>4 0.95177394 <a title="1723-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-18-More_studies_on_the_economic_effects_of_climate_change.html">1501 andrew gelman stats-2012-09-18-More studies on the economic effects of climate change</a></p>
<p>Introduction: After writing  yesterday’s post , I was going through Solomon Hsiang’s blog and found  a post  pointing to three studies from researchers at business schools:
  
Severe Weather and Automobile Assembly Productivity


Gérard P. Cachon, Santiago Gallino and Marcelo Olivares


Abstract: It is expected that climate change could lead to an increased frequency of severe weather. In turn, severe weather intuitively should hamper the productivity of work that occurs outside. But what is the effect of rain, snow, fog, heat and wind on work that occurs indoors, such as the production of automobiles? Using weekly production data from 64 automobile plants in the United States over a ten-year period, we ﬁnd that adverse weather conditions lead to a signiﬁcant reduction in production. For example, one additional day of high wind advisory by the National Weather Service (i.e., maximum winds generally in excess of 44 miles per hour) reduces production by 26%, which is comparable in order of magnitude t</p><p>5 0.94683033 <a title="1723-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-23-Doug_Hibbs_on_the_fundamentals_in_2010.html">292 andrew gelman stats-2010-09-23-Doug Hibbs on the fundamentals in 2010</a></p>
<p>Introduction: Hibbs, one of the original economy-and-elections guys,  writes :
  
The number of House seats won by the presidents party at midterm elections is well explained by three pre-determined or exogenous variables: (1) the number of House seats won by the in-party at the previous on-year election, (2) the vote margin of the in-partys candidate at the previous presidential election, and (3) the average growth rate of per capita real disposable personal income during the congressional term. Given the partisan division of House seats following the 2008 on-year election, President Obamas margin of victory in 2008, and the weak growth of per capita real income during the rst 6 quarters of the 111th Congress, the Democrats chances of holding on to a House majority by winning at least 218 seats at the 2010 midterm election will depend on real income growth in the 3rd quarter of 2010. The data available at this writing indicate the that Democrats will win 211 seats, a loss of 45 from the 2008 o</p><p>6 0.94481921 <a title="1723-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-23-Life_in_the_C-suite%3A__A_graph_that_is_both_ugly_and_bad%2C_and_an_unrelated_story.html">1734 andrew gelman stats-2013-02-23-Life in the C-suite:  A graph that is both ugly and bad, and an unrelated story</a></p>
<p>7 0.93569994 <a title="1723-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-17-%E2%80%9C2%25_per_degree_Celsius_._._._the_magic_number_for_how_worker_productivity_responds_to_warm-hot_temperatures%E2%80%9D.html">1500 andrew gelman stats-2012-09-17-“2% per degree Celsius . . . the magic number for how worker productivity responds to warm-hot temperatures”</a></p>
<p>8 0.93221891 <a title="1723-lda-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-09-Rasmussen_sez%3A__%E2%80%9C108%25_of_Respondents_Say_._._.%E2%80%9D.html">135 andrew gelman stats-2010-07-09-Rasmussen sez:  “108% of Respondents Say . . .”</a></p>
<p>9 0.92510998 <a title="1723-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-10-The_blog_of_the_Cultural_Cognition_Project.html">1111 andrew gelman stats-2012-01-10-The blog of the Cultural Cognition Project</a></p>
<p>10 0.92214787 <a title="1723-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-13-Hey%2C_you%21__Don%E2%80%99t_take_that_class%21.html">956 andrew gelman stats-2011-10-13-Hey, you!  Don’t take that class!</a></p>
<p>same-blog 11 0.92022151 <a title="1723-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-15-Wacky_priors_can_work_well%3F.html">1723 andrew gelman stats-2013-02-15-Wacky priors can work well?</a></p>
<p>12 0.91755909 <a title="1723-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-02-Covariate_Adjustment_in_RCT_-_Model_Overfitting_in_Multilevel_Regression.html">936 andrew gelman stats-2011-10-02-Covariate Adjustment in RCT - Model Overfitting in Multilevel Regression</a></p>
<p>13 0.90739453 <a title="1723-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-05-Cleaning_up_science.html">1842 andrew gelman stats-2013-05-05-Cleaning up science</a></p>
<p>14 0.90011472 <a title="1723-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-01-My_course_this_fall_on_Bayesian_Computation.html">884 andrew gelman stats-2011-09-01-My course this fall on Bayesian Computation</a></p>
<p>15 0.88363492 <a title="1723-lda-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-10-Small_multiples_of_lineplots_%3E_maps_%28ok%2C_not_always%2C_but_yes_in_this_case%29.html">2288 andrew gelman stats-2014-04-10-Small multiples of lineplots > maps (ok, not always, but yes in this case)</a></p>
<p>16 0.88306767 <a title="1723-lda-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-25-Revised_statistical_standards_for_evidence_%28comments_to_Val_Johnson%E2%80%99s_comments_on_our_comments_on_Val%E2%80%99s_comments_on_p-values%29.html">2305 andrew gelman stats-2014-04-25-Revised statistical standards for evidence (comments to Val Johnson’s comments on our comments on Val’s comments on p-values)</a></p>
<p>17 0.88119566 <a title="1723-lda-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-26-Infoviz_on_top_of_stat_graphic_on_top_of_spreadsheet.html">2186 andrew gelman stats-2014-01-26-Infoviz on top of stat graphic on top of spreadsheet</a></p>
<p>18 0.88109207 <a title="1723-lda-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-02-%E2%80%9CRegression_to_the_mean%E2%80%9D_is_fine.__But_what%E2%80%99s_the_%E2%80%9Cmean%E2%80%9D%3F.html">312 andrew gelman stats-2010-10-02-“Regression to the mean” is fine.  But what’s the “mean”?</a></p>
<p>19 0.87757522 <a title="1723-lda-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-08-%E2%80%9CGuys_who_do_more_housework_get_less_sex%E2%80%9D.html">2203 andrew gelman stats-2014-02-08-“Guys who do more housework get less sex”</a></p>
<p>20 0.87750447 <a title="1723-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-19-Revised_evidence_for_statistical_standards.html">2140 andrew gelman stats-2013-12-19-Revised evidence for statistical standards</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
