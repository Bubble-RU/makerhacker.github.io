<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1956 andrew gelman stats-2013-07-25-What should be in a machine learning course?</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-1956" href="#">andrew_gelman_stats-2013-1956</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1956 andrew gelman stats-2013-07-25-What should be in a machine learning course?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-1956-html" href="http://andrewgelman.com/2013/07/25/what-should-be-in-a-machine-learning-course/">html</a></p><p>Introduction: Nando de Freitas writes:
  
We’re designing two machine learning (ML) courses at Oxford (introductory and advanced ML).


In doing this, we have many questions and wonder what your thoughts are on the following:


- Which do you think are the key optimization papers/ideas that should be covered. 
- Which topics do you think are coolest things in ML? 
- Which are the essential ideas, tools and approaches? 
- Are there other courses you would recommend? 
- Which are good resources for students to learn to code and apply convolutional nets? Theano? What are the key deep learning things to know first? 
- Which are the best scalable classifiers? … pegasos .. etc. 
- Which are the coolest applications that can be easily given as a programming exercise? 
- What theory to teach? PAC? PAC-Bayes? CLTs? 
- What are the best tutorials on sample complexity for ML? 
- How much should we emphasize the trade-offs of computing/optimization-approximation-estimation. 
- What are the ML algorithms mostly</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Nando de Freitas writes:    We’re designing two machine learning (ML) courses at Oxford (introductory and advanced ML). [sent-1, score-0.532]
</p><p>2 In doing this, we have many questions and wonder what your thoughts are on the following:   - Which do you think are the key optimization papers/ideas that should be covered. [sent-2, score-0.147]
</p><p>3 - Which topics do you think are coolest things in ML? [sent-3, score-0.244]
</p><p>4 - Which are the essential ideas, tools and approaches? [sent-4, score-0.193]
</p><p>5 - Which are good resources for students to learn to code and apply convolutional nets? [sent-6, score-0.061]
</p><p>6 What are the key deep learning things to know first? [sent-8, score-0.275]
</p><p>7 - Which are the coolest applications that can be easily given as a programming exercise? [sent-13, score-0.305]
</p><p>8 - What are the best tutorials on sample complexity for ML? [sent-18, score-0.254]
</p><p>9 - How much should we emphasize the trade-offs of computing/optimization-approximation-estimation. [sent-19, score-0.105]
</p><p>10 - What are the ML algorithms mostly used in industry? [sent-20, score-0.064]
</p><p>11 - What are the essential ML papers that every student should read? [sent-21, score-0.25]
</p><p>12 - How much should we teach hashing, bloom filters & other sketches? [sent-22, score-0.364]
</p><p>13 - What are the most introductory papers on learning causal models from data (or from interventions)? [sent-23, score-0.383]
</p><p>14 - How much of nonparametrics ala DPs, CRPs, IBPs etc. [sent-24, score-0.146]
</p><p>15 - What are the best bootstrapping papers for the ML audience? [sent-28, score-0.282]
</p><p>16 We’d love diverse answers, short statements, long opinion articles, etc. [sent-32, score-0.069]
</p><p>17 The ultimate course in a way says something about what we think the term ML refers to. [sent-33, score-0.134]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ml', 0.749), ('coolest', 0.194), ('essential', 0.14), ('learning', 0.137), ('introductory', 0.136), ('industry', 0.135), ('courses', 0.132), ('answers', 0.114), ('teach', 0.113), ('papers', 0.11), ('hottest', 0.108), ('filters', 0.108), ('sketches', 0.108), ('tutorials', 0.101), ('nonparametrics', 0.097), ('classifiers', 0.097), ('bloom', 0.094), ('scalable', 0.089), ('best', 0.087), ('bootstrapping', 0.085), ('nets', 0.085), ('oxford', 0.083), ('key', 0.08), ('designing', 0.077), ('interventions', 0.076), ('diverse', 0.069), ('ultimate', 0.069), ('optimization', 0.067), ('complexity', 0.066), ('refers', 0.065), ('exercise', 0.065), ('mcmc', 0.065), ('de', 0.064), ('algorithms', 0.064), ('advanced', 0.064), ('resources', 0.061), ('challenges', 0.06), ('programming', 0.06), ('deep', 0.058), ('machine', 0.058), ('emphasize', 0.056), ('audience', 0.053), ('tools', 0.053), ('approaches', 0.053), ('statements', 0.052), ('impact', 0.051), ('applications', 0.051), ('topics', 0.05), ('word', 0.049), ('much', 0.049)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1956-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-25-What_should_be_in_a_machine_learning_course%3F.html">1956 andrew gelman stats-2013-07-25-What should be in a machine learning course?</a></p>
<p>Introduction: Nando de Freitas writes:
  
We’re designing two machine learning (ML) courses at Oxford (introductory and advanced ML).


In doing this, we have many questions and wonder what your thoughts are on the following:


- Which do you think are the key optimization papers/ideas that should be covered. 
- Which topics do you think are coolest things in ML? 
- Which are the essential ideas, tools and approaches? 
- Are there other courses you would recommend? 
- Which are good resources for students to learn to code and apply convolutional nets? Theano? What are the key deep learning things to know first? 
- Which are the best scalable classifiers? … pegasos .. etc. 
- Which are the coolest applications that can be easily given as a programming exercise? 
- What theory to teach? PAC? PAC-Bayes? CLTs? 
- What are the best tutorials on sample complexity for ML? 
- How much should we emphasize the trade-offs of computing/optimization-approximation-estimation. 
- What are the ML algorithms mostly</p><p>2 0.63783747 <a title="1956-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-28-More_on_that_machine_learning_course.html">1960 andrew gelman stats-2013-07-28-More on that machine learning course</a></p>
<p>Introduction: Following up on our  discussion  the other day, Andrew Ng writes: 
  
  
Looking at the “typical” ML syllabus, I think most classes do a great job teaching the core ideas, but that there’re two recent trends in ML that are usually not yet reflected. 


First, unlike 10 years ago, a lot of our students are now taking ML not to do ML research, but to apply it in other research areas or in  industry.  I’d like to serve these students as well.  While many ML classes do a nice job teaching the theory and core algorithms, I’ve seen very few that teach the “hands-on” tactics for how to actually build a high-performance ML system, or on how to think about piecing together a complex ML architecture.  For example, what sorts of diagnostics do you run to figure out why your algorithm isn’t giving reasonable accuracy?  How much do you invest in collecting additional training data?  How do you structure your org chart and metrics if you think there’re 3 components that need to be built and plugged</p><p>3 0.1069802 <a title="1956-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-13-A_question_about_AIC.html">1377 andrew gelman stats-2012-06-13-A question about AIC</a></p>
<p>Introduction: Jacob Oaknin asks: 
  
  
 Akaike ‘s selection criterion is often justified on the basis of the empirical risk of a ML estimate being a biased estimate of the true generalization error of a parametric family, say the family, S_m, of linear regressors on a m-dimensional variable x=(x_1,..,x_m) with gaussian noise independent of x (for instance in “Unifying the derivations for the Akaike and Corrected Akaike information criteria”, by J.E.Cavanaugh, Statistics and Probability Letters, vol. 33, 1997, pp. 201-208).


On the other hand, the family S_m is known to have finite VC-dimension (VC = m+1), and this fact should grant  that empirical risk minimizer is asymtotically consistent regardless of the underlying probability distribution, and in particular for the assumed gaussian distribution of noise(“An overview of statistical learning theory”, by V.N.Vapnik, IEEE Transactions On Neural Networks, vol. 10, No. 5, 1999, pp. 988-999)


What am I missing?
  
My reply:  I’m no expert on AIC so</p><p>4 0.10429857 <a title="1956-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-13-Flexibility_is_good.html">2133 andrew gelman stats-2013-12-13-Flexibility is good</a></p>
<p>Introduction: If I made a separate post for each interesting blog discussion, we’d get overwhelmed.  That’s why I often leave detailed responses in the comments section, even though I’m pretty sure that most readers don’t look in the comments at all.
 
Sometimes, though, I think it’s good to bring such discussions to light.  Here’s a recent example.
 
Michael  wrote :
  
Poor predictive performance usually indicates that the model isn’t sufficiently flexible to explain the data, and my understanding of the proper Bayesian strategy is to feed that back into your original model and try again until you achieve better performance.
  
Corey  replied :
  
It was my impression that — in ML at least — poor predictive performance is more often due to the model being too flexible and fitting noise.
  
And Rahul  agreed :
  
Good point. A very flexible model will describe your training data perfectly and then go bonkers when unleashed on wild data.
  
But I  wrote :
  
Overfitting comes from a model being flex</p><p>5 0.092124738 <a title="1956-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-08-I_Am_Too_Absolutely_Heteroskedastic_for_This_Probit_Model.html">1047 andrew gelman stats-2011-12-08-I Am Too Absolutely Heteroskedastic for This Probit Model</a></p>
<p>Introduction: Soren Lorensen wrote:
  
I’m working on a project that uses a binary choice model on panel data. Since I have panel data and am using MLE, I’m concerned about heteroskedasticity making my estimates inconsistent and biased. 


Are you familiar with any statistical packages with pre-built tests for heteroskedasticity in binary choice ML models? If not, is there value in cutting my data into groups over which I guess the error variance might vary and eyeballing residual plots? Have you other suggestions about how I might resolve this concern?
  
I replied that I wouldn’t worry so much about heteroskedasticity.  Breaking up the data into pieces might make sense, but for the purpose of estimating how the coefficients might vary—that is, nonlinearity and interactions.
 
Soren shot back:
  
I’m somewhat puzzled however: homoskedasticity is an identifying assumption in estimating a probit model: if we don’t have it all sorts of bad things can happen to our parameter estimates. Do you suggest n</p><p>6 0.089899801 <a title="1956-tfidf-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-24-An_interesting_mosaic_of_a_data_programming_course.html">2345 andrew gelman stats-2014-05-24-An interesting mosaic of a data programming course</a></p>
<p>7 0.089861386 <a title="1956-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-%E2%80%9CIs_machine_learning_a_subset_of_statistics%3F%E2%80%9D.html">1740 andrew gelman stats-2013-02-26-“Is machine learning a subset of statistics?”</a></p>
<p>8 0.080056593 <a title="1956-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-18-Postdoc_positions_at_Microsoft_Research_%E2%80%93_NYC.html">1630 andrew gelman stats-2012-12-18-Postdoc positions at Microsoft Research – NYC</a></p>
<p>9 0.074747771 <a title="1956-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-20-Stan%3A_A_%28Bayesian%29_Directed_Graphical_Model_Compiler.html">1131 andrew gelman stats-2012-01-20-Stan: A (Bayesian) Directed Graphical Model Compiler</a></p>
<p>10 0.073330939 <a title="1956-tfidf-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-12-More_on_publishing_in_journals.html">2245 andrew gelman stats-2014-03-12-More on publishing in journals</a></p>
<p>11 0.072438836 <a title="1956-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-20-Evaluating_Columbia_University%E2%80%99s_Frontiers_of_Science_course.html">1864 andrew gelman stats-2013-05-20-Evaluating Columbia University’s Frontiers of Science course</a></p>
<p>12 0.071130648 <a title="1956-tfidf-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-04-U-Haul_statistics.html">318 andrew gelman stats-2010-10-04-U-Haul statistics</a></p>
<p>13 0.070758902 <a title="1956-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-25-Chris_Schmid_on_Evidence_Based_Medicine.html">1138 andrew gelman stats-2012-01-25-Chris Schmid on Evidence Based Medicine</a></p>
<p>14 0.070037745 <a title="1956-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-04-Model_checking_and_model_understanding_in_machine_learning.html">1482 andrew gelman stats-2012-09-04-Model checking and model understanding in machine learning</a></p>
<p>15 0.069898747 <a title="1956-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-19-Scalability_in_education.html">1502 andrew gelman stats-2012-09-19-Scalability in education</a></p>
<p>16 0.068969823 <a title="1956-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-05-A_locally_organized_online_BDA_course_on_G%2B_hangout%3F.html">2009 andrew gelman stats-2013-09-05-A locally organized online BDA course on G+ hangout?</a></p>
<p>17 0.068019331 <a title="1956-tfidf-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-15-Postdoc_involving_pathbreaking_work_in_MRP%2C_Stan%2C_and_the_2014_election%21.html">2173 andrew gelman stats-2014-01-15-Postdoc involving pathbreaking work in MRP, Stan, and the 2014 election!</a></p>
<p>18 0.067484416 <a title="1956-tfidf-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-14-In_an_introductory_course%2C_when_does_learning_occur%3F.html">277 andrew gelman stats-2010-09-14-In an introductory course, when does learning occur?</a></p>
<p>19 0.066911563 <a title="1956-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-04-Bayesian_Learning_via_Stochastic_Gradient_Langevin_Dynamics.html">1443 andrew gelman stats-2012-08-04-Bayesian Learning via Stochastic Gradient Langevin Dynamics</a></p>
<p>20 0.061565097 <a title="1956-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-04-When_is_there_%E2%80%9Chidden_structure_in_data%E2%80%9D_to_be_discovered%3F.html">1788 andrew gelman stats-2013-04-04-When is there “hidden structure in data” to be discovered?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.102), (1, 0.002), (2, -0.049), (3, 0.004), (4, 0.034), (5, 0.091), (6, -0.019), (7, -0.007), (8, -0.016), (9, -0.002), (10, 0.004), (11, 0.029), (12, -0.03), (13, -0.01), (14, 0.003), (15, -0.044), (16, -0.006), (17, -0.008), (18, -0.029), (19, -0.0), (20, 0.006), (21, -0.025), (22, 0.007), (23, 0.046), (24, -0.01), (25, 0.044), (26, 0.021), (27, 0.006), (28, 0.026), (29, 0.013), (30, 0.01), (31, -0.029), (32, -0.035), (33, -0.024), (34, 0.028), (35, 0.007), (36, -0.027), (37, 0.027), (38, -0.01), (39, -0.049), (40, -0.016), (41, -0.002), (42, -0.017), (43, 0.049), (44, 0.038), (45, 0.059), (46, 0.029), (47, 0.013), (48, 0.025), (49, 0.016)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93270564 <a title="1956-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-25-What_should_be_in_a_machine_learning_course%3F.html">1956 andrew gelman stats-2013-07-25-What should be in a machine learning course?</a></p>
<p>Introduction: Nando de Freitas writes:
  
We’re designing two machine learning (ML) courses at Oxford (introductory and advanced ML).


In doing this, we have many questions and wonder what your thoughts are on the following:


- Which do you think are the key optimization papers/ideas that should be covered. 
- Which topics do you think are coolest things in ML? 
- Which are the essential ideas, tools and approaches? 
- Are there other courses you would recommend? 
- Which are good resources for students to learn to code and apply convolutional nets? Theano? What are the key deep learning things to know first? 
- Which are the best scalable classifiers? … pegasos .. etc. 
- Which are the coolest applications that can be easily given as a programming exercise? 
- What theory to teach? PAC? PAC-Bayes? CLTs? 
- What are the best tutorials on sample complexity for ML? 
- How much should we emphasize the trade-offs of computing/optimization-approximation-estimation. 
- What are the ML algorithms mostly</p><p>2 0.76965243 <a title="1956-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-28-More_on_that_machine_learning_course.html">1960 andrew gelman stats-2013-07-28-More on that machine learning course</a></p>
<p>Introduction: Following up on our  discussion  the other day, Andrew Ng writes: 
  
  
Looking at the “typical” ML syllabus, I think most classes do a great job teaching the core ideas, but that there’re two recent trends in ML that are usually not yet reflected. 


First, unlike 10 years ago, a lot of our students are now taking ML not to do ML research, but to apply it in other research areas or in  industry.  I’d like to serve these students as well.  While many ML classes do a nice job teaching the theory and core algorithms, I’ve seen very few that teach the “hands-on” tactics for how to actually build a high-performance ML system, or on how to think about piecing together a complex ML architecture.  For example, what sorts of diagnostics do you run to figure out why your algorithm isn’t giving reasonable accuracy?  How much do you invest in collecting additional training data?  How do you structure your org chart and metrics if you think there’re 3 components that need to be built and plugged</p><p>3 0.67418092 <a title="1956-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-14-Wickham_R_short_course.html">1009 andrew gelman stats-2011-11-14-Wickham R short course</a></p>
<p>Introduction: Hadley  writes:
  
I [Hadley] am going to be teaching an R development master class in New York City on Dec 
12-13. The basic idea of the class is to help you write better code, 
focused on the mantra of “do not repeat yourself”. In day one you will 
learn powerful new tools of abstraction, allowing you to solve a wider 
range of problems with fewer lines of code. Day two will teach you how 
to make packages, the fundamental unit of code distribution in R, 
allowing others to save time by allowing them to use your code.


To get the most out of this course, you should have some experience 
programming in R already: you should be familiar with writing 
functions, and the basic data structures of R: vectors, matrices, 
arrays, lists and data frames. You will find the course particularly 
useful if you’re an experienced R user looking to take the next step, 
or if you’re moving to R from other programming languages and you want 
to quickly get up to speed with R’s unique features. A coupl</p><p>4 0.67237771 <a title="1956-lsi-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-30-Nano-project_qualifying_exam_process%3A__An_intensified_dialogue_between_students_and_faculty.html">308 andrew gelman stats-2010-09-30-Nano-project qualifying exam process:  An intensified dialogue between students and faculty</a></p>
<p>Introduction: Joe Blitzstein and Xiao-Li Meng  write :
  
An effectively designed examination process goes far beyond revealing students’ knowledge or skills. It also serves as a great teaching and learning tool, incentivizing the students to think more deeply and to connect the dots at a higher level. This extends throughout the entire process: pre-exam preparation, the exam itself, and the post-exam period (the aftermath or, more appropriately, afterstat of the exam). As in the publication process, the first submission is essential but still just one piece in the dialogue.


Viewing the entire exam process as an extended dialogue between students and faculty, we discuss ideas for making this dialogue induce more inspiration than perspiration, and thereby making it a memorable deep-learning triumph rather than a wish-to-forget test-taking trauma. We illustrate such a dialogue through a recently introduced course in the Harvard Statistics Department, Stat 399: Problem Solving in Statistics, and tw</p><p>5 0.64327633 <a title="1956-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-01-%E2%80%9COn_Inspiring_Students_and_Being_Human%E2%80%9D.html">1517 andrew gelman stats-2012-10-01-“On Inspiring Students and Being Human”</a></p>
<p>Introduction: Rachel Schutt (the author of the Taxonomy of Confusion)  has a blog!  for the course she’s teaching at Columbia, “Introduction to Data Science.”  It sounds like a great course—I wish I could take it!
 
Her latest post is “On Inspiring Students and Being Human”:
 
   
  
Of course one hopes as a teacher that one will inspire students . . . But what I actually mean by “inspiring students” is that you are inspiring me; you are students who inspire: “inspiring students”. This is one of the happy unintended consequences of this course so far for me.
  
She then gives examples of some of the students in her class and some of their interesting ideas:
  
Phillip is a PhD student in the sociology department . . . He’s in the process of developing his thesis topic around some of the themes we’ve been discussing in this class, such as the emerging data science community.


Arvi works at the College Board and is a part time student . . . He analyzes user-level data of students who have signed up f</p><p>6 0.63943017 <a title="1956-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-05-New_issue_of_Symposium_magazine.html">1969 andrew gelman stats-2013-08-05-New issue of Symposium magazine</a></p>
<p>7 0.62942725 <a title="1956-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-07-Feedback_on_my_Bayesian_Data_Analysis_class_at_Columbia.html">1611 andrew gelman stats-2012-12-07-Feedback on my Bayesian Data Analysis class at Columbia</a></p>
<p>8 0.62313133 <a title="1956-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-14-A_new_idea_for_a_science_core_course_based_entirely_on_computer_simulation.html">516 andrew gelman stats-2011-01-14-A new idea for a science core course based entirely on computer simulation</a></p>
<p>9 0.60944146 <a title="1956-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-21-Workshop_for_Women_in_Machine_Learning.html">1992 andrew gelman stats-2013-08-21-Workshop for Women in Machine Learning</a></p>
<p>10 0.60263032 <a title="1956-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-03-New_New_York_data_research_organizations.html">1297 andrew gelman stats-2012-05-03-New New York data research organizations</a></p>
<p>11 0.602314 <a title="1956-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-26-Data_Science_for_Social_Good_summer_fellowship_program.html">1777 andrew gelman stats-2013-03-26-Data Science for Social Good summer fellowship program</a></p>
<p>12 0.60067523 <a title="1956-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-13-Drawing_to_Learn_in_Science.html">1056 andrew gelman stats-2011-12-13-Drawing to Learn in Science</a></p>
<p>13 0.59942496 <a title="1956-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-09-Frontiers_of_Science_update.html">1890 andrew gelman stats-2013-06-09-Frontiers of Science update</a></p>
<p>14 0.59420407 <a title="1956-lsi-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-24-An_interesting_mosaic_of_a_data_programming_course.html">2345 andrew gelman stats-2014-05-24-An interesting mosaic of a data programming course</a></p>
<p>15 0.58930254 <a title="1956-lsi-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-14-In_an_introductory_course%2C_when_does_learning_occur%3F.html">277 andrew gelman stats-2010-09-14-In an introductory course, when does learning occur?</a></p>
<p>16 0.58707416 <a title="1956-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-%E2%80%9CIs_machine_learning_a_subset_of_statistics%3F%E2%80%9D.html">1740 andrew gelman stats-2013-02-26-“Is machine learning a subset of statistics?”</a></p>
<p>17 0.58688861 <a title="1956-lsi-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-10-Who%E2%80%99s_holding_the_pen%3F%2C_The_split_screen%2C_and_other_ideas_for_one-on-one_instruction.html">462 andrew gelman stats-2010-12-10-Who’s holding the pen?, The split screen, and other ideas for one-on-one instruction</a></p>
<p>18 0.5849877 <a title="1956-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-11-Zipfian_Academy%2C_A_School_for_Data_Science.html">2016 andrew gelman stats-2013-09-11-Zipfian Academy, A School for Data Science</a></p>
<p>19 0.58126432 <a title="1956-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-09-I_was_at_a_meeting_a_couple_months_ago_._._..html">999 andrew gelman stats-2011-11-09-I was at a meeting a couple months ago . . .</a></p>
<p>20 0.57906419 <a title="1956-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-13-Duke_postdoctoral_fellowships_in_nonparametric_Bayes_%26_high-dimensional_data.html">903 andrew gelman stats-2011-09-13-Duke postdoctoral fellowships in nonparametric Bayes & high-dimensional data</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.01), (8, 0.016), (9, 0.013), (16, 0.045), (19, 0.015), (24, 0.142), (35, 0.012), (37, 0.028), (43, 0.081), (44, 0.038), (53, 0.112), (63, 0.014), (65, 0.01), (68, 0.013), (72, 0.013), (73, 0.016), (77, 0.012), (86, 0.039), (91, 0.015), (99, 0.247)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94588345 <a title="1956-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-25-What_should_be_in_a_machine_learning_course%3F.html">1956 andrew gelman stats-2013-07-25-What should be in a machine learning course?</a></p>
<p>Introduction: Nando de Freitas writes:
  
We’re designing two machine learning (ML) courses at Oxford (introductory and advanced ML).


In doing this, we have many questions and wonder what your thoughts are on the following:


- Which do you think are the key optimization papers/ideas that should be covered. 
- Which topics do you think are coolest things in ML? 
- Which are the essential ideas, tools and approaches? 
- Are there other courses you would recommend? 
- Which are good resources for students to learn to code and apply convolutional nets? Theano? What are the key deep learning things to know first? 
- Which are the best scalable classifiers? … pegasos .. etc. 
- Which are the coolest applications that can be easily given as a programming exercise? 
- What theory to teach? PAC? PAC-Bayes? CLTs? 
- What are the best tutorials on sample complexity for ML? 
- How much should we emphasize the trade-offs of computing/optimization-approximation-estimation. 
- What are the ML algorithms mostly</p><p>2 0.94020474 <a title="1956-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-08-I_Am_Too_Absolutely_Heteroskedastic_for_This_Probit_Model.html">1047 andrew gelman stats-2011-12-08-I Am Too Absolutely Heteroskedastic for This Probit Model</a></p>
<p>Introduction: Soren Lorensen wrote:
  
I’m working on a project that uses a binary choice model on panel data. Since I have panel data and am using MLE, I’m concerned about heteroskedasticity making my estimates inconsistent and biased. 


Are you familiar with any statistical packages with pre-built tests for heteroskedasticity in binary choice ML models? If not, is there value in cutting my data into groups over which I guess the error variance might vary and eyeballing residual plots? Have you other suggestions about how I might resolve this concern?
  
I replied that I wouldn’t worry so much about heteroskedasticity.  Breaking up the data into pieces might make sense, but for the purpose of estimating how the coefficients might vary—that is, nonlinearity and interactions.
 
Soren shot back:
  
I’m somewhat puzzled however: homoskedasticity is an identifying assumption in estimating a probit model: if we don’t have it all sorts of bad things can happen to our parameter estimates. Do you suggest n</p><p>3 0.93584895 <a title="1956-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-14-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">1856 andrew gelman stats-2013-05-14-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>Introduction: I think it’s part of my duty as a blogger to intersperse, along with the steady flow of jokes, rants, and literary criticism, some material that will actually be useful to you.
 
So here goes.
 
Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, and Aki Vehtari  write :
  
The  GPstuff  toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods.
  
We can actually now fit Gaussian processes in  Stan .  But for big problems (or even moderately-sized problems), full Bayes can be slow.  GPstuff uses EP, which is faster.  At some point we’d like to implement EP in Stan.  (Right now we’re working with Dave Blei to implement VB.)
 
GPstuff really works.  I saw Aki use it to fit a nonparametric version of the Bangladesh well-switching example in ARM.  He was sitting in his office and just whip</p><p>4 0.9353503 <a title="1956-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-04-Insecure_researchers_aren%E2%80%99t_sharing_their_data.html">991 andrew gelman stats-2011-11-04-Insecure researchers aren’t sharing their data</a></p>
<p>Introduction: Jelte Wicherts writes:
  
I thought you might be interested in reading  this paper  that is to appear this week in PLoS ONE.


In it we [Wicherts, Marjan Bakker, and Dylan Molenaar] show that the willingness to share data from published psychological research is associated both with “the strength of the evidence” (against H0) and the prevalence of errors in the reporting of p-values. 


The issue of data archiving will likely be put on the agenda of granting bodies and the APA/APS because of what Diederik Stapel  did .
  
I hate hate hate hate hate when people don’t share their data.  In fact, that’s the subject of my very first column on ethics for Chance magazine.  I have a story from 22 years ago, when I contacted some scientists and showed them how I could reanalyze their data more efficiently (based on a preliminary analysis of their published summary statistics).  They seemed to feel threatened by the suggestion and refused to send me their raw data.  (It was an animal experiment</p><p>5 0.93438208 <a title="1956-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-18-There_are_no_fat_sprinters.html">1905 andrew gelman stats-2013-06-18-There are no fat sprinters</a></p>
<p>Introduction: This post is by Phil.
 
A little over three years ago I wrote a  post about exercise and weight loss  in which I described losing a fair amount of weight due to (I believe) an exercise regime, with no effort to change my diet; this contradicted the prediction of studies that had recently been released. The comment thread on that post is quite interesting: a lot of people had had similar experiences — losing weight, or keeping it off, with an exercise program that includes very short periods of exercise at maximal intensity — while other people expressed some skepticism about my claims. Some commenters said that I risked injury; others said it was too early to judge anything because my weight loss might not last.
 
The people who predicted injury were right: running the curve during a 200m sprint a month or two after that post, I strained my Achilles tendon. Nothing really serious, but it did keep me off the track for a couple of months, and rather than go back to sprinting I switched t</p><p>6 0.93309927 <a title="1956-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-31-Social_scientists_who_use_medical_analogies_to_explain_causal_inference_are%2C_I_think%2C_implicitly_trying_to_borrow_some_of_the_scientific_and_cultural_authority_of_that_field_for_our_own_purposes.html">1555 andrew gelman stats-2012-10-31-Social scientists who use medical analogies to explain causal inference are, I think, implicitly trying to borrow some of the scientific and cultural authority of that field for our own purposes</a></p>
<p>7 0.92846996 <a title="1956-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-31-%E2%80%9CThreshold_earners%E2%80%9D_and_economic_inequality.html">495 andrew gelman stats-2010-12-31-“Threshold earners” and economic inequality</a></p>
<p>8 0.92585516 <a title="1956-lda-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-03-Is_0.05_too_strict_as_a_p-value_threshold%3F.html">446 andrew gelman stats-2010-12-03-Is 0.05 too strict as a p-value threshold?</a></p>
<p>9 0.92330503 <a title="1956-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-24-Multilevel_modeling_and_instrumental_variables.html">1468 andrew gelman stats-2012-08-24-Multilevel modeling and instrumental variables</a></p>
<p>10 0.92258102 <a title="1956-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-31-Using_sample_size_in_the_prior_distribution.html">547 andrew gelman stats-2011-01-31-Using sample size in the prior distribution</a></p>
<p>11 0.92122531 <a title="1956-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-17-Job_opening_at_new_%E2%80%9Cbig_data%E2%80%9D_consulting_firm%21.html">1902 andrew gelman stats-2013-06-17-Job opening at new “big data” consulting firm!</a></p>
<p>12 0.91706526 <a title="1956-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-30-Annals_of_spam.html">880 andrew gelman stats-2011-08-30-Annals of spam</a></p>
<p>13 0.9167555 <a title="1956-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-29-Zero_is_zero.html">687 andrew gelman stats-2011-04-29-Zero is zero</a></p>
<p>14 0.91561079 <a title="1956-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-31-No_on_Yes-No_decisions.html">2155 andrew gelman stats-2013-12-31-No on Yes-No decisions</a></p>
<p>15 0.91555297 <a title="1956-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-27-Another_silly_graph.html">733 andrew gelman stats-2011-05-27-Another silly graph</a></p>
<p>16 0.91481113 <a title="1956-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-27-Macromuddle.html">1347 andrew gelman stats-2012-05-27-Macromuddle</a></p>
<p>17 0.91394132 <a title="1956-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-13-You_heard_it_here_first%3A_Intense_exercise_can_suppress_appetite.html">2022 andrew gelman stats-2013-09-13-You heard it here first: Intense exercise can suppress appetite</a></p>
<p>18 0.91024685 <a title="1956-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-06-Would_today%E2%80%99s_captains_of_industry_be_happier_in_a_1950s-style_world%3F.html">2010 andrew gelman stats-2013-09-06-Would today’s captains of industry be happier in a 1950s-style world?</a></p>
<p>19 0.90916002 <a title="1956-lda-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-30-Seth_Roberts.html">2313 andrew gelman stats-2014-04-30-Seth Roberts</a></p>
<p>20 0.90849066 <a title="1956-lda-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-21-Careers%2C_one-hit_wonders%2C_and_an_offer_of_a_free_book.html">46 andrew gelman stats-2010-05-21-Careers, one-hit wonders, and an offer of a free book</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
