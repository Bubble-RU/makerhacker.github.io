<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2040 andrew gelman stats-2013-09-26-Difficulties in making inferences about scientific truth from distributions of published p-values</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-2040" href="#">andrew_gelman_stats-2013-2040</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2040 andrew gelman stats-2013-09-26-Difficulties in making inferences about scientific truth from distributions of published p-values</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-2040-html" href="http://andrewgelman.com/2013/09/26/difficulties-in-making-inferences-about-scientific-truth-from-distributions-of-published-p-values/">html</a></p><p>Introduction: Jeff Leek  just posted  the discussions of his paper (with Leah Jager), “An estimate of the science-wise false discovery rate and application to the top medical literature,” along with some further comments of his own.
 
 Here  are my original thoughts on an earlier version of their article.  Keith O’Rourke and I expanded these thoughts into  a formal comment  for the journal.  We’re pretty much in agreement with John Ioannidis (you can find his discussion in the top link above).
 
In quick summary, I agree with Jager and Leek that this is an important topic.  I think there are two key places where Keith and I disagree with them:
 
1.  They take published p-values at face value whereas we consider them as the result of a complicated process of selection.  This is something I didn’t used to think much about, but now I’ve become increasingly convinced that the problems with published p-values is not a simple file-drawer effect or the case of a few p=0.051 values nudged toward p=0.049, bu</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Jeff Leek  just posted  the discussions of his paper (with Leah Jager), “An estimate of the science-wise false discovery rate and application to the top medical literature,” along with some further comments of his own. [sent-1, score-0.393]
</p><p>2 Keith O’Rourke and I expanded these thoughts into  a formal comment  for the journal. [sent-3, score-0.136]
</p><p>3 We’re pretty much in agreement with John Ioannidis (you can find his discussion in the top link above). [sent-4, score-0.129]
</p><p>4 They take published p-values at face value whereas we consider them as the result of a complicated process of selection. [sent-7, score-0.167]
</p><p>5 This is something I didn’t used to think much about, but now I’ve become increasingly convinced that the problems with published p-values is not a simple file-drawer effect or the case of a few p=0. [sent-8, score-0.274]
</p><p>6 049, but rather an ongoing process in which tests are performed contingent on data. [sent-10, score-0.233]
</p><p>7 Jager and Leek talk about things such as “the science-wise false discovery rate. [sent-13, score-0.224]
</p><p>8 To start with, I don’t think people are usually studying zero effects. [sent-15, score-0.195]
</p><p>9 I think what is happening is that there are many effects that are small, and these effects can vary (so that, for example, a treatment could have a small positive effect in one scenario and a small negative effect in another scenario). [sent-16, score-0.763]
</p><p>10 Errors can be defined in various ways, but as a start I like to think about type S (sign) and type M (magnitude) errors. [sent-17, score-0.469]
</p><p>11 I certainly believe that the Type S error rate is less than 50%:  we’d expect to get the sign of any comparison correct more than half the time, if there’s any signal whatsoever! [sent-18, score-0.398]
</p><p>12 Are true effects overestimated by more than a factor of 2, more than 50% of the time? [sent-21, score-0.304]
</p><p>13 In short, I think there are a few more steps needed before your method maps to science as practiced. [sent-24, score-0.159]
</p><p>14 Simple calculations have their place, as long as their limitations are understood, and I believe that this sort of discussion pushes the field forward. [sent-26, score-0.182]
</p><p>15 A separate, but related, issue, is I think an idea of Jager and Leek that underlies all this work, which is that scientists are generally pretty reasonable people and science as a whole seems to be pretty sensible. [sent-27, score-0.37]
</p><p>16 And even the routine finding of statistical significance amid noise is, I’m willing to believe, usually done in the service of some true underlying effects. [sent-31, score-0.357]
</p><p>17 This makes it hard to believe that most papers are false etc. [sent-32, score-0.231]
</p><p>18 I wonder whether this particular issue can be resolved by considering areas of research rather than single papers. [sent-33, score-0.125]
</p><p>19 This can be ok if these scientific subfields are lurching toward the truth in some way. [sent-35, score-0.144]
</p><p>20 I think this could be a useful way forward, to see if it’s possible to reconcile the feeling that science is basically OK with the evidence that individual claims are quite noisy. [sent-36, score-0.298]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('jager', 0.438), ('leek', 0.339), ('keith', 0.266), ('type', 0.187), ('overestimated', 0.141), ('rourke', 0.141), ('ioannidis', 0.129), ('false', 0.121), ('scenario', 0.112), ('believe', 0.11), ('rate', 0.104), ('discovery', 0.103), ('published', 0.101), ('usually', 0.1), ('performed', 0.099), ('noise', 0.097), ('think', 0.095), ('error', 0.092), ('sign', 0.092), ('hausers', 0.088), ('wegmans', 0.088), ('leah', 0.088), ('pottis', 0.088), ('stapels', 0.088), ('underlies', 0.083), ('effects', 0.082), ('true', 0.081), ('many', 0.08), ('amid', 0.079), ('small', 0.078), ('effect', 0.078), ('comparisons', 0.074), ('reconcile', 0.074), ('toward', 0.072), ('subfields', 0.072), ('pushes', 0.072), ('anil', 0.07), ('expanded', 0.068), ('essence', 0.068), ('contingent', 0.068), ('thoughts', 0.068), ('whatsoever', 0.067), ('process', 0.066), ('top', 0.065), ('claims', 0.065), ('single', 0.064), ('pretty', 0.064), ('science', 0.064), ('john', 0.062), ('resolved', 0.061)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="2040-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-26-Difficulties_in_making_inferences_about_scientific_truth_from_distributions_of_published_p-values.html">2040 andrew gelman stats-2013-09-26-Difficulties in making inferences about scientific truth from distributions of published p-values</a></p>
<p>Introduction: Jeff Leek  just posted  the discussions of his paper (with Leah Jager), “An estimate of the science-wise false discovery rate and application to the top medical literature,” along with some further comments of his own.
 
 Here  are my original thoughts on an earlier version of their article.  Keith O’Rourke and I expanded these thoughts into  a formal comment  for the journal.  We’re pretty much in agreement with John Ioannidis (you can find his discussion in the top link above).
 
In quick summary, I agree with Jager and Leek that this is an important topic.  I think there are two key places where Keith and I disagree with them:
 
1.  They take published p-values at face value whereas we consider them as the result of a complicated process of selection.  This is something I didn’t used to think much about, but now I’ve become increasingly convinced that the problems with published p-values is not a simple file-drawer effect or the case of a few p=0.051 values nudged toward p=0.049, bu</p><p>2 0.2302606 <a title="2040-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-06-Against_optimism_about_social_science.html">1844 andrew gelman stats-2013-05-06-Against optimism about social science</a></p>
<p>Introduction: Social science research has been getting pretty bad press recently, what with the Excel buccaneers who didn’t know how to handle data with different numbers of observations per country, and the psychologist who published dozens of papers based on fabricated data, and the Evilicious guy who wouldn’t let people review his data tapes, etc etc.  And that’s not even considering Dr. Anil Potti.
 
On the other hand, the revelation of all these problems can be taken as evidence that things are getting better.  Psychology researcher Gary Marcus  writes :
  
There is something positive that has come out of the crisis of replicability—something vitally important for all experimental sciences. For years, it was extremely difficult to publish a direct replication, or a failure to replicate an experiment, in a good journal. . . . Now, happily, the scientific culture has changed. . . . The Reproducibility Project, from the Center for Open Science is now underway . . .
  
And sociologist Fabio Rojas</p><p>3 0.18992105 <a title="2040-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-07-I%E2%80%99m_negative_on_the_expression_%E2%80%9Cfalse_positives%E2%80%9D.html">2093 andrew gelman stats-2013-11-07-I’m negative on the expression “false positives”</a></p>
<p>Introduction: After seeing a document sent to me and others regarding the  crisis  of spurious, statistically-significant research findings in psychology research, I had the following reaction:
  
I am unhappy with the use in the document of the phrase “false positives.”  I feel that this expression is unhelpful as it frames science in terms of “true” and “false” claims, which I don’t think is particularly accurate.  In particular, in most of the recent disputed Psych Science type studies (the ESP study excepted, perhaps), there is little doubt that there is _some_ underlying effect.  The issue, as I see it, as that the underlying effects are much smaller, and much more variable, than mainstream researchers imagine.  So what happens is that Psych Science or Nature or whatever will publish a result that is purported to be some sort of universal truth, but it is actually a pattern specific to one data set, one population, and one experimental condition.  In a sense, yes, these journals are publishing</p><p>4 0.18857442 <a title="2040-tfidf-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-31-Type_S_error_rates_for_classical_and_Bayesian_single_and_multiple_comparison_procedures.html">494 andrew gelman stats-2010-12-31-Type S error rates for classical and Bayesian single and multiple comparison procedures</a></p>
<p>Introduction: Type S error:  When your estimate is the wrong sign, compared to the true value of the parameter
 
Type M error:  When the magnitude of your estimate is far off, compared to the true value of the parameter 
  
More here.</p><p>5 0.16247542 <a title="2040-tfidf-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-06-Priors_I_don%E2%80%99t_believe.html">2322 andrew gelman stats-2014-05-06-Priors I don’t believe</a></p>
<p>Introduction: Biostatistician Jeff Leek  writes :
  
Think about this headline: “Hospital checklist cut infections, saved lives.” I [Leek] am a pretty skeptical person, so I’m a little surprised that a checklist could really save lives. I say the odds of this being true are 1 in 4.
  
I’m actually surprised that he’s surprised, since over the years I’ve heard about the benefits of checklists in various arenas, including hospital care.  In particular, there was  this article  by Atul Gawande from a few years back.  I mean, sure, I could imagine that checklists might hurt:  after all, it takes some time and effort to put together the checklist and to use it, and perhaps the very existence of the checklist could give hospital staff a false feeling of security, which would ultimately cost lives.  But my first guess would be that people still don’t do enough checklisting, and that the probability is greater than 1/4 that a checklist in a hospital will save lives.
 
Later on, Leek writes:
  
Let’s try ano</p><p>6 0.16036549 <a title="2040-tfidf-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-19-The_replication_and_criticism_movement_is_not_about_suppressing_speculative_research%3B_rather%2C_it%E2%80%99s_all_about_enabling_science%E2%80%99s_fabled_self-correcting_nature.html">2217 andrew gelman stats-2014-02-19-The replication and criticism movement is not about suppressing speculative research; rather, it’s all about enabling science’s fabled self-correcting nature</a></p>
<p>7 0.14810231 <a title="2040-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-22-Top_5_stat_papers_since_2000%3F.html">1951 andrew gelman stats-2013-07-22-Top 5 stat papers since 2000?</a></p>
<p>8 0.14764419 <a title="2040-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>9 0.13662273 <a title="2040-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-08-Statistical_significance_and_the_dangerous_lure_of_certainty.html">1974 andrew gelman stats-2013-08-08-Statistical significance and the dangerous lure of certainty</a></p>
<p>10 0.13364491 <a title="2040-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-31-No_on_Yes-No_decisions.html">2155 andrew gelman stats-2013-12-31-No on Yes-No decisions</a></p>
<p>11 0.12519987 <a title="2040-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-31-How_to_fix_the_tabloids%3F__Toward_replicable_social_science_research.html">1878 andrew gelman stats-2013-05-31-How to fix the tabloids?  Toward replicable social science research</a></p>
<p>12 0.12376262 <a title="2040-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>13 0.12273192 <a title="2040-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-11-One_more_time_on_that_ESP_study%3A__The_problem_of_overestimates_and_the_shrinkage_solution.html">511 andrew gelman stats-2011-01-11-One more time on that ESP study:  The problem of overestimates and the shrinkage solution</a></p>
<p>14 0.11857566 <a title="2040-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-14-Subtleties_with_measurement-error_models_for_the_evaluation_of_wacky_claims.html">803 andrew gelman stats-2011-07-14-Subtleties with measurement-error models for the evaluation of wacky claims</a></p>
<p>15 0.11742273 <a title="2040-tfidf-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-06-How_much_time_%28if_any%29_should_we_spend_criticizing_research_that%E2%80%99s_fraudulent%2C_crappy%2C_or_just_plain_pointless%3F.html">2235 andrew gelman stats-2014-03-06-How much time (if any) should we spend criticizing research that’s fraudulent, crappy, or just plain pointless?</a></p>
<p>16 0.11536593 <a title="2040-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-01-Why_big_effects_are_more_important_than_small_effects.html">1744 andrew gelman stats-2013-03-01-Why big effects are more important than small effects</a></p>
<p>17 0.11513557 <a title="2040-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-06-That_silly_ESP_paper_and_some_silliness_in_a_rebuttal_as_well.html">506 andrew gelman stats-2011-01-06-That silly ESP paper and some silliness in a rebuttal as well</a></p>
<p>18 0.11375172 <a title="2040-tfidf-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-12-More_on_publishing_in_journals.html">2245 andrew gelman stats-2014-03-12-More on publishing in journals</a></p>
<p>19 0.11364643 <a title="2040-tfidf-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-07-Selection_bias_in_the_reporting_of_shaky_research.html">2236 andrew gelman stats-2014-03-07-Selection bias in the reporting of shaky research</a></p>
<p>20 0.11342535 <a title="2040-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.243), (1, 0.001), (2, 0.01), (3, -0.149), (4, -0.027), (5, -0.083), (6, 0.016), (7, -0.016), (8, 0.016), (9, -0.038), (10, -0.034), (11, 0.026), (12, -0.006), (13, -0.045), (14, 0.004), (15, -0.002), (16, -0.035), (17, 0.011), (18, -0.036), (19, 0.023), (20, -0.011), (21, -0.013), (22, -0.013), (23, -0.022), (24, -0.071), (25, -0.002), (26, -0.009), (27, 0.025), (28, -0.022), (29, -0.031), (30, 0.009), (31, 0.024), (32, -0.016), (33, -0.029), (34, 0.032), (35, -0.017), (36, -0.019), (37, -0.024), (38, -0.005), (39, -0.034), (40, -0.015), (41, 0.002), (42, -0.051), (43, 0.023), (44, -0.01), (45, 0.013), (46, -0.026), (47, 0.056), (48, -0.021), (49, 0.035)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97646779 <a title="2040-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-26-Difficulties_in_making_inferences_about_scientific_truth_from_distributions_of_published_p-values.html">2040 andrew gelman stats-2013-09-26-Difficulties in making inferences about scientific truth from distributions of published p-values</a></p>
<p>Introduction: Jeff Leek  just posted  the discussions of his paper (with Leah Jager), “An estimate of the science-wise false discovery rate and application to the top medical literature,” along with some further comments of his own.
 
 Here  are my original thoughts on an earlier version of their article.  Keith O’Rourke and I expanded these thoughts into  a formal comment  for the journal.  We’re pretty much in agreement with John Ioannidis (you can find his discussion in the top link above).
 
In quick summary, I agree with Jager and Leek that this is an important topic.  I think there are two key places where Keith and I disagree with them:
 
1.  They take published p-values at face value whereas we consider them as the result of a complicated process of selection.  This is something I didn’t used to think much about, but now I’ve become increasingly convinced that the problems with published p-values is not a simple file-drawer effect or the case of a few p=0.051 values nudged toward p=0.049, bu</p><p>2 0.87640303 <a title="2040-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-07-I%E2%80%99m_negative_on_the_expression_%E2%80%9Cfalse_positives%E2%80%9D.html">2093 andrew gelman stats-2013-11-07-I’m negative on the expression “false positives”</a></p>
<p>Introduction: After seeing a document sent to me and others regarding the  crisis  of spurious, statistically-significant research findings in psychology research, I had the following reaction:
  
I am unhappy with the use in the document of the phrase “false positives.”  I feel that this expression is unhelpful as it frames science in terms of “true” and “false” claims, which I don’t think is particularly accurate.  In particular, in most of the recent disputed Psych Science type studies (the ESP study excepted, perhaps), there is little doubt that there is _some_ underlying effect.  The issue, as I see it, as that the underlying effects are much smaller, and much more variable, than mainstream researchers imagine.  So what happens is that Psych Science or Nature or whatever will publish a result that is purported to be some sort of universal truth, but it is actually a pattern specific to one data set, one population, and one experimental condition.  In a sense, yes, these journals are publishing</p><p>3 0.85927945 <a title="2040-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-16-%E2%80%9CFalse-positive_psychology%E2%80%9D.html">1171 andrew gelman stats-2012-02-16-“False-positive psychology”</a></p>
<p>Introduction: Everybody’s  talkin bout  this paper by Joseph Simmons, Leif Nelson and Uri Simonsohn, who  write :
  
Despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We [Simmons, Nelson, and Simonsohn] present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.
  
Whatever you think about these recommend</p><p>4 0.85859787 <a title="2040-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>Introduction: This article  is a discussion of a  paper  by Greg Francis for a special issue, edited by E. J. Wagenmakers, of the Journal of Mathematical Psychology.  Here’s what I wrote:
  
Much of statistical practice is an effort to reduce or deny variation and uncertainty. The reduction is done through standardization, replication, and other practices of experimental design, with the idea being to isolate and stabilize the quantity being estimated and then average over many cases. Even so, however, uncertainty persists, and statistical hypothesis testing is in many ways an endeavor to deny this, by reporting binary accept/reject decisions.


Classical statistical methods produce binary statements, but there is no reason to assume that the world works that way. Expressions such as Type 1 error, Type 2 error, false positive, and so on, are based on a model in which the world is divided into real and non-real effects. To put it another way, I understand the general scientific distinction of real vs</p><p>5 0.85812414 <a title="2040-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-11-One_more_time_on_that_ESP_study%3A__The_problem_of_overestimates_and_the_shrinkage_solution.html">511 andrew gelman stats-2011-01-11-One more time on that ESP study:  The problem of overestimates and the shrinkage solution</a></p>
<p>Introduction: Benedict Carey  writes  a follow-up article on ESP studies and Bayesian statistics.  ( See here  for my previous thoughts on the topic.)  Everything Carey writes is fine, and he even uses an example I recommended:
  
The statistical approach that has dominated the social sciences for almost a century is called significance testing. The idea is straightforward. A finding from any well-designed study — say, a correlation between a personality trait and the risk of depression — is considered “significant” if its probability of occurring by chance is less than 5 percent.


This arbitrary cutoff makes sense when the effect being studied is a large one — for example, when measuring the so-called Stroop effect. This effect predicts that naming the color of a word is faster and more accurate when the word and color match (“red” in red letters) than when they do not (“red” in blue letters), and is very strong in almost everyone.


“But if the true effect of what you are measuring is small,” sai</p><p>6 0.85541034 <a title="2040-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-With_a_bit_of_precognition%2C_you%E2%80%99d_have_known_I_was_going_to_post_again_on_this_topic%2C_and_with_a_lot_of_precognition%2C_you%E2%80%99d_have_known_I_was_going_to_post_today.html">576 andrew gelman stats-2011-02-15-With a bit of precognition, you’d have known I was going to post again on this topic, and with a lot of precognition, you’d have known I was going to post today</a></p>
<p>7 0.84277534 <a title="2040-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-26-Is_it_plausible_that_1%25_of_people_pick_a_career_based_on_their_first_name%3F.html">629 andrew gelman stats-2011-03-26-Is it plausible that 1% of people pick a career based on their first name?</a></p>
<p>8 0.83985394 <a title="2040-lsi-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-24-%E2%80%9CEdlin%E2%80%99s_rule%E2%80%9D_for_routinely_scaling_down_published_estimates.html">2223 andrew gelman stats-2014-02-24-“Edlin’s rule” for routinely scaling down published estimates</a></p>
<p>9 0.83215827 <a title="2040-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-12-Misunderstanding_the_p-value.html">1760 andrew gelman stats-2013-03-12-Misunderstanding the p-value</a></p>
<p>10 0.82074851 <a title="2040-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-11-How_do_we_evaluate_a_new_and_wacky_claim%3F.html">797 andrew gelman stats-2011-07-11-How do we evaluate a new and wacky claim?</a></p>
<p>11 0.81317598 <a title="2040-lsi-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-10-Preregistration%3A_what%E2%80%99s_in_it_for_you%3F.html">2241 andrew gelman stats-2014-03-10-Preregistration: what’s in it for you?</a></p>
<p>12 0.81076097 <a title="2040-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-28-Difficulties_of_using_statistical_significance_%28or_lack_thereof%29_to_sift_through_and_compare_research_hypotheses.html">2042 andrew gelman stats-2013-09-28-Difficulties of using statistical significance (or lack thereof) to sift through and compare research hypotheses</a></p>
<p>13 0.81016469 <a title="2040-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-18-%E2%80%9CIf_scientists_wrote_horoscopes%2C_this_is_what_yours_would_say%E2%80%9D.html">1680 andrew gelman stats-2013-01-18-“If scientists wrote horoscopes, this is what yours would say”</a></p>
<p>14 0.80168581 <a title="2040-lsi-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-07-Selection_bias_in_the_reporting_of_shaky_research.html">2236 andrew gelman stats-2014-03-07-Selection bias in the reporting of shaky research</a></p>
<p>15 0.79960769 <a title="2040-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-29-Decline_Effect_in_Linguistics%3F.html">1400 andrew gelman stats-2012-06-29-Decline Effect in Linguistics?</a></p>
<p>16 0.79887313 <a title="2040-lsi-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-31-Jessica_Tracy_and_Alec_Beall_%28authors_of_the_fertile-women-wear-pink_study%29_comment_on_our_Garden_of_Forking_Paths_paper%2C_and_I_comment_on_their_comments.html">2355 andrew gelman stats-2014-05-31-Jessica Tracy and Alec Beall (authors of the fertile-women-wear-pink study) comment on our Garden of Forking Paths paper, and I comment on their comments</a></p>
<p>17 0.7973991 <a title="2040-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-Fourteen_magic_words%3A_an_update.html">898 andrew gelman stats-2011-09-10-Fourteen magic words: an update</a></p>
<p>18 0.79408926 <a title="2040-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-06-That_silly_ESP_paper_and_some_silliness_in_a_rebuttal_as_well.html">506 andrew gelman stats-2011-01-06-That silly ESP paper and some silliness in a rebuttal as well</a></p>
<p>19 0.79216123 <a title="2040-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-01-Why_big_effects_are_more_important_than_small_effects.html">1744 andrew gelman stats-2013-03-01-Why big effects are more important than small effects</a></p>
<p>20 0.78940427 <a title="2040-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-14-Subtleties_with_measurement-error_models_for_the_evaluation_of_wacky_claims.html">803 andrew gelman stats-2011-07-14-Subtleties with measurement-error models for the evaluation of wacky claims</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.011), (15, 0.043), (16, 0.077), (24, 0.19), (27, 0.023), (42, 0.096), (45, 0.021), (53, 0.024), (63, 0.032), (70, 0.042), (73, 0.014), (86, 0.08), (99, 0.27)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9669596 <a title="2040-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-26-Difficulties_in_making_inferences_about_scientific_truth_from_distributions_of_published_p-values.html">2040 andrew gelman stats-2013-09-26-Difficulties in making inferences about scientific truth from distributions of published p-values</a></p>
<p>Introduction: Jeff Leek  just posted  the discussions of his paper (with Leah Jager), “An estimate of the science-wise false discovery rate and application to the top medical literature,” along with some further comments of his own.
 
 Here  are my original thoughts on an earlier version of their article.  Keith O’Rourke and I expanded these thoughts into  a formal comment  for the journal.  We’re pretty much in agreement with John Ioannidis (you can find his discussion in the top link above).
 
In quick summary, I agree with Jager and Leek that this is an important topic.  I think there are two key places where Keith and I disagree with them:
 
1.  They take published p-values at face value whereas we consider them as the result of a complicated process of selection.  This is something I didn’t used to think much about, but now I’ve become increasingly convinced that the problems with published p-values is not a simple file-drawer effect or the case of a few p=0.051 values nudged toward p=0.049, bu</p><p>2 0.943187 <a title="2040-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-16-Bayesian_analogue_to_stepwise_regression%3F.html">1535 andrew gelman stats-2012-10-16-Bayesian analogue to stepwise regression?</a></p>
<p>Introduction: Bill Harris writes:
  
On pp. 250-251 of BDA second edition, you write about multiple comparisons, and you write about stepwise regression on p. 405.  How would you look at stepwise regression analyses in light of the multiple comparisons problem?  Is there an issue?
  
My reply:
 
In this case I think the right approach is to keep all the coefs but partially pool them toward 0 (after suitable transformation).  But then the challenge is coming up with a general way to construct good prior distributions.  Iâ&euro;&trade;m still thinking about that one!  Yet another approach is to put something together purely nonparametrically as with Bart.</p><p>3 0.94267011 <a title="2040-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-The_statistical_significance_filter.html">899 andrew gelman stats-2011-09-10-The statistical significance filter</a></p>
<p>Introduction: I’ve talked about this a bit but it’s never had its own blog entry (until now).
 
Statistically significant findings tend to overestimate the magnitude of effects.  This holds in general (because E(|x|) > |E(x)|) but even more so if you restrict to statistically significant results.
 
Here’s an example.  Suppose a true effect of theta is unbiasedly estimated by y ~ N (theta, 1).  Further suppose that we will only consider statistically significant results, that is, cases in which |y| > 2.
 
The estimate “|y| conditional on |y|>2″ is clearly an overestimate of |theta|.  First off, if |theta|<2, the estimate |y| conditional on statistical significance is not only too high in expectation, it's  always  too high.  This is a problem, given that |theta| is in reality probably is less than 2.  (The low-hangning fruit have already been picked, remember?)
 
But even if |theta|>2, the estimate |y| conditional on statistical significance will still be too high in expectation.
 
For a discussion o</p><p>4 0.94195193 <a title="2040-lda-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-30-That_puzzle-solving_feeling.html">492 andrew gelman stats-2010-12-30-That puzzle-solving feeling</a></p>
<p>Introduction: Since  this blog  in November, I’ve given my talk on infovis vs. statistical graphics about five times:  once in person (at the visualization meetup in NYC, a blog away from Num Pang!) and the rest via telephone conferencing or skype. The live presentation was best, but the  remote talks  have been improving, and I’m looking forward to doing more of these in the future to save time and reduce pollution.
 
 Here are the powerpoints of the talk. 
 
Now that I’ve got it working well (mostly by cutting lots of words on the slides), my next step will be to improve the interactive experience.  At the very least, I need to allocate time after the talk for discussion.  People usually don’t ask a lot of questions when I speak, so maybe the best strategy is to allow a half hour following the talk for people to speak with me individually.  It could be set up so that I’m talking with one person but the others who are hanging out could hear the conversation too.
 
Anyway, one of the times I gave th</p><p>5 0.94140297 <a title="2040-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-06-Josh_Tenenbaum_presents_._._._a_model_of_folk_physics%21.html">994 andrew gelman stats-2011-11-06-Josh Tenenbaum presents . . . a model of folk physics!</a></p>
<p>Introduction: Josh Tenenbaum describes some new work modeling people’s physical reasoning as probabilistic inferences over intuitive theories of mechanics. 
  
A general-purpose capacity for “physical intelligence”—inferring physical properties of objects and predicting future states in complex dynamical scenes—is central to how humans interpret their environment and plan safe and effective actions.  The computations and representations underlying physical intelligence remain unclear, however. Cognitive studies have focused on mapping out judgment biases and errors, or on testing simple heuristic models suitable only for highly specific cases; they have not attempted to give general-purpose unifying models.  In computer science, artificial intelligence and robotics researchers have long sought to formalize common-sense physical reasoning but without success in approaching human-level competence.  Here we show that a wide range of human physical judgments can be explained by positing an “intuitive me</p><p>6 0.93979287 <a title="2040-lda-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-07-Cause_he_thinks_he%E2%80%99s_so-phisticated.html">2323 andrew gelman stats-2014-05-07-Cause he thinks he’s so-phisticated</a></p>
<p>7 0.93965864 <a title="2040-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-07-A_compelling_reason_to_go_to_London%2C_Ontario%3F%3F.html">1104 andrew gelman stats-2012-01-07-A compelling reason to go to London, Ontario??</a></p>
<p>8 0.93922722 <a title="2040-lda-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-31-Type_S_error_rates_for_classical_and_Bayesian_single_and_multiple_comparison_procedures.html">494 andrew gelman stats-2010-12-31-Type S error rates for classical and Bayesian single and multiple comparison procedures</a></p>
<p>9 0.93782604 <a title="2040-lda-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-20-The_AAA_Tranche_of_Subprime_Science.html">2179 andrew gelman stats-2014-01-20-The AAA Tranche of Subprime Science</a></p>
<p>10 0.93592709 <a title="2040-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-18-The_estimated_effect_size_is_implausibly_large.__Under_what_models_is_this_a_piece_of_evidence_that_the_true_effect_is_small%3F.html">808 andrew gelman stats-2011-07-18-The estimated effect size is implausibly large.  Under what models is this a piece of evidence that the true effect is small?</a></p>
<p>11 0.9358722 <a title="2040-lda-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-29-Ya_don%E2%80%99t_know_Bayes%2C_Jack.html">117 andrew gelman stats-2010-06-29-Ya don’t know Bayes, Jack</a></p>
<p>12 0.9358204 <a title="2040-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-23-Combining_survey_data_obtained_using_different_modes_of_sampling.html">777 andrew gelman stats-2011-06-23-Combining survey data obtained using different modes of sampling</a></p>
<p>13 0.93440264 <a title="2040-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-20-A_kaleidoscope_of_responses_to_Dubner%E2%80%99s_criticisms_of_our_criticisms_of_Freaknomics.html">1223 andrew gelman stats-2012-03-20-A kaleidoscope of responses to Dubner’s criticisms of our criticisms of Freaknomics</a></p>
<p>14 0.93390572 <a title="2040-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-05-Question_26_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1367 andrew gelman stats-2012-06-05-Question 26 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>15 0.93336433 <a title="2040-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-29-How_to_grab_power_in_a_democracy_%E2%80%93_in_5_easy_non-violent_steps.html">116 andrew gelman stats-2010-06-29-How to grab power in a democracy – in 5 easy non-violent steps</a></p>
<p>16 0.93314898 <a title="2040-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-10-The_ethics_of_lying%2C_cheating%2C_and_stealing_with_data%3A__A_case_study.html">2015 andrew gelman stats-2013-09-10-The ethics of lying, cheating, and stealing with data:  A case study</a></p>
<p>17 0.93208295 <a title="2040-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>18 0.93206519 <a title="2040-lda-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-25-Basketball_Stats%3A__Don%E2%80%99t_model_the_probability_of_win%2C_model_the_expected_score_differential..html">2224 andrew gelman stats-2014-02-25-Basketball Stats:  Don’t model the probability of win, model the expected score differential.</a></p>
<p>19 0.93146396 <a title="2040-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>20 0.93144917 <a title="2040-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-03-Boot.html">1881 andrew gelman stats-2013-06-03-Boot</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
