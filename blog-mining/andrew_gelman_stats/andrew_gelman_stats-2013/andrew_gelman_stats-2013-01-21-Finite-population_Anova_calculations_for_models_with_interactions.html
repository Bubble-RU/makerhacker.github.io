<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1686 andrew gelman stats-2013-01-21-Finite-population Anova calculations for models with interactions</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-1686" href="#">andrew_gelman_stats-2013-1686</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1686 andrew gelman stats-2013-01-21-Finite-population Anova calculations for models with interactions</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-1686-html" href="http://andrewgelman.com/2013/01/21/finite-population-anova-calculations-for-models-with-interactions/">html</a></p><p>Introduction: Jim Thomson writes:
  
I wonder if you could provide some clarification on the correct way to calculate the finite-population standard deviations for interaction terms in your Bayesian approach to ANOVA (as explained in your 2005 paper, and Gelman and Hill 2007).


I understand that it is the SD of the constrained batch coefficients that is of interest, but in most WinBUGS examples I have seen, the SDs are all calculated directly as sd.fin<-sd(beta.main[]) for main effects and sd(beta.int[,]) for interaction effects,  where beta.main and beta.int are the unconstrained coefficients, e.g. beta.int[i,j]~dnorm(0,tau).


For main effects, I can see that it makes no difference, since the constrained value is calculated by subtracting the mean, and sd(B[]) = sd(B[]-mean(B[])).


But the conventional sum-to-zero constraint for interaction terms in linear models is more complicated than subtracting the mean (there are only (n1-1)*(n2-1) free coefficients for an interaction b/w factors with n1 a</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Jim Thomson writes:    I wonder if you could provide some clarification on the correct way to calculate the finite-population standard deviations for interaction terms in your Bayesian approach to ANOVA (as explained in your 2005 paper, and Gelman and Hill 2007). [sent-1, score-0.675]
</p><p>2 I understand that it is the SD of the constrained batch coefficients that is of interest, but in most WinBUGS examples I have seen, the SDs are all calculated directly as sd. [sent-2, score-1.085]
</p><p>3 For main effects, I can see that it makes no difference, since the constrained value is calculated by subtracting the mean, and sd(B[]) = sd(B[]-mean(B[])). [sent-11, score-0.798]
</p><p>4 But the conventional sum-to-zero constraint for interaction terms in linear models is more complicated than subtracting the mean (there are only (n1-1)*(n2-1) free coefficients for an interaction b/w factors with n1 and n2 levels). [sent-12, score-1.4]
</p><p>5 When I estimate a model with constrained coefficients directly the SD of those constrained coefficients is different to the SD of unconstrained coefficients for the same model, and I am assuming the later are incorrect? [sent-13, score-2.112]
</p><p>6 Can you confirm that the finite pop SD must be calculated from the constrained coefficients, and that sd(beta. [sent-14, score-0.814]
</p><p>7 int[,]) were valid, because using constrained coefficients slows WinBUGS down considerably, but I can’t see that it is (unless I am misunderstanding the constraints). [sent-17, score-0.893]
</p><p>8 My reply:   Yes, I find it more complicated with interactions because then I do in fact subtract row means and column means (ab[i,j] – a[i] – b[j] + mu). [sent-18, score-0.451]
</p><p>9 It’s do-able but I feel a little uncomfortable doing all this post-processing of inferences. [sent-19, score-0.061]
</p><p>10 I have this feeling that if I really were doing things right, everything would just pop out of the model. [sent-20, score-0.172]
</p><p>11 The post-processing feels too much like a duplication. [sent-21, score-0.052]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sd', 0.564), ('constrained', 0.376), ('coefficients', 0.368), ('interaction', 0.253), ('calculated', 0.193), ('subtracting', 0.158), ('winbugs', 0.154), ('unconstrained', 0.143), ('pop', 0.128), ('terms', 0.107), ('complicated', 0.091), ('thomson', 0.091), ('sds', 0.091), ('slows', 0.086), ('effects', 0.085), ('duplication', 0.082), ('considerably', 0.082), ('batch', 0.079), ('dnorm', 0.077), ('mu', 0.077), ('clarification', 0.077), ('subtract', 0.073), ('correct', 0.072), ('main', 0.071), ('constraint', 0.07), ('directly', 0.069), ('means', 0.065), ('anova', 0.065), ('row', 0.064), ('misunderstanding', 0.063), ('deviations', 0.063), ('confirm', 0.061), ('uncomfortable', 0.061), ('jim', 0.059), ('calculate', 0.058), ('incorrect', 0.058), ('hill', 0.056), ('finite', 0.056), ('constraints', 0.055), ('mean', 0.052), ('feels', 0.052), ('valid', 0.049), ('conventional', 0.048), ('interactions', 0.047), ('column', 0.046), ('explained', 0.045), ('feeling', 0.044), ('assuming', 0.044), ('levels', 0.044), ('unless', 0.044)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="1686-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-21-Finite-population_Anova_calculations_for_models_with_interactions.html">1686 andrew gelman stats-2013-01-21-Finite-population Anova calculations for models with interactions</a></p>
<p>Introduction: Jim Thomson writes:
  
I wonder if you could provide some clarification on the correct way to calculate the finite-population standard deviations for interaction terms in your Bayesian approach to ANOVA (as explained in your 2005 paper, and Gelman and Hill 2007).


I understand that it is the SD of the constrained batch coefficients that is of interest, but in most WinBUGS examples I have seen, the SDs are all calculated directly as sd.fin<-sd(beta.main[]) for main effects and sd(beta.int[,]) for interaction effects,  where beta.main and beta.int are the unconstrained coefficients, e.g. beta.int[i,j]~dnorm(0,tau).


For main effects, I can see that it makes no difference, since the constrained value is calculated by subtracting the mean, and sd(B[]) = sd(B[]-mean(B[])).


But the conventional sum-to-zero constraint for interaction terms in linear models is more complicated than subtracting the mean (there are only (n1-1)*(n2-1) free coefficients for an interaction b/w factors with n1 a</p><p>2 0.27936298 <a title="1686-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-07-An_%28impressive%29_increase_in_survival_rate_from_50%25_to_60%25_corresponds_to_an_R-squared_of_%28only%29_1%25.__Counterintuitive%2C_huh%3F.html">1524 andrew gelman stats-2012-10-07-An (impressive) increase in survival rate from 50% to 60% corresponds to an R-squared of (only) 1%.  Counterintuitive, huh?</a></p>
<p>Introduction: I was just reading  an old post  and came across this example which I’d like to share with you again:
  
Here’s a story of R-squared = 1%. Consider a 0/1 outcome with about half the people in each category. For.example, half the people with some disease die in a year and half live. Now suppose there’s a treatment that increases survival rate from 50% to 60%. The unexplained sd is 0.5 and the explained sd is 0.05, hence R-squared is 0.01.</p><p>3 0.1987163 <a title="1686-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-21-Avoiding_boundary_estimates_in_linear_mixed_models.html">918 andrew gelman stats-2011-09-21-Avoiding boundary estimates in linear mixed models</a></p>
<p>Introduction: Pablo Verde sends in  this letter  he and Daniel Curcio just published in the Journal of Antimicrobial Chemotherapy. They had published a meta-analysis with a boundary estimate which, he said, gave nonsense results.  Here’s Curcio and Verde’s key paragraph:
  
The authors [of the study they are criticizing] performed a test of heterogeneity between studies. Given that the test result was not significant at 5%, they decided to pool all the RRs by using a fixed-effect meta-analysis model. Unfortunately, this is a common practice in meta-analysis, which usually leads to very misleading results. First of all, the pooled RR as well as its standard	error are sensitive to 2 the estimation of the between-studies standard deviation (SD).	SD is difficult to estimate with a small number of studies. On the other hand, it is very well known that the significant test of hetero- geneity lacks statistical power to detect values of SD greater than zero. In addition, the statistically non-significant re</p><p>4 0.17525181 <a title="1686-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-18-Standardizing_regression_inputs.html">1462 andrew gelman stats-2012-08-18-Standardizing regression inputs</a></p>
<p>Introduction: Andy Flies, Ph.D. candidate in zoology, writes:
  
After reading your paper about scaling regression inputs by two standard deviations I found your  blog post  stating that you wished you had scaled by 1 sd and coded the binary inputs as -1 and 1.  Here is my question:


If you code the binary input as -1 and 1, do you then standardize it?  This makes sense to me because the mean of the standardized input is then zero and the sd is 1, which is what the mean and sd are for all of the other standardized inputs.  I know that if you code the binary input as 0 and 1 it should not be standardized.


Also, I am not interested in the actual units (i.e. mg/ml) of my response variable and I would like to compare a couple of different response variables that are on different scales.  Would it make sense to standardize the response variable also?
  
My reply:  No, I donâ&euro;&trade;t standardize the binary input.  The point of standardizing inputs is to make the coefs directly interpretable, but with binary i</p><p>5 0.16455337 <a title="1686-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-24-Estimating_and_summarizing_inference_for_hierarchical_variance_parameters_when_the_number_of_groups_is_small.html">2145 andrew gelman stats-2013-12-24-Estimating and summarizing inference for hierarchical variance parameters when the number of groups is small</a></p>
<p>Introduction: Chris Che-Castaldo writes:
  
I am trying to compute variance components for a hierarchical model where the group level has two binary predictors and their interaction. When I model each of these three predictors as N(0, tau) the model will not converge, perhaps because the number of coefficients in each batch is so small (2 for the main effects and 4 for the interaction). Although I could simply leave all these as predictors as unmodeled fixed effects, the last sentence of section 21.2 on page 462 of Gelman and Hill (2007) suggests this would not be a wise course of action:

 
For example, it is not clear how to define the (finite) standard deviation of variables that are included in interactions.
 

I am curious – is there still no clear cut way to directly compute the finite standard deviation for binary unmodeled variables that are also part of an interaction as well as the interaction itself?
  
My reply:  I’d recommend including these in your model (it’s probably easiest to do so</p><p>6 0.14882237 <a title="1686-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>7 0.14616747 <a title="1686-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-03-Hierarchical_array_priors_for_ANOVA_decompositions.html">1786 andrew gelman stats-2013-04-03-Hierarchical array priors for ANOVA decompositions</a></p>
<p>8 0.13966249 <a title="1686-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-26-Including_interactions_or_not.html">823 andrew gelman stats-2011-07-26-Including interactions or not</a></p>
<p>9 0.13421635 <a title="1686-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Interpreting_interactions_in_discrete-data_regression.html">1908 andrew gelman stats-2013-06-21-Interpreting interactions in discrete-data regression</a></p>
<p>10 0.13335559 <a title="1686-tfidf-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-05-Update_on_state_size_and_governors%E2%80%99_popularity.html">187 andrew gelman stats-2010-08-05-Update on state size and governors’ popularity</a></p>
<p>11 0.12623958 <a title="1686-tfidf-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-14-The_maximal_information_coefficient.html">2247 andrew gelman stats-2014-03-14-The maximal information coefficient</a></p>
<p>12 0.10996865 <a title="1686-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Allowing_interaction_terms_to_vary.html">753 andrew gelman stats-2011-06-09-Allowing interaction terms to vary</a></p>
<p>13 0.10249317 <a title="1686-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-17-Jumping_off_the_edge_of_the_world.html">858 andrew gelman stats-2011-08-17-Jumping off the edge of the world</a></p>
<p>14 0.095361009 <a title="1686-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-21-D._Buggin.html">1465 andrew gelman stats-2012-08-21-D. Buggin</a></p>
<p>15 0.095331259 <a title="1686-tfidf-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-31-Somewhat_Bayesian_multilevel_modeling.html">246 andrew gelman stats-2010-08-31-Somewhat Bayesian multilevel modeling</a></p>
<p>16 0.091093093 <a title="1686-tfidf-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>17 0.086519077 <a title="1686-tfidf-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-13-Update_on_marathon_statistics.html">273 andrew gelman stats-2010-09-13-Update on marathon statistics</a></p>
<p>18 0.085670635 <a title="1686-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>19 0.076351658 <a title="1686-tfidf-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-30-Adjudicating_between_alternative_interpretations_of_a_statistical_interaction%3F.html">2274 andrew gelman stats-2014-03-30-Adjudicating between alternative interpretations of a statistical interaction?</a></p>
<p>20 0.075996712 <a title="1686-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-02-Covariate_Adjustment_in_RCT_-_Model_Overfitting_in_Multilevel_Regression.html">936 andrew gelman stats-2011-10-02-Covariate Adjustment in RCT - Model Overfitting in Multilevel Regression</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.091), (1, 0.07), (2, 0.042), (3, -0.025), (4, 0.041), (5, -0.007), (6, 0.033), (7, -0.031), (8, 0.027), (9, 0.057), (10, -0.003), (11, 0.027), (12, 0.01), (13, -0.037), (14, 0.034), (15, 0.031), (16, -0.03), (17, 0.018), (18, -0.004), (19, 0.016), (20, 0.015), (21, 0.022), (22, -0.002), (23, -0.033), (24, 0.001), (25, -0.001), (26, -0.042), (27, 0.023), (28, -0.019), (29, -0.041), (30, 0.031), (31, 0.054), (32, -0.011), (33, -0.001), (34, 0.041), (35, -0.068), (36, 0.011), (37, -0.012), (38, 0.016), (39, -0.028), (40, -0.019), (41, -0.061), (42, 0.026), (43, 0.012), (44, -0.013), (45, -0.002), (46, 0.023), (47, -0.019), (48, 0.014), (49, 0.019)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95785391 <a title="1686-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-21-Finite-population_Anova_calculations_for_models_with_interactions.html">1686 andrew gelman stats-2013-01-21-Finite-population Anova calculations for models with interactions</a></p>
<p>Introduction: Jim Thomson writes:
  
I wonder if you could provide some clarification on the correct way to calculate the finite-population standard deviations for interaction terms in your Bayesian approach to ANOVA (as explained in your 2005 paper, and Gelman and Hill 2007).


I understand that it is the SD of the constrained batch coefficients that is of interest, but in most WinBUGS examples I have seen, the SDs are all calculated directly as sd.fin<-sd(beta.main[]) for main effects and sd(beta.int[,]) for interaction effects,  where beta.main and beta.int are the unconstrained coefficients, e.g. beta.int[i,j]~dnorm(0,tau).


For main effects, I can see that it makes no difference, since the constrained value is calculated by subtracting the mean, and sd(B[]) = sd(B[]-mean(B[])).


But the conventional sum-to-zero constraint for interaction terms in linear models is more complicated than subtracting the mean (there are only (n1-1)*(n2-1) free coefficients for an interaction b/w factors with n1 a</p><p>2 0.75059712 <a title="1686-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-24-Estimating_and_summarizing_inference_for_hierarchical_variance_parameters_when_the_number_of_groups_is_small.html">2145 andrew gelman stats-2013-12-24-Estimating and summarizing inference for hierarchical variance parameters when the number of groups is small</a></p>
<p>Introduction: Chris Che-Castaldo writes:
  
I am trying to compute variance components for a hierarchical model where the group level has two binary predictors and their interaction. When I model each of these three predictors as N(0, tau) the model will not converge, perhaps because the number of coefficients in each batch is so small (2 for the main effects and 4 for the interaction). Although I could simply leave all these as predictors as unmodeled fixed effects, the last sentence of section 21.2 on page 462 of Gelman and Hill (2007) suggests this would not be a wise course of action:

 
For example, it is not clear how to define the (finite) standard deviation of variables that are included in interactions.
 

I am curious – is there still no clear cut way to directly compute the finite standard deviation for binary unmodeled variables that are also part of an interaction as well as the interaction itself?
  
My reply:  I’d recommend including these in your model (it’s probably easiest to do so</p><p>3 0.73958427 <a title="1686-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-18-Standardizing_regression_inputs.html">1462 andrew gelman stats-2012-08-18-Standardizing regression inputs</a></p>
<p>Introduction: Andy Flies, Ph.D. candidate in zoology, writes:
  
After reading your paper about scaling regression inputs by two standard deviations I found your  blog post  stating that you wished you had scaled by 1 sd and coded the binary inputs as -1 and 1.  Here is my question:


If you code the binary input as -1 and 1, do you then standardize it?  This makes sense to me because the mean of the standardized input is then zero and the sd is 1, which is what the mean and sd are for all of the other standardized inputs.  I know that if you code the binary input as 0 and 1 it should not be standardized.


Also, I am not interested in the actual units (i.e. mg/ml) of my response variable and I would like to compare a couple of different response variables that are on different scales.  Would it make sense to standardize the response variable also?
  
My reply:  No, I donâ&euro;&trade;t standardize the binary input.  The point of standardizing inputs is to make the coefs directly interpretable, but with binary i</p><p>4 0.72641873 <a title="1686-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Allowing_interaction_terms_to_vary.html">753 andrew gelman stats-2011-06-09-Allowing interaction terms to vary</a></p>
<p>Introduction: Zoltan Fazekas writes:
  
I am a 2nd year graduate student in political science at the University of Vienna. In my empirical research I often employ multilevel modeling, and recently I came across a situation that kept me wondering for quite a while. As I did not find much on this in the literature and considering the topics that you work on and blog about, I figured I will try to contact you.
      
The situation is as follows: in a linear multilevel model, there are two important individual level predictors (x1 and x2) and a set of controls. Let us assume that there is a theoretically grounded argument suggesting that an interaction between x1 and x2 should be included in the model (x1 * x2). Both x1 and x2 are let to vary randomly across groups. Would this directly imply that the coefficient of the interaction should also be left to vary across country? This is even more burning if there is no specific hypothesis on the variance of the conditional effect across countries. And then i</p><p>5 0.70458704 <a title="1686-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>Introduction: A research psychologist writes in with a question that’s so long that I’ll put my answer first, then put the question itself below the fold.
 
Here’s my reply:
 
As I wrote in my Anova paper and in my book with Jennifer Hill, I do think that multilevel models can completely replace Anova.  At the same time, I think the central idea of Anova should persist in our understanding of these models.  To me the central idea of Anova is not F-tests or p-values or sums of squares, but rather the idea of predicting an outcome based on factors with discrete levels, and understanding these factors using variance components.
 
The continuous or categorical response thing doesn’t really matter so much to me.  I have no problem using a normal linear model for continuous outcomes (perhaps suitably transformed) and a logistic model for binary outcomes.
 
I don’t want to throw away interactions just because they’re not statistically significant.  I’d rather partially pool them toward zero using an inform</p><p>6 0.68786883 <a title="1686-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-03-Hierarchical_array_priors_for_ANOVA_decompositions.html">1786 andrew gelman stats-2013-04-03-Hierarchical array priors for ANOVA decompositions</a></p>
<p>7 0.68402869 <a title="1686-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-03-Uncertainty_in_parameter_estimates_using_multilevel_models.html">1966 andrew gelman stats-2013-08-03-Uncertainty in parameter estimates using multilevel models</a></p>
<p>8 0.68162912 <a title="1686-lsi-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Question_about_standard_range_for_social_science_correlations.html">257 andrew gelman stats-2010-09-04-Question about standard range for social science correlations</a></p>
<p>9 0.67459071 <a title="1686-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Interpreting_interactions_in_discrete-data_regression.html">1908 andrew gelman stats-2013-06-21-Interpreting interactions in discrete-data regression</a></p>
<p>10 0.65698189 <a title="1686-lsi-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-12-Finite-population_standard_deviation_in_a_hierarchical_model.html">464 andrew gelman stats-2010-12-12-Finite-population standard deviation in a hierarchical model</a></p>
<p>11 0.65547514 <a title="1686-lsi-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-Correlation%2C_prediction%2C_variation%2C_etc..html">301 andrew gelman stats-2010-09-28-Correlation, prediction, variation, etc.</a></p>
<p>12 0.65214652 <a title="1686-lsi-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Interactions_of_predictors_in_a_causal_model.html">251 andrew gelman stats-2010-09-02-Interactions of predictors in a causal model</a></p>
<p>13 0.65049577 <a title="1686-lsi-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-19-Index_or_indicator_variables.html">2296 andrew gelman stats-2014-04-19-Index or indicator variables</a></p>
<p>14 0.64400691 <a title="1686-lsi-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-17-So-called_fixed_and_random_effects.html">472 andrew gelman stats-2010-12-17-So-called fixed and random effects</a></p>
<p>15 0.64068079 <a title="1686-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-03-How_best_to_compare_effects_measured_in_two_different_time_periods%3F.html">2086 andrew gelman stats-2013-11-03-How best to compare effects measured in two different time periods?</a></p>
<p>16 0.62165684 <a title="1686-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-15-R-squared_for_multilevel_models.html">1121 andrew gelman stats-2012-01-15-R-squared for multilevel models</a></p>
<p>17 0.61027193 <a title="1686-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-12-year_%2B_%281%7Cyear%29.html">851 andrew gelman stats-2011-08-12-year + (1|year)</a></p>
<p>18 0.60804766 <a title="1686-lsi-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-17-Clutering_and_variance_components.html">417 andrew gelman stats-2010-11-17-Clutering and variance components</a></p>
<p>19 0.60012078 <a title="1686-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-25-Correlation_of_1_._._._too_good_to_be_true%3F.html">1737 andrew gelman stats-2013-02-25-Correlation of 1 . . . too good to be true?</a></p>
<p>20 0.59922057 <a title="1686-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-06-Bayesian_Anova_found_useful_in_ecology.html">1102 andrew gelman stats-2012-01-06-Bayesian Anova found useful in ecology</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(13, 0.011), (15, 0.073), (16, 0.091), (21, 0.022), (24, 0.117), (36, 0.012), (40, 0.013), (47, 0.021), (52, 0.211), (58, 0.023), (63, 0.01), (76, 0.01), (77, 0.012), (79, 0.01), (84, 0.026), (86, 0.043), (99, 0.178)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9064393 <a title="1686-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-21-Finite-population_Anova_calculations_for_models_with_interactions.html">1686 andrew gelman stats-2013-01-21-Finite-population Anova calculations for models with interactions</a></p>
<p>Introduction: Jim Thomson writes:
  
I wonder if you could provide some clarification on the correct way to calculate the finite-population standard deviations for interaction terms in your Bayesian approach to ANOVA (as explained in your 2005 paper, and Gelman and Hill 2007).


I understand that it is the SD of the constrained batch coefficients that is of interest, but in most WinBUGS examples I have seen, the SDs are all calculated directly as sd.fin<-sd(beta.main[]) for main effects and sd(beta.int[,]) for interaction effects,  where beta.main and beta.int are the unconstrained coefficients, e.g. beta.int[i,j]~dnorm(0,tau).


For main effects, I can see that it makes no difference, since the constrained value is calculated by subtracting the mean, and sd(B[]) = sd(B[]-mean(B[])).


But the conventional sum-to-zero constraint for interaction terms in linear models is more complicated than subtracting the mean (there are only (n1-1)*(n2-1) free coefficients for an interaction b/w factors with n1 a</p><p>2 0.88539934 <a title="1686-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-25-Unlogging.html">485 andrew gelman stats-2010-12-25-Unlogging</a></p>
<p>Introduction: Catherine Bueker writes:
  
I [Bueker] am analyzing the effect of various contextual factors on the voter turnout of naturalized Latino citizens.  I have included the natural log of the number of Spanish Language ads run in each state during the election cycle to predict voter turnout.  I now want to calculate the predicted probabilities of turnout for those in states with 0 ads, 500 ads, 1000 ads, etc.  The problem is that I do not know how to handle the beta coefficient of the LN(Spanish language ads).  Is there someway to “unlog” the coefficient?
  
My reply:  Calculate these probabilities for specific values of predictors, then graph the predictions of interest.  Also, you can average over the other inputs in your model to get summaries.  See  this article  with Pardoe for further discussion.</p><p>3 0.87928522 <a title="1686-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-31-Infovis_vs._statistical_graphics%3A__My_talk_tomorrow_%28Tues%29_1pm_at_Columbia.html">546 andrew gelman stats-2011-01-31-Infovis vs. statistical graphics:  My talk tomorrow (Tues) 1pm at Columbia</a></p>
<p>Introduction: Infovis vs. statistical graphics .  Tues 1 Feb 2011 1pm, Avery Hall room 114.  Itâ&euro;&trade;s for the  Lectures in Planning Series  at the School of Architecture, Planning, and Preservation.
 
Background on the talk (joint with Antony Unwin) is  here .  And  here  are more of my thoughts on statistical graphics.</p><p>4 0.87737936 <a title="1686-lda-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-21-Statoverflow.html">223 andrew gelman stats-2010-08-21-Statoverflow</a></p>
<p>Introduction: Skirant Vadali writes:
  
 
I am writing to seek your help in building a community driven Q&A; website tentatively called called ‘Statistics Analysis’. I am neither a founder of this website nor do I have any financial stake in its success. 


By way of background to this website, please see Stackoverflow (http://stackoverflow.com/) and Mathoverflow (http://mathoverflow.net/). Stackoverflow is a Q&A; website targeted at software developers and is designed to help them ask questions and get answers from other developers.  Mathoverflow is a Q&A; website targeted at research mathematicians and is designed to help them ask and answer questions from other mathematicians across the world. The success of both these sites in helping their respective communities is a strong indicator that sites designed along these lines are very useful.


The company that runs Stackoverflow (who also host Mathoverflow.net) has recently decided to develop other community driven websites for various other topic are</p><p>5 0.87460065 <a title="1686-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-04-Data_visualization_panel_at_the_New_York_Public_Library_this_evening%21.html">1246 andrew gelman stats-2012-04-04-Data visualization panel at the New York Public Library this evening!</a></p>
<p>Introduction: I’ll be participating in  a panel  (along with Kaiser Fung, Mark Hansen, Tahir Hemphill, and Manuel Lima), “What Makes Good Data Visualization?”, at the 42nd St. library this evening.
 
The event is organized by Isabel Walcott Draves and is part of the  Leaders in Software and Art  series.
 
 This article  with Antony Unwin should be relevant (although I won’t be “presenting”; I’ll be part of a panel and we’ll be having a wide-ranging conversation).</p><p>6 0.84469938 <a title="1686-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-26-%E2%80%9CThe_Inside_Story_Of_The_Harvard_Dissertation_That_Became_Too_Racist_For_Heritage%E2%80%9D.html">1957 andrew gelman stats-2013-07-26-“The Inside Story Of The Harvard Dissertation That Became Too Racist For Heritage”</a></p>
<p>7 0.84322715 <a title="1686-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-16-meta-infographic.html">914 andrew gelman stats-2011-09-16-meta-infographic</a></p>
<p>8 0.83867449 <a title="1686-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-10-Our_data_visualization_panel_at_the_New_York_Public_Library.html">1256 andrew gelman stats-2012-04-10-Our data visualization panel at the New York Public Library</a></p>
<p>9 0.82339972 <a title="1686-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-05-Related_to_z-statistics.html">1301 andrew gelman stats-2012-05-05-Related to z-statistics</a></p>
<p>10 0.82185268 <a title="1686-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-12-Elderpedia.html">1531 andrew gelman stats-2012-10-12-Elderpedia</a></p>
<p>11 0.81430501 <a title="1686-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-04-The_acupuncture_paradox.html">889 andrew gelman stats-2011-09-04-The acupuncture paradox</a></p>
<p>12 0.80942678 <a title="1686-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-22-Seeking_balance.html">104 andrew gelman stats-2010-06-22-Seeking balance</a></p>
<p>13 0.80155504 <a title="1686-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-20-No_no_no_no_no.html">1020 andrew gelman stats-2011-11-20-No no no no no</a></p>
<p>14 0.77307743 <a title="1686-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-10-Combining_data_from_many_sources.html">948 andrew gelman stats-2011-10-10-Combining data from many sources</a></p>
<p>15 0.77260786 <a title="1686-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-04-Questions_about_quantum_computing.html">786 andrew gelman stats-2011-07-04-Questions about quantum computing</a></p>
<p>16 0.76856327 <a title="1686-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-06-Your_conclusion_is_only_as_good_as_your_data.html">1369 andrew gelman stats-2012-06-06-Your conclusion is only as good as your data</a></p>
<p>17 0.76772058 <a title="1686-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-11-Separating_national_and_state_swings_in_voting_and_public_opinion%2C_or%2C_How_I_avoided_blogorific_embarrassment%3A__An_agony_in_four_acts.html">200 andrew gelman stats-2010-08-11-Separating national and state swings in voting and public opinion, or, How I avoided blogorific embarrassment:  An agony in four acts</a></p>
<p>18 0.74880236 <a title="1686-lda-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-31-On_deck_this_week.html">2276 andrew gelman stats-2014-03-31-On deck this week</a></p>
<p>19 0.73989689 <a title="1686-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-26-A_statistician%E2%80%99s_rants_and_raves.html">1185 andrew gelman stats-2012-02-26-A statistician’s rants and raves</a></p>
<p>20 0.73827219 <a title="1686-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-16-He%E2%80%99s_adult_entertainer%2C_Child_educator%2C_King_of_the_crossfader%2C_He%E2%80%99s_the_greatest_of_the_greater%2C_He%E2%80%99s_a_big_bad_wolf_in_your_neighborhood%2C_Not_bad_meaning_bad_but_bad_meaning_good.html">2026 andrew gelman stats-2013-09-16-He’s adult entertainer, Child educator, King of the crossfader, He’s the greatest of the greater, He’s a big bad wolf in your neighborhood, Not bad meaning bad but bad meaning good</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
