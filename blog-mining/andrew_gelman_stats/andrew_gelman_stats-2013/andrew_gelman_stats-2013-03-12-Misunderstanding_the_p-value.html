<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1760 andrew gelman stats-2013-03-12-Misunderstanding the p-value</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-1760" href="#">andrew_gelman_stats-2013-1760</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1760 andrew gelman stats-2013-03-12-Misunderstanding the p-value</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-1760-html" href="http://andrewgelman.com/2013/03/12/misunderstanding-the-p-value/">html</a></p><p>Introduction: The New York Times has a feature in its Tuesday science section, Take a Number, to which I occasionally contribute (see  here  and  here ).
 
Today’s  column , by Nicholas Balakar, is in error.  The column begins:
  
When medical researchers report their findings, they need to know whether their result is a real effect of what they are testing, or just a random occurrence. To figure this out, they most commonly use the p-value.
  
This is wrong on two counts.  First, whatever researchers might feel, this is something they’ll never know.  Second, results are a combination of real effects and chance, it’s not either/or.
 
Perhaps the above is a forgivable simplification, but I don’t think so; I think it’s a simplification that destroys the reason for writing the article in the first place.  But in any case I think there’s no excuse for this, later on:
  
By convention, a p-value higher than 0.05 usually indicates that the results of the study, however good or bad, were probably due only</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The New York Times has a feature in its Tuesday science section, Take a Number, to which I occasionally contribute (see  here  and  here ). [sent-1, score-0.171]
</p><p>2 Today’s  column , by Nicholas Balakar, is in error. [sent-2, score-0.215]
</p><p>3 The column begins:    When medical researchers report their findings, they need to know whether their result is a real effect of what they are testing, or just a random occurrence. [sent-3, score-0.565]
</p><p>4 To figure this out, they most commonly use the p-value. [sent-4, score-0.09]
</p><p>5 First, whatever researchers might feel, this is something they’ll never know. [sent-6, score-0.087]
</p><p>6 Second, results are a combination of real effects and chance, it’s not either/or. [sent-7, score-0.2]
</p><p>7 Perhaps the above is a forgivable simplification, but I don’t think so; I think it’s a simplification that destroys the reason for writing the article in the first place. [sent-8, score-0.384]
</p><p>8 But in any case I think there’s no excuse for this, later on:    By convention, a p-value higher than 0. [sent-9, score-0.096]
</p><p>9 05 usually indicates that the results of the study, however good or bad, were probably due only to chance. [sent-10, score-0.405]
</p><p>10 This is the old, old error of confusing p(A|B) with p(B|A). [sent-11, score-0.301]
</p><p>11 I’m too rushed right now to explain this one, but it’s in just about every introductory statistics textbook ever written. [sent-12, score-0.309]
</p><p>12 The formal view of the P value as a probability conditional on the null is mathematically correct but typically irrelevant to research goals (hence, the popularity of alternative—if wrong—interpretations). [sent-14, score-1.379]
</p><p>13 I can’t get too annoyed at science writer Bakalar for garbling the point—it confuses lots and lots of people—but, still, I hate to see this error in the newspaper. [sent-18, score-0.486]
</p><p>14 On the plus side, if a newspaper column runs 20 times, I guess it’s ok for it to be wrong once—we still have 95% confidence in it, right? [sent-19, score-0.431]
</p><p>15 Various commenters remark that it’s not so easy to define p-values accurately. [sent-22, score-0.083]
</p><p>16 I agree, and I think it’s for reasons described in my quote immediately above:  the formal view of the p-value is mathematically correct but typically irrelevant to research goals. [sent-23, score-1.003]
</p><p>17 Phil  nails  it:    The p-value does not tell you if the result was due to chance. [sent-27, score-0.417]
</p><p>18 It tells you whether the results are  consistent  with being due to chance. [sent-28, score-0.401]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('simplification', 0.249), ('column', 0.215), ('due', 0.2), ('irrelevant', 0.187), ('mathematically', 0.185), ('view', 0.172), ('null', 0.161), ('formal', 0.16), ('begins', 0.157), ('rushed', 0.135), ('destroys', 0.135), ('wrong', 0.133), ('confuses', 0.129), ('results', 0.119), ('nails', 0.117), ('correct', 0.113), ('persists', 0.112), ('typically', 0.109), ('nicholas', 0.108), ('old', 0.108), ('greenland', 0.107), ('convention', 0.105), ('value', 0.104), ('result', 0.1), ('error', 0.1), ('tuesday', 0.098), ('popularity', 0.098), ('misunderstanding', 0.098), ('excuse', 0.096), ('confusing', 0.093), ('interpretations', 0.093), ('times', 0.091), ('probability', 0.09), ('introductory', 0.09), ('commonly', 0.09), ('contribute', 0.09), ('casual', 0.09), ('annoyed', 0.089), ('researchers', 0.087), ('indicates', 0.086), ('textbook', 0.084), ('lots', 0.084), ('remark', 0.083), ('runs', 0.083), ('whether', 0.082), ('real', 0.081), ('occasionally', 0.081), ('immediately', 0.077), ('phil', 0.077), ('valid', 0.077)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="1760-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-12-Misunderstanding_the_p-value.html">1760 andrew gelman stats-2013-03-12-Misunderstanding the p-value</a></p>
<p>Introduction: The New York Times has a feature in its Tuesday science section, Take a Number, to which I occasionally contribute (see  here  and  here ).
 
Today’s  column , by Nicholas Balakar, is in error.  The column begins:
  
When medical researchers report their findings, they need to know whether their result is a real effect of what they are testing, or just a random occurrence. To figure this out, they most commonly use the p-value.
  
This is wrong on two counts.  First, whatever researchers might feel, this is something they’ll never know.  Second, results are a combination of real effects and chance, it’s not either/or.
 
Perhaps the above is a forgivable simplification, but I don’t think so; I think it’s a simplification that destroys the reason for writing the article in the first place.  But in any case I think there’s no excuse for this, later on:
  
By convention, a p-value higher than 0.05 usually indicates that the results of the study, however good or bad, were probably due only</p><p>2 0.28582358 <a title="1760-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-08-P-values_and_statistical_practice.html">1713 andrew gelman stats-2013-02-08-P-values and statistical practice</a></p>
<p>Introduction: From  my new article  in the journal Epidemiology:
  
Sander Greenland and Charles Poole accept that P values are here to stay but recognize that some of their most common interpretations have problems. The casual view of the P value as posterior probability of the truth of the null hypothesis is false and not even close to valid under any reasonable model, yet this misunderstanding persists even in high-stakes settings (as discussed, for example, by Greenland in 2011). The formal view of the P value as a probability conditional on the null is mathematically correct but typically irrelevant to research goals (hence, the popularity of alternative—if wrong—interpretations). A Bayesian interpretation based on a spike-and-slab model makes little sense in applied contexts in epidemiology, political science, and other fields in which true effects are typically nonzero and bounded (thus violating both the “spike” and the “slab” parts of the model).


I find Greenland and Poole’s perspective t</p><p>3 0.18908873 <a title="1760-tfidf-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21.html">256 andrew gelman stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</a></p>
<p>Introduction: Masanao sends  this one  in, under the heading, “another incident of misunderstood p-value”:
  
Warren Davies, a positive psychology MSc student at UEL, provides the latest in our ongoing series of guest features for students. Warren has just released a Psychology Study Guide, which covers information on statistics, research methods and study skills for psychology students.

 
Despite the myriad rules and procedures of science, some research findings are pure flukes. Perhaps you’re testing a new drug, and by chance alone, a large number of people spontaneously get better. The better your study is conducted, the lower the chance that your result was a fluke – but still, there is always a certain probability that it was.


Statistical significance testing gives you an idea of what this probability is.


In science we’re always testing hypotheses. We never conduct a study to ‘see what happens’, because there’s always at least one way to make any useless set of data look important. We take</p><p>4 0.14880459 <a title="1760-tfidf-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-18-One-tailed_or_two-tailed%3F.html">2295 andrew gelman stats-2014-04-18-One-tailed or two-tailed?</a></p>
<p>Introduction: Someone writes:
  
Suppose I have two groups of people, A and B, which differ on some characteristic of interest to me;  and for each person I measure a single real-valued quantity X.  I have a theory that group A has a higher mean value of X than group B.  I test this theory by using a t-test.  Am I entitled to use a *one-tailed* t-test?  Or should I use a *two-tailed* one (thereby giving a p-value that is twice as large)?


I know you will probably answer:  Forget the t-test; you should use Bayesian methods instead.


But what is the standard frequentist answer to this question?
  
My reply:
 
The quick answer here is that different people will do different things here.  I would say the 2-tailed p-value is more standard but some people will insist on the one-tailed version, and itâ&euro;&trade;s hard to make a big stand on this one, given all the other problems with p-values in practice:
 
http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf
 
http://www.stat.columbia.edu/~gelm</p><p>5 0.14747837 <a title="1760-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-20-NYT_%28non%29-retraction_watch.html">2107 andrew gelman stats-2013-11-20-NYT (non)-retraction watch</a></p>
<p>Introduction: Mark Palko is  irritated  by the Times’s refusal to retract a recounting of a hoax regarding Dickens and Dostoevsky.  All I can say is, the Times refuses to retract mistakes of fact that are far more current than that!   See here  for two examples that particularly annoyed me, to the extent that I contacted various people at the Times but ran into refusals to retract.
 
I guess a daily newspaper publishes so much material that they can’t be expected to run a retraction every time they publish something false, even when such things are brought to their attention.
 
Speaking of corrections, I wonder if later editions of the Samuelson economics textbook discussed their notorious  graph  predicting Soviet economic performance.  The easiest thing would be just to remove the graph, but I think it would be a better economics lesson to discuss the error!
 
Similarly, I think the NYT would do well to run an article on their Dickens-Dostoevsky mistake, along with a column by Arthur Brooks on how</p><p>6 0.14020213 <a title="1760-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-26-%E2%80%9CA_Vast_Graveyard_of_Undead_Theories%3A__Publication_Bias_and_Psychological_Science%E2%80%99s_Aversion_to_the_Null%E2%80%9D.html">1826 andrew gelman stats-2013-04-26-“A Vast Graveyard of Undead Theories:  Publication Bias and Psychological Science’s Aversion to the Null”</a></p>
<p>7 0.13289271 <a title="1760-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-08-The_never-ending_%28and_often_productive%29_race_between_theory_and_practice.html">2127 andrew gelman stats-2013-12-08-The never-ending (and often productive) race between theory and practice</a></p>
<p>8 0.13244548 <a title="1760-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-17-Data_problems%2C_coding_errors%E2%80%A6what_can_be_done%3F.html">1807 andrew gelman stats-2013-04-17-Data problems, coding errors…what can be done?</a></p>
<p>9 0.11491492 <a title="1760-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>10 0.11400396 <a title="1760-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-07-Scatterplot_charades%21.html">1791 andrew gelman stats-2013-04-07-Scatterplot charades!</a></p>
<p>11 0.11372195 <a title="1760-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-13-What_are_the_important_issues_in_ethics_and_statistics%3F__I%E2%80%99m_looking_for_your_input%21.html">1117 andrew gelman stats-2012-01-13-What are the important issues in ethics and statistics?  I’m looking for your input!</a></p>
<p>12 0.11348516 <a title="1760-tfidf-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>13 0.11296377 <a title="1760-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-06-That_silly_ESP_paper_and_some_silliness_in_a_rebuttal_as_well.html">506 andrew gelman stats-2011-01-06-That silly ESP paper and some silliness in a rebuttal as well</a></p>
<p>14 0.1087053 <a title="1760-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-31-How_to_fix_the_tabloids%3F__Toward_replicable_social_science_research.html">1878 andrew gelman stats-2013-05-31-How to fix the tabloids?  Toward replicable social science research</a></p>
<p>15 0.10807255 <a title="1760-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-18-Understanding_posterior_p-values.html">2029 andrew gelman stats-2013-09-18-Understanding posterior p-values</a></p>
<p>16 0.10491271 <a title="1760-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-31-Lindley%E2%80%99s_paradox.html">1355 andrew gelman stats-2012-05-31-Lindley’s paradox</a></p>
<p>17 0.10453768 <a title="1760-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-04-Write_This_Book.html">1605 andrew gelman stats-2012-12-04-Write This Book</a></p>
<p>18 0.1044345 <a title="1760-tfidf-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-23-Discussion_on_preregistration_of_research_studies.html">2183 andrew gelman stats-2014-01-23-Discussion on preregistration of research studies</a></p>
<p>19 0.1036144 <a title="1760-tfidf-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-22-Philosophy_of_Bayes_and_non-Bayes%3A__A_dialogue_with_Deborah_Mayo.html">291 andrew gelman stats-2010-09-22-Philosophy of Bayes and non-Bayes:  A dialogue with Deborah Mayo</a></p>
<p>20 0.10323818 <a title="1760-tfidf-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-24-Empirical_implications_of_Empirical_Implications_of_Theoretical_Models.html">2263 andrew gelman stats-2014-03-24-Empirical implications of Empirical Implications of Theoretical Models</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.233), (1, 0.01), (2, -0.004), (3, -0.094), (4, -0.055), (5, -0.057), (6, 0.03), (7, 0.023), (8, 0.034), (9, -0.085), (10, -0.069), (11, 0.033), (12, -0.005), (13, -0.068), (14, -0.032), (15, -0.009), (16, -0.039), (17, -0.025), (18, 0.014), (19, -0.045), (20, 0.054), (21, -0.01), (22, 0.008), (23, -0.008), (24, -0.055), (25, -0.01), (26, -0.012), (27, 0.059), (28, -0.001), (29, -0.059), (30, -0.007), (31, 0.006), (32, 0.006), (33, -0.014), (34, -0.029), (35, -0.048), (36, 0.06), (37, -0.036), (38, 0.022), (39, -0.081), (40, -0.017), (41, -0.05), (42, 0.025), (43, -0.008), (44, 0.01), (45, 0.052), (46, -0.018), (47, 0.002), (48, 0.042), (49, 0.024)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97125947 <a title="1760-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-12-Misunderstanding_the_p-value.html">1760 andrew gelman stats-2013-03-12-Misunderstanding the p-value</a></p>
<p>Introduction: The New York Times has a feature in its Tuesday science section, Take a Number, to which I occasionally contribute (see  here  and  here ).
 
Today’s  column , by Nicholas Balakar, is in error.  The column begins:
  
When medical researchers report their findings, they need to know whether their result is a real effect of what they are testing, or just a random occurrence. To figure this out, they most commonly use the p-value.
  
This is wrong on two counts.  First, whatever researchers might feel, this is something they’ll never know.  Second, results are a combination of real effects and chance, it’s not either/or.
 
Perhaps the above is a forgivable simplification, but I don’t think so; I think it’s a simplification that destroys the reason for writing the article in the first place.  But in any case I think there’s no excuse for this, later on:
  
By convention, a p-value higher than 0.05 usually indicates that the results of the study, however good or bad, were probably due only</p><p>2 0.82443053 <a title="1760-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>Introduction: This article  is a discussion of a  paper  by Greg Francis for a special issue, edited by E. J. Wagenmakers, of the Journal of Mathematical Psychology.  Here’s what I wrote:
  
Much of statistical practice is an effort to reduce or deny variation and uncertainty. The reduction is done through standardization, replication, and other practices of experimental design, with the idea being to isolate and stabilize the quantity being estimated and then average over many cases. Even so, however, uncertainty persists, and statistical hypothesis testing is in many ways an endeavor to deny this, by reporting binary accept/reject decisions.


Classical statistical methods produce binary statements, but there is no reason to assume that the world works that way. Expressions such as Type 1 error, Type 2 error, false positive, and so on, are based on a model in which the world is divided into real and non-real effects. To put it another way, I understand the general scientific distinction of real vs</p><p>3 0.80890763 <a title="1760-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-15-%E2%80%9CAre_all_significant_p-values_created_equal%3F%E2%80%9D.html">2102 andrew gelman stats-2013-11-15-“Are all significant p-values created equal?”</a></p>
<p>Introduction: The answer is no, as explained in  this classic article  by Warren Browner and Thomas Newman from 1987.  If I were to rewrite this article today, I would frame things slightly differently—referring to Type S and Type M errors rather than speaking of “the probability that the research hypothesis is true”—but overall they make good points, and I like their analogy to medical diagnostic testing.</p><p>4 0.79953301 <a title="1760-lsi-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21.html">256 andrew gelman stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</a></p>
<p>Introduction: Masanao sends  this one  in, under the heading, “another incident of misunderstood p-value”:
  
Warren Davies, a positive psychology MSc student at UEL, provides the latest in our ongoing series of guest features for students. Warren has just released a Psychology Study Guide, which covers information on statistics, research methods and study skills for psychology students.

 
Despite the myriad rules and procedures of science, some research findings are pure flukes. Perhaps you’re testing a new drug, and by chance alone, a large number of people spontaneously get better. The better your study is conducted, the lower the chance that your result was a fluke – but still, there is always a certain probability that it was.


Statistical significance testing gives you an idea of what this probability is.


In science we’re always testing hypotheses. We never conduct a study to ‘see what happens’, because there’s always at least one way to make any useless set of data look important. We take</p><p>5 0.7958675 <a title="1760-lsi-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>Introduction: A recent  discussion  between commenters Question and Fernando captured one of the recurrent themes here from the past year.
 
 Question:   The problem is simple, the researchers are disproving always false null hypotheses and taking this disproof as near proof that their theory is correct.
 
 Fernando:   Whereas it is probably true that researchers misuse NHT, the problem with tabloid science is broader and deeper. It is systemic.
 
 Question:   I do not see how anything can be deeper than replacing careful description, prediction, falsification, and independent replication with dynamite plots, p-values, affirming the consequent, and peer review. From my own experience I am confident in saying that confusion caused by NHST is at the root of this problem.
 
 Fernando:   Incentives? Impact factors? Publish or die? “Interesting” and “new” above quality and reliability, or actually answering a research question, and a silly and unbecoming obsession with being quoted in NYT, etc. . . . Giv</p><p>6 0.78872687 <a title="1760-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-26-%E2%80%9CA_Vast_Graveyard_of_Undead_Theories%3A__Publication_Bias_and_Psychological_Science%E2%80%99s_Aversion_to_the_Null%E2%80%9D.html">1826 andrew gelman stats-2013-04-26-“A Vast Graveyard of Undead Theories:  Publication Bias and Psychological Science’s Aversion to the Null”</a></p>
<p>7 0.7760365 <a title="1760-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>8 0.77518296 <a title="1760-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-08-P-values_and_statistical_practice.html">1713 andrew gelman stats-2013-02-08-P-values and statistical practice</a></p>
<p>9 0.76453125 <a title="1760-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-31-Lindley%E2%80%99s_paradox.html">1355 andrew gelman stats-2012-05-31-Lindley’s paradox</a></p>
<p>10 0.74544001 <a title="1760-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-19-Revised_evidence_for_statistical_standards.html">2140 andrew gelman stats-2013-12-19-Revised evidence for statistical standards</a></p>
<p>11 0.74475259 <a title="1760-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-26-Difficulties_in_making_inferences_about_scientific_truth_from_distributions_of_published_p-values.html">2040 andrew gelman stats-2013-09-26-Difficulties in making inferences about scientific truth from distributions of published p-values</a></p>
<p>12 0.7370798 <a title="1760-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-07-I%E2%80%99m_negative_on_the_expression_%E2%80%9Cfalse_positives%E2%80%9D.html">2093 andrew gelman stats-2013-11-07-I’m negative on the expression “false positives”</a></p>
<p>13 0.73477376 <a title="1760-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-Multiple_comparisons_dispute_in_the_tabloids.html">1195 andrew gelman stats-2012-03-04-Multiple comparisons dispute in the tabloids</a></p>
<p>14 0.7331847 <a title="1760-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-06-That_silly_ESP_paper_and_some_silliness_in_a_rebuttal_as_well.html">506 andrew gelman stats-2011-01-06-That silly ESP paper and some silliness in a rebuttal as well</a></p>
<p>15 0.72941864 <a title="1760-lsi-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-11-The_myth_of_the_myth_of_the_myth_of_the_hot_hand.html">2243 andrew gelman stats-2014-03-11-The myth of the myth of the myth of the hot hand</a></p>
<p>16 0.72630972 <a title="1760-lsi-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-29-I_agree_with_this_comment.html">2272 andrew gelman stats-2014-03-29-I agree with this comment</a></p>
<p>17 0.7237556 <a title="1760-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-17-Where_do_theories_come_from%3F.html">1861 andrew gelman stats-2013-05-17-Where do theories come from?</a></p>
<p>18 0.71692801 <a title="1760-lsi-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-23-Discussion_on_preregistration_of_research_studies.html">2183 andrew gelman stats-2014-01-23-Discussion on preregistration of research studies</a></p>
<p>19 0.71401644 <a title="1760-lsi-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-18-One-tailed_or_two-tailed%3F.html">2295 andrew gelman stats-2014-04-18-One-tailed or two-tailed?</a></p>
<p>20 0.71179521 <a title="1760-lsi-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-27-Hype_about_conditional_probability_puzzles.html">54 andrew gelman stats-2010-05-27-Hype about conditional probability puzzles</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(15, 0.015), (16, 0.124), (18, 0.014), (21, 0.033), (24, 0.206), (42, 0.05), (53, 0.024), (86, 0.037), (89, 0.015), (94, 0.066), (99, 0.303)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97652936 <a title="1760-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-12-Misunderstanding_the_p-value.html">1760 andrew gelman stats-2013-03-12-Misunderstanding the p-value</a></p>
<p>Introduction: The New York Times has a feature in its Tuesday science section, Take a Number, to which I occasionally contribute (see  here  and  here ).
 
Today’s  column , by Nicholas Balakar, is in error.  The column begins:
  
When medical researchers report their findings, they need to know whether their result is a real effect of what they are testing, or just a random occurrence. To figure this out, they most commonly use the p-value.
  
This is wrong on two counts.  First, whatever researchers might feel, this is something they’ll never know.  Second, results are a combination of real effects and chance, it’s not either/or.
 
Perhaps the above is a forgivable simplification, but I don’t think so; I think it’s a simplification that destroys the reason for writing the article in the first place.  But in any case I think there’s no excuse for this, later on:
  
By convention, a p-value higher than 0.05 usually indicates that the results of the study, however good or bad, were probably due only</p><p>2 0.96865487 <a title="1760-lda-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-20-The_AAA_Tranche_of_Subprime_Science.html">2179 andrew gelman stats-2014-01-20-The AAA Tranche of Subprime Science</a></p>
<p>Introduction: In  our new ethics column for Chance , Eric Loken and I write about our current favorite topic:
  
One of our ongoing themes when discussing scientific ethics is the central role of statistics in recognizing and communicating uncer- 
tainty. Unfortunately, statistics—and the scientific process more generally—often seems to be used more as a way of laundering uncertainty, processing data until researchers and consumers of research can feel safe acting as if various scientific hypotheses are unquestionably true. . . .


We have in mind an analogy with the notorious AAA-class bonds created during the mid-2000s that led to the subprime mortgage crisis. Lower-quality mortgages—that is, mortgages with high probability of default and, thus, high uncertainty—were packaged and transformed into financial instruments that were (in retrospect, falsely) characterized as low risk. There was a tremendous interest in these securities, not just among the most unscrupulous market manipulators, but in a</p><p>3 0.9658339 <a title="1760-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-03-Boot.html">1881 andrew gelman stats-2013-06-03-Boot</a></p>
<p>Introduction: Joshua Hartshorne writes: 
  
  
I ran several large-N experiments (separate participants) and looked at performance against age. What we want to do is compare age-of-peak-performance across the different tasks (again, different participants).


We bootstrapped age-of-peak-performance. On each iteration, we sampled (with replacement) the X scores at each age, where X=num of participants at that age, and recorded the age at which performance peaked on that task. We then recorded the age at which performance was at peak and repeated. Once we had distributions of age-of-peak-performance, we used the means and SDs to calculate t-statistics to compare the results across different tasks. For graphical presentation, we used medians, interquartile ranges, and 95% confidence intervals (based on the distributions: the range within which 75% and 95% of the bootstrapped peaks appeared). 


While a number of people we consulted with thought this made a lot of sense, one reviewer of the paper insist</p><p>4 0.96539766 <a title="1760-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-17-Macro_causality.html">807 andrew gelman stats-2011-07-17-Macro causality</a></p>
<p>Introduction: David Backus writes:
  
 This  is from my area of work, macroeconomics.  The suggestion here is that the economy is growing slowly because consumers aren’t spending money.  But how do we know it’s not the reverse:  that consumers are spending less because the economy isn’t doing well.  As a teacher, I can tell you that it’s almost impossible to get students to understand that the first statement isn’t obviously true.  What I’d call the demand-side story (more spending leads to more output) is everywhere, including this piece, from the usually reliable David Leonhardt.
  
This whole situation reminds me of the story of the village whose inhabitants support themselves by taking in each others’ laundry.  I guess we’re rich enough in the U.S. that we can stay afloat for a few decades just buying things from each other?
 
Regarding the causal question, I’d like to move away from the idea of “Does A causes B or does B cause A” and toward a more intervention-based framework (Rubin’s model for</p><p>5 0.96458268 <a title="1760-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-26-Difficulties_in_making_inferences_about_scientific_truth_from_distributions_of_published_p-values.html">2040 andrew gelman stats-2013-09-26-Difficulties in making inferences about scientific truth from distributions of published p-values</a></p>
<p>Introduction: Jeff Leek  just posted  the discussions of his paper (with Leah Jager), “An estimate of the science-wise false discovery rate and application to the top medical literature,” along with some further comments of his own.
 
 Here  are my original thoughts on an earlier version of their article.  Keith O’Rourke and I expanded these thoughts into  a formal comment  for the journal.  We’re pretty much in agreement with John Ioannidis (you can find his discussion in the top link above).
 
In quick summary, I agree with Jager and Leek that this is an important topic.  I think there are two key places where Keith and I disagree with them:
 
1.  They take published p-values at face value whereas we consider them as the result of a complicated process of selection.  This is something I didn’t used to think much about, but now I’ve become increasingly convinced that the problems with published p-values is not a simple file-drawer effect or the case of a few p=0.051 values nudged toward p=0.049, bu</p><p>6 0.96345127 <a title="1760-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-Fourteen_magic_words%3A_an_update.html">898 andrew gelman stats-2011-09-10-Fourteen magic words: an update</a></p>
<p>7 0.9627341 <a title="1760-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-03-Reinventing_the_wheel%2C_only_more_so..html">447 andrew gelman stats-2010-12-03-Reinventing the wheel, only more so.</a></p>
<p>8 0.96214813 <a title="1760-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-13-Hypothesis_testing_with_multiple_imputations.html">799 andrew gelman stats-2011-07-13-Hypothesis testing with multiple imputations</a></p>
<p>9 0.9614011 <a title="1760-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-10-95%25_intervals_that_I_don%E2%80%99t_believe%2C_because_they%E2%80%99re_from_a_flat_prior_I_don%E2%80%99t_believe.html">1206 andrew gelman stats-2012-03-10-95% intervals that I don’t believe, because they’re from a flat prior I don’t believe</a></p>
<p>10 0.9608084 <a title="1760-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>11 0.96079481 <a title="1760-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-19-Validation_of_Software_for_Bayesian_Models_Using_Posterior_Quantiles.html">1019 andrew gelman stats-2011-11-19-Validation of Software for Bayesian Models Using Posterior Quantiles</a></p>
<p>12 0.9602114 <a title="1760-lda-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-06-Bootstrap_averaging%3A_Examples_where_it_works_and_where_it_doesn%E2%80%99t_work.html">2201 andrew gelman stats-2014-02-06-Bootstrap averaging: Examples where it works and where it doesn’t work</a></p>
<p>13 0.9597562 <a title="1760-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-23-A_statistical_version_of_Arrow%E2%80%99s_paradox.html">586 andrew gelman stats-2011-02-23-A statistical version of Arrow’s paradox</a></p>
<p>14 0.95874798 <a title="1760-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-16-Chess_vs._checkers.html">615 andrew gelman stats-2011-03-16-Chess vs. checkers</a></p>
<p>15 0.95776469 <a title="1760-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-27-Graph_of_the_year.html">488 andrew gelman stats-2010-12-27-Graph of the year</a></p>
<p>16 0.9576 <a title="1760-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-04-Clarity_on_my_email_policy.html">503 andrew gelman stats-2011-01-04-Clarity on my email policy</a></p>
<p>17 0.95736086 <a title="1760-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-20-Likelihood_thresholds_and_decisions.html">1422 andrew gelman stats-2012-07-20-Likelihood thresholds and decisions</a></p>
<p>18 0.95724618 <a title="1760-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-18-Check_your_missing-data_imputations_using_cross-validation.html">1218 andrew gelman stats-2012-03-18-Check your missing-data imputations using cross-validation</a></p>
<p>19 0.95718241 <a title="1760-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-The_statistical_significance_filter.html">899 andrew gelman stats-2011-09-10-The statistical significance filter</a></p>
<p>20 0.95705545 <a title="1760-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-31-Bayes%3A_radical%2C_liberal%2C_or_conservative%3F.html">639 andrew gelman stats-2011-03-31-Bayes: radical, liberal, or conservative?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
