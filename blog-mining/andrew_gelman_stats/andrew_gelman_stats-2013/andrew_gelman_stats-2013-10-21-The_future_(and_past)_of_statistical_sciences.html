<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2072 andrew gelman stats-2013-10-21-The future (and past) of statistical sciences</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-2072" href="#">andrew_gelman_stats-2013-2072</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2072 andrew gelman stats-2013-10-21-The future (and past) of statistical sciences</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-2072-html" href="http://andrewgelman.com/2013/10/21/the-future-and-past-of-statistical-sciences/">html</a></p><p>Introduction: In connection with  this  workshop, I was asked to write a few paragraphs describing my perspective on “the current and near-term future state of the statistical sciences you are most familiar with.”  Here’s what I wrote:
  
I think that, at any given time, the field of statistics has a core, but that core changes over time.  There are different paradigmatic ways to solve problems.


100 or 150 years ago, the thing to do was to identify a phenomenon of interest with some probability distribution and then use the mathematics of that distribution to gain insight into the underlying process.  Thus, for example, if certain data looked like they came from a normal distribution, one could surmise that the values in question arose by adding many small independent pieces.  If the data looked like they came from a binomial distribution, that would imply independence and equal probabilities.  Waiting times that followed an exponential distribution could be considered as coming from a memoryless</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In connection with  this  workshop, I was asked to write a few paragraphs describing my perspective on “the current and near-term future state of the statistical sciences you are most familiar with. [sent-1, score-0.079]
</p><p>2 ”  Here’s what I wrote:    I think that, at any given time, the field of statistics has a core, but that core changes over time. [sent-2, score-0.269]
</p><p>3 There are different paradigmatic ways to solve problems. [sent-3, score-0.118]
</p><p>4 100 or 150 years ago, the thing to do was to identify a phenomenon of interest with some probability distribution and then use the mathematics of that distribution to gain insight into the underlying process. [sent-4, score-0.927]
</p><p>5 Thus, for example, if certain data looked like they came from a normal distribution, one could surmise that the values in question arose by adding many small independent pieces. [sent-5, score-0.358]
</p><p>6 If the data looked like they came from a binomial distribution, that would imply independence and equal probabilities. [sent-6, score-0.461]
</p><p>7 Waiting times that followed an exponential distribution could be considered as coming from a memoryless process. [sent-7, score-0.472]
</p><p>8 I remember in graduate school hearing rumors of Pearson’s system in which he characterized 12 classes of distributions, or something like that. [sent-9, score-0.299]
</p><p>9 It all sounds silly now, but it’s not so ridiculous, especially if you keep in mind that the name of the game is understanding the process, not just fitting the data. [sent-10, score-0.224]
</p><p>10 The point of picking the right distribution is to capture general features of the underlying system. [sent-11, score-0.607]
</p><p>11 The idea is typically to identify the factors that influence some outcome of interest. [sent-13, score-0.131]
</p><p>12 Meanwhile, other approaches to statistics go in and out of favor. [sent-14, score-0.109]
</p><p>13 It’s my impression that statistics in the twenty or thirty years following World War II was strongly influenced by the war experiences of various mathematicians who worked on problems of statistical inference and operations research. [sent-15, score-0.778]
</p><p>14 What we saw after the war was a framing of many statistical problems as optimization, and a general attitude that optimization (and, to a lesser extent, game theory) were the fundamental principles underlying all the data sciences. [sent-16, score-0.953]
</p><p>15 Even with fast computing, though, we still need as much mathematical understanding as we can get. [sent-18, score-0.094]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('distribution', 0.271), ('war', 0.21), ('underlying', 0.176), ('optimization', 0.164), ('core', 0.16), ('capture', 0.16), ('computing', 0.14), ('identify', 0.131), ('rumors', 0.131), ('game', 0.13), ('paradigmatic', 0.118), ('distributions', 0.116), ('extent', 0.113), ('statistics', 0.109), ('power', 0.108), ('looked', 0.107), ('exponential', 0.103), ('constraint', 0.101), ('expressions', 0.099), ('workshop', 0.099), ('mathematicians', 0.098), ('coming', 0.098), ('steadily', 0.096), ('thirty', 0.096), ('pearson', 0.095), ('understanding', 0.094), ('operations', 0.094), ('lesser', 0.094), ('binomial', 0.092), ('independence', 0.091), ('influenced', 0.091), ('framing', 0.09), ('characterized', 0.09), ('data', 0.089), ('collect', 0.086), ('ii', 0.084), ('regression', 0.084), ('nonparametric', 0.083), ('waiting', 0.083), ('approach', 0.083), ('came', 0.082), ('laws', 0.081), ('ridiculous', 0.081), ('twenty', 0.08), ('generalized', 0.08), ('arose', 0.08), ('descriptive', 0.08), ('paragraphs', 0.079), ('phenomenon', 0.078), ('hearing', 0.078)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="2072-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-21-The_future_%28and_past%29_of_statistical_sciences.html">2072 andrew gelman stats-2013-10-21-The future (and past) of statistical sciences</a></p>
<p>Introduction: In connection with  this  workshop, I was asked to write a few paragraphs describing my perspective on “the current and near-term future state of the statistical sciences you are most familiar with.”  Here’s what I wrote:
  
I think that, at any given time, the field of statistics has a core, but that core changes over time.  There are different paradigmatic ways to solve problems.


100 or 150 years ago, the thing to do was to identify a phenomenon of interest with some probability distribution and then use the mathematics of that distribution to gain insight into the underlying process.  Thus, for example, if certain data looked like they came from a normal distribution, one could surmise that the values in question arose by adding many small independent pieces.  If the data looked like they came from a binomial distribution, that would imply independence and equal probabilities.  Waiting times that followed an exponential distribution could be considered as coming from a memoryless</p><p>2 0.14992581 <a title="2072-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-%E2%80%9CIs_machine_learning_a_subset_of_statistics%3F%E2%80%9D.html">1740 andrew gelman stats-2013-02-26-“Is machine learning a subset of statistics?”</a></p>
<p>Introduction: Following up on our  previous post , Andrew Wilson writes:
  
I agree we are in a really exciting time for statistics and machine learning.  There has been a lot of talk lately comparing machine learning with statistics.  I am curious whether you think there are many fundamental differences between the fields, or just superficial differences — different popular approximate inference methods, slightly different popular application areas, etc.  Is machine learning a subset of statistics?


In the paper we discuss how we think machine learning is fundamentally about pattern discovery, and ultimately, fully automating the learning and decision making process.  In other words, whatever a human does when he or she uses tools to analyze data, can be written down algorithmically and automated on a computer.  I am not sure if the ambitions are similar in statistics — and I don’t have any conventional statistics background, which makes it harder to tell. I think it’s an interesting discussion.</p><p>3 0.14989704 <a title="2072-tfidf-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-21-Random_matrices_in_the_news.html">2258 andrew gelman stats-2014-03-21-Random matrices in the news</a></p>
<p>Introduction: From 2010 :
  
Mark Buchanan wrote  a cover article  for the New Scientist on random matrices, a heretofore obscure area of probability theory that his headline writer characterizes as “the deep law that shapes our reality.”


It’s interesting stuff, and he gets into some statistical applications at the end, so I’ll give you my take on it.


But first, some background.


About two hundred years ago, the mathematician/physicist Laplace discovered what is now called the central limit theorem, which is that, under certain conditions, the average of a large number of small random variables has an approximate normal (bell-shaped) distribution. A bit over 100 years ago, social scientists such as Galton applied this theorem to all sorts of biological and social phenomena. The central limit theorem, in its generality, is also important in the information that it indirectly conveys when it fails.


For example, the distribution of the heights of adult men or women is nicely bell-shaped, but the</p><p>4 0.14810249 <a title="2072-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-16-The_%E2%80%9CWashington_read%E2%80%9D_and_the_algebra_of_conditional_distributions.html">961 andrew gelman stats-2011-10-16-The “Washington read” and the algebra of conditional distributions</a></p>
<p>Introduction: I was trying to explain in class how a (Bayesian) statistician reads the formula for a probability distribution.  In old-fashioned statistics textbooks you’re told that if you want to compute a conditional distribution from a joint distribution you need to do some heavy math:  p(a|b) = p(a,b)/\int p(a’,b)da’.
 
When doing Bayesian statistics, though, you usually don’t have to do the integration or the division. If you have parameters theta and data y, you first write p(y,theta).  Then to get p(theta|y), you  don’t  need to integrate or divide.  All you have to do is look at p(y,theta) in a certain way:  Treat y as a constant and theta as a variable.  Similarly, if you’re doing the Gibbs sampler and want a conditional distribution, just consider the parameter you’re updating as the variable and everything else as a constant.  No need to integrate or divide, you just take the joint distribution and look at it from the right perspective.
 
Awhile ago Yair told me there’s something called</p><p>5 0.14736235 <a title="2072-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>Introduction: My (coauthored) books on Bayesian data analysis and applied regression are like almost all the other statistics textbooks out there, in that we spend most of our time on the basic distributions such as normal and logistic and then, only as an aside, discuss robust models such as t and robit.
 
Why aren’t the t and robit front and center?  Sure, I can see starting with the normal (at least in the Bayesian book, where we actually work out all the algebra), but then why don’t we move on immediately to the real stuff?
 
This isn’t just (or mainly) a question of textbooks or teaching; I’m really thinking here about statistical practice.  My statistical practice.  Should t and robit be the default?  If not, why not?
 
Some possible answers:
  
10.  Estimating the degrees of freedom in the error distribution isn’t so easy, and throwing this extra parameter into the model could make inference unstable.


9.  Real data usually don’t have outliers.  In practice, fitting a robust model costs you</p><p>6 0.14691548 <a title="2072-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-Another_reason_why_you_can_get_good_inferences_from_a_bad_model.html">1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</a></p>
<p>7 0.13697959 <a title="2072-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-09-How_to_model_distributions_that_have_outliers_in_one_direction.html">2128 andrew gelman stats-2013-12-09-How to model distributions that have outliers in one direction</a></p>
<p>8 0.13629068 <a title="2072-tfidf-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-Transformations_for_non-normal_data.html">2176 andrew gelman stats-2014-01-19-Transformations for non-normal data</a></p>
<p>9 0.13564689 <a title="2072-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-30-More_on_the_correlation_between_statistical_and_political_ideology.html">638 andrew gelman stats-2011-03-30-More on the correlation between statistical and political ideology</a></p>
<p>10 0.13370992 <a title="2072-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-04-Model_checking_and_model_understanding_in_machine_learning.html">1482 andrew gelman stats-2012-09-04-Model checking and model understanding in machine learning</a></p>
<p>11 0.13289347 <a title="2072-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-09-A_tale_of_two_discussion_papers.html">1848 andrew gelman stats-2013-05-09-A tale of two discussion papers</a></p>
<p>12 0.13102315 <a title="2072-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>13 0.12822443 <a title="2072-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>14 0.12580913 <a title="2072-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-25-Bayes-respecting_experimental_design_and_other_things.html">1955 andrew gelman stats-2013-07-25-Bayes-respecting experimental design and other things</a></p>
<p>15 0.12526394 <a title="2072-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>16 0.12321827 <a title="2072-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<p>17 0.12043865 <a title="2072-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>18 0.11948728 <a title="2072-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-04-Write_This_Book.html">1605 andrew gelman stats-2012-12-04-Write This Book</a></p>
<p>19 0.11747026 <a title="2072-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>20 0.1126271 <a title="2072-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.248), (1, 0.088), (2, -0.024), (3, 0.03), (4, -0.01), (5, 0.042), (6, -0.034), (7, 0.032), (8, -0.026), (9, 0.034), (10, -0.036), (11, -0.023), (12, -0.017), (13, -0.023), (14, -0.048), (15, 0.012), (16, -0.008), (17, -0.028), (18, 0.026), (19, -0.06), (20, 0.042), (21, -0.012), (22, -0.017), (23, 0.027), (24, 0.046), (25, 0.087), (26, -0.021), (27, 0.034), (28, 0.001), (29, -0.052), (30, 0.044), (31, 0.057), (32, -0.013), (33, 0.041), (34, -0.005), (35, -0.028), (36, -0.03), (37, 0.069), (38, -0.035), (39, 0.026), (40, 0.002), (41, 0.037), (42, -0.013), (43, -0.055), (44, 0.019), (45, 0.006), (46, 0.024), (47, 0.002), (48, 0.012), (49, -0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9693352 <a title="2072-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-21-The_future_%28and_past%29_of_statistical_sciences.html">2072 andrew gelman stats-2013-10-21-The future (and past) of statistical sciences</a></p>
<p>Introduction: In connection with  this  workshop, I was asked to write a few paragraphs describing my perspective on “the current and near-term future state of the statistical sciences you are most familiar with.”  Here’s what I wrote:
  
I think that, at any given time, the field of statistics has a core, but that core changes over time.  There are different paradigmatic ways to solve problems.


100 or 150 years ago, the thing to do was to identify a phenomenon of interest with some probability distribution and then use the mathematics of that distribution to gain insight into the underlying process.  Thus, for example, if certain data looked like they came from a normal distribution, one could surmise that the values in question arose by adding many small independent pieces.  If the data looked like they came from a binomial distribution, that would imply independence and equal probabilities.  Waiting times that followed an exponential distribution could be considered as coming from a memoryless</p><p>2 0.8181228 <a title="2072-lsi-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-21-Random_matrices_in_the_news.html">2258 andrew gelman stats-2014-03-21-Random matrices in the news</a></p>
<p>Introduction: From 2010 :
  
Mark Buchanan wrote  a cover article  for the New Scientist on random matrices, a heretofore obscure area of probability theory that his headline writer characterizes as “the deep law that shapes our reality.”


It’s interesting stuff, and he gets into some statistical applications at the end, so I’ll give you my take on it.


But first, some background.


About two hundred years ago, the mathematician/physicist Laplace discovered what is now called the central limit theorem, which is that, under certain conditions, the average of a large number of small random variables has an approximate normal (bell-shaped) distribution. A bit over 100 years ago, social scientists such as Galton applied this theorem to all sorts of biological and social phenomena. The central limit theorem, in its generality, is also important in the information that it indirectly conveys when it fails.


For example, the distribution of the heights of adult men or women is nicely bell-shaped, but the</p><p>3 0.7574451 <a title="2072-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-17-Probability-processing_hardware.html">214 andrew gelman stats-2010-08-17-Probability-processing hardware</a></p>
<p>Introduction: Lyric Semiconductor   posted:
  

For over 60 years, computers have been based on digital computing principles. Data is represented as bits (0s and 1s). Boolean logic gates perform operations on these bits. A processor steps through many of these operations serially in order to perform a function. However, today’s most interesting problems are not at all suited to this approach.


Here at Lyric Semiconductor, we are redesigning information processing circuits from the ground up to natively process probabilities: from the gate circuits to the processor architecture to the programming language.  As a result, many applications that today require a thousand conventional processors will soon run in just one Lyric processor, providing 1,000x efficiencies in cost, power, and size.

  
 Om Malik  has some more information, also relating to the team and the business.
 
The fundamental idea is that computing architectures work deterministically, even though the world is fundamentally stochastic.</p><p>4 0.75146806 <a title="2072-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-30-More_on_the_correlation_between_statistical_and_political_ideology.html">638 andrew gelman stats-2011-03-30-More on the correlation between statistical and political ideology</a></p>
<p>Introduction: This is a chance for me to combine two of my interests–politics and statistics–and probably to irritate both halves of the readership of this blog.  Anyway…
 
I recently  wrote  about the apparent correlation between Bayes/non-Bayes statistical ideology and liberal/conservative political ideology:
  
The Bayes/non-Bayes fissure had a bit of a political dimension–with anti-Bayesians being the old-line conservatives (for example, Ronald Fisher) and Bayesians having a more of a left-wing flavor (for example, Dennis Lindley). Lots of counterexamples at an individual level, but my impression is that on average the old curmudgeonly, get-off-my-lawn types were (with some notable exceptions) more likely to be anti-Bayesian.
  
This was somewhat based on my experiences at Berkeley.  Actually, some of the cranky anti-Bayesians were probably Democrats as well, but when they were being anti-Bayesian they seemed pretty conservative.
 
Recently I received an interesting item from Gerald Cliff, a pro</p><p>5 0.7380628 <a title="2072-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-27-Should_statistics_have_a_Nobel_prize%3F.html">2151 andrew gelman stats-2013-12-27-Should statistics have a Nobel prize?</a></p>
<p>Introduction: Xiao-Li  says  yes:
  
The most compelling reason for having highly visible awards in any field is to enhance its ability to attract future talent. Virtually all the media and public attention our profession received in recent years has been on the utility of statistics in all walks of life. We are extremely happy for and proud of this recognition—it is long overdue. However, the media and public have given much more attention to the Fields Medal than to the COPSS Award, even though the former has hardly been about direct or even indirect impact on everyday life. Why this difference? . . . these awards arouse media and public interest by featuring how ingenious the awardees are and how difficult the problems they solved, much like how conquering Everest bestows admiration not because the admirers care or even know much about Everest itself but because it represents the ultimate physical feat. In this sense, the biggest winner of the Fields Medal is mathematics itself: enticing the brig</p><p>6 0.72599888 <a title="2072-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-%E2%80%9CIs_machine_learning_a_subset_of_statistics%3F%E2%80%9D.html">1740 andrew gelman stats-2013-02-26-“Is machine learning a subset of statistics?”</a></p>
<p>7 0.72494441 <a title="2072-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<p>8 0.72449696 <a title="2072-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-13-Philosophy_of_Bayesian_statistics%3A__my_reactions_to_Wasserman.html">1165 andrew gelman stats-2012-02-13-Philosophy of Bayesian statistics:  my reactions to Wasserman</a></p>
<p>9 0.71857464 <a title="2072-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-16-How_do_we_choose_our_default_methods%3F.html">1859 andrew gelman stats-2013-05-16-How do we choose our default methods?</a></p>
<p>10 0.71509564 <a title="2072-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-14-Everyone%E2%80%99s_trading_bias_for_variance_at_some_point%2C_it%E2%80%99s_just_done_at_different_places_in_the_analyses.html">1763 andrew gelman stats-2013-03-14-Everyone’s trading bias for variance at some point, it’s just done at different places in the analyses</a></p>
<p>11 0.71351147 <a title="2072-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-26-Bad_news_about_%28some%29_statisticians.html">1282 andrew gelman stats-2012-04-26-Bad news about (some) statisticians</a></p>
<p>12 0.71222192 <a title="2072-lsi-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-19-David_Blackwell.html">155 andrew gelman stats-2010-07-19-David Blackwell</a></p>
<p>13 0.70748925 <a title="2072-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-03-Statistical_methods_for_healthcare_regulation%3A_rating%2C_screening_and_surveillance.html">744 andrew gelman stats-2011-06-03-Statistical methods for healthcare regulation: rating, screening and surveillance</a></p>
<p>14 0.70726627 <a title="2072-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-19-Demystifying_Blup.html">1270 andrew gelman stats-2012-04-19-Demystifying Blup</a></p>
<p>15 0.70121557 <a title="2072-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-05-Watership_Down%2C_thick_description%2C_applied_statistics%2C_immutability_of_stories%2C_and_playing_tennis_with_a_net.html">1750 andrew gelman stats-2013-03-05-Watership Down, thick description, applied statistics, immutability of stories, and playing tennis with a net</a></p>
<p>16 0.70050478 <a title="2072-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-19-Statistical_discrimination_again.html">1541 andrew gelman stats-2012-10-19-Statistical discrimination again</a></p>
<p>17 0.69805235 <a title="2072-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-03-It_depends_upon_what_the_meaning_of_the_word_%E2%80%9Cfirm%E2%80%9D_is..html">940 andrew gelman stats-2011-10-03-It depends upon what the meaning of the word “firm” is.</a></p>
<p>18 0.69617838 <a title="2072-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-04-Model_checking_and_model_understanding_in_machine_learning.html">1482 andrew gelman stats-2012-09-04-Model checking and model understanding in machine learning</a></p>
<p>19 0.69334567 <a title="2072-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>20 0.68441725 <a title="2072-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-07-Descriptive_statistics%2C_causal_inference%2C_and_story_time.html">789 andrew gelman stats-2011-07-07-Descriptive statistics, causal inference, and story time</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(5, 0.015), (15, 0.043), (16, 0.087), (17, 0.022), (24, 0.104), (25, 0.01), (31, 0.024), (43, 0.013), (51, 0.014), (53, 0.035), (59, 0.047), (76, 0.011), (86, 0.054), (93, 0.015), (99, 0.41)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99033248 <a title="2072-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-21-The_future_%28and_past%29_of_statistical_sciences.html">2072 andrew gelman stats-2013-10-21-The future (and past) of statistical sciences</a></p>
<p>Introduction: In connection with  this  workshop, I was asked to write a few paragraphs describing my perspective on “the current and near-term future state of the statistical sciences you are most familiar with.”  Here’s what I wrote:
  
I think that, at any given time, the field of statistics has a core, but that core changes over time.  There are different paradigmatic ways to solve problems.


100 or 150 years ago, the thing to do was to identify a phenomenon of interest with some probability distribution and then use the mathematics of that distribution to gain insight into the underlying process.  Thus, for example, if certain data looked like they came from a normal distribution, one could surmise that the values in question arose by adding many small independent pieces.  If the data looked like they came from a binomial distribution, that would imply independence and equal probabilities.  Waiting times that followed an exponential distribution could be considered as coming from a memoryless</p><p>2 0.98262364 <a title="2072-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-19-Participate_in_a_short_survey_about_the_weight_of_evidence_provided_by_statistics.html">1681 andrew gelman stats-2013-01-19-Participate in a short survey about the weight of evidence provided by statistics</a></p>
<p>Introduction: Richard Morey writes:
  
Rink Hoekstra and I are undertaking some research to explore how people use classical statistical results to evaluate the weight of evidence. Bayesians often critique classical techniques for being difficult to interpret in terms of what scientists want to know, but there is clearly information in the statistics themselves. We wonder how people extract that information. Below is our official announcement; it would be great if you could let people on your blog know about the survey, as we want to get a wide variety of statistical users to take the survey.


Announcement follows:

 
Empirical science is grounded on the belief that data can be used as evidence. The convincingness of data — the “weight” of the evidence they provide — is crucial to deciding between rival scientific positions. In situations with no uncertainty, reasoning about evidence is often straightforward; in practice, however, most conclusions from data involve uncertainty. In these situations,</p><p>3 0.98168993 <a title="2072-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>Introduction: In response to  my remarks  on his online book, Think Bayes, Allen Downey wrote: 
   
I [Downey] have a question about one of your comments: 
   My [Gelman's] main criticism with both books is that they talk a lot about inference but not so much about model building or model checking (recall the three steps of Bayesian data analysis). I think it’s ok for an introductory book to focus on inference, which of course is central to the data-analytic process—but I’d like them to at least mention that Bayesian ideas arise in model building and model checking as well. 

This sounds like something I agree with, and one of the things I tried to do in the book is to put modeling decisions front and center.  But the word “modeling” is used in lots of ways, so I want to see if we are talking about the same thing.


For example, in many chapters, I start with a simple model of the scenario, do some analysis, then check whether the model is good enough, and iterate.  Here’s the discussion of modeling</p><p>4 0.98148364 <a title="2072-lda-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-03-Booze%3A_Been_There._Done_That..html">2158 andrew gelman stats-2014-01-03-Booze: Been There. Done That.</a></p>
<p>Introduction: Our research assistants have unearthed the following guest column by H. L. Mencken which appeared in the New York Times of 5 Nov 1933, the date at which Prohibition ended in the United States. As a public service we are reprinting it here.
 
I’m particularly impressed at how the Sage of Baltimore buttressed his article with references to the latest scientific literature of the time. I think you’ll all agree that Mencken’s column, in which he took a stand against the legality of alcohol consumption, has  contemporary relevance , more than 80 years later.
 
Because of the challenge of interpreting decades-old references, we have asked a leading scholar of Mencken’s writings to add notes where appropriate, to clarify any points of confusion. And now here’s Mencken’s column (with notes added in brackets), in its entirety:
  
For a little while in my teenage years, my friends and I drank alcohol. It was fun. I have some fond memories of us all being silly together. I think those moments of</p><p>5 0.98049319 <a title="2072-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-12-Some_thoughts_on_academic_cheating%2C_inspired_by_Frey%2C_Wegman%2C_Fischer%2C_Hauser%2C_Stapel.html">901 andrew gelman stats-2011-09-12-Some thoughts on academic cheating, inspired by Frey, Wegman, Fischer, Hauser, Stapel</a></p>
<p>Introduction: As regular readers of this blog are aware, I am fascinated by academic and scientific cheating and the  excuses  people give for it.
 
 Bruno Frey  and colleagues published a single article (with only minor variants) in five different major journals, and these articles did not cite each other.  And there have been several other cases of his self-plagiarism (see  this review  from Olaf Storbeck).  I do not mind the general practice of repeating oneself for different audiences—in the social sciences, we call this  Arrow’s Theorem —but in this case Frey seems to have gone a bit too far.  Blogger Economic Logic has  looked into  this and concluded that this sort of common practice is standard in “the context of the German(-speaking) academic environment,” and what sets Frey apart is not his self-plagiarism or even his brazenness but rather his practice of doing it in high-visibility journals.  Economic Logic writes that “[Frey's] contribution is pedagogical, he found a good and interesting</p><p>6 0.98004353 <a title="2072-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-20-NYT_%28non%29-retraction_watch.html">2107 andrew gelman stats-2013-11-20-NYT (non)-retraction watch</a></p>
<p>7 0.979918 <a title="2072-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-09-R_on_the_cloud.html">793 andrew gelman stats-2011-07-09-R on the cloud</a></p>
<p>8 0.97924358 <a title="2072-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-23-No_one_knows_what_it%E2%80%99s_like_to_be_the_bad_man.html">1588 andrew gelman stats-2012-11-23-No one knows what it’s like to be the bad man</a></p>
<p>9 0.97912377 <a title="2072-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-04-Does_it_matter_that_a_sample_is_unrepresentative%3F__It_depends_on_the_size_of_the_treatment_interactions.html">2008 andrew gelman stats-2013-09-04-Does it matter that a sample is unrepresentative?  It depends on the size of the treatment interactions</a></p>
<p>10 0.97910911 <a title="2072-lda-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-04-Literal_vs._rhetorical.html">2233 andrew gelman stats-2014-03-04-Literal vs. rhetorical</a></p>
<p>11 0.97893286 <a title="2072-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-03-Evaluating_evidence_from_published_research.html">2006 andrew gelman stats-2013-09-03-Evaluating evidence from published research</a></p>
<p>12 0.97876078 <a title="2072-lda-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-12-More_on_publishing_in_journals.html">2245 andrew gelman stats-2014-03-12-More on publishing in journals</a></p>
<p>13 0.97861171 <a title="2072-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-27-Historian_and_journalist_slug_it_out.html">1030 andrew gelman stats-2011-11-27-Historian and journalist slug it out</a></p>
<p>14 0.97843862 <a title="2072-lda-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-06-How_much_time_%28if_any%29_should_we_spend_criticizing_research_that%E2%80%99s_fraudulent%2C_crappy%2C_or_just_plain_pointless%3F.html">2235 andrew gelman stats-2014-03-06-How much time (if any) should we spend criticizing research that’s fraudulent, crappy, or just plain pointless?</a></p>
<p>15 0.97839594 <a title="2072-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-22-That_claim_that_students_whose_parents_pay_for_more_of_college_get_worse_grades.html">1688 andrew gelman stats-2013-01-22-That claim that students whose parents pay for more of college get worse grades</a></p>
<p>16 0.97809625 <a title="2072-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-05-A_locally_organized_online_BDA_course_on_G%2B_hangout%3F.html">2009 andrew gelman stats-2013-09-05-A locally organized online BDA course on G+ hangout?</a></p>
<p>17 0.97760439 <a title="2072-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-12-Niall_Ferguson%2C_the_John_Yoo_line%2C_and_the_paradox_of_influence.html">1493 andrew gelman stats-2012-09-12-Niall Ferguson, the John Yoo line, and the paradox of influence</a></p>
<p>18 0.97756898 <a title="2072-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-22-Battle_of_the_Repo_Man_quotes%3A__Reid_Hastie%E2%80%99s_turn.html">1336 andrew gelman stats-2012-05-22-Battle of the Repo Man quotes:  Reid Hastie’s turn</a></p>
<p>19 0.97729826 <a title="2072-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-22-More_Pinker_Pinker_Pinker.html">1635 andrew gelman stats-2012-12-22-More Pinker Pinker Pinker</a></p>
<p>20 0.97721165 <a title="2072-lda-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-26-Philosophy_and_the_practice_of_Bayesian_statistics.html">110 andrew gelman stats-2010-06-26-Philosophy and the practice of Bayesian statistics</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
