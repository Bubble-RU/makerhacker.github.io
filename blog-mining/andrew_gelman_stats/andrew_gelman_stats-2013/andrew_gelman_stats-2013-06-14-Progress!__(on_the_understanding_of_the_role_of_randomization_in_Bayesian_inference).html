<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1898 andrew gelman stats-2013-06-14-Progress!  (on the understanding of the role of randomization in Bayesian inference)</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-1898" href="#">andrew_gelman_stats-2013-1898</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1898 andrew gelman stats-2013-06-14-Progress!  (on the understanding of the role of randomization in Bayesian inference)</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-1898-html" href="http://andrewgelman.com/2013/06/14/progress-on-the-understanding-of-the-role-of-randomization-in-bayesian-inference/">html</a></p><p>Introduction: Leading theoretical statistician Larry Wassserman  in 2008 :  
  
Some of the greatest contributions of statistics to science involve adding additional randomness and leveraging that randomness. Examples are randomized experiments, permutation tests, cross-validation and data-splitting. These are unabashedly frequentist ideas and, while one can strain to fit them into a Bayesian framework, they don’t really have a place in Bayesian inference. The fact that Bayesian methods do not naturally accommodate such a powerful set of statistical ideas seems like a serious deficiency.
  
To which I responded on the second-to-last paragraph of page 8  here .
 
Larry Wasserman in  2013 :
  
Some people say that there is no role for randomization in Bayesian inference. In other words, the randomization mechanism plays no role in Bayes’ theorem. But this is not really true. Without randomization, we can indeed derive a posterior for theta but it is highly sensitive to the prior. This is just a restat</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Leading theoretical statistician Larry Wassserman  in 2008 :      Some of the greatest contributions of statistics to science involve adding additional randomness and leveraging that randomness. [sent-1, score-0.501]
</p><p>2 Examples are randomized experiments, permutation tests, cross-validation and data-splitting. [sent-2, score-0.173]
</p><p>3 These are unabashedly frequentist ideas and, while one can strain to fit them into a Bayesian framework, they don’t really have a place in Bayesian inference. [sent-3, score-0.388]
</p><p>4 The fact that Bayesian methods do not naturally accommodate such a powerful set of statistical ideas seems like a serious deficiency. [sent-4, score-0.376]
</p><p>5 To which I responded on the second-to-last paragraph of page 8  here . [sent-5, score-0.065]
</p><p>6 Larry Wasserman in  2013 :    Some people say that there is no role for randomization in Bayesian inference. [sent-6, score-0.547]
</p><p>7 In other words, the randomization mechanism plays no role in Bayes’ theorem. [sent-7, score-0.635]
</p><p>8 Without randomization, we can indeed derive a posterior for theta but it is highly sensitive to the prior. [sent-9, score-0.776]
</p><p>9 With randomization, the posterior is much less sensitive to the prior. [sent-11, score-0.486]
</p><p>10 And I think most practical Bayesians would consider it valuable to increase the robustness of the posterior. [sent-12, score-0.25]
</p><p>11 I completely agree with 2013 Larry (and it’s what we say in our Bayesian book, following the ideas of Rubin and others). [sent-14, score-0.194]
</p><p>12 And, indeed, our models typically assume simple random sampling within poststratification cells. [sent-17, score-0.431]
</p><p>13 Such models are never correct (even if the survey is conducted by a probability sampling design, nonresponse will not be random) but it’s a useful starting point that we try to approximate in many of our designs. [sent-18, score-0.391]
</p><p>14 In other settings, we simply don’t have random sampling or random assignment, and then, indeed, our inferences can be more sensitive to our assumptions. [sent-19, score-0.923]
</p><p>15 The only place I’d disagree with Larry is when he writes “sensitive to the prior,” I’d say, “sensitive to the model,” because the data model comes into play too, not just the prior distribution (that is, the model for the parameters). [sent-20, score-0.323]
</p><p>16 Beyond appreciating Larry’s recognition of this particular issue, I find his larger point interesting, that we add noise in different ways to achieve robustness or computational efficiency. [sent-23, score-0.592]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sensitive', 0.384), ('randomization', 0.361), ('larry', 0.35), ('bayesian', 0.194), ('random', 0.189), ('robustness', 0.18), ('sampling', 0.161), ('wassserman', 0.124), ('ideas', 0.123), ('indeed', 0.121), ('appreciating', 0.117), ('leveraging', 0.117), ('role', 0.115), ('accommodate', 0.108), ('strain', 0.105), ('permutation', 0.102), ('posterior', 0.102), ('derive', 0.093), ('randomness', 0.091), ('greatest', 0.091), ('place', 0.09), ('nonresponse', 0.09), ('wasserman', 0.089), ('assignment', 0.087), ('prior', 0.085), ('achieve', 0.084), ('plays', 0.083), ('poststratification', 0.081), ('recognition', 0.079), ('mechanism', 0.076), ('theta', 0.076), ('efficiency', 0.075), ('model', 0.074), ('naturally', 0.073), ('powerful', 0.072), ('involve', 0.072), ('say', 0.071), ('randomized', 0.071), ('approximate', 0.071), ('valuable', 0.07), ('bayesians', 0.07), ('frequentist', 0.07), ('conducted', 0.069), ('noise', 0.069), ('contributions', 0.066), ('rubin', 0.066), ('responded', 0.065), ('adding', 0.064), ('framework', 0.063), ('computational', 0.063)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="1898-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-14-Progress%21__%28on_the_understanding_of_the_role_of_randomization_in_Bayesian_inference%29.html">1898 andrew gelman stats-2013-06-14-Progress!  (on the understanding of the role of randomization in Bayesian inference)</a></p>
<p>Introduction: Leading theoretical statistician Larry Wassserman  in 2008 :  
  
Some of the greatest contributions of statistics to science involve adding additional randomness and leveraging that randomness. Examples are randomized experiments, permutation tests, cross-validation and data-splitting. These are unabashedly frequentist ideas and, while one can strain to fit them into a Bayesian framework, they don’t really have a place in Bayesian inference. The fact that Bayesian methods do not naturally accommodate such a powerful set of statistical ideas seems like a serious deficiency.
  
To which I responded on the second-to-last paragraph of page 8  here .
 
Larry Wasserman in  2013 :
  
Some people say that there is no role for randomization in Bayesian inference. In other words, the randomization mechanism plays no role in Bayes’ theorem. But this is not really true. Without randomization, we can indeed derive a posterior for theta but it is highly sensitive to the prior. This is just a restat</p><p>2 0.27382606 <a title="1898-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-23-Larry_Wasserman%E2%80%99s_statistics_blog.html">1389 andrew gelman stats-2012-06-23-Larry Wasserman’s statistics blog</a></p>
<p>Introduction: Larry Wasserman, a leading theoretical statistician and generally thoughtful guy, has started  a blog  on . . . theoretical statistics!  Good stuff, and readers of this blog should enjoy the different perspective that Larry offers.
 
 Here  are some earlier references to Larry on this blog, and  here’s  a discussion that gives a sense of our different (but not extremely different) attitudes about statistical methods.</p><p>3 0.23678873 <a title="1898-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-07-Assumptions_vs._conditions%2C_part_2.html">603 andrew gelman stats-2011-03-07-Assumptions vs. conditions, part 2</a></p>
<p>Introduction: In response to the  discussion  of his remarks on assumptions vs. conditions, Jeff Witmer  writes :
  
If [certain conditions hold] , then the t-test p-value gives a remarkably good approximation to “the real thing” — namely the randomization reference p-value. . . .





I [Witmer] make assumptions about conditions that I cannot check, e.g., that the data arose from a random sample. Of course, just as there is no such thing as a normal population, there is no such thing as a random sample.
  
I disagree strongly with both the above paragraphs!  I say this not to pick a fight with Jeff Witmer but to illustrate how, in statistics, even the most basic points that people take for granted, can’t be.
 
Let’s take the claims in order:
 
1.   The purpose of a t test is to approximate the randomization p-value.   Not to me.  In my world, the purpose of t tests and intervals is to summarize uncertainty in estimates and comparisons.  I don’t care about a p-value and almost certainly don’t care a</p><p>4 0.23505932 <a title="1898-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-13-Philosophy_of_Bayesian_statistics%3A__my_reactions_to_Wasserman.html">1165 andrew gelman stats-2012-02-13-Philosophy of Bayesian statistics:  my reactions to Wasserman</a></p>
<p>Introduction: Continuing with  my discussion of the articles in the special issue  of the journal Rationality, Markets and Morals on the philosophy of Bayesian statistics:
 
   
 
Larry Wasserman, “Low Assumptions, High Dimensions”:
 
This article was refreshing to me because it was so different from anything I’ve seen before.  Larry works in a statistics department and I work in a statistics department but there’s so little overlap in what we do.  Larry and I both work in high dimesions (maybe his dimensions are higher than mine, but a few thousand dimensions seems like a lot to me!), but there the similarity ends.  His article is all about using few to no assumptions, while I use assumptions all the time.  Here’s an example.  Larry writes:
  
P. Laurie Davies (and his co-workers) have written several interesting papers where probability models, at least in the sense that we usually use them, are eliminated. Data are treated as deterministic. One then looks for adequate models rather than true mode</p><p>5 0.22002678 <a title="1898-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-06-Yes%2C_checking_calibration_of_probability_forecasts_is_part_of_Bayesian_statistics.html">1610 andrew gelman stats-2012-12-06-Yes, checking calibration of probability forecasts is part of Bayesian statistics</a></p>
<p>Introduction: Yes, checking calibration of probability forecasts is part of Bayesian statistics.  At the end of this post are three figures from Chapter 1 of Bayesian Data Analysis illustrating empirical evaluation of forecasts.
 
But first the background.  Why am I bringing this up now?  It’s because of something Larry Wasserman  wrote the other day : 
  
  
One of the striking facts about [baseball/political forecaster Nate Silver's recent] book is the emphasis the Silver places on frequency calibration. . . . Have no doubt about it: Nate Silver is a frequentist. For example, he says:

 
One of the most important tests of a forecast — I would argue that it is the single most important one — is called calibration. Out of all the times you said there was a 40 percent chance of rain, how often did rain actually occur? If over the long run, it really did rain about 40 percent of the time, that means your forecasts were well calibrated.
 
  
I had some discussion with Larry in the comments section of h</p><p>6 0.20858149 <a title="1898-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-03-Statistical_methods_that_work_in_some_settings_but_not_others.html">1560 andrew gelman stats-2012-11-03-Statistical methods that work in some settings but not others</a></p>
<p>7 0.19378236 <a title="1898-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-09-Coming_to_agreement_on_philosophy_of_statistics.html">1205 andrew gelman stats-2012-03-09-Coming to agreement on philosophy of statistics</a></p>
<p>8 0.18786527 <a title="1898-tfidf-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Useful_models%2C_model_checking%2C_and_external_validation%3A__a_mini-discussion.html">244 andrew gelman stats-2010-08-30-Useful models, model checking, and external validation:  a mini-discussion</a></p>
<p>9 0.1849169 <a title="1898-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-17-Statistics_in_a_world_where_nothing_is_random.html">1628 andrew gelman stats-2012-12-17-Statistics in a world where nothing is random</a></p>
<p>10 0.17793743 <a title="1898-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>11 0.1676195 <a title="1898-tfidf-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-22-Philosophy_of_Bayes_and_non-Bayes%3A__A_dialogue_with_Deborah_Mayo.html">291 andrew gelman stats-2010-09-22-Philosophy of Bayes and non-Bayes:  A dialogue with Deborah Mayo</a></p>
<p>12 0.16585357 <a title="1898-tfidf-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-23-Bayesian_adaptive_methods_for_clinical_trials.html">427 andrew gelman stats-2010-11-23-Bayesian adaptive methods for clinical trials</a></p>
<p>13 0.16311786 <a title="1898-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>14 0.16240725 <a title="1898-tfidf-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-23-Thinking_of_doing_a_list_experiment%3F__Here%E2%80%99s_a_list_of_reasons_why_you_should_think_again.html">2303 andrew gelman stats-2014-04-23-Thinking of doing a list experiment?  Here’s a list of reasons why you should think again</a></p>
<p>15 0.15424857 <a title="1898-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-15-Bayesian_statistical_pragmatism.html">662 andrew gelman stats-2011-04-15-Bayesian statistical pragmatism</a></p>
<p>16 0.15032172 <a title="1898-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-28-Using_randomized_incentives_as_an_instrument_for_survey_nonresponse%3F.html">2152 andrew gelman stats-2013-12-28-Using randomized incentives as an instrument for survey nonresponse?</a></p>
<p>17 0.14841338 <a title="1898-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-02-Fighting_a_losing_battle.html">1518 andrew gelman stats-2012-10-02-Fighting a losing battle</a></p>
<p>18 0.14651935 <a title="1898-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<p>19 0.14529587 <a title="1898-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-23-A_statistical_version_of_Arrow%E2%80%99s_paradox.html">586 andrew gelman stats-2011-02-23-A statistical version of Arrow’s paradox</a></p>
<p>20 0.14439009 <a title="1898-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.213), (1, 0.206), (2, -0.036), (3, 0.039), (4, -0.108), (5, 0.031), (6, -0.022), (7, 0.064), (8, 0.038), (9, -0.102), (10, -0.001), (11, -0.094), (12, -0.024), (13, 0.059), (14, 0.019), (15, -0.027), (16, 0.002), (17, 0.024), (18, -0.006), (19, -0.001), (20, -0.007), (21, -0.018), (22, -0.007), (23, 0.077), (24, 0.005), (25, 0.007), (26, -0.042), (27, 0.074), (28, 0.052), (29, 0.064), (30, -0.051), (31, -0.012), (32, -0.029), (33, 0.006), (34, -0.022), (35, 0.021), (36, -0.05), (37, -0.014), (38, -0.036), (39, 0.007), (40, 0.078), (41, 0.009), (42, 0.029), (43, -0.021), (44, -0.056), (45, -0.036), (46, 0.029), (47, 0.055), (48, -0.049), (49, 0.005)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97283572 <a title="1898-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-14-Progress%21__%28on_the_understanding_of_the_role_of_randomization_in_Bayesian_inference%29.html">1898 andrew gelman stats-2013-06-14-Progress!  (on the understanding of the role of randomization in Bayesian inference)</a></p>
<p>Introduction: Leading theoretical statistician Larry Wassserman  in 2008 :  
  
Some of the greatest contributions of statistics to science involve adding additional randomness and leveraging that randomness. Examples are randomized experiments, permutation tests, cross-validation and data-splitting. These are unabashedly frequentist ideas and, while one can strain to fit them into a Bayesian framework, they don’t really have a place in Bayesian inference. The fact that Bayesian methods do not naturally accommodate such a powerful set of statistical ideas seems like a serious deficiency.
  
To which I responded on the second-to-last paragraph of page 8  here .
 
Larry Wasserman in  2013 :
  
Some people say that there is no role for randomization in Bayesian inference. In other words, the randomization mechanism plays no role in Bayes’ theorem. But this is not really true. Without randomization, we can indeed derive a posterior for theta but it is highly sensitive to the prior. This is just a restat</p><p>2 0.77692068 <a title="1898-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-03-Statistical_methods_that_work_in_some_settings_but_not_others.html">1560 andrew gelman stats-2012-11-03-Statistical methods that work in some settings but not others</a></p>
<p>Introduction: David Hogg pointed me to  this post  by Larry Wasserman:
  
1. The Horwitz-Thompson estimator    satisfies the following condition: for every   ,

  


  where   — the parameter space — is the set of all functions  . (There are practical improvements to the Horwitz-Thompson estimator that we discussed in our earlier posts but we won’t revisit those here.)


2. A Bayes estimator requires a prior   for  . In general, if   is not a function of   then (1) will not hold. . . .


3. If you let   be a function if  , (1) still, in general, does not hold.


4. If you make   a function if   in just the right way, then (1) will hold. . . . There is nothing wrong with doing this, but in our opinion this is not in the spirit of Bayesian inference. . . .


7. This example is only meant to show that Bayesian estimators do not necessarily have good frequentist properties. This should not be surprising. There is no reason why we should in general expect a Bayesian method to have a frequentist property</p><p>3 0.77439749 <a title="1898-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-13-Philosophy_of_Bayesian_statistics%3A__my_reactions_to_Wasserman.html">1165 andrew gelman stats-2012-02-13-Philosophy of Bayesian statistics:  my reactions to Wasserman</a></p>
<p>Introduction: Continuing with  my discussion of the articles in the special issue  of the journal Rationality, Markets and Morals on the philosophy of Bayesian statistics:
 
   
 
Larry Wasserman, “Low Assumptions, High Dimensions”:
 
This article was refreshing to me because it was so different from anything I’ve seen before.  Larry works in a statistics department and I work in a statistics department but there’s so little overlap in what we do.  Larry and I both work in high dimesions (maybe his dimensions are higher than mine, but a few thousand dimensions seems like a lot to me!), but there the similarity ends.  His article is all about using few to no assumptions, while I use assumptions all the time.  Here’s an example.  Larry writes:
  
P. Laurie Davies (and his co-workers) have written several interesting papers where probability models, at least in the sense that we usually use them, are eliminated. Data are treated as deterministic. One then looks for adequate models rather than true mode</p><p>4 0.77323073 <a title="1898-lsi-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-22-Philosophy_of_Bayes_and_non-Bayes%3A__A_dialogue_with_Deborah_Mayo.html">291 andrew gelman stats-2010-09-22-Philosophy of Bayes and non-Bayes:  A dialogue with Deborah Mayo</a></p>
<p>Introduction: I sent Deborah Mayo a link to  my paper  with Cosma Shalizi on the philosophy of statistics, and she sent me the link to this conference which unfortunately already occurred.  (It’s too bad, because I’d have liked to have been there.)  I summarized my philosophy as follows:
  
I am highly sympathetic to the approach of Lakatos (or of Popper, if you consider Lakatos’s “Popper_2″ to be a reasonable simulation of the true Popperism), in that (a) I view statistical models as being built within theoretical structures, and (b) I see the checking and refutation of models to be a key part of scientific progress.  A big problem I have with mainstream Bayesianism is its “inductivist” view that science can operate completely smoothly with posterior updates:  the idea that new data causes us to increase the posterior probability of good models and decrease the posterior probability of bad models.  I don’t buy that:  I see models as ever-changing entities that are flexible and can be patched and ex</p><p>5 0.77250022 <a title="1898-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-06-Yes%2C_checking_calibration_of_probability_forecasts_is_part_of_Bayesian_statistics.html">1610 andrew gelman stats-2012-12-06-Yes, checking calibration of probability forecasts is part of Bayesian statistics</a></p>
<p>Introduction: Yes, checking calibration of probability forecasts is part of Bayesian statistics.  At the end of this post are three figures from Chapter 1 of Bayesian Data Analysis illustrating empirical evaluation of forecasts.
 
But first the background.  Why am I bringing this up now?  It’s because of something Larry Wasserman  wrote the other day : 
  
  
One of the striking facts about [baseball/political forecaster Nate Silver's recent] book is the emphasis the Silver places on frequency calibration. . . . Have no doubt about it: Nate Silver is a frequentist. For example, he says:

 
One of the most important tests of a forecast — I would argue that it is the single most important one — is called calibration. Out of all the times you said there was a 40 percent chance of rain, how often did rain actually occur? If over the long run, it really did rain about 40 percent of the time, that means your forecasts were well calibrated.
 
  
I had some discussion with Larry in the comments section of h</p><p>6 0.77091277 <a title="1898-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<p>7 0.76555598 <a title="1898-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-31-What_is_a_Bayesian%3F.html">1438 andrew gelman stats-2012-07-31-What is a Bayesian?</a></p>
<p>8 0.75502467 <a title="1898-lsi-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-28-Bayesian_nonparametric_weighted_sampling_inference.html">2351 andrew gelman stats-2014-05-28-Bayesian nonparametric weighted sampling inference</a></p>
<p>9 0.75073767 <a title="1898-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-15-Bayesian_statistical_pragmatism.html">662 andrew gelman stats-2011-04-15-Bayesian statistical pragmatism</a></p>
<p>10 0.75015783 <a title="1898-lsi-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-04-Rob_Kass_on_statistical_pragmatism%2C_and_my_reactions.html">317 andrew gelman stats-2010-10-04-Rob Kass on statistical pragmatism, and my reactions</a></p>
<p>11 0.74649978 <a title="1898-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-17-Christian_Robert_on_the_Jeffreys-Lindley_paradox%3B_more_generally%2C_it%E2%80%99s_good_news_when_philosophical_arguments_can_be_transformed_into_technical_modeling_issues.html">2027 andrew gelman stats-2013-09-17-Christian Robert on the Jeffreys-Lindley paradox; more generally, it’s good news when philosophical arguments can be transformed into technical modeling issues</a></p>
<p>12 0.73766148 <a title="1898-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-24-Untangling_the_Jeffreys-Lindley_paradox.html">1182 andrew gelman stats-2012-02-24-Untangling the Jeffreys-Lindley paradox</a></p>
<p>13 0.73713303 <a title="1898-lsi-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-23-Bayesian_adaptive_methods_for_clinical_trials.html">427 andrew gelman stats-2010-11-23-Bayesian adaptive methods for clinical trials</a></p>
<p>14 0.73248112 <a title="1898-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>15 0.72636682 <a title="1898-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-10-I_don%E2%80%99t_like_this_cartoon.html">1572 andrew gelman stats-2012-11-10-I don’t like this cartoon</a></p>
<p>16 0.72335315 <a title="1898-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-09-Coming_to_agreement_on_philosophy_of_statistics.html">1205 andrew gelman stats-2012-03-09-Coming to agreement on philosophy of statistics</a></p>
<p>17 0.70717537 <a title="1898-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-23-Validation_of_Software_for_Bayesian_Models_Using_Posterior_Quantiles.html">1868 andrew gelman stats-2013-05-23-Validation of Software for Bayesian Models Using Posterior Quantiles</a></p>
<p>18 0.70669633 <a title="1898-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-07-Philosophy_of_Bayesian_statistics%3A_my_reactions_to_Hendry.html">1157 andrew gelman stats-2012-02-07-Philosophy of Bayesian statistics: my reactions to Hendry</a></p>
<p>19 0.70363867 <a title="1898-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-11-Gelman_on_Hennig_on_Gelman_on_Bayes.html">1208 andrew gelman stats-2012-03-11-Gelman on Hennig on Gelman on Bayes</a></p>
<p>20 0.70150119 <a title="1898-lsi-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-22-Spell-checking_example_demonstrates_key_aspects_of_Bayesian_data_analysis.html">2182 andrew gelman stats-2014-01-22-Spell-checking example demonstrates key aspects of Bayesian data analysis</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(15, 0.019), (16, 0.079), (21, 0.018), (24, 0.211), (25, 0.012), (36, 0.186), (82, 0.01), (84, 0.01), (86, 0.029), (89, 0.01), (98, 0.025), (99, 0.293)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98694712 <a title="1898-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-Stan_is_fast.html">1476 andrew gelman stats-2012-08-30-Stan is fast</a></p>
<p>Introduction: 10,000 iterations for 4 chains on the (precompiled) efficiently-parameterized 8-schools model: 
   
 > date () 
[1] "Thu Aug 30 22:12:53 2012" 
> fit3 <- stan (fit=fit2, data = schools_dat, iter = 1e4, n_chains = 4) 
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). 
Iteration: 10000 / 10000 [100%]  (Sampling) 
 
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2). 
Iteration: 10000 / 10000 [100%]  (Sampling)
 
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3). 
Iteration: 10000 / 10000 [100%]  (Sampling)
 
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4). 
Iteration: 10000 / 10000 [100%]  (Sampling)
 
> date () 
[1] "Thu Aug 30 22:12:55 2012" 
> print (fit3) 
Inference for Stan model: anon_model. 
4 chains: each with iter=10000; warmup=5000; thin=1; 10000 iterations saved.
 
         mean se_mean  sd  2.5%  25%  50%  75% 97.5% n_eff Rhat 
mu        8.0     0.1 5.1  -2.0  4.7  8.0 11.3  18.4  4032    1 
tau       6.7     0.1 5.6   0.3  2.5  5.4  9.3  21.2  2958    1 
eta[1]    0.4     0.0 0.9  -1.5 -0</p><p>2 0.95640087 <a title="1898-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-31-Watercolor_regression.html">1478 andrew gelman stats-2012-08-31-Watercolor regression</a></p>
<p>Introduction: Solomon Hsiang writes:
  
Two small follow-ups based on the  discussion  (the second/bigger one is to address your comment about the 95% CI edges).


1. I realized that if we plot the confidence intervals as a solid color that fades (eg. using the “fixed ink” scheme from before) we can make sure the regression line also has heightened visual weight where confidence is high by plotting the line white. This makes the contrast (and thus visual weight) between the regression line and the CI highest when the CI is narrow and dark. As the CI fade near the edges, so does the contrast with the regression line. This is a small adjustment, but I like it because it is so simple and it makes the graph much nicer. (see “visually_weighted_fill_reverse” attached). My posted code has been updated to do this automatically.


2. You and your readers didn’t like that the edges of the filled CI were so sharp and arbitrary. But I didn’t like that the contrast between the spaghetti lines and the background</p><p>3 0.95467246 <a title="1898-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-10-%E2%80%9CProposition_and_experiment%E2%80%9D.html">1797 andrew gelman stats-2013-04-10-“Proposition and experiment”</a></p>
<p>Introduction: Anna Lena Phillips  writes :
  
I. Many people will not, of their own accord, look at a poem.


II. Millions of people will, of their own accord, spend lots and lots of time looking at photographs of cats.


III. Therefore, earlier this year, I concluded that the best strategy for increasing the number of viewers for poems would be to print them on top of photographs of cats.


IV. I happen to like looking at both poems and cats.


V. So this is, for me, a win-win situation.


VI. Fortunately, my own cat is a patient model, and (if I am to be believed) quite photogenic.


VII. The aforementioned cat is Tisko Tansi, small hero.


VII. Thus I present to you (albeit in digital rather than physical form) an Endearments broadside, featuring a poem that originally appeared in BlazeVOX spring 2011.


VIII. If you want to share a copy of this image, please ask first. If you want a real copy, you can ask about that too.
  
She follows up with an image of a cat, on which is superimposed a short</p><p>same-blog 4 0.9516018 <a title="1898-lda-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-14-Progress%21__%28on_the_understanding_of_the_role_of_randomization_in_Bayesian_inference%29.html">1898 andrew gelman stats-2013-06-14-Progress!  (on the understanding of the role of randomization in Bayesian inference)</a></p>
<p>Introduction: Leading theoretical statistician Larry Wassserman  in 2008 :  
  
Some of the greatest contributions of statistics to science involve adding additional randomness and leveraging that randomness. Examples are randomized experiments, permutation tests, cross-validation and data-splitting. These are unabashedly frequentist ideas and, while one can strain to fit them into a Bayesian framework, they don’t really have a place in Bayesian inference. The fact that Bayesian methods do not naturally accommodate such a powerful set of statistical ideas seems like a serious deficiency.
  
To which I responded on the second-to-last paragraph of page 8  here .
 
Larry Wasserman in  2013 :
  
Some people say that there is no role for randomization in Bayesian inference. In other words, the randomization mechanism plays no role in Bayes’ theorem. But this is not really true. Without randomization, we can indeed derive a posterior for theta but it is highly sensitive to the prior. This is just a restat</p><p>5 0.94973975 <a title="1898-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-02-Obama_and_Reagan%2C_sitting_in_a_tree%2C_etc..html">551 andrew gelman stats-2011-02-02-Obama and Reagan, sitting in a tree, etc.</a></p>
<p>Introduction: I saw this picture staring at me from the newsstand the other day:
 
 
 
 Here’s  the accompanying article, by Michael Scherer and Michael Duffy, which echoes some of the points I made  a few months ago , following the midterm election:
  
Why didn’t Obama do a better job of leveling with the American people? In his first months in office, why didn’t he anticipate the example of the incoming British government and warn people of economic blood, sweat, and tears? Why did his economic team release overly-optimistic graphs such as shown here? Wouldn’t it have been better to have set low expectations and then exceed them, rather than the reverse?


I don’t know, but here’s my theory. When Obama came into office, I imagine one of his major goals was to avoid repeating the experiences of Bill Clinton and Jimmy Carter in their first two years.


Clinton, you may recall, was elected with less then 50% of the vote, was never given the respect of a “mandate” by congressional Republicans, wasted</p><p>6 0.94415021 <a title="1898-lda-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-25-Who_gets_wedding_announcements_in_the_Times%3F.html">370 andrew gelman stats-2010-10-25-Who gets wedding announcements in the Times?</a></p>
<p>7 0.9414652 <a title="1898-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-15-The_two_faces_of_Erving_Goffman%3A__Subtle_observer_of_human_interactions%2C_and_Smug_organzation_man.html">415 andrew gelman stats-2010-11-15-The two faces of Erving Goffman:  Subtle observer of human interactions, and Smug organzation man</a></p>
<p>8 0.94071639 <a title="1898-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-01-Arrow%E2%80%99s_theorem_update.html">883 andrew gelman stats-2011-09-01-Arrow’s theorem update</a></p>
<p>9 0.93481725 <a title="1898-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-02-Information_is_good.html">176 andrew gelman stats-2010-08-02-Information is good</a></p>
<p>10 0.93346834 <a title="1898-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-08-Of_parsing_and_chess.html">1847 andrew gelman stats-2013-05-08-Of parsing and chess</a></p>
<p>11 0.93145144 <a title="1898-lda-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-20-%E2%80%9CPeople_with_an_itch_to_scratch%E2%80%9D.html">101 andrew gelman stats-2010-06-20-“People with an itch to scratch”</a></p>
<p>12 0.92976344 <a title="1898-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-11-Actually%2C_I_have_no_problem_with_this_graph.html">1851 andrew gelman stats-2013-05-11-Actually, I have no problem with this graph</a></p>
<p>13 0.92823708 <a title="1898-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-17-NSF_program_%E2%80%9Cto_support_analytic_and_methodological_research_in_support_of_its_surveys%E2%80%9D.html">1217 andrew gelman stats-2012-03-17-NSF program “to support analytic and methodological research in support of its surveys”</a></p>
<p>14 0.92322063 <a title="1898-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-26-Graphs_showing_regression_uncertainty%3A__the_code%21.html">1470 andrew gelman stats-2012-08-26-Graphs showing regression uncertainty:  the code!</a></p>
<p>15 0.91063088 <a title="1898-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-08-A_Bayesian_approach_for_peer-review_panels%3F__and_a_speculation_about_Bruno_Frey.html">2055 andrew gelman stats-2013-10-08-A Bayesian approach for peer-review panels?  and a speculation about Bruno Frey</a></p>
<p>16 0.90569675 <a title="1898-lda-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-07-My_recent_debugging_experience.html">2161 andrew gelman stats-2014-01-07-My recent debugging experience</a></p>
<p>17 0.90548623 <a title="1898-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-16-Stantastic%21.html">1580 andrew gelman stats-2012-11-16-Stantastic!</a></p>
<p>18 0.90176499 <a title="1898-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-03-Boot.html">1881 andrew gelman stats-2013-06-03-Boot</a></p>
<p>19 0.90037715 <a title="1898-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-18-What%E2%80%99s_my_Kasparov_number%3F.html">2105 andrew gelman stats-2013-11-18-What’s my Kasparov number?</a></p>
<p>20 0.89943731 <a title="1898-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-04-Cash_in%2C_cash_out_graph.html">502 andrew gelman stats-2011-01-04-Cash in, cash out graph</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
