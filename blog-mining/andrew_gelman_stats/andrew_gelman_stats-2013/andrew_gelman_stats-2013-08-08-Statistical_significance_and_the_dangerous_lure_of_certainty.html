<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1974 andrew gelman stats-2013-08-08-Statistical significance and the dangerous lure of certainty</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-1974" href="#">andrew_gelman_stats-2013-1974</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1974 andrew gelman stats-2013-08-08-Statistical significance and the dangerous lure of certainty</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-1974-html" href="http://andrewgelman.com/2013/08/08/statistical-significance-and-the-dangerous-lure-of-certainty/">html</a></p><p>Introduction: In a discussion of some of the recent controversy over promiscuously statistically-significant science,  Jeff Leek  Rafael Irizarry  points out  there is a tradeoff between stringency and discovery and suggests that raising the bar of statistical significance (for example, to the .01 or .001 level instead of the conventional .05) will reduce the noise level but will also reduce the rate of identification of actual discoveries.
 
I agree.  But I should clarify that when I criticize a claim of statistical significance, arguing that the claimed “p less than .05″ could easily occur under the null hypothesis, given that the hypothesis test that is chosen is contingent on the data (see examples  here  of clothing and menstrual cycle, arm circumference and political attitudes, and ESP), I am  not  recommending a switch to a more stringent p-value threshold.  Rather, I would prefer p-values not to be used as a threshold for publication at all.
 
Here’s my point:  The question is not  whether</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In a discussion of some of the recent controversy over promiscuously statistically-significant science,  Jeff Leek  Rafael Irizarry  points out  there is a tradeoff between stringency and discovery and suggests that raising the bar of statistical significance (for example, to the . [sent-1, score-0.68]
</p><p>2 05) will reduce the noise level but will also reduce the rate of identification of actual discoveries. [sent-4, score-0.37]
</p><p>3 Rather, I would prefer p-values not to be used as a threshold for publication at all. [sent-8, score-0.109]
</p><p>4 )   For most if not all of the studies we’ve been discussing lately, I think the raw data should be published too. [sent-12, score-0.332]
</p><p>5 Sometimes the authors will share their data to people who request, but that shouldn’t even be an issue if the data and survey forms are just posted. [sent-13, score-0.266]
</p><p>6 Assuming that both of these are limited quantities in some flexible way (in the same way that the government cannot simply print unlimited amounts of money and that banks cannot simply give out unlimited amounts of government-backed loans), some selection is necessary. [sent-16, score-0.888]
</p><p>7 I would make that selection based on the quality of the data collection and analysis, the scientific interest of the research, and the importance of the topic—but not on the significance level of the results. [sent-17, score-0.749]
</p><p>8 There are exceptions, of course:  I could imagine a clean, well-defined, internally-replicated study with a surprising effect, where statistical significance would be part of the argument for why to believe it. [sent-18, score-0.342]
</p><p>9 Instead, over and over again we see poorly-measured data with analyses that are iffy or data-dependent. [sent-20, score-0.211]
</p><p>10 Studies such as those should demand our attention because of their data quality or scientific importance, not because they are attention-grabbing and have a p-value of . [sent-21, score-0.295]
</p><p>11 Summary    I think Rafa’s point of the tradeoff between stringency and discovery is important, and I’d like to move this discussion away from p-values and toward concerns of data quality. [sent-23, score-0.546]
</p><p>12 Of course I also think data should be analyzed appropriately. [sent-24, score-0.133]
</p><p>13 For example, with Bem’s ESP study, a proper analysis would  not  pull out some wacky interactions and declare them statistically significant; instead, it would display  all  interactions and show the estimates and uncertainties for all of them. [sent-25, score-0.389]
</p><p>14 Attention and resources are limited so there will always be some sort of selection of what studies get followed up. [sent-27, score-0.458]
</p><p>15 I’d just like to do a different selection than that based on p-values. [sent-28, score-0.162]
</p><p>16 Especially considering all the small-N studies of small effects where any statistical significance is  essentially noise  anyway. [sent-29, score-0.488]
</p><p>17 The dangerous lure of certainty    Regarding the title of this post:  I’m not saying that Rafa is suffering from the dangerous lure of certainty. [sent-30, score-1.323]
</p><p>18 Rather, I’m saying that p-value thresholds are connected to this dangerous lure, present among producers and consumers of science. [sent-31, score-0.322]
</p><p>19 And the lure of certainty even arises in completely non-quantitative studies. [sent-37, score-0.514]
</p><p>20 Consider disgraced primatologist Marc Hauser, who refused to let others look at his data tapes. [sent-38, score-0.29]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lure', 0.392), ('significance', 0.198), ('rafa', 0.196), ('stringency', 0.179), ('dangerous', 0.171), ('selection', 0.162), ('unlimited', 0.151), ('data', 0.133), ('tradeoff', 0.129), ('amounts', 0.128), ('studies', 0.122), ('certainty', 0.122), ('esp', 0.119), ('arxiv', 0.117), ('threshold', 0.109), ('discovery', 0.105), ('noise', 0.099), ('limited', 0.095), ('interactions', 0.092), ('reduce', 0.092), ('importance', 0.089), ('level', 0.087), ('instead', 0.086), ('rafael', 0.084), ('primatologist', 0.084), ('attention', 0.082), ('repositories', 0.081), ('producers', 0.081), ('quality', 0.08), ('followed', 0.079), ('loans', 0.078), ('iffy', 0.078), ('direction', 0.078), ('published', 0.077), ('suffering', 0.075), ('stringent', 0.075), ('clothing', 0.075), ('study', 0.075), ('banks', 0.073), ('disgraced', 0.073), ('hypothesis', 0.072), ('thresholds', 0.07), ('menstrual', 0.07), ('declare', 0.069), ('contingent', 0.069), ('leek', 0.069), ('statistical', 0.069), ('wacky', 0.068), ('recommending', 0.068), ('uncertainties', 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1974-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-08-Statistical_significance_and_the_dangerous_lure_of_certainty.html">1974 andrew gelman stats-2013-08-08-Statistical significance and the dangerous lure of certainty</a></p>
<p>Introduction: In a discussion of some of the recent controversy over promiscuously statistically-significant science,  Jeff Leek  Rafael Irizarry  points out  there is a tradeoff between stringency and discovery and suggests that raising the bar of statistical significance (for example, to the .01 or .001 level instead of the conventional .05) will reduce the noise level but will also reduce the rate of identification of actual discoveries.
 
I agree.  But I should clarify that when I criticize a claim of statistical significance, arguing that the claimed “p less than .05″ could easily occur under the null hypothesis, given that the hypothesis test that is chosen is contingent on the data (see examples  here  of clothing and menstrual cycle, arm circumference and political attitudes, and ESP), I am  not  recommending a switch to a more stringent p-value threshold.  Rather, I would prefer p-values not to be used as a threshold for publication at all.
 
Here’s my point:  The question is not  whether</p><p>2 0.14939621 <a title="1974-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>Introduction: In response to the  discussion  of X and me of his recent  paper , Val Johnson writes:
  
I would like to thank Andrew for forwarding his comments on uniformly most powerful Bayesian tests (UMPBTs) to me and his invitation to respond to them.  I think he  (and also Christian Robert) raise a number of interesting points concerning this new class of Bayesian tests, but I think that they may have confounded several issues that might more usefully be examined separately.


The first issue involves the choice of the Bayesian evidence threshold, gamma, used in rejecting a null hypothesis in favor of an alternative hypothesis.  Andrew objects to the higher values of gamma proposed in my recent PNAS article on grounds that too many important scientific effects would be missed if thresholds of 25-50 were routinely used.  These evidence thresholds correspond roughly to p-values of 0.005; Andrew suggests that evidence thresholds around 5 should continue to be used (gamma=5 corresponds approximate</p><p>3 0.14841069 <a title="1974-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-11-One_more_time_on_that_ESP_study%3A__The_problem_of_overestimates_and_the_shrinkage_solution.html">511 andrew gelman stats-2011-01-11-One more time on that ESP study:  The problem of overestimates and the shrinkage solution</a></p>
<p>Introduction: Benedict Carey  writes  a follow-up article on ESP studies and Bayesian statistics.  ( See here  for my previous thoughts on the topic.)  Everything Carey writes is fine, and he even uses an example I recommended:
  
The statistical approach that has dominated the social sciences for almost a century is called significance testing. The idea is straightforward. A finding from any well-designed study — say, a correlation between a personality trait and the risk of depression — is considered “significant” if its probability of occurring by chance is less than 5 percent.


This arbitrary cutoff makes sense when the effect being studied is a large one — for example, when measuring the so-called Stroop effect. This effect predicts that naming the color of a word is faster and more accurate when the word and color match (“red” in red letters) than when they do not (“red” in blue letters), and is very strong in almost everyone.


“But if the true effect of what you are measuring is small,” sai</p><p>4 0.14533728 <a title="1974-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-06-That_silly_ESP_paper_and_some_silliness_in_a_rebuttal_as_well.html">506 andrew gelman stats-2011-01-06-That silly ESP paper and some silliness in a rebuttal as well</a></p>
<p>Introduction: John Talbott points me to  this , which I briefly  mocked  a couple months ago.  I largely agree with the critics of this research, but I want to reiterate my point from earlier that all the statistical sophistication in the world won’t help you if you’re studying a null effect. This is not to say that the actual effect is zero—who am I to say?—just that the comments about the high-quality statistics in the article don’t say much to me.
 
There’s lots of discussion of the lack of science underlying ESP claims.  I can’t offer anything useful on that account (not being a psychologist, I could imagine all sorts of stories about brain waves or whatever), but I would like to point out something that usually doesn’t seem to get mentioned in these discussions, which is that lots of people  want  to believe in ESP.  After all, it would be cool to read minds.  (It wouldn’t be so cool, maybe, if other people could read your mind and you couldn’t read theirs, but I suspect most people don’t think</p><p>5 0.1368479 <a title="1974-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-31-How_to_fix_the_tabloids%3F__Toward_replicable_social_science_research.html">1878 andrew gelman stats-2013-05-31-How to fix the tabloids?  Toward replicable social science research</a></p>
<p>Introduction: This seems to be the topic of the week.  Yesterday I posted on the sister blog  some further thoughts  on those “Psychological Science” papers on menstrual cycles, biceps size, and political attitudes, tied to a horrible press release from the journal Psychological Science hyping the biceps and politics study.
 
Then I was pointed to these  suggestions  from Richard Lucas and M. Brent Donnellan have on improving the replicability and reproducibility of research published in the Journal of Research in Personality:
  
It goes without saying that editors of scientific journals strive to publish research that is not only theoretically interesting but also methodologically rigorous. The goal is to select papers that advance the field. Accordingly, editors want to publish findings that can be reproduced and replicated by other scientists. Unfortunately, there has been a recent “crisis in confidence” among psychologists about the quality of psychological research (Pashler & Wagenmakers, 2012)</p><p>6 0.13662273 <a title="1974-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-26-Difficulties_in_making_inferences_about_scientific_truth_from_distributions_of_published_p-values.html">2040 andrew gelman stats-2013-09-26-Difficulties in making inferences about scientific truth from distributions of published p-values</a></p>
<p>7 0.13565713 <a title="1974-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-19-Revised_evidence_for_statistical_standards.html">2140 andrew gelman stats-2013-12-19-Revised evidence for statistical standards</a></p>
<p>8 0.131493 <a title="1974-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-With_a_bit_of_precognition%2C_you%E2%80%99d_have_known_I_was_going_to_post_again_on_this_topic%2C_and_with_a_lot_of_precognition%2C_you%E2%80%99d_have_known_I_was_going_to_post_today.html">576 andrew gelman stats-2011-02-15-With a bit of precognition, you’d have known I was going to post again on this topic, and with a lot of precognition, you’d have known I was going to post today</a></p>
<p>9 0.13064535 <a title="1974-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-25-A_new_Bem_theory.html">1998 andrew gelman stats-2013-08-25-A new Bem theory</a></p>
<p>10 0.12925388 <a title="1974-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>11 0.12378117 <a title="1974-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-20-Proposals_for_alternative_review_systems_for_scientific_work.html">1273 andrew gelman stats-2012-04-20-Proposals for alternative review systems for scientific work</a></p>
<p>12 0.12156476 <a title="1974-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-17-How_can_statisticians_help_psychologists_do_their_research_better%3F.html">1860 andrew gelman stats-2013-05-17-How can statisticians help psychologists do their research better?</a></p>
<p>13 0.11878782 <a title="1974-tfidf-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>14 0.11712787 <a title="1974-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-26-%E2%80%9CA_Vast_Graveyard_of_Undead_Theories%3A__Publication_Bias_and_Psychological_Science%E2%80%99s_Aversion_to_the_Null%E2%80%9D.html">1826 andrew gelman stats-2013-04-26-“A Vast Graveyard of Undead Theories:  Publication Bias and Psychological Science’s Aversion to the Null”</a></p>
<p>15 0.11699911 <a title="1974-tfidf-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-23-Discussion_on_preregistration_of_research_studies.html">2183 andrew gelman stats-2014-01-23-Discussion on preregistration of research studies</a></p>
<p>16 0.11649311 <a title="1974-tfidf-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-The_statistical_significance_filter.html">899 andrew gelman stats-2011-09-10-The statistical significance filter</a></p>
<p>17 0.11327362 <a title="1974-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-26-Suggested_resolution_of_the_Bem_paradox.html">1139 andrew gelman stats-2012-01-26-Suggested resolution of the Bem paradox</a></p>
<p>18 0.11159328 <a title="1974-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-20-What_happened_that_the_journal_Psychological_Science_published_a_paper_with_no_identifiable_strengths%3F.html">1865 andrew gelman stats-2013-05-20-What happened that the journal Psychological Science published a paper with no identifiable strengths?</a></p>
<p>19 0.11133698 <a title="1974-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-06-Against_optimism_about_social_science.html">1844 andrew gelman stats-2013-05-06-Against optimism about social science</a></p>
<p>20 0.10961068 <a title="1974-tfidf-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21.html">256 andrew gelman stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.233), (1, 0.004), (2, -0.001), (3, -0.149), (4, -0.047), (5, -0.084), (6, -0.043), (7, -0.003), (8, -0.033), (9, -0.034), (10, -0.025), (11, 0.027), (12, -0.014), (13, -0.061), (14, 0.021), (15, -0.02), (16, -0.009), (17, -0.017), (18, -0.005), (19, -0.018), (20, 0.01), (21, 0.021), (22, -0.047), (23, -0.005), (24, -0.097), (25, 0.006), (26, 0.034), (27, -0.008), (28, 0.019), (29, -0.005), (30, -0.003), (31, -0.026), (32, -0.008), (33, -0.001), (34, -0.022), (35, 0.072), (36, -0.018), (37, -0.025), (38, 0.002), (39, 0.018), (40, 0.01), (41, 0.001), (42, -0.002), (43, 0.026), (44, 0.048), (45, -0.014), (46, -0.008), (47, 0.008), (48, 0.01), (49, -0.022)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96138716 <a title="1974-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-08-Statistical_significance_and_the_dangerous_lure_of_certainty.html">1974 andrew gelman stats-2013-08-08-Statistical significance and the dangerous lure of certainty</a></p>
<p>Introduction: In a discussion of some of the recent controversy over promiscuously statistically-significant science,  Jeff Leek  Rafael Irizarry  points out  there is a tradeoff between stringency and discovery and suggests that raising the bar of statistical significance (for example, to the .01 or .001 level instead of the conventional .05) will reduce the noise level but will also reduce the rate of identification of actual discoveries.
 
I agree.  But I should clarify that when I criticize a claim of statistical significance, arguing that the claimed “p less than .05″ could easily occur under the null hypothesis, given that the hypothesis test that is chosen is contingent on the data (see examples  here  of clothing and menstrual cycle, arm circumference and political attitudes, and ESP), I am  not  recommending a switch to a more stringent p-value threshold.  Rather, I would prefer p-values not to be used as a threshold for publication at all.
 
Here’s my point:  The question is not  whether</p><p>2 0.86572146 <a title="1974-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-16-%E2%80%9CFalse-positive_psychology%E2%80%9D.html">1171 andrew gelman stats-2012-02-16-“False-positive psychology”</a></p>
<p>Introduction: Everybody’s  talkin bout  this paper by Joseph Simmons, Leif Nelson and Uri Simonsohn, who  write :
  
Despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We [Simmons, Nelson, and Simonsohn] present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.
  
Whatever you think about these recommend</p><p>3 0.86033475 <a title="1974-lsi-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-23-Discussion_on_preregistration_of_research_studies.html">2183 andrew gelman stats-2014-01-23-Discussion on preregistration of research studies</a></p>
<p>Introduction: Chris Chambers and I had an enlightening discussion the other day at the blog of Rolf Zwaan, regarding the Garden of Forking Paths ( go here  and scroll down through the comments).
 
Chris sent me the following note:
  
I’m writing a book at the moment about reforming practices in psychological research (focusing on various bad practices such as p-hacking, HARKing, low statistical power, publication bias, lack of data sharing etc. – and posing solutions such as pre-registration, Bayesian hypothesis testing, mandatory data archiving etc.) and I am arriving at rather unsettling conclusion: that null hypothesis significance testing (NHST) simply isn’t valid for observational research. If this is true then most of the psychological literature is statistically flawed.


I was wonder what your thoughts were on this, both from a statistical point of view and from your experience working in an observational field. 


We all know about the dangers of researcher degrees of freedom. We also know</p><p>4 0.84871674 <a title="1974-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-18-You%E2%80%99ll_get_a_high_Type_S_error_rate_if_you_use_classical_statistical_methods_to_analyze_data_from_underpowered_studies.html">1944 andrew gelman stats-2013-07-18-You’ll get a high Type S error rate if you use classical statistical methods to analyze data from underpowered studies</a></p>
<p>Introduction: Brendan Nyhan sends me  this article  from the research-methods all-star team of Katherine Button, John Ioannidis, Claire Mokrysz,  Brian Nosek , Jonathan Flint, Emma Robinson, and Marcus Munafo:
  
A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.
  
I agree completely.  In my terminology, with small sample size, the classical approach of looking for statistical significance leads</p><p>5 0.84830076 <a title="1974-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-26-%E2%80%9CA_Vast_Graveyard_of_Undead_Theories%3A__Publication_Bias_and_Psychological_Science%E2%80%99s_Aversion_to_the_Null%E2%80%9D.html">1826 andrew gelman stats-2013-04-26-“A Vast Graveyard of Undead Theories:  Publication Bias and Psychological Science’s Aversion to the Null”</a></p>
<p>Introduction: Erin Jonaitis points us to  this article  by Christopher Ferguson and Moritz Heene, who write:
  
Publication bias remains a controversial issue in psychological science. . . . that the field often constructs arguments to block the publication and interpretation of null results and that null results may be further extinguished through questionable researcher practices. Given that science is dependent on the process of falsification, we argue that these problems reduce psychological science’s capability to have a proper mechanism for theory falsification, thus resulting in the promulgation of numerous “undead” theories that are ideologically popular but have little basis in fact.
  
They mention the infamous Daryl Bem article.  It is pretty much only because Bem’s claims are (presumably) false that they got published in a major research journal.  Had the claims been true—that is, had Bem run identical experiments, analyzed his data more carefully and objectively, and reported that the r</p><p>6 0.847417 <a title="1974-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-25-The_harm_done_by_tests_of_significance.html">1776 andrew gelman stats-2013-03-25-The harm done by tests of significance</a></p>
<p>7 0.83845758 <a title="1974-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>8 0.83016062 <a title="1974-lsi-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-08-Discussion_with_Steven_Pinker_on_research_that_is_attached_to_data_that_are_so_noisy_as_to_be_essentially_uninformative.html">2326 andrew gelman stats-2014-05-08-Discussion with Steven Pinker on research that is attached to data that are so noisy as to be essentially uninformative</a></p>
<p>9 0.82641798 <a title="1974-lsi-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-20-The_AAA_Tranche_of_Subprime_Science.html">2179 andrew gelman stats-2014-01-20-The AAA Tranche of Subprime Science</a></p>
<p>10 0.82162976 <a title="1974-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-08-Gregor_Mendel%E2%80%99s_suspicious_data.html">1449 andrew gelman stats-2012-08-08-Gregor Mendel’s suspicious data</a></p>
<p>11 0.81919414 <a title="1974-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-01-%E2%80%98Researcher_Degrees_of_Freedom%E2%80%99.html">1557 andrew gelman stats-2012-11-01-‘Researcher Degrees of Freedom’</a></p>
<p>12 0.81878012 <a title="1974-lsi-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-10-Preregistration%3A_what%E2%80%99s_in_it_for_you%3F.html">2241 andrew gelman stats-2014-03-10-Preregistration: what’s in it for you?</a></p>
<p>13 0.81844592 <a title="1974-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-13-Preregistration_of_Studies_and_Mock_Reports.html">1671 andrew gelman stats-2013-01-13-Preregistration of Studies and Mock Reports</a></p>
<p>14 0.81731182 <a title="1974-lsi-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-31-Jessica_Tracy_and_Alec_Beall_%28authors_of_the_fertile-women-wear-pink_study%29_comment_on_our_Garden_of_Forking_Paths_paper%2C_and_I_comment_on_their_comments.html">2355 andrew gelman stats-2014-05-31-Jessica Tracy and Alec Beall (authors of the fertile-women-wear-pink study) comment on our Garden of Forking Paths paper, and I comment on their comments</a></p>
<p>15 0.81486505 <a title="1974-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-07-I%E2%80%99m_negative_on_the_expression_%E2%80%9Cfalse_positives%E2%80%9D.html">2093 andrew gelman stats-2013-11-07-I’m negative on the expression “false positives”</a></p>
<p>16 0.81268978 <a title="1974-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-29-Another_one_of_those_%E2%80%9CPsychological_Science%E2%80%9D_papers_%28this_time_on_biceps_size_and_political_attitudes_among_college_students%29.html">1876 andrew gelman stats-2013-05-29-Another one of those “Psychological Science” papers (this time on biceps size and political attitudes among college students)</a></p>
<p>17 0.81195503 <a title="1974-lsi-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-04-%E2%80%9CDogs_are_sensitive_to_small_variations_of_the_Earth%E2%80%99s_magnetic_field%E2%80%9D.html">2159 andrew gelman stats-2014-01-04-“Dogs are sensitive to small variations of the Earth’s magnetic field”</a></p>
<p>18 0.81128985 <a title="1974-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-Fourteen_magic_words%3A_an_update.html">898 andrew gelman stats-2011-09-10-Fourteen magic words: an update</a></p>
<p>19 0.81095797 <a title="1974-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-31-Response_by_Jessica_Tracy_and_Alec_Beall_to_my_critique_of_the_methods_in_their_paper%2C_%E2%80%9CWomen_Are_More_Likely_to_Wear_Red_or_Pink_at_Peak_Fertility%E2%80%9D.html">1963 andrew gelman stats-2013-07-31-Response by Jessica Tracy and Alec Beall to my critique of the methods in their paper, “Women Are More Likely to Wear Red or Pink at Peak Fertility”</a></p>
<p>20 0.80856305 <a title="1974-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-12-More_frustrations_trying_to_replicate_an_analysis_published_in_a_reputable_journal.html">1054 andrew gelman stats-2011-12-12-More frustrations trying to replicate an analysis published in a reputable journal</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(9, 0.013), (10, 0.114), (15, 0.053), (16, 0.062), (20, 0.015), (21, 0.044), (24, 0.186), (42, 0.013), (50, 0.012), (53, 0.028), (63, 0.016), (99, 0.291)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96877658 <a title="1974-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-17-Is_chartjunk_really_%E2%80%9Cmore_useful%E2%80%9D_than_plain_graphs%3F__I_don%E2%80%99t_think_so..html">37 andrew gelman stats-2010-05-17-Is chartjunk really “more useful” than plain graphs?  I don’t think so.</a></p>
<p>Introduction: Helen DeWitt  links  to  this blog  that reports on  a study  by Scott Bateman, Carl Gutwin, David McDine, Regan Mandryk, Aaron Genest, and Christopher Brooks that claims the following:
  
Guidelines for designing information charts often state that the presentation should reduce ‘chart junk’–visual embellishments that are not essential to understanding the data. . . . we conducted an experiment that compared embellished charts with plain ones, and measured both interpretation accuracy and long-term recall. We found that people’s accuracy in describing the embellished charts was no worse than for plain charts, and that their recall after a two-to-three-week gap was significantly better.
  
As the above-linked blogger puts it, “chartjunk is more useful than plain graphs. . . . Tufte is not going to like this.”
 
I can’t speak for Ed Tufte, but I’m not gonna take this claim about chartjunk lying down.
 
I have two points to make which I hope can stop the above-linked study from being sla</p><p>same-blog 2 0.96555877 <a title="1974-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-08-Statistical_significance_and_the_dangerous_lure_of_certainty.html">1974 andrew gelman stats-2013-08-08-Statistical significance and the dangerous lure of certainty</a></p>
<p>Introduction: In a discussion of some of the recent controversy over promiscuously statistically-significant science,  Jeff Leek  Rafael Irizarry  points out  there is a tradeoff between stringency and discovery and suggests that raising the bar of statistical significance (for example, to the .01 or .001 level instead of the conventional .05) will reduce the noise level but will also reduce the rate of identification of actual discoveries.
 
I agree.  But I should clarify that when I criticize a claim of statistical significance, arguing that the claimed “p less than .05″ could easily occur under the null hypothesis, given that the hypothesis test that is chosen is contingent on the data (see examples  here  of clothing and menstrual cycle, arm circumference and political attitudes, and ESP), I am  not  recommending a switch to a more stringent p-value threshold.  Rather, I would prefer p-values not to be used as a threshold for publication at all.
 
Here’s my point:  The question is not  whether</p><p>3 0.96187758 <a title="1974-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-01-Ice_cream%21_and_temperature.html">1402 andrew gelman stats-2012-07-01-Ice cream! and temperature</a></p>
<p>Introduction: Just in time for the hot weather . . . Aleks points me to  this link  to a graph of % check-ins at 
NYC ice cream shops plotted against temperature in 2011.  Aleks writes, “interesting how the ice cream response lags temperature in spring/fall 
but during the summer, the response is immediate.”
 
   
 
This graph is a good starting point but I think more could be done, both in the analysis and purely in the graphics.  Putting the two lines together like this with a fixed ratio is just too crude a tool.  A series of graphs done just right could show a lot, I think!</p><p>4 0.95802987 <a title="1974-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-14-Looking_at_many_comparisons_may_increase_the_risk_of_finding_something_statistically_significant_by_epidemiologists%2C_a_population_with_relatively_low_multilevel_modeling_consumption.html">1059 andrew gelman stats-2011-12-14-Looking at many comparisons may increase the risk of finding something statistically significant by epidemiologists, a population with relatively low multilevel modeling consumption</a></p>
<p>Introduction: To understand the above title, see  here .
 
Masanao writes:
  
 This  report claims that eating meat increases the risk of cancer.  I’m sure you can’t read the page but you probably can understand the graphs. Different bars represent subdivision in the amount of the particular type of meat one consumes. And each chunk is different types of meat. Left is for male right is for female.


They claim that the difference is significant, but they are clearly not!!


I’m for not eating much meat but this is just way too much…
  
Here’s the graph:
 
   
 
I don’t know what to think.  If you look carefully you can find one or two statistically significant differences but overall the pattern doesn’t look so compelling.  I don’t know what the top and bottom rows are, though.  Overall, the pattern in the top row looks like it could represent a real trend, while the graphs on the bottom row look like noise.
 
This could be a good example for our multiple comparisons paper.  If the researchers won’t</p><p>5 0.95663333 <a title="1974-lda-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-17-The_Washington_Post_reprints_university_press_releases_without_editing_them.html">2215 andrew gelman stats-2014-02-17-The Washington Post reprints university press releases without editing them</a></p>
<p>Introduction: Somebody points me to  this  horrifying exposé by Paul Raeburn on a new series by the Washington Post where they reprint press releases as if they are actual news.  And the gimmick is, the reason why it’s appearing on this blog, is that these are  university press releases  on  science stories .  What could possibly go wrong there?
 
After all, Steve Chaplin, a self-identified “science-writing PIO from an R1,” writes in a comment to Raeburn’s post:
  
We write about peer-reviewed research accepted for publication or published by the world’s leading scientific journals after that research has been determined to be legitimate.  Repeatability of new research is a publication requisite. 
  
I emphasized that last sentence myself because it was such a stunner.  Do people really think that???
 
So I guess what he’s saying is, they don’t do press releases for articles from  Psychological Science  or the  Journal of Personality and Social Psychology .  But I wonder how the profs in the psych d</p><p>6 0.95588869 <a title="1974-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-01-Why_big_effects_are_more_important_than_small_effects.html">1744 andrew gelman stats-2013-03-01-Why big effects are more important than small effects</a></p>
<p>7 0.94999695 <a title="1974-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-27-Alfred_Kahn.html">487 andrew gelman stats-2010-12-27-Alfred Kahn</a></p>
<p>8 0.94877064 <a title="1974-lda-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-20-The_candy_weighing_demonstration%2C_or%2C_the_unwisdom_of_crowds.html">2257 andrew gelman stats-2014-03-20-The candy weighing demonstration, or, the unwisdom of crowds</a></p>
<p>9 0.94672728 <a title="1974-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-10-Hey%2C_where%E2%80%99s_my_kickback%3F.html">78 andrew gelman stats-2010-06-10-Hey, where’s my kickback?</a></p>
<p>10 0.94296938 <a title="1974-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>11 0.94169503 <a title="1974-lda-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-25-A_statistical_graphics_course_and_statistical_graphics_advice.html">2266 andrew gelman stats-2014-03-25-A statistical graphics course and statistical graphics advice</a></p>
<p>12 0.94046712 <a title="1974-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-11-One_more_time_on_that_ESP_study%3A__The_problem_of_overestimates_and_the_shrinkage_solution.html">511 andrew gelman stats-2011-01-11-One more time on that ESP study:  The problem of overestimates and the shrinkage solution</a></p>
<p>13 0.94038242 <a title="1974-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>14 0.93872607 <a title="1974-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-08-P-values_and_statistical_practice.html">1713 andrew gelman stats-2013-02-08-P-values and statistical practice</a></p>
<p>15 0.93794501 <a title="1974-lda-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>16 0.93650532 <a title="1974-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-28-Writing_for_free.html">2080 andrew gelman stats-2013-10-28-Writing for free</a></p>
<p>17 0.93613064 <a title="1974-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-08-What_we_need_here_is_some_peer_review_for_statistical_graphics.html">2013 andrew gelman stats-2013-09-08-What we need here is some peer review for statistical graphics</a></p>
<p>18 0.93518853 <a title="1974-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-24-Bell_Labs.html">970 andrew gelman stats-2011-10-24-Bell Labs</a></p>
<p>19 0.93516946 <a title="1974-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-02-The_inevitable_problems_with_statistical_significance_and_95%25_intervals.html">1150 andrew gelman stats-2012-02-02-The inevitable problems with statistical significance and 95% intervals</a></p>
<p>20 0.93460035 <a title="1974-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
