<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1788 andrew gelman stats-2013-04-04-When is there “hidden structure in data” to be discovered?</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-1788" href="#">andrew_gelman_stats-2013-1788</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1788 andrew gelman stats-2013-04-04-When is there “hidden structure in data” to be discovered?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-1788-html" href="http://andrewgelman.com/2013/04/04/when-is-there-hidden-structure-in-data-to-be-discovered/">html</a></p><p>Introduction: Michael Collins sent along the following announcement for a talk:
  
Fast learning algorithms for discovering the hidden structure in data


Daniel Hsu, Microsoft Research


11am, Wednesday April 10th, Interschool lab, 7th floor CEPSR, Columbia University


A major challenge in machine learning is to reliably and automatically discover hidden structure in data with minimal human intervention.  For instance, one may be interested in understanding the stratification of a population into subgroups, the thematic make-up of a collection of documents, or the dynamical process governing a complex time series.  Many of the core statistical estimation problems for these applications are, in general, provably intractable for both computational and statistical reasons; and therefore progress is made by shifting the focus to realistic instances that rule out the intractable cases.  In this talk, I’ll describe a general computational approach for correctly estimating a wide class of statistical mod</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 For instance, one may be interested in understanding the stratification of a population into subgroups, the thematic make-up of a collection of documents, or the dynamical process governing a complex time series. [sent-2, score-0.319]
</p><p>2 Many of the core statistical estimation problems for these applications are, in general, provably intractable for both computational and statistical reasons; and therefore progress is made by shifting the focus to realistic instances that rule out the intractable cases. [sent-3, score-0.743]
</p><p>3 In this talk, I’ll describe a general computational approach for correctly estimating a wide class of statistical models, including Gaussian mixture models, Hidden Markov models, Latent Dirichlet Allocation, Probabilistic Context Free Grammars, and several more. [sent-4, score-0.194]
</p><p>4 The key idea is to exploit the structure of low-order correlations that is present in high-dimensional data. [sent-5, score-0.554]
</p><p>5 The scope of the new approach extends beyond the purview of previous algorithms; and it leads to both new theoretical guarantees for unsupervised machine learning, as well as fast and practical algorithms for large-scale data analysis. [sent-6, score-0.739]
</p><p>6 I won’t have a chance to attend, but I had the following question or thought (beyond the question of why he capitalized the words Hidden, Latent, Allocation, Probabilistic, Context, Free, and Grammars, but not mixture, models, algorithm, or data analysis). [sent-7, score-0.085]
</p><p>7 The key seems to be in the title:  “the hidden structure in data. [sent-9, score-0.824]
</p><p>8 So I don’t think there’s any structure of low-order correlations to be found. [sent-12, score-0.406]
</p><p>9 There are some subtleties in the data (for example, the notorious interaction between individual and state income in predicting vote), but it’s hard to imagine that the methods described above would be good tools for studying these patterns. [sent-13, score-0.302]
</p><p>10 But other data in political science are indeed characterized by low-dimensional structure. [sent-14, score-0.085]
</p><p>11 The most prominent example is legislative roll-call voting, but there should be lots of other examples. [sent-15, score-0.074]
</p><p>12 Or personal networks, for example a study of covert terrorist organizations using external data, or a model of hidden links between financial organizations. [sent-17, score-0.528]
</p><p>13 I wonder if the methods discussed by Hsu in his talk come with guidelines for when they can most effectively be applied. [sent-18, score-0.09]
</p><p>14 Or maybe he happens to focus on applications with hidden structure? [sent-19, score-0.603]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hidden', 0.447), ('structure', 0.31), ('grammars', 0.179), ('intractable', 0.169), ('allocation', 0.162), ('algorithms', 0.16), ('vote', 0.159), ('hsu', 0.132), ('learning', 0.114), ('probabilistic', 0.113), ('latent', 0.108), ('mixture', 0.104), ('instance', 0.101), ('fast', 0.096), ('machine', 0.096), ('correlations', 0.096), ('computational', 0.09), ('models', 0.09), ('provably', 0.09), ('unsupervised', 0.09), ('talk', 0.09), ('data', 0.085), ('applications', 0.085), ('thematic', 0.085), ('governing', 0.085), ('cepsr', 0.085), ('collins', 0.085), ('mostly', 0.082), ('exploit', 0.081), ('terrorist', 0.081), ('income', 0.08), ('wednesday', 0.078), ('dynamical', 0.078), ('legislative', 0.074), ('extends', 0.074), ('dirichlet', 0.074), ('subgroups', 0.072), ('reliably', 0.072), ('guarantees', 0.072), ('context', 0.071), ('focus', 0.071), ('imagine', 0.071), ('stratification', 0.071), ('shifting', 0.069), ('attend', 0.068), ('marital', 0.067), ('key', 0.067), ('beyond', 0.066), ('individual', 0.066), ('free', 0.066)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1788-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-04-When_is_there_%E2%80%9Chidden_structure_in_data%E2%80%9D_to_be_discovered%3F.html">1788 andrew gelman stats-2013-04-04-When is there “hidden structure in data” to be discovered?</a></p>
<p>Introduction: Michael Collins sent along the following announcement for a talk:
  
Fast learning algorithms for discovering the hidden structure in data


Daniel Hsu, Microsoft Research


11am, Wednesday April 10th, Interschool lab, 7th floor CEPSR, Columbia University


A major challenge in machine learning is to reliably and automatically discover hidden structure in data with minimal human intervention.  For instance, one may be interested in understanding the stratification of a population into subgroups, the thematic make-up of a collection of documents, or the dynamical process governing a complex time series.  Many of the core statistical estimation problems for these applications are, in general, provably intractable for both computational and statistical reasons; and therefore progress is made by shifting the focus to realistic instances that rule out the intractable cases.  In this talk, I’ll describe a general computational approach for correctly estimating a wide class of statistical mod</p><p>2 0.21705963 <a title="1788-tfidf-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-27-what_is_%3D_what_%E2%80%9Cshould_be%E2%80%9D_%3F%3F.html">299 andrew gelman stats-2010-09-27-what is = what “should be” ??</a></p>
<p>Introduction: This  hidden assumption  is a biggie.</p><p>3 0.14060548 <a title="1788-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>Introduction: David Duvenaud writes:
  
I’ve been following  your recent discussions  about how an AI could do statistics [see also  here ].  I was especially excited about your suggestion for new statistical methods using “a language-like approach to recursively creating new models from a specified list of distributions and transformations, and an automatic approach to checking model fit.”


Your discussion of these ideas was exciting to me and my colleagues because we recently  did some work  taking a step in this direction, automatically searching through a grammar over Gaussian process regression models.


Roger Grosse previously  did the same thing , but over matrix decomposition models using held-out predictive likelihood to check model fit.


These are both examples of automatic Bayesian model-building by a search over more and more complex models, as you suggested.  One nice thing is that both grammars include lots of standard models for free, and they seem to work pretty well, although the</p><p>4 0.1372589 <a title="1788-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-17-Good_examples_of_lurking_variables%3F.html">1015 andrew gelman stats-2011-11-17-Good examples of lurking variables?</a></p>
<p>Introduction: Rama Ganesan writes:
  
I have been using many of your demos from the Teaching Stats book . . . Do you by any chance have a nice easy dataset that I can use to show students how ‘lurking variables’ work using regression? For instance, in your book you talk about the relationship between height and salaries – where gender is the hidden variable.
  
Any suggestions?</p><p>5 0.13710086 <a title="1788-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>Introduction: Larry Wasserman  refers  to finite mixture models as “beasts” and  writes  jokes that they “should be avoided at all costs.”
 
I’ve thought a lot about mixture models, ever since using them in an  analysis  of voting patterns that was published in 1990.  First off, I’d like to say that our model was useful so I’d prefer not to pay the cost of avoiding it.  For a quick description of our mixture model and its context, see pp. 379-380 of my  article  in the Jim Berger volume).  Actually, our case was particularly difficult because we were not even fitting a mixture model to data, we were fitting it to latent data and using the model to perform partial pooling.  My difficulties in trying to fit this model inspired our discussion of mixture models in Bayesian Data Analysis (page 109 in the second edition, in the section on “Counterexamples to the theorems”).
 
I agree with Larry that if you’re fitting a mixture model, it’s good to be aware of the problems that arise if you try to estimate</p><p>6 0.12281575 <a title="1788-tfidf-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-19-How_Americans_vote.html">2255 andrew gelman stats-2014-03-19-How Americans vote</a></p>
<p>7 0.12173895 <a title="1788-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-21-Derman%2C_Rodrik_and_the_nature_of_statistical_models.html">1076 andrew gelman stats-2011-12-21-Derman, Rodrik and the nature of statistical models</a></p>
<p>8 0.11956403 <a title="1788-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-04-Model_checking_and_model_understanding_in_machine_learning.html">1482 andrew gelman stats-2012-09-04-Model checking and model understanding in machine learning</a></p>
<p>9 0.11881656 <a title="1788-tfidf-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-19-Just_chaid.html">421 andrew gelman stats-2010-11-19-Just chaid</a></p>
<p>10 0.11736916 <a title="1788-tfidf-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-21-Discussion_of_the_paper_by_Girolami_and_Calderhead_on_Bayesian_computation.html">288 andrew gelman stats-2010-09-21-Discussion of the paper by Girolami and Calderhead on Bayesian computation</a></p>
<p>11 0.11665884 <a title="1788-tfidf-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-25-Misunderstanding_of_divided_government.html">369 andrew gelman stats-2010-10-25-Misunderstanding of divided government</a></p>
<p>12 0.11554673 <a title="1788-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-18-Postdoc_positions_at_Microsoft_Research_%E2%80%93_NYC.html">1630 andrew gelman stats-2012-12-18-Postdoc positions at Microsoft Research – NYC</a></p>
<p>13 0.11528775 <a title="1788-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-%E2%80%9CIs_machine_learning_a_subset_of_statistics%3F%E2%80%9D.html">1740 andrew gelman stats-2013-02-26-“Is machine learning a subset of statistics?”</a></p>
<p>14 0.1094055 <a title="1788-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-06-17_groups%2C_6_group-level_predictors%3A__What_to_do%3F.html">1248 andrew gelman stats-2012-04-06-17 groups, 6 group-level predictors:  What to do?</a></p>
<p>15 0.10051984 <a title="1788-tfidf-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-21-Random_matrices_in_the_news.html">2258 andrew gelman stats-2014-03-21-Random matrices in the news</a></p>
<p>16 0.098476693 <a title="1788-tfidf-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-10-What_happens_when_the_Democrats_are_%E2%80%9Cfighting_Wall_Street_with_one_hand%2C_unions_with_the_other%2C%E2%80%9D_while_the_Republicans_are_fighting_unions_with_two_hands%3F.html">79 andrew gelman stats-2010-06-10-What happens when the Democrats are “fighting Wall Street with one hand, unions with the other,” while the Republicans are fighting unions with two hands?</a></p>
<p>17 0.092111707 <a title="1788-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>18 0.091319725 <a title="1788-tfidf-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-20-%E2%80%9CPeople_with_an_itch_to_scratch%E2%80%9D.html">101 andrew gelman stats-2010-06-20-“People with an itch to scratch”</a></p>
<p>19 0.091194123 <a title="1788-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-25-Note_to_student_journalists%3A__Google_is_your_friend.html">1027 andrew gelman stats-2011-11-25-Note to student journalists:  Google is your friend</a></p>
<p>20 0.090284243 <a title="1788-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.192), (1, 0.029), (2, 0.046), (3, 0.064), (4, -0.009), (5, 0.098), (6, -0.118), (7, -0.043), (8, -0.025), (9, 0.047), (10, 0.001), (11, 0.036), (12, -0.011), (13, -0.03), (14, -0.013), (15, -0.021), (16, 0.01), (17, -0.052), (18, 0.003), (19, -0.033), (20, -0.016), (21, -0.049), (22, 0.017), (23, -0.012), (24, -0.025), (25, 0.014), (26, -0.003), (27, -0.01), (28, 0.034), (29, -0.033), (30, -0.004), (31, -0.009), (32, -0.002), (33, -0.009), (34, 0.067), (35, -0.041), (36, 0.011), (37, -0.038), (38, -0.048), (39, 0.007), (40, -0.02), (41, 0.012), (42, 0.036), (43, 0.064), (44, 0.019), (45, 0.01), (46, 0.018), (47, 0.028), (48, -0.007), (49, -0.056)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95326281 <a title="1788-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-04-When_is_there_%E2%80%9Chidden_structure_in_data%E2%80%9D_to_be_discovered%3F.html">1788 andrew gelman stats-2013-04-04-When is there “hidden structure in data” to be discovered?</a></p>
<p>Introduction: Michael Collins sent along the following announcement for a talk:
  
Fast learning algorithms for discovering the hidden structure in data


Daniel Hsu, Microsoft Research


11am, Wednesday April 10th, Interschool lab, 7th floor CEPSR, Columbia University


A major challenge in machine learning is to reliably and automatically discover hidden structure in data with minimal human intervention.  For instance, one may be interested in understanding the stratification of a population into subgroups, the thematic make-up of a collection of documents, or the dynamical process governing a complex time series.  Many of the core statistical estimation problems for these applications are, in general, provably intractable for both computational and statistical reasons; and therefore progress is made by shifting the focus to realistic instances that rule out the intractable cases.  In this talk, I’ll describe a general computational approach for correctly estimating a wide class of statistical mod</p><p>2 0.69484204 <a title="1788-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-%E2%80%9CIs_machine_learning_a_subset_of_statistics%3F%E2%80%9D.html">1740 andrew gelman stats-2013-02-26-“Is machine learning a subset of statistics?”</a></p>
<p>Introduction: Following up on our  previous post , Andrew Wilson writes:
  
I agree we are in a really exciting time for statistics and machine learning.  There has been a lot of talk lately comparing machine learning with statistics.  I am curious whether you think there are many fundamental differences between the fields, or just superficial differences — different popular approximate inference methods, slightly different popular application areas, etc.  Is machine learning a subset of statistics?


In the paper we discuss how we think machine learning is fundamentally about pattern discovery, and ultimately, fully automating the learning and decision making process.  In other words, whatever a human does when he or she uses tools to analyze data, can be written down algorithmically and automated on a computer.  I am not sure if the ambitions are similar in statistics — and I don’t have any conventional statistics background, which makes it harder to tell. I think it’s an interesting discussion.</p><p>3 0.68990672 <a title="1788-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>Introduction: David Duvenaud writes:
  
I’ve been following  your recent discussions  about how an AI could do statistics [see also  here ].  I was especially excited about your suggestion for new statistical methods using “a language-like approach to recursively creating new models from a specified list of distributions and transformations, and an automatic approach to checking model fit.”


Your discussion of these ideas was exciting to me and my colleagues because we recently  did some work  taking a step in this direction, automatically searching through a grammar over Gaussian process regression models.


Roger Grosse previously  did the same thing , but over matrix decomposition models using held-out predictive likelihood to check model fit.


These are both examples of automatic Bayesian model-building by a search over more and more complex models, as you suggested.  One nice thing is that both grammars include lots of standard models for free, and they seem to work pretty well, although the</p><p>4 0.68869793 <a title="1788-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-04-Model_checking_and_model_understanding_in_machine_learning.html">1482 andrew gelman stats-2012-09-04-Model checking and model understanding in machine learning</a></p>
<p>Introduction: Last month I  wrote :
  
Computer scientists are often brilliant but they can be unfamiliar with what is done in the worlds of data collection and analysis. This goes the other way too: statisticians such as myself can look pretty awkward, reinventing (or failing to reinvent) various wheels when we write computer programs or, even worse, try to design software.Andrew MacNamara writes:
  
Andrew MacNamara followed up with some thoughts:
  
I [MacNamara] had some basic statistics training through my MBA program, after having completed an undergrad degree in computer science. Since then I’ve been very interested in learning more about statistical techniques, including things like GLM and censored data analyses as well as machine learning topics like neural nets, SVMs, etc. I began following your blog after some research into Bayesian analysis topics and I am trying to dig deeper on that side of things.


One thing I have noticed is that there seems to be a distinction between data analysi</p><p>5 0.68438023 <a title="1788-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-21-Derman%2C_Rodrik_and_the_nature_of_statistical_models.html">1076 andrew gelman stats-2011-12-21-Derman, Rodrik and the nature of statistical models</a></p>
<p>Introduction: Interesting  thoughts  from Kaiser Fung.
 
Derman seems to have a point in his criticisms of economic models—and things are just as bad in other social sciences.  (I’ve  criticized  economists and political scientists for taking a crude, 80-year-old model of psychology as “foundational,” but even more sophisticated models in psychology and sociology have a lot of holes, if you go outside of certain clearly bounded areas such as psychometrics.)
 
What can be done, then?  One approach, which appeals to me as a statistician, is to more carefully define one’s range of inquiry.  Even if we don’t have a great model of political bargaining, we can still use ideal-point models to capture a lot of the variation in legislative voting.  And, in my blog post linked to above, I recommended that economists forget about coming up with the grand unified theory of human behavior (pretty impossible, given that they still don’t want to let go of much of their folk-psychology models) and put more effort i</p><p>6 0.6631332 <a title="1788-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-13-Duke_postdoctoral_fellowships_in_nonparametric_Bayes_%26_high-dimensional_data.html">903 andrew gelman stats-2011-09-13-Duke postdoctoral fellowships in nonparametric Bayes & high-dimensional data</a></p>
<p>7 0.64903015 <a title="1788-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-06-Josh_Tenenbaum_presents_._._._a_model_of_folk_physics%21.html">994 andrew gelman stats-2011-11-06-Josh Tenenbaum presents . . . a model of folk physics!</a></p>
<p>8 0.6481303 <a title="1788-lsi-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-19-Just_chaid.html">421 andrew gelman stats-2010-11-19-Just chaid</a></p>
<p>9 0.64363724 <a title="1788-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-24-The_Tweets-Votes_Curve.html">1823 andrew gelman stats-2013-04-24-The Tweets-Votes Curve</a></p>
<p>10 0.63246155 <a title="1788-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-23-Peter_Bartlett_on_model_complexity_and_sample_size.html">1636 andrew gelman stats-2012-12-23-Peter Bartlett on model complexity and sample size</a></p>
<p>11 0.6276257 <a title="1788-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-29-Postdocs_in_probabilistic_modeling%21__With_David_Blei%21__And_Stan%21.html">1961 andrew gelman stats-2013-07-29-Postdocs in probabilistic modeling!  With David Blei!  And Stan!</a></p>
<p>12 0.62302279 <a title="1788-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-10-Controversy_over_the_Christakis-Fowler_findings_on_the_contagion_of_obesity.html">757 andrew gelman stats-2011-06-10-Controversy over the Christakis-Fowler findings on the contagion of obesity</a></p>
<p>13 0.61637419 <a title="1788-lsi-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-17-Probability-processing_hardware.html">214 andrew gelman stats-2010-08-17-Probability-processing hardware</a></p>
<p>14 0.61508965 <a title="1788-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-03-New_New_York_data_research_organizations.html">1297 andrew gelman stats-2012-05-03-New New York data research organizations</a></p>
<p>15 0.61149132 <a title="1788-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-27-Cross-validation_%28What_is_it_good_for%3F%29.html">1395 andrew gelman stats-2012-06-27-Cross-validation (What is it good for?)</a></p>
<p>16 0.61088067 <a title="1788-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-19-An_interweaving-transformation_strategy_for_boosting_MCMC_efficiency.html">964 andrew gelman stats-2011-10-19-An interweaving-transformation strategy for boosting MCMC efficiency</a></p>
<p>17 0.6089192 <a title="1788-lsi-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-01-References_on_predicting_elections.html">249 andrew gelman stats-2010-09-01-References on predicting elections</a></p>
<p>18 0.60880822 <a title="1788-lsi-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-16-Wanted%3A__Probability_distributions_for_rank_orderings.html">151 andrew gelman stats-2010-07-16-Wanted:  Probability distributions for rank orderings</a></p>
<p>19 0.60871834 <a title="1788-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-28-Cool_job_opening_with_brilliant_researchers_at_Yahoo.html">978 andrew gelman stats-2011-10-28-Cool job opening with brilliant researchers at Yahoo</a></p>
<p>20 0.6038025 <a title="1788-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-23-When_are_complicated_models_helpful_in_psychology_research_and_when_are_they_overkill%3F.html">1690 andrew gelman stats-2013-01-23-When are complicated models helpful in psychology research and when are they overkill?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(15, 0.024), (16, 0.089), (21, 0.055), (24, 0.131), (36, 0.031), (43, 0.013), (66, 0.017), (77, 0.059), (79, 0.019), (84, 0.015), (86, 0.026), (87, 0.122), (99, 0.285)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96351874 <a title="1788-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-04-When_is_there_%E2%80%9Chidden_structure_in_data%E2%80%9D_to_be_discovered%3F.html">1788 andrew gelman stats-2013-04-04-When is there “hidden structure in data” to be discovered?</a></p>
<p>Introduction: Michael Collins sent along the following announcement for a talk:
  
Fast learning algorithms for discovering the hidden structure in data


Daniel Hsu, Microsoft Research


11am, Wednesday April 10th, Interschool lab, 7th floor CEPSR, Columbia University


A major challenge in machine learning is to reliably and automatically discover hidden structure in data with minimal human intervention.  For instance, one may be interested in understanding the stratification of a population into subgroups, the thematic make-up of a collection of documents, or the dynamical process governing a complex time series.  Many of the core statistical estimation problems for these applications are, in general, provably intractable for both computational and statistical reasons; and therefore progress is made by shifting the focus to realistic instances that rule out the intractable cases.  In this talk, I’ll describe a general computational approach for correctly estimating a wide class of statistical mod</p><p>2 0.95997143 <a title="1788-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-20-Andy_vs._the_Ideal_Point_Model_of_Voting.html">355 andrew gelman stats-2010-10-20-Andy vs. the Ideal Point Model of Voting</a></p>
<p>Introduction: Last week, as I walked into Andrew’s office for a meeting, he was 
formulating some misgivings about applying an ideal-point model to 
budgetary bills in the U.S. Senate.  Andrew didn’t like that the model 
of a senator’s position was an indifference point rather than at their 
optimal point, and that the effect of moving away from a position was 
automatically modeled as increasing in one direction and decreasing in 
the other.
 
 Executive Summary 
 
The monotonicity of inverse logit entails that the expected vote 
for a bill among any fixed collection of senators’ ideal points is 
monotonically increasing (or decreasing) with the bill’s position, 
with direction determined by the outcome coding.
 
 The Ideal-Point Model 
 

The ideal-point model’s easy to write down, but hard to reason about 
because of all the polarity shifting going on.  To recapitulate from 
Gelman and Hill’s 
  Regression   
book (p. 317), using the U.S. Senate instead of the Supreme Court, and 
ignoring the dis</p><p>3 0.94724762 <a title="1788-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-03-The_Employment_Nondiscrimination_Act_is_overwhelmingly_popular_in_nearly_every_one_of_the_50_states.html">2087 andrew gelman stats-2013-11-03-The Employment Nondiscrimination Act is overwhelmingly popular in nearly every one of the 50 states</a></p>
<p>Introduction: The above graph shows the estimated support, by state, for the Employment Nondiscrimination Act, a gay rights bill that the Senate will be voting on this Monday.  The estimates were constructed by Kate Krimmel, Jeff Lax, and Justin Phillips using multilevel regression and poststratification.
 
Check out that graph again.  The scale goes from 20% to 80%, but  every state  is in the yellow-to-red range.  Support for a law making it illegal to discriminate against gays has majority support in every state. And in most states the support is very strong.
 
And  here’s the research paper  by Krimmel, Lax, and Phillips, which begins:
  
Public majorities have supported several gay rights policies for some time, yet Congress has responded slowly if at all. We address this puzzle through dyadic analysis of the opinion- vote relationship on 23 roll-call votes between 1993 and 2010, matching members of Congress to policy-specific opinion in their state or district. We also extend the MRP opinion e</p><p>4 0.93710947 <a title="1788-lda-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-23-Thinking_outside_the_%28graphical%29_box%3A__Instead_of_arguing_about_how_best_to_fix_a_bar_chart%2C_graph_it_as_a_time_series_lineplot_instead.html">294 andrew gelman stats-2010-09-23-Thinking outside the (graphical) box:  Instead of arguing about how best to fix a bar chart, graph it as a time series lineplot instead</a></p>
<p>Introduction: John Kastellec points me to  this blog  by Ezra Klein criticizing the following graph from a recent Republican Party report:
 
 
 
Klein (following  Alexander Hart ) slams the graph for not going all the way to zero on the y-axis, thus making the projected change seem bigger than it really is.
 
I agree with Klein and Hart that, if you’re gonna do a bar chart, you want the bars to go down to 0.  On the other hand, a projected change from 19% to 23% is actually pretty big, and I don’t see the point of using a graphical display that hides it.
 
 The solution:  Ditch the bar graph entirely and replace it by a lineplot , in particular, a time series with year-by-year data.  The time series would have several advantages:
 
1.  Data are placed in context.  You’d see every year, instead of discrete averages, and you’d get to see the changes in the context of year-to-year variation.
 
2.  With the time series, you can use whatever y-axis works with the data.  No need to go to zero.
 
P.S.  I l</p><p>5 0.93507183 <a title="1788-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-23-Validation_of_Software_for_Bayesian_Models_Using_Posterior_Quantiles.html">1868 andrew gelman stats-2013-05-23-Validation of Software for Bayesian Models Using Posterior Quantiles</a></p>
<p>Introduction: Every once in awhile I get a question that I can directly answer from my published research.  When that happens it makes me so happy.
 
Here’s an example.  Patrick Lam wrote,
  
Suppose one develops a Bayesian model to estimate a parameter theta.  Now suppose one wants to evaluate the model via simulation by generating fake data where you know the value of theta and see how well you recover theta with your model, assuming that you use the posterior mean as the estimate.  The traditional frequentist way of evaluating it might be to generate many datasets and see how well your estimator performs each time in terms of unbiasedness or mean squared error or something.  But given that unbiasedness means nothing to a Bayesian and there is no repeated sampling interpretation in a Bayesian model, how would you suggest one would evaluate a Bayesian model?
  
My reply:
  
I actually have  a paper on this !  It is by Cook, Gelman, and Rubin.  The idea is to draw theta from the prior distribution.</p><p>6 0.93498302 <a title="1788-lda-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-17-Distorting_the_Electoral_Connection%3F_Partisan_Representation_in_Confirmation_Politics.html">152 andrew gelman stats-2010-07-17-Distorting the Electoral Connection? Partisan Representation in Confirmation Politics</a></p>
<p>7 0.93396604 <a title="1788-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-13-Question_of_the_week%3A__Will_the_authors_of_a_controversial_new_study_apologize_to_busy_statistician_Don_Berry_for_wasting_his_time_reading_and_responding_to_their_flawed_article%3F.html">1263 andrew gelman stats-2012-04-13-Question of the week:  Will the authors of a controversial new study apologize to busy statistician Don Berry for wasting his time reading and responding to their flawed article?</a></p>
<p>8 0.93268919 <a title="1788-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-21-Avoiding_boundary_estimates_in_linear_mixed_models.html">918 andrew gelman stats-2011-09-21-Avoiding boundary estimates in linear mixed models</a></p>
<p>9 0.93128276 <a title="1788-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-30-Don%E2%80%99t_stop_being_a_statistician_once_the_analysis_is_done.html">783 andrew gelman stats-2011-06-30-Don’t stop being a statistician once the analysis is done</a></p>
<p>10 0.92725295 <a title="1788-lda-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-R_needs_a_good_function_to_make_line_plots.html">252 andrew gelman stats-2010-09-02-R needs a good function to make line plots</a></p>
<p>11 0.92711198 <a title="1788-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-04-An_epithet_I_can_live_with.html">1604 andrew gelman stats-2012-12-04-An epithet I can live with</a></p>
<p>12 0.92167974 <a title="1788-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-25-Darn_that_Lindsey_Graham%21_%28or%2C_%E2%80%9CMr._P_Predicts_the_Kagan_vote%E2%80%9D%29.html">162 andrew gelman stats-2010-07-25-Darn that Lindsey Graham! (or, “Mr. P Predicts the Kagan vote”)</a></p>
<p>13 0.92080742 <a title="1788-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-21-An_interesting_assignment_for_statistical_graphics.html">583 andrew gelman stats-2011-02-21-An interesting assignment for statistical graphics</a></p>
<p>14 0.91799134 <a title="1788-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-12-How_to_Lie_With_Statistics_example_number_12%2C498%2C122.html">1574 andrew gelman stats-2012-11-12-How to Lie With Statistics example number 12,498,122</a></p>
<p>15 0.91738677 <a title="1788-lda-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-28-Bayesian_nonparametric_weighted_sampling_inference.html">2351 andrew gelman stats-2014-05-28-Bayesian nonparametric weighted sampling inference</a></p>
<p>16 0.91680264 <a title="1788-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-01-What_goes_around_._._..html">548 andrew gelman stats-2011-02-01-What goes around . . .</a></p>
<p>17 0.91654682 <a title="1788-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-14-Pourquoi_Google_search_est_devenu_plus_raisonnable%3F.html">207 andrew gelman stats-2010-08-14-Pourquoi Google search est devenu plus raisonnable?</a></p>
<p>18 0.91541231 <a title="1788-lda-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-25-Lauryn_Hill_update.html">233 andrew gelman stats-2010-08-25-Lauryn Hill update</a></p>
<p>19 0.91539925 <a title="1788-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-03-Popper_and_Jaynes.html">2007 andrew gelman stats-2013-09-03-Popper and Jaynes</a></p>
<p>20 0.91498315 <a title="1788-lda-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
