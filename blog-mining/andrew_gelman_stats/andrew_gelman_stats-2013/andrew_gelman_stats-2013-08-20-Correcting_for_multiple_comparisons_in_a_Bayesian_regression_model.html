<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1989 andrew gelman stats-2013-08-20-Correcting for multiple comparisons in a Bayesian regression model</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-1989" href="#">andrew_gelman_stats-2013-1989</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1989 andrew gelman stats-2013-08-20-Correcting for multiple comparisons in a Bayesian regression model</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-1989-html" href="http://andrewgelman.com/2013/08/20/correcting-for-multiple-comparisons-in-a-bayesian-regression-model/">html</a></p><p>Introduction: Joe Northrup writes:
  
I have a question about correcting for multiple comparisons in a Bayesian regression model. I believe I understand the argument in  your 2012 paper  in Journal of Research on Educational Effectiveness that when you have a hierarchical model there is shrinkage of estimates towards the group-level mean and thus there is no need to add any additional penalty to correct for multiple comparisons. In my case I do not have hierarchically structured dataâ&euro;&rdquo;i.e. I have only 1 observation per group but have a categorical variable with a large number of categories. Thus, I am fitting a simple multiple regression in a Bayesian framework. Would putting a strong, mean 0, multivariate normal prior on the betas in this model accomplish the same sort of shrinkage (it seems to me that it would) and do you believe this is a valid way to address criticism of multiple comparisons in this setting?
  
My reply:  Yes, I think this makes sense.  One way to address concerns of multiple com</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Joe Northrup writes:    I have a question about correcting for multiple comparisons in a Bayesian regression model. [sent-1, score-1.055]
</p><p>2 In my case I do not have hierarchically structured dataâ&euro;&rdquo;i. [sent-3, score-0.251]
</p><p>3 I have only 1 observation per group but have a categorical variable with a large number of categories. [sent-5, score-0.215]
</p><p>4 Thus, I am fitting a simple multiple regression in a Bayesian framework. [sent-6, score-0.631]
</p><p>5 Would putting a strong, mean 0, multivariate normal prior on the betas in this model accomplish the same sort of shrinkage (it seems to me that it would) and do you believe this is a valid way to address criticism of multiple comparisons in this setting? [sent-7, score-2.206]
</p><p>6 One way to address concerns of multiple comparisons is to do a simulation study conditional on some reasonable assumptions about the true effects, then see how often you end up obtaining statistically-significant but wrong claims. [sent-9, score-1.681]
</p><p>7 Or, if you want to put in even more effort, you could do several simulation studies, demonstrating that if the true effects are concentrated near zero but you assume a weak prior, that then the multiple comparisons issue would arise. [sent-10, score-1.658]
</p><p>8 That would be an interesting research direction, actually, to study how the multiple comparisons problems gradually arise as the prior becomes weaker and weaker. [sent-11, score-1.526]
</p><p>9 Such an analysis could aid our understanding by bridging between the classical and fully-informative Bayesian perspectives on multiple comparisons. [sent-12, score-0.733]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('multiple', 0.463), ('comparisons', 0.385), ('shrinkage', 0.215), ('simulation', 0.173), ('address', 0.157), ('prior', 0.155), ('hierarchically', 0.152), ('betas', 0.137), ('accomplish', 0.128), ('bayesian', 0.118), ('categorical', 0.117), ('concentrated', 0.114), ('weaker', 0.11), ('correcting', 0.11), ('obtaining', 0.107), ('demonstrating', 0.106), ('thus', 0.103), ('joe', 0.103), ('penalty', 0.103), ('aid', 0.101), ('gradually', 0.1), ('structured', 0.099), ('perspectives', 0.099), ('towards', 0.098), ('observation', 0.098), ('regression', 0.097), ('effectiveness', 0.097), ('believe', 0.095), ('effects', 0.094), ('true', 0.094), ('educational', 0.091), ('multivariate', 0.09), ('mean', 0.087), ('study', 0.085), ('becomes', 0.083), ('weak', 0.083), ('valid', 0.082), ('concerns', 0.08), ('near', 0.079), ('arise', 0.078), ('putting', 0.074), ('additional', 0.072), ('conditional', 0.071), ('fitting', 0.071), ('normal', 0.07), ('classical', 0.07), ('criticism', 0.068), ('would', 0.067), ('assumptions', 0.066), ('setting', 0.066)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="1989-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-20-Correcting_for_multiple_comparisons_in_a_Bayesian_regression_model.html">1989 andrew gelman stats-2013-08-20-Correcting for multiple comparisons in a Bayesian regression model</a></p>
<p>Introduction: Joe Northrup writes:
  
I have a question about correcting for multiple comparisons in a Bayesian regression model. I believe I understand the argument in  your 2012 paper  in Journal of Research on Educational Effectiveness that when you have a hierarchical model there is shrinkage of estimates towards the group-level mean and thus there is no need to add any additional penalty to correct for multiple comparisons. In my case I do not have hierarchically structured dataâ&euro;&rdquo;i.e. I have only 1 observation per group but have a categorical variable with a large number of categories. Thus, I am fitting a simple multiple regression in a Bayesian framework. Would putting a strong, mean 0, multivariate normal prior on the betas in this model accomplish the same sort of shrinkage (it seems to me that it would) and do you believe this is a valid way to address criticism of multiple comparisons in this setting?
  
My reply:  Yes, I think this makes sense.  One way to address concerns of multiple com</p><p>2 0.26266214 <a title="1989-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-17-I_got_99_comparisons_but_multiplicity_ain%E2%80%99t_one.html">1016 andrew gelman stats-2011-11-17-I got 99 comparisons but multiplicity ain’t one</a></p>
<p>Introduction: After I gave  my talk  at an econ seminar on Why We (Usually) Don’t Care About Multiple Comparisons, I got the following comment:
  
One question that came up later was whether your argument is really with testing in general, rather than only with testing in multiple comparison settings.
  
My reply:
 
Yes, my argument is with testing in general.  But it arises with particular force in multiple comparisons.  With a single test, we can just say we dislike testing so we use confidence intervals or Bayesian inference instead, and it’s no problem—really more of a change in emphasis than a change in methods.  But with multiple tests, the classical advice is not simply to look at type 1 error rates but more specifically to make a multiplicity adjustment, for example to make confidence intervals wider to account for multiplicity.  I don’t want to do this!  So here there is a real battle to fight.
 
P.S.   Here’s  the article (with Jennifer and Masanao), to appear in the Journal of Research on</p><p>3 0.2311337 <a title="1989-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-16-Bayesian_analogue_to_stepwise_regression%3F.html">1535 andrew gelman stats-2012-10-16-Bayesian analogue to stepwise regression?</a></p>
<p>Introduction: Bill Harris writes:
  
On pp. 250-251 of BDA second edition, you write about multiple comparisons, and you write about stepwise regression on p. 405.  How would you look at stepwise regression analyses in light of the multiple comparisons problem?  Is there an issue?
  
My reply:
 
In this case I think the right approach is to keep all the coefs but partially pool them toward 0 (after suitable transformation).  But then the challenge is coming up with a general way to construct good prior distributions.  Iâ&euro;&trade;m still thinking about that one!  Yet another approach is to put something together purely nonparametrically as with Bart.</p><p>4 0.22048603 <a title="1989-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-11-That_xkcd_cartoon_on_multiple_comparisons_that_all_of_you_were_sending_me_a_couple_months_ago.html">848 andrew gelman stats-2011-08-11-That xkcd cartoon on multiple comparisons that all of you were sending me a couple months ago</a></p>
<p>Introduction: John Transue  sent it in  with the following thoughtful comment:
  
I’d imagine you’ve already received this, but just in case, here’s a cartoon you’d like. At first blush it seems to go against your advice (more nuanced than what I’m about to say by quoting the paper title) to not worry about multiple comparisons.


However, if I understand correctly your argument about multiple comparisons in multilevel models, the situation in this comic might have been avoided if shrinkage toward the grand mean (of all colors) had prevented the greens from clearing the .05 threshold. Is that right?</p><p>5 0.2013564 <a title="1989-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-12-Single_or_multiple_imputation%3F.html">608 andrew gelman stats-2011-03-12-Single or multiple imputation?</a></p>
<p>Introduction: Vishnu Ganglani writes:
  
It appears that multiple imputation appears to be the best way to impute missing data because of the more accurate quantification of variance. However, when imputing missing data for income values in national household surveys, would you recommend it would be practical to maintain the multiple datasets associated with multiple imputations, or a single imputation method would suffice. I have worked on household survey projects (in Scotland) and in the past gone with suggesting single methods for ease of implementation, but with the availability of open source R software I am think of performing multiple imputation methodologies, but a bit apprehensive because of the complexity and also the need to maintain multiple datasets (ease of implementation).
  
My reply:  In many applications I’ve just used a single random imputation to avoid the awkwardness of working with multiple datasets.  But if there’s any concern, I’d recommend doing parallel analyses on multipl</p><p>6 0.18296884 <a title="1989-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>7 0.18000813 <a title="1989-tfidf-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-08-Discussion_with_Steven_Pinker_on_research_that_is_attached_to_data_that_are_so_noisy_as_to_be_essentially_uninformative.html">2326 andrew gelman stats-2014-05-08-Discussion with Steven Pinker on research that is attached to data that are so noisy as to be essentially uninformative</a></p>
<p>8 0.17497559 <a title="1989-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>9 0.16543353 <a title="1989-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-25-Extreem_p-values%21.html">1691 andrew gelman stats-2013-01-25-Extreem p-values!</a></p>
<p>10 0.16271093 <a title="1989-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-21-D._Buggin.html">1465 andrew gelman stats-2012-08-21-D. Buggin</a></p>
<p>11 0.15138449 <a title="1989-tfidf-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-05-The_greatest_works_of_statistics_never_published.html">128 andrew gelman stats-2010-07-05-The greatest works of statistics never published</a></p>
<p>12 0.14837864 <a title="1989-tfidf-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-07-The_%24900_kindergarten_teacher.html">261 andrew gelman stats-2010-09-07-The $900 kindergarten teacher</a></p>
<p>13 0.14544816 <a title="1989-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-26-My_talk_at_Berkeley_on_Wednesday.html">680 andrew gelman stats-2011-04-26-My talk at Berkeley on Wednesday</a></p>
<p>14 0.14513117 <a title="1989-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>15 0.14277127 <a title="1989-tfidf-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-11-Bayes_in_the_research_conversation.html">2368 andrew gelman stats-2014-06-11-Bayes in the research conversation</a></p>
<p>16 0.14055213 <a title="1989-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-28-Difficulties_of_using_statistical_significance_%28or_lack_thereof%29_to_sift_through_and_compare_research_hypotheses.html">2042 andrew gelman stats-2013-09-28-Difficulties of using statistical significance (or lack thereof) to sift through and compare research hypotheses</a></p>
<p>17 0.14045884 <a title="1989-tfidf-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>18 0.13857521 <a title="1989-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-17-Graphical_tools_for_understanding_multilevel_models.html">772 andrew gelman stats-2011-06-17-Graphical tools for understanding multilevel models</a></p>
<p>19 0.13721129 <a title="1989-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-10-Multiple_imputation_and_multilevel_analysis.html">704 andrew gelman stats-2011-05-10-Multiple imputation and multilevel analysis</a></p>
<p>20 0.13462928 <a title="1989-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-13-On_the_half-Cauchy_prior_for_a_global_scale_parameter.html">801 andrew gelman stats-2011-07-13-On the half-Cauchy prior for a global scale parameter</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.209), (1, 0.17), (2, 0.027), (3, -0.074), (4, -0.02), (5, -0.056), (6, 0.03), (7, 0.005), (8, -0.049), (9, 0.05), (10, 0.018), (11, 0.039), (12, 0.044), (13, 0.0), (14, 0.078), (15, 0.033), (16, -0.031), (17, -0.022), (18, -0.01), (19, 0.04), (20, -0.03), (21, 0.093), (22, 0.004), (23, -0.005), (24, -0.041), (25, -0.051), (26, 0.01), (27, -0.046), (28, -0.019), (29, -0.023), (30, 0.013), (31, -0.012), (32, 0.034), (33, 0.063), (34, 0.003), (35, -0.003), (36, 0.034), (37, 0.053), (38, -0.052), (39, 0.037), (40, -0.006), (41, 0.106), (42, -0.042), (43, -0.056), (44, 0.02), (45, -0.072), (46, -0.02), (47, 0.07), (48, -0.02), (49, -0.091)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97805995 <a title="1989-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-20-Correcting_for_multiple_comparisons_in_a_Bayesian_regression_model.html">1989 andrew gelman stats-2013-08-20-Correcting for multiple comparisons in a Bayesian regression model</a></p>
<p>Introduction: Joe Northrup writes:
  
I have a question about correcting for multiple comparisons in a Bayesian regression model. I believe I understand the argument in  your 2012 paper  in Journal of Research on Educational Effectiveness that when you have a hierarchical model there is shrinkage of estimates towards the group-level mean and thus there is no need to add any additional penalty to correct for multiple comparisons. In my case I do not have hierarchically structured dataâ&euro;&rdquo;i.e. I have only 1 observation per group but have a categorical variable with a large number of categories. Thus, I am fitting a simple multiple regression in a Bayesian framework. Would putting a strong, mean 0, multivariate normal prior on the betas in this model accomplish the same sort of shrinkage (it seems to me that it would) and do you believe this is a valid way to address criticism of multiple comparisons in this setting?
  
My reply:  Yes, I think this makes sense.  One way to address concerns of multiple com</p><p>2 0.77669084 <a title="1989-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-16-Bayesian_analogue_to_stepwise_regression%3F.html">1535 andrew gelman stats-2012-10-16-Bayesian analogue to stepwise regression?</a></p>
<p>Introduction: Bill Harris writes:
  
On pp. 250-251 of BDA second edition, you write about multiple comparisons, and you write about stepwise regression on p. 405.  How would you look at stepwise regression analyses in light of the multiple comparisons problem?  Is there an issue?
  
My reply:
 
In this case I think the right approach is to keep all the coefs but partially pool them toward 0 (after suitable transformation).  But then the challenge is coming up with a general way to construct good prior distributions.  Iâ&euro;&trade;m still thinking about that one!  Yet another approach is to put something together purely nonparametrically as with Bart.</p><p>3 0.74325913 <a title="1989-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-10-Multiple_imputation_and_multilevel_analysis.html">704 andrew gelman stats-2011-05-10-Multiple imputation and multilevel analysis</a></p>
<p>Introduction: Robert Birkelbach:
  
I am writing my Bachelor Thesis in which I want to assess the reading competencies of German elementary school children using the PIRLS2006 data. My levels are classrooms and the individuals. However, my dependent variable is a multiple imputed (m=5) reading test. The problem I have is, that I do not know, whether I can just calculate 5 linear multilevel models and then average all the results (the coefficients, standard deviation, bic, intra class correlation, R2, t-statistics, p-values etc) or if I need different formulas for integrating the results of the five models into one because it is a multilevel analysis? Do you think there’s a better way in solving my problem? I would greatly appreciate if you could help me with a problem regarding my analysis — I am quite a newbie to multilevel modeling and especially to multiple imputation. Also: Is it okay to use frequentist models when the multiple imputation was done bayesian? Would the different philosophies of sc</p><p>4 0.70154136 <a title="1989-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-11-That_xkcd_cartoon_on_multiple_comparisons_that_all_of_you_were_sending_me_a_couple_months_ago.html">848 andrew gelman stats-2011-08-11-That xkcd cartoon on multiple comparisons that all of you were sending me a couple months ago</a></p>
<p>Introduction: John Transue  sent it in  with the following thoughtful comment:
  
I’d imagine you’ve already received this, but just in case, here’s a cartoon you’d like. At first blush it seems to go against your advice (more nuanced than what I’m about to say by quoting the paper title) to not worry about multiple comparisons.


However, if I understand correctly your argument about multiple comparisons in multilevel models, the situation in this comic might have been avoided if shrinkage toward the grand mean (of all colors) had prevented the greens from clearing the .05 threshold. Is that right?</p><p>5 0.70066446 <a title="1989-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-15-Static_sensitivity_analysis.html">804 andrew gelman stats-2011-07-15-Static sensitivity analysis</a></p>
<p>Introduction: This is one of my favorite ideas.  I used it in an application but have never formally studied it or written it up as a general method.
 
Sensitivity analysis is when you check how inferences change when you vary fit several different models or when you vary inputs within a model.  Sensitivity analysis is often recommended but is typically difficult to do, what with the hassle of carrying around all these different estimates.  In Bayesian inference, sensitivity analysis is associated with varying the prior distribution, which irritates me:  why not consider sensitivity to the likelihood, as that’s typically just as arbitrary as the prior while having a much larger effect on the inferences.
 
So we came up with  static sensitivity analysis , which is a way to assess sensitivity to assumptions while fitting only one model.  The idea is that Bayesian posterior simulation gives you a range of parameter values, and from these you can learn about sensitivity directly.
 
The  published exampl</p><p>6 0.68676019 <a title="1989-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-22-Handling_multiple_versions_of_an_outcome_variable.html">726 andrew gelman stats-2011-05-22-Handling multiple versions of an outcome variable</a></p>
<p>7 0.68311572 <a title="1989-lsi-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-13-%E2%80%9CThe_truth_wears_off%3A__Is_there_something_wrong_with_the_scientific_method%3F%E2%80%9D.html">466 andrew gelman stats-2010-12-13-“The truth wears off:  Is there something wrong with the scientific method?”</a></p>
<p>8 0.6750198 <a title="1989-lsi-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-04-%E2%80%9CDogs_are_sensitive_to_small_variations_of_the_Earth%E2%80%99s_magnetic_field%E2%80%9D.html">2159 andrew gelman stats-2014-01-04-“Dogs are sensitive to small variations of the Earth’s magnetic field”</a></p>
<p>9 0.67452294 <a title="1989-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-06-Slow_progress.html">1445 andrew gelman stats-2012-08-06-Slow progress</a></p>
<p>10 0.6643917 <a title="1989-lsi-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-02-The_problem_of_overestimation_of_group-level_variance_parameters.html">63 andrew gelman stats-2010-06-02-The problem of overestimation of group-level variance parameters</a></p>
<p>11 0.66370541 <a title="1989-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-12-As_a_Bayesian_I_want_scientists_to_report_their_data_non-Bayesianly.html">1209 andrew gelman stats-2012-03-12-As a Bayesian I want scientists to report their data non-Bayesianly</a></p>
<p>12 0.65389448 <a title="1989-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-26-How_to_understand_coefficients_that_reverse_sign_when_you_start_controlling_for_things%3F.html">1870 andrew gelman stats-2013-05-26-How to understand coefficients that reverse sign when you start controlling for things?</a></p>
<p>13 0.64999443 <a title="1989-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-13-%E2%80%9CWhat_are_some_situations_in_which_the_classical_approach_%28or_a_naive_implementation_of_it%2C_based_on_cookbook_recipes%29_gives_worse_results_than_a_Bayesian_approach%2C_results_that_actually_impeded_the_science%3F%E2%80%9D.html">2099 andrew gelman stats-2013-11-13-“What are some situations in which the classical approach (or a naive implementation of it, based on cookbook recipes) gives worse results than a Bayesian approach, results that actually impeded the science?”</a></p>
<p>14 0.64755398 <a title="1989-lsi-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-14-Trying_to_be_precise_about_vagueness.html">342 andrew gelman stats-2010-10-14-Trying to be precise about vagueness</a></p>
<p>15 0.64713144 <a title="1989-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-11-Folic_acid_and_autism.html">1893 andrew gelman stats-2013-06-11-Folic acid and autism</a></p>
<p>16 0.64567792 <a title="1989-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-25-Extreem_p-values%21.html">1691 andrew gelman stats-2013-01-25-Extreem p-values!</a></p>
<p>17 0.64376867 <a title="1989-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>18 0.63568699 <a title="1989-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-13-On_the_half-Cauchy_prior_for_a_global_scale_parameter.html">801 andrew gelman stats-2011-07-13-On the half-Cauchy prior for a global scale parameter</a></p>
<p>19 0.63205361 <a title="1989-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-23-More_on_Bayesian_methods_and_multilevel_modeling.html">2033 andrew gelman stats-2013-09-23-More on Bayesian methods and multilevel modeling</a></p>
<p>20 0.63176692 <a title="1989-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-31-Untunable_Metropolis.html">833 andrew gelman stats-2011-07-31-Untunable Metropolis</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(13, 0.013), (15, 0.035), (16, 0.03), (21, 0.231), (24, 0.159), (30, 0.024), (40, 0.013), (76, 0.024), (93, 0.013), (99, 0.358)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99080837 <a title="1989-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-27-Neumann_update.html">432 andrew gelman stats-2010-11-27-Neumann update</a></p>
<p>Introduction: Steve Hsu, who  started off  this discussion, had some comments on  my speculations  on the personality of John von Neumann and others.  Steve writes:
  
I [Hsu] actually knew Feynman a bit when I was an undergrad, and found him to be very nice to students. Since then I have heard quite a few stories from people in theoretical physics which emphasize his nastier side, and I think in the end he was quite a complicated person like everyone else.


There are a couple of pseudo-biographies of vN, but none as high quality as, e.g., Gleick’s book on Feynman or Hodges book about Turing. (Gleick studied physics as an undergrad at Harvard, and Hodges is a PhD in mathematical physics — pretty rare backgrounds for biographers!)  For example, as mentioned on the comment thread to your post, Steve Heims wrote a book about both vN and Wiener (!),  and Norman Macrae wrote a biography of vN. Both books are worth reading, but I think neither really do him justice. The breadth of vN’s work is just too m</p><p>2 0.98258322 <a title="1989-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-15-%E2%80%9C10_Things_You_Need_to_Know_About_Causal_Effects%E2%80%9D.html">1675 andrew gelman stats-2013-01-15-“10 Things You Need to Know About Causal Effects”</a></p>
<p>Introduction: Macartan Humphreys pointed me to  this excellent guide .
 
Here are the 10 items:
  
1. A causal claim is a statement about what didn’t happen. 
2. There is a fundamental problem of causal inference. 
3. You can estimate average causal effects even if you cannot observe any individual causal effects. 
4. If you know that, on average, A causes B and that B causes C, this does not mean that you know that A causes C. 
5. The counterfactual model is all about contribution, not attribution. 
6. X can cause Y even if there is no “causal path” connecting X and Y. 
7. Correlation is not causation. 
8. X can cause Y even if X is not a necessary condition or a sufficient condition for Y. 
9. Estimating average causal effects does not require that treatment and control groups are identical. 
10. There is no causation without manipulation.
  
The  article  follows with crisp discussions of each point.  My favorite is item #6, not because it’s the most important but because it brings in some real s</p><p>3 0.98021138 <a title="1989-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-16-Wanted%3A__Probability_distributions_for_rank_orderings.html">151 andrew gelman stats-2010-07-16-Wanted:  Probability distributions for rank orderings</a></p>
<p>Introduction: Dietrich Stoyan writes:
  
 
I asked the IMS people for an expert in statistics of voting/elections and they wrote me your name. I am a statistician, but never worked in the field voting/elections. It was my son-in-law who asked me for statistical theories in that field.


He posed in particular the following problem:


The aim of the voting is to come to a ranking of c candidates. Every vote is a permutation of these c candidates. The problem is to have probability distributions in the set of all permutations of c elements.


Are there theories for such distributions?


I should be very grateful for a fast answer with hints to literature. (I confess that I do not know your books.) 
 

 
My reply:  Rather than trying to model the ranks directly, Iâ&euro;&trade;d recommend modeling a latent continuous outcome which then implies a distribution on ranks, if the ranks are of interest. There are lots of distributions of c-dimensional continuous outcomes.  In political science, the usual way to start is</p><p>4 0.97904885 <a title="1989-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-15-A_silly_paper_that_tries_to_make_fun_of_multilevel_models.html">854 andrew gelman stats-2011-08-15-A silly paper that tries to make fun of multilevel models</a></p>
<p>Introduction: Torkild Hovde Lyngstad writes:
  
I wondered what your reaction would be to  this paper  from a recent issue of European Political Science.


It came out already in March this year, so you might have seen it or even commented on it before. Is is a joke at the expense of the whole polisci discipline, a joke the Editors did not catch, or the sequel to the Sokal affair, just with quanto social science as the target? 
  
My reply:  Yes, several people pointed me to this article.  I don’t think it’s a hoax, it’s more of a joke:  the author is making the point that with fancy statistics you can discover all sorts of patterns that don’t make sense.  The implication, I believe, is that many patterns that social scientists  do  find through statistical analysis are not actually meaningful.
 
I agree with this point, which could be even more pithily stated as “correlation does not imply causation.”  I am irritated, however, by the singling out of multilevel models here, as the point could be mad</p><p>5 0.97785187 <a title="1989-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-01-Two_Postdoc_Positions_Available_on_Bayesian_Hierarchical_Modeling.html">62 andrew gelman stats-2010-06-01-Two Postdoc Positions Available on Bayesian Hierarchical Modeling</a></p>
<p>Introduction: Postdoc #1. Hierarchical Modeling and Computation: We are fitting hierarchical regression models with deep interactions. We’re working on new models with structured prior distributions, and this also requires advances in Bayesian computation. Applications include public opinion, climate reconstruction, and education research.
 
Postdoc #1 is funded by grants from the Department of Energy, Institute of Education Sciences, and National Science Foundation.
 
Postdoc #2. Hierarchical Modeling and Statistical Graphics: The goal of this research program is to investigate the application of the latest methods of multi-level data analysis, visualization and regression modeling to an important commercial problem: forecasting retail sales at the individual item level. These forecasts are used to make ordering, pricing and promotions decisions which can have significant economic impact to the retail chain such that even modest improvements in the accuracy of predictions, across a large retailer’s</p><p>6 0.97550094 <a title="1989-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-30-David_Hogg_on_statistics.html">1401 andrew gelman stats-2012-06-30-David Hogg on statistics</a></p>
<p>7 0.97420132 <a title="1989-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-22-Please_stop_me_before_I_barf_again.html">1275 andrew gelman stats-2012-04-22-Please stop me before I barf again</a></p>
<p>8 0.9731133 <a title="1989-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-13-News_coverage_of_statistical_issues%E2%80%A6how_did_I_do%3F.html">514 andrew gelman stats-2011-01-13-News coverage of statistical issues…how did I do?</a></p>
<p>9 0.97159219 <a title="1989-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-10-A_defense_of_Tom_Wolfe_based_on_the_impossibility_of_the_law_of_small_numbers_in_network_structure.html">1615 andrew gelman stats-2012-12-10-A defense of Tom Wolfe based on the impossibility of the law of small numbers in network structure</a></p>
<p>10 0.97100431 <a title="1989-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-26-%E2%80%9CA_Vast_Graveyard_of_Undead_Theories%3A__Publication_Bias_and_Psychological_Science%E2%80%99s_Aversion_to_the_Null%E2%80%9D.html">1826 andrew gelman stats-2013-04-26-“A Vast Graveyard of Undead Theories:  Publication Bias and Psychological Science’s Aversion to the Null”</a></p>
<p>11 0.96967614 <a title="1989-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-15-Does_quantum_uncertainty_have_a_place_in_everyday_applied_statistics%3F.html">1857 andrew gelman stats-2013-05-15-Does quantum uncertainty have a place in everyday applied statistics?</a></p>
<p>12 0.96301436 <a title="1989-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-13-Jim_Campbell_argues_that_Larry_Bartels%E2%80%99s_%E2%80%9CUnequal_Democracy%E2%80%9D_findings_are_not_robust.html">659 andrew gelman stats-2011-04-13-Jim Campbell argues that Larry Bartels’s “Unequal Democracy” findings are not robust</a></p>
<p>13 0.96295547 <a title="1989-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-19-The_grasshopper_wins%2C_and_Greg_Mankiw%E2%80%99s_grandmother_would_be_%E2%80%9Cshocked_and_appalled%E2%80%9D_all_over_again.html">1728 andrew gelman stats-2013-02-19-The grasshopper wins, and Greg Mankiw’s grandmother would be “shocked and appalled” all over again</a></p>
<p>same-blog 14 0.96145296 <a title="1989-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-20-Correcting_for_multiple_comparisons_in_a_Bayesian_regression_model.html">1989 andrew gelman stats-2013-08-20-Correcting for multiple comparisons in a Bayesian regression model</a></p>
<p>15 0.95872676 <a title="1989-lda-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-26-Sleazy_sock_puppet_can%E2%80%99t_stop_spamming_our_discussion_of_compressed_sensing_and_promoting_the_work_of_Xiteng_Liu.html">2306 andrew gelman stats-2014-04-26-Sleazy sock puppet can’t stop spamming our discussion of compressed sensing and promoting the work of Xiteng Liu</a></p>
<p>16 0.95715231 <a title="1989-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Adding_more_information_can_make_the_variance_go_up_%28depending_on_your_model%29.html">810 andrew gelman stats-2011-07-20-Adding more information can make the variance go up (depending on your model)</a></p>
<p>17 0.95710111 <a title="1989-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-07-Hipmunk_FAIL%3A__Graphics_without_content_is_not_enough.html">894 andrew gelman stats-2011-09-07-Hipmunk FAIL:  Graphics without content is not enough</a></p>
<p>18 0.95635641 <a title="1989-lda-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-15-Quote_of_the_day%3A__statisticians_and_defaults.html">147 andrew gelman stats-2010-07-15-Quote of the day:  statisticians and defaults</a></p>
<p>19 0.94856608 <a title="1989-lda-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-04-%E2%80%9CDogs_are_sensitive_to_small_variations_of_the_Earth%E2%80%99s_magnetic_field%E2%80%9D.html">2159 andrew gelman stats-2014-01-04-“Dogs are sensitive to small variations of the Earth’s magnetic field”</a></p>
<p>20 0.94313234 <a title="1989-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-25-Classical_probability_does_not_apply_to_quantum_systems_%28causal_inference_edition%29.html">2037 andrew gelman stats-2013-09-25-Classical probability does not apply to quantum systems (causal inference edition)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
