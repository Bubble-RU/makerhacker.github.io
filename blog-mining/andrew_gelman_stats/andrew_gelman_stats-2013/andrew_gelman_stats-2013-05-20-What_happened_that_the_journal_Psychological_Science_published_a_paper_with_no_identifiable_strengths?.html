<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1865 andrew gelman stats-2013-05-20-What happened that the journal Psychological Science published a paper with no identifiable strengths?</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-1865" href="#">andrew_gelman_stats-2013-1865</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1865 andrew gelman stats-2013-05-20-What happened that the journal Psychological Science published a paper with no identifiable strengths?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-1865-html" href="http://andrewgelman.com/2013/05/20/what-happened-that-the-journal-psychological-science-published-a-paper-with-no-identifiable-strengths/">html</a></p><p>Introduction: The other day we  discussed  that paper on ovulation and voting (you may recall that the authors reported a scattered bunch of comparisons, significance tests, and p-values, and I recommended that they would’ve done better to simply report complete summaries of their data, so that readers could see the comparisons of interest in full context), and I was thinking a bit more about why I was so bothered that it was published in Psychological Science, which I’d thought of as a serious research journal. 
   
My concern isn’t just that that the paper is bad—after all, lots of bad papers get published—but rather that it had nothing really going for it,  except  that it was headline bait.  It was a survey done on Mechanical Turk, that’s it.  No clever design, no clever questions, no care in dealing with nonresponse problems, no innovative data analysis, no nothing.  The paper had nothing to offer, except that it had no obvious flaws.  Psychology is a huge field full of brilliant researchers.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 My concern isn’t just that that the paper is bad—after all, lots of bad papers get published—but rather that it had nothing really going for it,  except  that it was headline bait. [sent-2, score-0.797]
</p><p>2 The paper had nothing to offer, except that it had no obvious flaws. [sent-5, score-0.443]
</p><p>3 Its top journal can choose among so many papers. [sent-7, score-0.534]
</p><p>4 To pick this one, a paper that had nothing to offer, that seems to me like a sign of a serious problem. [sent-8, score-0.487]
</p><p>5 Just to be clear:  I’m really really really really not trying to censor such work, and I’m really really really really not saying this work should not be published. [sent-13, score-0.937]
</p><p>6 What I’m saying is that the  top  journal in a field should not be publishing such routine work. [sent-14, score-0.826]
</p><p>7 And, once you decide to start publishing mediocre papers in your top journal, you’re asking for trouble. [sent-19, score-0.784]
</p><p>8 This was published in top journals and was later found to have some serious methodological issues. [sent-22, score-0.693]
</p><p>9 OK, they made some mistakes, but I can’t fault a leading journal for publishing this work. [sent-25, score-0.505]
</p><p>10 The difference is that Kanazawa’s papers were published in a middling place—the Journal of Theoretical Biology—not in a top journal of their field. [sent-34, score-0.948]
</p><p>11 This paper had a gaping hole (not adjusting for the selection effect arising from less well-funded students dropping out) and I think it was a mistake for it to be published as is—but that’s just something the reviewers didn’t catch. [sent-38, score-0.557]
</p><p>12 My point is that, in all these cases of the publication of flawed work (and one could add the work of Mark Hauser and Bruno Frey as well), the published papers either had clear strengths or else were not published in top journals. [sent-42, score-1.358]
</p><p>13 When an interesting, exciting, but flawed paper (such as those by Bem, Hauser, etc) is published in a top journal, that’s too bad, but it’s understandable. [sent-43, score-0.954]
</p><p>14 When a possibly interesting paper (such as those by Kanazawa) is published in an OK journal, that makes sense too. [sent-44, score-0.492]
</p><p>15 But when a mediocre paper (which also happens to have serious methodological flaws) is published in a top journal, there’s something seriously wrong going on. [sent-46, score-1.242]
</p><p>16 There are lots of things that can make a research paper special, and this paper had none of those things (unless anything combining voting and sex in an election year is considered special). [sent-47, score-0.696]
</p><p>17 In fact, I’ve refrained linking to the paper here, just to give the authors a break. [sent-51, score-0.416]
</p><p>18 I’ve done lots of little studies that happened to be flawed, and sometimes my flawed work gets published. [sent-54, score-0.487]
</p><p>19 I’m criticizing the journal for publishing a mediocre paper with little to offer. [sent-57, score-1.13]
</p><p>20 That’s not just a retrospective mistake; it seems like a problem with their policies that they would think that such an unremarkable paper could even be seriously considered for publication in the top journal of their field. [sent-58, score-0.992]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('journal', 0.296), ('paper', 0.266), ('top', 0.238), ('published', 0.226), ('flawed', 0.224), ('mediocre', 0.216), ('publishing', 0.209), ('kanazawa', 0.185), ('methodological', 0.121), ('papers', 0.121), ('hamilton', 0.12), ('ovulation', 0.12), ('methodologically', 0.117), ('strengths', 0.114), ('nothing', 0.113), ('mistakes', 0.111), ('serious', 0.108), ('really', 0.108), ('nonresponse', 0.103), ('voting', 0.102), ('hauser', 0.1), ('special', 0.092), ('bem', 0.091), ('clever', 0.088), ('flaws', 0.087), ('field', 0.083), ('claiming', 0.081), ('criticizing', 0.08), ('authors', 0.079), ('editors', 0.078), ('work', 0.073), ('ok', 0.073), ('ve', 0.072), ('refrained', 0.071), ('mturk', 0.071), ('schoolyard', 0.067), ('middling', 0.067), ('seriously', 0.067), ('offer', 0.065), ('mistake', 0.065), ('done', 0.065), ('except', 0.064), ('bad', 0.063), ('publication', 0.063), ('little', 0.063), ('lots', 0.062), ('unremarkable', 0.062), ('scattered', 0.062), ('comparisons', 0.06), ('study', 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1865-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-20-What_happened_that_the_journal_Psychological_Science_published_a_paper_with_no_identifiable_strengths%3F.html">1865 andrew gelman stats-2013-05-20-What happened that the journal Psychological Science published a paper with no identifiable strengths?</a></p>
<p>Introduction: The other day we  discussed  that paper on ovulation and voting (you may recall that the authors reported a scattered bunch of comparisons, significance tests, and p-values, and I recommended that they would’ve done better to simply report complete summaries of their data, so that readers could see the comparisons of interest in full context), and I was thinking a bit more about why I was so bothered that it was published in Psychological Science, which I’d thought of as a serious research journal. 
   
My concern isn’t just that that the paper is bad—after all, lots of bad papers get published—but rather that it had nothing really going for it,  except  that it was headline bait.  It was a survey done on Mechanical Turk, that’s it.  No clever design, no clever questions, no care in dealing with nonresponse problems, no innovative data analysis, no nothing.  The paper had nothing to offer, except that it had no obvious flaws.  Psychology is a huge field full of brilliant researchers.</p><p>2 0.31238997 <a title="1865-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-06-How_to_think_about_papers_published_in_low-grade_journals%3F.html">1928 andrew gelman stats-2013-07-06-How to think about papers published in low-grade journals?</a></p>
<p>Introduction: We’ve had lots of lively discussions of fatally-flawed papers that have been published in top, top journals such as the  American Economic Review  or the  Journal of Personality and Social Psychology  or the  American Sociological Review  or the  tabloids .  And we also know about mistakes that make their way into mid-ranking outlets such as the Journal of Theoretical Biology.
 
But what about results that appear in the lower tier of legitimate journals?  I was thinking about this after reading a  post  by Dan Kahan slamming a paper that recently appeared in PLOS-One.  I won’t discuss the paper itself here because that’s not my point.  Rather, I had some thoughts regarding Kahan’s annoyance that a paper with fatal errors was published at all.  I commented as follows:
  
Read between the lines. The paper originally was released in 2009 and was published in 2013 in PLOS-One, which is one step above appearing on Arxiv. PLOS-One publishes some good things (so does Arxiv) but it’s the place</p><p>3 0.27593288 <a title="1865-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-26-Suggested_resolution_of_the_Bem_paradox.html">1139 andrew gelman stats-2012-01-26-Suggested resolution of the Bem paradox</a></p>
<p>Introduction: There has been an increasing discussion about the proliferation of flawed research in psychology and medicine, with some landmark events being John Ioannides’s  article , “Why most published research findings are false” (according to Google Scholar, cited 973 times since its appearance in 2005), the scandals of Marc Hauser and Diederik Stapel, two leading psychology professors who resigned after disclosures of scientific misconduct, and Daryl Bem’s  dubious  recent paper on ESP, published to much  fanfare  in Journal of Personality and Social Psychology, one of the top journals in the field.
 
Alongside all this are the plagiarism scandals, which are uninteresting from a scientific context but are relevant in that, in many cases, neither the institutions housing the plagiarists nor the editors and publishers of the plagiarized material seem to care.  Perhaps these universities and publishers are more worried about bad publicity (and maybe lawsuits, given that many of the plagiarism cas</p><p>4 0.27154002 <a title="1865-tfidf-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-12-More_on_publishing_in_journals.html">2245 andrew gelman stats-2014-03-12-More on publishing in journals</a></p>
<p>Introduction: I’m postponing today’s scheduled post (“Empirical implications of Empirical Implications of Theoretical Models”) to continue the lively discussion from yesterday,  What if I were to stop publishing in journals? . 
   
 An example:  my papers with Basbøll 
 
Thomas Basbøll and I got into a long discussion on our blogs about business school professor Karl Weick and other cases of  plagiarism  copying text without attribution.  We felt it useful to take our ideas to the next level and write them up as a manuscript, which ended up being logical to split into two papers.  At that point I put some effort into getting these papers published, which I eventually did:   To throw away data: Plagiarism as a statistical crime  went into American Scientist and  When do stories work? Evidence and illustration in the social sciences  will appear in Sociological Methods and Research.  The second paper, in particular, took some effort to place; I got some advice from colleagues in sociology as to where</p><p>5 0.25584888 <a title="1865-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-12-The_importance_of_style_in_academic_writing.html">902 andrew gelman stats-2011-09-12-The importance of style in academic writing</a></p>
<p>Introduction: In my comments on  academic cheating , I briefly discussed the question of how some of these papers could’ve been published in the first place, given that they tend to be of low quality.  (It’s rare that people plagiarize the good stuff, and, when they do—for example when a senior scholar takes credit for a junior researcher’s contributions without giving proper credit—there’s not always a paper trail, and there can be legitimate differences of opinion about the relative contributions of the participants.)
 
Anyway, to get back to the cases at hand:  how did these rulebreakers get published in the first place?  The question here is not how did they get away with cheating but how is it that top journals were publishing mediocre research? 
   
In the case of the profs who falsified data (Diederik Stapel) or did not follow scientific protocol (Mark Hauser), the answer is clear:  By cheating, they were able to get the sort of too-good-to-be-true results which, if they  were  true, would be</p><p>6 0.24512893 <a title="1865-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-30-Retracted_articles_and_unethical_behavior_in_economics_journals%3F.html">1435 andrew gelman stats-2012-07-30-Retracted articles and unethical behavior in economics journals?</a></p>
<p>7 0.23430806 <a title="1865-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-The_reverse-journal-submission_system.html">1393 andrew gelman stats-2012-06-26-The reverse-journal-submission system</a></p>
<p>8 0.23023306 <a title="1865-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-17-How_can_statisticians_help_psychologists_do_their_research_better%3F.html">1860 andrew gelman stats-2013-05-17-How can statisticians help psychologists do their research better?</a></p>
<p>9 0.21275835 <a title="1865-tfidf-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-04-Literal_vs._rhetorical.html">2233 andrew gelman stats-2014-03-04-Literal vs. rhetorical</a></p>
<p>10 0.20422363 <a title="1865-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-25-A_new_Bem_theory.html">1998 andrew gelman stats-2013-08-25-A new Bem theory</a></p>
<p>11 0.19694427 <a title="1865-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-31-How_to_fix_the_tabloids%3F__Toward_replicable_social_science_research.html">1878 andrew gelman stats-2013-05-31-How to fix the tabloids?  Toward replicable social science research</a></p>
<p>12 0.19581854 <a title="1865-tfidf-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-30-I_posted_this_as_a_comment_on_a_sociology_blog.html">2353 andrew gelman stats-2014-05-30-I posted this as a comment on a sociology blog</a></p>
<p>13 0.19560464 <a title="1865-tfidf-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-11-What_if_I_were_to_stop_publishing_in_journals%3F.html">2244 andrew gelman stats-2014-03-11-What if I were to stop publishing in journals?</a></p>
<p>14 0.1953962 <a title="1865-tfidf-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-27-Beyond_the_Valley_of_the_Trolls.html">2269 andrew gelman stats-2014-03-27-Beyond the Valley of the Trolls</a></p>
<p>15 0.19277236 <a title="1865-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-22-Arrow%E2%80%99s_other_theorem.html">675 andrew gelman stats-2011-04-22-Arrow’s other theorem</a></p>
<p>16 0.19243652 <a title="1865-tfidf-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-06-How_much_time_%28if_any%29_should_we_spend_criticizing_research_that%E2%80%99s_fraudulent%2C_crappy%2C_or_just_plain_pointless%3F.html">2235 andrew gelman stats-2014-03-06-How much time (if any) should we spend criticizing research that’s fraudulent, crappy, or just plain pointless?</a></p>
<p>17 0.18500675 <a title="1865-tfidf-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-19-The_replication_and_criticism_movement_is_not_about_suppressing_speculative_research%3B_rather%2C_it%E2%80%99s_all_about_enabling_science%E2%80%99s_fabled_self-correcting_nature.html">2217 andrew gelman stats-2014-02-19-The replication and criticism movement is not about suppressing speculative research; rather, it’s all about enabling science’s fabled self-correcting nature</a></p>
<p>18 0.18354788 <a title="1865-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-06-Against_optimism_about_social_science.html">1844 andrew gelman stats-2013-05-06-Against optimism about social science</a></p>
<p>19 0.17496361 <a title="1865-tfidf-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-26-Musical_chairs_in_econ_journals.html">371 andrew gelman stats-2010-10-26-Musical chairs in econ journals</a></p>
<p>20 0.17334381 <a title="1865-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-20-Reading_a_research_paper_%21%3D_agreeing_with_its_claims.html">1074 andrew gelman stats-2011-12-20-Reading a research paper != agreeing with its claims</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.279), (1, -0.113), (2, -0.084), (3, -0.221), (4, -0.097), (5, -0.113), (6, 0.029), (7, -0.152), (8, -0.068), (9, -0.014), (10, 0.211), (11, 0.044), (12, -0.125), (13, 0.01), (14, 0.032), (15, -0.11), (16, 0.035), (17, 0.043), (18, -0.038), (19, 0.008), (20, -0.027), (21, 0.037), (22, 0.035), (23, -0.008), (24, -0.029), (25, -0.014), (26, -0.077), (27, -0.012), (28, 0.004), (29, 0.023), (30, -0.034), (31, -0.033), (32, 0.01), (33, -0.034), (34, -0.034), (35, 0.004), (36, 0.016), (37, 0.056), (38, -0.106), (39, 0.063), (40, 0.009), (41, 0.039), (42, -0.043), (43, -0.01), (44, -0.021), (45, 0.024), (46, 0.024), (47, 0.04), (48, -0.008), (49, 0.015)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99057931 <a title="1865-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-20-What_happened_that_the_journal_Psychological_Science_published_a_paper_with_no_identifiable_strengths%3F.html">1865 andrew gelman stats-2013-05-20-What happened that the journal Psychological Science published a paper with no identifiable strengths?</a></p>
<p>Introduction: The other day we  discussed  that paper on ovulation and voting (you may recall that the authors reported a scattered bunch of comparisons, significance tests, and p-values, and I recommended that they would’ve done better to simply report complete summaries of their data, so that readers could see the comparisons of interest in full context), and I was thinking a bit more about why I was so bothered that it was published in Psychological Science, which I’d thought of as a serious research journal. 
   
My concern isn’t just that that the paper is bad—after all, lots of bad papers get published—but rather that it had nothing really going for it,  except  that it was headline bait.  It was a survey done on Mechanical Turk, that’s it.  No clever design, no clever questions, no care in dealing with nonresponse problems, no innovative data analysis, no nothing.  The paper had nothing to offer, except that it had no obvious flaws.  Psychology is a huge field full of brilliant researchers.</p><p>2 0.94667459 <a title="1865-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-06-How_to_think_about_papers_published_in_low-grade_journals%3F.html">1928 andrew gelman stats-2013-07-06-How to think about papers published in low-grade journals?</a></p>
<p>Introduction: We’ve had lots of lively discussions of fatally-flawed papers that have been published in top, top journals such as the  American Economic Review  or the  Journal of Personality and Social Psychology  or the  American Sociological Review  or the  tabloids .  And we also know about mistakes that make their way into mid-ranking outlets such as the Journal of Theoretical Biology.
 
But what about results that appear in the lower tier of legitimate journals?  I was thinking about this after reading a  post  by Dan Kahan slamming a paper that recently appeared in PLOS-One.  I won’t discuss the paper itself here because that’s not my point.  Rather, I had some thoughts regarding Kahan’s annoyance that a paper with fatal errors was published at all.  I commented as follows:
  
Read between the lines. The paper originally was released in 2009 and was published in 2013 in PLOS-One, which is one step above appearing on Arxiv. PLOS-One publishes some good things (so does Arxiv) but it’s the place</p><p>3 0.91543669 <a title="1865-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-The_reverse-journal-submission_system.html">1393 andrew gelman stats-2012-06-26-The reverse-journal-submission system</a></p>
<p>Introduction: I’ve whined before in this space that some of my most important, innovative, and influential papers are really hard to get published.  I’ll go through endless hassle with a journal or sometimes several journals until I find some place willing to publish.  It’s just irritating.
 
I was thinking about this recently because a colleague and I just finished a paper that I love love love.  But I can’t figure out where to submit it.  This is a paper for which I would prefer the so-called reverse-journal-submission approach.  Instead of sending the paper to journal after journal after journal, waiting years until an acceptance (recall that, unless you’re Bruno Frey, you’re not allowed to submit the same paper to multiple journals simultaneously), you post the paper on a public site, and then journals compete to see who gets to publish it.  I think that system would work well with a paper like this which is offbeat but has a nontrivial chance of becoming highly influential.
 
P.S.  Just to clar</p><p>4 0.90145147 <a title="1865-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-01-Arrow%E2%80%99s_theorem_update.html">883 andrew gelman stats-2011-09-01-Arrow’s theorem update</a></p>
<p>Introduction: Someone pointed me to  this letter  to Bruno Frey from the editor of the Journal of Economic Perspectives.  ( Background here , also  more here  from Olaf Storbeck.)  The journal editor was upset about Frey’s self-plagiarism, and Frey responded with an apology:
  
It was a grave mistake on our part for which we deeply apologize. It should never have happened. This is deplorable. . . . Please be assured that we take all precautions and measures that this unfortunate event does not happen again, with any journal.
  
What I wonder is:  How “deplorable” does Frey really think this is?  You don’t publish a paper in 5 different places by accident!  Is Frey saying that he knew this was deplorable back then and he did it anyway, based on calculation balancing the gains from multiple publications vs. the potential losses if he got caught?  Or is he saying that the conduct is deplorable, but he didn’t realize it was deplorable when he did it?
 
My guess is that Frey does not actually think the r</p><p>5 0.90068024 <a title="1865-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-24-Difficulties_in_publishing_non-replications_of_implausible_findings.html">1137 andrew gelman stats-2012-01-24-Difficulties in publishing non-replications of implausible findings</a></p>
<p>Introduction: Eric Tassone points me to  this  news article by Christopher Shea on the challenges of debunking ESP.  Shea  writes :
  
Earlier this year, a major psychology journal published a paper suggesting that there was some evidence for “pre-cognition,” a form of ESP. Stuart Ritchie, a doctoral student at the University of Edinburgh, is part of a team that tried, but failed, to replicate those results. Here, he tells the Chronicle of Higher Education’s Tom Bartlett about the difficulties he’s had getting the results published.


Several journals told the team they wouldn’t publish a study that did no more than disprove a previous study. . . . An editor at another journal said he’d “only accept our paper if we ran a fourth experiment where we got a believer [in ESP] to run all the participants, to control for . . . experimenter effects.”
  
My reaction is, this isn’t as easy a question as it might seem.  At first, one’s reaction might share Ritchie’s frustration that a shoddy paper by Bem got p</p><p>6 0.89530909 <a title="1865-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-01-I_owe_it_all_to_the_haters.html">834 andrew gelman stats-2011-08-01-I owe it all to the haters</a></p>
<p>7 0.89138669 <a title="1865-lsi-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-30-I_posted_this_as_a_comment_on_a_sociology_blog.html">2353 andrew gelman stats-2014-05-30-I posted this as a comment on a sociology blog</a></p>
<p>8 0.88308704 <a title="1865-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-30-Retracted_articles_and_unethical_behavior_in_economics_journals%3F.html">1435 andrew gelman stats-2012-07-30-Retracted articles and unethical behavior in economics journals?</a></p>
<p>9 0.88198584 <a title="1865-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-15-A_statistical_research_project%3A__Weeding_out_the_fraudulent_citations.html">1321 andrew gelman stats-2012-05-15-A statistical research project:  Weeding out the fraudulent citations</a></p>
<p>10 0.88130254 <a title="1865-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-26-Suggested_resolution_of_the_Bem_paradox.html">1139 andrew gelman stats-2012-01-26-Suggested resolution of the Bem paradox</a></p>
<p>11 0.87268674 <a title="1865-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-25-A_new_Bem_theory.html">1998 andrew gelman stats-2013-08-25-A new Bem theory</a></p>
<p>12 0.87003863 <a title="1865-lsi-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-04-Literal_vs._rhetorical.html">2233 andrew gelman stats-2014-03-04-Literal vs. rhetorical</a></p>
<p>13 0.86963308 <a title="1865-lsi-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-26-Musical_chairs_in_econ_journals.html">371 andrew gelman stats-2010-10-26-Musical chairs in econ journals</a></p>
<p>14 0.86900645 <a title="1865-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-16-%E2%80%9CGroundbreaking_or_Definitive%3F_Journals_Need_to_Pick_One%E2%80%9D.html">1122 andrew gelman stats-2012-01-16-“Groundbreaking or Definitive? Journals Need to Pick One”</a></p>
<p>15 0.86837041 <a title="1865-lsi-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-19-The_replication_and_criticism_movement_is_not_about_suppressing_speculative_research%3B_rather%2C_it%E2%80%99s_all_about_enabling_science%E2%80%99s_fabled_self-correcting_nature.html">2217 andrew gelman stats-2014-02-19-The replication and criticism movement is not about suppressing speculative research; rather, it’s all about enabling science’s fabled self-correcting nature</a></p>
<p>16 0.84552729 <a title="1865-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-22-Arrow%E2%80%99s_other_theorem.html">675 andrew gelman stats-2011-04-22-Arrow’s other theorem</a></p>
<p>17 0.84093374 <a title="1865-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-13-How_should_journals_handle_replication_studies%3F.html">762 andrew gelman stats-2011-06-13-How should journals handle replication studies?</a></p>
<p>18 0.83102244 <a title="1865-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-14-A_model_rejection_letter.html">1118 andrew gelman stats-2012-01-14-A model rejection letter</a></p>
<p>19 0.82313275 <a title="1865-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-20-Proposals_for_alternative_review_systems_for_scientific_work.html">1273 andrew gelman stats-2012-04-20-Proposals for alternative review systems for scientific work</a></p>
<p>20 0.82244092 <a title="1865-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-01-Post-publication_peer_review%3A__How_it_%28sometimes%29_really_works.html">2004 andrew gelman stats-2013-09-01-Post-publication peer review:  How it (sometimes) really works</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(15, 0.139), (16, 0.119), (18, 0.021), (21, 0.017), (24, 0.142), (28, 0.016), (45, 0.017), (48, 0.053), (63, 0.017), (86, 0.019), (99, 0.311)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9719733 <a title="1865-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-20-What_happened_that_the_journal_Psychological_Science_published_a_paper_with_no_identifiable_strengths%3F.html">1865 andrew gelman stats-2013-05-20-What happened that the journal Psychological Science published a paper with no identifiable strengths?</a></p>
<p>Introduction: The other day we  discussed  that paper on ovulation and voting (you may recall that the authors reported a scattered bunch of comparisons, significance tests, and p-values, and I recommended that they would’ve done better to simply report complete summaries of their data, so that readers could see the comparisons of interest in full context), and I was thinking a bit more about why I was so bothered that it was published in Psychological Science, which I’d thought of as a serious research journal. 
   
My concern isn’t just that that the paper is bad—after all, lots of bad papers get published—but rather that it had nothing really going for it,  except  that it was headline bait.  It was a survey done on Mechanical Turk, that’s it.  No clever design, no clever questions, no care in dealing with nonresponse problems, no innovative data analysis, no nothing.  The paper had nothing to offer, except that it had no obvious flaws.  Psychology is a huge field full of brilliant researchers.</p><p>2 0.96828246 <a title="1865-lda-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-30-I_posted_this_as_a_comment_on_a_sociology_blog.html">2353 andrew gelman stats-2014-05-30-I posted this as a comment on a sociology blog</a></p>
<p>Introduction: I discussed two problems:
 
1.  An artificial scarcity applied to journal publication, a scarcity which I believe is being enforced based on a monetary principle of not wanting to reduce the value of publication.  The problem is that journals don’t just spread information and improve communication, they also represent chits for hiring and promotion.  I’d prefer to separate these two aspects of publication. To keep these functions tied together seems to me like a terrible mistake.  It would be as if, instead of using dollar bills as currency, we were to just use  paper , and then if the government kept paper artificially scarce to retain the value of money, so that we were reduced to scratching notes to each other on walls and tables.
 
2.  The discontinuous way in which unpublished papers and submissions to journals are taken as highly suspect and requiring a strong justification of all methods and assumptions, but once a paper becomes published its conclusions are taken as true unless</p><p>3 0.96802604 <a title="1865-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-06-W%E2%80%99man_%3C_W%E2%80%99pedia%2C_again.html">945 andrew gelman stats-2011-10-06-W’man < W’pedia, again</a></p>
<p>Introduction: Blogger Deep Climate  looks at  another paper by the 2002 recipient of the American Statistical Association’s Founders award. This time it’s not funny, it’s just sad. 
   
Here’s Wikipedia on simulated annealing:
  
By analogy with this physical process, each step of the SA algorithm replaces the current solution by a random “nearby” solution, chosen with a probability that depends on the difference between the corresponding function values and on a global parameter T (called the temperature), that is gradually decreased during the process. The dependency is such that the current solution changes almost randomly when T is large, but increasingly “downhill” as T goes to zero. The allowance for “uphill” moves saves the method from becoming stuck at local minima—which are the bane of greedier methods.
  
And here’s Wegman:
  
During each step of the algorithm, the variable that will eventually represent the minimum is replaced by a random solution that is chosen according to a temperature</p><p>4 0.96733654 <a title="1865-lda-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>Introduction: Prasanta Bandyopadhyay and Gordon Brittan  write :
  
We introduce a distinction, unnoticed in the literature, between four varieties of objective Bayesianism. What we call ‘strong objective Bayesianism’ is characterized by two claims, that all scientific inference is ‘logical’ and that, given the same background information two agents will ascribe a unique probability to their priors. We think that neither of these claims can be sustained; in this sense, they are ‘dogmatic’. The first fails to recognize that some scientific inference, in particular that concerning evidential relations, is not (in the appropriate sense) logical, the second fails to provide a non-question-begging account of ‘same background information’. We urge that a suitably objective Bayesian account of scientific inference does not require either of the claims. Finally, we argue that Bayesianism needs to be fine-grained in the same way that Bayesians fine-grain their beliefs.
  
I have not read their paper in detai</p><p>5 0.9634589 <a title="1865-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-08-More_on_those_dudes_who_will_pay_your_professor_%248000_to_assign_a_book_to_your_class%2C_and_related_stories_about_small-time_sleazoids.html">329 andrew gelman stats-2010-10-08-More on those dudes who will pay your professor $8000 to assign a book to your class, and related stories about small-time sleazoids</a></p>
<p>Introduction: After noticing  these remarks  on expensive textbooks and  this comment  on the company that bribes professors to use their books, Preston McAfee pointed me to  this update  (complete with a picture of some guy who keeps threatening to sue him but never gets around to it).
 
The story McAfee tells is sad but also hilarious. Especially the part about “smuck.”  It all looks like one more symptom of the imploding market for books.  Prices for intro stat and econ books go up and up (even mediocre textbooks routinely cost $150), and the publishers put more and more effort into promotion.
 
McAfee adds:
  
I [McAfee] hope a publisher sues me about posting the articles I wrote.  Even a takedown notice would be fun.  I would be pretty happy to start posting about that, especially when some of them are charging $30 per article.


Ted Bergstrom and I used state Freedom of Information acts to extract the journal price deals at state university libraries.  We have about 35 of them so far.  Like te</p><p>6 0.96098852 <a title="1865-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Interpreting_interactions_in_discrete-data_regression.html">1908 andrew gelman stats-2013-06-21-Interpreting interactions in discrete-data regression</a></p>
<p>7 0.96093911 <a title="1865-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-12-The_importance_of_style_in_academic_writing.html">902 andrew gelman stats-2011-09-12-The importance of style in academic writing</a></p>
<p>8 0.96041518 <a title="1865-lda-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-08-Gratuitous_use_of_%E2%80%9CBayesian_Statistics%2C%E2%80%9D_a_branding_issue%3F.html">133 andrew gelman stats-2010-07-08-Gratuitous use of “Bayesian Statistics,” a branding issue?</a></p>
<p>9 0.96026063 <a title="1865-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-19-Statistical_discrimination_again.html">1541 andrew gelman stats-2012-10-19-Statistical discrimination again</a></p>
<p>10 0.95997047 <a title="1865-lda-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-29-%E2%80%9CQuestioning_The_Lancet%2C_PLOS%2C_And_Other_Surveys_On_Iraqi_Deaths%2C_An_Interview_With_Univ._of_London_Professor_Michael_Spagat%E2%80%9D.html">2191 andrew gelman stats-2014-01-29-“Questioning The Lancet, PLOS, And Other Surveys On Iraqi Deaths, An Interview With Univ. of London Professor Michael Spagat”</a></p>
<p>11 0.95951217 <a title="1865-lda-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-19-The_replication_and_criticism_movement_is_not_about_suppressing_speculative_research%3B_rather%2C_it%E2%80%99s_all_about_enabling_science%E2%80%99s_fabled_self-correcting_nature.html">2217 andrew gelman stats-2014-02-19-The replication and criticism movement is not about suppressing speculative research; rather, it’s all about enabling science’s fabled self-correcting nature</a></p>
<p>12 0.95680016 <a title="1865-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-14-Battle_of_the_Americans%3A__Writer_at_the_American_Enterprise_Institute_disparages_the_American_Political_Science_Association.html">274 andrew gelman stats-2010-09-14-Battle of the Americans:  Writer at the American Enterprise Institute disparages the American Political Science Association</a></p>
<p>13 0.95467508 <a title="1865-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-25-A_new_Bem_theory.html">1998 andrew gelman stats-2013-08-25-A new Bem theory</a></p>
<p>14 0.95383674 <a title="1865-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-30-Retracted_articles_and_unethical_behavior_in_economics_journals%3F.html">1435 andrew gelman stats-2012-07-30-Retracted articles and unethical behavior in economics journals?</a></p>
<p>15 0.95370924 <a title="1865-lda-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-27-%E2%80%9CWhat_Can_we_Learn_from_the_Many_Labs_Replication_Project%3F%E2%80%9D.html">2227 andrew gelman stats-2014-02-27-“What Can we Learn from the Many Labs Replication Project?”</a></p>
<p>16 0.95276916 <a title="1865-lda-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-%E2%80%9CThe_British_amateur_who_debunked_the_mathematics_of_happiness%E2%80%9D.html">2177 andrew gelman stats-2014-01-19-“The British amateur who debunked the mathematics of happiness”</a></p>
<p>17 0.95193493 <a title="1865-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-31-How_to_fix_the_tabloids%3F__Toward_replicable_social_science_research.html">1878 andrew gelman stats-2013-05-31-How to fix the tabloids?  Toward replicable social science research</a></p>
<p>18 0.95142257 <a title="1865-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-22-Likelihood_Ratio_%E2%89%A0_1_Journal.html">1774 andrew gelman stats-2013-03-22-Likelihood Ratio ≠ 1 Journal</a></p>
<p>19 0.95118964 <a title="1865-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-With_a_bit_of_precognition%2C_you%E2%80%99d_have_known_I_was_going_to_post_again_on_this_topic%2C_and_with_a_lot_of_precognition%2C_you%E2%80%99d_have_known_I_was_going_to_post_today.html">576 andrew gelman stats-2011-02-15-With a bit of precognition, you’d have known I was going to post again on this topic, and with a lot of precognition, you’d have known I was going to post today</a></p>
<p>20 0.95094156 <a title="1865-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-09-A_tale_of_two_discussion_papers.html">1848 andrew gelman stats-2013-05-09-A tale of two discussion papers</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
