<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-1981" href="#">andrew_gelman_stats-2013-1981</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-1981-html" href="http://andrewgelman.com/2013/08/14/the-robust-beauty-of-improper-linear-models-in-decision-making/">html</a></p><p>Introduction: Andreas Graefe writes (see  here   here   here ):
  
The usual procedure for developing linear models to predict any kind of target variable is to identify a subset of most important predictors and to estimate weights that provide the best possible solution for a given sample. The resulting “optimally” weighted linear composite is then used when predicting new data. This approach is useful in situations with large and reliable datasets and few predictor variables. However, a large body of analytical and empirical evidence since the 1970s shows that the weighting of variables is of little, if any, value in situations with small and noisy datasets and a large number of predictor variables. In such situations, including all relevant variables is more important than their weighting. These findings have yet to impact many fields. This study uses data from nine established U.S. election-forecasting models whose forecasts are regularly published in academic journals to demonstrate the value o</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Andreas Graefe writes (see  here   here   here ):    The usual procedure for developing linear models to predict any kind of target variable is to identify a subset of most important predictors and to estimate weights that provide the best possible solution for a given sample. [sent-1, score-0.873]
</p><p>2 The resulting “optimally” weighted linear composite is then used when predicting new data. [sent-2, score-0.608]
</p><p>3 This approach is useful in situations with large and reliable datasets and few predictor variables. [sent-3, score-0.912]
</p><p>4 However, a large body of analytical and empirical evidence since the 1970s shows that the weighting of variables is of little, if any, value in situations with small and noisy datasets and a large number of predictor variables. [sent-4, score-1.779]
</p><p>5 In such situations, including all relevant variables is more important than their weighting. [sent-5, score-0.413]
</p><p>6 election-forecasting models whose forecasts are regularly published in academic journals to demonstrate the value of weighting all predictors equally and including all relevant variables in the model. [sent-9, score-1.53]
</p><p>7 Across the ten elections from 1976 to 2012, equally weighted predictors reduced the forecast error of the original regression models on average by four percent. [sent-10, score-1.421]
</p><p>8 An equal-weights model that includes all variables provided well-calibrated forecasts that reduced the error of the most accurate regression model by 29 percent. [sent-11, score-1.134]
</p><p>9 I haven’t actually read the paper, but I have no reason to disbelieve it. [sent-12, score-0.138]
</p><p>10 I assume that you could get even better performance using a Bayesian approach that puts a strong prior distribution on the coefficients being close to each other. [sent-13, score-0.432]
</p><p>11 This can be done, for example, in a multiplicative model like this:   Suppose your original model is y = b_0 + b_1*x_1 + b_2*x_2 + . [sent-14, score-0.518]
</p><p>12 + b_10*x_10, and suppose you want the coefficients b_3,…,b_10 to be close to each other. [sent-17, score-0.368]
</p><p>13 A Bayesian version could set g_j ~ N(1,s^2), where s is some small value such as 0. [sent-20, score-0.247]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('situations', 0.249), ('variables', 0.213), ('predictors', 0.207), ('weighted', 0.197), ('reduced', 0.182), ('equally', 0.182), ('weighting', 0.181), ('forecasts', 0.179), ('datasets', 0.172), ('predictor', 0.168), ('value', 0.16), ('coefficients', 0.148), ('model', 0.144), ('andreas', 0.138), ('disbelieve', 0.138), ('large', 0.131), ('linear', 0.13), ('optimally', 0.127), ('multiplicative', 0.124), ('composite', 0.118), ('close', 0.112), ('analytical', 0.111), ('models', 0.11), ('suppose', 0.108), ('relevant', 0.107), ('original', 0.106), ('nine', 0.105), ('error', 0.103), ('reliable', 0.099), ('regularly', 0.098), ('regression', 0.094), ('established', 0.093), ('approach', 0.093), ('including', 0.093), ('weights', 0.092), ('body', 0.092), ('subset', 0.091), ('small', 0.087), ('forecast', 0.087), ('target', 0.087), ('resulting', 0.086), ('noisy', 0.084), ('developing', 0.08), ('ten', 0.079), ('puts', 0.079), ('predicting', 0.077), ('bayesian', 0.076), ('procedure', 0.076), ('accurate', 0.075), ('elections', 0.074)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1981-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>Introduction: Andreas Graefe writes (see  here   here   here ):
  
The usual procedure for developing linear models to predict any kind of target variable is to identify a subset of most important predictors and to estimate weights that provide the best possible solution for a given sample. The resulting “optimally” weighted linear composite is then used when predicting new data. This approach is useful in situations with large and reliable datasets and few predictor variables. However, a large body of analytical and empirical evidence since the 1970s shows that the weighting of variables is of little, if any, value in situations with small and noisy datasets and a large number of predictor variables. In such situations, including all relevant variables is more important than their weighting. These findings have yet to impact many fields. This study uses data from nine established U.S. election-forecasting models whose forecasts are regularly published in academic journals to demonstrate the value o</p><p>2 0.18051054 <a title="1981-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-21-Building_a_regression_model_._._._with_only_27_data_points.html">1506 andrew gelman stats-2012-09-21-Building a regression model . . . with only 27 data points</a></p>
<p>Introduction: Dan Silitonga writes:
  
I was wondering whether you would have any advice on building a regression model on a very small datasets. I’m in the midst of revamping the model to predict tax collections from unincorporated businesses. But I only have 27 data points, 27 years of annual data.  Any advice would be much appreciated.
  
My reply:
 
This sounds tough, especially given that 27 years of annual data isn’t even 27 independent data points.  
 
I have various essentially orthogonal suggestions:
 
1 [added after seeing John Cook's comment below].  Do your best, making as many assumptions as you need.  In a Bayesian context, this means that you’d use a strong and informative prior and let the data update it as appropriate.  In a less formal setting, you’d start with a guess of a model and then alter it to the extent that your data contradict your original guess.
 
2.  Get more data.  Not by getting information on more years (I assume you can’t do that) but by breaking up the data you do</p><p>3 0.17437741 <a title="1981-tfidf-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Blending_results_from_two_relatively_independent_multi-level_models.html">250 andrew gelman stats-2010-09-02-Blending results from two relatively independent multi-level models</a></p>
<p>Introduction: David Shor writes:
  
 
I [Shor] am working on a Bayesian Forecasting model for the Mid-term elections that has two components:


1) A poll aggregation system with pooled and hierarchical house and design effects across every race with polls (Average Standard error for house seat level vote-share ~.055)


2) A Bafumi-style regression that applies national-swing to individual seats. (Average Standard error for house seat level vote-share ~.06)


Since these two estimates are essentially independent, estimates can probably be made more accurate by pooling them together. But If a house effect changes in one draw, that changes estimates in every race. Changes in regression coefficients and National swing have a similar effect.  In the face of high and possibly differing seat-to-seat correlations from each method, I’m not sure what the correct way to “blend” these models would be, either for individual or top-line seat estimates.


In the mean-time, I’m just creating variance-weighted avera</p><p>4 0.16535161 <a title="1981-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-06-17_groups%2C_6_group-level_predictors%3A__What_to_do%3F.html">1248 andrew gelman stats-2012-04-06-17 groups, 6 group-level predictors:  What to do?</a></p>
<p>Introduction: Yi-Chun Ou writes: 
  
  
I am using a multilevel model with three levels. I read that you wrote a book about multilevel models, and wonder if you can solve the following question.  


The data structure is like this: 


Level one: customer (8444 customers) 
Level two: companys (90 companies) 
Level three: industry (17 industries) 


I use 6 level-three variables (i.e. industry characteristics) to explain the variance of the level-one effect across industries. The question here is whether there is an over-fitting problem since there are only 17 industries. I understand that this must be a problem for non-multilevel models, but is it also a problem for multilevel models?
  
My reply:  Yes, this could be a problem.  I’d suggest combining some of your variables into a common score, or using only some of the variables, or using strong priors to control the inferences.  This is an interesting and important area of statistics research, to do this sort of thing systematically.  There’s lots o</p><p>5 0.15438822 <a title="1981-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-26-Some_thoughts_on_survey_weighting.html">1430 andrew gelman stats-2012-07-26-Some thoughts on survey weighting</a></p>
<p>Introduction: From a comment I made in an email exchange:
 
My  work  on survey adjustments has very much been inspired by the ideas of Rod Little.  Much of my efforts have gone toward the goal of integrating hierarchical modeling (which is so helpful for small-area estimation) with post stratification (which adjusts for known differences between sample and population).  In the surveys I’ve dealt with, nonresponse/nonavailability can be a big issue, and I’ve always tried to emphasize that (a) the probability of a person being included in the sample is just about never known, and (b) even if this probability were known, I’d rather know the empirical n/N than the probability p (which is only valid in expectation).  Regarding nonparametric modeling:  I haven’t done much of that (although I hope to at some point) but Rod and his students have.
 
As I wrote in the first sentence of the above-linked paper, I do think the current theory and practice of survey weighting is a mess, in that much depends on so</p><p>6 0.15221897 <a title="1981-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-Piss-poor_monocausal_social_science.html">1196 andrew gelman stats-2012-03-04-Piss-poor monocausal social science</a></p>
<p>7 0.1521244 <a title="1981-tfidf-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-19-Analysis_of_survey_data%3A_Design_based_models_vs._hierarchical_modeling%3F.html">352 andrew gelman stats-2010-10-19-Analysis of survey data: Design based models vs. hierarchical modeling?</a></p>
<p>8 0.14681755 <a title="1981-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-23-Question_13_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1340 andrew gelman stats-2012-05-23-Question 13 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>9 0.14513856 <a title="1981-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-05-Understanding_regression_models_and_regression_coefficients.html">1656 andrew gelman stats-2013-01-05-Understanding regression models and regression coefficients</a></p>
<p>10 0.14495718 <a title="1981-tfidf-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-03-Some_thoughts_on_election_forecasting.html">391 andrew gelman stats-2010-11-03-Some thoughts on election forecasting</a></p>
<p>11 0.14265393 <a title="1981-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Allowing_interaction_terms_to_vary.html">753 andrew gelman stats-2011-06-09-Allowing interaction terms to vary</a></p>
<p>12 0.14073414 <a title="1981-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-11-%E2%80%9CInformative_g-Priors_for_Logistic_Regression%E2%80%9D.html">2017 andrew gelman stats-2013-09-11-“Informative g-Priors for Logistic Regression”</a></p>
<p>13 0.13533959 <a title="1981-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>14 0.13479897 <a title="1981-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-07-Prior_distributions_for_regression_coefficients.html">1486 andrew gelman stats-2012-09-07-Prior distributions for regression coefficients</a></p>
<p>15 0.13311532 <a title="1981-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>16 0.1317268 <a title="1981-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>17 0.13167182 <a title="1981-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-05-Question_26_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1367 andrew gelman stats-2012-06-05-Question 26 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>18 0.12714127 <a title="1981-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>19 0.12677287 <a title="1981-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>20 0.12669091 <a title="1981-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-25-Bayes-respecting_experimental_design_and_other_things.html">1955 andrew gelman stats-2013-07-25-Bayes-respecting experimental design and other things</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.23), (1, 0.188), (2, 0.101), (3, -0.042), (4, 0.052), (5, 0.022), (6, 0.011), (7, -0.058), (8, 0.07), (9, 0.065), (10, 0.105), (11, 0.017), (12, -0.024), (13, 0.023), (14, -0.047), (15, 0.015), (16, 0.03), (17, 0.008), (18, 0.028), (19, -0.012), (20, -0.018), (21, 0.082), (22, 0.009), (23, -0.021), (24, 0.005), (25, 0.015), (26, 0.042), (27, -0.065), (28, -0.019), (29, -0.017), (30, 0.064), (31, 0.06), (32, 0.017), (33, -0.02), (34, 0.007), (35, -0.002), (36, 0.057), (37, 0.014), (38, -0.011), (39, -0.063), (40, -0.045), (41, -0.041), (42, 0.022), (43, 0.029), (44, 0.01), (45, 0.009), (46, 0.03), (47, 0.05), (48, 0.045), (49, 0.034)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96630758 <a title="1981-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>Introduction: Andreas Graefe writes (see  here   here   here ):
  
The usual procedure for developing linear models to predict any kind of target variable is to identify a subset of most important predictors and to estimate weights that provide the best possible solution for a given sample. The resulting “optimally” weighted linear composite is then used when predicting new data. This approach is useful in situations with large and reliable datasets and few predictor variables. However, a large body of analytical and empirical evidence since the 1970s shows that the weighting of variables is of little, if any, value in situations with small and noisy datasets and a large number of predictor variables. In such situations, including all relevant variables is more important than their weighting. These findings have yet to impact many fields. This study uses data from nine established U.S. election-forecasting models whose forecasts are regularly published in academic journals to demonstrate the value o</p><p>2 0.82940316 <a title="1981-lsi-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Question_about_standard_range_for_social_science_correlations.html">257 andrew gelman stats-2010-09-04-Question about standard range for social science correlations</a></p>
<p>Introduction: Andrew Eppig writes:
  
I’m a physicist by training who is transitioning to the social sciences. I recently came across a  reference  in the Economist to a paper on IQ and parasites which I read as I have more than a passing interest in IQ research (having read much that you and others (e.g., Shalizi, Wicherts) have written). In this paper I note that the authors find a very high correlation between national IQ and parasite prevalence. The strength of the correlation (-0.76 to -0.82) surprised me, as I’m used to much weaker correlations in the social sciences. To me, it’s a bit too high, suggesting that there are other factors at play or that one of the variables is merely a proxy for a large number of other variables. But I have no basis for this other than a gut feeling and a memory of a plot on  Language Log  about the distribution of correlation coefficients in social psychology.


So my question is this: Is a correlation in the range of (-0.82,-0.76) more likely to be a correlatio</p><p>3 0.80331892 <a title="1981-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-31-Using_factor_analysis_or_principal_components_analysis_or_measurement-error_models_for_biological_measurements_in_archaeology%3F.html">1094 andrew gelman stats-2011-12-31-Using factor analysis or principal components analysis or measurement-error models for biological measurements in archaeology?</a></p>
<p>Introduction: Greg Campbell writes:
  
I am a Canadian archaeologist (BSc in Chemistry) researching the past human use of European Atlantic shellfish. After two decades of practice I am finally getting a MA in archaeology at Reading. I am seeing if the habitat or size of harvested mussels (Mytilus edulis) can be reconstructed from measurements of the umbo (the pointy end, and the only bit that survives well in archaeological deposits) using log-transformed measurements (or allometry; relationships between dimensions are more likely exponential than linear). 
Of course multivariate regressions in most statistics packages (Minitab, SPSS, SAS) assume you are trying to predict one variable from all the others (a Model I regression), and use ordinary least squares to fit the regression line. For organismal dimensions this makes little sense, since all the dimensions are (at least in theory) free to change their mutual proportions during growth. So there is no predictor and predicted, mutual variation of</p><p>4 0.80075222 <a title="1981-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-01-Modeling_y_%3D_a_%2B_b_%2B_c.html">1294 andrew gelman stats-2012-05-01-Modeling y = a + b + c</a></p>
<p>Introduction: Brandon Behlendorf writes: 
  
  
I [Behlendorf] am replicating some previous research using OLS [he's talking about what we call "linear regression"---ed.] to regress a logged rate (to reduce skew) of Y on a number of predictors (Xs).  Y is the count of a phenomena divided by the population of the unit of the analysis.  The problem that I am encountering is that Y is composite count of a number of distinct phenomena [A+B+C], and these phenomena are not uniformly distributed across the sample.   Most of the research in this area has conducted regressions either with Y or with individual phenomena [A or B or C] as the dependent variable.  Yet it seems that if [A, B, C] are not uniformly distributed across the sample of units in the same proportion, then the use of Y would be biased, since as a count of [A+B+C] divided by the population, it would treat as equivalent units both [2+0.5+1.5] and [4+0+0]. 


My goal is trying to find a methodology which allows a researcher to regress Y on a</p><p>5 0.79499233 <a title="1981-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-18-Check_your_missing-data_imputations_using_cross-validation.html">1218 andrew gelman stats-2012-03-18-Check your missing-data imputations using cross-validation</a></p>
<p>Introduction: Elena Grewal writes:
  
I am currently using the iterative regression imputation model as implemented in the Stata ICE package. I am using data from a survey of about 90,000 students in 142 schools and my variable of interest is parent level of education. I want only this variable to be imputed with as little bias as possible as I am not using any other variable. So I scoured the survey for every variable I thought could possibly predict parent education. The main variable I found is parent occupation, which explains about 35% of the variance in parent education for the students with complete data on both. I then include the 20 other variables I found in the survey in a regression predicting parent education, which explains about 40% of the variance in parent education for students with complete data on all the variables. 


My question is this: many of the other variables I found have more missing values than the parent education variable, and also, although statistically significant</p><p>6 0.78706229 <a title="1981-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-24-How_few_respondents_are_reasonable_to_use_when_calculating_the_average_by_county%3F.html">627 andrew gelman stats-2011-03-24-How few respondents are reasonable to use when calculating the average by county?</a></p>
<p>7 0.78379107 <a title="1981-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-15-R-squared_for_multilevel_models.html">1121 andrew gelman stats-2012-01-15-R-squared for multilevel models</a></p>
<p>8 0.76719099 <a title="1981-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-18-Standardizing_regression_inputs.html">1462 andrew gelman stats-2012-08-18-Standardizing regression inputs</a></p>
<p>9 0.75983816 <a title="1981-lsi-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Blending_results_from_two_relatively_independent_multi-level_models.html">250 andrew gelman stats-2010-09-02-Blending results from two relatively independent multi-level models</a></p>
<p>10 0.75627017 <a title="1981-lsi-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-01-Imputing_count_data.html">14 andrew gelman stats-2010-05-01-Imputing count data</a></p>
<p>11 0.75062197 <a title="1981-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-15-Still_more_Mr._P_in_public_health.html">770 andrew gelman stats-2011-06-15-Still more Mr. P in public health</a></p>
<p>12 0.74292189 <a title="1981-lsi-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Interactions_of_predictors_in_a_causal_model.html">251 andrew gelman stats-2010-09-02-Interactions of predictors in a causal model</a></p>
<p>13 0.73973471 <a title="1981-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-03-Uncertainty_in_parameter_estimates_using_multilevel_models.html">1966 andrew gelman stats-2013-08-03-Uncertainty in parameter estimates using multilevel models</a></p>
<p>14 0.73677826 <a title="1981-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-Piss-poor_monocausal_social_science.html">1196 andrew gelman stats-2012-03-04-Piss-poor monocausal social science</a></p>
<p>15 0.73475736 <a title="1981-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-15-Exploratory_multilevel_analysis_when_group-level_variables_are_of_importance.html">1900 andrew gelman stats-2013-06-15-Exploratory multilevel analysis when group-level variables are of importance</a></p>
<p>16 0.72407746 <a title="1981-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-04-What_are_the_key_assumptions_of_linear_regression%3F.html">1967 andrew gelman stats-2013-08-04-What are the key assumptions of linear regression?</a></p>
<p>17 0.72318095 <a title="1981-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-02-Interaction-based_feature_selection_and_classification_for_high-dimensional_biological_data.html">1703 andrew gelman stats-2013-02-02-Interaction-based feature selection and classification for high-dimensional biological data</a></p>
<p>18 0.72309744 <a title="1981-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-05-Understanding_regression_models_and_regression_coefficients.html">1656 andrew gelman stats-2013-01-05-Understanding regression models and regression coefficients</a></p>
<p>19 0.72027892 <a title="1981-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Interpreting_interactions_in_discrete-data_regression.html">1908 andrew gelman stats-2013-06-21-Interpreting interactions in discrete-data regression</a></p>
<p>20 0.71846884 <a title="1981-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.029), (16, 0.076), (21, 0.032), (24, 0.177), (50, 0.08), (53, 0.027), (63, 0.034), (74, 0.013), (76, 0.027), (86, 0.071), (96, 0.019), (99, 0.324)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9823482 <a title="1981-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>Introduction: Andreas Graefe writes (see  here   here   here ):
  
The usual procedure for developing linear models to predict any kind of target variable is to identify a subset of most important predictors and to estimate weights that provide the best possible solution for a given sample. The resulting “optimally” weighted linear composite is then used when predicting new data. This approach is useful in situations with large and reliable datasets and few predictor variables. However, a large body of analytical and empirical evidence since the 1970s shows that the weighting of variables is of little, if any, value in situations with small and noisy datasets and a large number of predictor variables. In such situations, including all relevant variables is more important than their weighting. These findings have yet to impact many fields. This study uses data from nine established U.S. election-forecasting models whose forecasts are regularly published in academic journals to demonstrate the value o</p><p>2 0.9724623 <a title="1981-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-16-Memo_to_Reinhart_and_Rogoff%3A__I_think_it%E2%80%99s_best_to_admit_your_errors_and_go_on_from_there.html">1805 andrew gelman stats-2013-04-16-Memo to Reinhart and Rogoff:  I think it’s best to admit your errors and go on from there</a></p>
<p>Introduction: Jeff Ratto points me to  this  news article by Dean Baker reporting the work of three economists, Thomas Herndon, Michael Ash, and Robert Pollin, who  found  errors in a much-cited  article  by Carmen Reinhart and Kenneth Rogoff analyzing historical statistics of economic growth and public debt.  Mike Konczal  provides  a clear summary; that’s where I got the above image.
 
 Errors in data processing and data analysis 
 
It turns out that Reinhart and Rogoff flubbed it.  Herndon et al. write of “spreadsheet errors, omission of available data, weighting, and transcription.”  The spreadsheet errors are the most embarrassing, but the other choices in data analysis seem pretty bad too.  It can be tough to work with small datasets, so I have sympathy for Reinhart and Rogoff, but it does look like they were jumping to conclusions in their paper.  Perhaps the urgency of the topic moved them to publish as fast as possible rather than carefully considering the impact of their data-analytic choi</p><p>3 0.96793872 <a title="1981-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-27-Why_can%E2%80%99t_I_be_more_like_Bill_James%2C_or%2C_The_use_of_default_and_default-like_models.html">541 andrew gelman stats-2011-01-27-Why can’t I be more like Bill James, or, The use of default and default-like models</a></p>
<p>Introduction: During our discussion of estimates of teacher performance, Steve Sailer  wrote :
  
I suspect we’re going to take years to work the kinks out of overall rating systems.


By way of analogy, Bill James kicked off the modern era of baseball statistics analysis around 1975. But he stuck to doing smaller scale analyses and avoided trying to build one giant overall model for rating players. In contrast, other analysts such as Pete Palmer rushed into building overall ranking systems, such as his 1984 book, but they tended to generate curious results such as the greatness of Roy Smalley Jr.. James held off until 1999 before unveiling his win share model for overall rankings.
  
I remember looking at Pete Palmer’s book many years ago and being disappointed that he did everything through his Linear Weights formula. A hit is worth X, a walk is worth Y, etc.  Some of this is good–it’s presumably an improvement on counting walks as 0 or 1 hits, also an improvement on counting doubles and triples a</p><p>4 0.96508157 <a title="1981-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-23-Peter_Bartlett_on_model_complexity_and_sample_size.html">1636 andrew gelman stats-2012-12-23-Peter Bartlett on model complexity and sample size</a></p>
<p>Introduction: Zach Shahn saw  this  and writes:
  
I just heard  a talk by Peter Bartlett  about model selection in “unlimited” data situations that essentially addresses this curve.


He talks about the problem of model selection given a computational budget (rather than given a sample size). You can either use your computational budget to get more data or fit a more complex model. He shows that you can get oracle inequalities for model selection algorithms under this paradigm (as long as the candidate models are nested).
  
I can’t follow all the details but it looks cool!  This is what they should be teaching in theoretical statistics class, instead of sufficient statistics and the Neyman-Pearson lemma and all that other old stuff.
 
Zach also asks:
  
I have a question about political science.  I always hear that the direction of the economy is one of the best predictors of election outcome.  What’s your thinking about the causal mechanism(s) behind the success of economic trend indicators as pr</p><p>5 0.96282089 <a title="1981-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-08-The_Supreme_Court_meets_the_fallacy_of_the_one-sided_bet.html">1793 andrew gelman stats-2013-04-08-The Supreme Court meets the fallacy of the one-sided bet</a></p>
<p>Introduction: Doug Hartmann  writes  ( link  from Jay Livingston):
  
Justice Antonin Scalia’s comment in the Supreme Court hearings on the U.S. law defining marriage that “there’s considerable disagreement among sociologists as to what the consequences of raising a child in a single-sex family, whether that is harmful to the child or not.”
  
Hartman argues that Scalia is factually incorrect—there is not actually “considerable disagreement among sociologists” on this issue—and quotes a recent  report  from the American Sociological Association to this effect.  Assuming there’s no other considerable group of sociologists (Hartman knows of only one  small  group) arguing otherwise, it seems that Hartman has a point.  Scalia would’ve been better off omitting the phrase “among sociologists”—then he’d have been on safe ground, because you can always find  somebody  to take a position on the issue. Jerry Falwell’s no longer around but there’s a lot more where he came from.  Even among scientists, there’s</p><p>6 0.96123296 <a title="1981-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>7 0.95966315 <a title="1981-lda-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-26-Difficulties_in_making_inferences_about_scientific_truth_from_distributions_of_published_p-values.html">2040 andrew gelman stats-2013-09-26-Difficulties in making inferences about scientific truth from distributions of published p-values</a></p>
<p>8 0.95959121 <a title="1981-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-19-Revised_evidence_for_statistical_standards.html">2140 andrew gelman stats-2013-12-19-Revised evidence for statistical standards</a></p>
<p>9 0.95915246 <a title="1981-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-23-Combining_survey_data_obtained_using_different_modes_of_sampling.html">777 andrew gelman stats-2011-06-23-Combining survey data obtained using different modes of sampling</a></p>
<p>10 0.95893276 <a title="1981-lda-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-23-Scientists_can_read_your_mind_._._._as_long_as_the%E2%80%99re_allowed_to_look_at_more_than_one_place_in_your_brain_and_then_make_a_prediction_after_seeing_what_you_actually_did.html">106 andrew gelman stats-2010-06-23-Scientists can read your mind . . . as long as the’re allowed to look at more than one place in your brain and then make a prediction after seeing what you actually did</a></p>
<p>11 0.95882922 <a title="1981-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>12 0.95845413 <a title="1981-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-02-Fighting_a_losing_battle.html">1518 andrew gelman stats-2012-10-02-Fighting a losing battle</a></p>
<p>13 0.9584384 <a title="1981-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-18-%E2%80%9CI_was_finding_the_test_so_irritating_and_boring_that_I_just_started_to_click_through_as_fast_as_I_could%E2%80%9D.html">351 andrew gelman stats-2010-10-18-“I was finding the test so irritating and boring that I just started to click through as fast as I could”</a></p>
<p>14 0.95839047 <a title="1981-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-The_statistical_significance_filter.html">899 andrew gelman stats-2011-09-10-The statistical significance filter</a></p>
<p>15 0.95832431 <a title="1981-lda-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-13-What_are_the_important_issues_in_ethics_and_statistics%3F__I%E2%80%99m_looking_for_your_input%21.html">1117 andrew gelman stats-2012-01-13-What are the important issues in ethics and statistics?  I’m looking for your input!</a></p>
<p>16 0.95801878 <a title="1981-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-14-Everyone%E2%80%99s_trading_bias_for_variance_at_some_point%2C_it%E2%80%99s_just_done_at_different_places_in_the_analyses.html">1763 andrew gelman stats-2013-03-14-Everyone’s trading bias for variance at some point, it’s just done at different places in the analyses</a></p>
<p>17 0.9578796 <a title="1981-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-31-A_data_visualization_manifesto.html">61 andrew gelman stats-2010-05-31-A data visualization manifesto</a></p>
<p>18 0.95772529 <a title="1981-lda-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>19 0.95758241 <a title="1981-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-21-Discussion_of_the_paper_by_Girolami_and_Calderhead_on_Bayesian_computation.html">288 andrew gelman stats-2010-09-21-Discussion of the paper by Girolami and Calderhead on Bayesian computation</a></p>
<p>20 0.95728016 <a title="1981-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-02-Does_a_professor%E2%80%99s_intervention_in_online_discussions_have_the_effect_of_prolonging_discussion_or_cutting_it_off%3F.html">2120 andrew gelman stats-2013-12-02-Does a professor’s intervention in online discussions have the effect of prolonging discussion or cutting it off?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
