<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1878 andrew gelman stats-2013-05-31-How to fix the tabloids?  Toward replicable social science research</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-1878" href="#">andrew_gelman_stats-2013-1878</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1878 andrew gelman stats-2013-05-31-How to fix the tabloids?  Toward replicable social science research</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-1878-html" href="http://andrewgelman.com/2013/05/31/how-to-fix-the-tabloids-toward-replicable-social-science-research/">html</a></p><p>Introduction: This seems to be the topic of the week.  Yesterday I posted on the sister blog  some further thoughts  on those “Psychological Science” papers on menstrual cycles, biceps size, and political attitudes, tied to a horrible press release from the journal Psychological Science hyping the biceps and politics study.
 
Then I was pointed to these  suggestions  from Richard Lucas and M. Brent Donnellan have on improving the replicability and reproducibility of research published in the Journal of Research in Personality:
  
It goes without saying that editors of scientific journals strive to publish research that is not only theoretically interesting but also methodologically rigorous. The goal is to select papers that advance the field. Accordingly, editors want to publish findings that can be reproduced and replicated by other scientists. Unfortunately, there has been a recent “crisis in confidence” among psychologists about the quality of psychological research (Pashler & Wagenmakers, 2012)</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Yesterday I posted on the sister blog  some further thoughts  on those “Psychological Science” papers on menstrual cycles, biceps size, and political attitudes, tied to a horrible press release from the journal Psychological Science hyping the biceps and politics study. [sent-2, score-0.496]
</p><p>2 Accordingly, editors want to publish findings that can be reproduced and replicated by other scientists. [sent-6, score-0.313]
</p><p>3 Unfortunately, there has been a recent “crisis in confidence” among psychologists about the quality of psychological research (Pashler & Wagenmakers, 2012). [sent-7, score-0.271]
</p><p>4 Once the reality of limited resources is acknowledged, then agreement about the precise steps that should be taken is harder to attain. [sent-19, score-0.312]
</p><p>5 Consider all the cases where journals have flat-out refused to run correction letters, non-replications, and the like. [sent-26, score-0.295]
</p><p>6 And there are many others, along with various bystanders who reflexively defend fraudulent research and  analogize  retractions of flawed papers to “medieval torture. [sent-29, score-0.449]
</p><p>7 This not only leads to problems detecting effects that actually exist, it also results in lower precision in parameter estimates and systematically inflated effect size estimates. [sent-32, score-0.393]
</p><p>8 Thus, the problem of low-powered studies is not just they have problems detecting effects that actually exist, but also that they apparently “detect” results in the wrong direction. [sent-37, score-0.503]
</p><p>9 And, contrary to what might be implied by the last sentence above, it is not necessary to run large numbers of weakly powered studies to get artifactual results (i. [sent-38, score-0.589]
</p><p>10 The underlying experiment is a randomized controlled study of a bunch of mice, there’s also a picture of slides showing the live and dead cancer cells, and the entire experiment was replicated in another lab (hence the 50 coauthors on the paper). [sent-51, score-0.393]
</p><p>11 It’s a short crisp paper, but underlying it are three years of research and a definitive experiment. [sent-52, score-0.409]
</p><p>12 people can still write short crisp papers that  look  like Science and Nature papers. [sent-62, score-0.41]
</p><p>13 And I think this might be the problem with Science, Nature, Psychological Science, and any other “tabloid” journals that might be out there. [sent-63, score-0.321]
</p><p>14 People submit social science papers that have the  look  of legitimate scientific papers. [sent-64, score-0.53]
</p><p>15 But, instead of the crisp tabloid paper being a concise summary of years of careful research, it’s a quickie job, a false front. [sent-65, score-0.485]
</p><p>16 A place for little studies    I’d also like to repeat the point that there’s nothing wrong with writing a paper with an inconclusive observational study coupled with speculative conclusions. [sent-66, score-0.463]
</p><p>17 Researchers A and B publish a speculative paper based on data from a convenience sample, researchers C and D publish their attempted replications, E and F publish criticisms, and so forth. [sent-68, score-0.852]
</p><p>18 publish quickie papers, and so there’s a motivation to send stuff there, and this in turn devalues the papers that  don’t  make it into the top journals. [sent-70, score-0.543]
</p><p>19 Currently, journals hold criticisms and replications to such a high standard of publication that papers with errors often just stand by themselves in the scientific literature. [sent-71, score-0.721]
</p><p>20 I hope that the occasional high-profile criticisms of flawed papers (for example,  here ) will serve as  some  incentive for researchers to get things right the first time, and to avoid labeling speculation as certainty. [sent-75, score-0.501]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('papers', 0.25), ('science', 0.178), ('crisp', 0.16), ('publish', 0.158), ('psychological', 0.15), ('artifactual', 0.135), ('donnellan', 0.135), ('quickie', 0.135), ('nature', 0.133), ('speculative', 0.132), ('definitive', 0.128), ('powered', 0.123), ('biceps', 0.123), ('research', 0.121), ('correction', 0.117), ('journals', 0.111), ('studies', 0.105), ('resources', 0.104), ('replicability', 0.104), ('lucas', 0.104), ('scientific', 0.102), ('tabloid', 0.101), ('criticisms', 0.099), ('detecting', 0.09), ('paper', 0.089), ('replicated', 0.087), ('results', 0.085), ('replications', 0.085), ('experiment', 0.084), ('convenience', 0.083), ('problem', 0.08), ('paradigm', 0.08), ('flawed', 0.078), ('weakly', 0.076), ('effects', 0.076), ('agreement', 0.076), ('precision', 0.075), ('researchers', 0.074), ('errors', 0.074), ('cancer', 0.07), ('observational', 0.069), ('exist', 0.069), ('study', 0.068), ('editors', 0.068), ('cases', 0.067), ('problems', 0.067), ('reality', 0.066), ('limited', 0.066), ('might', 0.065), ('richard', 0.065)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000007 <a title="1878-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-31-How_to_fix_the_tabloids%3F__Toward_replicable_social_science_research.html">1878 andrew gelman stats-2013-05-31-How to fix the tabloids?  Toward replicable social science research</a></p>
<p>Introduction: This seems to be the topic of the week.  Yesterday I posted on the sister blog  some further thoughts  on those “Psychological Science” papers on menstrual cycles, biceps size, and political attitudes, tied to a horrible press release from the journal Psychological Science hyping the biceps and politics study.
 
Then I was pointed to these  suggestions  from Richard Lucas and M. Brent Donnellan have on improving the replicability and reproducibility of research published in the Journal of Research in Personality:
  
It goes without saying that editors of scientific journals strive to publish research that is not only theoretically interesting but also methodologically rigorous. The goal is to select papers that advance the field. Accordingly, editors want to publish findings that can be reproduced and replicated by other scientists. Unfortunately, there has been a recent “crisis in confidence” among psychologists about the quality of psychological research (Pashler & Wagenmakers, 2012)</p><p>2 0.27541614 <a title="1878-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-06-How_to_think_about_papers_published_in_low-grade_journals%3F.html">1928 andrew gelman stats-2013-07-06-How to think about papers published in low-grade journals?</a></p>
<p>Introduction: We’ve had lots of lively discussions of fatally-flawed papers that have been published in top, top journals such as the  American Economic Review  or the  Journal of Personality and Social Psychology  or the  American Sociological Review  or the  tabloids .  And we also know about mistakes that make their way into mid-ranking outlets such as the Journal of Theoretical Biology.
 
But what about results that appear in the lower tier of legitimate journals?  I was thinking about this after reading a  post  by Dan Kahan slamming a paper that recently appeared in PLOS-One.  I won’t discuss the paper itself here because that’s not my point.  Rather, I had some thoughts regarding Kahan’s annoyance that a paper with fatal errors was published at all.  I commented as follows:
  
Read between the lines. The paper originally was released in 2009 and was published in 2013 in PLOS-One, which is one step above appearing on Arxiv. PLOS-One publishes some good things (so does Arxiv) but it’s the place</p><p>3 0.25003755 <a title="1878-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-26-Suggested_resolution_of_the_Bem_paradox.html">1139 andrew gelman stats-2012-01-26-Suggested resolution of the Bem paradox</a></p>
<p>Introduction: There has been an increasing discussion about the proliferation of flawed research in psychology and medicine, with some landmark events being John Ioannides’s  article , “Why most published research findings are false” (according to Google Scholar, cited 973 times since its appearance in 2005), the scandals of Marc Hauser and Diederik Stapel, two leading psychology professors who resigned after disclosures of scientific misconduct, and Daryl Bem’s  dubious  recent paper on ESP, published to much  fanfare  in Journal of Personality and Social Psychology, one of the top journals in the field.
 
Alongside all this are the plagiarism scandals, which are uninteresting from a scientific context but are relevant in that, in many cases, neither the institutions housing the plagiarists nor the editors and publishers of the plagiarized material seem to care.  Perhaps these universities and publishers are more worried about bad publicity (and maybe lawsuits, given that many of the plagiarism cas</p><p>4 0.23325808 <a title="1878-tfidf-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-19-The_replication_and_criticism_movement_is_not_about_suppressing_speculative_research%3B_rather%2C_it%E2%80%99s_all_about_enabling_science%E2%80%99s_fabled_self-correcting_nature.html">2217 andrew gelman stats-2014-02-19-The replication and criticism movement is not about suppressing speculative research; rather, it’s all about enabling science’s fabled self-correcting nature</a></p>
<p>Introduction: Jeff Leek  points to  a post by Alex Holcombe, who disputes the idea that science is self-correcting.  Holcombe  writes  [scroll down to get to his part]:
  
The pace of scientific production has quickened, and self-correction has suffered. Findings that might correct old results are considered less interesting than results from more original research questions. Potential corrections are also more contested. As the competition for space in prestigious journals has become increasingly frenzied, doing and publishing studies that would confirm the rapidly accumulating new discoveries, or would correct them, became a losing proposition.
  
Holcombe picks up on some points that we’ve discussed a lot here in the past year.  Here’s Holcombe:
  
In certain subfields, almost all new work appears in only a very few journals, all associated with a single professional society. There is then no way around the senior gatekeepers, who may then suppress corrections with impunity. . . .


The bias agai</p><p>5 0.23082158 <a title="1878-tfidf-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-12-More_on_publishing_in_journals.html">2245 andrew gelman stats-2014-03-12-More on publishing in journals</a></p>
<p>Introduction: I’m postponing today’s scheduled post (“Empirical implications of Empirical Implications of Theoretical Models”) to continue the lively discussion from yesterday,  What if I were to stop publishing in journals? . 
   
 An example:  my papers with Basbøll 
 
Thomas Basbøll and I got into a long discussion on our blogs about business school professor Karl Weick and other cases of  plagiarism  copying text without attribution.  We felt it useful to take our ideas to the next level and write them up as a manuscript, which ended up being logical to split into two papers.  At that point I put some effort into getting these papers published, which I eventually did:   To throw away data: Plagiarism as a statistical crime  went into American Scientist and  When do stories work? Evidence and illustration in the social sciences  will appear in Sociological Methods and Research.  The second paper, in particular, took some effort to place; I got some advice from colleagues in sociology as to where</p><p>6 0.2202372 <a title="1878-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-30-Retracted_articles_and_unethical_behavior_in_economics_journals%3F.html">1435 andrew gelman stats-2012-07-30-Retracted articles and unethical behavior in economics journals?</a></p>
<p>7 0.21661577 <a title="1878-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-06-Against_optimism_about_social_science.html">1844 andrew gelman stats-2013-05-06-Against optimism about social science</a></p>
<p>8 0.19694427 <a title="1878-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-20-What_happened_that_the_journal_Psychological_Science_published_a_paper_with_no_identifiable_strengths%3F.html">1865 andrew gelman stats-2013-05-20-What happened that the journal Psychological Science published a paper with no identifiable strengths?</a></p>
<p>9 0.19144118 <a title="1878-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-17-Replication_backlash.html">2137 andrew gelman stats-2013-12-17-Replication backlash</a></p>
<p>10 0.19034226 <a title="1878-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-29-Another_one_of_those_%E2%80%9CPsychological_Science%E2%80%9D_papers_%28this_time_on_biceps_size_and_political_attitudes_among_college_students%29.html">1876 andrew gelman stats-2013-05-29-Another one of those “Psychological Science” papers (this time on biceps size and political attitudes among college students)</a></p>
<p>11 0.17723981 <a title="1878-tfidf-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-06-How_much_time_%28if_any%29_should_we_spend_criticizing_research_that%E2%80%99s_fraudulent%2C_crappy%2C_or_just_plain_pointless%3F.html">2235 andrew gelman stats-2014-03-06-How much time (if any) should we spend criticizing research that’s fraudulent, crappy, or just plain pointless?</a></p>
<p>12 0.17590931 <a title="1878-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-13-Preregistration_of_Studies_and_Mock_Reports.html">1671 andrew gelman stats-2013-01-13-Preregistration of Studies and Mock Reports</a></p>
<p>13 0.17562872 <a title="1878-tfidf-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-01-Association_for_Psychological_Science_announces_a_new_journal.html">2278 andrew gelman stats-2014-04-01-Association for Psychological Science announces a new journal</a></p>
<p>14 0.17439982 <a title="1878-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-12-The_importance_of_style_in_academic_writing.html">902 andrew gelman stats-2011-09-12-The importance of style in academic writing</a></p>
<p>15 0.1712938 <a title="1878-tfidf-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-26-New_research_journal_on_observational_studies.html">2268 andrew gelman stats-2014-03-26-New research journal on observational studies</a></p>
<p>16 0.16691968 <a title="1878-tfidf-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-11-What_if_I_were_to_stop_publishing_in_journals%3F.html">2244 andrew gelman stats-2014-03-11-What if I were to stop publishing in journals?</a></p>
<p>17 0.16188627 <a title="1878-tfidf-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-27-Beyond_the_Valley_of_the_Trolls.html">2269 andrew gelman stats-2014-03-27-Beyond the Valley of the Trolls</a></p>
<p>18 0.15842022 <a title="1878-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-03-Evaluating_evidence_from_published_research.html">2006 andrew gelman stats-2013-09-03-Evaluating evidence from published research</a></p>
<p>19 0.15786378 <a title="1878-tfidf-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-08-Discussion_with_Steven_Pinker_on_research_that_is_attached_to_data_that_are_so_noisy_as_to_be_essentially_uninformative.html">2326 andrew gelman stats-2014-05-08-Discussion with Steven Pinker on research that is attached to data that are so noisy as to be essentially uninformative</a></p>
<p>20 0.15747464 <a title="1878-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-16-%E2%80%9CGroundbreaking_or_Definitive%3F_Journals_Need_to_Pick_One%E2%80%9D.html">1122 andrew gelman stats-2012-01-16-“Groundbreaking or Definitive? Journals Need to Pick One”</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.3), (1, -0.068), (2, -0.077), (3, -0.269), (4, -0.112), (5, -0.073), (6, -0.032), (7, -0.134), (8, -0.098), (9, -0.005), (10, 0.068), (11, 0.027), (12, -0.039), (13, -0.015), (14, -0.002), (15, -0.034), (16, -0.036), (17, 0.015), (18, -0.028), (19, -0.018), (20, 0.003), (21, -0.017), (22, -0.055), (23, 0.021), (24, -0.05), (25, 0.015), (26, 0.0), (27, -0.004), (28, -0.0), (29, -0.001), (30, -0.029), (31, -0.056), (32, 0.002), (33, -0.027), (34, -0.009), (35, 0.034), (36, -0.013), (37, 0.024), (38, -0.005), (39, 0.006), (40, 0.011), (41, 0.017), (42, 0.013), (43, 0.012), (44, -0.002), (45, -0.028), (46, 0.036), (47, 0.059), (48, -0.01), (49, 0.022)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98711443 <a title="1878-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-31-How_to_fix_the_tabloids%3F__Toward_replicable_social_science_research.html">1878 andrew gelman stats-2013-05-31-How to fix the tabloids?  Toward replicable social science research</a></p>
<p>Introduction: This seems to be the topic of the week.  Yesterday I posted on the sister blog  some further thoughts  on those “Psychological Science” papers on menstrual cycles, biceps size, and political attitudes, tied to a horrible press release from the journal Psychological Science hyping the biceps and politics study.
 
Then I was pointed to these  suggestions  from Richard Lucas and M. Brent Donnellan have on improving the replicability and reproducibility of research published in the Journal of Research in Personality:
  
It goes without saying that editors of scientific journals strive to publish research that is not only theoretically interesting but also methodologically rigorous. The goal is to select papers that advance the field. Accordingly, editors want to publish findings that can be reproduced and replicated by other scientists. Unfortunately, there has been a recent “crisis in confidence” among psychologists about the quality of psychological research (Pashler & Wagenmakers, 2012)</p><p>2 0.91840494 <a title="1878-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-26-Suggested_resolution_of_the_Bem_paradox.html">1139 andrew gelman stats-2012-01-26-Suggested resolution of the Bem paradox</a></p>
<p>Introduction: There has been an increasing discussion about the proliferation of flawed research in psychology and medicine, with some landmark events being John Ioannides’s  article , “Why most published research findings are false” (according to Google Scholar, cited 973 times since its appearance in 2005), the scandals of Marc Hauser and Diederik Stapel, two leading psychology professors who resigned after disclosures of scientific misconduct, and Daryl Bem’s  dubious  recent paper on ESP, published to much  fanfare  in Journal of Personality and Social Psychology, one of the top journals in the field.
 
Alongside all this are the plagiarism scandals, which are uninteresting from a scientific context but are relevant in that, in many cases, neither the institutions housing the plagiarists nor the editors and publishers of the plagiarized material seem to care.  Perhaps these universities and publishers are more worried about bad publicity (and maybe lawsuits, given that many of the plagiarism cas</p><p>3 0.90782833 <a title="1878-lsi-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-19-The_replication_and_criticism_movement_is_not_about_suppressing_speculative_research%3B_rather%2C_it%E2%80%99s_all_about_enabling_science%E2%80%99s_fabled_self-correcting_nature.html">2217 andrew gelman stats-2014-02-19-The replication and criticism movement is not about suppressing speculative research; rather, it’s all about enabling science’s fabled self-correcting nature</a></p>
<p>Introduction: Jeff Leek  points to  a post by Alex Holcombe, who disputes the idea that science is self-correcting.  Holcombe  writes  [scroll down to get to his part]:
  
The pace of scientific production has quickened, and self-correction has suffered. Findings that might correct old results are considered less interesting than results from more original research questions. Potential corrections are also more contested. As the competition for space in prestigious journals has become increasingly frenzied, doing and publishing studies that would confirm the rapidly accumulating new discoveries, or would correct them, became a losing proposition.
  
Holcombe picks up on some points that we’ve discussed a lot here in the past year.  Here’s Holcombe:
  
In certain subfields, almost all new work appears in only a very few journals, all associated with a single professional society. There is then no way around the senior gatekeepers, who may then suppress corrections with impunity. . . .


The bias agai</p><p>4 0.88559753 <a title="1878-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-22-Likelihood_Ratio_%E2%89%A0_1_Journal.html">1774 andrew gelman stats-2013-03-22-Likelihood Ratio ≠ 1 Journal</a></p>
<p>Introduction: Dan Kahan  writes :
  
The basic idea . . . is to promote identification of study designs that scholars who disagree about a proposition would agree would generate evidence relevant to their competing conjectures—regardless of what studies based on such designs actually find. Articles proposing designs of this sort would be selected for publication and only then be carried out, by the proposing researchers with funding from the journal, which would publish the results too.


Now I [Kahan] am aware of a set of real journals that have a similar motivation.


One is the  Journal of Articles in Support of the Null Hypothesis, which as its title implies publishes papers reporting studies that fail to “reject” the null. Like JASNH, LR ≠1J would try to offset the “file drawer” bias and like bad consequences associated with the convention of publishing only findings that are “significant at p < 0.05."


But it would try to do more. By publishing studies that are deemed to have valid designs an</p><p>5 0.88417673 <a title="1878-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-24-Too_Good_To_Be_True%3A__The_Scientific_Mass_Production_of_Spurious_Statistical_Significance.html">1954 andrew gelman stats-2013-07-24-Too Good To Be True:  The Scientific Mass Production of Spurious Statistical Significance</a></p>
<p>Introduction: Are women three times more likely to wear red or pink when they are most fertile? No, probably not. But here’s how hardworking researchers, prestigious scientific journals, and gullible journalists have been fooled into believing so.


The paper I’ll be talking about appeared online this month in Psychological Science, the flagship journal of the Association for Psychological Science, which represents the serious, research-focused (as opposed to therapeutic) end of the psychology profession. . . .


In focusing on this (literally) colorful example, I don’t mean to be singling out this particular research team for following what are, unfortunately, standard practices in experimental research. Indeed, that this article was published in a leading journal is evidence that its statistical methods were considered acceptable. Statistics textbooks do warn against multiple comparisons, but there is a tendency for researchers to consider any given comparison alone without considering it as one o</p><p>6 0.88104868 <a title="1878-lsi-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-20-The_AAA_Tranche_of_Subprime_Science.html">2179 andrew gelman stats-2014-01-20-The AAA Tranche of Subprime Science</a></p>
<p>7 0.87808269 <a title="1878-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-13-How_should_journals_handle_replication_studies%3F.html">762 andrew gelman stats-2011-06-13-How should journals handle replication studies?</a></p>
<p>8 0.8741858 <a title="1878-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-13-Preregistration_of_Studies_and_Mock_Reports.html">1671 andrew gelman stats-2013-01-13-Preregistration of Studies and Mock Reports</a></p>
<p>9 0.86942685 <a title="1878-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-16-%E2%80%9CGroundbreaking_or_Definitive%3F_Journals_Need_to_Pick_One%E2%80%9D.html">1122 andrew gelman stats-2012-01-16-“Groundbreaking or Definitive? Journals Need to Pick One”</a></p>
<p>10 0.86807913 <a title="1878-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-25-A_new_Bem_theory.html">1998 andrew gelman stats-2013-08-25-A new Bem theory</a></p>
<p>11 0.86731124 <a title="1878-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-17-Replication_backlash.html">2137 andrew gelman stats-2013-12-17-Replication backlash</a></p>
<p>12 0.86470747 <a title="1878-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-05-Against_double-blind_reviewing%3A__Political_science_and_statistics_are_not_like_biology_and_physics.html">601 andrew gelman stats-2011-03-05-Against double-blind reviewing:  Political science and statistics are not like biology and physics</a></p>
<p>13 0.8558197 <a title="1878-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-28-50_shades_of_gray%3A__A_research_story.html">1959 andrew gelman stats-2013-07-28-50 shades of gray:  A research story</a></p>
<p>14 0.84771186 <a title="1878-lsi-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-26-New_research_journal_on_observational_studies.html">2268 andrew gelman stats-2014-03-26-New research journal on observational studies</a></p>
<p>15 0.84746963 <a title="1878-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-20-Proposals_for_alternative_review_systems_for_scientific_work.html">1273 andrew gelman stats-2012-04-20-Proposals for alternative review systems for scientific work</a></p>
<p>16 0.8453266 <a title="1878-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-13-Data_sharing_update.html">1055 andrew gelman stats-2011-12-13-Data sharing update</a></p>
<p>17 0.84315413 <a title="1878-lsi-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-30-I_posted_this_as_a_comment_on_a_sociology_blog.html">2353 andrew gelman stats-2014-05-30-I posted this as a comment on a sociology blog</a></p>
<p>18 0.84084755 <a title="1878-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-30-Retracted_articles_and_unethical_behavior_in_economics_journals%3F.html">1435 andrew gelman stats-2012-07-30-Retracted articles and unethical behavior in economics journals?</a></p>
<p>19 0.84064174 <a title="1878-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-01-Post-publication_peer_review%3A__How_it_%28sometimes%29_really_works.html">2004 andrew gelman stats-2013-09-01-Post-publication peer review:  How it (sometimes) really works</a></p>
<p>20 0.83813 <a title="1878-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-06-Against_optimism_about_social_science.html">1844 andrew gelman stats-2013-05-06-Against optimism about social science</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(15, 0.063), (16, 0.13), (20, 0.012), (21, 0.034), (24, 0.128), (42, 0.015), (47, 0.013), (59, 0.021), (76, 0.012), (77, 0.044), (86, 0.012), (89, 0.023), (90, 0.017), (99, 0.294)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97996521 <a title="1878-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-31-How_to_fix_the_tabloids%3F__Toward_replicable_social_science_research.html">1878 andrew gelman stats-2013-05-31-How to fix the tabloids?  Toward replicable social science research</a></p>
<p>Introduction: This seems to be the topic of the week.  Yesterday I posted on the sister blog  some further thoughts  on those “Psychological Science” papers on menstrual cycles, biceps size, and political attitudes, tied to a horrible press release from the journal Psychological Science hyping the biceps and politics study.
 
Then I was pointed to these  suggestions  from Richard Lucas and M. Brent Donnellan have on improving the replicability and reproducibility of research published in the Journal of Research in Personality:
  
It goes without saying that editors of scientific journals strive to publish research that is not only theoretically interesting but also methodologically rigorous. The goal is to select papers that advance the field. Accordingly, editors want to publish findings that can be reproduced and replicated by other scientists. Unfortunately, there has been a recent “crisis in confidence” among psychologists about the quality of psychological research (Pashler & Wagenmakers, 2012)</p><p>2 0.97798514 <a title="1878-lda-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-27-%E2%80%9CWhat_Can_we_Learn_from_the_Many_Labs_Replication_Project%3F%E2%80%9D.html">2227 andrew gelman stats-2014-02-27-“What Can we Learn from the Many Labs Replication Project?”</a></p>
<p>Introduction: Aki points us to  this discussion  from Rolf Zwaan:
  
The first massive replication project in psychology has just reached completion (several others are to follow). . . . What can we learn from the ManyLabs project? The results here show the effect sizes for the replication efforts (in green and grey) as well as the original studies (in blue). The 99% confidence intervals are for the meta-analysis of the effect size (the green dots); the studies are ordered by effect size.


 


Let’s first consider what we canNOT learn from these data. Of the 13 replication attempts (when the first four are taken together), 11 succeeded and 2 did not (in fact, at some point ManyLabs suggests that a third one, Imagined Contact also doesn’t really replicate). We cannot learn from this that the vast majority of psychological findings will replicate . . .


But even if we had an accurate estimate of the percentage of findings that replicate, how useful would that be? Rather than trying to arrive at a mo</p><p>3 0.97698593 <a title="1878-lda-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-20-The_AAA_Tranche_of_Subprime_Science.html">2179 andrew gelman stats-2014-01-20-The AAA Tranche of Subprime Science</a></p>
<p>Introduction: In  our new ethics column for Chance , Eric Loken and I write about our current favorite topic:
  
One of our ongoing themes when discussing scientific ethics is the central role of statistics in recognizing and communicating uncer- 
tainty. Unfortunately, statistics—and the scientific process more generally—often seems to be used more as a way of laundering uncertainty, processing data until researchers and consumers of research can feel safe acting as if various scientific hypotheses are unquestionably true. . . .


We have in mind an analogy with the notorious AAA-class bonds created during the mid-2000s that led to the subprime mortgage crisis. Lower-quality mortgages—that is, mortgages with high probability of default and, thus, high uncertainty—were packaged and transformed into financial instruments that were (in retrospect, falsely) characterized as low risk. There was a tremendous interest in these securities, not just among the most unscrupulous market manipulators, but in a</p><p>4 0.97618699 <a title="1878-lda-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-17-Replication_backlash.html">2137 andrew gelman stats-2013-12-17-Replication backlash</a></p>
<p>Introduction: Raghuveer Parthasarathy pointed me to an article in Nature by Mina Bissell, who  writes , “The push to replicate findings could shelve promising research and unfairly damage the reputations of careful, meticulous scientists.”
 
I can see where she’s coming from:  if you work hard day after day in the lab, it’s gotta be a bit frustrating to find all your work questioned, for the frauds of the  Dr. Anil Pottis  and Diederik Stapels to be treated as a reason for everyone else’s work to be considered guilty until proven innocent.
 
That said, I pretty much disagree with Bissell’s article, and really the best thing I can say about it is that I think it’s a good sign that the push for replication is so strong that now there’s a backlash against it.  Traditionally, leading scientists have been able to simply ignore the push for replication.  If they are feeling that the replication movement is strong enough that they need to fight it, that to me is good news.
 
I’ll explain a bit in the conte</p><p>5 0.96989954 <a title="1878-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-23-A_statistical_version_of_Arrow%E2%80%99s_paradox.html">586 andrew gelman stats-2011-02-23-A statistical version of Arrow’s paradox</a></p>
<p>Introduction: Unfortunately, when we deal with scientists, statisticians are often put in a setting reminiscent of Arrow’s paradox, where we are asked to provide estimates that are informative and unbiased and confidence statements that are correct conditional on the data and also on the underlying true parameter. [It's not generally possible for an estimate to do all these things at the same time -- ed.]  Larry Wasserman feels that scientists are truly frequentist, and Don Rubin has told me how he feels that scientists interpret all statistical estimates Bayesianly. I have no doubt that both Larry and Don are correct. Voters want lower taxes and more services, and scientists want both Bayesian and frequency coverage; as the saying goes, everybody wants to go to heaven but nobody wants to die.</p><p>6 0.96933472 <a title="1878-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-07-Philosophy_and_the_practice_of_Bayesian_statistics_%28with_all_the_discussions%21%29.html">1712 andrew gelman stats-2013-02-07-Philosophy and the practice of Bayesian statistics (with all the discussions!)</a></p>
<p>7 0.96923679 <a title="1878-lda-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-30-I_posted_this_as_a_comment_on_a_sociology_blog.html">2353 andrew gelman stats-2014-05-30-I posted this as a comment on a sociology blog</a></p>
<p>8 0.96892214 <a title="1878-lda-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-15-Problematic_interpretations_of_confidence_intervals.html">2248 andrew gelman stats-2014-03-15-Problematic interpretations of confidence intervals</a></p>
<p>9 0.96886051 <a title="1878-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-22-The_Jumpstart_financial_literacy_survey_and_the_different_purposes_of_tests.html">481 andrew gelman stats-2010-12-22-The Jumpstart financial literacy survey and the different purposes of tests</a></p>
<p>10 0.96749789 <a title="1878-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-03-Popper_and_Jaynes.html">2007 andrew gelman stats-2013-09-03-Popper and Jaynes</a></p>
<p>11 0.96742857 <a title="1878-lda-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-30-You_can%E2%80%99t_put_Pandora_back_in_the_box.html">120 andrew gelman stats-2010-06-30-You can’t put Pandora back in the box</a></p>
<p>12 0.96718276 <a title="1878-lda-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-19-The_replication_and_criticism_movement_is_not_about_suppressing_speculative_research%3B_rather%2C_it%E2%80%99s_all_about_enabling_science%E2%80%99s_fabled_self-correcting_nature.html">2217 andrew gelman stats-2014-02-19-The replication and criticism movement is not about suppressing speculative research; rather, it’s all about enabling science’s fabled self-correcting nature</a></p>
<p>13 0.96708989 <a title="1878-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-20-My_beef_with_Brooks%3A__the_alternative_to_%E2%80%9Cgood_statistics%E2%80%9D_is_not_%E2%80%9Cno_statistics%2C%E2%80%9D_it%E2%80%99s_%E2%80%9Cbad_statistics%E2%80%9D.html">1729 andrew gelman stats-2013-02-20-My beef with Brooks:  the alternative to “good statistics” is not “no statistics,” it’s “bad statistics”</a></p>
<p>14 0.96708733 <a title="1878-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-17-I_got_99_comparisons_but_multiplicity_ain%E2%80%99t_one.html">1016 andrew gelman stats-2011-11-17-I got 99 comparisons but multiplicity ain’t one</a></p>
<p>15 0.96633965 <a title="1878-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-14-Bayes_in_China_update.html">517 andrew gelman stats-2011-01-14-Bayes in China update</a></p>
<p>16 0.96615851 <a title="1878-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-12-Meta-analysis%2C_game_theory%2C_and_incentives_to_do_replicable_research.html">1163 andrew gelman stats-2012-02-12-Meta-analysis, game theory, and incentives to do replicable research</a></p>
<p>17 0.96585655 <a title="1878-lda-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-%E2%80%9CThe_British_amateur_who_debunked_the_mathematics_of_happiness%E2%80%9D.html">2177 andrew gelman stats-2014-01-19-“The British amateur who debunked the mathematics of happiness”</a></p>
<p>18 0.96548498 <a title="1878-lda-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-17-SAT_stories.html">94 andrew gelman stats-2010-06-17-SAT stories</a></p>
<p>19 0.9653666 <a title="1878-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-14-Steven_Rhoads%E2%80%99s_book%2C_%E2%80%9CThe_Economist%E2%80%99s_View_of_the_World%E2%80%9D.html">711 andrew gelman stats-2011-05-14-Steven Rhoads’s book, “The Economist’s View of the World”</a></p>
<p>20 0.96523106 <a title="1878-lda-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-26-One_fun_thing_about_physicists_._._..html">431 andrew gelman stats-2010-11-26-One fun thing about physicists . . .</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
