<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-1739" href="#">andrew_gelman_stats-2013-1739</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-1739-html" href="http://andrewgelman.com/2013/02/26/an-ai-that-builds-a-statistical-model/">html</a></p><p>Introduction: David Duvenaud writes:
  
I’ve been following  your recent discussions  about how an AI could do statistics [see also  here ].  I was especially excited about your suggestion for new statistical methods using “a language-like approach to recursively creating new models from a specified list of distributions and transformations, and an automatic approach to checking model fit.”


Your discussion of these ideas was exciting to me and my colleagues because we recently  did some work  taking a step in this direction, automatically searching through a grammar over Gaussian process regression models.


Roger Grosse previously  did the same thing , but over matrix decomposition models using held-out predictive likelihood to check model fit.


These are both examples of automatic Bayesian model-building by a search over more and more complex models, as you suggested.  One nice thing is that both grammars include lots of standard models for free, and they seem to work pretty well, although the</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I was especially excited about your suggestion for new statistical methods using “a language-like approach to recursively creating new models from a specified list of distributions and transformations, and an automatic approach to checking model fit. [sent-2, score-0.856]
</p><p>2 ”   Your discussion of these ideas was exciting to me and my colleagues because we recently  did some work  taking a step in this direction, automatically searching through a grammar over Gaussian process regression models. [sent-3, score-0.549]
</p><p>3 These are both examples of automatic Bayesian model-building by a search over more and more complex models, as you suggested. [sent-5, score-0.424]
</p><p>4 One nice thing is that both grammars include lots of standard models for free, and they seem to work pretty well, although the search is of course computationally expensive. [sent-6, score-0.567]
</p><p>5 But when you consider the size and scope of the space of models that is searched, and the fact that all steps of model construction, evaluation and search are automatic, it doesn’t seem like such an expensive process. [sent-8, score-0.642]
</p><p>6 In my experience, working statisticians, machine learners, and data scientists rarely if ever explore such a space so systematically in large part because it seems impractically expensive to do so (in terms of both their own time and computation time, as well as perhaps other scarce resources). [sent-9, score-0.439]
</p><p>7 Of course the “AI” in our work is still quite primitive and naive, both in terms of good modeling methods as you have developed and taught, and in terms of human intelligence more generally. [sent-10, score-0.397]
</p><p>8 And the space of models we can consider automatically is still quite limited compared to what humans can do. [sent-11, score-0.42]
</p><p>9 We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. [sent-15, score-0.72]
</p><p>10 We present a method for searching over this space of structures which mirrors the scientific discovery process. [sent-16, score-0.732]
</p><p>11 The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. [sent-17, score-0.56]
</p><p>12 Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks. [sent-18, score-1.005]
</p><p>13 I can’t comment on the details, especially as this sort of predictive regression problem isn’t the thing I typically work on, but I like the general idea of constructing models through some sort of generative grammar. [sent-19, score-0.444]
</p><p>14 It seems to me a big step forward from the previous graphical-model paradigm in which the model is a static mixture of a bunch of conditional independence structures on a fixed set of variables. [sent-20, score-0.492]
</p><p>15 As I’ve written many times (for example, with Shalizi in our instant-classic  paper , rejoinder  here ), I think discrete Bayesian model averaging is a poor model for science and a poor model for statistical inference. [sent-21, score-0.418]
</p><p>16 Adams), introducing covariance kernels which enable automatic pattern discovery and extrapolation with Gaussian processes. [sent-31, score-0.938]
</p><p>17 Our method is computationally simple (comparable to using standard smoothing kernels), and is grounded in modelling a spectral density with a Gaussian mixture. [sent-33, score-0.478]
</p><p>18 With enough components in the mixture, we can approximate any spectral density (and thus any stationary covariance kernel) with arbitrary accuracy. [sent-34, score-0.394]
</p><p>19 We show that the proposed method can automatically discover complex structure and extrapolate over long ranges. [sent-35, score-0.385]
</p><p>20 However, our approach to automatic structure discovery is fundamentally different from the discussed “grammar of kernels” approach and previous related kernel composition approaches. [sent-36, score-1.042]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kernel', 0.301), ('kernels', 0.277), ('automatic', 0.236), ('structures', 0.191), ('computationally', 0.164), ('ai', 0.164), ('space', 0.163), ('models', 0.139), ('spectral', 0.13), ('grammar', 0.13), ('gaussian', 0.126), ('search', 0.124), ('discovery', 0.122), ('expensive', 0.12), ('automatically', 0.118), ('extrapolation', 0.111), ('approach', 0.11), ('generative', 0.105), ('structure', 0.102), ('method', 0.101), ('methods', 0.1), ('enable', 0.1), ('model', 0.096), ('covariance', 0.092), ('components', 0.089), ('searching', 0.086), ('density', 0.083), ('terms', 0.082), ('mixture', 0.08), ('exciting', 0.078), ('machine', 0.074), ('work', 0.071), ('decompose', 0.069), ('formalisms', 0.069), ('mirrors', 0.069), ('shoehorn', 0.069), ('grammars', 0.069), ('regression', 0.066), ('wald', 0.065), ('multiplying', 0.065), ('recursively', 0.065), ('poor', 0.065), ('complex', 0.064), ('forward', 0.064), ('predictive', 0.063), ('learners', 0.062), ('primitive', 0.062), ('previous', 0.061), ('hopeless', 0.06), ('loom', 0.06)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000004 <a title="1739-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>Introduction: David Duvenaud writes:
  
I’ve been following  your recent discussions  about how an AI could do statistics [see also  here ].  I was especially excited about your suggestion for new statistical methods using “a language-like approach to recursively creating new models from a specified list of distributions and transformations, and an automatic approach to checking model fit.”


Your discussion of these ideas was exciting to me and my colleagues because we recently  did some work  taking a step in this direction, automatically searching through a grammar over Gaussian process regression models.


Roger Grosse previously  did the same thing , but over matrix decomposition models using held-out predictive likelihood to check model fit.


These are both examples of automatic Bayesian model-building by a search over more and more complex models, as you suggested.  One nice thing is that both grammars include lots of standard models for free, and they seem to work pretty well, although the</p><p>2 0.20160452 <a title="1739-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-04-Model_checking_and_model_understanding_in_machine_learning.html">1482 andrew gelman stats-2012-09-04-Model checking and model understanding in machine learning</a></p>
<p>Introduction: Last month I  wrote :
  
Computer scientists are often brilliant but they can be unfamiliar with what is done in the worlds of data collection and analysis. This goes the other way too: statisticians such as myself can look pretty awkward, reinventing (or failing to reinvent) various wheels when we write computer programs or, even worse, try to design software.Andrew MacNamara writes:
  
Andrew MacNamara followed up with some thoughts:
  
I [MacNamara] had some basic statistics training through my MBA program, after having completed an undergrad degree in computer science. Since then I’ve been very interested in learning more about statistical techniques, including things like GLM and censored data analyses as well as machine learning topics like neural nets, SVMs, etc. I began following your blog after some research into Bayesian analysis topics and I am trying to dig deeper on that side of things.


One thing I have noticed is that there seems to be a distinction between data analysi</p><p>3 0.19666083 <a title="1739-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-%E2%80%9CIs_machine_learning_a_subset_of_statistics%3F%E2%80%9D.html">1740 andrew gelman stats-2013-02-26-“Is machine learning a subset of statistics?”</a></p>
<p>Introduction: Following up on our  previous post , Andrew Wilson writes:
  
I agree we are in a really exciting time for statistics and machine learning.  There has been a lot of talk lately comparing machine learning with statistics.  I am curious whether you think there are many fundamental differences between the fields, or just superficial differences — different popular approximate inference methods, slightly different popular application areas, etc.  Is machine learning a subset of statistics?


In the paper we discuss how we think machine learning is fundamentally about pattern discovery, and ultimately, fully automating the learning and decision making process.  In other words, whatever a human does when he or she uses tools to analyze data, can be written down algorithmically and automated on a computer.  I am not sure if the ambitions are similar in statistics — and I don’t have any conventional statistics background, which makes it harder to tell. I think it’s an interesting discussion.</p><p>4 0.1922949 <a title="1739-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>Introduction: I’ve been writing a lot about my philosophy of Bayesian statistics and how it fits into Popper’s ideas about falsification and Kuhn’s ideas about scientific revolutions.
 
 Here’s  my long, somewhat technical paper with Cosma Shalizi. 
 Here’s  our shorter overview for the volume on the philosophy of social science. 
 Here’s  my latest try (for an online symposium), focusing on the key issues.
 
I’m pretty happy with my approach–the familiar idea that Bayesian data analysis iterates the three steps of model building, inference, and model checking–but it does have some unresolved (maybe unresolvable) problems.  Here are a couple mentioned in the third of the above links.
 
Consider a simple model with independent data y_1, y_2, .., y_10 ~ N(θ,σ^2), with a prior distribution θ ~ N(0,10^2) and σ known and taking on some value of approximately 10. Inference about μ is straightforward, as is model checking, whether based on graphs or numerical summaries such as the sample variance and skewn</p><p>5 0.19164093 <a title="1739-tfidf-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-12-%E2%80%9CThe_results_%28not_shown%29_._._.%E2%80%9D.html">2332 andrew gelman stats-2014-05-12-“The results (not shown) . . .”</a></p>
<p>Introduction: Pro tip:  Don’t believe any claims about results not shown in a paper.  Even if the paper has been published.  Even if it’s been cited hundreds of times.  If the results aren’t shown, they haven’t been checked.
 
 
 
I learned this the hard way after receiving this note from Bin Liu, who wrote:
  
Today I saw  a paper  [by Ziheng Yang and Carlos Rodríguez] titled “Searching for efficient Markov chain Monte Carlo proposal kernels.”  The authors cited your work: “Gelman A, Roberts GO, Gilks WR (1996) Bayesian Statistics 5, eds Bernardo JM, et al. (Oxford Univ Press, Oxford), Vol 5, pp 599-607″, i.e. ref.6 in the paper.


In the last sentence of pp.19310, the authors write that “… virtually no study has examined alternative kernels; this appears to be due to the influence of ref. 6, which claimed that different kernels had nearly identical performance. This conclusion is incorrect.”
  
 Here’s  our paper, and here’s the offending quote, which appeared after we discussed results for the no</p><p>6 0.16909626 <a title="1739-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-11-Why_waste_time_philosophizing%3F.html">1719 andrew gelman stats-2013-02-11-Why waste time philosophizing?</a></p>
<p>7 0.14482844 <a title="1739-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Interpreting_interactions_in_discrete-data_regression.html">1908 andrew gelman stats-2013-06-21-Interpreting interactions in discrete-data regression</a></p>
<p>8 0.14060548 <a title="1739-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-04-When_is_there_%E2%80%9Chidden_structure_in_data%E2%80%9D_to_be_discovered%3F.html">1788 andrew gelman stats-2013-04-04-When is there “hidden structure in data” to be discovered?</a></p>
<p>9 0.14041811 <a title="1739-tfidf-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-01-%E2%80%9CThough_They_May_Be_Unaware%2C_Newlyweds_Implicitly_Know_Whether_Their_Marriage_Will_Be_Satisfying%E2%80%9D.html">2156 andrew gelman stats-2014-01-01-“Though They May Be Unaware, Newlyweds Implicitly Know Whether Their Marriage Will Be Satisfying”</a></p>
<p>10 0.13551171 <a title="1739-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>11 0.13412613 <a title="1739-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>12 0.13048306 <a title="1739-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>13 0.13017146 <a title="1739-tfidf-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-22-Philosophy_of_Bayes_and_non-Bayes%3A__A_dialogue_with_Deborah_Mayo.html">291 andrew gelman stats-2010-09-22-Philosophy of Bayes and non-Bayes:  A dialogue with Deborah Mayo</a></p>
<p>14 0.12860906 <a title="1739-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-21-BDA3_table_of_contents_%28also_a_new_paper_on_visualization%29.html">1991 andrew gelman stats-2013-08-21-BDA3 table of contents (also a new paper on visualization)</a></p>
<p>15 0.12711528 <a title="1739-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-09-A_tale_of_two_discussion_papers.html">1848 andrew gelman stats-2013-05-09-A tale of two discussion papers</a></p>
<p>16 0.12601739 <a title="1739-tfidf-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>17 0.12429907 <a title="1739-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-28-Reference_on_longitudinal_models%3F.html">1188 andrew gelman stats-2012-02-28-Reference on longitudinal models?</a></p>
<p>18 0.11606069 <a title="1739-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>19 0.11564583 <a title="1739-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-05-Xiao-Li_Meng_and_Xianchao_Xie_rethink_asymptotics.html">1406 andrew gelman stats-2012-07-05-Xiao-Li Meng and Xianchao Xie rethink asymptotics</a></p>
<p>20 0.1149279 <a title="1739-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-04-David_MacKay_and_Occam%E2%80%99s_Razor.html">1041 andrew gelman stats-2011-12-04-David MacKay and Occam’s Razor</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.234), (1, 0.134), (2, -0.052), (3, 0.048), (4, 0.018), (5, 0.042), (6, -0.064), (7, -0.045), (8, 0.045), (9, 0.052), (10, 0.015), (11, -0.006), (12, -0.063), (13, -0.013), (14, -0.033), (15, 0.003), (16, 0.034), (17, -0.028), (18, -0.013), (19, -0.018), (20, 0.03), (21, -0.035), (22, -0.016), (23, -0.006), (24, 0.015), (25, 0.011), (26, -0.027), (27, 0.052), (28, 0.011), (29, -0.023), (30, -0.016), (31, 0.023), (32, 0.031), (33, -0.024), (34, 0.019), (35, -0.032), (36, -0.016), (37, -0.006), (38, -0.013), (39, -0.018), (40, -0.025), (41, 0.014), (42, 0.035), (43, 0.051), (44, 0.029), (45, 0.011), (46, -0.059), (47, 0.008), (48, 0.016), (49, -0.071)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97250265 <a title="1739-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>Introduction: David Duvenaud writes:
  
I’ve been following  your recent discussions  about how an AI could do statistics [see also  here ].  I was especially excited about your suggestion for new statistical methods using “a language-like approach to recursively creating new models from a specified list of distributions and transformations, and an automatic approach to checking model fit.”


Your discussion of these ideas was exciting to me and my colleagues because we recently  did some work  taking a step in this direction, automatically searching through a grammar over Gaussian process regression models.


Roger Grosse previously  did the same thing , but over matrix decomposition models using held-out predictive likelihood to check model fit.


These are both examples of automatic Bayesian model-building by a search over more and more complex models, as you suggested.  One nice thing is that both grammars include lots of standard models for free, and they seem to work pretty well, although the</p><p>2 0.87502187 <a title="1739-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-19-An_interweaving-transformation_strategy_for_boosting_MCMC_efficiency.html">964 andrew gelman stats-2011-10-19-An interweaving-transformation strategy for boosting MCMC efficiency</a></p>
<p>Introduction: Yaming Yu and Xiao-Li Meng  write in  with a cool new idea for improving the efficiency of Gibbs and Metropolis in multilevel models:
  
For a broad class of multilevel models, there exist two well-known competing parameterizations, the centered parameterization (CP) and the non-centered parameterization (NCP), for effective MCMC implementation. Much literature has been devoted to the questions of when to use which and how to compromise between them via partial CP/NCP. This article introduces an alternative strategy for boosting MCMC efficiency via simply interweaving—but not alternating—the two parameterizations. This strategy has the surprising property that failure of both the CP and NCP chains to converge geometrically does not prevent the interweaving algorithm from doing so. It achieves this seemingly magical property by taking advantage of the discordance of the two parameterizations, namely, the sufficiency of CP and the ancillarity of NCP, to substantially reduce the Markovian</p><p>3 0.84334642 <a title="1739-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-04-Model_checking_and_model_understanding_in_machine_learning.html">1482 andrew gelman stats-2012-09-04-Model checking and model understanding in machine learning</a></p>
<p>Introduction: Last month I  wrote :
  
Computer scientists are often brilliant but they can be unfamiliar with what is done in the worlds of data collection and analysis. This goes the other way too: statisticians such as myself can look pretty awkward, reinventing (or failing to reinvent) various wheels when we write computer programs or, even worse, try to design software.Andrew MacNamara writes:
  
Andrew MacNamara followed up with some thoughts:
  
I [MacNamara] had some basic statistics training through my MBA program, after having completed an undergrad degree in computer science. Since then I’ve been very interested in learning more about statistical techniques, including things like GLM and censored data analyses as well as machine learning topics like neural nets, SVMs, etc. I began following your blog after some research into Bayesian analysis topics and I am trying to dig deeper on that side of things.


One thing I have noticed is that there seems to be a distinction between data analysi</p><p>4 0.83245581 <a title="1739-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-05-Xiao-Li_Meng_and_Xianchao_Xie_rethink_asymptotics.html">1406 andrew gelman stats-2012-07-05-Xiao-Li Meng and Xianchao Xie rethink asymptotics</a></p>
<p>Introduction: In  an article  catchily entitled, “I got more data, my model is more refined, but my estimator is getting worse!  Am I just dumb?”, Meng and Xie write:
  
Possibly, but more likely you are merely a victim of conventional wisdom. More data or better models by no means guarantee better estimators (e.g., with a smaller mean squared error), when you are not following probabilistically principled methods such as MLE (for large samples) or Bayesian approaches. Estimating equations are par- ticularly vulnerable in this regard, almost a necessary price for their robustness. These points will be demonstrated via common tasks of estimating regression parameters and correlations, under simple mod- els such as bivariate normal and ARCH(1). Some general strategies for detecting and avoiding such pitfalls are suggested, including checking for self-efficiency (Meng, 1994, Statistical Science) and adopting a guiding working model.


Using the example of estimating the autocorrelation ρ under a statio</p><p>5 0.82463384 <a title="1739-lsi-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-19-Just_chaid.html">421 andrew gelman stats-2010-11-19-Just chaid</a></p>
<p>Introduction: Reading somebody else’s statistics rant made me realize the inherent contradictions in much of my own statistical advice.
  

 
Jeff Lax sent along  this article  by Philip Schrodt, along with the cryptic comment:
  
Perhaps of interest to you. perhaps not.  Not meant to be an excuse for you to rant against hypothesis testing again.
  
In his article, Schrodt makes a reasonable and entertaining argument against the overfitting of data and the overuse of linear models.  He states that his article is motivated by the quantitative papers he has been sent to review for journals or conferences, and he explicitly excludes “studies of United States voting behavior,” so at least I think Mister P is off the hook.
 
I notice a bit of incoherence in Schrodt’s position–on one hand, he criticizes “kitchen-sink models” for overfitting and he criticizes “using complex methods without understanding the underlying assumptions” . . . but then later on he suggests that political scientists in this countr</p><p>6 0.81573242 <a title="1739-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-01-Tukey%E2%80%99s_philosophy.html">496 andrew gelman stats-2011-01-01-Tukey’s philosophy</a></p>
<p>7 0.81278682 <a title="1739-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>8 0.80721313 <a title="1739-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<p>9 0.80319089 <a title="1739-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-19-R_package_for_Bayes_factors.html">1682 andrew gelman stats-2013-01-19-R package for Bayes factors</a></p>
<p>10 0.80301231 <a title="1739-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-14-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">1856 andrew gelman stats-2013-05-14-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>11 0.80212492 <a title="1739-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-What_are_the_trickiest_models_to_fit%3F.html">575 andrew gelman stats-2011-02-15-What are the trickiest models to fit?</a></p>
<p>12 0.8001703 <a title="1739-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-24-New_ideas_on_DIC_from_Martyn_Plummer_and_Sumio_Watanabe.html">778 andrew gelman stats-2011-06-24-New ideas on DIC from Martyn Plummer and Sumio Watanabe</a></p>
<p>13 0.79697269 <a title="1739-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-23-When_are_complicated_models_helpful_in_psychology_research_and_when_are_they_overkill%3F.html">1690 andrew gelman stats-2013-01-23-When are complicated models helpful in psychology research and when are they overkill?</a></p>
<p>14 0.78213185 <a title="1739-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-21-BDA3_table_of_contents_%28also_a_new_paper_on_visualization%29.html">1991 andrew gelman stats-2013-08-21-BDA3 table of contents (also a new paper on visualization)</a></p>
<p>15 0.7809248 <a title="1739-lsi-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Computer_models_of_the_oil_spill.html">243 andrew gelman stats-2010-08-30-Computer models of the oil spill</a></p>
<p>16 0.78088337 <a title="1739-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-15-More_on_AIC%2C_WAIC%2C_etc.html">1983 andrew gelman stats-2013-08-15-More on AIC, WAIC, etc</a></p>
<p>17 0.78062254 <a title="1739-lsi-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-17-Probability-processing_hardware.html">214 andrew gelman stats-2010-08-17-Probability-processing hardware</a></p>
<p>18 0.77944177 <a title="1739-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-%E2%80%9CIs_machine_learning_a_subset_of_statistics%3F%E2%80%9D.html">1740 andrew gelman stats-2013-02-26-“Is machine learning a subset of statistics?”</a></p>
<p>19 0.778938 <a title="1739-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>20 0.77503371 <a title="1739-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-04-David_MacKay_and_Occam%E2%80%99s_Razor.html">1041 andrew gelman stats-2011-12-04-David MacKay and Occam’s Razor</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.012), (15, 0.057), (16, 0.06), (17, 0.012), (21, 0.033), (24, 0.142), (30, 0.04), (43, 0.012), (53, 0.022), (61, 0.135), (73, 0.011), (79, 0.013), (86, 0.037), (99, 0.264)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96824503 <a title="1739-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-26-Tenure_lets_you_handle_students_who_cheat.html">1028 andrew gelman stats-2011-11-26-Tenure lets you handle students who cheat</a></p>
<p>Introduction: The other day, a friend of mine who is an untenured professor (not in statistics or political science) was telling me about a class where many of the students seemed to be resubmitting papers that they had already written for previous classes.  (The supposition was based on internal evidence of the topics of the submitted papers.)  It would be possible to check this and then kick the cheating students out of the program—but why do it?  It would be a lot of work, also some of the students who are caught might complain, then word would get around that my friend is a troublemaker.  And nobody likes a troublemaker.
 
Once my friend has tenure it would be possible to do the right thing.  But . . . here’s the hitch:  most college instructors do  not  have tenure, and one result, I suspect, is a decline in ethical standards.
 
This is something I hadn’t thought of in our  earlier discussion  of job security for teachers:  tenure gives you the freedom to kick out cheating students.</p><p>2 0.96673131 <a title="1739-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-09-Understanding_predictive_information_criteria_for_Bayesian_models.html">1975 andrew gelman stats-2013-08-09-Understanding predictive information criteria for Bayesian models</a></p>
<p>Introduction: Jessy, Aki, and I  write :
  
We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a bias-corrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this review is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.
  
I like this paper.  It came about as a result of preparing Chapter 7 for the  new BDA .  I had difficulty understanding AIC, DIC, WAIC, etc., but I recognized that these methods served a need.  My first plan was to just apply DIC and WAIC on a couple of simple examples (a linear regression and the 8 schools) and leave it at that.  But when I did the calculations, I couldnâ&euro;&trade;t understand the resu</p><p>3 0.96106631 <a title="1739-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-04-Burgess_on_Kipling.html">16 andrew gelman stats-2010-05-04-Burgess on Kipling</a></p>
<p>Introduction: This is my last entry derived from  Anthony Burgess’s book reviews , and it’ll be short.  His review of Angus Wilson’s “The Strange Ride of Rudyard Kipling:  His Life and Works” is a wonderfully balanced little thing.  Nothing incredibly deep–like most items in the collection, the review is only two pages long–but I give it credit for being a rare piece of Kipling criticism I’ve seen that (a) seriously engages with the politics, without (b) congratulating itself on bravely going against the fashions of the politically incorrect chattering classes by celebrating Kipling’s magnificent achievement blah blah blah.  Instead, Burgess shows respect for Kipling’s work and puts it in historical, biographical, and literary context.
 
Burgess concludes that Wilson’s book “reminds us, in John Gross’s words, that Kipling ‘remains a haunting, unsettling presence, with whom we still have to come to terms.’  Still.”  Well put, and generous of Burgess to end his review with another’s quote.
 
Other cri</p><p>4 0.95623219 <a title="1739-lda-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-09-The_difference_between_%E2%80%9Csignificant%E2%80%9D_and_%E2%80%9Cnon-significant%E2%80%9D_is_not_itself_statistically_significant.html">1662 andrew gelman stats-2013-01-09-The difference between “significant” and “non-significant” is not itself statistically significant</a></p>
<p>Introduction: Commenter Rahul  asked  what I thought of this  note  by Scott Firestone ( link  from Tyler Cowen) criticizing a recent  discussion  by Kevin Drum suggesting that lead exposure causes violent crime.  Firestone writes: 
  
  
It turns out there was in fact a prospective study done—but its implications for Drum’s argument are mixed. The study was a cohort study done by researchers at the University of Cincinnati. Between 1979 and 1984, 376 infants were recruited. Their parents consented to have lead levels in their blood tested over time; this was matched with records over subsequent decades of the individuals’ arrest records, and specifically arrest for violent crime. Ultimately, some of these individuals were dropped from the study; by the end, 250 were selected for the results.


The researchers found that for each increase of 5 micrograms of lead per deciliter of blood, there was a higher risk for being arrested for a violent crime, but a further look at the numbers shows a more mixe</p><p>5 0.95594347 <a title="1739-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-07-Duncan_Watts_and_the_Titanic.html">1370 andrew gelman stats-2012-06-07-Duncan Watts and the Titanic</a></p>
<p>Introduction: Daniel Mendelsohn recently  asked , “Why do we love the Titanic?”, seeking to understand how it has happened that: 
  
  
It may not be true that ‘the three most written-about subjects of all time are Jesus, the Civil War, and the Titanic,’ as one historian has put it, but it’s not much of an exaggeration. . . . The inexhaustible interest suggests that the Titanic’s story taps a vein much deeper than the morbid fascination that has attached to other disasters. The explosion of the Hindenburg, for instance, and even the torpedoing, just three years after the Titanic sank, of the Lusitania, another great liner whose passenger list boasted the rich and the famous, were calamities that shocked the world but have failed to generate an obsessive preoccupation. . . . 


If the Titanic has gripped our imagination so forcefully for the past century, it must be because of something bigger than any fact of social or political or cultural history. To get to the bottom of why we can’t forget it, yo</p><p>6 0.95554042 <a title="1739-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-16-NYT_Labs_releases_Openpaths%2C_a_utility_for_saving_your_iphone_data.html">714 andrew gelman stats-2011-05-16-NYT Labs releases Openpaths, a utility for saving your iphone data</a></p>
<p>7 0.94786239 <a title="1739-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-28-Amusing_case_of_self-defeating_science_writing.html">827 andrew gelman stats-2011-07-28-Amusing case of self-defeating science writing</a></p>
<p>same-blog 8 0.94778293 <a title="1739-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>9 0.9473114 <a title="1739-lda-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-26-WAIC_and_cross-validation_in_Stan%21.html">2349 andrew gelman stats-2014-05-26-WAIC and cross-validation in Stan!</a></p>
<p>10 0.94585764 <a title="1739-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-02-Not_so_fast_on_levees_and_seawalls_for_NY_harbor%3F.html">1558 andrew gelman stats-2012-11-02-Not so fast on levees and seawalls for NY harbor?</a></p>
<p>11 0.93886423 <a title="1739-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-22-Deviance%2C_DIC%2C_AIC%2C_cross-validation%2C_etc.html">776 andrew gelman stats-2011-06-22-Deviance, DIC, AIC, cross-validation, etc</a></p>
<p>12 0.93739116 <a title="1739-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-07-Environmentally_induced_cancer_%E2%80%9Cgrossly_underestimated%E2%80%9D%3F__Doubtful..html">21 andrew gelman stats-2010-05-07-Environmentally induced cancer “grossly underestimated”?  Doubtful.</a></p>
<p>13 0.92972827 <a title="1739-lda-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-01-%E2%80%9CThough_They_May_Be_Unaware%2C_Newlyweds_Implicitly_Know_Whether_Their_Marriage_Will_Be_Satisfying%E2%80%9D.html">2156 andrew gelman stats-2014-01-01-“Though They May Be Unaware, Newlyweds Implicitly Know Whether Their Marriage Will Be Satisfying”</a></p>
<p>14 0.92459077 <a title="1739-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-06-Poverty%2C_educational_performance_%E2%80%93_and_can_be_done_about_it.html">561 andrew gelman stats-2011-02-06-Poverty, educational performance – and can be done about it</a></p>
<p>15 0.92077422 <a title="1739-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-09-Partial_least_squares_path_analysis.html">1714 andrew gelman stats-2013-02-09-Partial least squares path analysis</a></p>
<p>16 0.91928178 <a title="1739-lda-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-04-28-But_it_all_goes_to_pay_for_gas%2C_car_insurance%2C_and_tolls_on_the_turnpike.html">9 andrew gelman stats-2010-04-28-But it all goes to pay for gas, car insurance, and tolls on the turnpike</a></p>
<p>17 0.91727453 <a title="1739-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-23-More_on_Bayesian_methods_and_multilevel_modeling.html">2033 andrew gelman stats-2013-09-23-More on Bayesian methods and multilevel modeling</a></p>
<p>18 0.91525537 <a title="1739-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-28-LOL_without_the_CATS.html">1433 andrew gelman stats-2012-07-28-LOL without the CATS</a></p>
<p>19 0.9122743 <a title="1739-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-12-The_importance_of_style_in_academic_writing.html">902 andrew gelman stats-2011-09-12-The importance of style in academic writing</a></p>
<p>20 0.91162193 <a title="1739-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-22-Struggles_over_the_criticism_of_the_%E2%80%9Ccannabis_users_and_IQ_change%E2%80%9D_paper.html">1910 andrew gelman stats-2013-06-22-Struggles over the criticism of the “cannabis users and IQ change” paper</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
