<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2004 andrew gelman stats-2013-09-01-Post-publication peer review:  How it (sometimes) really works</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-2004" href="#">andrew_gelman_stats-2013-2004</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2004 andrew gelman stats-2013-09-01-Post-publication peer review:  How it (sometimes) really works</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-2004-html" href="http://andrewgelman.com/2013/09/01/post-publication-peer-review-how-it-sometimes-really-works/">html</a></p><p>Introduction: In an ideal world, research articles would be open to criticism and discussion in the same place where they are published, in a sort of non-corrupt version of Yelp.  What is happening now is that the occasional paper or research area gets lots of press coverage, and this inspires reactions on science-focused blogs.  The trouble here is that it’s easier to give off-the-cuff comments than detailed criticisms.
 
Here’s an example.  It starts a couple years ago with  this article  by Ryota Kanai, Tom Feilden, Colin Firth, and Geraint Rees, on brain size and political orientation:
  
In a large sample of young adults, we related self-reported political attitudes to gray matter volume using structural MRI. We found that greater liberalism was associated with increased gray matter volume in the anterior cingulate cortex, whereas greater conservatism was associated with increased volume of the right amygdala. These results were replicated in an independent sample of additional participants. Ou</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 It starts a couple years ago with  this article  by Ryota Kanai, Tom Feilden, Colin Firth, and Geraint Rees, on brain size and political orientation:    In a large sample of young adults, we related self-reported political attitudes to gray matter volume using structural MRI. [sent-5, score-0.49]
</p><p>2 We found that greater liberalism was associated with increased gray matter volume in the anterior cingulate cortex, whereas greater conservatism was associated with increased volume of the right amygdala. [sent-6, score-0.61]
</p><p>3 Our findings extend previous observations that political attitudes reflect differences in self-regulatory conflict monitoring . [sent-8, score-0.243]
</p><p>4 So these researchers can test their hypotheses by more directly correlating the brain functions with the asshole/pussy dimension, no? [sent-14, score-0.269]
</p><p>5 A couple years later, the paper by Schreiber et al. [sent-20, score-0.381]
</p><p>6 the paper supplies zero reason to adjust any view I have—or anyone else does, in my opinion—on any matter relating to individual differences in cognition & ideology. [sent-32, score-0.332]
</p><p>7 used a fundamentally flawed statistical approach in which they basically went searching for statistical significance:    There are literally hundreds of thousands of potential “observations” in the brain of each study subject. [sent-35, score-0.376]
</p><p>8 Because there is constantly varying activation levels going on throughout the brain at all time, one can always find “statistically significant” correlations between stimuli and brain activation by chance. [sent-36, score-0.793]
</p><p>9 They did initially offer hypotheses based on four precisely defined brain ROIs in “the right amygdala, left insula, right entorhinal cortex, and anterior cingulate. [sent-44, score-0.806]
</p><p>10 didn’t find any significant differences in the activation levels within the portions of either the amygdala or the anterior cingulate cortex singled out in the 2011 Kanai et al. [sent-49, score-1.408]
</p><p>11 find any such differences in a host of other precisely defined areas (the “entorhinal cortex,” “left insula,” or “Right Entorhinal”) that Kanai et al. [sent-52, score-0.408]
</p><p>12 in the anterior cingulate cortex,” but they did manage to find some “significant” differences among Democrats’ and Republicans’ brain activation levels in portions of the “right amygdala” and “insula. [sent-65, score-0.921]
</p><p>13 Here’s Kahan again:    They selected observations of activating “voxels” in the amygdala of Republican subjects precisely because those voxels—as opposed to others that Schreiber et al. [sent-67, score-0.685]
</p><p>14 We can deduce that the paper was rejected by Science, Nature, various other biology journals, and maybe some political science journals as well. [sent-86, score-0.429]
</p><p>15 I’m not saying you shouldn’t criticize the paper in question, but you can’t really demand better from a paper published in a bottom-feeder journal. [sent-87, score-0.445]
</p><p>16 But it’s certainly no surprise if a paper published in a low-grade journal happens to be crap. [sent-89, score-0.282]
</p><p>17 What were the reasons that those 3 journals (I’m guessing) rejected the paper? [sent-95, score-0.257]
</p><p>18 Were they procedural reasons (“We don’t publish political science papers”), or irrelevant reasons (“I just don’t like this paper”), or valid criticisms (such as Kahan’s noted above)? [sent-96, score-0.229]
</p><p>19 As we know so well, fatally flawed papers can appear in top journals and get fawning press; the pre-publication peer-review process is far from perfect. [sent-98, score-0.235]
</p><p>20 You can get lots of “Andy Gelmans” and “Erik Voetens” who just post a paper without reading it, and only the occasional “Dan Kahan” who takes a detailed examination. [sent-101, score-0.268]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('schreiber', 0.416), ('kahan', 0.221), ('et', 0.214), ('brain', 0.197), ('cortex', 0.19), ('amygdala', 0.185), ('kanai', 0.185), ('anterior', 0.169), ('activation', 0.169), ('paper', 0.167), ('erik', 0.152), ('cingulate', 0.139), ('entorhinal', 0.139), ('differences', 0.117), ('rejected', 0.109), ('journals', 0.096), ('significant', 0.095), ('activating', 0.093), ('expanse', 0.093), ('rois', 0.093), ('voeten', 0.093), ('papers', 0.086), ('voxels', 0.084), ('insula', 0.084), ('searching', 0.079), ('precisely', 0.077), ('conservatives', 0.077), ('volume', 0.075), ('hypotheses', 0.072), ('portions', 0.069), ('observations', 0.069), ('valid', 0.068), ('published', 0.061), ('levels', 0.061), ('political', 0.057), ('nobody', 0.056), ('gray', 0.056), ('left', 0.056), ('crap', 0.055), ('liberals', 0.054), ('journal', 0.054), ('flawed', 0.053), ('occasional', 0.052), ('reasons', 0.052), ('criticize', 0.05), ('detailed', 0.049), ('right', 0.048), ('matter', 0.048), ('selected', 0.047), ('hundreds', 0.047)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="2004-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-01-Post-publication_peer_review%3A__How_it_%28sometimes%29_really_works.html">2004 andrew gelman stats-2013-09-01-Post-publication peer review:  How it (sometimes) really works</a></p>
<p>Introduction: In an ideal world, research articles would be open to criticism and discussion in the same place where they are published, in a sort of non-corrupt version of Yelp.  What is happening now is that the occasional paper or research area gets lots of press coverage, and this inspires reactions on science-focused blogs.  The trouble here is that it’s easier to give off-the-cuff comments than detailed criticisms.
 
Here’s an example.  It starts a couple years ago with  this article  by Ryota Kanai, Tom Feilden, Colin Firth, and Geraint Rees, on brain size and political orientation:
  
In a large sample of young adults, we related self-reported political attitudes to gray matter volume using structural MRI. We found that greater liberalism was associated with increased gray matter volume in the anterior cingulate cortex, whereas greater conservatism was associated with increased volume of the right amygdala. These results were replicated in an independent sample of additional participants. Ou</p><p>2 0.27047625 <a title="2004-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-06-How_to_think_about_papers_published_in_low-grade_journals%3F.html">1928 andrew gelman stats-2013-07-06-How to think about papers published in low-grade journals?</a></p>
<p>Introduction: We’ve had lots of lively discussions of fatally-flawed papers that have been published in top, top journals such as the  American Economic Review  or the  Journal of Personality and Social Psychology  or the  American Sociological Review  or the  tabloids .  And we also know about mistakes that make their way into mid-ranking outlets such as the Journal of Theoretical Biology.
 
But what about results that appear in the lower tier of legitimate journals?  I was thinking about this after reading a  post  by Dan Kahan slamming a paper that recently appeared in PLOS-One.  I won’t discuss the paper itself here because that’s not my point.  Rather, I had some thoughts regarding Kahan’s annoyance that a paper with fatal errors was published at all.  I commented as follows:
  
Read between the lines. The paper originally was released in 2009 and was published in 2013 in PLOS-One, which is one step above appearing on Arxiv. PLOS-One publishes some good things (so does Arxiv) but it’s the place</p><p>3 0.16879317 <a title="2004-tfidf-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-29-Brain_Structure_and_the_Big_Five.html">490 andrew gelman stats-2010-12-29-Brain Structure and the Big Five</a></p>
<p>Introduction: Many years ago, a research psychologist whose judgment I greatly respect told me that the characterization of personality by the so-called Big Five traits (extraversion, etc.) was old-fashioned.  So I’m always surprised to see that the Big Five keeps cropping up.  I guess not everyone agrees that it’s a bad idea.
 
For example, Hamdan Azhar wrote to me:
  
 
I was wondering if you’d seen  this recent paper  (De Young et al. 2010) that finds significant correlations between brain volume in selected regions and personality trait measures (from the Big Five). This is quite a ground-breaking finding and it was covered extensively in the mainstream media. I think readers of your blog would be interested in your thoughts, statistically speaking, on their methodology and findings.
 

 
My reply:   I’d  be interested in my thoughts on this too!  But I don’t know enough to say anything useful.
 
From the abstract of the paper under discussion:
  
Controlling for age, sex, and whole-brain volume</p><p>4 0.15298828 <a title="2004-tfidf-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-23-Scientists_can_read_your_mind_._._._as_long_as_the%E2%80%99re_allowed_to_look_at_more_than_one_place_in_your_brain_and_then_make_a_prediction_after_seeing_what_you_actually_did.html">106 andrew gelman stats-2010-06-23-Scientists can read your mind . . . as long as the’re allowed to look at more than one place in your brain and then make a prediction after seeing what you actually did</a></p>
<p>Introduction: Maggie Fox  writes :
  
Brain scans may be able to predict what you will do better than you can yourself . . .  They found a way to interpret “real time” brain images to show whether people who viewed messages about using sunscreen would actually use sunscreen during the following week.


The scans were more accurate than the volunteers were, Emily Falk and colleagues at the University of California Los Angeles reported in the Journal of Neuroscience. . . .


About half the volunteers had correctly predicted whether they would use sunscreen. The research team analyzed and re-analyzed the MRI scans to see if they could find any brain activity that would do better.


Activity in one area of the brain, a particular part of the medial prefrontal cortex, provided the best information.


“From this region of the brain, we can predict for about three-quarters of the people whether they will increase their use of sunscreen beyond what they say they will do,” Lieberman said.


“It is the one re</p><p>5 0.14846185 <a title="2004-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-20-What_happened_that_the_journal_Psychological_Science_published_a_paper_with_no_identifiable_strengths%3F.html">1865 andrew gelman stats-2013-05-20-What happened that the journal Psychological Science published a paper with no identifiable strengths?</a></p>
<p>Introduction: The other day we  discussed  that paper on ovulation and voting (you may recall that the authors reported a scattered bunch of comparisons, significance tests, and p-values, and I recommended that they would’ve done better to simply report complete summaries of their data, so that readers could see the comparisons of interest in full context), and I was thinking a bit more about why I was so bothered that it was published in Psychological Science, which I’d thought of as a serious research journal. 
   
My concern isn’t just that that the paper is bad—after all, lots of bad papers get published—but rather that it had nothing really going for it,  except  that it was headline bait.  It was a survey done on Mechanical Turk, that’s it.  No clever design, no clever questions, no care in dealing with nonresponse problems, no innovative data analysis, no nothing.  The paper had nothing to offer, except that it had no obvious flaws.  Psychology is a huge field full of brilliant researchers.</p><p>6 0.14824495 <a title="2004-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-03-Evaluating_evidence_from_published_research.html">2006 andrew gelman stats-2013-09-03-Evaluating evidence from published research</a></p>
<p>7 0.14795133 <a title="2004-tfidf-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-12-More_on_publishing_in_journals.html">2245 andrew gelman stats-2014-03-12-More on publishing in journals</a></p>
<p>8 0.14391689 <a title="2004-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-31-How_to_fix_the_tabloids%3F__Toward_replicable_social_science_research.html">1878 andrew gelman stats-2013-05-31-How to fix the tabloids?  Toward replicable social science research</a></p>
<p>9 0.13054782 <a title="2004-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-With_a_bit_of_precognition%2C_you%E2%80%99d_have_known_I_was_going_to_post_again_on_this_topic%2C_and_with_a_lot_of_precognition%2C_you%E2%80%99d_have_known_I_was_going_to_post_today.html">576 andrew gelman stats-2011-02-15-With a bit of precognition, you’d have known I was going to post again on this topic, and with a lot of precognition, you’d have known I was going to post today</a></p>
<p>10 0.12350617 <a title="2004-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-30-Retracted_articles_and_unethical_behavior_in_economics_journals%3F.html">1435 andrew gelman stats-2012-07-30-Retracted articles and unethical behavior in economics journals?</a></p>
<p>11 0.12265887 <a title="2004-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-01-I_owe_it_all_to_the_haters.html">834 andrew gelman stats-2011-08-01-I owe it all to the haters</a></p>
<p>12 0.12150216 <a title="2004-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-21-Kahan_on_Pinker_on_politics.html">1633 andrew gelman stats-2012-12-21-Kahan on Pinker on politics</a></p>
<p>13 0.12083839 <a title="2004-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-The_reverse-journal-submission_system.html">1393 andrew gelman stats-2012-06-26-The reverse-journal-submission system</a></p>
<p>14 0.12059465 <a title="2004-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-10-Don%E2%80%99t_trust_the_Turk.html">1932 andrew gelman stats-2013-07-10-Don’t trust the Turk</a></p>
<p>15 0.11655109 <a title="2004-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-31-Response_by_Jessica_Tracy_and_Alec_Beall_to_my_critique_of_the_methods_in_their_paper%2C_%E2%80%9CWomen_Are_More_Likely_to_Wear_Red_or_Pink_at_Peak_Fertility%E2%80%9D.html">1963 andrew gelman stats-2013-07-31-Response by Jessica Tracy and Alec Beall to my critique of the methods in their paper, “Women Are More Likely to Wear Red or Pink at Peak Fertility”</a></p>
<p>16 0.11518255 <a title="2004-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-17-100%21.html">1537 andrew gelman stats-2012-10-17-100!</a></p>
<p>17 0.11452618 <a title="2004-tfidf-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-23-The_bane_of_many_causes.html">48 andrew gelman stats-2010-05-23-The bane of many causes</a></p>
<p>18 0.11237204 <a title="2004-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-17-How_can_statisticians_help_psychologists_do_their_research_better%3F.html">1860 andrew gelman stats-2013-05-17-How can statisticians help psychologists do their research better?</a></p>
<p>19 0.1072481 <a title="2004-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-The_unitary_nature_of_consciousness%3A__%E2%80%9CIt%E2%80%99s_impossible_to_be_insanely_frustrated_about_2_things_at_once%E2%80%9D.html">1375 andrew gelman stats-2012-06-11-The unitary nature of consciousness:  “It’s impossible to be insanely frustrated about 2 things at once”</a></p>
<p>20 0.10684327 <a title="2004-tfidf-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-11-What_if_I_were_to_stop_publishing_in_journals%3F.html">2244 andrew gelman stats-2014-03-11-What if I were to stop publishing in journals?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.207), (1, -0.061), (2, 0.005), (3, -0.134), (4, -0.078), (5, -0.075), (6, -0.027), (7, -0.103), (8, -0.019), (9, -0.003), (10, 0.093), (11, 0.031), (12, -0.029), (13, -0.012), (14, 0.057), (15, -0.014), (16, 0.01), (17, 0.017), (18, -0.037), (19, -0.032), (20, 0.007), (21, -0.008), (22, -0.041), (23, -0.041), (24, 0.019), (25, 0.013), (26, -0.024), (27, -0.006), (28, 0.058), (29, -0.048), (30, -0.004), (31, 0.003), (32, 0.052), (33, -0.02), (34, -0.057), (35, -0.02), (36, -0.024), (37, 0.031), (38, 0.005), (39, -0.015), (40, 0.03), (41, -0.02), (42, 0.003), (43, 0.022), (44, -0.039), (45, -0.064), (46, 0.054), (47, 0.005), (48, 0.004), (49, -0.045)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97802263 <a title="2004-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-01-Post-publication_peer_review%3A__How_it_%28sometimes%29_really_works.html">2004 andrew gelman stats-2013-09-01-Post-publication peer review:  How it (sometimes) really works</a></p>
<p>Introduction: In an ideal world, research articles would be open to criticism and discussion in the same place where they are published, in a sort of non-corrupt version of Yelp.  What is happening now is that the occasional paper or research area gets lots of press coverage, and this inspires reactions on science-focused blogs.  The trouble here is that it’s easier to give off-the-cuff comments than detailed criticisms.
 
Here’s an example.  It starts a couple years ago with  this article  by Ryota Kanai, Tom Feilden, Colin Firth, and Geraint Rees, on brain size and political orientation:
  
In a large sample of young adults, we related self-reported political attitudes to gray matter volume using structural MRI. We found that greater liberalism was associated with increased gray matter volume in the anterior cingulate cortex, whereas greater conservatism was associated with increased volume of the right amygdala. These results were replicated in an independent sample of additional participants. Ou</p><p>2 0.79560333 <a title="2004-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-06-How_to_think_about_papers_published_in_low-grade_journals%3F.html">1928 andrew gelman stats-2013-07-06-How to think about papers published in low-grade journals?</a></p>
<p>Introduction: We’ve had lots of lively discussions of fatally-flawed papers that have been published in top, top journals such as the  American Economic Review  or the  Journal of Personality and Social Psychology  or the  American Sociological Review  or the  tabloids .  And we also know about mistakes that make their way into mid-ranking outlets such as the Journal of Theoretical Biology.
 
But what about results that appear in the lower tier of legitimate journals?  I was thinking about this after reading a  post  by Dan Kahan slamming a paper that recently appeared in PLOS-One.  I won’t discuss the paper itself here because that’s not my point.  Rather, I had some thoughts regarding Kahan’s annoyance that a paper with fatal errors was published at all.  I commented as follows:
  
Read between the lines. The paper originally was released in 2009 and was published in 2013 in PLOS-One, which is one step above appearing on Arxiv. PLOS-One publishes some good things (so does Arxiv) but it’s the place</p><p>3 0.79350841 <a title="2004-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-31-How_to_fix_the_tabloids%3F__Toward_replicable_social_science_research.html">1878 andrew gelman stats-2013-05-31-How to fix the tabloids?  Toward replicable social science research</a></p>
<p>Introduction: This seems to be the topic of the week.  Yesterday I posted on the sister blog  some further thoughts  on those “Psychological Science” papers on menstrual cycles, biceps size, and political attitudes, tied to a horrible press release from the journal Psychological Science hyping the biceps and politics study.
 
Then I was pointed to these  suggestions  from Richard Lucas and M. Brent Donnellan have on improving the replicability and reproducibility of research published in the Journal of Research in Personality:
  
It goes without saying that editors of scientific journals strive to publish research that is not only theoretically interesting but also methodologically rigorous. The goal is to select papers that advance the field. Accordingly, editors want to publish findings that can be reproduced and replicated by other scientists. Unfortunately, there has been a recent “crisis in confidence” among psychologists about the quality of psychological research (Pashler & Wagenmakers, 2012)</p><p>4 0.78599274 <a title="2004-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-05-Against_double-blind_reviewing%3A__Political_science_and_statistics_are_not_like_biology_and_physics.html">601 andrew gelman stats-2011-03-05-Against double-blind reviewing:  Political science and statistics are not like biology and physics</a></p>
<p>Introduction: Responding to a proposal to move the journal Political Analysis from double-blind to single-blind reviewing (that is, authors would not know who is reviewing their papers but reviewers would know the authors’ names), Tom Palfrey writes:
  
I agree with the editors’ recommendation. I have served on quite a few editorial boards of journals with different blinding policies, and have seen no evidence that double blind procedures are a useful way to improve the quality of articles published in a journal. Aside from the obvious administrative nuisance and the fact that authorship anonymity is a thing of the past in our discipline, the theoretical and empirical arguments in both directions lead to an ambiguous conclusion. Also keep in mind that the editors know the identity of the authors (they need to know for practical reasons), their identity is not hidden from authors, and ultimately it is they who make the accept/reject decision, and also lobby their friends and colleagues to submit “the</p><p>5 0.77494109 <a title="2004-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-22-Likelihood_Ratio_%E2%89%A0_1_Journal.html">1774 andrew gelman stats-2013-03-22-Likelihood Ratio ≠ 1 Journal</a></p>
<p>Introduction: Dan Kahan  writes :
  
The basic idea . . . is to promote identification of study designs that scholars who disagree about a proposition would agree would generate evidence relevant to their competing conjectures—regardless of what studies based on such designs actually find. Articles proposing designs of this sort would be selected for publication and only then be carried out, by the proposing researchers with funding from the journal, which would publish the results too.


Now I [Kahan] am aware of a set of real journals that have a similar motivation.


One is the  Journal of Articles in Support of the Null Hypothesis, which as its title implies publishes papers reporting studies that fail to “reject” the null. Like JASNH, LR ≠1J would try to offset the “file drawer” bias and like bad consequences associated with the convention of publishing only findings that are “significant at p < 0.05."


But it would try to do more. By publishing studies that are deemed to have valid designs an</p><p>6 0.77224261 <a title="2004-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-01-I_owe_it_all_to_the_haters.html">834 andrew gelman stats-2011-08-01-I owe it all to the haters</a></p>
<p>7 0.77086323 <a title="2004-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-20-%E2%80%9CI_know_you_aren%E2%80%99t_the_plagiarism_police%2C_but_._._.%E2%80%9D.html">1585 andrew gelman stats-2012-11-20-“I know you aren’t the plagiarism police, but . . .”</a></p>
<p>8 0.76948804 <a title="2004-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-15-Fun_fight_over_the_Grover_search_algorithm.html">1120 andrew gelman stats-2012-01-15-Fun fight over the Grover search algorithm</a></p>
<p>9 0.76639181 <a title="2004-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-20-What_happened_that_the_journal_Psychological_Science_published_a_paper_with_no_identifiable_strengths%3F.html">1865 andrew gelman stats-2013-05-20-What happened that the journal Psychological Science published a paper with no identifiable strengths?</a></p>
<p>10 0.75689381 <a title="2004-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-24-Difficulties_in_publishing_non-replications_of_implausible_findings.html">1137 andrew gelman stats-2012-01-24-Difficulties in publishing non-replications of implausible findings</a></p>
<p>11 0.75532156 <a title="2004-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-22-Arrow%E2%80%99s_other_theorem.html">675 andrew gelman stats-2011-04-22-Arrow’s other theorem</a></p>
<p>12 0.75294101 <a title="2004-lsi-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-30-I_posted_this_as_a_comment_on_a_sociology_blog.html">2353 andrew gelman stats-2014-05-30-I posted this as a comment on a sociology blog</a></p>
<p>13 0.75096112 <a title="2004-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-26-Suggested_resolution_of_the_Bem_paradox.html">1139 andrew gelman stats-2012-01-26-Suggested resolution of the Bem paradox</a></p>
<p>14 0.74885929 <a title="2004-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-17-How_can_statisticians_help_psychologists_do_their_research_better%3F.html">1860 andrew gelman stats-2013-05-17-How can statisticians help psychologists do their research better?</a></p>
<p>15 0.74875909 <a title="2004-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-30-Retracted_articles_and_unethical_behavior_in_economics_journals%3F.html">1435 andrew gelman stats-2012-07-30-Retracted articles and unethical behavior in economics journals?</a></p>
<p>16 0.7466417 <a title="2004-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-13-How_should_journals_handle_replication_studies%3F.html">762 andrew gelman stats-2011-06-13-How should journals handle replication studies?</a></p>
<p>17 0.74230421 <a title="2004-lsi-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-27-Beyond_the_Valley_of_the_Trolls.html">2269 andrew gelman stats-2014-03-27-Beyond the Valley of the Trolls</a></p>
<p>18 0.73682624 <a title="2004-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-17-Replication_backlash.html">2137 andrew gelman stats-2013-12-17-Replication backlash</a></p>
<p>19 0.73589242 <a title="2004-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-13-Preregistration_of_Studies_and_Mock_Reports.html">1671 andrew gelman stats-2013-01-13-Preregistration of Studies and Mock Reports</a></p>
<p>20 0.73571736 <a title="2004-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-28-50_shades_of_gray%3A__A_research_story.html">1959 andrew gelman stats-2013-07-28-50 shades of gray:  A research story</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(3, 0.029), (15, 0.057), (16, 0.121), (21, 0.02), (24, 0.123), (43, 0.013), (56, 0.018), (61, 0.02), (71, 0.018), (72, 0.017), (79, 0.018), (84, 0.166), (86, 0.047), (99, 0.195)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.94740427 <a title="2004-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-23-Philosophy%3A__Pointer_to_Salmon.html">1181 andrew gelman stats-2012-02-23-Philosophy:  Pointer to Salmon</a></p>
<p>Introduction: Larry Brownstein writes:
  
I read  your article  on induction and deduction and your comments on Deborah Mayo’s approach and thought you might find the following useful in this discussion. It is Wesley Salmon’s Reality and Rationality (2005). Here he argues that Bayesian inferential procedures can replace the hypothetical-deductive method aka the Hempel-Oppenheim theory of explanation. He is concerned about the subjectivity problem, so takes a frequentist approach to the use of Bayes in this context.


Hardly anyone agrees that the H-D approach accounts for scientific explanation. The problem has been to find a replacement. Salmon thought he had found it.
  
I don’t know this book—but that’s no surprise since I know just about none of the philosophy of science literature that came after Popper, Kuhn, and Lakatos.  That’s why I collaborated with Cosma Shalizi.  He’s the one who connected me to Deborah Mayo and who put in the recent philosophy references in our articles.  Anyway, I’m pa</p><p>2 0.94174135 <a title="2004-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-06-Sociotropic_Voting_and_the_Media.html">323 andrew gelman stats-2010-10-06-Sociotropic Voting and the Media</a></p>
<p>Introduction: Stephen Ansolabehere, Marc Meredith, and Erik Snowberg  write :
  
The literature on economic voting notes that voters’ subjective evaluations of the overall state of the economy are correlated with vote choice, whereas personal economic experiences are not. Missing from this literature is a description of how voters acquire information about the general state of the economy, and how that information is used to form perceptions. In order to begin understanding this process, we [Ansolabehere, Meredith, and Snowberg] asked a series of questions on the 2006 ANES Pilot about respondents’ perceptions of the average price of gas and the unemployment rate in their home state.


We find that questions about gas prices and unemployment show differences in the sources of information about these two economic variables. Information about unemployment rates come from media sources, and are systematically biased by partisan factors. Information about gas prices, in contrast, comes only from everyday</p><p>3 0.93689352 <a title="2004-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-29-Brain_Structure_and_the_Big_Five.html">490 andrew gelman stats-2010-12-29-Brain Structure and the Big Five</a></p>
<p>Introduction: Many years ago, a research psychologist whose judgment I greatly respect told me that the characterization of personality by the so-called Big Five traits (extraversion, etc.) was old-fashioned.  So I’m always surprised to see that the Big Five keeps cropping up.  I guess not everyone agrees that it’s a bad idea.
 
For example, Hamdan Azhar wrote to me:
  
 
I was wondering if you’d seen  this recent paper  (De Young et al. 2010) that finds significant correlations between brain volume in selected regions and personality trait measures (from the Big Five). This is quite a ground-breaking finding and it was covered extensively in the mainstream media. I think readers of your blog would be interested in your thoughts, statistically speaking, on their methodology and findings.
 

 
My reply:   I’d  be interested in my thoughts on this too!  But I don’t know enough to say anything useful.
 
From the abstract of the paper under discussion:
  
Controlling for age, sex, and whole-brain volume</p><p>same-blog 4 0.91652876 <a title="2004-lda-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-01-Post-publication_peer_review%3A__How_it_%28sometimes%29_really_works.html">2004 andrew gelman stats-2013-09-01-Post-publication peer review:  How it (sometimes) really works</a></p>
<p>Introduction: In an ideal world, research articles would be open to criticism and discussion in the same place where they are published, in a sort of non-corrupt version of Yelp.  What is happening now is that the occasional paper or research area gets lots of press coverage, and this inspires reactions on science-focused blogs.  The trouble here is that it’s easier to give off-the-cuff comments than detailed criticisms.
 
Here’s an example.  It starts a couple years ago with  this article  by Ryota Kanai, Tom Feilden, Colin Firth, and Geraint Rees, on brain size and political orientation:
  
In a large sample of young adults, we related self-reported political attitudes to gray matter volume using structural MRI. We found that greater liberalism was associated with increased gray matter volume in the anterior cingulate cortex, whereas greater conservatism was associated with increased volume of the right amygdala. These results were replicated in an independent sample of additional participants. Ou</p><p>5 0.91488844 <a title="2004-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-21-Forensic_bioinformatics%2C_or%2C_Don%E2%80%99t_believe_everything_you_read_in_the_%28scientific%29_papers.html">360 andrew gelman stats-2010-10-21-Forensic bioinformatics, or, Don’t believe everything you read in the (scientific) papers</a></p>
<p>Introduction: Hadley Wickham sent me  this , by Keith Baggerly and Kevin Coombes:
  
In this report we [Baggerly and Coombes] examine several related papers purporting to use microarray-based signatures of drug sensitivity derived from cell lines to predict patient response. Patients in clinical trials are currently being allocated to treatment arms on the basis of these results. However, we show in five case studies that the results incorporate several simple errors that may be putting patients at risk. One theme that emerges is that the most common errors are simple (e.g., row or column offsets); conversely, it is our experience that the most simple errors are common.
  
This is horrible!  But, in a way, it’s not surprising.  I make big mistakes in my applied work all the time.  I mean, all the time.  Sometimes I scramble the order of the 50 states, or I’m plotting a pure noise variable, or whatever.  But usually I don’t drift too far from reality because I have a lot of cross-checks and I (or my</p><p>6 0.90250039 <a title="2004-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-Free_%245_gift_certificate%21.html">667 andrew gelman stats-2011-04-19-Free $5 gift certificate!</a></p>
<p>7 0.89413226 <a title="2004-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-25-Term_Limits_for_the_Supreme_Court%3F.html">235 andrew gelman stats-2010-08-25-Term Limits for the Supreme Court?</a></p>
<p>8 0.88725257 <a title="2004-lda-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-19-Updated_solutions_to_Bayesian_Data_Analysis_homeworks.html">42 andrew gelman stats-2010-05-19-Updated solutions to Bayesian Data Analysis homeworks</a></p>
<p>9 0.87774217 <a title="2004-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-25-The_harm_done_by_tests_of_significance.html">1776 andrew gelman stats-2013-03-25-The harm done by tests of significance</a></p>
<p>10 0.86829793 <a title="2004-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-30-Infill_asymptotics_and_sprawl_asymptotics.html">1877 andrew gelman stats-2013-05-30-Infill asymptotics and sprawl asymptotics</a></p>
<p>11 0.86101973 <a title="2004-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-21-More_on_Bayesian_model_selection_in_high-dimensional_settings.html">1817 andrew gelman stats-2013-04-21-More on Bayesian model selection in high-dimensional settings</a></p>
<p>12 0.85630643 <a title="2004-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>13 0.85562128 <a title="2004-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-06-Ideas_that_spread_fast_and_slow.html">2053 andrew gelman stats-2013-10-06-Ideas that spread fast and slow</a></p>
<p>14 0.85211766 <a title="2004-lda-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-Stan_Model_of_the_Week%3A_Hierarchical_Modeling_of_Supernovas.html">2299 andrew gelman stats-2014-04-21-Stan Model of the Week: Hierarchical Modeling of Supernovas</a></p>
<p>15 0.84013653 <a title="2004-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-22-Likelihood_Ratio_%E2%89%A0_1_Journal.html">1774 andrew gelman stats-2013-03-22-Likelihood Ratio ≠ 1 Journal</a></p>
<p>16 0.83931267 <a title="2004-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-21-Handbook_of_Markov_Chain_Monte_Carlo.html">674 andrew gelman stats-2011-04-21-Handbook of Markov Chain Monte Carlo</a></p>
<p>17 0.83843893 <a title="2004-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-04-That_half-Cauchy_prior.html">184 andrew gelman stats-2010-08-04-That half-Cauchy prior</a></p>
<p>18 0.83672148 <a title="2004-lda-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-05-Update_on_state_size_and_governors%E2%80%99_popularity.html">187 andrew gelman stats-2010-08-05-Update on state size and governors’ popularity</a></p>
<p>19 0.8357963 <a title="2004-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-13-Philosophy_of_Bayesian_statistics%3A__my_reactions_to_Wasserman.html">1165 andrew gelman stats-2012-02-13-Philosophy of Bayesian statistics:  my reactions to Wasserman</a></p>
<p>20 0.8357898 <a title="2004-lda-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-03-Web_equation.html">1152 andrew gelman stats-2012-02-03-Web equation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
