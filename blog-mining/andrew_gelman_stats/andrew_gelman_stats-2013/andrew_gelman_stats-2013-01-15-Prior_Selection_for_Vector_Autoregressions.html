<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1674 andrew gelman stats-2013-01-15-Prior Selection for Vector Autoregressions</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-1674" href="#">andrew_gelman_stats-2013-1674</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1674 andrew gelman stats-2013-01-15-Prior Selection for Vector Autoregressions</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-1674-html" href="http://andrewgelman.com/2013/01/15/prior-selection-for-vector-autoregressions/">html</a></p><p>Introduction: Brendan Nyhan sends along  this paper  by Domenico Giannone, Michele Lenza, and Giorgio Primiceri:
  
Vector autoregressions are flexible time series models that can capture complex dynamic interrelationships among macroeconomic variables. However, their dense parameterization leads to unstable inference and inaccurate out-of-sample forecasts, particularly for models with many variables. A solution to this problem is to use informative priors, in order to shrink the richly parameterized unrestricted model towards a parsimonious naive benchmark, and thus reduce estimation uncertainty. This paper studies the optimal choice of the informativeness of these priors, which we treat as additional parameters, in the spirit of hierarchical modeling. This approach is theoretically grounded, easy to implement, and greatly reduces the number and importance of subjective choices in the setting of the prior. Moreover, it performs very well both in terms of out-of-sample forecastingâ&euro;&rdquo;as well as factor</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Brendan Nyhan sends along  this paper  by Domenico Giannone, Michele Lenza, and Giorgio Primiceri:    Vector autoregressions are flexible time series models that can capture complex dynamic interrelationships among macroeconomic variables. [sent-1, score-0.97]
</p><p>2 However, their dense parameterization leads to unstable inference and inaccurate out-of-sample forecasts, particularly for models with many variables. [sent-2, score-0.633]
</p><p>3 A solution to this problem is to use informative priors, in order to shrink the richly parameterized unrestricted model towards a parsimonious naive benchmark, and thus reduce estimation uncertainty. [sent-3, score-1.299]
</p><p>4 This paper studies the optimal choice of the informativeness of these priors, which we treat as additional parameters, in the spirit of hierarchical modeling. [sent-4, score-0.451]
</p><p>5 This approach is theoretically grounded, easy to implement, and greatly reduces the number and importance of subjective choices in the setting of the prior. [sent-5, score-0.502]
</p><p>6 Moreover, it performs very well both in terms of out-of-sample forecastingâ&euro;&rdquo;as well as factor modelsâ&euro;&rdquo;and accuracy in the estimation of impulse response functions. [sent-6, score-0.488]
</p><p>7 I have no experience with vector autoregressions and have not read the article carefully, but I love how they frame the problem above. [sent-7, score-0.646]
</p><p>8 This is looking to me like another brick in the wall of weakly informative priors. [sent-8, score-0.493]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('autoregressions', 0.321), ('vector', 0.221), ('giorgio', 0.16), ('richly', 0.16), ('informativeness', 0.16), ('unrestricted', 0.16), ('domenico', 0.16), ('michele', 0.151), ('brick', 0.151), ('estimation', 0.151), ('priors', 0.15), ('informative', 0.147), ('parsimonious', 0.145), ('dense', 0.14), ('macroeconomic', 0.14), ('impulse', 0.14), ('parameterized', 0.132), ('inaccurate', 0.129), ('grounded', 0.126), ('unstable', 0.126), ('benchmark', 0.124), ('models', 0.12), ('shrink', 0.12), ('theoretically', 0.118), ('parameterization', 0.118), ('nyhan', 0.116), ('brendan', 0.115), ('reduces', 0.112), ('performs', 0.111), ('moreover', 0.108), ('flexible', 0.104), ('dynamic', 0.104), ('frame', 0.104), ('towards', 0.103), ('optimal', 0.102), ('greatly', 0.102), ('spirit', 0.101), ('weakly', 0.1), ('implement', 0.1), ('forecasts', 0.098), ('capture', 0.098), ('naive', 0.098), ('wall', 0.095), ('forecasting', 0.094), ('treat', 0.088), ('subjective', 0.087), ('accuracy', 0.086), ('sends', 0.083), ('reduce', 0.083), ('choices', 0.083)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="1674-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-15-Prior_Selection_for_Vector_Autoregressions.html">1674 andrew gelman stats-2013-01-15-Prior Selection for Vector Autoregressions</a></p>
<p>Introduction: Brendan Nyhan sends along  this paper  by Domenico Giannone, Michele Lenza, and Giorgio Primiceri:
  
Vector autoregressions are flexible time series models that can capture complex dynamic interrelationships among macroeconomic variables. However, their dense parameterization leads to unstable inference and inaccurate out-of-sample forecasts, particularly for models with many variables. A solution to this problem is to use informative priors, in order to shrink the richly parameterized unrestricted model towards a parsimonious naive benchmark, and thus reduce estimation uncertainty. This paper studies the optimal choice of the informativeness of these priors, which we treat as additional parameters, in the spirit of hierarchical modeling. This approach is theoretically grounded, easy to implement, and greatly reduces the number and importance of subjective choices in the setting of the prior. Moreover, it performs very well both in terms of out-of-sample forecastingâ&euro;&rdquo;as well as factor</p><p>2 0.16785631 <a title="1674-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-29-More_by_Berger_and_me_on_weakly_informative_priors.html">1092 andrew gelman stats-2011-12-29-More by Berger and me on weakly informative priors</a></p>
<p>Introduction: A couple days ago we  discussed  some remarks by Tony O’Hagan and Jim Berger on weakly informative priors.  Jim  followed up  on Deborah Mayo’s blog with this:
  
Objective Bayesian priors are often improper (i.e., have infinite total mass), but this is not a problem when they are developed correctly. But not every improper prior is satisfactory. For instance, the constant prior is known to be unsatisfactory in many situations. The ‘solution’ pseudo-Bayesians often use is to choose a constant prior over a large but bounded set (a ‘weakly informative’ prior), saying it is now proper and so all is well. This is not true; if the constant prior on the whole parameter space is bad, so will be the constant prior over the bounded set. The problem is, in part, that some people confuse proper priors with subjective priors and, having learned that true subjective priors are fine, incorrectly presume that weakly informative proper priors are fine.
  
I have a few reactions to this:
 
1.  I agree</p><p>3 0.16222274 <a title="1674-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-27-%E2%80%9CKeeping_things_unridiculous%E2%80%9D%3A__Berger%2C_O%E2%80%99Hagan%2C_and_me_on_weakly_informative_priors.html">1087 andrew gelman stats-2011-12-27-“Keeping things unridiculous”:  Berger, O’Hagan, and me on weakly informative priors</a></p>
<p>Introduction: Deborah Mayo sent me  this quote  from Jim Berger:
  
Too often I see people pretending to be subjectivists, and then using “weakly informative” priors that the objective Bayesian community knows are terrible and will give ridiculous answers; subjectivism is then being used as a shield to hide ignorance. . . . In my own more provocative moments, I claim that the only true subjectivists are the objective Bayesians, because they refuse to use subjectivism as a shield against criticism of sloppy pseudo-Bayesian practice.
  
This caught my attention because I’ve become more and more convinced that weakly informative priors are  the right way to go  in many different situations.  I don’t think Berger was talking about  me , though, as the above quote came from a publication in 2006, at which time I’d only started writing about weakly informative priors.
 
Going back to Berger’s  article , I see that his “weakly informative priors” remark was aimed at  this article  by Anthony O’Hagan, who w</p><p>4 0.14940144 <a title="1674-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>Introduction: Ryan King writes:
  
I was wondering if you have a brief comment on the state of the art for objective priors for hierarchical generalized linear models (generalized linear mixed models).  I have been working off the papers in Bayesian Analysis (2006) 1, Number 3 (Browne and Draper, Kass and Natarajan, Gelman).  There seems to have been continuous work for matching priors in linear mixed models, but GLMMs less so because of the lack of an analytic marginal likelihood for the variance components.  There are a number of additional suggestions in the literature since 2006, but little robust practical guidance.


I’m interested in both mean parameters and the variance components.  I’m almost always concerned with logistic random effect models.  I’m fascinated by the matching-priors idea of higher-order asymptotic improvements to maximum likelihood, and need to make some kind of defensible default recommendation.  Given the massive scale of the datasets (genetics …), extensive sensitivity a</p><p>5 0.14188145 <a title="1674-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-19-Prior_distributions_on_derived_quantities_rather_than_on_parameters_themselves.html">1946 andrew gelman stats-2013-07-19-Prior distributions on derived quantities rather than on parameters themselves</a></p>
<p>Introduction: Following up on our  discussion of the other day , Nick Firoozye writes: 
  
  
One thing I meant by my initial query (but really didn’t manage to get across) was this: I have no idea what my prior would be on many many models, but just like Utility Theory expects ALL consumers to attach a utility to any and all consumption goods (even those I haven’t seen or heard of), Bayesian Stats (almost) expects the same for priors. (Of course it’s not a religious edict much in the way Utility Theory has, since there is no theory of a “modeler” in the Bayesian paradigm—nonetheless there is still an expectation that we should have priors over all sorts of parameters which mean almost nothing to us).


For most models with sufficient complexity, I also have no idea what my informative priors are actually doing and the only way to know anything is through something I can see and experience, through data, not parameters or state variables.


My question was more on the—let’s use the prior to come up</p><p>6 0.11678457 <a title="1674-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-12-As_a_Bayesian_I_want_scientists_to_report_their_data_non-Bayesianly.html">1209 andrew gelman stats-2012-03-12-As a Bayesian I want scientists to report their data non-Bayesianly</a></p>
<p>7 0.11660071 <a title="1674-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-11-Weakly_informative_priors_for_Bayesian_nonparametric_models%3F.html">1454 andrew gelman stats-2012-08-11-Weakly informative priors for Bayesian nonparametric models?</a></p>
<p>8 0.10496546 <a title="1674-tfidf-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-15-Weakly_informative_priors_and_imprecise_probabilities.html">468 andrew gelman stats-2010-12-15-Weakly informative priors and imprecise probabilities</a></p>
<p>9 0.1031476 <a title="1674-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>10 0.10307218 <a title="1674-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>11 0.10138714 <a title="1674-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-13-On_the_half-Cauchy_prior_for_a_global_scale_parameter.html">801 andrew gelman stats-2011-07-13-On the half-Cauchy prior for a global scale parameter</a></p>
<p>12 0.099778615 <a title="1674-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-07-Prior_distributions_for_regression_coefficients.html">1486 andrew gelman stats-2012-09-07-Prior distributions for regression coefficients</a></p>
<p>13 0.094964869 <a title="1674-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-21-D._Buggin.html">1465 andrew gelman stats-2012-08-21-D. Buggin</a></p>
<p>14 0.092602849 <a title="1674-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-24-Mister_P_in_Stata.html">869 andrew gelman stats-2011-08-24-Mister P in Stata</a></p>
<p>15 0.092240423 <a title="1674-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-10-Cross-validation_and_Bayesian_estimation_of_tuning_parameters.html">2129 andrew gelman stats-2013-12-10-Cross-validation and Bayesian estimation of tuning parameters</a></p>
<p>16 0.091976583 <a title="1674-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-15-Reputations_changeable%2C_situations_tolerable.html">1858 andrew gelman stats-2013-05-15-Reputations changeable, situations tolerable</a></p>
<p>17 0.089802489 <a title="1674-tfidf-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Useful_models%2C_model_checking%2C_and_external_validation%3A__a_mini-discussion.html">244 andrew gelman stats-2010-08-30-Useful models, model checking, and external validation:  a mini-discussion</a></p>
<p>18 0.089546487 <a title="1674-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-21-BDA3_table_of_contents_%28also_a_new_paper_on_visualization%29.html">1991 andrew gelman stats-2013-08-21-BDA3 table of contents (also a new paper on visualization)</a></p>
<p>19 0.087878004 <a title="1674-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-21-Readings_for_a_two-week_segment_on_Bayesian_modeling%3F.html">1586 andrew gelman stats-2012-11-21-Readings for a two-week segment on Bayesian modeling?</a></p>
<p>20 0.087430432 <a title="1674-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-17-Hierarchical-multilevel_modeling_with_%E2%80%9Cbig_data%E2%80%9D.html">1267 andrew gelman stats-2012-04-17-Hierarchical-multilevel modeling with “big data”</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.118), (1, 0.122), (2, 0.011), (3, 0.027), (4, -0.006), (5, -0.014), (6, 0.03), (7, -0.032), (8, -0.073), (9, 0.061), (10, 0.037), (11, 0.012), (12, -0.0), (13, 0.021), (14, -0.01), (15, -0.017), (16, 0.003), (17, 0.015), (18, -0.013), (19, 0.009), (20, -0.054), (21, -0.029), (22, -0.023), (23, 0.035), (24, -0.004), (25, -0.018), (26, 0.025), (27, -0.009), (28, 0.005), (29, 0.015), (30, -0.057), (31, -0.034), (32, 0.019), (33, -0.047), (34, -0.001), (35, -0.011), (36, -0.015), (37, -0.004), (38, 0.05), (39, 0.045), (40, -0.033), (41, 0.005), (42, 0.034), (43, 0.02), (44, -0.027), (45, -0.009), (46, -0.066), (47, 0.009), (48, 0.024), (49, -0.024)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96333104 <a title="1674-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-15-Prior_Selection_for_Vector_Autoregressions.html">1674 andrew gelman stats-2013-01-15-Prior Selection for Vector Autoregressions</a></p>
<p>Introduction: Brendan Nyhan sends along  this paper  by Domenico Giannone, Michele Lenza, and Giorgio Primiceri:
  
Vector autoregressions are flexible time series models that can capture complex dynamic interrelationships among macroeconomic variables. However, their dense parameterization leads to unstable inference and inaccurate out-of-sample forecasts, particularly for models with many variables. A solution to this problem is to use informative priors, in order to shrink the richly parameterized unrestricted model towards a parsimonious naive benchmark, and thus reduce estimation uncertainty. This paper studies the optimal choice of the informativeness of these priors, which we treat as additional parameters, in the spirit of hierarchical modeling. This approach is theoretically grounded, easy to implement, and greatly reduces the number and importance of subjective choices in the setting of the prior. Moreover, it performs very well both in terms of out-of-sample forecastingâ&euro;&rdquo;as well as factor</p><p>2 0.81167048 <a title="1674-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>Introduction: Ryan King writes:
  
I was wondering if you have a brief comment on the state of the art for objective priors for hierarchical generalized linear models (generalized linear mixed models).  I have been working off the papers in Bayesian Analysis (2006) 1, Number 3 (Browne and Draper, Kass and Natarajan, Gelman).  There seems to have been continuous work for matching priors in linear mixed models, but GLMMs less so because of the lack of an analytic marginal likelihood for the variance components.  There are a number of additional suggestions in the literature since 2006, but little robust practical guidance.


I’m interested in both mean parameters and the variance components.  I’m almost always concerned with logistic random effect models.  I’m fascinated by the matching-priors idea of higher-order asymptotic improvements to maximum likelihood, and need to make some kind of defensible default recommendation.  Given the massive scale of the datasets (genetics …), extensive sensitivity a</p><p>3 0.78195912 <a title="1674-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-11-Weakly_informative_priors_for_Bayesian_nonparametric_models%3F.html">1454 andrew gelman stats-2012-08-11-Weakly informative priors for Bayesian nonparametric models?</a></p>
<p>Introduction: Nathaniel Egwu writes:
  
I am a PhD student working on machine learning using artificial neural networks . . . Do you have some recent publications related to how one can construct priors depending on the type of input data available for training? I intend to construct a prior distribution for a given trade-off parameter of my non model obtained through training a neural network. At this stage, my argument is due to the fact that Bayesian nonparameteric estimation offers some insight on how to proceed on this problem.
  
As I’ve been writing here for awhile, I’ve been interested in weakly informative priors.  But I have little experience with nonparametric models.  Perhaps Aki Vehtari or David Dunson or some other expert on these models can discuss how to set them up with weakly informative priors?  This sounds like it could be important to me.</p><p>4 0.76138175 <a title="1674-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-13-On_the_half-Cauchy_prior_for_a_global_scale_parameter.html">801 andrew gelman stats-2011-07-13-On the half-Cauchy prior for a global scale parameter</a></p>
<p>Introduction: Nick Polson and James Scott  write :
  
We generalize the half-Cauchy prior for a global scale parameter to the wider class of hypergeometric inverted-beta priors. We derive expressions for posterior moments and marginal densities when these priors are used for a top-level normal variance in a Bayesian hierarchical model. Finally, we prove a result that characterizes the frequentist risk of the Bayes estimators under all priors in the class. These arguments provide an alternative, classical justification for the use of the half-Cauchy prior in Bayesian hierarchical models, complementing the arguments in Gelman (2006).
  
This makes me happy, of course.  It’s great to be validated.
 
The only think I didn’t catch is how they set the scale parameter for the half-Cauchy prior.  In my 2006 paper I frame it as a weakly informative prior and recommend that the scale be set based on actual prior knowledge.  But Polson and Scott are talking about a default choice.  I used to think that such a</p><p>5 0.76024103 <a title="1674-lsi-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-15-Weakly_informative_priors_and_imprecise_probabilities.html">468 andrew gelman stats-2010-12-15-Weakly informative priors and imprecise probabilities</a></p>
<p>Introduction: Giorgio Corani writes:
  
Your work on weakly informative priors is close to some research I [Corani] did (together with Prof. Zaffalon) in the last years using the so-called imprecise probabilities. The idea is to work with a set of priors (containing  even very different priors); to update them via Bayes’ rule and  then compute a set of posteriors.


The set of priors is convex and the priors are Dirichlet (thus, conjugate to the likelihood); this allows to compute the set of posteriors exactly and efficiently.


I [Corani] have used this approach for classification, extending  naive Bayes and TAN to imprecise probabilities. Classifiers based on imprecise probabilities return more classes when they find that the most probable class is prior-dependent, i.e., if picking different priors in the convex set leads to identify different classes as the most probable one. Instead of returning a single (unreliable) prior-dependent class, credal classifiers in this case preserve reliability by</p><p>6 0.74369818 <a title="1674-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-19-Prior_distributions_on_derived_quantities_rather_than_on_parameters_themselves.html">1946 andrew gelman stats-2013-07-19-Prior distributions on derived quantities rather than on parameters themselves</a></p>
<p>7 0.73710507 <a title="1674-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-15-Reputations_changeable%2C_situations_tolerable.html">1858 andrew gelman stats-2013-05-15-Reputations changeable, situations tolerable</a></p>
<p>8 0.73389113 <a title="1674-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-21-D._Buggin.html">1465 andrew gelman stats-2012-08-21-D. Buggin</a></p>
<p>9 0.73141932 <a title="1674-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-21-BDA3_table_of_contents_%28also_a_new_paper_on_visualization%29.html">1991 andrew gelman stats-2013-08-21-BDA3 table of contents (also a new paper on visualization)</a></p>
<p>10 0.72639912 <a title="1674-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-30-Infill_asymptotics_and_sprawl_asymptotics.html">1877 andrew gelman stats-2013-05-30-Infill asymptotics and sprawl asymptotics</a></p>
<p>11 0.71946645 <a title="1674-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-27-%E2%80%9CKeeping_things_unridiculous%E2%80%9D%3A__Berger%2C_O%E2%80%99Hagan%2C_and_me_on_weakly_informative_priors.html">1087 andrew gelman stats-2011-12-27-“Keeping things unridiculous”:  Berger, O’Hagan, and me on weakly informative priors</a></p>
<p>12 0.71179855 <a title="1674-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>13 0.71040618 <a title="1674-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-22-The_scaled_inverse_Wishart_prior_distribution_for_a_covariance_matrix_in_a_hierarchical_model.html">1466 andrew gelman stats-2012-08-22-The scaled inverse Wishart prior distribution for a covariance matrix in a hierarchical model</a></p>
<p>14 0.71015167 <a title="1674-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-29-More_by_Berger_and_me_on_weakly_informative_priors.html">1092 andrew gelman stats-2011-12-29-More by Berger and me on weakly informative priors</a></p>
<p>15 0.70783269 <a title="1674-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-07-Prior_distributions_for_regression_coefficients.html">1486 andrew gelman stats-2012-09-07-Prior distributions for regression coefficients</a></p>
<p>16 0.69773114 <a title="1674-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-11-Understanding_how_estimates_change_when_you_move_to_a_multilevel_model.html">850 andrew gelman stats-2011-08-11-Understanding how estimates change when you move to a multilevel model</a></p>
<p>17 0.68930095 <a title="1674-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-18-Multimodality_in_hierarchical_models.html">916 andrew gelman stats-2011-09-18-Multimodality in hierarchical models</a></p>
<p>18 0.68917936 <a title="1674-lsi-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-04-That_half-Cauchy_prior.html">184 andrew gelman stats-2010-08-04-That half-Cauchy prior</a></p>
<p>19 0.67884022 <a title="1674-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-10-Cross-validation_and_Bayesian_estimation_of_tuning_parameters.html">2129 andrew gelman stats-2013-12-10-Cross-validation and Bayesian estimation of tuning parameters</a></p>
<p>20 0.67721254 <a title="1674-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-11-%E2%80%9CInformative_g-Priors_for_Logistic_Regression%E2%80%9D.html">2017 andrew gelman stats-2013-09-11-“Informative g-Priors for Logistic Regression”</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.024), (10, 0.013), (15, 0.013), (16, 0.075), (23, 0.018), (24, 0.174), (31, 0.014), (42, 0.014), (45, 0.012), (50, 0.011), (65, 0.016), (68, 0.228), (77, 0.022), (81, 0.016), (84, 0.01), (86, 0.049), (99, 0.203)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93420231 <a title="1674-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-15-Prior_Selection_for_Vector_Autoregressions.html">1674 andrew gelman stats-2013-01-15-Prior Selection for Vector Autoregressions</a></p>
<p>Introduction: Brendan Nyhan sends along  this paper  by Domenico Giannone, Michele Lenza, and Giorgio Primiceri:
  
Vector autoregressions are flexible time series models that can capture complex dynamic interrelationships among macroeconomic variables. However, their dense parameterization leads to unstable inference and inaccurate out-of-sample forecasts, particularly for models with many variables. A solution to this problem is to use informative priors, in order to shrink the richly parameterized unrestricted model towards a parsimonious naive benchmark, and thus reduce estimation uncertainty. This paper studies the optimal choice of the informativeness of these priors, which we treat as additional parameters, in the spirit of hierarchical modeling. This approach is theoretically grounded, easy to implement, and greatly reduces the number and importance of subjective choices in the setting of the prior. Moreover, it performs very well both in terms of out-of-sample forecastingâ&euro;&rdquo;as well as factor</p><p>2 0.92984378 <a title="1674-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-01-Is_that_what_she_said%3F.html">689 andrew gelman stats-2011-05-01-Is that what she said?</a></p>
<p>Introduction: Eric Booth cozies up to  this article  by Chloe Kiddon and Yuriy Brun (software  here ).  I think they make their point in a gentle yet forceful manner.</p><p>3 0.86503404 <a title="1674-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-24-%E2%80%9CIncome_can%E2%80%99t_be_used_to_predict_political_opinion%E2%80%9D.html">924 andrew gelman stats-2011-09-24-“Income can’t be used to predict political opinion”</a></p>
<p>Introduction: What really irritates me about  this column  (by John Steele Gordon) is not how stupid it is (an article about “millionaires” that switches  within the very same paragraph  between “a nest egg of $1 million” and “a $1 million annual income” without acknowledging the difference between these concepts) or the ignorance it displays (no, it’s not true that “McCain carried the middle class” in 2008—unless by “middle class” you mean “middle class whites”).
 
No, what really ticks me off is that, when the Red State Blue State book was coming out, we pitched a “5 myths” article for the Washington Post, and they turned us down!  Perhaps the rule is:  if it’s in the Opinions section of the paper, it can’t contain any facts?  Or, to be more precise, any facts it contains must be counterbalanced by an equal number of inanities?
 
Grrrrr . . . I haven’t been so annoyed since reading that  New York Times article  that argued that electoral politics is just like high school.  Who needs political scie</p><p>4 0.8515498 <a title="1674-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-16-Groundhog_day_in_August%3F.html">913 andrew gelman stats-2011-09-16-Groundhog day in August?</a></p>
<p>Introduction: A colleague writes:
  
Due to my similar interest in  plagiarism , I went to The Human Cultural and Social Landscape session. [The recipient of the American Statistical Association's Founders Award in 2002] gave the first talk in the session instead of Yasmin Said, which was modestly attended (20 or so people) and gave a sociology talk with no numbers — and no attribution to where these ideas (on Afghanistan culture) came from. Would it really have hurt to give the source of this? I’m on board with plain laziness for this one.


I think he may have mentioned a number of his collaborators at the beginning, and all he talked about were cultural customs and backgrounds, no science to speak of.
  
It’s kind of amazing to me that he actually showed up at JSM, but of course if he had any shame, he wouldn’t have repeatedly  stolen  copied without proper attribution in the first place.  It’s not even like Doris Kearns Goodwin who reportedly produced a well-written book out of it!</p><p>5 0.83260554 <a title="1674-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-07-That_last_satisfaction_at_the_end_of_the_career.html">1568 andrew gelman stats-2012-11-07-That last satisfaction at the end of the career</a></p>
<p>Introduction: I just finished reading an amusing but somewhat disturbing  article  by Mark Singer, a reporter for the New Yorker who follows in that magazine’s tradition of writing about amiable frauds.  (For those who are keeping score at home, Singer employs a McKelway-style relaxed tolerance rather than Liebling-style pyrotechnics.)  Singer’s topic was a midwestern dentist named Kip Litton who fradulently invented a side career for himself as a sub-3-hour marathoner.  What was amazing was not so much that Litton lied about his accomplishments but, rather, the huge efforts that he undertook to support these lies.  He went to faraway cities to not run marathons.  He fabricated multiple personas on running message boards.  He even invented an entire marathon and made up a list of participants.
 
This got me thinking about Ed Wegman (sorry!), the statistician who got tangled in a  series of plagiarism scandals .  As with Litton, once Wegman was caught once, energetic people looked at the records and</p><p>6 0.83063042 <a title="1674-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-29-Applying_quantum_probability_to_political_science.html">877 andrew gelman stats-2011-08-29-Applying quantum probability to political science</a></p>
<p>7 0.82389855 <a title="1674-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-21-A_possible_resolution_of_the_albedo_mystery%21.html">622 andrew gelman stats-2011-03-21-A possible resolution of the albedo mystery!</a></p>
<p>8 0.81849468 <a title="1674-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-28-Better_than_Dennis_the_dentist_or_Laura_the_lawyer.html">875 andrew gelman stats-2011-08-28-Better than Dennis the dentist or Laura the lawyer</a></p>
<p>9 0.80382174 <a title="1674-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-14-The_General_Social_Survey_is_a_great_resource.html">958 andrew gelman stats-2011-10-14-The General Social Survey is a great resource</a></p>
<p>10 0.79487425 <a title="1674-lda-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-06-Bootstrap_averaging%3A_Examples_where_it_works_and_where_it_doesn%E2%80%99t_work.html">2201 andrew gelman stats-2014-02-06-Bootstrap averaging: Examples where it works and where it doesn’t work</a></p>
<p>11 0.78522515 <a title="1674-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>12 0.7850771 <a title="1674-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-26-Modeling_probability_data.html">1284 andrew gelman stats-2012-04-26-Modeling probability data</a></p>
<p>13 0.78338718 <a title="1674-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-23-Of_home_runs_and_grand_slams.html">47 andrew gelman stats-2010-05-23-Of home runs and grand slams</a></p>
<p>14 0.77738512 <a title="1674-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-08-A_Bayesian_approach_for_peer-review_panels%3F__and_a_speculation_about_Bruno_Frey.html">2055 andrew gelman stats-2013-10-08-A Bayesian approach for peer-review panels?  and a speculation about Bruno Frey</a></p>
<p>15 0.77153897 <a title="1674-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-03-Boot.html">1881 andrew gelman stats-2013-06-03-Boot</a></p>
<p>16 0.76313043 <a title="1674-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-05-Question_26_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1367 andrew gelman stats-2012-06-05-Question 26 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>17 0.76127124 <a title="1674-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-30-%3F%3F%3F.html">2118 andrew gelman stats-2013-11-30-???</a></p>
<p>18 0.75994611 <a title="1674-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-22-Procrastination_as_a_positive_productivity_strategy.html">1225 andrew gelman stats-2012-03-22-Procrastination as a positive productivity strategy</a></p>
<p>19 0.75937384 <a title="1674-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-10-95%25_intervals_that_I_don%E2%80%99t_believe%2C_because_they%E2%80%99re_from_a_flat_prior_I_don%E2%80%99t_believe.html">1206 andrew gelman stats-2012-03-10-95% intervals that I don’t believe, because they’re from a flat prior I don’t believe</a></p>
<p>20 0.75915921 <a title="1674-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
