<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1870 andrew gelman stats-2013-05-26-How to understand coefficients that reverse sign when you start controlling for things?</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-1870" href="#">andrew_gelman_stats-2013-1870</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1870 andrew gelman stats-2013-05-26-How to understand coefficients that reverse sign when you start controlling for things?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-1870-html" href="http://andrewgelman.com/2013/05/26/how-to-understand-coefficients-that-reverse-sign-when-you-start-controlling-for-things/">html</a></p><p>Introduction: Denis Cote writes:
  
Just read  this  today and my unsophisticated statistical mind is confused.


“Initial bivariate analyses suggest that union membership is actually associated with worse health. This association disappears when controlling for demographics, then reverses and becomes significant when controlling for labor market characteristics.”


From my education about statistics, I remember to be suspicious about multiple regression coefficients that are in the opposite direction of the bivariate coefficients. What I am missing? I vaguely remember something about the suppression effect.
  
My reply:
 
There’s a long literature on this from many decades ago.  My general feeling about such situations is that, when the coefficient changes a lot after controlling for other variables, it is important to visualize this change, to understand what is the interaction among variables that is associated with the change in the coefficients.  This is what we did in our Red State Blue State</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Denis Cote writes:    Just read  this  today and my unsophisticated statistical mind is confused. [sent-1, score-0.176]
</p><p>2 “Initial bivariate analyses suggest that union membership is actually associated with worse health. [sent-2, score-0.869]
</p><p>3 This association disappears when controlling for demographics, then reverses and becomes significant when controlling for labor market characteristics. [sent-3, score-1.37]
</p><p>4 ”   From my education about statistics, I remember to be suspicious about multiple regression coefficients that are in the opposite direction of the bivariate coefficients. [sent-4, score-1.03]
</p><p>5 My reply:   There’s a long literature on this from many decades ago. [sent-7, score-0.085]
</p><p>6 My general feeling about such situations is that, when the coefficient changes a lot after controlling for other variables, it is important to visualize this change, to understand what is the interaction among variables that is associated with the change in the coefficients. [sent-8, score-1.55]
</p><p>7 This is what we did in our Red State Blue State  paper , for example, and we also developed some such tools in our  paper  on police stop-and-frisk. [sent-9, score-0.306]
</p><p>8 I have not read the particular article you cite, so I can’t comment on that particular application, but in general I think these multiple regression analyses can be fine but I like to understand where in he data the switching of sign is coming from. [sent-10, score-1.194]
</p><p>9 I think there’s still a lot of useful work to be done on graphical methods for understanding the effects of conditioning on regression models. [sent-11, score-0.393]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('controlling', 0.319), ('bivariate', 0.27), ('reverses', 0.181), ('cote', 0.181), ('denis', 0.181), ('regression', 0.174), ('suppression', 0.171), ('associated', 0.161), ('disappears', 0.158), ('analyses', 0.157), ('remember', 0.149), ('switching', 0.146), ('membership', 0.146), ('visualize', 0.143), ('multiple', 0.138), ('union', 0.135), ('variables', 0.132), ('vaguely', 0.128), ('demographics', 0.126), ('change', 0.126), ('conditioning', 0.123), ('state', 0.121), ('police', 0.12), ('suspicious', 0.117), ('cite', 0.113), ('labor', 0.109), ('understand', 0.107), ('situations', 0.103), ('initial', 0.103), ('interaction', 0.101), ('becomes', 0.099), ('market', 0.097), ('coefficient', 0.097), ('developed', 0.096), ('graphical', 0.096), ('sign', 0.095), ('particular', 0.095), ('read', 0.094), ('general', 0.093), ('coefficients', 0.092), ('opposite', 0.09), ('tools', 0.09), ('blue', 0.088), ('feeling', 0.088), ('association', 0.088), ('application', 0.088), ('decades', 0.085), ('mind', 0.082), ('red', 0.081), ('changes', 0.08)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="1870-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-26-How_to_understand_coefficients_that_reverse_sign_when_you_start_controlling_for_things%3F.html">1870 andrew gelman stats-2013-05-26-How to understand coefficients that reverse sign when you start controlling for things?</a></p>
<p>Introduction: Denis Cote writes:
  
Just read  this  today and my unsophisticated statistical mind is confused.


“Initial bivariate analyses suggest that union membership is actually associated with worse health. This association disappears when controlling for demographics, then reverses and becomes significant when controlling for labor market characteristics.”


From my education about statistics, I remember to be suspicious about multiple regression coefficients that are in the opposite direction of the bivariate coefficients. What I am missing? I vaguely remember something about the suppression effect.
  
My reply:
 
There’s a long literature on this from many decades ago.  My general feeling about such situations is that, when the coefficient changes a lot after controlling for other variables, it is important to visualize this change, to understand what is the interaction among variables that is associated with the change in the coefficients.  This is what we did in our Red State Blue State</p><p>2 0.13165523 <a title="1870-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-05-Understanding_regression_models_and_regression_coefficients.html">1656 andrew gelman stats-2013-01-05-Understanding regression models and regression coefficients</a></p>
<p>Introduction: David Hoaglin writes:
  
After seeing it cited, I just read  your paper  in Technometrics.  The home radon levels provide an interesting and instructive example.


I [Hoaglin] have a different take on the difficulty of interpreting the estimated coefficient of the county-level basement proportion (gamma-sub-2) on page 434.  An important part of the difficulty involves “other things being equal.”  That sounds like the widespread interpretation of a regression coefficient as telling how the dependent variable responds to change in that predictor when the other predictors are held constant.  Unfortunately, as a general interpretation, that language is oversimplified; it doesn’t reflect how regression actually works.  The appropriate general interpretation is that the coefficient tells how the dependent variable responds to change in that predictor after allowing for simultaneous change in the other predictors in the data at hand.  Thus, in the county-level regression gamma-sub-2 summarize</p><p>3 0.11568899 <a title="1870-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-20-Correcting_for_multiple_comparisons_in_a_Bayesian_regression_model.html">1989 andrew gelman stats-2013-08-20-Correcting for multiple comparisons in a Bayesian regression model</a></p>
<p>Introduction: Joe Northrup writes:
  
I have a question about correcting for multiple comparisons in a Bayesian regression model. I believe I understand the argument in  your 2012 paper  in Journal of Research on Educational Effectiveness that when you have a hierarchical model there is shrinkage of estimates towards the group-level mean and thus there is no need to add any additional penalty to correct for multiple comparisons. In my case I do not have hierarchically structured dataâ&euro;&rdquo;i.e. I have only 1 observation per group but have a categorical variable with a large number of categories. Thus, I am fitting a simple multiple regression in a Bayesian framework. Would putting a strong, mean 0, multivariate normal prior on the betas in this model accomplish the same sort of shrinkage (it seems to me that it would) and do you believe this is a valid way to address criticism of multiple comparisons in this setting?
  
My reply:  Yes, I think this makes sense.  One way to address concerns of multiple com</p><p>4 0.11166951 <a title="1870-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>Introduction: A research psychologist writes in with a question that’s so long that I’ll put my answer first, then put the question itself below the fold.
 
Here’s my reply:
 
As I wrote in my Anova paper and in my book with Jennifer Hill, I do think that multilevel models can completely replace Anova.  At the same time, I think the central idea of Anova should persist in our understanding of these models.  To me the central idea of Anova is not F-tests or p-values or sums of squares, but rather the idea of predicting an outcome based on factors with discrete levels, and understanding these factors using variance components.
 
The continuous or categorical response thing doesn’t really matter so much to me.  I have no problem using a normal linear model for continuous outcomes (perhaps suitably transformed) and a logistic model for binary outcomes.
 
I don’t want to throw away interactions just because they’re not statistically significant.  I’d rather partially pool them toward zero using an inform</p><p>5 0.11134394 <a title="1870-tfidf-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-02-Discovering_general_multidimensional_associations.html">2315 andrew gelman stats-2014-05-02-Discovering general multidimensional associations</a></p>
<p>Introduction: Continuing our discussion of general measures of correlations, Ben Murrell sends along  this paper  (with  corresponding  R package), which begins:
  
When two variables are related by a known function, the coefficient of determination (denoted R-squared) measures the proportion of the total variance in the observations that is explained by that function. This quantifies the strength of the relationship between variables by describing what proportion of the variance is signal as opposed to noise. For linear relationships, this is equal to the square of the correlation coefficient, ρ. When the parametric form of the relationship is unknown, however, it is unclear how to estimate the proportion of explained variance equitably – assigning similar values to equally noisy relationships. Here we demonstrate how to directly estimate a generalized R-squared when the form of the relationship is unknown, and we question the performance of the Maximal Information Coefficient (MIC) – a recently pr</p><p>6 0.10968781 <a title="1870-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>7 0.10496332 <a title="1870-tfidf-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-09-Statistics_gifts%3F.html">460 andrew gelman stats-2010-12-09-Statistics gifts?</a></p>
<p>8 0.10389376 <a title="1870-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-17-I_got_99_comparisons_but_multiplicity_ain%E2%80%99t_one.html">1016 andrew gelman stats-2011-11-17-I got 99 comparisons but multiplicity ain’t one</a></p>
<p>9 0.10165861 <a title="1870-tfidf-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-23-Popular_governor%2C_small_state.html">159 andrew gelman stats-2010-07-23-Popular governor, small state</a></p>
<p>10 0.1003704 <a title="1870-tfidf-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-05-What_do_practitioners_need_to_know_about_regression%3F.html">451 andrew gelman stats-2010-12-05-What do practitioners need to know about regression?</a></p>
<p>11 0.10010361 <a title="1870-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-16-Bayesian_analogue_to_stepwise_regression%3F.html">1535 andrew gelman stats-2012-10-16-Bayesian analogue to stepwise regression?</a></p>
<p>12 0.099212497 <a title="1870-tfidf-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-28-Matching_for_preprocessing_data_for_causal_inference.html">375 andrew gelman stats-2010-10-28-Matching for preprocessing data for causal inference</a></p>
<p>13 0.095854342 <a title="1870-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-12-Single_or_multiple_imputation%3F.html">608 andrew gelman stats-2011-03-12-Single or multiple imputation?</a></p>
<p>14 0.091980129 <a title="1870-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-04-What_are_the_key_assumptions_of_linear_regression%3F.html">1967 andrew gelman stats-2013-08-04-What are the key assumptions of linear regression?</a></p>
<p>15 0.087789312 <a title="1870-tfidf-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Question_about_standard_range_for_social_science_correlations.html">257 andrew gelman stats-2010-09-04-Question about standard range for social science correlations</a></p>
<p>16 0.087703668 <a title="1870-tfidf-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-14-The_statistics_and_the_science.html">146 andrew gelman stats-2010-07-14-The statistics and the science</a></p>
<p>17 0.087382205 <a title="1870-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-21-The_future_%28and_past%29_of_statistical_sciences.html">2072 andrew gelman stats-2013-10-21-The future (and past) of statistical sciences</a></p>
<p>18 0.084844328 <a title="1870-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-10-Matching_and_regression%3A__two_great_tastes_etc_etc.html">796 andrew gelman stats-2011-07-10-Matching and regression:  two great tastes etc etc</a></p>
<p>19 0.083982736 <a title="1870-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-07-Prior_distributions_for_regression_coefficients.html">1486 andrew gelman stats-2012-09-07-Prior distributions for regression coefficients</a></p>
<p>20 0.083715677 <a title="1870-tfidf-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-08-Understanding_Simpson%E2%80%99s_paradox_using_a_graph.html">2286 andrew gelman stats-2014-04-08-Understanding Simpson’s paradox using a graph</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.165), (1, 0.032), (2, 0.044), (3, -0.035), (4, 0.037), (5, -0.013), (6, -0.047), (7, -0.022), (8, 0.044), (9, 0.087), (10, 0.027), (11, 0.016), (12, 0.023), (13, 0.005), (14, 0.092), (15, 0.051), (16, -0.017), (17, 0.005), (18, -0.008), (19, -0.021), (20, 0.02), (21, 0.042), (22, 0.004), (23, -0.025), (24, 0.021), (25, 0.013), (26, 0.028), (27, -0.051), (28, -0.003), (29, -0.012), (30, 0.087), (31, 0.042), (32, 0.054), (33, 0.03), (34, -0.004), (35, -0.023), (36, 0.058), (37, 0.049), (38, -0.03), (39, -0.013), (40, -0.015), (41, 0.009), (42, 0.001), (43, -0.012), (44, 0.039), (45, -0.023), (46, -0.05), (47, -0.014), (48, -0.009), (49, -0.047)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97941202 <a title="1870-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-26-How_to_understand_coefficients_that_reverse_sign_when_you_start_controlling_for_things%3F.html">1870 andrew gelman stats-2013-05-26-How to understand coefficients that reverse sign when you start controlling for things?</a></p>
<p>Introduction: Denis Cote writes:
  
Just read  this  today and my unsophisticated statistical mind is confused.


“Initial bivariate analyses suggest that union membership is actually associated with worse health. This association disappears when controlling for demographics, then reverses and becomes significant when controlling for labor market characteristics.”


From my education about statistics, I remember to be suspicious about multiple regression coefficients that are in the opposite direction of the bivariate coefficients. What I am missing? I vaguely remember something about the suppression effect.
  
My reply:
 
There’s a long literature on this from many decades ago.  My general feeling about such situations is that, when the coefficient changes a lot after controlling for other variables, it is important to visualize this change, to understand what is the interaction among variables that is associated with the change in the coefficients.  This is what we did in our Red State Blue State</p><p>2 0.79501784 <a title="1870-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-05-Understanding_regression_models_and_regression_coefficients.html">1656 andrew gelman stats-2013-01-05-Understanding regression models and regression coefficients</a></p>
<p>Introduction: David Hoaglin writes:
  
After seeing it cited, I just read  your paper  in Technometrics.  The home radon levels provide an interesting and instructive example.


I [Hoaglin] have a different take on the difficulty of interpreting the estimated coefficient of the county-level basement proportion (gamma-sub-2) on page 434.  An important part of the difficulty involves “other things being equal.”  That sounds like the widespread interpretation of a regression coefficient as telling how the dependent variable responds to change in that predictor when the other predictors are held constant.  Unfortunately, as a general interpretation, that language is oversimplified; it doesn’t reflect how regression actually works.  The appropriate general interpretation is that the coefficient tells how the dependent variable responds to change in that predictor after allowing for simultaneous change in the other predictors in the data at hand.  Thus, in the county-level regression gamma-sub-2 summarize</p><p>3 0.78244847 <a title="1870-lsi-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-02-Why_we_hate_stepwise_regression.html">2357 andrew gelman stats-2014-06-02-Why we hate stepwise regression</a></p>
<p>Introduction: Haynes Goddard writes:
  
I have been slowly working my way through the grad program in stats here, and the latest course was a biostats course on categorical and survival analysis.  I noticed in the semi-parametric  and parametric material (Wang and Lee is the text) that they use stepwise regression a lot.


I learned in econometrics that stepwise is poor practice, as it defaults to the “theory of the regression line”, that is no theory at all, just the variation in the data.


I don’t find the topic on your blog, and wonder if you have addressed the issue.
  
My reply:
 
Stepwise regression is one of these things, like outlier detection and pie charts, which appear to be popular among non-statisticans but are considered by statisticians to be a bit of a joke.  For example, Jennifer and I don’t mention stepwise regression in our book, not even once.
 
To address the issue more directly:  the motivation behind stepwise regression is that you have a lot of potential predictors but not e</p><p>4 0.78120708 <a title="1870-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-09-The_effects_of_fiscal_consolidation.html">1663 andrew gelman stats-2013-01-09-The effects of fiscal consolidation</a></p>
<p>Introduction: José Iparraguirre writes:
  
I’ve read  a recent paper  by the International Monetary Fund on the effects of fiscal consolidation measures on income inequality (Fiscal Monitor October 2012, Appendix 1).


They run a panel regression with 48 countries and 30 years (annual data) of a measure of income inequality (Gini coefficient) on a number of covariates, including a measure of fiscal consolidation.


Footnote 39 (page 51) informs that they’ve employed seemingly unrelated regression and panel-corrected standard errors, and that to double-check they’ve also run ordinary least squares and fixed-effects panel regressions—all with similar results.


So far, so good. However, the footnote goes on to explain that “Some of the results (e.g. the causal relationship between consolidation and inequality) may be subject to endogeneity and should be interpreted with caution”. (Italics are mine).


Therefore, it seems that the crux of the exercise—i.e. estimating the relationship between fiscal con</p><p>5 0.77936637 <a title="1870-lsi-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-05-What_do_practitioners_need_to_know_about_regression%3F.html">451 andrew gelman stats-2010-12-05-What do practitioners need to know about regression?</a></p>
<p>Introduction: Fabio Rojas writes:
  
 
In much of the social sciences outside economics, it’s very common for people to take a regression course or two in graduate school and then stop their statistical education. This creates a situation where you have a large pool of people who have some knowledge, but not a lot of knowledge. As a result, you have a pretty big gap between people like yourself, who are heavily invested in the cutting edge of applied statistics, and other folks.


So here is the question: What are the major lessons about good statistical practice that “rank and file” social scientists should know? Sure, most people can recite “Correlation is not causation” or “statistical significance is not substantive significance.” But what are the other big lessons?


This question comes from my own experience. I have a math degree and took regression analysis in graduate school, but I definitely do not have the level of knowledge of a statistician. I also do mixed method research, and field wor</p><p>6 0.76775974 <a title="1870-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-31-Using_factor_analysis_or_principal_components_analysis_or_measurement-error_models_for_biological_measurements_in_archaeology%3F.html">1094 andrew gelman stats-2011-12-31-Using factor analysis or principal components analysis or measurement-error models for biological measurements in archaeology?</a></p>
<p>7 0.75807863 <a title="1870-lsi-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Question_about_standard_range_for_social_science_correlations.html">257 andrew gelman stats-2010-09-04-Question about standard range for social science correlations</a></p>
<p>8 0.75745165 <a title="1870-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-20-Displaying_inferences_from_complex_models.html">1815 andrew gelman stats-2013-04-20-Displaying inferences from complex models</a></p>
<p>9 0.74080408 <a title="1870-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-10-Matching_and_regression%3A__two_great_tastes_etc_etc.html">796 andrew gelman stats-2011-07-10-Matching and regression:  two great tastes etc etc</a></p>
<p>10 0.73062241 <a title="1870-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-16-Bayesian_analogue_to_stepwise_regression%3F.html">1535 andrew gelman stats-2012-10-16-Bayesian analogue to stepwise regression?</a></p>
<p>11 0.72621012 <a title="1870-lsi-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-23-Popular_governor%2C_small_state.html">159 andrew gelman stats-2010-07-23-Popular governor, small state</a></p>
<p>12 0.72030646 <a title="1870-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-15-Still_more_Mr._P_in_public_health.html">770 andrew gelman stats-2011-06-15-Still more Mr. P in public health</a></p>
<p>13 0.71757782 <a title="1870-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-04-What_are_the_key_assumptions_of_linear_regression%3F.html">1967 andrew gelman stats-2013-08-04-What are the key assumptions of linear regression?</a></p>
<p>14 0.70266932 <a title="1870-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-02-Interaction-based_feature_selection_and_classification_for_high-dimensional_biological_data.html">1703 andrew gelman stats-2013-02-02-Interaction-based feature selection and classification for high-dimensional biological data</a></p>
<p>15 0.69716889 <a title="1870-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Interpreting_interactions_in_discrete-data_regression.html">1908 andrew gelman stats-2013-06-21-Interpreting interactions in discrete-data regression</a></p>
<p>16 0.69166088 <a title="1870-lsi-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-13-Hey%21__Here%E2%80%99s_a_referee_report_for_you%21.html">144 andrew gelman stats-2010-07-13-Hey!  Here’s a referee report for you!</a></p>
<p>17 0.68624502 <a title="1870-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-19-Cross-validation_to_check_missing-data_imputation.html">1330 andrew gelman stats-2012-05-19-Cross-validation to check missing-data imputation</a></p>
<p>18 0.68531871 <a title="1870-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-15-R-squared_for_multilevel_models.html">1121 andrew gelman stats-2012-01-15-R-squared for multilevel models</a></p>
<p>19 0.68335521 <a title="1870-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-I_doubt_they_cheated.html">1971 andrew gelman stats-2013-08-07-I doubt they cheated</a></p>
<p>20 0.66517335 <a title="1870-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-11-Folic_acid_and_autism.html">1893 andrew gelman stats-2013-06-11-Folic acid and autism</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.016), (16, 0.127), (21, 0.021), (24, 0.112), (57, 0.223), (69, 0.015), (83, 0.014), (89, 0.015), (99, 0.359)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96737456 <a title="1870-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-06-One_reason_New_York_isn%E2%80%99t_as_rich_as_it_used_to_be%3A__Redistribution_of_federal_tax_money_to_other_states.html">1485 andrew gelman stats-2012-09-06-One reason New York isn’t as rich as it used to be:  Redistribution of federal tax money to other states</a></p>
<p>Introduction: Uberbloggers  Andrew Sullivan  and  Matthew Yglesias  were kind enough to link to my five-year-old post with graphs from Red State Blue State on time trends of average income by state.
 
Here are the  graphs : 
   
 
 
Yglesias’s take-home point:
  
There isn’t that much change over time in states’ economic well-being. All things considered the best predictor of how rich a state was in 2000 was simply how rich it was in 1929…. Massachusetts and Connecticut have always been rich and Arkansas and Mississippi have always been poor.
  
I’d like to point to a different feature of the graphs, which is that, although the  rankings  of the states haven’t changed much (as can be seen from the “2000 compared to 1929″ scale), the relative  values  of the incomes have converged quite a bit—at least, they converged from about 1930 to 1980 before hitting some level of stability.  And the rankings have changed a bit.  My impression (without checking the numbers) is that New York and Connecticut were</p><p>2 0.95719832 <a title="1870-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-20-A_statistical_model_for_underdispersion.html">1542 andrew gelman stats-2012-10-20-A statistical model for underdispersion</a></p>
<p>Introduction: We have lots of models for overdispersed count data but we rarely see underdispersed data.  But now I know what example I’ll be giving when this next comes up in class.  From a  book review  by Theo Tait:
  
A number of shark species go in for oophagy, or uterine cannibalism. Sand tiger foetuses ‘eat each other in utero, acting out the harshest form of sibling rivalry imaginable’. Only two babies emerge, one from each of the mother shark’s uteruses: the survivors have eaten everything else. ‘A female sand tiger gives birth to a baby that’s already a metre long and an experienced killer,’ explains Demian Chapman, an expert on the subject.
  
That’s what I call underdispersion.  E(y)=2, var(y)=0.  Take that, M. Poisson!</p><p>3 0.95425284 <a title="1870-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-05-What_are_the_standards_for_reliability_in_experimental_psychology%3F.html">1101 andrew gelman stats-2012-01-05-What are the standards for reliability in experimental psychology?</a></p>
<p>Introduction: An experimental psychologist was wondering about the standards in that field for “acceptable reliability” (when looking at inter-rater reliability in coding data).  He wondered, for example, if some variation on signal detectability theory might be applied to adjust for inter-rater differences in criteria for saying some code is present.
 
What about Cohen’s kappa?  The psychologist wrote:
  
Cohen’s kappa does adjust for “guessing,” but its assumptions are not well motivated, perhaps not any more than adjustments for guessing versus the application of signal detectability theory where that can be applied. But one can’t do a straightforward application of signal detectability theory for reliability in that you don’t know whether the signal is present or not.
  
I think measurement issues are important but I don’t have enough experience in this area to answer the question without knowing more about the problem that this researcher is working on.
 
I’m posting it here because I imagine t</p><p>4 0.95319188 <a title="1870-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-06-Krugman_disses_Hayek_as_%E2%80%9Cbeing_almost_entirely_about_politics_rather_than_economics%E2%80%9D.html">1043 andrew gelman stats-2011-12-06-Krugman disses Hayek as “being almost entirely about politics rather than economics”</a></p>
<p>Introduction: That’s ok , Krugman earlier  slammed  Galbraith.  (I wonder if Krugman is as big a fan of “tough choices” now as he was  in 1996 .)  Given Krugman’s politicization in recent years, I’m surprised he’s so dismissive of the political (rather than technical-economic) nature of Hayek’s influence.  (I don’t know if he’s changed his views on Galbraith in recent years.)
 
P.S.  Greg Mankiw, in contrast,  labels  Galbraith and Hayek as “two of the great economists of the 20th century” and writes, “even though their most famous works were written many decades ago, they are still well worth reading today.”</p><p>5 0.94612348 <a title="1870-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-19-Will_Stan_work_well_with_40%C3%9740_matrices%3F.html">861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</a></p>
<p>Introduction: Tomas Iesmantas writes:
  
I’m dealing with high dimensional (40-50 parameters) hierarchical bayesian model applied to nonlinear Poisson regression problem.


Now I’m using an adaptive version for the Metropolis adjusted Langevin algorithm with a truncated drift (Yves F. Atchade, 2003) to obtain samples from posterior.


But this algorithm is not very efficient in my case, it needs several millions iterations as burn-in period. And simulation takes quite a long time, since algorithm has to work with 40×40 matrices.


Maybe you know another MCMC algorithm which could take not so many burn-in samples and would be able to deal with nonlinear regression? In non-hierarchical nonlinear regression model adaptive metropolis algorithm is enough, but in hierarchical case I could use something more effective.
  
My reply:
 
Try fitting the model in Stan.  If that doesn’t work, let me know.</p><p>same-blog 6 0.94271129 <a title="1870-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-26-How_to_understand_coefficients_that_reverse_sign_when_you_start_controlling_for_things%3F.html">1870 andrew gelman stats-2013-05-26-How to understand coefficients that reverse sign when you start controlling for things?</a></p>
<p>7 0.94268829 <a title="1870-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-19-Tempering_and_modes.html">1018 andrew gelman stats-2011-11-19-Tempering and modes</a></p>
<p>8 0.9380821 <a title="1870-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-15-Fun_fight_over_the_Grover_search_algorithm.html">1120 andrew gelman stats-2012-01-15-Fun fight over the Grover search algorithm</a></p>
<p>9 0.93377656 <a title="1870-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-06-The_K_Foundation_burns_Cosma%E2%80%99s_turkey.html">1044 andrew gelman stats-2011-12-06-The K Foundation burns Cosma’s turkey</a></p>
<p>10 0.93132722 <a title="1870-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-03-This_post_does_not_mention_Wegman.html">989 andrew gelman stats-2011-11-03-This post does not mention Wegman</a></p>
<p>11 0.92743248 <a title="1870-lda-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-29-Statistics_and_the_end_of_time.html">306 andrew gelman stats-2010-09-29-Statistics and the end of time</a></p>
<p>12 0.92652279 <a title="1870-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-16-%E2%80%9CReal_data_can_be_a_pain%E2%80%9D.html">1460 andrew gelman stats-2012-08-16-“Real data can be a pain”</a></p>
<p>13 0.91853392 <a title="1870-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-22-%E2%80%9CInformation_visualization%E2%80%9D_vs._%E2%80%9CStatistical_graphics%E2%80%9D.html">816 andrew gelman stats-2011-07-22-“Information visualization” vs. “Statistical graphics”</a></p>
<p>14 0.91316265 <a title="1870-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-30-Stan_uses_Nuts%21.html">1036 andrew gelman stats-2011-11-30-Stan uses Nuts!</a></p>
<p>15 0.91006911 <a title="1870-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-18-DataMarket.html">215 andrew gelman stats-2010-08-18-DataMarket</a></p>
<p>16 0.90985024 <a title="1870-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-29-Another_one_of_those_%E2%80%9CPsychological_Science%E2%80%9D_papers_%28this_time_on_biceps_size_and_political_attitudes_among_college_students%29.html">1876 andrew gelman stats-2013-05-29-Another one of those “Psychological Science” papers (this time on biceps size and political attitudes among college students)</a></p>
<p>17 0.90733337 <a title="1870-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-18-Question_8_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1328 andrew gelman stats-2012-05-18-Question 8 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>18 0.89075965 <a title="1870-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-17-Question_7_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1326 andrew gelman stats-2012-05-17-Question 7 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>19 0.89048433 <a title="1870-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-09-Blogging%2C_polemical_and_otherwise.html">1108 andrew gelman stats-2012-01-09-Blogging, polemical and otherwise</a></p>
<p>20 0.88850659 <a title="1870-lda-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-04-Stan_%28%26_JAGS%29_Tutorial_on_Linear_Mixed_Models.html">2318 andrew gelman stats-2014-05-04-Stan (& JAGS) Tutorial on Linear Mixed Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
