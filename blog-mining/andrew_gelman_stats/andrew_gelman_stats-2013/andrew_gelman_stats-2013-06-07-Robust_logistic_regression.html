<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1886 andrew gelman stats-2013-06-07-Robust logistic regression</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-1886" href="#">andrew_gelman_stats-2013-1886</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1886 andrew gelman stats-2013-06-07-Robust logistic regression</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-1886-html" href="http://andrewgelman.com/2013/06/07/robust-logistic-regression/">html</a></p><p>Introduction: Corey Yanofsky writes:
  
In your work, you’ve robustificated logistic regression by having the logit function saturate at, e.g., 0.01 and 0.99, instead of  0 and 1. Do you have any thoughts on a sensible setting for the saturation values? My intuition suggests that it has something to do with proportion of outliers expected in the data (assuming a reasonable model fit). 


It would be desirable to have them fit in the model, but my intuition is that integrability of the posterior distribution might become an issue.
  
My reply:  it should be no problem to put these saturation values in the model, I bet it would work fine in Stan if you give them uniform (0,.1) priors or something like that.  Or you could just fit the robit model.
 
And this reminds me . . . I’ve been told that when Stan’s on its optimization setting, it fits generalized linear models just about as fast as regular glm or bayesglm in R.  This suggests to me that we should have some precompiled regression models in Stan,</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Corey Yanofsky writes:    In your work, you’ve robustificated logistic regression by having the logit function saturate at, e. [sent-1, score-0.451]
</p><p>2 Do you have any thoughts on a sensible setting for the saturation values? [sent-6, score-0.749]
</p><p>3 My intuition suggests that it has something to do with proportion of outliers expected in the data (assuming a reasonable model fit). [sent-7, score-1.03]
</p><p>4 It would be desirable to have them fit in the model, but my intuition is that integrability of the posterior distribution might become an issue. [sent-8, score-0.813]
</p><p>5 My reply:  it should be no problem to put these saturation values in the model, I bet it would work fine in Stan if you give them uniform (0,. [sent-9, score-0.916]
</p><p>6 I’ve been told that when Stan’s on its optimization setting, it fits generalized linear models just about as fast as regular glm or bayesglm in R. [sent-15, score-1.164]
</p><p>7 This suggests to me that we should have some precompiled regression models in Stan, then we could run all those regressions that way, and we could feel free to use whatever priors we want. [sent-16, score-1.321]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('saturation', 0.366), ('stan', 0.265), ('intuition', 0.234), ('precompiled', 0.203), ('corey', 0.203), ('fit', 0.197), ('robit', 0.191), ('priors', 0.19), ('suggests', 0.183), ('setting', 0.177), ('values', 0.168), ('bayesglm', 0.167), ('glm', 0.152), ('outliers', 0.147), ('desirable', 0.145), ('regression', 0.13), ('logit', 0.128), ('sensible', 0.128), ('optimization', 0.127), ('generalized', 0.124), ('bet', 0.123), ('uniform', 0.12), ('model', 0.12), ('regressions', 0.113), ('fast', 0.109), ('proportion', 0.109), ('fits', 0.108), ('logistic', 0.104), ('regular', 0.103), ('models', 0.102), ('reminds', 0.1), ('assuming', 0.098), ('linear', 0.09), ('function', 0.089), ('could', 0.088), ('expected', 0.087), ('posterior', 0.084), ('become', 0.083), ('told', 0.082), ('something', 0.079), ('thoughts', 0.078), ('run', 0.077), ('free', 0.074), ('whatever', 0.073), ('reasonable', 0.071), ('distribution', 0.07), ('fine', 0.07), ('work', 0.069), ('ve', 0.069), ('instead', 0.066)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="1886-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-07-Robust_logistic_regression.html">1886 andrew gelman stats-2013-06-07-Robust logistic regression</a></p>
<p>Introduction: Corey Yanofsky writes:
  
In your work, you’ve robustificated logistic regression by having the logit function saturate at, e.g., 0.01 and 0.99, instead of  0 and 1. Do you have any thoughts on a sensible setting for the saturation values? My intuition suggests that it has something to do with proportion of outliers expected in the data (assuming a reasonable model fit). 


It would be desirable to have them fit in the model, but my intuition is that integrability of the posterior distribution might become an issue.
  
My reply:  it should be no problem to put these saturation values in the model, I bet it would work fine in Stan if you give them uniform (0,.1) priors or something like that.  Or you could just fit the robit model.
 
And this reminds me . . . I’ve been told that when Stan’s on its optimization setting, it fits generalized linear models just about as fast as regular glm or bayesglm in R.  This suggests to me that we should have some precompiled regression models in Stan,</p><p>2 0.18028325 <a title="1886-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-04-PyStan%21.html">1748 andrew gelman stats-2013-03-04-PyStan!</a></p>
<p>Introduction: Stan  is written in C++ and can be run from the command line and from R.  We’d like for  Python  users to be able to run Stan as well.  If anyone is interested in doing this, please let us know and we’d be happy to work with you on it.
 
Stan, like Python, is completely free and open-source.
 
P.S.  Because Stan is open-source, it of course would also be possible for people to translate Stan into Python, or to take whatever features they like from Stan and incorporate them into a Python package.  That’s fine too.  But we think it would make sense in addition for users to be able to run Stan directly from Python, in the same way that it can be run from R.</p><p>3 0.17724152 <a title="1886-tfidf-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-07-My_recent_debugging_experience.html">2161 andrew gelman stats-2014-01-07-My recent debugging experience</a></p>
<p>Introduction: OK, so this sort of thing happens sometimes.  I was working on a new idea (still working on it; if it ultimately works out—or if it doesn’t—I’ll let you know) and as part of it I was fitting little models in Stan, in a loop.  I thought it would make sense to start with linear regression with normal priors and known data variance, because then the exact solution is Gaussian and I can also work with the problem analytically.  So I programmed up the algorithm and, no surprise, it didn’t work.  I went through my R code, put in print statements here and there, and cleared out bug after bug until at least it stopped crashing.  But the algorithm still wasn’t doing what it was supposed to do.
 
So I decided to do something simpler, and just check that the Stan linear regression gave the same answer as the analytic posterior distribution:  I ran Stan for tons of iterations, then computed the sample mean and variance of the simulations.  It was an example with two coefficients—I’d originally cho</p><p>4 0.17418006 <a title="1886-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-30-Computational_problems_with_glm_etc..html">1516 andrew gelman stats-2012-09-30-Computational problems with glm etc.</a></p>
<p>Introduction: John Mount  provides some useful background and follow-up  on our discussion  from last year  on computational instability of the usual logistic regression solver.
 
Just to refresh your memory, here’s a simple logistic regression with only a constant term and no separation, nothing pathological at all:
 
 > y <- rep (c(1,0),c(10,5)) 
> display (glm (y ~ 1, family=binomial(link="logit"))) 
glm(formula = y ~ 1, family = binomial(link = "logit")) 
            coef.est coef.se 
(Intercept) 0.69     0.55 
--- 
  n = 15, k = 1 
  residual deviance = 19.1, null deviance = 19.1 (difference = 0.0) 
 
And here’s what happens when we give it the not-outrageous starting value of -2:
 
 > display (glm (y ~ 1, family=binomial(link="logit"), start=-2)) 
glm(formula = y ~ 1, family = binomial(link = "logit"), start = -2) 
            coef.est    coef.se 
(Intercept)       71.97 17327434.18 
--- 
  n = 15, k = 1 
  residual deviance = 360.4, null deviance = 19.1 (difference = -341.3) 
Warning message:</p><p>5 0.1738185 <a title="1886-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-16-Stantastic%21.html">1580 andrew gelman stats-2012-11-16-Stantastic!</a></p>
<p>Introduction: Richard McElreath writes:
  
I’ve been translating a few ongoing data analysis projects into  Stan  code, mostly with success. The most important for me right now has been a hierarchical zero-inflated gamma problem. This a “hurdle” model, in which a bernoulli GLM produces zeros/nonzeros, and then a gamma GLM produces the nonzero values, using varying effects correlated with those in the bernoulli process.


The data are 20 years of human foraging returns from a subsistence hunting population in Paraguay (the Ache), comprising about 15k hunts in total (Hill & Kintigh. 2009. Current Anthropology 50:369-377). Observed values are kilograms of meat returned to camp. The more complex models contain a 147-by-9 matrix of varying effects (147 unique hunters), as well as imputation of missing values.


Originally, I had written the sampler myself in raw R code. It was very slow, but I knew what it was doing at least. Just before Stan version 1.0 was released, I had managed to get JAGS to do it a</p><p>6 0.16715962 <a title="1886-tfidf-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-14-Transitioning_to_Stan.html">2291 andrew gelman stats-2014-04-14-Transitioning to Stan</a></p>
<p>7 0.16366842 <a title="1886-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-29-Putting_together_multinomial_discrete_regressions_by_combining_simple_logits.html">782 andrew gelman stats-2011-06-29-Putting together multinomial discrete regressions by combining simple logits</a></p>
<p>8 0.16146155 <a title="1886-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-A_Stan_is_Born.html">1475 andrew gelman stats-2012-08-30-A Stan is Born</a></p>
<p>9 0.15843734 <a title="1886-tfidf-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-Stan_Model_of_the_Week%3A_Hierarchical_Modeling_of_Supernovas.html">2299 andrew gelman stats-2014-04-21-Stan Model of the Week: Hierarchical Modeling of Supernovas</a></p>
<p>10 0.15786752 <a title="1886-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>11 0.1476185 <a title="1886-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-22-A_Bayesian_model_for_an_increasing_function%2C_in_Stan%21.html">2110 andrew gelman stats-2013-11-22-A Bayesian model for an increasing function, in Stan!</a></p>
<p>12 0.14229453 <a title="1886-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-22-My_talks_that_were_scheduled_for_Tues_at_the_Data_Skeptics_meetup_and_Wed_at_the_Open_Statistical_Programming_meetup.html">1950 andrew gelman stats-2013-07-22-My talks that were scheduled for Tues at the Data Skeptics meetup and Wed at the Open Statistical Programming meetup</a></p>
<p>13 0.13306676 <a title="1886-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-10-Schiminovich_is_on_The_Simpsons.html">2096 andrew gelman stats-2013-11-10-Schiminovich is on The Simpsons</a></p>
<p>14 0.13266484 <a title="1886-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-21-D._Buggin.html">1465 andrew gelman stats-2012-08-21-D. Buggin</a></p>
<p>15 0.12630352 <a title="1886-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-Another_reason_why_you_can_get_good_inferences_from_a_bad_model.html">1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</a></p>
<p>16 0.12558289 <a title="1886-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>17 0.12516126 <a title="1886-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-07-Prior_distributions_for_regression_coefficients.html">1486 andrew gelman stats-2012-09-07-Prior distributions for regression coefficients</a></p>
<p>18 0.12503666 <a title="1886-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-18-Understanding_posterior_p-values.html">2029 andrew gelman stats-2013-09-18-Understanding posterior p-values</a></p>
<p>19 0.12450627 <a title="1886-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>20 0.12380464 <a title="1886-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.166), (1, 0.186), (2, 0.012), (3, 0.095), (4, 0.099), (5, 0.039), (6, 0.074), (7, -0.185), (8, -0.023), (9, -0.003), (10, -0.057), (11, 0.041), (12, -0.079), (13, -0.035), (14, 0.003), (15, -0.043), (16, -0.006), (17, 0.007), (18, -0.015), (19, -0.017), (20, 0.011), (21, -0.042), (22, -0.036), (23, -0.043), (24, -0.004), (25, -0.005), (26, 0.029), (27, -0.1), (28, -0.084), (29, -0.013), (30, 0.025), (31, 0.019), (32, 0.012), (33, 0.014), (34, -0.014), (35, 0.001), (36, 0.014), (37, 0.044), (38, -0.015), (39, -0.012), (40, 0.024), (41, 0.007), (42, 0.012), (43, -0.01), (44, 0.06), (45, -0.001), (46, -0.021), (47, 0.017), (48, -0.007), (49, 0.055)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9620232 <a title="1886-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-07-Robust_logistic_regression.html">1886 andrew gelman stats-2013-06-07-Robust logistic regression</a></p>
<p>Introduction: Corey Yanofsky writes:
  
In your work, you’ve robustificated logistic regression by having the logit function saturate at, e.g., 0.01 and 0.99, instead of  0 and 1. Do you have any thoughts on a sensible setting for the saturation values? My intuition suggests that it has something to do with proportion of outliers expected in the data (assuming a reasonable model fit). 


It would be desirable to have them fit in the model, but my intuition is that integrability of the posterior distribution might become an issue.
  
My reply:  it should be no problem to put these saturation values in the model, I bet it would work fine in Stan if you give them uniform (0,.1) priors or something like that.  Or you could just fit the robit model.
 
And this reminds me . . . I’ve been told that when Stan’s on its optimization setting, it fits generalized linear models just about as fast as regular glm or bayesglm in R.  This suggests to me that we should have some precompiled regression models in Stan,</p><p>2 0.8762812 <a title="1886-lsi-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-07-My_recent_debugging_experience.html">2161 andrew gelman stats-2014-01-07-My recent debugging experience</a></p>
<p>Introduction: OK, so this sort of thing happens sometimes.  I was working on a new idea (still working on it; if it ultimately works out—or if it doesn’t—I’ll let you know) and as part of it I was fitting little models in Stan, in a loop.  I thought it would make sense to start with linear regression with normal priors and known data variance, because then the exact solution is Gaussian and I can also work with the problem analytically.  So I programmed up the algorithm and, no surprise, it didn’t work.  I went through my R code, put in print statements here and there, and cleared out bug after bug until at least it stopped crashing.  But the algorithm still wasn’t doing what it was supposed to do.
 
So I decided to do something simpler, and just check that the Stan linear regression gave the same answer as the analytic posterior distribution:  I ran Stan for tons of iterations, then computed the sample mean and variance of the simulations.  It was an example with two coefficients—I’d originally cho</p><p>3 0.86306095 <a title="1886-lsi-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-10-Stan_Model_of_the_Week%3A__PK_Calculation_of_IV_and_Oral_Dosing.html">2242 andrew gelman stats-2014-03-10-Stan Model of the Week:  PK Calculation of IV and Oral Dosing</a></p>
<p>Introduction: [Update: Revised given comments from Wingfeet, Andrew and germo.  Thanks!  I'd mistakenly translated the dlnorm priors in the first version --- amazing what a difference the priors make.  I also escaped the less-than and greater-than signs in the constraints in the model so they're visible.  I also updated to match the thin=2 output of JAGS.]
 
We’re going to be starting a Stan “model of the P” (for some time period P) column, so I thought I’d kick things off with one of my own.  I’ve been following the  Wingvoet blog , the author of which is identified only by the Blogger handle  Wingfeet ;  a couple of days ago this lovely post came out:
  
  PK calculation of IV and oral dosing in JAGS 
   
Wingfeet’s post implemented an answer to question 6 from chapter 6 of problem from Rowland and Tozer’s 2010 book,   Clinical Pharmacokinetics and Pharmacodynamics  , Fourth edition, Lippincott, Williams & Wilkins.   
 
So in the grand tradition of using this blog to procrastinate, I thought I’d t</p><p>4 0.85445446 <a title="1886-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-22-A_Bayesian_model_for_an_increasing_function%2C_in_Stan%21.html">2110 andrew gelman stats-2013-11-22-A Bayesian model for an increasing function, in Stan!</a></p>
<p>Introduction: Following up on yesterday’s post, here’s David Chudzicki’s  story  (with graphs and Stan/R code!) of how he fit a model for an increasing function (“isotonic regression”).  Chudzicki writes:
  
This post will describe a way I came up with of fitting a function that’s constrained to be increasing, using Stan. If you want practical help, standard statistical approaches, or expert research, this isn’t the place for you (look up “isotonic regression” or “Bayesian isotonic regression” or David Dunson). This is the place for you if you want to read about how I thought about setting up a model, implemented the model in Stan, and created graphics to understand what was going on.
  
The background is that a simple, natural-seeming uniform prior on the function values does not work so well—it’s a much stronger prior distribution than one might naively think, just one of those unexpected aspects of high-dimensional probability distributions.  So Chudzicki sets up a more general family with a hype</p><p>5 0.80768549 <a title="1886-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-16-Stantastic%21.html">1580 andrew gelman stats-2012-11-16-Stantastic!</a></p>
<p>Introduction: Richard McElreath writes:
  
I’ve been translating a few ongoing data analysis projects into  Stan  code, mostly with success. The most important for me right now has been a hierarchical zero-inflated gamma problem. This a “hurdle” model, in which a bernoulli GLM produces zeros/nonzeros, and then a gamma GLM produces the nonzero values, using varying effects correlated with those in the bernoulli process.


The data are 20 years of human foraging returns from a subsistence hunting population in Paraguay (the Ache), comprising about 15k hunts in total (Hill & Kintigh. 2009. Current Anthropology 50:369-377). Observed values are kilograms of meat returned to camp. The more complex models contain a 147-by-9 matrix of varying effects (147 unique hunters), as well as imputation of missing values.


Originally, I had written the sampler myself in raw R code. It was very slow, but I knew what it was doing at least. Just before Stan version 1.0 was released, I had managed to get JAGS to do it a</p><p>6 0.79830796 <a title="1886-lsi-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-Stan_Model_of_the_Week%3A_Hierarchical_Modeling_of_Supernovas.html">2299 andrew gelman stats-2014-04-21-Stan Model of the Week: Hierarchical Modeling of Supernovas</a></p>
<p>7 0.79492736 <a title="1886-lsi-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-14-Transitioning_to_Stan.html">2291 andrew gelman stats-2014-04-14-Transitioning to Stan</a></p>
<p>8 0.76076376 <a title="1886-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-30-Stan_Project%3A__Continuous_Relaxations_for_Discrete_MRFs.html">2003 andrew gelman stats-2013-08-30-Stan Project:  Continuous Relaxations for Discrete MRFs</a></p>
<p>9 0.73961753 <a title="1886-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-28-Migrating_from_dot_to_underscore.html">1472 andrew gelman stats-2012-08-28-Migrating from dot to underscore</a></p>
<p>10 0.72456205 <a title="1886-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-12-Samplers_for_Big_Science%3A__emcee_and_BAT.html">2020 andrew gelman stats-2013-09-12-Samplers for Big Science:  emcee and BAT</a></p>
<p>11 0.72027391 <a title="1886-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-10-Schiminovich_is_on_The_Simpsons.html">2096 andrew gelman stats-2013-11-10-Schiminovich is on The Simpsons</a></p>
<p>12 0.71298403 <a title="1886-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-17-Stan_and_RStan_1.1.0.html">1627 andrew gelman stats-2012-12-17-Stan and RStan 1.1.0</a></p>
<p>13 0.71146488 <a title="1886-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-23-Scalable_Stan.html">2035 andrew gelman stats-2013-09-23-Scalable Stan</a></p>
<p>14 0.7109139 <a title="1886-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-06-Stan_1.2.0_and_RStan_1.2.0.html">1753 andrew gelman stats-2013-03-06-Stan 1.2.0 and RStan 1.2.0</a></p>
<p>15 0.70052975 <a title="1886-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-27-%28R-Py-Cmd%29Stan_2.1.0.html">2150 andrew gelman stats-2013-12-27-(R-Py-Cmd)Stan 2.1.0</a></p>
<p>16 0.699579 <a title="1886-lsi-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-20-Mailing_List_Degree-of-Difficulty_Difficulty.html">2178 andrew gelman stats-2014-01-20-Mailing List Degree-of-Difficulty Difficulty</a></p>
<p>17 0.69668579 <a title="1886-lsi-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-04-Stan_%28%26_JAGS%29_Tutorial_on_Linear_Mixed_Models.html">2318 andrew gelman stats-2014-05-04-Stan (& JAGS) Tutorial on Linear Mixed Models</a></p>
<p>18 0.69482034 <a title="1886-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-30-Stan_uses_Nuts%21.html">1036 andrew gelman stats-2011-11-30-Stan uses Nuts!</a></p>
<p>19 0.69392484 <a title="1886-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-14-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">1856 andrew gelman stats-2013-05-14-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>20 0.68250263 <a title="1886-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-29-Putting_together_multinomial_discrete_regressions_by_combining_simple_logits.html">782 andrew gelman stats-2011-06-29-Putting together multinomial discrete regressions by combining simple logits</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(9, 0.035), (16, 0.031), (21, 0.029), (24, 0.21), (53, 0.014), (54, 0.042), (58, 0.124), (59, 0.017), (63, 0.017), (86, 0.043), (99, 0.326)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97401166 <a title="1886-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-07-Robust_logistic_regression.html">1886 andrew gelman stats-2013-06-07-Robust logistic regression</a></p>
<p>Introduction: Corey Yanofsky writes:
  
In your work, you’ve robustificated logistic regression by having the logit function saturate at, e.g., 0.01 and 0.99, instead of  0 and 1. Do you have any thoughts on a sensible setting for the saturation values? My intuition suggests that it has something to do with proportion of outliers expected in the data (assuming a reasonable model fit). 


It would be desirable to have them fit in the model, but my intuition is that integrability of the posterior distribution might become an issue.
  
My reply:  it should be no problem to put these saturation values in the model, I bet it would work fine in Stan if you give them uniform (0,.1) priors or something like that.  Or you could just fit the robit model.
 
And this reminds me . . . I’ve been told that when Stan’s on its optimization setting, it fits generalized linear models just about as fast as regular glm or bayesglm in R.  This suggests to me that we should have some precompiled regression models in Stan,</p><p>2 0.96531999 <a title="1886-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-22-Statistical_inference_based_on_the_minimum_description_length_principle.html">815 andrew gelman stats-2011-07-22-Statistical inference based on the minimum description length principle</a></p>
<p>Introduction: Tom Ball writes:
  
Here’s another query to add to the stats backlog…Minimum Description Length (MDL).  I’m  attaching  a 2002 Psych Rev paper on same.  Basically, it’s an approach to model selection that replaces goodness of fit with generalizability or complexity.


Would be great to get your response to this approach.
  
My reply:
 
I’ve heard about the minimum description length principle for a long time but have never really understood it.  So I have nothing to say!  Anyone who has anything useful to say on the topic, feel free to add in the comments.
 
The rest of you might wonder why I posted this.  I just thought it would be good for you to have some sense of the boundaries of my knowledge.</p><p>3 0.96098167 <a title="1886-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-03-Uncertainty_in_parameter_estimates_using_multilevel_models.html">1966 andrew gelman stats-2013-08-03-Uncertainty in parameter estimates using multilevel models</a></p>
<p>Introduction: David Hsu writes: 
   
 I have a (perhaps) simple question about uncertainty in parameter estimates using multilevel models — what is an appropriate threshold for measure parameter uncertainty in a multilevel model? 
 
The reason why I ask is that I set out to do a crossed two-way model with two varying intercepts, similar to your flight simulator example in your 2007 book.  The difference is that I have a lot of predictors specific to each cell (I think equivalent to airport and pilot in your example), and I find after modeling this in JAGS, I happily find that the predictors are much less important than the variability by cell (airport and pilot effects).  Happily because this is what I am writing a paper about.
 
However, I then went to check subsets of predictors using lm() and lmer().  I understand that they all use different estimation methods, but what I can’t figure out is why the errors on all of the coefficient estimates are *so* different.  
 
For example, using JAGS, and th</p><p>4 0.95416987 <a title="1886-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-14-%E2%80%9CThe_best_data_visualizations_should_stand_on_their_own%E2%80%9D%3F__I_don%E2%80%99t_think_so..html">574 andrew gelman stats-2011-02-14-“The best data visualizations should stand on their own”?  I don’t think so.</a></p>
<p>Introduction: Jimmy pointed me to  this  blog by Drew Conway on word clouds.  I don’t have much to say about Conway’s specifics–word clouds aren’t really my thing, but I’m glad that people are thinking about how to do them better–but I did notice one phrase of his that I’ll dispute.  Conway writes
  
The best data visualizations should stand on their own . . .
  
I disagree.  I prefer the saying, “A picture plus 1000 words is better than two pictures or 2000 words.”  That is, I see a positive interaction between words and pictures or, to put it another way, diminishing returns for words or pictures on their own.  I don’t have any big theory for this, but I think, when expressed as a joint value function, my idea makes sense.  Also, I live this suggestion in my own work.  I typically accompany my graphs with long captions and I try to accompany my words with pictures (although I’m not doing it here, because with the software I use, it’s much easier to type more words than to find, scale, and insert i</p><p>5 0.95184386 <a title="1886-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-29-Bayesian_inference_for_the_parameter_of_a_uniform_distribution.html">979 andrew gelman stats-2011-10-29-Bayesian inference for the parameter of a uniform distribution</a></p>
<p>Introduction: Subhash Lele writes:
  
I was wondering if you might know some good references to Bayesian treatment of parameter estimation for U(0,b) type distributions. I am looking for cases where the parameter is on the boundary. I would appreciate any help and advice you could provide. I am, in particular, looking for an MCMC (preferably in WinBUGS) based approach.  I figured out the WinBUGS part but I am still curious about the theoretical papers, asymptotics etc.
  
I actually can’t think of any examples!  But maybe you, the readers, can.
 
We also should think of the best way to implement this model in Stan.  We like to transform to avoid hard boundary constraints, but it seems a bit tacky to do a data-based transformation (which itself would not work if there are latent variables).
 
P.S.    I actually saw Lele speak at a statistics conference around 20 years ago.  There was a lively exchange between Lele and an older guy who was working on similar problems using a different method.  The oth</p><p>6 0.94618344 <a title="1886-lda-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-30-Why_is_George_Apley_overrated%3F.html">119 andrew gelman stats-2010-06-30-Why is George Apley overrated?</a></p>
<p>7 0.94569975 <a title="1886-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-25-The_problem_with_realistic_advice%3F.html">1428 andrew gelman stats-2012-07-25-The problem with realistic advice?</a></p>
<p>8 0.94020683 <a title="1886-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-30-Fixed_effects%2C_followed_by_Bayes_shrinkage%3F.html">1644 andrew gelman stats-2012-12-30-Fixed effects, followed by Bayes shrinkage?</a></p>
<p>9 0.93955135 <a title="1886-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-02-The_inevitable_problems_with_statistical_significance_and_95%25_intervals.html">1150 andrew gelman stats-2012-02-02-The inevitable problems with statistical significance and 95% intervals</a></p>
<p>10 0.93943036 <a title="1886-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-14-Extra_babies_on_Valentine%E2%80%99s_Day%2C_fewer_on_Halloween%3F.html">1167 andrew gelman stats-2012-02-14-Extra babies on Valentine’s Day, fewer on Halloween?</a></p>
<p>11 0.93939906 <a title="1886-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-04-Cash_in%2C_cash_out_graph.html">502 andrew gelman stats-2011-01-04-Cash in, cash out graph</a></p>
<p>12 0.93899626 <a title="1886-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-24-Bell_Labs.html">970 andrew gelman stats-2011-10-24-Bell Labs</a></p>
<p>13 0.93829304 <a title="1886-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-21-D._Buggin.html">1465 andrew gelman stats-2012-08-21-D. Buggin</a></p>
<p>14 0.93789649 <a title="1886-lda-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-14-%E2%80%9CToo_much_data%E2%80%9D%3F.html">86 andrew gelman stats-2010-06-14-“Too much data”?</a></p>
<p>15 0.93787915 <a title="1886-lda-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-19-Scalability_in_education.html">1502 andrew gelman stats-2012-09-19-Scalability in education</a></p>
<p>16 0.93783069 <a title="1886-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>17 0.93759143 <a title="1886-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-07-X_on_JLP.html">1792 andrew gelman stats-2013-04-07-X on JLP</a></p>
<p>18 0.93727827 <a title="1886-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-The_statistical_significance_filter.html">899 andrew gelman stats-2011-09-10-The statistical significance filter</a></p>
<p>19 0.93696845 <a title="1886-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-01-Hoe_noem_je%3F.html">1191 andrew gelman stats-2012-03-01-Hoe noem je?</a></p>
<p>20 0.93687499 <a title="1886-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
