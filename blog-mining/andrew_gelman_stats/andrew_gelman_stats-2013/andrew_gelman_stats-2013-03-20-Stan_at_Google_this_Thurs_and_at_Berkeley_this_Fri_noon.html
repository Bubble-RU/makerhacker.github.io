<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1772 andrew gelman stats-2013-03-20-Stan at Google this Thurs and at Berkeley this Fri noon</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-1772" href="#">andrew_gelman_stats-2013-1772</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1772 andrew gelman stats-2013-03-20-Stan at Google this Thurs and at Berkeley this Fri noon</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-1772-html" href="http://andrewgelman.com/2013/03/20/stan-at-google-this-thurs-and-at-berkeley-this-fri-330pm/">html</a></p><p>Introduction: Michael Betancourt  will be speaking at Google and at the University of California, Berkeley.  The Google talk is closed to outsiders (but if you work at Google, you should go!); the Berkeley talk is open to all:
  
Friday March 22, 12:10 pm, Evans Hall 1011.


Title of talk:  Stan : Practical Bayesian Inference with Hamiltonian Monte Carlo


Abstract: Practical implementations of Bayesian inference are often limited to approximation methods that only slowly explore the posterior distribution.  By taking advantage of the curvature of the posterior, however, Hamiltonian Monte Carlo (HMC) efficiently explores even the most highly contorted distributions.  In this talk I will review the foundations of and recent developments within HMC, concluding with a discussion of Stan, a powerful inference engine that utilizes HMC, automatic differentiation, and adaptive methods to minimize user input.
  
This is cool stuff.  And heâ&euro;&trade;ll be showing the whirlpool movie!</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Michael Betancourt  will be speaking at Google and at the University of California, Berkeley. [sent-1, score-0.081]
</p><p>2 The Google talk is closed to outsiders (but if you work at Google, you should go! [sent-2, score-0.436]
</p><p>3 ); the Berkeley talk is open to all:    Friday March 22, 12:10 pm, Evans Hall 1011. [sent-3, score-0.219]
</p><p>4 Title of talk:  Stan : Practical Bayesian Inference with Hamiltonian Monte Carlo   Abstract: Practical implementations of Bayesian inference are often limited to approximation methods that only slowly explore the posterior distribution. [sent-4, score-0.911]
</p><p>5 By taking advantage of the curvature of the posterior, however, Hamiltonian Monte Carlo (HMC) efficiently explores even the most highly contorted distributions. [sent-5, score-0.648]
</p><p>6 In this talk I will review the foundations of and recent developments within HMC, concluding with a discussion of Stan, a powerful inference engine that utilizes HMC, automatic differentiation, and adaptive methods to minimize user input. [sent-6, score-1.655]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hmc', 0.348), ('google', 0.223), ('talk', 0.219), ('hamiltonian', 0.215), ('carlo', 0.205), ('monte', 0.194), ('contorted', 0.164), ('utilizes', 0.164), ('whirlpool', 0.164), ('inference', 0.157), ('practical', 0.152), ('pm', 0.148), ('curvature', 0.148), ('concluding', 0.148), ('stan', 0.143), ('explores', 0.139), ('evans', 0.139), ('implementations', 0.135), ('posterior', 0.135), ('minimize', 0.127), ('differentiation', 0.127), ('betancourt', 0.125), ('hall', 0.119), ('friday', 0.115), ('efficiently', 0.115), ('adaptive', 0.115), ('closed', 0.113), ('developments', 0.112), ('automatic', 0.112), ('engine', 0.11), ('march', 0.108), ('slowly', 0.107), ('foundations', 0.107), ('approximation', 0.105), ('outsiders', 0.104), ('california', 0.103), ('berkeley', 0.101), ('movie', 0.097), ('powerful', 0.095), ('methods', 0.095), ('user', 0.094), ('explore', 0.089), ('limited', 0.088), ('bayesian', 0.085), ('advantage', 0.082), ('speaking', 0.081), ('abstract', 0.079), ('cool', 0.078), ('showing', 0.075), ('michael', 0.074)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1772-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-20-Stan_at_Google_this_Thurs_and_at_Berkeley_this_Fri_noon.html">1772 andrew gelman stats-2013-03-20-Stan at Google this Thurs and at Berkeley this Fri noon</a></p>
<p>Introduction: Michael Betancourt  will be speaking at Google and at the University of California, Berkeley.  The Google talk is closed to outsiders (but if you work at Google, you should go!); the Berkeley talk is open to all:
  
Friday March 22, 12:10 pm, Evans Hall 1011.


Title of talk:  Stan : Practical Bayesian Inference with Hamiltonian Monte Carlo


Abstract: Practical implementations of Bayesian inference are often limited to approximation methods that only slowly explore the posterior distribution.  By taking advantage of the curvature of the posterior, however, Hamiltonian Monte Carlo (HMC) efficiently explores even the most highly contorted distributions.  In this talk I will review the foundations of and recent developments within HMC, concluding with a discussion of Stan, a powerful inference engine that utilizes HMC, automatic differentiation, and adaptive methods to minimize user input.
  
This is cool stuff.  And heâ&euro;&trade;ll be showing the whirlpool movie!</p><p>2 0.70145905 <a title="1772-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-04-Stan_in_L.A._this_Wed_3%3A30pm.html">1749 andrew gelman stats-2013-03-04-Stan in L.A. this Wed 3:30pm</a></p>
<p>Introduction: Michael Betancourt  will be speaking at UCLA:
  
The location for refreshment is in room 51-254 CHS at 3:00 PM.


The place for the seminar is at CHS 33-105A  at 3:30pm – 4:30pm, Wed 6 Mar.


["CHS" stands for Center for Health Sciences, the building of the UCLA schools of medicine and public health.   Here's a map with directions .]


Title of talk:  Stan : Practical Bayesian Inference with Hamiltonian Monte Carlo


Abstract: Practical implementations of Bayesian inference are often limited to approximation methods that only slowly explore the posterior distribution.  By taking advantage of the curvature of the posterior, however, Hamiltonian Monte Carlo (HMC) efficiently explores even the most highly contorted distributions.  In this talk I will review the foundations of and recent developments within HMC, concluding with a discussion of Stan, a powerful inference engine that utilizes HMC, automatic differentiation, and adaptive methods to minimize user input.
  
This is cool stuff.</p><p>3 0.28546807 <a title="1772-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<p>Introduction: Tomas Iesmantas had  asked me  for advice on a regression problem with 50 parameters, and I’d recommended Hamiltonian Monte Carlo.
 
A few weeks later he reported back: 
  
  
After trying several modifications (HMC for all parameters at once, HMC just for first level parameters and Riemman manifold Hamiltonian Monte Carlo method), I finally got it running with HMC just for first level parameters and for others using direct sampling, since conditional distributions turned out to have closed form.


However, even in this case it is quite tricky, since I had to employ mass matrix and not just diagonal but at the beginning of algorithm generated it randomly (ensuring it is positive definite). Such random generation of mass matrix is quite blind step, but it proved to be quite helpful.


Riemman manifold HMC is quite vagarious, or to be more specific, metric of manifold is very sensitive. In my model log-likelihood I had exponents and values of metrics matrix elements was very large and wh</p><p>4 0.21947649 <a title="1772-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-23-Learning_Differential_Geometry_for_Hamiltonian_Monte_Carlo.html">1339 andrew gelman stats-2012-05-23-Learning Differential Geometry for Hamiltonian Monte Carlo</a></p>
<p>Introduction: You can get a taste of Hamiltonian Monte Carlo (HMC) by reading the very gentle introduction in David MacKay’s general text on information theory:
  
  MacKay, D.  2003.    Information Theory, Inference, and Learning Algorithms  .  Cambridge University Press.  [see Chapter 31, which is relatively standalone and can be downloaded separately.]
   
Follow this up with Radford Neal’s much more thorough introduction to HMC:
  
 Neal, R. 2011.   MCMC Using Hamiltonian Dynamics .  In Brooks, Gelman, Jones and Meng, eds.,  Handbook of Markov Chain Monte Carlo .  Chapman and Hall/CRC Press.
   
To understand why HMC works and set yourself on the path to understanding generalizations like  Riemann manifold HMC , you’ll need to know a bit about differential geometry.  I really liked the combination of these two books:
  
  Magnus, J. R. and H. Neudecker.  2007.   Matrix Differential Calculus with Application in Statistics and Econometrics .  3rd Edition.  Wiley?
   
and
  
  Leimkuhler, B. and S.</p><p>5 0.20361349 <a title="1772-tfidf-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-Running_into_a_Stan_Reference_by_Accident.html">2231 andrew gelman stats-2014-03-03-Running into a Stan Reference by Accident</a></p>
<p>Introduction: We were talking about parallelizing MCMC and I came up with what I thought was a neat idea for parallelizing MCMC (sample with fractional prior, average samples on a per-draw basis).  But then I realized this approach could get the right posterior mean or right posterior variance, but not both, depending on how the prior was divided (for a beta-binomial example).  Then  Aki  told me it had already been done in a more general form in a paper of Scott et al.,  Bayes and Big Data , which was then used as the baseline in: 
 
Willie Neiswanger, Chong Wang, and Eric Xing. 2013.   Asymptotically Exact, Embarrassingly Parallel MCMC .  arXiv  1311.4780. 
 
It’s a neat paper, which Xi’an  already blogged  about months ago.  But what really struck me was the following quote:
  

We use Stan, an automated Hamiltonian Monte Carlo (HMC) software package, to perform sampling for both the true posterior (for groundtruth and comparison methods) and for the subposteriors on each machine. One advantage o</p><p>6 0.18656103 <a title="1772-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-30-Stan_uses_Nuts%21.html">1036 andrew gelman stats-2011-11-30-Stan uses Nuts!</a></p>
<p>7 0.1797767 <a title="1772-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-30-Stan_Project%3A__Continuous_Relaxations_for_Discrete_MRFs.html">2003 andrew gelman stats-2013-08-30-Stan Project:  Continuous Relaxations for Discrete MRFs</a></p>
<p>8 0.1617287 <a title="1772-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-15-More_data_tools_worth_using_from_Google.html">911 andrew gelman stats-2011-09-15-More data tools worth using from Google</a></p>
<p>9 0.15528587 <a title="1772-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-24-Bleg%3A_Automatic_Differentiation_for_Log_Prob_Gradients%3F.html">535 andrew gelman stats-2011-01-24-Bleg: Automatic Differentiation for Log Prob Gradients?</a></p>
<p>10 0.15093283 <a title="1772-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-22-My_talks_that_were_scheduled_for_Tues_at_the_Data_Skeptics_meetup_and_Wed_at_the_Open_Statistical_Programming_meetup.html">1950 andrew gelman stats-2013-07-22-My talks that were scheduled for Tues at the Data Skeptics meetup and Wed at the Open Statistical Programming meetup</a></p>
<p>11 0.15033016 <a title="1772-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-A_Stan_is_Born.html">1475 andrew gelman stats-2012-08-30-A Stan is Born</a></p>
<p>12 0.13791233 <a title="1772-tfidf-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-14-Transitioning_to_Stan.html">2291 andrew gelman stats-2014-04-14-Transitioning to Stan</a></p>
<p>13 0.1356125 <a title="1772-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-My_talk_at_MIT_on_Thurs_11_Oct.html">1528 andrew gelman stats-2012-10-10-My talk at MIT on Thurs 11 Oct</a></p>
<p>14 0.12980519 <a title="1772-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-01-What_goes_around_._._..html">548 andrew gelman stats-2011-02-01-What goes around . . .</a></p>
<p>15 0.12819815 <a title="1772-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-07-Update_on_the_new_Handbook_of_MCMC.html">844 andrew gelman stats-2011-08-07-Update on the new Handbook of MCMC</a></p>
<p>16 0.12632433 <a title="1772-tfidf-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-04-Handy_Matrix_Cheat_Sheet%2C_with_Gradients.html">555 andrew gelman stats-2011-02-04-Handy Matrix Cheat Sheet, with Gradients</a></p>
<p>17 0.12078857 <a title="1772-tfidf-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-31-Just_gave_a_talk.html">2275 andrew gelman stats-2014-03-31-Just gave a talk</a></p>
<p>18 0.11098604 <a title="1772-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-26-My_talk_at_Berkeley_on_Wednesday.html">680 andrew gelman stats-2011-04-26-My talk at Berkeley on Wednesday</a></p>
<p>19 0.10958353 <a title="1772-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-27-%28R-Py-Cmd%29Stan_2.1.0.html">2150 andrew gelman stats-2013-12-27-(R-Py-Cmd)Stan 2.1.0</a></p>
<p>20 0.095773607 <a title="1772-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-%E2%80%9CAll_Models_are_Right%2C_Most_are_Useless%E2%80%9D.html">1197 andrew gelman stats-2012-03-04-“All Models are Right, Most are Useless”</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.101), (1, 0.078), (2, -0.101), (3, 0.081), (4, 0.002), (5, 0.086), (6, -0.058), (7, -0.121), (8, -0.087), (9, -0.167), (10, -0.113), (11, -0.044), (12, -0.064), (13, 0.015), (14, 0.089), (15, -0.061), (16, 0.017), (17, 0.022), (18, -0.003), (19, 0.033), (20, -0.079), (21, -0.03), (22, 0.047), (23, -0.016), (24, 0.067), (25, 0.034), (26, -0.083), (27, -0.014), (28, 0.027), (29, 0.015), (30, 0.005), (31, 0.007), (32, 0.02), (33, -0.024), (34, -0.03), (35, 0.007), (36, 0.059), (37, -0.106), (38, 0.015), (39, -0.026), (40, -0.057), (41, 0.051), (42, 0.007), (43, 0.034), (44, 0.024), (45, -0.071), (46, -0.008), (47, -0.003), (48, 0.017), (49, -0.105)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97581768 <a title="1772-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-20-Stan_at_Google_this_Thurs_and_at_Berkeley_this_Fri_noon.html">1772 andrew gelman stats-2013-03-20-Stan at Google this Thurs and at Berkeley this Fri noon</a></p>
<p>Introduction: Michael Betancourt  will be speaking at Google and at the University of California, Berkeley.  The Google talk is closed to outsiders (but if you work at Google, you should go!); the Berkeley talk is open to all:
  
Friday March 22, 12:10 pm, Evans Hall 1011.


Title of talk:  Stan : Practical Bayesian Inference with Hamiltonian Monte Carlo


Abstract: Practical implementations of Bayesian inference are often limited to approximation methods that only slowly explore the posterior distribution.  By taking advantage of the curvature of the posterior, however, Hamiltonian Monte Carlo (HMC) efficiently explores even the most highly contorted distributions.  In this talk I will review the foundations of and recent developments within HMC, concluding with a discussion of Stan, a powerful inference engine that utilizes HMC, automatic differentiation, and adaptive methods to minimize user input.
  
This is cool stuff.  And heâ&euro;&trade;ll be showing the whirlpool movie!</p><p>2 0.94562817 <a title="1772-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-04-Stan_in_L.A._this_Wed_3%3A30pm.html">1749 andrew gelman stats-2013-03-04-Stan in L.A. this Wed 3:30pm</a></p>
<p>Introduction: Michael Betancourt  will be speaking at UCLA:
  
The location for refreshment is in room 51-254 CHS at 3:00 PM.


The place for the seminar is at CHS 33-105A  at 3:30pm – 4:30pm, Wed 6 Mar.


["CHS" stands for Center for Health Sciences, the building of the UCLA schools of medicine and public health.   Here's a map with directions .]


Title of talk:  Stan : Practical Bayesian Inference with Hamiltonian Monte Carlo


Abstract: Practical implementations of Bayesian inference are often limited to approximation methods that only slowly explore the posterior distribution.  By taking advantage of the curvature of the posterior, however, Hamiltonian Monte Carlo (HMC) efficiently explores even the most highly contorted distributions.  In this talk I will review the foundations of and recent developments within HMC, concluding with a discussion of Stan, a powerful inference engine that utilizes HMC, automatic differentiation, and adaptive methods to minimize user input.
  
This is cool stuff.</p><p>3 0.68691951 <a title="1772-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-My_talk_at_MIT_on_Thurs_11_Oct.html">1528 andrew gelman stats-2012-10-10-My talk at MIT on Thurs 11 Oct</a></p>
<p>Introduction: Stan: open-source Bayesian inference 
 
Speaker: Andrew Gelman, Columbia University 
Date: Thursday, October 11 2012 
Time: 4:00PM to 5:00PM 
Location: 32-D507 
Host: Polina Golland, CSAIL 
Contact: Polina Golland, 6172538005, polina@csail.mit.edu
 
Stan ( mc-stan.org ) is an open-source package for obtaining Bayesian inference using the No-U-Turn sampler, a variant of Hamiltonian Monte Carlo. We discuss how Stan works and what it can do, the problems that motivated us to write Stan, current challenges, and areas of planned development, including tools for improved generality and usability, more efficient sampling algorithms, and fuller integration of model building, model checking, and model understanding in Bayesian data analysis.
 
P.S.   Hereâ&euro;&trade;s the talk .</p><p>4 0.66561878 <a title="1772-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-30-Stan_uses_Nuts%21.html">1036 andrew gelman stats-2011-11-30-Stan uses Nuts!</a></p>
<p>Introduction: We interrupt our usual program of  Ed Wegman   Gregg Easterbrook  Niall Ferguson mockery to deliver a serious update on our statistical computing project.
 
 Stan  (“Sampling Through Adaptive Neighborhoods”) is our new C++ program (written mostly by Bob Carpenter) that draws samples from Bayesian models.  Stan can take different sorts of inputs:  you can write the model in a Bugs-like syntax and it goes from there, or you can write the log-posterior directly as a C++ function.
 
Most of the computation is done using Hamiltonian Monte Carlo.  HMC requires some tuning, so Matt Hoffman up and wrote a new algorithm, Nuts (the “No-U-Turn Sampler”) which optimizes HMC adaptively.  In many settings, Nuts is actually more computationally efficient than the optimal static HMC!
 
 When the  the Nuts paper  appeared on Arxiv, Christian Robert noticed it and had  some reactions .
 
In response to Xian’s comments, Matt writes:
  

Christian writes:

 
I wonder about the computing time (and the “una</p><p>5 0.6625728 <a title="1772-lsi-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-Running_into_a_Stan_Reference_by_Accident.html">2231 andrew gelman stats-2014-03-03-Running into a Stan Reference by Accident</a></p>
<p>Introduction: We were talking about parallelizing MCMC and I came up with what I thought was a neat idea for parallelizing MCMC (sample with fractional prior, average samples on a per-draw basis).  But then I realized this approach could get the right posterior mean or right posterior variance, but not both, depending on how the prior was divided (for a beta-binomial example).  Then  Aki  told me it had already been done in a more general form in a paper of Scott et al.,  Bayes and Big Data , which was then used as the baseline in: 
 
Willie Neiswanger, Chong Wang, and Eric Xing. 2013.   Asymptotically Exact, Embarrassingly Parallel MCMC .  arXiv  1311.4780. 
 
It’s a neat paper, which Xi’an  already blogged  about months ago.  But what really struck me was the following quote:
  

We use Stan, an automated Hamiltonian Monte Carlo (HMC) software package, to perform sampling for both the true posterior (for groundtruth and comparison methods) and for the subposteriors on each machine. One advantage o</p><p>6 0.66209424 <a title="1772-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-20-Stan%3A_A_%28Bayesian%29_Directed_Graphical_Model_Compiler.html">1131 andrew gelman stats-2012-01-20-Stan: A (Bayesian) Directed Graphical Model Compiler</a></p>
<p>7 0.65912396 <a title="1772-lsi-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-26-WAIC_and_cross-validation_in_Stan%21.html">2349 andrew gelman stats-2014-05-26-WAIC and cross-validation in Stan!</a></p>
<p>8 0.65785617 <a title="1772-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-A_Stan_is_Born.html">1475 andrew gelman stats-2012-08-30-A Stan is Born</a></p>
<p>9 0.6452322 <a title="1772-lsi-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-05-Identifying_pathways_for_managing_multiple_disturbances_to_limit_plant_invasions.html">2360 andrew gelman stats-2014-06-05-Identifying pathways for managing multiple disturbances to limit plant invasions</a></p>
<p>10 0.63789582 <a title="1772-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-27-%28R-Py-Cmd%29Stan_2.1.0.html">2150 andrew gelman stats-2013-12-27-(R-Py-Cmd)Stan 2.1.0</a></p>
<p>11 0.61382139 <a title="1772-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-23-Learning_Differential_Geometry_for_Hamiltonian_Monte_Carlo.html">1339 andrew gelman stats-2012-05-23-Learning Differential Geometry for Hamiltonian Monte Carlo</a></p>
<p>12 0.61001927 <a title="1772-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-30-Stan_Project%3A__Continuous_Relaxations_for_Discrete_MRFs.html">2003 andrew gelman stats-2013-08-30-Stan Project:  Continuous Relaxations for Discrete MRFs</a></p>
<p>13 0.59105569 <a title="1772-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-13-Stan%21.html">1855 andrew gelman stats-2013-05-13-Stan!</a></p>
<p>14 0.56970924 <a title="1772-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-13-Stan_at_NIPS_2012_Workshop_on_Probabilistic_Programming.html">1576 andrew gelman stats-2012-11-13-Stan at NIPS 2012 Workshop on Probabilistic Programming</a></p>
<p>15 0.56853527 <a title="1772-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-22-My_talks_that_were_scheduled_for_Tues_at_the_Data_Skeptics_meetup_and_Wed_at_the_Open_Statistical_Programming_meetup.html">1950 andrew gelman stats-2013-07-22-My talks that were scheduled for Tues at the Data Skeptics meetup and Wed at the Open Statistical Programming meetup</a></p>
<p>16 0.56406158 <a title="1772-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-17-Stan_and_RStan_1.1.0.html">1627 andrew gelman stats-2012-12-17-Stan and RStan 1.1.0</a></p>
<p>17 0.54983872 <a title="1772-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-12-Samplers_for_Big_Science%3A__emcee_and_BAT.html">2020 andrew gelman stats-2013-09-12-Samplers for Big Science:  emcee and BAT</a></p>
<p>18 0.53742659 <a title="1772-lsi-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-04-Stan_%28%26_JAGS%29_Tutorial_on_Linear_Mixed_Models.html">2318 andrew gelman stats-2014-05-04-Stan (& JAGS) Tutorial on Linear Mixed Models</a></p>
<p>19 0.53391522 <a title="1772-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-01-What_goes_around_._._..html">548 andrew gelman stats-2011-02-01-What goes around . . .</a></p>
<p>20 0.53130162 <a title="1772-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(13, 0.024), (16, 0.047), (21, 0.041), (24, 0.066), (45, 0.014), (53, 0.015), (54, 0.016), (61, 0.015), (66, 0.033), (74, 0.017), (77, 0.01), (82, 0.357), (84, 0.014), (86, 0.05), (99, 0.178)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91691697 <a title="1772-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-20-Stan_at_Google_this_Thurs_and_at_Berkeley_this_Fri_noon.html">1772 andrew gelman stats-2013-03-20-Stan at Google this Thurs and at Berkeley this Fri noon</a></p>
<p>Introduction: Michael Betancourt  will be speaking at Google and at the University of California, Berkeley.  The Google talk is closed to outsiders (but if you work at Google, you should go!); the Berkeley talk is open to all:
  
Friday March 22, 12:10 pm, Evans Hall 1011.


Title of talk:  Stan : Practical Bayesian Inference with Hamiltonian Monte Carlo


Abstract: Practical implementations of Bayesian inference are often limited to approximation methods that only slowly explore the posterior distribution.  By taking advantage of the curvature of the posterior, however, Hamiltonian Monte Carlo (HMC) efficiently explores even the most highly contorted distributions.  In this talk I will review the foundations of and recent developments within HMC, concluding with a discussion of Stan, a powerful inference engine that utilizes HMC, automatic differentiation, and adaptive methods to minimize user input.
  
This is cool stuff.  And heâ&euro;&trade;ll be showing the whirlpool movie!</p><p>2 0.87192976 <a title="1772-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-04-Stan_in_L.A._this_Wed_3%3A30pm.html">1749 andrew gelman stats-2013-03-04-Stan in L.A. this Wed 3:30pm</a></p>
<p>Introduction: Michael Betancourt  will be speaking at UCLA:
  
The location for refreshment is in room 51-254 CHS at 3:00 PM.


The place for the seminar is at CHS 33-105A  at 3:30pm – 4:30pm, Wed 6 Mar.


["CHS" stands for Center for Health Sciences, the building of the UCLA schools of medicine and public health.   Here's a map with directions .]


Title of talk:  Stan : Practical Bayesian Inference with Hamiltonian Monte Carlo


Abstract: Practical implementations of Bayesian inference are often limited to approximation methods that only slowly explore the posterior distribution.  By taking advantage of the curvature of the posterior, however, Hamiltonian Monte Carlo (HMC) efficiently explores even the most highly contorted distributions.  In this talk I will review the foundations of and recent developments within HMC, concluding with a discussion of Stan, a powerful inference engine that utilizes HMC, automatic differentiation, and adaptive methods to minimize user input.
  
This is cool stuff.</p><p>3 0.85043275 <a title="1772-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-21-Applied_Statistics_Center_miniconference%3A__Statistical_sampling_in_developing_countries.html">359 andrew gelman stats-2010-10-21-Applied Statistics Center miniconference:  Statistical sampling in developing countries</a></p>
<p>Introduction: Speakers:
 
Cyrus Samii, PhD candidate, Department of Political Science, Columbia University: 
 “Peacebuilding Policies as Quasi-Experiments: Some Examples” 
 
Macartan Humphreys, Associate Professor, Department of Political Science, Columbia University: 
 “Sampling in developing countries: Five challenges from the field” 
 
Friday 22 Oct, 3-5pm in the Playroom (707 International Affairs Building).  Open to all.</p><p>4 0.80355716 <a title="1772-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-03-It_depends_upon_what_the_meaning_of_the_word_%E2%80%9Cfirm%E2%80%9D_is..html">940 andrew gelman stats-2011-10-03-It depends upon what the meaning of the word “firm” is.</a></p>
<p>Introduction: David Hogg pointed me to  this news article  by Angela Saini: 
  
  
It’s not often that the quiet world of mathematics is rocked by a murder case. But last summer saw a trial that sent academics into a tailspin, and has since swollen into a fevered clash between science and the law.


At its heart, this is a story about chance. And it begins with a convicted killer, “T”, who took his case to the court of appeal in 2010. Among the evidence against him was a shoeprint from a pair of Nike trainers, which seemed to match a pair found at his home. While appeals often unmask shaky evidence, this was different. This time, a mathematical formula was thrown out of court. The footwear expert made what the judge believed were poor calculations about the likelihood of the match, compounded by a bad explanation of how he reached his opinion. The conviction was quashed. . . .


“The impact will be quite shattering,” says Professor Norman Fenton, a mathematician at Queen Mary, University of London.</p><p>5 0.7988025 <a title="1772-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-11-How_to_think_about_Lou_Dobbs.html">335 andrew gelman stats-2010-10-11-How to think about Lou Dobbs</a></p>
<p>Introduction: I was  unsurprised  to read that Lou Dobbs, the former CNN host who crusaded against illegal immigrants, had actually hired a bunch of them himself to maintain his large house and his horse farm.  (OK, I have to admit I was surprised by the part about the horse farm.)
 
But I think most of the reactions to this story missed the point. Isabel Macdonald’s  article  that broke the story was entitled, “Lou Dobbs, American Hypocrite,” and most of the  discussion  went from there, with some commenters piling on Dobbs and others defending him by saying that Dobbs hired his laborers through contractors and may not have known they were in the country illegally.
 
To me, though, the key issue is slightly different.  And Macdonald’s story is relevant whether or not Dobbs knew he was hiring illegals.  My point is not that Dobbs is a bad guy, or a hypocrite, or whatever.  My point is that, in his setting, it would take an extraordinary effort to not hire illegal immigrants to take care of his house</p><p>6 0.77336138 <a title="1772-lda-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-03-%28Partisan%29_visualization_of_health_care_legislation.html">178 andrew gelman stats-2010-08-03-(Partisan) visualization of health care legislation</a></p>
<p>7 0.75621033 <a title="1772-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-13-Randomized_experiments%2C_non-randomized_experiments%2C_and_observational_studies.html">340 andrew gelman stats-2010-10-13-Randomized experiments, non-randomized experiments, and observational studies</a></p>
<p>8 0.75461352 <a title="1772-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-06-Another_stereotype_demolished.html">699 andrew gelman stats-2011-05-06-Another stereotype demolished</a></p>
<p>9 0.74920154 <a title="1772-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-27-Teaching_is_hard.html">1958 andrew gelman stats-2013-07-27-Teaching is hard</a></p>
<p>10 0.71787345 <a title="1772-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-31-Using_factor_analysis_or_principal_components_analysis_or_measurement-error_models_for_biological_measurements_in_archaeology%3F.html">1094 andrew gelman stats-2011-12-31-Using factor analysis or principal components analysis or measurement-error models for biological measurements in archaeology?</a></p>
<p>11 0.70892006 <a title="1772-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-02-%E2%80%9CA_Christmas_Carol%E2%80%9D_as_applied_to_plagiarism.html">1440 andrew gelman stats-2012-08-02-“A Christmas Carol” as applied to plagiarism</a></p>
<p>12 0.687428 <a title="1772-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-30-Stan_Project%3A__Continuous_Relaxations_for_Discrete_MRFs.html">2003 andrew gelman stats-2013-08-30-Stan Project:  Continuous Relaxations for Discrete MRFs</a></p>
<p>13 0.67777169 <a title="1772-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-08-Annals_of_spam.html">1488 andrew gelman stats-2012-09-08-Annals of spam</a></p>
<p>14 0.67732835 <a title="1772-lda-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-09-Besag.html">193 andrew gelman stats-2010-08-09-Besag</a></p>
<p>15 0.67332399 <a title="1772-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-24-Mankiw_tax_update.html">366 andrew gelman stats-2010-10-24-Mankiw tax update</a></p>
<p>16 0.65169072 <a title="1772-lda-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-03-More_on_that_Dartmouth_health_care_study.html">67 andrew gelman stats-2010-06-03-More on that Dartmouth health care study</a></p>
<p>17 0.64979762 <a title="1772-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-07-Peer_pressure%2C_selection%2C_and_educational_reform.html">326 andrew gelman stats-2010-10-07-Peer pressure, selection, and educational reform</a></p>
<p>18 0.64727956 <a title="1772-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-21-Lessons_learned_from_a_recent_R_package_submission.html">1134 andrew gelman stats-2012-01-21-Lessons learned from a recent R package submission</a></p>
<p>19 0.64291787 <a title="1772-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-20-Sas_and_R.html">357 andrew gelman stats-2010-10-20-Sas and R</a></p>
<p>20 0.63708031 <a title="1772-lda-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-30-Real_rothko%2C_fake_rothko.html">1553 andrew gelman stats-2012-10-30-Real rothko, fake rothko</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
