<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1740 andrew gelman stats-2013-02-26-“Is machine learning a subset of statistics?”</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-1740" href="#">andrew_gelman_stats-2013-1740</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1740 andrew gelman stats-2013-02-26-“Is machine learning a subset of statistics?”</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-1740-html" href="http://andrewgelman.com/2013/02/26/18052/">html</a></p><p>Introduction: Following up on our  previous post , Andrew Wilson writes:
  
I agree we are in a really exciting time for statistics and machine learning.  There has been a lot of talk lately comparing machine learning with statistics.  I am curious whether you think there are many fundamental differences between the fields, or just superficial differences — different popular approximate inference methods, slightly different popular application areas, etc.  Is machine learning a subset of statistics?


In the paper we discuss how we think machine learning is fundamentally about pattern discovery, and ultimately, fully automating the learning and decision making process.  In other words, whatever a human does when he or she uses tools to analyze data, can be written down algorithmically and automated on a computer.  I am not sure if the ambitions are similar in statistics — and I don’t have any conventional statistics background, which makes it harder to tell. I think it’s an interesting discussion.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Following up on our  previous post , Andrew Wilson writes:    I agree we are in a really exciting time for statistics and machine learning. [sent-1, score-0.534]
</p><p>2 There has been a lot of talk lately comparing machine learning with statistics. [sent-2, score-0.815]
</p><p>3 I am curious whether you think there are many fundamental differences between the fields, or just superficial differences — different popular approximate inference methods, slightly different popular application areas, etc. [sent-3, score-0.648]
</p><p>4 In the paper we discuss how we think machine learning is fundamentally about pattern discovery, and ultimately, fully automating the learning and decision making process. [sent-5, score-1.641]
</p><p>5 In other words, whatever a human does when he or she uses tools to analyze data, can be written down algorithmically and automated on a computer. [sent-6, score-0.208]
</p><p>6 I am not sure if the ambitions are similar in statistics — and I don’t have any conventional statistics background, which makes it harder to tell. [sent-7, score-0.408]
</p><p>7 My reply:   I don’t know enough about machine learning to know what differences there are between the fields. [sent-9, score-0.797]
</p><p>8 One of my sayings is that theoretical statistics is another name for the theory of applied statistics. [sent-10, score-0.468]
</p><p>9 That is, statistics is all about modeling what we do, and modeling what we should be doing. [sent-11, score-0.47]
</p><p>10 As always in the social sciences, normative modeling has a descriptive flavor and descriptive modeling has a normative flavor:  to the extent that we’re not doing what we say we should be doing, this suggests potential changes in our theory or in our practice. [sent-12, score-1.411]
</p><p>11 And much of my work over the years has been to give theoretical foundations for various areas of statistical practice that have typically been treated informally. [sent-13, score-0.279]
</p><p>12 Thus, compared to other academic statisticians, I think I spend more time monitoring convergence of my iterative simulations, checking the fit of my models, and graphing data and fitted curves—but at the same time I do these things more formally than many statisticians have been trained to do. [sent-14, score-0.805]
</p><p>13 I think that some of the research we’ve been discussing lately on automatic model construction (done by people other than me, let me emphasize! [sent-15, score-0.459]
</p><p>14 ) is important in that is moving toward a better description—and thus also a better normative theory—of model building. [sent-16, score-0.405]
</p><p>15 To me, it’s a big step forward from that thing where “learning a model” is associated with taking a big multivariate dataset and trying to identify conditional independence structures. [sent-17, score-0.258]
</p><p>16 To me, all that stuff is static, and I’m much happier with a framework in which models are built out recursively in a language-like fashion. [sent-18, score-0.343]
</p><p>17 We still have a ways to go in fitting models that we’ve already specified. [sent-20, score-0.087]
</p><p>18 Are we at the stage of “fully automating the learning and decision making process”? [sent-22, score-0.792]
</p><p>19 But the only way forward is to try, not getting too stuck in our current understanding at any time. [sent-24, score-0.108]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('learning', 0.346), ('machine', 0.312), ('normative', 0.257), ('automating', 0.203), ('flavor', 0.171), ('modeling', 0.162), ('lately', 0.157), ('statistics', 0.146), ('descriptive', 0.142), ('differences', 0.139), ('theory', 0.118), ('algorithmically', 0.116), ('ambitions', 0.116), ('fully', 0.115), ('recursively', 0.11), ('forward', 0.108), ('sayings', 0.105), ('areas', 0.104), ('decision', 0.1), ('theoretical', 0.099), ('superficial', 0.098), ('popular', 0.098), ('statisticians', 0.095), ('automated', 0.092), ('static', 0.09), ('models', 0.087), ('monitoring', 0.087), ('wilson', 0.084), ('graphing', 0.082), ('iterative', 0.082), ('independence', 0.081), ('curves', 0.081), ('automatic', 0.079), ('thus', 0.079), ('happier', 0.078), ('construction', 0.078), ('convergence', 0.078), ('formally', 0.077), ('trained', 0.076), ('time', 0.076), ('think', 0.076), ('foundations', 0.076), ('fundamentally', 0.073), ('stage', 0.073), ('subset', 0.072), ('making', 0.07), ('multivariate', 0.069), ('model', 0.069), ('discovery', 0.068), ('built', 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="1740-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-%E2%80%9CIs_machine_learning_a_subset_of_statistics%3F%E2%80%9D.html">1740 andrew gelman stats-2013-02-26-“Is machine learning a subset of statistics?”</a></p>
<p>Introduction: Following up on our  previous post , Andrew Wilson writes:
  
I agree we are in a really exciting time for statistics and machine learning.  There has been a lot of talk lately comparing machine learning with statistics.  I am curious whether you think there are many fundamental differences between the fields, or just superficial differences — different popular approximate inference methods, slightly different popular application areas, etc.  Is machine learning a subset of statistics?


In the paper we discuss how we think machine learning is fundamentally about pattern discovery, and ultimately, fully automating the learning and decision making process.  In other words, whatever a human does when he or she uses tools to analyze data, can be written down algorithmically and automated on a computer.  I am not sure if the ambitions are similar in statistics — and I don’t have any conventional statistics background, which makes it harder to tell. I think it’s an interesting discussion.</p><p>2 0.22344738 <a title="1740-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-20-Stan%3A_A_%28Bayesian%29_Directed_Graphical_Model_Compiler.html">1131 andrew gelman stats-2012-01-20-Stan: A (Bayesian) Directed Graphical Model Compiler</a></p>
<p>Introduction: Here’s Bob’s talk  from the NYC machine learning  meetup .  And here’s Stan himself:</p><p>3 0.21062808 <a title="1740-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-04-Model_checking_and_model_understanding_in_machine_learning.html">1482 andrew gelman stats-2012-09-04-Model checking and model understanding in machine learning</a></p>
<p>Introduction: Last month I  wrote :
  
Computer scientists are often brilliant but they can be unfamiliar with what is done in the worlds of data collection and analysis. This goes the other way too: statisticians such as myself can look pretty awkward, reinventing (or failing to reinvent) various wheels when we write computer programs or, even worse, try to design software.Andrew MacNamara writes:
  
Andrew MacNamara followed up with some thoughts:
  
I [MacNamara] had some basic statistics training through my MBA program, after having completed an undergrad degree in computer science. Since then I’ve been very interested in learning more about statistical techniques, including things like GLM and censored data analyses as well as machine learning topics like neural nets, SVMs, etc. I began following your blog after some research into Bayesian analysis topics and I am trying to dig deeper on that side of things.


One thing I have noticed is that there seems to be a distinction between data analysi</p><p>4 0.19666083 <a title="1740-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>Introduction: David Duvenaud writes:
  
I’ve been following  your recent discussions  about how an AI could do statistics [see also  here ].  I was especially excited about your suggestion for new statistical methods using “a language-like approach to recursively creating new models from a specified list of distributions and transformations, and an automatic approach to checking model fit.”


Your discussion of these ideas was exciting to me and my colleagues because we recently  did some work  taking a step in this direction, automatically searching through a grammar over Gaussian process regression models.


Roger Grosse previously  did the same thing , but over matrix decomposition models using held-out predictive likelihood to check model fit.


These are both examples of automatic Bayesian model-building by a search over more and more complex models, as you suggested.  One nice thing is that both grammars include lots of standard models for free, and they seem to work pretty well, although the</p><p>5 0.1747674 <a title="1740-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-03-New_New_York_data_research_organizations.html">1297 andrew gelman stats-2012-05-03-New New York data research organizations</a></p>
<p>Introduction: In a single day, New York City obtained two data analysis/statistics/machine learning organizations:
  
  Microsoft Research  New York City with  John Langford  (machine learning),  Duncan Watts  (networks), and  Dave Pennock  (algorithmic economics). 
  eBay technology center  focusing on data – led by  Chris Dixon , the co-founder of the recommendation engine company Hunch, which has recently been acquired by eBay. 
  
New York already has Facebook’s engineering  unit , Twitter’s East Coast  headquarters , and Google’s  second-largest  engineering office.
 
The data community here is on an upswing, and it might be one of the best places to be if you’re into applied statistics, machine learning or data analysis.
 
Post by  Aleks Jakulin .
 
P.S. (from Andrew):  The formerly-Yahoo-now-Microsoft researchers have a more-or-less formal connection to Columbia, through the Applied Statistics Center, where some of them will be organizing occasional mini-conferences and workshops!</p><p>6 0.16689508 <a title="1740-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-10-Cross-validation_and_Bayesian_estimation_of_tuning_parameters.html">2129 andrew gelman stats-2013-12-10-Cross-validation and Bayesian estimation of tuning parameters</a></p>
<p>7 0.16473351 <a title="1740-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-18-Postdoc_positions_at_Microsoft_Research_%E2%80%93_NYC.html">1630 andrew gelman stats-2012-12-18-Postdoc positions at Microsoft Research – NYC</a></p>
<p>8 0.16271034 <a title="1740-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-21-Workshop_for_Women_in_Machine_Learning.html">1992 andrew gelman stats-2013-08-21-Workshop for Women in Machine Learning</a></p>
<p>9 0.15651402 <a title="1740-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>10 0.15372163 <a title="1740-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-17-Job_opening_at_new_%E2%80%9Cbig_data%E2%80%9D_consulting_firm%21.html">1902 andrew gelman stats-2013-06-17-Job opening at new “big data” consulting firm!</a></p>
<p>11 0.15254402 <a title="1740-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-18-Bob_on_Stan.html">1126 andrew gelman stats-2012-01-18-Bob on Stan</a></p>
<p>12 0.14992581 <a title="1740-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-21-The_future_%28and_past%29_of_statistical_sciences.html">2072 andrew gelman stats-2013-10-21-The future (and past) of statistical sciences</a></p>
<p>13 0.143474 <a title="1740-tfidf-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-09-On_deck_this_week.html">2366 andrew gelman stats-2014-06-09-On deck this week</a></p>
<p>14 0.14246646 <a title="1740-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-24-Non-Bayesian_analysis_of_Bayesian_agents%3F.html">1280 andrew gelman stats-2012-04-24-Non-Bayesian analysis of Bayesian agents?</a></p>
<p>15 0.13844813 <a title="1740-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>16 0.13835245 <a title="1740-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>17 0.13712069 <a title="1740-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-10-Jobs_in_statistics_research%21__In_New_Jersey%21.html">1110 andrew gelman stats-2012-01-10-Jobs in statistics research!  In New Jersey!</a></p>
<p>18 0.12703209 <a title="1740-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<p>19 0.12522167 <a title="1740-tfidf-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-03-How_best_to_learn_R%3F.html">65 andrew gelman stats-2010-06-03-How best to learn R?</a></p>
<p>20 0.12122904 <a title="1740-tfidf-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-27-Nothing_is_Linear%2C_Nothing_is_Additive%3A_Bayesian_Models_for_Interactions_in_Social_Science.html">165 andrew gelman stats-2010-07-27-Nothing is Linear, Nothing is Additive: Bayesian Models for Interactions in Social Science</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.229), (1, 0.068), (2, -0.097), (3, 0.044), (4, -0.007), (5, 0.089), (6, -0.127), (7, -0.002), (8, 0.037), (9, 0.081), (10, -0.039), (11, 0.028), (12, -0.023), (13, 0.0), (14, -0.065), (15, -0.022), (16, -0.011), (17, -0.031), (18, 0.002), (19, -0.034), (20, 0.031), (21, -0.083), (22, -0.012), (23, 0.044), (24, -0.022), (25, 0.038), (26, -0.032), (27, -0.001), (28, -0.024), (29, -0.012), (30, 0.001), (31, 0.011), (32, -0.005), (33, -0.025), (34, 0.01), (35, -0.029), (36, 0.013), (37, 0.012), (38, -0.063), (39, -0.032), (40, -0.054), (41, -0.031), (42, -0.002), (43, 0.056), (44, 0.02), (45, 0.026), (46, 0.044), (47, 0.007), (48, 0.058), (49, -0.06)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96110475 <a title="1740-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-%E2%80%9CIs_machine_learning_a_subset_of_statistics%3F%E2%80%9D.html">1740 andrew gelman stats-2013-02-26-“Is machine learning a subset of statistics?”</a></p>
<p>Introduction: Following up on our  previous post , Andrew Wilson writes:
  
I agree we are in a really exciting time for statistics and machine learning.  There has been a lot of talk lately comparing machine learning with statistics.  I am curious whether you think there are many fundamental differences between the fields, or just superficial differences — different popular approximate inference methods, slightly different popular application areas, etc.  Is machine learning a subset of statistics?


In the paper we discuss how we think machine learning is fundamentally about pattern discovery, and ultimately, fully automating the learning and decision making process.  In other words, whatever a human does when he or she uses tools to analyze data, can be written down algorithmically and automated on a computer.  I am not sure if the ambitions are similar in statistics — and I don’t have any conventional statistics background, which makes it harder to tell. I think it’s an interesting discussion.</p><p>2 0.81528157 <a title="1740-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-04-Model_checking_and_model_understanding_in_machine_learning.html">1482 andrew gelman stats-2012-09-04-Model checking and model understanding in machine learning</a></p>
<p>Introduction: Last month I  wrote :
  
Computer scientists are often brilliant but they can be unfamiliar with what is done in the worlds of data collection and analysis. This goes the other way too: statisticians such as myself can look pretty awkward, reinventing (or failing to reinvent) various wheels when we write computer programs or, even worse, try to design software.Andrew MacNamara writes:
  
Andrew MacNamara followed up with some thoughts:
  
I [MacNamara] had some basic statistics training through my MBA program, after having completed an undergrad degree in computer science. Since then I’ve been very interested in learning more about statistical techniques, including things like GLM and censored data analyses as well as machine learning topics like neural nets, SVMs, etc. I began following your blog after some research into Bayesian analysis topics and I am trying to dig deeper on that side of things.


One thing I have noticed is that there seems to be a distinction between data analysi</p><p>3 0.78663844 <a title="1740-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>Introduction: David Duvenaud writes:
  
I’ve been following  your recent discussions  about how an AI could do statistics [see also  here ].  I was especially excited about your suggestion for new statistical methods using “a language-like approach to recursively creating new models from a specified list of distributions and transformations, and an automatic approach to checking model fit.”


Your discussion of these ideas was exciting to me and my colleagues because we recently  did some work  taking a step in this direction, automatically searching through a grammar over Gaussian process regression models.


Roger Grosse previously  did the same thing , but over matrix decomposition models using held-out predictive likelihood to check model fit.


These are both examples of automatic Bayesian model-building by a search over more and more complex models, as you suggested.  One nice thing is that both grammars include lots of standard models for free, and they seem to work pretty well, although the</p><p>4 0.77688146 <a title="1740-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-04-When_is_there_%E2%80%9Chidden_structure_in_data%E2%80%9D_to_be_discovered%3F.html">1788 andrew gelman stats-2013-04-04-When is there “hidden structure in data” to be discovered?</a></p>
<p>Introduction: Michael Collins sent along the following announcement for a talk:
  
Fast learning algorithms for discovering the hidden structure in data


Daniel Hsu, Microsoft Research


11am, Wednesday April 10th, Interschool lab, 7th floor CEPSR, Columbia University


A major challenge in machine learning is to reliably and automatically discover hidden structure in data with minimal human intervention.  For instance, one may be interested in understanding the stratification of a population into subgroups, the thematic make-up of a collection of documents, or the dynamical process governing a complex time series.  Many of the core statistical estimation problems for these applications are, in general, provably intractable for both computational and statistical reasons; and therefore progress is made by shifting the focus to realistic instances that rule out the intractable cases.  In this talk, I’ll describe a general computational approach for correctly estimating a wide class of statistical mod</p><p>5 0.72982156 <a title="1740-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-16-How_do_we_choose_our_default_methods%3F.html">1859 andrew gelman stats-2013-05-16-How do we choose our default methods?</a></p>
<p>Introduction: I was asked to write an article for the Committee of Presidents of Statistical Societies (COPSS) 50th anniversary volume.   Here it is  (it’s labeled as “Chapter 1,” which isn’t right; that’s just what came out when I used the template that was supplied).  The article begins as follows:
  
The field of statistics continues to be divided into competing schools of thought. In theory one might imagine choosing the uniquely best method for each problem as it arises, but in practice we choose for ourselves (and recom- mend to others) default principles, models, and methods to be used in a wide variety of settings. This article briefly considers the informal criteria we use to decide what methods to use and what principles to apply in statistics problems.
  
And then I follow up with these sections:
  
Statistics: the science of defaults


Ways of knowing


The pluralist’s dilemma
  
And here’s the concluding paragraph:
  
Statistics is a young science in which progress is being made in many</p><p>6 0.72786587 <a title="1740-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-13-Duke_postdoctoral_fellowships_in_nonparametric_Bayes_%26_high-dimensional_data.html">903 andrew gelman stats-2011-09-13-Duke postdoctoral fellowships in nonparametric Bayes & high-dimensional data</a></p>
<p>7 0.72782522 <a title="1740-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-23-When_are_complicated_models_helpful_in_psychology_research_and_when_are_they_overkill%3F.html">1690 andrew gelman stats-2013-01-23-When are complicated models helpful in psychology research and when are they overkill?</a></p>
<p>8 0.72757584 <a title="1740-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-26-Bad_news_about_%28some%29_statisticians.html">1282 andrew gelman stats-2012-04-26-Bad news about (some) statisticians</a></p>
<p>9 0.72401851 <a title="1740-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-27-Should_statistics_have_a_Nobel_prize%3F.html">2151 andrew gelman stats-2013-12-27-Should statistics have a Nobel prize?</a></p>
<p>10 0.71608496 <a title="1740-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-21-The_future_%28and_past%29_of_statistical_sciences.html">2072 andrew gelman stats-2013-10-21-The future (and past) of statistical sciences</a></p>
<p>11 0.71197087 <a title="1740-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-14-Everyone%E2%80%99s_trading_bias_for_variance_at_some_point%2C_it%E2%80%99s_just_done_at_different_places_in_the_analyses.html">1763 andrew gelman stats-2013-03-14-Everyone’s trading bias for variance at some point, it’s just done at different places in the analyses</a></p>
<p>12 0.71148926 <a title="1740-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-01-Tukey%E2%80%99s_philosophy.html">496 andrew gelman stats-2011-01-01-Tukey’s philosophy</a></p>
<p>13 0.71060663 <a title="1740-lsi-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-17-Probability-processing_hardware.html">214 andrew gelman stats-2010-08-17-Probability-processing hardware</a></p>
<p>14 0.70926106 <a title="1740-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-21-Derman%2C_Rodrik_and_the_nature_of_statistical_models.html">1076 andrew gelman stats-2011-12-21-Derman, Rodrik and the nature of statistical models</a></p>
<p>15 0.70607048 <a title="1740-lsi-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-19-David_Blackwell.html">155 andrew gelman stats-2010-07-19-David Blackwell</a></p>
<p>16 0.68940276 <a title="1740-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-25-What_should_be_in_a_machine_learning_course%3F.html">1956 andrew gelman stats-2013-07-25-What should be in a machine learning course?</a></p>
<p>17 0.68463457 <a title="1740-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-03-New_New_York_data_research_organizations.html">1297 andrew gelman stats-2012-05-03-New New York data research organizations</a></p>
<p>18 0.68340689 <a title="1740-lsi-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-19-Just_chaid.html">421 andrew gelman stats-2010-11-19-Just chaid</a></p>
<p>19 0.68323421 <a title="1740-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-11-Toward_a_framework_for_automatic_model_building.html">1718 andrew gelman stats-2013-02-11-Toward a framework for automatic model building</a></p>
<p>20 0.68304324 <a title="1740-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-19-Demystifying_Blup.html">1270 andrew gelman stats-2012-04-19-Demystifying Blup</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.06), (5, 0.025), (9, 0.016), (15, 0.014), (16, 0.056), (21, 0.011), (24, 0.131), (66, 0.024), (73, 0.016), (84, 0.02), (86, 0.068), (95, 0.018), (99, 0.438)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99042571 <a title="1740-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-%E2%80%9CIs_machine_learning_a_subset_of_statistics%3F%E2%80%9D.html">1740 andrew gelman stats-2013-02-26-“Is machine learning a subset of statistics?”</a></p>
<p>Introduction: Following up on our  previous post , Andrew Wilson writes:
  
I agree we are in a really exciting time for statistics and machine learning.  There has been a lot of talk lately comparing machine learning with statistics.  I am curious whether you think there are many fundamental differences between the fields, or just superficial differences — different popular approximate inference methods, slightly different popular application areas, etc.  Is machine learning a subset of statistics?


In the paper we discuss how we think machine learning is fundamentally about pattern discovery, and ultimately, fully automating the learning and decision making process.  In other words, whatever a human does when he or she uses tools to analyze data, can be written down algorithmically and automated on a computer.  I am not sure if the ambitions are similar in statistics — and I don’t have any conventional statistics background, which makes it harder to tell. I think it’s an interesting discussion.</p><p>2 0.9883222 <a title="1740-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-14-Another_day%2C_another_stats_postdoc.html">906 andrew gelman stats-2011-09-14-Another day, another stats postdoc</a></p>
<p>Introduction: This post is from Phil Price.  I work in the Environmental Energy Technologies Division at Lawrence Berkeley National Laboratory, and I am looking for a postdoc who knows substantially more than I do about time-series modeling; in practice this probably means someone whose dissertation work involved that sort of thing.  The work involves developing models to predict and/or forecast the time-dependent energy use in buildings, given historical data and some covariates such as outdoor temperature.  Simple regression approaches (e.g. using time-of-week indicator variables, plus outdoor temperature) work fine for a lot of things, but we still have a variety of problems.  To give one example, sometimes building behavior changes — due to retrofits, or a change in occupant behavior — so that a single model won’t fit well over a long time period. We want to recognize these changes automatically .  We have many other issues besides: heteroskedasticity, need for good uncertainty estimates, abilit</p><p>3 0.98744011 <a title="1740-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-30-Works_well_versus_well_understood.html">738 andrew gelman stats-2011-05-30-Works well versus well understood</a></p>
<p>Introduction: John Cook  discusses  the John Tukey quote, “The test of a good procedure is how well it works, not how well it is understood.”  Cook writes:
  
At some level, it’s hard to argue against this. Statistical procedures operate on empirical data, so it makes sense that the procedures themselves be evaluated empirically.


But I [Cook] question whether we really know that a statistical procedure works well if it isn’t well understood. Specifically, I’m skeptical of complex statistical methods whose only credentials are a handful of simulations. “We don’t have any theoretical results, buy hey, it works well in practice. Just look at the simulations.”


Every method works well on the scenarios its author publishes, almost by definition. If the method didn’t handle a scenario well, the author would publish a different scenario.
  
I agree with Cook but would give a slightly different emphasis.  I’d say that a lot of methods can work when they are done well.  See the second meta-principle liste</p><p>4 0.98587555 <a title="1740-lda-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-03-He_doesn%E2%80%99t_trust_the_fit_._._._r%3D.999.html">315 andrew gelman stats-2010-10-03-He doesn’t trust the fit . . . r=.999</a></p>
<p>Introduction: I received the following question from an education researcher:
  
 
I was wondering if I could ask you a question about an HLM model I’m working on.  The basic design is that we have 5 years of 8th grade student achievement data (standardized test scores, this is the dependent variable), 4th grade test scores, demographics (e.g., gender and ethnicity) and status wrt special ed or ELL, etc..  In addition, we have some school- or second-level information such as school averages of the student information, type of school (grade configuration), enrollment and so.  In total there are thousands of students and many schools over the 5 years of information.


The model we’re using is quite parsimonious, using only 7 student-level effects and 4 school-level effects.   What’s puzzling us is that the correlation between predicted and actual is unrealistically high…r=0.999.   We’re using the HPMIXED procedure in SAS but that shouldn’t matter.  By dropping variables, obviously we can get the corre</p><p>5 0.98565209 <a title="1740-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-05-A_statistician_rereads_Bill_James.html">697 andrew gelman stats-2011-05-05-A statistician rereads Bill James</a></p>
<p>Introduction: Ben Lindbergh invited me to write an article for Baseball Prospectus.  I first sent him  this item  on the differences between baseball and politics but he said it was too political for them.  I then sent him  this review  of a book on baseball’s greatest fielders but he said they already had someone slotted to review that book.  Then I sent him some reflections on the great Bill James and  he published it !  If anybody out there knows Bill James, please send this on to him:  I have some questions at the end that I’m curious about.
 
Here’s how it begins:
  
 
I read my first Bill James book in 1984, took my first statistics class in 1985, and began graduate study in statistics the next year. Besides giving me the opportunity to study with the best applied statistician of the late 20th century (Don Rubin) and the best theoretical statistician of the early 21st (Xiao-Li Meng), going to graduate school at Harvard in 1986 gave me the opportunity to sit in a basement room one evening that</p><p>6 0.9849031 <a title="1740-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-10-That_controversial_claim_that_high_genetic_diversity%2C_or_low_genetic_diversity%2C_is_bad_for_the_economy.html">1665 andrew gelman stats-2013-01-10-That controversial claim that high genetic diversity, or low genetic diversity, is bad for the economy</a></p>
<p>7 0.9844442 <a title="1740-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-13-Ross_Ihaka_to_R%3A__Drop_Dead.html">272 andrew gelman stats-2010-09-13-Ross Ihaka to R:  Drop Dead</a></p>
<p>8 0.98387975 <a title="1740-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-14-As_the_saying_goes%2C_when_they_argue_that_you%E2%80%99re_taking_over%2C_that%E2%80%99s_when_you_know_you%E2%80%99ve_won.html">611 andrew gelman stats-2011-03-14-As the saying goes, when they argue that you’re taking over, that’s when you know you’ve won</a></p>
<p>9 0.98228836 <a title="1740-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-05-A_locally_organized_online_BDA_course_on_G%2B_hangout%3F.html">2009 andrew gelman stats-2013-09-05-A locally organized online BDA course on G+ hangout?</a></p>
<p>10 0.98183811 <a title="1740-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-22-Is_it_meaningful_to_talk_about_a_probability_of_%E2%80%9C65.7%25%E2%80%9D_that_Obama_will_win_the_election%3F.html">1544 andrew gelman stats-2012-10-22-Is it meaningful to talk about a probability of “65.7%” that Obama will win the election?</a></p>
<p>11 0.98164833 <a title="1740-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-26-Lottery_probability_update.html">731 andrew gelman stats-2011-05-26-Lottery probability update</a></p>
<p>12 0.981493 <a title="1740-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-07-Feedback_on_my_Bayesian_Data_Analysis_class_at_Columbia.html">1611 andrew gelman stats-2012-12-07-Feedback on my Bayesian Data Analysis class at Columbia</a></p>
<p>13 0.98117638 <a title="1740-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-16-Question_6_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1323 andrew gelman stats-2012-05-16-Question 6 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>14 0.98105907 <a title="1740-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-17-Data_problems%2C_coding_errors%E2%80%A6what_can_be_done%3F.html">1807 andrew gelman stats-2013-04-17-Data problems, coding errors…what can be done?</a></p>
<p>15 0.98101455 <a title="1740-lda-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-01-Martin_and_Liu%3A__Probabilistic_inference_based_on_consistency_of_model_with_data.html">1095 andrew gelman stats-2012-01-01-Martin and Liu:  Probabilistic inference based on consistency of model with data</a></p>
<p>16 0.98096895 <a title="1740-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-26-Any_good_articles_on_the_use_of_error_bars%3F.html">822 andrew gelman stats-2011-07-26-Any good articles on the use of error bars?</a></p>
<p>17 0.98080993 <a title="1740-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-14-Statistics_for_firefighters%3A__update.html">1722 andrew gelman stats-2013-02-14-Statistics for firefighters:  update</a></p>
<p>18 0.98067302 <a title="1740-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<p>19 0.9803074 <a title="1740-lda-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-21-Random_matrices_in_the_news.html">2258 andrew gelman stats-2014-03-21-Random matrices in the news</a></p>
<p>20 0.98009372 <a title="1740-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-27-Setting_up_Jitts_online.html">2041 andrew gelman stats-2013-09-27-Setting up Jitts online</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
