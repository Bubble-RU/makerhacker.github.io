<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2140 andrew gelman stats-2013-12-19-Revised evidence for statistical standards</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-2140" href="#">andrew_gelman_stats-2013-2140</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2140 andrew gelman stats-2013-12-19-Revised evidence for statistical standards</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-2140-html" href="http://andrewgelman.com/2013/12/19/revised-evidence-statistical-standards/">html</a></p><p>Introduction: X  and I heard about  this  much-publicized recent paper by Val Johnson, who suggests changing the default level of statistical significance from z=2 to z=3 (or, as he puts it, going from p=.05 to p=.005 or .001).  Val argues that you need to go out to 3 standard errors to get a Bayes factor of 25 or 50 in favor of the alternative hypothesis.  I don’t really buy this, first because Val’s model is a weird (to me) mixture of two point masses, which he creates in order to make a minimax argument, and second because I don’t see why you need a Bayes factor of 25 to 50 in order to make a claim.  I’d think that a factor of 5:1, say, provides strong information already—if you really believe those odds.  The real issue, as I see it, is that we’re getting Bayes factors and posterior probabilities we don’t believe, because we’re assuming flat priors that don’t really make sense.  This is a topic that’s come up over and over in recent months on this blog, for example in this discussion of why I  d</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 X  and I heard about  this  much-publicized recent paper by Val Johnson, who suggests changing the default level of statistical significance from z=2 to z=3 (or, as he puts it, going from p=. [sent-1, score-0.299]
</p><p>2 Val argues that you need to go out to 3 standard errors to get a Bayes factor of 25 or 50 in favor of the alternative hypothesis. [sent-5, score-0.366]
</p><p>3 I don’t really buy this, first because Val’s model is a weird (to me) mixture of two point masses, which he creates in order to make a minimax argument, and second because I don’t see why you need a Bayes factor of 25 to 50 in order to make a claim. [sent-6, score-0.428]
</p><p>4 I’d think that a factor of 5:1, say, provides strong information already—if you really believe those odds. [sent-7, score-0.242]
</p><p>5 The real issue, as I see it, is that we’re getting Bayes factors and posterior probabilities we don’t believe, because we’re assuming flat priors that don’t really make sense. [sent-8, score-0.44]
</p><p>6 05 standard for significance with the more stringent p = 0. [sent-14, score-0.316]
</p><p>7 Ultimately such decisions should depend on costs, benefits, and probabilities of all outcomes. [sent-17, score-0.147]
</p><p>8 Johnson’s minimax prior is not intended to correspond to any distribution of effect sizes; rather it represents a worst-case scenario under some mathematical assumptions. [sent-18, score-0.565]
</p><p>9 Johnson’s evidence threshold is chosen relative to a conventional value, namely Jeffreys’ target Bayes factor of 1/25 or 1/50, for which we do not see any particular justification except with reference to the tail-area probability of 0. [sent-20, score-0.588]
</p><p>10 To understand the difficulty of this approach, consider the hypothetical scenario in which R. [sent-22, score-0.14]
</p><p>11 In this alternative history, the discrepancy between p-values and Bayes factors remains and Johnson could have written a paper noting that the accepted 0. [sent-27, score-0.324]
</p><p>12 005 standard fails to correspond to 200-to-1 evidence against the null. [sent-28, score-0.28]
</p><p>13 Indeed, a 200:1 evidence in a minimax sense gets processed by his fixed-point equation γ = exp[z*sqrt(2 log(γ)) − log(γ)] at the value γ = 0. [sent-29, score-0.408]
</p><p>14 Moreover, the proposition approximately divides any small initial p-level by a factor of sqrt(−4π log p), roughly equal to 10 for the p’s of interest. [sent-35, score-0.551]
</p><p>15 005 stems from taking 1/20 as a starting point; p = 0. [sent-37, score-0.069]
</p><p>16 005 has no justification on its own (any more than does the p = 0. [sent-38, score-0.102]
</p><p>17 0005 threshold derived from the alternative default standard of 1/200). [sent-39, score-0.403]
</p><p>18 05 rule that has caused so many problems in later decades? [sent-41, score-0.068]
</p><p>19 We would argue that the appropriate significance level depends on the scenario, and that what worked well for agricultural experiments in the 1920s might not be so appropriate for many applications in modern biosciences. [sent-42, score-0.343]
</p><p>20 Thus, Johnson’s recommendation to rethink significance thresholds seems like a good idea that needs to include assessments of actual costs, benefits, and probabilities, rather than being based on an abstract calculation. [sent-43, score-0.284]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('johnson', 0.364), ('val', 0.283), ('minimax', 0.255), ('bayes', 0.225), ('factor', 0.173), ('log', 0.172), ('significance', 0.162), ('sqrt', 0.153), ('probabilities', 0.147), ('scenario', 0.14), ('threshold', 0.134), ('flat', 0.133), ('justification', 0.102), ('alternative', 0.101), ('factors', 0.1), ('correspond', 0.095), ('evidence', 0.093), ('standard', 0.092), ('fisher', 0.09), ('chosen', 0.086), ('approximately', 0.08), ('costs', 0.079), ('default', 0.076), ('prior', 0.075), ('valen', 0.073), ('benefits', 0.072), ('stems', 0.069), ('believe', 0.069), ('rule', 0.068), ('proposition', 0.066), ('stimulation', 0.066), ('masses', 0.066), ('jeffreys', 0.066), ('rethink', 0.064), ('discrepancy', 0.062), ('exp', 0.062), ('stringent', 0.062), ('appropriate', 0.061), ('paper', 0.061), ('settle', 0.06), ('processed', 0.06), ('divides', 0.06), ('posterior', 0.06), ('agricultural', 0.059), ('horribly', 0.059), ('earnings', 0.059), ('suffers', 0.059), ('expresses', 0.059), ('usual', 0.059), ('thresholds', 0.058)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="2140-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-19-Revised_evidence_for_statistical_standards.html">2140 andrew gelman stats-2013-12-19-Revised evidence for statistical standards</a></p>
<p>Introduction: X  and I heard about  this  much-publicized recent paper by Val Johnson, who suggests changing the default level of statistical significance from z=2 to z=3 (or, as he puts it, going from p=.05 to p=.005 or .001).  Val argues that you need to go out to 3 standard errors to get a Bayes factor of 25 or 50 in favor of the alternative hypothesis.  I don’t really buy this, first because Val’s model is a weird (to me) mixture of two point masses, which he creates in order to make a minimax argument, and second because I don’t see why you need a Bayes factor of 25 to 50 in order to make a claim.  I’d think that a factor of 5:1, say, provides strong information already—if you really believe those odds.  The real issue, as I see it, is that we’re getting Bayes factors and posterior probabilities we don’t believe, because we’re assuming flat priors that don’t really make sense.  This is a topic that’s come up over and over in recent months on this blog, for example in this discussion of why I  d</p><p>2 0.49864289 <a title="2140-tfidf-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-25-Revised_statistical_standards_for_evidence_%28comments_to_Val_Johnson%E2%80%99s_comments_on_our_comments_on_Val%E2%80%99s_comments_on_p-values%29.html">2305 andrew gelman stats-2014-04-25-Revised statistical standards for evidence (comments to Val Johnson’s comments on our comments on Val’s comments on p-values)</a></p>
<p>Introduction: As regular readers of this blog are aware, a few months ago Val Johnson  published  an article, “Revised standards for statistical evidence,” making a Bayesian argument that researchers and journals should use a p=0.005 publication threshold rather than the usual p=0.05.
 
Christian Robert and I were unconvinced by Val’s reasoning and wrote a  response , “Revised evidence for statistical standards,” in which we wrote:
  
Johnson’s minimax prior is not intended to correspond to any distribution of effect sizes; rather, it represents a worst case scenario under some mathematical assumptions. Minimax and tradeoffs do well together, and it is hard for us to see how any worst case procedure can supply much guidance on how to balance between two different losses. . . .


We would argue that the appropriate significance level depends on the scenario and that what worked well for agricultural experiments in the 1920s might not be so appropriate for many applications in modern biosciences . . .</p><p>3 0.33157128 <a title="2140-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>Introduction: In response to the  discussion  of X and me of his recent  paper , Val Johnson writes:
  
I would like to thank Andrew for forwarding his comments on uniformly most powerful Bayesian tests (UMPBTs) to me and his invitation to respond to them.  I think he  (and also Christian Robert) raise a number of interesting points concerning this new class of Bayesian tests, but I think that they may have confounded several issues that might more usefully be examined separately.


The first issue involves the choice of the Bayesian evidence threshold, gamma, used in rejecting a null hypothesis in favor of an alternative hypothesis.  Andrew objects to the higher values of gamma proposed in my recent PNAS article on grounds that too many important scientific effects would be missed if thresholds of 25-50 were routinely used.  These evidence thresholds correspond roughly to p-values of 0.005; Andrew suggests that evidence thresholds around 5 should continue to be used (gamma=5 corresponds approximate</p><p>4 0.25233898 <a title="2140-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-15-Wacky_priors_can_work_well%3F.html">1723 andrew gelman stats-2013-02-15-Wacky priors can work well?</a></p>
<p>Introduction: Dave Judkins writes:
  
I would love to see a blog entry on  this article , Bayesian Model Selection in High-Dimensional Settings, by Valen Johnson and David Rossell.  The simulation results are very encouraging although the choice of colors for some of the graphics is unfortunate.  Unless I am colorblind in some way that I am unaware of, they have two thin charcoal lines that are indistinguishable.
  
When Dave Judkins puts in a request, I’ll respond.  Also, I’m always happy to see a new Val Johnson paper.  Val and I are contemporaries—he and I got our PhD’s at around the same time, with both of us working on Bayesian image reconstruction, then in the early 1990s Val was part of the legendary group at Duke’s Institute of Statistics and Decision Sciences—a veritable ’27 Yankees featuring Mike West, Merlise Clyde, Michael Lavine, Dave Higdon, Peter Mueller, Val, and a bunch of others.  I always thought it was too bad they all had to go their separate ways.
 
Val also wrote two classic p</p><p>5 0.23050542 <a title="2140-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-10-Bringing_Causal_Models_Into_the_Mainstream.html">703 andrew gelman stats-2011-05-10-Bringing Causal Models Into the Mainstream</a></p>
<p>Introduction: John Johnson writes  at the Statistics Forum.</p><p>6 0.16101253 <a title="2140-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>7 0.16067484 <a title="2140-tfidf-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-24-%E2%80%9CEdlin%E2%80%99s_rule%E2%80%9D_for_routinely_scaling_down_published_estimates.html">2223 andrew gelman stats-2014-02-24-“Edlin’s rule” for routinely scaling down published estimates</a></p>
<p>8 0.15017581 <a title="2140-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-21-More_on_Bayesian_model_selection_in_high-dimensional_settings.html">1817 andrew gelman stats-2013-04-21-More on Bayesian model selection in high-dimensional settings</a></p>
<p>9 0.1382369 <a title="2140-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>10 0.13565713 <a title="2140-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-08-Statistical_significance_and_the_dangerous_lure_of_certainty.html">1974 andrew gelman stats-2013-08-08-Statistical significance and the dangerous lure of certainty</a></p>
<p>11 0.12860514 <a title="2140-tfidf-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Ken_Rice_presents_a_unifying_approach_to_statistical_inference_and_hypothesis_testing.html">2312 andrew gelman stats-2014-04-29-Ken Rice presents a unifying approach to statistical inference and hypothesis testing</a></p>
<p>12 0.1277013 <a title="2140-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>13 0.12533185 <a title="2140-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>14 0.12452625 <a title="2140-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-21-Readings_for_a_two-week_segment_on_Bayesian_modeling%3F.html">1586 andrew gelman stats-2012-11-21-Readings for a two-week segment on Bayesian modeling?</a></p>
<p>15 0.11202852 <a title="2140-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-11-One_more_time_on_that_ESP_study%3A__The_problem_of_overestimates_and_the_shrinkage_solution.html">511 andrew gelman stats-2011-01-11-One more time on that ESP study:  The problem of overestimates and the shrinkage solution</a></p>
<p>16 0.11193901 <a title="2140-tfidf-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-22-Philosophy_of_Bayes_and_non-Bayes%3A__A_dialogue_with_Deborah_Mayo.html">291 andrew gelman stats-2010-09-22-Philosophy of Bayes and non-Bayes:  A dialogue with Deborah Mayo</a></p>
<p>17 0.11114446 <a title="2140-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-03-Statistical_methods_that_work_in_some_settings_but_not_others.html">1560 andrew gelman stats-2012-11-03-Statistical methods that work in some settings but not others</a></p>
<p>18 0.11113585 <a title="2140-tfidf-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-04-26-Bayes_in_the_news%E2%80%A6in_a_somewhat_frustrating_way.html">3 andrew gelman stats-2010-04-26-Bayes in the news…in a somewhat frustrating way</a></p>
<p>19 0.11093384 <a title="2140-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-13-On_the_half-Cauchy_prior_for_a_global_scale_parameter.html">801 andrew gelman stats-2011-07-13-On the half-Cauchy prior for a global scale parameter</a></p>
<p>20 0.11017384 <a title="2140-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.236), (1, 0.082), (2, 0.018), (3, -0.055), (4, -0.059), (5, -0.067), (6, 0.067), (7, 0.048), (8, -0.063), (9, -0.063), (10, -0.037), (11, 0.005), (12, 0.026), (13, -0.013), (14, 0.011), (15, -0.026), (16, -0.03), (17, 0.039), (18, -0.0), (19, -0.012), (20, 0.021), (21, 0.015), (22, -0.041), (23, 0.012), (24, -0.014), (25, 0.004), (26, 0.037), (27, 0.018), (28, 0.009), (29, -0.025), (30, -0.027), (31, 0.035), (32, 0.024), (33, 0.014), (34, -0.028), (35, -0.047), (36, 0.023), (37, -0.015), (38, 0.029), (39, 0.01), (40, -0.098), (41, -0.062), (42, -0.01), (43, 0.08), (44, 0.035), (45, -0.006), (46, -0.049), (47, -0.043), (48, 0.002), (49, 0.016)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94627988 <a title="2140-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-19-Revised_evidence_for_statistical_standards.html">2140 andrew gelman stats-2013-12-19-Revised evidence for statistical standards</a></p>
<p>Introduction: X  and I heard about  this  much-publicized recent paper by Val Johnson, who suggests changing the default level of statistical significance from z=2 to z=3 (or, as he puts it, going from p=.05 to p=.005 or .001).  Val argues that you need to go out to 3 standard errors to get a Bayes factor of 25 or 50 in favor of the alternative hypothesis.  I don’t really buy this, first because Val’s model is a weird (to me) mixture of two point masses, which he creates in order to make a minimax argument, and second because I don’t see why you need a Bayes factor of 25 to 50 in order to make a claim.  I’d think that a factor of 5:1, say, provides strong information already—if you really believe those odds.  The real issue, as I see it, is that we’re getting Bayes factors and posterior probabilities we don’t believe, because we’re assuming flat priors that don’t really make sense.  This is a topic that’s come up over and over in recent months on this blog, for example in this discussion of why I  d</p><p>2 0.89929157 <a title="2140-lsi-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-25-Revised_statistical_standards_for_evidence_%28comments_to_Val_Johnson%E2%80%99s_comments_on_our_comments_on_Val%E2%80%99s_comments_on_p-values%29.html">2305 andrew gelman stats-2014-04-25-Revised statistical standards for evidence (comments to Val Johnson’s comments on our comments on Val’s comments on p-values)</a></p>
<p>Introduction: As regular readers of this blog are aware, a few months ago Val Johnson  published  an article, “Revised standards for statistical evidence,” making a Bayesian argument that researchers and journals should use a p=0.005 publication threshold rather than the usual p=0.05.
 
Christian Robert and I were unconvinced by Val’s reasoning and wrote a  response , “Revised evidence for statistical standards,” in which we wrote:
  
Johnson’s minimax prior is not intended to correspond to any distribution of effect sizes; rather, it represents a worst case scenario under some mathematical assumptions. Minimax and tradeoffs do well together, and it is hard for us to see how any worst case procedure can supply much guidance on how to balance between two different losses. . . .


We would argue that the appropriate significance level depends on the scenario and that what worked well for agricultural experiments in the 1920s might not be so appropriate for many applications in modern biosciences . . .</p><p>3 0.86851066 <a title="2140-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>Introduction: In response to the  discussion  of X and me of his recent  paper , Val Johnson writes:
  
I would like to thank Andrew for forwarding his comments on uniformly most powerful Bayesian tests (UMPBTs) to me and his invitation to respond to them.  I think he  (and also Christian Robert) raise a number of interesting points concerning this new class of Bayesian tests, but I think that they may have confounded several issues that might more usefully be examined separately.


The first issue involves the choice of the Bayesian evidence threshold, gamma, used in rejecting a null hypothesis in favor of an alternative hypothesis.  Andrew objects to the higher values of gamma proposed in my recent PNAS article on grounds that too many important scientific effects would be missed if thresholds of 25-50 were routinely used.  These evidence thresholds correspond roughly to p-values of 0.005; Andrew suggests that evidence thresholds around 5 should continue to be used (gamma=5 corresponds approximate</p><p>4 0.77465904 <a title="2140-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-08-P-values_and_statistical_practice.html">1713 andrew gelman stats-2013-02-08-P-values and statistical practice</a></p>
<p>Introduction: From  my new article  in the journal Epidemiology:
  
Sander Greenland and Charles Poole accept that P values are here to stay but recognize that some of their most common interpretations have problems. The casual view of the P value as posterior probability of the truth of the null hypothesis is false and not even close to valid under any reasonable model, yet this misunderstanding persists even in high-stakes settings (as discussed, for example, by Greenland in 2011). The formal view of the P value as a probability conditional on the null is mathematically correct but typically irrelevant to research goals (hence, the popularity of alternative—if wrong—interpretations). A Bayesian interpretation based on a spike-and-slab model makes little sense in applied contexts in epidemiology, political science, and other fields in which true effects are typically nonzero and bounded (thus violating both the “spike” and the “slab” parts of the model).


I find Greenland and Poole’s perspective t</p><p>5 0.72326201 <a title="2140-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-02-So-called_Bayesian_hypothesis_testing_is_just_as_bad_as_regular_hypothesis_testing.html">643 andrew gelman stats-2011-04-02-So-called Bayesian hypothesis testing is just as bad as regular hypothesis testing</a></p>
<p>Introduction: Steve Ziliak points me to  this article  by the always-excellent Carl Bialik, slamming hypothesis tests.  I only wish Carl had talked with me before so hastily posting, though!  I would’ve argued with some of the things in the article.  In particular, he writes:
  
Reese and Brad Carlin . . . suggest that Bayesian statistics are a better alternative, because they tackle the probability that the hypothesis is true head-on, and incorporate prior knowledge about the variables involved.
  
Brad Carlin does great work in theory, methods, and applications, and I like the bit about the prior knowledge (although I might prefer the more general phrase “additional information”), but I hate that quote!  
 
My quick response is that the hypothesis of zero effect is almost never true!  The problem with the significance testing framework–Bayesian or otherwise–is in the obsession with the possibility of an exact zero effect.  The real concern is not with zero, it’s with claiming a positive effect whe</p><p>6 0.71471864 <a title="2140-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-31-Lindley%E2%80%99s_paradox.html">1355 andrew gelman stats-2012-05-31-Lindley’s paradox</a></p>
<p>7 0.70591122 <a title="2140-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>8 0.69840342 <a title="2140-lsi-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>9 0.69754088 <a title="2140-lsi-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-03-Disconnect_between_drug_and_medical_device_approval.html">314 andrew gelman stats-2010-10-03-Disconnect between drug and medical device approval</a></p>
<p>10 0.69695979 <a title="2140-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-12-Misunderstanding_the_p-value.html">1760 andrew gelman stats-2013-03-12-Misunderstanding the p-value</a></p>
<p>11 0.69281644 <a title="2140-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-13-%E2%80%9CWhat_are_some_situations_in_which_the_classical_approach_%28or_a_naive_implementation_of_it%2C_based_on_cookbook_recipes%29_gives_worse_results_than_a_Bayesian_approach%2C_results_that_actually_impeded_the_science%3F%E2%80%9D.html">2099 andrew gelman stats-2013-11-13-“What are some situations in which the classical approach (or a naive implementation of it, based on cookbook recipes) gives worse results than a Bayesian approach, results that actually impeded the science?”</a></p>
<p>12 0.68814099 <a title="2140-lsi-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-10-Bayes_jumps_the_shark.html">331 andrew gelman stats-2010-10-10-Bayes jumps the shark</a></p>
<p>13 0.67928624 <a title="2140-lsi-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-24-%E2%80%9CEdlin%E2%80%99s_rule%E2%80%9D_for_routinely_scaling_down_published_estimates.html">2223 andrew gelman stats-2014-02-24-“Edlin’s rule” for routinely scaling down published estimates</a></p>
<p>14 0.67704993 <a title="2140-lsi-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Ken_Rice_presents_a_unifying_approach_to_statistical_inference_and_hypothesis_testing.html">2312 andrew gelman stats-2014-04-29-Ken Rice presents a unifying approach to statistical inference and hypothesis testing</a></p>
<p>15 0.67350525 <a title="2140-lsi-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-06-That_silly_ESP_paper_and_some_silliness_in_a_rebuttal_as_well.html">506 andrew gelman stats-2011-01-06-That silly ESP paper and some silliness in a rebuttal as well</a></p>
<p>16 0.66812336 <a title="2140-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-20-Non-statistical_thinking_in_the_US_foreign_policy_establishment.html">721 andrew gelman stats-2011-05-20-Non-statistical thinking in the US foreign policy establishment</a></p>
<p>17 0.65827608 <a title="2140-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-26-%E2%80%9CA_Vast_Graveyard_of_Undead_Theories%3A__Publication_Bias_and_Psychological_Science%E2%80%99s_Aversion_to_the_Null%E2%80%9D.html">1826 andrew gelman stats-2013-04-26-“A Vast Graveyard of Undead Theories:  Publication Bias and Psychological Science’s Aversion to the Null”</a></p>
<p>18 0.65487963 <a title="2140-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-12-Thinking_like_a_statistician_%28continuously%29_rather_than_like_a_civilian_%28discretely%29.html">1575 andrew gelman stats-2012-11-12-Thinking like a statistician (continuously) rather than like a civilian (discretely)</a></p>
<p>19 0.65377474 <a title="2140-lsi-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-23-Discussion_on_preregistration_of_research_studies.html">2183 andrew gelman stats-2014-01-23-Discussion on preregistration of research studies</a></p>
<p>20 0.65356958 <a title="2140-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-10-I_don%E2%80%99t_like_this_cartoon.html">1572 andrew gelman stats-2012-11-10-I don’t like this cartoon</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(7, 0.019), (15, 0.032), (16, 0.095), (21, 0.02), (24, 0.183), (34, 0.064), (42, 0.018), (49, 0.024), (53, 0.035), (65, 0.025), (69, 0.013), (84, 0.022), (86, 0.064), (95, 0.013), (99, 0.254)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97114468 <a title="2140-lda-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-25-Revised_statistical_standards_for_evidence_%28comments_to_Val_Johnson%E2%80%99s_comments_on_our_comments_on_Val%E2%80%99s_comments_on_p-values%29.html">2305 andrew gelman stats-2014-04-25-Revised statistical standards for evidence (comments to Val Johnson’s comments on our comments on Val’s comments on p-values)</a></p>
<p>Introduction: As regular readers of this blog are aware, a few months ago Val Johnson  published  an article, “Revised standards for statistical evidence,” making a Bayesian argument that researchers and journals should use a p=0.005 publication threshold rather than the usual p=0.05.
 
Christian Robert and I were unconvinced by Val’s reasoning and wrote a  response , “Revised evidence for statistical standards,” in which we wrote:
  
Johnson’s minimax prior is not intended to correspond to any distribution of effect sizes; rather, it represents a worst case scenario under some mathematical assumptions. Minimax and tradeoffs do well together, and it is hard for us to see how any worst case procedure can supply much guidance on how to balance between two different losses. . . .


We would argue that the appropriate significance level depends on the scenario and that what worked well for agricultural experiments in the 1920s might not be so appropriate for many applications in modern biosciences . . .</p><p>same-blog 2 0.96966761 <a title="2140-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-19-Revised_evidence_for_statistical_standards.html">2140 andrew gelman stats-2013-12-19-Revised evidence for statistical standards</a></p>
<p>Introduction: X  and I heard about  this  much-publicized recent paper by Val Johnson, who suggests changing the default level of statistical significance from z=2 to z=3 (or, as he puts it, going from p=.05 to p=.005 or .001).  Val argues that you need to go out to 3 standard errors to get a Bayes factor of 25 or 50 in favor of the alternative hypothesis.  I don’t really buy this, first because Val’s model is a weird (to me) mixture of two point masses, which he creates in order to make a minimax argument, and second because I don’t see why you need a Bayes factor of 25 to 50 in order to make a claim.  I’d think that a factor of 5:1, say, provides strong information already—if you really believe those odds.  The real issue, as I see it, is that we’re getting Bayes factors and posterior probabilities we don’t believe, because we’re assuming flat priors that don’t really make sense.  This is a topic that’s come up over and over in recent months on this blog, for example in this discussion of why I  d</p><p>3 0.96336782 <a title="2140-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-06-Josh_Tenenbaum_presents_._._._a_model_of_folk_physics%21.html">994 andrew gelman stats-2011-11-06-Josh Tenenbaum presents . . . a model of folk physics!</a></p>
<p>Introduction: Josh Tenenbaum describes some new work modeling people’s physical reasoning as probabilistic inferences over intuitive theories of mechanics. 
  
A general-purpose capacity for “physical intelligence”—inferring physical properties of objects and predicting future states in complex dynamical scenes—is central to how humans interpret their environment and plan safe and effective actions.  The computations and representations underlying physical intelligence remain unclear, however. Cognitive studies have focused on mapping out judgment biases and errors, or on testing simple heuristic models suitable only for highly specific cases; they have not attempted to give general-purpose unifying models.  In computer science, artificial intelligence and robotics researchers have long sought to formalize common-sense physical reasoning but without success in approaching human-level competence.  Here we show that a wide range of human physical judgments can be explained by positing an “intuitive me</p><p>4 0.96324182 <a title="2140-lda-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-09-Rasmussen_sez%3A__%E2%80%9C108%25_of_Respondents_Say_._._.%E2%80%9D.html">135 andrew gelman stats-2010-07-09-Rasmussen sez:  “108% of Respondents Say . . .”</a></p>
<p>Introduction: The recent discussion of pollsters reminded me of  a story  from a couple years ago that perhaps is still relevant . . .
 
I was looking up the governors’ popularity numbers on the web, and came across  this page from Rasmussen Reports  which shows Sarah Palin as the 3rd-most-popular governor. But then I looked more carefully. Janet Napolitano of Arizona was viewed as Excellent by 28% of respondents, Good by 27%, Fair by 26%, and Poor by 27%. That adds up to 108%! What’s going on?
 
I’d think they would have a computer program to pipe the survey results directly into the spreadsheet. But I guess not, someone must be typing in these numbers one at a time.  Another possibility is that they are altering their numbers by hand, and someone made a mistake with the Napolitano numbers, adding a few percent in one place and forgetting to subtract elsewhere.  Or maybe there’s another explanation?
  

 
P.S.  Here  are some thoughts from Mark Blumenthal
 
P.P.S.  I checked the Rasmussen link toda</p><p>5 0.963153 <a title="2140-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-02-Covariate_Adjustment_in_RCT_-_Model_Overfitting_in_Multilevel_Regression.html">936 andrew gelman stats-2011-10-02-Covariate Adjustment in RCT - Model Overfitting in Multilevel Regression</a></p>
<p>Introduction: Makoto Hanita writes:
  
We have been discussing the following two issues amongst ourselves, then with our methodological consultant for several days. However, we have not been able to arrive at a consensus. Consequently, we decided to seek an opinion from nationally known experts. FYI, we sent a similar inquiry to Larry Hedges and David Rogosa . . .  


1)      We are wondering if a post-hoc covariate adjustment is a good practice in the context of RCTs [randomized clinical trials]. We have a situation where we found a significant baseline difference between the treatment and the control groups in 3 variables. Some of us argue that adding those three variables to the original impact analysis model is a good idea, as that would remove the confound from the impact estimate. Others among us, on the other hand, argue that a post-hoc covariate adjustment should never be done, on the ground that those covariates are correlated with the treatment, which makes the analysis model that of quasi</p><p>6 0.96231627 <a title="2140-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>7 0.9567256 <a title="2140-lda-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>8 0.95433187 <a title="2140-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-The_statistical_significance_filter.html">899 andrew gelman stats-2011-09-10-The statistical significance filter</a></p>
<p>9 0.95413548 <a title="2140-lda-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-Stan_Model_of_the_Week%3A_Hierarchical_Modeling_of_Supernovas.html">2299 andrew gelman stats-2014-04-21-Stan Model of the Week: Hierarchical Modeling of Supernovas</a></p>
<p>10 0.95290637 <a title="2140-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-26-Difficulties_in_making_inferences_about_scientific_truth_from_distributions_of_published_p-values.html">2040 andrew gelman stats-2013-09-26-Difficulties in making inferences about scientific truth from distributions of published p-values</a></p>
<p>11 0.95214754 <a title="2140-lda-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-07-My_recent_debugging_experience.html">2161 andrew gelman stats-2014-01-07-My recent debugging experience</a></p>
<p>12 0.95187485 <a title="2140-lda-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-06-Bootstrap_averaging%3A_Examples_where_it_works_and_where_it_doesn%E2%80%99t_work.html">2201 andrew gelman stats-2014-02-06-Bootstrap averaging: Examples where it works and where it doesn’t work</a></p>
<p>13 0.95104092 <a title="2140-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-05-Question_26_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1367 andrew gelman stats-2012-06-05-Question 26 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>14 0.95081127 <a title="2140-lda-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-10-Small_multiples_of_lineplots_%3E_maps_%28ok%2C_not_always%2C_but_yes_in_this_case%29.html">2288 andrew gelman stats-2014-04-10-Small multiples of lineplots > maps (ok, not always, but yes in this case)</a></p>
<p>15 0.9496218 <a title="2140-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-08-A_Bayesian_approach_for_peer-review_panels%3F__and_a_speculation_about_Bruno_Frey.html">2055 andrew gelman stats-2013-10-08-A Bayesian approach for peer-review panels?  and a speculation about Bruno Frey</a></p>
<p>16 0.94955969 <a title="2140-lda-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-19-Updated_solutions_to_Bayesian_Data_Analysis_homeworks.html">42 andrew gelman stats-2010-05-19-Updated solutions to Bayesian Data Analysis homeworks</a></p>
<p>17 0.94931072 <a title="2140-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-02-Blogads_update.html">1240 andrew gelman stats-2012-04-02-Blogads update</a></p>
<p>18 0.94906962 <a title="2140-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-10-95%25_intervals_that_I_don%E2%80%99t_believe%2C_because_they%E2%80%99re_from_a_flat_prior_I_don%E2%80%99t_believe.html">1206 andrew gelman stats-2012-03-10-95% intervals that I don’t believe, because they’re from a flat prior I don’t believe</a></p>
<p>19 0.94902611 <a title="2140-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-11-Adding_an_error_model_to_a_deterministic_model.html">1162 andrew gelman stats-2012-02-11-Adding an error model to a deterministic model</a></p>
<p>20 0.94900215 <a title="2140-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-Fourteen_magic_words%3A_an_update.html">898 andrew gelman stats-2011-09-10-Fourteen magic words: an update</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
