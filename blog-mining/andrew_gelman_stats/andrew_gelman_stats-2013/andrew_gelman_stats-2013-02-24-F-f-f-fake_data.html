<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1735 andrew gelman stats-2013-02-24-F-f-f-fake data</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-1735" href="#">andrew_gelman_stats-2013-1735</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1735 andrew gelman stats-2013-02-24-F-f-f-fake data</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-1735-html" href="http://andrewgelman.com/2013/02/24/f-f-f-fake-data/">html</a></p><p>Introduction: Tiago Fragoso writes:
  
Suppose I fit a two stage regression model


Y = a + bx + e 
a = cw + d + e1


I could fit it all in one step by using MCMC for example (my model is more complicated than that, so I’ll have to do it by MCMC). However, I could fit the first regression only using MCMC because those estimates are hard to obtain and perform the second regression using least squares or a separate MCMC. 


So there’s an ‘one step’ inference based on doing it all at the same time and a ‘two step’ inference by fitting one and using the estimates on the further steps. What is gained or lost between both? Is anything done in this question?
  
My response:
 
Rather than answering your particular question, I’ll give you my generic answer, which is to simulate fake data from your model, then fit your model both ways and see how the results differ.  Repeat the simulation a few thousand times and you can make all the statistical comparisons you like.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Tiago Fragoso writes:    Suppose I fit a two stage regression model   Y = a + bx + e  a = cw + d + e1   I could fit it all in one step by using MCMC for example (my model is more complicated than that, so I’ll have to do it by MCMC). [sent-1, score-2.395]
</p><p>2 However, I could fit the first regression only using MCMC because those estimates are hard to obtain and perform the second regression using least squares or a separate MCMC. [sent-2, score-2.116]
</p><p>3 So there’s an ‘one step’ inference based on doing it all at the same time and a ‘two step’ inference by fitting one and using the estimates on the further steps. [sent-3, score-0.88]
</p><p>4 My response:   Rather than answering your particular question, I’ll give you my generic answer, which is to simulate fake data from your model, then fit your model both ways and see how the results differ. [sent-6, score-1.36]
</p><p>5 Repeat the simulation a few thousand times and you can make all the statistical comparisons you like. [sent-7, score-0.493]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mcmc', 0.416), ('fit', 0.298), ('step', 0.28), ('regression', 0.222), ('bx', 0.218), ('using', 0.193), ('model', 0.182), ('gained', 0.168), ('answering', 0.165), ('simulate', 0.165), ('estimates', 0.159), ('inference', 0.147), ('stage', 0.145), ('thousand', 0.145), ('fake', 0.145), ('squares', 0.144), ('obtain', 0.141), ('generic', 0.138), ('repeat', 0.137), ('simulation', 0.132), ('lost', 0.123), ('perform', 0.12), ('complicated', 0.116), ('separate', 0.111), ('question', 0.109), ('fitting', 0.108), ('ll', 0.105), ('comparisons', 0.098), ('two', 0.087), ('suppose', 0.085), ('ways', 0.082), ('response', 0.078), ('however', 0.078), ('times', 0.074), ('answer', 0.073), ('second', 0.072), ('done', 0.07), ('one', 0.069), ('could', 0.067), ('hard', 0.066), ('anything', 0.065), ('results', 0.064), ('least', 0.063), ('give', 0.061), ('particular', 0.06), ('based', 0.057), ('rather', 0.048), ('first', 0.045), ('statistical', 0.044), ('example', 0.038)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="1735-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-24-F-f-f-fake_data.html">1735 andrew gelman stats-2013-02-24-F-f-f-fake data</a></p>
<p>Introduction: Tiago Fragoso writes:
  
Suppose I fit a two stage regression model


Y = a + bx + e 
a = cw + d + e1


I could fit it all in one step by using MCMC for example (my model is more complicated than that, so I’ll have to do it by MCMC). However, I could fit the first regression only using MCMC because those estimates are hard to obtain and perform the second regression using least squares or a separate MCMC. 


So there’s an ‘one step’ inference based on doing it all at the same time and a ‘two step’ inference by fitting one and using the estimates on the further steps. What is gained or lost between both? Is anything done in this question?
  
My response:
 
Rather than answering your particular question, I’ll give you my generic answer, which is to simulate fake data from your model, then fit your model both ways and see how the results differ.  Repeat the simulation a few thousand times and you can make all the statistical comparisons you like.</p><p>2 0.18005039 <a title="1735-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>Introduction: Lots of good statistical methods make use of two models.  For example:
 
- Classical statistics:  estimates and standard errors using the likelihood function; tests and p-values using the sampling distribution.  (The sampling distribution is  not  equivalent to the likelihood, as has been much discussed, for example in sequential stopping problems.)
 
- Bayesian data analysis:  inference using the posterior distribution; model checking using the predictive distribution (which, again, depends on the data-generating process in a way that the likelihood does not).
 
- Machine learning:  estimation using the data; evaluation using cross-validation (which requires some rule for partitioning the data, a rule that stands outside of the data themselves).
 
- Bootstrap, jackknife, etc:  estimation using an “estimator” (which, I would argue, is based in some sense on a model for the data), uncertainties using resampling (which, I would argue, is close to the idea of a “sampling distribution” in</p><p>3 0.15762764 <a title="1735-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-19-Will_Stan_work_well_with_40%C3%9740_matrices%3F.html">861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</a></p>
<p>Introduction: Tomas Iesmantas writes:
  
I’m dealing with high dimensional (40-50 parameters) hierarchical bayesian model applied to nonlinear Poisson regression problem.


Now I’m using an adaptive version for the Metropolis adjusted Langevin algorithm with a truncated drift (Yves F. Atchade, 2003) to obtain samples from posterior.


But this algorithm is not very efficient in my case, it needs several millions iterations as burn-in period. And simulation takes quite a long time, since algorithm has to work with 40×40 matrices.


Maybe you know another MCMC algorithm which could take not so many burn-in samples and would be able to deal with nonlinear regression? In non-hierarchical nonlinear regression model adaptive metropolis algorithm is enough, but in hierarchical case I could use something more effective.
  
My reply:
 
Try fitting the model in Stan.  If that doesn’t work, let me know.</p><p>4 0.15704645 <a title="1735-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>Introduction: In response to  my remarks  on his online book, Think Bayes, Allen Downey wrote: 
   
I [Downey] have a question about one of your comments: 
   My [Gelman's] main criticism with both books is that they talk a lot about inference but not so much about model building or model checking (recall the three steps of Bayesian data analysis). I think it’s ok for an introductory book to focus on inference, which of course is central to the data-analytic process—but I’d like them to at least mention that Bayesian ideas arise in model building and model checking as well. 

This sounds like something I agree with, and one of the things I tried to do in the book is to put modeling decisions front and center.  But the word “modeling” is used in lots of ways, so I want to see if we are talking about the same thing.


For example, in many chapters, I start with a simple model of the scenario, do some analysis, then check whether the model is good enough, and iterate.  Here’s the discussion of modeling</p><p>5 0.15268701 <a title="1735-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-09-Commercial_Bayesian_inference_software_is_popping_up_all_over.html">1489 andrew gelman stats-2012-09-09-Commercial Bayesian inference software is popping up all over</a></p>
<p>Introduction: Steve Cohen writes:
  
As someone who has been working with Bayesian statistical models for the past several years, I [Cohen] have been challenged recently to describe the difference between Bayesian Networks (as implemented in BayesiaLab software) and modeling and inference using MCMC methods.


I hope you have the time to give me (or to write on your blog) and relatively simple explanation that an advanced layman could understand.
  
My reply:
 
I skimmed the above website but I couldn’t quite see what they do.  My guess is that they use MCMC and also various parametric approximations such as variational Bayes.  They also seem to have something set up for decision analysis.
 
My guess is that, compared to a general-purpose tool such as Stan, this Bayesia software is more accessible to non-academics in particular application areas (in this case, it looks like business marketing).  But I can’t be sure.
 
I’ve also heard about another company that looks to be doing something similar:  h</p><p>6 0.14725092 <a title="1735-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-05-Question_26_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1367 andrew gelman stats-2012-06-05-Question 26 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>7 0.14583057 <a title="1735-tfidf-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-31-Somewhat_Bayesian_multilevel_modeling.html">246 andrew gelman stats-2010-08-31-Somewhat Bayesian multilevel modeling</a></p>
<p>8 0.14275444 <a title="1735-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-21-D._Buggin.html">1465 andrew gelman stats-2012-08-21-D. Buggin</a></p>
<p>9 0.14202181 <a title="1735-tfidf-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-Running_into_a_Stan_Reference_by_Accident.html">2231 andrew gelman stats-2014-03-03-Running into a Stan Reference by Accident</a></p>
<p>10 0.13961866 <a title="1735-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-13-Checking_your_model_using_fake_data.html">852 andrew gelman stats-2011-08-13-Checking your model using fake data</a></p>
<p>11 0.13756596 <a title="1735-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-27-Bridges_between_deterministic_and_probabilistic_models_for_binary_data.html">780 andrew gelman stats-2011-06-27-Bridges between deterministic and probabilistic models for binary data</a></p>
<p>12 0.13613775 <a title="1735-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-28-Using_predator-prey_models_on_the_Canadian_lynx_series.html">1141 andrew gelman stats-2012-01-28-Using predator-prey models on the Canadian lynx series</a></p>
<p>13 0.1360935 <a title="1735-tfidf-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-18-Derivative-based_MCMC_as_a_breakthrough_technique_for_implementing_Bayesian_statistics.html">419 andrew gelman stats-2010-11-18-Derivative-based MCMC as a breakthrough technique for implementing Bayesian statistics</a></p>
<p>14 0.13517281 <a title="1735-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-16-%E2%80%9CReal_data_can_be_a_pain%E2%80%9D.html">1460 andrew gelman stats-2012-08-16-“Real data can be a pain”</a></p>
<p>15 0.13338956 <a title="1735-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-04-Bayesian_Learning_via_Stochastic_Gradient_Langevin_Dynamics.html">1443 andrew gelman stats-2012-08-04-Bayesian Learning via Stochastic Gradient Langevin Dynamics</a></p>
<p>16 0.12543821 <a title="1735-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-Another_reason_why_you_can_get_good_inferences_from_a_bad_model.html">1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</a></p>
<p>17 0.12434521 <a title="1735-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-What_are_the_trickiest_models_to_fit%3F.html">575 andrew gelman stats-2011-02-15-What are the trickiest models to fit?</a></p>
<p>18 0.12325312 <a title="1735-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>19 0.12117176 <a title="1735-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-07-Robust_logistic_regression.html">1886 andrew gelman stats-2013-06-07-Robust logistic regression</a></p>
<p>20 0.11993922 <a title="1735-tfidf-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-09-Special_journal_issue_on_statistical_methods_for_the_social_sciences.html">24 andrew gelman stats-2010-05-09-Special journal issue on statistical methods for the social sciences</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.192), (1, 0.165), (2, 0.051), (3, 0.023), (4, 0.091), (5, 0.036), (6, -0.028), (7, -0.061), (8, 0.136), (9, 0.023), (10, 0.062), (11, 0.045), (12, -0.063), (13, -0.003), (14, -0.059), (15, -0.006), (16, 0.021), (17, -0.026), (18, -0.032), (19, 0.021), (20, 0.019), (21, 0.004), (22, -0.003), (23, -0.091), (24, 0.004), (25, 0.022), (26, -0.033), (27, -0.091), (28, -0.005), (29, -0.002), (30, 0.022), (31, -0.016), (32, -0.02), (33, 0.019), (34, 0.046), (35, -0.011), (36, -0.037), (37, 0.005), (38, -0.029), (39, 0.001), (40, 0.01), (41, 0.084), (42, -0.033), (43, 0.006), (44, 0.082), (45, -0.039), (46, -0.056), (47, 0.02), (48, 0.064), (49, -0.023)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98163778 <a title="1735-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-24-F-f-f-fake_data.html">1735 andrew gelman stats-2013-02-24-F-f-f-fake data</a></p>
<p>Introduction: Tiago Fragoso writes:
  
Suppose I fit a two stage regression model


Y = a + bx + e 
a = cw + d + e1


I could fit it all in one step by using MCMC for example (my model is more complicated than that, so I’ll have to do it by MCMC). However, I could fit the first regression only using MCMC because those estimates are hard to obtain and perform the second regression using least squares or a separate MCMC. 


So there’s an ‘one step’ inference based on doing it all at the same time and a ‘two step’ inference by fitting one and using the estimates on the further steps. What is gained or lost between both? Is anything done in this question?
  
My response:
 
Rather than answering your particular question, I’ll give you my generic answer, which is to simulate fake data from your model, then fit your model both ways and see how the results differ.  Repeat the simulation a few thousand times and you can make all the statistical comparisons you like.</p><p>2 0.86991036 <a title="1735-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-19-Will_Stan_work_well_with_40%C3%9740_matrices%3F.html">861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</a></p>
<p>Introduction: Tomas Iesmantas writes:
  
I’m dealing with high dimensional (40-50 parameters) hierarchical bayesian model applied to nonlinear Poisson regression problem.


Now I’m using an adaptive version for the Metropolis adjusted Langevin algorithm with a truncated drift (Yves F. Atchade, 2003) to obtain samples from posterior.


But this algorithm is not very efficient in my case, it needs several millions iterations as burn-in period. And simulation takes quite a long time, since algorithm has to work with 40×40 matrices.


Maybe you know another MCMC algorithm which could take not so many burn-in samples and would be able to deal with nonlinear regression? In non-hierarchical nonlinear regression model adaptive metropolis algorithm is enough, but in hierarchical case I could use something more effective.
  
My reply:
 
Try fitting the model in Stan.  If that doesn’t work, let me know.</p><p>3 0.84937823 <a title="1735-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-13-Checking_your_model_using_fake_data.html">852 andrew gelman stats-2011-08-13-Checking your model using fake data</a></p>
<p>Introduction: Someone sent me the following email:
  
I tried to do a logistic regression . . . I programmed the model in different ways and got different answers . . . can’t get the results to match . . . What am I doing wrong? . . . Here’s my code . . . 
  
I didn’t have the time to look at his code so I gave the following general response:
 
One way to check things is to try simulating data from the fitted model, then fit your model again to the simulated data and see what happens.
 
P.S.  He followed my suggestion and responded a few days later:
  
Yeah, that did the trick!  I was treating a factor variable as a covariate!
  
I love it when generic advice works out!</p><p>4 0.78436583 <a title="1735-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-05-Xiao-Li_Meng_and_Xianchao_Xie_rethink_asymptotics.html">1406 andrew gelman stats-2012-07-05-Xiao-Li Meng and Xianchao Xie rethink asymptotics</a></p>
<p>Introduction: In  an article  catchily entitled, “I got more data, my model is more refined, but my estimator is getting worse!  Am I just dumb?”, Meng and Xie write:
  
Possibly, but more likely you are merely a victim of conventional wisdom. More data or better models by no means guarantee better estimators (e.g., with a smaller mean squared error), when you are not following probabilistically principled methods such as MLE (for large samples) or Bayesian approaches. Estimating equations are par- ticularly vulnerable in this regard, almost a necessary price for their robustness. These points will be demonstrated via common tasks of estimating regression parameters and correlations, under simple mod- els such as bivariate normal and ARCH(1). Some general strategies for detecting and avoiding such pitfalls are suggested, including checking for self-efficiency (Meng, 1994, Statistical Science) and adopting a guiding working model.


Using the example of estimating the autocorrelation ρ under a statio</p><p>5 0.78413469 <a title="1735-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-19-An_interweaving-transformation_strategy_for_boosting_MCMC_efficiency.html">964 andrew gelman stats-2011-10-19-An interweaving-transformation strategy for boosting MCMC efficiency</a></p>
<p>Introduction: Yaming Yu and Xiao-Li Meng  write in  with a cool new idea for improving the efficiency of Gibbs and Metropolis in multilevel models:
  
For a broad class of multilevel models, there exist two well-known competing parameterizations, the centered parameterization (CP) and the non-centered parameterization (NCP), for effective MCMC implementation. Much literature has been devoted to the questions of when to use which and how to compromise between them via partial CP/NCP. This article introduces an alternative strategy for boosting MCMC efficiency via simply interweaving—but not alternating—the two parameterizations. This strategy has the surprising property that failure of both the CP and NCP chains to converge geometrically does not prevent the interweaving algorithm from doing so. It achieves this seemingly magical property by taking advantage of the discordance of the two parameterizations, namely, the sufficiency of CP and the ancillarity of NCP, to substantially reduce the Markovian</p><p>6 0.78006077 <a title="1735-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-01-When_should_you_worry_about_imputed_data%3F.html">935 andrew gelman stats-2011-10-01-When should you worry about imputed data?</a></p>
<p>7 0.77934712 <a title="1735-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-24-Multilevel_modeling_and_instrumental_variables.html">1468 andrew gelman stats-2012-08-24-Multilevel modeling and instrumental variables</a></p>
<p>8 0.77689314 <a title="1735-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-28-Using_predator-prey_models_on_the_Canadian_lynx_series.html">1141 andrew gelman stats-2012-01-28-Using predator-prey models on the Canadian lynx series</a></p>
<p>9 0.77681577 <a title="1735-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>10 0.77247673 <a title="1735-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-17-Graphical_tools_for_understanding_multilevel_models.html">772 andrew gelman stats-2011-06-17-Graphical tools for understanding multilevel models</a></p>
<p>11 0.77138054 <a title="1735-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-27-Cross-validation_%28What_is_it_good_for%3F%29.html">1395 andrew gelman stats-2012-06-27-Cross-validation (What is it good for?)</a></p>
<p>12 0.76891571 <a title="1735-lsi-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-18-The_1.6_rule.html">39 andrew gelman stats-2010-05-18-The 1.6 rule</a></p>
<p>13 0.76699823 <a title="1735-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-28-Simplify_until_your_fake-data_check_works%2C_then_add_complications_until_you_can_figure_out_where_the_problem_is_coming_from.html">1875 andrew gelman stats-2013-05-28-Simplify until your fake-data check works, then add complications until you can figure out where the problem is coming from</a></p>
<p>14 0.76196277 <a title="1735-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-26-Including_interactions_or_not.html">823 andrew gelman stats-2011-07-26-Including interactions or not</a></p>
<p>15 0.75989789 <a title="1735-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>16 0.75522143 <a title="1735-lsi-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Blending_results_from_two_relatively_independent_multi-level_models.html">250 andrew gelman stats-2010-09-02-Blending results from two relatively independent multi-level models</a></p>
<p>17 0.74875516 <a title="1735-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-31-Using_factor_analysis_or_principal_components_analysis_or_measurement-error_models_for_biological_measurements_in_archaeology%3F.html">1094 andrew gelman stats-2011-12-31-Using factor analysis or principal components analysis or measurement-error models for biological measurements in archaeology?</a></p>
<p>18 0.74455136 <a title="1735-lsi-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-08-Displaying_a_fitted_multilevel_model.html">328 andrew gelman stats-2010-10-08-Displaying a fitted multilevel model</a></p>
<p>19 0.74355161 <a title="1735-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>20 0.73897809 <a title="1735-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(16, 0.028), (21, 0.189), (24, 0.159), (53, 0.024), (99, 0.468)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99159211 <a title="1735-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-20-Correcting_for_multiple_comparisons_in_a_Bayesian_regression_model.html">1989 andrew gelman stats-2013-08-20-Correcting for multiple comparisons in a Bayesian regression model</a></p>
<p>Introduction: Joe Northrup writes:
  
I have a question about correcting for multiple comparisons in a Bayesian regression model. I believe I understand the argument in  your 2012 paper  in Journal of Research on Educational Effectiveness that when you have a hierarchical model there is shrinkage of estimates towards the group-level mean and thus there is no need to add any additional penalty to correct for multiple comparisons. In my case I do not have hierarchically structured dataâ&euro;&rdquo;i.e. I have only 1 observation per group but have a categorical variable with a large number of categories. Thus, I am fitting a simple multiple regression in a Bayesian framework. Would putting a strong, mean 0, multivariate normal prior on the betas in this model accomplish the same sort of shrinkage (it seems to me that it would) and do you believe this is a valid way to address criticism of multiple comparisons in this setting?
  
My reply:  Yes, I think this makes sense.  One way to address concerns of multiple com</p><p>2 0.98955512 <a title="1735-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-15-Quote_of_the_day%3A__statisticians_and_defaults.html">147 andrew gelman stats-2010-07-15-Quote of the day:  statisticians and defaults</a></p>
<p>Introduction: On statisticians and statistical software:
  
Statisticians are particularly sensitive to default settings, which makes sense considering that statistics is, in many ways, a science based on defaults.  What is a “statistical method” if not a recommended default analysis, backed up by some combination of theory and experience?</p><p>3 0.98879808 <a title="1735-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-15-A_silly_paper_that_tries_to_make_fun_of_multilevel_models.html">854 andrew gelman stats-2011-08-15-A silly paper that tries to make fun of multilevel models</a></p>
<p>Introduction: Torkild Hovde Lyngstad writes:
  
I wondered what your reaction would be to  this paper  from a recent issue of European Political Science.


It came out already in March this year, so you might have seen it or even commented on it before. Is is a joke at the expense of the whole polisci discipline, a joke the Editors did not catch, or the sequel to the Sokal affair, just with quanto social science as the target? 
  
My reply:  Yes, several people pointed me to this article.  I don’t think it’s a hoax, it’s more of a joke:  the author is making the point that with fancy statistics you can discover all sorts of patterns that don’t make sense.  The implication, I believe, is that many patterns that social scientists  do  find through statistical analysis are not actually meaningful.
 
I agree with this point, which could be even more pithily stated as “correlation does not imply causation.”  I am irritated, however, by the singling out of multilevel models here, as the point could be mad</p><p>4 0.98831964 <a title="1735-lda-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-15-%E2%80%9C10_Things_You_Need_to_Know_About_Causal_Effects%E2%80%9D.html">1675 andrew gelman stats-2013-01-15-“10 Things You Need to Know About Causal Effects”</a></p>
<p>Introduction: Macartan Humphreys pointed me to  this excellent guide .
 
Here are the 10 items:
  
1. A causal claim is a statement about what didn’t happen. 
2. There is a fundamental problem of causal inference. 
3. You can estimate average causal effects even if you cannot observe any individual causal effects. 
4. If you know that, on average, A causes B and that B causes C, this does not mean that you know that A causes C. 
5. The counterfactual model is all about contribution, not attribution. 
6. X can cause Y even if there is no “causal path” connecting X and Y. 
7. Correlation is not causation. 
8. X can cause Y even if X is not a necessary condition or a sufficient condition for Y. 
9. Estimating average causal effects does not require that treatment and control groups are identical. 
10. There is no causation without manipulation.
  
The  article  follows with crisp discussions of each point.  My favorite is item #6, not because it’s the most important but because it brings in some real s</p><p>5 0.9876461 <a title="1735-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-27-Neumann_update.html">432 andrew gelman stats-2010-11-27-Neumann update</a></p>
<p>Introduction: Steve Hsu, who  started off  this discussion, had some comments on  my speculations  on the personality of John von Neumann and others.  Steve writes:
  
I [Hsu] actually knew Feynman a bit when I was an undergrad, and found him to be very nice to students. Since then I have heard quite a few stories from people in theoretical physics which emphasize his nastier side, and I think in the end he was quite a complicated person like everyone else.


There are a couple of pseudo-biographies of vN, but none as high quality as, e.g., Gleick’s book on Feynman or Hodges book about Turing. (Gleick studied physics as an undergrad at Harvard, and Hodges is a PhD in mathematical physics — pretty rare backgrounds for biographers!)  For example, as mentioned on the comment thread to your post, Steve Heims wrote a book about both vN and Wiener (!),  and Norman Macrae wrote a biography of vN. Both books are worth reading, but I think neither really do him justice. The breadth of vN’s work is just too m</p><p>6 0.98265588 <a title="1735-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-08-Robert_Kosara_reviews_Ed_Tufte%E2%80%99s_short_course.html">1451 andrew gelman stats-2012-08-08-Robert Kosara reviews Ed Tufte’s short course</a></p>
<p>7 0.98141813 <a title="1735-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-01-Two_Postdoc_Positions_Available_on_Bayesian_Hierarchical_Modeling.html">62 andrew gelman stats-2010-06-01-Two Postdoc Positions Available on Bayesian Hierarchical Modeling</a></p>
<p>8 0.98080903 <a title="1735-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-13-News_coverage_of_statistical_issues%E2%80%A6how_did_I_do%3F.html">514 andrew gelman stats-2011-01-13-News coverage of statistical issues…how did I do?</a></p>
<p>9 0.98057193 <a title="1735-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-13-Jim_Campbell_argues_that_Larry_Bartels%E2%80%99s_%E2%80%9CUnequal_Democracy%E2%80%9D_findings_are_not_robust.html">659 andrew gelman stats-2011-04-13-Jim Campbell argues that Larry Bartels’s “Unequal Democracy” findings are not robust</a></p>
<p>10 0.97996503 <a title="1735-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-15-Does_quantum_uncertainty_have_a_place_in_everyday_applied_statistics%3F.html">1857 andrew gelman stats-2013-05-15-Does quantum uncertainty have a place in everyday applied statistics?</a></p>
<p>11 0.97919518 <a title="1735-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-19-The_grasshopper_wins%2C_and_Greg_Mankiw%E2%80%99s_grandmother_would_be_%E2%80%9Cshocked_and_appalled%E2%80%9D_all_over_again.html">1728 andrew gelman stats-2013-02-19-The grasshopper wins, and Greg Mankiw’s grandmother would be “shocked and appalled” all over again</a></p>
<p>same-blog 12 0.97884542 <a title="1735-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-24-F-f-f-fake_data.html">1735 andrew gelman stats-2013-02-24-F-f-f-fake data</a></p>
<p>13 0.97644109 <a title="1735-lda-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-04-%E2%80%9CDogs_are_sensitive_to_small_variations_of_the_Earth%E2%80%99s_magnetic_field%E2%80%9D.html">2159 andrew gelman stats-2014-01-04-“Dogs are sensitive to small variations of the Earth’s magnetic field”</a></p>
<p>14 0.97542608 <a title="1735-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>15 0.97510707 <a title="1735-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-20-Evaluating_Columbia_University%E2%80%99s_Frontiers_of_Science_course.html">1864 andrew gelman stats-2013-05-20-Evaluating Columbia University’s Frontiers of Science course</a></p>
<p>16 0.97378027 <a title="1735-lda-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-26-Age_and_happiness%3A__The_pattern_isn%E2%80%99t_as_clear_as_you_might_think.html">486 andrew gelman stats-2010-12-26-Age and happiness:  The pattern isn’t as clear as you might think</a></p>
<p>17 0.97377813 <a title="1735-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-07-Descriptive_statistics%2C_causal_inference%2C_and_story_time.html">789 andrew gelman stats-2011-07-07-Descriptive statistics, causal inference, and story time</a></p>
<p>18 0.97321069 <a title="1735-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-25-Classical_probability_does_not_apply_to_quantum_systems_%28causal_inference_edition%29.html">2037 andrew gelman stats-2013-09-25-Classical probability does not apply to quantum systems (causal inference edition)</a></p>
<p>19 0.97250426 <a title="1735-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-31-Somewhat_Bayesian_multilevel_modeling.html">246 andrew gelman stats-2010-08-31-Somewhat Bayesian multilevel modeling</a></p>
<p>20 0.97188985 <a title="1735-lda-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-22-Please_stop_me_before_I_barf_again.html">1275 andrew gelman stats-2012-04-22-Please stop me before I barf again</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
