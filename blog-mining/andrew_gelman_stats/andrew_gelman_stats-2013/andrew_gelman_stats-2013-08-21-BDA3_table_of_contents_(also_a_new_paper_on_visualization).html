<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1991 andrew gelman stats-2013-08-21-BDA3 table of contents (also a new paper on visualization)</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2013" href="../home/andrew_gelman_stats-2013_home.html">andrew_gelman_stats-2013</a> <a title="andrew_gelman_stats-2013-1991" href="#">andrew_gelman_stats-2013-1991</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1991 andrew gelman stats-2013-08-21-BDA3 table of contents (also a new paper on visualization)</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2013-1991-html" href="http://andrewgelman.com/2013/08/21/bda3-table-of-contents-also-a-new-paper-on-visualization/">html</a></p><p>Introduction: In response to our  recent posting  of Amazon’s offer of Bayesian Data Analysis 3rd edition at 40% off, some people asked what was in this new edition, with more information beyond the beautiful cover image and the  brief paragraph  I’d posted earlier.
 
 Here’s  the table of contents.  The following sections have all-new material:
 
1.4 New introduction of BDA principles using a simple spell checking example 
2.9 Weakly informative prior distributions 
5.7 Weakly informative priors for hierarchical variance parameters 
7.1-7.4 Predictive accuracy for model evaluation and comparison 
10.6 Computing environments 
11.4 Split R-hat 
11.5 New measure of effective number of simulation draws 
13.7 Variational inference 
13.8 Expectation propagation 
13.9 Other approximations 
14.6 Regularization for regression models 
C.1 Getting started with R and Stan 
C.2 Fitting a hierarchical model in Stan 
C.4 Programming Hamiltonian Monte Carlo in R
 
And the new chapters: 
20 Basis function models 
2</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In response to our  recent posting  of Amazon’s offer of Bayesian Data Analysis 3rd edition at 40% off, some people asked what was in this new edition, with more information beyond the beautiful cover image and the  brief paragraph  I’d posted earlier. [sent-1, score-0.313]
</p><p>2 The following sections have all-new material:   1. [sent-3, score-0.077]
</p><p>3 4 New introduction of BDA principles using a simple spell checking example  2. [sent-4, score-0.167]
</p><p>4 7 Weakly informative priors for hierarchical variance parameters  7. [sent-6, score-0.307]
</p><p>5 4 Programming Hamiltonian Monte Carlo in R   And the new chapters:  20 Basis function models  21 Gaussian process models  22 Finite mixture models  23 Dirichlet process models   And there are various little changes throughout. [sent-18, score-0.981]
</p><p>6 Our visualizations follow the principle of decomposing a covariance matrix into scale parameters and correlations, pulling out marginal summaries where possible and using two and three-dimensional plots to reveal multivariate structure. [sent-20, score-1.122]
</p><p>7 Visualizing a distribution of covariance matrices is a step beyond visualizing a single covariance matrix or a single multivariate dataset. [sent-21, score-1.844]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('covariance', 0.394), ('visualizing', 0.267), ('matrices', 0.246), ('models', 0.179), ('distributions', 0.159), ('weakly', 0.148), ('edition', 0.141), ('multivariate', 0.141), ('matrix', 0.141), ('tokuda', 0.119), ('tomoki', 0.119), ('viscov', 0.119), ('decomposing', 0.112), ('informative', 0.109), ('goodrich', 0.107), ('stan', 0.104), ('hierarchical', 0.102), ('iven', 0.101), ('mechelen', 0.101), ('tuerlinckx', 0.101), ('dirichlet', 0.098), ('wishart', 0.098), ('variational', 0.096), ('parameters', 0.096), ('propagation', 0.094), ('scaled', 0.094), ('process', 0.09), ('reward', 0.089), ('francis', 0.089), ('spell', 0.089), ('beyond', 0.087), ('single', 0.087), ('environments', 0.086), ('amazon', 0.086), ('new', 0.085), ('graphing', 0.084), ('approximations', 0.084), ('van', 0.083), ('pulling', 0.083), ('regularization', 0.08), ('patient', 0.079), ('introduction', 0.078), ('summaries', 0.078), ('hamiltonian', 0.078), ('split', 0.077), ('visualizations', 0.077), ('expectation', 0.077), ('sections', 0.077), ('families', 0.077), ('bda', 0.074)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1991-tfidf-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-21-BDA3_table_of_contents_%28also_a_new_paper_on_visualization%29.html">1991 andrew gelman stats-2013-08-21-BDA3 table of contents (also a new paper on visualization)</a></p>
<p>Introduction: In response to our  recent posting  of Amazon’s offer of Bayesian Data Analysis 3rd edition at 40% off, some people asked what was in this new edition, with more information beyond the beautiful cover image and the  brief paragraph  I’d posted earlier.
 
 Here’s  the table of contents.  The following sections have all-new material:
 
1.4 New introduction of BDA principles using a simple spell checking example 
2.9 Weakly informative prior distributions 
5.7 Weakly informative priors for hierarchical variance parameters 
7.1-7.4 Predictive accuracy for model evaluation and comparison 
10.6 Computing environments 
11.4 Split R-hat 
11.5 New measure of effective number of simulation draws 
13.7 Variational inference 
13.8 Expectation propagation 
13.9 Other approximations 
14.6 Regularization for regression models 
C.1 Getting started with R and Stan 
C.2 Fitting a hierarchical model in Stan 
C.4 Programming Hamiltonian Monte Carlo in R
 
And the new chapters: 
20 Basis function models 
2</p><p>2 0.76115799 <a title="1991-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-Visualizing_Distributions_of_Covariance_Matrices.html">1477 andrew gelman stats-2012-08-30-Visualizing Distributions of Covariance Matrices</a></p>
<p>Introduction: Since weâ&euro;&trade;ve been  discussing  prior distributions on covariance matrices, I will recommend  this recent article  (coauthored with Tomoki Tokuda, Ben Goodrich, Iven Van Mechelen, and Francis Tuerlinckx) on their visualization:
  
We present some methods for graphing distributions of covariance matrices and demonstrate them on several models, including the Wishart, inverse-Wishart, and scaled inverse-Wishart families in different dimensions. Our visualizations follow the principle of decomposing a covariance matrix into scale parameters and correlations, pulling out marginal summaries where possible and using two and three-dimensional plots to reveal multivariate structure. Visualizing a distribution of covariance matrices is a step beyond visualizing a single covariance matrix or a single multivariate dataset. Our visualization methods are available through the R package VisCov.</p><p>3 0.20378457 <a title="1991-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>Introduction: I’ve had a couple of email conversations in the past couple days on dependence in multivariate prior distributions.
 
 Modeling the degrees of freedom and scale parameters in the t distribution 
 
First, in our Stan group we’ve been discussing the choice of priors for the degrees-of-freedom parameter in the t distribution.  I wrote that also there’s the question of parameterization.  It does not necessarily make sense to have independent priors on the df and scale parameters.  In some sense, the meaning of the scale parameter changes with the df.
 
 Prior dependence between correlation and scale parameters in the scaled inverse-Wishart model 
 
The second case of parameterization in prior distribution arose from an email I received from Chris Chatham pointing me to  this exploration  by Matt Simpson of the scaled inverse-Wishart prior distribution for hierarchical covariance matrices.  Simpson writes:
  
A popular prior for Σ is the inverse-Wishart distribution [ not  the same as the</p><p>4 0.2033347 <a title="1991-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-22-The_scaled_inverse_Wishart_prior_distribution_for_a_covariance_matrix_in_a_hierarchical_model.html">1466 andrew gelman stats-2012-08-22-The scaled inverse Wishart prior distribution for a covariance matrix in a hierarchical model</a></p>
<p>Introduction: Since we’re  talking  about the scaled inverse Wishart . . . here’s a recent message from Chris Chatham:
 
I have been reading your book on Bayesian Hierarchical/Multilevel Modeling but have been struggling a bit with deciding whether to model my multivariate normal distribution using the scaled inverse Wishart approach you advocate given the arguments at  this blog post  [entitled "Why an inverse-Wishart prior may not be such a good idea"].
 
My reply:  We discuss this in our book.  We know the inverse-Wishart has problems, that’s why we recommend the  scaled  inverse-Wishart, which is a more general class of models.   Here ‘s an old blog post on the topic.  And also of course there’s the description in our book.
 
Chris pointed me to the following  comment  by Simon Barthelmé:
  
Using the scaled inverse Wishart doesn’t change anything, the standard deviations of the invidual coefficients and their covariance are still dependent. My answer would be to use a prior that models the stan</p><p>5 0.19190779 <a title="1991-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-16-My_talk_in_Chicago_this_Thurs_6%3A30pm.html">1806 andrew gelman stats-2013-04-16-My talk in Chicago this Thurs 6:30pm</a></p>
<p>Introduction: Choices in Visualizing Data 
 
This time, it’s not at the university, it’s at a data science meetup.   Here are the slides .
 
I actually prefer the term “statistical graphics” or “visualizing quantitative information” rather than “visualizing data.”  I spend a lot of time graphing inferences and fitted models, understanding my fits and doing exploratory model analysis.  Graphs aren’t just for raw data.
 
P.S.  Mike Stringer, who prepared the blurb for my talk at the above link, wrote that ARM “has the most understandable description of causal inference I’ve ever read.”  I appreciate the compliment, but, to be fair, Jennifer deserves most of the credit for the  causal chapters  of that book.</p><p>6 0.18338671 <a title="1991-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-04-Handy_Matrix_Cheat_Sheet%2C_with_Gradients.html">555 andrew gelman stats-2011-02-04-Handy Matrix Cheat Sheet, with Gradients</a></p>
<p>7 0.17079081 <a title="1991-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>8 0.16904408 <a title="1991-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>9 0.16472127 <a title="1991-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-27-%E2%80%9CKeeping_things_unridiculous%E2%80%9D%3A__Berger%2C_O%E2%80%99Hagan%2C_and_me_on_weakly_informative_priors.html">1087 andrew gelman stats-2011-12-27-“Keeping things unridiculous”:  Berger, O’Hagan, and me on weakly informative priors</a></p>
<p>10 0.16218883 <a title="1991-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>11 0.15353374 <a title="1991-tfidf-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-07-My_recent_debugging_experience.html">2161 andrew gelman stats-2014-01-07-My recent debugging experience</a></p>
<p>12 0.15300326 <a title="1991-tfidf-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-18-DataMarket.html">215 andrew gelman stats-2010-08-18-DataMarket</a></p>
<p>13 0.14671686 <a title="1991-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-21-D._Buggin.html">1465 andrew gelman stats-2012-08-21-D. Buggin</a></p>
<p>14 0.14567088 <a title="1991-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-22-My_talks_that_were_scheduled_for_Tues_at_the_Data_Skeptics_meetup_and_Wed_at_the_Open_Statistical_Programming_meetup.html">1950 andrew gelman stats-2013-07-22-My talks that were scheduled for Tues at the Data Skeptics meetup and Wed at the Open Statistical Programming meetup</a></p>
<p>15 0.14263543 <a title="1991-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-29-More_by_Berger_and_me_on_weakly_informative_priors.html">1092 andrew gelman stats-2011-12-29-More by Berger and me on weakly informative priors</a></p>
<p>16 0.14249101 <a title="1991-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-06-The_new_Stan_1.1.1%2C_featuring_Gaussian_processes%21.html">1710 andrew gelman stats-2013-02-06-The new Stan 1.1.1, featuring Gaussian processes!</a></p>
<p>17 0.13131088 <a title="1991-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-03-Hierarchical_array_priors_for_ANOVA_decompositions.html">1786 andrew gelman stats-2013-04-03-Hierarchical array priors for ANOVA decompositions</a></p>
<p>18 0.12912759 <a title="1991-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-27-Visual_diagnostics_for_discrete-data_regressions.html">929 andrew gelman stats-2011-09-27-Visual diagnostics for discrete-data regressions</a></p>
<p>19 0.12860906 <a title="1991-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>20 0.12712623 <a title="1991-tfidf-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-21-Random_matrices_in_the_news.html">2258 andrew gelman stats-2014-03-21-Random matrices in the news</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.165), (1, 0.208), (2, -0.036), (3, 0.114), (4, 0.075), (5, 0.006), (6, 0.021), (7, -0.127), (8, -0.128), (9, 0.032), (10, -0.013), (11, 0.018), (12, -0.045), (13, 0.027), (14, 0.056), (15, -0.038), (16, 0.001), (17, 0.024), (18, 0.014), (19, 0.005), (20, 0.01), (21, -0.04), (22, 0.046), (23, 0.046), (24, 0.05), (25, 0.041), (26, -0.002), (27, 0.071), (28, 0.088), (29, -0.003), (30, -0.081), (31, 0.025), (32, 0.059), (33, -0.0), (34, 0.076), (35, -0.025), (36, -0.002), (37, -0.019), (38, 0.031), (39, 0.026), (40, -0.091), (41, 0.015), (42, 0.081), (43, 0.031), (44, 0.058), (45, -0.003), (46, -0.116), (47, -0.044), (48, 0.076), (49, -0.178)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96983081 <a title="1991-lsi-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-21-BDA3_table_of_contents_%28also_a_new_paper_on_visualization%29.html">1991 andrew gelman stats-2013-08-21-BDA3 table of contents (also a new paper on visualization)</a></p>
<p>Introduction: In response to our  recent posting  of Amazon’s offer of Bayesian Data Analysis 3rd edition at 40% off, some people asked what was in this new edition, with more information beyond the beautiful cover image and the  brief paragraph  I’d posted earlier.
 
 Here’s  the table of contents.  The following sections have all-new material:
 
1.4 New introduction of BDA principles using a simple spell checking example 
2.9 Weakly informative prior distributions 
5.7 Weakly informative priors for hierarchical variance parameters 
7.1-7.4 Predictive accuracy for model evaluation and comparison 
10.6 Computing environments 
11.4 Split R-hat 
11.5 New measure of effective number of simulation draws 
13.7 Variational inference 
13.8 Expectation propagation 
13.9 Other approximations 
14.6 Regularization for regression models 
C.1 Getting started with R and Stan 
C.2 Fitting a hierarchical model in Stan 
C.4 Programming Hamiltonian Monte Carlo in R
 
And the new chapters: 
20 Basis function models 
2</p><p>2 0.93721265 <a title="1991-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-Visualizing_Distributions_of_Covariance_Matrices.html">1477 andrew gelman stats-2012-08-30-Visualizing Distributions of Covariance Matrices</a></p>
<p>Introduction: Since weâ&euro;&trade;ve been  discussing  prior distributions on covariance matrices, I will recommend  this recent article  (coauthored with Tomoki Tokuda, Ben Goodrich, Iven Van Mechelen, and Francis Tuerlinckx) on their visualization:
  
We present some methods for graphing distributions of covariance matrices and demonstrate them on several models, including the Wishart, inverse-Wishart, and scaled inverse-Wishart families in different dimensions. Our visualizations follow the principle of decomposing a covariance matrix into scale parameters and correlations, pulling out marginal summaries where possible and using two and three-dimensional plots to reveal multivariate structure. Visualizing a distribution of covariance matrices is a step beyond visualizing a single covariance matrix or a single multivariate dataset. Our visualization methods are available through the R package VisCov.</p><p>3 0.81352812 <a title="1991-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-04-Handy_Matrix_Cheat_Sheet%2C_with_Gradients.html">555 andrew gelman stats-2011-02-04-Handy Matrix Cheat Sheet, with Gradients</a></p>
<p>Introduction: This post is an (unpaid) advertisement for the following extremely useful resource:
  
 Petersen, K. B. and M. S. Pedersen. 2008.   The Matrix Cookbook  .  Tehcnical Report, Technical University of Denmark. 
  
It contains 70+ pages of useful relations and derivations involving matrices.  What grabbed my eye was the computation of gradients for matrix operations ranging from eigenvalues and determinants to multivariate normal density functions.   I had no idea the multivariate normal had such a clean gradient (see section 8).
  

 
We’ve been playing around with  Hamiltonian (aka Hybrid) Monte Carlo  for sampling from the posterior of hierarchical generalized linear models with lots of interactions.  HMC speeds up Metropolis sampling by using the gradient of the log probability to drive samples in the direction of higher probability density, which is particularly useful for correlated parameters that mix slowly with standard Gibbs sampling.   Matt “III” Hoffman ‘s already got it workin</p><p>4 0.739465 <a title="1991-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<p>Introduction: Tomas Iesmantas had  asked me  for advice on a regression problem with 50 parameters, and I’d recommended Hamiltonian Monte Carlo.
 
A few weeks later he reported back: 
  
  
After trying several modifications (HMC for all parameters at once, HMC just for first level parameters and Riemman manifold Hamiltonian Monte Carlo method), I finally got it running with HMC just for first level parameters and for others using direct sampling, since conditional distributions turned out to have closed form.


However, even in this case it is quite tricky, since I had to employ mass matrix and not just diagonal but at the beginning of algorithm generated it randomly (ensuring it is positive definite). Such random generation of mass matrix is quite blind step, but it proved to be quite helpful.


Riemman manifold HMC is quite vagarious, or to be more specific, metric of manifold is very sensitive. In my model log-likelihood I had exponents and values of metrics matrix elements was very large and wh</p><p>5 0.70759648 <a title="1991-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-23-Learning_Differential_Geometry_for_Hamiltonian_Monte_Carlo.html">1339 andrew gelman stats-2012-05-23-Learning Differential Geometry for Hamiltonian Monte Carlo</a></p>
<p>Introduction: You can get a taste of Hamiltonian Monte Carlo (HMC) by reading the very gentle introduction in David MacKay’s general text on information theory:
  
  MacKay, D.  2003.    Information Theory, Inference, and Learning Algorithms  .  Cambridge University Press.  [see Chapter 31, which is relatively standalone and can be downloaded separately.]
   
Follow this up with Radford Neal’s much more thorough introduction to HMC:
  
 Neal, R. 2011.   MCMC Using Hamiltonian Dynamics .  In Brooks, Gelman, Jones and Meng, eds.,  Handbook of Markov Chain Monte Carlo .  Chapman and Hall/CRC Press.
   
To understand why HMC works and set yourself on the path to understanding generalizations like  Riemann manifold HMC , you’ll need to know a bit about differential geometry.  I really liked the combination of these two books:
  
  Magnus, J. R. and H. Neudecker.  2007.   Matrix Differential Calculus with Application in Statistics and Econometrics .  3rd Edition.  Wiley?
   
and
  
  Leimkuhler, B. and S.</p><p>6 0.69838411 <a title="1991-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-19-R_package_for_Bayes_factors.html">1682 andrew gelman stats-2013-01-19-R package for Bayes factors</a></p>
<p>7 0.68484318 <a title="1991-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-15-Prior_Selection_for_Vector_Autoregressions.html">1674 andrew gelman stats-2013-01-15-Prior Selection for Vector Autoregressions</a></p>
<p>8 0.65150732 <a title="1991-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-24-Bleg%3A_Automatic_Differentiation_for_Log_Prob_Gradients%3F.html">535 andrew gelman stats-2011-01-24-Bleg: Automatic Differentiation for Log Prob Gradients?</a></p>
<p>9 0.6430372 <a title="1991-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-22-The_scaled_inverse_Wishart_prior_distribution_for_a_covariance_matrix_in_a_hierarchical_model.html">1466 andrew gelman stats-2012-08-22-The scaled inverse Wishart prior distribution for a covariance matrix in a hierarchical model</a></p>
<p>10 0.64205557 <a title="1991-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-21-Handbook_of_Markov_Chain_Monte_Carlo.html">674 andrew gelman stats-2011-04-21-Handbook of Markov Chain Monte Carlo</a></p>
<p>11 0.63878709 <a title="1991-lsi-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-Running_into_a_Stan_Reference_by_Accident.html">2231 andrew gelman stats-2014-03-03-Running into a Stan Reference by Accident</a></p>
<p>12 0.63450456 <a title="1991-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>13 0.62835062 <a title="1991-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-06-The_new_Stan_1.1.1%2C_featuring_Gaussian_processes%21.html">1710 andrew gelman stats-2013-02-06-The new Stan 1.1.1, featuring Gaussian processes!</a></p>
<p>14 0.60938525 <a title="1991-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-03-Hierarchical_array_priors_for_ANOVA_decompositions.html">1786 andrew gelman stats-2013-04-03-Hierarchical array priors for ANOVA decompositions</a></p>
<p>15 0.60846889 <a title="1991-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-06-Stan_1.2.0_and_RStan_1.2.0.html">1753 andrew gelman stats-2013-03-06-Stan 1.2.0 and RStan 1.2.0</a></p>
<p>16 0.60237956 <a title="1991-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-24-New_ideas_on_DIC_from_Martyn_Plummer_and_Sumio_Watanabe.html">778 andrew gelman stats-2011-06-24-New ideas on DIC from Martyn Plummer and Sumio Watanabe</a></p>
<p>17 0.59956241 <a title="1991-lsi-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-31-The_most-cited_statistics_papers_ever.html">2277 andrew gelman stats-2014-03-31-The most-cited statistics papers ever</a></p>
<p>18 0.59945703 <a title="1991-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-30-Infill_asymptotics_and_sprawl_asymptotics.html">1877 andrew gelman stats-2013-05-30-Infill asymptotics and sprawl asymptotics</a></p>
<p>19 0.59347349 <a title="1991-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-04-A_new_R_package_for_fititng_multilevel_models.html">501 andrew gelman stats-2011-01-04-A new R package for fititng multilevel models</a></p>
<p>20 0.59245425 <a title="1991-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(11, 0.019), (16, 0.054), (21, 0.025), (24, 0.178), (34, 0.046), (66, 0.028), (77, 0.017), (84, 0.032), (86, 0.037), (89, 0.168), (98, 0.033), (99, 0.23)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96485627 <a title="1991-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-05-Wouldn%E2%80%99t_it_be_cool_if_Glenn_Hubbard_were_consulting_for_Herbalife_and_I_were_on_the_other_side%3F.html">1708 andrew gelman stats-2013-02-05-Wouldn’t it be cool if Glenn Hubbard were consulting for Herbalife and I were on the other side?</a></p>
<p>Introduction: I remember in 4th grade or so, the teacher would give us a list of vocabulary words each week and we’d have to show we learned them by using each in a sentence.  We quickly got bored and decided to do the assignment by writing a single sentence using all ten words.  (Which the teacher hated, of course.)
 
The above headline is in that spirit, combining  blog   posts  rather than vocabulary words.
 
But that only uses two of the entries.  To really do the job, I’d need to throw in bivariate associations, ecological fallacies, high-dimensional feature selection, statistical significance, the suddenly unpopular name Hilary, snotty reviewers, the contagion of obesity, and milk-related spam.
 
Or we could bring in some of the all-time favorites, such as Bayesians, economists, Finland, beautiful parents and their daughters, goofy graphics, red and blue states, essentialism in children’s reasoning, chess running, and zombies.  Putting 8 of these in a single sentence (along with Glenn Hubbard</p><p>same-blog 2 0.96181726 <a title="1991-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-21-BDA3_table_of_contents_%28also_a_new_paper_on_visualization%29.html">1991 andrew gelman stats-2013-08-21-BDA3 table of contents (also a new paper on visualization)</a></p>
<p>Introduction: In response to our  recent posting  of Amazon’s offer of Bayesian Data Analysis 3rd edition at 40% off, some people asked what was in this new edition, with more information beyond the beautiful cover image and the  brief paragraph  I’d posted earlier.
 
 Here’s  the table of contents.  The following sections have all-new material:
 
1.4 New introduction of BDA principles using a simple spell checking example 
2.9 Weakly informative prior distributions 
5.7 Weakly informative priors for hierarchical variance parameters 
7.1-7.4 Predictive accuracy for model evaluation and comparison 
10.6 Computing environments 
11.4 Split R-hat 
11.5 New measure of effective number of simulation draws 
13.7 Variational inference 
13.8 Expectation propagation 
13.9 Other approximations 
14.6 Regularization for regression models 
C.1 Getting started with R and Stan 
C.2 Fitting a hierarchical model in Stan 
C.4 Programming Hamiltonian Monte Carlo in R
 
And the new chapters: 
20 Basis function models 
2</p><p>3 0.95685089 <a title="1991-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-31-Untunable_Metropolis.html">833 andrew gelman stats-2011-07-31-Untunable Metropolis</a></p>
<p>Introduction: Michael Margolis writes:
  
What are we to make of it when a Metropolis-Hastings step just won’t tune? That is, the acceptance rate is zero at expected-jump-size X, and way above 1/2 at X-exp(-16) (i.e.,  machine precision ).


I’ve solved my practical problem by writing that I would have liked to include results from a diffuse prior, but couldn’t. But I’m bothered by the poverty of my intuition. And since everything I’ve read says this is an issue of efficiency, rather than accuracy, I wonder if I could solve it just by running massive and heavily thinned chains.
  
My reply:
 
I can’t see how this could happen in a well-specified problem!  I suspect it’s a bug.  Otherwise try rescaling your variables so that your parameters will have values on the order of magnitude of 1.
 
To which Margolis responded:
  
I hardly wrote any of the code, so I can’t speak to the bug question — it’s binomial kriging from the R package geoRglm. And there are no covariates to scale — just the zero and one</p><p>4 0.95421791 <a title="1991-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-Visualizing_Distributions_of_Covariance_Matrices.html">1477 andrew gelman stats-2012-08-30-Visualizing Distributions of Covariance Matrices</a></p>
<p>Introduction: Since weâ&euro;&trade;ve been  discussing  prior distributions on covariance matrices, I will recommend  this recent article  (coauthored with Tomoki Tokuda, Ben Goodrich, Iven Van Mechelen, and Francis Tuerlinckx) on their visualization:
  
We present some methods for graphing distributions of covariance matrices and demonstrate them on several models, including the Wishart, inverse-Wishart, and scaled inverse-Wishart families in different dimensions. Our visualizations follow the principle of decomposing a covariance matrix into scale parameters and correlations, pulling out marginal summaries where possible and using two and three-dimensional plots to reveal multivariate structure. Visualizing a distribution of covariance matrices is a step beyond visualizing a single covariance matrix or a single multivariate dataset. Our visualization methods are available through the R package VisCov.</p><p>5 0.94990814 <a title="1991-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-16-The_%E2%80%9Chot_hand%E2%80%9D_and_problems_with_hypothesis_testing.html">1215 andrew gelman stats-2012-03-16-The “hot hand” and problems with hypothesis testing</a></p>
<p>Introduction: Gur Yaari  writes :
  
Anyone who has ever watched a sports competition is familiar with expressions like “on fire”, “in the zone”, “on a roll”, “momentum” and so on. But what do these expressions really mean? In 1985 when Thomas Gilovich, Robert Vallone and Amos Tversky  studied  this phenomenon for the first time, they defined it as: “. . . these phrases express a belief that the performance of a player during a particular period is significantly better than expected on the basis of the player’s overall record”. Their conclusion was that what people tend to perceive as a “hot hand” is essentially a cognitive illusion caused by a misperception of random sequences. Until recently there was little, if any, evidence to rule out their conclusion. Increased computing power and new data availability from various sports now provide surprising evidence of this phenomenon, thus reigniting the debate.
  
Yaari goes on to some studies that have found time dependence in basketball, baseball, voll</p><p>6 0.94493383 <a title="1991-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-10-He_said_he_was_sorry.html">1756 andrew gelman stats-2013-03-10-He said he was sorry</a></p>
<p>7 0.94372523 <a title="1991-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-11-Data_Visualization_vs._Statistical_Graphics.html">407 andrew gelman stats-2010-11-11-Data Visualization vs. Statistical Graphics</a></p>
<p>8 0.93749642 <a title="1991-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-09-Familial_Linkage_between_Neuropsychiatric_Disorders_and_Intellectual_Interests.html">1160 andrew gelman stats-2012-02-09-Familial Linkage between Neuropsychiatric Disorders and Intellectual Interests</a></p>
<p>9 0.9353478 <a title="1991-lda-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-11-The_myth_of_the_myth_of_the_myth_of_the_hot_hand.html">2243 andrew gelman stats-2014-03-11-The myth of the myth of the myth of the hot hand</a></p>
<p>10 0.92818862 <a title="1991-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-10-I_don%E2%80%99t_like_this_cartoon.html">1572 andrew gelman stats-2012-11-10-I don’t like this cartoon</a></p>
<p>11 0.91709834 <a title="1991-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-14-Question_4_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1320 andrew gelman stats-2012-05-14-Question 4 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>12 0.913481 <a title="1991-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-01-Don%E2%80%99t_let_your_standard_errors_drive_your_research_agenda.html">1702 andrew gelman stats-2013-02-01-Don’t let your standard errors drive your research agenda</a></p>
<p>13 0.91082811 <a title="1991-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-24-Yet_another_Bayesian_job_opportunity.html">231 andrew gelman stats-2010-08-24-Yet another Bayesian job opportunity</a></p>
<p>14 0.90699446 <a title="1991-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-09-The_boxer%2C_the_wrestler%2C_and_the_coin_flip%2C_again.html">566 andrew gelman stats-2011-02-09-The boxer, the wrestler, and the coin flip, again</a></p>
<p>15 0.9001171 <a title="1991-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-09-Solve_mazes_by_starting_at_the_exit.html">459 andrew gelman stats-2010-12-09-Solve mazes by starting at the exit</a></p>
<p>16 0.89554471 <a title="1991-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-17-Statistics_in_a_world_where_nothing_is_random.html">1628 andrew gelman stats-2012-12-17-Statistics in a world where nothing is random</a></p>
<p>17 0.89511788 <a title="1991-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-16-Stantastic%21.html">1580 andrew gelman stats-2012-11-16-Stantastic!</a></p>
<p>18 0.89441395 <a title="1991-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-23-Traditionalist_claims_that_modern_art_could_just_as_well_be_replaced_by_a_%E2%80%9Cpaint-throwing_chimp%E2%80%9D.html">1390 andrew gelman stats-2012-06-23-Traditionalist claims that modern art could just as well be replaced by a “paint-throwing chimp”</a></p>
<p>19 0.89407039 <a title="1991-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-17-Weak_identification_provides_partial_information.html">1903 andrew gelman stats-2013-06-17-Weak identification provides partial information</a></p>
<p>20 0.89375705 <a title="1991-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-21-Baseball%E2%80%99s_greatest_fielders.html">623 andrew gelman stats-2011-03-21-Baseball’s greatest fielders</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
