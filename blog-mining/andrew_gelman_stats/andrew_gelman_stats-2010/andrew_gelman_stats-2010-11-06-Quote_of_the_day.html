<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>398 andrew gelman stats-2010-11-06-Quote of the day</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-398" href="#">andrew_gelman_stats-2010-398</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>398 andrew gelman stats-2010-11-06-Quote of the day</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-398-html" href="http://andrewgelman.com/2010/11/06/quote_of_the_da_1/">html</a></p><p>Introduction: “A statistical model is usually taken to be summarized by a likelihood, or a likelihood and a prior distribution, but we go an extra step by noting that the parameters of a model are typically batched, and we take this batching as an essential part of the model.”</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 “A statistical model is usually taken to be summarized by a likelihood, or a likelihood and a prior distribution, but we go an extra step by noting that the parameters of a model are typically batched, and we take this batching as an essential part of the model. [sent-1, score-3.843]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('batching', 0.444), ('likelihood', 0.391), ('summarized', 0.357), ('noting', 0.322), ('essential', 0.288), ('extra', 0.245), ('taken', 0.181), ('step', 0.179), ('parameters', 0.178), ('model', 0.175), ('typically', 0.17), ('usually', 0.168), ('distribution', 0.154), ('prior', 0.151), ('part', 0.12), ('take', 0.114), ('go', 0.1), ('statistical', 0.085)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="398-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-06-Quote_of_the_day.html">398 andrew gelman stats-2010-11-06-Quote of the day</a></p>
<p>Introduction: “A statistical model is usually taken to be summarized by a likelihood, or a likelihood and a prior distribution, but we go an extra step by noting that the parameters of a model are typically batched, and we take this batching as an essential part of the model.”</p><p>2 0.18123178 <a title="398-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-31-Using_sample_size_in_the_prior_distribution.html">547 andrew gelman stats-2011-01-31-Using sample size in the prior distribution</a></p>
<p>Introduction: Mike McLaughlin writes:
  
Consider the Seeds example in vol. 1 of the BUGS examples.  There, a binomial likelihood has a p parameter constructed, via logit, from two covariates.  What I am wondering is: Would it be legitimate, in a binomial + logit problem like this, to allow binomial p[i] to be a function of the corresponding n[i] or would that amount to using the data in the prior?  In other words, in the context of the Seeds example, is r[] the only data or is n[] data as well and therefore not permissible in a prior formulation?


I [McLaughlin] currently have a model with a common beta prior for all p[i] but would like to mitigate this commonality (a kind of James-Stein effect) when there are lots of observations for some i.  But this seems to feed the data back into the prior.  Does it really?


It also occurs to me [McLaughlin] that, perhaps, a binomial likelihood is not the one to use here (not flexible enough).
  
My reply:
 
Strictly speaking, “n” is data, and so what you wa</p><p>3 0.16858289 <a title="398-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>Introduction: For awhile I’ve been fitting most of my multilevel models using lmer/glmer, which gives point estimates of the group-level variance parameters (maximum marginal likelihood estimate for lmer and an approximation for glmer).  I’m usually satisfied with this–sure, point estimation understates the uncertainty in model fitting, but that’s typically the least of our worries. 
 
Sometimes, though, lmer/glmer estimates group-level variances at 0 or estimates group-level correlation parameters at +/- 1.  Typically, when this happens, it’s not that we’re so sure the variance is close to zero or that the correlation is close to 1 or -1; rather, the marginal likelihood does not provide a lot of information about these parameters of the group-level error distribution.
 
I don’t want point estimates on the boundary.  I don’t want to say that the unexplained variance in some dimension is exactly zero.
 
One way to handle this problem is full Bayes:  slap a prior on sigma, do your Gibbs and Metropolis</p><p>4 0.16237609 <a title="398-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>Introduction: I’ve had a couple of email conversations in the past couple days on dependence in multivariate prior distributions.
 
 Modeling the degrees of freedom and scale parameters in the t distribution 
 
First, in our Stan group we’ve been discussing the choice of priors for the degrees-of-freedom parameter in the t distribution.  I wrote that also there’s the question of parameterization.  It does not necessarily make sense to have independent priors on the df and scale parameters.  In some sense, the meaning of the scale parameter changes with the df.
 
 Prior dependence between correlation and scale parameters in the scaled inverse-Wishart model 
 
The second case of parameterization in prior distribution arose from an email I received from Chris Chatham pointing me to  this exploration  by Matt Simpson of the scaled inverse-Wishart prior distribution for hierarchical covariance matrices.  Simpson writes:
  
A popular prior for Σ is the inverse-Wishart distribution [ not  the same as the</p><p>5 0.15732108 <a title="398-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-20-Likelihood_thresholds_and_decisions.html">1422 andrew gelman stats-2012-07-20-Likelihood thresholds and decisions</a></p>
<p>Introduction: David Hogg points me to  this  discussion:
  
Martin Strasbourg and I [Hogg] discussed his project to detect new satellites of M31 in the PAndAS survey. He can construct a likelihood ratio (possibly even a marginalized likelihood ratio) at every position in the M31 imaging, between the best-fit satellite-plus-background model and the best nothing-plus-background model. He can make a two-dimensional map of these likelihood ratios and show a the histogram of them. Looking at this histogram, which has a tail to very large ratios, he asked me, where should I put my cut? That is, at what likelihood ratio does a candidate deserve follow-up? Here’s my unsatisfying answer:


To a statistician, the distribution of likelihood ratios is interesting and valuable to study. To an astronomer, it is uninteresting. You don’t want to know the distribution of likelihoods, you want to find satellites . . .
  
I wrote that I think this makes sense and that it would actualy be an interesting and useful rese</p><p>6 0.15520874 <a title="398-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>7 0.14929464 <a title="398-tfidf-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-01-How_does_Bayes_do_it%3F.html">247 andrew gelman stats-2010-09-01-How does Bayes do it?</a></p>
<p>8 0.14239669 <a title="398-tfidf-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>9 0.13567451 <a title="398-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>10 0.13023084 <a title="398-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>11 0.11688837 <a title="398-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>12 0.11346442 <a title="398-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>13 0.11229108 <a title="398-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>14 0.11184394 <a title="398-tfidf-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-01-bayesglm_in_Stata%3F.html">442 andrew gelman stats-2010-12-01-bayesglm in Stata?</a></p>
<p>15 0.11002759 <a title="398-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>16 0.10895004 <a title="398-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-02-Fighting_a_losing_battle.html">1518 andrew gelman stats-2012-10-02-Fighting a losing battle</a></p>
<p>17 0.10783512 <a title="398-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>18 0.10749643 <a title="398-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>19 0.10682081 <a title="398-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-06-Early_stopping_and_penalized_likelihood.html">788 andrew gelman stats-2011-07-06-Early stopping and penalized likelihood</a></p>
<p>20 0.10643288 <a title="398-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-28-Understanding_simulations_in_terms_of_predictive_inference%3F.html">1287 andrew gelman stats-2012-04-28-Understanding simulations in terms of predictive inference?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.099), (1, 0.167), (2, 0.019), (3, 0.066), (4, -0.03), (5, -0.023), (6, 0.081), (7, 0.011), (8, -0.053), (9, 0.037), (10, 0.015), (11, 0.022), (12, -0.018), (13, 0.001), (14, -0.078), (15, -0.051), (16, 0.003), (17, -0.01), (18, 0.017), (19, -0.018), (20, 0.037), (21, -0.056), (22, 0.002), (23, -0.027), (24, -0.023), (25, 0.033), (26, -0.02), (27, 0.008), (28, 0.027), (29, 0.001), (30, -0.056), (31, -0.01), (32, -0.006), (33, 0.046), (34, 0.012), (35, 0.044), (36, -0.022), (37, -0.013), (38, -0.017), (39, -0.004), (40, 0.038), (41, 0.019), (42, -0.026), (43, 0.026), (44, 0.049), (45, -0.001), (46, 0.007), (47, -0.01), (48, 0.018), (49, 0.018)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96974671 <a title="398-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-06-Quote_of_the_day.html">398 andrew gelman stats-2010-11-06-Quote of the day</a></p>
<p>Introduction: “A statistical model is usually taken to be summarized by a likelihood, or a likelihood and a prior distribution, but we go an extra step by noting that the parameters of a model are typically batched, and we take this batching as an essential part of the model.”</p><p>2 0.80507606 <a title="398-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>Introduction: I’ve had a couple of email conversations in the past couple days on dependence in multivariate prior distributions.
 
 Modeling the degrees of freedom and scale parameters in the t distribution 
 
First, in our Stan group we’ve been discussing the choice of priors for the degrees-of-freedom parameter in the t distribution.  I wrote that also there’s the question of parameterization.  It does not necessarily make sense to have independent priors on the df and scale parameters.  In some sense, the meaning of the scale parameter changes with the df.
 
 Prior dependence between correlation and scale parameters in the scaled inverse-Wishart model 
 
The second case of parameterization in prior distribution arose from an email I received from Chris Chatham pointing me to  this exploration  by Matt Simpson of the scaled inverse-Wishart prior distribution for hierarchical covariance matrices.  Simpson writes:
  
A popular prior for Σ is the inverse-Wishart distribution [ not  the same as the</p><p>3 0.80080992 <a title="398-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-31-Using_sample_size_in_the_prior_distribution.html">547 andrew gelman stats-2011-01-31-Using sample size in the prior distribution</a></p>
<p>Introduction: Mike McLaughlin writes:
  
Consider the Seeds example in vol. 1 of the BUGS examples.  There, a binomial likelihood has a p parameter constructed, via logit, from two covariates.  What I am wondering is: Would it be legitimate, in a binomial + logit problem like this, to allow binomial p[i] to be a function of the corresponding n[i] or would that amount to using the data in the prior?  In other words, in the context of the Seeds example, is r[] the only data or is n[] data as well and therefore not permissible in a prior formulation?


I [McLaughlin] currently have a model with a common beta prior for all p[i] but would like to mitigate this commonality (a kind of James-Stein effect) when there are lots of observations for some i.  But this seems to feed the data back into the prior.  Does it really?


It also occurs to me [McLaughlin] that, perhaps, a binomial likelihood is not the one to use here (not flexible enough).
  
My reply:
 
Strictly speaking, “n” is data, and so what you wa</p><p>4 0.79440355 <a title="398-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>Introduction: A student writes:
  
I have a question about an earlier recommendation of yours on the election of the prior distribution for the precision hyperparameter of a normal distribution, and a reference for the recommendation. If I recall correctly I have read that you have suggested to use Gamma(1.4, 0.4) instead of Gamma(0.01,0.01) for the prior distribution of the precision hyper parameter of a normal distribution.


I would very much appreciate if you would have the time to point me to this publication of yours. The reason is that I have used the prior distribution (Gamma(1.4, 0.4)) in a study which we now revise for publication, and where a reviewer question the choice of the distribution (claiming that it is too informative!).


I am well aware of that you in recent publications (Prior distributions for variance parameters in hierarchical models. Bayesian Analysis; Data Analysis using regression and multilevel/hierarchical models) suggest to model the precision as pow(standard deviatio</p><p>5 0.76952249 <a title="398-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-28-Understanding_simulations_in_terms_of_predictive_inference%3F.html">1287 andrew gelman stats-2012-04-28-Understanding simulations in terms of predictive inference?</a></p>
<p>Introduction: David Hogg writes:
  
My (now deceased) collaborator and guru in all things inference, Sam Roweis, used to emphasize to me that we should evaluate models in the data space — not the parameter space — because models are always effectively “effective” and not really, fundamentally true. Or, in other words, models should be compared in the space of their predictions, not in the space of their parameters (the  parameters didn’t really “exist” at all for Sam).  In that spirit, when we estimate the effectiveness of a MCMC method or tuning — by autocorrelation time or ESJD or anything else — shouldn’t we be looking at the changes in the model predictions over time, rather than the changes in the parameters over time?  That is, the autocorrelation time should be the autocorrelation time in what the model (at the walker position) predicts for the data, and the ESJD should be the expected squared jump distance in what the model predicts for the data?  This might resolve the concern I expressed a</p><p>6 0.75899261 <a title="398-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>7 0.7516923 <a title="398-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-20-Prior_beliefs_about_locations_of_decision_boundaries.html">1130 andrew gelman stats-2012-01-20-Prior beliefs about locations of decision boundaries</a></p>
<p>8 0.74329484 <a title="398-lsi-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-01-bayesglm_in_Stata%3F.html">442 andrew gelman stats-2010-12-01-bayesglm in Stata?</a></p>
<p>9 0.74307406 <a title="398-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-16-%E2%80%9CReal_data_can_be_a_pain%E2%80%9D.html">1460 andrew gelman stats-2012-08-16-“Real data can be a pain”</a></p>
<p>10 0.73656213 <a title="398-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>11 0.73414481 <a title="398-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>12 0.7295714 <a title="398-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>13 0.72868466 <a title="398-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-30-David_Hogg_on_statistics.html">1401 andrew gelman stats-2012-06-30-David Hogg on statistics</a></p>
<p>14 0.72573984 <a title="398-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>15 0.72394192 <a title="398-lsi-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-23-Unhappy_with_improvement_by_a_factor_of_10%5E29.html">160 andrew gelman stats-2010-07-23-Unhappy with improvement by a factor of 10^29</a></p>
<p>16 0.72370255 <a title="398-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-19-Whassup_with_deviance_having_a_high_posterior_correlation_with_a_parameter_in_the_model%3F.html">1221 andrew gelman stats-2012-03-19-Whassup with deviance having a high posterior correlation with a parameter in the model?</a></p>
<p>17 0.72244763 <a title="398-lsi-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-04-That_half-Cauchy_prior.html">184 andrew gelman stats-2010-08-04-That half-Cauchy prior</a></p>
<p>18 0.72233671 <a title="398-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-02-Fighting_a_losing_battle.html">1518 andrew gelman stats-2012-10-02-Fighting a losing battle</a></p>
<p>19 0.71992463 <a title="398-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-21-D._Buggin.html">1465 andrew gelman stats-2012-08-21-D. Buggin</a></p>
<p>20 0.71981019 <a title="398-lsi-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(16, 0.629), (24, 0.168)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96618426 <a title="398-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-02-Classification_error.html">1745 andrew gelman stats-2013-03-02-Classification error</a></p>
<p>Introduction: 15-2040 != 19-3010  (and, for that matter, 25-1022 != 25-1063).</p><p>same-blog 2 0.96274179 <a title="398-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-06-Quote_of_the_day.html">398 andrew gelman stats-2010-11-06-Quote of the day</a></p>
<p>Introduction: “A statistical model is usually taken to be summarized by a likelihood, or a likelihood and a prior distribution, but we go an extra step by noting that the parameters of a model are typically batched, and we take this batching as an essential part of the model.”</p><p>3 0.95656645 <a title="398-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-25-Bayes_wikipedia_update.html">1026 andrew gelman stats-2011-11-25-Bayes wikipedia update</a></p>
<p>Introduction: I  checked  and somebody went in and screwed up  my fixes  to the wikipedia page on Bayesian inference.  I give up.</p><p>4 0.92324948 <a title="398-lda-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-29-Where_36%25_of_all_boys_end_up_nowadays.html">1697 andrew gelman stats-2013-01-29-Where 36% of all boys end up nowadays</a></p>
<p>Introduction: My  Take a Number feature  appears in today’s Times.  And here are the graphs that I wish they’d had space to include!
 
 
 
 
 
Original story  here .</p><p>5 0.92296606 <a title="398-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-14-Desecration_of_valuable_real_estate.html">572 andrew gelman stats-2011-02-14-Desecration of valuable real estate</a></p>
<p>Introduction: Malecki asks:
  
Is  this  the worst infographic ever to appear in NYT?  USA Today is not something to aspire to.
  
To connect to some of our recent  themes , I agree this is a pretty horrible data display.  But it’s not bad as a series of images.  Considering the competition to be a cartoon or series of photos, these images aren’t so bad.
 
One issue, I think, is that designers get credit for creativity and originality (unusual color combinations!  Histogram bars shaped like mosques!) , which is often the opposite of what we want in a clear graph.  It’s Martin Amis vs. George Orwell all over again.</p><p>6 0.87818813 <a title="398-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-16-Visualizations_of_NYPD_stop-and-frisk_data.html">1014 andrew gelman stats-2011-11-16-Visualizations of NYPD stop-and-frisk data</a></p>
<p>7 0.8593632 <a title="398-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-24-ESPN_is_looking_to_hire_a_research_analyst.html">1279 andrew gelman stats-2012-04-24-ESPN is looking to hire a research analyst</a></p>
<p>8 0.85470551 <a title="398-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-12-Where_are_the_larger-than-life_athletes%3F.html">1115 andrew gelman stats-2012-01-12-Where are the larger-than-life athletes?</a></p>
<p>9 0.84260821 <a title="398-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-21-Elevator_shame_is_a_two-way_street.html">528 andrew gelman stats-2011-01-21-Elevator shame is a two-way street</a></p>
<p>10 0.83617449 <a title="398-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-05-How_do_segregation_measures_change_when_you_change_the_level_of_aggregation%3F.html">1366 andrew gelman stats-2012-06-05-How do segregation measures change when you change the level of aggregation?</a></p>
<p>11 0.81541342 <a title="398-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-07-Some_silly_things_you_%28didn%E2%80%99t%29_miss_by_not_reading_the_sister_blog.html">1659 andrew gelman stats-2013-01-07-Some silly things you (didn’t) miss by not reading the sister blog</a></p>
<p>12 0.78802156 <a title="398-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-06-Picking_on_Stephen_Wolfram.html">1304 andrew gelman stats-2012-05-06-Picking on Stephen Wolfram</a></p>
<p>13 0.77507079 <a title="398-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-22-I%E2%80%99m_officially_no_longer_a_%E2%80%9Crogue%E2%80%9D.html">1180 andrew gelman stats-2012-02-22-I’m officially no longer a “rogue”</a></p>
<p>14 0.75656021 <a title="398-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-08-Animated_drought_maps.html">1487 andrew gelman stats-2012-09-08-Animated drought maps</a></p>
<p>15 0.73206145 <a title="398-lda-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-19-Cross-validation_to_check_missing-data_imputation.html">1330 andrew gelman stats-2012-05-19-Cross-validation to check missing-data imputation</a></p>
<p>16 0.72945029 <a title="398-lda-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-03-Getting_a_job_in_pro_sports%E2%80%A6_as_a_statistician.html">445 andrew gelman stats-2010-12-03-Getting a job in pro sports… as a statistician</a></p>
<p>17 0.71879107 <a title="398-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-30-A_graphics_talk_with_no_visuals%21.html">1598 andrew gelman stats-2012-11-30-A graphics talk with no visuals!</a></p>
<p>18 0.71735132 <a title="398-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-24-Always_check_your_evidence.html">1025 andrew gelman stats-2011-11-24-Always check your evidence</a></p>
<p>19 0.70363683 <a title="398-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-06-Suspicious_pattern_of_too-strong_replications_of_medical_research.html">700 andrew gelman stats-2011-05-06-Suspicious pattern of too-strong replications of medical research</a></p>
<p>20 0.69973969 <a title="398-lda-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-04-23-Modeling_heterogenous_treatment_effects.html">2 andrew gelman stats-2010-04-23-Modeling heterogenous treatment effects</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
