<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>228 andrew gelman stats-2010-08-24-A new efficient lossless compression algorithm</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-228" href="#">andrew_gelman_stats-2010-228</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>228 andrew gelman stats-2010-08-24-A new efficient lossless compression algorithm</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-228-html" href="http://andrewgelman.com/2010/08/24/a_new_efficient/">html</a></p><p>Introduction: Frank Wood and Nick Bartlett  write :
  
Deplump works the same as all probabilistic lossless compressors. A datastream is fed one observation at a time into a predictor which emits both the data stream and predictions about what the next observation in the stream should be for every observation. An encoder takes this output and produces a compressed stream which can be piped over a network or to a file. A receiver then takes this stream and decompresses it by doing everything in reverse. In order to ensure that the decoder has the same information available to it that the encoder had when compressing the stream, the decoded datastream is both emitted and directed to another predictor. This second predictor’s job is to produce exactly the same predictions as the initial predictor so that the decoder has the same information at every step of the process as the encoder did.


The difference between probabilistic lossless compressors is in the prediction engine, encoding and decoding bein</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Frank Wood and Nick Bartlett  write :    Deplump works the same as all probabilistic lossless compressors. [sent-1, score-0.499]
</p><p>2 A datastream is fed one observation at a time into a predictor which emits both the data stream and predictions about what the next observation in the stream should be for every observation. [sent-2, score-1.635]
</p><p>3 An encoder takes this output and produces a compressed stream which can be piped over a network or to a file. [sent-3, score-1.072]
</p><p>4 A receiver then takes this stream and decompresses it by doing everything in reverse. [sent-4, score-0.548]
</p><p>5 In order to ensure that the decoder has the same information available to it that the encoder had when compressing the stream, the decoded datastream is both emitted and directed to another predictor. [sent-5, score-0.959]
</p><p>6 This second predictor’s job is to produce exactly the same predictions as the initial predictor so that the decoder has the same information at every step of the process as the encoder did. [sent-6, score-0.854]
</p><p>7 The difference between probabilistic lossless compressors is in the prediction engine, encoding and decoding being “solved” optimally already. [sent-7, score-0.921]
</p><p>8 Deplump compression technology is built on a probabilistic discrete sequence predictor called the sequence memoizer. [sent-8, score-1.167]
</p><p>9 The sequence memoizer has been demonstrated to be a very good predictor for discrete sequences. [sent-9, score-0.633]
</p><p>10 The advantage deplump demonstrates in comparison to other general purpose lossless compressors is largely attributable to the better guesses made by the sequence memoizer. [sent-10, score-1.304]
</p><p>11 Deplump is algorithmically quite closely related to infinite context prediction by partial matching (PPM) although it has differences and advantages which derive from being grounded on a coherent probabilistic predictive model. [sent-11, score-0.796]
</p><p>12 They also have a  cool website  where you can compress your own files. [sent-13, score-0.081]
</p><p>13 According to Frank, it gives you an compressed file that’s quite a bit smaller than what you get from gzip. [sent-14, score-0.24]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stream', 0.388), ('deplump', 0.378), ('encoder', 0.283), ('lossless', 0.283), ('predictor', 0.246), ('sequence', 0.24), ('probabilistic', 0.216), ('compressors', 0.189), ('datastream', 0.189), ('decoder', 0.189), ('compressed', 0.133), ('observation', 0.111), ('frank', 0.104), ('discrete', 0.095), ('piped', 0.086), ('algorithmically', 0.086), ('attributable', 0.086), ('emitted', 0.086), ('receiver', 0.086), ('predictions', 0.083), ('prediction', 0.083), ('compress', 0.081), ('compressing', 0.081), ('bartlett', 0.078), ('compression', 0.078), ('encoding', 0.075), ('optimally', 0.075), ('takes', 0.074), ('guesses', 0.069), ('directed', 0.069), ('grounded', 0.068), ('fed', 0.066), ('wood', 0.066), ('derive', 0.064), ('infinite', 0.064), ('ensure', 0.062), ('demonstrates', 0.059), ('nick', 0.059), ('engine', 0.057), ('quite', 0.055), ('solved', 0.055), ('produces', 0.055), ('advantages', 0.055), ('matching', 0.054), ('output', 0.053), ('every', 0.053), ('demonstrated', 0.052), ('file', 0.052), ('technology', 0.052), ('coherent', 0.051)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="228-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-24-A_new_efficient_lossless_compression_algorithm.html">228 andrew gelman stats-2010-08-24-A new efficient lossless compression algorithm</a></p>
<p>Introduction: Frank Wood and Nick Bartlett  write :
  
Deplump works the same as all probabilistic lossless compressors. A datastream is fed one observation at a time into a predictor which emits both the data stream and predictions about what the next observation in the stream should be for every observation. An encoder takes this output and produces a compressed stream which can be piped over a network or to a file. A receiver then takes this stream and decompresses it by doing everything in reverse. In order to ensure that the decoder has the same information available to it that the encoder had when compressing the stream, the decoded datastream is both emitted and directed to another predictor. This second predictor’s job is to produce exactly the same predictions as the initial predictor so that the decoder has the same information at every step of the process as the encoder did.


The difference between probabilistic lossless compressors is in the prediction engine, encoding and decoding bein</p><p>2 0.11925094 <a title="228-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-Piss-poor_monocausal_social_science.html">1196 andrew gelman stats-2012-03-04-Piss-poor monocausal social science</a></p>
<p>Introduction: Dan Kahan writes: 
  
  
Okay, have done due diligence here & can’t find the reference. It was in recent blog — and was more or less an aside — but you ripped into researchers (pretty sure econometricians, but this could be my memory adding to your account recollections it conjured from my own experience) who purport to make estimates or predictions based on multivariate regression in which the value of particular predictor is set at some level while others “held constant” etc., on ground that variance in that particular predictor independent of covariance in other model predictors is unrealistic.  You made it sound, too, as if this were one of the pet peeves in your menagerie — leading me to think you had blasted into it before.


Know what I’m talking about?


Also — isn’t this really just a way of saying that the model is misspecified — at least if the goal is to try to make a valid & unbiased estimate of the impact of that particular predictor? The problem can’t be that one is usin</p><p>3 0.094861612 <a title="228-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-29-Postdocs_in_probabilistic_modeling%21__With_David_Blei%21__And_Stan%21.html">1961 andrew gelman stats-2013-07-29-Postdocs in probabilistic modeling!  With David Blei!  And Stan!</a></p>
<p>Introduction: David Blei writes:
  
I have  two postdoc openings for basic research in probabilistic modeling .


The thrusts are (a) scalable inference and (b) model checking.  We 
will be developing new methods and implementing them in probabilistic 
programming systems.  I am open to applicants interested in many kinds 
of applications and from any field.
  
“Scalable inference” means black-box VB and related ideas, and “probabilistic programming systems” means Stan!  (You might be familiar with Stan as an implementation of Nuts for posterior sampling, but Stan is also an efficient program for computing probability densities and their gradients, and as such is an ideal platform for developing scalable implementations of variational inference and related algorithms.)
 
And you know I like model checking.
 
Here’s the full ad: 
  
  
===== POSTDOC POSITIONS IN PROBABILISTIC MODELING =====


We expect to have two postdoctoral positions available for January 2014 (or later).  These positions are in D</p><p>4 0.084284641 <a title="228-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-05-Question_26_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1367 andrew gelman stats-2012-06-05-Question 26 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>Introduction: 26. You have just graded an an exam with 28 questions and 15 students. You fit a logistic item- response model estimating ability, difficulty, and discrimination parameters. Which of the following statements are basically true? (Indicate all that apply.)
 
(a) If a question is answered correctly by students with very low and very high ability, but is missed by students in the middle, it will have a high value for its discrimination parameter.
 
(b) It is not possible to fit an item-response model when you have more questions than students. In order to fit the model, you either need to reduce the number of questions (for example, by discarding some questions or by putting together some questions into a combined score) or increase the number of students in the dataset.
 
(c) To keep the model identified, you can set one of the difficulty parameters or one of the ability parameters to zero and set one of the discrimination parameters to 1.
 
(d) If two students answer the same number of q</p><p>5 0.076864325 <a title="228-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-04-Question_25_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1365 andrew gelman stats-2012-06-04-Question 25 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>Introduction: 25. You are using multilevel regression and poststratification (MRP) to a survey of 1500 people to estimate support for the space program, by state. The model is fit using, as a state- level predictor, the Republican presidential vote in the state, which turns out to have a low correlation with support for the space program. Which of the following statements are basically true? (Indicate all that apply.)
 
(a) For small states, the MRP estimates will be determined almost entirely by the demo- graphic characteristics of the respondents in the sample from that state.
 
(b) For small states, the MRP estimates will be determined almost entirely by the demographic characteristics of the population in that state.
 
(c) Adding a predictor specifically for this model (for example, a measure of per-capita space-program spending in the state) could dramatically improve the estimates of state-level opinion.
 
(d) It would not be appropriate to add a predictor such as per-capita space-program spen</p><p>6 0.067002073 <a title="228-tfidf-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-Correlation%2C_prediction%2C_variation%2C_etc..html">301 andrew gelman stats-2010-09-28-Correlation, prediction, variation, etc.</a></p>
<p>7 0.06460166 <a title="228-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-13-Stan_at_NIPS_2012_Workshop_on_Probabilistic_Programming.html">1576 andrew gelman stats-2012-11-13-Stan at NIPS 2012 Workshop on Probabilistic Programming</a></p>
<p>8 0.06013421 <a title="228-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-27-Uncompressing_the_concept_of_compressed_sensing.html">2079 andrew gelman stats-2013-10-27-Uncompressing the concept of compressed sensing</a></p>
<p>9 0.058435358 <a title="228-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-30-David_Hogg_on_statistics.html">1401 andrew gelman stats-2012-06-30-David Hogg on statistics</a></p>
<p>10 0.054789621 <a title="228-tfidf-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-%E2%80%9CGenomics%E2%80%9D_vs._genetics.html">303 andrew gelman stats-2010-09-28-“Genomics” vs. genetics</a></p>
<p>11 0.052860502 <a title="228-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>12 0.052810211 <a title="228-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-05-Understanding_regression_models_and_regression_coefficients.html">1656 andrew gelman stats-2013-01-05-Understanding regression models and regression coefficients</a></p>
<p>13 0.052693527 <a title="228-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-30-Stan_Project%3A__Continuous_Relaxations_for_Discrete_MRFs.html">2003 andrew gelman stats-2013-08-30-Stan Project:  Continuous Relaxations for Discrete MRFs</a></p>
<p>14 0.052553389 <a title="228-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-19-Weather_visualization_with_WeatherSpark.html">580 andrew gelman stats-2011-02-19-Weather visualization with WeatherSpark</a></p>
<p>15 0.051646717 <a title="228-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-12-Val%E2%80%99s_Number_Scroll%3A_Helping_kids_visualize_math.html">1006 andrew gelman stats-2011-11-12-Val’s Number Scroll: Helping kids visualize math</a></p>
<p>16 0.049651735 <a title="228-tfidf-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-07-Analysis_of_Power_Law_of_Participation.html">946 andrew gelman stats-2011-10-07-Analysis of Power Law of Participation</a></p>
<p>17 0.045997411 <a title="228-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>18 0.04483486 <a title="228-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-15-More_on_AIC%2C_WAIC%2C_etc.html">1983 andrew gelman stats-2013-08-15-More on AIC, WAIC, etc</a></p>
<p>19 0.044761628 <a title="228-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-27-Bridges_between_deterministic_and_probabilistic_models_for_binary_data.html">780 andrew gelman stats-2011-06-27-Bridges between deterministic and probabilistic models for binary data</a></p>
<p>20 0.044295639 <a title="228-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-06-Josh_Tenenbaum_presents_._._._a_model_of_folk_physics%21.html">994 andrew gelman stats-2011-11-06-Josh Tenenbaum presents . . . a model of folk physics!</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.06), (1, 0.015), (2, 0.006), (3, 0.01), (4, 0.02), (5, 0.006), (6, -0.003), (7, -0.017), (8, 0.011), (9, 0.01), (10, 0.004), (11, 0.004), (12, -0.009), (13, -0.008), (14, -0.037), (15, 0.009), (16, 0.032), (17, -0.014), (18, 0.01), (19, -0.0), (20, -0.002), (21, -0.005), (22, -0.005), (23, -0.005), (24, 0.016), (25, 0.013), (26, 0.013), (27, 0.02), (28, 0.009), (29, 0.004), (30, 0.034), (31, -0.016), (32, 0.002), (33, -0.017), (34, 0.003), (35, -0.002), (36, 0.019), (37, -0.013), (38, 0.012), (39, -0.029), (40, -0.005), (41, -0.005), (42, -0.002), (43, 0.012), (44, 0.021), (45, 0.005), (46, -0.004), (47, -0.008), (48, 0.028), (49, 0.008)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93986195 <a title="228-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-24-A_new_efficient_lossless_compression_algorithm.html">228 andrew gelman stats-2010-08-24-A new efficient lossless compression algorithm</a></p>
<p>Introduction: Frank Wood and Nick Bartlett  write :
  
Deplump works the same as all probabilistic lossless compressors. A datastream is fed one observation at a time into a predictor which emits both the data stream and predictions about what the next observation in the stream should be for every observation. An encoder takes this output and produces a compressed stream which can be piped over a network or to a file. A receiver then takes this stream and decompresses it by doing everything in reverse. In order to ensure that the decoder has the same information available to it that the encoder had when compressing the stream, the decoded datastream is both emitted and directed to another predictor. This second predictor’s job is to produce exactly the same predictions as the initial predictor so that the decoder has the same information at every step of the process as the encoder did.


The difference between probabilistic lossless compressors is in the prediction engine, encoding and decoding bein</p><p>2 0.65572757 <a title="228-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-14-Another_day%2C_another_stats_postdoc.html">906 andrew gelman stats-2011-09-14-Another day, another stats postdoc</a></p>
<p>Introduction: This post is from Phil Price.  I work in the Environmental Energy Technologies Division at Lawrence Berkeley National Laboratory, and I am looking for a postdoc who knows substantially more than I do about time-series modeling; in practice this probably means someone whose dissertation work involved that sort of thing.  The work involves developing models to predict and/or forecast the time-dependent energy use in buildings, given historical data and some covariates such as outdoor temperature.  Simple regression approaches (e.g. using time-of-week indicator variables, plus outdoor temperature) work fine for a lot of things, but we still have a variety of problems.  To give one example, sometimes building behavior changes — due to retrofits, or a change in occupant behavior — so that a single model won’t fit well over a long time period. We want to recognize these changes automatically .  We have many other issues besides: heteroskedasticity, need for good uncertainty estimates, abilit</p><p>3 0.63096035 <a title="228-lsi-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Bayesian_Uncertainty_Quantification_for_Differential_Equations%21.html">2311 andrew gelman stats-2014-04-29-Bayesian Uncertainty Quantification for Differential Equations!</a></p>
<p>Introduction: Mark Girolami points us to  this paper and software  (with Oksana Chkrebtii, David Campbell, and Ben Calderhead).  They write:
  
We develop a general methodology for the probabilistic integration of differential equations via model based updating of a joint prior measure on the space of functions and their temporal and spatial derivatives. This results in a posterior measure over functions reflecting how well they satisfy the system of differential equations and corresponding initial and boundary values. We show how this posterior measure can be naturally incorporated within the Kennedy and O’Hagan framework for uncertainty quantification and provides a fully Bayesian approach to model calibration. . . . A broad variety of examples are provided to illustrate the potential of this framework for characterising discretization uncertainty, including initial value, delay, and boundary value differential equations, as well as partial differential equations. We also demonstrate our methodolo</p><p>4 0.61618686 <a title="228-lsi-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Question_about_standard_range_for_social_science_correlations.html">257 andrew gelman stats-2010-09-04-Question about standard range for social science correlations</a></p>
<p>Introduction: Andrew Eppig writes:
  
I’m a physicist by training who is transitioning to the social sciences. I recently came across a  reference  in the Economist to a paper on IQ and parasites which I read as I have more than a passing interest in IQ research (having read much that you and others (e.g., Shalizi, Wicherts) have written). In this paper I note that the authors find a very high correlation between national IQ and parasite prevalence. The strength of the correlation (-0.76 to -0.82) surprised me, as I’m used to much weaker correlations in the social sciences. To me, it’s a bit too high, suggesting that there are other factors at play or that one of the variables is merely a proxy for a large number of other variables. But I have no basis for this other than a gut feeling and a memory of a plot on  Language Log  about the distribution of correlation coefficients in social psychology.


So my question is this: Is a correlation in the range of (-0.82,-0.76) more likely to be a correlatio</p><p>5 0.61316526 <a title="228-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-05-Question_26_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1367 andrew gelman stats-2012-06-05-Question 26 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>Introduction: 26. You have just graded an an exam with 28 questions and 15 students. You fit a logistic item- response model estimating ability, difficulty, and discrimination parameters. Which of the following statements are basically true? (Indicate all that apply.)
 
(a) If a question is answered correctly by students with very low and very high ability, but is missed by students in the middle, it will have a high value for its discrimination parameter.
 
(b) It is not possible to fit an item-response model when you have more questions than students. In order to fit the model, you either need to reduce the number of questions (for example, by discarding some questions or by putting together some questions into a combined score) or increase the number of students in the dataset.
 
(c) To keep the model identified, you can set one of the difficulty parameters or one of the ability parameters to zero and set one of the discrimination parameters to 1.
 
(d) If two students answer the same number of q</p><p>6 0.60680312 <a title="228-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-09-Mister_P%3A__What%E2%80%99s_its_secret_sauce%3F.html">2056 andrew gelman stats-2013-10-09-Mister P:  What’s its secret sauce?</a></p>
<p>7 0.60554069 <a title="228-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-06-Research_Directions_for_Machine_Learning_and_Algorithms.html">747 andrew gelman stats-2011-06-06-Research Directions for Machine Learning and Algorithms</a></p>
<p>8 0.60502934 <a title="228-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-03-Bribing_statistics.html">500 andrew gelman stats-2011-01-03-Bribing statistics</a></p>
<p>9 0.60446203 <a title="228-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-Piss-poor_monocausal_social_science.html">1196 andrew gelman stats-2012-03-04-Piss-poor monocausal social science</a></p>
<p>10 0.59484643 <a title="228-lsi-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-07-Contest_for_developing_an_R_package_recommendation_system.html">324 andrew gelman stats-2010-10-07-Contest for developing an R package recommendation system</a></p>
<p>11 0.5938257 <a title="228-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-02-RStudio_%E2%80%93_new_cross-platform_IDE_for_R.html">597 andrew gelman stats-2011-03-02-RStudio – new cross-platform IDE for R</a></p>
<p>12 0.59262496 <a title="228-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-27-Visual_diagnostics_for_discrete-data_regressions.html">929 andrew gelman stats-2011-09-27-Visual diagnostics for discrete-data regressions</a></p>
<p>13 0.59184998 <a title="228-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-15-Last_word_on_Mister_P_%28for_now%29.html">2062 andrew gelman stats-2013-10-15-Last word on Mister P (for now)</a></p>
<p>14 0.59144014 <a title="228-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>15 0.59103799 <a title="228-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<p>16 0.59034425 <a title="228-lsi-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-29-The_Subtle_Micro-Effects_of_Peacekeeping.html">242 andrew gelman stats-2010-08-29-The Subtle Micro-Effects of Peacekeeping</a></p>
<p>17 0.58765173 <a title="228-lsi-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Blending_results_from_two_relatively_independent_multi-level_models.html">250 andrew gelman stats-2010-09-02-Blending results from two relatively independent multi-level models</a></p>
<p>18 0.58738917 <a title="228-lsi-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-20-When_Kerry_Met_Sally%3A_Politics_and_Perceptions_in_the_Demand_for_Movies.html">358 andrew gelman stats-2010-10-20-When Kerry Met Sally: Politics and Perceptions in the Demand for Movies</a></p>
<p>19 0.57899213 <a title="228-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-13-Help_with_this_problem%2C_win_valuable_prizes.html">1164 andrew gelman stats-2012-02-13-Help with this problem, win valuable prizes</a></p>
<p>20 0.5754149 <a title="228-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-15-Induction_within_a_model%2C_deductive_inference_for_model_evaluation.html">614 andrew gelman stats-2011-03-15-Induction within a model, deductive inference for model evaluation</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(5, 0.394), (16, 0.05), (21, 0.063), (24, 0.086), (48, 0.027), (53, 0.014), (73, 0.018), (74, 0.011), (86, 0.056), (97, 0.016), (98, 0.015), (99, 0.105)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.89423048 <a title="228-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-22-Mister_P_gets_married.html">224 andrew gelman stats-2010-08-22-Mister P gets married</a></p>
<p>Introduction: Jeff, Justin, and I  write :
  
Gay marriage is not going away as a highly emotional, contested issue. Proposition 8, the California ballot measure that bans same-sex marriage, has seen to that, as it winds its way through the federal courts.  But perhaps the public has reached a turning point.
  
And check out the (mildly) dynamic graphics.  The picture below is ok but for the full effect you have to click through and  play the movie.</p><p>same-blog 2 0.89404625 <a title="228-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-24-A_new_efficient_lossless_compression_algorithm.html">228 andrew gelman stats-2010-08-24-A new efficient lossless compression algorithm</a></p>
<p>Introduction: Frank Wood and Nick Bartlett  write :
  
Deplump works the same as all probabilistic lossless compressors. A datastream is fed one observation at a time into a predictor which emits both the data stream and predictions about what the next observation in the stream should be for every observation. An encoder takes this output and produces a compressed stream which can be piped over a network or to a file. A receiver then takes this stream and decompresses it by doing everything in reverse. In order to ensure that the decoder has the same information available to it that the encoder had when compressing the stream, the decoded datastream is both emitted and directed to another predictor. This second predictor’s job is to produce exactly the same predictions as the initial predictor so that the decoder has the same information at every step of the process as the encoder did.


The difference between probabilistic lossless compressors is in the prediction engine, encoding and decoding bein</p><p>3 0.80751002 <a title="228-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-20-A_Gapminder-like_data_visualization_package.html">422 andrew gelman stats-2010-11-20-A Gapminder-like data visualization package</a></p>
<p>Introduction: Ossama Hamed writes in with a new dynamic graphing software:
  
 
I have the pleasure to brief you on our Data Visualization software “Trend Compass”.


TC is a new concept in viewing statistics and trends in an animated way by displaying in one chart 5 axis (X, Y, Time, Bubble size & Bubble color) instead of just the traditional X and Y axis. . . .</p><p>4 0.77869213 <a title="228-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-07-Hangman_tips.html">1250 andrew gelman stats-2012-04-07-Hangman tips</a></p>
<p>Introduction: Jeff pointed me to  this  article by Nick Berry.  It’s kind of fun but of course if you know your opponent will be following this strategy you can figure out how to outwit it.  Also, Berry writes that ETAOIN SHRDLU CMFWYP VBGKQJ XZ is the “ordering of letter frequency in English language.”  Indeed this is the conventional ordering but nobody thinks it’s right anymore.  See  here  (with further discussion  here ).  I wonder what corpus he’s using.
 
P.S.  Klutz was my personal standby.</p><p>5 0.76778632 <a title="228-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-17-Yes%2C_your_wish_shall_be_granted_%28in_25_years%29.html">665 andrew gelman stats-2011-04-17-Yes, your wish shall be granted (in 25 years)</a></p>
<p>Introduction: This one  was so beautiful I just had to repost it:
 
From the New York Times, 9 Sept 1981:
  
IF I COULD CHANGE PARK SLOPE


If I could change Park Slope I would turn it into a palace with queens and kings and princesses to dance the night away at the ball. The trees would look like garden stalks. The lights would look like silver pearls and the dresses would look like soft silver silk. You should see the ball. It looks so luxurious to me.


The Park Slope ball is great. Can you guess what street it’s on? “Yes. My street. That’s Carroll Street.”


– Jennifer Chatmon, second grade, P.S. 321
  
This was a few years before my sister told me that she felt safer having a crack house down the block because the cops were surveilling it all the time.</p><p>6 0.73871887 <a title="228-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-02-%E2%80%9CIl_y_a_beaucoup_de_candidats_d%C3%A9mocrates%2C_et_leurs_id%C3%A9ologies_ne_sont_pas_tr%C3%A8s_diff%C3%A9rentes._Et_la_participation_est_impr%C3%A9visible.%E2%80%9D.html">2005 andrew gelman stats-2013-09-02-“Il y a beaucoup de candidats démocrates, et leurs idéologies ne sont pas très différentes. Et la participation est imprévisible.”</a></p>
<p>7 0.72681296 <a title="228-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-26-A_very_short_story.html">164 andrew gelman stats-2010-07-26-A very short story</a></p>
<p>8 0.71320128 <a title="228-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-12-%E2%80%9CTied_for_Warmest_Year_On_Record%E2%80%9D.html">513 andrew gelman stats-2011-01-12-“Tied for Warmest Year On Record”</a></p>
<p>9 0.70789921 <a title="228-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-15-Statistical_analysis_and_visualization_of_the_drug_war_in_Mexico.html">87 andrew gelman stats-2010-06-15-Statistical analysis and visualization of the drug war in Mexico</a></p>
<p>10 0.65615523 <a title="228-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-05-The_Grinch_Comes_Back.html">1606 andrew gelman stats-2012-12-05-The Grinch Comes Back</a></p>
<p>11 0.63459373 <a title="228-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-06-Unconvincing_defense_of_the_recent_Russian_elections%2C_and_a_problem_when_an_official_organ_of_an_academic_society_has_low_standards_for_publication.html">1103 andrew gelman stats-2012-01-06-Unconvincing defense of the recent Russian elections, and a problem when an official organ of an academic society has low standards for publication</a></p>
<p>12 0.6236701 <a title="228-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-28-Agreement_Groups_in_US_Senate_and_Dynamic_Clustering.html">1286 andrew gelman stats-2012-04-28-Agreement Groups in US Senate and Dynamic Clustering</a></p>
<p>13 0.61840957 <a title="228-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-27-A_Non-random_Walk_Down_Campaign_Street.html">1512 andrew gelman stats-2012-09-27-A Non-random Walk Down Campaign Street</a></p>
<p>14 0.59443176 <a title="228-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-04-The_Folk_Theorem_of_Statistical_Computing.html">1841 andrew gelman stats-2013-05-04-The Folk Theorem of Statistical Computing</a></p>
<p>15 0.57667232 <a title="228-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-11-Rational_Turbulence.html">1052 andrew gelman stats-2011-12-11-Rational Turbulence</a></p>
<p>16 0.57406038 <a title="228-lda-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-01-Recently_in_the_sister_blog.html">2194 andrew gelman stats-2014-02-01-Recently in the sister blog</a></p>
<p>17 0.56831169 <a title="228-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-22-Politics_is_not_a_random_walk%3A__Momentum_and_mean_reversion_in_polling.html">364 andrew gelman stats-2010-10-22-Politics is not a random walk:  Momentum and mean reversion in polling</a></p>
<p>18 0.5606063 <a title="228-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-14-Examining_US_Legislative_process_with_%E2%80%9CMany_Bills%E2%80%9D.html">764 andrew gelman stats-2011-06-14-Examining US Legislative process with “Many Bills”</a></p>
<p>19 0.55820513 <a title="228-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-01-Truth_in_headlines.html">123 andrew gelman stats-2010-07-01-Truth in headlines</a></p>
<p>20 0.53679538 <a title="228-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-11-Data_mining_efforts_for_Obama%E2%80%99s_campaign.html">951 andrew gelman stats-2011-10-11-Data mining efforts for Obama’s campaign</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
