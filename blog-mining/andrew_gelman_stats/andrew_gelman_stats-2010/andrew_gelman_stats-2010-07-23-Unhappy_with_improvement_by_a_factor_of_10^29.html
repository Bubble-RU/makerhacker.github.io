<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>160 andrew gelman stats-2010-07-23-Unhappy with improvement by a factor of 10^29</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-160" href="#">andrew_gelman_stats-2010-160</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>160 andrew gelman stats-2010-07-23-Unhappy with improvement by a factor of 10^29</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-160-html" href="http://andrewgelman.com/2010/07/23/unhappy_with_im/">html</a></p><p>Introduction: I have an optimization problem:  I have a complicated physical model that predicts energy and thermal behavior of a building, given the values of a slew of parameters, such as insulation effectiveness, window transmissivity, etc.  I’m trying to find the parameter set that best fits several weeks of thermal and energy use data from the real building that we modeled. (Of course I would rather explore parameter space and come up with probability distributions for the parameters, and maybe that will come later, but for now I’m just optimizing).  To do the optimization, colleagues and I implemented a “particle swarm optimization” algorithm on a massively parallel machine. This involves giving each of about 120 “particles” an initial position in parameter space, then letting them move around, trying to move to better positions according to a specific algorithm.  We gave each particle an initial position sampled from our prior distribution for each parameter.  So far we’ve run about 140 itera</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I have an optimization problem:  I have a complicated physical model that predicts energy and thermal behavior of a building, given the values of a slew of parameters, such as insulation effectiveness, window transmissivity, etc. [sent-1, score-1.041]
</p><p>2 I’m trying to find the parameter set that best fits several weeks of thermal and energy use data from the real building that we modeled. [sent-2, score-1.196]
</p><p>3 (Of course I would rather explore parameter space and come up with probability distributions for the parameters, and maybe that will come later, but for now I’m just optimizing). [sent-3, score-0.875]
</p><p>4 To do the optimization, colleagues and I implemented a “particle swarm optimization” algorithm on a massively parallel machine. [sent-4, score-0.441]
</p><p>5 This involves giving each of about 120 “particles” an initial position in parameter space, then letting them move around, trying to move to better positions according to a specific algorithm. [sent-5, score-1.051]
</p><p>6 We gave each particle an initial position sampled from our prior distribution for each parameter. [sent-6, score-0.689]
</p><p>7 So far we’ve run about 140 iterations, and I just took a look at where the particles are now. [sent-7, score-0.328]
</p><p>8 They are indeed converging — that is, they’re coming to some agreement on what the best region of parameter space is. [sent-8, score-1.007]
</p><p>9 But the standard deviation for each parameter is still about 0. [sent-9, score-0.42]
</p><p>10 (For instance, we put in a very wide prior distribution for the thermal mass of the furnishings in the building’s offices, and after running the optimization the distribution is about 0. [sent-11, score-1.101]
</p><p>11 I was, and still am, a bit disappointed by this, but: we have 74 parameters. [sent-13, score-0.085]
</p><p>12 Our particles were spread through a huge volume of parameter space, and now they’re spread through a space that is about 0. [sent-14, score-1.41]
</p><p>13 That means they’ve agreed on a volume of parameter space that is about 0. [sent-16, score-0.884]
</p><p>14 4^74 times smaller than it was before, or about a factor of 10^29 smaller. [sent-17, score-0.207]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('parameter', 0.351), ('particles', 0.328), ('space', 0.327), ('thermal', 0.294), ('optimization', 0.291), ('particle', 0.182), ('building', 0.17), ('times', 0.148), ('volume', 0.138), ('spread', 0.133), ('wide', 0.132), ('initial', 0.131), ('energy', 0.129), ('distribution', 0.12), ('converging', 0.116), ('massively', 0.116), ('swarm', 0.116), ('slew', 0.109), ('offices', 0.101), ('optimizing', 0.101), ('position', 0.1), ('move', 0.097), ('parameters', 0.093), ('disappointed', 0.085), ('iterations', 0.084), ('window', 0.082), ('prior', 0.079), ('region', 0.078), ('sampled', 0.077), ('letting', 0.076), ('predicts', 0.076), ('parallel', 0.075), ('effectiveness', 0.074), ('implemented', 0.073), ('agreement', 0.072), ('deviation', 0.069), ('agreed', 0.068), ('trying', 0.067), ('come', 0.067), ('positions', 0.067), ('involves', 0.065), ('mass', 0.065), ('instance', 0.065), ('explore', 0.063), ('best', 0.063), ('fits', 0.062), ('algorithm', 0.061), ('physical', 0.06), ('weeks', 0.06), ('smaller', 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="160-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-23-Unhappy_with_improvement_by_a_factor_of_10%5E29.html">160 andrew gelman stats-2010-07-23-Unhappy with improvement by a factor of 10^29</a></p>
<p>Introduction: I have an optimization problem:  I have a complicated physical model that predicts energy and thermal behavior of a building, given the values of a slew of parameters, such as insulation effectiveness, window transmissivity, etc.  I’m trying to find the parameter set that best fits several weeks of thermal and energy use data from the real building that we modeled. (Of course I would rather explore parameter space and come up with probability distributions for the parameters, and maybe that will come later, but for now I’m just optimizing).  To do the optimization, colleagues and I implemented a “particle swarm optimization” algorithm on a massively parallel machine. This involves giving each of about 120 “particles” an initial position in parameter space, then letting them move around, trying to move to better positions according to a specific algorithm.  We gave each particle an initial position sampled from our prior distribution for each parameter.  So far we’ve run about 140 itera</p><p>2 0.22350384 <a title="160-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-11-My_problem_with_the_Lindley_paradox.html">1757 andrew gelman stats-2013-03-11-My problem with the Lindley paradox</a></p>
<p>Introduction: From  a couple years ago but still relevant, I think:
  
To me, the Lindley paradox falls apart because of its noninformative prior distribution on the parameter of interest. If you really think there’s a high probability the parameter is nearly exactly zero, I don’t see the point of the model saying that you have no prior information at all on the parameter. In short: my criticism of so-called Bayesian hypothesis testing is that it’s insufficiently Bayesian.
  
P.S.  To clarify (in response to Bill’s comment below):  I’m speaking of all the examples I’ve ever worked on in social and environmental science, where in some settings I can imagine a parameter being very close to zero and in other settings I can imagine a parameter taking on just about any value in a wide range, but where I’ve never seen an example where a parameter could be  either  right at zero  or  taking on any possible value.  But such examples might occur in areas of application that I haven’t worked on.</p><p>3 0.20928937 <a title="160-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-07-X_on_JLP.html">1792 andrew gelman stats-2013-04-07-X on JLP</a></p>
<p>Introduction: Christian Robert  writes  on the Jeffreys-Lindley paradox.  I have nothing to add to this beyond my recent  comments :
  
To me, the Lindley paradox falls apart because of its noninformative prior distribution on the parameter of interest. If you really think there’s a high probability the parameter is nearly exactly zero, I don’t see the point of the model saying that you have no prior information at all on the parameter. In short: my criticism of so-called Bayesian hypothesis testing is that it’s insufficiently Bayesian.


To clarify, I’m speaking of all the examples I’ve ever worked on in social and environmental science, where in some settings I can imagine a parameter being very close to zero and in other settings I can imagine a parameter taking on just about any value in a wide range, but where I’ve never seen an example where a parameter could be either right at zero or taking on any possible value. But such examples might occur in areas of application that I haven’t worked on.</p><p>4 0.1727702 <a title="160-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-28-Understanding_simulations_in_terms_of_predictive_inference%3F.html">1287 andrew gelman stats-2012-04-28-Understanding simulations in terms of predictive inference?</a></p>
<p>Introduction: David Hogg writes:
  
My (now deceased) collaborator and guru in all things inference, Sam Roweis, used to emphasize to me that we should evaluate models in the data space — not the parameter space — because models are always effectively “effective” and not really, fundamentally true. Or, in other words, models should be compared in the space of their predictions, not in the space of their parameters (the  parameters didn’t really “exist” at all for Sam).  In that spirit, when we estimate the effectiveness of a MCMC method or tuning — by autocorrelation time or ESJD or anything else — shouldn’t we be looking at the changes in the model predictions over time, rather than the changes in the parameters over time?  That is, the autocorrelation time should be the autocorrelation time in what the model (at the walker position) predicts for the data, and the ESJD should be the expected squared jump distance in what the model predicts for the data?  This might resolve the concern I expressed a</p><p>5 0.15711007 <a title="160-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-28-Those_darn_physicists.html">1189 andrew gelman stats-2012-02-28-Those darn physicists</a></p>
<p>Introduction: X pointed me to  this  atrocity:
  
The data on obesity are pretty unequivocal: we’re fat, and we’re getting fatter. Explanations for this trend, however, vary widely, with the blame alternately pinned on individual behaviour, genetics and the environment. In other words, it’s a race between “we eat too much”, “we’re born that way” and “it’s society’s fault”. 
Now, research by Lazaros Gallos has come down strongly in favour of the third option. Gallos and his colleagues at City College of New York treated the obesity rates in some 3000 US counties as “particles” in a physical system, and calculated the correlation between pairs of “particles” as a function of the distance between them. . . . the data indicated that the size of the “obesity cities” – geographic regions with correlated obesity rates – was huge, up to 1000 km. . . .
  
Just to be clear:  I have no problem with people calculating spatial autocorrelations (or even with them using quaint terminology such as referring to coun</p><p>6 0.15397087 <a title="160-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-03-Libertarians_in_Space.html">1097 andrew gelman stats-2012-01-03-Libertarians in Space</a></p>
<p>7 0.15389207 <a title="160-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-12-The_Naval_Research_Lab.html">1261 andrew gelman stats-2012-04-12-The Naval Research Lab</a></p>
<p>8 0.14981514 <a title="160-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-17-Jumping_off_the_edge_of_the_world.html">858 andrew gelman stats-2011-08-17-Jumping off the edge of the world</a></p>
<p>9 0.14598998 <a title="160-tfidf-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-31-Type_S_error_rates_for_classical_and_Bayesian_single_and_multiple_comparison_procedures.html">494 andrew gelman stats-2010-12-31-Type S error rates for classical and Bayesian single and multiple comparison procedures</a></p>
<p>10 0.13695601 <a title="160-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-15-Reputations_changeable%2C_situations_tolerable.html">1858 andrew gelman stats-2013-05-15-Reputations changeable, situations tolerable</a></p>
<p>11 0.13396838 <a title="160-tfidf-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>12 0.12403421 <a title="160-tfidf-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-04-That_half-Cauchy_prior.html">184 andrew gelman stats-2010-08-04-That half-Cauchy prior</a></p>
<p>13 0.12264518 <a title="160-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>14 0.11626115 <a title="160-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-29-More_by_Berger_and_me_on_weakly_informative_priors.html">1092 andrew gelman stats-2011-12-29-More by Berger and me on weakly informative priors</a></p>
<p>15 0.11545008 <a title="160-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-29-Where_36%25_of_all_boys_end_up_nowadays.html">1697 andrew gelman stats-2013-01-29-Where 36% of all boys end up nowadays</a></p>
<p>16 0.11391545 <a title="160-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-21-Optimizing_software_in_C%2B%2B.html">1423 andrew gelman stats-2012-07-21-Optimizing software in C++</a></p>
<p>17 0.1128016 <a title="160-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-19-Whassup_with_deviance_having_a_high_posterior_correlation_with_a_parameter_in_the_model%3F.html">1221 andrew gelman stats-2012-03-19-Whassup with deviance having a high posterior correlation with a parameter in the model?</a></p>
<p>18 0.10941766 <a title="160-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>19 0.10924783 <a title="160-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-10-Cross-validation_and_Bayesian_estimation_of_tuning_parameters.html">2129 andrew gelman stats-2013-12-10-Cross-validation and Bayesian estimation of tuning parameters</a></p>
<p>20 0.10824825 <a title="160-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-25-Bayes-respecting_experimental_design_and_other_things.html">1955 andrew gelman stats-2013-07-25-Bayes-respecting experimental design and other things</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.129), (1, 0.098), (2, 0.02), (3, 0.052), (4, 0.016), (5, -0.009), (6, 0.116), (7, -0.012), (8, -0.105), (9, 0.024), (10, -0.004), (11, 0.0), (12, -0.012), (13, -0.017), (14, -0.079), (15, -0.024), (16, 0.009), (17, -0.025), (18, 0.029), (19, -0.043), (20, 0.017), (21, -0.062), (22, -0.002), (23, -0.016), (24, 0.025), (25, 0.034), (26, -0.052), (27, 0.003), (28, 0.04), (29, -0.024), (30, -0.007), (31, -0.043), (32, -0.001), (33, -0.006), (34, -0.049), (35, -0.022), (36, -0.005), (37, 0.017), (38, 0.008), (39, -0.007), (40, -0.016), (41, 0.008), (42, -0.077), (43, 0.06), (44, -0.047), (45, -0.014), (46, 0.055), (47, 0.048), (48, -0.015), (49, 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98174137 <a title="160-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-23-Unhappy_with_improvement_by_a_factor_of_10%5E29.html">160 andrew gelman stats-2010-07-23-Unhappy with improvement by a factor of 10^29</a></p>
<p>Introduction: I have an optimization problem:  I have a complicated physical model that predicts energy and thermal behavior of a building, given the values of a slew of parameters, such as insulation effectiveness, window transmissivity, etc.  I’m trying to find the parameter set that best fits several weeks of thermal and energy use data from the real building that we modeled. (Of course I would rather explore parameter space and come up with probability distributions for the parameters, and maybe that will come later, but for now I’m just optimizing).  To do the optimization, colleagues and I implemented a “particle swarm optimization” algorithm on a massively parallel machine. This involves giving each of about 120 “particles” an initial position in parameter space, then letting them move around, trying to move to better positions according to a specific algorithm.  We gave each particle an initial position sampled from our prior distribution for each parameter.  So far we’ve run about 140 itera</p><p>2 0.78941673 <a title="160-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-17-Jumping_off_the_edge_of_the_world.html">858 andrew gelman stats-2011-08-17-Jumping off the edge of the world</a></p>
<p>Introduction: Tomas Iesmantas writes:
  
I’m facing a problem where parameter space is bounded, e.g. all parameters have to be positive. If in MCMC as proposal distribution I use normal distribution, then at some iterations I get negative proposals. So my question is: should I use recalculation of acceptance probability every time I reject the proposal (something like in delayed rejection method), or I have to use another proposal (like lognormal, truncated normal, etc.)?
  
The simplest solution is to just calculate p(theta)=0 for theta outside the legal region, thus reject those jumps.  This will work fine (just remember that when you reject, you have to stay at the last value for one more iteration), but if you’re doing these rejections all the time, you might want to reparameterize your space, for example using logs for positive parameters, logits for constrained parameters, and softmax for parameters that are constrained to sum to 1.</p><p>3 0.74628496 <a title="160-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-11-My_problem_with_the_Lindley_paradox.html">1757 andrew gelman stats-2013-03-11-My problem with the Lindley paradox</a></p>
<p>Introduction: From  a couple years ago but still relevant, I think:
  
To me, the Lindley paradox falls apart because of its noninformative prior distribution on the parameter of interest. If you really think there’s a high probability the parameter is nearly exactly zero, I don’t see the point of the model saying that you have no prior information at all on the parameter. In short: my criticism of so-called Bayesian hypothesis testing is that it’s insufficiently Bayesian.
  
P.S.  To clarify (in response to Bill’s comment below):  I’m speaking of all the examples I’ve ever worked on in social and environmental science, where in some settings I can imagine a parameter being very close to zero and in other settings I can imagine a parameter taking on just about any value in a wide range, but where I’ve never seen an example where a parameter could be  either  right at zero  or  taking on any possible value.  But such examples might occur in areas of application that I haven’t worked on.</p><p>4 0.73945421 <a title="160-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-07-X_on_JLP.html">1792 andrew gelman stats-2013-04-07-X on JLP</a></p>
<p>Introduction: Christian Robert  writes  on the Jeffreys-Lindley paradox.  I have nothing to add to this beyond my recent  comments :
  
To me, the Lindley paradox falls apart because of its noninformative prior distribution on the parameter of interest. If you really think there’s a high probability the parameter is nearly exactly zero, I don’t see the point of the model saying that you have no prior information at all on the parameter. In short: my criticism of so-called Bayesian hypothesis testing is that it’s insufficiently Bayesian.


To clarify, I’m speaking of all the examples I’ve ever worked on in social and environmental science, where in some settings I can imagine a parameter being very close to zero and in other settings I can imagine a parameter taking on just about any value in a wide range, but where I’ve never seen an example where a parameter could be either right at zero or taking on any possible value. But such examples might occur in areas of application that I haven’t worked on.</p><p>5 0.7084856 <a title="160-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-19-Whassup_with_deviance_having_a_high_posterior_correlation_with_a_parameter_in_the_model%3F.html">1221 andrew gelman stats-2012-03-19-Whassup with deviance having a high posterior correlation with a parameter in the model?</a></p>
<p>Introduction: Jean Richardson writes: 
  
  
Do you know what might lead to a large negative cross-correlation (-0.95) between deviance and one of the model parameters?


Here’s the (brief) background:


I [Richardson] have written a Bayesian hierarchical site occupancy model for presence of disease on individual amphibians. The response variable is therefore binary (disease present/absent) and the probability of disease being present in an individual (psi) depends on various covariates (species of amphibian, location sampled, etc.) paramaterized using a logit link function.  Replicates are individuals sampled (tested for presence of disease) together.  The possibility of imperfect detection is included as p = (prob. disease detected given disease is present).


Posterior distributions were estimated using WinBUGS via R2WinBUGS. 
Simulated data from the model fit the real data very well and posterior distribution densities seem robust to any changes in the model (different priors, etc.)  All autocor</p><p>6 0.69466311 <a title="160-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-20-Prior_beliefs_about_locations_of_decision_boundaries.html">1130 andrew gelman stats-2012-01-20-Prior beliefs about locations of decision boundaries</a></p>
<p>7 0.69420135 <a title="160-lsi-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-04-That_half-Cauchy_prior.html">184 andrew gelman stats-2010-08-04-That half-Cauchy prior</a></p>
<p>8 0.6936909 <a title="160-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>9 0.68704689 <a title="160-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-28-Path_sampling_for_models_of_varying_dimension.html">1089 andrew gelman stats-2011-12-28-Path sampling for models of varying dimension</a></p>
<p>10 0.6718483 <a title="160-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-31-Using_sample_size_in_the_prior_distribution.html">547 andrew gelman stats-2011-01-31-Using sample size in the prior distribution</a></p>
<p>11 0.66790831 <a title="160-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>12 0.66684741 <a title="160-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>13 0.66523594 <a title="160-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>14 0.65617806 <a title="160-lsi-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>15 0.65580654 <a title="160-lsi-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-16-Update_on_the_generalized_method_of_moments.html">519 andrew gelman stats-2011-01-16-Update on the generalized method of moments</a></p>
<p>16 0.64191878 <a title="160-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-28-Understanding_simulations_in_terms_of_predictive_inference%3F.html">1287 andrew gelman stats-2012-04-28-Understanding simulations in terms of predictive inference?</a></p>
<p>17 0.63772756 <a title="160-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-05-Monitor_the_efficiency_of_your_Markov_chain_sampler_using_expected_squared_jumped_distance%21.html">650 andrew gelman stats-2011-04-05-Monitor the efficiency of your Markov chain sampler using expected squared jumped distance!</a></p>
<p>18 0.63508791 <a title="160-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-07-Neutral_noninformative_and_informative_conjugate_beta_and_gamma_prior_distributions.html">1046 andrew gelman stats-2011-12-07-Neutral noninformative and informative conjugate beta and gamma prior distributions</a></p>
<p>19 0.63435525 <a title="160-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-15-Reputations_changeable%2C_situations_tolerable.html">1858 andrew gelman stats-2013-05-15-Reputations changeable, situations tolerable</a></p>
<p>20 0.63156056 <a title="160-lsi-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-06-Quote_of_the_day.html">398 andrew gelman stats-2010-11-06-Quote of the day</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(16, 0.047), (21, 0.018), (24, 0.203), (36, 0.011), (69, 0.013), (72, 0.011), (80, 0.011), (84, 0.034), (86, 0.03), (95, 0.023), (97, 0.245), (99, 0.232)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96966827 <a title="160-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-11-Incredibly_strange_spam.html">1573 andrew gelman stats-2012-11-11-Incredibly strange spam</a></p>
<p>Introduction: Unsolicited (of course) in the email the other day:
  
Just wanted to touch base with you to see if you needed any quotes on 
Parking lot lighting or Garage Lighting? (Induction, LED, Canopy etc…)


We help retrofit 1000′s of garages around the country.


Let me know your specs and ill send you a quote in 24 hours. 


** 
Owner 
Emergency Lights Co.
  
Ill indeed. . . .</p><p>same-blog 2 0.93574005 <a title="160-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-23-Unhappy_with_improvement_by_a_factor_of_10%5E29.html">160 andrew gelman stats-2010-07-23-Unhappy with improvement by a factor of 10^29</a></p>
<p>Introduction: I have an optimization problem:  I have a complicated physical model that predicts energy and thermal behavior of a building, given the values of a slew of parameters, such as insulation effectiveness, window transmissivity, etc.  I’m trying to find the parameter set that best fits several weeks of thermal and energy use data from the real building that we modeled. (Of course I would rather explore parameter space and come up with probability distributions for the parameters, and maybe that will come later, but for now I’m just optimizing).  To do the optimization, colleagues and I implemented a “particle swarm optimization” algorithm on a massively parallel machine. This involves giving each of about 120 “particles” an initial position in parameter space, then letting them move around, trying to move to better positions according to a specific algorithm.  We gave each particle an initial position sampled from our prior distribution for each parameter.  So far we’ve run about 140 itera</p><p>3 0.9089309 <a title="160-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-31-Meanwhile%2C_on_the_sister_blog_._._..html">882 andrew gelman stats-2011-08-31-Meanwhile, on the sister blog . . .</a></p>
<p>Introduction: NYT columnist Douthat asks: Should we be disturbed that a leading presidential candidate endorses a pro-slavery position? 
 
 Who’s on the web? And where are they? 
 
 Sowell, Carlson, Barone: fools, knaves, or simply victims of a cognitive illusion? 
 
 Don’t blame the American public for the D.C. deadlock 
 
 Calvin College update 
 
 Help reform the Institutional Review Board (IRB) system! 
 
 Powerful credit-rating agencies are a creation of the government . . . what does it mean when they bite the hand that feeds them? 
 
 “Waiting for a landslide” 
 
 A simple theory of why Obama didn’t come out fighting in 2009 
 
 A modest proposal 
 
 Noooooooooooooooo!!!!!!!!!!!!!!! 
 
 The Family Research Council and the Barnard Center for Research on Women 
 
 Sleazy data miners 
 
 Genetic essentialism is in our genes 
 
Wow, that was a lot!  No wonder I don’t get any research done…</p><p>4 0.90322012 <a title="160-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-07-Chi-square_FAIL_when_many_cells_have_small_expected_values.html">996 andrew gelman stats-2011-11-07-Chi-square FAIL when many cells have small expected values</a></p>
<p>Introduction: William Perkins, Mark Tygert, and Rachel Ward  write :
  
If a discrete probability distribution in a model being tested for goodness-of-fit is not close to uniform, then forming the Pearson χ2 statistic can involve division by nearly zero. This often leads to serious trouble in practice — even in the absence of round-off errors . . .
  
The problem is not merely that the chi-squared  statistic  doesn’t have the advertised chi-squared  distribution —a reference distribution can always be computed via simulation, either using the posterior predictive distribution or by conditioning on a point estimate of the cell expectations and then making a degrees-of-freedom sort of adjustment.
 
Rather, the problem is that, when there are lots of cells with near-zero expectation, the chi-squared test is mostly noise.
 
And this is not merely a theoretical problem.  It comes up in real examples.
 
Here’s one, taken from the classic 1992 genetics paper of Guo and Thomspson:
 
   
 
And here are the e</p><p>5 0.88576198 <a title="160-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-12-God%2C_Guns%2C_and_Gaydar%3A__The_Laws_of_Probability_Push_You_to_Overestimate_Small_Groups.html">142 andrew gelman stats-2010-07-12-God, Guns, and Gaydar:  The Laws of Probability Push You to Overestimate Small Groups</a></p>
<p>Introduction: Earlier today, Nate  criticized  a U.S. military survey that asks troops the question, “Do you currently serve with a male or female Service member you  believe  to be homosexual.” [emphasis added]  As Nate points out, by asking this question in such a speculative way, “it would seem that you’ll be picking up a tremendous number of false positives–soldiers who are believed to be gay, but aren’t–and that these false positives will swamp any instances in which soldiers (in spite of DADT) are actually somewhat open about their same-sex attractions.”
 
This is a general problem in survey research. In an  article  in Chance magazine in 1997, “The myth of millions of annual self-defense gun uses:  a case study of survey overestimates of rare events” [see  here  for related references], David Hemenway uses the false-positive, false-negative reasoning to explain this bias in terms of probability theory.  Misclassifications that induce seemingly minor biases in estimates of certain small probab</p><p>6 0.86378139 <a title="160-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-03-is_it_possible_to_%E2%80%9Coverstratify%E2%80%9D_when_assigning_a_treatment_in_a_randomized_control_trial%3F.html">553 andrew gelman stats-2011-02-03-is it possible to “overstratify” when assigning a treatment in a randomized control trial?</a></p>
<p>7 0.84818131 <a title="160-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-10-Three_hours_in_the_life_of_a_statistician.html">1001 andrew gelman stats-2011-11-10-Three hours in the life of a statistician</a></p>
<p>8 0.84778357 <a title="160-lda-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-04-30-Things_I_learned_from_the_Mickey_Kaus_for_Senate_campaign.html">13 andrew gelman stats-2010-04-30-Things I learned from the Mickey Kaus for Senate campaign</a></p>
<p>9 0.82906562 <a title="160-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-03-Faculty_Position_in_Visualization%2C_Visual_Analytics%2C_Imaging%2C_and_Human_Centered_Computing.html">1651 andrew gelman stats-2013-01-03-Faculty Position in Visualization, Visual Analytics, Imaging, and Human Centered Computing</a></p>
<p>10 0.81354797 <a title="160-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-30-%3F%3F%3F.html">2118 andrew gelman stats-2013-11-30-???</a></p>
<p>11 0.81176782 <a title="160-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-19-%E2%80%9CIf_it_saves_the_life_of_a_single_child%E2%80%A6%E2%80%9D_and_other_nonsense.html">526 andrew gelman stats-2011-01-19-“If it saves the life of a single child…” and other nonsense</a></p>
<p>12 0.79916263 <a title="160-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-25-Design_of_nonrandomized_cluster_sample_study.html">820 andrew gelman stats-2011-07-25-Design of nonrandomized cluster sample study</a></p>
<p>13 0.79337776 <a title="160-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-Correlation%2C_prediction%2C_variation%2C_etc..html">301 andrew gelman stats-2010-09-28-Correlation, prediction, variation, etc.</a></p>
<p>14 0.79239225 <a title="160-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-19-Chomsky_chomsky_chomsky_chomsky_furiously.html">1812 andrew gelman stats-2013-04-19-Chomsky chomsky chomsky chomsky furiously</a></p>
<p>15 0.79061294 <a title="160-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-03-How_best_to_compare_effects_measured_in_two_different_time_periods%3F.html">2086 andrew gelman stats-2013-11-03-How best to compare effects measured in two different time periods?</a></p>
<p>16 0.78976858 <a title="160-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-02-Should_personal_genetic_testing_be_regulated%3F__Battle_of_the_blogroll.html">2121 andrew gelman stats-2013-12-02-Should personal genetic testing be regulated?  Battle of the blogroll</a></p>
<p>17 0.78803372 <a title="160-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-06-%2463%2C000_worth_of_abusive_research_._._._or_just_a_really_stupid_waste_of_time%3F.html">18 andrew gelman stats-2010-05-06-$63,000 worth of abusive research . . . or just a really stupid waste of time?</a></p>
<p>18 0.787516 <a title="160-lda-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-13-An_Economist%E2%80%99s_Guide_to_Visualizing_Data.html">2246 andrew gelman stats-2014-03-13-An Economist’s Guide to Visualizing Data</a></p>
<p>19 0.78070527 <a title="160-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-03-Setting_aside_the_politics%2C_the_debate_over_the_new_health-care_study_reveals_that_we%E2%80%99re_moving_to_a_new_high_standard_of_statistical_journalism.html">1838 andrew gelman stats-2013-05-03-Setting aside the politics, the debate over the new health-care study reveals that we’re moving to a new high standard of statistical journalism</a></p>
<p>20 0.7805649 <a title="160-lda-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Ken_Rice_presents_a_unifying_approach_to_statistical_inference_and_hypothesis_testing.html">2312 andrew gelman stats-2014-04-29-Ken Rice presents a unifying approach to statistical inference and hypothesis testing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
