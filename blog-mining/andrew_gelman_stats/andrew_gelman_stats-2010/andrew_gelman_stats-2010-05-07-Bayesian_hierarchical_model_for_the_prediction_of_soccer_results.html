<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>20 andrew gelman stats-2010-05-07-Bayesian hierarchical model for the prediction of soccer results</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-20" href="#">andrew_gelman_stats-2010-20</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>20 andrew gelman stats-2010-05-07-Bayesian hierarchical model for the prediction of soccer results</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-20-html" href="http://andrewgelman.com/2010/05/07/bayesian_hierar_1/">html</a></p><p>Introduction: Gianluca Baio sends along  this article  (coauthored with Marta Blangiardo):
  
 
The problem of modelling football [soccer] data has become increasingly popular in the last few years and many different models have been proposed with the aim of estimating the characteristics that bring a team to lose or win a game, or to predict the score of a particular match. We propose a Bayesian hierarchical model to address both these aims and test its predictive strength on data about the Italian Serie A championship 1991-1992. To overcome the issue of overshrinkage produced by the Bayesian hierarchical model, we specify a more complex mixture model that results in better fit to the observed data. We test its performance using an example about the Italian Serie A championship 2007-2008.
 

 
I like the use of the hierarchical model and the focus on prediction.  I’m wondering, though, shouldn’t the model include a correlation between the “attack” and “defense” parameters?  Or maybe that’s in the m</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We propose a Bayesian hierarchical model to address both these aims and test its predictive strength on data about the Italian Serie A championship 1991-1992. [sent-2, score-0.937]
</p><p>2 To overcome the issue of overshrinkage produced by the Bayesian hierarchical model, we specify a more complex mixture model that results in better fit to the observed data. [sent-3, score-0.755]
</p><p>3 We test its performance using an example about the Italian Serie A championship 2007-2008. [sent-4, score-0.337]
</p><p>4 I like the use of the hierarchical model and the focus on prediction. [sent-5, score-0.328]
</p><p>5 I’m wondering, though, shouldn’t the model include a correlation between the “attack” and “defense” parameters? [sent-6, score-0.157]
</p><p>6 Or maybe that’s in the model but I didn’t notice it. [sent-7, score-0.296]
</p><p>7 Table 2 breaks every rule in the book–and I don’t mean that in a good way. [sent-9, score-0.167]
</p><p>8 Too many significant digits (if the 95% interval is [-0. [sent-10, score-0.181]
</p><p>9 06], then, no, you don’t need to report the posterior mean to four decimal places), no group-level predictors, and the teams are laid out in alphabetical order. [sent-12, score-0.582]
</p><p>10 Table 3 is much better (although would be much better as a graph, I think). [sent-13, score-0.172]
</p><p>11 The words on the graph are too tiny–they’re unreadable. [sent-15, score-0.091]
</p><p>12 And it would be my preference to add a few sentences to the caption to explain what’s going on. [sent-16, score-0.275]
</p><p>13 Figure 5 is looking pretty, but it reverts to the horrible, horrible alphabetical order–this time being made even worse by putting the alphabet in reverse. [sent-17, score-0.497]
</p><p>14 And the Winbugs code has that  discredited  dgamma (epsilon, epsilon) model. [sent-18, score-0.101]
</p><p>15 )  Not a huge deal, but something I notice because I’ve spent a lot of time thinking about it. [sent-20, score-0.139]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('serie', 0.291), ('italian', 0.25), ('epsilon', 0.25), ('championship', 0.239), ('alphabetical', 0.224), ('table', 0.198), ('hierarchical', 0.171), ('ugly', 0.158), ('model', 0.157), ('horrible', 0.141), ('notice', 0.139), ('alphabet', 0.132), ('oooh', 0.125), ('soccer', 0.119), ('winbugs', 0.112), ('caption', 0.109), ('decimal', 0.106), ('digits', 0.106), ('coauthored', 0.102), ('figure', 0.102), ('discredited', 0.101), ('modelling', 0.099), ('overcome', 0.099), ('test', 0.098), ('aims', 0.096), ('laid', 0.095), ('breaks', 0.092), ('graph', 0.091), ('specify', 0.09), ('propose', 0.089), ('aim', 0.088), ('strength', 0.087), ('better', 0.086), ('increasingly', 0.086), ('football', 0.085), ('tiny', 0.085), ('preference', 0.084), ('sentences', 0.082), ('teams', 0.082), ('characteristics', 0.082), ('flat', 0.081), ('defense', 0.08), ('attack', 0.079), ('produced', 0.076), ('mixture', 0.076), ('interval', 0.075), ('mean', 0.075), ('lose', 0.073), ('proposed', 0.072), ('score', 0.07)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="20-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-07-Bayesian_hierarchical_model_for_the_prediction_of_soccer_results.html">20 andrew gelman stats-2010-05-07-Bayesian hierarchical model for the prediction of soccer results</a></p>
<p>Introduction: Gianluca Baio sends along  this article  (coauthored with Marta Blangiardo):
  
 
The problem of modelling football [soccer] data has become increasingly popular in the last few years and many different models have been proposed with the aim of estimating the characteristics that bring a team to lose or win a game, or to predict the score of a particular match. We propose a Bayesian hierarchical model to address both these aims and test its predictive strength on data about the Italian Serie A championship 1991-1992. To overcome the issue of overshrinkage produced by the Bayesian hierarchical model, we specify a more complex mixture model that results in better fit to the observed data. We test its performance using an example about the Italian Serie A championship 2007-2008.
 

 
I like the use of the hierarchical model and the focus on prediction.  I’m wondering, though, shouldn’t the model include a correlation between the “attack” and “defense” parameters?  Or maybe that’s in the m</p><p>2 0.12615389 <a title="20-tfidf-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-31-A_data_visualization_manifesto.html">61 andrew gelman stats-2010-05-31-A data visualization manifesto</a></p>
<p>Introduction: Details matter (at least, they do for me), but we don’t yet have a systematic way of going back and forth between the structure of a graph, its details, and the underlying questions that motivate our visualizations.  (Cleveland, Wilkinson, and others have written a bit on how to formalize these connections, and I’ve thought about it too, but we have a ways to go.)
 
I was thinking about this difficulty after reading an article on graphics by some computer scientists that was well-written but to me lacked a feeling for the linkages between substantive/statistical goals and graphical details.  I have problems with these issues too, and my point here is not to criticize but to  move the discussion forward.
 
 When thinking about visualization, how important are the details? 
 
Aleks pointed me to  this article  by Jeffrey Heer, Michael Bostock, and Vadim Ogievetsky, “A Tour through the Visualization Zoo:  A survey of powerful visualization techniques, from the obvious to the obscure.”  Th</p><p>3 0.12118496 <a title="20-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-16-Recently_in_the_sister_blog.html">1765 andrew gelman stats-2013-03-16-Recently in the sister blog</a></p>
<p>Introduction: 1.  New Italian production of  Life on Mars .
 
  
 
2.  Psychological essentialism in  everyday thought .</p><p>4 0.1210143 <a title="20-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>Introduction: Nick Firoozye writes:
  
I had a question about BMA [Bayesian model averaging] and model combinations in general, and direct it to you since they are a basic form of hierarchical model, albeit in the simplest of forms. I wanted to ask what the underlying assumptions are that could lead to BMA improving on a larger model. 


I know model combination is a topic of interest in the (frequentist) econometrics community (e.g., Bates & Granger, http://www.jstor.org/discover/10.2307/3008764?uid=3738032&uid;=2&uid;=4&sid;=21101948653381) but at the time it was considered a bit of a puzzle. Perhaps small models combined outperform a big model due to standard errors, insufficient data, etc. But I haven’t seen much in way of Bayesian justification.


In simplest terms, you might have a joint density P(Y,theta_1,theta_2) from which you could use the two marginals P(Y,theta_1) and P(Y,theta_2) to derive two separate forecasts. A BMA-er would do a weighted average of the two forecast densities, having p</p><p>5 0.1190399 <a title="20-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>Introduction: In response to  my remarks  on his online book, Think Bayes, Allen Downey wrote: 
   
I [Downey] have a question about one of your comments: 
   My [Gelman's] main criticism with both books is that they talk a lot about inference but not so much about model building or model checking (recall the three steps of Bayesian data analysis). I think it’s ok for an introductory book to focus on inference, which of course is central to the data-analytic process—but I’d like them to at least mention that Bayesian ideas arise in model building and model checking as well. 

This sounds like something I agree with, and one of the things I tried to do in the book is to put modeling decisions front and center.  But the word “modeling” is used in lots of ways, so I want to see if we are talking about the same thing.


For example, in many chapters, I start with a simple model of the scenario, do some analysis, then check whether the model is good enough, and iterate.  Here’s the discussion of modeling</p><p>6 0.11686378 <a title="20-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-23-Examples_of_the_use_of_hierarchical_modeling_to_generalize_to_new_settings.html">1425 andrew gelman stats-2012-07-23-Examples of the use of hierarchical modeling to generalize to new settings</a></p>
<p>7 0.11636017 <a title="20-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>8 0.11326838 <a title="20-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-13-Flexibility_is_good.html">2133 andrew gelman stats-2013-12-13-Flexibility is good</a></p>
<p>9 0.11111827 <a title="20-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-02-Moving_beyond_hopeless_graphics.html">1403 andrew gelman stats-2012-07-02-Moving beyond hopeless graphics</a></p>
<p>10 0.11053169 <a title="20-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>11 0.10932391 <a title="20-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>12 0.10763197 <a title="20-tfidf-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-25-Basketball_Stats%3A__Don%E2%80%99t_model_the_probability_of_win%2C_model_the_expected_score_differential..html">2224 andrew gelman stats-2014-02-25-Basketball Stats:  Don’t model the probability of win, model the expected score differential.</a></p>
<p>13 0.10728092 <a title="20-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-25-Why_it_doesn%E2%80%99t_make_sense_in_general_to_form_confidence_intervals_by_inverting_hypothesis_tests.html">870 andrew gelman stats-2011-08-25-Why it doesn’t make sense in general to form confidence intervals by inverting hypothesis tests</a></p>
<p>14 0.10725413 <a title="20-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>15 0.10639762 <a title="20-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-24-Why_it_doesn%E2%80%99t_make_sense_in_general_to_form_confidence_intervals_by_inverting_hypothesis_tests.html">1913 andrew gelman stats-2013-06-24-Why it doesn’t make sense in general to form confidence intervals by inverting hypothesis tests</a></p>
<p>16 0.10273442 <a title="20-tfidf-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Useful_models%2C_model_checking%2C_and_external_validation%3A__a_mini-discussion.html">244 andrew gelman stats-2010-08-30-Useful models, model checking, and external validation:  a mini-discussion</a></p>
<p>17 0.10243525 <a title="20-tfidf-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-Stan_Model_of_the_Week%3A_Hierarchical_Modeling_of_Supernovas.html">2299 andrew gelman stats-2014-04-21-Stan Model of the Week: Hierarchical Modeling of Supernovas</a></p>
<p>18 0.10197522 <a title="20-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>19 0.098117463 <a title="20-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>20 0.09678638 <a title="20-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.194), (1, 0.118), (2, 0.02), (3, 0.073), (4, 0.061), (5, -0.049), (6, 0.016), (7, -0.001), (8, 0.038), (9, 0.015), (10, 0.03), (11, 0.027), (12, -0.04), (13, -0.033), (14, -0.013), (15, 0.005), (16, 0.068), (17, -0.023), (18, 0.024), (19, -0.024), (20, 0.01), (21, 0.037), (22, 0.005), (23, -0.031), (24, 0.03), (25, -0.029), (26, -0.02), (27, -0.03), (28, 0.007), (29, -0.04), (30, -0.034), (31, -0.06), (32, -0.008), (33, -0.031), (34, 0.016), (35, 0.016), (36, -0.011), (37, -0.037), (38, 0.024), (39, 0.005), (40, 0.022), (41, -0.011), (42, 0.019), (43, 0.02), (44, -0.022), (45, -0.045), (46, -0.001), (47, 0.008), (48, 0.024), (49, -0.0)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96070993 <a title="20-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-07-Bayesian_hierarchical_model_for_the_prediction_of_soccer_results.html">20 andrew gelman stats-2010-05-07-Bayesian hierarchical model for the prediction of soccer results</a></p>
<p>Introduction: Gianluca Baio sends along  this article  (coauthored with Marta Blangiardo):
  
 
The problem of modelling football [soccer] data has become increasingly popular in the last few years and many different models have been proposed with the aim of estimating the characteristics that bring a team to lose or win a game, or to predict the score of a particular match. We propose a Bayesian hierarchical model to address both these aims and test its predictive strength on data about the Italian Serie A championship 1991-1992. To overcome the issue of overshrinkage produced by the Bayesian hierarchical model, we specify a more complex mixture model that results in better fit to the observed data. We test its performance using an example about the Italian Serie A championship 2007-2008.
 

 
I like the use of the hierarchical model and the focus on prediction.  I’m wondering, though, shouldn’t the model include a correlation between the “attack” and “defense” parameters?  Or maybe that’s in the m</p><p>2 0.81023145 <a title="20-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-15-Wacky_priors_can_work_well%3F.html">1723 andrew gelman stats-2013-02-15-Wacky priors can work well?</a></p>
<p>Introduction: Dave Judkins writes:
  
I would love to see a blog entry on  this article , Bayesian Model Selection in High-Dimensional Settings, by Valen Johnson and David Rossell.  The simulation results are very encouraging although the choice of colors for some of the graphics is unfortunate.  Unless I am colorblind in some way that I am unaware of, they have two thin charcoal lines that are indistinguishable.
  
When Dave Judkins puts in a request, I’ll respond.  Also, I’m always happy to see a new Val Johnson paper.  Val and I are contemporaries—he and I got our PhD’s at around the same time, with both of us working on Bayesian image reconstruction, then in the early 1990s Val was part of the legendary group at Duke’s Institute of Statistics and Decision Sciences—a veritable ’27 Yankees featuring Mike West, Merlise Clyde, Michael Lavine, Dave Higdon, Peter Mueller, Val, and a bunch of others.  I always thought it was too bad they all had to go their separate ways.
 
Val also wrote two classic p</p><p>3 0.79648465 <a title="20-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>Introduction: Nick Firoozye writes:
  
I had a question about BMA [Bayesian model averaging] and model combinations in general, and direct it to you since they are a basic form of hierarchical model, albeit in the simplest of forms. I wanted to ask what the underlying assumptions are that could lead to BMA improving on a larger model. 


I know model combination is a topic of interest in the (frequentist) econometrics community (e.g., Bates & Granger, http://www.jstor.org/discover/10.2307/3008764?uid=3738032&uid;=2&uid;=4&sid;=21101948653381) but at the time it was considered a bit of a puzzle. Perhaps small models combined outperform a big model due to standard errors, insufficient data, etc. But I haven’t seen much in way of Bayesian justification.


In simplest terms, you might have a joint density P(Y,theta_1,theta_2) from which you could use the two marginals P(Y,theta_1) and P(Y,theta_2) to derive two separate forecasts. A BMA-er would do a weighted average of the two forecast densities, having p</p><p>4 0.77687401 <a title="20-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<p>Introduction: Becky Passonneau and colleagues at the Center for Computational Learning Systems (CCLS) at Columbia have been working on a project for ConEd (New York’s major electric utility) to  rank structures based on vulnerability to secondary events  (e.g., transformer explosions, cable meltdowns, electrical fires).  They’ve been using the R implementation  BayesTree  of Chipman, George and McCulloch’s  Bayesian Additive Regression Trees  (BART).
 
BART is a Bayesian non-parametric method that is non-identifiable in two ways.  Firstly, it is an additive tree model with a fixed number of trees, the indexes of which aren’t identified (you get the same predictions in a model swapping the order of the trees).  This is the same kind of non-identifiability you get with any mixture model (additive or interpolated) with an exchangeable prior on the mixture components.  Secondly, the trees themselves have varying structure over samples in terms of number of nodes and their topology (depth, branching, etc</p><p>5 0.77677739 <a title="20-lsi-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-08-Belief_aggregation.html">2162 andrew gelman stats-2014-01-08-Belief aggregation</a></p>
<p>Introduction: Johannes Castner writes:
  
Suppose there are k scientists, each with her own model (Bayesian Net) over m random variables.  Then, because the space of Bayesian Nets over these m variables, with the square-root of the Jensen-Shannon Divergence as a distance metric is a closed and bounded space, there exists one unique Bayes Net that is a mixture of the k model joint-distributions which is at equal distance to each of the k models and may be called a “consensus graph.”  This consensus graph is in turn a Bayes Net, which can be updated with evidence.  The first question is: What are the conditions for which, given a new bit of evidence, the updated consensus graph is exactly the same graph as the consensus graph of the updated k Bayes Nets? In other words, if we arrive at a synthetic model from k models and then update this synthetic model, under what conditions is this the same thing as if we had first updated all k models and then build a synthesis.  The second question is: If these ar</p><p>6 0.77429795 <a title="20-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-27-Visual_diagnostics_for_discrete-data_regressions.html">929 andrew gelman stats-2011-09-27-Visual diagnostics for discrete-data regressions</a></p>
<p>7 0.77229571 <a title="20-lsi-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-Transformations_for_non-normal_data.html">2176 andrew gelman stats-2014-01-19-Transformations for non-normal data</a></p>
<p>8 0.77226198 <a title="20-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-04-David_MacKay_and_Occam%E2%80%99s_Razor.html">1041 andrew gelman stats-2011-12-04-David MacKay and Occam’s Razor</a></p>
<p>9 0.77156776 <a title="20-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>10 0.7677514 <a title="20-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-08-I_Am_Too_Absolutely_Heteroskedastic_for_This_Probit_Model.html">1047 andrew gelman stats-2011-12-08-I Am Too Absolutely Heteroskedastic for This Probit Model</a></p>
<p>11 0.76596045 <a title="20-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-28-Understanding_simulations_in_terms_of_predictive_inference%3F.html">1287 andrew gelman stats-2012-04-28-Understanding simulations in terms of predictive inference?</a></p>
<p>12 0.76425672 <a title="20-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-16-%E2%80%9CReal_data_can_be_a_pain%E2%80%9D.html">1460 andrew gelman stats-2012-08-16-“Real data can be a pain”</a></p>
<p>13 0.76200223 <a title="20-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-11-Gelman_on_Hennig_on_Gelman_on_Bayes.html">1208 andrew gelman stats-2012-03-11-Gelman on Hennig on Gelman on Bayes</a></p>
<p>14 0.75740415 <a title="20-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-28-Using_predator-prey_models_on_the_Canadian_lynx_series.html">1141 andrew gelman stats-2012-01-28-Using predator-prey models on the Canadian lynx series</a></p>
<p>15 0.75561005 <a title="20-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-27-Cross-validation_%28What_is_it_good_for%3F%29.html">1395 andrew gelman stats-2012-06-27-Cross-validation (What is it good for?)</a></p>
<p>16 0.75343788 <a title="20-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>17 0.752289 <a title="20-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Adding_more_information_can_make_the_variance_go_up_%28depending_on_your_model%29.html">810 andrew gelman stats-2011-07-20-Adding more information can make the variance go up (depending on your model)</a></p>
<p>18 0.74979752 <a title="20-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>19 0.74547619 <a title="20-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-06-Bayesian_model-building_by_pure_thought%3A__Some_principles_and_examples.html">1156 andrew gelman stats-2012-02-06-Bayesian model-building by pure thought:  Some principles and examples</a></p>
<p>20 0.74065703 <a title="20-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.034), (16, 0.057), (21, 0.049), (24, 0.158), (43, 0.01), (44, 0.039), (53, 0.012), (55, 0.016), (58, 0.011), (63, 0.038), (79, 0.011), (86, 0.063), (92, 0.12), (95, 0.033), (99, 0.236)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94421482 <a title="20-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-07-Bayesian_hierarchical_model_for_the_prediction_of_soccer_results.html">20 andrew gelman stats-2010-05-07-Bayesian hierarchical model for the prediction of soccer results</a></p>
<p>Introduction: Gianluca Baio sends along  this article  (coauthored with Marta Blangiardo):
  
 
The problem of modelling football [soccer] data has become increasingly popular in the last few years and many different models have been proposed with the aim of estimating the characteristics that bring a team to lose or win a game, or to predict the score of a particular match. We propose a Bayesian hierarchical model to address both these aims and test its predictive strength on data about the Italian Serie A championship 1991-1992. To overcome the issue of overshrinkage produced by the Bayesian hierarchical model, we specify a more complex mixture model that results in better fit to the observed data. We test its performance using an example about the Italian Serie A championship 2007-2008.
 

 
I like the use of the hierarchical model and the focus on prediction.  I’m wondering, though, shouldn’t the model include a correlation between the “attack” and “defense” parameters?  Or maybe that’s in the m</p><p>2 0.94253242 <a title="20-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-20-Paul_Rosenbaum_on_those_annoying_pre-treatment_variables_that_are_sort-of_instruments_and_sort-of_covariates.html">287 andrew gelman stats-2010-09-20-Paul Rosenbaum on those annoying pre-treatment variables that are sort-of instruments and sort-of covariates</a></p>
<p>Introduction: Last year  we discussed  an important challenge in causal inference:  The standard advice (given in many books, including ours) for causal inference is to control for relevant pre-treatment variables as much as possible.  But, as Judea Pearl has pointed out, instruments (as in “instrumental variables”) are pre-treatment variables that we would  not  want to “control for” in a matching or regression sense.
 
At first, this seems like a minor modification, with the new recommendation being to apply instrumental variables estimation using all pre-treatment instruments, and to control for all other pre-treatment variables.  But that can’t really work as general advice.  What about weak instruments or covariates that have some instrumental aspects?
 
I asked Paul Rosenbaum for his thoughts on the matter, and he wrote the following:
  
In section 18.2 of Design of Observational Studies (DOS), I [Rosenbaum] discuss “seemingly innocuous confounding” defined to be a covariate that predicts a su</p><p>3 0.93037164 <a title="20-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-12-That_claim_that_Harvard_admissions_discriminate_in_favor_of_Jews%3F__After_seeing_the_statistics%2C_I_don%E2%80%99t_see_it..html">1720 andrew gelman stats-2013-02-12-That claim that Harvard admissions discriminate in favor of Jews?  After seeing the statistics, I don’t see it.</a></p>
<p>Introduction: A few months ago  we discussed  Ron Unz’s claim that Jews are massively overrepresented in Ivy League college admissions, not just in comparison to the general population of college-age Americans, but even in comparison to other white kids with comparable academic ability and preparation.
 
Most of Unz’s article concerns admissions of Asian-Americans, and he also has a proposal to admit certain students at random (see my discussion in the link above).  In the present post, I concentrate on the statistics about Jewish students, because this is where I have learned that his statistics are particularly suspect, with various numbers being off by factors of 2 or 4 or more.
 
Unz’s  article  was discussed, largely favorably, by academic bloggers  Tyler Cowen ,  Steve Hsu , and . . . me!  Hsu writes:  “Don’t miss the statistical supplement.”  But a lot of our trust in those statistics seems to be misplaced.  Some people have sent me some information showing serious problems with Unz’s methods</p><p>4 0.92792708 <a title="20-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-05-Someone_is_wrong_on_the_internet%2C_part_2.html">1563 andrew gelman stats-2012-11-05-Someone is wrong on the internet, part 2</a></p>
<p>Introduction: My coblogger John Sides  feeds  a troll.  It’s a tough call.  Yesterday I gave my  reasoning  for ignoring these provocateurs, but in this case the troll in question is writing for a major newspaper so it makes sense for John to go to the trouble of shooting him down.  Even though I suspect the columnist was trolling for no better reason than . . . he had a deadline and nothing to say so he thought he’d wade into a controversy.
 
On the plus side, as a statistician I’m happy that statistics is considered important enough that it’s worth trolling!  When they start attacking like this, they must feel a bit on the defensive. . . .</p><p>5 0.92011106 <a title="20-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-11-Kaiser_Fung_on_how_not_to_critique_models.html">1004 andrew gelman stats-2011-11-11-Kaiser Fung on how not to critique models</a></p>
<p>Introduction: In the context of a debate between economists Brad DeLong and Tyler Cowen on the “IS-LM model” [no, I don't know what it is, either!], Kaiser  writes :
  
Since a model is an abstraction, a simplification of reality, no model is above critique.


I [Kaiser] consider the following types of critique not deserving:


1) The critique that the modeler makes an assumption 
2) The critique that the modeler makes an assumption for mathematical convenience 
3) The critique that the model omits some feature 
4) The critique that the model doesn’t fit one’s intuition 
5) The critique that the model fails to make a specific prediction


Above all, a serious critique must include an alternative model that is provably better than the one it criticises. It is not enough to show that the alternative solves the problems being pointed out; the alternative must do so while preserving the useful aspects of the model being criticized.
  
I have mixed feelings about Kaiser’s rules.  On one hand, I agree wit</p><p>6 0.90136594 <a title="20-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-22-Ivy_Jew_update.html">2073 andrew gelman stats-2013-10-22-Ivy Jew update</a></p>
<p>7 0.90029228 <a title="20-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-09-Blogging%2C_polemical_and_otherwise.html">1108 andrew gelman stats-2012-01-09-Blogging, polemical and otherwise</a></p>
<p>8 0.89545858 <a title="20-lda-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-Correlation%2C_prediction%2C_variation%2C_etc..html">301 andrew gelman stats-2010-09-28-Correlation, prediction, variation, etc.</a></p>
<p>9 0.89485276 <a title="20-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-28-Different_modes_of_discourse.html">1743 andrew gelman stats-2013-02-28-Different modes of discourse</a></p>
<p>10 0.89312816 <a title="20-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-02-So_much_artistic_talent.html">1785 andrew gelman stats-2013-04-02-So much artistic talent</a></p>
<p>11 0.89218092 <a title="20-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-20-Unz_on_Unz.html">1730 andrew gelman stats-2013-02-20-Unz on Unz</a></p>
<p>12 0.89081573 <a title="20-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-06-Early_stopping_and_penalized_likelihood.html">788 andrew gelman stats-2011-07-06-Early stopping and penalized likelihood</a></p>
<p>13 0.89052832 <a title="20-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-25-An_interesting_but_flawed_attempt_to_apply_general_forecasting_principles_to_contextualize_attitudes_toward_risks_of_global_warming.html">2112 andrew gelman stats-2013-11-25-An interesting but flawed attempt to apply general forecasting principles to contextualize attitudes toward risks of global warming</a></p>
<p>14 0.890288 <a title="20-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-06-Janet_Mertz%E2%80%99s_response_to_%E2%80%9CThe_Myth_of_American_Meritocracy%E2%80%9D.html">1751 andrew gelman stats-2013-03-06-Janet Mertz’s response to “The Myth of American Meritocracy”</a></p>
<p>15 0.88961732 <a title="20-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-18-%E2%80%9CI_was_finding_the_test_so_irritating_and_boring_that_I_just_started_to_click_through_as_fast_as_I_could%E2%80%9D.html">351 andrew gelman stats-2010-10-18-“I was finding the test so irritating and boring that I just started to click through as fast as I could”</a></p>
<p>16 0.88932765 <a title="20-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>17 0.88925338 <a title="20-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-13-What_are_the_important_issues_in_ethics_and_statistics%3F__I%E2%80%99m_looking_for_your_input%21.html">1117 andrew gelman stats-2012-01-13-What are the important issues in ethics and statistics?  I’m looking for your input!</a></p>
<p>18 0.88903928 <a title="20-lda-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>19 0.88896924 <a title="20-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-04-Cash_in%2C_cash_out_graph.html">502 andrew gelman stats-2011-01-04-Cash in, cash out graph</a></p>
<p>20 0.88882917 <a title="20-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-04-Don%E2%80%99t_any_statisticians_work_for_the_IRS%3F.html">693 andrew gelman stats-2011-05-04-Don’t any statisticians work for the IRS?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
