<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>151 andrew gelman stats-2010-07-16-Wanted:  Probability distributions for rank orderings</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-151" href="#">andrew_gelman_stats-2010-151</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>151 andrew gelman stats-2010-07-16-Wanted:  Probability distributions for rank orderings</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-151-html" href="http://andrewgelman.com/2010/07/16/wanted_probabil/">html</a></p><p>Introduction: Dietrich Stoyan writes:
  
 
I asked the IMS people for an expert in statistics of voting/elections and they wrote me your name. I am a statistician, but never worked in the field voting/elections. It was my son-in-law who asked me for statistical theories in that field.


He posed in particular the following problem:


The aim of the voting is to come to a ranking of c candidates. Every vote is a permutation of these c candidates. The problem is to have probability distributions in the set of all permutations of c elements.


Are there theories for such distributions?


I should be very grateful for a fast answer with hints to literature. (I confess that I do not know your books.) 
 

 
My reply:  Rather than trying to model the ranks directly, Iâ&euro;&trade;d recommend modeling a latent continuous outcome which then implies a distribution on ranks, if the ranks are of interest. There are lots of distributions of c-dimensional continuous outcomes.  In political science, the usual way to start is</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Dietrich Stoyan writes:      I asked the IMS people for an expert in statistics of voting/elections and they wrote me your name. [sent-1, score-0.242]
</p><p>2 I am a statistician, but never worked in the field voting/elections. [sent-2, score-0.213]
</p><p>3 It was my son-in-law who asked me for statistical theories in that field. [sent-3, score-0.362]
</p><p>4 He posed in particular the following problem:   The aim of the voting is to come to a ranking of c candidates. [sent-4, score-0.694]
</p><p>5 Every vote is a permutation of these c candidates. [sent-5, score-0.252]
</p><p>6 The problem is to have probability distributions in the set of all permutations of c elements. [sent-6, score-0.672]
</p><p>7 I should be very grateful for a fast answer with hints to literature. [sent-8, score-0.514]
</p><p>8 )       My reply:  Rather than trying to model the ranks directly, Iâ&euro;&trade;d recommend modeling a latent continuous outcome which then implies a distribution on ranks, if the ranks are of interest. [sent-10, score-1.871]
</p><p>9 There are lots of distributions of c-dimensional continuous outcomes. [sent-11, score-0.525]
</p><p>10 In political science, the usual way to start is to model the positions of the candidates and of the voters, and then to have a model mapping relative positions to relative preferences. [sent-12, score-1.524]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ranks', 0.471), ('distributions', 0.266), ('positions', 0.23), ('theories', 0.214), ('continuous', 0.201), ('permutations', 0.199), ('relative', 0.192), ('ims', 0.188), ('hints', 0.18), ('confess', 0.168), ('grateful', 0.164), ('permutation', 0.164), ('posed', 0.154), ('ranking', 0.151), ('asked', 0.148), ('mapping', 0.133), ('aim', 0.132), ('preferences', 0.121), ('latent', 0.12), ('model', 0.118), ('implies', 0.114), ('candidates', 0.111), ('fast', 0.107), ('voters', 0.103), ('voting', 0.095), ('expert', 0.094), ('outcome', 0.093), ('vote', 0.088), ('problem', 0.087), ('recommend', 0.087), ('worked', 0.081), ('usual', 0.08), ('statistician', 0.079), ('field', 0.078), ('directly', 0.075), ('modeling', 0.069), ('distribution', 0.069), ('start', 0.066), ('probability', 0.063), ('answer', 0.063), ('every', 0.062), ('lots', 0.058), ('trying', 0.058), ('reply', 0.058), ('come', 0.058), ('set', 0.057), ('never', 0.054), ('political', 0.054), ('following', 0.052), ('particular', 0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="151-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-16-Wanted%3A__Probability_distributions_for_rank_orderings.html">151 andrew gelman stats-2010-07-16-Wanted:  Probability distributions for rank orderings</a></p>
<p>Introduction: Dietrich Stoyan writes:
  
 
I asked the IMS people for an expert in statistics of voting/elections and they wrote me your name. I am a statistician, but never worked in the field voting/elections. It was my son-in-law who asked me for statistical theories in that field.


He posed in particular the following problem:


The aim of the voting is to come to a ranking of c candidates. Every vote is a permutation of these c candidates. The problem is to have probability distributions in the set of all permutations of c elements.


Are there theories for such distributions?


I should be very grateful for a fast answer with hints to literature. (I confess that I do not know your books.) 
 

 
My reply:  Rather than trying to model the ranks directly, Iâ&euro;&trade;d recommend modeling a latent continuous outcome which then implies a distribution on ranks, if the ranks are of interest. There are lots of distributions of c-dimensional continuous outcomes.  In political science, the usual way to start is</p><p>2 0.23397486 <a title="151-tfidf-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-09-Using_ranks_as_numbers.html">136 andrew gelman stats-2010-07-09-Using ranks as numbers</a></p>
<p>Introduction: David Shor writes:
  
I’m dealing with a situation where I have two datasets, one that assigns each participant a discrete score out of five for a set of particular traits (Dog behavior characteristics by breed), and another from an independent source that ranks each breed by each characteristic. It’s also possible to obtain the results of a survey, where experts were asked to rank 7 randomly picked breeds by characteristics.


I’m interested in obtaining estimates for each trait, and intuitively, it seems clear that the second and third dataset provide a lot of information. But it’s unclear how to incorporate them to infer latent variables, since only sample ranks are observed. This seems like it is a common problem, do you have any suggestions?
  
My quick answer is that you can treat ranks as numbers (a point we make somewhere in Bayesian Data Analysis, I believe) and just fit an item-response model from there.
 
Val Johnson wrote an article on this in Jasa a few years ago, “Bayesia</p><p>3 0.16629958 <a title="151-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-25-Continuous_variables_in_Bayesian_networks.html">1228 andrew gelman stats-2012-03-25-Continuous variables in Bayesian networks</a></p>
<p>Introduction: Antti Rasinen writes:
  
I’m a former undergrad machine learning student and a current software engineer with a Bayesian hobby. Today my two worlds collided. I ask for some enlightenment.


On your blog you’ve repeatedly advocated continuous distributions with Bayesian models. Today I read  this article  by Ricky Ho, who writes:

 
The strength of Bayesian network is it is highly scalable and can learn incrementally because all we do is to count the observed variables and update the probability distribution table. Similar to Neural Network, Bayesian network expects all data to be binary, categorical variable will need to be transformed into multiple binary variable as described above. Numeric variable is generally not a good fit for Bayesian network.
 

The last sentence seems to be at odds with what you’ve said. Sadly, I don’t have enough expertise to say which view of the world is correct. During my undergrad years our team wrote an implementation of the Junction Tree algorithm. We r</p><p>4 0.15550032 <a title="151-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-03-Statistical_methods_for_healthcare_regulation%3A_rating%2C_screening_and_surveillance.html">744 andrew gelman stats-2011-06-03-Statistical methods for healthcare regulation: rating, screening and surveillance</a></p>
<p>Introduction: Here is my discussion of  a recent article  by David Spiegelhalter, Christopher 
Sherlaw-Johnson, Martin Bardsley, Ian Blunt, Christopher Wood and Olivia Grigg, that is scheduled to appear in the Journal of the Royal Statistical Society:
 
I applaud the authors’ use of a mix of statistical methods to attack an important real-world problem. Policymakers need results right away, and I admire the authors’ ability and willingness to combine several different modeling and significance testing ideas for the purposes of rating and surveillance.
 
That said, I am uncomfortable with the statistical ideas here, for three reasons. First, I feel that the proposed methods, centered as they are around data manipulation and corrections for uncertainty, has serious defects compared to a more model-based approach. My problem with methods based on p-values and z-scores–however they happen to be adjusted–is that they draw discussion toward error rates, sequential analysis, and other technical statistical</p><p>5 0.10662965 <a title="151-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>Introduction: Ilya Esteban writes:
  
In traditional machine learning and statistical learning techniques, you spend a lot of time selecting your input features, fiddling with model parameter values, etc., all of which leads to the problem of overfitting the data and producing overly optimistic estimates for how good the model really is. You can use techniques such as cross-validation and out-of-sample validation data to try to limit the damage, but they are imperfect solutions at best.


While Bayesian models have the great advantage of not forcing you to manually select among the various weights and input features, you still often end up trying different priors and model structures (especially with hierarchical models), before coming up with a “final” model. When applying Bayesian modeling to real world data sets, how does should you evaluate alternate priors and topologies for the model without falling into the same overfitting trap as you do with non-Bayesian models? If you try several different</p><p>6 0.10461695 <a title="151-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>7 0.10417613 <a title="151-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-28-The_Supreme_Court%E2%80%99s_Many_Median_Justices.html">1234 andrew gelman stats-2012-03-28-The Supreme Court’s Many Median Justices</a></p>
<p>8 0.095040724 <a title="151-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-21-The_future_%28and_past%29_of_statistical_sciences.html">2072 andrew gelman stats-2013-10-21-The future (and past) of statistical sciences</a></p>
<p>9 0.094126277 <a title="151-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-01-Martin_and_Liu%3A__Probabilistic_inference_based_on_consistency_of_model_with_data.html">1095 andrew gelman stats-2012-01-01-Martin and Liu:  Probabilistic inference based on consistency of model with data</a></p>
<p>10 0.093286186 <a title="151-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>11 0.092323624 <a title="151-tfidf-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-26-Econometrics%2C_political_science%2C_epidemiology%2C_etc.%3A__Don%E2%80%99t_model_the_probability_of_a_discrete_outcome%2C_model_the_underlying_continuous_variable.html">2226 andrew gelman stats-2014-02-26-Econometrics, political science, epidemiology, etc.:  Don’t model the probability of a discrete outcome, model the underlying continuous variable</a></p>
<p>12 0.091338247 <a title="151-tfidf-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-01-References_on_predicting_elections.html">249 andrew gelman stats-2010-09-01-References on predicting elections</a></p>
<p>13 0.09063036 <a title="151-tfidf-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-01-Why_it_can_be_rational_to_vote.html">389 andrew gelman stats-2010-11-01-Why it can be rational to vote</a></p>
<p>14 0.09063036 <a title="151-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-06-Why_it_can_be_rational_to_vote.html">1565 andrew gelman stats-2012-11-06-Why it can be rational to vote</a></p>
<p>15 0.090049386 <a title="151-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>16 0.085655376 <a title="151-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-18-Understanding_posterior_p-values.html">2029 andrew gelman stats-2013-09-18-Understanding posterior p-values</a></p>
<p>17 0.084131636 <a title="151-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-09-There%E2%80%99s_no_evidence_that_voters_choose_presidential_candidates_based_on_their_looks.html">654 andrew gelman stats-2011-04-09-There’s no evidence that voters choose presidential candidates based on their looks</a></p>
<p>18 0.081891328 <a title="151-tfidf-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-20-Thermodynamic_Monte_Carlo%3A__Michael_Betancourt%E2%80%99s_new_method_for_simulating_from_difficult_distributions_and_evaluating_normalizing_constants.html">2340 andrew gelman stats-2014-05-20-Thermodynamic Monte Carlo:  Michael Betancourt’s new method for simulating from difficult distributions and evaluating normalizing constants</a></p>
<p>19 0.079251133 <a title="151-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>20 0.078446113 <a title="151-tfidf-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-Transformations_for_non-normal_data.html">2176 andrew gelman stats-2014-01-19-Transformations for non-normal data</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.128), (1, 0.068), (2, 0.05), (3, 0.058), (4, -0.03), (5, 0.024), (6, -0.039), (7, -0.005), (8, 0.016), (9, -0.006), (10, 0.032), (11, 0.039), (12, -0.016), (13, -0.038), (14, -0.055), (15, -0.016), (16, -0.006), (17, -0.025), (18, 0.024), (19, -0.021), (20, 0.033), (21, -0.034), (22, 0.015), (23, -0.062), (24, 0.001), (25, 0.007), (26, -0.002), (27, -0.005), (28, -0.011), (29, -0.029), (30, -0.019), (31, 0.002), (32, -0.014), (33, 0.025), (34, -0.004), (35, -0.027), (36, 0.019), (37, 0.006), (38, -0.049), (39, 0.019), (40, 0.02), (41, -0.018), (42, -0.008), (43, -0.011), (44, -0.01), (45, -0.044), (46, 0.018), (47, -0.011), (48, -0.023), (49, 0.023)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.965101 <a title="151-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-16-Wanted%3A__Probability_distributions_for_rank_orderings.html">151 andrew gelman stats-2010-07-16-Wanted:  Probability distributions for rank orderings</a></p>
<p>Introduction: Dietrich Stoyan writes:
  
 
I asked the IMS people for an expert in statistics of voting/elections and they wrote me your name. I am a statistician, but never worked in the field voting/elections. It was my son-in-law who asked me for statistical theories in that field.


He posed in particular the following problem:


The aim of the voting is to come to a ranking of c candidates. Every vote is a permutation of these c candidates. The problem is to have probability distributions in the set of all permutations of c elements.


Are there theories for such distributions?


I should be very grateful for a fast answer with hints to literature. (I confess that I do not know your books.) 
 

 
My reply:  Rather than trying to model the ranks directly, Iâ&euro;&trade;d recommend modeling a latent continuous outcome which then implies a distribution on ranks, if the ranks are of interest. There are lots of distributions of c-dimensional continuous outcomes.  In political science, the usual way to start is</p><p>2 0.78522766 <a title="151-lsi-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-26-Econometrics%2C_political_science%2C_epidemiology%2C_etc.%3A__Don%E2%80%99t_model_the_probability_of_a_discrete_outcome%2C_model_the_underlying_continuous_variable.html">2226 andrew gelman stats-2014-02-26-Econometrics, political science, epidemiology, etc.:  Don’t model the probability of a discrete outcome, model the underlying continuous variable</a></p>
<p>Introduction: This is an echo of yesterday’s post,  Basketball Stats: Don’t model the probability of win, model the expected score differential .
 
As with basketball, so with baseball:  as the great Bill James wrote, if you want to predict a pitcher’s win-loss record, it’s better to use last year’s ERA than last year’s W-L.
 
As with basketball and baseball, so with epidemiology:  as Joseph Delaney  points out  in my favorite blog that nobody reads, you will see much better prediction if you first model change in the parameter (e.g. blood pressure) and then convert that to the binary disease state (e.g. hypertension) then if you just develop a logistic model for prob(hypertension).
 
As with basketball, baseball, and epidemiology, so with political science:  instead of modeling election winners, better to model vote differential, a point that I made back in 1993 (see page 120  here ) but which seems to continually need  repeating .  A forecasting method should get essentially no credit for correctl</p><p>3 0.73928857 <a title="151-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-19-The_%E2%80%9Ceither-or%E2%80%9D_fallacy_of_believing_in_discrete_models%3A__an_example_of_folk_statistics.html">217 andrew gelman stats-2010-08-19-The “either-or” fallacy of believing in discrete models:  an example of folk statistics</a></p>
<p>Introduction: Psychologists talk about “folk psychology”:  ideas that make sense to us about how people think and behave, even if these ideas are not accurate descriptions of reality.  And physicists talk about “folk physics” (for example, the idea that a thrown ball falls in a straight line and then suddenly drops, rather than following an approximate parabola).
 
There’s also “folk statistics.”  Some of the ideas of folk statistics are so strong that even educated people–even well-known researchers–can make these mistakes.
 
One of the ideas of folk statistics that bothers me a lot is what might be called the “either/or fallacy”:  the idea that if there are two possible stories, the truth has to be one or the other.
 
I have often encountered the either/or fallacy in Bayesian statistics, for example the vast literature on “model selection” or “variable selection” or “model averaging” in which it is assumed that one of some pre-specified discrete set of models is the truth, and that this true model</p><p>4 0.73522043 <a title="151-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-18-Understanding_posterior_p-values.html">2029 andrew gelman stats-2013-09-18-Understanding posterior p-values</a></p>
<p>Introduction: David Kaplan writes:
  
I came across your  paper  “Understanding Posterior Predictive P-values”, and I have a question regarding your statement “If a posterior predictive p-value is 0.4, say, that means that, if we believe the model, we think there is a 40% chance that tomorrow’s value of T(y_rep) will exceed today’s T(y).” This is perfectly understandable to me and represents the idea of calibration.  However, I am unsure how this relates to statements about fit.  If T is the LR chi-square or Pearson chi-square, then your statement that there is a 40% chance that tomorrows value exceeds today’s value indicates bad fit, I think.  Yet, some literature indicates that high p-values suggest good fit.  Could you clarify this?
  
My reply:
 
I think that “fit” depends on the question being asked.  In this case, I’d say the model fits for this particular purpose, even though it might not fit for other purposes.
 
And here’s the abstract of the paper:
  
Posterior predictive p-values do not i</p><p>5 0.73206371 <a title="151-lsi-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-25-Basketball_Stats%3A__Don%E2%80%99t_model_the_probability_of_win%2C_model_the_expected_score_differential..html">2224 andrew gelman stats-2014-02-25-Basketball Stats:  Don’t model the probability of win, model the expected score differential.</a></p>
<p>Introduction: Someone who wants to remain anonymous writes:
  
I am working to create a more accurate in-game win probability model for basketball games. My idea is for each timestep in a game (a second, 5 seconds, etc), use the Vegas line, the current score differential, who has the ball, and the number of possessions played already (to account for differences in pace) to create a point estimate probability of the home team winning.


This problem would seem to fit a multi-level model structure well. It seems silly to estimate 2,000 regressions (one for each timestep), but the coefficients should vary at each timestep. Do you have suggestions for what type of model this could/would be? Additionally, I believe this needs to be some form of logit/probit given the binary dependent variable (win or loss).


Finally, do you have suggestions for what package could accomplish this in Stata or R?
  
To answer the questions in reverse order: 
3.  I’d hope this could be done in Stan (which can be run from R)</p><p>6 0.71382266 <a title="151-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>7 0.705841 <a title="151-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-29-Putting_together_multinomial_discrete_regressions_by_combining_simple_logits.html">782 andrew gelman stats-2011-06-29-Putting together multinomial discrete regressions by combining simple logits</a></p>
<p>8 0.70223361 <a title="151-lsi-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-01-References_on_predicting_elections.html">249 andrew gelman stats-2010-09-01-References on predicting elections</a></p>
<p>9 0.70123869 <a title="151-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-24-Deviance_as_a_difference.html">729 andrew gelman stats-2011-05-24-Deviance as a difference</a></p>
<p>10 0.6941222 <a title="151-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-19-Whassup_with_deviance_having_a_high_posterior_correlation_with_a_parameter_in_the_model%3F.html">1221 andrew gelman stats-2012-03-19-Whassup with deviance having a high posterior correlation with a parameter in the model?</a></p>
<p>11 0.69146538 <a title="151-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-03-Popper_and_Jaynes.html">2007 andrew gelman stats-2013-09-03-Popper and Jaynes</a></p>
<p>12 0.69064438 <a title="151-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-27-Cross-validation_%28What_is_it_good_for%3F%29.html">1395 andrew gelman stats-2012-06-27-Cross-validation (What is it good for?)</a></p>
<p>13 0.68584585 <a title="151-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>14 0.68537045 <a title="151-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-15-Wacky_priors_can_work_well%3F.html">1723 andrew gelman stats-2013-02-15-Wacky priors can work well?</a></p>
<p>15 0.6838668 <a title="151-lsi-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-12-UnConMax_%E2%80%93_uncertainty_consideration_maxims_7_%2B--_2.html">82 andrew gelman stats-2010-06-12-UnConMax – uncertainty consideration maxims 7 +-- 2</a></p>
<p>16 0.68338621 <a title="151-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>17 0.68113977 <a title="151-lsi-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-21-Models_with_constraints.html">2342 andrew gelman stats-2014-05-21-Models with constraints</a></p>
<p>18 0.68046463 <a title="151-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-26-Modeling_probability_data.html">1284 andrew gelman stats-2012-04-26-Modeling probability data</a></p>
<p>19 0.67832261 <a title="151-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-Another_reason_why_you_can_get_good_inferences_from_a_bad_model.html">1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</a></p>
<p>20 0.67473835 <a title="151-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(9, 0.061), (16, 0.036), (21, 0.4), (24, 0.097), (44, 0.014), (86, 0.012), (95, 0.02), (99, 0.234)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97718829 <a title="151-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-27-Banned_in_NYC_school_tests.html">1232 andrew gelman stats-2012-03-27-Banned in NYC school tests</a></p>
<p>Introduction: The list  includes “hunting” but not “fishing,” so that’s cool.  I wonder how they’d feel about a question involving different cuts of meat.  In any case, I’m happy to see that  “Bayes”  is not on the banned list.
 
P.S.  Russell  explains .</p><p>2 0.93200201 <a title="151-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-20-The_R_code_for_those_time-use_graphs.html">672 andrew gelman stats-2011-04-20-The R code for those time-use graphs</a></p>
<p>Introduction: By popular demand, hereâ&euro;&trade;s my R script for the  time-use graphs :
  

 
 
# The data
a1 <- c(4.2,3.2,11.1,1.3,2.2,2.0)
a2 <- c(3.9,3.2,10.0,0.8,3.1,3.1)
a3 <- c(6.3,2.5,9.8,0.9,2.2,2.4)
a4 <- c(4.4,3.1,9.8,0.8,3.3,2.7)
a5 <- c(4.8,3.0,9.9,0.7,3.3,2.4)
a6 <- c(4.0,3.4,10.5,0.7,3.3,2.1)
a <- rbind(a1,a2,a3,a4,a5,a6)
avg <- colMeans (a)
avg.array <- t (array (avg, rev(dim(a))))
diff <- a - avg.array
country.name <- c("France", "Germany", "Japan", "Britain", "USA", "Turkey")

# The line plots

par (mfrow=c(2,3), mar=c(4,4,2,.5), mgp=c(2,.7,0), tck=-.02, oma=c(3,0,4,0),
  bg="gray96", fg="gray30")
for (i in 1:6){
  plot (c(1,6), c(-1,1.7), xlab="", ylab="", xaxt="n", yaxt="n",
    bty="l", type="n")
  lines (1:6, diff[i,], col="blue")
  points (1:6, diff[i,], pch=19, col="black")
  if (i>3){
    axis (1, c(1,3,5), c ("Work,\nstudy", "Eat,\nsleep",
      "Leisure"), mgp=c(2,1.5,0), tck=0, cex.axis=1.2)
    axis (1, c(2,4,6), c ("Unpaid\nwork",
      "Personal\nCare", "Other"), mgp=c(2,1.5,0),</p><p>same-blog 3 0.92419231 <a title="151-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-16-Wanted%3A__Probability_distributions_for_rank_orderings.html">151 andrew gelman stats-2010-07-16-Wanted:  Probability distributions for rank orderings</a></p>
<p>Introduction: Dietrich Stoyan writes:
  
 
I asked the IMS people for an expert in statistics of voting/elections and they wrote me your name. I am a statistician, but never worked in the field voting/elections. It was my son-in-law who asked me for statistical theories in that field.


He posed in particular the following problem:


The aim of the voting is to come to a ranking of c candidates. Every vote is a permutation of these c candidates. The problem is to have probability distributions in the set of all permutations of c elements.


Are there theories for such distributions?


I should be very grateful for a fast answer with hints to literature. (I confess that I do not know your books.) 
 

 
My reply:  Rather than trying to model the ranks directly, Iâ&euro;&trade;d recommend modeling a latent continuous outcome which then implies a distribution on ranks, if the ranks are of interest. There are lots of distributions of c-dimensional continuous outcomes.  In political science, the usual way to start is</p><p>4 0.9164111 <a title="151-lda-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-On_deck_this_week.html">2298 andrew gelman stats-2014-04-21-On deck this week</a></p>
<p>Introduction: Mon :  Ticket to Baaaath
 
 Tues :  Ticket to Baaaaarf
 
 Wed :  Thinking of doing a list experiment?  Here’s a list of reasons why you should think again
 
 Thurs :  An open site for researchers to post and share papers
 
 Fri :  Questions about “Too Good to Be True”
 
 Sat :  Sleazy sock puppet can’t stop spamming our discussion of compressed sensing and promoting the work of Xiteng Liu
 
 Sun :  White stripes and dead armadillos</p><p>5 0.86002213 <a title="151-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-27-Neumann_update.html">432 andrew gelman stats-2010-11-27-Neumann update</a></p>
<p>Introduction: Steve Hsu, who  started off  this discussion, had some comments on  my speculations  on the personality of John von Neumann and others.  Steve writes:
  
I [Hsu] actually knew Feynman a bit when I was an undergrad, and found him to be very nice to students. Since then I have heard quite a few stories from people in theoretical physics which emphasize his nastier side, and I think in the end he was quite a complicated person like everyone else.


There are a couple of pseudo-biographies of vN, but none as high quality as, e.g., Gleick’s book on Feynman or Hodges book about Turing. (Gleick studied physics as an undergrad at Harvard, and Hodges is a PhD in mathematical physics — pretty rare backgrounds for biographers!)  For example, as mentioned on the comment thread to your post, Steve Heims wrote a book about both vN and Wiener (!),  and Norman Macrae wrote a biography of vN. Both books are worth reading, but I think neither really do him justice. The breadth of vN’s work is just too m</p><p>6 0.85726917 <a title="151-lda-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-29-I_agree_with_this_comment.html">2272 andrew gelman stats-2014-03-29-I agree with this comment</a></p>
<p>7 0.85024285 <a title="151-lda-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-15-Does_quantum_uncertainty_have_a_place_in_everyday_applied_statistics%3F.html">1857 andrew gelman stats-2013-05-15-Does quantum uncertainty have a place in everyday applied statistics?</a></p>
<p>8 0.84737051 <a title="151-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-07-Hipmunk_FAIL%3A__Graphics_without_content_is_not_enough.html">894 andrew gelman stats-2011-09-07-Hipmunk FAIL:  Graphics without content is not enough</a></p>
<p>9 0.84393656 <a title="151-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-15-A_silly_paper_that_tries_to_make_fun_of_multilevel_models.html">854 andrew gelman stats-2011-08-15-A silly paper that tries to make fun of multilevel models</a></p>
<p>10 0.84374714 <a title="151-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-22-Please_stop_me_before_I_barf_again.html">1275 andrew gelman stats-2012-04-22-Please stop me before I barf again</a></p>
<p>11 0.83472836 <a title="151-lda-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-01-Two_Postdoc_Positions_Available_on_Bayesian_Hierarchical_Modeling.html">62 andrew gelman stats-2010-06-01-Two Postdoc Positions Available on Bayesian Hierarchical Modeling</a></p>
<p>12 0.83210421 <a title="151-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-10-A_defense_of_Tom_Wolfe_based_on_the_impossibility_of_the_law_of_small_numbers_in_network_structure.html">1615 andrew gelman stats-2012-12-10-A defense of Tom Wolfe based on the impossibility of the law of small numbers in network structure</a></p>
<p>13 0.82821727 <a title="151-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-26-%E2%80%9CA_Vast_Graveyard_of_Undead_Theories%3A__Publication_Bias_and_Psychological_Science%E2%80%99s_Aversion_to_the_Null%E2%80%9D.html">1826 andrew gelman stats-2013-04-26-“A Vast Graveyard of Undead Theories:  Publication Bias and Psychological Science’s Aversion to the Null”</a></p>
<p>14 0.82442713 <a title="151-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-30-David_Hogg_on_statistics.html">1401 andrew gelman stats-2012-06-30-David Hogg on statistics</a></p>
<p>15 0.81470019 <a title="151-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-15-%E2%80%9C10_Things_You_Need_to_Know_About_Causal_Effects%E2%80%9D.html">1675 andrew gelman stats-2013-01-15-“10 Things You Need to Know About Causal Effects”</a></p>
<p>16 0.7940774 <a title="151-lda-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-26-Sleazy_sock_puppet_can%E2%80%99t_stop_spamming_our_discussion_of_compressed_sensing_and_promoting_the_work_of_Xiteng_Liu.html">2306 andrew gelman stats-2014-04-26-Sleazy sock puppet can’t stop spamming our discussion of compressed sensing and promoting the work of Xiteng Liu</a></p>
<p>17 0.79258806 <a title="151-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-13-News_coverage_of_statistical_issues%E2%80%A6how_did_I_do%3F.html">514 andrew gelman stats-2011-01-13-News coverage of statistical issues…how did I do?</a></p>
<p>18 0.78899467 <a title="151-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-19-The_grasshopper_wins%2C_and_Greg_Mankiw%E2%80%99s_grandmother_would_be_%E2%80%9Cshocked_and_appalled%E2%80%9D_all_over_again.html">1728 andrew gelman stats-2013-02-19-The grasshopper wins, and Greg Mankiw’s grandmother would be “shocked and appalled” all over again</a></p>
<p>19 0.76644611 <a title="151-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-11-Symptomatic_innumeracy.html">900 andrew gelman stats-2011-09-11-Symptomatic innumeracy</a></p>
<p>20 0.75676906 <a title="151-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-09-Today_in_the_sister_blog.html">1049 andrew gelman stats-2011-12-09-Today in the sister blog</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
