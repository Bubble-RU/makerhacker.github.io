<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>250 andrew gelman stats-2010-09-02-Blending results from two relatively independent multi-level models</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-250" href="#">andrew_gelman_stats-2010-250</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>250 andrew gelman stats-2010-09-02-Blending results from two relatively independent multi-level models</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-250-html" href="http://andrewgelman.com/2010/09/02/blending_result/">html</a></p><p>Introduction: David Shor writes:
  
 
I [Shor] am working on a Bayesian Forecasting model for the Mid-term elections that has two components:


1) A poll aggregation system with pooled and hierarchical house and design effects across every race with polls (Average Standard error for house seat level vote-share ~.055)


2) A Bafumi-style regression that applies national-swing to individual seats. (Average Standard error for house seat level vote-share ~.06)


Since these two estimates are essentially independent, estimates can probably be made more accurate by pooling them together. But If a house effect changes in one draw, that changes estimates in every race. Changes in regression coefficients and National swing have a similar effect.  In the face of high and possibly differing seat-to-seat correlations from each method, I’m not sure what the correct way to “blend” these models would be, either for individual or top-line seat estimates.


In the mean-time, I’m just creating variance-weighted avera</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 055)   2) A Bafumi-style regression that applies national-swing to individual seats. [sent-2, score-0.222]
</p><p>2 (Average Standard error for house seat level vote-share ~. [sent-3, score-0.865]
</p><p>3 06)   Since these two estimates are essentially independent, estimates can probably be made more accurate by pooling them together. [sent-4, score-0.483]
</p><p>4 But If a house effect changes in one draw, that changes estimates in every race. [sent-5, score-0.796]
</p><p>5 Changes in regression coefficients and National swing have a similar effect. [sent-6, score-0.143]
</p><p>6 In the face of high and possibly differing seat-to-seat correlations from each method, I’m not sure what the correct way to “blend” these models would be, either for individual or top-line seat estimates. [sent-7, score-0.654]
</p><p>7 In the mean-time, I’m just creating variance-weighted averages in excel for seat level estimates and using bayes rule to mix the two seat distributions to get pdfs, which I suspect is sufficient for this particular application. [sent-8, score-1.307]
</p><p>8 But I’m very curious what the “right” thing to do would be. [sent-9, score-0.074]
</p><p>9 If, instead of trying to figure out how to do a weighting, you directly model your data, an appropriate weighting might very well pop our satisfyingly from the posterior distribution. [sent-11, score-0.51]
</p><p>10 That’s been my experience in various contexts, from elections to radon gas. [sent-12, score-0.193]
</p><p>11 I suppose they arn’t radically different, and your book would recommend trying to build some greater model that incorporates the two interpretations as special cases or construct some hybrid. [sent-14, score-0.737]
</p><p>12 But I understand the short-term goal of having a good weighted average. [sent-16, score-0.15]
</p><p>13 I suppose that a weighted average, weighting by inverse of forecast variance (which is what would be appropriate if the forecasts were truly independent) would make sense. [sent-17, score-0.827]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('seat', 0.398), ('house', 0.28), ('shor', 0.254), ('races', 0.242), ('weighting', 0.206), ('estimates', 0.153), ('weighted', 0.15), ('changes', 0.147), ('walk', 0.144), ('model', 0.132), ('average', 0.113), ('elections', 0.113), ('blend', 0.111), ('level', 0.109), ('independent', 0.107), ('create', 0.107), ('pdfs', 0.105), ('two', 0.105), ('incorporates', 0.1), ('differing', 0.1), ('radically', 0.1), ('clarifies', 0.094), ('appropriate', 0.093), ('aggregation', 0.092), ('horse', 0.088), ('pooled', 0.088), ('random', 0.084), ('suppose', 0.082), ('contexts', 0.082), ('individual', 0.082), ('inverse', 0.08), ('radon', 0.08), ('pop', 0.079), ('error', 0.078), ('excel', 0.076), ('would', 0.074), ('interpretations', 0.073), ('pooling', 0.072), ('swing', 0.072), ('components', 0.072), ('specified', 0.072), ('regression', 0.071), ('construct', 0.071), ('standard', 0.07), ('year', 0.069), ('every', 0.069), ('applies', 0.069), ('competing', 0.069), ('averages', 0.068), ('forecasts', 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="250-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Blending_results_from_two_relatively_independent_multi-level_models.html">250 andrew gelman stats-2010-09-02-Blending results from two relatively independent multi-level models</a></p>
<p>Introduction: David Shor writes:
  
 
I [Shor] am working on a Bayesian Forecasting model for the Mid-term elections that has two components:


1) A poll aggregation system with pooled and hierarchical house and design effects across every race with polls (Average Standard error for house seat level vote-share ~.055)


2) A Bafumi-style regression that applies national-swing to individual seats. (Average Standard error for house seat level vote-share ~.06)


Since these two estimates are essentially independent, estimates can probably be made more accurate by pooling them together. But If a house effect changes in one draw, that changes estimates in every race. Changes in regression coefficients and National swing have a similar effect.  In the face of high and possibly differing seat-to-seat correlations from each method, I’m not sure what the correct way to “blend” these models would be, either for individual or top-line seat estimates.


In the mean-time, I’m just creating variance-weighted avera</p><p>2 0.17618032 <a title="250-tfidf-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-19-All_politics_are_local_%E2%80%94_not.html">475 andrew gelman stats-2010-12-19-All politics are local — not</a></p>
<p>Introduction: Mickey Kaus does a public service by  trashing  Tip O’Neill’s famous dictum that “all politics are local.”  As Kaus point out, all the congressional elections in recent decades have been nationalized.
 
I’d go one step further and say that, sure, all politics are local–if you’re Tip O’Neill and represent a ironclad Democratic seat in Congress.  It’s easy to be smug about your political skills if you’re in a safe seat and have enough pull in state politics to avoid your district getting gerrymandered.  Then you can sit there and sagely attribute your success to your continuing mastery of local politics rather than to whatever it took to get the seat in the first place.</p><p>3 0.17437741 <a title="250-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>Introduction: Andreas Graefe writes (see  here   here   here ):
  
The usual procedure for developing linear models to predict any kind of target variable is to identify a subset of most important predictors and to estimate weights that provide the best possible solution for a given sample. The resulting “optimally” weighted linear composite is then used when predicting new data. This approach is useful in situations with large and reliable datasets and few predictor variables. However, a large body of analytical and empirical evidence since the 1970s shows that the weighting of variables is of little, if any, value in situations with small and noisy datasets and a large number of predictor variables. In such situations, including all relevant variables is more important than their weighting. These findings have yet to impact many fields. This study uses data from nine established U.S. election-forecasting models whose forecasts are regularly published in academic journals to demonstrate the value o</p><p>4 0.17148858 <a title="250-tfidf-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-14-Prior_distribution_for_design_effects.html">85 andrew gelman stats-2010-06-14-Prior distribution for design effects</a></p>
<p>Introduction: David Shor writes:
  
I’m fitting a state-space model right now that estimates the “design effect” of individual pollsters (Ratio of poll variance to that predicted by perfect random sampling). What would be a good prior distribution for that?
  
My quickest suggestion is start with something simple, such as a uniform from 1 to 10, and then to move to something hierarchical, such as a lognormal on (design.effect – 1), with the hyperparameters estimated from data.
 
My longer suggestion is to take things apart.  What exactly do you mean by “design effect”?  There are lots of things going on, both in sampling error (the classical “design effect” that comes from cluster sampling, stratification, weighting, etc.) and nonsampling error (nonresponse bias, likeliy voter screening, bad questions, etc.)  It would be best if you could model both pieces.</p><p>5 0.16081557 <a title="250-tfidf-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-03-Some_thoughts_on_election_forecasting.html">391 andrew gelman stats-2010-11-03-Some thoughts on election forecasting</a></p>
<p>Introduction: I’ve written a lot on polls and elections (“a poll is a snapshot, not a forecast,” etc., or see here for a  more technical paper  with Kari Lock) but had a few things to add in light of Sam Wang’s  recent efforts . As a biologist with a physics degree, Wang brings an outsider’s perspective to political forecasting, which can be a good thing.  (I’m a bit of an outsider to political science myself, as is my sometime collaborator Nate Silver, who’s done a lot of good work in the past few years.)
 
But there are two places where Wang misses the point, I think.
  

 
He refers to his method as a “transparent, low-assumption calculation” and compares it favorably to “fancy modeling” and “assumption-laden models.”  Assumptions are a bad thing, right?  Well, no, I don’t think so.   Bad  assumptions are a bad thing.  Good assumptions are just fine.  Similarly for fancy modeling.  I don’t see why a model should get credit for  not  including a factor that might be important.
 
Let me clarify.  I</p><p>6 0.15845527 <a title="250-tfidf-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-22-Politics_is_not_a_random_walk%3A__Momentum_and_mean_reversion_in_polling.html">364 andrew gelman stats-2010-10-22-Politics is not a random walk:  Momentum and mean reversion in polling</a></p>
<p>7 0.15652701 <a title="250-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-27-A_Non-random_Walk_Down_Campaign_Street.html">1512 andrew gelman stats-2012-09-27-A Non-random Walk Down Campaign Street</a></p>
<p>8 0.15289788 <a title="250-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-26-Is_it_plausible_that_1%25_of_people_pick_a_career_based_on_their_first_name%3F.html">629 andrew gelman stats-2011-03-26-Is it plausible that 1% of people pick a career based on their first name?</a></p>
<p>9 0.15225306 <a title="250-tfidf-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-23-Doug_Hibbs_on_the_fundamentals_in_2010.html">292 andrew gelman stats-2010-09-23-Doug Hibbs on the fundamentals in 2010</a></p>
<p>10 0.14953414 <a title="250-tfidf-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-12-Comparison_of_forecasts_for_the_2010_congressional_elections.html">270 andrew gelman stats-2010-09-12-Comparison of forecasts for the 2010 congressional elections</a></p>
<p>11 0.14684801 <a title="250-tfidf-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-27-Bafumi-Erikson-Wlezien_predict_a_50-seat_loss_for_Democrats_in_November.html">237 andrew gelman stats-2010-08-27-Bafumi-Erikson-Wlezien predict a 50-seat loss for Democrats in November</a></p>
<p>12 0.14529039 <a title="250-tfidf-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-19-Analysis_of_survey_data%3A_Design_based_models_vs._hierarchical_modeling%3F.html">352 andrew gelman stats-2010-10-19-Analysis of survey data: Design based models vs. hierarchical modeling?</a></p>
<p>13 0.13459183 <a title="250-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-08-Poll_aggregation_and_election_forecasting.html">1570 andrew gelman stats-2012-11-08-Poll aggregation and election forecasting</a></p>
<p>14 0.12877011 <a title="250-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>15 0.12504771 <a title="250-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-03-How_best_to_compare_effects_measured_in_two_different_time_periods%3F.html">2086 andrew gelman stats-2013-11-03-How best to compare effects measured in two different time periods?</a></p>
<p>16 0.12483308 <a title="250-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-26-Some_thoughts_on_survey_weighting.html">1430 andrew gelman stats-2012-07-26-Some thoughts on survey weighting</a></p>
<p>17 0.11624123 <a title="250-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-17-Hierarchical-multilevel_modeling_with_%E2%80%9Cbig_data%E2%80%9D.html">1267 andrew gelman stats-2012-04-17-Hierarchical-multilevel modeling with “big data”</a></p>
<p>18 0.11200858 <a title="250-tfidf-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-02-The_problem_of_overestimation_of_group-level_variance_parameters.html">63 andrew gelman stats-2010-06-02-The problem of overestimation of group-level variance parameters</a></p>
<p>19 0.11188645 <a title="250-tfidf-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-20-More_on_why_%E2%80%9Call_politics_is_local%E2%80%9D_is_an_outdated_slogan.html">478 andrew gelman stats-2010-12-20-More on why “all politics is local” is an outdated slogan</a></p>
<p>20 0.10749361 <a title="250-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-27-Cross-validation_%28What_is_it_good_for%3F%29.html">1395 andrew gelman stats-2012-06-27-Cross-validation (What is it good for?)</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.211), (1, 0.11), (2, 0.136), (3, 0.017), (4, 0.041), (5, 0.04), (6, -0.013), (7, -0.046), (8, 0.076), (9, 0.02), (10, 0.091), (11, 0.029), (12, 0.013), (13, -0.026), (14, -0.05), (15, 0.002), (16, -0.005), (17, 0.016), (18, 0.009), (19, 0.012), (20, -0.05), (21, 0.059), (22, 0.015), (23, 0.039), (24, 0.001), (25, -0.004), (26, 0.001), (27, 0.016), (28, 0.007), (29, 0.047), (30, 0.0), (31, 0.003), (32, -0.022), (33, -0.03), (34, 0.05), (35, 0.004), (36, -0.024), (37, -0.039), (38, 0.032), (39, -0.002), (40, -0.047), (41, 0.036), (42, -0.015), (43, 0.002), (44, -0.023), (45, 0.018), (46, -0.012), (47, -0.017), (48, 0.07), (49, 0.04)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95077431 <a title="250-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Blending_results_from_two_relatively_independent_multi-level_models.html">250 andrew gelman stats-2010-09-02-Blending results from two relatively independent multi-level models</a></p>
<p>Introduction: David Shor writes:
  
 
I [Shor] am working on a Bayesian Forecasting model for the Mid-term elections that has two components:


1) A poll aggregation system with pooled and hierarchical house and design effects across every race with polls (Average Standard error for house seat level vote-share ~.055)


2) A Bafumi-style regression that applies national-swing to individual seats. (Average Standard error for house seat level vote-share ~.06)


Since these two estimates are essentially independent, estimates can probably be made more accurate by pooling them together. But If a house effect changes in one draw, that changes estimates in every race. Changes in regression coefficients and National swing have a similar effect.  In the face of high and possibly differing seat-to-seat correlations from each method, I’m not sure what the correct way to “blend” these models would be, either for individual or top-line seat estimates.


In the mean-time, I’m just creating variance-weighted avera</p><p>2 0.78630787 <a title="250-lsi-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-12-Comparison_of_forecasts_for_the_2010_congressional_elections.html">270 andrew gelman stats-2010-09-12-Comparison of forecasts for the 2010 congressional elections</a></p>
<p>Introduction: Yesterday at the  sister blog , Nate Silver forecast that the Republicans have a two-thirds chance of regaining the House of Representatives in the upcoming election, with an expected gain of 45 House seats.
 
Last month, Bafumi, Erikson, and Wlezien  released their forecast  that gives the Republicans an 80% chance of takeover and an expected gain of 50 seats.
 
As all the above writers emphasize, these forecasts are full of uncertainty, so I treat the two predictions–a 45-seat swing or a 50-seat swing–as essentially identical at the national level.
 
And, as regular readers know, as far back as  a year ago , the generic Congressional ballot (those questions of the form, “Which party do you plan to vote for in November?”) was also pointing to big Republican gains.
 
As Bafumi et al. point out, early generic polls are strongly predictive of the election outcome, but they need to be interpreted carefully.  The polls move in a generally predictable manner during the year leading up to an</p><p>3 0.77148789 <a title="250-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-03-Some_thoughts_on_election_forecasting.html">391 andrew gelman stats-2010-11-03-Some thoughts on election forecasting</a></p>
<p>Introduction: I’ve written a lot on polls and elections (“a poll is a snapshot, not a forecast,” etc., or see here for a  more technical paper  with Kari Lock) but had a few things to add in light of Sam Wang’s  recent efforts . As a biologist with a physics degree, Wang brings an outsider’s perspective to political forecasting, which can be a good thing.  (I’m a bit of an outsider to political science myself, as is my sometime collaborator Nate Silver, who’s done a lot of good work in the past few years.)
 
But there are two places where Wang misses the point, I think.
  

 
He refers to his method as a “transparent, low-assumption calculation” and compares it favorably to “fancy modeling” and “assumption-laden models.”  Assumptions are a bad thing, right?  Well, no, I don’t think so.   Bad  assumptions are a bad thing.  Good assumptions are just fine.  Similarly for fancy modeling.  I don’t see why a model should get credit for  not  including a factor that might be important.
 
Let me clarify.  I</p><p>4 0.76539832 <a title="250-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-13-Help_with_this_problem%2C_win_valuable_prizes.html">1164 andrew gelman stats-2012-02-13-Help with this problem, win valuable prizes</a></p>
<p>Introduction: Corrected equation
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
This post is by Phil.
 
In the comments to  an earlier post , I mentioned a problem I am struggling with right now. Several people mentioned having (and solving!) similar problems in the past, so this seems like a great way for me and a bunch of other blog readers to learn something. I will describe the problem, one or more of you will tell me how to solve it, and you will win…wait for it….my thanks, and the approval and admiration of your fellow blog readers, and a big thank-you in any publication that includes results from fitting the model.  You can’t ask fairer than that!
 
Here’s the problem.  The goal is to estimate six parameters that characterize the leakiness (or air-tightness) of a house with an attached garage.  We are specifically interested in the parameters that describe the connection between the house and the garage; this is of interest because of the effect on the air quality in the house  if there are toxic chemic</p><p>5 0.75073111 <a title="250-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>Introduction: Andreas Graefe writes (see  here   here   here ):
  
The usual procedure for developing linear models to predict any kind of target variable is to identify a subset of most important predictors and to estimate weights that provide the best possible solution for a given sample. The resulting “optimally” weighted linear composite is then used when predicting new data. This approach is useful in situations with large and reliable datasets and few predictor variables. However, a large body of analytical and empirical evidence since the 1970s shows that the weighting of variables is of little, if any, value in situations with small and noisy datasets and a large number of predictor variables. In such situations, including all relevant variables is more important than their weighting. These findings have yet to impact many fields. This study uses data from nine established U.S. election-forecasting models whose forecasts are regularly published in academic journals to demonstrate the value o</p><p>6 0.7405858 <a title="250-lsi-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-31-Somewhat_Bayesian_multilevel_modeling.html">246 andrew gelman stats-2010-08-31-Somewhat Bayesian multilevel modeling</a></p>
<p>7 0.73870355 <a title="250-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-08-Poll_aggregation_and_election_forecasting.html">1570 andrew gelman stats-2012-11-08-Poll aggregation and election forecasting</a></p>
<p>8 0.73828357 <a title="250-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-27-Cross-validation_%28What_is_it_good_for%3F%29.html">1395 andrew gelman stats-2012-06-27-Cross-validation (What is it good for?)</a></p>
<p>9 0.73312593 <a title="250-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-27-A_Non-random_Walk_Down_Campaign_Street.html">1512 andrew gelman stats-2012-09-27-A Non-random Walk Down Campaign Street</a></p>
<p>10 0.72701746 <a title="250-lsi-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-A_calibrated_Cook_gives_Dems_the_edge_in_Nov%2C_sez_Sandy.html">300 andrew gelman stats-2010-09-28-A calibrated Cook gives Dems the edge in Nov, sez Sandy</a></p>
<p>11 0.72538722 <a title="250-lsi-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-22-Politics_is_not_a_random_walk%3A__Momentum_and_mean_reversion_in_polling.html">364 andrew gelman stats-2010-10-22-Politics is not a random walk:  Momentum and mean reversion in polling</a></p>
<p>12 0.7238065 <a title="250-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-17-Death%21.html">962 andrew gelman stats-2011-10-17-Death!</a></p>
<p>13 0.72043145 <a title="250-lsi-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-22-Big_Data_needs_Big_Model.html">2343 andrew gelman stats-2014-05-22-Big Data needs Big Model</a></p>
<p>14 0.7157715 <a title="250-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<p>15 0.71020293 <a title="250-lsi-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-22-Handling_multiple_versions_of_an_outcome_variable.html">726 andrew gelman stats-2011-05-22-Handling multiple versions of an outcome variable</a></p>
<p>16 0.70814264 <a title="250-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-03-Uncertainty_in_parameter_estimates_using_multilevel_models.html">1966 andrew gelman stats-2013-08-03-Uncertainty in parameter estimates using multilevel models</a></p>
<p>17 0.70669556 <a title="250-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-11-%E2%80%9C2_level_logit_with_2_REs_%26_large_sample._computational_nightmare_%E2%80%93_please_help%E2%80%9D.html">759 andrew gelman stats-2011-06-11-“2 level logit with 2 REs & large sample. computational nightmare – please help”</a></p>
<p>18 0.70573556 <a title="250-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-08-I_Am_Too_Absolutely_Heteroskedastic_for_This_Probit_Model.html">1047 andrew gelman stats-2011-12-08-I Am Too Absolutely Heteroskedastic for This Probit Model</a></p>
<p>19 0.70505506 <a title="250-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-17-Hierarchical-multilevel_modeling_with_%E2%80%9Cbig_data%E2%80%9D.html">1267 andrew gelman stats-2012-04-17-Hierarchical-multilevel modeling with “big data”</a></p>
<p>20 0.70114911 <a title="250-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-03-How_best_to_compare_effects_measured_in_two_different_time_periods%3F.html">2086 andrew gelman stats-2013-11-03-How best to compare effects measured in two different time periods?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.062), (9, 0.013), (15, 0.017), (16, 0.055), (20, 0.013), (21, 0.019), (24, 0.152), (31, 0.01), (45, 0.014), (59, 0.011), (63, 0.025), (66, 0.011), (86, 0.047), (87, 0.044), (90, 0.052), (99, 0.333)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98061174 <a title="250-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Blending_results_from_two_relatively_independent_multi-level_models.html">250 andrew gelman stats-2010-09-02-Blending results from two relatively independent multi-level models</a></p>
<p>Introduction: David Shor writes:
  
 
I [Shor] am working on a Bayesian Forecasting model for the Mid-term elections that has two components:


1) A poll aggregation system with pooled and hierarchical house and design effects across every race with polls (Average Standard error for house seat level vote-share ~.055)


2) A Bafumi-style regression that applies national-swing to individual seats. (Average Standard error for house seat level vote-share ~.06)


Since these two estimates are essentially independent, estimates can probably be made more accurate by pooling them together. But If a house effect changes in one draw, that changes estimates in every race. Changes in regression coefficients and National swing have a similar effect.  In the face of high and possibly differing seat-to-seat correlations from each method, I’m not sure what the correct way to “blend” these models would be, either for individual or top-line seat estimates.


In the mean-time, I’m just creating variance-weighted avera</p><p>2 0.97541308 <a title="250-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-23-Scientists_can_read_your_mind_._._._as_long_as_the%E2%80%99re_allowed_to_look_at_more_than_one_place_in_your_brain_and_then_make_a_prediction_after_seeing_what_you_actually_did.html">106 andrew gelman stats-2010-06-23-Scientists can read your mind . . . as long as the’re allowed to look at more than one place in your brain and then make a prediction after seeing what you actually did</a></p>
<p>Introduction: Maggie Fox  writes :
  
Brain scans may be able to predict what you will do better than you can yourself . . .  They found a way to interpret “real time” brain images to show whether people who viewed messages about using sunscreen would actually use sunscreen during the following week.


The scans were more accurate than the volunteers were, Emily Falk and colleagues at the University of California Los Angeles reported in the Journal of Neuroscience. . . .


About half the volunteers had correctly predicted whether they would use sunscreen. The research team analyzed and re-analyzed the MRI scans to see if they could find any brain activity that would do better.


Activity in one area of the brain, a particular part of the medial prefrontal cortex, provided the best information.


“From this region of the brain, we can predict for about three-quarters of the people whether they will increase their use of sunscreen beyond what they say they will do,” Lieberman said.


“It is the one re</p><p>3 0.97460443 <a title="250-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-Piss-poor_monocausal_social_science.html">1196 andrew gelman stats-2012-03-04-Piss-poor monocausal social science</a></p>
<p>Introduction: Dan Kahan writes: 
  
  
Okay, have done due diligence here & can’t find the reference. It was in recent blog — and was more or less an aside — but you ripped into researchers (pretty sure econometricians, but this could be my memory adding to your account recollections it conjured from my own experience) who purport to make estimates or predictions based on multivariate regression in which the value of particular predictor is set at some level while others “held constant” etc., on ground that variance in that particular predictor independent of covariance in other model predictors is unrealistic.  You made it sound, too, as if this were one of the pet peeves in your menagerie — leading me to think you had blasted into it before.


Know what I’m talking about?


Also — isn’t this really just a way of saying that the model is misspecified — at least if the goal is to try to make a valid & unbiased estimate of the impact of that particular predictor? The problem can’t be that one is usin</p><p>4 0.97399318 <a title="250-lda-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-20-More_on_why_%E2%80%9Call_politics_is_local%E2%80%9D_is_an_outdated_slogan.html">478 andrew gelman stats-2010-12-20-More on why “all politics is local” is an outdated slogan</a></p>
<p>Introduction: Yesterday I  wrote  that Mickey Kaus was right to point out that it’s time to retire Tip O’Neill’s famous dictum that “all politics are local.” As Kaus points out, all the congressional elections in recent decades have been nationalized.  The slogan is particularly silly for Tip O’Neill himself.  Sure, O’Neill had to have a firm grip on local politics to get his safe seat in the first place, but after that it was smooth sailing.
 
Jonathan Bernstein  disagrees , writing:
  
 
Yes, but: don’t most Members of the House have ironclad partisan districts?  And isn’t the most important single thing they can do to protect themselves involve having pull in state politics to avoid being gerrymandered?  That is “all politics is local,” no? 


There’s also a fair amount they can do to stay on the good side of their local party, thus avoiding a primary fight.  And, even in an era of nationalized elections, there’s still plenty a Member of Congress can do to to influence elections on the margins, a</p><p>5 0.97207564 <a title="250-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-26-Some_thoughts_on_survey_weighting.html">1430 andrew gelman stats-2012-07-26-Some thoughts on survey weighting</a></p>
<p>Introduction: From a comment I made in an email exchange:
 
My  work  on survey adjustments has very much been inspired by the ideas of Rod Little.  Much of my efforts have gone toward the goal of integrating hierarchical modeling (which is so helpful for small-area estimation) with post stratification (which adjusts for known differences between sample and population).  In the surveys I’ve dealt with, nonresponse/nonavailability can be a big issue, and I’ve always tried to emphasize that (a) the probability of a person being included in the sample is just about never known, and (b) even if this probability were known, I’d rather know the empirical n/N than the probability p (which is only valid in expectation).  Regarding nonparametric modeling:  I haven’t done much of that (although I hope to at some point) but Rod and his students have.
 
As I wrote in the first sentence of the above-linked paper, I do think the current theory and practice of survey weighting is a mess, in that much depends on so</p><p>6 0.96952373 <a title="250-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-07-Happy_news_on_happiness%3B_what_can_we_believe%3F.html">1305 andrew gelman stats-2012-05-07-Happy news on happiness; what can we believe?</a></p>
<p>7 0.96935731 <a title="250-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-09-In_the_future%2C_everyone_will_publish_everything..html">1254 andrew gelman stats-2012-04-09-In the future, everyone will publish everything.</a></p>
<p>8 0.96853966 <a title="250-lda-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-16-How_much_can_we_learn_about_individual-level_causal_claims_from_state-level_correlations%3F.html">2336 andrew gelman stats-2014-05-16-How much can we learn about individual-level causal claims from state-level correlations?</a></p>
<p>9 0.96840376 <a title="250-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-16-%E2%80%9CFalse-positive_psychology%E2%80%9D.html">1171 andrew gelman stats-2012-02-16-“False-positive psychology”</a></p>
<p>10 0.96743143 <a title="250-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-15-Regression_discontinuity_designs%3A__looking_for_the_keys_under_the_lamppost%3F.html">518 andrew gelman stats-2011-01-15-Regression discontinuity designs:  looking for the keys under the lamppost?</a></p>
<p>11 0.96708977 <a title="250-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>12 0.96693975 <a title="250-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-15-Still_more_Mr._P_in_public_health.html">770 andrew gelman stats-2011-06-15-Still more Mr. P in public health</a></p>
<p>13 0.96688396 <a title="250-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-11-Update_on_the_spam_email_study.html">27 andrew gelman stats-2010-05-11-Update on the spam email study</a></p>
<p>14 0.96686995 <a title="250-lda-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-27-Why_don%E2%80%99t_more_medical_discoveries_become_cures%3F.html">167 andrew gelman stats-2010-07-27-Why don’t more medical discoveries become cures?</a></p>
<p>15 0.9661153 <a title="250-lda-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-24-Empirical_implications_of_Empirical_Implications_of_Theoretical_Models.html">2263 andrew gelman stats-2014-03-24-Empirical implications of Empirical Implications of Theoretical Models</a></p>
<p>16 0.96604347 <a title="250-lda-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-06-%2463%2C000_worth_of_abusive_research_._._._or_just_a_really_stupid_waste_of_time%3F.html">18 andrew gelman stats-2010-05-06-$63,000 worth of abusive research . . . or just a really stupid waste of time?</a></p>
<p>17 0.96592778 <a title="250-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-30-Don%E2%80%99t_stop_being_a_statistician_once_the_analysis_is_done.html">783 andrew gelman stats-2011-06-30-Don’t stop being a statistician once the analysis is done</a></p>
<p>18 0.96563965 <a title="250-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-09-Does_it_feel_like_cheating_when_I_do_this%3F__Variation_in_ethical_standards_and_expectations.html">605 andrew gelman stats-2011-03-09-Does it feel like cheating when I do this?  Variation in ethical standards and expectations</a></p>
<p>19 0.9652828 <a title="250-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-19-Everything_is_Obvious_%28once_you_know_the_answer%29.html">719 andrew gelman stats-2011-05-19-Everything is Obvious (once you know the answer)</a></p>
<p>20 0.96513861 <a title="250-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-11-How_do_we_evaluate_a_new_and_wacky_claim%3F.html">797 andrew gelman stats-2011-07-11-How do we evaluate a new and wacky claim?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
