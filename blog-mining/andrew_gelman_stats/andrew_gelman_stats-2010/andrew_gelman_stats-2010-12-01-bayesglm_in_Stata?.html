<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>442 andrew gelman stats-2010-12-01-bayesglm in Stata?</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-442" href="#">andrew_gelman_stats-2010-442</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>442 andrew gelman stats-2010-12-01-bayesglm in Stata?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-442-html" href="http://andrewgelman.com/2010/12/01/bayesglm_in_sta/">html</a></p><p>Introduction: Is there an implementation of bayesglm in Stata?  (That is, approximate maximum penalized likelihood estimation with specified normal or t prior distributions on the coefficients.)</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 (That is, approximate maximum penalized likelihood estimation with specified normal or t prior distributions on the coefficients. [sent-2, score-2.344]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bayesglm', 0.412), ('penalized', 0.374), ('specified', 0.322), ('stata', 0.316), ('implementation', 0.298), ('approximate', 0.285), ('maximum', 0.284), ('estimation', 0.235), ('normal', 0.232), ('distributions', 0.222), ('likelihood', 0.22), ('prior', 0.17)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="442-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-01-bayesglm_in_Stata%3F.html">442 andrew gelman stats-2010-12-01-bayesglm in Stata?</a></p>
<p>Introduction: Is there an implementation of bayesglm in Stata?  (That is, approximate maximum penalized likelihood estimation with specified normal or t prior distributions on the coefficients.)</p><p>2 0.22726306 <a title="442-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-06-Early_stopping_and_penalized_likelihood.html">788 andrew gelman stats-2011-07-06-Early stopping and penalized likelihood</a></p>
<p>Introduction: Maximum likelihood gives the beat fit to the training data but in general overfits, yielding overly-noisy parameter estimates that don’t perform so well when predicting new data.  A popular solution to this overfitting problem takes advantage of the iterative nature of most maximum likelihood algorithms by stopping early.  In general, an iterative optimization algorithm goes from a starting point to the maximum of some objective function.  If the starting point has some good properties, then early stopping can work well, keeping some of the virtues of the starting point while respecting the data.  
 
This trick can be performed the other way, too, starting with the data and then processing it to move it toward a model.  That’s how the iterative proportional fitting algorithm of Deming and Stephan (1940) works to fit multivariate categorical data to known margins.
 
In any case, the trick is to stop at the right point–not so soon that you’re ignoring the data but not so late that you en</p><p>3 0.20299828 <a title="442-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>Introduction: For awhile I’ve been fitting most of my multilevel models using lmer/glmer, which gives point estimates of the group-level variance parameters (maximum marginal likelihood estimate for lmer and an approximation for glmer).  I’m usually satisfied with this–sure, point estimation understates the uncertainty in model fitting, but that’s typically the least of our worries. 
 
Sometimes, though, lmer/glmer estimates group-level variances at 0 or estimates group-level correlation parameters at +/- 1.  Typically, when this happens, it’s not that we’re so sure the variance is close to zero or that the correlation is close to 1 or -1; rather, the marginal likelihood does not provide a lot of information about these parameters of the group-level error distribution.
 
I don’t want point estimates on the boundary.  I don’t want to say that the unexplained variance in some dimension is exactly zero.
 
One way to handle this problem is full Bayes:  slap a prior on sigma, do your Gibbs and Metropolis</p><p>4 0.16424081 <a title="442-tfidf-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-09-Both_R_and_Stata.html">76 andrew gelman stats-2010-06-09-Both R and Stata</a></p>
<p>Introduction: A student I’m working with writes:
  
I was planning on getting a applied stat text as a desk reference, and for that I’m assuming you’d recommend your own book. Also, being an economics student, I was initially planning on doing my analysis in STATA, but I noticed on your blog that you use R, and apparently so does the rest of the statistics profession. Would you rather I do my programming in R this summer, or does it not matter? It doesn’t look too hard to learn, so just let me know what’s most convenient for you.
  
My reply:  Yes, I recommend my book with Jennifer Hill.  Also the book by John Fox, An R and S-plus Companion to Applied Regression, is a good way to get into R.  I recommend you use both Stata and R.  If you’re already familiar with Stata, then stick with it–it’s a great system for working with big datasets.  You can grab your data in Stata, do some basic manipulations, then save a smaller dataset to read into R (using R’s read.dta() function).  Once you want to make fu</p><p>5 0.16026713 <a title="442-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-24-Mister_P_in_Stata.html">869 andrew gelman stats-2011-08-24-Mister P in Stata</a></p>
<p>Introduction: Maurizio Pisati sends along  this presentation  of work with Valeria Glorioso.  He writes:  “Our major problem, now, is uncertainty estimation — we’re still struggling to find a solution appropriate to the Stata environment.”</p><p>6 0.15903521 <a title="442-tfidf-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-01-How_does_Bayes_do_it%3F.html">247 andrew gelman stats-2010-09-01-How does Bayes do it?</a></p>
<p>7 0.15650707 <a title="442-tfidf-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-11-Free_online_course_in_multilevel_modeling.html">80 andrew gelman stats-2010-06-11-Free online course in multilevel modeling</a></p>
<p>8 0.13028035 <a title="442-tfidf-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-10-R_vs._Stata%2C_or%2C_Different_ways_to_estimate_multilevel_models.html">269 andrew gelman stats-2010-09-10-R vs. Stata, or, Different ways to estimate multilevel models</a></p>
<p>9 0.1295585 <a title="442-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-03-Statistical_methods_that_work_in_some_settings_but_not_others.html">1560 andrew gelman stats-2012-11-03-Statistical methods that work in some settings but not others</a></p>
<p>10 0.11962494 <a title="442-tfidf-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-22-Spell-checking_example_demonstrates_key_aspects_of_Bayesian_data_analysis.html">2182 andrew gelman stats-2014-01-22-Spell-checking example demonstrates key aspects of Bayesian data analysis</a></p>
<p>11 0.11438645 <a title="442-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-15-Reputations_changeable%2C_situations_tolerable.html">1858 andrew gelman stats-2013-05-15-Reputations changeable, situations tolerable</a></p>
<p>12 0.1138701 <a title="442-tfidf-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-Transformations_for_non-normal_data.html">2176 andrew gelman stats-2014-01-19-Transformations for non-normal data</a></p>
<p>13 0.11184394 <a title="442-tfidf-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-06-Quote_of_the_day.html">398 andrew gelman stats-2010-11-06-Quote of the day</a></p>
<p>14 0.10868007 <a title="442-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>15 0.10564148 <a title="442-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>16 0.10498375 <a title="442-tfidf-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>17 0.10147719 <a title="442-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>18 0.1000923 <a title="442-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>19 0.09766645 <a title="442-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>20 0.091550812 <a title="442-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-16-Update_on_the_generalized_method_of_moments.html">519 andrew gelman stats-2011-01-16-Update on the generalized method of moments</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.047), (1, 0.127), (2, 0.013), (3, 0.05), (4, -0.013), (5, -0.011), (6, 0.085), (7, -0.01), (8, -0.108), (9, 0.006), (10, -0.011), (11, -0.028), (12, 0.031), (13, -0.001), (14, -0.005), (15, -0.035), (16, -0.021), (17, 0.001), (18, 0.023), (19, -0.039), (20, 0.034), (21, -0.026), (22, 0.023), (23, 0.018), (24, 0.015), (25, 0.003), (26, 0.0), (27, 0.003), (28, 0.042), (29, 0.006), (30, -0.028), (31, 0.02), (32, 0.01), (33, -0.004), (34, 0.015), (35, -0.011), (36, -0.032), (37, 0.015), (38, -0.052), (39, 0.03), (40, 0.006), (41, 0.014), (42, 0.005), (43, 0.01), (44, 0.053), (45, 0.026), (46, 0.008), (47, 0.029), (48, 0.012), (49, -0.011)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98735416 <a title="442-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-01-bayesglm_in_Stata%3F.html">442 andrew gelman stats-2010-12-01-bayesglm in Stata?</a></p>
<p>Introduction: Is there an implementation of bayesglm in Stata?  (That is, approximate maximum penalized likelihood estimation with specified normal or t prior distributions on the coefficients.)</p><p>2 0.75795525 <a title="442-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>Introduction: For awhile I’ve been fitting most of my multilevel models using lmer/glmer, which gives point estimates of the group-level variance parameters (maximum marginal likelihood estimate for lmer and an approximation for glmer).  I’m usually satisfied with this–sure, point estimation understates the uncertainty in model fitting, but that’s typically the least of our worries. 
 
Sometimes, though, lmer/glmer estimates group-level variances at 0 or estimates group-level correlation parameters at +/- 1.  Typically, when this happens, it’s not that we’re so sure the variance is close to zero or that the correlation is close to 1 or -1; rather, the marginal likelihood does not provide a lot of information about these parameters of the group-level error distribution.
 
I don’t want point estimates on the boundary.  I don’t want to say that the unexplained variance in some dimension is exactly zero.
 
One way to handle this problem is full Bayes:  slap a prior on sigma, do your Gibbs and Metropolis</p><p>3 0.742984 <a title="442-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>Introduction: I’ve had a couple of email conversations in the past couple days on dependence in multivariate prior distributions.
 
 Modeling the degrees of freedom and scale parameters in the t distribution 
 
First, in our Stan group we’ve been discussing the choice of priors for the degrees-of-freedom parameter in the t distribution.  I wrote that also there’s the question of parameterization.  It does not necessarily make sense to have independent priors on the df and scale parameters.  In some sense, the meaning of the scale parameter changes with the df.
 
 Prior dependence between correlation and scale parameters in the scaled inverse-Wishart model 
 
The second case of parameterization in prior distribution arose from an email I received from Chris Chatham pointing me to  this exploration  by Matt Simpson of the scaled inverse-Wishart prior distribution for hierarchical covariance matrices.  Simpson writes:
  
A popular prior for Σ is the inverse-Wishart distribution [ not  the same as the</p><p>4 0.73945093 <a title="442-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>Introduction: A student writes:
  
I have a question about an earlier recommendation of yours on the election of the prior distribution for the precision hyperparameter of a normal distribution, and a reference for the recommendation. If I recall correctly I have read that you have suggested to use Gamma(1.4, 0.4) instead of Gamma(0.01,0.01) for the prior distribution of the precision hyper parameter of a normal distribution.


I would very much appreciate if you would have the time to point me to this publication of yours. The reason is that I have used the prior distribution (Gamma(1.4, 0.4)) in a study which we now revise for publication, and where a reviewer question the choice of the distribution (claiming that it is too informative!).


I am well aware of that you in recent publications (Prior distributions for variance parameters in hierarchical models. Bayesian Analysis; Data Analysis using regression and multilevel/hierarchical models) suggest to model the precision as pow(standard deviatio</p><p>5 0.73372287 <a title="442-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-07-Neutral_noninformative_and_informative_conjugate_beta_and_gamma_prior_distributions.html">1046 andrew gelman stats-2011-12-07-Neutral noninformative and informative conjugate beta and gamma prior distributions</a></p>
<p>Introduction: Jouni Kerman did a cool bit of research justifying the Beta (1/3, 1/3) prior as noninformative for binomial data, and the Gamma (1/3, 0) prior for Poisson data.
 
You probably thought that nothing new could be said about noninformative priors in such basic problems, but you were wrong!
 
Here’s  the story :
  
The conjugate binomial and Poisson models are commonly used for estimating proportions or rates. However, it is not well known that the conventional noninformative conjugate priors tend to shrink the posterior quantiles toward the boundary or toward the middle of the parameter space, making them thus appear excessively informative. The shrinkage is always largest when the number of observed events is small. This behavior persists for all sample sizes and exposures. The effect of the prior is therefore most conspicuous and potentially controversial when analyzing rare events. As alternative default conjugate priors, I [Jouni] introduce Beta(1/3, 1/3) and Gamma(1/3, 0), which I cal</p><p>6 0.73241466 <a title="442-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-20-Prior_beliefs_about_locations_of_decision_boundaries.html">1130 andrew gelman stats-2012-01-20-Prior beliefs about locations of decision boundaries</a></p>
<p>7 0.72669071 <a title="442-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-15-Reputations_changeable%2C_situations_tolerable.html">1858 andrew gelman stats-2013-05-15-Reputations changeable, situations tolerable</a></p>
<p>8 0.69103116 <a title="442-lsi-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>9 0.67240834 <a title="442-lsi-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-01-How_does_Bayes_do_it%3F.html">247 andrew gelman stats-2010-09-01-How does Bayes do it?</a></p>
<p>10 0.66661239 <a title="442-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>11 0.66657448 <a title="442-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>12 0.66442078 <a title="442-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-21-D._Buggin.html">1465 andrew gelman stats-2012-08-21-D. Buggin</a></p>
<p>13 0.64284122 <a title="442-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-13-On_the_half-Cauchy_prior_for_a_global_scale_parameter.html">801 andrew gelman stats-2011-07-13-On the half-Cauchy prior for a global scale parameter</a></p>
<p>14 0.64272124 <a title="442-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-31-Using_sample_size_in_the_prior_distribution.html">547 andrew gelman stats-2011-01-31-Using sample size in the prior distribution</a></p>
<p>15 0.64033723 <a title="442-lsi-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-06-Bootstrap_averaging%3A_Examples_where_it_works_and_where_it_doesn%E2%80%99t_work.html">2201 andrew gelman stats-2014-02-06-Bootstrap averaging: Examples where it works and where it doesn’t work</a></p>
<p>16 0.63875192 <a title="442-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-11-%E2%80%9CInformative_g-Priors_for_Logistic_Regression%E2%80%9D.html">2017 andrew gelman stats-2013-09-11-“Informative g-Priors for Logistic Regression”</a></p>
<p>17 0.63096231 <a title="442-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-16-Update_on_the_generalized_method_of_moments.html">519 andrew gelman stats-2011-01-16-Update on the generalized method of moments</a></p>
<p>18 0.62468553 <a title="442-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>19 0.62371773 <a title="442-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-17-Jumping_off_the_edge_of_the_world.html">858 andrew gelman stats-2011-08-17-Jumping off the edge of the world</a></p>
<p>20 0.61979538 <a title="442-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-28-Path_sampling_for_models_of_varying_dimension.html">1089 andrew gelman stats-2011-12-28-Path sampling for models of varying dimension</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(5, 0.068), (16, 0.154), (24, 0.157), (54, 0.097), (81, 0.088), (86, 0.169), (99, 0.055)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98264313 <a title="442-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-01-bayesglm_in_Stata%3F.html">442 andrew gelman stats-2010-12-01-bayesglm in Stata?</a></p>
<p>Introduction: Is there an implementation of bayesglm in Stata?  (That is, approximate maximum penalized likelihood estimation with specified normal or t prior distributions on the coefficients.)</p><p>2 0.76568919 <a title="442-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-29-Where_36%25_of_all_boys_end_up_nowadays.html">1697 andrew gelman stats-2013-01-29-Where 36% of all boys end up nowadays</a></p>
<p>Introduction: My  Take a Number feature  appears in today’s Times.  And here are the graphs that I wish they’d had space to include!
 
 
 
 
 
Original story  here .</p><p>3 0.69200587 <a title="442-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-29-Quality_control_problems_at_the_New_York_Times.html">436 andrew gelman stats-2010-11-29-Quality control problems at the New York Times</a></p>
<p>Introduction: I guess thereâ&euro;&trade;s a reason they put  this stuff  in the Opinion section and not in the Science section, huh?
 
P.S.  More  here .</p><p>4 0.67426521 <a title="442-lda-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-04-Why_does_anyone_support_private_macroeconomic_forecasts%3F.html">185 andrew gelman stats-2010-08-04-Why does anyone support private macroeconomic forecasts?</a></p>
<p>Introduction: Tyler Cowen  asks  the above question.  I donâ&euro;&trade;t have a full answer, but, in the Economics section of  A Quantitative Tour of the Social Sciences , Richard Clarida discusses in detail the ways that researchers have tried to estimate the extent to which government or private forecasts supply additional information.</p><p>5 0.6675446 <a title="442-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-13-Stan_at_NIPS_2012_Workshop_on_Probabilistic_Programming.html">1576 andrew gelman stats-2012-11-13-Stan at NIPS 2012 Workshop on Probabilistic Programming</a></p>
<p>Introduction: If you need an excuse to go skiing in Tahoe next month, our paper on Stan as a probabilistic programming language was accepted for:
  
  Workshop on Probabilistic Programming  
NIPS 2012 
7–8 December, 2012, Lake Tahoe, Nevada
   
The workshop is organized by the folks behind the probabilistic programming language  Church  and has a great lineup of invited speakers (Chris Bishop, Josh Tennenbaum, and Stuart Russell).  And in case you’re interested in the main conference, here’s the list of accepted  NIPS 2012 papers and posters .  
 
To learn more about Stan, check out the links to the manual on the
  
  Stan Home Page  
  
We’ll put up a link to our final NIPS workshop paper there when we finish it.</p><p>6 0.65683609 <a title="442-lda-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-03-Gladwell_vs_Pinker.html">253 andrew gelman stats-2010-09-03-Gladwell vs Pinker</a></p>
<p>7 0.65637773 <a title="442-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-26-A_very_short_story.html">164 andrew gelman stats-2010-07-26-A very short story</a></p>
<p>8 0.65480673 <a title="442-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-10-Aleks_says_this_is_the_future_of_visualization.html">795 andrew gelman stats-2011-07-10-Aleks says this is the future of visualization</a></p>
<p>9 0.65259981 <a title="442-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-26-Luck_or_knowledge%3F.html">873 andrew gelman stats-2011-08-26-Luck or knowledge?</a></p>
<p>10 0.64927959 <a title="442-lda-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-06-Quote_of_the_day.html">398 andrew gelman stats-2010-11-06-Quote of the day</a></p>
<p>11 0.64892244 <a title="442-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-14-A_model_rejection_letter.html">1118 andrew gelman stats-2012-01-14-A model rejection letter</a></p>
<p>12 0.64396721 <a title="442-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-20-Bugs_Bunny%2C_the_governor_of_Massachusetts%2C_the_Dow_36%2C000_guy%2C_presidential_qualifications%2C_and_Peggy_Noonan.html">1129 andrew gelman stats-2012-01-20-Bugs Bunny, the governor of Massachusetts, the Dow 36,000 guy, presidential qualifications, and Peggy Noonan</a></p>
<p>13 0.63722962 <a title="442-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-24-A_new_efficient_lossless_compression_algorithm.html">228 andrew gelman stats-2010-08-24-A new efficient lossless compression algorithm</a></p>
<p>14 0.63139647 <a title="442-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>15 0.62959433 <a title="442-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-11-Toward_a_framework_for_automatic_model_building.html">1718 andrew gelman stats-2013-02-11-Toward a framework for automatic model building</a></p>
<p>16 0.6272729 <a title="442-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-24-More_from_the_sister_blog.html">1427 andrew gelman stats-2012-07-24-More from the sister blog</a></p>
<p>17 0.62726802 <a title="442-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-02-Reintegrating_rebels_into_civilian_life%3A_Quasi-experimental_evidence_from_Burundi.html">177 andrew gelman stats-2010-08-02-Reintegrating rebels into civilian life: Quasi-experimental evidence from Burundi</a></p>
<p>18 0.62662417 <a title="442-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-25-College_football%2C_voting%2C_and_the_law_of_large_numbers.html">1547 andrew gelman stats-2012-10-25-College football, voting, and the law of large numbers</a></p>
<p>19 0.62240326 <a title="442-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-11-Migrating_your_blog_from_Movable_Type_to_WordPress.html">1530 andrew gelman stats-2012-10-11-Migrating your blog from Movable Type to WordPress</a></p>
<p>20 0.62014955 <a title="442-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-04-To_commenters_who_are_trying_to_sell_something.html">839 andrew gelman stats-2011-08-04-To commenters who are trying to sell something</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
