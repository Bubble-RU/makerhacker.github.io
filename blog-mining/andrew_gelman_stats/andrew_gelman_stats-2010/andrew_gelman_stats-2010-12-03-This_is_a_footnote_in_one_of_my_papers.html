<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>448 andrew gelman stats-2010-12-03-This is a footnote in one of my papers</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-448" href="#">andrew_gelman_stats-2010-448</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>448 andrew gelman stats-2010-12-03-This is a footnote in one of my papers</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-448-html" href="http://andrewgelman.com/2010/12/03/this_is_a_footn/">html</a></p><p>Introduction: In the annals of hack literature, it is sometimes said that if you aim to write best-selling crap, all you’ll end up with is crap.  To truly produce best-selling crap, you have to have a conviction, perhaps misplaced, that your writing has integrity.  Whether or not this is a good generalization about writing, I have seen an analogous phenomenon in statistics:  If you try to do nothing but model the data, you can be in for a wild and unpleasant ride:  real data always seem to have one more twist beyond our ability to model (von Neumann’s elephant’s trunk notwithstanding).  But if you model the underlying process, sometimes your model can fit surprisingly well as well as inviting openings for future research progress.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In the annals of hack literature, it is sometimes said that if you aim to write best-selling crap, all you’ll end up with is crap. [sent-1, score-0.849]
</p><p>2 To truly produce best-selling crap, you have to have a conviction, perhaps misplaced, that your writing has integrity. [sent-2, score-0.448]
</p><p>3 But if you model the underlying process, sometimes your model can fit surprisingly well as well as inviting openings for future research progress. [sent-4, score-1.567]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('crap', 0.298), ('trunk', 0.228), ('misplaced', 0.215), ('notwithstanding', 0.215), ('openings', 0.215), ('conviction', 0.198), ('elephant', 0.192), ('inviting', 0.192), ('neumann', 0.188), ('von', 0.183), ('model', 0.18), ('wild', 0.176), ('analogous', 0.176), ('annals', 0.176), ('unpleasant', 0.173), ('hack', 0.17), ('ride', 0.161), ('twist', 0.159), ('generalization', 0.157), ('aim', 0.151), ('sometimes', 0.148), ('surprisingly', 0.147), ('writing', 0.137), ('phenomenon', 0.137), ('truly', 0.129), ('produce', 0.125), ('progress', 0.12), ('ability', 0.106), ('underlying', 0.102), ('well', 0.095), ('future', 0.094), ('process', 0.086), ('literature', 0.084), ('beyond', 0.084), ('seen', 0.08), ('end', 0.077), ('fit', 0.074), ('nothing', 0.073), ('try', 0.067), ('whether', 0.065), ('said', 0.065), ('real', 0.065), ('write', 0.062), ('data', 0.062), ('seem', 0.059), ('always', 0.059), ('perhaps', 0.057), ('ll', 0.052), ('statistics', 0.048), ('research', 0.045)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="448-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-03-This_is_a_footnote_in_one_of_my_papers.html">448 andrew gelman stats-2010-12-03-This is a footnote in one of my papers</a></p>
<p>Introduction: In the annals of hack literature, it is sometimes said that if you aim to write best-selling crap, all you’ll end up with is crap.  To truly produce best-selling crap, you have to have a conviction, perhaps misplaced, that your writing has integrity.  Whether or not this is a good generalization about writing, I have seen an analogous phenomenon in statistics:  If you try to do nothing but model the data, you can be in for a wild and unpleasant ride:  real data always seem to have one more twist beyond our ability to model (von Neumann’s elephant’s trunk notwithstanding).  But if you model the underlying process, sometimes your model can fit surprisingly well as well as inviting openings for future research progress.</p><p>2 0.11621992 <a title="448-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-13-Watching_the_sharks_jump.html">1494 andrew gelman stats-2012-09-13-Watching the sharks jump</a></p>
<p>Introduction: Recently in the sister blog:
 
   
 
Niall Ferguson is a  hack .
 
Niall Ferguson is not always a hack, sometimes he just makes  silly mistakes .
 
Paul Krugman is not a hack, but he sometimes he goes  over the top .
 
 Reflections on hacks .
 
P.S.  Yes, technically I’m misusing the expression, it should really be something like, “Watching the sharks get jumped.”  But I liked the image of the jumping shark.</p><p>3 0.10770185 <a title="448-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-13-Flexibility_is_good.html">2133 andrew gelman stats-2013-12-13-Flexibility is good</a></p>
<p>Introduction: If I made a separate post for each interesting blog discussion, we’d get overwhelmed.  That’s why I often leave detailed responses in the comments section, even though I’m pretty sure that most readers don’t look in the comments at all.
 
Sometimes, though, I think it’s good to bring such discussions to light.  Here’s a recent example.
 
Michael  wrote :
  
Poor predictive performance usually indicates that the model isn’t sufficiently flexible to explain the data, and my understanding of the proper Bayesian strategy is to feed that back into your original model and try again until you achieve better performance.
  
Corey  replied :
  
It was my impression that — in ML at least — poor predictive performance is more often due to the model being too flexible and fitting noise.
  
And Rahul  agreed :
  
Good point. A very flexible model will describe your training data perfectly and then go bonkers when unleashed on wild data.
  
But I  wrote :
  
Overfitting comes from a model being flex</p><p>4 0.099342801 <a title="448-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>Introduction: In response to  my remarks  on his online book, Think Bayes, Allen Downey wrote: 
   
I [Downey] have a question about one of your comments: 
   My [Gelman's] main criticism with both books is that they talk a lot about inference but not so much about model building or model checking (recall the three steps of Bayesian data analysis). I think it’s ok for an introductory book to focus on inference, which of course is central to the data-analytic process—but I’d like them to at least mention that Bayesian ideas arise in model building and model checking as well. 

This sounds like something I agree with, and one of the things I tried to do in the book is to put modeling decisions front and center.  But the word “modeling” is used in lots of ways, so I want to see if we are talking about the same thing.


For example, in many chapters, I start with a simple model of the scenario, do some analysis, then check whether the model is good enough, and iterate.  Here’s the discussion of modeling</p><p>5 0.097510524 <a title="448-tfidf-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-25-The_von_Neumann_paradox.html">430 andrew gelman stats-2010-11-25-The von Neumann paradox</a></p>
<p>Introduction: I, like  Steve Hsu , I too would love to read a definitive biography of John von Neumann (or, as we’d say in the U.S., “John Neumann”).  I’ve read little things about him in various places such as Stanislaw Ulam’s classic autobiography, and two things I’ve repeatedly noticed are:
 
1.  Neumann comes off as a obnoxious, self-satisfied jerk.  He just seems like the kind of guy I wouldn’t like in real life.
 
2.  All these great men seem to really have loved the guy.
 
It’s hard for me to reconcile  two impressions above.  Of course, lots of people have a good side and a bad side, but what’s striking here is that my impressions of Neumann’s bad side come from the very stories that his friends use to demonstrate how lovable he was!  So, yes, I’d like to see the biography–but only if it could resolve this paradox.
 
Also, I don’t know how relevant this is, but Neumann shares one thing with the more-lovable Ulam and the less-lovable Mandelbrot:  all had Jewish backgrounds but didn’t seem to</p><p>6 0.092316128 <a title="448-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-06-How_to_think_about_papers_published_in_low-grade_journals%3F.html">1928 andrew gelman stats-2013-07-06-How to think about papers published in low-grade journals?</a></p>
<p>7 0.088083714 <a title="448-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-Another_reason_why_you_can_get_good_inferences_from_a_bad_model.html">1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</a></p>
<p>8 0.085257672 <a title="448-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-07-Free_advice_from_an_academic_writing_coach%21.html">1658 andrew gelman stats-2013-01-07-Free advice from an academic writing coach!</a></p>
<p>9 0.08441437 <a title="448-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-04-Retraction_Watch.html">838 andrew gelman stats-2011-08-04-Retraction Watch</a></p>
<p>10 0.080837913 <a title="448-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-21-Literary_blurb_translation_guide.html">723 andrew gelman stats-2011-05-21-Literary blurb translation guide</a></p>
<p>11 0.078979209 <a title="448-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>12 0.07819432 <a title="448-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-28-Using_predator-prey_models_on_the_Canadian_lynx_series.html">1141 andrew gelman stats-2012-01-28-Using predator-prey models on the Canadian lynx series</a></p>
<p>13 0.077655047 <a title="448-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>14 0.07696604 <a title="448-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-27-Bridges_between_deterministic_and_probabilistic_models_for_binary_data.html">780 andrew gelman stats-2011-06-27-Bridges between deterministic and probabilistic models for binary data</a></p>
<p>15 0.074734382 <a title="448-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-21-More_on_Bayesian_model_selection_in_high-dimensional_settings.html">1817 andrew gelman stats-2013-04-21-More on Bayesian model selection in high-dimensional settings</a></p>
<p>16 0.074422069 <a title="448-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-05-Question_26_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1367 andrew gelman stats-2012-06-05-Question 26 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>17 0.071201488 <a title="448-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-11-Adding_an_error_model_to_a_deterministic_model.html">1162 andrew gelman stats-2012-02-11-Adding an error model to a deterministic model</a></p>
<p>18 0.071179718 <a title="448-tfidf-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-27-Neumann_update.html">432 andrew gelman stats-2010-11-27-Neumann update</a></p>
<p>19 0.070703812 <a title="448-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-06-Question_27_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1368 andrew gelman stats-2012-06-06-Question 27 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>20 0.06878943 <a title="448-tfidf-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Useful_models%2C_model_checking%2C_and_external_validation%3A__a_mini-discussion.html">244 andrew gelman stats-2010-08-30-Useful models, model checking, and external validation:  a mini-discussion</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.114), (1, 0.044), (2, -0.03), (3, 0.022), (4, 0.004), (5, 0.013), (6, 0.003), (7, -0.019), (8, 0.074), (9, 0.023), (10, 0.035), (11, 0.043), (12, -0.053), (13, -0.006), (14, -0.064), (15, -0.033), (16, 0.001), (17, -0.036), (18, -0.005), (19, -0.013), (20, 0.013), (21, -0.042), (22, -0.05), (23, -0.052), (24, -0.035), (25, 0.018), (26, -0.022), (27, -0.008), (28, 0.022), (29, 0.018), (30, -0.015), (31, -0.036), (32, -0.04), (33, 0.038), (34, 0.009), (35, 0.034), (36, -0.003), (37, -0.004), (38, 0.012), (39, -0.009), (40, 0.002), (41, -0.025), (42, -0.011), (43, 0.01), (44, 0.01), (45, -0.004), (46, -0.064), (47, -0.014), (48, 0.001), (49, 0.026)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97149628 <a title="448-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-03-This_is_a_footnote_in_one_of_my_papers.html">448 andrew gelman stats-2010-12-03-This is a footnote in one of my papers</a></p>
<p>Introduction: In the annals of hack literature, it is sometimes said that if you aim to write best-selling crap, all you’ll end up with is crap.  To truly produce best-selling crap, you have to have a conviction, perhaps misplaced, that your writing has integrity.  Whether or not this is a good generalization about writing, I have seen an analogous phenomenon in statistics:  If you try to do nothing but model the data, you can be in for a wild and unpleasant ride:  real data always seem to have one more twist beyond our ability to model (von Neumann’s elephant’s trunk notwithstanding).  But if you model the underlying process, sometimes your model can fit surprisingly well as well as inviting openings for future research progress.</p><p>2 0.87030917 <a title="448-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-13-Flexibility_is_good.html">2133 andrew gelman stats-2013-12-13-Flexibility is good</a></p>
<p>Introduction: If I made a separate post for each interesting blog discussion, we’d get overwhelmed.  That’s why I often leave detailed responses in the comments section, even though I’m pretty sure that most readers don’t look in the comments at all.
 
Sometimes, though, I think it’s good to bring such discussions to light.  Here’s a recent example.
 
Michael  wrote :
  
Poor predictive performance usually indicates that the model isn’t sufficiently flexible to explain the data, and my understanding of the proper Bayesian strategy is to feed that back into your original model and try again until you achieve better performance.
  
Corey  replied :
  
It was my impression that — in ML at least — poor predictive performance is more often due to the model being too flexible and fitting noise.
  
And Rahul  agreed :
  
Good point. A very flexible model will describe your training data perfectly and then go bonkers when unleashed on wild data.
  
But I  wrote :
  
Overfitting comes from a model being flex</p><p>3 0.83832151 <a title="448-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-11-Kaiser_Fung_on_how_not_to_critique_models.html">1004 andrew gelman stats-2011-11-11-Kaiser Fung on how not to critique models</a></p>
<p>Introduction: In the context of a debate between economists Brad DeLong and Tyler Cowen on the “IS-LM model” [no, I don't know what it is, either!], Kaiser  writes :
  
Since a model is an abstraction, a simplification of reality, no model is above critique.


I [Kaiser] consider the following types of critique not deserving:


1) The critique that the modeler makes an assumption 
2) The critique that the modeler makes an assumption for mathematical convenience 
3) The critique that the model omits some feature 
4) The critique that the model doesn’t fit one’s intuition 
5) The critique that the model fails to make a specific prediction


Above all, a serious critique must include an alternative model that is provably better than the one it criticises. It is not enough to show that the alternative solves the problems being pointed out; the alternative must do so while preserving the useful aspects of the model being criticized.
  
I have mixed feelings about Kaiser’s rules.  On one hand, I agree wit</p><p>4 0.82955003 <a title="448-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-28-Using_predator-prey_models_on_the_Canadian_lynx_series.html">1141 andrew gelman stats-2012-01-28-Using predator-prey models on the Canadian lynx series</a></p>
<p>Introduction: The “Canadian lynx data” is one of the famous examples used in time series analysis.  And the usual models that are fit to these data in the statistics time-series literature, don’t work well.  Cavan Reilly and Angelique Zeringue  write :
 
   
 
Reilly and Zeringue then present their analysis.  Their simple little predator-prey model with a weakly informative prior way outperforms the standard big-ass autoregression models.  Check this out:
 
   
 
Or, to put it into numbers, when they fit their model to the first 80 years and predict to the next 34, their root mean square out-of-sample error is 1480 (see scale of data above).  In contrast, the standard model fit to these data (the SETAR model of Tong, 1990) has more than twice as many parameters but gets a worse-performing root mean square error of 1600, even when that model is fit to the entire dataset.  (If you fit the SETAR or any similar autoregressive model to the first 80 years and use it to predict the next 34, the predictions</p><p>5 0.82576662 <a title="448-lsi-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-09-Special_journal_issue_on_statistical_methods_for_the_social_sciences.html">24 andrew gelman stats-2010-05-09-Special journal issue on statistical methods for the social sciences</a></p>
<p>Introduction: Last year I  spoke at  a conference celebrating the 10th anniversary of the University of Washington’s Center for Statistics and the Social Sciences, and just today a  special issue  of the journal Statistical Methodology came out in honor of the center’s anniversary.   My article  in the special issue actually has nothing to do with my talk at the conference; rather, it’s an exploration of an idea that Iven Van Mechelen and I had for understanding deterministic models probabilistically:
  
 
For the analysis of binary data, various deterministic models have been proposed, which are generally simpler to fit and easier to understand than probabilistic models. We claim that corresponding to any deterministic model is an implicit stochastic model in which the deterministic model fits imperfectly, with errors occurring at random. In the context of binary data, we consider a model in which the probability of error depends on the model prediction. We show how to fit this model using a stocha</p><p>6 0.82036769 <a title="448-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-03-Popper_and_Jaynes.html">2007 andrew gelman stats-2013-09-03-Popper and Jaynes</a></p>
<p>7 0.81325883 <a title="448-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-27-Bridges_between_deterministic_and_probabilistic_models_for_binary_data.html">780 andrew gelman stats-2011-06-27-Bridges between deterministic and probabilistic models for binary data</a></p>
<p>8 0.810305 <a title="448-lsi-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-08-Displaying_a_fitted_multilevel_model.html">328 andrew gelman stats-2010-10-08-Displaying a fitted multilevel model</a></p>
<p>9 0.80254948 <a title="448-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-21-More_on_Bayesian_model_selection_in_high-dimensional_settings.html">1817 andrew gelman stats-2013-04-21-More on Bayesian model selection in high-dimensional settings</a></p>
<p>10 0.80234081 <a title="448-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>11 0.79537845 <a title="448-lsi-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-12-UnConMax_%E2%80%93_uncertainty_consideration_maxims_7_%2B--_2.html">82 andrew gelman stats-2010-06-12-UnConMax – uncertainty consideration maxims 7 +-- 2</a></p>
<p>12 0.78306437 <a title="448-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-04-Columbo_does_posterior_predictive_checks.html">1521 andrew gelman stats-2012-10-04-Columbo does posterior predictive checks</a></p>
<p>13 0.78174186 <a title="448-lsi-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-19-The_%E2%80%9Ceither-or%E2%80%9D_fallacy_of_believing_in_discrete_models%3A__an_example_of_folk_statistics.html">217 andrew gelman stats-2010-08-19-The “either-or” fallacy of believing in discrete models:  an example of folk statistics</a></p>
<p>14 0.77075952 <a title="448-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>15 0.77067357 <a title="448-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-11-Adding_an_error_model_to_a_deterministic_model.html">1162 andrew gelman stats-2012-02-11-Adding an error model to a deterministic model</a></p>
<p>16 0.76939481 <a title="448-lsi-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-09-Removing_the_blindfold%3A_visualising_statistical_models.html">265 andrew gelman stats-2010-09-09-Removing the blindfold: visualising statistical models</a></p>
<p>17 0.7668041 <a title="448-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>18 0.76512605 <a title="448-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-04-An_addition_to_the_model-makers%E2%80%99_oath.html">554 andrew gelman stats-2011-02-04-An addition to the model-makers’ oath</a></p>
<p>19 0.75824714 <a title="448-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>20 0.7449742 <a title="448-lsi-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-05-Does_posterior_predictive_model_checking_fit_with_the_operational_subjective_approach%3F.html">320 andrew gelman stats-2010-10-05-Does posterior predictive model checking fit with the operational subjective approach?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(15, 0.021), (16, 0.077), (21, 0.057), (22, 0.346), (24, 0.101), (41, 0.022), (61, 0.025), (80, 0.024), (96, 0.026), (99, 0.181)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.8587029 <a title="448-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-03-This_is_a_footnote_in_one_of_my_papers.html">448 andrew gelman stats-2010-12-03-This is a footnote in one of my papers</a></p>
<p>Introduction: In the annals of hack literature, it is sometimes said that if you aim to write best-selling crap, all you’ll end up with is crap.  To truly produce best-selling crap, you have to have a conviction, perhaps misplaced, that your writing has integrity.  Whether or not this is a good generalization about writing, I have seen an analogous phenomenon in statistics:  If you try to do nothing but model the data, you can be in for a wild and unpleasant ride:  real data always seem to have one more twist beyond our ability to model (von Neumann’s elephant’s trunk notwithstanding).  But if you model the underlying process, sometimes your model can fit surprisingly well as well as inviting openings for future research progress.</p><p>2 0.78849268 <a title="448-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-01-Lamentably_common_misunderstanding_of_meritocracy.html">1037 andrew gelman stats-2011-12-01-Lamentably common misunderstanding of meritocracy</a></p>
<p>Introduction: Tyler Cowen  pointed  to  an article  by business-school professor Luigi Zingales about meritocracy.  I’d expect a b-school prof to support the idea of meritocracy, and Zingales does not disappoint.
 
But he says a bunch of other things that to me represent a confused conflation of ideas.  Here’s Zingales:
  
America became known as a land of opportunity—a place whose capitalist system benefited  the hardworking and the virtuous  [emphasis added]. In a word, it was a meritocracy.
  
That’s interesting—and revealing.  Here’s what I get when I look up “meritocracy” in the  dictionary :
  
1 : a system in which the talented are chosen and moved ahead on the basis of their achievement 
2 : leadership selected on the basis of intellectual criteria
  
Nothing here about “hardworking” or “virtuous.”  In a meritocracy, you can be as hardworking as John Kruk or as virtuous as Kobe Bryant and you’ll still get ahead—if you have the talent and achievement.  Throwing in “hardworking” and “virtuous”</p><p>3 0.77925062 <a title="448-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-31-Snotty_reviewers.html">1700 andrew gelman stats-2013-01-31-Snotty reviewers</a></p>
<p>Introduction: I had a submission a couple years ago that was rejected by a journal.  One of the reviewers began with the following snotty aside:
  
In this manuscript Gelman and Shalizi (there’s no anonymity here; this thing has been floating around the web for some time) . . .
  
Actually, we posted it on the same day we submitted it to the journal.  But double-blindness allowed the reviewer to act as if we had done something wrong!  And, even if it  had  been “floating around the web for some time,” that’s not necessarily a bad thing.  Perhaps it just meant that the article had previously been rejected by a bad-attitude reviewer!</p><p>4 0.76013494 <a title="448-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-28-Every_time_you_take_a_sample%2C_you%E2%80%99ll_have_to_pay_this_guy_a_quarter.html">1398 andrew gelman stats-2012-06-28-Every time you take a sample, you’ll have to pay this guy a quarter</a></p>
<p>Introduction: Roy Mendelssohn pointed me to  this  heartwarming story of Jay Vadiveloo, an actuary who got a patent for the idea of statistical sampling.  Vadiveloo writes, “the results were astounding: statistical sampling worked.”
 
You may laugh, but wait till Albedo Man buys the patent and makes everybody do his bidding.  They’re gonna dig up Laplace and make him pay retroactive royalties.  And somehow Clippy will get involved in all this.
 
P.S.  Mendelssohn writes:  “Yes, I felt it was a heartwarming story also.  Perhaps we can get a patent for regression.”
 
I say, forget a patent for regression.  I want a patent for the sample mean.  That’s where the real money is.  You can’t charge a lot for each use, but consider the volume!</p><p>5 0.74081099 <a title="448-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-17-Drug_testing_for_recipents_of_NSF_and_NIH_grants%3F.html">92 andrew gelman stats-2010-06-17-Drug testing for recipents of NSF and NIH grants?</a></p>
<p>Introduction: People seeking unemployment benefits or welfare would have to first pass a drug test under a proposal Sen. Orrin Hatch will try to add to legislation extending the social safety net during this time of economic turmoil.  
 
Hatch … said his idea would help battle drug addiction and could reduce the nation’s debt. He will try to get the Senate to include his amendment to a $140 billion bill extending tax breaks and social programs this week.
 
“This amendment is a way to help people get off of drugs to become productive and healthy members of society, while ensuring that valuable taxpayer dollars aren’t wasted,” he said after announcing his amendment. “Too many Americans are locked into a life of a dangerous dependency not only on drugs, but the federal assistance that serves to enable their addiction.”
 

 
I have a horrible vision of NSF and NIH dollars used to support the amphetamine dependencies of students pulling all-nighters in their bio labs. Something’s gotta be done about this</p><p>6 0.72569907 <a title="448-lda-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-13-Statistical_controversy_regarding_human_rights_violations_in_Colomnbia.html">145 andrew gelman stats-2010-07-13-Statistical controversy regarding human rights violations in Colomnbia</a></p>
<p>7 0.721084 <a title="448-lda-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-16-BDA_at_40%25_off%21.html">1984 andrew gelman stats-2013-08-16-BDA at 40% off!</a></p>
<p>8 0.71777523 <a title="448-lda-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-31-Wacky_surveys_where_they_don%E2%80%99t_tell_you_the_questions_they_asked.html">385 andrew gelman stats-2010-10-31-Wacky surveys where they don’t tell you the questions they asked</a></p>
<p>9 0.71125591 <a title="448-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-20-Costless_false_beliefs.html">477 andrew gelman stats-2010-12-20-Costless false beliefs</a></p>
<p>10 0.70602435 <a title="448-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-05-For_those_of_you_in_the_U.K.%2C_also_an_amusing_paradox_involving_the_infamous_hookah_story.html">504 andrew gelman stats-2011-01-05-For those of you in the U.K., also an amusing paradox involving the infamous hookah story</a></p>
<p>11 0.70537102 <a title="448-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-17-Modeling_group-level_predictors_in_a_multilevel_regression.html">1216 andrew gelman stats-2012-03-17-Modeling group-level predictors in a multilevel regression</a></p>
<p>12 0.69572353 <a title="448-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-29-Edgar_Allan_Poe_was_a_statistician.html">2001 andrew gelman stats-2013-08-29-Edgar Allan Poe was a statistician</a></p>
<p>13 0.66589284 <a title="448-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-04-Tesla_fires%21.html">2123 andrew gelman stats-2013-12-04-Tesla fires!</a></p>
<p>14 0.66578001 <a title="448-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-29-New_journal_on_causal_inference.html">879 andrew gelman stats-2011-08-29-New journal on causal inference</a></p>
<p>15 0.66307509 <a title="448-lda-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-10-If_an_entire_article_in_Computational_Statistics_and_Data_Analysis_were_put_together_from_other%2C_unacknowledged%2C_sources%2C_would_that_be_a_work_of_art%3F.html">1161 andrew gelman stats-2012-02-10-If an entire article in Computational Statistics and Data Analysis were put together from other, unacknowledged, sources, would that be a work of art?</a></p>
<p>16 0.65079534 <a title="448-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-01-Non-topical_blogging.html">1964 andrew gelman stats-2013-08-01-Non-topical blogging</a></p>
<p>17 0.63926935 <a title="448-lda-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-04-Honored_oldsters_write_about_statistics.html">2317 andrew gelman stats-2014-05-04-Honored oldsters write about statistics</a></p>
<p>18 0.6324138 <a title="448-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-15-How_effective_are_football_coaches%3F.html">1804 andrew gelman stats-2013-04-15-How effective are football coaches?</a></p>
<p>19 0.61290592 <a title="448-lda-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-10-Do_you_believe_that_%E2%80%9Chumans_and_other_living_things_have_evolved_over_time%E2%80%9D%3F.html">2167 andrew gelman stats-2014-01-10-Do you believe that “humans and other living things have evolved over time”?</a></p>
<p>20 0.61270159 <a title="448-lda-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-11-News_flash%3A__Probability_and_statistics_are_hard_to_understand.html">1413 andrew gelman stats-2012-07-11-News flash:  Probability and statistics are hard to understand</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
