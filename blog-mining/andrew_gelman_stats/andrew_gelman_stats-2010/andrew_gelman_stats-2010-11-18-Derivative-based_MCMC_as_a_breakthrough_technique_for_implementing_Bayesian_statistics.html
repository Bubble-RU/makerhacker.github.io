<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>419 andrew gelman stats-2010-11-18-Derivative-based MCMC as a breakthrough technique for implementing Bayesian statistics</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-419" href="#">andrew_gelman_stats-2010-419</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>419 andrew gelman stats-2010-11-18-Derivative-based MCMC as a breakthrough technique for implementing Bayesian statistics</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-419-html" href="http://andrewgelman.com/2010/11/18/derivative-base/">html</a></p><p>Introduction: John Salvatier pointed me to  this blog  on derivative based MCMC algorithms (also sometimes called “hybrid” or “Hamiltonian” Monte Carlo) and automatic differentiation as the future of MCMC.
 
This all makes sense to me and is consistent both with my mathematical intuition from studying Metropolis algorithms and my experience with Matt using hybrid MCMC when fitting hierarchical spline models. In particular, I agree with Salvatier’s point about the potential for computation of analytic derivatives of the log-density function.  As long as we’re mostly snapping together our models using analytically-simple pieces, the same part of the program that handles the computation of log-posterior densities should also be able to compute derivatives analytically.
 
I’ve been a big fan of automatic derivative-based MCMC methods since I started hearing about them a couple years ago (I’m thinking of the DREAM project and of Mark Girolami’s paper), and I too wonder why they haven’t been used more.  I</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 John Salvatier pointed me to  this blog  on derivative based MCMC algorithms (also sometimes called “hybrid” or “Hamiltonian” Monte Carlo) and automatic differentiation as the future of MCMC. [sent-1, score-0.635]
</p><p>2 This all makes sense to me and is consistent both with my mathematical intuition from studying Metropolis algorithms and my experience with Matt using hybrid MCMC when fitting hierarchical spline models. [sent-2, score-1.039]
</p><p>3 In particular, I agree with Salvatier’s point about the potential for computation of analytic derivatives of the log-density function. [sent-3, score-0.468]
</p><p>4 As long as we’re mostly snapping together our models using analytically-simple pieces, the same part of the program that handles the computation of log-posterior densities should also be able to compute derivatives analytically. [sent-4, score-0.827]
</p><p>5 I’ve been a big fan of automatic derivative-based MCMC methods since I started hearing about them a couple years ago (I’m thinking of the DREAM project and of Mark Girolami’s paper), and I too wonder why they haven’t been used more. [sent-5, score-0.548]
</p><p>6 I guess we can try implementing them in our current project in which we’re trying to fit models with deep interactions. [sent-6, score-0.383]
</p><p>7 I also suspect there are some underlying connections between derivative-based jumping rules and  redundant parameterizations for hierarchical models . [sent-7, score-0.679]
</p><p>8 Salvatier is saying what I’ve been saying (not very convincingly) for a couple years. [sent-9, score-0.256]
</p><p>9 But, somehow, seeing it in somebody else’s words makes it much more persuasive, and again I’m all excited about this stuff. [sent-10, score-0.227]
</p><p>10 My only amendment to Salvatier’s blog is that I wouldn’t refer to these as “new” algorithms; they’ve been around for something like 25 years, I think. [sent-11, score-0.199]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('salvatier', 0.509), ('mcmc', 0.243), ('algorithms', 0.241), ('hybrid', 0.235), ('derivatives', 0.217), ('automatic', 0.184), ('computation', 0.153), ('amendment', 0.122), ('girolami', 0.122), ('handles', 0.122), ('hierarchical', 0.116), ('project', 0.115), ('parameterizations', 0.114), ('redundant', 0.111), ('spline', 0.109), ('derivative', 0.106), ('convincingly', 0.104), ('differentiation', 0.104), ('models', 0.101), ('persuasive', 0.101), ('analytic', 0.098), ('densities', 0.097), ('dream', 0.094), ('implementing', 0.094), ('metropolis', 0.094), ('jumping', 0.094), ('couple', 0.092), ('excited', 0.092), ('matt', 0.088), ('hamiltonian', 0.088), ('carlo', 0.084), ('saying', 0.082), ('hearing', 0.081), ('monte', 0.08), ('pieces', 0.078), ('connections', 0.078), ('intuition', 0.078), ('refer', 0.077), ('fan', 0.076), ('compute', 0.075), ('deep', 0.073), ('somehow', 0.071), ('makes', 0.069), ('ve', 0.069), ('somebody', 0.066), ('studying', 0.066), ('rules', 0.065), ('fitting', 0.063), ('mostly', 0.062), ('consistent', 0.062)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="419-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-18-Derivative-based_MCMC_as_a_breakthrough_technique_for_implementing_Bayesian_statistics.html">419 andrew gelman stats-2010-11-18-Derivative-based MCMC as a breakthrough technique for implementing Bayesian statistics</a></p>
<p>Introduction: John Salvatier pointed me to  this blog  on derivative based MCMC algorithms (also sometimes called “hybrid” or “Hamiltonian” Monte Carlo) and automatic differentiation as the future of MCMC.
 
This all makes sense to me and is consistent both with my mathematical intuition from studying Metropolis algorithms and my experience with Matt using hybrid MCMC when fitting hierarchical spline models. In particular, I agree with Salvatier’s point about the potential for computation of analytic derivatives of the log-density function.  As long as we’re mostly snapping together our models using analytically-simple pieces, the same part of the program that handles the computation of log-posterior densities should also be able to compute derivatives analytically.
 
I’ve been a big fan of automatic derivative-based MCMC methods since I started hearing about them a couple years ago (I’m thinking of the DREAM project and of Mark Girolami’s paper), and I too wonder why they haven’t been used more.  I</p><p>2 0.28773969 <a title="419-tfidf-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-03-MCMC_in_Python.html">181 andrew gelman stats-2010-08-03-MCMC in Python</a></p>
<p>Introduction: John Salvatier forwards a note from Anand Patil that  a paper on PyMC  has appeared in the Journal of Statistical Software,  Weâ&euro;&trade;ll have to check this out.</p><p>3 0.20028323 <a title="419-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-What_are_the_trickiest_models_to_fit%3F.html">575 andrew gelman stats-2011-02-15-What are the trickiest models to fit?</a></p>
<p>Introduction: John Salvatier writes:
  
What do you and your readers think are the trickiest models to fit? If I had an algorithm that I claimed could fit many models with little fuss, what kinds of models would really impress you? I am interested in testing different MCMC sampling methods to evaluate their performance and I want to stretch the bounds of their abilities.
  
I don’t know what’s the trickiest, but just about anything I work on in a serious way gives me some troubles.  This reminds me that we should finish our Bayesian Benchmarks paper already.</p><p>4 0.16746351 <a title="419-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-24-Bleg%3A_Automatic_Differentiation_for_Log_Prob_Gradients%3F.html">535 andrew gelman stats-2011-01-24-Bleg: Automatic Differentiation for Log Prob Gradients?</a></p>
<p>Introduction: We need help picking out an automatic differentiation package for Hamiltonian Monte Carlo sampling from the posterior of a  generalized linear model with deep interactions.  Specifically, we need to compute gradients for log probability functions with thousands of parameters that involve matrix (determinants, eigenvalues, inverses), stats (distributions), and math (log gamma) functions.  Any suggestions?
    
 The Application: Hybrid Monte Carlo for Posteriors 
 
We’re getting serious about implementing posterior sampling using Hamiltonian Monte Carlo.  HMC speeds up mixing by including gradient information to help guide the Metropolis proposals toward areas  high probability.  In practice, the algorithm requires a handful or  of gradient calculations per sample, but there are many dimensions and the functions are hairy enough we don’t want to compute derivaties by hand.
 

 Auto Diff: Perhaps not What you Think 
 
It may not have been clear to readers of this blog that automatic diffe</p><p>5 0.13739911 <a title="419-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-04-Handy_Matrix_Cheat_Sheet%2C_with_Gradients.html">555 andrew gelman stats-2011-02-04-Handy Matrix Cheat Sheet, with Gradients</a></p>
<p>Introduction: This post is an (unpaid) advertisement for the following extremely useful resource:
  
 Petersen, K. B. and M. S. Pedersen. 2008.   The Matrix Cookbook  .  Tehcnical Report, Technical University of Denmark. 
  
It contains 70+ pages of useful relations and derivations involving matrices.  What grabbed my eye was the computation of gradients for matrix operations ranging from eigenvalues and determinants to multivariate normal density functions.   I had no idea the multivariate normal had such a clean gradient (see section 8).
  

 
We’ve been playing around with  Hamiltonian (aka Hybrid) Monte Carlo  for sampling from the posterior of hierarchical generalized linear models with lots of interactions.  HMC speeds up Metropolis sampling by using the gradient of the log probability to drive samples in the direction of higher probability density, which is particularly useful for correlated parameters that mix slowly with standard Gibbs sampling.   Matt “III” Hoffman ‘s already got it workin</p><p>6 0.1360935 <a title="419-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-24-F-f-f-fake_data.html">1735 andrew gelman stats-2013-02-24-F-f-f-fake data</a></p>
<p>7 0.12043784 <a title="419-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-04-Bayesian_Learning_via_Stochastic_Gradient_Langevin_Dynamics.html">1443 andrew gelman stats-2012-08-04-Bayesian Learning via Stochastic Gradient Langevin Dynamics</a></p>
<p>8 0.10592052 <a title="419-tfidf-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-01-%E2%80%9CThough_They_May_Be_Unaware%2C_Newlyweds_Implicitly_Know_Whether_Their_Marriage_Will_Be_Satisfying%E2%80%9D.html">2156 andrew gelman stats-2014-01-01-“Though They May Be Unaware, Newlyweds Implicitly Know Whether Their Marriage Will Be Satisfying”</a></p>
<p>9 0.10501287 <a title="419-tfidf-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-Running_into_a_Stan_Reference_by_Accident.html">2231 andrew gelman stats-2014-03-03-Running into a Stan Reference by Accident</a></p>
<p>10 0.10177125 <a title="419-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-09-Commercial_Bayesian_inference_software_is_popping_up_all_over.html">1489 andrew gelman stats-2012-09-09-Commercial Bayesian inference software is popping up all over</a></p>
<p>11 0.09511669 <a title="419-tfidf-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-19-Just_chaid.html">421 andrew gelman stats-2010-11-19-Just chaid</a></p>
<p>12 0.091294773 <a title="419-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-18-EP_and_ABC.html">2067 andrew gelman stats-2013-10-18-EP and ABC</a></p>
<p>13 0.089410126 <a title="419-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-20-Stan_at_Google_this_Thurs_and_at_Berkeley_this_Fri_noon.html">1772 andrew gelman stats-2013-03-20-Stan at Google this Thurs and at Berkeley this Fri noon</a></p>
<p>14 0.087879486 <a title="419-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-19-Will_Stan_work_well_with_40%C3%9740_matrices%3F.html">861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</a></p>
<p>15 0.087143183 <a title="419-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>16 0.085460119 <a title="419-tfidf-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-25-Good_introductory_book_for_statistical_computation%3F.html">590 andrew gelman stats-2011-02-25-Good introductory book for statistical computation?</a></p>
<p>17 0.084520854 <a title="419-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-30-Stan_Project%3A__Continuous_Relaxations_for_Discrete_MRFs.html">2003 andrew gelman stats-2013-08-30-Stan Project:  Continuous Relaxations for Discrete MRFs</a></p>
<p>18 0.081207663 <a title="419-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-28-Life_imitates_blog.html">1399 andrew gelman stats-2012-06-28-Life imitates blog</a></p>
<p>19 0.079604894 <a title="419-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-23-Learning_Differential_Geometry_for_Hamiltonian_Monte_Carlo.html">1339 andrew gelman stats-2012-05-23-Learning Differential Geometry for Hamiltonian Monte Carlo</a></p>
<p>20 0.079171717 <a title="419-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-04-Stan_in_L.A._this_Wed_3%3A30pm.html">1749 andrew gelman stats-2013-03-04-Stan in L.A. this Wed 3:30pm</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.13), (1, 0.035), (2, -0.038), (3, 0.054), (4, 0.026), (5, 0.045), (6, 0.005), (7, -0.078), (8, 0.005), (9, 0.006), (10, -0.005), (11, -0.018), (12, -0.042), (13, -0.036), (14, 0.025), (15, 0.008), (16, -0.003), (17, 0.008), (18, -0.014), (19, -0.01), (20, 0.01), (21, -0.003), (22, -0.04), (23, 0.002), (24, 0.012), (25, -0.015), (26, -0.063), (27, 0.048), (28, 0.027), (29, -0.011), (30, -0.023), (31, 0.004), (32, 0.033), (33, -0.05), (34, 0.021), (35, -0.041), (36, -0.018), (37, 0.01), (38, -0.028), (39, 0.021), (40, -0.065), (41, 0.062), (42, 0.002), (43, -0.008), (44, -0.004), (45, -0.054), (46, -0.072), (47, 0.022), (48, 0.06), (49, -0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95599359 <a title="419-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-18-Derivative-based_MCMC_as_a_breakthrough_technique_for_implementing_Bayesian_statistics.html">419 andrew gelman stats-2010-11-18-Derivative-based MCMC as a breakthrough technique for implementing Bayesian statistics</a></p>
<p>Introduction: John Salvatier pointed me to  this blog  on derivative based MCMC algorithms (also sometimes called “hybrid” or “Hamiltonian” Monte Carlo) and automatic differentiation as the future of MCMC.
 
This all makes sense to me and is consistent both with my mathematical intuition from studying Metropolis algorithms and my experience with Matt using hybrid MCMC when fitting hierarchical spline models. In particular, I agree with Salvatier’s point about the potential for computation of analytic derivatives of the log-density function.  As long as we’re mostly snapping together our models using analytically-simple pieces, the same part of the program that handles the computation of log-posterior densities should also be able to compute derivatives analytically.
 
I’ve been a big fan of automatic derivative-based MCMC methods since I started hearing about them a couple years ago (I’m thinking of the DREAM project and of Mark Girolami’s paper), and I too wonder why they haven’t been used more.  I</p><p>2 0.75959092 <a title="419-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-04-Handy_Matrix_Cheat_Sheet%2C_with_Gradients.html">555 andrew gelman stats-2011-02-04-Handy Matrix Cheat Sheet, with Gradients</a></p>
<p>Introduction: This post is an (unpaid) advertisement for the following extremely useful resource:
  
 Petersen, K. B. and M. S. Pedersen. 2008.   The Matrix Cookbook  .  Tehcnical Report, Technical University of Denmark. 
  
It contains 70+ pages of useful relations and derivations involving matrices.  What grabbed my eye was the computation of gradients for matrix operations ranging from eigenvalues and determinants to multivariate normal density functions.   I had no idea the multivariate normal had such a clean gradient (see section 8).
  

 
We’ve been playing around with  Hamiltonian (aka Hybrid) Monte Carlo  for sampling from the posterior of hierarchical generalized linear models with lots of interactions.  HMC speeds up Metropolis sampling by using the gradient of the log probability to drive samples in the direction of higher probability density, which is particularly useful for correlated parameters that mix slowly with standard Gibbs sampling.   Matt “III” Hoffman ‘s already got it workin</p><p>3 0.75281715 <a title="419-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-What_are_the_trickiest_models_to_fit%3F.html">575 andrew gelman stats-2011-02-15-What are the trickiest models to fit?</a></p>
<p>Introduction: John Salvatier writes:
  
What do you and your readers think are the trickiest models to fit? If I had an algorithm that I claimed could fit many models with little fuss, what kinds of models would really impress you? I am interested in testing different MCMC sampling methods to evaluate their performance and I want to stretch the bounds of their abilities.
  
I don’t know what’s the trickiest, but just about anything I work on in a serious way gives me some troubles.  This reminds me that we should finish our Bayesian Benchmarks paper already.</p><p>4 0.7509591 <a title="419-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<p>Introduction: Tomas Iesmantas had  asked me  for advice on a regression problem with 50 parameters, and I’d recommended Hamiltonian Monte Carlo.
 
A few weeks later he reported back: 
  
  
After trying several modifications (HMC for all parameters at once, HMC just for first level parameters and Riemman manifold Hamiltonian Monte Carlo method), I finally got it running with HMC just for first level parameters and for others using direct sampling, since conditional distributions turned out to have closed form.


However, even in this case it is quite tricky, since I had to employ mass matrix and not just diagonal but at the beginning of algorithm generated it randomly (ensuring it is positive definite). Such random generation of mass matrix is quite blind step, but it proved to be quite helpful.


Riemman manifold HMC is quite vagarious, or to be more specific, metric of manifold is very sensitive. In my model log-likelihood I had exponents and values of metrics matrix elements was very large and wh</p><p>5 0.73620075 <a title="419-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-19-R_package_for_Bayes_factors.html">1682 andrew gelman stats-2013-01-19-R package for Bayes factors</a></p>
<p>Introduction: Richard Morey writes:
  
You and your blog readers may be interested to know that a we’ve released a major new version of the BayesFactor package to CRAN. The package computes Bayes factors for linear mixed models and regression models. Of course, I’m aware you don’t like point-null model comparisons, but the package does more than that; it also allows sampling from posterior distributions of the compared models, in much the same way that your arm package does with lmer objects. The sampling (both for the Bayes factors and posteriors) is quite fast, since the back end is written in C.


Some basic examples using the package can be found  here , and the CRAN page is  here .
  
Indeed I don’t like point-null model comparisons . . . but maybe this will be useful to some of you!</p><p>6 0.73062998 <a title="419-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-24-Bleg%3A_Automatic_Differentiation_for_Log_Prob_Gradients%3F.html">535 andrew gelman stats-2011-01-24-Bleg: Automatic Differentiation for Log Prob Gradients?</a></p>
<p>7 0.73045039 <a title="419-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-30-Stan_uses_Nuts%21.html">1036 andrew gelman stats-2011-11-30-Stan uses Nuts!</a></p>
<p>8 0.68504614 <a title="419-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-09-Commercial_Bayesian_inference_software_is_popping_up_all_over.html">1489 andrew gelman stats-2012-09-09-Commercial Bayesian inference software is popping up all over</a></p>
<p>9 0.68076533 <a title="419-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-21-BDA3_table_of_contents_%28also_a_new_paper_on_visualization%29.html">1991 andrew gelman stats-2013-08-21-BDA3 table of contents (also a new paper on visualization)</a></p>
<p>10 0.67784888 <a title="419-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-04-A_new_R_package_for_fititng_multilevel_models.html">501 andrew gelman stats-2011-01-04-A new R package for fititng multilevel models</a></p>
<p>11 0.67360568 <a title="419-lsi-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Computer_models_of_the_oil_spill.html">243 andrew gelman stats-2010-08-30-Computer models of the oil spill</a></p>
<p>12 0.67012876 <a title="419-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-04-Bayesian_Learning_via_Stochastic_Gradient_Langevin_Dynamics.html">1443 andrew gelman stats-2012-08-04-Bayesian Learning via Stochastic Gradient Langevin Dynamics</a></p>
<p>13 0.66969347 <a title="419-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>14 0.66123414 <a title="419-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-12-Samplers_for_Big_Science%3A__emcee_and_BAT.html">2020 andrew gelman stats-2013-09-12-Samplers for Big Science:  emcee and BAT</a></p>
<p>15 0.64513302 <a title="419-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-30-Stan_Project%3A__Continuous_Relaxations_for_Discrete_MRFs.html">2003 andrew gelman stats-2013-08-30-Stan Project:  Continuous Relaxations for Discrete MRFs</a></p>
<p>16 0.63351703 <a title="419-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-21-Lessons_learned_from_a_recent_R_package_submission.html">1134 andrew gelman stats-2012-01-21-Lessons learned from a recent R package submission</a></p>
<p>17 0.63094759 <a title="419-lsi-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-Running_into_a_Stan_Reference_by_Accident.html">2231 andrew gelman stats-2014-03-03-Running into a Stan Reference by Accident</a></p>
<p>18 0.62089211 <a title="419-lsi-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-12-%E2%80%9CThe_results_%28not_shown%29_._._.%E2%80%9D.html">2332 andrew gelman stats-2014-05-12-“The results (not shown) . . .”</a></p>
<p>19 0.61992407 <a title="419-lsi-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-19-Just_chaid.html">421 andrew gelman stats-2010-11-19-Just chaid</a></p>
<p>20 0.61572057 <a title="419-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(4, 0.197), (6, 0.012), (16, 0.054), (21, 0.042), (22, 0.029), (23, 0.025), (24, 0.149), (36, 0.025), (57, 0.012), (61, 0.029), (73, 0.012), (82, 0.023), (99, 0.286)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9587602 <a title="419-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-08-GiveWell_sez%3A__Cost-effectiveness_of_de-worming_was_overstated_by_a_factor_of_100_%28%21%29_due_to_a_series_of_sloppy_calculations.html">947 andrew gelman stats-2011-10-08-GiveWell sez:  Cost-effectiveness of de-worming was overstated by a factor of 100 (!) due to a series of sloppy calculations</a></p>
<p>Introduction: Alexander at GiveWell  writes :
  
The Disease Control Priorities in Developing Countries (DCP2), a major report funded by the Gates Foundation . . . provides an estimate of $3.41 per disability-adjusted life-year (DALY) for the cost-effectiveness of soil-transmitted-helminth (STH) treatment, implying that STH treatment is one of the most cost-effective interventions for global health. In investigating this figure, we have corresponded, over a period of months, with six scholars who had been directly or indirectly involved in the production of the estimate. Eventually, we were able to obtain the spreadsheet that was used to generate the $3.41/DALY estimate. That spreadsheet contains five separate errors that, when corrected, shift the estimated cost effectiveness of deworming from $3.41 to $326.43. [I think they mean to say $300 -- ed.] We came to this conclusion a year after learning that the DCP2’s published cost-effectiveness estimate for schistosomiasis treatment – another kind of</p><p>2 0.95101309 <a title="419-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-11-The_consulting_biz.html">1618 andrew gelman stats-2012-12-11-The consulting biz</a></p>
<p>Introduction: I received the following (unsolicited) email:
  
Hello, 


*** LLC, a ***-based market research company, has a financial client who is interested in speaking with a statistician who has done research in the field of Alzheimer’s Disease and preferably familiar with the SOLA and BAPI trials.  We offer an honorarium of $200 for a 30 minute telephone interview.


Please advise us if you have an employment or consulting agreement with any organization or operate professionally pursuant to an organization’s code of conduct or employee manual that may control activities by you outside of your regular present and former employment, such as participating in this consulting project for MedPanel.  If there are such contracts or other documents that do apply to you, please forward MedPanel a copy of each such document asap as we are obligated to review such documents to determine if you are permitted to participate as a consultant for MedPanel on a project with this particular client.


If you are</p><p>3 0.94361174 <a title="419-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-13-Can_you_write_a_program_to_determine_the_causal_order%3F.html">1801 andrew gelman stats-2013-04-13-Can you write a program to determine the causal order?</a></p>
<p>Introduction: Mike Zyphur writes:
  
Kaggle.com has launched a  competition  to determine what’s an effect and what’s a cause. They’ve got correlated variables, they’re deprived of context, and you’re asked to determine the causal order.  $5,000 prizes.
  
I followed the link and the example they gave didn’t make much sense to me (the two variables were temperature and altitude of cities in Germany, and they said that altitude causes temperature).  It has the feeling to me of one of those weird standardized tests we used to see sometimes in school, where there’s no real correct answer so the goal is to figure out what the test-writer wanted you to say.
 
Nonetheless, this might be of interest, so I’m passing it along to you.</p><p>4 0.94165385 <a title="419-lda-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-29-R_sucks.html">1919 andrew gelman stats-2013-06-29-R sucks</a></p>
<p>Introduction: I was trying to make some new graphs using 5-year-old R code and I got all these problems because I was reading in files with variable names such as “co.fipsid” and now R is automatically changing them to “co_fipsid”.  Or maybe the names had underbars all along, and the old R had changed them into dots.  Whatever.  I understand that backward compatibility can be hard to maintain, but this is just annoying.</p><p>5 0.93186212 <a title="419-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-29-Going_negative.html">1918 andrew gelman stats-2013-06-29-Going negative</a></p>
<p>Introduction: Troels Ring writes: 
  
  
I have measured total phosphorus, TP, on a number of dialysis patients, and also measured conventional phosphate, Pi. Now P is exchanged with the environment as Pi, so in principle a correlation between TP and Pi could perhaps be expected. I’m really most interested in the fraction of TP which is not Pi, that is TP-Pi. I would also expect that to be positively correlated with Pi. However, looking at the data using a mixed model an insignificant negative correlation is obtained. Then I thought, that since TP-Pi is bound to be small if Pi is large a negative correlation is almost dictated by the math even if the biology would have it otherwise in so far as the the TP-Pi, likely organic P, must someday have been Pi. Hence I thought about correcting the slight negative correlation between TP-Pi and Pi for the expected large negative correlation due to the math – to eventually recover what I came from: a positive correlation. People seems to agree that this thinki</p><p>6 0.92376471 <a title="419-lda-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-27-No_radon_lobby.html">238 andrew gelman stats-2010-08-27-No radon lobby</a></p>
<p>same-blog 7 0.92156708 <a title="419-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-18-Derivative-based_MCMC_as_a_breakthrough_technique_for_implementing_Bayesian_statistics.html">419 andrew gelman stats-2010-11-18-Derivative-based MCMC as a breakthrough technique for implementing Bayesian statistics</a></p>
<p>8 0.91908681 <a title="419-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-14-Reproducibility_in_Practice.html">907 andrew gelman stats-2011-09-14-Reproducibility in Practice</a></p>
<p>9 0.9085443 <a title="419-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-28-Advocacy_in_the_form_of_a_%E2%80%9Cdeliberative_forum%E2%80%9D.html">113 andrew gelman stats-2010-06-28-Advocacy in the form of a “deliberative forum”</a></p>
<p>10 0.90321267 <a title="419-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-26-Graphs_showing_regression_uncertainty%3A__the_code%21.html">1470 andrew gelman stats-2012-08-26-Graphs showing regression uncertainty:  the code!</a></p>
<p>11 0.89921439 <a title="419-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-28-Plain_old_everyday_Bayesianism%21.html">1829 andrew gelman stats-2013-04-28-Plain old everyday Bayesianism!</a></p>
<p>12 0.89224732 <a title="419-lda-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-14-The_popularity_of_certain_baby_names_is_falling_off_the_clifffffffffffff.html">2211 andrew gelman stats-2014-02-14-The popularity of certain baby names is falling off the clifffffffffffff</a></p>
<p>13 0.88886887 <a title="419-lda-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-15-Mary%2C_Mary%2C_why_ya_buggin.html">2212 andrew gelman stats-2014-02-15-Mary, Mary, why ya buggin</a></p>
<p>14 0.8860966 <a title="419-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-26-%E2%80%9CThe_Bayesian_approach_to_forensic_evidence%E2%80%9D.html">2078 andrew gelman stats-2013-10-26-“The Bayesian approach to forensic evidence”</a></p>
<p>15 0.88475955 <a title="419-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-28-Why_during_the_1950-1960%E2%80%B2s_did_Jerry_Cornfield_become_a_Bayesian%3F.html">2000 andrew gelman stats-2013-08-28-Why during the 1950-1960′s did Jerry Cornfield become a Bayesian?</a></p>
<p>16 0.8758561 <a title="419-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-24-Measurement_error_in_monkey_studies.html">1997 andrew gelman stats-2013-08-24-Measurement error in monkey studies</a></p>
<p>17 0.86854303 <a title="419-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-04-Write_This_Book.html">1605 andrew gelman stats-2012-12-04-Write This Book</a></p>
<p>18 0.86763597 <a title="419-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-28-Value-added_assessment%3A__What_went_wrong%3F.html">1350 andrew gelman stats-2012-05-28-Value-added assessment:  What went wrong?</a></p>
<p>19 0.86444569 <a title="419-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-24-All_inference_is_about_generalizing_from_sample_to_population.html">1996 andrew gelman stats-2013-08-24-All inference is about generalizing from sample to population</a></p>
<p>20 0.85586107 <a title="419-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-02-Does_a_professor%E2%80%99s_intervention_in_online_discussions_have_the_effect_of_prolonging_discussion_or_cutting_it_off%3F.html">2120 andrew gelman stats-2013-12-02-Does a professor’s intervention in online discussions have the effect of prolonging discussion or cutting it off?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
