<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>301 andrew gelman stats-2010-09-28-Correlation, prediction, variation, etc.</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-301" href="#">andrew_gelman_stats-2010-301</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>301 andrew gelman stats-2010-09-28-Correlation, prediction, variation, etc.</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-301-html" href="http://andrewgelman.com/2010/09/28/correlation_pre/">html</a></p><p>Introduction: Hamdan Azhar writes:
  
 
I [Azhar] write with a question about language in the context of statistics. Consider the three statements below.


a) Y is significantly associated (correlated) with X;


b) knowledge of X allows us to account for __% of the variance in Y;


c) Y can be predicted to a significant extent given knowledge of X.


To what extent are these statements equivalent? Much of the (non-statistical) scientific literature doesn’t seem to distinguish between these notions. Is this just about semantics — or are there meaningful differences here, particularly between b and c?


Consider a framework where X constitutes a predictor space of p variables (x1,…,xp). We wish to generate a linear combination of these variables to yield a score that optimally correlates with Y. Can we substitute the word “predicts” for “optimally correlates with” in this context?


One can argue that “correlating” or “accounting for variance” suggests that we are trying to maximize goodness-of-fit (i</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Hamdan Azhar writes:      I [Azhar] write with a question about language in the context of statistics. [sent-1, score-0.253]
</p><p>2 a) Y is significantly associated (correlated) with X;   b) knowledge of X allows us to account for __% of the variance in Y;   c) Y can be predicted to a significant extent given knowledge of X. [sent-3, score-0.952]
</p><p>3 To what extent are these statements equivalent? [sent-4, score-0.339]
</p><p>4 Much of the (non-statistical) scientific literature doesn’t seem to distinguish between these notions. [sent-5, score-0.087]
</p><p>5 Is this just about semantics — or are there meaningful differences here, particularly between b and c? [sent-6, score-0.22]
</p><p>6 Consider a framework where X constitutes a predictor space of p variables (x1,…,xp). [sent-7, score-0.377]
</p><p>7 We wish to generate a linear combination of these variables to yield a score that optimally correlates with Y. [sent-8, score-1.112]
</p><p>8 Can we substitute the word “predicts” for “optimally correlates with” in this context? [sent-9, score-0.432]
</p><p>9 One can argue that “correlating” or “accounting for variance” suggests that we are trying to maximize goodness-of-fit (i. [sent-10, score-0.109]
</p><p>10 On the other hand, “prediction” implies that we engage in some form of cross-validation where we seek to minimize some measure of prediction error. [sent-13, score-0.588]
</p><p>11 Is it alright to substitute “prediction” for “accounting for variance”? [sent-15, score-0.211]
</p><p>12 Or are these distinct concepts that we should be careful not to conflate? [sent-16, score-0.191]
</p><p>13 My reply:  If interpreted generally enough, these statements are equivalent. [sent-17, score-0.3]
</p><p>14 “Correlation” refers to a linear relation, whereas “association” is more general. [sent-18, score-0.216]
</p><p>15 Similarly, you can get information without accounting for “variance,” but if you replace the term by “variation” then this might work. [sent-19, score-0.387]
</p><p>16 I don’t think you get anything useful out of worrying about these different expressions in general. [sent-20, score-0.217]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('accounting', 0.301), ('azhar', 0.274), ('variance', 0.263), ('optimally', 0.253), ('correlates', 0.221), ('statements', 0.213), ('substitute', 0.211), ('prediction', 0.21), ('language', 0.138), ('hamdan', 0.137), ('correlating', 0.131), ('semantics', 0.131), ('linear', 0.129), ('extent', 0.126), ('knowledge', 0.122), ('context', 0.115), ('constitutes', 0.114), ('minimize', 0.112), ('expressions', 0.11), ('maximize', 0.109), ('worrying', 0.107), ('variables', 0.106), ('distinct', 0.101), ('predicts', 0.095), ('seek', 0.093), ('significantly', 0.091), ('concepts', 0.09), ('engage', 0.09), ('consider', 0.089), ('meaningful', 0.089), ('refers', 0.087), ('interpreted', 0.087), ('yield', 0.087), ('distinguish', 0.087), ('replace', 0.086), ('focusing', 0.084), ('predictor', 0.083), ('generate', 0.083), ('implies', 0.083), ('relation', 0.081), ('wish', 0.081), ('allows', 0.078), ('predicted', 0.078), ('score', 0.077), ('combination', 0.075), ('equivalent', 0.075), ('correlated', 0.075), ('framework', 0.074), ('account', 0.072), ('association', 0.07)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="301-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-Correlation%2C_prediction%2C_variation%2C_etc..html">301 andrew gelman stats-2010-09-28-Correlation, prediction, variation, etc.</a></p>
<p>Introduction: Hamdan Azhar writes:
  
 
I [Azhar] write with a question about language in the context of statistics. Consider the three statements below.


a) Y is significantly associated (correlated) with X;


b) knowledge of X allows us to account for __% of the variance in Y;


c) Y can be predicted to a significant extent given knowledge of X.


To what extent are these statements equivalent? Much of the (non-statistical) scientific literature doesn’t seem to distinguish between these notions. Is this just about semantics — or are there meaningful differences here, particularly between b and c?


Consider a framework where X constitutes a predictor space of p variables (x1,…,xp). We wish to generate a linear combination of these variables to yield a score that optimally correlates with Y. Can we substitute the word “predicts” for “optimally correlates with” in this context?


One can argue that “correlating” or “accounting for variance” suggests that we are trying to maximize goodness-of-fit (i</p><p>2 0.17484698 <a title="301-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Adding_more_information_can_make_the_variance_go_up_%28depending_on_your_model%29.html">810 andrew gelman stats-2011-07-20-Adding more information can make the variance go up (depending on your model)</a></p>
<p>Introduction: Andy McKenzie writes:
  
In their March 9 “ counterpoint ” in nature biotech to the prospect that we should try to integrate more sources of data in clinical practice (see “ point ” arguing for this), Isaac Kohane and David Margulies claim that,


“Finally, how much better is our new knowledge than older knowledge? When is the incremental benefit of a genomic variant(s) or gene expression profile relative to a family history or classic histopathology insufficient and when does it add rather than subtract variance?”  


Perhaps I am mistaken (thus this email), but it seems that this claim runs contra to the definition of conditional probability. That is, if you have a hierarchical model, and the family history / classical histopathology already suggests a parameter estimate with some variance, how could the new genomic info possibly increase the variance of that parameter estimate? Surely the question is how much variance the new genomic info reduces and whether it therefore justifies t</p><p>3 0.14030479 <a title="301-tfidf-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-29-Brain_Structure_and_the_Big_Five.html">490 andrew gelman stats-2010-12-29-Brain Structure and the Big Five</a></p>
<p>Introduction: Many years ago, a research psychologist whose judgment I greatly respect told me that the characterization of personality by the so-called Big Five traits (extraversion, etc.) was old-fashioned.  So I’m always surprised to see that the Big Five keeps cropping up.  I guess not everyone agrees that it’s a bad idea.
 
For example, Hamdan Azhar wrote to me:
  
 
I was wondering if you’d seen  this recent paper  (De Young et al. 2010) that finds significant correlations between brain volume in selected regions and personality trait measures (from the Big Five). This is quite a ground-breaking finding and it was covered extensively in the mainstream media. I think readers of your blog would be interested in your thoughts, statistically speaking, on their methodology and findings.
 

 
My reply:   I’d  be interested in my thoughts on this too!  But I don’t know enough to say anything useful.
 
From the abstract of the paper under discussion:
  
Controlling for age, sex, and whole-brain volume</p><p>4 0.13863762 <a title="301-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-18-uuuuuuuuuuuuugly.html">1862 andrew gelman stats-2013-05-18-uuuuuuuuuuuuugly</a></p>
<p>Introduction: Hamdan Azhar writes:
  
I came across this graphic of vaccine-attributed decreases in mortality and was curious if you found it as unattractive and unintuitive as I did. Hope all is well with you!
  
My reply:  All’s well with me.  And yes, that’s one horrible graph.  It has all the problems with a bad infographic with none of the virtues.  Compared to this monstrosity, the typical USA Today graph is a stunning, beautiful masterpiece.  I don’t think I want to soil this webpage with the image.  In fact, I don’t even want to link to it.</p><p>5 0.12349215 <a title="301-tfidf-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-02-Discovering_general_multidimensional_associations.html">2315 andrew gelman stats-2014-05-02-Discovering general multidimensional associations</a></p>
<p>Introduction: Continuing our discussion of general measures of correlations, Ben Murrell sends along  this paper  (with  corresponding  R package), which begins:
  
When two variables are related by a known function, the coefficient of determination (denoted R-squared) measures the proportion of the total variance in the observations that is explained by that function. This quantifies the strength of the relationship between variables by describing what proportion of the variance is signal as opposed to noise. For linear relationships, this is equal to the square of the correlation coefficient, ρ. When the parametric form of the relationship is unknown, however, it is unclear how to estimate the proportion of explained variance equitably – assigning similar values to equally noisy relationships. Here we demonstrate how to directly estimate a generalized R-squared when the form of the relationship is unknown, and we question the performance of the Maximal Information Coefficient (MIC) – a recently pr</p><p>6 0.11606237 <a title="301-tfidf-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-02-The_problem_of_overestimation_of_group-level_variance_parameters.html">63 andrew gelman stats-2010-06-02-The problem of overestimation of group-level variance parameters</a></p>
<p>7 0.10985494 <a title="301-tfidf-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-19-Analysis_of_survey_data%3A_Design_based_models_vs._hierarchical_modeling%3F.html">352 andrew gelman stats-2010-10-19-Analysis of survey data: Design based models vs. hierarchical modeling?</a></p>
<p>8 0.10142849 <a title="301-tfidf-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-12-Finite-population_standard_deviation_in_a_hierarchical_model.html">464 andrew gelman stats-2010-12-12-Finite-population standard deviation in a hierarchical model</a></p>
<p>9 0.098277748 <a title="301-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-Piss-poor_monocausal_social_science.html">1196 andrew gelman stats-2012-03-04-Piss-poor monocausal social science</a></p>
<p>10 0.095188588 <a title="301-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-25-Correlation_of_1_._._._too_good_to_be_true%3F.html">1737 andrew gelman stats-2013-02-25-Correlation of 1 . . . too good to be true?</a></p>
<p>11 0.093451582 <a title="301-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-10-95%25_intervals_that_I_don%E2%80%99t_believe%2C_because_they%E2%80%99re_from_a_flat_prior_I_don%E2%80%99t_believe.html">1206 andrew gelman stats-2012-03-10-95% intervals that I don’t believe, because they’re from a flat prior I don’t believe</a></p>
<p>12 0.092314132 <a title="301-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-04-Question_25_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1365 andrew gelman stats-2012-06-04-Question 25 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>13 0.091956653 <a title="301-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>14 0.08923097 <a title="301-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>15 0.086630382 <a title="301-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>16 0.08467792 <a title="301-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-06-17_groups%2C_6_group-level_predictors%3A__What_to_do%3F.html">1248 andrew gelman stats-2012-04-06-17 groups, 6 group-level predictors:  What to do?</a></p>
<p>17 0.083622605 <a title="301-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-04-What_are_the_key_assumptions_of_linear_regression%3F.html">1967 andrew gelman stats-2013-08-04-What are the key assumptions of linear regression?</a></p>
<p>18 0.083550893 <a title="301-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-22-The_kluges_of_today_are_the_textbook_solutions_of_tomorrow..html">2143 andrew gelman stats-2013-12-22-The kluges of today are the textbook solutions of tomorrow.</a></p>
<p>19 0.080591701 <a title="301-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-24-Estimating_and_summarizing_inference_for_hierarchical_variance_parameters_when_the_number_of_groups_is_small.html">2145 andrew gelman stats-2013-12-24-Estimating and summarizing inference for hierarchical variance parameters when the number of groups is small</a></p>
<p>20 0.079202183 <a title="301-tfidf-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-05-What_do_practitioners_need_to_know_about_regression%3F.html">451 andrew gelman stats-2010-12-05-What do practitioners need to know about regression?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.123), (1, 0.053), (2, 0.055), (3, -0.028), (4, 0.025), (5, -0.009), (6, 0.022), (7, -0.02), (8, 0.019), (9, 0.072), (10, 0.003), (11, 0.016), (12, 0.021), (13, -0.001), (14, -0.001), (15, 0.011), (16, -0.015), (17, -0.007), (18, 0.01), (19, 0.015), (20, 0.009), (21, -0.01), (22, 0.037), (23, 0.015), (24, 0.036), (25, 0.023), (26, 0.044), (27, 0.031), (28, -0.004), (29, -0.017), (30, 0.051), (31, 0.042), (32, 0.003), (33, -0.02), (34, 0.035), (35, 0.015), (36, 0.028), (37, -0.021), (38, 0.013), (39, -0.037), (40, 0.021), (41, -0.056), (42, 0.042), (43, 0.052), (44, -0.04), (45, -0.019), (46, 0.024), (47, -0.0), (48, 0.009), (49, -0.012)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98018003 <a title="301-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-Correlation%2C_prediction%2C_variation%2C_etc..html">301 andrew gelman stats-2010-09-28-Correlation, prediction, variation, etc.</a></p>
<p>Introduction: Hamdan Azhar writes:
  
 
I [Azhar] write with a question about language in the context of statistics. Consider the three statements below.


a) Y is significantly associated (correlated) with X;


b) knowledge of X allows us to account for __% of the variance in Y;


c) Y can be predicted to a significant extent given knowledge of X.


To what extent are these statements equivalent? Much of the (non-statistical) scientific literature doesn’t seem to distinguish between these notions. Is this just about semantics — or are there meaningful differences here, particularly between b and c?


Consider a framework where X constitutes a predictor space of p variables (x1,…,xp). We wish to generate a linear combination of these variables to yield a score that optimally correlates with Y. Can we substitute the word “predicts” for “optimally correlates with” in this context?


One can argue that “correlating” or “accounting for variance” suggests that we are trying to maximize goodness-of-fit (i</p><p>2 0.77836132 <a title="301-lsi-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-02-Discovering_general_multidimensional_associations.html">2315 andrew gelman stats-2014-05-02-Discovering general multidimensional associations</a></p>
<p>Introduction: Continuing our discussion of general measures of correlations, Ben Murrell sends along  this paper  (with  corresponding  R package), which begins:
  
When two variables are related by a known function, the coefficient of determination (denoted R-squared) measures the proportion of the total variance in the observations that is explained by that function. This quantifies the strength of the relationship between variables by describing what proportion of the variance is signal as opposed to noise. For linear relationships, this is equal to the square of the correlation coefficient, ρ. When the parametric form of the relationship is unknown, however, it is unclear how to estimate the proportion of explained variance equitably – assigning similar values to equally noisy relationships. Here we demonstrate how to directly estimate a generalized R-squared when the form of the relationship is unknown, and we question the performance of the Maximal Information Coefficient (MIC) – a recently pr</p><p>3 0.71641725 <a title="301-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Question_about_standard_range_for_social_science_correlations.html">257 andrew gelman stats-2010-09-04-Question about standard range for social science correlations</a></p>
<p>Introduction: Andrew Eppig writes:
  
I’m a physicist by training who is transitioning to the social sciences. I recently came across a  reference  in the Economist to a paper on IQ and parasites which I read as I have more than a passing interest in IQ research (having read much that you and others (e.g., Shalizi, Wicherts) have written). In this paper I note that the authors find a very high correlation between national IQ and parasite prevalence. The strength of the correlation (-0.76 to -0.82) surprised me, as I’m used to much weaker correlations in the social sciences. To me, it’s a bit too high, suggesting that there are other factors at play or that one of the variables is merely a proxy for a large number of other variables. But I have no basis for this other than a gut feeling and a memory of a plot on  Language Log  about the distribution of correlation coefficients in social psychology.


So my question is this: Is a correlation in the range of (-0.82,-0.76) more likely to be a correlatio</p><p>4 0.70383 <a title="301-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Adding_more_information_can_make_the_variance_go_up_%28depending_on_your_model%29.html">810 andrew gelman stats-2011-07-20-Adding more information can make the variance go up (depending on your model)</a></p>
<p>Introduction: Andy McKenzie writes:
  
In their March 9 “ counterpoint ” in nature biotech to the prospect that we should try to integrate more sources of data in clinical practice (see “ point ” arguing for this), Isaac Kohane and David Margulies claim that,


“Finally, how much better is our new knowledge than older knowledge? When is the incremental benefit of a genomic variant(s) or gene expression profile relative to a family history or classic histopathology insufficient and when does it add rather than subtract variance?”  


Perhaps I am mistaken (thus this email), but it seems that this claim runs contra to the definition of conditional probability. That is, if you have a hierarchical model, and the family history / classical histopathology already suggests a parameter estimate with some variance, how could the new genomic info possibly increase the variance of that parameter estimate? Surely the question is how much variance the new genomic info reduces and whether it therefore justifies t</p><p>5 0.68889832 <a title="301-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-03-Uncertainty_in_parameter_estimates_using_multilevel_models.html">1966 andrew gelman stats-2013-08-03-Uncertainty in parameter estimates using multilevel models</a></p>
<p>Introduction: David Hsu writes: 
   
 I have a (perhaps) simple question about uncertainty in parameter estimates using multilevel models — what is an appropriate threshold for measure parameter uncertainty in a multilevel model? 
 
The reason why I ask is that I set out to do a crossed two-way model with two varying intercepts, similar to your flight simulator example in your 2007 book.  The difference is that I have a lot of predictors specific to each cell (I think equivalent to airport and pilot in your example), and I find after modeling this in JAGS, I happily find that the predictors are much less important than the variability by cell (airport and pilot effects).  Happily because this is what I am writing a paper about.
 
However, I then went to check subsets of predictors using lm() and lmer().  I understand that they all use different estimation methods, but what I can’t figure out is why the errors on all of the coefficient estimates are *so* different.  
 
For example, using JAGS, and th</p><p>6 0.68653804 <a title="301-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-29-Going_negative.html">1918 andrew gelman stats-2013-06-29-Going negative</a></p>
<p>7 0.68642539 <a title="301-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-21-Finite-population_Anova_calculations_for_models_with_interactions.html">1686 andrew gelman stats-2013-01-21-Finite-population Anova calculations for models with interactions</a></p>
<p>8 0.67604703 <a title="301-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>9 0.67260826 <a title="301-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Allowing_interaction_terms_to_vary.html">753 andrew gelman stats-2011-06-09-Allowing interaction terms to vary</a></p>
<p>10 0.6576038 <a title="301-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-24-Estimating_and_summarizing_inference_for_hierarchical_variance_parameters_when_the_number_of_groups_is_small.html">2145 andrew gelman stats-2013-12-24-Estimating and summarizing inference for hierarchical variance parameters when the number of groups is small</a></p>
<p>11 0.6482451 <a title="301-lsi-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-%E2%80%9CGenomics%E2%80%9D_vs._genetics.html">303 andrew gelman stats-2010-09-28-“Genomics” vs. genetics</a></p>
<p>12 0.64776635 <a title="301-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Interpreting_interactions_in_discrete-data_regression.html">1908 andrew gelman stats-2013-06-21-Interpreting interactions in discrete-data regression</a></p>
<p>13 0.64164901 <a title="301-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-15-R-squared_for_multilevel_models.html">1121 andrew gelman stats-2012-01-15-R-squared for multilevel models</a></p>
<p>14 0.63978493 <a title="301-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-18-Standardizing_regression_inputs.html">1462 andrew gelman stats-2012-08-18-Standardizing regression inputs</a></p>
<p>15 0.63834608 <a title="301-lsi-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-30-Adjudicating_between_alternative_interpretations_of_a_statistical_interaction%3F.html">2274 andrew gelman stats-2014-03-30-Adjudicating between alternative interpretations of a statistical interaction?</a></p>
<p>16 0.63745701 <a title="301-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-04-What_are_the_key_assumptions_of_linear_regression%3F.html">1967 andrew gelman stats-2013-08-04-What are the key assumptions of linear regression?</a></p>
<p>17 0.63554668 <a title="301-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-02-Interaction-based_feature_selection_and_classification_for_high-dimensional_biological_data.html">1703 andrew gelman stats-2013-02-02-Interaction-based feature selection and classification for high-dimensional biological data</a></p>
<p>18 0.63368034 <a title="301-lsi-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-09-Keli_Liu_and_Xiao-Li_Meng_on_Simpson%E2%80%99s_paradox.html">2204 andrew gelman stats-2014-02-09-Keli Liu and Xiao-Li Meng on Simpson’s paradox</a></p>
<p>19 0.63356024 <a title="301-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-09-The_effects_of_fiscal_consolidation.html">1663 andrew gelman stats-2013-01-09-The effects of fiscal consolidation</a></p>
<p>20 0.6324271 <a title="301-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-26-Further_thoughts_on_nonparametric_correlation_measures.html">1230 andrew gelman stats-2012-03-26-Further thoughts on nonparametric correlation measures</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.018), (6, 0.013), (7, 0.024), (16, 0.074), (17, 0.013), (19, 0.025), (21, 0.019), (24, 0.186), (42, 0.012), (43, 0.021), (53, 0.013), (60, 0.015), (74, 0.055), (86, 0.075), (95, 0.097), (97, 0.02), (99, 0.229)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97695351 <a title="301-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-Correlation%2C_prediction%2C_variation%2C_etc..html">301 andrew gelman stats-2010-09-28-Correlation, prediction, variation, etc.</a></p>
<p>Introduction: Hamdan Azhar writes:
  
 
I [Azhar] write with a question about language in the context of statistics. Consider the three statements below.


a) Y is significantly associated (correlated) with X;


b) knowledge of X allows us to account for __% of the variance in Y;


c) Y can be predicted to a significant extent given knowledge of X.


To what extent are these statements equivalent? Much of the (non-statistical) scientific literature doesn’t seem to distinguish between these notions. Is this just about semantics — or are there meaningful differences here, particularly between b and c?


Consider a framework where X constitutes a predictor space of p variables (x1,…,xp). We wish to generate a linear combination of these variables to yield a score that optimally correlates with Y. Can we substitute the word “predicts” for “optimally correlates with” in this context?


One can argue that “correlating” or “accounting for variance” suggests that we are trying to maximize goodness-of-fit (i</p><p>2 0.94158673 <a title="301-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-09-The_future_of_R.html">266 andrew gelman stats-2010-09-09-The future of R</a></p>
<p>Introduction: Some thoughts from Christian , including this bit:
  
We need to consider separately


1. R’s brilliant library


2. R’s not-so-brilliant language and/or interpreter.
  
I don’t know that R’s library is so brilliant as all that–if necessary, I don’t think it would be hard to reprogram the important packages in a new language.
 
I would say, though, that the problems with R are not just in the technical details of the language.  I think the culture of R has some problems too.  As I’ve written before, R functions used to be lean and mean, and now they’re full of exception-handling and calls to other packages.  R functions are spaghetti-like messes of connections in which I keep expecting to run into syntax like “GOTO 120.”
 
I learned about these problems a couple years ago when writing bayesglm(), which is a simple adaptation of glm().  But glm(), and its workhorse, glm.fit(), are a mess:  They’re about 10 lines of functioning code, plus about 20 lines of necessary front-end, plus a cou</p><p>3 0.92840457 <a title="301-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-15-The_UN_Plot_to_Force_Bayesianism_on_Unsuspecting_Americans_%28penalized_B-Spline_edition%29.html">2135 andrew gelman stats-2013-12-15-The UN Plot to Force Bayesianism on Unsuspecting Americans (penalized B-Spline edition)</a></p>
<p>Introduction: Mike Spagat sent me an email with the above heading, referring to  this paper  by Leontine Alkema and Jin Rou New, which begins:
  
National estimates of the under-5 mortality rate (U5MR) are used to track progress in reducing child mortality and to evaluate countries’ performance related to United Nations Millennium Development Goal 4, which calls for a reduction in the U5MR by two-thirds between 1990 and 2015. However, for the great majority of developing countries without well-functioning vital registration systems, estimating levels and trends in child mortality is challenging, not only because of limited data availability but also because of issues with data quality. Global U5MR estimates are often constructed without accounting for potential biases in data series, which may lead to inaccurate point estimates and/or credible intervals.


We describe a Bayesian penalized B-spline regression model for assessing levels and trends in the U5MR for all countries in the world, whereby bi</p><p>4 0.92421263 <a title="301-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-03-Two_interesting_posts_elsewhere_on_graphics.html">599 andrew gelman stats-2011-03-03-Two interesting posts elsewhere on graphics</a></p>
<p>Introduction: Have data graphics progressed in the last century?  The first addresses  familiar   subjects  to readers of the blog, with some nice examples of where infographics emphasize the obvious, or increase the probability of an incorrect insight.   Your Help Needed: the Effect of Aesthetics on Visualization  I borrow the term ‘insight’ from the second link, a study by a group of design & software researchers based around a single interactive graphic. This is similar in spirit to Unwin’s  ‘caption this graphic’  assignment.</p><p>5 0.92132926 <a title="301-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-29-Infovis_vs._statgraphics%3A__A_clear_example_of_their_different_goals.html">829 andrew gelman stats-2011-07-29-Infovis vs. statgraphics:  A clear example of their different goals</a></p>
<p>Introduction: I recently came across a data visualization that perfectly demonstrates the difference between the “infovis” and “statgraphics” perspectives.
 
 Here’s  the image ( link  from Tyler Cowen):
 
   
 
That’s the infovis. The statgraphic version would simply be a dotplot, something like this:
 
   
 
(I purposely used the default settings in R with only minor modifications here to demonstrate what happens if you just want to plot the data with minimal effort.)
 
Let’s compare the two graphs:
 
From a  statistical graphics  perspective, the second graph dominates.  The countries are directly comparable and the numbers are indicated by positions rather than area.  The first graph is full of distracting color and gives the misleading visual impression that the total GDP of countries 5-10 is about equal to that of countries 1-4.
 
If the goal is to  get attention , though, it’s another story.  There’s nothing special about the top graph above  except  how it looks.  It represents neither a dat</p><p>6 0.92070872 <a title="301-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-30-Bill_Gates%E2%80%99s_favorite_graph_of_the_year.html">2154 andrew gelman stats-2013-12-30-Bill Gates’s favorite graph of the year</a></p>
<p>7 0.92061365 <a title="301-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-09-%E2%80%9CMuch_of_the_recent_reported_drop_in_interstate_migration_is_a_statistical_artifact%E2%80%9D.html">404 andrew gelman stats-2010-11-09-“Much of the recent reported drop in interstate migration is a statistical artifact”</a></p>
<p>8 0.91998649 <a title="301-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-12-Thinking_like_a_statistician_%28continuously%29_rather_than_like_a_civilian_%28discretely%29.html">1575 andrew gelman stats-2012-11-12-Thinking like a statistician (continuously) rather than like a civilian (discretely)</a></p>
<p>9 0.91995633 <a title="301-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-25-Correlation_of_1_._._._too_good_to_be_true%3F.html">1737 andrew gelman stats-2013-02-25-Correlation of 1 . . . too good to be true?</a></p>
<p>10 0.91937351 <a title="301-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-31-Bayes%3A_radical%2C_liberal%2C_or_conservative%3F.html">639 andrew gelman stats-2011-03-31-Bayes: radical, liberal, or conservative?</a></p>
<p>11 0.91865909 <a title="301-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-30-Don%E2%80%99t_stop_being_a_statistician_once_the_analysis_is_done.html">783 andrew gelman stats-2011-06-30-Don’t stop being a statistician once the analysis is done</a></p>
<p>12 0.91772282 <a title="301-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>13 0.91749394 <a title="301-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-05-Question_26_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1367 andrew gelman stats-2012-06-05-Question 26 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>14 0.9168337 <a title="301-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-01-A_graph_at_war_with_its_caption.__Also%2C_how_to_visualize_the_same_numbers_without_giving_the_display_a_misleading_causal_feel%3F.html">1834 andrew gelman stats-2013-05-01-A graph at war with its caption.  Also, how to visualize the same numbers without giving the display a misleading causal feel?</a></p>
<p>15 0.91669714 <a title="301-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-The_statistical_significance_filter.html">899 andrew gelman stats-2011-09-10-The statistical significance filter</a></p>
<p>16 0.91619432 <a title="301-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-08-The_Case_for_More_False_Positives_in_Anti-doping_Testing.html">1612 andrew gelman stats-2012-12-08-The Case for More False Positives in Anti-doping Testing</a></p>
<p>17 0.91606808 <a title="301-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-20-Likelihood_thresholds_and_decisions.html">1422 andrew gelman stats-2012-07-20-Likelihood thresholds and decisions</a></p>
<p>18 0.91539878 <a title="301-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-02-Blogads_update.html">1240 andrew gelman stats-2012-04-02-Blogads update</a></p>
<p>19 0.91456246 <a title="301-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-13-Help_with_this_problem%2C_win_valuable_prizes.html">1164 andrew gelman stats-2012-02-13-Help with this problem, win valuable prizes</a></p>
<p>20 0.91207248 <a title="301-lda-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-31-Type_S_error_rates_for_classical_and_Bayesian_single_and_multiple_comparison_procedures.html">494 andrew gelman stats-2010-12-31-Type S error rates for classical and Bayesian single and multiple comparison procedures</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
