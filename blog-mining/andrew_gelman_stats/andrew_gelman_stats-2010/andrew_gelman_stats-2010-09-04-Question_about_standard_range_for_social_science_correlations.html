<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>257 andrew gelman stats-2010-09-04-Question about standard range for social science correlations</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-257" href="#">andrew_gelman_stats-2010-257</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>257 andrew gelman stats-2010-09-04-Question about standard range for social science correlations</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-257-html" href="http://andrewgelman.com/2010/09/04/question_about_6/">html</a></p><p>Introduction: Andrew Eppig writes:
  
I’m a physicist by training who is transitioning to the social sciences. I recently came across a  reference  in the Economist to a paper on IQ and parasites which I read as I have more than a passing interest in IQ research (having read much that you and others (e.g., Shalizi, Wicherts) have written). In this paper I note that the authors find a very high correlation between national IQ and parasite prevalence. The strength of the correlation (-0.76 to -0.82) surprised me, as I’m used to much weaker correlations in the social sciences. To me, it’s a bit too high, suggesting that there are other factors at play or that one of the variables is merely a proxy for a large number of other variables. But I have no basis for this other than a gut feeling and a memory of a plot on  Language Log  about the distribution of correlation coefficients in social psychology.


So my question is this: Is a correlation in the range of (-0.82,-0.76) more likely to be a correlatio</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Andrew Eppig writes:    I’m a physicist by training who is transitioning to the social sciences. [sent-1, score-0.251]
</p><p>2 I recently came across a  reference  in the Economist to a paper on IQ and parasites which I read as I have more than a passing interest in IQ research (having read much that you and others (e. [sent-2, score-0.23]
</p><p>3 In this paper I note that the authors find a very high correlation between national IQ and parasite prevalence. [sent-5, score-0.334]
</p><p>4 82) surprised me, as I’m used to much weaker correlations in the social sciences. [sent-8, score-0.328]
</p><p>5 To me, it’s a bit too high, suggesting that there are other factors at play or that one of the variables is merely a proxy for a large number of other variables. [sent-9, score-0.397]
</p><p>6 But I have no basis for this other than a gut feeling and a memory of a plot on  Language Log  about the distribution of correlation coefficients in social psychology. [sent-10, score-0.605]
</p><p>7 So my question is this: Is a correlation in the range of (-0. [sent-11, score-0.316]
</p><p>8 76) more likely to be a correlation between two variables with no deeper relationship or indicative of a missing set of underlying variables? [sent-13, score-0.714]
</p><p>9 My reply:       First off, I don’t think you can ever distinguish between correlations of . [sent-14, score-0.226]
</p><p>10 I don’t think you can treat the high correlations as evidence  against  their argument. [sent-19, score-0.256]
</p><p>11 Finally, are you related to the first author of the linked article, or is it just that you did a search on Eppig and encountered this stuff? [sent-21, score-0.305]
</p><p>12 Eppig responds:     I am in fact related to the first author of the study — he’s my brother. [sent-24, score-0.245]
</p><p>13 Since my first question, I’ve been wondering about how to interpret the results of a regression when some of the dependent variables have been imputed via regression. [sent-25, score-0.666]
</p><p>14 + xn)   where x1 has had its missing values imputed using:   fit. [sent-30, score-0.402]
</p><p>15 + xn)   Are there extra considerations required in interpreting the model fit. [sent-34, score-0.226]
</p><p>16 Can one read off the coefficient values and errors from fit. [sent-36, score-0.289]
</p><p>17 Naively, I feel that the errors in xn are now correlated with the other independent variables and a simple linear regression is no longer appropriate/valid. [sent-40, score-0.826]
</p><p>18 Are the coefficients of x1, x2,…, xn valid but the errors invalid? [sent-41, score-0.639]
</p><p>19 In general you want to fit both models together, or, in general, to model all the variables jointly. [sent-43, score-0.306]
</p><p>20 That said, in practice I’ll typically just take the imputed x-values as exact and not think too hard about it. [sent-44, score-0.23]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('xn', 0.411), ('eppig', 0.338), ('correlation', 0.243), ('iq', 0.234), ('imputed', 0.23), ('variables', 0.225), ('wicherts', 0.174), ('lm', 0.169), ('correlations', 0.165), ('errors', 0.124), ('coefficients', 0.104), ('gut', 0.103), ('indicative', 0.097), ('invalid', 0.097), ('transitioning', 0.097), ('factors', 0.093), ('high', 0.091), ('social', 0.088), ('author', 0.088), ('missing', 0.087), ('values', 0.085), ('simultaneous', 0.085), ('model', 0.081), ('first', 0.08), ('read', 0.08), ('proxy', 0.079), ('knowledgeable', 0.079), ('related', 0.077), ('considerations', 0.077), ('naively', 0.076), ('weaker', 0.075), ('question', 0.073), ('equations', 0.07), ('passing', 0.07), ('responds', 0.069), ('interpreting', 0.068), ('strength', 0.067), ('memory', 0.067), ('regression', 0.066), ('physicist', 0.066), ('dependent', 0.065), ('imputation', 0.064), ('shalizi', 0.063), ('deeper', 0.062), ('hearing', 0.062), ('assess', 0.062), ('distinguish', 0.061), ('log', 0.06), ('encountered', 0.06), ('hypotheses', 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="257-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Question_about_standard_range_for_social_science_correlations.html">257 andrew gelman stats-2010-09-04-Question about standard range for social science correlations</a></p>
<p>Introduction: Andrew Eppig writes:
  
I’m a physicist by training who is transitioning to the social sciences. I recently came across a  reference  in the Economist to a paper on IQ and parasites which I read as I have more than a passing interest in IQ research (having read much that you and others (e.g., Shalizi, Wicherts) have written). In this paper I note that the authors find a very high correlation between national IQ and parasite prevalence. The strength of the correlation (-0.76 to -0.82) surprised me, as I’m used to much weaker correlations in the social sciences. To me, it’s a bit too high, suggesting that there are other factors at play or that one of the variables is merely a proxy for a large number of other variables. But I have no basis for this other than a gut feeling and a memory of a plot on  Language Log  about the distribution of correlation coefficients in social psychology.


So my question is this: Is a correlation in the range of (-0.82,-0.76) more likely to be a correlatio</p><p>2 0.21391922 <a title="257-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-01-When_should_you_worry_about_imputed_data%3F.html">935 andrew gelman stats-2011-10-01-When should you worry about imputed data?</a></p>
<p>Introduction: Majid Ezzati writes:
  
My research group is increasingly focusing on a series of problems that involve data that either have missingness or measurements that may have bias/error.  We have at times developed our own approaches to imputation (as simple as interpolating a missing unit and as sophisticated as a problem-specific Bayesian hierarchical model) and at other times, other groups impute the data. 


The outputs are being used to investigate the basic associations between pairs of variables, Xs and Ys, in regressions; we may or may not interpret these as causal.  I am contacting colleagues with relevant expertise to suggest good references on whether having imputed X and/or Y in a subsequent regression is correct or if it could somehow lead to biased/spurious associations.   Thinking about this, we can have at least the following situations (these could all be Bayesian or not):


1)  X and Y both measured (perhaps with error) 
2)  Y imputed using some data and a model and X measur</p><p>3 0.19675247 <a title="257-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-13-Hypothesis_testing_with_multiple_imputations.html">799 andrew gelman stats-2011-07-13-Hypothesis testing with multiple imputations</a></p>
<p>Introduction: Vincent Yip writes:
  
I have read  your paper  [with Kobi Abayomi and Marc Levy] regarding multiple imputation application.


In order to diagnostic my imputed data, I used Kolmogorov-Smirnov (K-S) tests to compare the distribution differences between the imputed and observed values of a single attribute as mentioned in your paper. My question is:


For example I have this attribute X with the following data:  (NA = missing)


Original dataset: 1, NA, 3, 4, 1, 5, NA


Imputed dataset: 1, 2  , 3, 4, 1, 5, 6


a) in order to run the KS test, will I treat the observed data as 1, 3, 4,1, 5?


b) and for the observed data, will I treat 1, 2  , 3, 4, 1, 5, 6 as the imputed dataset for the K-S test? or just 2 ,6?


c) if I used m=5, I will have 5 set of imputed data sets. How would I apply K-S test to 5 of them and compare to the single observed distribution? Do I combine the 5 imputed data set into one by averaging each imputed values so I get one single imputed data and compare with the ob</p><p>4 0.17459713 <a title="257-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-06-Poverty%2C_educational_performance_%E2%80%93_and_can_be_done_about_it.html">561 andrew gelman stats-2011-02-06-Poverty, educational performance – and can be done about it</a></p>
<p>Introduction: Andrew has pointed to Jonathan Livengood’s  analysis  of the correlation between poverty and PISA results, whereby schools with poorer students get poorer test results. I’d have written a comment, but then I couldn’t have inserted a chart.
 
Andrew points out that a causal analysis is needed. This reminds me of an intervention that has been done before: take a child out of poverty, and bring him up in a better-off family. What’s going to happen? There have been several studies examining correlations between adoptive and biological parents’ IQ (assuming IQ is a test analogous to the math and verbal tests, and that parent IQ is analogous to the quality of instruction – but the point is in the analysis not in the metric). This is the result (from  Adoption Strategies  by Robin P Corley in Encyclopedia of Life Sciences):
 
 
 
So, while it did make a difference at an early age, with increasing age of the adopted child, the intelligence of adoptive parents might not be making any difference</p><p>5 0.15990552 <a title="257-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-22-Struggles_over_the_criticism_of_the_%E2%80%9Ccannabis_users_and_IQ_change%E2%80%9D_paper.html">1910 andrew gelman stats-2013-06-22-Struggles over the criticism of the “cannabis users and IQ change” paper</a></p>
<p>Introduction: Ole Rogeberg points me to a discussion of a discussion of a paper:
  
Did pre-release of my [Rogeberg's] PNAS  paper  on methodological problems with Meier et al’s 2012 paper on cannabis and IQ reduce the chances that it will have its intended effect? In my case, serious methodological issues related to causal inference from non-random observational data became framed as a conflict over conclusions, forcing the original research team to respond rapidly and insufficiently to my concerns, and prompting them to defend their conclusions and original paper in a way that makes a later, more comprehensive reanalysis of their data less likely.
  
This fits with a recurring theme on this blog: the defensiveness of researchers who don’t want to admit they were wrong. Setting aside cases of outright fraud and plagiarism, I think the worst case remains that of psychologists Neil Anderson and Deniz Ones, who denied any problems even in the  presence  of a smoking gun of a graph revealing their data</p><p>6 0.13386521 <a title="257-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-19-Cross-validation_to_check_missing-data_imputation.html">1330 andrew gelman stats-2012-05-19-Cross-validation to check missing-data imputation</a></p>
<p>7 0.13004081 <a title="257-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>8 0.1290292 <a title="257-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-03-Uncertainty_in_parameter_estimates_using_multilevel_models.html">1966 andrew gelman stats-2013-08-03-Uncertainty in parameter estimates using multilevel models</a></p>
<p>9 0.12814157 <a title="257-tfidf-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-03-He_doesn%E2%80%99t_trust_the_fit_._._._r%3D.999.html">315 andrew gelman stats-2010-10-03-He doesn’t trust the fit . . . r=.999</a></p>
<p>10 0.12757735 <a title="257-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-15-Exploratory_multilevel_analysis_when_group-level_variables_are_of_importance.html">1900 andrew gelman stats-2013-06-15-Exploratory multilevel analysis when group-level variables are of importance</a></p>
<p>11 0.12417243 <a title="257-tfidf-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-05-What_do_practitioners_need_to_know_about_regression%3F.html">451 andrew gelman stats-2010-12-05-What do practitioners need to know about regression?</a></p>
<p>12 0.12251225 <a title="257-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-05-Understanding_regression_models_and_regression_coefficients.html">1656 andrew gelman stats-2013-01-05-Understanding regression models and regression coefficients</a></p>
<p>13 0.11935133 <a title="257-tfidf-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-02-Discovering_general_multidimensional_associations.html">2315 andrew gelman stats-2014-05-02-Discovering general multidimensional associations</a></p>
<p>14 0.1164207 <a title="257-tfidf-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-08-Regression_and_causality_and_variable_ordering.html">2364 andrew gelman stats-2014-06-08-Regression and causality and variable ordering</a></p>
<p>15 0.11396313 <a title="257-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-18-Check_your_missing-data_imputations_using_cross-validation.html">1218 andrew gelman stats-2012-03-18-Check your missing-data imputations using cross-validation</a></p>
<p>16 0.1124558 <a title="257-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>17 0.11164731 <a title="257-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-07-Prior_distributions_for_regression_coefficients.html">1486 andrew gelman stats-2012-09-07-Prior distributions for regression coefficients</a></p>
<p>18 0.10957573 <a title="257-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-10-Multiple_imputation_and_multilevel_analysis.html">704 andrew gelman stats-2011-05-10-Multiple imputation and multilevel analysis</a></p>
<p>19 0.10941154 <a title="257-tfidf-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-21-Random_matrices_in_the_news.html">2258 andrew gelman stats-2014-03-21-Random matrices in the news</a></p>
<p>20 0.10679671 <a title="257-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-12-%E2%80%9CTeaching_effectiveness%E2%80%9D_as_another_dimension_in_cognitive_ability.html">1620 andrew gelman stats-2012-12-12-“Teaching effectiveness” as another dimension in cognitive ability</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.176), (1, 0.063), (2, 0.057), (3, -0.049), (4, 0.054), (5, 0.022), (6, 0.02), (7, -0.049), (8, 0.076), (9, 0.096), (10, 0.023), (11, 0.04), (12, 0.003), (13, -0.012), (14, 0.017), (15, 0.034), (16, 0.03), (17, 0.015), (18, 0.005), (19, -0.016), (20, 0.01), (21, 0.022), (22, 0.023), (23, -0.059), (24, 0.038), (25, 0.011), (26, 0.034), (27, -0.043), (28, 0.011), (29, -0.014), (30, 0.081), (31, 0.044), (32, 0.063), (33, 0.046), (34, 0.002), (35, -0.023), (36, 0.084), (37, 0.038), (38, 0.023), (39, -0.042), (40, -0.015), (41, -0.05), (42, 0.072), (43, 0.011), (44, 0.004), (45, -0.015), (46, 0.01), (47, -0.025), (48, 0.061), (49, 0.006)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96342027 <a title="257-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Question_about_standard_range_for_social_science_correlations.html">257 andrew gelman stats-2010-09-04-Question about standard range for social science correlations</a></p>
<p>Introduction: Andrew Eppig writes:
  
I’m a physicist by training who is transitioning to the social sciences. I recently came across a  reference  in the Economist to a paper on IQ and parasites which I read as I have more than a passing interest in IQ research (having read much that you and others (e.g., Shalizi, Wicherts) have written). In this paper I note that the authors find a very high correlation between national IQ and parasite prevalence. The strength of the correlation (-0.76 to -0.82) surprised me, as I’m used to much weaker correlations in the social sciences. To me, it’s a bit too high, suggesting that there are other factors at play or that one of the variables is merely a proxy for a large number of other variables. But I have no basis for this other than a gut feeling and a memory of a plot on  Language Log  about the distribution of correlation coefficients in social psychology.


So my question is this: Is a correlation in the range of (-0.82,-0.76) more likely to be a correlatio</p><p>2 0.79878986 <a title="257-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Interpreting_interactions_in_discrete-data_regression.html">1908 andrew gelman stats-2013-06-21-Interpreting interactions in discrete-data regression</a></p>
<p>Introduction: Mike Johns writes:
  
Are you familiar with the work of Ai and Norton on interactions in logit/probit models? I’d be curious to hear your thoughts.


Ai, C.R. and Norton E.C. 2003. Interaction terms in logit and probit models. Economics Letters 80(1): 123-129.


A peer ref just cited this paper in reaction to a logistic model we tested and claimed that the “only” way to test an interaction in logit/probit regression is to use the cross derivative method of Ai & Norton. I’ve never heard of this issue or method. It leaves me wondering what the interaction term actually tests (something Ai & Norton don’t discuss) and why such an important discovery is not more widely known. Is this an issue that is of particular relevance to econometric analysis because they approach interactions from the difference-in-difference perspective?


Full disclosure, I’m coming from a social science/epi background. Thus, i’m not interested in the d-in-d estimator; I want to know if any variables modify the rela</p><p>3 0.77088684 <a title="257-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-15-R-squared_for_multilevel_models.html">1121 andrew gelman stats-2012-01-15-R-squared for multilevel models</a></p>
<p>Introduction: Fred Schiff writes: 
  
  
I’m writing to you to ask about the “R-squared” approximation procedure you suggest in your 2004 book with Dr. Hill.  [See also  this paper  with Pardoe---ed.]


I’m a media sociologist at the University of Houston.  I’ve been using HLM3 for about two years.  


Briefly about my data.  It’s a content analysis of news stories with a continuous scale dependent variable, story prominence.  I have 6090 news stories, 114 newspapers, and 59 newspaper group owners.  All the Level-1, Level-2 and dependent variables have been standardized. Since the means were zero anyway, we left the variables uncentered.  All the Level-3 ownership groups and characteristics are dichotomous scales that were left uncentered.  


PROBLEM:  The single most important result I am looking for is to compare the strength of nine competing Level-1 variables in their ability to predict and explain the outcome variable, story prominence.  We are trying to use the residuals to calculate a “R-squ</p><p>4 0.76677066 <a title="257-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-18-Standardizing_regression_inputs.html">1462 andrew gelman stats-2012-08-18-Standardizing regression inputs</a></p>
<p>Introduction: Andy Flies, Ph.D. candidate in zoology, writes:
  
After reading your paper about scaling regression inputs by two standard deviations I found your  blog post  stating that you wished you had scaled by 1 sd and coded the binary inputs as -1 and 1.  Here is my question:


If you code the binary input as -1 and 1, do you then standardize it?  This makes sense to me because the mean of the standardized input is then zero and the sd is 1, which is what the mean and sd are for all of the other standardized inputs.  I know that if you code the binary input as 0 and 1 it should not be standardized.


Also, I am not interested in the actual units (i.e. mg/ml) of my response variable and I would like to compare a couple of different response variables that are on different scales.  Would it make sense to standardize the response variable also?
  
My reply:  No, I donâ&euro;&trade;t standardize the binary input.  The point of standardizing inputs is to make the coefs directly interpretable, but with binary i</p><p>5 0.7460857 <a title="257-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-04-What_are_the_key_assumptions_of_linear_regression%3F.html">1967 andrew gelman stats-2013-08-04-What are the key assumptions of linear regression?</a></p>
<p>Introduction: Andy Cooper writes:
  
A link to an  article , “Four Assumptions Of Multiple Regression That Researchers Should Always Test”, has been making  the rounds  on Twitter.  Their first rule is “Variables are Normally distributed.”  And they seem to be talking about the independent variables – but then later bring in tests on the residuals (while admitting that the normally-distributed error assumption is a weak assumption).  


I thought we had long-since moved away from transforming our independent variables to make them normally distributed for statistical reasons (as opposed to standardizing them for interpretability, etc.)  Am I missing something?  I agree that leverage in a influence is important, but normality of the variables? The article is from 2002, so it might be dated, but given the popularity of the tweet, I thought I’d ask your opinion.
  
My response:  There’s some useful advice on that page but overall I think the advice was dated even in 2002.  In section 3.6 of my book wit</p><p>6 0.74331623 <a title="257-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>7 0.74049312 <a title="257-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-26-How_to_understand_coefficients_that_reverse_sign_when_you_start_controlling_for_things%3F.html">1870 andrew gelman stats-2013-05-26-How to understand coefficients that reverse sign when you start controlling for things?</a></p>
<p>8 0.7379697 <a title="257-lsi-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-01-Imputing_count_data.html">14 andrew gelman stats-2010-05-01-Imputing count data</a></p>
<p>9 0.73736989 <a title="257-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-05-Understanding_regression_models_and_regression_coefficients.html">1656 andrew gelman stats-2013-01-05-Understanding regression models and regression coefficients</a></p>
<p>10 0.73721886 <a title="257-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-31-Using_factor_analysis_or_principal_components_analysis_or_measurement-error_models_for_biological_measurements_in_archaeology%3F.html">1094 andrew gelman stats-2011-12-31-Using factor analysis or principal components analysis or measurement-error models for biological measurements in archaeology?</a></p>
<p>11 0.73477644 <a title="257-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-18-Check_your_missing-data_imputations_using_cross-validation.html">1218 andrew gelman stats-2012-03-18-Check your missing-data imputations using cross-validation</a></p>
<p>12 0.73377585 <a title="257-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-19-Cross-validation_to_check_missing-data_imputation.html">1330 andrew gelman stats-2012-05-19-Cross-validation to check missing-data imputation</a></p>
<p>13 0.72724533 <a title="257-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-01-Modeling_y_%3D_a_%2B_b_%2B_c.html">1294 andrew gelman stats-2012-05-01-Modeling y = a + b + c</a></p>
<p>14 0.72623038 <a title="257-lsi-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-Correlation%2C_prediction%2C_variation%2C_etc..html">301 andrew gelman stats-2010-09-28-Correlation, prediction, variation, etc.</a></p>
<p>15 0.72491515 <a title="257-lsi-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Interactions_of_predictors_in_a_causal_model.html">251 andrew gelman stats-2010-09-02-Interactions of predictors in a causal model</a></p>
<p>16 0.71499795 <a title="257-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-09-The_effects_of_fiscal_consolidation.html">1663 andrew gelman stats-2013-01-09-The effects of fiscal consolidation</a></p>
<p>17 0.7013765 <a title="257-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-29-Going_negative.html">1918 andrew gelman stats-2013-06-29-Going negative</a></p>
<p>18 0.69888896 <a title="257-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-02-Interaction-based_feature_selection_and_classification_for_high-dimensional_biological_data.html">1703 andrew gelman stats-2013-02-02-Interaction-based feature selection and classification for high-dimensional biological data</a></p>
<p>19 0.69828218 <a title="257-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-15-Still_more_Mr._P_in_public_health.html">770 andrew gelman stats-2011-06-15-Still more Mr. P in public health</a></p>
<p>20 0.69662052 <a title="257-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-03-is_it_possible_to_%E2%80%9Coverstratify%E2%80%9D_when_assigning_a_treatment_in_a_randomized_control_trial%3F.html">553 andrew gelman stats-2011-02-03-is it possible to “overstratify” when assigning a treatment in a randomized control trial?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.011), (16, 0.105), (21, 0.029), (24, 0.137), (56, 0.018), (58, 0.018), (61, 0.024), (76, 0.166), (86, 0.056), (99, 0.308)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96741438 <a title="257-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-02-Roads%2C_traffic%2C_and_the_importance_in_decision_analysis_of_carefully_examining_your_goals.html">988 andrew gelman stats-2011-11-02-Roads, traffic, and the importance in decision analysis of carefully examining your goals</a></p>
<p>Introduction: Sandeep Baliga  writes : 
  
  
[In a  recent study , Gilles Duranton and Matthew Turner write:]

 
For interstate highways in metropolitan areas we [Duranton and Turner] ﬁnd that VKT (vehicle kilometers traveled) increases one for one with interstate highways, conﬁrming the fundamental law of highway congestion.’
 

Provision of public transit also simply leads to the people taking public transport being replaced by drivers on the road.  Therefore:

 
These ﬁndings suggest that both road capacity expansions and extensions to public transit are not appropriate policies with which to combat trafﬁc congestion. This leaves congestion pricing as the main candidate tool to curb trafﬁc congestion.
 
  
To which I reply:  Sure,  if your goal is to curb traffic congestion .  But what sort of goal is that?  Thinking like a microeconomist, my policy goal is to increase people’s utility.  Sure, traffic congestion is annoying, but there must be some advantages to driving on that crowded road or pe</p><p>2 0.96143454 <a title="257-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-A_calibrated_Cook_gives_Dems_the_edge_in_Nov%2C_sez_Sandy.html">300 andrew gelman stats-2010-09-28-A calibrated Cook gives Dems the edge in Nov, sez Sandy</a></p>
<p>Introduction: Sandy Gordon sends along this  fun little paper  forecasting the 2010 midterm election using expert predictions (the Cook and Rothenberg Political Reports).  Gordon’s gimmick is that he uses past performance to calibrate the reports’ judgments based on “solid,” “likely,” “leaning,” and “toss-up” categories, and then he uses the calibrated versions of the current predictions to make his forecast.
 
As I wrote a  few weeks ago  in response to Nate’s forecasts, I think the right way to go, if you really want to forecast the election outcome, is to use national information to predict the national swing and then do regional, state, and district-level adjustments using whatever local information is available.  I don’t see the point of using  only  the expert forecasts and no other data.
 
Still, Gordon is bringing new information (his calibrations) to the table, so I wanted to share it with you.  Ultimately I like the throw-in-everything approach that Nate uses (although I think Nate’s descr</p><p>3 0.9575752 <a title="257-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-29-A_Ph.D._thesis_is_not_really_a_marathon.html">1351 andrew gelman stats-2012-05-29-A Ph.D. thesis is not really a marathon</a></p>
<p>Introduction: Thomas Basbøll  writes :
  
A blog called The Thesis Whisperer was recently pointed out to me. I [Basbøll] haven’t looked at it closely, but I’ll be reading it regularly for a while before I recommend it. I’m sure it’s a good place to go to discover that you’re not alone, especially when you’re struggling with your dissertation.  One post  caught my eye immediately. It suggested that writing a thesis is not a sprint, it’s a marathon.


As a metaphorical adjustment to a particular attitude about writing, it’s probably going to help some people. But if we think it through, it’s not really a very good analogy. No one is really a “sprinter”; and writing a dissertation is nothing like running a marathon. . . .


Here’s Ben’s explication of the analogy at the Thesis Whisperer, which seems initially plausible.

 
…writing a dissertation is a lot like running a marathon. They are both endurance events, they last a long time and they require a consistent and carefully calculated amount of effor</p><p>4 0.95605201 <a title="257-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-28-A_convenience_sample_and_selected_treatments.html">1551 andrew gelman stats-2012-10-28-A convenience sample and selected treatments</a></p>
<p>Introduction: Charlie Saunders writes:
  
A study has recently been  published  in the New England Journal of Medicine (NEJM) which uses survival analysis to examine long-acting reversible contraception (e.g. intrauterine devices [IUDs]) vs. short-term commonly prescribed methods of contraception (e.g. oral contraceptive pills) on unintended pregnancies.


The authors use a convenience sample of over 7,000 women.  I am not well versed-enough in sampling theory to determine the appropriateness of this but it would seem that the use of a non-probability sampling would be a significant drawback.  If you could give me your opinion on this, I would appreciate it.


The NEJM is one of the top medical journals in the country.  Could this type of sampling method coupled with this method of analysis be published in a journal like JASA?
  
My reply:  There are two concerns, first that it is a convenience sample and thus not representative of the population, and second that the treatments are chosen rather tha</p><p>5 0.95443177 <a title="257-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-02-7_ways_to_separate_errors_from_statistics.html">1835 andrew gelman stats-2013-05-02-7 ways to separate errors from statistics</a></p>
<p>Introduction: Betsey Stevenson and Justin Wolfers have been inspired by the recent Reinhardt and Rogoff  debacle  to  list  “six ways to separate lies from statistics” in economics research:
  
1. “Focus on how robust a finding is, meaning that different ways of looking at the evidence point to the same conclusion.”


2.  Don’t confuse statistical with practical significance.


3.  “Be wary of scholars using high-powered statistical techniques as a bludgeon to silence critics who are not specialists.”


4.  “Don’t fall into the trap of thinking about an empirical finding as ‘right’ or ‘wrong.’ At best, data provide an imperfect guide.”


5.  “Don’t mistake correlation for causation.”


6.  “Always ask ‘so what?’”
  
I like all these points, especially #4, which I think doesn’t get said enough.  As I  wrote  a few months ago, high-profile social science research aims for proof, not for understanding—and that’s a problem.
 
 My addition to the list 
 
If you compare my title above to that of Stevenson</p><p>6 0.95388591 <a title="257-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-10-The_recursion_of_pop-econ.html">1850 andrew gelman stats-2013-05-10-The recursion of pop-econ</a></p>
<p>7 0.95163369 <a title="257-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-17-Vote_Buying%3A_Evidence_from_a_List_Experiment_in_Lebanon.html">283 andrew gelman stats-2010-09-17-Vote Buying: Evidence from a List Experiment in Lebanon</a></p>
<p>8 0.95140517 <a title="257-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-06-Stephen_Kosslyn%E2%80%99s_principles_of_graphics_and_one_more%3A__There%E2%80%99s_no_need_to_cram_everything_into_a_single_plot.html">1609 andrew gelman stats-2012-12-06-Stephen Kosslyn’s principles of graphics and one more:  There’s no need to cram everything into a single plot</a></p>
<p>same-blog 9 0.95024097 <a title="257-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Question_about_standard_range_for_social_science_correlations.html">257 andrew gelman stats-2010-09-04-Question about standard range for social science correlations</a></p>
<p>10 0.94092762 <a title="257-lda-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-12-Election_symposium_at_Columbia_Journalism_School.html">337 andrew gelman stats-2010-10-12-Election symposium at Columbia Journalism School</a></p>
<p>11 0.93940932 <a title="257-lda-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-14-Causal_inference_in_economics.html">32 andrew gelman stats-2010-05-14-Causal inference in economics</a></p>
<p>12 0.93703759 <a title="257-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-22-Goal%3A__Rules_for_Turing_chess.html">1818 andrew gelman stats-2013-04-22-Goal:  Rules for Turing chess</a></p>
<p>13 0.92989516 <a title="257-lda-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-13-An_Economist%E2%80%99s_Guide_to_Visualizing_Data.html">2246 andrew gelman stats-2014-03-13-An Economist’s Guide to Visualizing Data</a></p>
<p>14 0.92915541 <a title="257-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-01-%24241%2C364.83_%E2%80%93_%2413%2C000_%3D_%24228%2C364.83.html">1600 andrew gelman stats-2012-12-01-$241,364.83 – $13,000 = $228,364.83</a></p>
<p>15 0.92807698 <a title="257-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-25-Is_instrumental_variables_analysis_particularly_susceptible_to_Type_M_errors%3F.html">368 andrew gelman stats-2010-10-25-Is instrumental variables analysis particularly susceptible to Type M errors?</a></p>
<p>16 0.92526937 <a title="257-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-24-Economists_don%E2%80%99t_think_like_accountants%E2%80%94but_maybe_they_should.html">922 andrew gelman stats-2011-09-24-Economists don’t think like accountants—but maybe they should</a></p>
<p>17 0.92502314 <a title="257-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-26-If_statistics_is_so_significantly_great%2C_why_don%E2%80%99t_statisticians_use_statistics%3F.html">51 andrew gelman stats-2010-05-26-If statistics is so significantly great, why don’t statisticians use statistics?</a></p>
<p>18 0.92499363 <a title="257-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-12-Single_or_multiple_imputation%3F.html">608 andrew gelman stats-2011-03-12-Single or multiple imputation?</a></p>
<p>19 0.9216401 <a title="257-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-08-Econ_debate_about_prices_at_a_fancy_restaurant.html">1105 andrew gelman stats-2012-01-08-Econ debate about prices at a fancy restaurant</a></p>
<p>20 0.91050482 <a title="257-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-08-What_we_need_here_is_some_peer_review_for_statistical_graphics.html">2013 andrew gelman stats-2013-09-08-What we need here is some peer review for statistical graphics</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
