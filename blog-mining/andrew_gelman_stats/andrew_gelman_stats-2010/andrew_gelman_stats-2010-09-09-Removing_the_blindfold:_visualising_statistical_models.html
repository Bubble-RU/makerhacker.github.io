<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>265 andrew gelman stats-2010-09-09-Removing the blindfold: visualising statistical models</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-265" href="#">andrew_gelman_stats-2010-265</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>265 andrew gelman stats-2010-09-09-Removing the blindfold: visualising statistical models</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-265-html" href="http://andrewgelman.com/2010/09/09/removing_the_bl/">html</a></p><p>Introduction: Hadley Wickham’s talk  for Monday 13 Sept at noon in the statistics dept:
  
As the volume of data increases, so to does the complexity of our models. Visualisation is a powerful tool for both understanding how models work, and what they say about a particularly dataset. There are very many well-known techniques for visualising data, but far fewer for visualising models. In this talk I [Wichkam] will discuss three broad strategies for model visualisation: display the model in the data space; look all members of a collection; and explore the process of model fitting, not just the end result. I will demonstrate these techniques with two examples: neural networks, and ensembles of linear 
models.
  
Hey–this is one of my favorite topics!</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Hadley Wickham’s talk  for Monday 13 Sept at noon in the statistics dept:    As the volume of data increases, so to does the complexity of our models. [sent-1, score-0.711]
</p><p>2 Visualisation is a powerful tool for both understanding how models work, and what they say about a particularly dataset. [sent-2, score-0.472]
</p><p>3 There are very many well-known techniques for visualising data, but far fewer for visualising models. [sent-3, score-1.352]
</p><p>4 In this talk I [Wichkam] will discuss three broad strategies for model visualisation: display the model in the data space; look all members of a collection; and explore the process of model fitting, not just the end result. [sent-4, score-1.513]
</p><p>5 I will demonstrate these techniques with two examples: neural networks, and ensembles of linear  models. [sent-5, score-0.815]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('visualising', 0.454), ('visualisation', 0.413), ('techniques', 0.225), ('ensembles', 0.195), ('noon', 0.195), ('sept', 0.18), ('dept', 0.166), ('neural', 0.163), ('wickham', 0.16), ('hadley', 0.152), ('monday', 0.146), ('talk', 0.138), ('strategies', 0.134), ('complexity', 0.128), ('volume', 0.123), ('model', 0.122), ('networks', 0.12), ('powerful', 0.12), ('broad', 0.119), ('fewer', 0.114), ('collection', 0.114), ('explore', 0.112), ('increases', 0.109), ('members', 0.108), ('tool', 0.106), ('display', 0.105), ('favorite', 0.104), ('demonstrate', 0.101), ('space', 0.097), ('fitting', 0.097), ('topics', 0.096), ('linear', 0.092), ('hey', 0.09), ('data', 0.084), ('discuss', 0.079), ('particularly', 0.079), ('process', 0.078), ('understanding', 0.075), ('examples', 0.071), ('end', 0.07), ('three', 0.067), ('far', 0.067), ('look', 0.053), ('models', 0.052), ('statistics', 0.043), ('say', 0.04), ('two', 0.039), ('many', 0.038), ('work', 0.035), ('one', 0.02)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="265-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-09-Removing_the_blindfold%3A_visualising_statistical_models.html">265 andrew gelman stats-2010-09-09-Removing the blindfold: visualising statistical models</a></p>
<p>Introduction: Hadley Wickham’s talk  for Monday 13 Sept at noon in the statistics dept:
  
As the volume of data increases, so to does the complexity of our models. Visualisation is a powerful tool for both understanding how models work, and what they say about a particularly dataset. There are very many well-known techniques for visualising data, but far fewer for visualising models. In this talk I [Wichkam] will discuss three broad strategies for model visualisation: display the model in the data space; look all members of a collection; and explore the process of model fitting, not just the end result. I will demonstrate these techniques with two examples: neural networks, and ensembles of linear 
models.
  
Hey–this is one of my favorite topics!</p><p>2 0.15831441 <a title="265-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-16-Uri_Simonsohn_is_speaking_at_Columbia_tomorrow_%28Mon%29.html">1499 andrew gelman stats-2012-09-16-Uri Simonsohn is speaking at Columbia tomorrow (Mon)</a></p>
<p>Introduction: Noon in the stat dept (room 903 School of Social Work, at 122/Amsterdam).  Heâ&euro;&trade;ll be talking about ways of finding fishy p-values.
 
See  here  and  here  for background.  This stuff is cool and important.</p><p>3 0.10823748 <a title="265-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-17-Ripley_on_model_selection%2C_and_some_links_on_exploratory_model_analysis.html">1066 andrew gelman stats-2011-12-17-Ripley on model selection, and some links on exploratory model analysis</a></p>
<p>Introduction: This  is really fun.  I love how Ripley thinks, with just about every concept considered in broad generality while being connected to real-data examples.  He’s a great statistical storyteller as well.
 
 . . . and Wickham on exploratory model analysis 
 
I came across Ripley’s slides in a reference from Hadley Wickham’s  article on exploratory model analysis .  I’ve been interested for awhile in statistical graphics for understanding fitted models (which is different than the usual use of graphics to visualize data or to understand discrepancies of data from models).  Recently I’ve started using the term “exploratory model analysis,” and it seemed like such a natural phrase that I thought I’d google it and see what’s up.  I found the above-linked paper by Hadley, which in turn refers to  a paper  by Antony Unwin, Chris Volinksy, and Sylvia Winkler that defines “exploratory modelling analysis” as “the evaluation and comparison of many models simultaneously.”  That’s not exactly what I h</p><p>4 0.10470606 <a title="265-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>Introduction: Cosma Shalizi  and  Larry Wasserman  discuss some papers from a conference on Ockham’s Razor.  I don’t have anything new to add on this so let me link to  past blog entries  on the topic and repost the following  from 2004 :
  
A lot has been written in statistics about “parsimony”—that is, the desire to explain phenomena using fewer parameters–but I’ve never seen any good general justification for parsimony.  (I don’t count “Occam’s Razor,” or “Ockham’s Razor,” or whatever, as a justification.  You gotta do better than digging up a 700-year-old quote.)


Maybe it’s because I work in social science, but my feeling is:  if you can approximate reality with just a few parameters, fine.  If you can use more parameters to fold in more information, that’s even better.


In practice, I often use simple models—because they are less effort to fit and, especially, to understand.  But I don’t kid myself that they’re better than more complicated efforts!


My favorite quote on this comes from  Rad</p><p>5 0.10387743 <a title="265-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-04-Model_checking_and_model_understanding_in_machine_learning.html">1482 andrew gelman stats-2012-09-04-Model checking and model understanding in machine learning</a></p>
<p>Introduction: Last month I  wrote :
  
Computer scientists are often brilliant but they can be unfamiliar with what is done in the worlds of data collection and analysis. This goes the other way too: statisticians such as myself can look pretty awkward, reinventing (or failing to reinvent) various wheels when we write computer programs or, even worse, try to design software.Andrew MacNamara writes:
  
Andrew MacNamara followed up with some thoughts:
  
I [MacNamara] had some basic statistics training through my MBA program, after having completed an undergrad degree in computer science. Since then I’ve been very interested in learning more about statistical techniques, including things like GLM and censored data analyses as well as machine learning topics like neural nets, SVMs, etc. I began following your blog after some research into Bayesian analysis topics and I am trying to dig deeper on that side of things.


One thing I have noticed is that there seems to be a distinction between data analysi</p><p>6 0.089149781 <a title="265-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>7 0.089017309 <a title="265-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-11-Weakly_informative_priors_for_Bayesian_nonparametric_models%3F.html">1454 andrew gelman stats-2012-08-11-Weakly informative priors for Bayesian nonparametric models?</a></p>
<p>8 0.084931366 <a title="265-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>9 0.079032443 <a title="265-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-13-Flexibility_is_good.html">2133 andrew gelman stats-2013-12-13-Flexibility is good</a></p>
<p>10 0.078560129 <a title="265-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-26-Any_good_articles_on_the_use_of_error_bars%3F.html">822 andrew gelman stats-2011-07-26-Any good articles on the use of error bars?</a></p>
<p>11 0.078019552 <a title="265-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-01-What_goes_around_._._..html">548 andrew gelman stats-2011-02-01-What goes around . . .</a></p>
<p>12 0.07737191 <a title="265-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-24-Bayes_at_the_end.html">534 andrew gelman stats-2011-01-24-Bayes at the end</a></p>
<p>13 0.074684151 <a title="265-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-10-Using_a_%E2%80%9Cpure_infographic%E2%80%9D_to_explore_differences_between_information_visualization_and_statistical_graphics.html">847 andrew gelman stats-2011-08-10-Using a “pure infographic” to explore differences between information visualization and statistical graphics</a></p>
<p>14 0.073530115 <a title="265-tfidf-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-20-How_to_schedule_projects_in_an_introductory_statistics_course%3F.html">423 andrew gelman stats-2010-11-20-How to schedule projects in an introductory statistics course?</a></p>
<p>15 0.07056094 <a title="265-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>16 0.066703185 <a title="265-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>17 0.066490814 <a title="265-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-%E2%80%9CAll_Models_are_Right%2C_Most_are_Useless%E2%80%9D.html">1197 andrew gelman stats-2012-03-04-“All Models are Right, Most are Useless”</a></p>
<p>18 0.066430651 <a title="265-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-12-The_power_of_the_puzzlegraph.html">1669 andrew gelman stats-2013-01-12-The power of the puzzlegraph</a></p>
<p>19 0.066063315 <a title="265-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-06-Another_stereotype_demolished.html">699 andrew gelman stats-2011-05-06-Another stereotype demolished</a></p>
<p>20 0.06601882 <a title="265-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-14-Wickham_R_short_course.html">1009 andrew gelman stats-2011-11-14-Wickham R short course</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.098), (1, 0.06), (2, -0.029), (3, 0.043), (4, 0.041), (5, 0.011), (6, -0.065), (7, -0.002), (8, 0.029), (9, 0.045), (10, 0.002), (11, 0.024), (12, -0.034), (13, -0.016), (14, -0.049), (15, -0.026), (16, 0.017), (17, -0.028), (18, 0.013), (19, -0.0), (20, -0.023), (21, -0.056), (22, -0.005), (23, -0.038), (24, -0.039), (25, 0.025), (26, -0.064), (27, -0.045), (28, 0.027), (29, -0.045), (30, -0.025), (31, -0.011), (32, -0.018), (33, 0.002), (34, 0.022), (35, 0.043), (36, 0.001), (37, -0.038), (38, 0.03), (39, 0.007), (40, -0.02), (41, -0.025), (42, -0.018), (43, 0.047), (44, -0.006), (45, 0.034), (46, -0.0), (47, -0.022), (48, -0.019), (49, -0.001)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96196824 <a title="265-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-09-Removing_the_blindfold%3A_visualising_statistical_models.html">265 andrew gelman stats-2010-09-09-Removing the blindfold: visualising statistical models</a></p>
<p>Introduction: Hadley Wickham’s talk  for Monday 13 Sept at noon in the statistics dept:
  
As the volume of data increases, so to does the complexity of our models. Visualisation is a powerful tool for both understanding how models work, and what they say about a particularly dataset. There are very many well-known techniques for visualising data, but far fewer for visualising models. In this talk I [Wichkam] will discuss three broad strategies for model visualisation: display the model in the data space; look all members of a collection; and explore the process of model fitting, not just the end result. I will demonstrate these techniques with two examples: neural networks, and ensembles of linear 
models.
  
Hey–this is one of my favorite topics!</p><p>2 0.84692103 <a title="265-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-%E2%80%9CAll_Models_are_Right%2C_Most_are_Useless%E2%80%9D.html">1197 andrew gelman stats-2012-03-04-“All Models are Right, Most are Useless”</a></p>
<p>Introduction: The above is the title of  a talk  that Thad Tarpey gave at the Joint Statistical Meetings in 2009.  Here’s the abstract:
  
Students of statistics are often introduced to George Box’s famous quote: “all models are wrong, some are useful.” In this talk I [Tarpey] argue that this quote, although useful, is wrong. A different and more positive perspective is to acknowledge that a model is simply a means of extracting information of interest from data. The truth is infinitely complex and a model is merely an approximation to the truth. If the approximation is poor or misleading, then the model is useless. In this talk I give examples of correct models that are not true models. I illustrate how the notion of a “wrong” model can lead to wrong conclusions.
  
I’m curious what he had to say—maybe he could post the slides?
 
P.S.  And  here they are !</p><p>3 0.80535662 <a title="265-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-09-Special_journal_issue_on_statistical_methods_for_the_social_sciences.html">24 andrew gelman stats-2010-05-09-Special journal issue on statistical methods for the social sciences</a></p>
<p>Introduction: Last year I  spoke at  a conference celebrating the 10th anniversary of the University of Washington’s Center for Statistics and the Social Sciences, and just today a  special issue  of the journal Statistical Methodology came out in honor of the center’s anniversary.   My article  in the special issue actually has nothing to do with my talk at the conference; rather, it’s an exploration of an idea that Iven Van Mechelen and I had for understanding deterministic models probabilistically:
  
 
For the analysis of binary data, various deterministic models have been proposed, which are generally simpler to fit and easier to understand than probabilistic models. We claim that corresponding to any deterministic model is an implicit stochastic model in which the deterministic model fits imperfectly, with errors occurring at random. In the context of binary data, we consider a model in which the probability of error depends on the model prediction. We show how to fit this model using a stocha</p><p>4 0.78381181 <a title="265-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-28-Using_predator-prey_models_on_the_Canadian_lynx_series.html">1141 andrew gelman stats-2012-01-28-Using predator-prey models on the Canadian lynx series</a></p>
<p>Introduction: The “Canadian lynx data” is one of the famous examples used in time series analysis.  And the usual models that are fit to these data in the statistics time-series literature, don’t work well.  Cavan Reilly and Angelique Zeringue  write :
 
   
 
Reilly and Zeringue then present their analysis.  Their simple little predator-prey model with a weakly informative prior way outperforms the standard big-ass autoregression models.  Check this out:
 
   
 
Or, to put it into numbers, when they fit their model to the first 80 years and predict to the next 34, their root mean square out-of-sample error is 1480 (see scale of data above).  In contrast, the standard model fit to these data (the SETAR model of Tong, 1990) has more than twice as many parameters but gets a worse-performing root mean square error of 1600, even when that model is fit to the entire dataset.  (If you fit the SETAR or any similar autoregressive model to the first 80 years and use it to predict the next 34, the predictions</p><p>5 0.76975286 <a title="265-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-27-Bridges_between_deterministic_and_probabilistic_models_for_binary_data.html">780 andrew gelman stats-2011-06-27-Bridges between deterministic and probabilistic models for binary data</a></p>
<p>Introduction: For the analysis of binary data, various deterministic models have been proposed, which are generally simpler to fit and easier to understand than probabilistic models. We claim that corresponding to any deterministic model is an implicit stochastic model in which the deterministic model fits imperfectly, with errors occurring at random. In the context of binary data, we consider a model in which the probability of error depends on the model prediction. We show how to fit this model using a stochastic modification of deterministic optimization schemes.
 
The advantages of fitting the stochastic model explicitly (rather than implicitly, by simply fitting a deterministic model and accepting the occurrence of errors) include quantification of uncertainty in the deterministic model’s parameter estimates, better estimation of the true model error rate, and the ability to check the fit of the model nontrivially. We illustrate this with a simple theoretical example of item response data and w</p><p>6 0.76365167 <a title="265-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-17-Ripley_on_model_selection%2C_and_some_links_on_exploratory_model_analysis.html">1066 andrew gelman stats-2011-12-17-Ripley on model selection, and some links on exploratory model analysis</a></p>
<p>7 0.75872105 <a title="265-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>8 0.74901271 <a title="265-lsi-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-03-This_is_a_footnote_in_one_of_my_papers.html">448 andrew gelman stats-2010-12-03-This is a footnote in one of my papers</a></p>
<p>9 0.74682826 <a title="265-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-11-Adding_an_error_model_to_a_deterministic_model.html">1162 andrew gelman stats-2012-02-11-Adding an error model to a deterministic model</a></p>
<p>10 0.74106687 <a title="265-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-03-Popper_and_Jaynes.html">2007 andrew gelman stats-2013-09-03-Popper and Jaynes</a></p>
<p>11 0.73865306 <a title="265-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-04-Model_checking_and_model_understanding_in_machine_learning.html">1482 andrew gelman stats-2012-09-04-Model checking and model understanding in machine learning</a></p>
<p>12 0.72269702 <a title="265-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>13 0.7192542 <a title="265-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-05-Xiao-Li_Meng_and_Xianchao_Xie_rethink_asymptotics.html">1406 andrew gelman stats-2012-07-05-Xiao-Li Meng and Xianchao Xie rethink asymptotics</a></p>
<p>14 0.71860516 <a title="265-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-13-Flexibility_is_good.html">2133 andrew gelman stats-2013-12-13-Flexibility is good</a></p>
<p>15 0.69452417 <a title="265-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-04-When_is_there_%E2%80%9Chidden_structure_in_data%E2%80%9D_to_be_discovered%3F.html">1788 andrew gelman stats-2013-04-04-When is there “hidden structure in data” to be discovered?</a></p>
<p>16 0.69173473 <a title="265-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-04-Columbo_does_posterior_predictive_checks.html">1521 andrew gelman stats-2012-10-04-Columbo does posterior predictive checks</a></p>
<p>17 0.69139588 <a title="265-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>18 0.6835317 <a title="265-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>19 0.67926788 <a title="265-lsi-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-05-Does_posterior_predictive_model_checking_fit_with_the_operational_subjective_approach%3F.html">320 andrew gelman stats-2010-10-05-Does posterior predictive model checking fit with the operational subjective approach?</a></p>
<p>20 0.67854476 <a title="265-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-11-Kaiser_Fung_on_how_not_to_critique_models.html">1004 andrew gelman stats-2011-11-11-Kaiser Fung on how not to critique models</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(16, 0.103), (24, 0.097), (26, 0.054), (65, 0.025), (69, 0.208), (84, 0.065), (86, 0.015), (90, 0.03), (99, 0.272)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.90918338 <a title="265-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-09-Removing_the_blindfold%3A_visualising_statistical_models.html">265 andrew gelman stats-2010-09-09-Removing the blindfold: visualising statistical models</a></p>
<p>Introduction: Hadley Wickham’s talk  for Monday 13 Sept at noon in the statistics dept:
  
As the volume of data increases, so to does the complexity of our models. Visualisation is a powerful tool for both understanding how models work, and what they say about a particularly dataset. There are very many well-known techniques for visualising data, but far fewer for visualising models. In this talk I [Wichkam] will discuss three broad strategies for model visualisation: display the model in the data space; look all members of a collection; and explore the process of model fitting, not just the end result. I will demonstrate these techniques with two examples: neural networks, and ensembles of linear 
models.
  
Hey–this is one of my favorite topics!</p><p>2 0.90766811 <a title="265-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-16-A_historical_perspective_on_financial_bailouts.html">89 andrew gelman stats-2010-06-16-A historical perspective on financial bailouts</a></p>
<p>Introduction: Thomas Ferguson and Robert Johnson  write :
  
Financial crises are staggeringly costly. Only major wars rival them in the burdens they place on public finances. Taxpayers typically transfer enormous resources to banks, their stockholders, and creditors, while public debt explodes and the economy runs below full employment for years. This paper compares how relatively large, developed countries have handled bailouts over time. It analyzes why some have done better than others at containing costs and protecting taxpayers. The paper argues that political variables – the nature of competition within party systems and voting turnout – help explain why some countries do more than others to limit the moral hazards of bailouts.
  
I know next to nothing about this topic, so I’ll just recommend you click through and read the article yourself.  Here’s a bit more:
  
Many recent papers have analyzed financial crises using large data bases filled with cases from all over the world. Our [Ferguson</p><p>3 0.88119221 <a title="265-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-10-Translating_into_Votes%3A_The_Electoral_Impact_of_Spanish-Language_Ballots.html">406 andrew gelman stats-2010-11-10-Translating into Votes: The Electoral Impact of Spanish-Language Ballots</a></p>
<p>Introduction: Dan Hopkins sends along  this article :
  
[Hopkins] uses regression discontinuity design to estimate the turnout and election impacts of Spanish-language assistance provided under Section 203 of the Voting Rights Act. Analyses of two different data sets – the Latino National Survey and California 1998 primary election returns – show that Spanish-language assistance increased turnout for citizens who speak little English. The California results also demonstrate that election procedures an influence outcomes, as support for ending bilingual education dropped markedly in heavily Spanish-speaking neighborhoods with Spanish-language assistance. The California analyses find hints of backlash among non-Hispanic white precincts, but not with the same size or certainty. Small changes in election procedures can influence who votes as well as what wins.
  
Beyond the direct relevance of these results, I find this paper interesting as an example of research that is fundamentally quantitative.  Th</p><p>4 0.87953222 <a title="265-lda-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-12-How_tall_is_Jon_Lee_Anderson%3F.html">1759 andrew gelman stats-2013-03-12-How tall is Jon Lee Anderson?</a></p>
<p>Introduction: The second best thing about  this story  (from Tom Scocca) is that Anderson spells “Tweets” with a capital T.
 
But the best thing is that Scocca is numerate—he compares numbers on the logarithmic scale:
  
Reminding Lake that he only had 169 Twitter followers was the saddest gambit of all. Jon Lee Anderson has 17,866 followers. And Kim Kardashian has, as I write this, 17,489,892 followers. That is: Jon Lee Anderson is 1/1,000 as important on Twitter, by his own standard, as Kim Kardashian. He is 10 times closer to Mitch Lake than he is to Kim Kardashian.
  
How often do we see a popular journalist who understands orders of magnitude?  Good job, Tom Scocca!
 
P.S.  Based on his “little twerp” comment, I also wonder if Anderson suffers from tall person syndrome—that’s the problem that some people of above-average height have, that they think they’re more important than other people because they literally look down on them.  Don’t get me wrong—I have lots of tall friends who are complete</p><p>5 0.87623239 <a title="265-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-22-Tenants_and_landlords.html">158 andrew gelman stats-2010-07-22-Tenants and landlords</a></p>
<p>Introduction: Matthew Yglesias  and  Megan McArdle  argue about the economics of landlord/tenant laws in D.C., a topic I know nothing about.  But it did remind me of a few stories . . .
 
1.  In grad school, I shared half of a two-family house with three other students.  At some point, our landlord (who lived in the other half of the house) decided he wanted to sell the place, so he had a real estate agent coming by occasionally to show the house to people.  She was just a flat-out liar (which I guess fits my impression based on screenings of Glengarry Glen Ross).  I could never decide, when I was around and she was lying to a prospective buyer, whether to call her on it.  Sometimes I did, sometimes I didn’t.
 
2.  A year after I graduated, the landlord actually did sell the place but then, when my friends moved out, he refused to pay back their security deposit.  There was some debate about getting the place repainted, I don’t remember the details.  So they sued the landlord in Mass. housing court</p><p>6 0.86479008 <a title="265-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-24-What_is_the_normal_range_of_values_in_a_medical_test%3F.html">923 andrew gelman stats-2011-09-24-What is the normal range of values in a medical test?</a></p>
<p>7 0.86055279 <a title="265-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-11-Jonathan_Chait_and_I_agree_about_the_importance_of_the_fundamentals_in_determining_presidential_elections.html">656 andrew gelman stats-2011-04-11-Jonathan Chait and I agree about the importance of the fundamentals in determining presidential elections</a></p>
<p>8 0.85110164 <a title="265-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Job_openings_at_conservative_political_analytics_firm%21.html">1909 andrew gelman stats-2013-06-21-Job openings at conservative political analytics firm!</a></p>
<p>9 0.84080046 <a title="265-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-16-Our_new_improved_blog%21__Thanks_to_Cord_Blomquist.html">856 andrew gelman stats-2011-08-16-Our new improved blog!  Thanks to Cord Blomquist</a></p>
<p>10 0.83981115 <a title="265-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-18-Tibshirani_announces_new_research_result%3A__A_significance_test_for_the_lasso.html">1769 andrew gelman stats-2013-03-18-Tibshirani announces new research result:  A significance test for the lasso</a></p>
<p>11 0.83755487 <a title="265-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-06-%E2%80%9CSampling%3A__Design_and_Analysis%E2%80%9D%3A__a_course_for_political_science_graduate_students.html">749 andrew gelman stats-2011-06-06-“Sampling:  Design and Analysis”:  a course for political science graduate students</a></p>
<p>12 0.83229494 <a title="265-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-01-Halloween-Valentine%E2%80%99s_update.html">1357 andrew gelman stats-2012-06-01-Halloween-Valentine’s update</a></p>
<p>13 0.82661456 <a title="265-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-Fourteen_magic_words%3A_an_update.html">898 andrew gelman stats-2011-09-10-Fourteen magic words: an update</a></p>
<p>14 0.8218562 <a title="265-lda-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-11-Multilevel_modeling_in_R_on_a_Mac.html">198 andrew gelman stats-2010-08-11-Multilevel modeling in R on a Mac</a></p>
<p>15 0.82118762 <a title="265-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-16-My_talk_19h_this_evening.html">2063 andrew gelman stats-2013-10-16-My talk 19h this evening</a></p>
<p>16 0.82102442 <a title="265-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-Varying_treatment_effects%2C_again.html">1310 andrew gelman stats-2012-05-09-Varying treatment effects, again</a></p>
<p>17 0.82019556 <a title="265-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-14-Extra_babies_on_Valentine%E2%80%99s_Day%2C_fewer_on_Halloween%3F.html">1167 andrew gelman stats-2012-02-14-Extra babies on Valentine’s Day, fewer on Halloween?</a></p>
<p>18 0.81906122 <a title="265-lda-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-07-The_%24900_kindergarten_teacher.html">261 andrew gelman stats-2010-09-07-The $900 kindergarten teacher</a></p>
<p>19 0.81668925 <a title="265-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-17-Hierarchical-multilevel_modeling_with_%E2%80%9Cbig_data%E2%80%9D.html">1267 andrew gelman stats-2012-04-17-Hierarchical-multilevel modeling with “big data”</a></p>
<p>20 0.81599212 <a title="265-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-23-The_economics_of_the_mac%3F__A_paradox_of_competition.html">867 andrew gelman stats-2011-08-23-The economics of the mac?  A paradox of competition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
