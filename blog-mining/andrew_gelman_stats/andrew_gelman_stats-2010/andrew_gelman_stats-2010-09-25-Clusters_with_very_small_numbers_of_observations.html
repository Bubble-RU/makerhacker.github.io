<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>295 andrew gelman stats-2010-09-25-Clusters with very small numbers of observations</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-295" href="#">andrew_gelman_stats-2010-295</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>295 andrew gelman stats-2010-09-25-Clusters with very small numbers of observations</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-295-html" href="http://andrewgelman.com/2010/09/25/clusters_with_v/">html</a></p><p>Introduction: James O’Brien writes:
  
How would you explain, to a “classically-trained” hypothesis-tester, that “It’s OK to fit a multilevel model even if some groups have only one observation each”?


I [O'Brien] think I understand the logic and the statistical principles at work in this, but I’ve having trouble being clear and persuasive. I also feel like I’m contending with some methodological conventional wisdom here. 
  
My reply:  I’m so used to this idea that I find it difficult to defend it in some sort of general conceptual way.  So let me retreat to a more functional defense, which is that multilevel modeling gives good estimates,  especially  when the number of observations per group is small.
 
One way to see this in any particular example in through cross-validation.  Another way is to consider the alternatives.   If you try really hard you can come up with a “classical hypothesis testing” approach which will do as well as the multilevel model.  It would just take a lot of work.  I’d r</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 James O’Brien writes:    How would you explain, to a “classically-trained” hypothesis-tester, that “It’s OK to fit a multilevel model even if some groups have only one observation each”? [sent-1, score-0.845]
</p><p>2 I [O'Brien] think I understand the logic and the statistical principles at work in this, but I’ve having trouble being clear and persuasive. [sent-2, score-0.683]
</p><p>3 I also feel like I’m contending with some methodological conventional wisdom here. [sent-3, score-0.751]
</p><p>4 My reply:  I’m so used to this idea that I find it difficult to defend it in some sort of general conceptual way. [sent-4, score-0.398]
</p><p>5 So let me retreat to a more functional defense, which is that multilevel modeling gives good estimates,  especially  when the number of observations per group is small. [sent-5, score-1.591]
</p><p>6 One way to see this in any particular example in through cross-validation. [sent-6, score-0.205]
</p><p>7 If you try really hard you can come up with a “classical hypothesis testing” approach which will do as well as the multilevel model. [sent-8, score-0.738]
</p><p>8 I’d rather put that effort into statistical modeling and data visualization instead. [sent-10, score-0.623]
</p><p>9 If you are in a situation where someone really doesn’t want to do the multilevel model, you could perhaps ask your skeptical colleague what his or her goals are in your particular statistical modeling problem. [sent-11, score-1.606]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('multilevel', 0.407), ('modeling', 0.255), ('contending', 0.245), ('retreat', 0.186), ('functional', 0.165), ('wisdom', 0.162), ('conceptual', 0.159), ('observation', 0.157), ('logic', 0.157), ('defense', 0.148), ('defend', 0.148), ('statistical', 0.141), ('methodological', 0.139), ('skeptical', 0.135), ('observations', 0.134), ('conventional', 0.129), ('particular', 0.128), ('visualization', 0.128), ('principles', 0.124), ('colleague', 0.124), ('goals', 0.121), ('situation', 0.12), ('trouble', 0.116), ('james', 0.116), ('classical', 0.113), ('testing', 0.111), ('groups', 0.106), ('per', 0.1), ('effort', 0.099), ('hypothesis', 0.099), ('explain', 0.098), ('model', 0.096), ('gives', 0.096), ('ask', 0.093), ('difficult', 0.091), ('especially', 0.088), ('group', 0.088), ('estimates', 0.084), ('ok', 0.083), ('really', 0.082), ('fit', 0.079), ('approach', 0.078), ('way', 0.077), ('feel', 0.076), ('consider', 0.075), ('clear', 0.073), ('understand', 0.072), ('number', 0.072), ('try', 0.072), ('reply', 0.071)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="295-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-25-Clusters_with_very_small_numbers_of_observations.html">295 andrew gelman stats-2010-09-25-Clusters with very small numbers of observations</a></p>
<p>Introduction: James O’Brien writes:
  
How would you explain, to a “classically-trained” hypothesis-tester, that “It’s OK to fit a multilevel model even if some groups have only one observation each”?


I [O'Brien] think I understand the logic and the statistical principles at work in this, but I’ve having trouble being clear and persuasive. I also feel like I’m contending with some methodological conventional wisdom here. 
  
My reply:  I’m so used to this idea that I find it difficult to defend it in some sort of general conceptual way.  So let me retreat to a more functional defense, which is that multilevel modeling gives good estimates,  especially  when the number of observations per group is small.
 
One way to see this in any particular example in through cross-validation.  Another way is to consider the alternatives.   If you try really hard you can come up with a “classical hypothesis testing” approach which will do as well as the multilevel model.  It would just take a lot of work.  I’d r</p><p>2 0.21039356 <a title="295-tfidf-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-31-Analyzing_the_entire_population_rather_than_a_sample.html">383 andrew gelman stats-2010-10-31-Analyzing the entire population rather than a sample</a></p>
<p>Introduction: Lee Mobley writes:
  
I recently read what you posted on your blog  How does statistical analysis differ when analyzing the entire population rather than a sample? 


What you said in the blog accords with my training in econometrics.  However I am concerned about a new wrinkle on this problem that derives from multilevel modeling.
      
We are analyzing multilevel models of the probability of using cancer screening for the entire Medicare population. I argue that every state has different systems in place (politics, cancer control efforts, culture, insurance regulations, etc) so that essentially a different probability generating mechanism is in place for each state. Thus I estimate 50 separate regressions for the populations in each state, and then note and map the variability in the effect estimates (slope parameters) for each covariate.


Reviewers argue that I should be using random slopes modeling, pooling all individuals in all states together. I am familiar with this approach</p><p>3 0.19979618 <a title="295-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-11-Yes%2C_worry_about_generalizing_from_data_to_population.__But_multilevel_modeling_is_the_solution%2C_not_the_problem.html">1934 andrew gelman stats-2013-07-11-Yes, worry about generalizing from data to population.  But multilevel modeling is the solution, not the problem</a></p>
<p>Introduction: A sociologist writes in:
  
Samuel Lucas has just published  a paper  in Quality and Quantity arguing that anything less than a full probability sample of higher levels in HLMs yields biased and unusable results. If I follow him correctly, he is arguing that not only are the SEs too small, but the parameter estimates themselves are biased and we cannot say in advance whether the bias is positive or negative.


Lucas has thrown down a big gauntlet, advising us throw away our data unless the sample of macro units is right and ignore the published results that fail this standard. Extreme. 
Is there another conclusion to be drawn? 
Other advice to be given? 
A Bayesian path out of the valley?
  
Heres’s the abstract to Lucas’s paper:
  
The multilevel model has become a staple of social research. I textually and formally explicate sample design features that, I contend, are required for unbiased estimation of macro-level multilevel model parameters and the use of tools for statistical infe</p><p>4 0.1949133 <a title="295-tfidf-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-09-Sof%5Bt%5D.html">77 andrew gelman stats-2010-06-09-Sof[t]</a></p>
<p>Introduction: Joe Fruehwald writes:
  
I’m working with linguistic data, specifically binomial hits and misses of a certain variable for certain words (specifically whether or not the “t” sound was pronounced at the end of words like “soft”). Word frequency follows a power law, with most words appearing just once, and with some words being hyperfrequent. I’m not interested in specific word effects, but I am interested in the effect of word frequency.


A logistic model fit is going to be heavily influenced by the effect of the hyperfrequent words which constitute only one type. To control for the item effect, I would fit a multilevel model with a random intercept by word, but like I said, most of the words appear only once.


Is there a principled approach to this problem?
  
My response:  It’s ok to fit a multilevel model even if most groups only have one observation each.  You’ll want to throw in some word-level predictors too.  Think of the multilevel model not as a substitute for the usual thoug</p><p>5 0.17745373 <a title="295-tfidf-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-10-Two_great_tastes_that_taste_great_together.html">25 andrew gelman stats-2010-05-10-Two great tastes that taste great together</a></p>
<p>Introduction: Vlad Kogan writes:
  
I’ve using your book on regression and multilevel modeling and have a quick R question for you. Do you happen to know if there is any R package that can estimate a two-stage (instrumental variable) multi-level model?
  
My reply:  I don’t know.  I’ll post on blog and maybe there will be a response.  You could also try the R help list.</p><p>6 0.16854015 <a title="295-tfidf-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-How_does_multilevel_modeling_affect_the_estimate_of_the_grand_mean%3F.html">255 andrew gelman stats-2010-09-04-How does multilevel modeling affect the estimate of the grand mean?</a></p>
<p>7 0.16616592 <a title="295-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-06-17_groups%2C_6_group-level_predictors%3A__What_to_do%3F.html">1248 andrew gelman stats-2012-04-06-17 groups, 6 group-level predictors:  What to do?</a></p>
<p>8 0.16409512 <a title="295-tfidf-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-17-If_you_get_to_the_point_of_asking%2C_just_do_it.__But_some_difficulties_do_arise_._._..html">2294 andrew gelman stats-2014-04-17-If you get to the point of asking, just do it.  But some difficulties do arise . . .</a></p>
<p>9 0.1630263 <a title="295-tfidf-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-06-Multilevel_quantile_regression.html">397 andrew gelman stats-2010-11-06-Multilevel quantile regression</a></p>
<p>10 0.15706718 <a title="295-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-23-More_on_Bayesian_methods_and_multilevel_modeling.html">2033 andrew gelman stats-2013-09-23-More on Bayesian methods and multilevel modeling</a></p>
<p>11 0.15660001 <a title="295-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-29-How_many_parameters_are_in_a_multilevel_model%3F.html">1144 andrew gelman stats-2012-01-29-How many parameters are in a multilevel model?</a></p>
<p>12 0.15548328 <a title="295-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-25-Correlation_of_1_._._._too_good_to_be_true%3F.html">1737 andrew gelman stats-2013-02-25-Correlation of 1 . . . too good to be true?</a></p>
<p>13 0.15520203 <a title="295-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-10-Multiple_imputation_and_multilevel_analysis.html">704 andrew gelman stats-2011-05-10-Multiple imputation and multilevel analysis</a></p>
<p>14 0.14079599 <a title="295-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-23-Examples_of_the_use_of_hierarchical_modeling_to_generalize_to_new_settings.html">1425 andrew gelman stats-2012-07-23-Examples of the use of hierarchical modeling to generalize to new settings</a></p>
<p>15 0.13834187 <a title="295-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-15-Exploratory_multilevel_analysis_when_group-level_variables_are_of_importance.html">1900 andrew gelman stats-2013-06-15-Exploratory multilevel analysis when group-level variables are of importance</a></p>
<p>16 0.13813356 <a title="295-tfidf-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-17-Graphical_tools_for_understanding_multilevel_models.html">772 andrew gelman stats-2011-06-17-Graphical tools for understanding multilevel models</a></p>
<p>17 0.1372375 <a title="295-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-14-Everyone%E2%80%99s_trading_bias_for_variance_at_some_point%2C_it%E2%80%99s_just_done_at_different_places_in_the_analyses.html">1763 andrew gelman stats-2013-03-14-Everyone’s trading bias for variance at some point, it’s just done at different places in the analyses</a></p>
<p>18 0.13565196 <a title="295-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-08-Multilevel_regression_with_shrinkage_for_%E2%80%9Cfixed%E2%80%9D_effects.html">653 andrew gelman stats-2011-04-08-Multilevel regression with shrinkage for “fixed” effects</a></p>
<p>19 0.13527273 <a title="295-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-05-A_statistician_rereads_Bill_James.html">697 andrew gelman stats-2011-05-05-A statistician rereads Bill James</a></p>
<p>20 0.13466907 <a title="295-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-20-A_mess_with_which_I_am_comfortable.html">1814 andrew gelman stats-2013-04-20-A mess with which I am comfortable</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.236), (1, 0.093), (2, 0.009), (3, -0.013), (4, 0.082), (5, -0.001), (6, -0.05), (7, -0.013), (8, 0.123), (9, 0.103), (10, 0.014), (11, 0.012), (12, 0.023), (13, -0.025), (14, 0.021), (15, -0.001), (16, -0.092), (17, -0.056), (18, 0.032), (19, -0.005), (20, 0.005), (21, -0.028), (22, -0.016), (23, 0.043), (24, -0.094), (25, -0.157), (26, -0.052), (27, 0.009), (28, -0.09), (29, -0.006), (30, -0.012), (31, -0.041), (32, 0.029), (33, 0.039), (34, -0.059), (35, 0.034), (36, 0.063), (37, 0.009), (38, 0.045), (39, 0.013), (40, 0.028), (41, 0.02), (42, -0.028), (43, -0.11), (44, -0.015), (45, 0.05), (46, 0.014), (47, -0.027), (48, -0.152), (49, -0.046)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96563059 <a title="295-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-25-Clusters_with_very_small_numbers_of_observations.html">295 andrew gelman stats-2010-09-25-Clusters with very small numbers of observations</a></p>
<p>Introduction: James O’Brien writes:
  
How would you explain, to a “classically-trained” hypothesis-tester, that “It’s OK to fit a multilevel model even if some groups have only one observation each”?


I [O'Brien] think I understand the logic and the statistical principles at work in this, but I’ve having trouble being clear and persuasive. I also feel like I’m contending with some methodological conventional wisdom here. 
  
My reply:  I’m so used to this idea that I find it difficult to defend it in some sort of general conceptual way.  So let me retreat to a more functional defense, which is that multilevel modeling gives good estimates,  especially  when the number of observations per group is small.
 
One way to see this in any particular example in through cross-validation.  Another way is to consider the alternatives.   If you try really hard you can come up with a “classical hypothesis testing” approach which will do as well as the multilevel model.  It would just take a lot of work.  I’d r</p><p>2 0.78737867 <a title="295-lsi-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-06-Multilevel_quantile_regression.html">397 andrew gelman stats-2010-11-06-Multilevel quantile regression</a></p>
<p>Introduction: Ryan Seals writes:
  
I’m an epidemiologist at Emory University, and I’m working on a project of release patterns in jails (basically trying to model how long individuals are in jail before they’re release, for purposes of designing short-term health interventions, i.e. HIV testing, drug counseling, etc…). The question lends itself to quantile regression; we’re interested in the # of days it takes for 50% and 75% of inmates to be released. But being a clustered/nested data structure, it also obviously lends itself to multilevel modeling, with the group-level being individual jails.


So: do you know of any work on multilevel quantile regression? My quick lit search didn’t yield much, and I don’t see any preprogrammed way to do it in SAS.
  
My reply:
 
To start with, I’m putting in the R keyword here, on the hope that some readers might be able to refer you to an R function that does what you want.  Beyond this, I think it should be possible to program something in Bugs.  In ARM we hav</p><p>3 0.78580868 <a title="295-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-11-Yes%2C_worry_about_generalizing_from_data_to_population.__But_multilevel_modeling_is_the_solution%2C_not_the_problem.html">1934 andrew gelman stats-2013-07-11-Yes, worry about generalizing from data to population.  But multilevel modeling is the solution, not the problem</a></p>
<p>Introduction: A sociologist writes in:
  
Samuel Lucas has just published  a paper  in Quality and Quantity arguing that anything less than a full probability sample of higher levels in HLMs yields biased and unusable results. If I follow him correctly, he is arguing that not only are the SEs too small, but the parameter estimates themselves are biased and we cannot say in advance whether the bias is positive or negative.


Lucas has thrown down a big gauntlet, advising us throw away our data unless the sample of macro units is right and ignore the published results that fail this standard. Extreme. 
Is there another conclusion to be drawn? 
Other advice to be given? 
A Bayesian path out of the valley?
  
Heres’s the abstract to Lucas’s paper:
  
The multilevel model has become a staple of social research. I textually and formally explicate sample design features that, I contend, are required for unbiased estimation of macro-level multilevel model parameters and the use of tools for statistical infe</p><p>4 0.78568971 <a title="295-lsi-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-17-If_you_get_to_the_point_of_asking%2C_just_do_it.__But_some_difficulties_do_arise_._._..html">2294 andrew gelman stats-2014-04-17-If you get to the point of asking, just do it.  But some difficulties do arise . . .</a></p>
<p>Introduction: Nelson Villoria writes:
  
I find the multilevel approach very useful for a problem I am dealing with, and I was wondering whether you could point me to some references about poolability tests for multilevel models. I am working with time series of cross sectional data and I want to test whether the data supports cross sectional and/or time pooling. In a standard panel data setting I do this with Chow tests and/or CUSUM. Are these ideas directly transferable to the multilevel setting?
  
My reply:  I think you should do partial pooling.  Once the question arises, just do it.  Other models are just special cases.  I donâ&euro;&trade;t see the need for any test.
 
That said, if you do a group-level model, you need to consider including group-level averages of individual predictors (see  here ).  And if the number of groups is small, there can be real gains from using an informative prior distribution on the hierarchical variance parameters.  This is something that Jennifer and I do not discuss in our</p><p>5 0.76096559 <a title="295-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-10-Multiple_imputation_and_multilevel_analysis.html">704 andrew gelman stats-2011-05-10-Multiple imputation and multilevel analysis</a></p>
<p>Introduction: Robert Birkelbach:
  
I am writing my Bachelor Thesis in which I want to assess the reading competencies of German elementary school children using the PIRLS2006 data. My levels are classrooms and the individuals. However, my dependent variable is a multiple imputed (m=5) reading test. The problem I have is, that I do not know, whether I can just calculate 5 linear multilevel models and then average all the results (the coefficients, standard deviation, bic, intra class correlation, R2, t-statistics, p-values etc) or if I need different formulas for integrating the results of the five models into one because it is a multilevel analysis? Do you think there’s a better way in solving my problem? I would greatly appreciate if you could help me with a problem regarding my analysis — I am quite a newbie to multilevel modeling and especially to multiple imputation. Also: Is it okay to use frequentist models when the multiple imputation was done bayesian? Would the different philosophies of sc</p><p>6 0.76029539 <a title="295-lsi-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-09-Sof%5Bt%5D.html">77 andrew gelman stats-2010-06-09-Sof[t]</a></p>
<p>7 0.75779641 <a title="295-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-06-17_groups%2C_6_group-level_predictors%3A__What_to_do%3F.html">1248 andrew gelman stats-2012-04-06-17 groups, 6 group-level predictors:  What to do?</a></p>
<p>8 0.7577492 <a title="295-lsi-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-31-Analyzing_the_entire_population_rather_than_a_sample.html">383 andrew gelman stats-2010-10-31-Analyzing the entire population rather than a sample</a></p>
<p>9 0.7459088 <a title="295-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-10-Combining_data_from_many_sources.html">948 andrew gelman stats-2011-10-10-Combining data from many sources</a></p>
<p>10 0.74541575 <a title="295-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-17-Graphical_tools_for_understanding_multilevel_models.html">772 andrew gelman stats-2011-06-17-Graphical tools for understanding multilevel models</a></p>
<p>11 0.72601491 <a title="295-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-19-Data_exploration_and_multiple_comparisons.html">524 andrew gelman stats-2011-01-19-Data exploration and multiple comparisons</a></p>
<p>12 0.70855707 <a title="295-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-20-A_mess_with_which_I_am_comfortable.html">1814 andrew gelman stats-2013-04-20-A mess with which I am comfortable</a></p>
<p>13 0.70450693 <a title="295-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-18-Hierarchical_modeling_as_a_framework_for_extrapolation.html">1383 andrew gelman stats-2012-06-18-Hierarchical modeling as a framework for extrapolation</a></p>
<p>14 0.70061773 <a title="295-lsi-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-16-Mandelbrot_and_Akaike%3A__from_taxonomy_to_smooth_runways_%28pioneering_work_in_fractals_and_self-similarity%29.html">346 andrew gelman stats-2010-10-16-Mandelbrot and Akaike:  from taxonomy to smooth runways (pioneering work in fractals and self-similarity)</a></p>
<p>15 0.70020318 <a title="295-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-23-Examples_of_the_use_of_hierarchical_modeling_to_generalize_to_new_settings.html">1425 andrew gelman stats-2012-07-23-Examples of the use of hierarchical modeling to generalize to new settings</a></p>
<p>16 0.69917309 <a title="295-lsi-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-10-Two_great_tastes_that_taste_great_together.html">25 andrew gelman stats-2010-05-10-Two great tastes that taste great together</a></p>
<p>17 0.69752181 <a title="295-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-Multilevel_modeling_even_when_you%E2%80%99re_not_interested_in_predictions_for_new_groups.html">1194 andrew gelman stats-2012-03-04-Multilevel modeling even when you’re not interested in predictions for new groups</a></p>
<p>18 0.6898939 <a title="295-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-23-More_on_Bayesian_methods_and_multilevel_modeling.html">2033 andrew gelman stats-2013-09-23-More on Bayesian methods and multilevel modeling</a></p>
<p>19 0.6892969 <a title="295-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Allowing_interaction_terms_to_vary.html">753 andrew gelman stats-2011-06-09-Allowing interaction terms to vary</a></p>
<p>20 0.68510169 <a title="295-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-25-Correlation_of_1_._._._too_good_to_be_true%3F.html">1737 andrew gelman stats-2013-02-25-Correlation of 1 . . . too good to be true?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(15, 0.031), (16, 0.074), (21, 0.031), (24, 0.248), (43, 0.02), (64, 0.029), (86, 0.042), (89, 0.018), (99, 0.404)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99320745 <a title="295-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-11-One_more_time_on_that_ESP_study%3A__The_problem_of_overestimates_and_the_shrinkage_solution.html">511 andrew gelman stats-2011-01-11-One more time on that ESP study:  The problem of overestimates and the shrinkage solution</a></p>
<p>Introduction: Benedict Carey  writes  a follow-up article on ESP studies and Bayesian statistics.  ( See here  for my previous thoughts on the topic.)  Everything Carey writes is fine, and he even uses an example I recommended:
  
The statistical approach that has dominated the social sciences for almost a century is called significance testing. The idea is straightforward. A finding from any well-designed study — say, a correlation between a personality trait and the risk of depression — is considered “significant” if its probability of occurring by chance is less than 5 percent.


This arbitrary cutoff makes sense when the effect being studied is a large one — for example, when measuring the so-called Stroop effect. This effect predicts that naming the color of a word is faster and more accurate when the word and color match (“red” in red letters) than when they do not (“red” in blue letters), and is very strong in almost everyone.


“But if the true effect of what you are measuring is small,” sai</p><p>2 0.99277484 <a title="295-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>Introduction: Robert Bell pointed me to  this post  by Brad De Long on Bayesian statistics, and then I also noticed  this  from Noah Smith, who wrote:
  
My impression is that although the Bayesian/Frequentist debate is interesting and intellectually fun, there’s really not much “there” there… despite being so-hip-right-now, Bayesian is not the Statistical Jesus.
  
I’m happy to see the discussion going in this direction.  Twenty-five years ago or so, when I got into this biz, there were some serious anti-Bayesian attitudes floating around in mainstream statistics.  Discussions in the journals sometimes devolved into debates of the form, “Bayesians:  knaves or fools?”.  You’d get all sorts of free-floating skepticism about any prior distribution at all, even while people were accepting without question (and doing theory on) logistic regressions, proportional hazards models, and all sorts of strong strong models.  (In the subfield of survey sampling, various prominent researchers would refuse to mode</p><p>3 0.99206924 <a title="295-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>Introduction: Following up on Christian’s  post  [link fixed] on the topic, I’d like to offer a few thoughts of my own.
 
In BDA, we express the idea that a noninformative prior is a placeholder:  you can use the noninformative prior to get the analysis started, then if your posterior distribution is less informative than you would like, or if it does not make sense, you can go back and add prior information.
 
Same thing for the data model (the “likelihood”), for that matter:  it often makes sense to start with something simple and conventional and then go from there.
 
So, in that sense, noninformative priors are no big deal, they’re just a way to get started.  Just don’t take them too seriously.
 
Traditionally in statistics we’ve worked with the paradigm of a single highly informative dataset with only weak external information.  But if the data are sparse and prior information is strong, we have to think differently.  And, when you increase the dimensionality of a problem, both these things hap</p><p>4 0.99198419 <a title="295-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-30-Fixed_effects%2C_followed_by_Bayes_shrinkage%3F.html">1644 andrew gelman stats-2012-12-30-Fixed effects, followed by Bayes shrinkage?</a></p>
<p>Introduction: Stuart Buck writes:
  
I have a question about  fixed effects vs. random effects . Amongst economists who study teacher value-added, it has become common to see people saying that they estimated teacher fixed effects (via least squares dummy variables, so that there is a parameter for each teacher), but that they then applied empirical Bayes shrinkage so that the teacher effects are brought closer to the mean.  (See  this paper  by Jacob and Lefgren, for example.)


Can that really be what they are doing? Why wouldn’t they just run random (modeled) effects in the first place? I feel like there’s something I’m missing.
  
My reply:  I don’t know the full story here, but I’m thinking there are two goals, first to get an unbiased estimate of an overall treatment effect (and there the econometricians prefer so-called fixed effects; I disagree with them on this but I know where they’re coming from) and second to estimate individual teacher effects (and there it makes sense to use so-called</p><p>5 0.99181283 <a title="295-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-24-Bell_Labs.html">970 andrew gelman stats-2011-10-24-Bell Labs</a></p>
<p>Introduction: Sining Chen told me they’re hiring in the  statistics group at Bell Labs .  I’ll do my bit for economic stimulus by announcing this job (see below).
 
I love Bell Labs.  I worked there for three summers, in a physics lab in 1985-86 under the supervision of Loren Pfeiffer, and by myself in the statistics group in 1990.
 
I learned a lot working for Loren.  He was a really smart and driven guy.  His lab was a small set of rooms—in Bell Labs, everything’s in a small room, as they value the positive externality of close physical proximity of different labs, which you get by making each lab compact—and it was Loren, his assistant (a guy named Ken West who kept everything running in the lab), and three summer students: me, Gowton Achaibar, and a girl whose name I’ve forgotten.  Gowtan and I had a lot of fun chatting in the lab.  One day I made a silly comment about Gowton’s accent—he was from Guyana and pronounced “three” as “tree”—and then I apologized and said:  Hey, here I am making fun o</p><p>6 0.99141562 <a title="295-lda-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>7 0.99092811 <a title="295-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-The_statistical_significance_filter.html">899 andrew gelman stats-2011-09-10-The statistical significance filter</a></p>
<p>8 0.99085903 <a title="295-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>9 0.99062675 <a title="295-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>10 0.99024284 <a title="295-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-29-Infovis%2C_infographics%2C_and_data_visualization%3A__Where_I%E2%80%99m_coming_from%2C_and_where_I%E2%80%99d_like_to_go.html">878 andrew gelman stats-2011-08-29-Infovis, infographics, and data visualization:  Where I’m coming from, and where I’d like to go</a></p>
<p>11 0.99007195 <a title="295-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-11-Adding_an_error_model_to_a_deterministic_model.html">1162 andrew gelman stats-2012-02-11-Adding an error model to a deterministic model</a></p>
<p>12 0.98998177 <a title="295-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-04-Write_This_Book.html">1605 andrew gelman stats-2012-12-04-Write This Book</a></p>
<p>13 0.9894678 <a title="295-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-01-How_does_Bayes_do_it%3F.html">247 andrew gelman stats-2010-09-01-How does Bayes do it?</a></p>
<p>14 0.98939538 <a title="295-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-02-The_inevitable_problems_with_statistical_significance_and_95%25_intervals.html">1150 andrew gelman stats-2012-02-02-The inevitable problems with statistical significance and 95% intervals</a></p>
<p>15 0.98922265 <a title="295-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-07-X_on_JLP.html">1792 andrew gelman stats-2013-04-07-X on JLP</a></p>
<p>16 0.9891181 <a title="295-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-03-How_best_to_compare_effects_measured_in_two_different_time_periods%3F.html">2086 andrew gelman stats-2013-11-03-How best to compare effects measured in two different time periods?</a></p>
<p>17 0.98909914 <a title="295-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-08-P-values_and_statistical_practice.html">1713 andrew gelman stats-2013-02-08-P-values and statistical practice</a></p>
<p>18 0.98869807 <a title="295-lda-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-17-How_to_think_about_the_statistical_evidence_when_the_statistical_evidence_can%E2%80%99t_be_conclusive%3F.html">2174 andrew gelman stats-2014-01-17-How to think about the statistical evidence when the statistical evidence can’t be conclusive?</a></p>
<p>19 0.98856759 <a title="295-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>20 0.98851305 <a title="295-lda-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-08-Here%E2%80%99s_how_rumors_get_started%3A__Lineplots%2C_dotplots%2C_and_nonfunctional_modernist_architecture.html">262 andrew gelman stats-2010-09-08-Here’s how rumors get started:  Lineplots, dotplots, and nonfunctional modernist architecture</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
