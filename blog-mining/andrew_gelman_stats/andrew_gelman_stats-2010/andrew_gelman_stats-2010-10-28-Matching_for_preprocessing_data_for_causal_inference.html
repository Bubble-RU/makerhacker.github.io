<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>375 andrew gelman stats-2010-10-28-Matching for preprocessing data for causal inference</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-375" href="#">andrew_gelman_stats-2010-375</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>375 andrew gelman stats-2010-10-28-Matching for preprocessing data for causal inference</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-375-html" href="http://andrewgelman.com/2010/10/28/matching_for_pr/">html</a></p><p>Introduction: Chris Blattman  writes :
  
Matching is not an identification strategy a solution to your endogeneity problem; it is a weighting scheme. Saying matching will reduce endogeneity bias is like saying that the best way to get thin is to weigh yourself in kilos. The statement makes no sense. It confuses technique with substance. . . . When you run a regression, you control for the X you can observe. When you match, you are simply matching based on those same X. . . .
  
I see what Chris is getting at–matching, like regression, won’t help for the variables you’re not controlling for–but I disagree with his characterization of matching as a weighting scheme.  I see matching as a way to restrict your analysis to comparable cases.  The statistical motivation:  robustness.  If you had a good enough model, you wouldn’t neet to match, you’d just fit the model to the data.  But in common practice we often use simple regression models and so it can be helpful to do some matching first before regress</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Chris Blattman  writes :    Matching is not an identification strategy a solution to your endogeneity problem; it is a weighting scheme. [sent-1, score-0.568]
</p><p>2 Saying matching will reduce endogeneity bias is like saying that the best way to get thin is to weigh yourself in kilos. [sent-2, score-1.385]
</p><p>3 When you run a regression, you control for the X you can observe. [sent-8, score-0.099]
</p><p>4 When you match, you are simply matching based on those same X. [sent-9, score-0.863]
</p><p>5 I see what Chris is getting at–matching, like regression, won’t help for the variables you’re not controlling for–but I disagree with his characterization of matching as a weighting scheme. [sent-13, score-1.413]
</p><p>6 I see matching as a way to restrict your analysis to comparable cases. [sent-14, score-0.97]
</p><p>7 If you had a good enough model, you wouldn’t neet to match, you’d just fit the model to the data. [sent-16, score-0.05]
</p><p>8 But in common practice we often use simple regression models and so it can be helpful to do some matching first before regression. [sent-17, score-1.165]
</p><p>9 It’s not so difficult to match on dozens of variables, but it’s not so easy to include dozens of variables in your least squares regression. [sent-18, score-0.797]
</p><p>10 So in practice it’s not always the case that “you are simply matching based on those same X. [sent-19, score-0.962]
</p><p>11 To put it another way:  yes, you’ll often need to worry about potential X variables that you don’t have–but that shouldn’t stop you from controlling for everything that you  do  have, and matching can be a helpful tool in that effort. [sent-20, score-1.512]
</p><p>12 Beyond this, I think it’s useful to distinguish between two different problems:  imbalance and lack of complete overlap. [sent-21, score-0.298]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('matching', 0.711), ('endogeneity', 0.219), ('match', 0.213), ('variables', 0.184), ('dozens', 0.161), ('weighting', 0.156), ('controlling', 0.148), ('chris', 0.125), ('regression', 0.121), ('helpful', 0.118), ('confuses', 0.114), ('imbalance', 0.104), ('characterization', 0.104), ('weigh', 0.104), ('practice', 0.099), ('blattman', 0.099), ('thin', 0.094), ('simply', 0.09), ('technique', 0.082), ('squares', 0.078), ('restrict', 0.078), ('saying', 0.077), ('distinguish', 0.075), ('identification', 0.074), ('arm', 0.074), ('comparable', 0.072), ('motivation', 0.068), ('often', 0.067), ('reduce', 0.065), ('tool', 0.065), ('based', 0.062), ('strategy', 0.062), ('stop', 0.061), ('worry', 0.061), ('complete', 0.061), ('disagree', 0.06), ('way', 0.059), ('shouldn', 0.058), ('lack', 0.058), ('solution', 0.057), ('bias', 0.056), ('statement', 0.055), ('chapter', 0.053), ('control', 0.051), ('potential', 0.05), ('see', 0.05), ('model', 0.05), ('common', 0.049), ('run', 0.048), ('everything', 0.047)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="375-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-28-Matching_for_preprocessing_data_for_causal_inference.html">375 andrew gelman stats-2010-10-28-Matching for preprocessing data for causal inference</a></p>
<p>Introduction: Chris Blattman  writes :
  
Matching is not an identification strategy a solution to your endogeneity problem; it is a weighting scheme. Saying matching will reduce endogeneity bias is like saying that the best way to get thin is to weigh yourself in kilos. The statement makes no sense. It confuses technique with substance. . . . When you run a regression, you control for the X you can observe. When you match, you are simply matching based on those same X. . . .
  
I see what Chris is getting at–matching, like regression, won’t help for the variables you’re not controlling for–but I disagree with his characterization of matching as a weighting scheme.  I see matching as a way to restrict your analysis to comparable cases.  The statistical motivation:  robustness.  If you had a good enough model, you wouldn’t neet to match, you’d just fit the model to the data.  But in common practice we often use simple regression models and so it can be helpful to do some matching first before regress</p><p>2 0.5335117 <a title="375-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-10-Matching_and_regression%3A__two_great_tastes_etc_etc.html">796 andrew gelman stats-2011-07-10-Matching and regression:  two great tastes etc etc</a></p>
<p>Introduction: Matthew Bogard writes:
  
Regarding the book Mostly Harmless Econometrics, you  state :

 
A casual reader of the book might be left with the unfortunate impression that matching is a competitor to regression rather than a tool for making regression more effective.
 

But in fact isn’t that what they are arguing, that, in a  ‘mostly harmless way’ regression is in fact a matching estimator itself?


“Our view is that regression can be motivated as a particular sort of weighted matching estimator, and therefore the differences between regression and matching estimates are unlikely to be of major empirical importance” (Chapter 3 p. 70)


They seem to be distinguishing regression (without prior matching) from all other types of matching techniques, and therefore implying that regression can be a ‘mostly harmless’ substitute or competitor to matching. My previous understanding, before starting this book was as you say, that matching is a tool that makes regression more effective.


I have n</p><p>3 0.20174944 <a title="375-tfidf-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-17-Matching_at_two_levels.html">213 andrew gelman stats-2010-08-17-Matching at two levels</a></p>
<p>Introduction: Steve Porter writes with a question about matching for inferences in a hierarchical data structure.  I’ve never thought about this particular issue, but it seems potentially important.
 
Maybe one or more of you have some useful suggestions?
 
Porter writes:
  
 
After immersing myself in the relatively sparse literature on propensity scores with clustered data, it seems as if people take one of two approaches. If the treatment is at the cluster-level (like school policies), they match on only the cluster-level covariates. If the treatment is at the individual level, they match on individual-level covariates. (I have also found some papers that match on individual-level covariates when it seems as if the treatment is really at the cluster-level.) But what if there is a selection process at both levels?


For my research question (effect of tenure systems on faculty behavior) there is a two-step selection process: first colleges choose whether to have a tenure system for faculty; then f</p><p>4 0.18046817 <a title="375-tfidf-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-14-%E2%80%9CToo_much_data%E2%80%9D%3F.html">86 andrew gelman stats-2010-06-14-“Too much data”?</a></p>
<p>Introduction: Chris Hane writes:
  
I am scientist needing to model a treatment effect on a population of ~500 people.  The dependent variable in the model is the difference in a person’s pre-treatment 12 month total medical cost versus post-treatment cost.  So there is large variation in costs, but not so much by using the difference between the pre and post treatment costs.  The issue I’d like some advice on is that the treatment has already occurred so there is no possibility of creating a fully randomized control now.  I do have a very large population of people to use as possible controls via propensity scoring or exact matching.  


If I had a few thousand people to possibly match, then I would use standard techniques.  However, I have a potential population of over a hundred thousand people.  An exact match of the possible controls to age, gender and region of the country still leaves a population of 10,000 controls. Even if I use propensity scores to weight the 10,000 observations (understan</p><p>5 0.13851123 <a title="375-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-14-Questions_about_a_study_of_charter_schools.html">957 andrew gelman stats-2011-10-14-Questions about a study of charter schools</a></p>
<p>Introduction: Phil Harris and Bruce Smith write:
  
We have only recently chanced upon your blog while we were looking for responses to the “decline effect” in medical and scientific studies. We were especially taken by your comment that there is something wrong with the scientific method “if this method is defined as running experiments and doing data analysis in a patternless way and then reporting, as true, results that pass a statistical significance threshold.” To us, that seems to be standard operating procedure in much of social science, including our own field of education. Indeed quasi-experimental designs are the stock in trade of those who attempt to use “science” — we dare not say haruspicy, but you can if you like — to influence the course of public policy.


Now, a new entrant into the cherry pickers sweepstakes seems to have emerged. It is on  Charter School Performance in Indiana Schools .  We are by no means professional statisticians or data analysts, but we have some background in</p><p>6 0.11926653 <a title="375-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-26-Some_thoughts_on_survey_weighting.html">1430 andrew gelman stats-2012-07-26-Some thoughts on survey weighting</a></p>
<p>7 0.11700442 <a title="375-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-31-Value-added_modeling_in_education%3A__Gaming_the_system_by_sending_kids_on_a_field_trip_at_test_time.html">2083 andrew gelman stats-2013-10-31-Value-added modeling in education:  Gaming the system by sending kids on a field trip at test time</a></p>
<p>8 0.11117993 <a title="375-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-20-Cars_vs._trucks.html">527 andrew gelman stats-2011-01-20-Cars vs. trucks</a></p>
<p>9 0.11005585 <a title="375-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-09-The_effects_of_fiscal_consolidation.html">1663 andrew gelman stats-2013-01-09-The effects of fiscal consolidation</a></p>
<p>10 0.10706724 <a title="375-tfidf-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-05-What_do_practitioners_need_to_know_about_regression%3F.html">451 andrew gelman stats-2010-12-05-What do practitioners need to know about regression?</a></p>
<p>11 0.10218947 <a title="375-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-06-Comparing_people_from_two_surveys%2C_one_of_which_is_a_simple_random_sample_and_one_of_which_is_not.html">1523 andrew gelman stats-2012-10-06-Comparing people from two surveys, one of which is a simple random sample and one of which is not</a></p>
<p>12 0.10198836 <a title="375-tfidf-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-20-Paul_Rosenbaum_on_those_annoying_pre-treatment_variables_that_are_sort-of_instruments_and_sort-of_covariates.html">287 andrew gelman stats-2010-09-20-Paul Rosenbaum on those annoying pre-treatment variables that are sort-of instruments and sort-of covariates</a></p>
<p>13 0.099212497 <a title="375-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-26-How_to_understand_coefficients_that_reverse_sign_when_you_start_controlling_for_things%3F.html">1870 andrew gelman stats-2013-05-26-How to understand coefficients that reverse sign when you start controlling for things?</a></p>
<p>14 0.098904625 <a title="375-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>15 0.093390211 <a title="375-tfidf-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-19-Analysis_of_survey_data%3A_Design_based_models_vs._hierarchical_modeling%3F.html">352 andrew gelman stats-2010-10-19-Analysis of survey data: Design based models vs. hierarchical modeling?</a></p>
<p>16 0.089583017 <a title="375-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-24-Multilevel_modeling_and_instrumental_variables.html">1468 andrew gelman stats-2012-08-24-Multilevel modeling and instrumental variables</a></p>
<p>17 0.088879749 <a title="375-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>18 0.08483234 <a title="375-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>19 0.083714932 <a title="375-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>20 0.082945153 <a title="375-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-03-It_depends_upon_what_the_meaning_of_the_word_%E2%80%9Cfirm%E2%80%9D_is..html">940 andrew gelman stats-2011-10-03-It depends upon what the meaning of the word “firm” is.</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.132), (1, 0.061), (2, 0.04), (3, -0.019), (4, 0.064), (5, 0.015), (6, 0.007), (7, -0.021), (8, 0.087), (9, 0.064), (10, 0.038), (11, 0.016), (12, 0.002), (13, -0.005), (14, 0.007), (15, 0.003), (16, 0.001), (17, 0.015), (18, -0.02), (19, 0.014), (20, -0.016), (21, 0.014), (22, -0.019), (23, -0.01), (24, -0.019), (25, 0.059), (26, 0.079), (27, -0.042), (28, -0.055), (29, 0.011), (30, 0.03), (31, 0.051), (32, 0.008), (33, 0.059), (34, -0.021), (35, 0.005), (36, -0.004), (37, 0.032), (38, -0.016), (39, 0.042), (40, 0.02), (41, -0.048), (42, -0.006), (43, -0.005), (44, 0.1), (45, 0.041), (46, 0.002), (47, -0.005), (48, 0.011), (49, 0.034)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96222752 <a title="375-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-28-Matching_for_preprocessing_data_for_causal_inference.html">375 andrew gelman stats-2010-10-28-Matching for preprocessing data for causal inference</a></p>
<p>Introduction: Chris Blattman  writes :
  
Matching is not an identification strategy a solution to your endogeneity problem; it is a weighting scheme. Saying matching will reduce endogeneity bias is like saying that the best way to get thin is to weigh yourself in kilos. The statement makes no sense. It confuses technique with substance. . . . When you run a regression, you control for the X you can observe. When you match, you are simply matching based on those same X. . . .
  
I see what Chris is getting at–matching, like regression, won’t help for the variables you’re not controlling for–but I disagree with his characterization of matching as a weighting scheme.  I see matching as a way to restrict your analysis to comparable cases.  The statistical motivation:  robustness.  If you had a good enough model, you wouldn’t neet to match, you’d just fit the model to the data.  But in common practice we often use simple regression models and so it can be helpful to do some matching first before regress</p><p>2 0.87492239 <a title="375-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-10-Matching_and_regression%3A__two_great_tastes_etc_etc.html">796 andrew gelman stats-2011-07-10-Matching and regression:  two great tastes etc etc</a></p>
<p>Introduction: Matthew Bogard writes:
  
Regarding the book Mostly Harmless Econometrics, you  state :

 
A casual reader of the book might be left with the unfortunate impression that matching is a competitor to regression rather than a tool for making regression more effective.
 

But in fact isn’t that what they are arguing, that, in a  ‘mostly harmless way’ regression is in fact a matching estimator itself?


“Our view is that regression can be motivated as a particular sort of weighted matching estimator, and therefore the differences between regression and matching estimates are unlikely to be of major empirical importance” (Chapter 3 p. 70)


They seem to be distinguishing regression (without prior matching) from all other types of matching techniques, and therefore implying that regression can be a ‘mostly harmless’ substitute or competitor to matching. My previous understanding, before starting this book was as you say, that matching is a tool that makes regression more effective.


I have n</p><p>3 0.8189984 <a title="375-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-04-What_are_the_key_assumptions_of_linear_regression%3F.html">1967 andrew gelman stats-2013-08-04-What are the key assumptions of linear regression?</a></p>
<p>Introduction: Andy Cooper writes:
  
A link to an  article , “Four Assumptions Of Multiple Regression That Researchers Should Always Test”, has been making  the rounds  on Twitter.  Their first rule is “Variables are Normally distributed.”  And they seem to be talking about the independent variables – but then later bring in tests on the residuals (while admitting that the normally-distributed error assumption is a weak assumption).  


I thought we had long-since moved away from transforming our independent variables to make them normally distributed for statistical reasons (as opposed to standardizing them for interpretability, etc.)  Am I missing something?  I agree that leverage in a influence is important, but normality of the variables? The article is from 2002, so it might be dated, but given the popularity of the tweet, I thought I’d ask your opinion.
  
My response:  There’s some useful advice on that page but overall I think the advice was dated even in 2002.  In section 3.6 of my book wit</p><p>4 0.77543819 <a title="375-lsi-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-05-What_do_practitioners_need_to_know_about_regression%3F.html">451 andrew gelman stats-2010-12-05-What do practitioners need to know about regression?</a></p>
<p>Introduction: Fabio Rojas writes:
  
 
In much of the social sciences outside economics, it’s very common for people to take a regression course or two in graduate school and then stop their statistical education. This creates a situation where you have a large pool of people who have some knowledge, but not a lot of knowledge. As a result, you have a pretty big gap between people like yourself, who are heavily invested in the cutting edge of applied statistics, and other folks.


So here is the question: What are the major lessons about good statistical practice that “rank and file” social scientists should know? Sure, most people can recite “Correlation is not causation” or “statistical significance is not substantive significance.” But what are the other big lessons?


This question comes from my own experience. I have a math degree and took regression analysis in graduate school, but I definitely do not have the level of knowledge of a statistician. I also do mixed method research, and field wor</p><p>5 0.76920342 <a title="375-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-31-Using_factor_analysis_or_principal_components_analysis_or_measurement-error_models_for_biological_measurements_in_archaeology%3F.html">1094 andrew gelman stats-2011-12-31-Using factor analysis or principal components analysis or measurement-error models for biological measurements in archaeology?</a></p>
<p>Introduction: Greg Campbell writes:
  
I am a Canadian archaeologist (BSc in Chemistry) researching the past human use of European Atlantic shellfish. After two decades of practice I am finally getting a MA in archaeology at Reading. I am seeing if the habitat or size of harvested mussels (Mytilus edulis) can be reconstructed from measurements of the umbo (the pointy end, and the only bit that survives well in archaeological deposits) using log-transformed measurements (or allometry; relationships between dimensions are more likely exponential than linear). 
Of course multivariate regressions in most statistics packages (Minitab, SPSS, SAS) assume you are trying to predict one variable from all the others (a Model I regression), and use ordinary least squares to fit the regression line. For organismal dimensions this makes little sense, since all the dimensions are (at least in theory) free to change their mutual proportions during growth. So there is no predictor and predicted, mutual variation of</p><p>6 0.76301581 <a title="375-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-01-Modeling_y_%3D_a_%2B_b_%2B_c.html">1294 andrew gelman stats-2012-05-01-Modeling y = a + b + c</a></p>
<p>7 0.7611348 <a title="375-lsi-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-02-Why_we_hate_stepwise_regression.html">2357 andrew gelman stats-2014-06-02-Why we hate stepwise regression</a></p>
<p>8 0.74733895 <a title="375-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-03-is_it_possible_to_%E2%80%9Coverstratify%E2%80%9D_when_assigning_a_treatment_in_a_randomized_control_trial%3F.html">553 andrew gelman stats-2011-02-03-is it possible to “overstratify” when assigning a treatment in a randomized control trial?</a></p>
<p>9 0.74405843 <a title="375-lsi-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Interactions_of_predictors_in_a_causal_model.html">251 andrew gelman stats-2010-09-02-Interactions of predictors in a causal model</a></p>
<p>10 0.73822963 <a title="375-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-18-Check_your_missing-data_imputations_using_cross-validation.html">1218 andrew gelman stats-2012-03-18-Check your missing-data imputations using cross-validation</a></p>
<p>11 0.7286337 <a title="375-lsi-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-14-%E2%80%9CToo_much_data%E2%80%9D%3F.html">86 andrew gelman stats-2010-06-14-“Too much data”?</a></p>
<p>12 0.72597647 <a title="375-lsi-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-01-Imputing_count_data.html">14 andrew gelman stats-2010-05-01-Imputing count data</a></p>
<p>13 0.71771979 <a title="375-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-15-Exploratory_multilevel_analysis_when_group-level_variables_are_of_importance.html">1900 andrew gelman stats-2013-06-15-Exploratory multilevel analysis when group-level variables are of importance</a></p>
<p>14 0.71530575 <a title="375-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-15-Still_more_Mr._P_in_public_health.html">770 andrew gelman stats-2011-06-15-Still more Mr. P in public health</a></p>
<p>15 0.71350467 <a title="375-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-05-Understanding_regression_models_and_regression_coefficients.html">1656 andrew gelman stats-2013-01-05-Understanding regression models and regression coefficients</a></p>
<p>16 0.71171451 <a title="375-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-09-Same_old_same_old.html">1849 andrew gelman stats-2013-05-09-Same old same old</a></p>
<p>17 0.70821911 <a title="375-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-15-R-squared_for_multilevel_models.html">1121 andrew gelman stats-2012-01-15-R-squared for multilevel models</a></p>
<p>18 0.70157009 <a title="375-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-18-Standardizing_regression_inputs.html">1462 andrew gelman stats-2012-08-18-Standardizing regression inputs</a></p>
<p>19 0.69404602 <a title="375-lsi-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Question_about_standard_range_for_social_science_correlations.html">257 andrew gelman stats-2010-09-04-Question about standard range for social science correlations</a></p>
<p>20 0.68255687 <a title="375-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Interpreting_interactions_in_discrete-data_regression.html">1908 andrew gelman stats-2013-06-21-Interpreting interactions in discrete-data regression</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.054), (9, 0.018), (16, 0.109), (21, 0.012), (24, 0.18), (49, 0.016), (54, 0.014), (72, 0.012), (73, 0.016), (85, 0.136), (97, 0.016), (99, 0.295)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96454656 <a title="375-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-28-Matching_for_preprocessing_data_for_causal_inference.html">375 andrew gelman stats-2010-10-28-Matching for preprocessing data for causal inference</a></p>
<p>Introduction: Chris Blattman  writes :
  
Matching is not an identification strategy a solution to your endogeneity problem; it is a weighting scheme. Saying matching will reduce endogeneity bias is like saying that the best way to get thin is to weigh yourself in kilos. The statement makes no sense. It confuses technique with substance. . . . When you run a regression, you control for the X you can observe. When you match, you are simply matching based on those same X. . . .
  
I see what Chris is getting at–matching, like regression, won’t help for the variables you’re not controlling for–but I disagree with his characterization of matching as a weighting scheme.  I see matching as a way to restrict your analysis to comparable cases.  The statistical motivation:  robustness.  If you had a good enough model, you wouldn’t neet to match, you’d just fit the model to the data.  But in common practice we often use simple regression models and so it can be helpful to do some matching first before regress</p><p>2 0.95646584 <a title="375-lda-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-Ticket_to_Baaaath.html">2300 andrew gelman stats-2014-04-21-Ticket to Baaaath</a></p>
<p>Introduction: Ooooooh, I never ever thought I’d have a legitimate excuse to tell this story, and now I do!  The story took place many years ago, but first I have to tell you what made me think of it:
 
Rasmus Bååth  posted  the following comment last month:
  
On airplane tickets a Swedish “å” is written as “aa” resulting in Rasmus Baaaath. Once I bought a ticket online and five minutes later a guy from Lufthansa calls me and asks if I misspelled my name…
  
OK, now here’s my story (which is not nearly as good).  A long time ago (but when I was already an adult), I was in England for some reason, and I thought I’d take a day trip from London to Bath.  So here I am on line, trying to think of what to say at the ticket counter.  I remember that in England, they call Bath, Bahth.  So, should I ask for “a ticket to Bahth”?  I’m not sure, I’m afraid that it will sound silly, like I’m trying to fake an English accent.  So, when I get to the front of the line, I say, hesitantly, “I’d like a ticket to Bath?</p><p>3 0.95340574 <a title="375-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<p>Introduction: Becky Passonneau and colleagues at the Center for Computational Learning Systems (CCLS) at Columbia have been working on a project for ConEd (New York’s major electric utility) to  rank structures based on vulnerability to secondary events  (e.g., transformer explosions, cable meltdowns, electrical fires).  They’ve been using the R implementation  BayesTree  of Chipman, George and McCulloch’s  Bayesian Additive Regression Trees  (BART).
 
BART is a Bayesian non-parametric method that is non-identifiable in two ways.  Firstly, it is an additive tree model with a fixed number of trees, the indexes of which aren’t identified (you get the same predictions in a model swapping the order of the trees).  This is the same kind of non-identifiability you get with any mixture model (additive or interpolated) with an exchangeable prior on the mixture components.  Secondly, the trees themselves have varying structure over samples in terms of number of nodes and their topology (depth, branching, etc</p><p>4 0.94760674 <a title="375-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-15-The_strange_reappearance_of_Matthew_Klam.html">1534 andrew gelman stats-2012-10-15-The strange reappearance of Matthew Klam</a></p>
<p>Introduction: A few years ago I  asked  what happened to Matthew Klam, a talented writer who has a bizarrely professional-looking webpage but didn’t seem to be writing anymore.
 
Good news!  He published  a new story  in the New Yorker!  Confusingly, he wrote it under the name “Justin Taylor,” but I’m not fooled (any more than I was fooled when that posthumous Updike story was published under the name “ Antonya Nelson “).  I’m glad to see that Klam is back in action and look forward to seeing some stories under his own name as well.</p><p>5 0.94510615 <a title="375-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-17-Clutering_and_variance_components.html">417 andrew gelman stats-2010-11-17-Clutering and variance components</a></p>
<p>Introduction: Raymond Lim writes:
  
Do you have any recommendations on clustering and binary models? My particular problem is I’m running a firm fixed effect logit and want to cluster by industry-year (every combination of industry-year). My control variable of interest in measured by industry-year and when I cluster by industry-year, the standard errors are 300x larger than when I don’t cluster. Strangely, this problem only occurs when doing logit and not OLS (linear probability). Also, clustering just by field doesn’t blow up the errors. My hunch is it has something to do with the non-nested structure of year, but I don’t understand why this is only problematic under logit and not OLS.
  
My reply:
 
I’d recommend including four multilevel variance parameters, one for firm, one for industry, one for year, and one for industry-year.  (In lmer, that’s (1 | firm) + (1 | industry) + (1 | year) + (1 | industry.year)).  No need to include (1 | firm.year) since in your data this is the error term.  Try</p><p>6 0.94334972 <a title="375-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-28-Funniest_comment_ever.html">734 andrew gelman stats-2011-05-28-Funniest comment ever</a></p>
<p>7 0.94086266 <a title="375-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-09-The_pretty_picture_is_just_the_beginning_of_the_data_exploration.__But_the_pretty_picture_is_a_great_way_to_get_started.__Another_example_of_how_a_puzzle_can_make_a_graph_appealing.html">1614 andrew gelman stats-2012-12-09-The pretty picture is just the beginning of the data exploration.  But the pretty picture is a great way to get started.  Another example of how a puzzle can make a graph appealing</a></p>
<p>8 0.93671787 <a title="375-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-27-%E2%80%9CApple_confronts_the_law_of_large_numbers%E2%80%9D_._._._huh%3F.html">1187 andrew gelman stats-2012-02-27-“Apple confronts the law of large numbers” . . . huh?</a></p>
<p>9 0.93637681 <a title="375-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-13-Secret_weapon_with_rare_events.html">610 andrew gelman stats-2011-03-13-Secret weapon with rare events</a></p>
<p>10 0.93168211 <a title="375-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-26-Teaching_evaluations%2C_instructor_effectiveness%2C_the_Journal_of_Political_Economy%2C_and_the_Holy_Roman_Empire.html">540 andrew gelman stats-2011-01-26-Teaching evaluations, instructor effectiveness, the Journal of Political Economy, and the Holy Roman Empire</a></p>
<p>11 0.93071729 <a title="375-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-07-Non-rant.html">843 andrew gelman stats-2011-08-07-Non-rant</a></p>
<p>12 0.92692918 <a title="375-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-15-n_%3D_2.html">912 andrew gelman stats-2011-09-15-n = 2</a></p>
<p>13 0.9254775 <a title="375-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-03-How_best_to_compare_effects_measured_in_two_different_time_periods%3F.html">2086 andrew gelman stats-2013-11-03-How best to compare effects measured in two different time periods?</a></p>
<p>14 0.92450655 <a title="375-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-22-%E2%80%9CAre_Wisconsin_Public_Employees_Underpaid%3F%E2%80%9D.html">584 andrew gelman stats-2011-02-22-“Are Wisconsin Public Employees Underpaid?”</a></p>
<p>15 0.92258608 <a title="375-lda-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-16-%E2%80%9CFalse-positive_psychology%E2%80%9D.html">1171 andrew gelman stats-2012-02-16-“False-positive psychology”</a></p>
<p>16 0.92252737 <a title="375-lda-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-27-Why_don%E2%80%99t_more_medical_discoveries_become_cures%3F.html">167 andrew gelman stats-2010-07-27-Why don’t more medical discoveries become cures?</a></p>
<p>17 0.92102718 <a title="375-lda-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>18 0.92023998 <a title="375-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-17-Cool_dynamic_demographic_maps_provide_beautiful_illustration_of_Chris_Rock_effect.html">2065 andrew gelman stats-2013-10-17-Cool dynamic demographic maps provide beautiful illustration of Chris Rock effect</a></p>
<p>19 0.9160291 <a title="375-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-02-Does_a_professor%E2%80%99s_intervention_in_online_discussions_have_the_effect_of_prolonging_discussion_or_cutting_it_off%3F.html">2120 andrew gelman stats-2013-12-02-Does a professor’s intervention in online discussions have the effect of prolonging discussion or cutting it off?</a></p>
<p>20 0.91545868 <a title="375-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-17-Macro_causality.html">807 andrew gelman stats-2011-07-17-Macro causality</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
