<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>341 andrew gelman stats-2010-10-14-Confusion about continuous probability densities</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-341" href="#">andrew_gelman_stats-2010-341</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>341 andrew gelman stats-2010-10-14-Confusion about continuous probability densities</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-341-html" href="http://andrewgelman.com/2010/10/14/confusion_about_3/">html</a></p><p>Introduction: I had the following email exchange with a reader of Bayesian Data Analysis.
  

 
 My correspondent wrote:   Exercise 1(b) involves evaluating the normal pdf at a single point. But p(Y=y|mu,sigma) = 0 (and is not simply N(y|mu,sigma)), since the normal distribution is continuous. So it seems that part (b) of the exercise is inappropriate.
 
The solution does actually evaluate the probability as the value of the pdf at the single point, which is wrong. The probabilities should all be 0, so the answer to (b) is undefined.
 
 I replied:   The pdf is the probability density function, which for a continuous distribution is defined as the derivative of the cumulative density function.  The notation in BDA is rigorous but we do not spell out all the details, so I can see how confusion is possible. 
 
 My correspondent:   I agree that the pdf is the derivative of the cdf. But to compute P(a .lt. Y .lt. b) for a continuous distribution (with support in the real line) requires integrating over t</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 My correspondent wrote:   Exercise 1(b) involves evaluating the normal pdf at a single point. [sent-2, score-0.876]
</p><p>2 But p(Y=y|mu,sigma) = 0 (and is not simply N(y|mu,sigma)), since the normal distribution is continuous. [sent-3, score-0.432]
</p><p>3 The solution does actually evaluate the probability as the value of the pdf at the single point, which is wrong. [sent-5, score-1.08]
</p><p>4 I replied:   The pdf is the probability density function, which for a continuous distribution is defined as the derivative of the cumulative density function. [sent-7, score-1.276]
</p><p>5 My correspondent:   I agree that the pdf is the derivative of the cdf. [sent-9, score-0.471]
</p><p>6 b) for a continuous distribution (with support in the real line) requires integrating over the pdf on the interval [a,b]. [sent-14, score-0.69]
</p><p>7 ]   So the probability of drawing exactly y from a normal distribution is 0. [sent-27, score-0.647]
</p><p>8 The probability of drawing some value around a neighborhood of y, however, is nonzero. [sent-28, score-0.499]
</p><p>9 Since in this exercise P(Y=y) appears in the denominator of the conditional probability, that probability is undefined. [sent-29, score-0.574]
</p><p>10 The mistake made in the solution is to substitutes N(1|mu, sigma2) for P(Y=1|mu,sigma2), which is incorrect. [sent-30, score-0.463]
</p><p>11 (I had to state the probability that an individual’s height was, say, 5’0″, with height a normal r. [sent-35, score-0.634]
</p><p>12 I ended up “faking” the answer by evaluating the pdf as done in the solution to the exercise here, even though the probability should have been 0. [sent-37, score-1.221]
</p><p>13 In this case, theta is a discrete random variable and y is continuous. [sent-39, score-0.342]
</p><p>14 )  If you really want, you could compute the probability that theta=1 given that |y=1| . [sent-43, score-0.358]
</p><p>15 epsilon, and then take the limit as epsilon approaches 0. [sent-45, score-0.333]
</p><p>16 I wanted only to find the solution to your problem and was stopped in my tracks by the fact that P(y=1|theta=1) is 0. [sent-47, score-0.33]
</p><p>17 Indeed, I still don’t know how to solve the exercise on this account — short of computing the limit, as you suggested. [sent-48, score-0.316]
</p><p>18 For this reason I think the solution as provided is misleading at best, or wrong at worst. [sent-49, score-0.264]
</p><p>19 Me:   If you look at the solution, I write p(y=1) etc, not Pr(y=1), so I am in fact referring to probability densities rather than discrete probabilities. [sent-50, score-0.464]
</p><p>20 (Short of “knowing” that the true solution can be obtained by making this non sequitur substitution. [sent-54, score-0.326]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pdf', 0.339), ('pr', 0.274), ('probability', 0.265), ('solution', 0.264), ('exercise', 0.251), ('epsilon', 0.237), ('correspondent', 0.219), ('theta', 0.203), ('normal', 0.155), ('integrating', 0.151), ('discrete', 0.139), ('derivative', 0.132), ('distribution', 0.116), ('mistake', 0.115), ('drawing', 0.111), ('height', 0.107), ('bda', 0.104), ('evaluating', 0.102), ('density', 0.1), ('limit', 0.096), ('compute', 0.093), ('wikipedia', 0.09), ('simply', 0.09), ('evaluate', 0.09), ('continuous', 0.084), ('substitutes', 0.084), ('defined', 0.079), ('asking', 0.079), ('since', 0.071), ('mu', 0.071), ('html', 0.069), ('misread', 0.067), ('contributors', 0.067), ('faking', 0.067), ('tracks', 0.066), ('confuse', 0.066), ('short', 0.065), ('infinity', 0.063), ('notation', 0.062), ('non', 0.062), ('spell', 0.062), ('neighborhood', 0.062), ('single', 0.061), ('value', 0.061), ('cumulative', 0.061), ('limiting', 0.06), ('densities', 0.06), ('denominator', 0.058), ('exam', 0.057), ('conditioning', 0.057)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="341-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-14-Confusion_about_continuous_probability_densities.html">341 andrew gelman stats-2010-10-14-Confusion about continuous probability densities</a></p>
<p>Introduction: I had the following email exchange with a reader of Bayesian Data Analysis.
  

 
 My correspondent wrote:   Exercise 1(b) involves evaluating the normal pdf at a single point. But p(Y=y|mu,sigma) = 0 (and is not simply N(y|mu,sigma)), since the normal distribution is continuous. So it seems that part (b) of the exercise is inappropriate.
 
The solution does actually evaluate the probability as the value of the pdf at the single point, which is wrong. The probabilities should all be 0, so the answer to (b) is undefined.
 
 I replied:   The pdf is the probability density function, which for a continuous distribution is defined as the derivative of the cumulative density function.  The notation in BDA is rigorous but we do not spell out all the details, so I can see how confusion is possible. 
 
 My correspondent:   I agree that the pdf is the derivative of the cdf. But to compute P(a .lt. Y .lt. b) for a continuous distribution (with support in the real line) requires integrating over t</p><p>2 0.18191676 <a title="341-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-13-You_heard_it_here_first%3A_Intense_exercise_can_suppress_appetite.html">2022 andrew gelman stats-2013-09-13-You heard it here first: Intense exercise can suppress appetite</a></p>
<p>Introduction: This post is by Phil Price.
 
The New York Times recently ran an article entitled  “How Exercise Can Help Us Eat Less,”  which begins with this: “Strenuous exercise seems to dull the urge to eat afterward better than gentler workouts, several new studies show, adding to a growing body of science suggesting that intense exercise may have unique benefits.” The article is based on a couple of recent studies in which moderately overweight volunteers participated in different types of exercise, and had their food intake monitored at a subsequent meal.
 
The article also says “[The volunteers] also displayed significantly lower levels of the hormone ghrelin, which is known to stimulate appetite, and elevated levels of both blood lactate and blood sugar, which have been shown to lessen the drive to eat, after the most vigorous interval session than after the other workouts. And the appetite-suppressing effect of the highly intense intervals lingered into the next day, according to food diarie</p><p>3 0.17484722 <a title="341-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-28-Path_sampling_for_models_of_varying_dimension.html">1089 andrew gelman stats-2011-12-28-Path sampling for models of varying dimension</a></p>
<p>Introduction: Somebody asks: 
  
  
Iâ&euro;&trade;m reading your paper on path sampling. It essentially solves the 
problem of computing the ratio \int q0(omega)d omega/\int q1(omega) d omega. I.e the arguments in q0() and q1() are the same. But this assumption is not always true in Bayesian model selection using Bayes factor.


In general (for BF), we have this problem, t1 and t2 may have no relation at all.


\int f1(y|t1)p1(t1) d t1 / \int f2(y|t2)p2(t2) d t2


As an example, suppose that we want to compare two sets of normally distributed data with known variance whether they have the same mean (H0) or they are not necessarily have the same mean (H1). Then the dummy variable should be mu in H0 (which is the common mean of both set of samples), and should be (mu1, mu2) (which are the means for each set of samples).


One straight method to address my problem is to preform path integration for the numerate and the denominator, as both the numerate and the denominator are integrals. Each integral can be rewrit</p><p>4 0.16270585 <a title="341-tfidf-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>Introduction: I received the following email:
  
I have an interesting thought on a prior for a logistic regression, and would love your input on how to make it “work.”


Some of my research, two published papers, are on mathematical models of **.  Along those lines, I’m interested in developing more models for **. . . .  Empirical studies show that the public is rather smart and that the wisdom-of-the-crowd is fairly accurate.


So, my thought would be to tread the public’s probability of the event as a prior, and then see how adding data, through a model, would change or perturb our inferred probability of **.  (Similarly, I could envision using previously published epidemiological research as a prior probability of a disease, and then seeing how the addition of new testing protocols would update that belief.)


However, everything I learned about hierarchical Bayesian models has a prior as a distribution on the coefficients.  I don’t know how to start with a prior point estimate for the probabili</p><p>5 0.1549069 <a title="341-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>Introduction: Nick Firoozye writes:
  
While I am absolutely sympathetic to the Bayesian agenda I am often troubled by the requirement of having priors. We must have priors on the parameter of an infinite number of model we have never seen before and I find this troubling. There is a similarly troubling problem in economics of utility theory. Utility is on consumables. To be complete a consumer must assign utility to all sorts of things they never would have encountered. More recent versions of utility theory instead make consumption goods a portfolio of attributes. Cadillacs are x many units of luxury y of transport etc etc. And we can automatically have personal utilities to all these attributes.  


I don’t ever see parameters. Some model have few and some have hundreds. Instead, I see data. So I don’t know how to have an opinion on parameters themselves. Rather I think it far more natural to have opinions on the behavior of models. The prior predictive density is a good and sensible notion. Also</p><p>6 0.14381997 <a title="341-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-16-The_%E2%80%9CWashington_read%E2%80%9D_and_the_algebra_of_conditional_distributions.html">961 andrew gelman stats-2011-10-16-The “Washington read” and the algebra of conditional distributions</a></p>
<p>7 0.13957612 <a title="341-tfidf-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-15-%3F.html">343 andrew gelman stats-2010-10-15-?</a></p>
<p>8 0.13948797 <a title="341-tfidf-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-29-ARM_solutions.html">240 andrew gelman stats-2010-08-29-ARM solutions</a></p>
<p>9 0.13367692 <a title="341-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-04-Massive_confusion_about_a_study_that_purports_to_show_that_exercise_may_increase_heart_risk.html">1364 andrew gelman stats-2012-06-04-Massive confusion about a study that purports to show that exercise may increase heart risk</a></p>
<p>10 0.12949611 <a title="341-tfidf-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-17-ff.html">418 andrew gelman stats-2010-11-17-ff</a></p>
<p>11 0.12928502 <a title="341-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-17-Jumping_off_the_edge_of_the_world.html">858 andrew gelman stats-2011-08-17-Jumping off the edge of the world</a></p>
<p>12 0.12595175 <a title="341-tfidf-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-27-Hype_about_conditional_probability_puzzles.html">54 andrew gelman stats-2010-05-27-Hype about conditional probability puzzles</a></p>
<p>13 0.12559217 <a title="341-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-13-And_now%2C_here%E2%80%99s_something_that_would_make_Ed_Tufte_spin_in_his_._._._ummm%2C_Tufte%E2%80%99s_still_around%2C_actually%2C_so_let%E2%80%99s_just_say_I_don%E2%80%99t_think_he%E2%80%99d_like_it%21.html">2132 andrew gelman stats-2013-12-13-And now, here’s something that would make Ed Tufte spin in his . . . ummm, Tufte’s still around, actually, so let’s just say I don’t think he’d like it!</a></p>
<p>14 0.12328039 <a title="341-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-23-Validation_of_Software_for_Bayesian_Models_Using_Posterior_Quantiles.html">1868 andrew gelman stats-2013-05-23-Validation of Software for Bayesian Models Using Posterior Quantiles</a></p>
<p>15 0.12281869 <a title="341-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-02-Fighting_a_losing_battle.html">1518 andrew gelman stats-2012-10-02-Fighting a losing battle</a></p>
<p>16 0.12144229 <a title="341-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>17 0.11981149 <a title="341-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-24-Untangling_the_Jeffreys-Lindley_paradox.html">1182 andrew gelman stats-2012-02-24-Untangling the Jeffreys-Lindley paradox</a></p>
<p>18 0.11798085 <a title="341-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>19 0.11757493 <a title="341-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-The_statistical_significance_filter.html">899 andrew gelman stats-2011-09-10-The statistical significance filter</a></p>
<p>20 0.11444741 <a title="341-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-26-Modeling_probability_data.html">1284 andrew gelman stats-2012-04-26-Modeling probability data</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.173), (1, 0.087), (2, 0.048), (3, 0.022), (4, 0.001), (5, -0.031), (6, 0.09), (7, 0.052), (8, -0.014), (9, -0.142), (10, -0.014), (11, -0.041), (12, -0.035), (13, -0.006), (14, -0.066), (15, -0.014), (16, 0.035), (17, -0.012), (18, 0.024), (19, -0.047), (20, 0.072), (21, 0.03), (22, 0.022), (23, -0.017), (24, 0.055), (25, 0.03), (26, 0.034), (27, 0.018), (28, 0.017), (29, -0.03), (30, -0.023), (31, 0.042), (32, -0.085), (33, 0.035), (34, -0.002), (35, -0.061), (36, 0.024), (37, 0.066), (38, -0.037), (39, -0.036), (40, 0.012), (41, -0.04), (42, 0.003), (43, -0.063), (44, -0.027), (45, -0.01), (46, 0.086), (47, 0.09), (48, -0.048), (49, -0.0)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97495019 <a title="341-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-14-Confusion_about_continuous_probability_densities.html">341 andrew gelman stats-2010-10-14-Confusion about continuous probability densities</a></p>
<p>Introduction: I had the following email exchange with a reader of Bayesian Data Analysis.
  

 
 My correspondent wrote:   Exercise 1(b) involves evaluating the normal pdf at a single point. But p(Y=y|mu,sigma) = 0 (and is not simply N(y|mu,sigma)), since the normal distribution is continuous. So it seems that part (b) of the exercise is inappropriate.
 
The solution does actually evaluate the probability as the value of the pdf at the single point, which is wrong. The probabilities should all be 0, so the answer to (b) is undefined.
 
 I replied:   The pdf is the probability density function, which for a continuous distribution is defined as the derivative of the cumulative density function.  The notation in BDA is rigorous but we do not spell out all the details, so I can see how confusion is possible. 
 
 My correspondent:   I agree that the pdf is the derivative of the cdf. But to compute P(a .lt. Y .lt. b) for a continuous distribution (with support in the real line) requires integrating over t</p><p>2 0.76000845 <a title="341-lsi-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-28-Another_argument_in_favor_of_expressing_conditional_probability_statements_using_the_population_distribution.html">56 andrew gelman stats-2010-05-28-Another argument in favor of expressing conditional probability statements using the population distribution</a></p>
<p>Introduction: Yesterday we had a spirited  discussion  of the following conditional probability puzzle:
  
“I have two children. One is a boy born on a Tuesday. What is the probability I have two boys?”
  
This reminded me of the principle, familiar from  statistics instruction  and the  cognitive psychology  literature, that the best way to teach these sorts of examples is through integers rather than fractions.
 
For example, consider this classic problem:
  
“10% of persons have disease X.  You are tested for the disease and test positive, and the test has 80% accuracy.  What is the probability that you have the disease?”
  
This can be solved directly using conditional probability but it appears to be clearer to do it using integers:
  
Start with 100 people.  10 will have the disease and 90 will not.  Of the 10 with the disease, 8 will test positive and 2 will test negative.  Of the 90 without the disease, 18 will test positive and 72% will test negative.  (72% = 0.8*90.)  So, out of the origin</p><p>3 0.70589525 <a title="341-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-10-Creating_a_good_wager_based_on_probability_estimates.html">138 andrew gelman stats-2010-07-10-Creating a good wager based on probability estimates</a></p>
<p>Introduction: Suppose you and I agree on a probability estimate…perhaps we both agree there is a 2/3 chance Spain will beat Netherlands in tomorrow’s World Cup.  In this case, we could agree on a wager: if Spain beats Netherlands, I pay you $x.  If Netherlands beats Spain, you pay me $2x.  It is easy to see that my expected loss (or win) is $0, and that the same is true for you. Either of us should be indifferent to taking this bet, and to which side of the bet we are on.  We might make this bet just to increase our interest in watching the game, but neither of us would see a money-making opportunity here. 
 
By the way, the relationship between “odds” and the event probability — a 1/3 chance of winning turning into a bet at 2:1 odds — is that if the event probability is p, then a fair bet has odds of (1/p – 1):1.  
 
More interesting, and more relevant to many real-world situations, is the case that we disagree on the probability of an event.  If we disagree on the probability, then there should be</p><p>4 0.70263672 <a title="341-lsi-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-27-Hype_about_conditional_probability_puzzles.html">54 andrew gelman stats-2010-05-27-Hype about conditional probability puzzles</a></p>
<p>Introduction: Jason Kottke  posts  this puzzle from Gary Foshee that reportedly impressed people at a puzzle-designers’ convention:
  
I have two children. One is a boy born on a Tuesday. What is the probability I have two boys?


The first thing you think is “What has Tuesday got to do with it?” Well, it has everything to do with it.
  
I thought I should really figure this one out myself before reading any further, and I decided this was a good time to apply my general principle that it’s always best to solve such problems from scratch rather than trying to guess at the answer.
  

 
So I laid out all the 4 x 49 possibilities.  The 4 is bb, bg, gb, gg, and the 49 are all possible pairs of days of the week.  Then I ruled out all the possibilities that were inconsistent with the data:  this leaves the following:
 
bb with all pairs of days that include a Tuesday.  That’s 13 possibilities (Mon/Tues, Tues/Tues, Wed/Tues, …, Tues/Mon, …, Sun/Tues, remembering not to count Tues/Tues twice). 
bg with all</p><p>5 0.69823217 <a title="341-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-28-Path_sampling_for_models_of_varying_dimension.html">1089 andrew gelman stats-2011-12-28-Path sampling for models of varying dimension</a></p>
<p>Introduction: Somebody asks: 
  
  
Iâ&euro;&trade;m reading your paper on path sampling. It essentially solves the 
problem of computing the ratio \int q0(omega)d omega/\int q1(omega) d omega. I.e the arguments in q0() and q1() are the same. But this assumption is not always true in Bayesian model selection using Bayes factor.


In general (for BF), we have this problem, t1 and t2 may have no relation at all.


\int f1(y|t1)p1(t1) d t1 / \int f2(y|t2)p2(t2) d t2


As an example, suppose that we want to compare two sets of normally distributed data with known variance whether they have the same mean (H0) or they are not necessarily have the same mean (H1). Then the dummy variable should be mu in H0 (which is the common mean of both set of samples), and should be (mu1, mu2) (which are the means for each set of samples).


One straight method to address my problem is to preform path integration for the numerate and the denominator, as both the numerate and the denominator are integrals. Each integral can be rewrit</p><p>6 0.67043585 <a title="341-lsi-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-06-Bootstrap_averaging%3A_Examples_where_it_works_and_where_it_doesn%E2%80%99t_work.html">2201 andrew gelman stats-2014-02-06-Bootstrap averaging: Examples where it works and where it doesn’t work</a></p>
<p>7 0.66966313 <a title="341-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-17-Jumping_off_the_edge_of_the_world.html">858 andrew gelman stats-2011-08-17-Jumping off the edge of the world</a></p>
<p>8 0.66507757 <a title="341-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-16-The_%E2%80%9CWashington_read%E2%80%9D_and_the_algebra_of_conditional_distributions.html">961 andrew gelman stats-2011-10-16-The “Washington read” and the algebra of conditional distributions</a></p>
<p>9 0.66330731 <a title="341-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-13-A_question_about_AIC.html">1377 andrew gelman stats-2012-06-13-A question about AIC</a></p>
<p>10 0.64786375 <a title="341-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-07-Chi-square_FAIL_when_many_cells_have_small_expected_values.html">996 andrew gelman stats-2011-11-07-Chi-square FAIL when many cells have small expected values</a></p>
<p>11 0.64685863 <a title="341-lsi-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-06-Priors_I_don%E2%80%99t_believe.html">2322 andrew gelman stats-2014-05-06-Priors I don’t believe</a></p>
<p>12 0.63392413 <a title="341-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-10-I_don%E2%80%99t_like_this_cartoon.html">1572 andrew gelman stats-2012-11-10-I don’t like this cartoon</a></p>
<p>13 0.62046146 <a title="341-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-13-And_now%2C_here%E2%80%99s_something_that_would_make_Ed_Tufte_spin_in_his_._._._ummm%2C_Tufte%E2%80%99s_still_around%2C_actually%2C_so_let%E2%80%99s_just_say_I_don%E2%80%99t_think_he%E2%80%99d_like_it%21.html">2132 andrew gelman stats-2013-12-13-And now, here’s something that would make Ed Tufte spin in his . . . ummm, Tufte’s still around, actually, so let’s just say I don’t think he’d like it!</a></p>
<p>14 0.61756313 <a title="341-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-24-What_is_the_normal_range_of_values_in_a_medical_test%3F.html">923 andrew gelman stats-2011-09-24-What is the normal range of values in a medical test?</a></p>
<p>15 0.60864121 <a title="341-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-03-Statistical_methods_that_work_in_some_settings_but_not_others.html">1560 andrew gelman stats-2012-11-03-Statistical methods that work in some settings but not others</a></p>
<p>16 0.59710336 <a title="341-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-09-How_to_model_distributions_that_have_outliers_in_one_direction.html">2128 andrew gelman stats-2013-12-09-How to model distributions that have outliers in one direction</a></p>
<p>17 0.5934118 <a title="341-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-26-Modeling_probability_data.html">1284 andrew gelman stats-2012-04-26-Modeling probability data</a></p>
<p>18 0.59226292 <a title="341-lsi-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Bayesian_Uncertainty_Quantification_for_Differential_Equations%21.html">2311 andrew gelman stats-2014-04-29-Bayesian Uncertainty Quantification for Differential Equations!</a></p>
<p>19 0.59123325 <a title="341-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-21-Fundamental_difficulty_of_inference_for_a_ratio_when_the_denominator_could_be_positive_or_negative.html">775 andrew gelman stats-2011-06-21-Fundamental difficulty of inference for a ratio when the denominator could be positive or negative</a></p>
<p>20 0.58560371 <a title="341-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-09-The_boxer%2C_the_wrestler%2C_and_the_coin_flip%2C_again.html">566 andrew gelman stats-2011-02-09-The boxer, the wrestler, and the coin flip, again</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(15, 0.034), (16, 0.019), (24, 0.194), (27, 0.168), (41, 0.011), (42, 0.011), (45, 0.018), (53, 0.035), (63, 0.013), (76, 0.015), (84, 0.014), (89, 0.023), (99, 0.323)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97719562 <a title="341-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-28-Migrating_from_dot_to_underscore.html">1472 andrew gelman stats-2012-08-28-Migrating from dot to underscore</a></p>
<p>Introduction: My C-oriented Stan collaborators have convinced me to use underscore (_) rather than dot (.) as much as possible in expressions in R.  For example, I can name a variable n_years rather than n.years.  This is fine.  But I’m getting annoyed because I need to press the shift key every time I type the underscore.
 
What do people do about this?  I know that it’s easy enough to reassign keys (I could, for example, assign underscore to backslash, which I never use).  I’m just wondering what C programmers actually do.  Do they reassign the key or do they just get used to pressing Shift?
 
P.S.  In comments, Ben Hyde  points  to Google’s R style guide, which recommends that variable names use dots,  not  underscore or camel case, for variable names (for example, “avg.clicks” rather than “avg_Clicks” or “avgClicks”).  I think they’re recommending this to be consistent with  R coding conventions .
 
I am switching to underscores in R variable names to be consistent with C.  Otherwise we were run</p><p>2 0.96865046 <a title="341-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-08-%E2%80%9CWhat_do_you_think_about_curved_lines_connecting_discrete_data-points%3F%E2%80%9D.html">134 andrew gelman stats-2010-07-08-“What do you think about curved lines connecting discrete data-points?”</a></p>
<p>Introduction: John Keltz writes:
  
What do you think about curved lines connecting discrete data-points?  (For example,  here .)


The problem with the smoothed graph is it seems to imply that something is going on in between the discrete data points, which is false. However, the straight-line version isn’t representing actual events either- it is just helping the eye connect each point. So maybe the curved version is also just helping the eye connect each point, and looks better doing it.


In my own work (value-added modeling of achievement test scores) I use straight lines, but I guess I am not too bothered when people use smoothing. I’d appreciate your input.
  
Regular readers will be unsurprised that, yes, I have an opinion on this one, and that this opinion is connected to some more general ideas about statistical graphics.
 
In general I’m not a fan of the curved lines.  They’re ok, but I don’t really see the point.  I can connect the dots just fine without the curves.
 
The more general id</p><p>3 0.96855903 <a title="341-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-13-%243M_health_care_prediction_challenge.html">465 andrew gelman stats-2010-12-13-$3M health care prediction challenge</a></p>
<p>Introduction: i received the following press release from the Heritage Provider Network, “the largest limited Knox-Keene licensed managed care organization in California.”  I have no idea what this means, but I assume it’s some sort of HMO.
 
In any case,  this looks like  it could be interesting:
  
Participants in the Health Prize challenge will be given a data set comprised of the de-identified medical records of 100,000 individuals who are members of HPN.  The teams will then need to predict the hospitalization of a set percentage of those members who went to the hospital during the year following the start date, and do so with a defined accuracy rate.  The winners will receive the $3 million prize. . . . the contest is designed to spur involvement by others involved in analytics, such as those involved in data mining and predictive modeling who may not currently be working in health care.  “We believe that doing so will bring innovative thinking to health analytics and may allow us to solve at</p><p>4 0.96354878 <a title="341-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-12-Improvement_of_5_MPG%3A_how_many_more_auto_deaths%3F.html">708 andrew gelman stats-2011-05-12-Improvement of 5 MPG: how many more auto deaths?</a></p>
<p>Introduction: This entry was posted by Phil Price. 
 
	A colleague is looking at data on car (and SUV and light truck) collisions and casualties.  He’s interested in causal relationships.  For instance, suppose car manufacturers try to improve gas mileage without decreasing acceleration.  The most likely way they will do that is to make cars lighter.  But perhaps lighter cars are more dangerous; how many more people will die for each mpg increase in gas mileage?
 
	There are a few different data sources, all of them seriously deficient from the standpoint of answering this question.  Deaths are very well reported, so if someone dies in an auto accident you can find out what kind of car they were in, what other kinds of cars (if any) were involved in the accident, whether the person was a driver or passenger, and so on.  But it’s hard to normalize:  OK, I know that N people who were passengers in a particular model of car died in car accidents last year, but I don’t know how many passenger-miles that</p><p>same-blog 5 0.96248204 <a title="341-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-14-Confusion_about_continuous_probability_densities.html">341 andrew gelman stats-2010-10-14-Confusion about continuous probability densities</a></p>
<p>Introduction: I had the following email exchange with a reader of Bayesian Data Analysis.
  

 
 My correspondent wrote:   Exercise 1(b) involves evaluating the normal pdf at a single point. But p(Y=y|mu,sigma) = 0 (and is not simply N(y|mu,sigma)), since the normal distribution is continuous. So it seems that part (b) of the exercise is inappropriate.
 
The solution does actually evaluate the probability as the value of the pdf at the single point, which is wrong. The probabilities should all be 0, so the answer to (b) is undefined.
 
 I replied:   The pdf is the probability density function, which for a continuous distribution is defined as the derivative of the cumulative density function.  The notation in BDA is rigorous but we do not spell out all the details, so I can see how confusion is possible. 
 
 My correspondent:   I agree that the pdf is the derivative of the cdf. But to compute P(a .lt. Y .lt. b) for a continuous distribution (with support in the real line) requires integrating over t</p><p>6 0.9592495 <a title="341-lda-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-15-%3F.html">343 andrew gelman stats-2010-10-15-?</a></p>
<p>7 0.95894384 <a title="341-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-28-Wiley_Wegman_chutzpah_update%3A__Now_you_too_can_buy_a_selection_of_garbled_Wikipedia_articles%2C_for_a_mere_%241400-%242800_per_year%21.html">930 andrew gelman stats-2011-09-28-Wiley Wegman chutzpah update:  Now you too can buy a selection of garbled Wikipedia articles, for a mere $1400-$2800 per year!</a></p>
<p>8 0.95159352 <a title="341-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-15-Static_sensitivity_analysis.html">804 andrew gelman stats-2011-07-15-Static sensitivity analysis</a></p>
<p>9 0.95088363 <a title="341-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-13-Super_Sam_Fuld_Needs_Your_Help_%28with_Foul_Ball_stats%29.html">802 andrew gelman stats-2011-07-13-Super Sam Fuld Needs Your Help (with Foul Ball stats)</a></p>
<p>10 0.94586015 <a title="341-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-31-Dispute_about_ethics_of_data_sharing.html">1238 andrew gelman stats-2012-03-31-Dispute about ethics of data sharing</a></p>
<p>11 0.94432259 <a title="341-lda-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-03-How_can_news_reporters_avoid_making_mistakes_when_reporting_on_technical_issues%3F__Or%2C_Data_used_to_justify_%E2%80%9CData_Used_to_Justify_Health_Savings_Can_Be_Shaky%E2%80%9D_can_be_shaky.html">66 andrew gelman stats-2010-06-03-How can news reporters avoid making mistakes when reporting on technical issues?  Or, Data used to justify “Data Used to Justify Health Savings Can Be Shaky” can be shaky</a></p>
<p>12 0.94250047 <a title="341-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-24-In_which_I_side_with_Neyman_over_Fisher.html">1869 andrew gelman stats-2013-05-24-In which I side with Neyman over Fisher</a></p>
<p>13 0.939798 <a title="341-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-15-Blaming_scientific_fraud_on_the_Kuhnians.html">1982 andrew gelman stats-2013-08-15-Blaming scientific fraud on the Kuhnians</a></p>
<p>14 0.93953061 <a title="341-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-07-Minor-league_Stats_Predict_Major-league_Performance%2C_Sarah_Palin%2C_and_Some_Differences_Between_Baseball_and_Politics.html">652 andrew gelman stats-2011-04-07-Minor-league Stats Predict Major-league Performance, Sarah Palin, and Some Differences Between Baseball and Politics</a></p>
<p>15 0.93909007 <a title="341-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-13-And_now%2C_here%E2%80%99s_something_that_would_make_Ed_Tufte_spin_in_his_._._._ummm%2C_Tufte%E2%80%99s_still_around%2C_actually%2C_so_let%E2%80%99s_just_say_I_don%E2%80%99t_think_he%E2%80%99d_like_it%21.html">2132 andrew gelman stats-2013-12-13-And now, here’s something that would make Ed Tufte spin in his . . . ummm, Tufte’s still around, actually, so let’s just say I don’t think he’d like it!</a></p>
<p>16 0.93440342 <a title="341-lda-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-31-Editing_and_clutch_hitting.html">173 andrew gelman stats-2010-07-31-Editing and clutch hitting</a></p>
<p>17 0.93376088 <a title="341-lda-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-05-Can_we_make_better_graphs_of_global_temperature_history%3F.html">2319 andrew gelman stats-2014-05-05-Can we make better graphs of global temperature history?</a></p>
<p>18 0.92632818 <a title="341-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-19-BDA3_still_%28I_hope%29_at_40%25_off%21__%28and_a_link_to_one_of_my_favorite_papers%29.html">1988 andrew gelman stats-2013-08-19-BDA3 still (I hope) at 40% off!  (and a link to one of my favorite papers)</a></p>
<p>19 0.92526507 <a title="341-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-17-Getting_arm_and_lme4_running_on_the_Mac.html">347 andrew gelman stats-2010-10-17-Getting arm and lme4 running on the Mac</a></p>
<p>20 0.92453212 <a title="341-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-27-Uncompressing_the_concept_of_compressed_sensing.html">2079 andrew gelman stats-2013-10-27-Uncompressing the concept of compressed sensing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
