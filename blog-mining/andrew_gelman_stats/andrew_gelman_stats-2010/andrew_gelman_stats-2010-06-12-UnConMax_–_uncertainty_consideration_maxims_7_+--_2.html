<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>82 andrew gelman stats-2010-06-12-UnConMax – uncertainty consideration maxims 7 +-- 2</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-82" href="#">andrew_gelman_stats-2010-82</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>82 andrew gelman stats-2010-06-12-UnConMax – uncertainty consideration maxims 7 +-- 2</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-82-html" href="http://andrewgelman.com/2010/06/12/unconmax_-_unce/">html</a></p><p>Introduction: Warning – this blog post is meant to encourage some loose, fuzzy and possibly distracting thoughts about the practice of statistics in research endeavours. There maybe spelling and grammatical errors as well as a lack of proper sentence structure. It may not be understandable to many or even possibly any readers. 
 
But somewhat more seriously, its better that “ConUnMax”
 
So far I have five maxims
 
1. Explicit models of uncertanty are useful but – always wrong and can always be made less wrong 
2. If the model is formally a probability model –  always use probability calculus (Bayes) 
3. Always useful to make the model a formal probability model – no matter what (Bayesianisn) 
4. Never use a model that is not empirically motivated and strongly empirically testable (Frequentist – of the anti-Bayesian flavour) 
5. Quantitative tools are always just a means to grasp and manipulate models – never an end in itself (i.e. don’t obsess over “baby” mathematics) 
6. If one really understood st</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Warning – this blog post is meant to encourage some loose, fuzzy and possibly distracting thoughts about the practice of statistics in research endeavours. [sent-1, score-0.97]
</p><p>2 There maybe spelling and grammatical errors as well as a lack of proper sentence structure. [sent-2, score-0.784]
</p><p>3 It may not be understandable to many or even possibly any readers. [sent-3, score-0.334]
</p><p>4 But somewhat more seriously, its better that “ConUnMax”   So far I have five maxims   1. [sent-4, score-0.386]
</p><p>5 Explicit models of uncertanty are useful but – always wrong and can always be made less wrong  2. [sent-5, score-1.087]
</p><p>6 If the model is formally a probability model –  always use probability calculus (Bayes)  3. [sent-6, score-1.43]
</p><p>7 Always useful to make the model a formal probability model – no matter what (Bayesianisn)  4. [sent-7, score-0.892]
</p><p>8 Never use a model that is not empirically motivated and strongly empirically testable (Frequentist – of the anti-Bayesian flavour)  5. [sent-8, score-1.208]
</p><p>9 Quantitative tools are always just a means to grasp and manipulate models – never an end in itself (i. [sent-9, score-1.003]
</p><p>10 If one really understood statistics, they could always successfully explain it to any zoombie   K? [sent-12, score-0.646]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('always', 0.308), ('empirically', 0.292), ('grammatical', 0.198), ('maxims', 0.198), ('model', 0.196), ('possibly', 0.192), ('probability', 0.188), ('spelling', 0.187), ('grasp', 0.168), ('testable', 0.163), ('distracting', 0.16), ('fuzzy', 0.16), ('loose', 0.151), ('manipulate', 0.151), ('calculus', 0.146), ('understandable', 0.142), ('successfully', 0.14), ('explicit', 0.135), ('warning', 0.135), ('formally', 0.131), ('useful', 0.126), ('baby', 0.123), ('wrong', 0.123), ('proper', 0.122), ('understood', 0.118), ('encourage', 0.117), ('frequentist', 0.112), ('formal', 0.111), ('mathematics', 0.109), ('never', 0.108), ('sentence', 0.106), ('meant', 0.103), ('motivated', 0.1), ('models', 0.099), ('tools', 0.098), ('quantitative', 0.096), ('somewhat', 0.094), ('five', 0.094), ('seriously', 0.093), ('lack', 0.091), ('strongly', 0.088), ('bayes', 0.087), ('statistics', 0.083), ('explain', 0.08), ('errors', 0.08), ('practice', 0.078), ('thoughts', 0.077), ('use', 0.077), ('matter', 0.075), ('means', 0.071)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="82-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-12-UnConMax_%E2%80%93_uncertainty_consideration_maxims_7_%2B--_2.html">82 andrew gelman stats-2010-06-12-UnConMax – uncertainty consideration maxims 7 +-- 2</a></p>
<p>Introduction: Warning – this blog post is meant to encourage some loose, fuzzy and possibly distracting thoughts about the practice of statistics in research endeavours. There maybe spelling and grammatical errors as well as a lack of proper sentence structure. It may not be understandable to many or even possibly any readers. 
 
But somewhat more seriously, its better that “ConUnMax”
 
So far I have five maxims
 
1. Explicit models of uncertanty are useful but – always wrong and can always be made less wrong 
2. If the model is formally a probability model –  always use probability calculus (Bayes) 
3. Always useful to make the model a formal probability model – no matter what (Bayesianisn) 
4. Never use a model that is not empirically motivated and strongly empirically testable (Frequentist – of the anti-Bayesian flavour) 
5. Quantitative tools are always just a means to grasp and manipulate models – never an end in itself (i.e. don’t obsess over “baby” mathematics) 
6. If one really understood st</p><p>2 0.14581713 <a title="82-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>Introduction: In response to  this article  by Cosma Shalizi and myself on the philosophy of Bayesian statistics, David Hogg writes:
  
I [Hogg] agree–even in physics and astronomy–that the models are not “True” in the God-like sense of being absolute reality (that is, I am not a realist); and I  have argued  (a philosophically very naive 
paper, but hey, I was new to all this) that for pretty fundamental reasons we could never arrive at the True (with a capital “T”) model of the Universe.  The goal of inference is to find the “best” model, where “best” might have something to do with prediction, or explanation, or message length, or (horror!) our utility.  Needless to say, most of my physics friends *are* realists, even in the face of “effective theories” as Newtonian mechanics is an effective theory of GR and GR is an effective theory of “quantum gravity” (this plays to your point, because if you think any theory is possibly an effective theory, how could you ever find Truth?).  I also liked the i</p><p>3 0.14082231 <a title="82-tfidf-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-22-Philosophy_of_Bayes_and_non-Bayes%3A__A_dialogue_with_Deborah_Mayo.html">291 andrew gelman stats-2010-09-22-Philosophy of Bayes and non-Bayes:  A dialogue with Deborah Mayo</a></p>
<p>Introduction: I sent Deborah Mayo a link to  my paper  with Cosma Shalizi on the philosophy of statistics, and she sent me the link to this conference which unfortunately already occurred.  (It’s too bad, because I’d have liked to have been there.)  I summarized my philosophy as follows:
  
I am highly sympathetic to the approach of Lakatos (or of Popper, if you consider Lakatos’s “Popper_2″ to be a reasonable simulation of the true Popperism), in that (a) I view statistical models as being built within theoretical structures, and (b) I see the checking and refutation of models to be a key part of scientific progress.  A big problem I have with mainstream Bayesianism is its “inductivist” view that science can operate completely smoothly with posterior updates:  the idea that new data causes us to increase the posterior probability of good models and decrease the posterior probability of bad models.  I don’t buy that:  I see models as ever-changing entities that are flexible and can be patched and ex</p><p>4 0.13401856 <a title="82-tfidf-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Useful_models%2C_model_checking%2C_and_external_validation%3A__a_mini-discussion.html">244 andrew gelman stats-2010-08-30-Useful models, model checking, and external validation:  a mini-discussion</a></p>
<p>Introduction: I sent a copy of my paper (coauthored with Cosma Shalizi) on  Philosophy and the practice of Bayesian statistics in the social sciences  to  Richard Berk , who wrote:
  
I read your paper this morning. I think we are pretty much on the same page about all models being wrong. I like very much the way you handle this in the paper. Yes, Newton’s work is wrong, but surely useful. I also like your twist on Bayesian methods. Makes good sense to me. Perhaps most important, your paper raises some difficult issues I have been trying to think more carefully about.


1. If the goal of a model is to be useful, surely we need to explore that “useful” means. At the very least, usefulness will depend on use. So a model that is useful for forecasting may or may not be useful for causal inference.


2. Usefulness will be a matter of degree. So that for each use we will need one or more metrics to represent how useful the model is. In what looks at first to be simple example, if the use is forecasting,</p><p>5 0.13394901 <a title="82-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>Introduction: Nick Firoozye writes:
  
I had a question about BMA [Bayesian model averaging] and model combinations in general, and direct it to you since they are a basic form of hierarchical model, albeit in the simplest of forms. I wanted to ask what the underlying assumptions are that could lead to BMA improving on a larger model. 


I know model combination is a topic of interest in the (frequentist) econometrics community (e.g., Bates & Granger, http://www.jstor.org/discover/10.2307/3008764?uid=3738032&uid;=2&uid;=4&sid;=21101948653381) but at the time it was considered a bit of a puzzle. Perhaps small models combined outperform a big model due to standard errors, insufficient data, etc. But I haven’t seen much in way of Bayesian justification.


In simplest terms, you might have a joint density P(Y,theta_1,theta_2) from which you could use the two marginals P(Y,theta_1) and P(Y,theta_2) to derive two separate forecasts. A BMA-er would do a weighted average of the two forecast densities, having p</p><p>6 0.12070753 <a title="82-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-21-More_on_Bayesian_model_selection_in_high-dimensional_settings.html">1817 andrew gelman stats-2013-04-21-More on Bayesian model selection in high-dimensional settings</a></p>
<p>7 0.11166614 <a title="82-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>8 0.10803779 <a title="82-tfidf-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-05-Does_posterior_predictive_model_checking_fit_with_the_operational_subjective_approach%3F.html">320 andrew gelman stats-2010-10-05-Does posterior predictive model checking fit with the operational subjective approach?</a></p>
<p>9 0.10785605 <a title="82-tfidf-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-22-Spell-checking_example_demonstrates_key_aspects_of_Bayesian_data_analysis.html">2182 andrew gelman stats-2014-01-22-Spell-checking example demonstrates key aspects of Bayesian data analysis</a></p>
<p>10 0.10477727 <a title="82-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>11 0.10318739 <a title="82-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-15-Wacky_priors_can_work_well%3F.html">1723 andrew gelman stats-2013-02-15-Wacky priors can work well?</a></p>
<p>12 0.10285455 <a title="82-tfidf-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-09-Popper%E2%80%99s_great%2C_but_don%E2%80%99t_bother_with_his_theory_of_probability.html">23 andrew gelman stats-2010-05-09-Popper’s great, but don’t bother with his theory of probability</a></p>
<p>13 0.10168793 <a title="82-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>14 0.1013381 <a title="82-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-07-Philosophy_and_the_practice_of_Bayesian_statistics_%28with_all_the_discussions%21%29.html">1712 andrew gelman stats-2013-02-07-Philosophy and the practice of Bayesian statistics (with all the discussions!)</a></p>
<p>15 0.098542452 <a title="82-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>16 0.094852284 <a title="82-tfidf-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>17 0.094805196 <a title="82-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-%E2%80%9CAll_Models_are_Right%2C_Most_are_Useless%E2%80%9D.html">1197 andrew gelman stats-2012-03-04-“All Models are Right, Most are Useless”</a></p>
<p>18 0.094509028 <a title="82-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-08-The_never-ending_%28and_often_productive%29_race_between_theory_and_practice.html">2127 andrew gelman stats-2013-12-08-The never-ending (and often productive) race between theory and practice</a></p>
<p>19 0.094377786 <a title="82-tfidf-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-10-Bayes_jumps_the_shark.html">331 andrew gelman stats-2010-10-10-Bayes jumps the shark</a></p>
<p>20 0.093995787 <a title="82-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-13-Flexibility_is_good.html">2133 andrew gelman stats-2013-12-13-Flexibility is good</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.159), (1, 0.1), (2, -0.03), (3, 0.05), (4, -0.023), (5, 0.019), (6, 0.002), (7, 0.015), (8, 0.085), (9, -0.015), (10, 0.013), (11, 0.053), (12, -0.061), (13, -0.034), (14, -0.094), (15, 0.003), (16, 0.003), (17, 0.0), (18, -0.004), (19, -0.027), (20, 0.005), (21, -0.034), (22, -0.057), (23, -0.028), (24, -0.049), (25, 0.002), (26, -0.016), (27, 0.035), (28, -0.007), (29, -0.05), (30, -0.021), (31, 0.023), (32, -0.052), (33, 0.008), (34, -0.036), (35, -0.017), (36, 0.042), (37, -0.017), (38, -0.02), (39, -0.009), (40, -0.022), (41, -0.075), (42, 0.026), (43, -0.014), (44, 0.034), (45, 0.008), (46, -0.035), (47, 0.014), (48, 0.016), (49, -0.022)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98361301 <a title="82-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-12-UnConMax_%E2%80%93_uncertainty_consideration_maxims_7_%2B--_2.html">82 andrew gelman stats-2010-06-12-UnConMax – uncertainty consideration maxims 7 +-- 2</a></p>
<p>Introduction: Warning – this blog post is meant to encourage some loose, fuzzy and possibly distracting thoughts about the practice of statistics in research endeavours. There maybe spelling and grammatical errors as well as a lack of proper sentence structure. It may not be understandable to many or even possibly any readers. 
 
But somewhat more seriously, its better that “ConUnMax”
 
So far I have five maxims
 
1. Explicit models of uncertanty are useful but – always wrong and can always be made less wrong 
2. If the model is formally a probability model –  always use probability calculus (Bayes) 
3. Always useful to make the model a formal probability model – no matter what (Bayesianisn) 
4. Never use a model that is not empirically motivated and strongly empirically testable (Frequentist – of the anti-Bayesian flavour) 
5. Quantitative tools are always just a means to grasp and manipulate models – never an end in itself (i.e. don’t obsess over “baby” mathematics) 
6. If one really understood st</p><p>2 0.83791012 <a title="82-lsi-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-19-The_%E2%80%9Ceither-or%E2%80%9D_fallacy_of_believing_in_discrete_models%3A__an_example_of_folk_statistics.html">217 andrew gelman stats-2010-08-19-The “either-or” fallacy of believing in discrete models:  an example of folk statistics</a></p>
<p>Introduction: Psychologists talk about “folk psychology”:  ideas that make sense to us about how people think and behave, even if these ideas are not accurate descriptions of reality.  And physicists talk about “folk physics” (for example, the idea that a thrown ball falls in a straight line and then suddenly drops, rather than following an approximate parabola).
 
There’s also “folk statistics.”  Some of the ideas of folk statistics are so strong that even educated people–even well-known researchers–can make these mistakes.
 
One of the ideas of folk statistics that bothers me a lot is what might be called the “either/or fallacy”:  the idea that if there are two possible stories, the truth has to be one or the other.
 
I have often encountered the either/or fallacy in Bayesian statistics, for example the vast literature on “model selection” or “variable selection” or “model averaging” in which it is assumed that one of some pre-specified discrete set of models is the truth, and that this true model</p><p>3 0.82642281 <a title="82-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-21-More_on_Bayesian_model_selection_in_high-dimensional_settings.html">1817 andrew gelman stats-2013-04-21-More on Bayesian model selection in high-dimensional settings</a></p>
<p>Introduction: David Rossell writes:
  
A friend pointed out that you were having an interesting philosophical  discussion  on my paper with Val Johnson [on Bayesian model selection in high-dimensional settings].


I agree with the view that in almost all practical situations the true model is not in the set under consideration. Still, asking a model choice procedure to be able to pick up the correct model when it is in the set under consideration seems a minimal requirement (though perhaps not sufficient). In other words, if a procedure is unable to pick the data-generating model even when it is one of the models under consideration, I don’t have high hopes for it working well in more realistic scenarios either. 


Most results in the history in statistics seem to have been obtained under an assumed model, e.g. why even do MLE or penalized-likelihood if we don’t trust the model. While unrealistic, these results were useful to help understand important basic principles. In our case Val and I are defe</p><p>4 0.80944711 <a title="82-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>Introduction: In response to  this article  by Cosma Shalizi and myself on the philosophy of Bayesian statistics, David Hogg writes:
  
I [Hogg] agree–even in physics and astronomy–that the models are not “True” in the God-like sense of being absolute reality (that is, I am not a realist); and I  have argued  (a philosophically very naive 
paper, but hey, I was new to all this) that for pretty fundamental reasons we could never arrive at the True (with a capital “T”) model of the Universe.  The goal of inference is to find the “best” model, where “best” might have something to do with prediction, or explanation, or message length, or (horror!) our utility.  Needless to say, most of my physics friends *are* realists, even in the face of “effective theories” as Newtonian mechanics is an effective theory of GR and GR is an effective theory of “quantum gravity” (this plays to your point, because if you think any theory is possibly an effective theory, how could you ever find Truth?).  I also liked the i</p><p>5 0.80651218 <a title="82-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-03-Popper_and_Jaynes.html">2007 andrew gelman stats-2013-09-03-Popper and Jaynes</a></p>
<p>Introduction: Deborah Mayo quotes me as saying, “Popper has argued (convincingly, in my opinion) that scientific inference is not inductive but deductive.”  She then  follows up  with:
  
Gelman employs significance test-type reasoning to reject a model when the data sufficiently disagree.


Now, strictly speaking, a model falsification, even to inferring something as weak as “the model breaks down,” is not purely deductive, but Gelman is right to see it as about as close as one can get, in statistics, to a deductive falsification of a model. But where does that leave him as a Jaynesian?
  
My reply:
 
I was influenced by reading a toy example from Jaynes’s book where he sets up a model (for the probability of a die landing on each of its six sides) based on first principles, then presents some data that contradict the model, then expands the model.
 
I’d seen very little of this sort of this reasoning before in statistics!  In physics it’s the standard way to go:  you set up a model based on physic</p><p>6 0.80594879 <a title="82-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>7 0.79401404 <a title="82-lsi-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-05-Does_posterior_predictive_model_checking_fit_with_the_operational_subjective_approach%3F.html">320 andrew gelman stats-2010-10-05-Does posterior predictive model checking fit with the operational subjective approach?</a></p>
<p>8 0.79226601 <a title="82-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-15-Induction_within_a_model%2C_deductive_inference_for_model_evaluation.html">614 andrew gelman stats-2011-03-15-Induction within a model, deductive inference for model evaluation</a></p>
<p>9 0.77322775 <a title="82-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-18-Understanding_posterior_p-values.html">2029 andrew gelman stats-2013-09-18-Understanding posterior p-values</a></p>
<p>10 0.76725012 <a title="82-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-15-Wacky_priors_can_work_well%3F.html">1723 andrew gelman stats-2013-02-15-Wacky priors can work well?</a></p>
<p>11 0.76322991 <a title="82-lsi-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-16-Wanted%3A__Probability_distributions_for_rank_orderings.html">151 andrew gelman stats-2010-07-16-Wanted:  Probability distributions for rank orderings</a></p>
<p>12 0.75265211 <a title="82-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-04-David_MacKay_and_Occam%E2%80%99s_Razor.html">1041 andrew gelman stats-2011-12-04-David MacKay and Occam’s Razor</a></p>
<p>13 0.74837953 <a title="82-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-26-Modeling_probability_data.html">1284 andrew gelman stats-2012-04-26-Modeling probability data</a></p>
<p>14 0.74758422 <a title="82-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-11-Adding_an_error_model_to_a_deterministic_model.html">1162 andrew gelman stats-2012-02-11-Adding an error model to a deterministic model</a></p>
<p>15 0.74051565 <a title="82-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-13-Flexibility_is_good.html">2133 andrew gelman stats-2013-12-13-Flexibility is good</a></p>
<p>16 0.73797375 <a title="82-lsi-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-03-This_is_a_footnote_in_one_of_my_papers.html">448 andrew gelman stats-2010-12-03-This is a footnote in one of my papers</a></p>
<p>17 0.73399574 <a title="82-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-01-Tukey%E2%80%99s_philosophy.html">496 andrew gelman stats-2011-01-01-Tukey’s philosophy</a></p>
<p>18 0.73189348 <a title="82-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-30-David_Hogg_on_statistics.html">1401 andrew gelman stats-2012-06-30-David Hogg on statistics</a></p>
<p>19 0.7318908 <a title="82-lsi-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-07-Valencia%3A___Summer_of_1991.html">72 andrew gelman stats-2010-06-07-Valencia:   Summer of 1991</a></p>
<p>20 0.73110843 <a title="82-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(15, 0.022), (16, 0.034), (18, 0.021), (21, 0.02), (24, 0.139), (31, 0.018), (52, 0.14), (57, 0.021), (84, 0.018), (86, 0.025), (94, 0.024), (95, 0.024), (98, 0.034), (99, 0.356)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9771769 <a title="82-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-12-Elderpedia.html">1531 andrew gelman stats-2012-10-12-Elderpedia</a></p>
<p>Introduction: It’s good to remember that  wikis  aren’t just for looking up Dylan lyrics and the plots of old Three’s Company episodes.</p><p>2 0.97333688 <a title="82-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-22-Seeking_balance.html">104 andrew gelman stats-2010-06-22-Seeking balance</a></p>
<p>Introduction: I’m trying to temporarily kick the blogging habit as I seem to be addicted.  I’m currently on a binge and my plan is to schedule a bunch of already-written entries at one per weekday and not blog anything new for awhile.
 
Yesterday I fell off the wagon and posted 4 items, but maybe now I can show some restraint.
 
P.S.  In keeping with the spirit of this blog, I scheduled it to appear on 13 May, even though I wrote it on 15 Apr.  Just about everything you’ve been reading on this blog for the past several weeks (and lots of forthcoming items) were written a month ago.  The only exceptions are whatever my cobloggers have been posting and various items that were timely enough that I inserted them in the queue afterward.
 
P.P.S  I bumped it up to 22 Jun because, as of 14 Apr, I was continuing to write new entries.  I hope to slow down soon!
 
P.P.P.S. (20 June) I was going to bump it up again–the horizon’s now in mid-July–but I thought, enough is enough!
 
Right now I think that about ha</p><p>3 0.97322941 <a title="82-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-04-The_acupuncture_paradox.html">889 andrew gelman stats-2011-09-04-The acupuncture paradox</a></p>
<p>Introduction: The scientific consensus  appears to be  that, to the extent that acupuncture makes people feel better, it is through relaxing the patient, also the acupuncturist might help in other ways, encouraging the patient to focus on his or her lifestyle.
 
A friend recommended an acupuncturist to me awhile ago and I told her the above line, to which she replied:  No, I don’t feel at all relaxed when I go to the acupuncturist.  Those needles really hurt!
 
I don’t know anything about this, but one thing I do know is that whenever I discuss the topic with a Chinese friend, they assure me that acupuncture is real.  Real real.  Not “yeah, it works by calming people” real or “patients respond to a doctor who actually cares about them” real.  Real real.  The needles, the special places to put the needles, the whole thing.  I haven’t had a long discussion on this, but my impression is that Chinese people think of acupuncture as working in the same way that we think of TV’s or cars or refrigerators:</p><p>4 0.97296286 <a title="82-lda-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-26-%E2%80%9CThe_Inside_Story_Of_The_Harvard_Dissertation_That_Became_Too_Racist_For_Heritage%E2%80%9D.html">1957 andrew gelman stats-2013-07-26-“The Inside Story Of The Harvard Dissertation That Became Too Racist For Heritage”</a></p>
<p>Introduction: Mark Palko points me to a  news article  by Zack Beauchamp on Jason Richwine, the recent Ph.D. graduate from Harvard’s policy school who left the conservative Heritage Foundation after it came out that his Ph.D. thesis was said to be all  about  the low IQ’s of Hispanic immigrants.  Heritage and  others  apparently thought this association could discredit their anti-immigration-reform position.  Richwine’s mentor Charles Murray was  unhappy  about the whole episode.
 
Beauchamp’s article is worth reading in that it provides some interesting background, in particular by getting into the details of the Ph.D. review process.  In a sense, Beauchamp is too harsh.  Flawed Ph.D. theses get published all the time.  I’d say that  most  Ph.D. theses I’ve seen are flawed:  usually the plan is to get the papers into shape later, when submitting them to journals.  If a student doesn’t go into academia, the thesis typically just sits there and is rarely followed up on.  I don’t know the statistics o</p><p>5 0.96881771 <a title="82-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-21-Statoverflow.html">223 andrew gelman stats-2010-08-21-Statoverflow</a></p>
<p>Introduction: Skirant Vadali writes:
  
 
I am writing to seek your help in building a community driven Q&A; website tentatively called called ‘Statistics Analysis’. I am neither a founder of this website nor do I have any financial stake in its success. 


By way of background to this website, please see Stackoverflow (http://stackoverflow.com/) and Mathoverflow (http://mathoverflow.net/). Stackoverflow is a Q&A; website targeted at software developers and is designed to help them ask questions and get answers from other developers.  Mathoverflow is a Q&A; website targeted at research mathematicians and is designed to help them ask and answer questions from other mathematicians across the world. The success of both these sites in helping their respective communities is a strong indicator that sites designed along these lines are very useful.


The company that runs Stackoverflow (who also host Mathoverflow.net) has recently decided to develop other community driven websites for various other topic are</p><p>6 0.96561027 <a title="82-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-04-Questions_about_quantum_computing.html">786 andrew gelman stats-2011-07-04-Questions about quantum computing</a></p>
<p>7 0.96536577 <a title="82-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-06-Your_conclusion_is_only_as_good_as_your_data.html">1369 andrew gelman stats-2012-06-06-Your conclusion is only as good as your data</a></p>
<p>same-blog 8 0.96503347 <a title="82-lda-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-12-UnConMax_%E2%80%93_uncertainty_consideration_maxims_7_%2B--_2.html">82 andrew gelman stats-2010-06-12-UnConMax – uncertainty consideration maxims 7 +-- 2</a></p>
<p>9 0.963669 <a title="82-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-21-Finite-population_Anova_calculations_for_models_with_interactions.html">1686 andrew gelman stats-2013-01-21-Finite-population Anova calculations for models with interactions</a></p>
<p>10 0.95930314 <a title="82-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-20-No_no_no_no_no.html">1020 andrew gelman stats-2011-11-20-No no no no no</a></p>
<p>11 0.9586091 <a title="82-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-10-Combining_data_from_many_sources.html">948 andrew gelman stats-2011-10-10-Combining data from many sources</a></p>
<p>12 0.95393682 <a title="82-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-05-Related_to_z-statistics.html">1301 andrew gelman stats-2012-05-05-Related to z-statistics</a></p>
<p>13 0.95014846 <a title="82-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-08-Multilevel_regression_with_shrinkage_for_%E2%80%9Cfixed%E2%80%9D_effects.html">653 andrew gelman stats-2011-04-08-Multilevel regression with shrinkage for “fixed” effects</a></p>
<p>14 0.94830465 <a title="82-lda-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-24-On_deck_this_week.html">2265 andrew gelman stats-2014-03-24-On deck this week</a></p>
<p>15 0.94740939 <a title="82-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-11-Separating_national_and_state_swings_in_voting_and_public_opinion%2C_or%2C_How_I_avoided_blogorific_embarrassment%3A__An_agony_in_four_acts.html">200 andrew gelman stats-2010-08-11-Separating national and state swings in voting and public opinion, or, How I avoided blogorific embarrassment:  An agony in four acts</a></p>
<p>16 0.94445407 <a title="82-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-23-No_one_knows_what_it%E2%80%99s_like_to_be_the_bad_man.html">1588 andrew gelman stats-2012-11-23-No one knows what it’s like to be the bad man</a></p>
<p>17 0.9412778 <a title="82-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-07-Feedback_on_my_Bayesian_Data_Analysis_class_at_Columbia.html">1611 andrew gelman stats-2012-12-07-Feedback on my Bayesian Data Analysis class at Columbia</a></p>
<p>18 0.94053221 <a title="82-lda-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-25-Unlogging.html">485 andrew gelman stats-2010-12-25-Unlogging</a></p>
<p>19 0.93903518 <a title="82-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-27-Setting_up_Jitts_online.html">2041 andrew gelman stats-2013-09-27-Setting up Jitts online</a></p>
<p>20 0.93507594 <a title="82-lda-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-10-Spring_forward%2C_fall_back%2C_drop_dead%3F.html">2367 andrew gelman stats-2014-06-10-Spring forward, fall back, drop dead?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
