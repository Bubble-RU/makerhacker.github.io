<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>85 andrew gelman stats-2010-06-14-Prior distribution for design effects</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-85" href="#">andrew_gelman_stats-2010-85</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>85 andrew gelman stats-2010-06-14-Prior distribution for design effects</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-85-html" href="http://andrewgelman.com/2010/06/14/prior_distribut_4/">html</a></p><p>Introduction: David Shor writes:
  
I’m fitting a state-space model right now that estimates the “design effect” of individual pollsters (Ratio of poll variance to that predicted by perfect random sampling). What would be a good prior distribution for that?
  
My quickest suggestion is start with something simple, such as a uniform from 1 to 10, and then to move to something hierarchical, such as a lognormal on (design.effect – 1), with the hyperparameters estimated from data.
 
My longer suggestion is to take things apart.  What exactly do you mean by “design effect”?  There are lots of things going on, both in sampling error (the classical “design effect” that comes from cluster sampling, stratification, weighting, etc.) and nonsampling error (nonresponse bias, likeliy voter screening, bad questions, etc.)  It would be best if you could model both pieces.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 David Shor writes:    I’m fitting a state-space model right now that estimates the “design effect” of individual pollsters (Ratio of poll variance to that predicted by perfect random sampling). [sent-1, score-1.065]
</p><p>2 What would be a good prior distribution for that? [sent-2, score-0.153]
</p><p>3 My quickest suggestion is start with something simple, such as a uniform from 1 to 10, and then to move to something hierarchical, such as a lognormal on (design. [sent-3, score-1.116]
</p><p>4 There are lots of things going on, both in sampling error (the classical “design effect” that comes from cluster sampling, stratification, weighting, etc. [sent-7, score-0.936]
</p><p>5 ) and nonsampling error (nonresponse bias, likeliy voter screening, bad questions, etc. [sent-8, score-0.812]
</p><p>6 )  It would be best if you could model both pieces. [sent-9, score-0.148]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sampling', 0.289), ('design', 0.272), ('suggestion', 0.247), ('likeliy', 0.223), ('nonsampling', 0.223), ('quickest', 0.21), ('effect', 0.198), ('hyperparameters', 0.188), ('lognormal', 0.188), ('stratification', 0.176), ('pollsters', 0.172), ('shor', 0.169), ('screening', 0.169), ('nonresponse', 0.162), ('error', 0.157), ('cluster', 0.149), ('voter', 0.143), ('weighting', 0.138), ('ratio', 0.134), ('uniform', 0.132), ('pieces', 0.129), ('poll', 0.126), ('predicted', 0.119), ('perfect', 0.111), ('longer', 0.105), ('fitting', 0.104), ('classical', 0.103), ('variance', 0.101), ('bias', 0.1), ('things', 0.1), ('estimated', 0.098), ('hierarchical', 0.096), ('move', 0.093), ('model', 0.088), ('something', 0.086), ('exactly', 0.086), ('random', 0.085), ('individual', 0.082), ('david', 0.082), ('distribution', 0.077), ('estimates', 0.077), ('prior', 0.076), ('questions', 0.075), ('start', 0.074), ('comes', 0.073), ('simple', 0.069), ('bad', 0.066), ('lots', 0.065), ('mean', 0.063), ('best', 0.06)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="85-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-14-Prior_distribution_for_design_effects.html">85 andrew gelman stats-2010-06-14-Prior distribution for design effects</a></p>
<p>Introduction: David Shor writes:
  
I’m fitting a state-space model right now that estimates the “design effect” of individual pollsters (Ratio of poll variance to that predicted by perfect random sampling). What would be a good prior distribution for that?
  
My quickest suggestion is start with something simple, such as a uniform from 1 to 10, and then to move to something hierarchical, such as a lognormal on (design.effect – 1), with the hyperparameters estimated from data.
 
My longer suggestion is to take things apart.  What exactly do you mean by “design effect”?  There are lots of things going on, both in sampling error (the classical “design effect” that comes from cluster sampling, stratification, weighting, etc.) and nonsampling error (nonresponse bias, likeliy voter screening, bad questions, etc.)  It would be best if you could model both pieces.</p><p>2 0.1808129 <a title="85-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-12-Probabilistic_screening_to_get_an_approximate_self-weighted_sample.html">1455 andrew gelman stats-2012-08-12-Probabilistic screening to get an approximate self-weighted sample</a></p>
<p>Introduction: Sharad had a survey sampling question:
  
We’re trying to use mechanical turk to conduct some surveys, and have quickly discovered that turkers tend to be quite young. We’d really like a representative sample of the U.S., or at the least be able to recruit a diverse enough sample from turk that we can post-stratify to adjust the estimates. The approach we ended up taking is to pay turkers a small amount to answer a couple of screening questions (age & sex), and then probabilistically recruit individuals to complete the full survey (for more money) based on the estimated turk population parameters and our desired target distribution. We use rejection sampling, so the end result is that individuals who are invited to take the full survey look as if they came from a representative sample, at least in terms of age and sex. I’m wondering whether this sort of technique—a two step design in which participants are first screened and then probabilistically selected to mimic a target distributio</p><p>3 0.17148858 <a title="85-tfidf-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Blending_results_from_two_relatively_independent_multi-level_models.html">250 andrew gelman stats-2010-09-02-Blending results from two relatively independent multi-level models</a></p>
<p>Introduction: David Shor writes:
  
 
I [Shor] am working on a Bayesian Forecasting model for the Mid-term elections that has two components:


1) A poll aggregation system with pooled and hierarchical house and design effects across every race with polls (Average Standard error for house seat level vote-share ~.055)


2) A Bafumi-style regression that applies national-swing to individual seats. (Average Standard error for house seat level vote-share ~.06)


Since these two estimates are essentially independent, estimates can probably be made more accurate by pooling them together. But If a house effect changes in one draw, that changes estimates in every race. Changes in regression coefficients and National swing have a similar effect.  In the face of high and possibly differing seat-to-seat correlations from each method, I’m not sure what the correct way to “blend” these models would be, either for individual or top-line seat estimates.


In the mean-time, I’m just creating variance-weighted avera</p><p>4 0.16669385 <a title="85-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>Introduction: Lots of good statistical methods make use of two models.  For example:
 
- Classical statistics:  estimates and standard errors using the likelihood function; tests and p-values using the sampling distribution.  (The sampling distribution is  not  equivalent to the likelihood, as has been much discussed, for example in sequential stopping problems.)
 
- Bayesian data analysis:  inference using the posterior distribution; model checking using the predictive distribution (which, again, depends on the data-generating process in a way that the likelihood does not).
 
- Machine learning:  estimation using the data; evaluation using cross-validation (which requires some rule for partitioning the data, a rule that stands outside of the data themselves).
 
- Bootstrap, jackknife, etc:  estimation using an “estimator” (which, I would argue, is based in some sense on a model for the data), uncertainties using resampling (which, I would argue, is close to the idea of a “sampling distribution” in</p><p>5 0.15781507 <a title="85-tfidf-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-24-PPS_in_Georgia.html">107 andrew gelman stats-2010-06-24-PPS in Georgia</a></p>
<p>Introduction: Lucy Flynn writes:
  
I’m working at a non-profit organization called CRRC in the Republic of Georgia.
      
I’m having a methodological problem and I saw the syllabus for your sampling class online and thought I might be able to ask you about it?


We do a lot of complex surveys nationwide; our typical sample design is as follows:


- stratify by rural/urban/capital 
- sub-stratify the rural and urban strata into NE/NW/SE/SW geographic quadrants 
- select voting precincts as PSUs 
- select households as SSUs 
- select individual respondents as TSUs


I’m relatively new here, and past practice has been to sample voting precincts with probability proportional to size.  It’s desirable because it’s not logistically feasible for us to vary the number of interviews per precinct with precinct size, so it makes the selection probabilities for households more even across precinct sizes. However, I have a complex sampling textbook (Lohr 1999), and it explains how complex it is to calculate sel</p><p>6 0.15217531 <a title="85-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-17-Statistics_in_a_world_where_nothing_is_random.html">1628 andrew gelman stats-2012-12-17-Statistics in a world where nothing is random</a></p>
<p>7 0.14715935 <a title="85-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-14-Subtleties_with_measurement-error_models_for_the_evaluation_of_wacky_claims.html">803 andrew gelman stats-2011-07-14-Subtleties with measurement-error models for the evaluation of wacky claims</a></p>
<p>8 0.13556182 <a title="85-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-16-Question_6_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1323 andrew gelman stats-2012-05-16-Question 6 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>9 0.13516052 <a title="85-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-29-How_many_parameters_are_in_a_multilevel_model%3F.html">1144 andrew gelman stats-2012-01-29-How many parameters are in a multilevel model?</a></p>
<p>10 0.13447505 <a title="85-tfidf-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-19-Analysis_of_survey_data%3A_Design_based_models_vs._hierarchical_modeling%3F.html">352 andrew gelman stats-2010-10-19-Analysis of survey data: Design based models vs. hierarchical modeling?</a></p>
<p>11 0.12454726 <a title="85-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-07-Question_28_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1371 andrew gelman stats-2012-06-07-Question 28 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>12 0.12383892 <a title="85-tfidf-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-04-All_the_Assumptions_That_Are_My_Life.html">2359 andrew gelman stats-2014-06-04-All the Assumptions That Are My Life</a></p>
<p>13 0.11998323 <a title="85-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>14 0.11966085 <a title="85-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-25-Bayes-respecting_experimental_design_and_other_things.html">1955 andrew gelman stats-2013-07-25-Bayes-respecting experimental design and other things</a></p>
<p>15 0.11831184 <a title="85-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-14-Progress%21__%28on_the_understanding_of_the_role_of_randomization_in_Bayesian_inference%29.html">1898 andrew gelman stats-2013-06-14-Progress!  (on the understanding of the role of randomization in Bayesian inference)</a></p>
<p>16 0.11802901 <a title="85-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-28-Using_randomized_incentives_as_an_instrument_for_survey_nonresponse%3F.html">2152 andrew gelman stats-2013-12-28-Using randomized incentives as an instrument for survey nonresponse?</a></p>
<p>17 0.11389259 <a title="85-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-06-Question_27_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1368 andrew gelman stats-2012-06-06-Question 27 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>18 0.11291666 <a title="85-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-15-The_bias-variance_tradeoff.html">960 andrew gelman stats-2011-10-15-The bias-variance tradeoff</a></p>
<p>19 0.11158495 <a title="85-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-21-Fundamental_difficulty_of_inference_for_a_ratio_when_the_denominator_could_be_positive_or_negative.html">775 andrew gelman stats-2011-06-21-Fundamental difficulty of inference for a ratio when the denominator could be positive or negative</a></p>
<p>20 0.10704534 <a title="85-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-26-Some_thoughts_on_survey_weighting.html">1430 andrew gelman stats-2012-07-26-Some thoughts on survey weighting</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.15), (1, 0.124), (2, 0.122), (3, -0.053), (4, 0.054), (5, 0.021), (6, 0.038), (7, -0.0), (8, 0.017), (9, -0.044), (10, 0.031), (11, -0.045), (12, 0.023), (13, 0.045), (14, -0.067), (15, -0.056), (16, -0.036), (17, 0.016), (18, 0.012), (19, 0.009), (20, -0.04), (21, -0.068), (22, 0.01), (23, 0.063), (24, -0.044), (25, 0.037), (26, -0.052), (27, 0.081), (28, 0.031), (29, 0.034), (30, -0.05), (31, -0.014), (32, -0.056), (33, 0.015), (34, -0.053), (35, 0.016), (36, -0.055), (37, -0.059), (38, -0.038), (39, 0.033), (40, 0.011), (41, 0.021), (42, -0.028), (43, -0.036), (44, -0.041), (45, 0.007), (46, 0.02), (47, -0.004), (48, 0.058), (49, 0.016)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97208697 <a title="85-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-14-Prior_distribution_for_design_effects.html">85 andrew gelman stats-2010-06-14-Prior distribution for design effects</a></p>
<p>Introduction: David Shor writes:
  
I’m fitting a state-space model right now that estimates the “design effect” of individual pollsters (Ratio of poll variance to that predicted by perfect random sampling). What would be a good prior distribution for that?
  
My quickest suggestion is start with something simple, such as a uniform from 1 to 10, and then to move to something hierarchical, such as a lognormal on (design.effect – 1), with the hyperparameters estimated from data.
 
My longer suggestion is to take things apart.  What exactly do you mean by “design effect”?  There are lots of things going on, both in sampling error (the classical “design effect” that comes from cluster sampling, stratification, weighting, etc.) and nonsampling error (nonresponse bias, likeliy voter screening, bad questions, etc.)  It would be best if you could model both pieces.</p><p>2 0.72377664 <a title="85-lsi-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-10-Estimation_from_an_out-of-date_census.html">405 andrew gelman stats-2010-11-10-Estimation from an out-of-date census</a></p>
<p>Introduction: Suguru Mizunoya writes:
  
When we estimate the number of people from a national sampling survey (such as labor force survey) using sampling weights, don’t we obtain underestimated number of people, if the country’s population is growing and the sampling frame is based on an old census data?  In countries with increasing populations, the probability of inclusion changes over time, but the weights can’t be adjusted frequently because census takes place only once every five or ten years.


I am currently working for UNICEF for a project on estimating number of out-of-school children in developing countries. The project leader is comfortable to use estimates of number of people from DHS and other surveys.  But, I am concerned that we may need to adjust the estimated number of people by the population projection, otherwise the estimates will be underestimated.


I googled around on this issue, but I could not find a right article or paper on this.
  
My reply:  I don’t know if there’s a pa</p><p>3 0.71364754 <a title="85-lsi-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-04-All_the_Assumptions_That_Are_My_Life.html">2359 andrew gelman stats-2014-06-04-All the Assumptions That Are My Life</a></p>
<p>Introduction: Statisticians take tours in other people’s data.
 
All methods of statistical inference rest on statistical models. Experiments typically have problems with compliance, measurement error, generalizability to the real world, and representativeness of the sample. Surveys typically have problems of undercoverage, nonresponse, and measurement error.
 
Real surveys are done to learn about the general population. But real surveys are not random samples. For another example, consider educational tests: what are they exactly measuring? Nobody knows. Medical research: even if it’s a randomized experiment, the participants in the study won’t be a random sample from the population for whom you’d recommend treatment. You don’t need random sampling to generalize the results of a medical experiment to the general population but you need some substantive theory to make the assumption that effects in your nonrepresentative sample of people will be similar to effects in the population of interest.
 
Ve</p><p>4 0.69573832 <a title="85-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>Introduction: Klaas Metselaar writes: 
  
  
I [Metselaar] am currently involved in a discussion about the use of the notion “predictive” as used in “posterior predictive check”. I would argue that the notion “predictive” should be reserved for posterior checks using information not used in the determination of the posterior. 
I quote from the discussion: 
“However, the predictive uncertainty in a Bayesian calculation requires sampling from all the random variables, and this includes both the model parameters and the residual error”.


My [Metselaar's] comment:


This may be exactly the point I am worried about: shouldn’t the predictive uncertainty be defined as sampling from the posterior parameter distribution +  residual error + sampling from the prediction error distribution? 
Residual error reduces to measurement error in the case of a  model which is perfect for the sample of experiments. Measurement error could be reduced to almost zero by ideal and perfect measurement instruments. 
I would h</p><p>5 0.69366539 <a title="85-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-29-How_many_parameters_are_in_a_multilevel_model%3F.html">1144 andrew gelman stats-2012-01-29-How many parameters are in a multilevel model?</a></p>
<p>Introduction: Stephen Collins writes:
  
I’m reading your Multilevel modeling book and am trying to apply it to my work.  I’m concerned with how to estimate a random intercept model if there are hundreds/thousands of levels.  In the Gibbs sampling, am I sampling a parameter for each level?  Or, just the hyper-parameters?  In other words, say I had 500 zipcode intercepts modeled as ~ N(m,s).  Would my posterior be two dimensional, sampling for “m” and “s,” or would it have 502 dimensions?
  
My reply:  Indeed you will have hundreds or thousands of parameters—or, in classical terms, hundreds or thousands of predictive quantities.  But that’s ok.  Even if none of those predictions is precise, you’re learning  about the model.
 
See page 526 of the book for more discussion of the number of parameters in a multilevel model.</p><p>6 0.69362426 <a title="85-lsi-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-24-PPS_in_Georgia.html">107 andrew gelman stats-2010-06-24-PPS in Georgia</a></p>
<p>7 0.68066335 <a title="85-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-18-Is_it_really_true_that_only_8%25_of_people_who_buy_Herbalife_products_are_Herbalife_distributors%3F.html">1679 andrew gelman stats-2013-01-18-Is it really true that only 8% of people who buy Herbalife products are Herbalife distributors?</a></p>
<p>8 0.679506 <a title="85-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-17-Statistics_in_a_world_where_nothing_is_random.html">1628 andrew gelman stats-2012-12-17-Statistics in a world where nothing is random</a></p>
<p>9 0.67380857 <a title="85-lsi-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-10-R_vs._Stata%2C_or%2C_Different_ways_to_estimate_multilevel_models.html">269 andrew gelman stats-2010-09-10-R vs. Stata, or, Different ways to estimate multilevel models</a></p>
<p>10 0.66937625 <a title="85-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-01-Weighting_and_prediction_in_sample_surveys.html">784 andrew gelman stats-2011-07-01-Weighting and prediction in sample surveys</a></p>
<p>11 0.66547662 <a title="85-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_24_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1362 andrew gelman stats-2012-06-03-Question 24 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>12 0.66527951 <a title="85-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-12-Probabilistic_screening_to_get_an_approximate_self-weighted_sample.html">1455 andrew gelman stats-2012-08-12-Probabilistic screening to get an approximate self-weighted sample</a></p>
<p>13 0.6610024 <a title="85-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-25-Design_of_nonrandomized_cluster_sample_study.html">820 andrew gelman stats-2011-07-25-Design of nonrandomized cluster sample study</a></p>
<p>14 0.65985042 <a title="85-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-25-How_do_you_interpret_standard_errors_from_a_regression_fit_to_the_entire_population%3F.html">972 andrew gelman stats-2011-10-25-How do you interpret standard errors from a regression fit to the entire population?</a></p>
<p>15 0.63705796 <a title="85-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-14-Question_4_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1320 andrew gelman stats-2012-05-14-Question 4 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>16 0.63205415 <a title="85-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-15-The_bias-variance_tradeoff.html">960 andrew gelman stats-2011-10-15-The bias-variance tradeoff</a></p>
<p>17 0.6261192 <a title="85-lsi-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-12-Finite-population_standard_deviation_in_a_hierarchical_model.html">464 andrew gelman stats-2010-12-12-Finite-population standard deviation in a hierarchical model</a></p>
<p>18 0.62305927 <a title="85-lsi-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-19-Analysis_of_survey_data%3A_Design_based_models_vs._hierarchical_modeling%3F.html">352 andrew gelman stats-2010-10-19-Analysis of survey data: Design based models vs. hierarchical modeling?</a></p>
<p>19 0.61993104 <a title="85-lsi-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-04-27-Should_Mister_P_be_allowed-encouraged_to_reside_in_counter-factual_populations%3F.html">7 andrew gelman stats-2010-04-27-Should Mister P be allowed-encouraged to reside in counter-factual populations?</a></p>
<p>20 0.61988527 <a title="85-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-01-Question_22_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1358 andrew gelman stats-2012-06-01-Question 22 of my final exam for Design and Analysis of Sample Surveys</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(8, 0.153), (9, 0.042), (15, 0.018), (16, 0.019), (24, 0.305), (95, 0.033), (99, 0.31)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97915757 <a title="85-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-14-Prior_distribution_for_design_effects.html">85 andrew gelman stats-2010-06-14-Prior distribution for design effects</a></p>
<p>Introduction: David Shor writes:
  
I’m fitting a state-space model right now that estimates the “design effect” of individual pollsters (Ratio of poll variance to that predicted by perfect random sampling). What would be a good prior distribution for that?
  
My quickest suggestion is start with something simple, such as a uniform from 1 to 10, and then to move to something hierarchical, such as a lognormal on (design.effect – 1), with the hyperparameters estimated from data.
 
My longer suggestion is to take things apart.  What exactly do you mean by “design effect”?  There are lots of things going on, both in sampling error (the classical “design effect” that comes from cluster sampling, stratification, weighting, etc.) and nonsampling error (nonresponse bias, likeliy voter screening, bad questions, etc.)  It would be best if you could model both pieces.</p><p>2 0.95494682 <a title="85-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-21-Judea_Pearl_on_why_he_is_%E2%80%9Conly_a_half-Bayesian%E2%80%9D.html">1133 andrew gelman stats-2012-01-21-Judea Pearl on why he is “only a half-Bayesian”</a></p>
<p>Introduction: In  an article  published in 2001, Pearl wrote:
  
I [Pearl] turned Bayesian in 1971, as soon as I began reading Savage’s monograph The Foundations of Statistical Inference [Savage, 1962]. The arguments were unassailable: (i) It is plain silly to ignore what we know, (ii) It is natural and useful to cast what we know in the language of probabilities, and (iii) If our subjective probabilities are erroneous, their impact will get washed out in due time, as the number of observations increases.


Thirty years later, I [Pearl] am still a devout Bayesian in the sense of (i), but I now doubt the wisdom of (ii) and I know that, in general, (iii) is false.
  
He elaborates:
  
The bulk of human knowledge is organized around causal, not probabilistic relationships, and the grammar of probability calculus is insufficient for capturing those relationships. Specifically, the building blocks of our scientific and everyday knowledge are elementary facts such as “mud does not cause rain” and “symptom</p><p>3 0.95204324 <a title="85-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-19-Whassup_with_deviance_having_a_high_posterior_correlation_with_a_parameter_in_the_model%3F.html">1221 andrew gelman stats-2012-03-19-Whassup with deviance having a high posterior correlation with a parameter in the model?</a></p>
<p>Introduction: Jean Richardson writes: 
  
  
Do you know what might lead to a large negative cross-correlation (-0.95) between deviance and one of the model parameters?


Here’s the (brief) background:


I [Richardson] have written a Bayesian hierarchical site occupancy model for presence of disease on individual amphibians. The response variable is therefore binary (disease present/absent) and the probability of disease being present in an individual (psi) depends on various covariates (species of amphibian, location sampled, etc.) paramaterized using a logit link function.  Replicates are individuals sampled (tested for presence of disease) together.  The possibility of imperfect detection is included as p = (prob. disease detected given disease is present).


Posterior distributions were estimated using WinBUGS via R2WinBUGS. 
Simulated data from the model fit the real data very well and posterior distribution densities seem robust to any changes in the model (different priors, etc.)  All autocor</p><p>4 0.94828665 <a title="85-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-31-Lindley%E2%80%99s_paradox.html">1355 andrew gelman stats-2012-05-31-Lindley’s paradox</a></p>
<p>Introduction: Sam Seaver writes:
  
I [Seaver] happened to be reading an ironic  article  by Karl Friston when I learned something new about frequentist vs bayesian, namely Lindley’s paradox, on page 12.  The text is as follows:

 
So why are we worried about trivial effects? They are important because the probability that the true effect size is exactly zero is itself zero and could cause us to reject the null hypothesis inappropriately. This is a fallacy of classical inference and is not unrelated to Lindley’s paradox (Lindley 1957). Lindley’s paradox describes a counterintuitive situation in which Bayesian and frequentist approaches to hypothesis testing give opposite results. It occurs when; (i) a result is significant by a frequentist test, indicating sufficient evidence to reject the null hypothesis d=0 and (ii) priors render the posterior probability of d=0 high, indicating strong evidence that the null hypothesis is true. In his original 
treatment, Lindley (1957) showed that – under a parti</p><p>5 0.94552326 <a title="85-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-22-The_kluges_of_today_are_the_textbook_solutions_of_tomorrow..html">2143 andrew gelman stats-2013-12-22-The kluges of today are the textbook solutions of tomorrow.</a></p>
<p>Introduction: From a response on the Stan help list:
  
Yes, indeed, I think it would be a good idea to reduce the scale on priors of the form U(0,100) or N(0,100^2).  This won’t solve all problems but it can’t hurt.


If the issue is that the variance parameter can be very small in the estimation, yes, one approach would be to put in a prior that keeps the variance away from 0 (lognormal, gamma, whatever), another approach would be to use the Matt trick.  Some mixture of these ideas might help.


And, by the way:  when you do these things it might feel like an awkward bit of kluging to play around with the model to get it to convert properly.  But the kluges of today are the textbook solutions of tomorrow.  When it comes to statistical modeling, we’re living in beta-test world; we should appreciate the opportunities this gives us!</p><p>6 0.93595254 <a title="85-lda-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-10-The_last_great_essayist%3F.html">197 andrew gelman stats-2010-08-10-The last great essayist?</a></p>
<p>7 0.93574178 <a title="85-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-14-%E2%80%9CLike_a_group_of_teenagers_on_a_bus%2C_they_behave_in_public_as_if_they_were_in_private%E2%80%9D.html">414 andrew gelman stats-2010-11-14-“Like a group of teenagers on a bus, they behave in public as if they were in private”</a></p>
<p>8 0.93487912 <a title="85-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-21-Teaching_velocity_and_acceleration.html">1224 andrew gelman stats-2012-03-21-Teaching velocity and acceleration</a></p>
<p>9 0.93433964 <a title="85-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>10 0.93379235 <a title="85-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-10-Using_a_%E2%80%9Cpure_infographic%E2%80%9D_to_explore_differences_between_information_visualization_and_statistical_graphics.html">847 andrew gelman stats-2011-08-10-Using a “pure infographic” to explore differences between information visualization and statistical graphics</a></p>
<p>11 0.93345547 <a title="85-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-12-Probabilistic_screening_to_get_an_approximate_self-weighted_sample.html">1455 andrew gelman stats-2012-08-12-Probabilistic screening to get an approximate self-weighted sample</a></p>
<p>12 0.93300581 <a title="85-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-15-Advice_that_might_make_sense_for_individuals_but_is_negative-sum_overall.html">278 andrew gelman stats-2010-09-15-Advice that might make sense for individuals but is negative-sum overall</a></p>
<p>13 0.93192744 <a title="85-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-06-Question_27_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1368 andrew gelman stats-2012-06-06-Question 27 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>14 0.93080878 <a title="85-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-03-Setting_aside_the_politics%2C_the_debate_over_the_new_health-care_study_reveals_that_we%E2%80%99re_moving_to_a_new_high_standard_of_statistical_journalism.html">1838 andrew gelman stats-2013-05-03-Setting aside the politics, the debate over the new health-care study reveals that we’re moving to a new high standard of statistical journalism</a></p>
<p>15 0.93048155 <a title="85-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-27-%E2%80%9CKeeping_things_unridiculous%E2%80%9D%3A__Berger%2C_O%E2%80%99Hagan%2C_and_me_on_weakly_informative_priors.html">1087 andrew gelman stats-2011-12-27-“Keeping things unridiculous”:  Berger, O’Hagan, and me on weakly informative priors</a></p>
<p>16 0.93026412 <a title="85-lda-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-14-The_maximal_information_coefficient.html">2247 andrew gelman stats-2014-03-14-The maximal information coefficient</a></p>
<p>17 0.92784446 <a title="85-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-02-So_much_artistic_talent.html">1785 andrew gelman stats-2013-04-02-So much artistic talent</a></p>
<p>18 0.92691481 <a title="85-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-19-%E2%80%9CThe_difference_between_._._.%E2%80%9D%3A__It%E2%80%99s_not_just_p%3D.05_vs._p%3D.06.html">1072 andrew gelman stats-2011-12-19-“The difference between . . .”:  It’s not just p=.05 vs. p=.06</a></p>
<p>19 0.92667603 <a title="85-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-16-Mr._Pearson%2C_meet_Mr._Mandelbrot%3A__Detecting_Novel_Associations_in_Large_Data_Sets.html">1062 andrew gelman stats-2011-12-16-Mr. Pearson, meet Mr. Mandelbrot:  Detecting Novel Associations in Large Data Sets</a></p>
<p>20 0.92660463 <a title="85-lda-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-02-The_problem_of_overestimation_of_group-level_variance_parameters.html">63 andrew gelman stats-2010-06-02-The problem of overestimation of group-level variance parameters</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
