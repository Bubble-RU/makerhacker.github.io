<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>342 andrew gelman stats-2010-10-14-Trying to be precise about vagueness</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-342" href="#">andrew_gelman_stats-2010-342</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>342 andrew gelman stats-2010-10-14-Trying to be precise about vagueness</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-342-html" href="http://andrewgelman.com/2010/10/14/trying_to_be_pr/">html</a></p><p>Introduction: I recently saw  this article  that Stephen Senn wrote a couple of years ago, criticizing Bayesian sensitivity analyses that relied on vague prior distributions.  I’m moving more and more toward the idea that Bayesian analysis should include actual prior information, so I generally agree with his points.  As I used to say when teaching Bayesian data analysis, a Bayesian model is modular, and different pieces can be swapped in and out as needed.  So you might start with an extremely weak prior distribution, but if it makes a difference it’s time to bite the bullet and include more information.
 
My only disagreement with Senn’s paper is in its recommendation to try the so-called fixed-effects analysis.  Beyond the difficulties with terminology (the expressions “fixed” and “random” effects are defined in different ways by different people in the literature;  see here  for a rant on the topic which made its way into some of my articles and books), there is the problem that, when a model ge</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I recently saw  this article  that Stephen Senn wrote a couple of years ago, criticizing Bayesian sensitivity analyses that relied on vague prior distributions. [sent-1, score-0.968]
</p><p>2 I’m moving more and more toward the idea that Bayesian analysis should include actual prior information, so I generally agree with his points. [sent-2, score-0.805]
</p><p>3 As I used to say when teaching Bayesian data analysis, a Bayesian model is modular, and different pieces can be swapped in and out as needed. [sent-3, score-0.62]
</p><p>4 So you might start with an extremely weak prior distribution, but if it makes a difference it’s time to bite the bullet and include more information. [sent-4, score-0.926]
</p><p>5 My only disagreement with Senn’s paper is in its recommendation to try the so-called fixed-effects analysis. [sent-5, score-0.247]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('senn', 0.347), ('bayesian', 0.219), ('prior', 0.215), ('swapped', 0.199), ('modular', 0.199), ('fixed', 0.198), ('bite', 0.178), ('relied', 0.166), ('bullet', 0.166), ('expressions', 0.16), ('terminology', 0.157), ('include', 0.15), ('rant', 0.145), ('sensitivity', 0.145), ('sparse', 0.145), ('vague', 0.137), ('disagreement', 0.131), ('effects', 0.131), ('different', 0.125), ('pieces', 0.122), ('stephen', 0.122), ('noisy', 0.121), ('criticizing', 0.119), ('recommendation', 0.116), ('weak', 0.115), ('difficulties', 0.11), ('logistic', 0.108), ('arise', 0.107), ('complicated', 0.106), ('moving', 0.105), ('settings', 0.105), ('extremely', 0.102), ('defined', 0.1), ('saw', 0.095), ('analyses', 0.091), ('teaching', 0.091), ('analysis', 0.09), ('toward', 0.087), ('actual', 0.084), ('books', 0.084), ('model', 0.083), ('gets', 0.082), ('random', 0.08), ('articles', 0.078), ('literature', 0.078), ('beyond', 0.077), ('especially', 0.076), ('called', 0.075), ('ways', 0.075), ('generally', 0.074)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="342-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-14-Trying_to_be_precise_about_vagueness.html">342 andrew gelman stats-2010-10-14-Trying to be precise about vagueness</a></p>
<p>Introduction: I recently saw  this article  that Stephen Senn wrote a couple of years ago, criticizing Bayesian sensitivity analyses that relied on vague prior distributions.  I’m moving more and more toward the idea that Bayesian analysis should include actual prior information, so I generally agree with his points.  As I used to say when teaching Bayesian data analysis, a Bayesian model is modular, and different pieces can be swapped in and out as needed.  So you might start with an extremely weak prior distribution, but if it makes a difference it’s time to bite the bullet and include more information.
 
My only disagreement with Senn’s paper is in its recommendation to try the so-called fixed-effects analysis.  Beyond the difficulties with terminology (the expressions “fixed” and “random” effects are defined in different ways by different people in the literature;  see here  for a rant on the topic which made its way into some of my articles and books), there is the problem that, when a model ge</p><p>2 0.32484308 <a title="342-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-03-Philosophy_of_Bayesian_statistics%3A__my_reactions_to_Senn.html">1151 andrew gelman stats-2012-02-03-Philosophy of Bayesian statistics:  my reactions to Senn</a></p>
<p>Introduction: Continuing with  my discussion of the articles in the special issue  of the journal Rationality, Markets and Morals on the philosophy of Bayesian statistics:
 
   
 
Stephen Senn, “You May Believe You Are a Bayesian But You Are Probably Wrong”:
 
I agree with Senn’s comments on the impossibility of the de Finetti subjective Bayesian approach.  As I wrote in 2008, if you could really construct a subjective prior you believe in, why not just look at the data and write down your subjective posterior.  The immense practical difficulties with  any  serious system of inference render it absurd to think that it would be possible to just write down a probability distribution to represent uncertainty.  I wish, however, that Senn would recognize  my  Bayesian approach (which is also that of John Carlin, Hal Stern, Don Rubin, and, I believe, others).  De Finetti is no longer around, but we are!
 
I have to admit that my own Bayesian views and practices have changed.  In particular, I resonate wit</p><p>3 0.21068853 <a title="342-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>Introduction: Nick Firoozye writes:
  
While I am absolutely sympathetic to the Bayesian agenda I am often troubled by the requirement of having priors. We must have priors on the parameter of an infinite number of model we have never seen before and I find this troubling. There is a similarly troubling problem in economics of utility theory. Utility is on consumables. To be complete a consumer must assign utility to all sorts of things they never would have encountered. More recent versions of utility theory instead make consumption goods a portfolio of attributes. Cadillacs are x many units of luxury y of transport etc etc. And we can automatically have personal utilities to all these attributes.  


I don’t ever see parameters. Some model have few and some have hundreds. Instead, I see data. So I don’t know how to have an opinion on parameters themselves. Rather I think it far more natural to have opinions on the behavior of models. The prior predictive density is a good and sensible notion. Also</p><p>4 0.20732164 <a title="342-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>Introduction: Following up on Christian’s  post  [link fixed] on the topic, I’d like to offer a few thoughts of my own.
 
In BDA, we express the idea that a noninformative prior is a placeholder:  you can use the noninformative prior to get the analysis started, then if your posterior distribution is less informative than you would like, or if it does not make sense, you can go back and add prior information.
 
Same thing for the data model (the “likelihood”), for that matter:  it often makes sense to start with something simple and conventional and then go from there.
 
So, in that sense, noninformative priors are no big deal, they’re just a way to get started.  Just don’t take them too seriously.
 
Traditionally in statistics we’ve worked with the paradigm of a single highly informative dataset with only weak external information.  But if the data are sparse and prior information is strong, we have to think differently.  And, when you increase the dimensionality of a problem, both these things hap</p><p>5 0.20142116 <a title="342-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>Introduction: Some recent blog discussion revealed some confusion that I’ll try to resolve here.
 
I  wrote  that I’m not a big fan of subjective priors.  Various commenters had difficulty with this point, and I think the issue was most clearly stated by Bill Jeff re erys, who  wrote :
  
It seems to me that your prior has to reflect your subjective information before you look at the data. How can it not?


But this does not mean that the (subjective) prior that you choose is irrefutable; Surely a prior that reflects prior information just does not have to be inconsistent with that information. But that still leaves a range of priors that are consistent with it, the sort of priors that one would use in a sensitivity analysis, for example.
  
I think I see what Bill is getting at.  A prior represents your subjective belief, or some approximation to your subjective belief, even if it’s not perfect.  That sounds reasonable but I don’t think it works.  Or, at least, it often doesn’t work.
 
Let’s start</p><p>6 0.19997722 <a title="342-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>7 0.18695271 <a title="342-tfidf-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-17-So-called_fixed_and_random_effects.html">472 andrew gelman stats-2010-12-17-So-called fixed and random effects</a></p>
<p>8 0.18345273 <a title="342-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-01-Philosophy_of_Bayesian_statistics%3A__my_reactions_to_Cox_and_Mayo.html">1149 andrew gelman stats-2012-02-01-Philosophy of Bayesian statistics:  my reactions to Cox and Mayo</a></p>
<p>9 0.17972687 <a title="342-tfidf-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>10 0.17530879 <a title="342-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-09-Coming_to_agreement_on_philosophy_of_statistics.html">1205 andrew gelman stats-2012-03-09-Coming to agreement on philosophy of statistics</a></p>
<p>11 0.157479 <a title="342-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-11-%E2%80%9CInformative_g-Priors_for_Logistic_Regression%E2%80%9D.html">2017 andrew gelman stats-2013-09-11-“Informative g-Priors for Logistic Regression”</a></p>
<p>12 0.15483636 <a title="342-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-30-Articles_on_the_philosophy_of_Bayesian_statistics_by_Cox%2C_Mayo%2C_Senn%2C_and_others%21.html">932 andrew gelman stats-2011-09-30-Articles on the philosophy of Bayesian statistics by Cox, Mayo, Senn, and others!</a></p>
<p>13 0.15471543 <a title="342-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>14 0.14841762 <a title="342-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-31-It_not_necessary_that_Bayesian_methods_conform_to_the_likelihood_principle.html">1554 andrew gelman stats-2012-10-31-It not necessary that Bayesian methods conform to the likelihood principle</a></p>
<p>15 0.14370601 <a title="342-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-30-Fixed_effects%2C_followed_by_Bayes_shrinkage%3F.html">1644 andrew gelman stats-2012-12-30-Fixed effects, followed by Bayes shrinkage?</a></p>
<p>16 0.14289036 <a title="342-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-07-Philosophy_and_the_practice_of_Bayesian_statistics_%28with_all_the_discussions%21%29.html">1712 andrew gelman stats-2013-02-07-Philosophy and the practice of Bayesian statistics (with all the discussions!)</a></p>
<p>17 0.14246431 <a title="342-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>18 0.14212976 <a title="342-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-08-Multilevel_regression_with_shrinkage_for_%E2%80%9Cfixed%E2%80%9D_effects.html">653 andrew gelman stats-2011-04-08-Multilevel regression with shrinkage for “fixed” effects</a></p>
<p>19 0.14210062 <a title="342-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-17-Modeling_group-level_predictors_in_a_multilevel_regression.html">1216 andrew gelman stats-2012-03-17-Modeling group-level predictors in a multilevel regression</a></p>
<p>20 0.14131975 <a title="342-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-02-Fixed_effects_and_identification.html">1241 andrew gelman stats-2012-04-02-Fixed effects and identification</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.211), (1, 0.214), (2, -0.025), (3, 0.033), (4, -0.076), (5, -0.044), (6, 0.053), (7, 0.049), (8, -0.044), (9, 0.025), (10, 0.033), (11, -0.013), (12, 0.057), (13, 0.032), (14, 0.101), (15, 0.05), (16, -0.029), (17, 0.039), (18, -0.009), (19, 0.07), (20, -0.044), (21, 0.035), (22, -0.016), (23, -0.011), (24, -0.025), (25, -0.049), (26, -0.024), (27, -0.022), (28, -0.017), (29, 0.001), (30, -0.0), (31, 0.004), (32, -0.034), (33, -0.003), (34, 0.035), (35, 0.016), (36, -0.05), (37, 0.04), (38, -0.011), (39, 0.007), (40, 0.029), (41, 0.027), (42, 0.008), (43, 0.006), (44, -0.025), (45, 0.022), (46, 0.034), (47, -0.052), (48, 0.0), (49, 0.036)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97970021 <a title="342-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-14-Trying_to_be_precise_about_vagueness.html">342 andrew gelman stats-2010-10-14-Trying to be precise about vagueness</a></p>
<p>Introduction: I recently saw  this article  that Stephen Senn wrote a couple of years ago, criticizing Bayesian sensitivity analyses that relied on vague prior distributions.  I’m moving more and more toward the idea that Bayesian analysis should include actual prior information, so I generally agree with his points.  As I used to say when teaching Bayesian data analysis, a Bayesian model is modular, and different pieces can be swapped in and out as needed.  So you might start with an extremely weak prior distribution, but if it makes a difference it’s time to bite the bullet and include more information.
 
My only disagreement with Senn’s paper is in its recommendation to try the so-called fixed-effects analysis.  Beyond the difficulties with terminology (the expressions “fixed” and “random” effects are defined in different ways by different people in the literature;  see here  for a rant on the topic which made its way into some of my articles and books), there is the problem that, when a model ge</p><p>2 0.86789751 <a title="342-lsi-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-02-The_problem_of_overestimation_of_group-level_variance_parameters.html">63 andrew gelman stats-2010-06-02-The problem of overestimation of group-level variance parameters</a></p>
<p>Introduction: John Lawson writes:
  
I have been experimenting using Bayesian Methods to estimate variance components, and I have noticed that even when I use a noninformative prior, my estimates are never close to the method of moments or REML estimates. In every case I have tried, the sum of the Bayesian estimated variance components is always larger than the sum of the estimates obtained by method of moments or REML.
      
For data sets I have used that arise from a simple one-way random effects model, the Bayesian estimates of the between groups variance component is usually larger than the method of moments or REML estimates. When I use a uniform prior on the between standard deviation (as you recommended in  your 2006 paper ) rather than an inverse gamma prior on the between variance component, the between variance component is usually reduced.  However, for the dyestuff data in Davies(1949, p74), the opposite appears to be the case.


I am a worried that the Bayesian estimators of the varian</p><p>3 0.84108233 <a title="342-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-03-Philosophy_of_Bayesian_statistics%3A__my_reactions_to_Senn.html">1151 andrew gelman stats-2012-02-03-Philosophy of Bayesian statistics:  my reactions to Senn</a></p>
<p>Introduction: Continuing with  my discussion of the articles in the special issue  of the journal Rationality, Markets and Morals on the philosophy of Bayesian statistics:
 
   
 
Stephen Senn, “You May Believe You Are a Bayesian But You Are Probably Wrong”:
 
I agree with Senn’s comments on the impossibility of the de Finetti subjective Bayesian approach.  As I wrote in 2008, if you could really construct a subjective prior you believe in, why not just look at the data and write down your subjective posterior.  The immense practical difficulties with  any  serious system of inference render it absurd to think that it would be possible to just write down a probability distribution to represent uncertainty.  I wish, however, that Senn would recognize  my  Bayesian approach (which is also that of John Carlin, Hal Stern, Don Rubin, and, I believe, others).  De Finetti is no longer around, but we are!
 
I have to admit that my own Bayesian views and practices have changed.  In particular, I resonate wit</p><p>4 0.82084548 <a title="342-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-17-Christian_Robert_on_the_Jeffreys-Lindley_paradox%3B_more_generally%2C_it%E2%80%99s_good_news_when_philosophical_arguments_can_be_transformed_into_technical_modeling_issues.html">2027 andrew gelman stats-2013-09-17-Christian Robert on the Jeffreys-Lindley paradox; more generally, it’s good news when philosophical arguments can be transformed into technical modeling issues</a></p>
<p>Introduction: X  writes :
  
This paper discusses the dual interpretation of the Jeffreys– Lindley’s paradox associated with Bayesian posterior probabilities and Bayes factors, both as a differentiation between frequentist and Bayesian statistics and as a pointer to the difficulty of using improper priors while testing. We stress the considerable impact of this paradox on the foundations of both classical and Bayesian statistics.
  
I like this paper in that he is transforming what is often seen as a philosophical argument into a technical issue, in this case a question of priors.  Certain conventional priors (the so-called spike and slab) have poor statistical properties in settings such as model comparison (in addition to not making sense as prior distributions of any realistic state of knowledge).  This reminds me of the way that we nowadays think about hierarchical models.  In the old days there was much thoughtful debate about exchangeability and the so-called Stein paradox that partial pooling</p><p>5 0.82010812 <a title="342-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-07-Philosophy_of_Bayesian_statistics%3A_my_reactions_to_Hendry.html">1157 andrew gelman stats-2012-02-07-Philosophy of Bayesian statistics: my reactions to Hendry</a></p>
<p>Introduction: Continuing with my discussion  here  and  here  of the articles in the special issue of the journal Rationality, Markets and Morals on the philosophy of Bayesian statistics:
 
   
 
David Hendry, “Empirical Economic Model Discovery and Theory Evaluation”:
 
Hendry presents a wide-ranging overview of scientific learning, with an interesting comparison of physical with social sciences.  (For some reason, he discusses many physical sciences but restricts his social-science examples to economics and psychology.)
 
The only part of Hendry’s long and interesting article that I will discuss, however, is the part where he decides to take a gratuitous swing at Bayes.  I don’t know why he did this, but maybe it’s part of some fraternity initiation thing, like TP-ing the dean’s house on Halloween.
 
Here’s the story.  Hendry writes:
  
‘Prior distributions’ widely used in Bayesian analyses, whether subjective or ‘objective’, cannot be formed in such a setting either, absent a falsely assumed crys</p><p>6 0.80497694 <a title="342-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>7 0.78945929 <a title="342-lsi-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-11-Bayes_in_the_research_conversation.html">2368 andrew gelman stats-2014-06-11-Bayes in the research conversation</a></p>
<p>8 0.78386867 <a title="342-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-10-Cross-validation_and_Bayesian_estimation_of_tuning_parameters.html">2129 andrew gelman stats-2013-12-10-Cross-validation and Bayesian estimation of tuning parameters</a></p>
<p>9 0.77340502 <a title="342-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-24-Untangling_the_Jeffreys-Lindley_paradox.html">1182 andrew gelman stats-2012-02-24-Untangling the Jeffreys-Lindley paradox</a></p>
<p>10 0.7709803 <a title="342-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-13-On_the_half-Cauchy_prior_for_a_global_scale_parameter.html">801 andrew gelman stats-2011-07-13-On the half-Cauchy prior for a global scale parameter</a></p>
<p>11 0.76828885 <a title="342-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-30-Infill_asymptotics_and_sprawl_asymptotics.html">1877 andrew gelman stats-2013-05-30-Infill asymptotics and sprawl asymptotics</a></p>
<p>12 0.76703042 <a title="342-lsi-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-04-Generalized_Method_of_Moments%2C_whatever_that_is.html">449 andrew gelman stats-2010-12-04-Generalized Method of Moments, whatever that is</a></p>
<p>13 0.76174092 <a title="342-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-11-Bayesian_brains%3F.html">1529 andrew gelman stats-2012-10-11-Bayesian brains?</a></p>
<p>14 0.75775826 <a title="342-lsi-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-13-Silly_Sas_lays_out_old-fashioned_statistical_thinking.html">83 andrew gelman stats-2010-06-13-Silly Sas lays out old-fashioned statistical thinking</a></p>
<p>15 0.75539929 <a title="342-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-01-Philosophy_of_Bayesian_statistics%3A__my_reactions_to_Cox_and_Mayo.html">1149 andrew gelman stats-2012-02-01-Philosophy of Bayesian statistics:  my reactions to Cox and Mayo</a></p>
<p>16 0.75214112 <a title="342-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>17 0.74852407 <a title="342-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-03-Hierarchical_array_priors_for_ANOVA_decompositions.html">1786 andrew gelman stats-2013-04-03-Hierarchical array priors for ANOVA decompositions</a></p>
<p>18 0.74524003 <a title="342-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-25-Continuous_variables_in_Bayesian_networks.html">1228 andrew gelman stats-2012-03-25-Continuous variables in Bayesian networks</a></p>
<p>19 0.74370056 <a title="342-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-27-%E2%80%9CKeeping_things_unridiculous%E2%80%9D%3A__Berger%2C_O%E2%80%99Hagan%2C_and_me_on_weakly_informative_priors.html">1087 andrew gelman stats-2011-12-27-“Keeping things unridiculous”:  Berger, O’Hagan, and me on weakly informative priors</a></p>
<p>20 0.74333417 <a title="342-lsi-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-18-Those_wacky_anti-Bayesians_used_to_be_intimidating%2C_but_now_they%E2%80%99re_just_pathetic.html">2254 andrew gelman stats-2014-03-18-Those wacky anti-Bayesians used to be intimidating, but now they’re just pathetic</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(15, 0.015), (16, 0.062), (17, 0.019), (21, 0.031), (22, 0.04), (24, 0.177), (25, 0.041), (54, 0.02), (79, 0.019), (87, 0.06), (99, 0.415)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99223423 <a title="342-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-14-Trying_to_be_precise_about_vagueness.html">342 andrew gelman stats-2010-10-14-Trying to be precise about vagueness</a></p>
<p>Introduction: I recently saw  this article  that Stephen Senn wrote a couple of years ago, criticizing Bayesian sensitivity analyses that relied on vague prior distributions.  I’m moving more and more toward the idea that Bayesian analysis should include actual prior information, so I generally agree with his points.  As I used to say when teaching Bayesian data analysis, a Bayesian model is modular, and different pieces can be swapped in and out as needed.  So you might start with an extremely weak prior distribution, but if it makes a difference it’s time to bite the bullet and include more information.
 
My only disagreement with Senn’s paper is in its recommendation to try the so-called fixed-effects analysis.  Beyond the difficulties with terminology (the expressions “fixed” and “random” effects are defined in different ways by different people in the literature;  see here  for a rant on the topic which made its way into some of my articles and books), there is the problem that, when a model ge</p><p>2 0.98576474 <a title="342-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-21-An_interesting_assignment_for_statistical_graphics.html">583 andrew gelman stats-2011-02-21-An interesting assignment for statistical graphics</a></p>
<p>Introduction: Antony Unwin writes:
  
I [Unwin] find it an interesting exercise for students to ask them to write headlines (and subheadlines) for graphics, both for ones they have drawn themselves and for published ones.  The results are sometimes depressing, often thought-provoking and occasionally highly entertaining.
  
This seems like a great idea, both for teaching students how to read a graph and also for teaching how to make a graph.  I’ve long said that when making a graph (or, for that matter, a table), you want to think about what message the reader will get out of it. “Displaying a bunch of numbers” doesn’t cut it.</p><p>3 0.98006642 <a title="342-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-27-Three_unblinded_mice.html">2115 andrew gelman stats-2013-11-27-Three unblinded mice</a></p>
<p>Introduction: Howard Wainer points us to a recent  news article  by Jennifer Couzin-Frankel, who writes about the selection bias arising from the routine use of outcome criteria to exclude animals in medical trials.  In statistics and econometrics, this is drilled into us:  Selection on x is OK, selection on y is not OK.  But apparently in biomedical research this principle is not so well known (or, perhaps, it is all too well known).
 
Couzin-Frankel starts with an example of a drug trial in which 3 of the 10 mice in the treatment group were removed from the analysis because they had died from massive strokes.  This sounds pretty bad, but it’s even worse than that:  this was from a paper under review that “described how a new drug protected a rodent’s brain after a stroke.”  Death isn’t a very good way to protect a rodent’s brain!
 
The news article continues:
  
“This isn’t fraud,” says Dirnagl [the outside reviewer who caught this particular problem], who often works with mice. Dropping animals f</p><p>4 0.98002708 <a title="342-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-11-News_flash%3A__Probability_and_statistics_are_hard_to_understand.html">1413 andrew gelman stats-2012-07-11-News flash:  Probability and statistics are hard to understand</a></p>
<p>Introduction: Two people pointed me to an article by Emre Soyer and Robin Hogarth that was  linked to  by Felix Salmon.
 
Here are my reactions: 
   
1.  Soyer and Hogarth’s paper seems very strong to me, and Salmon’s presentation is an impressive condensation of it.  I’d say good job on the science and the reporting.
 
2.  I don’t see the point of focusing on economists.  This seems just like a gimmick to me.  But, then again, I’m not an economist.  So of course I’d be more interested in a similar paper studying political scientists or statisticians.  This should be easy enough for someone to do, of course.
 
3.  To elaborate on this last point:  I’m not surprised that people, even expert practitioners, screw up with statistics.  Kahneman and Tversky found this with psychology researchers back in the 1970s.  I’m not knocking the current paper by Soyer and Hogarth but I don’t see it as surprising. Perhaps the focus on economists is what allowed it to get all this attention.  If you want people to re</p><p>5 0.97996426 <a title="342-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-13-Question_of_the_week%3A__Will_the_authors_of_a_controversial_new_study_apologize_to_busy_statistician_Don_Berry_for_wasting_his_time_reading_and_responding_to_their_flawed_article%3F.html">1263 andrew gelman stats-2012-04-13-Question of the week:  Will the authors of a controversial new study apologize to busy statistician Don Berry for wasting his time reading and responding to their flawed article?</a></p>
<p>Introduction: Aaron Carroll  shoots down  a politically-loaded claim about cancer survival.  Lots of useful  background  from science reporter Sharon Begley:
  
With the United States spending more on healthcare than any other country — $2.5 trillion, or just over $8,000 per capita, in 2009 — the question has long been, is it worth it? At least for spending on cancer, a controversial new study answers with an emphatic “yes.” . . .


Experts shown an advance copy of the paper by Reuters argued that the tricky statistics of cancer outcomes tripped up the authors.


“This study is pure folly,” said biostatistician Dr. Don Berry of MD Anderson Cancer Center in Houston. “It’s completely misguided and it’s dangerous. Not only are the authors’ analyses flawed but their conclusions are also wrong.”
  
Ouch.  Arguably the study shouldn’t be getting any coverage at all, but given that it’s in the news, it’s good to see it get shot down.  I wonder if the  authors  will respond to Don Berry and say they’re sorr</p><p>6 0.97924101 <a title="342-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-01-Philosophy_of_Bayesian_statistics%3A__my_reactions_to_Cox_and_Mayo.html">1149 andrew gelman stats-2012-02-01-Philosophy of Bayesian statistics:  my reactions to Cox and Mayo</a></p>
<p>7 0.97851467 <a title="342-lda-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-28-Difficulties_of_using_statistical_significance_%28or_lack_thereof%29_to_sift_through_and_compare_research_hypotheses.html">2042 andrew gelman stats-2013-09-28-Difficulties of using statistical significance (or lack thereof) to sift through and compare research hypotheses</a></p>
<p>8 0.97851217 <a title="342-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>9 0.97829348 <a title="342-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-23-Validation_of_Software_for_Bayesian_Models_Using_Posterior_Quantiles.html">1868 andrew gelman stats-2013-05-23-Validation of Software for Bayesian Models Using Posterior Quantiles</a></p>
<p>10 0.97807777 <a title="342-lda-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-25-Clusters_with_very_small_numbers_of_observations.html">295 andrew gelman stats-2010-09-25-Clusters with very small numbers of observations</a></p>
<p>11 0.97786462 <a title="342-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-22-%E2%80%9CThe_comment_section_is_open%2C_but_I%E2%80%99m_not_going_to_read_them%E2%80%9D.html">1994 andrew gelman stats-2013-08-22-“The comment section is open, but I’m not going to read them”</a></p>
<p>12 0.97757494 <a title="342-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-04-When_is_there_%E2%80%9Chidden_structure_in_data%E2%80%9D_to_be_discovered%3F.html">1788 andrew gelman stats-2013-04-04-When is there “hidden structure in data” to be discovered?</a></p>
<p>13 0.97732091 <a title="342-lda-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-24-Empirical_implications_of_Empirical_Implications_of_Theoretical_Models.html">2263 andrew gelman stats-2014-03-24-Empirical implications of Empirical Implications of Theoretical Models</a></p>
<p>14 0.97731805 <a title="342-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-16-Long_discussion_about_causal_inference_and_the_use_of_hierarchical_models_to_bridge_between_different_inferential_settings.html">1418 andrew gelman stats-2012-07-16-Long discussion about causal inference and the use of hierarchical models to bridge between different inferential settings</a></p>
<p>15 0.97707236 <a title="342-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-16-Infovis_and_statgraphics_update_update.html">855 andrew gelman stats-2011-08-16-Infovis and statgraphics update update</a></p>
<p>16 0.97687352 <a title="342-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-12-As_a_Bayesian_I_want_scientists_to_report_their_data_non-Bayesianly.html">1209 andrew gelman stats-2012-03-12-As a Bayesian I want scientists to report their data non-Bayesianly</a></p>
<p>17 0.97675967 <a title="342-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-11-Bayesian_brains%3F.html">1529 andrew gelman stats-2012-10-11-Bayesian brains?</a></p>
<p>18 0.97664124 <a title="342-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-12-Do_you_ever_have_that_I-just-fit-a-model_feeling%3F.html">2018 andrew gelman stats-2013-09-12-Do you ever have that I-just-fit-a-model feeling?</a></p>
<p>19 0.97658998 <a title="342-lda-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-07-Selection_bias_in_the_reporting_of_shaky_research.html">2236 andrew gelman stats-2014-03-07-Selection bias in the reporting of shaky research</a></p>
<p>20 0.97649509 <a title="342-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Allowing_interaction_terms_to_vary.html">753 andrew gelman stats-2011-06-09-Allowing interaction terms to vary</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
