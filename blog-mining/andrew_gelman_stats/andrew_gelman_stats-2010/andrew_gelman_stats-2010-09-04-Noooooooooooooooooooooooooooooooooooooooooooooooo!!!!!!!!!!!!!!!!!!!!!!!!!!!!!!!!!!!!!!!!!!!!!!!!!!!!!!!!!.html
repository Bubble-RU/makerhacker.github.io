<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>256 andrew gelman stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-256" href="#">andrew_gelman_stats-2010-256</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>256 andrew gelman stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-256-html" href="http://andrewgelman.com/2010/09/04/noooooooooooooo_1/">html</a></p><p>Introduction: Masanao sends  this one  in, under the heading, “another incident of misunderstood p-value”:
  
Warren Davies, a positive psychology MSc student at UEL, provides the latest in our ongoing series of guest features for students. Warren has just released a Psychology Study Guide, which covers information on statistics, research methods and study skills for psychology students.

 
Despite the myriad rules and procedures of science, some research findings are pure flukes. Perhaps you’re testing a new drug, and by chance alone, a large number of people spontaneously get better. The better your study is conducted, the lower the chance that your result was a fluke – but still, there is always a certain probability that it was.


Statistical significance testing gives you an idea of what this probability is.


In science we’re always testing hypotheses. We never conduct a study to ‘see what happens’, because there’s always at least one way to make any useless set of data look important. We take</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Masanao sends  this one  in, under the heading, “another incident of misunderstood p-value”:    Warren Davies, a positive psychology MSc student at UEL, provides the latest in our ongoing series of guest features for students. [sent-1, score-0.584]
</p><p>2 Warren has just released a Psychology Study Guide, which covers information on statistics, research methods and study skills for psychology students. [sent-2, score-0.602]
</p><p>3 Despite the myriad rules and procedures of science, some research findings are pure flukes. [sent-3, score-0.194]
</p><p>4 Perhaps you’re testing a new drug, and by chance alone, a large number of people spontaneously get better. [sent-4, score-0.246]
</p><p>5 The better your study is conducted, the lower the chance that your result was a fluke – but still, there is always a certain probability that it was. [sent-5, score-0.551]
</p><p>6 Statistical significance testing gives you an idea of what this probability is. [sent-6, score-0.411]
</p><p>7 We never conduct a study to ‘see what happens’, because there’s always at least one way to make any useless set of data look important. [sent-8, score-0.368]
</p><p>8 We take a risk; we put our idea on the line and expose it to potential refutation. [sent-9, score-0.102]
</p><p>9 Therefore, all statistical tests in psychology test the possibility that the hypothesis is correct, versus the possibility that it isn’t. [sent-10, score-0.747]
</p><p>10 I don’t blame Warren Davies–it’s all-too-common for someone teaching statistics to (a) make a mistake and (b) not realize it. [sent-13, score-0.318]
</p><p>11 But I do blame the editors of the website for getting a non-expert to emit wrong information. [sent-14, score-0.255]
</p><p>12 One thing that any  research psychologist  should know is that statistics is tricky. [sent-15, score-0.237]
</p><p>13 I hate to see this sort of mistake (saying that statistical significance is a measure of the probability the null hypothesis is true) being given the official endorsement of British Psychological Society. [sent-16, score-1.103]
</p><p>14 To any confused readers out there:  The p-value is the probability of seeing something as extreme as the data or more so, if the null hypothesis were true. [sent-19, score-0.732]
</p><p>15 In social science (and I think in psychology as well), the null hypothesis is almost certainly false, false, false, and you don’t need a p-value to tell you this. [sent-20, score-0.983]
</p><p>16 The p-value tells you the extent to which a certain aspect of your data are consistent with the null hypothesis. [sent-21, score-0.611]
</p><p>17 A lack of rejection doesn’t tell you that the null hyp is likely true; rather, it tells you that you don’t have enough data to reject the null hyp. [sent-22, score-1.151]
</p><p>18 For more more more on this, see for example  this paper with David Weakliem  which was written for a nontechnical audience. [sent-23, score-0.117]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('null', 0.395), ('warren', 0.276), ('psychology', 0.226), ('davies', 0.204), ('hypothesis', 0.189), ('false', 0.162), ('testing', 0.159), ('probability', 0.148), ('blame', 0.138), ('possibility', 0.132), ('study', 0.13), ('tells', 0.121), ('emit', 0.117), ('nontechnical', 0.117), ('bps', 0.11), ('mistake', 0.107), ('weakliem', 0.105), ('significance', 0.104), ('expose', 0.102), ('myriad', 0.102), ('certain', 0.095), ('digest', 0.094), ('misunderstood', 0.094), ('guest', 0.094), ('research', 0.092), ('endorsement', 0.092), ('incident', 0.092), ('always', 0.091), ('masanao', 0.09), ('handy', 0.09), ('tell', 0.088), ('chance', 0.087), ('science', 0.085), ('covers', 0.083), ('heading', 0.082), ('zombies', 0.082), ('rejection', 0.08), ('ongoing', 0.078), ('conduct', 0.076), ('british', 0.075), ('statistics', 0.073), ('guide', 0.073), ('true', 0.072), ('reject', 0.072), ('ridiculous', 0.072), ('psychologist', 0.072), ('useless', 0.071), ('released', 0.071), ('versus', 0.068), ('official', 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="256-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21.html">256 andrew gelman stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</a></p>
<p>Introduction: Masanao sends  this one  in, under the heading, “another incident of misunderstood p-value”:
  
Warren Davies, a positive psychology MSc student at UEL, provides the latest in our ongoing series of guest features for students. Warren has just released a Psychology Study Guide, which covers information on statistics, research methods and study skills for psychology students.

 
Despite the myriad rules and procedures of science, some research findings are pure flukes. Perhaps you’re testing a new drug, and by chance alone, a large number of people spontaneously get better. The better your study is conducted, the lower the chance that your result was a fluke – but still, there is always a certain probability that it was.


Statistical significance testing gives you an idea of what this probability is.


In science we’re always testing hypotheses. We never conduct a study to ‘see what happens’, because there’s always at least one way to make any useless set of data look important. We take</p><p>2 0.21957479 <a title="256-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-31-Lindley%E2%80%99s_paradox.html">1355 andrew gelman stats-2012-05-31-Lindley’s paradox</a></p>
<p>Introduction: Sam Seaver writes:
  
I [Seaver] happened to be reading an ironic  article  by Karl Friston when I learned something new about frequentist vs bayesian, namely Lindley’s paradox, on page 12.  The text is as follows:

 
So why are we worried about trivial effects? They are important because the probability that the true effect size is exactly zero is itself zero and could cause us to reject the null hypothesis inappropriately. This is a fallacy of classical inference and is not unrelated to Lindley’s paradox (Lindley 1957). Lindley’s paradox describes a counterintuitive situation in which Bayesian and frequentist approaches to hypothesis testing give opposite results. It occurs when; (i) a result is significant by a frequentist test, indicating sufficient evidence to reject the null hypothesis d=0 and (ii) priors render the posterior probability of d=0 high, indicating strong evidence that the null hypothesis is true. In his original 
treatment, Lindley (1957) showed that – under a parti</p><p>3 0.21759981 <a title="256-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-26-%E2%80%9CA_Vast_Graveyard_of_Undead_Theories%3A__Publication_Bias_and_Psychological_Science%E2%80%99s_Aversion_to_the_Null%E2%80%9D.html">1826 andrew gelman stats-2013-04-26-“A Vast Graveyard of Undead Theories:  Publication Bias and Psychological Science’s Aversion to the Null”</a></p>
<p>Introduction: Erin Jonaitis points us to  this article  by Christopher Ferguson and Moritz Heene, who write:
  
Publication bias remains a controversial issue in psychological science. . . . that the field often constructs arguments to block the publication and interpretation of null results and that null results may be further extinguished through questionable researcher practices. Given that science is dependent on the process of falsification, we argue that these problems reduce psychological science’s capability to have a proper mechanism for theory falsification, thus resulting in the promulgation of numerous “undead” theories that are ideologically popular but have little basis in fact.
  
They mention the infamous Daryl Bem article.  It is pretty much only because Bem’s claims are (presumably) false that they got published in a major research journal.  Had the claims been true—that is, had Bem run identical experiments, analyzed his data more carefully and objectively, and reported that the r</p><p>4 0.20960732 <a title="256-tfidf-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>Introduction: A recent  discussion  between commenters Question and Fernando captured one of the recurrent themes here from the past year.
 
 Question:   The problem is simple, the researchers are disproving always false null hypotheses and taking this disproof as near proof that their theory is correct.
 
 Fernando:   Whereas it is probably true that researchers misuse NHT, the problem with tabloid science is broader and deeper. It is systemic.
 
 Question:   I do not see how anything can be deeper than replacing careful description, prediction, falsification, and independent replication with dynamite plots, p-values, affirming the consequent, and peer review. From my own experience I am confident in saying that confusion caused by NHST is at the root of this problem.
 
 Fernando:   Incentives? Impact factors? Publish or die? “Interesting” and “new” above quality and reliability, or actually answering a research question, and a silly and unbecoming obsession with being quoted in NYT, etc. . . . Giv</p><p>5 0.2076408 <a title="256-tfidf-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-18-One-tailed_or_two-tailed%3F.html">2295 andrew gelman stats-2014-04-18-One-tailed or two-tailed?</a></p>
<p>Introduction: Someone writes:
  
Suppose I have two groups of people, A and B, which differ on some characteristic of interest to me;  and for each person I measure a single real-valued quantity X.  I have a theory that group A has a higher mean value of X than group B.  I test this theory by using a t-test.  Am I entitled to use a *one-tailed* t-test?  Or should I use a *two-tailed* one (thereby giving a p-value that is twice as large)?


I know you will probably answer:  Forget the t-test; you should use Bayesian methods instead.


But what is the standard frequentist answer to this question?
  
My reply:
 
The quick answer here is that different people will do different things here.  I would say the 2-tailed p-value is more standard but some people will insist on the one-tailed version, and itâ&euro;&trade;s hard to make a big stand on this one, given all the other problems with p-values in practice:
 
http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf
 
http://www.stat.columbia.edu/~gelm</p><p>6 0.20305212 <a title="256-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-24-In_which_I_side_with_Neyman_over_Fisher.html">1869 andrew gelman stats-2013-05-24-In which I side with Neyman over Fisher</a></p>
<p>7 0.19944033 <a title="256-tfidf-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-24-Empirical_implications_of_Empirical_Implications_of_Theoretical_Models.html">2263 andrew gelman stats-2014-03-24-Empirical implications of Empirical Implications of Theoretical Models</a></p>
<p>8 0.19476987 <a title="256-tfidf-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-23-Discussion_on_preregistration_of_research_studies.html">2183 andrew gelman stats-2014-01-23-Discussion on preregistration of research studies</a></p>
<p>9 0.1938335 <a title="256-tfidf-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Ken_Rice_presents_a_unifying_approach_to_statistical_inference_and_hypothesis_testing.html">2312 andrew gelman stats-2014-04-29-Ken Rice presents a unifying approach to statistical inference and hypothesis testing</a></p>
<p>10 0.18908873 <a title="256-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-12-Misunderstanding_the_p-value.html">1760 andrew gelman stats-2013-03-12-Misunderstanding the p-value</a></p>
<p>11 0.18482777 <a title="256-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-23-Of_hypothesis_tests_and_Unitarians.html">1024 andrew gelman stats-2011-11-23-Of hypothesis tests and Unitarians</a></p>
<p>12 0.17291561 <a title="256-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-20-More_proposals_to_reform_the_peer-review_system.html">1272 andrew gelman stats-2012-04-20-More proposals to reform the peer-review system</a></p>
<p>13 0.17270106 <a title="256-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-06-That_silly_ESP_paper_and_some_silliness_in_a_rebuttal_as_well.html">506 andrew gelman stats-2011-01-06-That silly ESP paper and some silliness in a rebuttal as well</a></p>
<p>14 0.16503638 <a title="256-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>15 0.16342011 <a title="256-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-15-%E2%80%9CAre_all_significant_p-values_created_equal%3F%E2%80%9D.html">2102 andrew gelman stats-2013-11-15-“Are all significant p-values created equal?”</a></p>
<p>16 0.16061638 <a title="256-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-05-The_p-value_is_not_._._..html">1607 andrew gelman stats-2012-12-05-The p-value is not . . .</a></p>
<p>17 0.15392223 <a title="256-tfidf-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-29-I_agree_with_this_comment.html">2272 andrew gelman stats-2014-03-29-I agree with this comment</a></p>
<p>18 0.15329534 <a title="256-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-04-Write_This_Book.html">1605 andrew gelman stats-2012-12-04-Write This Book</a></p>
<p>19 0.15125999 <a title="256-tfidf-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-20-How_to_schedule_projects_in_an_introductory_statistics_course%3F.html">423 andrew gelman stats-2010-11-20-How to schedule projects in an introductory statistics course?</a></p>
<p>20 0.14690083 <a title="256-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-07-I%E2%80%99m_negative_on_the_expression_%E2%80%9Cfalse_positives%E2%80%9D.html">2093 andrew gelman stats-2013-11-07-I’m negative on the expression “false positives”</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.217), (1, -0.004), (2, -0.046), (3, -0.135), (4, -0.087), (5, -0.044), (6, -0.048), (7, 0.063), (8, -0.034), (9, -0.12), (10, -0.108), (11, 0.027), (12, 0.036), (13, -0.162), (14, -0.023), (15, -0.021), (16, -0.043), (17, -0.073), (18, 0.003), (19, -0.104), (20, 0.059), (21, 0.022), (22, -0.03), (23, -0.019), (24, -0.101), (25, -0.062), (26, 0.057), (27, 0.008), (28, -0.007), (29, -0.039), (30, 0.022), (31, -0.018), (32, 0.046), (33, 0.033), (34, -0.108), (35, -0.06), (36, 0.076), (37, -0.086), (38, -0.014), (39, 0.016), (40, -0.05), (41, 0.012), (42, 0.031), (43, -0.039), (44, 0.007), (45, 0.077), (46, -0.006), (47, -0.028), (48, 0.045), (49, 0.009)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95551068 <a title="256-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21.html">256 andrew gelman stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</a></p>
<p>Introduction: Masanao sends  this one  in, under the heading, “another incident of misunderstood p-value”:
  
Warren Davies, a positive psychology MSc student at UEL, provides the latest in our ongoing series of guest features for students. Warren has just released a Psychology Study Guide, which covers information on statistics, research methods and study skills for psychology students.

 
Despite the myriad rules and procedures of science, some research findings are pure flukes. Perhaps you’re testing a new drug, and by chance alone, a large number of people spontaneously get better. The better your study is conducted, the lower the chance that your result was a fluke – but still, there is always a certain probability that it was.


Statistical significance testing gives you an idea of what this probability is.


In science we’re always testing hypotheses. We never conduct a study to ‘see what happens’, because there’s always at least one way to make any useless set of data look important. We take</p><p>2 0.84193164 <a title="256-lsi-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-29-I_agree_with_this_comment.html">2272 andrew gelman stats-2014-03-29-I agree with this comment</a></p>
<p>Introduction: The anonymous commenter  puts it well :
  
The problem is simple, the researchers are disproving always false null hypotheses and taking this disproof as near proof that their theory is correct.</p><p>3 0.84140444 <a title="256-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-23-Of_hypothesis_tests_and_Unitarians.html">1024 andrew gelman stats-2011-11-23-Of hypothesis tests and Unitarians</a></p>
<p>Introduction: Xian, Judith, and I read this line in a book by statistician Murray Aitkin in which he considered the following hypothetical example:
  
A survey of 100 individuals expressing support (Yes/No) for the president, before and after a presidential address . . . The question of interest is whether there has been a change in support between the surveys . . . We want to assess the evidence for the hypothesis of equality H1 against the alternative hypothesis H2 of a change.
  
Here is  our response :
  
Based on our experience in public opinion research, this is not a real question. Support for any political position is always changing. The real question is how much the support has changed, or perhaps how this change is distributed across the population.


A defender of Aitkin (and of classical hypothesis testing) might respond at this point that, yes, everybody knows that changes are never exactly zero and that we should take a more “grown-up” view of the null hypothesis, not that the change</p><p>4 0.82776403 <a title="256-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-31-Lindley%E2%80%99s_paradox.html">1355 andrew gelman stats-2012-05-31-Lindley’s paradox</a></p>
<p>Introduction: Sam Seaver writes:
  
I [Seaver] happened to be reading an ironic  article  by Karl Friston when I learned something new about frequentist vs bayesian, namely Lindley’s paradox, on page 12.  The text is as follows:

 
So why are we worried about trivial effects? They are important because the probability that the true effect size is exactly zero is itself zero and could cause us to reject the null hypothesis inappropriately. This is a fallacy of classical inference and is not unrelated to Lindley’s paradox (Lindley 1957). Lindley’s paradox describes a counterintuitive situation in which Bayesian and frequentist approaches to hypothesis testing give opposite results. It occurs when; (i) a result is significant by a frequentist test, indicating sufficient evidence to reject the null hypothesis d=0 and (ii) priors render the posterior probability of d=0 high, indicating strong evidence that the null hypothesis is true. In his original 
treatment, Lindley (1957) showed that – under a parti</p><p>5 0.82243502 <a title="256-lsi-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>Introduction: A recent  discussion  between commenters Question and Fernando captured one of the recurrent themes here from the past year.
 
 Question:   The problem is simple, the researchers are disproving always false null hypotheses and taking this disproof as near proof that their theory is correct.
 
 Fernando:   Whereas it is probably true that researchers misuse NHT, the problem with tabloid science is broader and deeper. It is systemic.
 
 Question:   I do not see how anything can be deeper than replacing careful description, prediction, falsification, and independent replication with dynamite plots, p-values, affirming the consequent, and peer review. From my own experience I am confident in saying that confusion caused by NHST is at the root of this problem.
 
 Fernando:   Incentives? Impact factors? Publish or die? “Interesting” and “new” above quality and reliability, or actually answering a research question, and a silly and unbecoming obsession with being quoted in NYT, etc. . . . Giv</p><p>6 0.81951141 <a title="256-lsi-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-18-One-tailed_or_two-tailed%3F.html">2295 andrew gelman stats-2014-04-18-One-tailed or two-tailed?</a></p>
<p>7 0.79911041 <a title="256-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-15-%E2%80%9CAre_all_significant_p-values_created_equal%3F%E2%80%9D.html">2102 andrew gelman stats-2013-11-15-“Are all significant p-values created equal?”</a></p>
<p>8 0.79535902 <a title="256-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-26-%E2%80%9CA_Vast_Graveyard_of_Undead_Theories%3A__Publication_Bias_and_Psychological_Science%E2%80%99s_Aversion_to_the_Null%E2%80%9D.html">1826 andrew gelman stats-2013-04-26-“A Vast Graveyard of Undead Theories:  Publication Bias and Psychological Science’s Aversion to the Null”</a></p>
<p>9 0.79186052 <a title="256-lsi-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-23-Discussion_on_preregistration_of_research_studies.html">2183 andrew gelman stats-2014-01-23-Discussion on preregistration of research studies</a></p>
<p>10 0.79101276 <a title="256-lsi-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-24-Empirical_implications_of_Empirical_Implications_of_Theoretical_Models.html">2263 andrew gelman stats-2014-03-24-Empirical implications of Empirical Implications of Theoretical Models</a></p>
<p>11 0.77544338 <a title="256-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-24-In_which_I_side_with_Neyman_over_Fisher.html">1869 andrew gelman stats-2013-05-24-In which I side with Neyman over Fisher</a></p>
<p>12 0.77474928 <a title="256-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>13 0.76727146 <a title="256-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>14 0.74355298 <a title="256-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-12-Misunderstanding_the_p-value.html">1760 andrew gelman stats-2013-03-12-Misunderstanding the p-value</a></p>
<p>15 0.70485973 <a title="256-lsi-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-10-Bayes_jumps_the_shark.html">331 andrew gelman stats-2010-10-10-Bayes jumps the shark</a></p>
<p>16 0.70014042 <a title="256-lsi-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Ken_Rice_presents_a_unifying_approach_to_statistical_inference_and_hypothesis_testing.html">2312 andrew gelman stats-2014-04-29-Ken Rice presents a unifying approach to statistical inference and hypothesis testing</a></p>
<p>17 0.68264133 <a title="256-lsi-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-08-Discussion_with_Steven_Pinker_on_research_that_is_attached_to_data_that_are_so_noisy_as_to_be_essentially_uninformative.html">2326 andrew gelman stats-2014-05-08-Discussion with Steven Pinker on research that is attached to data that are so noisy as to be essentially uninformative</a></p>
<p>18 0.6697613 <a title="256-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-04-Write_This_Book.html">1605 andrew gelman stats-2012-12-04-Write This Book</a></p>
<p>19 0.66617388 <a title="256-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-25-The_harm_done_by_tests_of_significance.html">1776 andrew gelman stats-2013-03-25-The harm done by tests of significance</a></p>
<p>20 0.66539806 <a title="256-lsi-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-20-How_to_schedule_projects_in_an_introductory_statistics_course%3F.html">423 andrew gelman stats-2010-11-20-How to schedule projects in an introductory statistics course?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.045), (15, 0.043), (16, 0.041), (21, 0.06), (24, 0.146), (59, 0.02), (63, 0.03), (69, 0.083), (75, 0.026), (86, 0.018), (99, 0.344)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98221904 <a title="256-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-22-Tenants_and_landlords.html">158 andrew gelman stats-2010-07-22-Tenants and landlords</a></p>
<p>Introduction: Matthew Yglesias  and  Megan McArdle  argue about the economics of landlord/tenant laws in D.C., a topic I know nothing about.  But it did remind me of a few stories . . .
 
1.  In grad school, I shared half of a two-family house with three other students.  At some point, our landlord (who lived in the other half of the house) decided he wanted to sell the place, so he had a real estate agent coming by occasionally to show the house to people.  She was just a flat-out liar (which I guess fits my impression based on screenings of Glengarry Glen Ross).  I could never decide, when I was around and she was lying to a prospective buyer, whether to call her on it.  Sometimes I did, sometimes I didn’t.
 
2.  A year after I graduated, the landlord actually did sell the place but then, when my friends moved out, he refused to pay back their security deposit.  There was some debate about getting the place repainted, I don’t remember the details.  So they sued the landlord in Mass. housing court</p><p>same-blog 2 0.97583568 <a title="256-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21%21.html">256 andrew gelman stats-2010-09-04-Noooooooooooooooooooooooooooooooooooooooooooooooo!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</a></p>
<p>Introduction: Masanao sends  this one  in, under the heading, “another incident of misunderstood p-value”:
  
Warren Davies, a positive psychology MSc student at UEL, provides the latest in our ongoing series of guest features for students. Warren has just released a Psychology Study Guide, which covers information on statistics, research methods and study skills for psychology students.

 
Despite the myriad rules and procedures of science, some research findings are pure flukes. Perhaps you’re testing a new drug, and by chance alone, a large number of people spontaneously get better. The better your study is conducted, the lower the chance that your result was a fluke – but still, there is always a certain probability that it was.


Statistical significance testing gives you an idea of what this probability is.


In science we’re always testing hypotheses. We never conduct a study to ‘see what happens’, because there’s always at least one way to make any useless set of data look important. We take</p><p>3 0.97306931 <a title="256-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-24-What_is_the_normal_range_of_values_in_a_medical_test%3F.html">923 andrew gelman stats-2011-09-24-What is the normal range of values in a medical test?</a></p>
<p>Introduction: Geoffrey Sheean writes: 
  
  
I am having trouble thinking Bayesianly about the so-called ‘normal’ or ‘reference’ values that I am supposed to use in some of the tests I perform. These values are obtained from purportedly healthy people. Setting aside concerns about ascertainment bias, non-parametric distributions, and the like, the values are usually obtained by setting the limits at ± 2SD from the mean. In some cases, supposedly because of a non-normal distribution, the third highest and lowest value observed in the healthy group sets the limits, on the assumption that no more than 2 results (out of 20 samples) are allowed to exceed these values: if there are 3 or more, then the test is assumed to be abnormal and the reference range is said to reflect the 90th percentile. The results are binary – normal, abnormal.


The relevance to the diseased state is this. People who are known unequivocally to have condition X show Y abnormalities in these tests. Therefore, when people suspected</p><p>4 0.97248107 <a title="256-lda-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-18-Tibshirani_announces_new_research_result%3A__A_significance_test_for_the_lasso.html">1769 andrew gelman stats-2013-03-18-Tibshirani announces new research result:  A significance test for the lasso</a></p>
<p>Introduction: Lasso and me 
 
For a long time I was wrong about lasso.
 
Lasso (“least absolute shrinkage and selection operator”) is a regularization procedure that shrinks regression coefficients toward zero, and in its basic form is equivalent to maximum penalized likelihood estimation with a penalty function that is proportional to the sum of the absolute values of the regression coefficients.
 
I first heard about lasso from a talk that  Trevor Hastie  Rob Tibshirani gave at Berkeley in 1994 or 1995.  He demonstrated that it shrunk regression coefficients to zero.  I wasn’t impressed, first because it seemed like no big deal (if that’s the prior you use, that’s the shrinkage you get) and second because, from a Bayesian perspective, I don’t  want  to shrink things all the way to zero.  In the sorts of social and environmental science problems I’ve worked on, just about nothing is zero.  I’d like to control my noisy estimates but there’s nothing special about zero.  At the end of the talk I stood</p><p>5 0.97163224 <a title="256-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-10-Translating_into_Votes%3A_The_Electoral_Impact_of_Spanish-Language_Ballots.html">406 andrew gelman stats-2010-11-10-Translating into Votes: The Electoral Impact of Spanish-Language Ballots</a></p>
<p>Introduction: Dan Hopkins sends along  this article :
  
[Hopkins] uses regression discontinuity design to estimate the turnout and election impacts of Spanish-language assistance provided under Section 203 of the Voting Rights Act. Analyses of two different data sets – the Latino National Survey and California 1998 primary election returns – show that Spanish-language assistance increased turnout for citizens who speak little English. The California results also demonstrate that election procedures an influence outcomes, as support for ending bilingual education dropped markedly in heavily Spanish-speaking neighborhoods with Spanish-language assistance. The California analyses find hints of backlash among non-Hispanic white precincts, but not with the same size or certainty. Small changes in election procedures can influence who votes as well as what wins.
  
Beyond the direct relevance of these results, I find this paper interesting as an example of research that is fundamentally quantitative.  Th</p><p>6 0.97056305 <a title="256-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Job_openings_at_conservative_political_analytics_firm%21.html">1909 andrew gelman stats-2013-06-21-Job openings at conservative political analytics firm!</a></p>
<p>7 0.96953869 <a title="256-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-16-A_historical_perspective_on_financial_bailouts.html">89 andrew gelman stats-2010-06-16-A historical perspective on financial bailouts</a></p>
<p>8 0.96860534 <a title="256-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-15-Regression_discontinuity_designs%3A__looking_for_the_keys_under_the_lamppost%3F.html">518 andrew gelman stats-2011-01-15-Regression discontinuity designs:  looking for the keys under the lamppost?</a></p>
<p>9 0.96790111 <a title="256-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-06-%E2%80%9CSampling%3A__Design_and_Analysis%E2%80%9D%3A__a_course_for_political_science_graduate_students.html">749 andrew gelman stats-2011-06-06-“Sampling:  Design and Analysis”:  a course for political science graduate students</a></p>
<p>10 0.96639985 <a title="256-lda-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-31-Two_stories_about_the_election_that_I_don%E2%80%99t_believe.html">384 andrew gelman stats-2010-10-31-Two stories about the election that I don’t believe</a></p>
<p>11 0.96522474 <a title="256-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-01-Halloween-Valentine%E2%80%99s_update.html">1357 andrew gelman stats-2012-06-01-Halloween-Valentine’s update</a></p>
<p>12 0.96501231 <a title="256-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-17-Hierarchical-multilevel_modeling_with_%E2%80%9Cbig_data%E2%80%9D.html">1267 andrew gelman stats-2012-04-17-Hierarchical-multilevel modeling with “big data”</a></p>
<p>13 0.96386445 <a title="256-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-11-Jonathan_Chait_and_I_agree_about_the_importance_of_the_fundamentals_in_determining_presidential_elections.html">656 andrew gelman stats-2011-04-11-Jonathan Chait and I agree about the importance of the fundamentals in determining presidential elections</a></p>
<p>14 0.96322054 <a title="256-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-09-There%E2%80%99s_no_evidence_that_voters_choose_presidential_candidates_based_on_their_looks.html">654 andrew gelman stats-2011-04-09-There’s no evidence that voters choose presidential candidates based on their looks</a></p>
<p>15 0.96149313 <a title="256-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-23-Scientists_can_read_your_mind_._._._as_long_as_the%E2%80%99re_allowed_to_look_at_more_than_one_place_in_your_brain_and_then_make_a_prediction_after_seeing_what_you_actually_did.html">106 andrew gelman stats-2010-06-23-Scientists can read your mind . . . as long as the’re allowed to look at more than one place in your brain and then make a prediction after seeing what you actually did</a></p>
<p>16 0.96133411 <a title="256-lda-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-26-Age_and_happiness%3A__The_pattern_isn%E2%80%99t_as_clear_as_you_might_think.html">486 andrew gelman stats-2010-12-26-Age and happiness:  The pattern isn’t as clear as you might think</a></p>
<p>17 0.96124595 <a title="256-lda-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-10-Spring_forward%2C_fall_back%2C_drop_dead%3F.html">2367 andrew gelman stats-2014-06-10-Spring forward, fall back, drop dead?</a></p>
<p>18 0.96095836 <a title="256-lda-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-07-Selection_bias_in_the_reporting_of_shaky_research.html">2236 andrew gelman stats-2014-03-07-Selection bias in the reporting of shaky research</a></p>
<p>19 0.96050459 <a title="256-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-11-Update_on_the_spam_email_study.html">27 andrew gelman stats-2010-05-11-Update on the spam email study</a></p>
<p>20 0.9603951 <a title="256-lda-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-What_is_the_appropriate_time_scale_for_blogging%E2%80%94the_day_or_the_week%3F.html">2232 andrew gelman stats-2014-03-03-What is the appropriate time scale for blogging—the day or the week?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
