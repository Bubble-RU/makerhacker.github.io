<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>112 andrew gelman stats-2010-06-27-Sampling rate of human-scaled time series</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-112" href="#">andrew_gelman_stats-2010-112</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>112 andrew gelman stats-2010-06-27-Sampling rate of human-scaled time series</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-112-html" href="http://andrewgelman.com/2010/06/27/sampling_rate_o/">html</a></p><p>Introduction: Bill Harris writes with two interesting questions involving time series analysis:
  
 
I used to work in an organization that designed and made signal processing equipment.  Antialiasing and windowing of time series was a big deal in performing analysis accurately.  


Now I’m in a place where I have to make inferences about human-scaled time series.  It has dawned on me that the two are related.  I’m not sure we often have data sampled at a rate at least twice the highest frequency present (not just the highest frequency of interest).  The only articles I’ve seen about aliasing as applied to social science series are from  Hinich  or from  related works .  Box and Jenkins hint at it in section 13.3 of Time Series Analysis, but the analysis seems to be mostly heuristic.  Yet I can imagine all sorts of time series subject to similar problems, from analyses of stock prices based on closing prices (mentioned in the latter article) to other economic series measured on a monthly basis to en</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Bill Harris writes with two interesting questions involving time series analysis:      I used to work in an organization that designed and made signal processing equipment. [sent-1, score-0.804]
</p><p>2 Antialiasing and windowing of time series was a big deal in performing analysis accurately. [sent-2, score-1.231]
</p><p>3 Now I’m in a place where I have to make inferences about human-scaled time series. [sent-3, score-0.197]
</p><p>4 I’m not sure we often have data sampled at a rate at least twice the highest frequency present (not just the highest frequency of interest). [sent-5, score-0.456]
</p><p>5 The only articles I’ve seen about aliasing as applied to social science series are from  Hinich  or from  related works . [sent-6, score-0.844]
</p><p>6 Yet I can imagine all sorts of time series subject to similar problems, from analyses of stock prices based on closing prices (mentioned in the latter article) to other economic series measured on a monthly basis to energy usage measured on an hourly or quarter-hourly basis. [sent-9, score-1.885]
</p><p>7 What do statisticians in the social sciences and economics do to deal with such problems? [sent-11, score-0.281]
</p><p>8 Now that I think about this, I see advantages to your stance of repeated regressions at subsequent intervals rather than moving to a full time series analysis. [sent-12, score-0.787]
</p><p>9 At least when you’re transforming a time series with a discrete Fourier transform, the assumption is made that the time series is periodic. [sent-16, score-1.556]
</p><p>10 Because it’s rarely exactly periodic in the real world, the math will distort the signal. [sent-17, score-0.214]
</p><p>11 Windowing is a way of tapering the time series to zero at both ends, thus moving distortion products out of the band of interest. [sent-18, score-0.923]
</p><p>12 While aliasing is uncorrectable after sampling, windowing is done later. [sent-20, score-0.618]
</p><p>13 I don’t see attention to windowing in treating time series in the social sciences, either. [sent-21, score-1.189]
</p><p>14 I’m thinking back through my math to see if I can demonstrate whether the assumption of periodicity applies even if there is no Fourier transform in the picture. [sent-22, score-0.355]
</p><p>15 Do you see evidence of economists or statisticians applying windows to their time series? [sent-23, score-0.263]
</p><p>16 I wonder if this could apply to the fitting of model parameters via MCSim or other tools that offer model parameter estimation for time series analysis. [sent-25, score-0.852]
</p><p>17 If you have undersampled data and you try to fit a model to that data, even MCMC integration won’t fit the data properly, and so you could get erroneous parameter estimates, or so it would seem. [sent-26, score-0.288]
</p><p>18 That’s not a problem with MCSim, but it would seem to be a problem with the analysis that prepares the data for MCSim. [sent-27, score-0.179]
</p><p>19 For whatever reason, I’ve avoided time series modeling in most of my work  Also, classical time-series analysis hasn’t been so useful for me because that theory tends to focus on direct observations. [sent-29, score-0.939]
</p><p>20 For example, when we’re studying time trends in death penalty support by state, we have sample survey data that gives us estimates for each state and year–and, indeed, we’re fitting time series models to get good estimates–but issues of sampling frequencies seem a bit beside the point. [sent-31, score-1.339]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('series', 0.51), ('windowing', 0.353), ('aliasing', 0.265), ('time', 0.197), ('mcsim', 0.161), ('fourier', 0.14), ('transform', 0.115), ('analysis', 0.103), ('frequency', 0.101), ('signal', 0.097), ('prices', 0.095), ('measured', 0.09), ('energy', 0.09), ('highest', 0.089), ('estimates', 0.083), ('assumption', 0.081), ('hourly', 0.08), ('jenkins', 0.08), ('periodicity', 0.08), ('snapshots', 0.08), ('moving', 0.08), ('math', 0.079), ('sciences', 0.078), ('data', 0.076), ('prey', 0.076), ('fitting', 0.075), ('bill', 0.074), ('hint', 0.073), ('distortion', 0.07), ('periodic', 0.07), ('reconstructing', 0.07), ('sampling', 0.07), ('parameter', 0.07), ('social', 0.069), ('deal', 0.068), ('beside', 0.068), ('closing', 0.068), ('band', 0.066), ('erroneous', 0.066), ('problems', 0.066), ('statisticians', 0.066), ('avoided', 0.065), ('distort', 0.065), ('focus', 0.064), ('frequencies', 0.063), ('signals', 0.062), ('harris', 0.061), ('transforming', 0.061), ('treating', 0.06), ('usage', 0.06)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999976 <a title="112-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-27-Sampling_rate_of_human-scaled_time_series.html">112 andrew gelman stats-2010-06-27-Sampling rate of human-scaled time series</a></p>
<p>Introduction: Bill Harris writes with two interesting questions involving time series analysis:
  
 
I used to work in an organization that designed and made signal processing equipment.  Antialiasing and windowing of time series was a big deal in performing analysis accurately.  


Now I’m in a place where I have to make inferences about human-scaled time series.  It has dawned on me that the two are related.  I’m not sure we often have data sampled at a rate at least twice the highest frequency present (not just the highest frequency of interest).  The only articles I’ve seen about aliasing as applied to social science series are from  Hinich  or from  related works .  Box and Jenkins hint at it in section 13.3 of Time Series Analysis, but the analysis seems to be mostly heuristic.  Yet I can imagine all sorts of time series subject to similar problems, from analyses of stock prices based on closing prices (mentioned in the latter article) to other economic series measured on a monthly basis to en</p><p>2 0.26188561 <a title="112-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-17-Somebody%E2%80%99s_looking_for_a_book_on_time_series_analysis_in_the_style_of_Angrist_and_Pischke%2C_or_Gelman_and_Hill.html">1986 andrew gelman stats-2013-08-17-Somebody’s looking for a book on time series analysis in the style of Angrist and Pischke, or Gelman and Hill</a></p>
<p>Introduction: Devrup Ghatak writes:
  
I am a student of economics and recently read  your review  of Mostly Harmless Econometrics. In the review you mention that the book contains no time series. Given that your book on data analysis (Data Analysis using Regression) does not contain any time series material either, I wonder if you happen to have any favourite time series reference similar in style/level to the data analysis book.
  
I don’t know.  The closest thing might be Hierarchical Modeling and Analysis for Spatial Data by Banerjee, Carlin, and Gelfand, but I don’t know of anything focused on time series that’s quite in the format that I’d prefer. This is not my area, though.  Maybe you, the readers, have some suggestions?</p><p>3 0.19732666 <a title="112-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<p>Introduction: From August 1990.  It was in the form of a note sent to all the people in the statistics group of Bell Labs, where I’d worked that summer.
  
To all:


Here’s the abstract of the work I’ve done this summer.  It’s stored in the file, 
/fs5/gelman/abstract.bell, and copies of the Figures 1-3 are on Trevor’s desk. 
Any comments are of course appreciated; I’m at gelman@stat.berkeley.edu.


On the Routine Use of Markov Chains for Simulation


Andrew Gelman and Donald Rubin, 6 August 1990


corrected version:  8 August 1990
  
  
  
1.  Simulation


In probability and statistics we can often specify multivariate distributions 
many of whose properties we do not fully understand–perhaps, as in the 
Ising model of statistical physics, we can write the joint density function, up 
to a multiplicative constant that cannot be expressed in closed form. 
For an example in statistics, consider the Normal random 
effects model in the analysis of variance, which can be 
easily placed in a Bayesian fram</p><p>4 0.19664221 <a title="112-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-20-Donald_E._Westlake_on_George_W._Bush.html">1464 andrew gelman stats-2012-08-20-Donald E. Westlake on George W. Bush</a></p>
<p>Introduction: A post-WTC  time capsule .</p><p>5 0.14274985 <a title="112-tfidf-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-23-Thinking_outside_the_%28graphical%29_box%3A__Instead_of_arguing_about_how_best_to_fix_a_bar_chart%2C_graph_it_as_a_time_series_lineplot_instead.html">294 andrew gelman stats-2010-09-23-Thinking outside the (graphical) box:  Instead of arguing about how best to fix a bar chart, graph it as a time series lineplot instead</a></p>
<p>Introduction: John Kastellec points me to  this blog  by Ezra Klein criticizing the following graph from a recent Republican Party report:
 
 
 
Klein (following  Alexander Hart ) slams the graph for not going all the way to zero on the y-axis, thus making the projected change seem bigger than it really is.
 
I agree with Klein and Hart that, if you’re gonna do a bar chart, you want the bars to go down to 0.  On the other hand, a projected change from 19% to 23% is actually pretty big, and I don’t see the point of using a graphical display that hides it.
 
 The solution:  Ditch the bar graph entirely and replace it by a lineplot , in particular, a time series with year-by-year data.  The time series would have several advantages:
 
1.  Data are placed in context.  You’d see every year, instead of discrete averages, and you’d get to see the changes in the context of year-to-year variation.
 
2.  With the time series, you can use whatever y-axis works with the data.  No need to go to zero.
 
P.S.  I l</p><p>6 0.13931809 <a title="112-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-05-Call_for_book_proposals.html">557 andrew gelman stats-2011-02-05-Call for book proposals</a></p>
<p>7 0.13765775 <a title="112-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-20-Amazing_retro_gnu_graphics%21.html">1907 andrew gelman stats-2013-06-20-Amazing retro gnu graphics!</a></p>
<p>8 0.13029307 <a title="112-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-07-Inference_%3D_data_%2B_model.html">1201 andrew gelman stats-2012-03-07-Inference = data + model</a></p>
<p>9 0.12865388 <a title="112-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-19-Data_exploration_and_multiple_comparisons.html">524 andrew gelman stats-2011-01-19-Data exploration and multiple comparisons</a></p>
<p>10 0.12730449 <a title="112-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-14-The_General_Social_Survey_is_a_great_resource.html">958 andrew gelman stats-2011-10-14-The General Social Survey is a great resource</a></p>
<p>11 0.12725069 <a title="112-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-14-Cool-ass_signal_processing_using_Gaussian_processes_%28birthdays_again%29.html">1379 andrew gelman stats-2012-06-14-Cool-ass signal processing using Gaussian processes (birthdays again)</a></p>
<p>12 0.11914553 <a title="112-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-15-The_UN_Plot_to_Force_Bayesianism_on_Unsuspecting_Americans_%28penalized_B-Spline_edition%29.html">2135 andrew gelman stats-2013-12-15-The UN Plot to Force Bayesianism on Unsuspecting Americans (penalized B-Spline edition)</a></p>
<p>13 0.11316612 <a title="112-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-14-Desecration_of_valuable_real_estate.html">572 andrew gelman stats-2011-02-14-Desecration of valuable real estate</a></p>
<p>14 0.11006933 <a title="112-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>15 0.11000331 <a title="112-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-17-Death%21.html">962 andrew gelman stats-2011-10-17-Death!</a></p>
<p>16 0.10708057 <a title="112-tfidf-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-04-The_Joy_of_Stats.html">450 andrew gelman stats-2010-12-04-The Joy of Stats</a></p>
<p>17 0.10040572 <a title="112-tfidf-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-02-Am_I_too_negative%3F.html">2279 andrew gelman stats-2014-04-02-Am I too negative?</a></p>
<p>18 0.099899285 <a title="112-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>19 0.097789802 <a title="112-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-29-We_go_to_war_with_the_data_we_have%2C_not_the_data_we_want.html">1289 andrew gelman stats-2012-04-29-We go to war with the data we have, not the data we want</a></p>
<p>20 0.095645763 <a title="112-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-05-Xiao-Li_Meng_and_Xianchao_Xie_rethink_asymptotics.html">1406 andrew gelman stats-2012-07-05-Xiao-Li Meng and Xianchao Xie rethink asymptotics</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.205), (1, 0.04), (2, 0.009), (3, 0.017), (4, 0.055), (5, 0.018), (6, -0.04), (7, -0.002), (8, 0.025), (9, 0.007), (10, 0.001), (11, -0.021), (12, -0.025), (13, -0.011), (14, -0.045), (15, -0.01), (16, 0.017), (17, -0.023), (18, 0.007), (19, -0.036), (20, 0.009), (21, -0.034), (22, -0.055), (23, 0.037), (24, 0.005), (25, -0.004), (26, -0.021), (27, -0.06), (28, 0.058), (29, 0.008), (30, -0.034), (31, -0.028), (32, -0.013), (33, -0.024), (34, 0.004), (35, 0.005), (36, -0.068), (37, 0.001), (38, 0.032), (39, 0.057), (40, 0.041), (41, 0.12), (42, -0.095), (43, -0.029), (44, -0.001), (45, -0.003), (46, 0.024), (47, -0.028), (48, 0.012), (49, -0.04)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97786903 <a title="112-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-27-Sampling_rate_of_human-scaled_time_series.html">112 andrew gelman stats-2010-06-27-Sampling rate of human-scaled time series</a></p>
<p>Introduction: Bill Harris writes with two interesting questions involving time series analysis:
  
 
I used to work in an organization that designed and made signal processing equipment.  Antialiasing and windowing of time series was a big deal in performing analysis accurately.  


Now I’m in a place where I have to make inferences about human-scaled time series.  It has dawned on me that the two are related.  I’m not sure we often have data sampled at a rate at least twice the highest frequency present (not just the highest frequency of interest).  The only articles I’ve seen about aliasing as applied to social science series are from  Hinich  or from  related works .  Box and Jenkins hint at it in section 13.3 of Time Series Analysis, but the analysis seems to be mostly heuristic.  Yet I can imagine all sorts of time series subject to similar problems, from analyses of stock prices based on closing prices (mentioned in the latter article) to other economic series measured on a monthly basis to en</p><p>2 0.78070295 <a title="112-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-06-Bayesian_model-building_by_pure_thought%3A__Some_principles_and_examples.html">1156 andrew gelman stats-2012-02-06-Bayesian model-building by pure thought:  Some principles and examples</a></p>
<p>Introduction: This  is one of my favorite papers:
  
In applications, statistical models are often restricted to what produces reasonable estimates based on the data at hand. In many cases, however, the principles that allow a model to be restricted can be derived theoretically, in the absence of any data and with minimal applied context. We illustrate this point with three well-known theoretical examples from spatial statistics and time series. First, we show that an autoregressive model for local averages violates a principle of invariance under scaling. Second, we show how the Bayesian estimate of a strictly-increasing time series, using a uniform prior distribution, depends on the scale of estimation. Third, we interpret local smoothing of spatial lattice data as Bayesian estimation and show why uniform local smoothing does not make sense. In various forms, the results presented here have been derived in previous work; our contribution is to draw out some principles that can be derived theoretic</p><p>3 0.76723629 <a title="112-lsi-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-27-Big_Data%E2%80%A6Big_Deal%3F_Maybe%2C_if_Used_with_Caution..html">2307 andrew gelman stats-2014-04-27-Big Data…Big Deal? Maybe, if Used with Caution.</a></p>
<p>Introduction: This post is by  David K. Park  
  
 As we have witnessed, the term “big data” has been thrusted onto the zeitgeist in the past several years, however, when one pushes beyond the hype, there seems to be little substance there. We’ve always had “data” so what so unique about it this time? Yes, we recognize it’s “big” but is there anything unique about data this time around? 
  

I’ve spend some time thinking about this and the answer seems to be yes, and it falls on three dimensions:

 
 
  Capturing Conversations & Relationships : Individuals have always communicated with one another, but now we can capture some of that conversation – email, blogs, social media (Facebook, Twitter, Pinterest) – and we can now do it with machines via sensors, ie “the internet of things” as we hear so much about; 
  Granularity : We can now understand individuals at a much finer level of analysis. No longer do we need to rely on a sample size of 500 people to “represent” the nation, but instead we can acc</p><p>4 0.7359274 <a title="112-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-15-The_UN_Plot_to_Force_Bayesianism_on_Unsuspecting_Americans_%28penalized_B-Spline_edition%29.html">2135 andrew gelman stats-2013-12-15-The UN Plot to Force Bayesianism on Unsuspecting Americans (penalized B-Spline edition)</a></p>
<p>Introduction: Mike Spagat sent me an email with the above heading, referring to  this paper  by Leontine Alkema and Jin Rou New, which begins:
  
National estimates of the under-5 mortality rate (U5MR) are used to track progress in reducing child mortality and to evaluate countries’ performance related to United Nations Millennium Development Goal 4, which calls for a reduction in the U5MR by two-thirds between 1990 and 2015. However, for the great majority of developing countries without well-functioning vital registration systems, estimating levels and trends in child mortality is challenging, not only because of limited data availability but also because of issues with data quality. Global U5MR estimates are often constructed without accounting for potential biases in data series, which may lead to inaccurate point estimates and/or credible intervals.


We describe a Bayesian penalized B-spline regression model for assessing levels and trends in the U5MR for all countries in the world, whereby bi</p><p>5 0.73448068 <a title="112-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-28-Understanding_simulations_in_terms_of_predictive_inference%3F.html">1287 andrew gelman stats-2012-04-28-Understanding simulations in terms of predictive inference?</a></p>
<p>Introduction: David Hogg writes:
  
My (now deceased) collaborator and guru in all things inference, Sam Roweis, used to emphasize to me that we should evaluate models in the data space — not the parameter space — because models are always effectively “effective” and not really, fundamentally true. Or, in other words, models should be compared in the space of their predictions, not in the space of their parameters (the  parameters didn’t really “exist” at all for Sam).  In that spirit, when we estimate the effectiveness of a MCMC method or tuning — by autocorrelation time or ESJD or anything else — shouldn’t we be looking at the changes in the model predictions over time, rather than the changes in the parameters over time?  That is, the autocorrelation time should be the autocorrelation time in what the model (at the walker position) predicts for the data, and the ESJD should be the expected squared jump distance in what the model predicts for the data?  This might resolve the concern I expressed a</p><p>6 0.72156501 <a title="112-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-01-Peter_Huber%E2%80%99s_reflections_on_data_analysis.html">690 andrew gelman stats-2011-05-01-Peter Huber’s reflections on data analysis</a></p>
<p>7 0.71929425 <a title="112-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-07-Inference_%3D_data_%2B_model.html">1201 andrew gelman stats-2012-03-07-Inference = data + model</a></p>
<p>8 0.71706474 <a title="112-lsi-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-20-When_Kerry_Met_Sally%3A_Politics_and_Perceptions_in_the_Demand_for_Movies.html">358 andrew gelman stats-2010-10-20-When Kerry Met Sally: Politics and Perceptions in the Demand for Movies</a></p>
<p>9 0.7157129 <a title="112-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-19-Data_exploration_and_multiple_comparisons.html">524 andrew gelman stats-2011-01-19-Data exploration and multiple comparisons</a></p>
<p>10 0.70544332 <a title="112-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-07-Descriptive_statistics%2C_causal_inference%2C_and_story_time.html">789 andrew gelman stats-2011-07-07-Descriptive statistics, causal inference, and story time</a></p>
<p>11 0.70015806 <a title="112-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-21-How_many_data_points_do_you_really_have%3F.html">1178 andrew gelman stats-2012-02-21-How many data points do you really have?</a></p>
<p>12 0.69956011 <a title="112-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-14-Everyone%E2%80%99s_trading_bias_for_variance_at_some_point%2C_it%E2%80%99s_just_done_at_different_places_in_the_analyses.html">1763 andrew gelman stats-2013-03-14-Everyone’s trading bias for variance at some point, it’s just done at different places in the analyses</a></p>
<p>13 0.69530535 <a title="112-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-29-We_go_to_war_with_the_data_we_have%2C_not_the_data_we_want.html">1289 andrew gelman stats-2012-04-29-We go to war with the data we have, not the data we want</a></p>
<p>14 0.69238973 <a title="112-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-14-Cool-ass_signal_processing_using_Gaussian_processes_%28birthdays_again%29.html">1379 andrew gelman stats-2012-06-14-Cool-ass signal processing using Gaussian processes (birthdays again)</a></p>
<p>15 0.69192535 <a title="112-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-06-Your_conclusion_is_only_as_good_as_your_data.html">1369 andrew gelman stats-2012-06-06-Your conclusion is only as good as your data</a></p>
<p>16 0.69174922 <a title="112-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-17-Somebody%E2%80%99s_looking_for_a_book_on_time_series_analysis_in_the_style_of_Angrist_and_Pischke%2C_or_Gelman_and_Hill.html">1986 andrew gelman stats-2013-08-17-Somebody’s looking for a book on time series analysis in the style of Angrist and Pischke, or Gelman and Hill</a></p>
<p>17 0.69134235 <a title="112-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-21-Model_complexity_as_a_function_of_sample_size.html">1543 andrew gelman stats-2012-10-21-Model complexity as a function of sample size</a></p>
<p>18 0.68472922 <a title="112-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-19-Slick_time_series_decomposition_of_the_birthdays_data.html">1384 andrew gelman stats-2012-06-19-Slick time series decomposition of the birthdays data</a></p>
<p>19 0.67822629 <a title="112-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-11-Toward_a_framework_for_automatic_model_building.html">1718 andrew gelman stats-2013-02-11-Toward a framework for automatic model building</a></p>
<p>20 0.67696184 <a title="112-lsi-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-13-Stopping_rules_and_Bayesian_analysis.html">2210 andrew gelman stats-2014-02-13-Stopping rules and Bayesian analysis</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(5, 0.027), (9, 0.027), (15, 0.012), (16, 0.083), (21, 0.031), (24, 0.103), (42, 0.023), (63, 0.017), (65, 0.019), (72, 0.023), (79, 0.012), (84, 0.016), (97, 0.167), (99, 0.324)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96390063 <a title="112-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-03-Faculty_Position_in_Visualization%2C_Visual_Analytics%2C_Imaging%2C_and_Human_Centered_Computing.html">1651 andrew gelman stats-2013-01-03-Faculty Position in Visualization, Visual Analytics, Imaging, and Human Centered Computing</a></p>
<p>Introduction: David Ebert sends this along:
  
Purdue University School of ECE Faculty Position in Human-Centered Computing


The School of Electrical and Computer Engineering at Purdue University invites applications for a faculty position at any level in human-centered computing, including but not limited to visualization, visual analytics, human-computer interaction (HCI), imaging, and graphics. . . .  Applications  should consist of a cover letter, a CV, research and teaching statements, names and contact information for at least three references, and URLs for three to five online papers. . . . We will consider applications through March 2013.
  
It’s great to see this sort of thing.
 
P.S.  Amusingly enough, the Purdue Visualization and Analytics Center has an ugly, bureaucratic, text-heavy  webpage .  Not that I’m one to talk, the Columbia stat dept has an ugly webpage too (although I think we’ll be switching soon to something better).</p><p>2 0.95569372 <a title="112-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-12-God%2C_Guns%2C_and_Gaydar%3A__The_Laws_of_Probability_Push_You_to_Overestimate_Small_Groups.html">142 andrew gelman stats-2010-07-12-God, Guns, and Gaydar:  The Laws of Probability Push You to Overestimate Small Groups</a></p>
<p>Introduction: Earlier today, Nate  criticized  a U.S. military survey that asks troops the question, “Do you currently serve with a male or female Service member you  believe  to be homosexual.” [emphasis added]  As Nate points out, by asking this question in such a speculative way, “it would seem that you’ll be picking up a tremendous number of false positives–soldiers who are believed to be gay, but aren’t–and that these false positives will swamp any instances in which soldiers (in spite of DADT) are actually somewhat open about their same-sex attractions.”
 
This is a general problem in survey research. In an  article  in Chance magazine in 1997, “The myth of millions of annual self-defense gun uses:  a case study of survey overestimates of rare events” [see  here  for related references], David Hemenway uses the false-positive, false-negative reasoning to explain this bias in terms of probability theory.  Misclassifications that induce seemingly minor biases in estimates of certain small probab</p><p>3 0.95542014 <a title="112-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-03-is_it_possible_to_%E2%80%9Coverstratify%E2%80%9D_when_assigning_a_treatment_in_a_randomized_control_trial%3F.html">553 andrew gelman stats-2011-02-03-is it possible to “overstratify” when assigning a treatment in a randomized control trial?</a></p>
<p>Introduction: Peter Bergman writes:
  
is it possible to “overstratify” when assigning a treatment in a randomized control trial?  I [Bergman] have a sample size of roughly 400 people, and several binary variables correlate strongly with the outcome of interest and would also define interesting subgroups for analysis.  The problem is, stratifying over all of these (five or six) variables leaves me with strata that have only 1 person in them.  I have done some background reading on whether there is a rule of thumb for the maximum number of variables to stratify.  There does not seem to be much agreement (some say there should be between N/50-N/100 strata, others say as few as possible).  In economics, the paper I looked to is here, which seems to summarize literature related to clinical trials.  In short, my question is: is it bad to have several strata with 1 person in them? Should I group these people in with another stratum?


P.S. In the paper I mention above, they also say it is important to inc</p><p>4 0.94850707 <a title="112-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-07-Chi-square_FAIL_when_many_cells_have_small_expected_values.html">996 andrew gelman stats-2011-11-07-Chi-square FAIL when many cells have small expected values</a></p>
<p>Introduction: William Perkins, Mark Tygert, and Rachel Ward  write :
  
If a discrete probability distribution in a model being tested for goodness-of-fit is not close to uniform, then forming the Pearson χ2 statistic can involve division by nearly zero. This often leads to serious trouble in practice — even in the absence of round-off errors . . .
  
The problem is not merely that the chi-squared  statistic  doesn’t have the advertised chi-squared  distribution —a reference distribution can always be computed via simulation, either using the posterior predictive distribution or by conditioning on a point estimate of the cell expectations and then making a degrees-of-freedom sort of adjustment.
 
Rather, the problem is that, when there are lots of cells with near-zero expectation, the chi-squared test is mostly noise.
 
And this is not merely a theoretical problem.  It comes up in real examples.
 
Here’s one, taken from the classic 1992 genetics paper of Guo and Thomspson:
 
   
 
And here are the e</p><p>5 0.94743532 <a title="112-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-26-Reflections_on_ethicsblogging.html">1694 andrew gelman stats-2013-01-26-Reflections on ethicsblogging</a></p>
<p>Introduction: I have to say, it distorts my internal incentives when I am happy to see really blatant examples of ethical lapses.  Sort of like when you’re cleaning the attic and searching for roaches:  on one hand, you’d be happy if there were none, but, still, there’s a thrill each time you find a roach and catch it—and, at that point, you want it to be a big ugly one!</p><p>6 0.94741583 <a title="112-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-10-Three_hours_in_the_life_of_a_statistician.html">1001 andrew gelman stats-2011-11-10-Three hours in the life of a statistician</a></p>
<p>7 0.94355488 <a title="112-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-25-Design_of_nonrandomized_cluster_sample_study.html">820 andrew gelman stats-2011-07-25-Design of nonrandomized cluster sample study</a></p>
<p>8 0.93924832 <a title="112-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-31-Meanwhile%2C_on_the_sister_blog_._._..html">882 andrew gelman stats-2011-08-31-Meanwhile, on the sister blog . . .</a></p>
<p>same-blog 9 0.9374705 <a title="112-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-27-Sampling_rate_of_human-scaled_time_series.html">112 andrew gelman stats-2010-06-27-Sampling rate of human-scaled time series</a></p>
<p>10 0.93591827 <a title="112-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-19-%E2%80%9CIf_it_saves_the_life_of_a_single_child%E2%80%A6%E2%80%9D_and_other_nonsense.html">526 andrew gelman stats-2011-01-19-“If it saves the life of a single child…” and other nonsense</a></p>
<p>11 0.9328593 <a title="112-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-21-Responding_to_a_bizarre_anti-social-science_screed.html">1335 andrew gelman stats-2012-05-21-Responding to a bizarre anti-social-science screed</a></p>
<p>12 0.93084645 <a title="112-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-23-Unhappy_with_improvement_by_a_factor_of_10%5E29.html">160 andrew gelman stats-2010-07-23-Unhappy with improvement by a factor of 10^29</a></p>
<p>13 0.92612517 <a title="112-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-04-30-Things_I_learned_from_the_Mickey_Kaus_for_Senate_campaign.html">13 andrew gelman stats-2010-04-30-Things I learned from the Mickey Kaus for Senate campaign</a></p>
<p>14 0.92531776 <a title="112-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-19-Chomsky_chomsky_chomsky_chomsky_furiously.html">1812 andrew gelman stats-2013-04-19-Chomsky chomsky chomsky chomsky furiously</a></p>
<p>15 0.91808069 <a title="112-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-28-Whassup_with_those_crappy_thrillers%3F.html">115 andrew gelman stats-2010-06-28-Whassup with those crappy thrillers?</a></p>
<p>16 0.91718817 <a title="112-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-02-Bayes_alert%21__Cool_postdoc_position_here_on_missing_data_imputation_and_applications_in_health_disparities_research%21.html">2047 andrew gelman stats-2013-10-02-Bayes alert!  Cool postdoc position here on missing data imputation and applications in health disparities research!</a></p>
<p>17 0.91270638 <a title="112-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-06-%2463%2C000_worth_of_abusive_research_._._._or_just_a_really_stupid_waste_of_time%3F.html">18 andrew gelman stats-2010-05-06-$63,000 worth of abusive research . . . or just a really stupid waste of time?</a></p>
<p>18 0.90992606 <a title="112-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-11-Incredibly_strange_spam.html">1573 andrew gelman stats-2012-11-11-Incredibly strange spam</a></p>
<p>19 0.9077456 <a title="112-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-26-Politics_as_an_escape_hatch.html">1591 andrew gelman stats-2012-11-26-Politics as an escape hatch</a></p>
<p>20 0.90266788 <a title="112-lda-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-13-Postdoc_with_Liz_Stuart_on_propensity_score_methods_when_the_covariates_are_measured_with_error.html">2171 andrew gelman stats-2014-01-13-Postdoc with Liz Stuart on propensity score methods when the covariates are measured with error</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
