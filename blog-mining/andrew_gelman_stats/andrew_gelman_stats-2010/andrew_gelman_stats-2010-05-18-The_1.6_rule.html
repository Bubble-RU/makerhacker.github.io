<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>39 andrew gelman stats-2010-05-18-The 1.6 rule</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2010" href="../home/andrew_gelman_stats-2010_home.html">andrew_gelman_stats-2010</a> <a title="andrew_gelman_stats-2010-39" href="#">andrew_gelman_stats-2010-39</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>39 andrew gelman stats-2010-05-18-The 1.6 rule</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2010-39-html" href="http://andrewgelman.com/2010/05/18/the_16_rule/">html</a></p><p>Introduction: In ARM we discuss how you can go back and forth between logit and probit models by dividing by 1.6.  Or, to put it another way, logistic regression corresponds to a latent-variable model with errors that are approximately normally distributed with mean 0 and standard deviation 1.6.  (This is well known, itâ&euro;&trade;s nothing original with our book.)  Anyway, John Cook discusses the approximation  here .</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In ARM we discuss how you can go back and forth between logit and probit models by dividing by 1. [sent-1, score-1.459]
</p><p>2 Or, to put it another way, logistic regression corresponds to a latent-variable model with errors that are approximately normally distributed with mean 0 and standard deviation 1. [sent-3, score-2.064]
</p><p>3 (This is well known, itâ&euro;&trade;s nothing original with our book. [sent-5, score-0.329]
</p><p>4 )  Anyway, John Cook discusses the approximation  here . [sent-6, score-0.436]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dividing', 0.298), ('probit', 0.292), ('normally', 0.245), ('approximation', 0.236), ('cook', 0.236), ('distributed', 0.234), ('logit', 0.234), ('corresponds', 0.229), ('deviation', 0.221), ('forth', 0.219), ('arm', 0.216), ('approximately', 0.203), ('discusses', 0.2), ('logistic', 0.19), ('anyway', 0.151), ('known', 0.15), ('errors', 0.148), ('discuss', 0.141), ('original', 0.134), ('john', 0.131), ('regression', 0.119), ('nothing', 0.118), ('standard', 0.116), ('mean', 0.105), ('back', 0.098), ('put', 0.093), ('models', 0.093), ('another', 0.088), ('go', 0.084), ('well', 0.077), ('model', 0.073), ('way', 0.058)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="39-tfidf-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-18-The_1.6_rule.html">39 andrew gelman stats-2010-05-18-The 1.6 rule</a></p>
<p>Introduction: In ARM we discuss how you can go back and forth between logit and probit models by dividing by 1.6.  Or, to put it another way, logistic regression corresponds to a latent-variable model with errors that are approximately normally distributed with mean 0 and standard deviation 1.6.  (This is well known, itâ&euro;&trade;s nothing original with our book.)  Anyway, John Cook discusses the approximation  here .</p><p>2 0.14858583 <a title="39-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-28-Hierarchical_ordered_logit_or_probit.html">684 andrew gelman stats-2011-04-28-Hierarchical ordered logit or probit</a></p>
<p>Introduction: Jeff writes:
  
How far off is bglmer and can it handle ordered logit or multinom logit?
  
My reply:
 
bglmer is very close.  No ordered logit but I was just talking about it with Sophia today.  My guess is that the easiest way to fit a hierarchical ordered logit or multinom logit will be to use stan.  For right now I’d recommend using glmer/bglmer to fit the ordered logits in order (e.g., 1 vs. 2,3,4, then 2 vs. 3,4, then 3 vs. 4).  Or maybe there’s already a hierarchical multinomial logit in mcmcpack or somewhere?</p><p>3 0.14827694 <a title="39-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-29-Putting_together_multinomial_discrete_regressions_by_combining_simple_logits.html">782 andrew gelman stats-2011-06-29-Putting together multinomial discrete regressions by combining simple logits</a></p>
<p>Introduction: When predicting 0/1 data we can use logit (or probit or robit or some other robust model such as invlogit (0.01 + 0.98*X*beta)).  Logit is simple enough and we can use  bayesglm  to regularize and avoid the problem of separation.
 
What if there are more than 2 categories?  If they’re ordered (1, 2, 3, etc), we can do ordered logit (and use bayespolr() to avoid separation).  If the categories are unordered (vanilla, chocolate, strawberry), there are unordered multinomial logit and probit models out there.
 
But it’s not so easy to fit these multinomial model in a multilevel setting (with coefficients that vary by group), especially if the computation is embedded in an iterative routine such as mi where you have real time constraints at each step.
 
So this got me wondering whether we could kluge it with logits.  Here’s the basic idea (in the ordered and unordered forms):
 
- If you have a variable that goes 1, 2, 3, etc., set up a series of logits:  1 vs. 2,3,…; 2 vs. 3,…; and so forth</p><p>4 0.14232753 <a title="39-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-30-Works_well_versus_well_understood.html">738 andrew gelman stats-2011-05-30-Works well versus well understood</a></p>
<p>Introduction: John Cook  discusses  the John Tukey quote, “The test of a good procedure is how well it works, not how well it is understood.”  Cook writes:
  
At some level, it’s hard to argue against this. Statistical procedures operate on empirical data, so it makes sense that the procedures themselves be evaluated empirically.


But I [Cook] question whether we really know that a statistical procedure works well if it isn’t well understood. Specifically, I’m skeptical of complex statistical methods whose only credentials are a handful of simulations. “We don’t have any theoretical results, buy hey, it works well in practice. Just look at the simulations.”


Every method works well on the scenarios its author publishes, almost by definition. If the method didn’t handle a scenario well, the author would publish a different scenario.
  
I agree with Cook but would give a slightly different emphasis.  I’d say that a lot of methods can work when they are done well.  See the second meta-principle liste</p><p>5 0.13946322 <a title="39-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-30-Computational_problems_with_glm_etc..html">1516 andrew gelman stats-2012-09-30-Computational problems with glm etc.</a></p>
<p>Introduction: John Mount  provides some useful background and follow-up  on our discussion  from last year  on computational instability of the usual logistic regression solver.
 
Just to refresh your memory, here’s a simple logistic regression with only a constant term and no separation, nothing pathological at all:
 
 > y <- rep (c(1,0),c(10,5)) 
> display (glm (y ~ 1, family=binomial(link="logit"))) 
glm(formula = y ~ 1, family = binomial(link = "logit")) 
            coef.est coef.se 
(Intercept) 0.69     0.55 
--- 
  n = 15, k = 1 
  residual deviance = 19.1, null deviance = 19.1 (difference = 0.0) 
 
And here’s what happens when we give it the not-outrageous starting value of -2:
 
 > display (glm (y ~ 1, family=binomial(link="logit"), start=-2)) 
glm(formula = y ~ 1, family = binomial(link = "logit"), start = -2) 
            coef.est    coef.se 
(Intercept)       71.97 17327434.18 
--- 
  n = 15, k = 1 
  residual deviance = 360.4, null deviance = 19.1 (difference = -341.3) 
Warning message:</p><p>6 0.13764398 <a title="39-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>7 0.12827636 <a title="39-tfidf-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-08-How_to_display_multinominal_logit_results_graphically%3F.html">2163 andrew gelman stats-2014-01-08-How to display multinominal logit results graphically?</a></p>
<p>8 0.12104036 <a title="39-tfidf-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-14-%E2%80%9CLike_a_group_of_teenagers_on_a_bus%2C_they_behave_in_public_as_if_they_were_in_private%E2%80%9D.html">414 andrew gelman stats-2010-11-14-“Like a group of teenagers on a bus, they behave in public as if they were in private”</a></p>
<p>9 0.11631264 <a title="39-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-01-I%E2%80%99ll_say_it_again.html">2046 andrew gelman stats-2013-10-01-I’ll say it again</a></p>
<p>10 0.11267287 <a title="39-tfidf-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-17-Clutering_and_variance_components.html">417 andrew gelman stats-2010-11-17-Clutering and variance components</a></p>
<p>11 0.10707954 <a title="39-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-Another_reason_why_you_can_get_good_inferences_from_a_bad_model.html">1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</a></p>
<p>12 0.10197946 <a title="39-tfidf-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-08-Regression_and_causality_and_variable_ordering.html">2364 andrew gelman stats-2014-06-08-Regression and causality and variable ordering</a></p>
<p>13 0.09722849 <a title="39-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-24-Bayes_at_the_end.html">534 andrew gelman stats-2011-01-24-Bayes at the end</a></p>
<p>14 0.097126588 <a title="39-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Interpreting_interactions_in_discrete-data_regression.html">1908 andrew gelman stats-2013-06-21-Interpreting interactions in discrete-data regression</a></p>
<p>15 0.094343826 <a title="39-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-04-What_are_the_key_assumptions_of_linear_regression%3F.html">1967 andrew gelman stats-2013-08-04-What are the key assumptions of linear regression?</a></p>
<p>16 0.093124062 <a title="39-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-22-Question_12_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1337 andrew gelman stats-2012-05-22-Question 12 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>17 0.092120841 <a title="39-tfidf-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>18 0.090086065 <a title="39-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-07-Robust_logistic_regression.html">1886 andrew gelman stats-2013-06-07-Robust logistic regression</a></p>
<p>19 0.089728057 <a title="39-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-%E2%80%9CAll_Models_are_Right%2C_Most_are_Useless%E2%80%9D.html">1197 andrew gelman stats-2012-03-04-“All Models are Right, Most are Useless”</a></p>
<p>20 0.089103222 <a title="39-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-07-Mathematical_model_of_vote_operations.html">1251 andrew gelman stats-2012-04-07-Mathematical model of vote operations</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.104), (1, 0.085), (2, 0.034), (3, 0.017), (4, 0.047), (5, -0.003), (6, 0.034), (7, -0.033), (8, 0.053), (9, 0.031), (10, 0.04), (11, 0.017), (12, -0.005), (13, -0.003), (14, -0.026), (15, -0.007), (16, -0.039), (17, -0.01), (18, 0.009), (19, -0.024), (20, 0.036), (21, -0.001), (22, 0.021), (23, -0.041), (24, 0.005), (25, -0.022), (26, 0.015), (27, -0.069), (28, -0.045), (29, -0.02), (30, -0.018), (31, 0.065), (32, 0.015), (33, -0.008), (34, 0.017), (35, -0.038), (36, -0.024), (37, -0.002), (38, -0.02), (39, -0.004), (40, -0.028), (41, -0.019), (42, -0.002), (43, -0.006), (44, 0.057), (45, 0.064), (46, -0.039), (47, 0.019), (48, 0.044), (49, 0.029)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96740866 <a title="39-lsi-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-18-The_1.6_rule.html">39 andrew gelman stats-2010-05-18-The 1.6 rule</a></p>
<p>Introduction: In ARM we discuss how you can go back and forth between logit and probit models by dividing by 1.6.  Or, to put it another way, logistic regression corresponds to a latent-variable model with errors that are approximately normally distributed with mean 0 and standard deviation 1.6.  (This is well known, itâ&euro;&trade;s nothing original with our book.)  Anyway, John Cook discusses the approximation  here .</p><p>2 0.75940979 <a title="39-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-30-Computational_problems_with_glm_etc..html">1516 andrew gelman stats-2012-09-30-Computational problems with glm etc.</a></p>
<p>Introduction: John Mount  provides some useful background and follow-up  on our discussion  from last year  on computational instability of the usual logistic regression solver.
 
Just to refresh your memory, here’s a simple logistic regression with only a constant term and no separation, nothing pathological at all:
 
 > y <- rep (c(1,0),c(10,5)) 
> display (glm (y ~ 1, family=binomial(link="logit"))) 
glm(formula = y ~ 1, family = binomial(link = "logit")) 
            coef.est coef.se 
(Intercept) 0.69     0.55 
--- 
  n = 15, k = 1 
  residual deviance = 19.1, null deviance = 19.1 (difference = 0.0) 
 
And here’s what happens when we give it the not-outrageous starting value of -2:
 
 > display (glm (y ~ 1, family=binomial(link="logit"), start=-2)) 
glm(formula = y ~ 1, family = binomial(link = "logit"), start = -2) 
            coef.est    coef.se 
(Intercept)       71.97 17327434.18 
--- 
  n = 15, k = 1 
  residual deviance = 360.4, null deviance = 19.1 (difference = -341.3) 
Warning message:</p><p>3 0.7123521 <a title="39-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-28-Hierarchical_ordered_logit_or_probit.html">684 andrew gelman stats-2011-04-28-Hierarchical ordered logit or probit</a></p>
<p>Introduction: Jeff writes:
  
How far off is bglmer and can it handle ordered logit or multinom logit?
  
My reply:
 
bglmer is very close.  No ordered logit but I was just talking about it with Sophia today.  My guess is that the easiest way to fit a hierarchical ordered logit or multinom logit will be to use stan.  For right now I’d recommend using glmer/bglmer to fit the ordered logits in order (e.g., 1 vs. 2,3,4, then 2 vs. 3,4, then 3 vs. 4).  Or maybe there’s already a hierarchical multinomial logit in mcmcpack or somewhere?</p><p>4 0.71058434 <a title="39-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-29-Putting_together_multinomial_discrete_regressions_by_combining_simple_logits.html">782 andrew gelman stats-2011-06-29-Putting together multinomial discrete regressions by combining simple logits</a></p>
<p>Introduction: When predicting 0/1 data we can use logit (or probit or robit or some other robust model such as invlogit (0.01 + 0.98*X*beta)).  Logit is simple enough and we can use  bayesglm  to regularize and avoid the problem of separation.
 
What if there are more than 2 categories?  If they’re ordered (1, 2, 3, etc), we can do ordered logit (and use bayespolr() to avoid separation).  If the categories are unordered (vanilla, chocolate, strawberry), there are unordered multinomial logit and probit models out there.
 
But it’s not so easy to fit these multinomial model in a multilevel setting (with coefficients that vary by group), especially if the computation is embedded in an iterative routine such as mi where you have real time constraints at each step.
 
So this got me wondering whether we could kluge it with logits.  Here’s the basic idea (in the ordered and unordered forms):
 
- If you have a variable that goes 1, 2, 3, etc., set up a series of logits:  1 vs. 2,3,…; 2 vs. 3,…; and so forth</p><p>5 0.71040934 <a title="39-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-10-Matching_and_regression%3A__two_great_tastes_etc_etc.html">796 andrew gelman stats-2011-07-10-Matching and regression:  two great tastes etc etc</a></p>
<p>Introduction: Matthew Bogard writes:
  
Regarding the book Mostly Harmless Econometrics, you  state :

 
A casual reader of the book might be left with the unfortunate impression that matching is a competitor to regression rather than a tool for making regression more effective.
 

But in fact isn’t that what they are arguing, that, in a  ‘mostly harmless way’ regression is in fact a matching estimator itself?


“Our view is that regression can be motivated as a particular sort of weighted matching estimator, and therefore the differences between regression and matching estimates are unlikely to be of major empirical importance” (Chapter 3 p. 70)


They seem to be distinguishing regression (without prior matching) from all other types of matching techniques, and therefore implying that regression can be a ‘mostly harmless’ substitute or competitor to matching. My previous understanding, before starting this book was as you say, that matching is a tool that makes regression more effective.


I have n</p><p>6 0.70402294 <a title="39-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-04-What_are_the_key_assumptions_of_linear_regression%3F.html">1967 andrew gelman stats-2013-08-04-What are the key assumptions of linear regression?</a></p>
<p>7 0.7015022 <a title="39-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-31-Using_factor_analysis_or_principal_components_analysis_or_measurement-error_models_for_biological_measurements_in_archaeology%3F.html">1094 andrew gelman stats-2011-12-31-Using factor analysis or principal components analysis or measurement-error models for biological measurements in archaeology?</a></p>
<p>8 0.69962704 <a title="39-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>9 0.69539601 <a title="39-lsi-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-02-Why_we_hate_stepwise_regression.html">2357 andrew gelman stats-2014-06-02-Why we hate stepwise regression</a></p>
<p>10 0.69173688 <a title="39-lsi-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-28-Matching_for_preprocessing_data_for_causal_inference.html">375 andrew gelman stats-2010-10-28-Matching for preprocessing data for causal inference</a></p>
<p>11 0.68440688 <a title="39-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-22-A_Bayesian_model_for_an_increasing_function%2C_in_Stan%21.html">2110 andrew gelman stats-2013-11-22-A Bayesian model for an increasing function, in Stan!</a></p>
<p>12 0.68400961 <a title="39-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-19-Will_Stan_work_well_with_40%C3%9740_matrices%3F.html">861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</a></p>
<p>13 0.68253666 <a title="39-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>14 0.67122799 <a title="39-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Interpreting_interactions_in_discrete-data_regression.html">1908 andrew gelman stats-2013-06-21-Interpreting interactions in discrete-data regression</a></p>
<p>15 0.66512811 <a title="39-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-09-Same_old_same_old.html">1849 andrew gelman stats-2013-05-09-Same old same old</a></p>
<p>16 0.66421545 <a title="39-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-13-Lame_Statistics_Patents.html">1761 andrew gelman stats-2013-03-13-Lame Statistics Patents</a></p>
<p>17 0.66175139 <a title="39-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-18-Standardizing_regression_inputs.html">1462 andrew gelman stats-2012-08-18-Standardizing regression inputs</a></p>
<p>18 0.65656435 <a title="39-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-24-F-f-f-fake_data.html">1735 andrew gelman stats-2013-02-24-F-f-f-fake data</a></p>
<p>19 0.6485579 <a title="39-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-07-Robust_logistic_regression.html">1886 andrew gelman stats-2013-06-07-Robust logistic regression</a></p>
<p>20 0.64730215 <a title="39-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-15-Still_more_Mr._P_in_public_health.html">770 andrew gelman stats-2011-06-15-Still more Mr. P in public health</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(16, 0.023), (24, 0.104), (29, 0.04), (31, 0.084), (63, 0.086), (84, 0.04), (95, 0.078), (99, 0.399)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.986476 <a title="39-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-18-The_1.6_rule.html">39 andrew gelman stats-2010-05-18-The 1.6 rule</a></p>
<p>Introduction: In ARM we discuss how you can go back and forth between logit and probit models by dividing by 1.6.  Or, to put it another way, logistic regression corresponds to a latent-variable model with errors that are approximately normally distributed with mean 0 and standard deviation 1.6.  (This is well known, itâ&euro;&trade;s nothing original with our book.)  Anyway, John Cook discusses the approximation  here .</p><p>2 0.95592564 <a title="39-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-26-Ethnicity_and_Population_Structure_in_Personal_Naming_Networks.html">925 andrew gelman stats-2011-09-26-Ethnicity and Population Structure in Personal Naming Networks</a></p>
<p>Introduction: Aleks pointed me to  this recent article  by Pablo Mateos, Paul Longley, and David O’Sullivan on one of my favorite topics.
 
The authors produced a potentially cool  naming network of the city of Auckland New Zealand .  I say “potentially cool” because I have such difficulty reading the article–I speak English, statistics, and a bit of political science and economics, but this one is written in heavy sociologese–that I can’t quite be sure what they’re doing.  However, despite my (perhaps unfair) disdain for the particulars of their method, it’s probably good that they’re jumping in with this analysis.  Others can take their data (and similar datasets from elsewhere) and do better.  Ya gotta start somewhere, and the basic idea (to cluster first names that are associated with the same last names, and to cluster last names that are associated with the same first names) seems good.
 
I have to admit, though, that I was amused by the following line, which, amazingly, led off the paper:</p><p>3 0.95404834 <a title="39-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-01-Classic_probability_mistake%2C_this_time_in_the_%28virtual%29_pages_of_the_New_York_Times.html">386 andrew gelman stats-2010-11-01-Classic probability mistake, this time in the (virtual) pages of the New York Times</a></p>
<p>Introduction: Xian pointed me to  this recycling  of a classic probability error.  It’s too bad it was in the New York Times, but at least it was in the Opinion Pages, so I guess that’s not so bad.  And, on the plus side, several of the blog commenters got the point.
 
What I was wondering, though, was who was this “Yitzhak Melechson, a statistics professor at the University of Tel Aviv”?  This is such a standard problem, I’m surprised to find a statistics professor making this mistake.  I was curious what his area of research is and where he was trained.
 
I started by googling Yitzhak Melechson but all I could find was this news story, over and over and over and over again.  Then I found Tel Aviv University and navigated to its statistics department but couldn’t find any Melechson in the faculty list.  Next stop:  entering Melechson in the search engine at the Tel Aviv University website.  It came up blank.
 
One last try:  I entered the Yitzhak Melechson into Google Scholar.  Here’s what came up:</p><p>4 0.9535808 <a title="39-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-15-Economics_now_%3D_Freudian_psychology_in_the_1950s%3A__More_on_the_incoherence_of_%E2%80%9Ceconomics_exceptionalism%E2%80%9D.html">1213 andrew gelman stats-2012-03-15-Economics now = Freudian psychology in the 1950s:  More on the incoherence of “economics exceptionalism”</a></p>
<p>Introduction: What follows is a long response to a comment on  someone else’s blog .  The quote is, “Thinking like an economist simply means that you scientifically approach human social behavior. . . .”
 
I’ll give the context in a bit, but first let me say that I thought this topic might be worth one more discussion because I suspect that the sort of economics exceptionalism that I will discuss is widely disseminated in college econ courses as well as in books such as the Freakonomics series.
 
It’s great to have pride in human achievements but at some point too much group self-regard can be distorting.
 
My best analogy to economics exceptionalism is Freudianism in the 1950s:  Back then, Freudian psychiatrists were on the top of the world.  Not only were they well paid, well respected, and secure in their theoretical foundations, they were also at the center of many important conversations.  Even those people who disagreed with them felt the need to explain why the Freudians were wrong.  Freudian</p><p>5 0.95302004 <a title="39-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-01-Back_when_fifty_years_was_a_long_time_ago.html">1646 andrew gelman stats-2013-01-01-Back when fifty years was a long time ago</a></p>
<p>Introduction: New Year’s Day is an excellent time to look back at changes, not just in the past year, but in the past half-century.
 
Mark Palko has an interesting  post  on the pace of changes in everyday life.  We’ve been hearing a lot in the past few decades about how things are changing faster and faster.  But, as Palko points out, the difference between life in 1962 and life today does not seem so different, at least for many people in the United States.  Sure, there are some big changes:  nonwhites get more respect, people mostly live longer, many cancers can be cured, fewer people are really really poor but it’s harder to hold down a job, cars are more reliable, you can get fresh fish in the suburbs, containers are lighter and stronger, vacations in the Caribbean instead of the Catskills, people have a lot more stuff and a lot more place to put it, etc etc etc.  But life in the 1950s or 1960s just doesn’t seem so different from how we live today.
 
In contrast, Palko writes, “You can also get</p><p>6 0.95072949 <a title="39-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-29-Splitting_the_data.html">544 andrew gelman stats-2011-01-29-Splitting the data</a></p>
<p>7 0.95040816 <a title="39-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-08-More_evidence_of_growing_nationalization_of_congressional_elections.html">508 andrew gelman stats-2011-01-08-More evidence of growing nationalization of congressional elections</a></p>
<p>8 0.9488495 <a title="39-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-27-%E2%80%9CThe_ultimate_left-wing_novel%E2%80%9D.html">682 andrew gelman stats-2011-04-27-“The ultimate left-wing novel”</a></p>
<p>9 0.94833559 <a title="39-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-23-%E2%80%9CI_mean%2C_what_exact_buttons_do_I_have_to_hit%3F%E2%80%9D.html">1995 andrew gelman stats-2013-08-23-“I mean, what exact buttons do I have to hit?”</a></p>
<p>10 0.94752204 <a title="39-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-23-When_are_complicated_models_helpful_in_psychology_research_and_when_are_they_overkill%3F.html">1690 andrew gelman stats-2013-01-23-When are complicated models helpful in psychology research and when are they overkill?</a></p>
<p>11 0.94740283 <a title="39-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-16-Using_economics_to_reduce_bike_theft.html">1536 andrew gelman stats-2012-10-16-Using economics to reduce bike theft</a></p>
<p>12 0.94683945 <a title="39-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-07-Inference_%3D_data_%2B_model.html">1201 andrew gelman stats-2012-03-07-Inference = data + model</a></p>
<p>13 0.94643617 <a title="39-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-05-Deadwood_in_the_math_curriculum.html">992 andrew gelman stats-2011-11-05-Deadwood in the math curriculum</a></p>
<p>14 0.94545567 <a title="39-lda-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-02-Am_I_too_negative%3F.html">2279 andrew gelman stats-2014-04-02-Am I too negative?</a></p>
<p>15 0.9450385 <a title="39-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-09-Statistics_gifts%3F.html">460 andrew gelman stats-2010-12-09-Statistics gifts?</a></p>
<p>16 0.9450295 <a title="39-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-21-Building_a_regression_model_._._._with_only_27_data_points.html">1506 andrew gelman stats-2012-09-21-Building a regression model . . . with only 27 data points</a></p>
<p>17 0.94441712 <a title="39-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-19-Just_chaid.html">421 andrew gelman stats-2010-11-19-Just chaid</a></p>
<p>18 0.94423944 <a title="39-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-20-Don%E2%80%99t_douthat%2C_man%21__Please_give_this_fallacy_a_name..html">2141 andrew gelman stats-2013-12-20-Don’t douthat, man!  Please give this fallacy a name.</a></p>
<p>19 0.94399893 <a title="39-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-04-One_more_thought_on_Hoover_historian_Niall_Ferguson%E2%80%99s_thing_about_Keynes_being_gay_and_marrying_a_ballerina_and_talking_about_poetry.html">1840 andrew gelman stats-2013-05-04-One more thought on Hoover historian Niall Ferguson’s thing about Keynes being gay and marrying a ballerina and talking about poetry</a></p>
<p>20 0.94361424 <a title="39-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-02-Interaction-based_feature_selection_and_classification_for_high-dimensional_biological_data.html">1703 andrew gelman stats-2013-02-02-Interaction-based feature selection and classification for high-dimensional biological data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
