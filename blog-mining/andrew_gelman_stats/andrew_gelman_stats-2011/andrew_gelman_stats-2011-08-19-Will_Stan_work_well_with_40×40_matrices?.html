<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-861" href="#">andrew_gelman_stats-2011-861</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-861-html" href="http://andrewgelman.com/2011/08/19/will-stan-work-well-with-40x40-matrices/">html</a></p><p>Introduction: Tomas Iesmantas writes:
  
I’m dealing with high dimensional (40-50 parameters) hierarchical bayesian model applied to nonlinear Poisson regression problem.


Now I’m using an adaptive version for the Metropolis adjusted Langevin algorithm with a truncated drift (Yves F. Atchade, 2003) to obtain samples from posterior.


But this algorithm is not very efficient in my case, it needs several millions iterations as burn-in period. And simulation takes quite a long time, since algorithm has to work with 40×40 matrices.


Maybe you know another MCMC algorithm which could take not so many burn-in samples and would be able to deal with nonlinear regression? In non-hierarchical nonlinear regression model adaptive metropolis algorithm is enough, but in hierarchical case I could use something more effective.
  
My reply:
 
Try fitting the model in Stan.  If that doesn’t work, let me know.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Tomas Iesmantas writes:    I’m dealing with high dimensional (40-50 parameters) hierarchical bayesian model applied to nonlinear Poisson regression problem. [sent-1, score-1.231]
</p><p>2 Now I’m using an adaptive version for the Metropolis adjusted Langevin algorithm with a truncated drift (Yves F. [sent-2, score-1.313]
</p><p>3 But this algorithm is not very efficient in my case, it needs several millions iterations as burn-in period. [sent-4, score-0.977]
</p><p>4 And simulation takes quite a long time, since algorithm has to work with 40×40 matrices. [sent-5, score-0.903]
</p><p>5 Maybe you know another MCMC algorithm which could take not so many burn-in samples and would be able to deal with nonlinear regression? [sent-6, score-1.386]
</p><p>6 In non-hierarchical nonlinear regression model adaptive metropolis algorithm is enough, but in hierarchical case I could use something more effective. [sent-7, score-1.942]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('algorithm', 0.487), ('nonlinear', 0.36), ('metropolis', 0.258), ('adaptive', 0.258), ('samples', 0.191), ('regression', 0.178), ('iesmantas', 0.174), ('tomas', 0.174), ('langevin', 0.174), ('drift', 0.161), ('truncated', 0.161), ('hierarchical', 0.159), ('dimensional', 0.146), ('iterations', 0.134), ('poisson', 0.126), ('adjusted', 0.126), ('dealing', 0.114), ('obtain', 0.113), ('mcmc', 0.111), ('model', 0.109), ('simulation', 0.105), ('millions', 0.105), ('efficient', 0.104), ('needs', 0.089), ('fitting', 0.086), ('version', 0.081), ('case', 0.079), ('takes', 0.079), ('deal', 0.079), ('parameters', 0.074), ('able', 0.065), ('work', 0.063), ('applied', 0.062), ('quite', 0.06), ('know', 0.059), ('several', 0.058), ('long', 0.056), ('high', 0.055), ('try', 0.054), ('could', 0.054), ('reply', 0.054), ('since', 0.053), ('let', 0.052), ('bayesian', 0.048), ('take', 0.047), ('doesn', 0.047), ('enough', 0.046), ('another', 0.044), ('maybe', 0.041), ('using', 0.039)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="861-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-19-Will_Stan_work_well_with_40%C3%9740_matrices%3F.html">861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</a></p>
<p>Introduction: Tomas Iesmantas writes:
  
I’m dealing with high dimensional (40-50 parameters) hierarchical bayesian model applied to nonlinear Poisson regression problem.


Now I’m using an adaptive version for the Metropolis adjusted Langevin algorithm with a truncated drift (Yves F. Atchade, 2003) to obtain samples from posterior.


But this algorithm is not very efficient in my case, it needs several millions iterations as burn-in period. And simulation takes quite a long time, since algorithm has to work with 40×40 matrices.


Maybe you know another MCMC algorithm which could take not so many burn-in samples and would be able to deal with nonlinear regression? In non-hierarchical nonlinear regression model adaptive metropolis algorithm is enough, but in hierarchical case I could use something more effective.
  
My reply:
 
Try fitting the model in Stan.  If that doesn’t work, let me know.</p><p>2 0.15762764 <a title="861-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-24-F-f-f-fake_data.html">1735 andrew gelman stats-2013-02-24-F-f-f-fake data</a></p>
<p>Introduction: Tiago Fragoso writes:
  
Suppose I fit a two stage regression model


Y = a + bx + e 
a = cw + d + e1


I could fit it all in one step by using MCMC for example (my model is more complicated than that, so I’ll have to do it by MCMC). However, I could fit the first regression only using MCMC because those estimates are hard to obtain and perform the second regression using least squares or a separate MCMC. 


So there’s an ‘one step’ inference based on doing it all at the same time and a ‘two step’ inference by fitting one and using the estimates on the further steps. What is gained or lost between both? Is anything done in this question?
  
My response:
 
Rather than answering your particular question, I’ll give you my generic answer, which is to simulate fake data from your model, then fit your model both ways and see how the results differ.  Repeat the simulation a few thousand times and you can make all the statistical comparisons you like.</p><p>3 0.14633481 <a title="861-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-17-Jumping_off_the_edge_of_the_world.html">858 andrew gelman stats-2011-08-17-Jumping off the edge of the world</a></p>
<p>Introduction: Tomas Iesmantas writes:
  
I’m facing a problem where parameter space is bounded, e.g. all parameters have to be positive. If in MCMC as proposal distribution I use normal distribution, then at some iterations I get negative proposals. So my question is: should I use recalculation of acceptance probability every time I reject the proposal (something like in delayed rejection method), or I have to use another proposal (like lognormal, truncated normal, etc.)?
  
The simplest solution is to just calculate p(theta)=0 for theta outside the legal region, thus reject those jumps.  This will work fine (just remember that when you reject, you have to stay at the last value for one more iteration), but if you’re doing these rejections all the time, you might want to reparameterize your space, for example using logs for positive parameters, logits for constrained parameters, and softmax for parameters that are constrained to sum to 1.</p><p>4 0.14553311 <a title="861-tfidf-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-18-Those_wacky_anti-Bayesians_used_to_be_intimidating%2C_but_now_they%E2%80%99re_just_pathetic.html">2254 andrew gelman stats-2014-03-18-Those wacky anti-Bayesians used to be intimidating, but now they’re just pathetic</a></p>
<p>Introduction: From 2006 :
  
Eric Archer forwarded  this document  by Nick Freemantle, “The Reverend Bayes—was he really a prophet?”, in the Journal of the Royal Society of Medicine:

 

Does [Bayes's] contribution merit the enthusiasms of his followers? Or is his legacy overhyped? . . .


First, Bayesians appear to have an absolute right to disapprove of any conventional approach in statistics without offering a workable alternative—for example, a colleague recently stated at a meeting that ‘. . . it is OK to have multiple comparisons because Bayesians’ don’t believe in alpha spending’. . . .


Second, Bayesians appear to build an army of straw men—everything it seems is different and better from a Bayesian perspective, although many of the concepts seem remarkably familiar. For example, a very well known Bayesian statistician recently surprised the audience with his discovery of the P value as a useful Bayesian statistic at a meeting in Birmingham.


Third, Bayesians possess enormous enthusiasm fo</p><p>5 0.14093617 <a title="861-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<p>Introduction: Tomas Iesmantas had  asked me  for advice on a regression problem with 50 parameters, and I’d recommended Hamiltonian Monte Carlo.
 
A few weeks later he reported back: 
  
  
After trying several modifications (HMC for all parameters at once, HMC just for first level parameters and Riemman manifold Hamiltonian Monte Carlo method), I finally got it running with HMC just for first level parameters and for others using direct sampling, since conditional distributions turned out to have closed form.


However, even in this case it is quite tricky, since I had to employ mass matrix and not just diagonal but at the beginning of algorithm generated it randomly (ensuring it is positive definite). Such random generation of mass matrix is quite blind step, but it proved to be quite helpful.


Riemman manifold HMC is quite vagarious, or to be more specific, metric of manifold is very sensitive. In my model log-likelihood I had exponents and values of metrics matrix elements was very large and wh</p><p>6 0.12834077 <a title="861-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-25-It%E2%80%99s_binless%21__A_program_for_computing_normalizing_functions.html">1825 andrew gelman stats-2013-04-25-It’s binless!  A program for computing normalizing functions</a></p>
<p>7 0.12361915 <a title="861-tfidf-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-13-Hey%21__Here%E2%80%99s_a_referee_report_for_you%21.html">144 andrew gelman stats-2010-07-13-Hey!  Here’s a referee report for you!</a></p>
<p>8 0.12360671 <a title="861-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-01-David_MacKay_sez_._._._12%3F%3F.html">984 andrew gelman stats-2011-11-01-David MacKay sez . . . 12??</a></p>
<p>9 0.11528663 <a title="861-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-01-My_course_this_fall_on_Bayesian_Computation.html">884 andrew gelman stats-2011-09-01-My course this fall on Bayesian Computation</a></p>
<p>10 0.11395516 <a title="861-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-04-Bayesian_Learning_via_Stochastic_Gradient_Langevin_Dynamics.html">1443 andrew gelman stats-2012-08-04-Bayesian Learning via Stochastic Gradient Langevin Dynamics</a></p>
<p>11 0.11239074 <a title="861-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-19-Tempering_and_modes.html">1018 andrew gelman stats-2011-11-19-Tempering and modes</a></p>
<p>12 0.1061988 <a title="861-tfidf-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-23-Lowess_is_great.html">293 andrew gelman stats-2010-09-23-Lowess is great</a></p>
<p>13 0.10616311 <a title="861-tfidf-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-29-References_%28with_code%29_for_Bayesian_hierarchical_%28multilevel%29_modeling_and_structural_equation_modeling.html">2273 andrew gelman stats-2014-03-29-References (with code) for Bayesian hierarchical (multilevel) modeling and structural equation modeling</a></p>
<p>14 0.10289568 <a title="861-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>15 0.10005058 <a title="861-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<p>16 0.098412983 <a title="861-tfidf-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-What_are_the_trickiest_models_to_fit%3F.html">575 andrew gelman stats-2011-02-15-What are the trickiest models to fit?</a></p>
<p>17 0.097460531 <a title="861-tfidf-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-07-My_recent_debugging_experience.html">2161 andrew gelman stats-2014-01-07-My recent debugging experience</a></p>
<p>18 0.097014315 <a title="861-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-06-Slow_progress.html">1445 andrew gelman stats-2012-08-06-Slow progress</a></p>
<p>19 0.095724434 <a title="861-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-23-Examples_of_the_use_of_hierarchical_modeling_to_generalize_to_new_settings.html">1425 andrew gelman stats-2012-07-23-Examples of the use of hierarchical modeling to generalize to new settings</a></p>
<p>20 0.095556043 <a title="861-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-06-W%E2%80%99man_%3C_W%E2%80%99pedia%2C_again.html">945 andrew gelman stats-2011-10-06-W’man < W’pedia, again</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.123), (1, 0.12), (2, 0.0), (3, 0.039), (4, 0.061), (5, 0.047), (6, 0.025), (7, -0.069), (8, 0.047), (9, 0.039), (10, 0.039), (11, -0.004), (12, -0.028), (13, -0.008), (14, 0.007), (15, -0.007), (16, 0.009), (17, 0.02), (18, -0.004), (19, -0.014), (20, 0.008), (21, 0.045), (22, 0.014), (23, -0.011), (24, 0.004), (25, -0.002), (26, -0.026), (27, -0.033), (28, 0.013), (29, -0.003), (30, 0.029), (31, 0.033), (32, 0.007), (33, 0.031), (34, -0.009), (35, -0.058), (36, -0.06), (37, -0.019), (38, -0.05), (39, 0.005), (40, -0.016), (41, 0.054), (42, -0.051), (43, -0.008), (44, 0.055), (45, -0.049), (46, -0.037), (47, -0.002), (48, 0.071), (49, -0.012)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95010662 <a title="861-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-19-Will_Stan_work_well_with_40%C3%9740_matrices%3F.html">861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</a></p>
<p>Introduction: Tomas Iesmantas writes:
  
I’m dealing with high dimensional (40-50 parameters) hierarchical bayesian model applied to nonlinear Poisson regression problem.


Now I’m using an adaptive version for the Metropolis adjusted Langevin algorithm with a truncated drift (Yves F. Atchade, 2003) to obtain samples from posterior.


But this algorithm is not very efficient in my case, it needs several millions iterations as burn-in period. And simulation takes quite a long time, since algorithm has to work with 40×40 matrices.


Maybe you know another MCMC algorithm which could take not so many burn-in samples and would be able to deal with nonlinear regression? In non-hierarchical nonlinear regression model adaptive metropolis algorithm is enough, but in hierarchical case I could use something more effective.
  
My reply:
 
Try fitting the model in Stan.  If that doesn’t work, let me know.</p><p>2 0.74694085 <a title="861-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-24-F-f-f-fake_data.html">1735 andrew gelman stats-2013-02-24-F-f-f-fake data</a></p>
<p>Introduction: Tiago Fragoso writes:
  
Suppose I fit a two stage regression model


Y = a + bx + e 
a = cw + d + e1


I could fit it all in one step by using MCMC for example (my model is more complicated than that, so I’ll have to do it by MCMC). However, I could fit the first regression only using MCMC because those estimates are hard to obtain and perform the second regression using least squares or a separate MCMC. 


So there’s an ‘one step’ inference based on doing it all at the same time and a ‘two step’ inference by fitting one and using the estimates on the further steps. What is gained or lost between both? Is anything done in this question?
  
My response:
 
Rather than answering your particular question, I’ll give you my generic answer, which is to simulate fake data from your model, then fit your model both ways and see how the results differ.  Repeat the simulation a few thousand times and you can make all the statistical comparisons you like.</p><p>3 0.73756069 <a title="861-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-31-Using_factor_analysis_or_principal_components_analysis_or_measurement-error_models_for_biological_measurements_in_archaeology%3F.html">1094 andrew gelman stats-2011-12-31-Using factor analysis or principal components analysis or measurement-error models for biological measurements in archaeology?</a></p>
<p>Introduction: Greg Campbell writes:
  
I am a Canadian archaeologist (BSc in Chemistry) researching the past human use of European Atlantic shellfish. After two decades of practice I am finally getting a MA in archaeology at Reading. I am seeing if the habitat or size of harvested mussels (Mytilus edulis) can be reconstructed from measurements of the umbo (the pointy end, and the only bit that survives well in archaeological deposits) using log-transformed measurements (or allometry; relationships between dimensions are more likely exponential than linear). 
Of course multivariate regressions in most statistics packages (Minitab, SPSS, SAS) assume you are trying to predict one variable from all the others (a Model I regression), and use ordinary least squares to fit the regression line. For organismal dimensions this makes little sense, since all the dimensions are (at least in theory) free to change their mutual proportions during growth. So there is no predictor and predicted, mutual variation of</p><p>4 0.72115046 <a title="861-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<p>Introduction: Tomas Iesmantas had  asked me  for advice on a regression problem with 50 parameters, and I’d recommended Hamiltonian Monte Carlo.
 
A few weeks later he reported back: 
  
  
After trying several modifications (HMC for all parameters at once, HMC just for first level parameters and Riemman manifold Hamiltonian Monte Carlo method), I finally got it running with HMC just for first level parameters and for others using direct sampling, since conditional distributions turned out to have closed form.


However, even in this case it is quite tricky, since I had to employ mass matrix and not just diagonal but at the beginning of algorithm generated it randomly (ensuring it is positive definite). Such random generation of mass matrix is quite blind step, but it proved to be quite helpful.


Riemman manifold HMC is quite vagarious, or to be more specific, metric of manifold is very sensitive. In my model log-likelihood I had exponents and values of metrics matrix elements was very large and wh</p><p>5 0.69953835 <a title="861-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<p>Introduction: Becky Passonneau and colleagues at the Center for Computational Learning Systems (CCLS) at Columbia have been working on a project for ConEd (New York’s major electric utility) to  rank structures based on vulnerability to secondary events  (e.g., transformer explosions, cable meltdowns, electrical fires).  They’ve been using the R implementation  BayesTree  of Chipman, George and McCulloch’s  Bayesian Additive Regression Trees  (BART).
 
BART is a Bayesian non-parametric method that is non-identifiable in two ways.  Firstly, it is an additive tree model with a fixed number of trees, the indexes of which aren’t identified (you get the same predictions in a model swapping the order of the trees).  This is the same kind of non-identifiability you get with any mixture model (additive or interpolated) with an exchangeable prior on the mixture components.  Secondly, the trees themselves have varying structure over samples in terms of number of nodes and their topology (depth, branching, etc</p><p>6 0.678397 <a title="861-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-19-An_interweaving-transformation_strategy_for_boosting_MCMC_efficiency.html">964 andrew gelman stats-2011-10-19-An interweaving-transformation strategy for boosting MCMC efficiency</a></p>
<p>7 0.67838013 <a title="861-lsi-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-02-Why_we_hate_stepwise_regression.html">2357 andrew gelman stats-2014-06-02-Why we hate stepwise regression</a></p>
<p>8 0.67817557 <a title="861-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-What_are_the_trickiest_models_to_fit%3F.html">575 andrew gelman stats-2011-02-15-What are the trickiest models to fit?</a></p>
<p>9 0.67609364 <a title="861-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>10 0.67246038 <a title="861-lsi-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-18-The_1.6_rule.html">39 andrew gelman stats-2010-05-18-The 1.6 rule</a></p>
<p>11 0.65451008 <a title="861-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-19-R_package_for_Bayes_factors.html">1682 andrew gelman stats-2013-01-19-R package for Bayes factors</a></p>
<p>12 0.65272927 <a title="861-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>13 0.65065277 <a title="861-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-07-Robust_logistic_regression.html">1886 andrew gelman stats-2013-06-07-Robust logistic regression</a></p>
<p>14 0.64769113 <a title="861-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-22-Handling_multiple_versions_of_an_outcome_variable.html">726 andrew gelman stats-2011-05-22-Handling multiple versions of an outcome variable</a></p>
<p>15 0.64587355 <a title="861-lsi-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-31-Untunable_Metropolis.html">833 andrew gelman stats-2011-07-31-Untunable Metropolis</a></p>
<p>16 0.64352351 <a title="861-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-11-%E2%80%9C2_level_logit_with_2_REs_%26_large_sample._computational_nightmare_%E2%80%93_please_help%E2%80%9D.html">759 andrew gelman stats-2011-06-11-“2 level logit with 2 REs & large sample. computational nightmare – please help”</a></p>
<p>17 0.6381287 <a title="861-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>18 0.63605994 <a title="861-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-16-%E2%80%9CReal_data_can_be_a_pain%E2%80%9D.html">1460 andrew gelman stats-2012-08-16-“Real data can be a pain”</a></p>
<p>19 0.63605517 <a title="861-lsi-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Blending_results_from_two_relatively_independent_multi-level_models.html">250 andrew gelman stats-2010-09-02-Blending results from two relatively independent multi-level models</a></p>
<p>20 0.63473153 <a title="861-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-09-Same_old_same_old.html">1849 andrew gelman stats-2013-05-09-Same old same old</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(15, 0.074), (16, 0.038), (21, 0.011), (22, 0.017), (24, 0.12), (57, 0.27), (63, 0.017), (72, 0.025), (99, 0.295)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.92882448 <a title="861-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-20-A_statistical_model_for_underdispersion.html">1542 andrew gelman stats-2012-10-20-A statistical model for underdispersion</a></p>
<p>Introduction: We have lots of models for overdispersed count data but we rarely see underdispersed data.  But now I know what example I’ll be giving when this next comes up in class.  From a  book review  by Theo Tait:
  
A number of shark species go in for oophagy, or uterine cannibalism. Sand tiger foetuses ‘eat each other in utero, acting out the harshest form of sibling rivalry imaginable’. Only two babies emerge, one from each of the mother shark’s uteruses: the survivors have eaten everything else. ‘A female sand tiger gives birth to a baby that’s already a metre long and an experienced killer,’ explains Demian Chapman, an expert on the subject.
  
That’s what I call underdispersion.  E(y)=2, var(y)=0.  Take that, M. Poisson!</p><p>2 0.92645371 <a title="861-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-06-Krugman_disses_Hayek_as_%E2%80%9Cbeing_almost_entirely_about_politics_rather_than_economics%E2%80%9D.html">1043 andrew gelman stats-2011-12-06-Krugman disses Hayek as “being almost entirely about politics rather than economics”</a></p>
<p>Introduction: That’s ok , Krugman earlier  slammed  Galbraith.  (I wonder if Krugman is as big a fan of “tough choices” now as he was  in 1996 .)  Given Krugman’s politicization in recent years, I’m surprised he’s so dismissive of the political (rather than technical-economic) nature of Hayek’s influence.  (I don’t know if he’s changed his views on Galbraith in recent years.)
 
P.S.  Greg Mankiw, in contrast,  labels  Galbraith and Hayek as “two of the great economists of the 20th century” and writes, “even though their most famous works were written many decades ago, they are still well worth reading today.”</p><p>same-blog 3 0.91624439 <a title="861-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-19-Will_Stan_work_well_with_40%C3%9740_matrices%3F.html">861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</a></p>
<p>Introduction: Tomas Iesmantas writes:
  
I’m dealing with high dimensional (40-50 parameters) hierarchical bayesian model applied to nonlinear Poisson regression problem.


Now I’m using an adaptive version for the Metropolis adjusted Langevin algorithm with a truncated drift (Yves F. Atchade, 2003) to obtain samples from posterior.


But this algorithm is not very efficient in my case, it needs several millions iterations as burn-in period. And simulation takes quite a long time, since algorithm has to work with 40×40 matrices.


Maybe you know another MCMC algorithm which could take not so many burn-in samples and would be able to deal with nonlinear regression? In non-hierarchical nonlinear regression model adaptive metropolis algorithm is enough, but in hierarchical case I could use something more effective.
  
My reply:
 
Try fitting the model in Stan.  If that doesn’t work, let me know.</p><p>4 0.89943898 <a title="861-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-19-Tempering_and_modes.html">1018 andrew gelman stats-2011-11-19-Tempering and modes</a></p>
<p>Introduction: Gustavo  writes:
  
Tempering should always be done in the spirit of *searching* for important modes of the distribution.  If we assume that we know where they are, then there is no point to tempering. Now, tempering is actually a *bad* way of searching for important modes, it just happens to be easy to program.  As always, my [Gustavo's] prescription is to FIRST find the important modes (as a pre-processing step); THEN sample from each mode independently; and FINALLY weight the samples appropriately, based on the estimated probability mass of each mode, though things might get messy if you end 
up jumping between modes.
  
My reply:
 
1.  Parallel tempering has always seemed like a great idea, but I have to admit that the only time I tried it (with Matt2 on the tree-ring example), it didn’t work for us.
 
2.  You say you’d rather sample from the modes and then average over them.  But that won’t work if if you have a zillion modes.  Also, if you know where the modes are, the quickest w</p><p>5 0.89173925 <a title="861-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-05-What_are_the_standards_for_reliability_in_experimental_psychology%3F.html">1101 andrew gelman stats-2012-01-05-What are the standards for reliability in experimental psychology?</a></p>
<p>Introduction: An experimental psychologist was wondering about the standards in that field for “acceptable reliability” (when looking at inter-rater reliability in coding data).  He wondered, for example, if some variation on signal detectability theory might be applied to adjust for inter-rater differences in criteria for saying some code is present.
 
What about Cohen’s kappa?  The psychologist wrote:
  
Cohen’s kappa does adjust for “guessing,” but its assumptions are not well motivated, perhaps not any more than adjustments for guessing versus the application of signal detectability theory where that can be applied. But one can’t do a straightforward application of signal detectability theory for reliability in that you don’t know whether the signal is present or not.
  
I think measurement issues are important but I don’t have enough experience in this area to answer the question without knowing more about the problem that this researcher is working on.
 
I’m posting it here because I imagine t</p><p>6 0.88276589 <a title="861-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-06-One_reason_New_York_isn%E2%80%99t_as_rich_as_it_used_to_be%3A__Redistribution_of_federal_tax_money_to_other_states.html">1485 andrew gelman stats-2012-09-06-One reason New York isn’t as rich as it used to be:  Redistribution of federal tax money to other states</a></p>
<p>7 0.88111496 <a title="861-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-30-Convenient_page_of_data_sources_from_the_Washington_Post.html">1146 andrew gelman stats-2012-01-30-Convenient page of data sources from the Washington Post</a></p>
<p>8 0.87125129 <a title="861-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-16-%E2%80%9CReal_data_can_be_a_pain%E2%80%9D.html">1460 andrew gelman stats-2012-08-16-“Real data can be a pain”</a></p>
<p>9 0.86960012 <a title="861-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-18-DataMarket.html">215 andrew gelman stats-2010-08-18-DataMarket</a></p>
<p>10 0.86010051 <a title="861-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-15-Fun_fight_over_the_Grover_search_algorithm.html">1120 andrew gelman stats-2012-01-15-Fun fight over the Grover search algorithm</a></p>
<p>11 0.85897928 <a title="861-lda-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-29-Statistics_and_the_end_of_time.html">306 andrew gelman stats-2010-09-29-Statistics and the end of time</a></p>
<p>12 0.84683418 <a title="861-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-26-How_to_understand_coefficients_that_reverse_sign_when_you_start_controlling_for_things%3F.html">1870 andrew gelman stats-2013-05-26-How to understand coefficients that reverse sign when you start controlling for things?</a></p>
<p>13 0.84484768 <a title="861-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-03-This_post_does_not_mention_Wegman.html">989 andrew gelman stats-2011-11-03-This post does not mention Wegman</a></p>
<p>14 0.84333575 <a title="861-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-06-The_K_Foundation_burns_Cosma%E2%80%99s_turkey.html">1044 andrew gelman stats-2011-12-06-The K Foundation burns Cosma’s turkey</a></p>
<p>15 0.84142077 <a title="861-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-30-Stan_uses_Nuts%21.html">1036 andrew gelman stats-2011-11-30-Stan uses Nuts!</a></p>
<p>16 0.83364224 <a title="861-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-29-Another_one_of_those_%E2%80%9CPsychological_Science%E2%80%9D_papers_%28this_time_on_biceps_size_and_political_attitudes_among_college_students%29.html">1876 andrew gelman stats-2013-05-29-Another one of those “Psychological Science” papers (this time on biceps size and political attitudes among college students)</a></p>
<p>17 0.82445806 <a title="861-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-09-Blogging%2C_polemical_and_otherwise.html">1108 andrew gelman stats-2012-01-09-Blogging, polemical and otherwise</a></p>
<p>18 0.81260109 <a title="861-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-05-World_Bank_data_now_online.html">891 andrew gelman stats-2011-09-05-World Bank data now online</a></p>
<p>19 0.8109659 <a title="861-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-16-Another_update_on_the_spam_email_study.html">35 andrew gelman stats-2010-05-16-Another update on the spam email study</a></p>
<p>20 0.80938095 <a title="861-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-22-%E2%80%9CInformation_visualization%E2%80%9D_vs._%E2%80%9CStatistical_graphics%E2%80%9D.html">816 andrew gelman stats-2011-07-22-“Information visualization” vs. “Statistical graphics”</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
