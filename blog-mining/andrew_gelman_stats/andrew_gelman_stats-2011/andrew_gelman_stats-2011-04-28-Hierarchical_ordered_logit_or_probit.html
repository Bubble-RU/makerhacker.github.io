<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>684 andrew gelman stats-2011-04-28-Hierarchical ordered logit or probit</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-684" href="#">andrew_gelman_stats-2011-684</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>684 andrew gelman stats-2011-04-28-Hierarchical ordered logit or probit</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-684-html" href="http://andrewgelman.com/2011/04/28/hierarchical_or/">html</a></p><p>Introduction: Jeff writes:
  
How far off is bglmer and can it handle ordered logit or multinom logit?
  
My reply:
 
bglmer is very close.  No ordered logit but I was just talking about it with Sophia today.  My guess is that the easiest way to fit a hierarchical ordered logit or multinom logit will be to use stan.  For right now I’d recommend using glmer/bglmer to fit the ordered logits in order (e.g., 1 vs. 2,3,4, then 2 vs. 3,4, then 3 vs. 4).  Or maybe there’s already a hierarchical multinomial logit in mcmcpack or somewhere?</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Jeff writes:    How far off is bglmer and can it handle ordered logit or multinom logit? [sent-1, score-1.911]
</p><p>2 No ordered logit but I was just talking about it with Sophia today. [sent-3, score-1.151]
</p><p>3 My guess is that the easiest way to fit a hierarchical ordered logit or multinom logit will be to use stan. [sent-4, score-2.563]
</p><p>4 For right now I’d recommend using glmer/bglmer to fit the ordered logits in order (e. [sent-5, score-0.932]
</p><p>5 Or maybe there’s already a hierarchical multinomial logit in mcmcpack or somewhere? [sent-11, score-1.002]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('logit', 0.628), ('ordered', 0.462), ('multinom', 0.363), ('bglmer', 0.312), ('logits', 0.156), ('multinomial', 0.144), ('hierarchical', 0.142), ('sophia', 0.136), ('easiest', 0.122), ('fit', 0.107), ('handle', 0.093), ('jeff', 0.085), ('somewhere', 0.084), ('recommend', 0.072), ('order', 0.064), ('talking', 0.061), ('far', 0.053), ('guess', 0.053), ('already', 0.052), ('reply', 0.048), ('right', 0.037), ('maybe', 0.036), ('using', 0.034), ('use', 0.032), ('way', 0.026), ('writes', 0.025)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="684-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-28-Hierarchical_ordered_logit_or_probit.html">684 andrew gelman stats-2011-04-28-Hierarchical ordered logit or probit</a></p>
<p>Introduction: Jeff writes:
  
How far off is bglmer and can it handle ordered logit or multinom logit?
  
My reply:
 
bglmer is very close.  No ordered logit but I was just talking about it with Sophia today.  My guess is that the easiest way to fit a hierarchical ordered logit or multinom logit will be to use stan.  For right now I’d recommend using glmer/bglmer to fit the ordered logits in order (e.g., 1 vs. 2,3,4, then 2 vs. 3,4, then 3 vs. 4).  Or maybe there’s already a hierarchical multinomial logit in mcmcpack or somewhere?</p><p>2 0.46639845 <a title="684-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-29-Putting_together_multinomial_discrete_regressions_by_combining_simple_logits.html">782 andrew gelman stats-2011-06-29-Putting together multinomial discrete regressions by combining simple logits</a></p>
<p>Introduction: When predicting 0/1 data we can use logit (or probit or robit or some other robust model such as invlogit (0.01 + 0.98*X*beta)).  Logit is simple enough and we can use  bayesglm  to regularize and avoid the problem of separation.
 
What if there are more than 2 categories?  If they’re ordered (1, 2, 3, etc), we can do ordered logit (and use bayespolr() to avoid separation).  If the categories are unordered (vanilla, chocolate, strawberry), there are unordered multinomial logit and probit models out there.
 
But it’s not so easy to fit these multinomial model in a multilevel setting (with coefficients that vary by group), especially if the computation is embedded in an iterative routine such as mi where you have real time constraints at each step.
 
So this got me wondering whether we could kluge it with logits.  Here’s the basic idea (in the ordered and unordered forms):
 
- If you have a variable that goes 1, 2, 3, etc., set up a series of logits:  1 vs. 2,3,…; 2 vs. 3,…; and so forth</p><p>3 0.26875281 <a title="684-tfidf-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-08-How_to_display_multinominal_logit_results_graphically%3F.html">2163 andrew gelman stats-2014-01-08-How to display multinominal logit results graphically?</a></p>
<p>Introduction: Adriana Lins de Albuquerque writes:
  
Do you have any suggestions for the best way to represent multinominal logit results graphically? I am using stata.
  
My reply:  I donâ&euro;&trade;t know from Stata, but here are my suggestions:
 
1.  If the categories are unordered, break them up into a series of binary choices in a tree structure (for example, non-voter or voter, then voting for left or right, then voting for left party A or B, then voting for right party C or D).  Each of these is a binary split and so can be displayed using the usual techniques for logit (as in chapters 3 and 4 of ARM).
 
2.  If the categories are ordered, see Figure 6.4 of ARM for an example (from our analysis of storable votes).</p><p>4 0.23417822 <a title="684-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>Introduction: A research psychologist writes in with a question that’s so long that I’ll put my answer first, then put the question itself below the fold.
 
Here’s my reply:
 
As I wrote in my Anova paper and in my book with Jennifer Hill, I do think that multilevel models can completely replace Anova.  At the same time, I think the central idea of Anova should persist in our understanding of these models.  To me the central idea of Anova is not F-tests or p-values or sums of squares, but rather the idea of predicting an outcome based on factors with discrete levels, and understanding these factors using variance components.
 
The continuous or categorical response thing doesn’t really matter so much to me.  I have no problem using a normal linear model for continuous outcomes (perhaps suitably transformed) and a logistic model for binary outcomes.
 
I don’t want to throw away interactions just because they’re not statistically significant.  I’d rather partially pool them toward zero using an inform</p><p>5 0.19167611 <a title="684-tfidf-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-17-Clutering_and_variance_components.html">417 andrew gelman stats-2010-11-17-Clutering and variance components</a></p>
<p>Introduction: Raymond Lim writes:
  
Do you have any recommendations on clustering and binary models? My particular problem is I’m running a firm fixed effect logit and want to cluster by industry-year (every combination of industry-year). My control variable of interest in measured by industry-year and when I cluster by industry-year, the standard errors are 300x larger than when I don’t cluster. Strangely, this problem only occurs when doing logit and not OLS (linear probability). Also, clustering just by field doesn’t blow up the errors. My hunch is it has something to do with the non-nested structure of year, but I don’t understand why this is only problematic under logit and not OLS.
  
My reply:
 
I’d recommend including four multilevel variance parameters, one for firm, one for industry, one for year, and one for industry-year.  (In lmer, that’s (1 | firm) + (1 | industry) + (1 | year) + (1 | industry.year)).  No need to include (1 | firm.year) since in your data this is the error term.  Try</p><p>6 0.17375265 <a title="684-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-04-Whassup_with_glm%28%29%3F.html">696 andrew gelman stats-2011-05-04-Whassup with glm()?</a></p>
<p>7 0.14858583 <a title="684-tfidf-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-18-The_1.6_rule.html">39 andrew gelman stats-2010-05-18-The 1.6 rule</a></p>
<p>8 0.13787307 <a title="684-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-30-Computational_problems_with_glm_etc..html">1516 andrew gelman stats-2012-09-30-Computational problems with glm etc.</a></p>
<p>9 0.13077308 <a title="684-tfidf-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-06-Multilevel_quantile_regression.html">397 andrew gelman stats-2010-11-06-Multilevel quantile regression</a></p>
<p>10 0.10734029 <a title="684-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-07-Robust_logistic_regression.html">1886 andrew gelman stats-2013-06-07-Robust logistic regression</a></p>
<p>11 0.10494238 <a title="684-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-31-Using_sample_size_in_the_prior_distribution.html">547 andrew gelman stats-2011-01-31-Using sample size in the prior distribution</a></p>
<p>12 0.1048997 <a title="684-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-11-Debutante_Hill.html">1977 andrew gelman stats-2013-08-11-Debutante Hill</a></p>
<p>13 0.090950951 <a title="684-tfidf-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-08-Displaying_a_fitted_multilevel_model.html">328 andrew gelman stats-2010-10-08-Displaying a fitted multilevel model</a></p>
<p>14 0.074777767 <a title="684-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-11-%E2%80%9C2_level_logit_with_2_REs_%26_large_sample._computational_nightmare_%E2%80%93_please_help%E2%80%9D.html">759 andrew gelman stats-2011-06-11-“2 level logit with 2 REs & large sample. computational nightmare – please help”</a></p>
<p>15 0.066055268 <a title="684-tfidf-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-08-Silly_old_chi-square%21.html">401 andrew gelman stats-2010-11-08-Silly old chi-square!</a></p>
<p>16 0.065081239 <a title="684-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-28-Simplify_until_your_fake-data_check_works%2C_then_add_complications_until_you_can_figure_out_where_the_problem_is_coming_from.html">1875 andrew gelman stats-2013-05-28-Simplify until your fake-data check works, then add complications until you can figure out where the problem is coming from</a></p>
<p>17 0.064413056 <a title="684-tfidf-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>18 0.063599437 <a title="684-tfidf-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-10-Small_multiples_of_lineplots_%3E_maps_%28ok%2C_not_always%2C_but_yes_in_this_case%29.html">2288 andrew gelman stats-2014-04-10-Small multiples of lineplots > maps (ok, not always, but yes in this case)</a></p>
<p>19 0.063462459 <a title="684-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Interpreting_interactions_in_discrete-data_regression.html">1908 andrew gelman stats-2013-06-21-Interpreting interactions in discrete-data regression</a></p>
<p>20 0.062530488 <a title="684-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-26-Modeling_probability_data.html">1284 andrew gelman stats-2012-04-26-Modeling probability data</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.044), (1, 0.047), (2, 0.026), (3, 0.017), (4, 0.056), (5, 0.002), (6, 0.019), (7, -0.042), (8, 0.039), (9, 0.033), (10, 0.032), (11, 0.017), (12, -0.005), (13, -0.018), (14, -0.008), (15, 0.023), (16, -0.0), (17, -0.011), (18, -0.015), (19, -0.029), (20, 0.027), (21, -0.0), (22, 0.018), (23, -0.03), (24, -0.027), (25, -0.04), (26, -0.005), (27, -0.03), (28, -0.013), (29, -0.026), (30, -0.006), (31, 0.031), (32, 0.028), (33, 0.005), (34, -0.037), (35, -0.047), (36, -0.03), (37, 0.014), (38, -0.03), (39, 0.063), (40, 0.008), (41, -0.018), (42, 0.013), (43, -0.029), (44, 0.007), (45, 0.012), (46, -0.014), (47, 0.003), (48, 0.025), (49, 0.077)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97682488 <a title="684-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-28-Hierarchical_ordered_logit_or_probit.html">684 andrew gelman stats-2011-04-28-Hierarchical ordered logit or probit</a></p>
<p>Introduction: Jeff writes:
  
How far off is bglmer and can it handle ordered logit or multinom logit?
  
My reply:
 
bglmer is very close.  No ordered logit but I was just talking about it with Sophia today.  My guess is that the easiest way to fit a hierarchical ordered logit or multinom logit will be to use stan.  For right now I’d recommend using glmer/bglmer to fit the ordered logits in order (e.g., 1 vs. 2,3,4, then 2 vs. 3,4, then 3 vs. 4).  Or maybe there’s already a hierarchical multinomial logit in mcmcpack or somewhere?</p><p>2 0.74637896 <a title="684-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-29-Putting_together_multinomial_discrete_regressions_by_combining_simple_logits.html">782 andrew gelman stats-2011-06-29-Putting together multinomial discrete regressions by combining simple logits</a></p>
<p>Introduction: When predicting 0/1 data we can use logit (or probit or robit or some other robust model such as invlogit (0.01 + 0.98*X*beta)).  Logit is simple enough and we can use  bayesglm  to regularize and avoid the problem of separation.
 
What if there are more than 2 categories?  If they’re ordered (1, 2, 3, etc), we can do ordered logit (and use bayespolr() to avoid separation).  If the categories are unordered (vanilla, chocolate, strawberry), there are unordered multinomial logit and probit models out there.
 
But it’s not so easy to fit these multinomial model in a multilevel setting (with coefficients that vary by group), especially if the computation is embedded in an iterative routine such as mi where you have real time constraints at each step.
 
So this got me wondering whether we could kluge it with logits.  Here’s the basic idea (in the ordered and unordered forms):
 
- If you have a variable that goes 1, 2, 3, etc., set up a series of logits:  1 vs. 2,3,…; 2 vs. 3,…; and so forth</p><p>3 0.69238842 <a title="684-lsi-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-08-How_to_display_multinominal_logit_results_graphically%3F.html">2163 andrew gelman stats-2014-01-08-How to display multinominal logit results graphically?</a></p>
<p>Introduction: Adriana Lins de Albuquerque writes:
  
Do you have any suggestions for the best way to represent multinominal logit results graphically? I am using stata.
  
My reply:  I donâ&euro;&trade;t know from Stata, but here are my suggestions:
 
1.  If the categories are unordered, break them up into a series of binary choices in a tree structure (for example, non-voter or voter, then voting for left or right, then voting for left party A or B, then voting for right party C or D).  Each of these is a binary split and so can be displayed using the usual techniques for logit (as in chapters 3 and 4 of ARM).
 
2.  If the categories are ordered, see Figure 6.4 of ARM for an example (from our analysis of storable votes).</p><p>4 0.67787206 <a title="684-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-04-Whassup_with_glm%28%29%3F.html">696 andrew gelman stats-2011-05-04-Whassup with glm()?</a></p>
<p>Introduction: We’re having problem with starting values in glm().  A very simple logistic regression with just an intercept with a very simple starting value (beta=5) blows up.
  

 

Here’s the R code:
 
 
> y <- rep (c(1,0),c(10,5))
> glm (y ~ 1, family=binomial(link="logit"))

Call:  glm(formula = y ~ 1, family = binomial(link = "logit"))

Coefficients:
(Intercept)  
     0.6931  

Degrees of Freedom: 14 Total (i.e. Null);  14 Residual
Null Deviance:      19.1 
Residual Deviance: 19.1         AIC: 21.1 
> glm (y ~ 1, family=binomial(link="logit"), start=2)

Call:  glm(formula = y ~ 1, family = binomial(link = "logit"), start = 2)

Coefficients:
(Intercept)  
     0.6931  

Degrees of Freedom: 14 Total (i.e. Null);  14 Residual
Null Deviance:      19.1 
Residual Deviance: 19.1         AIC: 21.1 
> glm (y ~ 1, family=binomial(link="logit"), start=5)

Call:  glm(formula = y ~ 1, family = binomial(link = "logit"), start = 5)

Coefficients:
(Intercept)  
  1.501e+15  

Degrees of Freedom: 14 Total (i.</p><p>5 0.66049331 <a title="684-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-30-Computational_problems_with_glm_etc..html">1516 andrew gelman stats-2012-09-30-Computational problems with glm etc.</a></p>
<p>Introduction: John Mount  provides some useful background and follow-up  on our discussion  from last year  on computational instability of the usual logistic regression solver.
 
Just to refresh your memory, here’s a simple logistic regression with only a constant term and no separation, nothing pathological at all:
 
 > y <- rep (c(1,0),c(10,5)) 
> display (glm (y ~ 1, family=binomial(link="logit"))) 
glm(formula = y ~ 1, family = binomial(link = "logit")) 
            coef.est coef.se 
(Intercept) 0.69     0.55 
--- 
  n = 15, k = 1 
  residual deviance = 19.1, null deviance = 19.1 (difference = 0.0) 
 
And here’s what happens when we give it the not-outrageous starting value of -2:
 
 > display (glm (y ~ 1, family=binomial(link="logit"), start=-2)) 
glm(formula = y ~ 1, family = binomial(link = "logit"), start = -2) 
            coef.est    coef.se 
(Intercept)       71.97 17327434.18 
--- 
  n = 15, k = 1 
  residual deviance = 360.4, null deviance = 19.1 (difference = -341.3) 
Warning message:</p><p>6 0.65147507 <a title="684-lsi-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-18-The_1.6_rule.html">39 andrew gelman stats-2010-05-18-The 1.6 rule</a></p>
<p>7 0.63124269 <a title="684-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-18-Standardizing_regression_inputs.html">1462 andrew gelman stats-2012-08-18-Standardizing regression inputs</a></p>
<p>8 0.61975998 <a title="684-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Interpreting_interactions_in_discrete-data_regression.html">1908 andrew gelman stats-2013-06-21-Interpreting interactions in discrete-data regression</a></p>
<p>9 0.61805558 <a title="684-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-28-Simplify_until_your_fake-data_check_works%2C_then_add_complications_until_you_can_figure_out_where_the_problem_is_coming_from.html">1875 andrew gelman stats-2013-05-28-Simplify until your fake-data check works, then add complications until you can figure out where the problem is coming from</a></p>
<p>10 0.59885436 <a title="684-lsi-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-06-Multilevel_quantile_regression.html">397 andrew gelman stats-2010-11-06-Multilevel quantile regression</a></p>
<p>11 0.59372985 <a title="684-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-19-Will_Stan_work_well_with_40%C3%9740_matrices%3F.html">861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</a></p>
<p>12 0.59005368 <a title="684-lsi-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-01-Imputing_count_data.html">14 andrew gelman stats-2010-05-01-Imputing count data</a></p>
<p>13 0.58333439 <a title="684-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-13-Checking_your_model_using_fake_data.html">852 andrew gelman stats-2011-08-13-Checking your model using fake data</a></p>
<p>14 0.56988311 <a title="684-lsi-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-17-Clutering_and_variance_components.html">417 andrew gelman stats-2010-11-17-Clutering and variance components</a></p>
<p>15 0.55357718 <a title="684-lsi-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-08-Displaying_a_fitted_multilevel_model.html">328 andrew gelman stats-2010-10-08-Displaying a fitted multilevel model</a></p>
<p>16 0.54338121 <a title="684-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-07-Robust_logistic_regression.html">1886 andrew gelman stats-2013-06-07-Robust logistic regression</a></p>
<p>17 0.54174215 <a title="684-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-01-Modeling_y_%3D_a_%2B_b_%2B_c.html">1294 andrew gelman stats-2012-05-01-Modeling y = a + b + c</a></p>
<p>18 0.5376668 <a title="684-lsi-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-28-Matching_for_preprocessing_data_for_causal_inference.html">375 andrew gelman stats-2010-10-28-Matching for preprocessing data for causal inference</a></p>
<p>19 0.53456998 <a title="684-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-18-Check_your_missing-data_imputations_using_cross-validation.html">1218 andrew gelman stats-2012-03-18-Check your missing-data imputations using cross-validation</a></p>
<p>20 0.52719641 <a title="684-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-15-Exploratory_multilevel_analysis_when_group-level_variables_are_of_importance.html">1900 andrew gelman stats-2013-06-15-Exploratory multilevel analysis when group-level variables are of importance</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(16, 0.033), (24, 0.118), (50, 0.039), (55, 0.043), (63, 0.298), (84, 0.155), (99, 0.104)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92114109 <a title="684-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-28-Hierarchical_ordered_logit_or_probit.html">684 andrew gelman stats-2011-04-28-Hierarchical ordered logit or probit</a></p>
<p>Introduction: Jeff writes:
  
How far off is bglmer and can it handle ordered logit or multinom logit?
  
My reply:
 
bglmer is very close.  No ordered logit but I was just talking about it with Sophia today.  My guess is that the easiest way to fit a hierarchical ordered logit or multinom logit will be to use stan.  For right now I’d recommend using glmer/bglmer to fit the ordered logits in order (e.g., 1 vs. 2,3,4, then 2 vs. 3,4, then 3 vs. 4).  Or maybe there’s already a hierarchical multinomial logit in mcmcpack or somewhere?</p><p>2 0.79592842 <a title="684-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-11-Calibration_in_chess.html">568 andrew gelman stats-2011-02-11-Calibration in chess</a></p>
<p>Introduction: Has anybody done  this study  yet?  I’m curious about the results.  Perhaps there’s some chess-playing cognitive psychologist who’d like to collaborate on this?</p><p>3 0.76095575 <a title="684-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-31-When_Did_Girls_Start_Wearing_Pink%3F.html">739 andrew gelman stats-2011-05-31-When Did Girls Start Wearing Pink?</a></p>
<p>Introduction: That cute picture is of toddler FDR in a dress, from 1884.  Jeanne Maglaty  writes :
  
A Ladies’ Home Journal article [or maybe from a different source, according to a commenter] in June 1918 said, “The generally accepted rule is pink for the boys, and blue for the girls. The reason is that pink, being a more decided and stronger color, is more suitable for the boy, while blue, which is more delicate and dainty, is prettier for the girl.” Other sources said blue was flattering for blonds, pink for brunettes; or blue was for blue-eyed babies, pink for brown-eyed babies, according to Paoletti.


In 1927, Time magazine printed a chart showing sex-appropriate colors for girls and boys according to leading U.S. stores. In Boston, Filene’s told parents to dress boys in pink. So did Best & Co. in New York City, Halle’s in Cleveland and Marshall Field in Chicago.


Today’s color dictate wasn’t established until the 1940s . . .


When the women’s liberation movement arrived in the mid-1960s, w</p><p>4 0.74171734 <a title="684-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-25-100-year_floods.html">628 andrew gelman stats-2011-03-25-100-year floods</a></p>
<p>Introduction: According to the  National Weather Service :
  
What is a 100 year flood?  A 100 year flood is an event that statistically has a 1% chance of occurring in any given year. A 500 year flood has a .2% chance of occurring and a 1000 year flood has a .1% chance of occurring.
  
The accompanying map shows a part of Tennessee that in May 2010 had 1000-year levels of flooding.
 
 
 
At first, it seems hard to believe that a 1000-year flood would have just happened to occur last year.  But then, this is just a 1000-year flood for that particular place.  I donâ&euro;&trade;t really have a sense of the statistics of these events.  How many 100-year, 500-year, and 1000-year flood events have been recorded by the Weather Service, and when have they occurred?</p><p>5 0.72713995 <a title="684-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-03-A_question_for_psychometricians.html">313 andrew gelman stats-2010-10-03-A question for psychometricians</a></p>
<p>Introduction: Don Coffin writes:
  
A colleague of mine and I are doing a presentation for new faculty on a number of topics related to teaching.  Our charge is to identify interesting issues and to find research-based information for them about how to approach things.  So, what I wondered is, do you know of any published research dealing with the sort of issues about structuring a course and final exam in the ways you talk about in  this blog post ?  Some poking around in the usual places hasn’t turned anything up yet.
  
I don’t really know the psychometrics literature but I imagine that some good stuff has been written on principles of test design. There are probably some good papers from back in the 1920s.  Can anyone supply some references?</p><p>6 0.71098572 <a title="684-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-04-High-level_intellectual_discussions_in_the_Columbia_statistics_department.html">745 andrew gelman stats-2011-06-04-High-level intellectual discussions in the Columbia statistics department</a></p>
<p>7 0.70810634 <a title="684-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-12-black_and_Black%2C_white_and_White.html">1316 andrew gelman stats-2012-05-12-black and Black, white and White</a></p>
<p>8 0.70353359 <a title="684-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-22-Tables_as_graphs%3A_The_Ramanujan_principle.html">1078 andrew gelman stats-2011-12-22-Tables as graphs: The Ramanujan principle</a></p>
<p>9 0.69899184 <a title="684-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-29-Putting_together_multinomial_discrete_regressions_by_combining_simple_logits.html">782 andrew gelman stats-2011-06-29-Putting together multinomial discrete regressions by combining simple logits</a></p>
<p>10 0.66949946 <a title="684-lda-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-14-Felix_Salmon_wins_the_American_Statistical_Association%E2%80%99s_Excellence_in_Statistical_Reporting_Award.html">33 andrew gelman stats-2010-05-14-Felix Salmon wins the American Statistical Association’s Excellence in Statistical Reporting Award</a></p>
<p>11 0.65636897 <a title="684-lda-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-21-Why_modern_art_is_all_in_the_mind.html">102 andrew gelman stats-2010-06-21-Why modern art is all in the mind</a></p>
<p>12 0.65342855 <a title="684-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-13-Puzzles_of_criminal_justice.html">1621 andrew gelman stats-2012-12-13-Puzzles of criminal justice</a></p>
<p>13 0.63989919 <a title="684-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-03-Graphical_presentation_of_risk_ratios.html">126 andrew gelman stats-2010-07-03-Graphical presentation of risk ratios</a></p>
<p>14 0.62032765 <a title="684-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-11-Robert_H._Frank_and_P._J._O%E2%80%99Rourke_present_._._..html">1005 andrew gelman stats-2011-11-11-Robert H. Frank and P. J. O’Rourke present . . .</a></p>
<p>15 0.6191923 <a title="684-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-23-Lowess_is_great.html">293 andrew gelman stats-2010-09-23-Lowess is great</a></p>
<p>16 0.61003405 <a title="684-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-05-Two_exciting_movie_ideas%3A__%E2%80%9CSecond_Chance_U%E2%80%9D_and_%E2%80%9CThe_New_Dirty_Dozen%E2%80%9D.html">1484 andrew gelman stats-2012-09-05-Two exciting movie ideas:  “Second Chance U” and “The New Dirty Dozen”</a></p>
<p>17 0.60109508 <a title="684-lda-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-08-How_to_display_multinominal_logit_results_graphically%3F.html">2163 andrew gelman stats-2014-01-08-How to display multinominal logit results graphically?</a></p>
<p>18 0.60007387 <a title="684-lda-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-15-Recently_in_the_sister_blog.html">2249 andrew gelman stats-2014-03-15-Recently in the sister blog</a></p>
<p>19 0.59858966 <a title="684-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-23-Philosophy%3A__Pointer_to_Salmon.html">1181 andrew gelman stats-2012-02-23-Philosophy:  Pointer to Salmon</a></p>
<p>20 0.59411567 <a title="684-lda-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-02-%E2%80%9CIf_our_product_is_harmful_._._._we%E2%80%99ll_stop_making_it.%E2%80%9D.html">1480 andrew gelman stats-2012-09-02-“If our product is harmful . . . we’ll stop making it.”</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
