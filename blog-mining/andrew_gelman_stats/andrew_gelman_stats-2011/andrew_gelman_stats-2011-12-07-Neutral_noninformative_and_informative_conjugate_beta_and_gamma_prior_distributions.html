<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1046 andrew gelman stats-2011-12-07-Neutral noninformative and informative conjugate beta and gamma prior distributions</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-1046" href="#">andrew_gelman_stats-2011-1046</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1046 andrew gelman stats-2011-12-07-Neutral noninformative and informative conjugate beta and gamma prior distributions</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-1046-html" href="http://andrewgelman.com/2011/12/07/neutral-noninformative-and-informative-conjugate-beta-and-gamma-prior-distributions/">html</a></p><p>Introduction: Jouni Kerman did a cool bit of research justifying the Beta (1/3, 1/3) prior as noninformative for binomial data, and the Gamma (1/3, 0) prior for Poisson data.
 
You probably thought that nothing new could be said about noninformative priors in such basic problems, but you were wrong!
 
Here’s  the story :
  
The conjugate binomial and Poisson models are commonly used for estimating proportions or rates. However, it is not well known that the conventional noninformative conjugate priors tend to shrink the posterior quantiles toward the boundary or toward the middle of the parameter space, making them thus appear excessively informative. The shrinkage is always largest when the number of observed events is small. This behavior persists for all sample sizes and exposures. The effect of the prior is therefore most conspicuous and potentially controversial when analyzing rare events. As alternative default conjugate priors, I [Jouni] introduce Beta(1/3, 1/3) and Gamma(1/3, 0), which I cal</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Jouni Kerman did a cool bit of research justifying the Beta (1/3, 1/3) prior as noninformative for binomial data, and the Gamma (1/3, 0) prior for Poisson data. [sent-1, score-0.977]
</p><p>2 You probably thought that nothing new could be said about noninformative priors in such basic problems, but you were wrong! [sent-2, score-0.548]
</p><p>3 Here’s  the story :    The conjugate binomial and Poisson models are commonly used for estimating proportions or rates. [sent-3, score-0.667]
</p><p>4 However, it is not well known that the conventional noninformative conjugate priors tend to shrink the posterior quantiles toward the boundary or toward the middle of the parameter space, making them thus appear excessively informative. [sent-4, score-1.778]
</p><p>5 The shrinkage is always largest when the number of observed events is small. [sent-5, score-0.236]
</p><p>6 This behavior persists for all sample sizes and exposures. [sent-6, score-0.354]
</p><p>7 The effect of the prior is therefore most conspicuous and potentially controversial when analyzing rare events. [sent-7, score-0.618]
</p><p>8 As alternative default conjugate priors, I [Jouni] introduce Beta(1/3, 1/3) and Gamma(1/3, 0), which I call ‘neutral’ priors because they lead to posterior distributions with approximately 50 per cent probability that the true value is either smaller or larger than the maximum likelihood estimate. [sent-8, score-1.274]
</p><p>9 This holds for all sample sizes and exposures as long as the point estimate is not at the boundary of the parameter space. [sent-9, score-0.736]
</p><p>10 I also discuss the construction of informative prior distributions. [sent-10, score-0.297]
</p><p>11 Under the suggested formulation, the posterior median coincides approximately with the weighted average of the prior median and the sample mean, yielding priors that perform more intuitively than those obtained by matching moments and quantiles. [sent-11, score-1.987]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('conjugate', 0.317), ('priors', 0.292), ('noninformative', 0.256), ('jouni', 0.236), ('prior', 0.213), ('gamma', 0.187), ('boundary', 0.179), ('binomial', 0.177), ('poisson', 0.171), ('beta', 0.169), ('median', 0.167), ('posterior', 0.155), ('approximately', 0.137), ('sizes', 0.128), ('sample', 0.127), ('coincides', 0.125), ('conspicuous', 0.125), ('exposures', 0.118), ('justifying', 0.118), ('yielding', 0.113), ('parameter', 0.108), ('toward', 0.104), ('quantiles', 0.103), ('persists', 0.099), ('neutral', 0.097), ('intuitively', 0.097), ('proportions', 0.094), ('shrink', 0.094), ('cent', 0.091), ('formulation', 0.09), ('shrinkage', 0.088), ('moments', 0.087), ('weighted', 0.085), ('construction', 0.084), ('introduce', 0.082), ('largest', 0.082), ('commonly', 0.079), ('matching', 0.079), ('obtained', 0.078), ('holds', 0.076), ('controversial', 0.075), ('potentially', 0.071), ('maximum', 0.071), ('analyzing', 0.068), ('events', 0.066), ('rare', 0.066), ('conventional', 0.066), ('perform', 0.065), ('default', 0.065), ('smaller', 0.064)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1046-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-07-Neutral_noninformative_and_informative_conjugate_beta_and_gamma_prior_distributions.html">1046 andrew gelman stats-2011-12-07-Neutral noninformative and informative conjugate beta and gamma prior distributions</a></p>
<p>Introduction: Jouni Kerman did a cool bit of research justifying the Beta (1/3, 1/3) prior as noninformative for binomial data, and the Gamma (1/3, 0) prior for Poisson data.
 
You probably thought that nothing new could be said about noninformative priors in such basic problems, but you were wrong!
 
Here’s  the story :
  
The conjugate binomial and Poisson models are commonly used for estimating proportions or rates. However, it is not well known that the conventional noninformative conjugate priors tend to shrink the posterior quantiles toward the boundary or toward the middle of the parameter space, making them thus appear excessively informative. The shrinkage is always largest when the number of observed events is small. This behavior persists for all sample sizes and exposures. The effect of the prior is therefore most conspicuous and potentially controversial when analyzing rare events. As alternative default conjugate priors, I [Jouni] introduce Beta(1/3, 1/3) and Gamma(1/3, 0), which I cal</p><p>2 0.28049892 <a title="1046-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>Introduction: Following up on Christian’s  post  [link fixed] on the topic, I’d like to offer a few thoughts of my own.
 
In BDA, we express the idea that a noninformative prior is a placeholder:  you can use the noninformative prior to get the analysis started, then if your posterior distribution is less informative than you would like, or if it does not make sense, you can go back and add prior information.
 
Same thing for the data model (the “likelihood”), for that matter:  it often makes sense to start with something simple and conventional and then go from there.
 
So, in that sense, noninformative priors are no big deal, they’re just a way to get started.  Just don’t take them too seriously.
 
Traditionally in statistics we’ve worked with the paradigm of a single highly informative dataset with only weak external information.  But if the data are sparse and prior information is strong, we have to think differently.  And, when you increase the dimensionality of a problem, both these things hap</p><p>3 0.25313762 <a title="1046-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-29-More_by_Berger_and_me_on_weakly_informative_priors.html">1092 andrew gelman stats-2011-12-29-More by Berger and me on weakly informative priors</a></p>
<p>Introduction: A couple days ago we  discussed  some remarks by Tony O’Hagan and Jim Berger on weakly informative priors.  Jim  followed up  on Deborah Mayo’s blog with this:
  
Objective Bayesian priors are often improper (i.e., have infinite total mass), but this is not a problem when they are developed correctly. But not every improper prior is satisfactory. For instance, the constant prior is known to be unsatisfactory in many situations. The ‘solution’ pseudo-Bayesians often use is to choose a constant prior over a large but bounded set (a ‘weakly informative’ prior), saying it is now proper and so all is well. This is not true; if the constant prior on the whole parameter space is bad, so will be the constant prior over the bounded set. The problem is, in part, that some people confuse proper priors with subjective priors and, having learned that true subjective priors are fine, incorrectly presume that weakly informative proper priors are fine.
  
I have a few reactions to this:
 
1.  I agree</p><p>4 0.24564554 <a title="1046-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>Introduction: Nick Firoozye writes:
  
While I am absolutely sympathetic to the Bayesian agenda I am often troubled by the requirement of having priors. We must have priors on the parameter of an infinite number of model we have never seen before and I find this troubling. There is a similarly troubling problem in economics of utility theory. Utility is on consumables. To be complete a consumer must assign utility to all sorts of things they never would have encountered. More recent versions of utility theory instead make consumption goods a portfolio of attributes. Cadillacs are x many units of luxury y of transport etc etc. And we can automatically have personal utilities to all these attributes.  


I don’t ever see parameters. Some model have few and some have hundreds. Instead, I see data. So I don’t know how to have an opinion on parameters themselves. Rather I think it far more natural to have opinions on the behavior of models. The prior predictive density is a good and sensible notion. Also</p><p>5 0.22376499 <a title="1046-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>Introduction: For awhile I’ve been fitting most of my multilevel models using lmer/glmer, which gives point estimates of the group-level variance parameters (maximum marginal likelihood estimate for lmer and an approximation for glmer).  I’m usually satisfied with this–sure, point estimation understates the uncertainty in model fitting, but that’s typically the least of our worries. 
 
Sometimes, though, lmer/glmer estimates group-level variances at 0 or estimates group-level correlation parameters at +/- 1.  Typically, when this happens, it’s not that we’re so sure the variance is close to zero or that the correlation is close to 1 or -1; rather, the marginal likelihood does not provide a lot of information about these parameters of the group-level error distribution.
 
I don’t want point estimates on the boundary.  I don’t want to say that the unexplained variance in some dimension is exactly zero.
 
One way to handle this problem is full Bayes:  slap a prior on sigma, do your Gibbs and Metropolis</p><p>6 0.21603888 <a title="1046-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>7 0.20115411 <a title="1046-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>8 0.19514894 <a title="1046-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-15-Reputations_changeable%2C_situations_tolerable.html">1858 andrew gelman stats-2013-05-15-Reputations changeable, situations tolerable</a></p>
<p>9 0.19418362 <a title="1046-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>10 0.19185624 <a title="1046-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>11 0.18358393 <a title="1046-tfidf-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-02-The_problem_of_overestimation_of_group-level_variance_parameters.html">63 andrew gelman stats-2010-06-02-The problem of overestimation of group-level variance parameters</a></p>
<p>12 0.18254226 <a title="1046-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-27-%E2%80%9CKeeping_things_unridiculous%E2%80%9D%3A__Berger%2C_O%E2%80%99Hagan%2C_and_me_on_weakly_informative_priors.html">1087 andrew gelman stats-2011-12-27-“Keeping things unridiculous”:  Berger, O’Hagan, and me on weakly informative priors</a></p>
<p>13 0.18053688 <a title="1046-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-31-Using_sample_size_in_the_prior_distribution.html">547 andrew gelman stats-2011-01-31-Using sample size in the prior distribution</a></p>
<p>14 0.17743954 <a title="1046-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-30-Infill_asymptotics_and_sprawl_asymptotics.html">1877 andrew gelman stats-2013-05-30-Infill asymptotics and sprawl asymptotics</a></p>
<p>15 0.17497623 <a title="1046-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-19-Prior_distributions_on_derived_quantities_rather_than_on_parameters_themselves.html">1946 andrew gelman stats-2013-07-19-Prior distributions on derived quantities rather than on parameters themselves</a></p>
<p>16 0.17433551 <a title="1046-tfidf-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-13-On_the_half-Cauchy_prior_for_a_global_scale_parameter.html">801 andrew gelman stats-2011-07-13-On the half-Cauchy prior for a global scale parameter</a></p>
<p>17 0.16692446 <a title="1046-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-07-Prior_distributions_for_regression_coefficients.html">1486 andrew gelman stats-2012-09-07-Prior distributions for regression coefficients</a></p>
<p>18 0.1604833 <a title="1046-tfidf-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-15-Weakly_informative_priors_and_imprecise_probabilities.html">468 andrew gelman stats-2010-12-15-Weakly informative priors and imprecise probabilities</a></p>
<p>19 0.15899952 <a title="1046-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-21-D._Buggin.html">1465 andrew gelman stats-2012-08-21-D. Buggin</a></p>
<p>20 0.15699485 <a title="1046-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-11-%E2%80%9CInformative_g-Priors_for_Logistic_Regression%E2%80%9D.html">2017 andrew gelman stats-2013-09-11-“Informative g-Priors for Logistic Regression”</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.141), (1, 0.192), (2, 0.059), (3, 0.039), (4, -0.034), (5, -0.093), (6, 0.176), (7, 0.029), (8, -0.241), (9, 0.053), (10, -0.004), (11, -0.012), (12, 0.088), (13, 0.049), (14, -0.022), (15, -0.04), (16, -0.012), (17, -0.009), (18, 0.038), (19, -0.012), (20, -0.032), (21, -0.041), (22, -0.005), (23, 0.022), (24, -0.011), (25, 0.025), (26, 0.004), (27, 0.03), (28, 0.005), (29, 0.012), (30, -0.003), (31, -0.026), (32, 0.006), (33, -0.013), (34, -0.041), (35, 0.018), (36, 0.048), (37, -0.036), (38, 0.019), (39, 0.012), (40, -0.028), (41, -0.001), (42, -0.011), (43, 0.005), (44, 0.087), (45, 0.007), (46, -0.032), (47, -0.006), (48, -0.019), (49, 0.011)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99333775 <a title="1046-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-07-Neutral_noninformative_and_informative_conjugate_beta_and_gamma_prior_distributions.html">1046 andrew gelman stats-2011-12-07-Neutral noninformative and informative conjugate beta and gamma prior distributions</a></p>
<p>Introduction: Jouni Kerman did a cool bit of research justifying the Beta (1/3, 1/3) prior as noninformative for binomial data, and the Gamma (1/3, 0) prior for Poisson data.
 
You probably thought that nothing new could be said about noninformative priors in such basic problems, but you were wrong!
 
Here’s  the story :
  
The conjugate binomial and Poisson models are commonly used for estimating proportions or rates. However, it is not well known that the conventional noninformative conjugate priors tend to shrink the posterior quantiles toward the boundary or toward the middle of the parameter space, making them thus appear excessively informative. The shrinkage is always largest when the number of observed events is small. This behavior persists for all sample sizes and exposures. The effect of the prior is therefore most conspicuous and potentially controversial when analyzing rare events. As alternative default conjugate priors, I [Jouni] introduce Beta(1/3, 1/3) and Gamma(1/3, 0), which I cal</p><p>2 0.91248637 <a title="1046-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-15-Reputations_changeable%2C_situations_tolerable.html">1858 andrew gelman stats-2013-05-15-Reputations changeable, situations tolerable</a></p>
<p>Introduction: David Kessler, Peter Hoff, and David Dunson  write :
  
Marginally specified priors for nonparametric Bayesian estimation


Prior specification for nonparametric Bayesian inference involves the difficult task of quantifying prior knowledge about a parameter of high, often infinite, dimension. Realistically, a statistician is unlikely to have informed opinions about all aspects of such a parameter, but may have real information about functionals of the parameter, such the population mean or variance. This article proposes a new framework for nonparametric Bayes inference in which the prior distribution for a possibly infinite-dimensional parameter is decomposed into two parts: an informative prior on a finite set of functionals, and a nonparametric conditional prior for the parameter given the functionals. Such priors can be easily constructed from standard nonparametric prior distributions in common use, and inherit the large support of the standard priors upon which they are based. Ad</p><p>3 0.87482828 <a title="1046-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>Introduction: I’ve had a couple of email conversations in the past couple days on dependence in multivariate prior distributions.
 
 Modeling the degrees of freedom and scale parameters in the t distribution 
 
First, in our Stan group we’ve been discussing the choice of priors for the degrees-of-freedom parameter in the t distribution.  I wrote that also there’s the question of parameterization.  It does not necessarily make sense to have independent priors on the df and scale parameters.  In some sense, the meaning of the scale parameter changes with the df.
 
 Prior dependence between correlation and scale parameters in the scaled inverse-Wishart model 
 
The second case of parameterization in prior distribution arose from an email I received from Chris Chatham pointing me to  this exploration  by Matt Simpson of the scaled inverse-Wishart prior distribution for hierarchical covariance matrices.  Simpson writes:
  
A popular prior for Σ is the inverse-Wishart distribution [ not  the same as the</p><p>4 0.87425703 <a title="1046-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-29-More_by_Berger_and_me_on_weakly_informative_priors.html">1092 andrew gelman stats-2011-12-29-More by Berger and me on weakly informative priors</a></p>
<p>Introduction: A couple days ago we  discussed  some remarks by Tony O’Hagan and Jim Berger on weakly informative priors.  Jim  followed up  on Deborah Mayo’s blog with this:
  
Objective Bayesian priors are often improper (i.e., have infinite total mass), but this is not a problem when they are developed correctly. But not every improper prior is satisfactory. For instance, the constant prior is known to be unsatisfactory in many situations. The ‘solution’ pseudo-Bayesians often use is to choose a constant prior over a large but bounded set (a ‘weakly informative’ prior), saying it is now proper and so all is well. This is not true; if the constant prior on the whole parameter space is bad, so will be the constant prior over the bounded set. The problem is, in part, that some people confuse proper priors with subjective priors and, having learned that true subjective priors are fine, incorrectly presume that weakly informative proper priors are fine.
  
I have a few reactions to this:
 
1.  I agree</p><p>5 0.86701143 <a title="1046-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>Introduction: For awhile I’ve been fitting most of my multilevel models using lmer/glmer, which gives point estimates of the group-level variance parameters (maximum marginal likelihood estimate for lmer and an approximation for glmer).  I’m usually satisfied with this–sure, point estimation understates the uncertainty in model fitting, but that’s typically the least of our worries. 
 
Sometimes, though, lmer/glmer estimates group-level variances at 0 or estimates group-level correlation parameters at +/- 1.  Typically, when this happens, it’s not that we’re so sure the variance is close to zero or that the correlation is close to 1 or -1; rather, the marginal likelihood does not provide a lot of information about these parameters of the group-level error distribution.
 
I don’t want point estimates on the boundary.  I don’t want to say that the unexplained variance in some dimension is exactly zero.
 
One way to handle this problem is full Bayes:  slap a prior on sigma, do your Gibbs and Metropolis</p><p>6 0.85658211 <a title="1046-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>7 0.8373732 <a title="1046-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>8 0.82556629 <a title="1046-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-13-On_the_half-Cauchy_prior_for_a_global_scale_parameter.html">801 andrew gelman stats-2011-07-13-On the half-Cauchy prior for a global scale parameter</a></p>
<p>9 0.81502104 <a title="1046-lsi-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-01-bayesglm_in_Stata%3F.html">442 andrew gelman stats-2010-12-01-bayesglm in Stata?</a></p>
<p>10 0.80883056 <a title="1046-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>11 0.80683887 <a title="1046-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>12 0.80263919 <a title="1046-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-11-%E2%80%9CInformative_g-Priors_for_Logistic_Regression%E2%80%9D.html">2017 andrew gelman stats-2013-09-11-“Informative g-Priors for Logistic Regression”</a></p>
<p>13 0.79487711 <a title="1046-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-27-%E2%80%9CKeeping_things_unridiculous%E2%80%9D%3A__Berger%2C_O%E2%80%99Hagan%2C_and_me_on_weakly_informative_priors.html">1087 andrew gelman stats-2011-12-27-“Keeping things unridiculous”:  Berger, O’Hagan, and me on weakly informative priors</a></p>
<p>14 0.78684282 <a title="1046-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-20-Prior_beliefs_about_locations_of_decision_boundaries.html">1130 andrew gelman stats-2012-01-20-Prior beliefs about locations of decision boundaries</a></p>
<p>15 0.78313327 <a title="1046-lsi-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>16 0.77779716 <a title="1046-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>17 0.77615017 <a title="1046-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-19-Prior_distributions_on_derived_quantities_rather_than_on_parameters_themselves.html">1946 andrew gelman stats-2013-07-19-Prior distributions on derived quantities rather than on parameters themselves</a></p>
<p>18 0.77059746 <a title="1046-lsi-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-15-Weakly_informative_priors_and_imprecise_probabilities.html">468 andrew gelman stats-2010-12-15-Weakly informative priors and imprecise probabilities</a></p>
<p>19 0.7641589 <a title="1046-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-21-D._Buggin.html">1465 andrew gelman stats-2012-08-21-D. Buggin</a></p>
<p>20 0.74413037 <a title="1046-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-18-In_Memoriam_Dennis_Lindley.html">2138 andrew gelman stats-2013-12-18-In Memoriam Dennis Lindley</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(16, 0.024), (24, 0.673), (35, 0.02), (53, 0.029), (59, 0.012), (86, 0.015), (95, 0.011), (99, 0.1)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99428105 <a title="1046-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-30-Extended_Binary_Format_Support_for_Mac_OS_X.html">59 andrew gelman stats-2010-05-30-Extended Binary Format Support for Mac OS X</a></p>
<p>Introduction: Rodney Sparapani writes:
  
My Windows buddies have been bugging me about BRUGS and how great it is.  Now, running BRUGS on OS X may be possible.  Check out  this new amazing software  by Amit Singh.
  
Personally, I’d go with  R2jags , but I thought I’d pass this on in case others are interested.</p><p>2 0.99417609 <a title="1046-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-17-Attractive_models_%28and_data%29_wanted_for_statistical_art_show..html">471 andrew gelman stats-2010-12-17-Attractive models (and data) wanted for statistical art show.</a></p>
<p>Introduction: I have agreed to do a local art exhibition in February. 
 
An excuse to think about form, colour and style for plotting almost individual observation likelihoods – while invoking the artists privilege of refusing to give interpretations of their own work.
 
In order to make it possibly less dry I’ll try to use intuitive suggestive captions like in this example  TheTyranyof13.pdf 
 
thereby side stepping the technical discussions like here  RadfordNealBlog 
 
Suggested models and data sets (or even submissions) would be most appreciated.
 
I likely be sticking to realism i.e. plots that represent ‘statistical reality’ faithfully.
 
K?</p><p>3 0.99174488 <a title="1046-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-31-Paying_survey_respondents.html">1437 andrew gelman stats-2012-07-31-Paying survey respondents</a></p>
<p>Introduction: I  agree  with  Casey Mulligan  that participants in government surveys should be paid, and I think it should be part of the code of ethics for commercial pollsters to compensate their respondents also.
 
As Mulligan points out, if a survey is worth doing, it should be worth compensating the participants for their time and effort.
 
P.S.  Just to clarify, I do  not  recommend that Census surveys be made voluntary, I just think that respondents (who can be required to participate) should be paid a small amount.
 
P.P.S.  More rant  here .</p><p>same-blog 4 0.99085379 <a title="1046-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-07-Neutral_noninformative_and_informative_conjugate_beta_and_gamma_prior_distributions.html">1046 andrew gelman stats-2011-12-07-Neutral noninformative and informative conjugate beta and gamma prior distributions</a></p>
<p>Introduction: Jouni Kerman did a cool bit of research justifying the Beta (1/3, 1/3) prior as noninformative for binomial data, and the Gamma (1/3, 0) prior for Poisson data.
 
You probably thought that nothing new could be said about noninformative priors in such basic problems, but you were wrong!
 
Here’s  the story :
  
The conjugate binomial and Poisson models are commonly used for estimating proportions or rates. However, it is not well known that the conventional noninformative conjugate priors tend to shrink the posterior quantiles toward the boundary or toward the middle of the parameter space, making them thus appear excessively informative. The shrinkage is always largest when the number of observed events is small. This behavior persists for all sample sizes and exposures. The effect of the prior is therefore most conspicuous and potentially controversial when analyzing rare events. As alternative default conjugate priors, I [Jouni] introduce Beta(1/3, 1/3) and Gamma(1/3, 0), which I cal</p><p>5 0.98663747 <a title="1046-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-15-Gay-married_state_senator_shot_down_gay_marriage.html">613 andrew gelman stats-2011-03-15-Gay-married state senator shot down gay marriage</a></p>
<p>Introduction: This  is pretty amazing.</p><p>6 0.98663747 <a title="1046-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-14-The_joys_of_working_in_the_public_domain.html">712 andrew gelman stats-2011-05-14-The joys of working in the public domain</a></p>
<p>7 0.98663747 <a title="1046-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-21-Literary_blurb_translation_guide.html">723 andrew gelman stats-2011-05-21-Literary blurb translation guide</a></p>
<p>8 0.98663747 <a title="1046-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-03-Best_lottery_story_ever.html">1242 andrew gelman stats-2012-04-03-Best lottery story ever</a></p>
<p>9 0.98663747 <a title="1046-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-08-Jagdish_Bhagwati%E2%80%99s_definition_of_feminist_sincerity.html">1252 andrew gelman stats-2012-04-08-Jagdish Bhagwati’s definition of feminist sincerity</a></p>
<p>10 0.97412753 <a title="1046-lda-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-29-ARM_solutions.html">240 andrew gelman stats-2010-08-29-ARM solutions</a></p>
<p>11 0.96705347 <a title="1046-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-30-New_innovations_in_spam.html">545 andrew gelman stats-2011-01-30-New innovations in spam</a></p>
<p>12 0.95635676 <a title="1046-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-02-So-called_Bayesian_hypothesis_testing_is_just_as_bad_as_regular_hypothesis_testing.html">643 andrew gelman stats-2011-04-02-So-called Bayesian hypothesis testing is just as bad as regular hypothesis testing</a></p>
<p>13 0.94070792 <a title="1046-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-15-Swiss_Jonah_Lehrer_update.html">2024 andrew gelman stats-2013-09-15-Swiss Jonah Lehrer update</a></p>
<p>14 0.93588865 <a title="1046-lda-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-27-It%E2%80%99s_better_than_being_forwarded_the_latest_works_of_you-know-who.html">373 andrew gelman stats-2010-10-27-It’s better than being forwarded the latest works of you-know-who</a></p>
<p>15 0.93374068 <a title="1046-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-18-Breastfeeding%2C_infant_hyperbilirubinemia%2C_statistical_graphics%2C_and_modern_medicine.html">38 andrew gelman stats-2010-05-18-Breastfeeding, infant hyperbilirubinemia, statistical graphics, and modern medicine</a></p>
<p>16 0.91868216 <a title="1046-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-12-Fixing_the_race%2C_ethnicity%2C_and_national_origin_questions_on_the_U.S._Census.html">1978 andrew gelman stats-2013-08-12-Fixing the race, ethnicity, and national origin questions on the U.S. Census</a></p>
<p>17 0.91862392 <a title="1046-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-29-Ethics_and_statistics_in_development_research.html">241 andrew gelman stats-2010-08-29-Ethics and statistics in development research</a></p>
<p>18 0.91800338 <a title="1046-lda-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-28-God-leaf-tree.html">2229 andrew gelman stats-2014-02-28-God-leaf-tree</a></p>
<p>19 0.91554016 <a title="1046-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-03-Comparing_prediction_errors.html">938 andrew gelman stats-2011-10-03-Comparing prediction errors</a></p>
<p>20 0.91305006 <a title="1046-lda-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-01-Mothers_and_Moms.html">1479 andrew gelman stats-2012-09-01-Mothers and Moms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
