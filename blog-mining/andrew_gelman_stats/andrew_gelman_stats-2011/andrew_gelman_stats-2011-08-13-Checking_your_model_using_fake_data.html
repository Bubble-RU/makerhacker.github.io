<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>852 andrew gelman stats-2011-08-13-Checking your model using fake data</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-852" href="#">andrew_gelman_stats-2011-852</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>852 andrew gelman stats-2011-08-13-Checking your model using fake data</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-852-html" href="http://andrewgelman.com/2011/08/13/checking_your_m/">html</a></p><p>Introduction: Someone sent me the following email:
  
I tried to do a logistic regression . . . I programmed the model in different ways and got different answers . . . can’t get the results to match . . . What am I doing wrong? . . . Here’s my code . . . 
  
I didn’t have the time to look at his code so I gave the following general response:
 
One way to check things is to try simulating data from the fitted model, then fit your model again to the simulated data and see what happens.
 
P.S.  He followed my suggestion and responded a few days later:
  
Yeah, that did the trick!  I was treating a factor variable as a covariate!
  
I love it when generic advice works out!</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Someone sent me the following email:    I tried to do a logistic regression . [sent-1, score-0.666]
</p><p>2 I programmed the model in different ways and got different answers . [sent-4, score-1.025]
</p><p>3 I didn’t have the time to look at his code so I gave the following general response:   One way to check things is to try simulating data from the fitted model, then fit your model again to the simulated data and see what happens. [sent-17, score-2.186]
</p><p>4 He followed my suggestion and responded a few days later:    Yeah, that did the trick! [sent-20, score-0.598]
</p><p>5 I was treating a factor variable as a covariate! [sent-21, score-0.499]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('code', 0.267), ('programmed', 0.241), ('simulating', 0.233), ('covariate', 0.233), ('treating', 0.229), ('simulated', 0.205), ('trick', 0.185), ('generic', 0.183), ('model', 0.181), ('match', 0.173), ('fitted', 0.172), ('suggestion', 0.17), ('yeah', 0.167), ('answers', 0.162), ('following', 0.161), ('responded', 0.159), ('logistic', 0.157), ('factor', 0.145), ('followed', 0.136), ('days', 0.133), ('tried', 0.131), ('advice', 0.131), ('gave', 0.127), ('email', 0.126), ('love', 0.125), ('variable', 0.125), ('works', 0.123), ('check', 0.122), ('different', 0.121), ('sent', 0.119), ('later', 0.118), ('ways', 0.109), ('response', 0.103), ('fit', 0.099), ('regression', 0.098), ('wrong', 0.095), ('try', 0.09), ('got', 0.09), ('someone', 0.087), ('results', 0.085), ('data', 0.083), ('didn', 0.082), ('look', 0.079), ('general', 0.078), ('things', 0.068), ('time', 0.05), ('way', 0.048), ('get', 0.043), ('see', 0.04), ('one', 0.03)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="852-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-13-Checking_your_model_using_fake_data.html">852 andrew gelman stats-2011-08-13-Checking your model using fake data</a></p>
<p>Introduction: Someone sent me the following email:
  
I tried to do a logistic regression . . . I programmed the model in different ways and got different answers . . . can’t get the results to match . . . What am I doing wrong? . . . Here’s my code . . . 
  
I didn’t have the time to look at his code so I gave the following general response:
 
One way to check things is to try simulating data from the fitted model, then fit your model again to the simulated data and see what happens.
 
P.S.  He followed my suggestion and responded a few days later:
  
Yeah, that did the trick!  I was treating a factor variable as a covariate!
  
I love it when generic advice works out!</p><p>2 0.15201712 <a title="852-tfidf-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-08-Displaying_a_fitted_multilevel_model.html">328 andrew gelman stats-2010-10-08-Displaying a fitted multilevel model</a></p>
<p>Introduction: Elissa Brown writes:
  
I’m working on some data using a multinomial model (3 categories for the response &  2 predictors-1 continuous and 1 binary), and I’ve been looking and looking for some sort of nice graphical way to show my model at work. Something like a predicted probabilities plot. I know you can do this for the levels of Y with just one covariate, but is this still a valid way to describe the multinomial model (just doing a pred plot for each covariate)? What’s the deal, is there really no way to graphically represent a successful multinomial model? Also, is it unreasonable to break down your model into a binary response just to get some ROC curves? This seems like cheating. From what I’ve found so far, it seems that people just avoid graphical support when discussing their fitted multinomial models.
  
My reply:
 
It’s hard for me to think about this sort of thing in the abstract with no context.  We do have one example in chapter 6 of ARM where we display data and fitted m</p><p>3 0.1434236 <a title="852-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-07-Reproducible_science_FAIL_%28so_far%29%3A__What%E2%80%99s_stoppin_people_from_sharin_data_and_code%3F.html">1447 andrew gelman stats-2012-08-07-Reproducible science FAIL (so far):  What’s stoppin people from sharin data and code?</a></p>
<p>Introduction: David Karger writes:
  
Your  recent post  on sharing data was of great interest to me, as my own research in computer science asks how to incentivize and lower barriers to data sharing.   I was particularly curious about your highlighting of effort as the major dis-incentive to sharing.  I would love to hear more, as this question of effort is on we specifically target in our development of tools for data authoring and publishing.


As a straw man, let me point out that sharing data technically requires no more than posting an excel spreadsheet online.  And that you likely already produced that spreadsheet during your own analytic work.   So, in what way does such low-tech publishing fail to meet your data sharing objectives?


Our own hypothesis has been that the effort is really quite low, with the problem being a lack of *immediate/tangible* benefits (as opposed to the long-term values you accurately describe).  To attack this problem, we’re developing  tools  (and, since it appear</p><p>4 0.14040753 <a title="852-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-06-Slow_progress.html">1445 andrew gelman stats-2012-08-06-Slow progress</a></p>
<p>Introduction: I received the following message:
  
I am a Psychology postgraduate at the University of Glasgow and am writing for an article request. I’ve just read your 2008 published article titled “A weakly informative default prior distribution for logistic and other regression models” and found from it that your group also wrote a report on applying the Bayesian logistic regression approach to multilevel model, which is titled “An approximate EM algorithm for multilevel generalized linear models”. I have been looking for it online but did find it, and was wondering if I may request this report from you?
  
My first thought is that this is a good sign that psychology undergraduates are reading papers like this.  Unfortunately I had to reply as follows:
  
Hi, we actually programmed this up but never debugged it!  So no actual paper . . .
  
I think I could’ve done it if I had ever focused on the problem.  Between the messiness of the algebra and the messiness of the R code, I never got it all to</p><p>5 0.13961866 <a title="852-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-24-F-f-f-fake_data.html">1735 andrew gelman stats-2013-02-24-F-f-f-fake data</a></p>
<p>Introduction: Tiago Fragoso writes:
  
Suppose I fit a two stage regression model


Y = a + bx + e 
a = cw + d + e1


I could fit it all in one step by using MCMC for example (my model is more complicated than that, so I’ll have to do it by MCMC). However, I could fit the first regression only using MCMC because those estimates are hard to obtain and perform the second regression using least squares or a separate MCMC. 


So there’s an ‘one step’ inference based on doing it all at the same time and a ‘two step’ inference by fitting one and using the estimates on the further steps. What is gained or lost between both? Is anything done in this question?
  
My response:
 
Rather than answering your particular question, I’ll give you my generic answer, which is to simulate fake data from your model, then fit your model both ways and see how the results differ.  Repeat the simulation a few thousand times and you can make all the statistical comparisons you like.</p><p>6 0.13892429 <a title="852-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>7 0.13259037 <a title="852-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-Piss-poor_monocausal_social_science.html">1196 andrew gelman stats-2012-03-04-Piss-poor monocausal social science</a></p>
<p>8 0.13214657 <a title="852-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-17-Data_problems%2C_coding_errors%E2%80%A6what_can_be_done%3F.html">1807 andrew gelman stats-2013-04-17-Data problems, coding errors…what can be done?</a></p>
<p>9 0.12747532 <a title="852-tfidf-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-18-Predictive_checks_for_hierarchical_models.html">154 andrew gelman stats-2010-07-18-Predictive checks for hierarchical models</a></p>
<p>10 0.11971414 <a title="852-tfidf-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-19-Updated_R_code_and_data_for_ARM.html">41 andrew gelman stats-2010-05-19-Updated R code and data for ARM</a></p>
<p>11 0.11887547 <a title="852-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-01-When_should_you_worry_about_imputed_data%3F.html">935 andrew gelman stats-2011-10-01-When should you worry about imputed data?</a></p>
<p>12 0.10637002 <a title="852-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-28-Simplify_until_your_fake-data_check_works%2C_then_add_complications_until_you_can_figure_out_where_the_problem_is_coming_from.html">1875 andrew gelman stats-2013-05-28-Simplify until your fake-data check works, then add complications until you can figure out where the problem is coming from</a></p>
<p>13 0.10563438 <a title="852-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-28-Using_predator-prey_models_on_the_Canadian_lynx_series.html">1141 andrew gelman stats-2012-01-28-Using predator-prey models on the Canadian lynx series</a></p>
<p>14 0.10461496 <a title="852-tfidf-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-14-The_statistics_and_the_science.html">146 andrew gelman stats-2010-07-14-The statistics and the science</a></p>
<p>15 0.10297011 <a title="852-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-13-Lame_Statistics_Patents.html">1761 andrew gelman stats-2013-03-13-Lame Statistics Patents</a></p>
<p>16 0.10110158 <a title="852-tfidf-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-17-Matching_at_two_levels.html">213 andrew gelman stats-2010-08-17-Matching at two levels</a></p>
<p>17 0.1009469 <a title="852-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-27-Geophysicist_Discovers_Modeling_Error_%28in_Economics%29.html">976 andrew gelman stats-2011-10-27-Geophysicist Discovers Modeling Error (in Economics)</a></p>
<p>18 0.098452047 <a title="852-tfidf-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-07-Selection_bias_in_the_reporting_of_shaky_research.html">2236 andrew gelman stats-2014-03-07-Selection bias in the reporting of shaky research</a></p>
<p>19 0.0973837 <a title="852-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-17-Graphical_tools_for_understanding_multilevel_models.html">772 andrew gelman stats-2011-06-17-Graphical tools for understanding multilevel models</a></p>
<p>20 0.096583739 <a title="852-tfidf-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Useful_models%2C_model_checking%2C_and_external_validation%3A__a_mini-discussion.html">244 andrew gelman stats-2010-08-30-Useful models, model checking, and external validation:  a mini-discussion</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.17), (1, 0.081), (2, 0.013), (3, 0.038), (4, 0.112), (5, 0.007), (6, 0.03), (7, -0.079), (8, 0.1), (9, 0.016), (10, 0.058), (11, 0.054), (12, -0.007), (13, -0.021), (14, -0.049), (15, 0.042), (16, 0.034), (17, -0.067), (18, -0.007), (19, 0.026), (20, 0.03), (21, -0.021), (22, 0.01), (23, -0.084), (24, -0.051), (25, 0.004), (26, 0.024), (27, -0.113), (28, -0.001), (29, -0.013), (30, -0.003), (31, 0.012), (32, -0.033), (33, 0.059), (34, 0.003), (35, -0.031), (36, -0.044), (37, 0.052), (38, -0.045), (39, 0.014), (40, 0.027), (41, 0.017), (42, -0.011), (43, -0.016), (44, 0.044), (45, 0.012), (46, -0.016), (47, -0.021), (48, 0.039), (49, -0.003)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96438128 <a title="852-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-13-Checking_your_model_using_fake_data.html">852 andrew gelman stats-2011-08-13-Checking your model using fake data</a></p>
<p>Introduction: Someone sent me the following email:
  
I tried to do a logistic regression . . . I programmed the model in different ways and got different answers . . . can’t get the results to match . . . What am I doing wrong? . . . Here’s my code . . . 
  
I didn’t have the time to look at his code so I gave the following general response:
 
One way to check things is to try simulating data from the fitted model, then fit your model again to the simulated data and see what happens.
 
P.S.  He followed my suggestion and responded a few days later:
  
Yeah, that did the trick!  I was treating a factor variable as a covariate!
  
I love it when generic advice works out!</p><p>2 0.84894699 <a title="852-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-24-F-f-f-fake_data.html">1735 andrew gelman stats-2013-02-24-F-f-f-fake data</a></p>
<p>Introduction: Tiago Fragoso writes:
  
Suppose I fit a two stage regression model


Y = a + bx + e 
a = cw + d + e1


I could fit it all in one step by using MCMC for example (my model is more complicated than that, so I’ll have to do it by MCMC). However, I could fit the first regression only using MCMC because those estimates are hard to obtain and perform the second regression using least squares or a separate MCMC. 


So there’s an ‘one step’ inference based on doing it all at the same time and a ‘two step’ inference by fitting one and using the estimates on the further steps. What is gained or lost between both? Is anything done in this question?
  
My response:
 
Rather than answering your particular question, I’ll give you my generic answer, which is to simulate fake data from your model, then fit your model both ways and see how the results differ.  Repeat the simulation a few thousand times and you can make all the statistical comparisons you like.</p><p>3 0.80740601 <a title="852-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-08-Displaying_a_fitted_multilevel_model.html">328 andrew gelman stats-2010-10-08-Displaying a fitted multilevel model</a></p>
<p>Introduction: Elissa Brown writes:
  
I’m working on some data using a multinomial model (3 categories for the response &  2 predictors-1 continuous and 1 binary), and I’ve been looking and looking for some sort of nice graphical way to show my model at work. Something like a predicted probabilities plot. I know you can do this for the levels of Y with just one covariate, but is this still a valid way to describe the multinomial model (just doing a pred plot for each covariate)? What’s the deal, is there really no way to graphically represent a successful multinomial model? Also, is it unreasonable to break down your model into a binary response just to get some ROC curves? This seems like cheating. From what I’ve found so far, it seems that people just avoid graphical support when discussing their fitted multinomial models.
  
My reply:
 
It’s hard for me to think about this sort of thing in the abstract with no context.  We do have one example in chapter 6 of ARM where we display data and fitted m</p><p>4 0.80447918 <a title="852-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-28-Simplify_until_your_fake-data_check_works%2C_then_add_complications_until_you_can_figure_out_where_the_problem_is_coming_from.html">1875 andrew gelman stats-2013-05-28-Simplify until your fake-data check works, then add complications until you can figure out where the problem is coming from</a></p>
<p>Introduction: I received the following email: 
  
  
I am trying to develop a Bayesian model to represent the process through which individual consumers make online product rating decisions. In my model each individual faces total J product options and for each product option (j) each individual (i) needs to make three sequential decisions: 


- First he decides whether to consume a specific product option (j) or not (choice decision)


- If he decides to consume a product option j, then after consumption he decides whether to rate it or not (incidence decision) 


- If he decides to rate product j then what finally he decides what rating (k) to assign to it (evaluation decision)


We  model this decision sequence in terms of three equations. A binary response variable in the first equation represents the choice decision. Another binary response variable in the second equation represents the incidence decision that is observable only when first selection decision is 1. Finally, an ordered response v</p><p>5 0.79490149 <a title="852-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-29-Putting_together_multinomial_discrete_regressions_by_combining_simple_logits.html">782 andrew gelman stats-2011-06-29-Putting together multinomial discrete regressions by combining simple logits</a></p>
<p>Introduction: When predicting 0/1 data we can use logit (or probit or robit or some other robust model such as invlogit (0.01 + 0.98*X*beta)).  Logit is simple enough and we can use  bayesglm  to regularize and avoid the problem of separation.
 
What if there are more than 2 categories?  If they’re ordered (1, 2, 3, etc), we can do ordered logit (and use bayespolr() to avoid separation).  If the categories are unordered (vanilla, chocolate, strawberry), there are unordered multinomial logit and probit models out there.
 
But it’s not so easy to fit these multinomial model in a multilevel setting (with coefficients that vary by group), especially if the computation is embedded in an iterative routine such as mi where you have real time constraints at each step.
 
So this got me wondering whether we could kluge it with logits.  Here’s the basic idea (in the ordered and unordered forms):
 
- If you have a variable that goes 1, 2, 3, etc., set up a series of logits:  1 vs. 2,3,…; 2 vs. 3,…; and so forth</p><p>6 0.78336209 <a title="852-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-19-Will_Stan_work_well_with_40%C3%9740_matrices%3F.html">861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</a></p>
<p>7 0.75242013 <a title="852-lsi-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-18-The_1.6_rule.html">39 andrew gelman stats-2010-05-18-The 1.6 rule</a></p>
<p>8 0.7490108 <a title="852-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-22-A_Bayesian_model_for_an_increasing_function%2C_in_Stan%21.html">2110 andrew gelman stats-2013-11-22-A Bayesian model for an increasing function, in Stan!</a></p>
<p>9 0.74223214 <a title="852-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-07-Robust_logistic_regression.html">1886 andrew gelman stats-2013-06-07-Robust logistic regression</a></p>
<p>10 0.74130392 <a title="852-lsi-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-28-Matching_for_preprocessing_data_for_causal_inference.html">375 andrew gelman stats-2010-10-28-Matching for preprocessing data for causal inference</a></p>
<p>11 0.74008667 <a title="852-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-31-Using_factor_analysis_or_principal_components_analysis_or_measurement-error_models_for_biological_measurements_in_archaeology%3F.html">1094 andrew gelman stats-2011-12-31-Using factor analysis or principal components analysis or measurement-error models for biological measurements in archaeology?</a></p>
<p>12 0.73400855 <a title="852-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-01-When_should_you_worry_about_imputed_data%3F.html">935 andrew gelman stats-2011-10-01-When should you worry about imputed data?</a></p>
<p>13 0.72458833 <a title="852-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-18-Standardizing_regression_inputs.html">1462 andrew gelman stats-2012-08-18-Standardizing regression inputs</a></p>
<p>14 0.72177255 <a title="852-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-26-Including_interactions_or_not.html">823 andrew gelman stats-2011-07-26-Including interactions or not</a></p>
<p>15 0.7147193 <a title="852-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-28-Using_predator-prey_models_on_the_Canadian_lynx_series.html">1141 andrew gelman stats-2012-01-28-Using predator-prey models on the Canadian lynx series</a></p>
<p>16 0.70597023 <a title="852-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-04-What_are_the_key_assumptions_of_linear_regression%3F.html">1967 andrew gelman stats-2013-08-04-What are the key assumptions of linear regression?</a></p>
<p>17 0.7054435 <a title="852-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-12-Do_you_ever_have_that_I-just-fit-a-model_feeling%3F.html">2018 andrew gelman stats-2013-09-12-Do you ever have that I-just-fit-a-model feeling?</a></p>
<p>18 0.70516962 <a title="852-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-27-Cross-validation_%28What_is_it_good_for%3F%29.html">1395 andrew gelman stats-2012-06-27-Cross-validation (What is it good for?)</a></p>
<p>19 0.70437294 <a title="852-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>20 0.6987834 <a title="852-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-16-%E2%80%9CReal_data_can_be_a_pain%E2%80%9D.html">1460 andrew gelman stats-2012-08-16-“Real data can be a pain”</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(16, 0.033), (21, 0.026), (24, 0.199), (28, 0.024), (30, 0.094), (34, 0.032), (38, 0.032), (55, 0.035), (85, 0.026), (86, 0.018), (99, 0.363)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98529375 <a title="852-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-13-Checking_your_model_using_fake_data.html">852 andrew gelman stats-2011-08-13-Checking your model using fake data</a></p>
<p>Introduction: Someone sent me the following email:
  
I tried to do a logistic regression . . . I programmed the model in different ways and got different answers . . . can’t get the results to match . . . What am I doing wrong? . . . Here’s my code . . . 
  
I didn’t have the time to look at his code so I gave the following general response:
 
One way to check things is to try simulating data from the fitted model, then fit your model again to the simulated data and see what happens.
 
P.S.  He followed my suggestion and responded a few days later:
  
Yeah, that did the trick!  I was treating a factor variable as a covariate!
  
I love it when generic advice works out!</p><p>2 0.9763478 <a title="852-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-19-Standardized_writing_styles_and_standardized_graphing_styles.html">1176 andrew gelman stats-2012-02-19-Standardized writing styles and standardized graphing styles</a></p>
<p>Introduction: Back in the 1700s—JennyD can correct me if I’m wrong here—there was no standard style for writing.  You could be discursive, you could be descriptive, flowery, or terse.  Direct or indirect, serious or funny.  You could construct a novel out of letters or write a philosophical treatise in the form of a novel.
 
Nowadays there are rules.  You can break the rules, but then you’re Breaking. The. Rules.  Which is a distinctive choice all its own.
 
Consider academic writing.  Serious works of economics or statistics tend to be written in a serious style in some version of plain academic English.  The few exceptions (for example, by Tukey, Tufte, Mandelbrot, and Jaynes) are clearly exceptions, written in styles that are much celebrated but not so commonly followed.
 
A serious work of statistics, or economics, or political science  could  be written in a highly unconventional form (consider, for example, Wallace Shawn’s plays), but academic writers in these fields tend to stick with the sta</p><p>3 0.97618568 <a title="852-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-25-Incoherence_of_Bayesian_data_analysis.html">1510 andrew gelman stats-2012-09-25-Incoherence of Bayesian data analysis</a></p>
<p>Introduction: Hogg writes:
  
At the end  this article  you wonder about consistency.  Have you ever considered the possibility that utility might resolve some of the problems?  I have no idea if it 
would—I am not advocating that position—I just get some kind of intuition from phrases like “Judgment is required to decide…”. Perhaps there is a coherent and objective description of what is—or could be—done under a coherent “utility” model (like a utility that could be objectively agreed upon and computed).  Utilities are usually subjective—true—but priors are usually subjective too.
  
My reply:
 
I’m happy to think about utility, for some particular problem or class of problems going to the effort of assigning costs and benefits to different outcomes.  I agree that a utility analysis, even if (necessarily) imperfect, can usefully focus discussion.  For example, if a statistical method for selecting variables is justified on the basis of cost, I like the idea of attempting to quantify the costs of ga</p><p>4 0.97541106 <a title="852-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-11-The_happiness_gene%3A__My_bottom_line_%28for_now%29.html">706 andrew gelman stats-2011-05-11-The happiness gene:  My bottom line (for now)</a></p>
<p>Introduction: I had a couple of email exchanges with Jan-Emmanuel De Neve and James Fowler, two of the authors of the article on the gene that is associated with life satisfaction which we  blogged  the other day.  (Bruno Frey, the third author of the article in question, is out of town according to his email.)  Fowler also commented  directly  on the blog.
 
I won’t go through all the details, but now I have a better sense of what’s going on.  (Thanks, Jan and James!)  Here’s my current understanding:
 
 1.   The original manuscript was divided into two parts:   an article  by De Neve alone published in the Journal of Human Genetics, and  an article  by De Neve, Fowler, Frey, and Nicholas Christakis submitted to Econometrica.  The latter paper repeats the analysis from the Adolescent Health survey and also replicates with data from the Framingham heart study (hence Christakis’s involvement).
 
The Framingham study measures a slightly different gene and uses a slightly life-satisfaction question com</p><p>5 0.97470874 <a title="852-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-24-Bell_Labs.html">970 andrew gelman stats-2011-10-24-Bell Labs</a></p>
<p>Introduction: Sining Chen told me they’re hiring in the  statistics group at Bell Labs .  I’ll do my bit for economic stimulus by announcing this job (see below).
 
I love Bell Labs.  I worked there for three summers, in a physics lab in 1985-86 under the supervision of Loren Pfeiffer, and by myself in the statistics group in 1990.
 
I learned a lot working for Loren.  He was a really smart and driven guy.  His lab was a small set of rooms—in Bell Labs, everything’s in a small room, as they value the positive externality of close physical proximity of different labs, which you get by making each lab compact—and it was Loren, his assistant (a guy named Ken West who kept everything running in the lab), and three summer students: me, Gowton Achaibar, and a girl whose name I’ve forgotten.  Gowtan and I had a lot of fun chatting in the lab.  One day I made a silly comment about Gowton’s accent—he was from Guyana and pronounced “three” as “tree”—and then I apologized and said:  Hey, here I am making fun o</p><p>6 0.97444367 <a title="852-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-05-How_much_do_we_trust_a_new_claim_that_early_childhood_stimulation_raised_earnings_by_42%25%3F.html">2090 andrew gelman stats-2013-11-05-How much do we trust a new claim that early childhood stimulation raised earnings by 42%?</a></p>
<p>7 0.9743377 <a title="852-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-04-Write_This_Book.html">1605 andrew gelman stats-2012-12-04-Write This Book</a></p>
<p>8 0.97422421 <a title="852-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-29-Infovis%2C_infographics%2C_and_data_visualization%3A__Where_I%E2%80%99m_coming_from%2C_and_where_I%E2%80%99d_like_to_go.html">878 andrew gelman stats-2011-08-29-Infovis, infographics, and data visualization:  Where I’m coming from, and where I’d like to go</a></p>
<p>9 0.97408068 <a title="852-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-03-Advice_that%E2%80%99s_so_eminently_sensible_but_so_difficult_to_follow.html">1520 andrew gelman stats-2012-10-03-Advice that’s so eminently sensible but so difficult to follow</a></p>
<p>10 0.97376251 <a title="852-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>11 0.97373885 <a title="852-lda-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>12 0.97371936 <a title="852-lda-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-17-How_to_think_about_the_statistical_evidence_when_the_statistical_evidence_can%E2%80%99t_be_conclusive%3F.html">2174 andrew gelman stats-2014-01-17-How to think about the statistical evidence when the statistical evidence can’t be conclusive?</a></p>
<p>13 0.97360402 <a title="852-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-10-Cross-validation_and_Bayesian_estimation_of_tuning_parameters.html">2129 andrew gelman stats-2013-12-10-Cross-validation and Bayesian estimation of tuning parameters</a></p>
<p>14 0.9734723 <a title="852-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-16-Infovis_and_statgraphics_update_update.html">855 andrew gelman stats-2011-08-16-Infovis and statgraphics update update</a></p>
<p>15 0.97313213 <a title="852-lda-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-24-%E2%80%9CEdlin%E2%80%99s_rule%E2%80%9D_for_routinely_scaling_down_published_estimates.html">2223 andrew gelman stats-2014-02-24-“Edlin’s rule” for routinely scaling down published estimates</a></p>
<p>16 0.97309846 <a title="852-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-02-The_inevitable_problems_with_statistical_significance_and_95%25_intervals.html">1150 andrew gelman stats-2012-02-02-The inevitable problems with statistical significance and 95% intervals</a></p>
<p>17 0.97303712 <a title="852-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-06-Early_stopping_and_penalized_likelihood.html">788 andrew gelman stats-2011-07-06-Early stopping and penalized likelihood</a></p>
<p>18 0.9728809 <a title="852-lda-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-14-%E2%80%9CToo_much_data%E2%80%9D%3F.html">86 andrew gelman stats-2010-06-14-“Too much data”?</a></p>
<p>19 0.97267109 <a title="852-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-28-Behavioral_economics_doesn%E2%80%99t_seem_to_have_much_to_say_about_marriage.html">594 andrew gelman stats-2011-02-28-Behavioral economics doesn’t seem to have much to say about marriage</a></p>
<p>20 0.97266978 <a title="852-lda-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-08-Here%E2%80%99s_how_rumors_get_started%3A__Lineplots%2C_dotplots%2C_and_nonfunctional_modernist_architecture.html">262 andrew gelman stats-2010-09-08-Here’s how rumors get started:  Lineplots, dotplots, and nonfunctional modernist architecture</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
