<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>858 andrew gelman stats-2011-08-17-Jumping off the edge of the world</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-858" href="#">andrew_gelman_stats-2011-858</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>858 andrew gelman stats-2011-08-17-Jumping off the edge of the world</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-858-html" href="http://andrewgelman.com/2011/08/17/jumping_off_the/">html</a></p><p>Introduction: Tomas Iesmantas writes:
  
I’m facing a problem where parameter space is bounded, e.g. all parameters have to be positive. If in MCMC as proposal distribution I use normal distribution, then at some iterations I get negative proposals. So my question is: should I use recalculation of acceptance probability every time I reject the proposal (something like in delayed rejection method), or I have to use another proposal (like lognormal, truncated normal, etc.)?
  
The simplest solution is to just calculate p(theta)=0 for theta outside the legal region, thus reject those jumps.  This will work fine (just remember that when you reject, you have to stay at the last value for one more iteration), but if you’re doing these rejections all the time, you might want to reparameterize your space, for example using logs for positive parameters, logits for constrained parameters, and softmax for parameters that are constrained to sum to 1.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Tomas Iesmantas writes:    I’m facing a problem where parameter space is bounded, e. [sent-1, score-0.416]
</p><p>2 If in MCMC as proposal distribution I use normal distribution, then at some iterations I get negative proposals. [sent-4, score-0.921]
</p><p>3 So my question is: should I use recalculation of acceptance probability every time I reject the proposal (something like in delayed rejection method), or I have to use another proposal (like lognormal, truncated normal, etc. [sent-5, score-2.212]
</p><p>4 The simplest solution is to just calculate p(theta)=0 for theta outside the legal region, thus reject those jumps. [sent-7, score-1.112]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reject', 0.329), ('proposal', 0.324), ('parameters', 0.286), ('constrained', 0.245), ('theta', 0.216), ('recalculation', 0.178), ('delayed', 0.168), ('logs', 0.168), ('logits', 0.168), ('iesmantas', 0.168), ('rejections', 0.168), ('tomas', 0.168), ('space', 0.167), ('normal', 0.165), ('truncated', 0.155), ('lognormal', 0.15), ('iteration', 0.15), ('facing', 0.133), ('iterations', 0.129), ('bounded', 0.129), ('distribution', 0.123), ('simplest', 0.121), ('rejection', 0.121), ('region', 0.12), ('sum', 0.118), ('acceptance', 0.115), ('calculate', 0.114), ('legal', 0.109), ('mcmc', 0.107), ('stay', 0.106), ('use', 0.103), ('outside', 0.083), ('solution', 0.08), ('negative', 0.077), ('parameter', 0.077), ('remember', 0.073), ('positive', 0.07), ('value', 0.065), ('method', 0.065), ('fine', 0.061), ('thus', 0.06), ('time', 0.058), ('probability', 0.056), ('every', 0.055), ('last', 0.051), ('another', 0.042), ('question', 0.042), ('problem', 0.039), ('like', 0.039), ('using', 0.037)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="858-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-17-Jumping_off_the_edge_of_the_world.html">858 andrew gelman stats-2011-08-17-Jumping off the edge of the world</a></p>
<p>Introduction: Tomas Iesmantas writes:
  
I’m facing a problem where parameter space is bounded, e.g. all parameters have to be positive. If in MCMC as proposal distribution I use normal distribution, then at some iterations I get negative proposals. So my question is: should I use recalculation of acceptance probability every time I reject the proposal (something like in delayed rejection method), or I have to use another proposal (like lognormal, truncated normal, etc.)?
  
The simplest solution is to just calculate p(theta)=0 for theta outside the legal region, thus reject those jumps.  This will work fine (just remember that when you reject, you have to stay at the last value for one more iteration), but if you’re doing these rejections all the time, you might want to reparameterize your space, for example using logs for positive parameters, logits for constrained parameters, and softmax for parameters that are constrained to sum to 1.</p><p>2 0.16091919 <a title="858-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-16-The_%E2%80%9CWashington_read%E2%80%9D_and_the_algebra_of_conditional_distributions.html">961 andrew gelman stats-2011-10-16-The “Washington read” and the algebra of conditional distributions</a></p>
<p>Introduction: I was trying to explain in class how a (Bayesian) statistician reads the formula for a probability distribution.  In old-fashioned statistics textbooks you’re told that if you want to compute a conditional distribution from a joint distribution you need to do some heavy math:  p(a|b) = p(a,b)/\int p(a’,b)da’.
 
When doing Bayesian statistics, though, you usually don’t have to do the integration or the division. If you have parameters theta and data y, you first write p(y,theta).  Then to get p(theta|y), you  don’t  need to integrate or divide.  All you have to do is look at p(y,theta) in a certain way:  Treat y as a constant and theta as a variable.  Similarly, if you’re doing the Gibbs sampler and want a conditional distribution, just consider the parameter you’re updating as the variable and everything else as a constant.  No need to integrate or divide, you just take the joint distribution and look at it from the right perspective.
 
Awhile ago Yair told me there’s something called</p><p>3 0.16049966 <a title="858-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-28-Understanding_simulations_in_terms_of_predictive_inference%3F.html">1287 andrew gelman stats-2012-04-28-Understanding simulations in terms of predictive inference?</a></p>
<p>Introduction: David Hogg writes:
  
My (now deceased) collaborator and guru in all things inference, Sam Roweis, used to emphasize to me that we should evaluate models in the data space — not the parameter space — because models are always effectively “effective” and not really, fundamentally true. Or, in other words, models should be compared in the space of their predictions, not in the space of their parameters (the  parameters didn’t really “exist” at all for Sam).  In that spirit, when we estimate the effectiveness of a MCMC method or tuning — by autocorrelation time or ESJD or anything else — shouldn’t we be looking at the changes in the model predictions over time, rather than the changes in the parameters over time?  That is, the autocorrelation time should be the autocorrelation time in what the model (at the walker position) predicts for the data, and the ESJD should be the expected squared jump distance in what the model predicts for the data?  This might resolve the concern I expressed a</p><p>4 0.15932122 <a title="858-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<p>Introduction: Tomas Iesmantas had  asked me  for advice on a regression problem with 50 parameters, and I’d recommended Hamiltonian Monte Carlo.
 
A few weeks later he reported back: 
  
  
After trying several modifications (HMC for all parameters at once, HMC just for first level parameters and Riemman manifold Hamiltonian Monte Carlo method), I finally got it running with HMC just for first level parameters and for others using direct sampling, since conditional distributions turned out to have closed form.


However, even in this case it is quite tricky, since I had to employ mass matrix and not just diagonal but at the beginning of algorithm generated it randomly (ensuring it is positive definite). Such random generation of mass matrix is quite blind step, but it proved to be quite helpful.


Riemman manifold HMC is quite vagarious, or to be more specific, metric of manifold is very sensitive. In my model log-likelihood I had exponents and values of metrics matrix elements was very large and wh</p><p>5 0.15844834 <a title="858-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-28-Path_sampling_for_models_of_varying_dimension.html">1089 andrew gelman stats-2011-12-28-Path sampling for models of varying dimension</a></p>
<p>Introduction: Somebody asks: 
  
  
Iâ&euro;&trade;m reading your paper on path sampling. It essentially solves the 
problem of computing the ratio \int q0(omega)d omega/\int q1(omega) d omega. I.e the arguments in q0() and q1() are the same. But this assumption is not always true in Bayesian model selection using Bayes factor.


In general (for BF), we have this problem, t1 and t2 may have no relation at all.


\int f1(y|t1)p1(t1) d t1 / \int f2(y|t2)p2(t2) d t2


As an example, suppose that we want to compare two sets of normally distributed data with known variance whether they have the same mean (H0) or they are not necessarily have the same mean (H1). Then the dummy variable should be mu in H0 (which is the common mean of both set of samples), and should be (mu1, mu2) (which are the means for each set of samples).


One straight method to address my problem is to preform path integration for the numerate and the denominator, as both the numerate and the denominator are integrals. Each integral can be rewrit</p><p>6 0.1506522 <a title="858-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-Stan_is_fast.html">1476 andrew gelman stats-2012-08-30-Stan is fast</a></p>
<p>7 0.14981514 <a title="858-tfidf-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-23-Unhappy_with_improvement_by_a_factor_of_10%5E29.html">160 andrew gelman stats-2010-07-23-Unhappy with improvement by a factor of 10^29</a></p>
<p>8 0.14940551 <a title="858-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-31-Using_sample_size_in_the_prior_distribution.html">547 andrew gelman stats-2011-01-31-Using sample size in the prior distribution</a></p>
<p>9 0.14633481 <a title="858-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-19-Will_Stan_work_well_with_40%C3%9740_matrices%3F.html">861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</a></p>
<p>10 0.14631455 <a title="858-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-25-Why_it_doesn%E2%80%99t_make_sense_in_general_to_form_confidence_intervals_by_inverting_hypothesis_tests.html">870 andrew gelman stats-2011-08-25-Why it doesn’t make sense in general to form confidence intervals by inverting hypothesis tests</a></p>
<p>11 0.14545055 <a title="858-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>12 0.14046066 <a title="858-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-05-Monitor_the_efficiency_of_your_Markov_chain_sampler_using_expected_squared_jumped_distance%21.html">650 andrew gelman stats-2011-04-05-Monitor the efficiency of your Markov chain sampler using expected squared jumped distance!</a></p>
<p>13 0.13952535 <a title="858-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-24-Why_it_doesn%E2%80%99t_make_sense_in_general_to_form_confidence_intervals_by_inverting_hypothesis_tests.html">1913 andrew gelman stats-2013-06-24-Why it doesn’t make sense in general to form confidence intervals by inverting hypothesis tests</a></p>
<p>14 0.12928502 <a title="858-tfidf-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-14-Confusion_about_continuous_probability_densities.html">341 andrew gelman stats-2010-10-14-Confusion about continuous probability densities</a></p>
<p>15 0.10440574 <a title="858-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-The_statistical_significance_filter.html">899 andrew gelman stats-2011-09-10-The statistical significance filter</a></p>
<p>16 0.10373686 <a title="858-tfidf-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>17 0.10354345 <a title="858-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>18 0.10293671 <a title="858-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-31-No_on_Yes-No_decisions.html">2155 andrew gelman stats-2013-12-31-No on Yes-No decisions</a></p>
<p>19 0.10280804 <a title="858-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-10-Cross-validation_and_Bayesian_estimation_of_tuning_parameters.html">2129 andrew gelman stats-2013-12-10-Cross-validation and Bayesian estimation of tuning parameters</a></p>
<p>20 0.10249317 <a title="858-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-21-Finite-population_Anova_calculations_for_models_with_interactions.html">1686 andrew gelman stats-2013-01-21-Finite-population Anova calculations for models with interactions</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.105), (1, 0.117), (2, 0.036), (3, 0.033), (4, 0.023), (5, -0.018), (6, 0.098), (7, -0.003), (8, -0.069), (9, -0.035), (10, -0.012), (11, -0.022), (12, -0.015), (13, -0.051), (14, -0.072), (15, -0.042), (16, -0.021), (17, -0.005), (18, 0.019), (19, -0.061), (20, 0.074), (21, -0.022), (22, 0.017), (23, -0.024), (24, 0.044), (25, 0.009), (26, -0.056), (27, 0.025), (28, 0.044), (29, 0.028), (30, -0.007), (31, -0.005), (32, -0.026), (33, 0.035), (34, -0.027), (35, -0.037), (36, -0.024), (37, 0.036), (38, -0.032), (39, 0.032), (40, 0.044), (41, 0.034), (42, -0.066), (43, 0.004), (44, -0.04), (45, -0.048), (46, 0.056), (47, 0.065), (48, 0.04), (49, 0.033)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96849245 <a title="858-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-17-Jumping_off_the_edge_of_the_world.html">858 andrew gelman stats-2011-08-17-Jumping off the edge of the world</a></p>
<p>Introduction: Tomas Iesmantas writes:
  
I’m facing a problem where parameter space is bounded, e.g. all parameters have to be positive. If in MCMC as proposal distribution I use normal distribution, then at some iterations I get negative proposals. So my question is: should I use recalculation of acceptance probability every time I reject the proposal (something like in delayed rejection method), or I have to use another proposal (like lognormal, truncated normal, etc.)?
  
The simplest solution is to just calculate p(theta)=0 for theta outside the legal region, thus reject those jumps.  This will work fine (just remember that when you reject, you have to stay at the last value for one more iteration), but if you’re doing these rejections all the time, you might want to reparameterize your space, for example using logs for positive parameters, logits for constrained parameters, and softmax for parameters that are constrained to sum to 1.</p><p>2 0.86100632 <a title="858-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-28-Path_sampling_for_models_of_varying_dimension.html">1089 andrew gelman stats-2011-12-28-Path sampling for models of varying dimension</a></p>
<p>Introduction: Somebody asks: 
  
  
Iâ&euro;&trade;m reading your paper on path sampling. It essentially solves the 
problem of computing the ratio \int q0(omega)d omega/\int q1(omega) d omega. I.e the arguments in q0() and q1() are the same. But this assumption is not always true in Bayesian model selection using Bayes factor.


In general (for BF), we have this problem, t1 and t2 may have no relation at all.


\int f1(y|t1)p1(t1) d t1 / \int f2(y|t2)p2(t2) d t2


As an example, suppose that we want to compare two sets of normally distributed data with known variance whether they have the same mean (H0) or they are not necessarily have the same mean (H1). Then the dummy variable should be mu in H0 (which is the common mean of both set of samples), and should be (mu1, mu2) (which are the means for each set of samples).


One straight method to address my problem is to preform path integration for the numerate and the denominator, as both the numerate and the denominator are integrals. Each integral can be rewrit</p><p>3 0.81267381 <a title="858-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-23-Unhappy_with_improvement_by_a_factor_of_10%5E29.html">160 andrew gelman stats-2010-07-23-Unhappy with improvement by a factor of 10^29</a></p>
<p>Introduction: I have an optimization problem:  I have a complicated physical model that predicts energy and thermal behavior of a building, given the values of a slew of parameters, such as insulation effectiveness, window transmissivity, etc.  I’m trying to find the parameter set that best fits several weeks of thermal and energy use data from the real building that we modeled. (Of course I would rather explore parameter space and come up with probability distributions for the parameters, and maybe that will come later, but for now I’m just optimizing).  To do the optimization, colleagues and I implemented a “particle swarm optimization” algorithm on a massively parallel machine. This involves giving each of about 120 “particles” an initial position in parameter space, then letting them move around, trying to move to better positions according to a specific algorithm.  We gave each particle an initial position sampled from our prior distribution for each parameter.  So far we’ve run about 140 itera</p><p>4 0.72729683 <a title="858-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-09-How_to_model_distributions_that_have_outliers_in_one_direction.html">2128 andrew gelman stats-2013-12-09-How to model distributions that have outliers in one direction</a></p>
<p>Introduction: Shravan writes:
  
I have a problem very similar to the one presented chapter 6 of BDA, the speed of light example. You use the distribution of the minimum scores from the posterior predictive distribution, show that it’s not realistic given the data, and suggest that an asymmetric contaminated normal distribution or a symmetric long-tailed distribution would be better.


How does one use such a distribution?
  
My reply:
 
You can actually use a symmetric long-tailed distribution such as t with low degrees of freedom.  One striking feature of symmetric long-tailed distributions is that a small random sample from such a distribution can have outliers on one side or the other and look asymmetric.
 
Just to see this, try the following in R: 
  
par (mfrow=c(3,3), mar=c(1,1,1,1)) 
for (i in 1:9) hist (rt (100, 2), xlab="", ylab="", main="") 
 
 
You’ll see some skewed distributions.  So that’s the message (which I learned from an offhand comment of Rubin, actually):  if you want to model</p><p>5 0.71178323 <a title="858-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-16-The_%E2%80%9CWashington_read%E2%80%9D_and_the_algebra_of_conditional_distributions.html">961 andrew gelman stats-2011-10-16-The “Washington read” and the algebra of conditional distributions</a></p>
<p>Introduction: I was trying to explain in class how a (Bayesian) statistician reads the formula for a probability distribution.  In old-fashioned statistics textbooks you’re told that if you want to compute a conditional distribution from a joint distribution you need to do some heavy math:  p(a|b) = p(a,b)/\int p(a’,b)da’.
 
When doing Bayesian statistics, though, you usually don’t have to do the integration or the division. If you have parameters theta and data y, you first write p(y,theta).  Then to get p(theta|y), you  don’t  need to integrate or divide.  All you have to do is look at p(y,theta) in a certain way:  Treat y as a constant and theta as a variable.  Similarly, if you’re doing the Gibbs sampler and want a conditional distribution, just consider the parameter you’re updating as the variable and everything else as a constant.  No need to integrate or divide, you just take the joint distribution and look at it from the right perspective.
 
Awhile ago Yair told me there’s something called</p><p>6 0.6992563 <a title="858-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-31-Using_sample_size_in_the_prior_distribution.html">547 andrew gelman stats-2011-01-31-Using sample size in the prior distribution</a></p>
<p>7 0.66460532 <a title="858-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-16-Update_on_the_generalized_method_of_moments.html">519 andrew gelman stats-2011-01-16-Update on the generalized method of moments</a></p>
<p>8 0.66058236 <a title="858-lsi-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-06-Bootstrap_averaging%3A_Examples_where_it_works_and_where_it_doesn%E2%80%99t_work.html">2201 andrew gelman stats-2014-02-06-Bootstrap averaging: Examples where it works and where it doesn’t work</a></p>
<p>9 0.65577406 <a title="858-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-19-Whassup_with_deviance_having_a_high_posterior_correlation_with_a_parameter_in_the_model%3F.html">1221 andrew gelman stats-2012-03-19-Whassup with deviance having a high posterior correlation with a parameter in the model?</a></p>
<p>10 0.65047783 <a title="858-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-05-Monitor_the_efficiency_of_your_Markov_chain_sampler_using_expected_squared_jumped_distance%21.html">650 andrew gelman stats-2011-04-05-Monitor the efficiency of your Markov chain sampler using expected squared jumped distance!</a></p>
<p>11 0.64765912 <a title="858-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-07-Chi-square_FAIL_when_many_cells_have_small_expected_values.html">996 andrew gelman stats-2011-11-07-Chi-square FAIL when many cells have small expected values</a></p>
<p>12 0.64554107 <a title="858-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>13 0.63935828 <a title="858-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>14 0.63770574 <a title="858-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-20-Prior_beliefs_about_locations_of_decision_boundaries.html">1130 andrew gelman stats-2012-01-20-Prior beliefs about locations of decision boundaries</a></p>
<p>15 0.63402641 <a title="858-lsi-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-20-Thermodynamic_Monte_Carlo%3A__Michael_Betancourt%E2%80%99s_new_method_for_simulating_from_difficult_distributions_and_evaluating_normalizing_constants.html">2340 andrew gelman stats-2014-05-20-Thermodynamic Monte Carlo:  Michael Betancourt’s new method for simulating from difficult distributions and evaluating normalizing constants</a></p>
<p>16 0.62983406 <a title="858-lsi-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-01-bayesglm_in_Stata%3F.html">442 andrew gelman stats-2010-12-01-bayesglm in Stata?</a></p>
<p>17 0.6211592 <a title="858-lsi-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-14-Confusion_about_continuous_probability_densities.html">341 andrew gelman stats-2010-10-14-Confusion about continuous probability densities</a></p>
<p>18 0.61405766 <a title="858-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<p>19 0.60525399 <a title="858-lsi-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-28-Another_argument_in_favor_of_expressing_conditional_probability_statements_using_the_population_distribution.html">56 andrew gelman stats-2010-05-28-Another argument in favor of expressing conditional probability statements using the population distribution</a></p>
<p>20 0.59897965 <a title="858-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(8, 0.022), (15, 0.096), (24, 0.22), (52, 0.017), (81, 0.28), (99, 0.239)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.91846228 <a title="858-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-17-%28Worst%29_graph_of_the_year.html">915 andrew gelman stats-2011-09-17-(Worst) graph of the year</a></p>
<p>Introduction: This  (forwarded to me from Jeff, from a powerpoint by Willam Gawthrop) wins not on form but on content:
 
   
 
Really this graph should stand alone but it’s so wonderful that I can’t resist pointing out a few things: 
   
- The gap between 610 and 622 A.D. seems to be about the same as the previous 600 years, and only a little less than the 1400 years before that.
 
- “Pious and devout” Jews are portrayed as having steadily increased in nonviolence up to the present day.  Been to Israel lately?
 
- I assume the line labeled “Bible” is referring to Christians?  I’m sort of amazed to see pious and devout Christians listed as being maximally violent at the beginning.  Huh?  I thought Christ was supposed to be a nonviolent, mellow dude.  The line starts at 3 B.C., implying that  baby Jesus  was at the extreme of violence.
 
Gong forward, we can learn from the graph that pious and devout Christians in 1492 or 1618, say, were much more peaceful than Jesus and his crew.
 
- Most amusingly g</p><p>2 0.91500932 <a title="858-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-03-Model_Makers%E2%80%99_Hippocratic_Oath.html">552 andrew gelman stats-2011-02-03-Model Makers’ Hippocratic Oath</a></p>
<p>Introduction: Emanuel Derman  and  Paul Wilmott  wonder how to get their fellow modelers to give up their fantasy of perfection. In a  Business Week article  they proposed, not entirely in jest, a model makers’ Hippocratic Oath:
  
 I will remember that I didn’t make the world and that it doesn’t satisfy my equations. 
 Though I will use models boldly to estimate value, I will not be overly impressed by mathematics. 
 I will never sacrifice reality for elegance without explaining why I have done so. Nor will I give the people who use my model false comfort about its accuracy. Instead, I will make explicit its assumptions and oversights. 
 I understand that my work may have enormous effects on society and the economy, many of them beyond my comprehension.  
  
Found via  Abductive Intelligence .</p><p>3 0.90619767 <a title="858-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-13-%E2%80%9CI_have_no_idea_who_Catalina_Garcia_is%2C_but_she_makes_a_decent_ruler%E2%80%9D%3A__I_don%E2%80%99t_know_if_John_Lee_%E2%80%9Clittle_twerp%E2%80%9D_Anderson_actually_suffers_from_tall-person_syndrome%2C_but_he_is_indeed_tall.html">1762 andrew gelman stats-2013-03-13-“I have no idea who Catalina Garcia is, but she makes a decent ruler”:  I don’t know if John Lee “little twerp” Anderson actually suffers from tall-person syndrome, but he is indeed tall</a></p>
<p>Introduction: I just want to share with you the best comment we’ve every had in the nearly ten-year history of this blog.  Also it has statistical content!
 
Here’s the story.  After seeing an amusing  article  by Tom Scocca relating how reporter John Lee Anderson called someone as a “little twerp” on twitter:
 
 
 
I  conjectured  that Anderson suffered from “tall person syndrome,” that problem that some people of above-average height have, that they think they’re more important than other people because they literally look down on them.
 
But I had no idea of Anderson’s actual height.  Commenter Gary  responded  with this impressive bit of investigative reporting:
  
Based on  this  picture: 
  
he appears to be fairly tall. But the perspective makes it hard to judge.


Based on  this  picture: 
  
he appears to be about 9-10 inches taller than Catalina Garcia.


But how tall is Catalina Garcia? Not that tall – she’s  shorter  than the high-wire artist Phillipe Petit: 
 


And he doesn’t  appear</p><p>same-blog 4 0.87180454 <a title="858-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-17-Jumping_off_the_edge_of_the_world.html">858 andrew gelman stats-2011-08-17-Jumping off the edge of the world</a></p>
<p>Introduction: Tomas Iesmantas writes:
  
I’m facing a problem where parameter space is bounded, e.g. all parameters have to be positive. If in MCMC as proposal distribution I use normal distribution, then at some iterations I get negative proposals. So my question is: should I use recalculation of acceptance probability every time I reject the proposal (something like in delayed rejection method), or I have to use another proposal (like lognormal, truncated normal, etc.)?
  
The simplest solution is to just calculate p(theta)=0 for theta outside the legal region, thus reject those jumps.  This will work fine (just remember that when you reject, you have to stay at the last value for one more iteration), but if you’re doing these rejections all the time, you might want to reparameterize your space, for example using logs for positive parameters, logits for constrained parameters, and softmax for parameters that are constrained to sum to 1.</p><p>5 0.86572933 <a title="858-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-20-Bugs_Bunny%2C_the_governor_of_Massachusetts%2C_the_Dow_36%2C000_guy%2C_presidential_qualifications%2C_and_Peggy_Noonan.html">1129 andrew gelman stats-2012-01-20-Bugs Bunny, the governor of Massachusetts, the Dow 36,000 guy, presidential qualifications, and Peggy Noonan</a></p>
<p>Introduction: Elsewhere:
  
1. They asked me to  write about  my “favorite election- or campaign-related movie, novel, or TV show” (Salon)


2. The shopping period is over; the  time for buying  has begun (NYT)


3. If anybody’s gonna be criticizing my tax plan, I want it to be  this guy  (Monkey Cage)


4. The 4 key qualifications to be a great president;  unfortunately  George W. Bush satisfies all four, and Ronald Reagan doesn’t match any of them (Monkey Cage)


5. The politics of  eyeliner  (Monkey Cage)</p><p>6 0.84675467 <a title="858-lda-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-24-Foreign_language_skills_as_an_intrinsic_good%3B_also%2C_beware_the_tyranny_of_measurement.html">484 andrew gelman stats-2010-12-24-Foreign language skills as an intrinsic good; also, beware the tyranny of measurement</a></p>
<p>7 0.84251511 <a title="858-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-15-A_statistical_research_project%3A__Weeding_out_the_fraudulent_citations.html">1321 andrew gelman stats-2012-05-15-A statistical research project:  Weeding out the fraudulent citations</a></p>
<p>8 0.83664101 <a title="858-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-20-Who_exactly_are_those_silly_academics_who_aren%E2%80%99t_as_smart_as_a_Vegas_bookie%3F.html">1632 andrew gelman stats-2012-12-20-Who exactly are those silly academics who aren’t as smart as a Vegas bookie?</a></p>
<p>9 0.83257174 <a title="858-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-11-The_Reliability_of_Cluster_Surveys_of_Conflict_Mortality%3A_Violent_Deaths_and_Non-Violent_Deaths.html">849 andrew gelman stats-2011-08-11-The Reliability of Cluster Surveys of Conflict Mortality: Violent Deaths and Non-Violent Deaths</a></p>
<p>10 0.81355864 <a title="858-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-04-Patterns.html">556 andrew gelman stats-2011-02-04-Patterns</a></p>
<p>11 0.81183636 <a title="858-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-28-Greece_to_head_statistician%3A__Tell_the_truth%2C_go_to_jail.html">1033 andrew gelman stats-2011-11-28-Greece to head statistician:  Tell the truth, go to jail</a></p>
<p>12 0.80632257 <a title="858-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-10-Schiminovich_is_on_The_Simpsons.html">2096 andrew gelman stats-2013-11-10-Schiminovich is on The Simpsons</a></p>
<p>13 0.79754812 <a title="858-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-30-The_Roy_causal_model%3F.html">1962 andrew gelman stats-2013-07-30-The Roy causal model?</a></p>
<p>14 0.79562783 <a title="858-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-11-Statistics_in_high_schools%3A__Towards_more_accessible_conceptions_of_statistical_inference.html">658 andrew gelman stats-2011-04-11-Statistics in high schools:  Towards more accessible conceptions of statistical inference</a></p>
<p>15 0.79370534 <a title="858-lda-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-20-5_books_book.html">1222 andrew gelman stats-2012-03-20-5 books book</a></p>
<p>16 0.79226005 <a title="858-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-04-Recently_in_the_sister_blog.html">1705 andrew gelman stats-2013-02-04-Recently in the sister blog</a></p>
<p>17 0.78875768 <a title="858-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>18 0.78068715 <a title="858-lda-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-16-%E2%80%9CI_have_no_idea_who_Catalina_Garcia_is%2C_but_she_makes_a_decent_ruler%E2%80%9D.html">2250 andrew gelman stats-2014-03-16-“I have no idea who Catalina Garcia is, but she makes a decent ruler”</a></p>
<p>19 0.77417123 <a title="858-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-14-Hey%E2%80%94I_didn%E2%80%99t_know_that%21.html">1057 andrew gelman stats-2011-12-14-Hey—I didn’t know that!</a></p>
<p>20 0.76074207 <a title="858-lda-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-27-%E2%80%9CDisappointed_with_your_results%3F__Boost_your_scientific_paper%E2%80%9D.html">2188 andrew gelman stats-2014-01-27-“Disappointed with your results?  Boost your scientific paper”</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
