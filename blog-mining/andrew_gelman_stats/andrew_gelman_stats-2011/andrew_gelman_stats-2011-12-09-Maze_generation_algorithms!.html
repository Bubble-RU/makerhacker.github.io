<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1048 andrew gelman stats-2011-12-09-Maze generation algorithms!</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-1048" href="#">andrew_gelman_stats-2011-1048</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1048 andrew gelman stats-2011-12-09-Maze generation algorithms!</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-1048-html" href="http://andrewgelman.com/2011/12/09/maze-generation-algorithms/">html</a></p><p>Introduction: Super cool post  from Jamis Buck on mazemaking algorithms.  It’s set up so you can click and see the maze being formed, for each of 11 different algorithms!
 
When I was about 12, I was really into making mazes.  I’d make them on graph paper and give many of them out to my friends.  Somewhere along the way I lost most of them, but I remember it was a fun challenge to figure out how to make them difficult.  I don’t know about these automatic maze generation algorithms, but handmade mazes (of the sort that used to appear in puzzle books) often had the problem that they were really easy to solve if you started at the end and worked back to the beginning.  I didn’t want that, so when I designed my own mazes, I’d start from each of the two ends and then work out the middle.  I remember one maze I was particularly proud of that had no dead ends.  The dud paths just looped back to the beginning area, and you had to find the path that made it all the way through.
 
I never tried to formalize t</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Super cool post  from Jamis Buck on mazemaking algorithms. [sent-1, score-0.074]
</p><p>2 It’s set up so you can click and see the maze being formed, for each of 11 different algorithms! [sent-2, score-0.547]
</p><p>3 When I was about 12, I was really into making mazes. [sent-3, score-0.099]
</p><p>4 I’d make them on graph paper and give many of them out to my friends. [sent-4, score-0.13]
</p><p>5 Somewhere along the way I lost most of them, but I remember it was a fun challenge to figure out how to make them difficult. [sent-5, score-0.657]
</p><p>6 I don’t know about these automatic maze generation algorithms, but handmade mazes (of the sort that used to appear in puzzle books) often had the problem that they were really easy to solve if you started at the end and worked back to the beginning. [sent-6, score-1.86]
</p><p>7 I didn’t want that, so when I designed my own mazes, I’d start from each of the two ends and then work out the middle. [sent-7, score-0.232]
</p><p>8 I remember one maze I was particularly proud of that had no dead ends. [sent-8, score-0.925]
</p><p>9 The dud paths just looped back to the beginning area, and you had to find the path that made it all the way through. [sent-9, score-0.412]
</p><p>10 I never tried to formalize the algorithm I used to make mazes, and now I don’t remember how I used to do it. [sent-10, score-0.764]
</p><p>11 Jamis Buck also has a  family blog  full of adorable quotes from his kids. [sent-13, score-0.219]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('maze', 0.465), ('mazes', 0.465), ('jamis', 0.34), ('buck', 0.255), ('remember', 0.191), ('algorithms', 0.185), ('formalize', 0.125), ('super', 0.125), ('proud', 0.118), ('formed', 0.118), ('used', 0.112), ('automatic', 0.106), ('paths', 0.105), ('puzzle', 0.102), ('generation', 0.098), ('dead', 0.092), ('designed', 0.091), ('path', 0.091), ('ends', 0.09), ('quotes', 0.086), ('beginning', 0.085), ('lost', 0.082), ('click', 0.082), ('back', 0.082), ('algorithm', 0.082), ('challenge', 0.081), ('kids', 0.079), ('somewhere', 0.079), ('family', 0.077), ('make', 0.076), ('solve', 0.076), ('cool', 0.074), ('area', 0.067), ('fun', 0.067), ('started', 0.067), ('tried', 0.066), ('appear', 0.065), ('worked', 0.063), ('books', 0.062), ('figure', 0.06), ('particularly', 0.059), ('full', 0.056), ('easy', 0.054), ('graph', 0.054), ('end', 0.053), ('really', 0.052), ('start', 0.051), ('along', 0.051), ('way', 0.049), ('making', 0.047)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1048-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-09-Maze_generation_algorithms%21.html">1048 andrew gelman stats-2011-12-09-Maze generation algorithms!</a></p>
<p>Introduction: Super cool post  from Jamis Buck on mazemaking algorithms.  It’s set up so you can click and see the maze being formed, for each of 11 different algorithms!
 
When I was about 12, I was really into making mazes.  I’d make them on graph paper and give many of them out to my friends.  Somewhere along the way I lost most of them, but I remember it was a fun challenge to figure out how to make them difficult.  I don’t know about these automatic maze generation algorithms, but handmade mazes (of the sort that used to appear in puzzle books) often had the problem that they were really easy to solve if you started at the end and worked back to the beginning.  I didn’t want that, so when I designed my own mazes, I’d start from each of the two ends and then work out the middle.  I remember one maze I was particularly proud of that had no dead ends.  The dud paths just looped back to the beginning area, and you had to find the path that made it all the way through.
 
I never tried to formalize t</p><p>2 0.37749338 <a title="1048-tfidf-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-09-Solve_mazes_by_starting_at_the_exit.html">459 andrew gelman stats-2010-12-09-Solve mazes by starting at the exit</a></p>
<p>Introduction: It worked on  this one .
 
Good maze designers know this trick and are careful to design multiple branches in each direction. Back when I was in junior high, I used to make huge mazes, and the basic idea was to anticipate what the solver might try to do and to make the maze difficult by postponing the point at which he would realize a path was going nowhere.  For example, you might have 6 branches:  one dead end, two pairs that form loops going back to the start, and one that is the correct solution.  You do this from both directions and add some twists and turns, and there you are.
 
But the maze designer aiming for the naive solver–the sap who starts from the entrance and goes toward the exit–can simplify matters by just having 6 branches:  five dead ends and one winner.  This sort of thing is easy to solve in the reverse direction.  I’m surprised the Times didn’t do better for their special puzzle issue.</p><p>3 0.075690143 <a title="1048-tfidf-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-18-Derivative-based_MCMC_as_a_breakthrough_technique_for_implementing_Bayesian_statistics.html">419 andrew gelman stats-2010-11-18-Derivative-based MCMC as a breakthrough technique for implementing Bayesian statistics</a></p>
<p>Introduction: John Salvatier pointed me to  this blog  on derivative based MCMC algorithms (also sometimes called “hybrid” or “Hamiltonian” Monte Carlo) and automatic differentiation as the future of MCMC.
 
This all makes sense to me and is consistent both with my mathematical intuition from studying Metropolis algorithms and my experience with Matt using hybrid MCMC when fitting hierarchical spline models. In particular, I agree with Salvatier’s point about the potential for computation of analytic derivatives of the log-density function.  As long as we’re mostly snapping together our models using analytically-simple pieces, the same part of the program that handles the computation of log-posterior densities should also be able to compute derivatives analytically.
 
I’ve been a big fan of automatic derivative-based MCMC methods since I started hearing about them a couple years ago (I’m thinking of the DREAM project and of Mark Girolami’s paper), and I too wonder why they haven’t been used more.  I</p><p>4 0.068355441 <a title="1048-tfidf-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-28-When_Small_Numbers_Lead_to_Big_Errors.html">434 andrew gelman stats-2010-11-28-When Small Numbers Lead to Big Errors</a></p>
<p>Introduction: My  column  in  Scientific American .
 
Check out the comments.  I have to remember never ever to write about guns.</p><p>5 0.065888576 <a title="1048-tfidf-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-02-Fragment_of_statistical_autobiography.html">390 andrew gelman stats-2010-11-02-Fragment of statistical autobiography</a></p>
<p>Introduction: I studied math and physics at MIT. To be more precise, I started in math as default–ever since I was two years old, I’ve thought of myself as a mathematician, and I always did well in math class, so it seemed like a natural fit.
 
But I was concerned. In high school I’d been in the U.S. Mathematical Olympiad training program, and there I’d met kids who were clearly much much better at math than I was. In retrospect, I don’t think I was as bad as I’d thought at the time: there were 24 kids in the program, and I was probably around #20, if that, but I think a lot of the other kids had more practice working on “math olympiad”-type problems. Maybe I was really something like the tenth-best in the group.
 
Tenth-best or twentieth-best, whatever it was, I reached a crisis of confidence around my sophomore or junior year in college. At MIT, I started right off taking advanced math classes, and somewhere along the way I realized I wasn’t seeing the big picture. I was able to do the homework pr</p><p>6 0.065256171 <a title="1048-tfidf-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-01-%E2%80%9CThough_They_May_Be_Unaware%2C_Newlyweds_Implicitly_Know_Whether_Their_Marriage_Will_Be_Satisfying%E2%80%9D.html">2156 andrew gelman stats-2014-01-01-“Though They May Be Unaware, Newlyweds Implicitly Know Whether Their Marriage Will Be Satisfying”</a></p>
<p>7 0.058740363 <a title="1048-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-12-Val%E2%80%99s_Number_Scroll%3A_Helping_kids_visualize_math.html">1006 andrew gelman stats-2011-11-12-Val’s Number Scroll: Helping kids visualize math</a></p>
<p>8 0.05718071 <a title="1048-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-23-Advice_on_writing_research_articles.html">1338 andrew gelman stats-2012-05-23-Advice on writing research articles</a></p>
<p>9 0.057070266 <a title="1048-tfidf-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-31-A_data_visualization_manifesto.html">61 andrew gelman stats-2010-05-31-A data visualization manifesto</a></p>
<p>10 0.056569424 <a title="1048-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-09-False_memories_and_statistical_analysis.html">2014 andrew gelman stats-2013-09-09-False memories and statistical analysis</a></p>
<p>11 0.055833876 <a title="1048-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-29-Why_%E2%80%9CWhy%E2%80%9D%3F.html">1190 andrew gelman stats-2012-02-29-Why “Why”?</a></p>
<p>12 0.055793341 <a title="1048-tfidf-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-19-Just_chaid.html">421 andrew gelman stats-2010-11-19-Just chaid</a></p>
<p>13 0.054173172 <a title="1048-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-15-Mr._P_by_another_name_._._._is_still_great%21.html">769 andrew gelman stats-2011-06-15-Mr. P by another name . . . is still great!</a></p>
<p>14 0.051832236 <a title="1048-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-28-Funniest_comment_ever.html">734 andrew gelman stats-2011-05-28-Funniest comment ever</a></p>
<p>15 0.051487833 <a title="1048-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-19-Online_James%3F.html">620 andrew gelman stats-2011-03-19-Online James?</a></p>
<p>16 0.050650194 <a title="1048-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-30-Bill_Gates%E2%80%99s_favorite_graph_of_the_year.html">2154 andrew gelman stats-2013-12-30-Bill Gates’s favorite graph of the year</a></p>
<p>17 0.050418992 <a title="1048-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-24-Bell_Labs.html">970 andrew gelman stats-2011-10-24-Bell Labs</a></p>
<p>18 0.050030578 <a title="1048-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-25-The_problem_with_realistic_advice%3F.html">1428 andrew gelman stats-2012-07-25-The problem with realistic advice?</a></p>
<p>19 0.049807657 <a title="1048-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-03-5_books.html">499 andrew gelman stats-2011-01-03-5 books</a></p>
<p>20 0.049534183 <a title="1048-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-11-My_talk_at_the_NY_data_visualization_meetup_this_Monday%21.html">1668 andrew gelman stats-2013-01-11-My talk at the NY data visualization meetup this Monday!</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.1), (1, -0.027), (2, -0.026), (3, 0.036), (4, 0.05), (5, -0.031), (6, 0.013), (7, -0.008), (8, 0.017), (9, -0.008), (10, 0.002), (11, 0.005), (12, -0.001), (13, -0.015), (14, 0.007), (15, -0.005), (16, -0.006), (17, -0.007), (18, 0.013), (19, 0.012), (20, 0.007), (21, -0.027), (22, -0.007), (23, 0.019), (24, 0.013), (25, -0.002), (26, -0.022), (27, 0.007), (28, 0.017), (29, 0.004), (30, 0.007), (31, 0.002), (32, 0.006), (33, -0.04), (34, 0.001), (35, -0.038), (36, 0.007), (37, -0.007), (38, 0.029), (39, 0.005), (40, -0.021), (41, 0.04), (42, 0.018), (43, 0.026), (44, 0.002), (45, -0.024), (46, -0.018), (47, 0.0), (48, 0.002), (49, 0.014)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95448208 <a title="1048-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-09-Maze_generation_algorithms%21.html">1048 andrew gelman stats-2011-12-09-Maze generation algorithms!</a></p>
<p>Introduction: Super cool post  from Jamis Buck on mazemaking algorithms.  It’s set up so you can click and see the maze being formed, for each of 11 different algorithms!
 
When I was about 12, I was really into making mazes.  I’d make them on graph paper and give many of them out to my friends.  Somewhere along the way I lost most of them, but I remember it was a fun challenge to figure out how to make them difficult.  I don’t know about these automatic maze generation algorithms, but handmade mazes (of the sort that used to appear in puzzle books) often had the problem that they were really easy to solve if you started at the end and worked back to the beginning.  I didn’t want that, so when I designed my own mazes, I’d start from each of the two ends and then work out the middle.  I remember one maze I was particularly proud of that had no dead ends.  The dud paths just looped back to the beginning area, and you had to find the path that made it all the way through.
 
I never tried to formalize t</p><p>2 0.74867237 <a title="1048-lsi-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-09-Solve_mazes_by_starting_at_the_exit.html">459 andrew gelman stats-2010-12-09-Solve mazes by starting at the exit</a></p>
<p>Introduction: It worked on  this one .
 
Good maze designers know this trick and are careful to design multiple branches in each direction. Back when I was in junior high, I used to make huge mazes, and the basic idea was to anticipate what the solver might try to do and to make the maze difficult by postponing the point at which he would realize a path was going nowhere.  For example, you might have 6 branches:  one dead end, two pairs that form loops going back to the start, and one that is the correct solution.  You do this from both directions and add some twists and turns, and there you are.
 
But the maze designer aiming for the naive solver–the sap who starts from the entrance and goes toward the exit–can simplify matters by just having 6 branches:  five dead ends and one winner.  This sort of thing is easy to solve in the reverse direction.  I’m surprised the Times didn’t do better for their special puzzle issue.</p><p>3 0.73596728 <a title="1048-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-15-How_do_I_make_my_graphs%3F.html">1764 andrew gelman stats-2013-03-15-How do I make my graphs?</a></p>
<p>Introduction: Someone who wishes to remain anonymous writes: 
  
  
I’ve been following your blog a long time and enjoy your posts on visualization/statistical graphics matters.  I don’t recall however you ever describing the details of your setup for plotting.  I’m a new R user (convert from matplotlib) and would love to know your thoughts on the ideal setup: do you use mainly the R base?  Do you use lattice?  What do you think of ggplot2?  etc.  


I found ggplot2 nearly indecipherable until a recent eureka moment, and I think its default theme is a waste tremendous ink (all those silly grey backgrounds and grids are really unnecessary), but if you customize that away it can be made to look like ordinary, pretty statistical graphs.  


Feel free to respond on your blog, but if you do, please remove my name from the post (my colleagues already make fun of me for thinking about visualization too much.) 
  
I love that last bit!
 
Anyway, my response is that I do everything in base graphics (using my</p><p>4 0.73469293 <a title="1048-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-24-Bell_Labs.html">970 andrew gelman stats-2011-10-24-Bell Labs</a></p>
<p>Introduction: Sining Chen told me they’re hiring in the  statistics group at Bell Labs .  I’ll do my bit for economic stimulus by announcing this job (see below).
 
I love Bell Labs.  I worked there for three summers, in a physics lab in 1985-86 under the supervision of Loren Pfeiffer, and by myself in the statistics group in 1990.
 
I learned a lot working for Loren.  He was a really smart and driven guy.  His lab was a small set of rooms—in Bell Labs, everything’s in a small room, as they value the positive externality of close physical proximity of different labs, which you get by making each lab compact—and it was Loren, his assistant (a guy named Ken West who kept everything running in the lab), and three summer students: me, Gowton Achaibar, and a girl whose name I’ve forgotten.  Gowtan and I had a lot of fun chatting in the lab.  One day I made a silly comment about Gowton’s accent—he was from Guyana and pronounced “three” as “tree”—and then I apologized and said:  Hey, here I am making fun o</p><p>5 0.73302627 <a title="1048-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-29-Why_%E2%80%9CWhy%E2%80%9D%3F.html">1190 andrew gelman stats-2012-02-29-Why “Why”?</a></p>
<p>Introduction: In old books (and occasionally new books), you see the word “Why” used to indicate a pause or emphasis in dialogue.
 
For example, from 1952:
  
“Why, how perfectly simple!” she said to herself.  “The way to save Wilbur’s life is to play a trick on Zuckerman.  “If I can fool a bug,” thought Charlotte, “I can surely fool a man.  People are not as smart as bugs.”
  
That line about people and bugs was cute, but what really jumped out at me was the “Why.”  I don’t think I’ve ever ever heard anyone use “Why” in that way in conversation, but I see it all the time in books, and every time it’s jarring.
 
What’s the deal?  Is it that people used to talk that way?  Or is a Wasp thing, some regional speech pattern that was captured in books because it was considered standard conversational speech?  I suppose one way to learn more would be to watch a bunch of old movies.  I could sort of imagine Jimmy Stewart beginning his sentences with “Why” all the time.
 
Does anyone know more?
 
P.S.  I use</p><p>6 0.7182259 <a title="1048-lsi-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-18-Derivative-based_MCMC_as_a_breakthrough_technique_for_implementing_Bayesian_statistics.html">419 andrew gelman stats-2010-11-18-Derivative-based MCMC as a breakthrough technique for implementing Bayesian statistics</a></p>
<p>7 0.71767205 <a title="1048-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-03-More_research_on_the_role_of_puzzles_in_processing_data_graphics.html">1747 andrew gelman stats-2013-03-03-More research on the role of puzzles in processing data graphics</a></p>
<p>8 0.71281409 <a title="1048-lsi-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-31-A_data_visualization_manifesto.html">61 andrew gelman stats-2010-05-31-A data visualization manifesto</a></p>
<p>9 0.70290369 <a title="1048-lsi-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-09-The_future_of_R.html">266 andrew gelman stats-2010-09-09-The future of R</a></p>
<p>10 0.70106387 <a title="1048-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-23-Life_in_the_C-suite%3A__A_graph_that_is_both_ugly_and_bad%2C_and_an_unrelated_story.html">1734 andrew gelman stats-2013-02-23-Life in the C-suite:  A graph that is both ugly and bad, and an unrelated story</a></p>
<p>11 0.70079219 <a title="1048-lsi-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-05-Can_we_make_better_graphs_of_global_temperature_history%3F.html">2319 andrew gelman stats-2014-05-05-Can we make better graphs of global temperature history?</a></p>
<p>12 0.69986928 <a title="1048-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-19-It_is_difficult_to_convey_intonation_in_typed_speech.html">1463 andrew gelman stats-2012-08-19-It is difficult to convey intonation in typed speech</a></p>
<p>13 0.6974709 <a title="1048-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-31-Skepticism_about_skepticism_of_global_warming_skepticism_skepticism.html">983 andrew gelman stats-2011-10-31-Skepticism about skepticism of global warming skepticism skepticism</a></p>
<p>14 0.69665098 <a title="1048-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-02-At_least_he_didn%E2%80%99t_prove_a_false_theorem.html">741 andrew gelman stats-2011-06-02-At least he didn’t prove a false theorem</a></p>
<p>15 0.69621468 <a title="1048-lsi-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-07-Small_world%3A__MIT%2C_asymptotic_behavior_of_differential-difference_equations%2C_Susan_Assmann%2C_subgroup_analysis%2C_multilevel_modeling.html">507 andrew gelman stats-2011-01-07-Small world:  MIT, asymptotic behavior of differential-difference equations, Susan Assmann, subgroup analysis, multilevel modeling</a></p>
<p>16 0.69567525 <a title="1048-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-14-Learning_how_to_speak.html">1938 andrew gelman stats-2013-07-14-Learning how to speak</a></p>
<p>17 0.69175363 <a title="1048-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-23-My_new_writing_strategy.html">727 andrew gelman stats-2011-05-23-My new writing strategy</a></p>
<p>18 0.68652469 <a title="1048-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-08-Software_is_as_software_does.html">1661 andrew gelman stats-2013-01-08-Software is as software does</a></p>
<p>19 0.68279034 <a title="1048-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-04-%E2%80%9CTurn_a_Boring_Bar_Graph_into_a_3D_Masterpiece%E2%80%9D.html">1154 andrew gelman stats-2012-02-04-“Turn a Boring Bar Graph into a 3D Masterpiece”</a></p>
<p>20 0.68276364 <a title="1048-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-29-The_Great_Race.html">1831 andrew gelman stats-2013-04-29-The Great Race</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(15, 0.013), (21, 0.014), (22, 0.018), (24, 0.101), (61, 0.016), (79, 0.306), (86, 0.011), (89, 0.025), (99, 0.367)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96449399 <a title="1048-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-03-DBQQ_rounding_for_labeling_charts_and_communicating_tolerances.html">939 andrew gelman stats-2011-10-03-DBQQ rounding for labeling charts and communicating tolerances</a></p>
<p>Introduction: This is a mini research note, not deserving of a paper, but perhaps useful to others. It reinvents what has already  appeared  on this blog.
 
Let’s say we have a line chart with numbers between 152.134 and 210.823, with the mean of 183.463. How should we label the chart with about 3 tics? Perhaps 152.132, 181.4785 and 210.823? Don’t do it!
 
Objective is to fit about 3-7 tics at the optimal level of rounding. I use the following sequence:
  
  decimal rounding : fitting integer  power  and single-digit decimal  i , rounding to  i  * 10^ power  (example: 100 200 300) 
  binary  having  power , fitting single-digit decimal  i  and binary  b , rounding to 2* i /(1+ b ) * 10^ power  (150 200 250) 
 (optional)  quaternary  having  power , fitting single-digit decimal  i  and  quaternary  q  (0,1,2,3) round to 4* i /(1+ q ) * 10^ power  (150 175 200) 
  quinary  having  power , fitting single-digit decimal  i  and  quinary  f  (0,1,2,3,4) round to 5* i /(1+ f ) * 10^ power  (160 180 200)</p><p>2 0.95789444 <a title="1048-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-17-Rust.html">1538 andrew gelman stats-2012-10-17-Rust</a></p>
<p>Introduction: I happened to be referring to the  path sampling paper  today and took a look at Appendix A.2: 
   
    
    
    
I’m sure I could reconstruct all of this if I had to, but I certainly can’t read this sort of thing cold anymore.</p><p>3 0.93510246 <a title="1048-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-29-Jost_Haidt.html">1515 andrew gelman stats-2012-09-29-Jost Haidt</a></p>
<p>Introduction: Research psychologist John Jost  reviews  the recent book, “The Righteous Mind,” by research psychologist Jonathan Haidt.  Some of my thoughts on Haidt’s book are  here .  And here’s some of Jost’s review:
  
Haidt’s book is creative, interesting, and provocative. . . . The book shines a new light on moral psychology and presents a bold, confrontational message. From a scientific perspective, however, I worry that his theory raises more questions than it answers. Why do some individuals feel that it is morally good (or necessary) to obey authority, favor the ingroup, and maintain purity, whereas others are skeptical? (Perhaps parenting style is relevant after all.) Why do some people think that it is morally acceptable to judge or even mistreat others such as gay or lesbian couples or, only a generation ago, interracial couples because they dislike or feel disgusted by them, whereas others do not? Why does the present generation “care about violence toward many more classes of victims</p><p>4 0.92171103 <a title="1048-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-08-How_adoption_speed_affects_the_abandonment_of_cultural_tastes.html">845 andrew gelman stats-2011-08-08-How adoption speed affects the abandonment of cultural tastes</a></p>
<p>Introduction: Interesting  article  by Jonah Berger and Gael Le Mens:
  
Products, styles, and social movements often catch on and become popular, but little is known about why such identity-relevant cultural tastes and practices die out. We demonstrate that the velocity of adoption may affect abandonment: Analysis of over 100 years of data on first-name adoption in both France and the United States illustrates that cultural tastes that have been adopted quickly die faster (i.e., are less likely to persist). Mirroring this aggregate pattern, at the individual level, expecting parents are more hesitant to adopt names that recently experienced sharper increases in adoption. Further analysis indicate that these effects are driven by concerns about symbolic value: Fads are perceived negatively, so people avoid identity-relevant items with sharply increasing popularity because they believe that they will be short lived. Ancillary analyses also indicate that, in contrast to conventional wisdom, identity-r</p><p>5 0.91081703 <a title="1048-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-18-Bob_on_Stan.html">1126 andrew gelman stats-2012-01-18-Bob on Stan</a></p>
<p>Introduction: Thurs 19 Jan 7pm  at the NYC Machine Learning meetup.
 
 Stan ‘s entirely publicly funded and open-source and it has  no secrets .  Ask us about it and we’ll tell you everything you might want to know.
 
P.S.  And  here ‘s the talk.</p><p>6 0.90910441 <a title="1048-lda-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-16-2500_people_living_in_a_park_in_Chicago%3F.html">469 andrew gelman stats-2010-12-16-2500 people living in a park in Chicago?</a></p>
<p>same-blog 7 0.90628409 <a title="1048-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-09-Maze_generation_algorithms%21.html">1048 andrew gelman stats-2011-12-09-Maze generation algorithms!</a></p>
<p>8 0.87640965 <a title="1048-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-17-Rare_name_analysis_and_wealth_convergence.html">1172 andrew gelman stats-2012-02-17-Rare name analysis and wealth convergence</a></p>
<p>9 0.87150049 <a title="1048-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-21-Bad_graph.html">863 andrew gelman stats-2011-08-21-Bad graph</a></p>
<p>10 0.86008519 <a title="1048-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-03-Hierarchical_array_priors_for_ANOVA_decompositions.html">1786 andrew gelman stats-2013-04-03-Hierarchical array priors for ANOVA decompositions</a></p>
<p>11 0.85994291 <a title="1048-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-25-It%E2%80%99s_binless%21__A_program_for_computing_normalizing_functions.html">1825 andrew gelman stats-2013-04-25-It’s binless!  A program for computing normalizing functions</a></p>
<p>12 0.85285079 <a title="1048-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-05-A_story_of_fake-data_checking_being_used_to_shoot_down_a_flawed_analysis_at_the_Farm_Credit_Agency.html">1884 andrew gelman stats-2013-06-05-A story of fake-data checking being used to shoot down a flawed analysis at the Farm Credit Agency</a></p>
<p>13 0.84638286 <a title="1048-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-14-Cool-ass_signal_processing_using_Gaussian_processes_%28birthdays_again%29.html">1379 andrew gelman stats-2012-06-14-Cool-ass signal processing using Gaussian processes (birthdays again)</a></p>
<p>14 0.82619345 <a title="1048-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-19-Slick_time_series_decomposition_of_the_birthdays_data.html">1384 andrew gelman stats-2012-06-19-Slick time series decomposition of the birthdays data</a></p>
<p>15 0.82188755 <a title="1048-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-24-Estimating_and_summarizing_inference_for_hierarchical_variance_parameters_when_the_number_of_groups_is_small.html">2145 andrew gelman stats-2013-12-24-Estimating and summarizing inference for hierarchical variance parameters when the number of groups is small</a></p>
<p>16 0.81998003 <a title="1048-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-25-Same_old_story.html">1229 andrew gelman stats-2012-03-25-Same old story</a></p>
<p>17 0.81614369 <a title="1048-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-04-David_MacKay_and_Occam%E2%80%99s_Razor.html">1041 andrew gelman stats-2011-12-04-David MacKay and Occam’s Razor</a></p>
<p>18 0.80601686 <a title="1048-lda-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-30-Adjudicating_between_alternative_interpretations_of_a_statistical_interaction%3F.html">2274 andrew gelman stats-2014-03-30-Adjudicating between alternative interpretations of a statistical interaction?</a></p>
<p>19 0.80065304 <a title="1048-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-09-Partial_least_squares_path_analysis.html">1714 andrew gelman stats-2013-02-09-Partial least squares path analysis</a></p>
<p>20 0.79711878 <a title="1048-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-26-Including_interactions_or_not.html">823 andrew gelman stats-2011-07-26-Including interactions or not</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
