<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>897 andrew gelman stats-2011-09-09-The difference between significant and not significant…</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-897" href="#">andrew_gelman_stats-2011-897</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>897 andrew gelman stats-2011-09-09-The difference between significant and not significant…</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-897-html" href="http://andrewgelman.com/2011/09/09/the-difference-between-significant-and-not-significant/">html</a></p><p>Introduction: E. J. Wagenmakers writes:
  
You may be interested in  a recent article  [by Nieuwenhuis, Forstmann, and Wagenmakers] showing how often researchers draw conclusions by comparing p-values. As you and Hal Stern have pointed out, this is potentially misleading because the difference between significant and not significant is not necessarily significant.


We were really suprised to see how often researchers in the neurosciences make this mistake. In the paper we speculate a little bit on the cause of the error.
  
From their paper:
  
In theory, a comparison of two experimental effects requires a statistical test on their difference. In practice, this comparison is often based on an incorrect procedure involving two separate tests in which researchers conclude that effects differ when one effect is significant (P < 0.05) but the other 
is not (P > 0.05). We reviewed 513 behavioral, systems and cognitive neuroscience articles in five top-ranking journals (Science, Nature, Nature Neuroscien</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Wagenmakers writes:    You may be interested in  a recent article  [by Nieuwenhuis, Forstmann, and Wagenmakers] showing how often researchers draw conclusions by comparing p-values. [sent-3, score-0.623]
</p><p>2 As you and Hal Stern have pointed out, this is potentially misleading because the difference between significant and not significant is not necessarily significant. [sent-4, score-0.732]
</p><p>3 We were really suprised to see how often researchers in the neurosciences make this mistake. [sent-5, score-0.638]
</p><p>4 In the paper we speculate a little bit on the cause of the error. [sent-6, score-0.303]
</p><p>5 From their paper:    In theory, a comparison of two experimental effects requires a statistical test on their difference. [sent-7, score-0.421]
</p><p>6 In practice, this comparison is often based on an incorrect procedure involving two separate tests in which researchers conclude that effects differ when one effect is significant (P < 0. [sent-8, score-1.589]
</p><p>7 We reviewed 513 behavioral, systems and cognitive neuroscience articles in five top-ranking journals (Science, Nature, Nature Neuroscience, Neuron and The Journal of Neuroscience) and found that 78 used the correct procedure and 79 used the incorrect procedure. [sent-11, score-1.406]
</p><p>8 I assume this has been an issue for close to a century; it’s interesting that it’s been noticed more in the past few years. [sent-12, score-0.081]
</p><p>9 writes, “I know of no references that precede your work with Hal Stern. [sent-18, score-0.094]
</p><p>10 The idea is so important that I’d be surprised if Fisher, Yates, Neyman, Box, Tukey, etc. [sent-20, score-0.076]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('neuroscience', 0.386), ('wagenmakers', 0.258), ('hal', 0.25), ('incorrect', 0.22), ('significant', 0.195), ('procedure', 0.178), ('suprised', 0.172), ('neurosciences', 0.172), ('yates', 0.172), ('researchers', 0.157), ('comparison', 0.151), ('nature', 0.149), ('speculate', 0.142), ('often', 0.137), ('stern', 0.136), ('wonder', 0.129), ('neyman', 0.122), ('tukey', 0.118), ('behavioral', 0.112), ('reviewed', 0.107), ('effects', 0.107), ('fisher', 0.106), ('century', 0.103), ('box', 0.102), ('differ', 0.099), ('conclude', 0.098), ('potentially', 0.098), ('systems', 0.096), ('cognitive', 0.094), ('references', 0.094), ('misleading', 0.092), ('involving', 0.089), ('cause', 0.089), ('draw', 0.087), ('requires', 0.083), ('separate', 0.083), ('used', 0.083), ('conclusions', 0.082), ('five', 0.081), ('comparing', 0.081), ('necessarily', 0.081), ('noticed', 0.081), ('experimental', 0.08), ('showing', 0.079), ('journals', 0.078), ('surprised', 0.076), ('tests', 0.075), ('paper', 0.072), ('pointed', 0.071), ('practice', 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="897-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-09-The_difference_between_significant_and_not_significant%E2%80%A6.html">897 andrew gelman stats-2011-09-09-The difference between significant and not significant…</a></p>
<p>Introduction: E. J. Wagenmakers writes:
  
You may be interested in  a recent article  [by Nieuwenhuis, Forstmann, and Wagenmakers] showing how often researchers draw conclusions by comparing p-values. As you and Hal Stern have pointed out, this is potentially misleading because the difference between significant and not significant is not necessarily significant.


We were really suprised to see how often researchers in the neurosciences make this mistake. In the paper we speculate a little bit on the cause of the error.
  
From their paper:
  
In theory, a comparison of two experimental effects requires a statistical test on their difference. In practice, this comparison is often based on an incorrect procedure involving two separate tests in which researchers conclude that effects differ when one effect is significant (P < 0.05) but the other 
is not (P > 0.05). We reviewed 513 behavioral, systems and cognitive neuroscience articles in five top-ranking journals (Science, Nature, Nature Neuroscien</p><p>2 0.2296766 <a title="897-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-19-%E2%80%9CThe_difference_between_._._.%E2%80%9D%3A__It%E2%80%99s_not_just_p%3D.05_vs._p%3D.06.html">1072 andrew gelman stats-2011-12-19-“The difference between . . .”:  It’s not just p=.05 vs. p=.06</a></p>
<p>Introduction: The title of  this post  by Sanjay Srivastava illustrates an annoying misconception that’s crept into the (otherwise delightful) recent  publicity  related to my  article  with Hal Stern, he difference between “significant” and “not significant” is not itself statistically significant.
 
When people bring this up, they keep referring to the difference between p=0.05 and p=0.06, making the familiar (and correct) point about the arbitrariness of the conventional p-value threshold of 0.05.  And, sure, I agree with this, but everybody knows that already.
 
The point Hal and I were making was that even apparently large differences in p-values are not statistically significant. For example, if you have one study with z=2.5 (almost significant at the 1% level!) and another with z=1 (not statistically significant at all, only 1 se from zero!), then their difference has a z of about 1 (again, not statistically significant at all). So it’s not just a comparison of 0.05 vs. 0.06, even a differenc</p><p>3 0.20745532 <a title="897-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-11-Hey%2C_good_news%21__Your_p-value_just_passed_the_0.05_threshold%21.html">758 andrew gelman stats-2011-06-11-Hey, good news!  Your p-value just passed the 0.05 threshold!</a></p>
<p>Introduction: E. J. Wagenmakers writes:
  
 Here’s a link  for you.  The first sentences tell it all:

 
Climate warming since 1995 is now statistically significant, according to Phil Jones, the UK scientist targeted in the “ClimateGate” affair. Last year, he told BBC News that post-1995 warming was not significant–a statement still seen on blogs critical of the idea of man-made climate change. But another year of data has pushed the trend past the threshold usually used to assess whether trends are “real.”
 

Now I [Wagenmakers] don’t like p-values one bit, but even people who do like them must cringe when they read this. First, this apparently is a sequential design, so I’m not sure what sampling plan leads to these p-values. Secondly, comparing significance values suggests that the data have suddenly crossed some invisible line that divided nonsignificant from significant effects (as you pointed out in your paper with Hal Stern). Ugh!
  
I share Wagenmakers’s reaction.  There seems to be some con</p><p>4 0.14541262 <a title="897-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-24-In_which_I_side_with_Neyman_over_Fisher.html">1869 andrew gelman stats-2013-05-24-In which I side with Neyman over Fisher</a></p>
<p>Introduction: As a data analyst and a scientist, Fisher > Neyman, no question.  But as a theorist, Fisher came up with ideas that worked just fine in his applications but can fall apart when people try to apply them too generally.
 
Here’s an example that recently came up.
 
Deborah Mayo pointed me to a  comment  by Stephen Senn on the so-called Fisher and Neyman null hypotheses.  In an experiment with n participants (or, as we used to say, subjects or experimental units), the Fisher null hypothesis is that the treatment effect is exactly 0 for every one of the n units, while the Neyman null hypothesis is that the individual treatment effects can be negative or positive but have an average of zero.
 
Senn explains why Neyman’s hypothesis in general makes no sense—the short story is that Fisher’s hypothesis seems relevant in some problems (sometimes we really are studying effects that are zero or close enough for all practical purposes), whereas Neyman’s hypothesis just seems weird (it’s implausible</p><p>5 0.14248896 <a title="897-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-With_a_bit_of_precognition%2C_you%E2%80%99d_have_known_I_was_going_to_post_again_on_this_topic%2C_and_with_a_lot_of_precognition%2C_you%E2%80%99d_have_known_I_was_going_to_post_today.html">576 andrew gelman stats-2011-02-15-With a bit of precognition, you’d have known I was going to post again on this topic, and with a lot of precognition, you’d have known I was going to post today</a></p>
<p>Introduction: Chris Masse points me to  this response  by Daryl Bem and two statisticians (Jessica Utts and Wesley Johnson) to criticisms by Wagenmakers et.al. of Bem’s recent ESP study.  I have nothing to add but would like to repeat a couple bits of my discussions of last month, of  here :
  
Classical statistical methods that work reasonably well when studying moderate or large effects (see the work of Fisher, Snedecor, Cochran, etc.) fall apart in the presence of small effects.


I think it’s naive when people implicitly assume that the study’s claims are correct, or the study’s statistical methods are weak. Generally, the smaller the effects you’re studying, the better the statistics you need. ESP is a field of small effects and so ESP researchers use high-quality statistics.


To put it another way: whatever methodological errors happen to be in the paper in question, probably occur in lots of researcher papers in “legitimate” psychology research. The difference is that when you’re studying a</p><p>6 0.11936068 <a title="897-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-30-Retracted_articles_and_unethical_behavior_in_economics_journals%3F.html">1435 andrew gelman stats-2012-07-30-Retracted articles and unethical behavior in economics journals?</a></p>
<p>7 0.11878934 <a title="897-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-05-Cleaning_up_science.html">1842 andrew gelman stats-2013-05-05-Cleaning up science</a></p>
<p>8 0.11064504 <a title="897-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-18-You%E2%80%99ll_get_a_high_Type_S_error_rate_if_you_use_classical_statistical_methods_to_analyze_data_from_underpowered_studies.html">1944 andrew gelman stats-2013-07-18-You’ll get a high Type S error rate if you use classical statistical methods to analyze data from underpowered studies</a></p>
<p>9 0.10385481 <a title="897-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-31-How_to_fix_the_tabloids%3F__Toward_replicable_social_science_research.html">1878 andrew gelman stats-2013-05-31-How to fix the tabloids?  Toward replicable social science research</a></p>
<p>10 0.095337197 <a title="897-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-The_statistical_significance_filter.html">899 andrew gelman stats-2011-09-10-The statistical significance filter</a></p>
<p>11 0.094055198 <a title="897-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-12-Some_thoughts_on_academic_cheating%2C_inspired_by_Frey%2C_Wegman%2C_Fischer%2C_Hauser%2C_Stapel.html">901 andrew gelman stats-2011-09-12-Some thoughts on academic cheating, inspired by Frey, Wegman, Fischer, Hauser, Stapel</a></p>
<p>12 0.093359463 <a title="897-tfidf-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Ken_Rice_presents_a_unifying_approach_to_statistical_inference_and_hypothesis_testing.html">2312 andrew gelman stats-2014-04-29-Ken Rice presents a unifying approach to statistical inference and hypothesis testing</a></p>
<p>13 0.09261173 <a title="897-tfidf-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-25-Classics_of_statistics.html">109 andrew gelman stats-2010-06-25-Classics of statistics</a></p>
<p>14 0.087608948 <a title="897-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-02-Flame_bait.html">1880 andrew gelman stats-2013-06-02-Flame bait</a></p>
<p>15 0.087130293 <a title="897-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-30-More_bad_news%3A__The_%28mis%29reporting_of_statistical_results_in_psychology_journals.html">933 andrew gelman stats-2011-09-30-More bad news:  The (mis)reporting of statistical results in psychology journals</a></p>
<p>16 0.086833939 <a title="897-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-06-How_to_think_about_papers_published_in_low-grade_journals%3F.html">1928 andrew gelman stats-2013-07-06-How to think about papers published in low-grade journals?</a></p>
<p>17 0.086778343 <a title="897-tfidf-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-21-Data_cleaning_tool%21.html">424 andrew gelman stats-2010-11-21-Data cleaning tool!</a></p>
<p>18 0.086761437 <a title="897-tfidf-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-17-So-called_fixed_and_random_effects.html">472 andrew gelman stats-2010-12-17-So-called fixed and random effects</a></p>
<p>19 0.086259998 <a title="897-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>20 0.085637465 <a title="897-tfidf-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-26-Musical_chairs_in_econ_journals.html">371 andrew gelman stats-2010-10-26-Musical chairs in econ journals</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.153), (1, -0.002), (2, -0.024), (3, -0.147), (4, -0.037), (5, -0.065), (6, -0.049), (7, -0.018), (8, 0.007), (9, -0.011), (10, -0.012), (11, 0.018), (12, 0.029), (13, -0.065), (14, 0.046), (15, -0.016), (16, -0.026), (17, 0.009), (18, -0.022), (19, -0.027), (20, -0.002), (21, -0.012), (22, -0.006), (23, 0.004), (24, 0.006), (25, -0.024), (26, -0.001), (27, 0.002), (28, 0.009), (29, -0.042), (30, 0.001), (31, 0.006), (32, 0.024), (33, -0.018), (34, -0.008), (35, 0.017), (36, -0.028), (37, 0.018), (38, 0.018), (39, 0.01), (40, -0.027), (41, 0.044), (42, 0.03), (43, 0.061), (44, 0.001), (45, -0.02), (46, -0.038), (47, -0.065), (48, 0.009), (49, -0.009)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97970027 <a title="897-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-09-The_difference_between_significant_and_not_significant%E2%80%A6.html">897 andrew gelman stats-2011-09-09-The difference between significant and not significant…</a></p>
<p>Introduction: E. J. Wagenmakers writes:
  
You may be interested in  a recent article  [by Nieuwenhuis, Forstmann, and Wagenmakers] showing how often researchers draw conclusions by comparing p-values. As you and Hal Stern have pointed out, this is potentially misleading because the difference between significant and not significant is not necessarily significant.


We were really suprised to see how often researchers in the neurosciences make this mistake. In the paper we speculate a little bit on the cause of the error.
  
From their paper:
  
In theory, a comparison of two experimental effects requires a statistical test on their difference. In practice, this comparison is often based on an incorrect procedure involving two separate tests in which researchers conclude that effects differ when one effect is significant (P < 0.05) but the other 
is not (P > 0.05). We reviewed 513 behavioral, systems and cognitive neuroscience articles in five top-ranking journals (Science, Nature, Nature Neuroscien</p><p>2 0.81035531 <a title="897-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>Introduction: This article  is a discussion of a  paper  by Greg Francis for a special issue, edited by E. J. Wagenmakers, of the Journal of Mathematical Psychology.  Here’s what I wrote:
  
Much of statistical practice is an effort to reduce or deny variation and uncertainty. The reduction is done through standardization, replication, and other practices of experimental design, with the idea being to isolate and stabilize the quantity being estimated and then average over many cases. Even so, however, uncertainty persists, and statistical hypothesis testing is in many ways an endeavor to deny this, by reporting binary accept/reject decisions.


Classical statistical methods produce binary statements, but there is no reason to assume that the world works that way. Expressions such as Type 1 error, Type 2 error, false positive, and so on, are based on a model in which the world is divided into real and non-real effects. To put it another way, I understand the general scientific distinction of real vs</p><p>3 0.78643376 <a title="897-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-With_a_bit_of_precognition%2C_you%E2%80%99d_have_known_I_was_going_to_post_again_on_this_topic%2C_and_with_a_lot_of_precognition%2C_you%E2%80%99d_have_known_I_was_going_to_post_today.html">576 andrew gelman stats-2011-02-15-With a bit of precognition, you’d have known I was going to post again on this topic, and with a lot of precognition, you’d have known I was going to post today</a></p>
<p>Introduction: Chris Masse points me to  this response  by Daryl Bem and two statisticians (Jessica Utts and Wesley Johnson) to criticisms by Wagenmakers et.al. of Bem’s recent ESP study.  I have nothing to add but would like to repeat a couple bits of my discussions of last month, of  here :
  
Classical statistical methods that work reasonably well when studying moderate or large effects (see the work of Fisher, Snedecor, Cochran, etc.) fall apart in the presence of small effects.


I think it’s naive when people implicitly assume that the study’s claims are correct, or the study’s statistical methods are weak. Generally, the smaller the effects you’re studying, the better the statistics you need. ESP is a field of small effects and so ESP researchers use high-quality statistics.


To put it another way: whatever methodological errors happen to be in the paper in question, probably occur in lots of researcher papers in “legitimate” psychology research. The difference is that when you’re studying a</p><p>4 0.78033811 <a title="897-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-11-One_more_time_on_that_ESP_study%3A__The_problem_of_overestimates_and_the_shrinkage_solution.html">511 andrew gelman stats-2011-01-11-One more time on that ESP study:  The problem of overestimates and the shrinkage solution</a></p>
<p>Introduction: Benedict Carey  writes  a follow-up article on ESP studies and Bayesian statistics.  ( See here  for my previous thoughts on the topic.)  Everything Carey writes is fine, and he even uses an example I recommended:
  
The statistical approach that has dominated the social sciences for almost a century is called significance testing. The idea is straightforward. A finding from any well-designed study — say, a correlation between a personality trait and the risk of depression — is considered “significant” if its probability of occurring by chance is less than 5 percent.


This arbitrary cutoff makes sense when the effect being studied is a large one — for example, when measuring the so-called Stroop effect. This effect predicts that naming the color of a word is faster and more accurate when the word and color match (“red” in red letters) than when they do not (“red” in blue letters), and is very strong in almost everyone.


“But if the true effect of what you are measuring is small,” sai</p><p>5 0.7766152 <a title="897-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-16-%E2%80%9CFalse-positive_psychology%E2%80%9D.html">1171 andrew gelman stats-2012-02-16-“False-positive psychology”</a></p>
<p>Introduction: Everybody’s  talkin bout  this paper by Joseph Simmons, Leif Nelson and Uri Simonsohn, who  write :
  
Despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We [Simmons, Nelson, and Simonsohn] present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.
  
Whatever you think about these recommend</p><p>6 0.75388265 <a title="897-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-31-Response_by_Jessica_Tracy_and_Alec_Beall_to_my_critique_of_the_methods_in_their_paper%2C_%E2%80%9CWomen_Are_More_Likely_to_Wear_Red_or_Pink_at_Peak_Fertility%E2%80%9D.html">1963 andrew gelman stats-2013-07-31-Response by Jessica Tracy and Alec Beall to my critique of the methods in their paper, “Women Are More Likely to Wear Red or Pink at Peak Fertility”</a></p>
<p>7 0.73808062 <a title="897-lsi-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-23-Discussion_on_preregistration_of_research_studies.html">2183 andrew gelman stats-2014-01-23-Discussion on preregistration of research studies</a></p>
<p>8 0.73245579 <a title="897-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-Fourteen_magic_words%3A_an_update.html">898 andrew gelman stats-2011-09-10-Fourteen magic words: an update</a></p>
<p>9 0.73022991 <a title="897-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-26-%E2%80%9CA_Vast_Graveyard_of_Undead_Theories%3A__Publication_Bias_and_Psychological_Science%E2%80%99s_Aversion_to_the_Null%E2%80%9D.html">1826 andrew gelman stats-2013-04-26-“A Vast Graveyard of Undead Theories:  Publication Bias and Psychological Science’s Aversion to the Null”</a></p>
<p>10 0.72928506 <a title="897-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-28-Difficulties_of_using_statistical_significance_%28or_lack_thereof%29_to_sift_through_and_compare_research_hypotheses.html">2042 andrew gelman stats-2013-09-28-Difficulties of using statistical significance (or lack thereof) to sift through and compare research hypotheses</a></p>
<p>11 0.72910959 <a title="897-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-18-You%E2%80%99ll_get_a_high_Type_S_error_rate_if_you_use_classical_statistical_methods_to_analyze_data_from_underpowered_studies.html">1944 andrew gelman stats-2013-07-18-You’ll get a high Type S error rate if you use classical statistical methods to analyze data from underpowered studies</a></p>
<p>12 0.72352046 <a title="897-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-02-Flame_bait.html">1880 andrew gelman stats-2013-06-02-Flame bait</a></p>
<p>13 0.72276103 <a title="897-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-02-The_inevitable_problems_with_statistical_significance_and_95%25_intervals.html">1150 andrew gelman stats-2012-02-02-The inevitable problems with statistical significance and 95% intervals</a></p>
<p>14 0.71488428 <a title="897-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-11-Hey%2C_good_news%21__Your_p-value_just_passed_the_0.05_threshold%21.html">758 andrew gelman stats-2011-06-11-Hey, good news!  Your p-value just passed the 0.05 threshold!</a></p>
<p>15 0.70841652 <a title="897-lsi-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-13-%E2%80%9CThe_truth_wears_off%3A__Is_there_something_wrong_with_the_scientific_method%3F%E2%80%9D.html">466 andrew gelman stats-2010-12-13-“The truth wears off:  Is there something wrong with the scientific method?”</a></p>
<p>16 0.70543927 <a title="897-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-28-50_shades_of_gray%3A__A_research_story.html">1959 andrew gelman stats-2013-07-28-50 shades of gray:  A research story</a></p>
<p>17 0.70092183 <a title="897-lsi-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-08-Discussion_with_Steven_Pinker_on_research_that_is_attached_to_data_that_are_so_noisy_as_to_be_essentially_uninformative.html">2326 andrew gelman stats-2014-05-08-Discussion with Steven Pinker on research that is attached to data that are so noisy as to be essentially uninformative</a></p>
<p>18 0.70090687 <a title="897-lsi-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-20-The_AAA_Tranche_of_Subprime_Science.html">2179 andrew gelman stats-2014-01-20-The AAA Tranche of Subprime Science</a></p>
<p>19 0.69975245 <a title="897-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-30-More_bad_news%3A__The_%28mis%29reporting_of_statistical_results_in_psychology_journals.html">933 andrew gelman stats-2011-09-30-More bad news:  The (mis)reporting of statistical results in psychology journals</a></p>
<p>20 0.69906801 <a title="897-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-31-How_to_fix_the_tabloids%3F__Toward_replicable_social_science_research.html">1878 andrew gelman stats-2013-05-31-How to fix the tabloids?  Toward replicable social science research</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(15, 0.057), (16, 0.055), (21, 0.091), (24, 0.144), (27, 0.027), (30, 0.015), (84, 0.028), (86, 0.083), (93, 0.076), (96, 0.048), (99, 0.274)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97123063 <a title="897-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-09-The_difference_between_significant_and_not_significant%E2%80%A6.html">897 andrew gelman stats-2011-09-09-The difference between significant and not significant…</a></p>
<p>Introduction: E. J. Wagenmakers writes:
  
You may be interested in  a recent article  [by Nieuwenhuis, Forstmann, and Wagenmakers] showing how often researchers draw conclusions by comparing p-values. As you and Hal Stern have pointed out, this is potentially misleading because the difference between significant and not significant is not necessarily significant.


We were really suprised to see how often researchers in the neurosciences make this mistake. In the paper we speculate a little bit on the cause of the error.
  
From their paper:
  
In theory, a comparison of two experimental effects requires a statistical test on their difference. In practice, this comparison is often based on an incorrect procedure involving two separate tests in which researchers conclude that effects differ when one effect is significant (P < 0.05) but the other 
is not (P > 0.05). We reviewed 513 behavioral, systems and cognitive neuroscience articles in five top-ranking journals (Science, Nature, Nature Neuroscien</p><p>2 0.94346893 <a title="897-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-28-50_shades_of_gray%3A__A_research_story.html">1959 andrew gelman stats-2013-07-28-50 shades of gray:  A research story</a></p>
<p>Introduction: This is a killer story  (from Brian Nosek, Jeffrey Spies, and Matt Motyl).
 
Part 1:
  
Two of the present authors, Motyl and Nosek, share interests in political ideology. We were inspired by the fast growing literature on embodiment that demonstrates surprising links between body and mind (Markman & Brendl, 2005; Proffitt, 2006) to investigate embodiment of political extremism. Participants from the political left, right and center (N = 1,979) completed a perceptual judgment task in which words were presented in different shades of gray. Participants had to click along a gradient representing grays from near black to near white to select a shade that matched the shade of the word. We calculated accuracy: How close to the actual shade did participants get? The results were stunning. Moderates perceived the shades of gray more accurately than extremists on the left and right (p = .01). Our conclusion: political extremists perceive the world in black-and-white, figuratively and literally</p><p>3 0.93219995 <a title="897-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-23-%E2%80%9CAny_old_map_will_do%E2%80%9D_meets_%E2%80%9CGod_is_in_every_leaf_of_every_tree%E2%80%9D.html">1278 andrew gelman stats-2012-04-23-“Any old map will do” meets “God is in every leaf of every tree”</a></p>
<p>Introduction: As a statistician I am particularly worried about the rhetorical power of anecdotes (even though I use them in my own reasoning; see discussion below).  But much can be learned from a  true  anecdote.  The rough edges—the places where the anecdote doesn’t fit your thesis—these are where you learn.
 
We have recently had a discussion ( here  and  here ) of Karl Weick, a prominent scholar of business management who plagiarized a story and then went on to draw different lessons from the pilfered anecdote in several different publications published over many years.
 
Setting aside an issues of plagiarism and rulebreaking, I argue that, by hiding the source of the story and changing its form, Weick and his management-science audience are losing their ability to get anything out of it beyond empty confirmation.
 
A full discussion follows.
 
 1.  The lost Hungarian soldiers 
 
Thomas Basbøll (who has the unusual (to me) job of “writing consultant” at the Copenhagen Business School) has been</p><p>4 0.93204528 <a title="897-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>Introduction: Larry Wasserman  refers  to finite mixture models as “beasts” and  writes  jokes that they “should be avoided at all costs.”
 
I’ve thought a lot about mixture models, ever since using them in an  analysis  of voting patterns that was published in 1990.  First off, I’d like to say that our model was useful so I’d prefer not to pay the cost of avoiding it.  For a quick description of our mixture model and its context, see pp. 379-380 of my  article  in the Jim Berger volume).  Actually, our case was particularly difficult because we were not even fitting a mixture model to data, we were fitting it to latent data and using the model to perform partial pooling.  My difficulties in trying to fit this model inspired our discussion of mixture models in Bayesian Data Analysis (page 109 in the second edition, in the section on “Counterexamples to the theorems”).
 
I agree with Larry that if you’re fitting a mixture model, it’s good to be aware of the problems that arise if you try to estimate</p><p>5 0.93116128 <a title="897-lda-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>Introduction: A recent  discussion  between commenters Question and Fernando captured one of the recurrent themes here from the past year.
 
 Question:   The problem is simple, the researchers are disproving always false null hypotheses and taking this disproof as near proof that their theory is correct.
 
 Fernando:   Whereas it is probably true that researchers misuse NHT, the problem with tabloid science is broader and deeper. It is systemic.
 
 Question:   I do not see how anything can be deeper than replacing careful description, prediction, falsification, and independent replication with dynamite plots, p-values, affirming the consequent, and peer review. From my own experience I am confident in saying that confusion caused by NHST is at the root of this problem.
 
 Fernando:   Incentives? Impact factors? Publish or die? “Interesting” and “new” above quality and reliability, or actually answering a research question, and a silly and unbecoming obsession with being quoted in NYT, etc. . . . Giv</p><p>6 0.93047059 <a title="897-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-13-Jim_Campbell_argues_that_Larry_Bartels%E2%80%99s_%E2%80%9CUnequal_Democracy%E2%80%9D_findings_are_not_robust.html">659 andrew gelman stats-2011-04-13-Jim Campbell argues that Larry Bartels’s “Unequal Democracy” findings are not robust</a></p>
<p>7 0.92987251 <a title="897-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-06-Josh_Tenenbaum_presents_._._._a_model_of_folk_physics%21.html">994 andrew gelman stats-2011-11-06-Josh Tenenbaum presents . . . a model of folk physics!</a></p>
<p>8 0.92873001 <a title="897-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-25-Classical_probability_does_not_apply_to_quantum_systems_%28causal_inference_edition%29.html">2037 andrew gelman stats-2013-09-25-Classical probability does not apply to quantum systems (causal inference edition)</a></p>
<p>9 0.92863154 <a title="897-lda-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-31-The_most-cited_statistics_papers_ever.html">2277 andrew gelman stats-2014-03-31-The most-cited statistics papers ever</a></p>
<p>10 0.92755651 <a title="897-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-23-Combining_survey_data_obtained_using_different_modes_of_sampling.html">777 andrew gelman stats-2011-06-23-Combining survey data obtained using different modes of sampling</a></p>
<p>11 0.92697549 <a title="897-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-07-Descriptive_statistics%2C_causal_inference%2C_and_story_time.html">789 andrew gelman stats-2011-07-07-Descriptive statistics, causal inference, and story time</a></p>
<p>12 0.92611021 <a title="897-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>13 0.92539454 <a title="897-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-26-Age_and_happiness%3A__The_pattern_isn%E2%80%99t_as_clear_as_you_might_think.html">486 andrew gelman stats-2010-12-26-Age and happiness:  The pattern isn’t as clear as you might think</a></p>
<p>14 0.92486978 <a title="897-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-18-Postdoc_positions_at_Microsoft_Research_%E2%80%93_NYC.html">1630 andrew gelman stats-2012-12-18-Postdoc positions at Microsoft Research – NYC</a></p>
<p>15 0.92468786 <a title="897-lda-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-17-Big_corporations_are_more_popular_than_you_might_realize.html">1123 andrew gelman stats-2012-01-17-Big corporations are more popular than you might realize</a></p>
<p>16 0.9246183 <a title="897-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-02-Fighting_a_losing_battle.html">1518 andrew gelman stats-2012-10-02-Fighting a losing battle</a></p>
<p>17 0.92442739 <a title="897-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-16-Another_day%2C_another_plagiarist.html">1266 andrew gelman stats-2012-04-16-Another day, another plagiarist</a></p>
<p>18 0.92379165 <a title="897-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-26-%E2%80%9CA_Vast_Graveyard_of_Undead_Theories%3A__Publication_Bias_and_Psychological_Science%E2%80%99s_Aversion_to_the_Null%E2%80%9D.html">1826 andrew gelman stats-2013-04-26-“A Vast Graveyard of Undead Theories:  Publication Bias and Psychological Science’s Aversion to the Null”</a></p>
<p>19 0.92348576 <a title="897-lda-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-04-Stan_%28%26_JAGS%29_Tutorial_on_Linear_Mixed_Models.html">2318 andrew gelman stats-2014-05-04-Stan (& JAGS) Tutorial on Linear Mixed Models</a></p>
<p>20 0.92318547 <a title="897-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-22-Struggles_over_the_criticism_of_the_%E2%80%9Ccannabis_users_and_IQ_change%E2%80%9D_paper.html">1910 andrew gelman stats-2013-06-22-Struggles over the criticism of the “cannabis users and IQ change” paper</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
