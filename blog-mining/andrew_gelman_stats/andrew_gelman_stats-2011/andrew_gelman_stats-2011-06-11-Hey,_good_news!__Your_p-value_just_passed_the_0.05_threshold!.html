<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>758 andrew gelman stats-2011-06-11-Hey, good news!  Your p-value just passed the 0.05 threshold!</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-758" href="#">andrew_gelman_stats-2011-758</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>758 andrew gelman stats-2011-06-11-Hey, good news!  Your p-value just passed the 0.05 threshold!</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-758-html" href="http://andrewgelman.com/2011/06/11/hey_good_news_t/">html</a></p><p>Introduction: E. J. Wagenmakers writes:
  
 Here’s a link  for you.  The first sentences tell it all:

 
Climate warming since 1995 is now statistically significant, according to Phil Jones, the UK scientist targeted in the “ClimateGate” affair. Last year, he told BBC News that post-1995 warming was not significant–a statement still seen on blogs critical of the idea of man-made climate change. But another year of data has pushed the trend past the threshold usually used to assess whether trends are “real.”
 

Now I [Wagenmakers] don’t like p-values one bit, but even people who do like them must cringe when they read this. First, this apparently is a sequential design, so I’m not sure what sampling plan leads to these p-values. Secondly, comparing significance values suggests that the data have suddenly crossed some invisible line that divided nonsignificant from significant effects (as you pointed out in your paper with Hal Stern). Ugh!
  
I share Wagenmakers’s reaction.  There seems to be some con</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The first sentences tell it all:    Climate warming since 1995 is now statistically significant, according to Phil Jones, the UK scientist targeted in the “ClimateGate” affair. [sent-4, score-0.421]
</p><p>2 Last year, he told BBC News that post-1995 warming was not significant–a statement still seen on blogs critical of the idea of man-made climate change. [sent-5, score-0.572]
</p><p>3 But another year of data has pushed the trend past the threshold usually used to assess whether trends are “real. [sent-6, score-0.754]
</p><p>4 ”    Now I [Wagenmakers] don’t like p-values one bit, but even people who do like them must cringe when they read this. [sent-7, score-0.147]
</p><p>5 First, this apparently is a sequential design, so I’m not sure what sampling plan leads to these p-values. [sent-8, score-0.361]
</p><p>6 Secondly, comparing significance values suggests that the data have suddenly crossed some invisible line that divided nonsignificant from significant effects (as you pointed out in your paper with Hal Stern). [sent-9, score-0.81]
</p><p>7 There seems to be some confusion here between inferential thresholds and decision thresholds. [sent-12, score-0.518]
</p><p>8 Which reminds me how much I hate the old 1950s literature (both classical and Bayesian) on inference as decision, loss functions for estimators, and all the rest. [sent-13, score-0.331]
</p><p>9 I think the p-value serves a role in summarizing certain aspects of a model’s fit to data but I certainly don’t think it makes sense as any kind of decision threshold (despite that it is nearly universally used as such to decide on acceptance of research in scientific journals). [sent-14, score-1.08]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wagenmakers', 0.35), ('warming', 0.205), ('decision', 0.201), ('threshold', 0.191), ('climate', 0.184), ('significant', 0.177), ('bbc', 0.156), ('nonsignificant', 0.156), ('climategate', 0.147), ('cringe', 0.147), ('universally', 0.147), ('invisible', 0.141), ('crossed', 0.136), ('secondly', 0.132), ('sequential', 0.132), ('ugh', 0.129), ('stern', 0.123), ('uk', 0.123), ('thresholds', 0.123), ('jones', 0.123), ('pushed', 0.121), ('targeted', 0.119), ('estimators', 0.117), ('hal', 0.113), ('serves', 0.108), ('suddenly', 0.104), ('summarizing', 0.102), ('inferential', 0.102), ('acceptance', 0.101), ('sentences', 0.097), ('year', 0.097), ('divided', 0.096), ('critical', 0.095), ('assess', 0.094), ('confusion', 0.092), ('trend', 0.092), ('blogs', 0.088), ('loss', 0.087), ('functions', 0.086), ('phil', 0.084), ('trends', 0.084), ('hate', 0.081), ('decide', 0.079), ('leads', 0.079), ('despite', 0.078), ('reminds', 0.077), ('aspects', 0.076), ('apparently', 0.075), ('used', 0.075), ('plan', 0.075)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999976 <a title="758-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-11-Hey%2C_good_news%21__Your_p-value_just_passed_the_0.05_threshold%21.html">758 andrew gelman stats-2011-06-11-Hey, good news!  Your p-value just passed the 0.05 threshold!</a></p>
<p>Introduction: E. J. Wagenmakers writes:
  
 Here’s a link  for you.  The first sentences tell it all:

 
Climate warming since 1995 is now statistically significant, according to Phil Jones, the UK scientist targeted in the “ClimateGate” affair. Last year, he told BBC News that post-1995 warming was not significant–a statement still seen on blogs critical of the idea of man-made climate change. But another year of data has pushed the trend past the threshold usually used to assess whether trends are “real.”
 

Now I [Wagenmakers] don’t like p-values one bit, but even people who do like them must cringe when they read this. First, this apparently is a sequential design, so I’m not sure what sampling plan leads to these p-values. Secondly, comparing significance values suggests that the data have suddenly crossed some invisible line that divided nonsignificant from significant effects (as you pointed out in your paper with Hal Stern). Ugh!
  
I share Wagenmakers’s reaction.  There seems to be some con</p><p>2 0.20745532 <a title="758-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-09-The_difference_between_significant_and_not_significant%E2%80%A6.html">897 andrew gelman stats-2011-09-09-The difference between significant and not significant…</a></p>
<p>Introduction: E. J. Wagenmakers writes:
  
You may be interested in  a recent article  [by Nieuwenhuis, Forstmann, and Wagenmakers] showing how often researchers draw conclusions by comparing p-values. As you and Hal Stern have pointed out, this is potentially misleading because the difference between significant and not significant is not necessarily significant.


We were really suprised to see how often researchers in the neurosciences make this mistake. In the paper we speculate a little bit on the cause of the error.
  
From their paper:
  
In theory, a comparison of two experimental effects requires a statistical test on their difference. In practice, this comparison is often based on an incorrect procedure involving two separate tests in which researchers conclude that effects differ when one effect is significant (P < 0.05) but the other 
is not (P > 0.05). We reviewed 513 behavioral, systems and cognitive neuroscience articles in five top-ranking journals (Science, Nature, Nature Neuroscien</p><p>3 0.19080704 <a title="758-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-19-%E2%80%9CThe_difference_between_._._.%E2%80%9D%3A__It%E2%80%99s_not_just_p%3D.05_vs._p%3D.06.html">1072 andrew gelman stats-2011-12-19-“The difference between . . .”:  It’s not just p=.05 vs. p=.06</a></p>
<p>Introduction: The title of  this post  by Sanjay Srivastava illustrates an annoying misconception that’s crept into the (otherwise delightful) recent  publicity  related to my  article  with Hal Stern, he difference between “significant” and “not significant” is not itself statistically significant.
 
When people bring this up, they keep referring to the difference between p=0.05 and p=0.06, making the familiar (and correct) point about the arbitrariness of the conventional p-value threshold of 0.05.  And, sure, I agree with this, but everybody knows that already.
 
The point Hal and I were making was that even apparently large differences in p-values are not statistically significant. For example, if you have one study with z=2.5 (almost significant at the 1% level!) and another with z=1 (not statistically significant at all, only 1 se from zero!), then their difference has a z of about 1 (again, not statistically significant at all). So it’s not just a comparison of 0.05 vs. 0.06, even a differenc</p><p>4 0.16250585 <a title="758-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>Introduction: In response to the  discussion  of X and me of his recent  paper , Val Johnson writes:
  
I would like to thank Andrew for forwarding his comments on uniformly most powerful Bayesian tests (UMPBTs) to me and his invitation to respond to them.  I think he  (and also Christian Robert) raise a number of interesting points concerning this new class of Bayesian tests, but I think that they may have confounded several issues that might more usefully be examined separately.


The first issue involves the choice of the Bayesian evidence threshold, gamma, used in rejecting a null hypothesis in favor of an alternative hypothesis.  Andrew objects to the higher values of gamma proposed in my recent PNAS article on grounds that too many important scientific effects would be missed if thresholds of 25-50 were routinely used.  These evidence thresholds correspond roughly to p-values of 0.005; Andrew suggests that evidence thresholds around 5 should continue to be used (gamma=5 corresponds approximate</p><p>5 0.15443499 <a title="758-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-With_a_bit_of_precognition%2C_you%E2%80%99d_have_known_I_was_going_to_post_again_on_this_topic%2C_and_with_a_lot_of_precognition%2C_you%E2%80%99d_have_known_I_was_going_to_post_today.html">576 andrew gelman stats-2011-02-15-With a bit of precognition, you’d have known I was going to post again on this topic, and with a lot of precognition, you’d have known I was going to post today</a></p>
<p>Introduction: Chris Masse points me to  this response  by Daryl Bem and two statisticians (Jessica Utts and Wesley Johnson) to criticisms by Wagenmakers et.al. of Bem’s recent ESP study.  I have nothing to add but would like to repeat a couple bits of my discussions of last month, of  here :
  
Classical statistical methods that work reasonably well when studying moderate or large effects (see the work of Fisher, Snedecor, Cochran, etc.) fall apart in the presence of small effects.


I think it’s naive when people implicitly assume that the study’s claims are correct, or the study’s statistical methods are weak. Generally, the smaller the effects you’re studying, the better the statistics you need. ESP is a field of small effects and so ESP researchers use high-quality statistics.


To put it another way: whatever methodological errors happen to be in the paper in question, probably occur in lots of researcher papers in “legitimate” psychology research. The difference is that when you’re studying a</p><p>6 0.14834829 <a title="758-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-02-Selection_bias%2C_or%2C_How_you_can_think_the_experts_don%E2%80%99t_check_their_models%2C_if_you_simply_don%E2%80%99t_look_at_what_the_experts_actually_are_doing.html">1295 andrew gelman stats-2012-05-02-Selection bias, or, How you can think the experts don’t check their models, if you simply don’t look at what the experts actually are doing</a></p>
<p>7 0.12840673 <a title="758-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-31-Skepticism_about_skepticism_of_global_warming_skepticism_skepticism.html">983 andrew gelman stats-2011-10-31-Skepticism about skepticism of global warming skepticism skepticism</a></p>
<p>8 0.12593475 <a title="758-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>9 0.12038169 <a title="758-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-07-Inference_%3D_data_%2B_model.html">1201 andrew gelman stats-2012-03-07-Inference = data + model</a></p>
<p>10 0.11912242 <a title="758-tfidf-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-25-Revised_statistical_standards_for_evidence_%28comments_to_Val_Johnson%E2%80%99s_comments_on_our_comments_on_Val%E2%80%99s_comments_on_p-values%29.html">2305 andrew gelman stats-2014-04-25-Revised statistical standards for evidence (comments to Val Johnson’s comments on our comments on Val’s comments on p-values)</a></p>
<p>11 0.11354138 <a title="758-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-The_statistical_significance_filter.html">899 andrew gelman stats-2011-09-10-The statistical significance filter</a></p>
<p>12 0.10616582 <a title="758-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>13 0.10572621 <a title="758-tfidf-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-03-Climate_Change_News.html">180 andrew gelman stats-2010-08-03-Climate Change News</a></p>
<p>14 0.10074583 <a title="758-tfidf-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Ken_Rice_presents_a_unifying_approach_to_statistical_inference_and_hypothesis_testing.html">2312 andrew gelman stats-2014-04-29-Ken Rice presents a unifying approach to statistical inference and hypothesis testing</a></p>
<p>15 0.099919975 <a title="758-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>16 0.099229023 <a title="758-tfidf-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-27-A_whole_fleet_of_gremlins%3A__Looking_more_carefully_at_Richard_Tol%E2%80%99s_twice-corrected_paper%2C_%E2%80%9CThe_Economic_Effects_of_Climate_Change%E2%80%9D.html">2350 andrew gelman stats-2014-05-27-A whole fleet of gremlins:  Looking more carefully at Richard Tol’s twice-corrected paper, “The Economic Effects of Climate Change”</a></p>
<p>17 0.096535638 <a title="758-tfidf-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-31-%E2%80%9CThreshold_earners%E2%80%9D_and_economic_inequality.html">495 andrew gelman stats-2010-12-31-“Threshold earners” and economic inequality</a></p>
<p>18 0.092903331 <a title="758-tfidf-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-23-A_short_questionnaire_regarding_the_subjective_assessment_of_evidence.html">2302 andrew gelman stats-2014-04-23-A short questionnaire regarding the subjective assessment of evidence</a></p>
<p>19 0.090873808 <a title="758-tfidf-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-20-Some_things_are_just_really_hard_to_believe%3A_more_on_choosing_your_facts..html">219 andrew gelman stats-2010-08-20-Some things are just really hard to believe: more on choosing your facts.</a></p>
<p>20 0.089712575 <a title="758-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-08-Is_linear_regression_unethical_in_that_it_gives_more_weight_to_cases_that_are_far_from_the_average%3F.html">1409 andrew gelman stats-2012-07-08-Is linear regression unethical in that it gives more weight to cases that are far from the average?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.155), (1, 0.022), (2, -0.01), (3, -0.064), (4, -0.036), (5, -0.047), (6, -0.014), (7, -0.001), (8, 0.019), (9, -0.035), (10, -0.044), (11, -0.011), (12, 0.02), (13, -0.037), (14, 0.005), (15, 0.005), (16, 0.017), (17, 0.019), (18, -0.006), (19, -0.026), (20, 0.008), (21, 0.033), (22, -0.037), (23, -0.016), (24, -0.01), (25, -0.021), (26, 0.012), (27, -0.018), (28, 0.05), (29, -0.011), (30, 0.001), (31, 0.009), (32, -0.011), (33, -0.02), (34, -0.024), (35, 0.029), (36, -0.019), (37, -0.003), (38, -0.001), (39, 0.038), (40, -0.037), (41, 0.043), (42, -0.042), (43, 0.075), (44, 0.001), (45, -0.056), (46, -0.065), (47, -0.014), (48, -0.002), (49, 0.007)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93908387 <a title="758-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-11-Hey%2C_good_news%21__Your_p-value_just_passed_the_0.05_threshold%21.html">758 andrew gelman stats-2011-06-11-Hey, good news!  Your p-value just passed the 0.05 threshold!</a></p>
<p>Introduction: E. J. Wagenmakers writes:
  
 Here’s a link  for you.  The first sentences tell it all:

 
Climate warming since 1995 is now statistically significant, according to Phil Jones, the UK scientist targeted in the “ClimateGate” affair. Last year, he told BBC News that post-1995 warming was not significant–a statement still seen on blogs critical of the idea of man-made climate change. But another year of data has pushed the trend past the threshold usually used to assess whether trends are “real.”
 

Now I [Wagenmakers] don’t like p-values one bit, but even people who do like them must cringe when they read this. First, this apparently is a sequential design, so I’m not sure what sampling plan leads to these p-values. Secondly, comparing significance values suggests that the data have suddenly crossed some invisible line that divided nonsignificant from significant effects (as you pointed out in your paper with Hal Stern). Ugh!
  
I share Wagenmakers’s reaction.  There seems to be some con</p><p>2 0.74274945 <a title="758-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-09-The_difference_between_significant_and_not_significant%E2%80%A6.html">897 andrew gelman stats-2011-09-09-The difference between significant and not significant…</a></p>
<p>Introduction: E. J. Wagenmakers writes:
  
You may be interested in  a recent article  [by Nieuwenhuis, Forstmann, and Wagenmakers] showing how often researchers draw conclusions by comparing p-values. As you and Hal Stern have pointed out, this is potentially misleading because the difference between significant and not significant is not necessarily significant.


We were really suprised to see how often researchers in the neurosciences make this mistake. In the paper we speculate a little bit on the cause of the error.
  
From their paper:
  
In theory, a comparison of two experimental effects requires a statistical test on their difference. In practice, this comparison is often based on an incorrect procedure involving two separate tests in which researchers conclude that effects differ when one effect is significant (P < 0.05) but the other 
is not (P > 0.05). We reviewed 513 behavioral, systems and cognitive neuroscience articles in five top-ranking journals (Science, Nature, Nature Neuroscien</p><p>3 0.73935568 <a title="758-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-16-%E2%80%9CFalse-positive_psychology%E2%80%9D.html">1171 andrew gelman stats-2012-02-16-“False-positive psychology”</a></p>
<p>Introduction: Everybody’s  talkin bout  this paper by Joseph Simmons, Leif Nelson and Uri Simonsohn, who  write :
  
Despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We [Simmons, Nelson, and Simonsohn] present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.
  
Whatever you think about these recommend</p><p>4 0.72672796 <a title="758-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-With_a_bit_of_precognition%2C_you%E2%80%99d_have_known_I_was_going_to_post_again_on_this_topic%2C_and_with_a_lot_of_precognition%2C_you%E2%80%99d_have_known_I_was_going_to_post_today.html">576 andrew gelman stats-2011-02-15-With a bit of precognition, you’d have known I was going to post again on this topic, and with a lot of precognition, you’d have known I was going to post today</a></p>
<p>Introduction: Chris Masse points me to  this response  by Daryl Bem and two statisticians (Jessica Utts and Wesley Johnson) to criticisms by Wagenmakers et.al. of Bem’s recent ESP study.  I have nothing to add but would like to repeat a couple bits of my discussions of last month, of  here :
  
Classical statistical methods that work reasonably well when studying moderate or large effects (see the work of Fisher, Snedecor, Cochran, etc.) fall apart in the presence of small effects.


I think it’s naive when people implicitly assume that the study’s claims are correct, or the study’s statistical methods are weak. Generally, the smaller the effects you’re studying, the better the statistics you need. ESP is a field of small effects and so ESP researchers use high-quality statistics.


To put it another way: whatever methodological errors happen to be in the paper in question, probably occur in lots of researcher papers in “legitimate” psychology research. The difference is that when you’re studying a</p><p>5 0.71459299 <a title="758-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-25-The_harm_done_by_tests_of_significance.html">1776 andrew gelman stats-2013-03-25-The harm done by tests of significance</a></p>
<p>Introduction: After seeing  this recent discussion , Ezra Hauer sent along  an article of his  from the journal Accident Analysis and Prevention, describing three examples from accident research in which null hypothesis significance testing led researchers astray.  Hauer writes:
  
The problem is clear. Researchers obtain real data which, while noisy, time and again point in a certain direction. However, instead of saying: “here is my estimate of the safety effect, here is its precision, and this is how what I found relates to previous findings”, the data is processed by NHST, and the researcher says, correctly but pointlessly: “I cannot be sure that the safety effect is not zero”. Occasionally, the researcher adds, this time incorrectly and unjustifiably, a statement to the effect that: “since the result is not statistically significant, it is best to assume the safety effect to be zero”. In this manner, good data are drained of real content, the direction of empirical conclusions reversed, and ord</p><p>6 0.71024376 <a title="758-lsi-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-20-Burglars_are_local.html">156 andrew gelman stats-2010-07-20-Burglars are local</a></p>
<p>7 0.70803964 <a title="758-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-11-One_more_time_on_that_ESP_study%3A__The_problem_of_overestimates_and_the_shrinkage_solution.html">511 andrew gelman stats-2011-01-11-One more time on that ESP study:  The problem of overestimates and the shrinkage solution</a></p>
<p>8 0.70289248 <a title="758-lsi-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-04-%E2%80%9CDogs_are_sensitive_to_small_variations_of_the_Earth%E2%80%99s_magnetic_field%E2%80%9D.html">2159 andrew gelman stats-2014-01-04-“Dogs are sensitive to small variations of the Earth’s magnetic field”</a></p>
<p>9 0.69914991 <a title="758-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-The_statistical_significance_filter.html">899 andrew gelman stats-2011-09-10-The statistical significance filter</a></p>
<p>10 0.69862247 <a title="758-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-08-Statistical_significance_and_the_dangerous_lure_of_certainty.html">1974 andrew gelman stats-2013-08-08-Statistical significance and the dangerous lure of certainty</a></p>
<p>11 0.69639128 <a title="758-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-19-%E2%80%9CThe_difference_between_._._.%E2%80%9D%3A__It%E2%80%99s_not_just_p%3D.05_vs._p%3D.06.html">1072 andrew gelman stats-2011-12-19-“The difference between . . .”:  It’s not just p=.05 vs. p=.06</a></p>
<p>12 0.69143486 <a title="758-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-18-You%E2%80%99ll_get_a_high_Type_S_error_rate_if_you_use_classical_statistical_methods_to_analyze_data_from_underpowered_studies.html">1944 andrew gelman stats-2013-07-18-You’ll get a high Type S error rate if you use classical statistical methods to analyze data from underpowered studies</a></p>
<p>13 0.69102556 <a title="758-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-19-Revised_evidence_for_statistical_standards.html">2140 andrew gelman stats-2013-12-19-Revised evidence for statistical standards</a></p>
<p>14 0.68666667 <a title="758-lsi-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-27-A_whole_fleet_of_gremlins%3A__Looking_more_carefully_at_Richard_Tol%E2%80%99s_twice-corrected_paper%2C_%E2%80%9CThe_Economic_Effects_of_Climate_Change%E2%80%9D.html">2350 andrew gelman stats-2014-05-27-A whole fleet of gremlins:  Looking more carefully at Richard Tol’s twice-corrected paper, “The Economic Effects of Climate Change”</a></p>
<p>15 0.68644398 <a title="758-lsi-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-25-Revised_statistical_standards_for_evidence_%28comments_to_Val_Johnson%E2%80%99s_comments_on_our_comments_on_Val%E2%80%99s_comments_on_p-values%29.html">2305 andrew gelman stats-2014-04-25-Revised statistical standards for evidence (comments to Val Johnson’s comments on our comments on Val’s comments on p-values)</a></p>
<p>16 0.68575364 <a title="758-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-05-Evidence_on_the_impact_of_sustained_use_of_polynomial_regression_on_causal_inference_%28a_claim_that_coal_heating_is_reducing_lifespan_by_5_years_for_half_a_billion_people%29.html">1968 andrew gelman stats-2013-08-05-Evidence on the impact of sustained use of polynomial regression on causal inference (a claim that coal heating is reducing lifespan by 5 years for half a billion people)</a></p>
<p>17 0.68345618 <a title="758-lsi-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-23-Scientists_can_read_your_mind_._._._as_long_as_the%E2%80%99re_allowed_to_look_at_more_than_one_place_in_your_brain_and_then_make_a_prediction_after_seeing_what_you_actually_did.html">106 andrew gelman stats-2010-06-23-Scientists can read your mind . . . as long as the’re allowed to look at more than one place in your brain and then make a prediction after seeing what you actually did</a></p>
<p>18 0.68282747 <a title="758-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>19 0.67684197 <a title="758-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>20 0.67612964 <a title="758-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-06-That_silly_ESP_paper_and_some_silliness_in_a_rebuttal_as_well.html">506 andrew gelman stats-2011-01-06-That silly ESP paper and some silliness in a rebuttal as well</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.023), (12, 0.014), (15, 0.051), (16, 0.056), (24, 0.154), (25, 0.014), (27, 0.013), (28, 0.013), (31, 0.022), (39, 0.013), (43, 0.012), (53, 0.047), (55, 0.033), (65, 0.128), (68, 0.014), (89, 0.025), (96, 0.04), (99, 0.238)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.94335395 <a title="758-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-21-Don%E2%80%99t_judge_a_book_by_its_title.html">1021 andrew gelman stats-2011-11-21-Don’t judge a book by its title</a></p>
<p>Introduction: A correspondent writes:
  
I just want to spend a few words to point you to this book I have just found on Amazon:  “Understanding The New Statistics: Effect Sizes, Confidence Intervals, and Meta-Analysis” by G. Cumming.  I have been attracted by the rather unusual and ‘sexy’ title but it seems to be nothing more than an attempt at alerting the psychology community on considering point estimation procedures and confidence intervals, in place of hypothesis testing, the latter being ‘a terrible idea!’ in the author’s own words.


Some more quotes  here .  Then he says: “‘These are hardly new techniques, but I label them ‘The New Statistics’ because using them would for many researchers be quite new, as well as a highly beneficial change!’”


Of course the latter is not stated on the book cover.
  
That’s about as bad as writing a book with subtitle, “Why Americans vote the way they do,” but not actually telling the reader why Americans vote the way they do.
 
I guess what I’m saying is:</p><p>same-blog 2 0.94171476 <a title="758-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-11-Hey%2C_good_news%21__Your_p-value_just_passed_the_0.05_threshold%21.html">758 andrew gelman stats-2011-06-11-Hey, good news!  Your p-value just passed the 0.05 threshold!</a></p>
<p>Introduction: E. J. Wagenmakers writes:
  
 Here’s a link  for you.  The first sentences tell it all:

 
Climate warming since 1995 is now statistically significant, according to Phil Jones, the UK scientist targeted in the “ClimateGate” affair. Last year, he told BBC News that post-1995 warming was not significant–a statement still seen on blogs critical of the idea of man-made climate change. But another year of data has pushed the trend past the threshold usually used to assess whether trends are “real.”
 

Now I [Wagenmakers] don’t like p-values one bit, but even people who do like them must cringe when they read this. First, this apparently is a sequential design, so I’m not sure what sampling plan leads to these p-values. Secondly, comparing significance values suggests that the data have suddenly crossed some invisible line that divided nonsignificant from significant effects (as you pointed out in your paper with Hal Stern). Ugh!
  
I share Wagenmakers’s reaction.  There seems to be some con</p><p>3 0.94079286 <a title="758-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-%E2%80%9CAll_Models_are_Right%2C_Most_are_Useless%E2%80%9D.html">1197 andrew gelman stats-2012-03-04-“All Models are Right, Most are Useless”</a></p>
<p>Introduction: The above is the title of  a talk  that Thad Tarpey gave at the Joint Statistical Meetings in 2009.  Here’s the abstract:
  
Students of statistics are often introduced to George Box’s famous quote: “all models are wrong, some are useful.” In this talk I [Tarpey] argue that this quote, although useful, is wrong. A different and more positive perspective is to acknowledge that a model is simply a means of extracting information of interest from data. The truth is infinitely complex and a model is merely an approximation to the truth. If the approximation is poor or misleading, then the model is useless. In this talk I give examples of correct models that are not true models. I illustrate how the notion of a “wrong” model can lead to wrong conclusions.
  
I’m curious what he had to say—maybe he could post the slides?
 
P.S.  And  here they are !</p><p>4 0.93830693 <a title="758-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-20-One_more_time-use_graph.html">671 andrew gelman stats-2011-04-20-One more time-use graph</a></p>
<p>Introduction: Evan Hensleigh sens me this redesign of  the cross-national time use graph :
 
   
 
Here was my version:
 
   
 
And here was the original:
 
 
 
Compared to my graph, Evan’s has better fonts, and that’s important–good fonts can make a display look professional.  But I’m not sure about his other innovations.  To me, the different colors for the different time-use categories are more of a distraction than a visual aid, and I also don’t like how he made the bars fatter.  As I noted in my earlier entry, to me this draws unwanted attention to the negative space between the bars.  His country labels are slightly misaligned (particularly Japan and USA), and I really don’t like his horizontal axis at all!  He removed the units of hours and put + and – on the edges so that the axes run into each other.  What was the point of that?  It’s bad news.  Also I don’t see any advantage  at all  to the prehensile tick marks.  On the other hand, if Evgn and I were working together on such a graph, we w</p><p>5 0.93535614 <a title="758-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-23-Special_effects.html">1426 andrew gelman stats-2012-07-23-Special effects</a></p>
<p>Introduction: I just saw L’Age de Glace 4 and boy are my eyes tired.  I’m just glad it wasn’t in 3-D or I probably would’ve thrown up.  The special effects were amazing, way beyond George of the Jungle and that ilk.  Which was good, as I could only understand about 10% of the dialogue.  I’d heard about all this new animation technology but not actually seen it before.</p><p>6 0.93320602 <a title="758-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-04-Question_25_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1365 andrew gelman stats-2012-06-04-Question 25 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>7 0.9293772 <a title="758-lda-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-15-Last_word_on_Mister_P_%28for_now%29.html">2062 andrew gelman stats-2013-10-15-Last word on Mister P (for now)</a></p>
<p>8 0.92665279 <a title="758-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-A_Stan_is_Born.html">1475 andrew gelman stats-2012-08-30-A Stan is Born</a></p>
<p>9 0.92471069 <a title="758-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-09-Mister_P%3A__What%E2%80%99s_its_secret_sauce%3F.html">2056 andrew gelman stats-2013-10-09-Mister P:  What’s its secret sauce?</a></p>
<p>10 0.92462325 <a title="758-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-11-Weakly_informative_priors_for_Bayesian_nonparametric_models%3F.html">1454 andrew gelman stats-2012-08-11-Weakly informative priors for Bayesian nonparametric models?</a></p>
<p>11 0.9201107 <a title="758-lda-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-07-Whassup_with_phantom-limb_treatment%3F.html">457 andrew gelman stats-2010-12-07-Whassup with phantom-limb treatment?</a></p>
<p>12 0.91958106 <a title="758-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-14-More_on_Mister_P_and_how_it_does_what_it_does.html">2061 andrew gelman stats-2013-10-14-More on Mister P and how it does what it does</a></p>
<p>13 0.90828168 <a title="758-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-18-Psychology_experiments_to_understand_what%E2%80%99s_going_on_with_data_graphics%3F.html">1811 andrew gelman stats-2013-04-18-Psychology experiments to understand what’s going on with data graphics?</a></p>
<p>14 0.90641546 <a title="758-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-22-Improvements_to_Kindle_Version_of_BDA3.html">1993 andrew gelman stats-2013-08-22-Improvements to Kindle Version of BDA3</a></p>
<p>15 0.90458739 <a title="758-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-23-Can%E2%80%99t_Stop_Won%E2%80%99t_Stop_Mister_P_Beatdown.html">2074 andrew gelman stats-2013-10-23-Can’t Stop Won’t Stop Mister P Beatdown</a></p>
<p>16 0.90447438 <a title="758-lda-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-07-My_recent_debugging_experience.html">2161 andrew gelman stats-2014-01-07-My recent debugging experience</a></p>
<p>17 0.90046412 <a title="758-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-11-Compare_p-values_from_privately_funded_medical_trials_to_those_in_publicly_funded_research%3F.html">463 andrew gelman stats-2010-12-11-Compare p-values from privately funded medical trials to those in publicly funded research?</a></p>
<p>18 0.89499354 <a title="758-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-17-Weak_identification_provides_partial_information.html">1903 andrew gelman stats-2013-06-17-Weak identification provides partial information</a></p>
<p>19 0.89331865 <a title="758-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-03-Philosophy_of_Bayesian_statistics%3A__my_reactions_to_Senn.html">1151 andrew gelman stats-2012-02-03-Philosophy of Bayesian statistics:  my reactions to Senn</a></p>
<p>20 0.89261627 <a title="758-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-08-A_Bayesian_approach_for_peer-review_panels%3F__and_a_speculation_about_Bruno_Frey.html">2055 andrew gelman stats-2013-10-08-A Bayesian approach for peer-review panels?  and a speculation about Bruno Frey</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
