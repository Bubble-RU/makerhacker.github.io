<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>810 andrew gelman stats-2011-07-20-Adding more information can make the variance go up (depending on your model)</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-810" href="#">andrew_gelman_stats-2011-810</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>810 andrew gelman stats-2011-07-20-Adding more information can make the variance go up (depending on your model)</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-810-html" href="http://andrewgelman.com/2011/07/20/adding_more_inf/">html</a></p><p>Introduction: Andy McKenzie writes:
  
In their March 9 “ counterpoint ” in nature biotech to the prospect that we should try to integrate more sources of data in clinical practice (see “ point ” arguing for this), Isaac Kohane and David Margulies claim that,


“Finally, how much better is our new knowledge than older knowledge? When is the incremental benefit of a genomic variant(s) or gene expression profile relative to a family history or classic histopathology insufficient and when does it add rather than subtract variance?”  


Perhaps I am mistaken (thus this email), but it seems that this claim runs contra to the definition of conditional probability. That is, if you have a hierarchical model, and the family history / classical histopathology already suggests a parameter estimate with some variance, how could the new genomic info possibly increase the variance of that parameter estimate? Surely the question is how much variance the new genomic info reduces and whether it therefore justifies t</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 When is the incremental benefit of a genomic variant(s) or gene expression profile relative to a family history or classic histopathology insufficient and when does it add rather than subtract variance? [sent-2, score-1.491]
</p><p>2 ”     Perhaps I am mistaken (thus this email), but it seems that this claim runs contra to the definition of conditional probability. [sent-3, score-0.345]
</p><p>3 That is, if you have a hierarchical model, and the family history / classical histopathology already suggests a parameter estimate with some variance, how could the new genomic info possibly increase the variance of that parameter estimate? [sent-4, score-1.762]
</p><p>4 Surely the question is how much variance the new genomic info reduces and whether it therefore justifies the cost. [sent-5, score-1.086]
</p><p>5 Perhaps the authors mean the word “relative” as “replacing,” but then I don’t see why they use the word “incremental. [sent-6, score-0.216]
</p><p>6 My reply:   We consider this in chapter 2 in Bayesian Data Analysis, I think in a couple of the homework problems. [sent-8, score-0.155]
</p><p>7 The short answer is that,  in expectation , the posterior variance decreases as you get more information, but, depending on the model, in particular cases the variance can increase. [sent-9, score-1.34]
</p><p>8 For some models such as the normal and binomial, the posterior variance can only decrease. [sent-10, score-0.624]
</p><p>9 But consider the t model with low degrees of freedom (which can be interpreted as a mixture of normals with common mean and different variances). [sent-11, score-0.49]
</p><p>10 if you observe an extreme value, that’s evidence that the variance is high, and indeed your posterior variance can go up. [sent-12, score-1.186]
</p><p>11 That said, the quote above might be addressing a different issue, that of overfitting. [sent-13, score-0.088]
</p><p>12 But I always say that overfitting is not a problem of too much information, it’s a problem of a model that’s not set up to handle a given level of information. [sent-14, score-0.184]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('variance', 0.479), ('genomic', 0.354), ('histopathology', 0.259), ('info', 0.171), ('posterior', 0.145), ('mckenzie', 0.118), ('normals', 0.118), ('biotech', 0.118), ('family', 0.116), ('relative', 0.113), ('contra', 0.111), ('word', 0.108), ('history', 0.106), ('incremental', 0.103), ('prospect', 0.103), ('parameter', 0.102), ('isaac', 0.1), ('knowledge', 0.099), ('subtract', 0.095), ('integrate', 0.095), ('information', 0.093), ('model', 0.093), ('variant', 0.093), ('insufficient', 0.091), ('profile', 0.091), ('overfitting', 0.091), ('gene', 0.09), ('addressing', 0.088), ('decreases', 0.085), ('andy', 0.085), ('replacing', 0.084), ('mistaken', 0.084), ('homework', 0.083), ('binomial', 0.083), ('observe', 0.083), ('variances', 0.082), ('reduces', 0.082), ('claim', 0.082), ('march', 0.077), ('expectation', 0.077), ('depending', 0.075), ('estimate', 0.073), ('expression', 0.073), ('consider', 0.072), ('older', 0.071), ('interpreted', 0.07), ('surely', 0.069), ('freedom', 0.069), ('degrees', 0.068), ('runs', 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="810-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Adding_more_information_can_make_the_variance_go_up_%28depending_on_your_model%29.html">810 andrew gelman stats-2011-07-20-Adding more information can make the variance go up (depending on your model)</a></p>
<p>Introduction: Andy McKenzie writes:
  
In their March 9 “ counterpoint ” in nature biotech to the prospect that we should try to integrate more sources of data in clinical practice (see “ point ” arguing for this), Isaac Kohane and David Margulies claim that,


“Finally, how much better is our new knowledge than older knowledge? When is the incremental benefit of a genomic variant(s) or gene expression profile relative to a family history or classic histopathology insufficient and when does it add rather than subtract variance?”  


Perhaps I am mistaken (thus this email), but it seems that this claim runs contra to the definition of conditional probability. That is, if you have a hierarchical model, and the family history / classical histopathology already suggests a parameter estimate with some variance, how could the new genomic info possibly increase the variance of that parameter estimate? Surely the question is how much variance the new genomic info reduces and whether it therefore justifies t</p><p>2 0.2206772 <a title="810-tfidf-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-02-The_problem_of_overestimation_of_group-level_variance_parameters.html">63 andrew gelman stats-2010-06-02-The problem of overestimation of group-level variance parameters</a></p>
<p>Introduction: John Lawson writes:
  
I have been experimenting using Bayesian Methods to estimate variance components, and I have noticed that even when I use a noninformative prior, my estimates are never close to the method of moments or REML estimates. In every case I have tried, the sum of the Bayesian estimated variance components is always larger than the sum of the estimates obtained by method of moments or REML.
      
For data sets I have used that arise from a simple one-way random effects model, the Bayesian estimates of the between groups variance component is usually larger than the method of moments or REML estimates. When I use a uniform prior on the between standard deviation (as you recommended in  your 2006 paper ) rather than an inverse gamma prior on the between variance component, the between variance component is usually reduced.  However, for the dyestuff data in Davies(1949, p74), the opposite appears to be the case.


I am a worried that the Bayesian estimators of the varian</p><p>3 0.19625024 <a title="810-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>Introduction: For awhile I’ve been fitting most of my multilevel models using lmer/glmer, which gives point estimates of the group-level variance parameters (maximum marginal likelihood estimate for lmer and an approximation for glmer).  I’m usually satisfied with this–sure, point estimation understates the uncertainty in model fitting, but that’s typically the least of our worries. 
 
Sometimes, though, lmer/glmer estimates group-level variances at 0 or estimates group-level correlation parameters at +/- 1.  Typically, when this happens, it’s not that we’re so sure the variance is close to zero or that the correlation is close to 1 or -1; rather, the marginal likelihood does not provide a lot of information about these parameters of the group-level error distribution.
 
I don’t want point estimates on the boundary.  I don’t want to say that the unexplained variance in some dimension is exactly zero.
 
One way to handle this problem is full Bayes:  slap a prior on sigma, do your Gibbs and Metropolis</p><p>4 0.18246797 <a title="810-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-15-The_bias-variance_tradeoff.html">960 andrew gelman stats-2011-10-15-The bias-variance tradeoff</a></p>
<p>Introduction: Joshua Vogelstein asks for my thoughts as  a Bayesian on the above topic.  So here they are (briefly):
 
The concept of the bias-variance tradeoff can be useful if you don’t take it too seriously.  The basic idea is as follows:  if you’re estimating something, you can slice your data finer and finer, or perform more and more adjustments, each time getting a purer—and less biased—estimate.  But each subdivision or each adjustment reduces your sample size or increases potential estimation error, hence the variance of your estimate goes up.
 
That story is real.  In lots and lots of examples, there’s a continuum between a completely unadjusted general estimate (high bias, low variance) and a specific, focused, adjusted estimate (low bias, high variance).
 
Suppose, for example, you’re using data from a large experiment to estimate the effect of a treatment on a fairly narrow group, say, white men between the ages of 45 and 50.  At one extreme, you could just take the estimated treatment e</p><p>5 0.17484698 <a title="810-tfidf-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-Correlation%2C_prediction%2C_variation%2C_etc..html">301 andrew gelman stats-2010-09-28-Correlation, prediction, variation, etc.</a></p>
<p>Introduction: Hamdan Azhar writes:
  
 
I [Azhar] write with a question about language in the context of statistics. Consider the three statements below.


a) Y is significantly associated (correlated) with X;


b) knowledge of X allows us to account for __% of the variance in Y;


c) Y can be predicted to a significant extent given knowledge of X.


To what extent are these statements equivalent? Much of the (non-statistical) scientific literature doesn’t seem to distinguish between these notions. Is this just about semantics — or are there meaningful differences here, particularly between b and c?


Consider a framework where X constitutes a predictor space of p variables (x1,…,xp). We wish to generate a linear combination of these variables to yield a score that optimally correlates with Y. Can we substitute the word “predicts” for “optimally correlates with” in this context?


One can argue that “correlating” or “accounting for variance” suggests that we are trying to maximize goodness-of-fit (i</p><p>6 0.17255668 <a title="810-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-22-The_kluges_of_today_are_the_textbook_solutions_of_tomorrow..html">2143 andrew gelman stats-2013-12-22-The kluges of today are the textbook solutions of tomorrow.</a></p>
<p>7 0.16390534 <a title="810-tfidf-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-%E2%80%9CGenomics%E2%80%9D_vs._genetics.html">303 andrew gelman stats-2010-09-28-“Genomics” vs. genetics</a></p>
<p>8 0.15738274 <a title="810-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>9 0.14984991 <a title="810-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-25-Correlation_of_1_._._._too_good_to_be_true%3F.html">1737 andrew gelman stats-2013-02-25-Correlation of 1 . . . too good to be true?</a></p>
<p>10 0.14747486 <a title="810-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-06-Bayesian_Anova_found_useful_in_ecology.html">1102 andrew gelman stats-2012-01-06-Bayesian Anova found useful in ecology</a></p>
<p>11 0.14635079 <a title="810-tfidf-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-12-Finite-population_standard_deviation_in_a_hierarchical_model.html">464 andrew gelman stats-2010-12-12-Finite-population standard deviation in a hierarchical model</a></p>
<p>12 0.14569892 <a title="810-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-21-D._Buggin.html">1465 andrew gelman stats-2012-08-21-D. Buggin</a></p>
<p>13 0.14019755 <a title="810-tfidf-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-04-That_half-Cauchy_prior.html">184 andrew gelman stats-2010-08-04-That half-Cauchy prior</a></p>
<p>14 0.13860169 <a title="810-tfidf-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-02-Discovering_general_multidimensional_associations.html">2315 andrew gelman stats-2014-05-02-Discovering general multidimensional associations</a></p>
<p>15 0.13365251 <a title="810-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-09-%E2%80%9CHeterogeneity_of_variance_in_experimental_studies%3A__A_challenge_to_conventional_interpretations%E2%80%9D.html">1891 andrew gelman stats-2013-06-09-“Heterogeneity of variance in experimental studies:  A challenge to conventional interpretations”</a></p>
<p>16 0.13011034 <a title="810-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-24-Estimating_and_summarizing_inference_for_hierarchical_variance_parameters_when_the_number_of_groups_is_small.html">2145 andrew gelman stats-2013-12-24-Estimating and summarizing inference for hierarchical variance parameters when the number of groups is small</a></p>
<p>17 0.1227304 <a title="810-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>18 0.11779374 <a title="810-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-Piss-poor_monocausal_social_science.html">1196 andrew gelman stats-2012-03-04-Piss-poor monocausal social science</a></p>
<p>19 0.11744394 <a title="810-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-11-Understanding_how_estimates_change_when_you_move_to_a_multilevel_model.html">850 andrew gelman stats-2011-08-11-Understanding how estimates change when you move to a multilevel model</a></p>
<p>20 0.11307742 <a title="810-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-23-Examples_of_the_use_of_hierarchical_modeling_to_generalize_to_new_settings.html">1425 andrew gelman stats-2012-07-23-Examples of the use of hierarchical modeling to generalize to new settings</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.173), (1, 0.151), (2, 0.054), (3, -0.004), (4, 0.019), (5, -0.015), (6, 0.079), (7, -0.014), (8, -0.021), (9, 0.056), (10, -0.0), (11, 0.025), (12, 0.024), (13, 0.017), (14, -0.002), (15, 0.005), (16, -0.007), (17, -0.022), (18, 0.005), (19, 0.003), (20, 0.004), (21, -0.027), (22, 0.057), (23, -0.006), (24, 0.02), (25, 0.002), (26, -0.008), (27, 0.037), (28, 0.023), (29, -0.013), (30, -0.01), (31, 0.032), (32, -0.015), (33, -0.037), (34, 0.033), (35, 0.028), (36, -0.009), (37, -0.055), (38, 0.03), (39, 0.003), (40, 0.004), (41, -0.011), (42, -0.022), (43, 0.037), (44, -0.059), (45, -0.006), (46, 0.052), (47, -0.015), (48, -0.002), (49, -0.012)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95916468 <a title="810-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Adding_more_information_can_make_the_variance_go_up_%28depending_on_your_model%29.html">810 andrew gelman stats-2011-07-20-Adding more information can make the variance go up (depending on your model)</a></p>
<p>Introduction: Andy McKenzie writes:
  
In their March 9 “ counterpoint ” in nature biotech to the prospect that we should try to integrate more sources of data in clinical practice (see “ point ” arguing for this), Isaac Kohane and David Margulies claim that,


“Finally, how much better is our new knowledge than older knowledge? When is the incremental benefit of a genomic variant(s) or gene expression profile relative to a family history or classic histopathology insufficient and when does it add rather than subtract variance?”  


Perhaps I am mistaken (thus this email), but it seems that this claim runs contra to the definition of conditional probability. That is, if you have a hierarchical model, and the family history / classical histopathology already suggests a parameter estimate with some variance, how could the new genomic info possibly increase the variance of that parameter estimate? Surely the question is how much variance the new genomic info reduces and whether it therefore justifies t</p><p>2 0.86282325 <a title="810-lsi-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-04-That_half-Cauchy_prior.html">184 andrew gelman stats-2010-08-04-That half-Cauchy prior</a></p>
<p>Introduction: Xiaoyu Qian writes:
  
 
I have a question when I apply the half-Cauchy prior (Gelman, 2006) for the variance parameter in a hierarchical model. The model I used is a three level IRT model equivalent to a Rasch model. The variance parameter I try to estimate is at the third level. The group size ranges from 15 to 44. The data is TIMSS 2007 data.


I used the syntax provided by the paper and found that the convergence of the standard deviation term is good (sigma.theta), however, the convergence for the parameter “xi” is not very good. Does it mean the whole model has not converged? Do you have any suggestion for this situation.


I also used the uniform prior and correlate the result with the half-Cauchy result for the standard deviation term. The results correlated .99.
 

 
My reply:  It’s not a problem if xi does not converge well.  It’s |xi|*sigma that is relevant.  And, if the number of groups is large, the prior probably won’t matter so much, which would explain your 99% correlat</p><p>3 0.81935817 <a title="810-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-12-Finite-population_standard_deviation_in_a_hierarchical_model.html">464 andrew gelman stats-2010-12-12-Finite-population standard deviation in a hierarchical model</a></p>
<p>Introduction: Karri Seppa writes:
  
My topic is regional variation in the cause-specific survival of breast cancer patients across the 21 hospital districts in Finland, this component being modeled by random effects. I am interested mainly in the district-specific effects, and with a hierarchical model I can get reasonable estimates also for sparsely populated districts.


Based on the recommendation given in the book by yourself and Dr. Hill (2007) I tend to think that the finite-population variance would be an appropriate measure to summarize the overall variation across the 21 districts. However, I feel it is somewhat incoherent first to assume a Normal distribution for the district effects, involving a “superpopulation” variance parameter, and then to compute the finite-population variance from the estimated district-specific parameters. I wonder whether the finite-population variance were more appropriate in the context of a model with fixed district effects?
  
My reply:
  

 
I agree that th</p><p>4 0.80487049 <a title="810-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>Introduction: For awhile I’ve been fitting most of my multilevel models using lmer/glmer, which gives point estimates of the group-level variance parameters (maximum marginal likelihood estimate for lmer and an approximation for glmer).  I’m usually satisfied with this–sure, point estimation understates the uncertainty in model fitting, but that’s typically the least of our worries. 
 
Sometimes, though, lmer/glmer estimates group-level variances at 0 or estimates group-level correlation parameters at +/- 1.  Typically, when this happens, it’s not that we’re so sure the variance is close to zero or that the correlation is close to 1 or -1; rather, the marginal likelihood does not provide a lot of information about these parameters of the group-level error distribution.
 
I don’t want point estimates on the boundary.  I don’t want to say that the unexplained variance in some dimension is exactly zero.
 
One way to handle this problem is full Bayes:  slap a prior on sigma, do your Gibbs and Metropolis</p><p>5 0.79120827 <a title="810-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-03-Uncertainty_in_parameter_estimates_using_multilevel_models.html">1966 andrew gelman stats-2013-08-03-Uncertainty in parameter estimates using multilevel models</a></p>
<p>Introduction: David Hsu writes: 
   
 I have a (perhaps) simple question about uncertainty in parameter estimates using multilevel models — what is an appropriate threshold for measure parameter uncertainty in a multilevel model? 
 
The reason why I ask is that I set out to do a crossed two-way model with two varying intercepts, similar to your flight simulator example in your 2007 book.  The difference is that I have a lot of predictors specific to each cell (I think equivalent to airport and pilot in your example), and I find after modeling this in JAGS, I happily find that the predictors are much less important than the variability by cell (airport and pilot effects).  Happily because this is what I am writing a paper about.
 
However, I then went to check subsets of predictors using lm() and lmer().  I understand that they all use different estimation methods, but what I can’t figure out is why the errors on all of the coefficient estimates are *so* different.  
 
For example, using JAGS, and th</p><p>6 0.77541268 <a title="810-lsi-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-02-The_problem_of_overestimation_of_group-level_variance_parameters.html">63 andrew gelman stats-2010-06-02-The problem of overestimation of group-level variance parameters</a></p>
<p>7 0.76413268 <a title="810-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-03-Hierarchical_array_priors_for_ANOVA_decompositions.html">1786 andrew gelman stats-2013-04-03-Hierarchical array priors for ANOVA decompositions</a></p>
<p>8 0.76391011 <a title="810-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-21-D._Buggin.html">1465 andrew gelman stats-2012-08-21-D. Buggin</a></p>
<p>9 0.76326054 <a title="810-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>10 0.75999618 <a title="810-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-25-Correlation_of_1_._._._too_good_to_be_true%3F.html">1737 andrew gelman stats-2013-02-25-Correlation of 1 . . . too good to be true?</a></p>
<p>11 0.75826317 <a title="810-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-10-Cross-validation_and_Bayesian_estimation_of_tuning_parameters.html">2129 andrew gelman stats-2013-12-10-Cross-validation and Bayesian estimation of tuning parameters</a></p>
<p>12 0.75479162 <a title="810-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-22-The_scaled_inverse_Wishart_prior_distribution_for_a_covariance_matrix_in_a_hierarchical_model.html">1466 andrew gelman stats-2012-08-22-The scaled inverse Wishart prior distribution for a covariance matrix in a hierarchical model</a></p>
<p>13 0.74587911 <a title="810-lsi-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-Correlation%2C_prediction%2C_variation%2C_etc..html">301 andrew gelman stats-2010-09-28-Correlation, prediction, variation, etc.</a></p>
<p>14 0.73869562 <a title="810-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-24-Estimating_and_summarizing_inference_for_hierarchical_variance_parameters_when_the_number_of_groups_is_small.html">2145 andrew gelman stats-2013-12-24-Estimating and summarizing inference for hierarchical variance parameters when the number of groups is small</a></p>
<p>15 0.73201215 <a title="810-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>16 0.72893196 <a title="810-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>17 0.72746617 <a title="810-lsi-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-10-R_vs._Stata%2C_or%2C_Different_ways_to_estimate_multilevel_models.html">269 andrew gelman stats-2010-09-10-R vs. Stata, or, Different ways to estimate multilevel models</a></p>
<p>18 0.72696686 <a title="810-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<p>19 0.72573346 <a title="810-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-24-New_ideas_on_DIC_from_Martyn_Plummer_and_Sumio_Watanabe.html">778 andrew gelman stats-2011-06-24-New ideas on DIC from Martyn Plummer and Sumio Watanabe</a></p>
<p>20 0.72222453 <a title="810-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-15-The_bias-variance_tradeoff.html">960 andrew gelman stats-2011-10-15-The bias-variance tradeoff</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.023), (16, 0.056), (21, 0.254), (24, 0.211), (34, 0.017), (43, 0.017), (48, 0.012), (77, 0.027), (99, 0.259)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97366107 <a title="810-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-10-A_defense_of_Tom_Wolfe_based_on_the_impossibility_of_the_law_of_small_numbers_in_network_structure.html">1615 andrew gelman stats-2012-12-10-A defense of Tom Wolfe based on the impossibility of the law of small numbers in network structure</a></p>
<p>Introduction: A tall thin young man came to my office today to talk about one of my current pet topics:  stories and social science.  I brought up Tom Wolfe and his goal of compressing an entire city into a single novel, and how this reminded me of the psychologists Kahneman and Tversky’s concept of “the law of small numbers,” the idea that we expect any small sample to replicate all the properties of the larger population that it represents.  Strictly speaking, the law of small numbers is impossible—any small sample necessarily has its own unique features—but this is even more true if we consider network properties.
 
The average American knows about 700 people (depending on how you define “know”) and this defines a social network over the population.  Now suppose you look at a few hundred people and all their connections.  This mini-network will almost necessarily look much much sparser than the national network, as we’re removing the connections to the people not in the sample.
 
Now consider how</p><p>2 0.96670055 <a title="810-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-30-David_Hogg_on_statistics.html">1401 andrew gelman stats-2012-06-30-David Hogg on statistics</a></p>
<p>Introduction: Data analysis recipes: Fitting a model to data :
  
We go through the many considerations involved in fitting a model to data, using as an example the fit of a straight line to a set of points in a two-dimensional plane. Standard weighted least-squares fitting is only appropriate when there is a dimension along which the data points have negligible uncertainties, and another along which all the uncertainties can be described by Gaussians of known variance; these conditions are rarely met in practice. We consider cases of general, heterogeneous, and arbitrarily covariant two-dimensional uncertainties, and situations in which there are bad data (large outliers), unknown uncertainties, and unknown but expected intrinsic scatter in the linear relationship being fit. Above all we emphasize the importance of having a “generative model” for the data, even an approximate one. Once there is a generative model, the subsequent fitting is non-arbitrary because the model permits direct computation</p><p>3 0.96060115 <a title="810-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-20-The_R_code_for_those_time-use_graphs.html">672 andrew gelman stats-2011-04-20-The R code for those time-use graphs</a></p>
<p>Introduction: By popular demand, hereâ&euro;&trade;s my R script for the  time-use graphs :
  

 
 
# The data
a1 <- c(4.2,3.2,11.1,1.3,2.2,2.0)
a2 <- c(3.9,3.2,10.0,0.8,3.1,3.1)
a3 <- c(6.3,2.5,9.8,0.9,2.2,2.4)
a4 <- c(4.4,3.1,9.8,0.8,3.3,2.7)
a5 <- c(4.8,3.0,9.9,0.7,3.3,2.4)
a6 <- c(4.0,3.4,10.5,0.7,3.3,2.1)
a <- rbind(a1,a2,a3,a4,a5,a6)
avg <- colMeans (a)
avg.array <- t (array (avg, rev(dim(a))))
diff <- a - avg.array
country.name <- c("France", "Germany", "Japan", "Britain", "USA", "Turkey")

# The line plots

par (mfrow=c(2,3), mar=c(4,4,2,.5), mgp=c(2,.7,0), tck=-.02, oma=c(3,0,4,0),
  bg="gray96", fg="gray30")
for (i in 1:6){
  plot (c(1,6), c(-1,1.7), xlab="", ylab="", xaxt="n", yaxt="n",
    bty="l", type="n")
  lines (1:6, diff[i,], col="blue")
  points (1:6, diff[i,], pch=19, col="black")
  if (i>3){
    axis (1, c(1,3,5), c ("Work,\nstudy", "Eat,\nsleep",
      "Leisure"), mgp=c(2,1.5,0), tck=0, cex.axis=1.2)
    axis (1, c(2,4,6), c ("Unpaid\nwork",
      "Personal\nCare", "Other"), mgp=c(2,1.5,0),</p><p>4 0.95278466 <a title="810-lda-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-27-Neumann_update.html">432 andrew gelman stats-2010-11-27-Neumann update</a></p>
<p>Introduction: Steve Hsu, who  started off  this discussion, had some comments on  my speculations  on the personality of John von Neumann and others.  Steve writes:
  
I [Hsu] actually knew Feynman a bit when I was an undergrad, and found him to be very nice to students. Since then I have heard quite a few stories from people in theoretical physics which emphasize his nastier side, and I think in the end he was quite a complicated person like everyone else.


There are a couple of pseudo-biographies of vN, but none as high quality as, e.g., Gleick’s book on Feynman or Hodges book about Turing. (Gleick studied physics as an undergrad at Harvard, and Hodges is a PhD in mathematical physics — pretty rare backgrounds for biographers!)  For example, as mentioned on the comment thread to your post, Steve Heims wrote a book about both vN and Wiener (!),  and Norman Macrae wrote a biography of vN. Both books are worth reading, but I think neither really do him justice. The breadth of vN’s work is just too m</p><p>5 0.95204443 <a title="810-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-26-%E2%80%9CA_Vast_Graveyard_of_Undead_Theories%3A__Publication_Bias_and_Psychological_Science%E2%80%99s_Aversion_to_the_Null%E2%80%9D.html">1826 andrew gelman stats-2013-04-26-“A Vast Graveyard of Undead Theories:  Publication Bias and Psychological Science’s Aversion to the Null”</a></p>
<p>Introduction: Erin Jonaitis points us to  this article  by Christopher Ferguson and Moritz Heene, who write:
  
Publication bias remains a controversial issue in psychological science. . . . that the field often constructs arguments to block the publication and interpretation of null results and that null results may be further extinguished through questionable researcher practices. Given that science is dependent on the process of falsification, we argue that these problems reduce psychological science’s capability to have a proper mechanism for theory falsification, thus resulting in the promulgation of numerous “undead” theories that are ideologically popular but have little basis in fact.
  
They mention the infamous Daryl Bem article.  It is pretty much only because Bem’s claims are (presumably) false that they got published in a major research journal.  Had the claims been true—that is, had Bem run identical experiments, analyzed his data more carefully and objectively, and reported that the r</p><p>6 0.95072061 <a title="810-lda-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-16-Wanted%3A__Probability_distributions_for_rank_orderings.html">151 andrew gelman stats-2010-07-16-Wanted:  Probability distributions for rank orderings</a></p>
<p>7 0.94569838 <a title="810-lda-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-On_deck_this_week.html">2298 andrew gelman stats-2014-04-21-On deck this week</a></p>
<p>8 0.94556534 <a title="810-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-15-%E2%80%9C10_Things_You_Need_to_Know_About_Causal_Effects%E2%80%9D.html">1675 andrew gelman stats-2013-01-15-“10 Things You Need to Know About Causal Effects”</a></p>
<p>9 0.94552255 <a title="810-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-13-News_coverage_of_statistical_issues%E2%80%A6how_did_I_do%3F.html">514 andrew gelman stats-2011-01-13-News coverage of statistical issues…how did I do?</a></p>
<p>10 0.93262851 <a title="810-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-22-Please_stop_me_before_I_barf_again.html">1275 andrew gelman stats-2012-04-22-Please stop me before I barf again</a></p>
<p>same-blog 11 0.93231392 <a title="810-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Adding_more_information_can_make_the_variance_go_up_%28depending_on_your_model%29.html">810 andrew gelman stats-2011-07-20-Adding more information can make the variance go up (depending on your model)</a></p>
<p>12 0.93155193 <a title="810-lda-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-26-Sleazy_sock_puppet_can%E2%80%99t_stop_spamming_our_discussion_of_compressed_sensing_and_promoting_the_work_of_Xiteng_Liu.html">2306 andrew gelman stats-2014-04-26-Sleazy sock puppet can’t stop spamming our discussion of compressed sensing and promoting the work of Xiteng Liu</a></p>
<p>13 0.92906904 <a title="810-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-01-Two_Postdoc_Positions_Available_on_Bayesian_Hierarchical_Modeling.html">62 andrew gelman stats-2010-06-01-Two Postdoc Positions Available on Bayesian Hierarchical Modeling</a></p>
<p>14 0.92739618 <a title="810-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-07-Hipmunk_FAIL%3A__Graphics_without_content_is_not_enough.html">894 andrew gelman stats-2011-09-07-Hipmunk FAIL:  Graphics without content is not enough</a></p>
<p>15 0.90749228 <a title="810-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-19-The_grasshopper_wins%2C_and_Greg_Mankiw%E2%80%99s_grandmother_would_be_%E2%80%9Cshocked_and_appalled%E2%80%9D_all_over_again.html">1728 andrew gelman stats-2013-02-19-The grasshopper wins, and Greg Mankiw’s grandmother would be “shocked and appalled” all over again</a></p>
<p>16 0.90703273 <a title="810-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-13-Jim_Campbell_argues_that_Larry_Bartels%E2%80%99s_%E2%80%9CUnequal_Democracy%E2%80%9D_findings_are_not_robust.html">659 andrew gelman stats-2011-04-13-Jim Campbell argues that Larry Bartels’s “Unequal Democracy” findings are not robust</a></p>
<p>17 0.9024449 <a title="810-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-15-A_silly_paper_that_tries_to_make_fun_of_multilevel_models.html">854 andrew gelman stats-2011-08-15-A silly paper that tries to make fun of multilevel models</a></p>
<p>18 0.89838296 <a title="810-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-15-Does_quantum_uncertainty_have_a_place_in_everyday_applied_statistics%3F.html">1857 andrew gelman stats-2013-05-15-Does quantum uncertainty have a place in everyday applied statistics?</a></p>
<p>19 0.89667666 <a title="810-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-27-One_way_that_psychology_research_is_different_than_medical_research.html">433 andrew gelman stats-2010-11-27-One way that psychology research is different than medical research</a></p>
<p>20 0.89222574 <a title="810-lda-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-15-Quote_of_the_day%3A__statisticians_and_defaults.html">147 andrew gelman stats-2010-07-15-Quote of the day:  statisticians and defaults</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
