<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>851 andrew gelman stats-2011-08-12-year + (1|year)</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-851" href="#">andrew_gelman_stats-2011-851</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>851 andrew gelman stats-2011-08-12-year + (1|year)</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-851-html" href="http://andrewgelman.com/2011/08/12/year_1year/">html</a></p><p>Introduction: Ana Sequeira writes:
  
I am using a temporal data series and I am trying specifically to understand if there is a temporal trends in the occurrence of a species, for which I need to use “Year” in my models (and from what I understood from pages 244-246 [in ARM] is that factors should always be used as random effects).
      
I believe that in your book the closest example to my situation is the one shown in Figure 14.3: I also have 4 different regions in my study, states in your example are replaced by years in my study, and the x axis is a specific value for a climatic factor I am using in my analysis (IOD).


The reason why I am writing you, is because I am having troubles understanding if my variable “Year” (factor), should only be added as a random effect (1|Year) or if I should include the “Years” (used not as factor) in my models as well (Species ~ …Years + (1|Year))?


My doubt lies in the fact that I am looking for a trend and if I do not include “Years” as variable I believe</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I believe that in your book the closest example to my situation is the one shown in Figure 14. [sent-2, score-0.431]
</p><p>2 3: I also have 4 different regions in my study, states in your example are replaced by years in my study, and the x axis is a specific value for a climatic factor I am using in my analysis (IOD). [sent-3, score-0.967]
</p><p>3 The reason why I am writing you, is because I am having troubles understanding if my variable “Year” (factor), should only be added as a random effect (1|Year) or if I should include the “Years” (used not as factor) in my models as well (Species ~ …Years + (1|Year))? [sent-4, score-0.794]
</p><p>4 My doubt lies in the fact that I am looking for a trend and if I do not include “Years” as variable I believe the variance shown in the resulting random coefficients is conditional to the variables and effects used in the model, i. [sent-5, score-1.617]
</p><p>5 if I am not specifically accounting for a possible trend (linear or polynomial), would my model still give me a trustworthy answer regarding yearly trends? [sent-7, score-0.713]
</p><p>6 Also, some of my factors include only 4 and 5 levels (seasons and regions, respectively) – in which case, I understood that lmer() approximate inference is not reliable. [sent-8, score-0.688]
</p><p>7 My reply:   Yes, you can include year + (1|year). [sent-9, score-0.494]
</p><p>8 Also, you could fit using blmer/bglmer to get more stable estimates of the group-level variances. [sent-12, score-0.189]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('year', 0.28), ('temporal', 0.272), ('include', 0.214), ('factor', 0.213), ('species', 0.208), ('regions', 0.203), ('understood', 0.179), ('trend', 0.177), ('random', 0.171), ('trends', 0.162), ('shown', 0.154), ('specifically', 0.154), ('ana', 0.151), ('troubles', 0.142), ('yearly', 0.142), ('factors', 0.137), ('trustworthy', 0.136), ('climatic', 0.136), ('seasons', 0.136), ('years', 0.131), ('occurrence', 0.127), ('variable', 0.123), ('respectively', 0.114), ('lmer', 0.114), ('polynomial', 0.113), ('lies', 0.109), ('closest', 0.109), ('used', 0.109), ('variances', 0.105), ('accounting', 0.104), ('axis', 0.096), ('stable', 0.095), ('replaced', 0.094), ('believe', 0.094), ('using', 0.094), ('effects', 0.094), ('resulting', 0.089), ('arm', 0.088), ('approximate', 0.086), ('study', 0.084), ('coefficients', 0.076), ('pages', 0.076), ('models', 0.075), ('situation', 0.074), ('levels', 0.072), ('conditional', 0.071), ('added', 0.069), ('variance', 0.068), ('doubt', 0.068), ('linear', 0.067)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="851-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-12-year_%2B_%281%7Cyear%29.html">851 andrew gelman stats-2011-08-12-year + (1|year)</a></p>
<p>Introduction: Ana Sequeira writes:
  
I am using a temporal data series and I am trying specifically to understand if there is a temporal trends in the occurrence of a species, for which I need to use “Year” in my models (and from what I understood from pages 244-246 [in ARM] is that factors should always be used as random effects).
      
I believe that in your book the closest example to my situation is the one shown in Figure 14.3: I also have 4 different regions in my study, states in your example are replaced by years in my study, and the x axis is a specific value for a climatic factor I am using in my analysis (IOD).


The reason why I am writing you, is because I am having troubles understanding if my variable “Year” (factor), should only be added as a random effect (1|Year) or if I should include the “Years” (used not as factor) in my models as well (Species ~ …Years + (1|Year))?


My doubt lies in the fact that I am looking for a trend and if I do not include “Years” as variable I believe</p><p>2 0.14511204 <a title="851-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-17-Hierarchical-multilevel_modeling_with_%E2%80%9Cbig_data%E2%80%9D.html">1267 andrew gelman stats-2012-04-17-Hierarchical-multilevel modeling with “big data”</a></p>
<p>Introduction: Dean Eckles writes:
  
I make extensive use of random effects models in my academic and industry research, as they are very often appropriate.


However, with very large data sets, I am not sure what to do. Say I have thousands of levels of a grouping factor, and the number of observations totals in the billions. Despite having lots of observations, I am often either dealing with (a) small effects or (b) trying to fit models with many predictors.


So I would really like to use a random effects model to borrow strength across the levels of the grouping factor, but I am not sure how to practically do this. Are you aware of any approaches to fitting random effects models (including approximations) that work for very large data sets? For example, applying a procedure to each group, and then using the results of this to shrink each fit in some appropriate way.


Just to clarify, here I am only worried about the non-crossed and in fact single-level case. I don’t see any easy route for cross</p><p>3 0.14248878 <a title="851-tfidf-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-23-Scientists_can_read_your_mind_._._._as_long_as_the%E2%80%99re_allowed_to_look_at_more_than_one_place_in_your_brain_and_then_make_a_prediction_after_seeing_what_you_actually_did.html">106 andrew gelman stats-2010-06-23-Scientists can read your mind . . . as long as the’re allowed to look at more than one place in your brain and then make a prediction after seeing what you actually did</a></p>
<p>Introduction: Maggie Fox  writes :
  
Brain scans may be able to predict what you will do better than you can yourself . . .  They found a way to interpret “real time” brain images to show whether people who viewed messages about using sunscreen would actually use sunscreen during the following week.


The scans were more accurate than the volunteers were, Emily Falk and colleagues at the University of California Los Angeles reported in the Journal of Neuroscience. . . .


About half the volunteers had correctly predicted whether they would use sunscreen. The research team analyzed and re-analyzed the MRI scans to see if they could find any brain activity that would do better.


Activity in one area of the brain, a particular part of the medial prefrontal cortex, provided the best information.


“From this region of the brain, we can predict for about three-quarters of the people whether they will increase their use of sunscreen beyond what they say they will do,” Lieberman said.


“It is the one re</p><p>4 0.12891304 <a title="851-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-18-What_to_read_to_catch_up_on_multivariate_statistics%3F.html">1726 andrew gelman stats-2013-02-18-What to read to catch up on multivariate statistics?</a></p>
<p>Introduction: Henry Harpending writes:
  
I am writing to ask you for a recommendation of something I can read to catch up on multivariate statistics.  I am happy with random processes and linear algebra since they are important in population genetics.  My last encounter with  real  statistics was several decades ago.


Recently I have had to dip my toes into real multivariate statistics again and I am completely lost.  I can’t, for example, figure out how a random effects model is different from what we used to call “partialing out” nuisance covariates.  I have a hard time concentrating on exactly what a “BLURP” model is because the name is so silly.


Can you recommend something accessible to me that would put me on track?
  
My reply:  if you’re interested particularly in random effects models, I will (parochially) refer you to  my own book  with Jennifer Hill.  You can jump straight to the chapters on multilevel modeling.
 
If the question is about traditional multivariate methods such as factor</p><p>5 0.12573448 <a title="851-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-03-How_best_to_compare_effects_measured_in_two_different_time_periods%3F.html">2086 andrew gelman stats-2013-11-03-How best to compare effects measured in two different time periods?</a></p>
<p>Introduction: I received the following email from someone who wishes to remain anonymous:
  
My colleague and I are trying to understand the best way to approach a problem involving measuring a group of individuals’ abilities across time, and are hoping you can offer some guidance.


We are trying to analyze the combined effect of two distinct groups of people (A and B, with no overlap between A and B) who collaborate to produce a binary outcome, using a mixed logistic regression along the lines of the following.


Outcome ~ (1 | A) + (1 | B) + Other variables


What we’re interested in testing was whether the observed A random effects in period  1 are predictive of the A random effects in the following period 2.  Our idea being create two models, each using a different period’s worth of data, to create two sets of A coefficients, then observe the relationship between the two.  If the A’s have a persistent ability across periods, the coefficients should be correlated or show a linear-ish relationshi</p><p>6 0.12341961 <a title="851-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-03-Hierarchical_array_priors_for_ANOVA_decompositions.html">1786 andrew gelman stats-2013-04-03-Hierarchical array priors for ANOVA decompositions</a></p>
<p>7 0.1232246 <a title="851-tfidf-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-18-Economic_Disparities_and_Life_Satisfaction_in_European_Regions.html">97 andrew gelman stats-2010-06-18-Economic Disparities and Life Satisfaction in European Regions</a></p>
<p>8 0.12067072 <a title="851-tfidf-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-17-Clutering_and_variance_components.html">417 andrew gelman stats-2010-11-17-Clutering and variance components</a></p>
<p>9 0.11946695 <a title="851-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-07-Inference_%3D_data_%2B_model.html">1201 andrew gelman stats-2012-03-07-Inference = data + model</a></p>
<p>10 0.11761376 <a title="851-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>11 0.11725619 <a title="851-tfidf-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-24-%E2%80%9CEdlin%E2%80%99s_rule%E2%80%9D_for_routinely_scaling_down_published_estimates.html">2223 andrew gelman stats-2014-02-24-“Edlin’s rule” for routinely scaling down published estimates</a></p>
<p>12 0.11647694 <a title="851-tfidf-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-21-Random_matrices_in_the_news.html">2258 andrew gelman stats-2014-03-21-Random matrices in the news</a></p>
<p>13 0.11357017 <a title="851-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-06-Bayesian_Anova_found_useful_in_ecology.html">1102 andrew gelman stats-2012-01-06-Bayesian Anova found useful in ecology</a></p>
<p>14 0.10878407 <a title="851-tfidf-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-31-Somewhat_Bayesian_multilevel_modeling.html">246 andrew gelman stats-2010-08-31-Somewhat Bayesian multilevel modeling</a></p>
<p>15 0.10556082 <a title="851-tfidf-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-19-Analysis_of_survey_data%3A_Design_based_models_vs._hierarchical_modeling%3F.html">352 andrew gelman stats-2010-10-19-Analysis of survey data: Design based models vs. hierarchical modeling?</a></p>
<p>16 0.10106467 <a title="851-tfidf-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-17-Death%21.html">962 andrew gelman stats-2011-10-17-Death!</a></p>
<p>17 0.10058478 <a title="851-tfidf-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-29-Brain_Structure_and_the_Big_Five.html">490 andrew gelman stats-2010-12-29-Brain Structure and the Big Five</a></p>
<p>18 0.098323032 <a title="851-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-04-A_new_R_package_for_fititng_multilevel_models.html">501 andrew gelman stats-2011-01-04-A new R package for fititng multilevel models</a></p>
<p>19 0.096940465 <a title="851-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-06-Comparing_people_from_two_surveys%2C_one_of_which_is_a_simple_random_sample_and_one_of_which_is_not.html">1523 andrew gelman stats-2012-10-06-Comparing people from two surveys, one of which is a simple random sample and one of which is not</a></p>
<p>20 0.09660168 <a title="851-tfidf-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-17-So-called_fixed_and_random_effects.html">472 andrew gelman stats-2010-12-17-So-called fixed and random effects</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.181), (1, 0.09), (2, 0.087), (3, -0.022), (4, 0.1), (5, -0.011), (6, -0.016), (7, -0.018), (8, 0.072), (9, 0.074), (10, 0.008), (11, 0.011), (12, 0.008), (13, -0.024), (14, 0.068), (15, 0.036), (16, 0.017), (17, 0.011), (18, 0.015), (19, 0.014), (20, -0.026), (21, 0.03), (22, -0.003), (23, -0.013), (24, 0.004), (25, -0.02), (26, -0.08), (27, 0.028), (28, 0.008), (29, -0.013), (30, -0.032), (31, 0.008), (32, -0.06), (33, -0.011), (34, -0.01), (35, -0.014), (36, -0.027), (37, 0.045), (38, 0.016), (39, 0.006), (40, -0.02), (41, -0.01), (42, -0.006), (43, 0.057), (44, -0.026), (45, -0.045), (46, -0.032), (47, 0.022), (48, -0.0), (49, -0.008)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98158389 <a title="851-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-12-year_%2B_%281%7Cyear%29.html">851 andrew gelman stats-2011-08-12-year + (1|year)</a></p>
<p>Introduction: Ana Sequeira writes:
  
I am using a temporal data series and I am trying specifically to understand if there is a temporal trends in the occurrence of a species, for which I need to use “Year” in my models (and from what I understood from pages 244-246 [in ARM] is that factors should always be used as random effects).
      
I believe that in your book the closest example to my situation is the one shown in Figure 14.3: I also have 4 different regions in my study, states in your example are replaced by years in my study, and the x axis is a specific value for a climatic factor I am using in my analysis (IOD).


The reason why I am writing you, is because I am having troubles understanding if my variable “Year” (factor), should only be added as a random effect (1|Year) or if I should include the “Years” (used not as factor) in my models as well (Species ~ …Years + (1|Year))?


My doubt lies in the fact that I am looking for a trend and if I do not include “Years” as variable I believe</p><p>2 0.81599665 <a title="851-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-03-How_best_to_compare_effects_measured_in_two_different_time_periods%3F.html">2086 andrew gelman stats-2013-11-03-How best to compare effects measured in two different time periods?</a></p>
<p>Introduction: I received the following email from someone who wishes to remain anonymous:
  
My colleague and I are trying to understand the best way to approach a problem involving measuring a group of individuals’ abilities across time, and are hoping you can offer some guidance.


We are trying to analyze the combined effect of two distinct groups of people (A and B, with no overlap between A and B) who collaborate to produce a binary outcome, using a mixed logistic regression along the lines of the following.


Outcome ~ (1 | A) + (1 | B) + Other variables


What we’re interested in testing was whether the observed A random effects in period  1 are predictive of the A random effects in the following period 2.  Our idea being create two models, each using a different period’s worth of data, to create two sets of A coefficients, then observe the relationship between the two.  If the A’s have a persistent ability across periods, the coefficients should be correlated or show a linear-ish relationshi</p><p>3 0.79519778 <a title="851-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-17-Clutering_and_variance_components.html">417 andrew gelman stats-2010-11-17-Clutering and variance components</a></p>
<p>Introduction: Raymond Lim writes:
  
Do you have any recommendations on clustering and binary models? My particular problem is I’m running a firm fixed effect logit and want to cluster by industry-year (every combination of industry-year). My control variable of interest in measured by industry-year and when I cluster by industry-year, the standard errors are 300x larger than when I don’t cluster. Strangely, this problem only occurs when doing logit and not OLS (linear probability). Also, clustering just by field doesn’t blow up the errors. My hunch is it has something to do with the non-nested structure of year, but I don’t understand why this is only problematic under logit and not OLS.
  
My reply:
 
I’d recommend including four multilevel variance parameters, one for firm, one for industry, one for year, and one for industry-year.  (In lmer, that’s (1 | firm) + (1 | industry) + (1 | year) + (1 | industry.year)).  No need to include (1 | firm.year) since in your data this is the error term.  Try</p><p>4 0.77433616 <a title="851-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-17-Hierarchical-multilevel_modeling_with_%E2%80%9Cbig_data%E2%80%9D.html">1267 andrew gelman stats-2012-04-17-Hierarchical-multilevel modeling with “big data”</a></p>
<p>Introduction: Dean Eckles writes:
  
I make extensive use of random effects models in my academic and industry research, as they are very often appropriate.


However, with very large data sets, I am not sure what to do. Say I have thousands of levels of a grouping factor, and the number of observations totals in the billions. Despite having lots of observations, I am often either dealing with (a) small effects or (b) trying to fit models with many predictors.


So I would really like to use a random effects model to borrow strength across the levels of the grouping factor, but I am not sure how to practically do this. Are you aware of any approaches to fitting random effects models (including approximations) that work for very large data sets? For example, applying a procedure to each group, and then using the results of this to shrink each fit in some appropriate way.


Just to clarify, here I am only worried about the non-crossed and in fact single-level case. I don’t see any easy route for cross</p><p>5 0.76906425 <a title="851-lsi-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-10-R_vs._Stata%2C_or%2C_Different_ways_to_estimate_multilevel_models.html">269 andrew gelman stats-2010-09-10-R vs. Stata, or, Different ways to estimate multilevel models</a></p>
<p>Introduction: Cyrus writes:
  
I [Cyrus] was teaching a class on multilevel modeling, and we were playing around with different method to fit a random effects logit model with 2 random intercepts—one corresponding to “family” and another corresponding to “community” (labeled “mom” and “cluster” in the data, respectively).  There are also a few regressors at the individual, family, and community level.  We were replicating in part some of the results from the  following paper :  Improved estimation procedures for multilevel models with binary response: a case-study, by G Rodriguez, N Goldman.


(I say “replicating in part” because we didn’t include all the regressors that they use, only a subset.)  We were looking at the performance of estimation via glmer in R’s lme4 package, glmmPQL in R’s MASS package, and Stata’s xtmelogit.  We wanted to study the performance of various estimation methods, including adaptive quadrature methods and penalized quasi-likelihood.


I was shocked to discover that glmer</p><p>6 0.75324821 <a title="851-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-Multilevel_modeling_even_when_you%E2%80%99re_not_interested_in_predictions_for_new_groups.html">1194 andrew gelman stats-2012-03-04-Multilevel modeling even when you’re not interested in predictions for new groups</a></p>
<p>7 0.75230771 <a title="851-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-22-Handling_multiple_versions_of_an_outcome_variable.html">726 andrew gelman stats-2011-05-22-Handling multiple versions of an outcome variable</a></p>
<p>8 0.7509582 <a title="851-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>9 0.74408567 <a title="851-lsi-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-19-Index_or_indicator_variables.html">2296 andrew gelman stats-2014-04-19-Index or indicator variables</a></p>
<p>10 0.7387926 <a title="851-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-24-Estimating_and_summarizing_inference_for_hierarchical_variance_parameters_when_the_number_of_groups_is_small.html">2145 andrew gelman stats-2013-12-24-Estimating and summarizing inference for hierarchical variance parameters when the number of groups is small</a></p>
<p>11 0.7382834 <a title="851-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-06-17_groups%2C_6_group-level_predictors%3A__What_to_do%3F.html">1248 andrew gelman stats-2012-04-06-17 groups, 6 group-level predictors:  What to do?</a></p>
<p>12 0.73730636 <a title="851-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-03-Uncertainty_in_parameter_estimates_using_multilevel_models.html">1966 andrew gelman stats-2013-08-03-Uncertainty in parameter estimates using multilevel models</a></p>
<p>13 0.73612219 <a title="851-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-21-Finite-population_Anova_calculations_for_models_with_interactions.html">1686 andrew gelman stats-2013-01-21-Finite-population Anova calculations for models with interactions</a></p>
<p>14 0.73140466 <a title="851-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-11-%E2%80%9C2_level_logit_with_2_REs_%26_large_sample._computational_nightmare_%E2%80%93_please_help%E2%80%9D.html">759 andrew gelman stats-2011-06-11-“2 level logit with 2 REs & large sample. computational nightmare – please help”</a></p>
<p>15 0.73026317 <a title="851-lsi-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-12-Finite-population_standard_deviation_in_a_hierarchical_model.html">464 andrew gelman stats-2010-12-12-Finite-population standard deviation in a hierarchical model</a></p>
<p>16 0.72876853 <a title="851-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-11-Understanding_how_estimates_change_when_you_move_to_a_multilevel_model.html">850 andrew gelman stats-2011-08-11-Understanding how estimates change when you move to a multilevel model</a></p>
<p>17 0.72814763 <a title="851-lsi-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Interactions_of_predictors_in_a_causal_model.html">251 andrew gelman stats-2010-09-02-Interactions of predictors in a causal model</a></p>
<p>18 0.72149199 <a title="851-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-26-Including_interactions_or_not.html">823 andrew gelman stats-2011-07-26-Including interactions or not</a></p>
<p>19 0.71832132 <a title="851-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-08-Multilevel_regression_with_shrinkage_for_%E2%80%9Cfixed%E2%80%9D_effects.html">653 andrew gelman stats-2011-04-08-Multilevel regression with shrinkage for “fixed” effects</a></p>
<p>20 0.71611375 <a title="851-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Allowing_interaction_terms_to_vary.html">753 andrew gelman stats-2011-06-09-Allowing interaction terms to vary</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.057), (5, 0.043), (6, 0.165), (16, 0.097), (24, 0.075), (76, 0.013), (85, 0.023), (86, 0.056), (99, 0.362)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96731853 <a title="851-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-21-Busted%21.html">221 andrew gelman stats-2010-08-21-Busted!</a></p>
<p>Introduction: I’m just glad that universities don’t  sanction  professors for publishing false theorems.
 
If the guy really is nailed by the feds for fraud, I hope they don’t throw him in prison.  In general, prison time seems like a brutal, expensive, and inefficient way to punish people.  I’d prefer if the government just took 95% of his salary for several years, made him do community service (cleaning equipment at the local sewage treatment plant, perhaps; a lab scientist should be good at this sort of thing, no?), etc.  If restriction of this dude’s personal freedom is judged be part of the sentence, he could be given some sort of electronic tag that would send a message to the police if he were ever more than 3 miles from his home.  But no need to bill the taxpayers for the cost of keeping him in prison.</p><p>2 0.96257854 <a title="851-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-19-%E2%80%9CBehind_a_cancer-treatment_firm%E2%80%99s_rosy_survival_claims%E2%80%9D.html">1906 andrew gelman stats-2013-06-19-“Behind a cancer-treatment firm’s rosy survival claims”</a></p>
<p>Introduction: Brett Keller points to a recent  news article  by Sharon Begley and Robin Respaut:
  
A lot of doctors, hospitals and other healthcare providers in the United States decline to treat people who can’t pay, or have inadequate insurance, among other reasons. What sets CTCA [Cancer Treatment Centers of America] apart is that rejecting certain patients and, even more, culling some of its patients from its survival data lets the company tout in ads and post on its website patient outcomes that look dramatically better than they would if the company treated all comers. These are the rosy survival numbers . . .
  
Details:
  
CTCA reports on its website that the percentage of its patients who are alive after six months, a year, 18 months and longer regularly tops national figures. For instance, 60 percent of its non-small-cell lung cancer patients are alive at six months, CTCA says, compared to 38 percent nationally. And 64 percent of its prostate cancer patients are alive at three years, vers</p><p>3 0.95487112 <a title="851-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-24-Don%E2%80%99t_idealize_%E2%80%9Crisk_aversion%E2%80%9D.html">819 andrew gelman stats-2011-07-24-Don’t idealize “risk aversion”</a></p>
<p>Introduction: Richard Thaler writes (click  here  and search on Thaler):
  
Both risk and risk aversion are concepts that were once well defined, but are now in danger of becoming Aetherized [this is Thaler's term for adding free parameters to a model to make it work, thus destroying the purity and much of the value of the original model]. Stocks that earn surprisingly high returns are labeled as risky, because in the theory, excess returns must be accompanied by higher risk. If, inconveniently, the traditional measures of risk such as variance or covariance with the market are not high, then the Aetherists tell us there must be some other risk; we just don’t know what it is.


Similarly, traditionally the concept of risk aversion was taken to be a primitive; each person had a parameter, gamma, that measured her degree of risk aversion. Now risk aversion is allowed to be time varying, and Aetherists can say with a straight face that the market crashes of 2001 and 2008 were caused by sudden increases</p><p>same-blog 4 0.95258474 <a title="851-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-12-year_%2B_%281%7Cyear%29.html">851 andrew gelman stats-2011-08-12-year + (1|year)</a></p>
<p>Introduction: Ana Sequeira writes:
  
I am using a temporal data series and I am trying specifically to understand if there is a temporal trends in the occurrence of a species, for which I need to use “Year” in my models (and from what I understood from pages 244-246 [in ARM] is that factors should always be used as random effects).
      
I believe that in your book the closest example to my situation is the one shown in Figure 14.3: I also have 4 different regions in my study, states in your example are replaced by years in my study, and the x axis is a specific value for a climatic factor I am using in my analysis (IOD).


The reason why I am writing you, is because I am having troubles understanding if my variable “Year” (factor), should only be added as a random effect (1|Year) or if I should include the “Years” (used not as factor) in my models as well (Species ~ …Years + (1|Year))?


My doubt lies in the fact that I am looking for a trend and if I do not include “Years” as variable I believe</p><p>5 0.95095867 <a title="851-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-03-Kuhn%2C_1-f_noise%2C_and_the_fractal_nature_of_scientific_revolutions.html">1924 andrew gelman stats-2013-07-03-Kuhn, 1-f noise, and the fractal nature of scientific revolutions</a></p>
<p>Introduction: Bill Harris writes:
  
I was re-reading your and Shalizi’s  “Philosophy and the practice of Bayesian statistics”  [see also the  rejoinder ] and noticed a statement near the end of section 6 about paradigm shifts coming in different magnitudes over different time spans.  That reminded me of the almost-mystical ideas surrounding 1/f (f being frequency”) noise in some areas — the notion that almost everything exhibits that effect, and that effect extends to arbitrarily low f.  (I sense the idea only gets mystical when f gets low enough so that the event that may happen stochastically is really big—say, you model the height of waves in the Atlantic as 1/f and discover that, at some low frequency, Bermuda becomes submerged.  In other words, does the same mechanism that accounts for physical vibrations in the range of Hertz also account for the creation and destruction of islands that may occur in the range of reciprocal centuries?)


When I first encountered 1/f noise in the area of electr</p><p>6 0.94812059 <a title="851-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-06-The_new_Stan_1.1.1%2C_featuring_Gaussian_processes%21.html">1710 andrew gelman stats-2013-02-06-The new Stan 1.1.1, featuring Gaussian processes!</a></p>
<p>7 0.94369048 <a title="851-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-05-Monitor_the_efficiency_of_your_Markov_chain_sampler_using_expected_squared_jumped_distance%21.html">650 andrew gelman stats-2011-04-05-Monitor the efficiency of your Markov chain sampler using expected squared jumped distance!</a></p>
<p>8 0.94046104 <a title="851-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-18-Prior_information_._._._about_the_likelihood.html">618 andrew gelman stats-2011-03-18-Prior information . . . about the likelihood</a></p>
<p>9 0.940189 <a title="851-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-15-%E2%80%9CI_coach_the_jumpers_here_at_Boise_State_._._.%E2%80%9D.html">1625 andrew gelman stats-2012-12-15-“I coach the jumpers here at Boise State . . .”</a></p>
<p>10 0.93899 <a title="851-lda-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-16-Gaydar_update%3A__Additional_research_on_estimating_small_fractions_of_the_population.html">150 andrew gelman stats-2010-07-16-Gaydar update:  Additional research on estimating small fractions of the population</a></p>
<p>11 0.92800665 <a title="851-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-31-%E2%80%9Cthe_forces_of_native_stupidity_reinforced_by_that_blind_hostility_to_criticism%2C_reform%2C_new_ideas_and_superior_ability_which_is_human_as_well_as_academic_nature%E2%80%9D.html">1148 andrew gelman stats-2012-01-31-“the forces of native stupidity reinforced by that blind hostility to criticism, reform, new ideas and superior ability which is human as well as academic nature”</a></p>
<p>12 0.91925108 <a title="851-lda-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-12-%E2%80%9CThe_results_%28not_shown%29_._._.%E2%80%9D.html">2332 andrew gelman stats-2014-05-12-“The results (not shown) . . .”</a></p>
<p>13 0.91524047 <a title="851-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-07-Evaluating_predictions_of_political_events.html">563 andrew gelman stats-2011-02-07-Evaluating predictions of political events</a></p>
<p>14 0.91104257 <a title="851-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-08-Is_linear_regression_unethical_in_that_it_gives_more_weight_to_cases_that_are_far_from_the_average%3F.html">1409 andrew gelman stats-2012-07-08-Is linear regression unethical in that it gives more weight to cases that are far from the average?</a></p>
<p>15 0.9104318 <a title="851-lda-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-09-San_Fernando_Valley_cityscapes%3A__An_example_of_the_benefits_of_fractal_devastation%3F.html">2165 andrew gelman stats-2014-01-09-San Fernando Valley cityscapes:  An example of the benefits of fractal devastation?</a></p>
<p>16 0.90965414 <a title="851-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-09-Commercial_Bayesian_inference_software_is_popping_up_all_over.html">1489 andrew gelman stats-2012-09-09-Commercial Bayesian inference software is popping up all over</a></p>
<p>17 0.90810758 <a title="851-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-08-The_China_Study%3A__fact_or_fallacy%3F.html">263 andrew gelman stats-2010-09-08-The China Study:  fact or fallacy?</a></p>
<p>18 0.90210688 <a title="851-lda-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-10-What_property_is_important_in_a_risk_prediction_model%3F_Discrimination_or_calibration%3F.html">2328 andrew gelman stats-2014-05-10-What property is important in a risk prediction model? Discrimination or calibration?</a></p>
<p>19 0.89936471 <a title="851-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-18-Predictive_checks_for_hierarchical_models.html">154 andrew gelman stats-2010-07-18-Predictive checks for hierarchical models</a></p>
<p>20 0.89934319 <a title="851-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-28-Using_randomized_incentives_as_an_instrument_for_survey_nonresponse%3F.html">2152 andrew gelman stats-2013-12-28-Using randomized incentives as an instrument for survey nonresponse?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
