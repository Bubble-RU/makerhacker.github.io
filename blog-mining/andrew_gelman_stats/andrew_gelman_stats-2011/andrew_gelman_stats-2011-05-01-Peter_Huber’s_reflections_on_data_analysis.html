<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>690 andrew gelman stats-2011-05-01-Peter Huber’s reflections on data analysis</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-690" href="#">andrew_gelman_stats-2011-690</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>690 andrew gelman stats-2011-05-01-Peter Huber’s reflections on data analysis</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-690-html" href="http://andrewgelman.com/2011/05/01/peter_hubers_th/">html</a></p><p>Introduction: Peter Huber’s most famous work derives from  his paper  on robust statistics published nearly fifty years ago in which he introduced the concept of M-estimation (a generalization of maximum likelihood) to unify some ideas of Tukey and others for estimation procedures that were relatively insensitive to small departures from the assumed model.
 
Huber has in many ways been ahead of his time.  While remaining connected to the theoretical ideas from the early part of his career, his interests have shifted to computational and graphical statistics.  I never took Huber’s class on data analysis–he left Harvard while I was still in graduate school–but fortunately I have an opportunity to learn his lessons now, as he has just released a book, “Data Analysis:  What Can Be Learned from the Past 50 Years.”
 
The book puts together a few articles published in the past 15 years, along with some new material.  Many of the examples are decades old, which is appropriate given that Huber is reviewing f</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I never took Huber’s class on data analysis–he left Harvard while I was still in graduate school–but fortunately I have an opportunity to learn his lessons now, as he has just released a book, “Data Analysis:  What Can Be Learned from the Past 50 Years. [sent-4, score-0.193]
</p><p>2 The radon study is 15 years old, the data from the redistricting study are from the 1960s and 1970s, and so on. [sent-11, score-0.287]
</p><p>3 So at this point in my career I’d like to make a virtue of necessity and say that it’s  just fine  to work with old examples that we really understand. [sent-13, score-0.239]
</p><p>4 He also has worked on various graphical methods for data exploration and dimension reduction; although I have not used these programs myself, I view them as close in spirit to the graphical tools that we now use to explore our data in the context of our fitted models. [sent-17, score-0.426]
</p><p>5 Right now, data analysis seems dominated by three approaches:  - Machine learning  - Bayes  - Graphical exploratory data analysis  with some overlap, of course. [sent-18, score-0.532]
</p><p>6 ”   I like Huber’s pluralistic perspective, which ranges from contamination models to object-oriented programming, from geophysics to data cleaning. [sent-22, score-0.201]
</p><p>7 His is not a book to turn to for specific advice; rather, I enjoyed reading his thoughts on a variety of statistical issues and reflecting upon the connections between Huber’s strategies for data analysis and his better-known theoretical work. [sent-23, score-0.413]
</p><p>8 Within orthodox Bayesian statistics, we cannot even address the question whether a model Mi, under consideration at stage i of the investigation, is consonant with the data y. [sent-38, score-0.249]
</p><p>9 Also please see chapter 6 of Bayesian Data Analysis and  my article , “A Bayesian formulation of exploratory data analysis and goodness-of-fit testing,” which appeared in the International Statistical Review in 2003. [sent-41, score-0.407]
</p><p>10 (Huber’s chapter 5 was written in 2000 so too soon for my 2003 paper, but the first edition of our book and our paper on posterior predictive checks had already appeared several years before. [sent-42, score-0.192]
</p><p>11 I like what Huber writes about approximately specified models, and I think he’d be very comfortable with our formulation of Bayesian data analysis, from the very first page of our book, as comprising three steps:  (1) Model building, (2) Inference, (3) Model checking. [sent-46, score-0.232]
</p><p>12 For example, the schizophrenics’ reaction time example (featured in the mixture-modeling chapter of Bayesian Data Analysis), we used the model Don recommended of a mixture of normal distributions with a fixed lag between them. [sent-57, score-0.199]
</p><p>13 Looking at the data and thinking about the phenomenon, a fixed lag didn’t make sense to me, but Don emphasized that the psychology researchers were interested in an average difference and so it didn’t make sense in his perspective to try to do any further modeling on these data. [sent-58, score-0.412]
</p><p>14 He said that if we wanted to model the variation of the lag, that would be fine but it would make sense to gather more data rather than knocking ourselves out on this particular small data set. [sent-59, score-0.677]
</p><p>15 We’re often torn between modeling the raw raw data or modeling the processed data. [sent-79, score-0.435]
</p><p>16 The latter choice can throw away important information but has the advantage, not only of computational convenience but also, sometimes, conceptual simplicity:  processed data are typically closer to the form of the scientific concepts being modeled. [sent-80, score-0.197]
</p><p>17 For example, an economist might prefer to analyze some sort of preprocessed price data rather than data on individual transactions. [sent-81, score-0.359]
</p><p>18 8, Huber writes:    We found (through exploratory data analysis of a large environmental data set) that very high radon levels were tightly localized and occurred in houses sitting on the locations of old mine shafts. [sent-87, score-0.66]
</p><p>19 ”  Random samples would have been useless, too:  either one would have missed the exceptional values altogether, or one would have thrown them out as outliers. [sent-92, score-0.192]
</p><p>20 More generally, methods such as singular value decomposition and principal components analyses have their limitations–they can work fine for balanced data such as in this example but in more complicated problems I’d go with item-response or ideal-point models. [sent-109, score-0.322]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('huber', 0.747), ('data', 0.143), ('radon', 0.094), ('phil', 0.089), ('book', 0.086), ('analysis', 0.085), ('lag', 0.078), ('exploratory', 0.076), ('raw', 0.073), ('impart', 0.073), ('preprocessed', 0.073), ('graphical', 0.07), ('houses', 0.069), ('fine', 0.068), ('examples', 0.068), ('tukey', 0.068), ('student', 0.067), ('isp', 0.066), ('model', 0.065), ('statistician', 0.065), ('gather', 0.063), ('bayesian', 0.06), ('section', 0.058), ('models', 0.058), ('singular', 0.057), ('statistical', 0.057), ('judgment', 0.056), ('chapter', 0.056), ('knocking', 0.054), ('decomposition', 0.054), ('processed', 0.054), ('career', 0.053), ('samples', 0.051), ('perspective', 0.051), ('lessons', 0.05), ('years', 0.05), ('old', 0.05), ('approaches', 0.049), ('would', 0.047), ('formulation', 0.047), ('sense', 0.047), ('fifty', 0.047), ('modeling', 0.046), ('overlap', 0.044), ('advice', 0.042), ('theoretical', 0.042), ('inference', 0.042), ('comfortable', 0.042), ('statistics', 0.041), ('stage', 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="690-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-01-Peter_Huber%E2%80%99s_reflections_on_data_analysis.html">690 andrew gelman stats-2011-05-01-Peter Huber’s reflections on data analysis</a></p>
<p>Introduction: Peter Huber’s most famous work derives from  his paper  on robust statistics published nearly fifty years ago in which he introduced the concept of M-estimation (a generalization of maximum likelihood) to unify some ideas of Tukey and others for estimation procedures that were relatively insensitive to small departures from the assumed model.
 
Huber has in many ways been ahead of his time.  While remaining connected to the theoretical ideas from the early part of his career, his interests have shifted to computational and graphical statistics.  I never took Huber’s class on data analysis–he left Harvard while I was still in graduate school–but fortunately I have an opportunity to learn his lessons now, as he has just released a book, “Data Analysis:  What Can Be Learned from the Past 50 Years.”
 
The book puts together a few articles published in the past 15 years, along with some new material.  Many of the examples are decades old, which is appropriate given that Huber is reviewing f</p><p>2 0.15429997 <a title="690-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>Introduction: Robert Bell pointed me to  this post  by Brad De Long on Bayesian statistics, and then I also noticed  this  from Noah Smith, who wrote:
  
My impression is that although the Bayesian/Frequentist debate is interesting and intellectually fun, there’s really not much “there” there… despite being so-hip-right-now, Bayesian is not the Statistical Jesus.
  
I’m happy to see the discussion going in this direction.  Twenty-five years ago or so, when I got into this biz, there were some serious anti-Bayesian attitudes floating around in mainstream statistics.  Discussions in the journals sometimes devolved into debates of the form, “Bayesians:  knaves or fools?”.  You’d get all sorts of free-floating skepticism about any prior distribution at all, even while people were accepting without question (and doing theory on) logistic regressions, proportional hazards models, and all sorts of strong strong models.  (In the subfield of survey sampling, various prominent researchers would refuse to mode</p><p>3 0.14869106 <a title="690-tfidf-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-25-Classics_of_statistics.html">109 andrew gelman stats-2010-06-25-Classics of statistics</a></p>
<p>Introduction: Christian Robert is  planning  a graduate seminar in which students read 15 classic articles of statistics.  (See  here  for more details and a slightly different list.)
 
Actually, he just writes “classics,” but based on his list, I assume he only wants articles, not books.  If he wanted to include classic books, I’d nominate the following, just for starters: 
- Fisher’s Statistical Methods for Research Workers 
- Snedecor and Cochran’s Statistical Methods 
- Kish’s Survey Sampling 
- Box, Hunter, and Hunter’s Statistics for Experimenters 
- Tukey’s Exploratory Data Analysis 
- Cleveland’s The Elements of Graphing Data 
- Mosteller and Wallace’s book on the Federalist Papers. 
Probably Cox and Hinkley, too.  That’s a book that I don’t think has aged well, but it seems to have had a big influence.
 
I think there’s a lot more good and accessible material in these classic books than in the equivalent volume of classic articles.  Journal articles can be difficult to read and are typicall</p><p>4 0.14687394 <a title="690-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-04-Model_checking_and_model_understanding_in_machine_learning.html">1482 andrew gelman stats-2012-09-04-Model checking and model understanding in machine learning</a></p>
<p>Introduction: Last month I  wrote :
  
Computer scientists are often brilliant but they can be unfamiliar with what is done in the worlds of data collection and analysis. This goes the other way too: statisticians such as myself can look pretty awkward, reinventing (or failing to reinvent) various wheels when we write computer programs or, even worse, try to design software.Andrew MacNamara writes:
  
Andrew MacNamara followed up with some thoughts:
  
I [MacNamara] had some basic statistics training through my MBA program, after having completed an undergrad degree in computer science. Since then I’ve been very interested in learning more about statistical techniques, including things like GLM and censored data analyses as well as machine learning topics like neural nets, SVMs, etc. I began following your blog after some research into Bayesian analysis topics and I am trying to dig deeper on that side of things.


One thing I have noticed is that there seems to be a distinction between data analysi</p><p>5 0.14552414 <a title="690-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-21-Belief_in_hell_is_associated_with_lower_crime_rates.html">1386 andrew gelman stats-2012-06-21-Belief in hell is associated with lower crime rates</a></p>
<p>Introduction: I remember attending a talk a few years ago by my political science colleague John Huber in which he discussed cross-national comparisons of religious attitudes.  One thing I remember is that the U.S. is highly religious, another thing I remembered is that lots more Americans believe in heaven than believe in hell.
 
Some of this went into Red State Blue State—not the heaven/hell thing, but the graph of religiosity vs. GDP:
 
   
 
and the corresponding graph of religious attendance vs. GDP for U.S. states:
 
   
 
Also we learned that, at the individual level, the correlation of religious attendance with income is zero (according to survey reports, rich Americans are neither more nor less likely than poor Americans to go to church regularly):
 
   
 
while the correlation of prayer with income is strongly negative (poor Americans are much more likely than rich Americans to regularly pray):
 
   
 
Anyway, with all this, I was primed to be interested in a recent  study  by psychologist</p><p>6 0.1444364 <a title="690-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>7 0.14234754 <a title="690-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>8 0.1412477 <a title="690-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<p>9 0.13974681 <a title="690-tfidf-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Useful_models%2C_model_checking%2C_and_external_validation%3A__a_mini-discussion.html">244 andrew gelman stats-2010-08-30-Useful models, model checking, and external validation:  a mini-discussion</a></p>
<p>10 0.13735859 <a title="690-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-21-Bayes_related.html">1948 andrew gelman stats-2013-07-21-Bayes related</a></p>
<p>11 0.13379022 <a title="690-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-11-Why_waste_time_philosophizing%3F.html">1719 andrew gelman stats-2013-02-11-Why waste time philosophizing?</a></p>
<p>12 0.1309271 <a title="690-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-04-Write_This_Book.html">1605 andrew gelman stats-2012-12-04-Write This Book</a></p>
<p>13 0.12736785 <a title="690-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-09-A_tale_of_two_discussion_papers.html">1848 andrew gelman stats-2013-05-09-A tale of two discussion papers</a></p>
<p>14 0.12637848 <a title="690-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-29-Infovis%2C_infographics%2C_and_data_visualization%3A__Where_I%E2%80%99m_coming_from%2C_and_where_I%E2%80%99d_like_to_go.html">878 andrew gelman stats-2011-08-29-Infovis, infographics, and data visualization:  Where I’m coming from, and where I’d like to go</a></p>
<p>15 0.12595344 <a title="690-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-07-Feedback_on_my_Bayesian_Data_Analysis_class_at_Columbia.html">1611 andrew gelman stats-2012-12-07-Feedback on my Bayesian Data Analysis class at Columbia</a></p>
<p>16 0.12563431 <a title="690-tfidf-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-22-Philosophy_of_Bayes_and_non-Bayes%3A__A_dialogue_with_Deborah_Mayo.html">291 andrew gelman stats-2010-09-22-Philosophy of Bayes and non-Bayes:  A dialogue with Deborah Mayo</a></p>
<p>17 0.12514393 <a title="690-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-09-Coming_to_agreement_on_philosophy_of_statistics.html">1205 andrew gelman stats-2012-03-09-Coming to agreement on philosophy of statistics</a></p>
<p>18 0.12051509 <a title="690-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-23-Examples_of_the_use_of_hierarchical_modeling_to_generalize_to_new_settings.html">1425 andrew gelman stats-2012-07-23-Examples of the use of hierarchical modeling to generalize to new settings</a></p>
<p>19 0.11971253 <a title="690-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>20 0.11892032 <a title="690-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-16-Infovis_and_statgraphics_update_update.html">855 andrew gelman stats-2011-08-16-Infovis and statgraphics update update</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.301), (1, 0.075), (2, -0.062), (3, 0.019), (4, 0.016), (5, 0.04), (6, -0.064), (7, 0.054), (8, 0.052), (9, 0.001), (10, 0.019), (11, -0.007), (12, -0.021), (13, -0.017), (14, 0.042), (15, -0.016), (16, 0.008), (17, -0.002), (18, 0.032), (19, -0.033), (20, 0.022), (21, -0.001), (22, -0.013), (23, 0.028), (24, -0.015), (25, 0.015), (26, -0.008), (27, -0.009), (28, 0.044), (29, 0.015), (30, -0.024), (31, -0.002), (32, 0.007), (33, 0.023), (34, -0.002), (35, 0.045), (36, -0.018), (37, -0.015), (38, -0.029), (39, 0.018), (40, -0.007), (41, 0.015), (42, -0.032), (43, 0.021), (44, 0.007), (45, -0.006), (46, -0.019), (47, -0.037), (48, -0.006), (49, -0.004)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9808746 <a title="690-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-01-Peter_Huber%E2%80%99s_reflections_on_data_analysis.html">690 andrew gelman stats-2011-05-01-Peter Huber’s reflections on data analysis</a></p>
<p>Introduction: Peter Huber’s most famous work derives from  his paper  on robust statistics published nearly fifty years ago in which he introduced the concept of M-estimation (a generalization of maximum likelihood) to unify some ideas of Tukey and others for estimation procedures that were relatively insensitive to small departures from the assumed model.
 
Huber has in many ways been ahead of his time.  While remaining connected to the theoretical ideas from the early part of his career, his interests have shifted to computational and graphical statistics.  I never took Huber’s class on data analysis–he left Harvard while I was still in graduate school–but fortunately I have an opportunity to learn his lessons now, as he has just released a book, “Data Analysis:  What Can Be Learned from the Past 50 Years.”
 
The book puts together a few articles published in the past 15 years, along with some new material.  Many of the examples are decades old, which is appropriate given that Huber is reviewing f</p><p>2 0.86547863 <a title="690-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-04-Model_checking_and_model_understanding_in_machine_learning.html">1482 andrew gelman stats-2012-09-04-Model checking and model understanding in machine learning</a></p>
<p>Introduction: Last month I  wrote :
  
Computer scientists are often brilliant but they can be unfamiliar with what is done in the worlds of data collection and analysis. This goes the other way too: statisticians such as myself can look pretty awkward, reinventing (or failing to reinvent) various wheels when we write computer programs or, even worse, try to design software.Andrew MacNamara writes:
  
Andrew MacNamara followed up with some thoughts:
  
I [MacNamara] had some basic statistics training through my MBA program, after having completed an undergrad degree in computer science. Since then I’ve been very interested in learning more about statistical techniques, including things like GLM and censored data analyses as well as machine learning topics like neural nets, SVMs, etc. I began following your blog after some research into Bayesian analysis topics and I am trying to dig deeper on that side of things.


One thing I have noticed is that there seems to be a distinction between data analysi</p><p>3 0.8482433 <a title="690-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-23-When_are_complicated_models_helpful_in_psychology_research_and_when_are_they_overkill%3F.html">1690 andrew gelman stats-2013-01-23-When are complicated models helpful in psychology research and when are they overkill?</a></p>
<p>Introduction: Nick Brown is bothered by  this article , “An unscented Kalman filter approach to the estimation of nonlinear dynamical systems models,” by Sy-Miin Chow, Emilio Ferrer, and John Nesselroade.  The introduction of the article cites a bunch of articles in serious psych/statistics journals.  The question is, are such advanced statistical techniques really needed, or even legitimate, with the kind of very rough data that is usually available in psych applications? Or is it just fishing in the hope of discovering patterns that are not really there?
 
I wrote:
 
It seems like a pretty innocuous literature review.  I agree that many of the applications are silly (for example, they cite the work of the notorious  John Gottman  in fitting a predator-prey model to spousal relations (!)), but overall they just seem to be presenting very standard ideas for the mathematical-psychology audience.  It’s not clear whether advanced techniques are always appropriate here, but they come in through a natura</p><p>4 0.84229672 <a title="690-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-14-Everyone%E2%80%99s_trading_bias_for_variance_at_some_point%2C_it%E2%80%99s_just_done_at_different_places_in_the_analyses.html">1763 andrew gelman stats-2013-03-14-Everyone’s trading bias for variance at some point, it’s just done at different places in the analyses</a></p>
<p>Introduction: Some things I respect 
 
When it comes to meta-models of statistics, here are two philosophies that I respect:
 
1.  (My) Bayesian approach, which I associate with E. T. Jaynes, in which you construct models with strong assumptions, ride your models hard, check their fit to data, and then scrap them and improve them as necessary.
 
2.  At the other extreme, model-free statistical procedures that are designed to work well under very weak assumptions—for example, instead of assuming a distribution is Gaussian, you would just want the procedure to work well under some conditions on the smoothness of the second derivative of the log density function.
 
Both the above philosophies recognize that (almost) all important assumptions will be wrong, and they resolve this concern via aggressive model checking or via robustness.  And of course there are intermediate positions, such as working with Bayesian models that have been shown to be robust, and then still checking them.  Or, to flip it arou</p><p>5 0.84218031 <a title="690-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-03-Statistical_methods_for_healthcare_regulation%3A_rating%2C_screening_and_surveillance.html">744 andrew gelman stats-2011-06-03-Statistical methods for healthcare regulation: rating, screening and surveillance</a></p>
<p>Introduction: Here is my discussion of  a recent article  by David Spiegelhalter, Christopher 
Sherlaw-Johnson, Martin Bardsley, Ian Blunt, Christopher Wood and Olivia Grigg, that is scheduled to appear in the Journal of the Royal Statistical Society:
 
I applaud the authors’ use of a mix of statistical methods to attack an important real-world problem. Policymakers need results right away, and I admire the authors’ ability and willingness to combine several different modeling and significance testing ideas for the purposes of rating and surveillance.
 
That said, I am uncomfortable with the statistical ideas here, for three reasons. First, I feel that the proposed methods, centered as they are around data manipulation and corrections for uncertainty, has serious defects compared to a more model-based approach. My problem with methods based on p-values and z-scores–however they happen to be adjusted–is that they draw discussion toward error rates, sequential analysis, and other technical statistical</p><p>6 0.83633506 <a title="690-lsi-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Useful_models%2C_model_checking%2C_and_external_validation%3A__a_mini-discussion.html">244 andrew gelman stats-2010-08-30-Useful models, model checking, and external validation:  a mini-discussion</a></p>
<p>7 0.83365023 <a title="690-lsi-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-19-Just_chaid.html">421 andrew gelman stats-2010-11-19-Just chaid</a></p>
<p>8 0.8328923 <a title="690-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-21-Bayes_related.html">1948 andrew gelman stats-2013-07-21-Bayes related</a></p>
<p>9 0.83211994 <a title="690-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-11-Toward_a_framework_for_automatic_model_building.html">1718 andrew gelman stats-2013-02-11-Toward a framework for automatic model building</a></p>
<p>10 0.83141518 <a title="690-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-01-Doing_Data_Science%3A__What%E2%80%99s_it_all_about%3F.html">2084 andrew gelman stats-2013-11-01-Doing Data Science:  What’s it all about?</a></p>
<p>11 0.82970613 <a title="690-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-23-Examples_of_the_use_of_hierarchical_modeling_to_generalize_to_new_settings.html">1425 andrew gelman stats-2012-07-23-Examples of the use of hierarchical modeling to generalize to new settings</a></p>
<p>12 0.82482731 <a title="690-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-21-Readings_for_a_two-week_segment_on_Bayesian_modeling%3F.html">1586 andrew gelman stats-2012-11-21-Readings for a two-week segment on Bayesian modeling?</a></p>
<p>13 0.81829745 <a title="690-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-10-Controversy_over_the_Christakis-Fowler_findings_on_the_contagion_of_obesity.html">757 andrew gelman stats-2011-06-10-Controversy over the Christakis-Fowler findings on the contagion of obesity</a></p>
<p>14 0.81490195 <a title="690-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-06-Bayesian_model-building_by_pure_thought%3A__Some_principles_and_examples.html">1156 andrew gelman stats-2012-02-06-Bayesian model-building by pure thought:  Some principles and examples</a></p>
<p>15 0.81438184 <a title="690-lsi-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-Transformations_for_non-normal_data.html">2176 andrew gelman stats-2014-01-19-Transformations for non-normal data</a></p>
<p>16 0.81062168 <a title="690-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-22-Going_Beyond_the_Book%3A_Towards_Critical_Reading_in_Statistics_Teaching.html">1023 andrew gelman stats-2011-11-22-Going Beyond the Book: Towards Critical Reading in Statistics Teaching</a></p>
<p>17 0.80780697 <a title="690-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-04-Write_This_Book.html">1605 andrew gelman stats-2012-12-04-Write This Book</a></p>
<p>18 0.80483037 <a title="690-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>19 0.80219144 <a title="690-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-05-Watership_Down%2C_thick_description%2C_applied_statistics%2C_immutability_of_stories%2C_and_playing_tennis_with_a_net.html">1750 andrew gelman stats-2013-03-05-Watership Down, thick description, applied statistics, immutability of stories, and playing tennis with a net</a></p>
<p>20 0.79921204 <a title="690-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.025), (5, 0.014), (9, 0.017), (16, 0.089), (21, 0.033), (24, 0.137), (43, 0.022), (63, 0.03), (84, 0.017), (86, 0.035), (89, 0.01), (96, 0.097), (99, 0.298)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97701824 <a title="690-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-07-There_are_never_70_distinct_parameters.html">327 andrew gelman stats-2010-10-07-There are never 70 distinct parameters</a></p>
<p>Introduction: Sam Seaver writes:
  
I’m a graduate student in computational biology, and I’m relatively new to advanced statistics, and am trying to teach myself how best to approach a problem I have.


My dataset is a small sparse matrix of 150 cases and 70 predictors, it is sparse as in many zeros, not many ‘NA’s.  Each case is a nutrient that is fed into an in silico organism, and its response is whether or not it stimulates growth, and each predictor is one of 70 different pathways that the nutrient may or may not belong to.  Because all of the nutrients do not belong to all of the pathways, there are thus many zeros in my matrix.  My goal is to be able to use the pathways themselves to predict whether or not a nutrient could stimulate growth, thus I wanted to compute regression coefficients for each pathway, with which I could apply to other nutrients for other species.


There are quite a few singularities in the dataset (summary(glm) reports that 14 coefficients are not defined because of sin</p><p>2 0.97426999 <a title="690-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-21-If_a_lottery_is_encouraging_addictive_gambling%2C_don%E2%80%99t_expand_it%21.html">1731 andrew gelman stats-2013-02-21-If a lottery is encouraging addictive gambling, don’t expand it!</a></p>
<p>Introduction: This  story from Vivian Yee seems just horrible to me.  First the background:
  
Pronto Lotto’s real business takes place in the carpeted, hushed area where its most devoted customers watch video screens from a scattering of tall silver tables, hour after hour, day after day.


The players — mostly men, about a dozen at any given time — come on their lunch breaks or after work to study the screens, which are programmed with the Quick Draw lottery game, and flash a new set of winning numbers every four minutes. They have helped make Pronto Lotto the top Quick Draw vendor in the state, selling $3.3 million worth of tickets last year, more than $1 million more than the second busiest location, a World Books shop in Penn Station.


Some stay for just a few minutes. Others play for the length of a workday, repeatedly traversing the few yards between their seats and the cash register as they hand the next wager to a clerk with a dollar bill or two, and return to wait.


“It’s like my job, 24</p><p>3 0.97295284 <a title="690-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-22-Going_Beyond_the_Book%3A_Towards_Critical_Reading_in_Statistics_Teaching.html">1023 andrew gelman stats-2011-11-22-Going Beyond the Book: Towards Critical Reading in Statistics Teaching</a></p>
<p>Introduction: My article  with the above title is appearing in the journal Teaching Statistics.  Here’s the introduction:
  
We can improve our teaching of statistical examples from books by collecting further data, reading cited articles and performing further data analysis. This should not come as a surprise, but what might be new is the realization of how close to the surface these research opportunities are: even influential and celebrated books can have examples where more can be learned with a small amount of additional effort.


We discuss three examples that have arisen in our own teaching: an introductory textbook that motivated us to think more carefully about categorical and continuous variables; a book for the lay reader that misreported a study of menstruation and accidents; and a monograph on the foundations of probability that over interpreted statistically insignificant fluctuations in sex ratios.
  
And here’s the conclusion:
  
Individually, these examples are of little importance.</p><p>4 0.97113943 <a title="690-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-05-Different_goals%2C_different_looks%3A__Infovis_and_the_Chris_Rock_effect.html">787 andrew gelman stats-2011-07-05-Different goals, different looks:  Infovis and the Chris Rock effect</a></p>
<p>Introduction: Seth writes:
  
 Here’s  my candidate for bad graphic of the year:


   


I [Seth] studied it and learned nothing. I have no idea how they assigned colors to locations. I already knew that there were more within-city calls than calls to individual distant locations — for example that there are more SF-SF calls than SF-LA calls. The researchers took a huge rich database and boiled it down to nothing (in terms of information value) — and I have a funny feeling they don’t realize how awful this is and what a waste.


I send it to you because it isn’t obvious how to do better — at least not obvious to them.
  
My reply:
 
My first reaction is to agree–I don’t get anything out of this graph either! But let me step back.
 
I think it’s best to understand this using the framework of  my paper with Antony Unwin , by thinking of the goals that are satisfied by different sorts of graphs.
 
What does this graph convey? It doesn’t tell us much about phone calls, but it does tell us that some peop</p><p>5 0.96862316 <a title="690-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-17-Cool_dynamic_demographic_maps_provide_beautiful_illustration_of_Chris_Rock_effect.html">2065 andrew gelman stats-2013-10-17-Cool dynamic demographic maps provide beautiful illustration of Chris Rock effect</a></p>
<p>Introduction: Robert Gonzalez  reports  on some  beautiful graphs  from John Nelson. Here’s Nelson:
 
 
         The sexes start out homogenous, go super segregated in the teen years, segregate for business in the twenty-somethings, and re-couple for co-habitation years.  Then the lights fade into faint pockets of pink.   
 
  
I [Nelson] am using simple tract-level population/gender counts from the US Census Bureau. Because their tract boundaries extend into the water and vacant area, I used NYC’s Bytes of the Big Apple zoning shapes to clip the census tracts to residentially zoned areas -giving me a more realistic (and more recognizable) definition of populated areas. The census breaks out their population counts by gender for five-year age spans ranging from teeny tiny infants through esteemed 85+ year-olds.
  
And here’s Gonzalez:
  
Between ages 0 and 14, the entire map is more or less an evenly mixed purple landscape; newborns, children and adolescents, after all, can’t really choose where the</p><p>6 0.96839619 <a title="690-lda-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-This_is_a_link_to_a_news_article_about_a_scientific_paper.html">302 andrew gelman stats-2010-09-28-This is a link to a news article about a scientific paper</a></p>
<p>7 0.96839267 <a title="690-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-12-The_Wald_method_has_been_the_subject_of_extensive_criticism_by_statisticians_for_exaggerating_results%E2%80%9D.html">410 andrew gelman stats-2010-11-12-The Wald method has been the subject of extensive criticism by statisticians for exaggerating results”</a></p>
<p>8 0.9682591 <a title="690-lda-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-13-Arnold_Zellner.html">205 andrew gelman stats-2010-08-13-Arnold Zellner</a></p>
<p>9 0.96629894 <a title="690-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-04-%E2%80%9CWho_owns_Congress%E2%80%9D.html">319 andrew gelman stats-2010-10-04-“Who owns Congress”</a></p>
<p>same-blog 10 0.96465123 <a title="690-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-01-Peter_Huber%E2%80%99s_reflections_on_data_analysis.html">690 andrew gelman stats-2011-05-01-Peter Huber’s reflections on data analysis</a></p>
<p>11 0.95914924 <a title="690-lda-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-19-Index_or_indicator_variables.html">2296 andrew gelman stats-2014-04-19-Index or indicator variables</a></p>
<p>12 0.95599949 <a title="690-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-10-Estimation_from_an_out-of-date_census.html">405 andrew gelman stats-2010-11-10-Estimation from an out-of-date census</a></p>
<p>13 0.95414591 <a title="690-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-23-Advice_on_writing_research_articles.html">1338 andrew gelman stats-2012-05-23-Advice on writing research articles</a></p>
<p>14 0.95300961 <a title="690-lda-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-19-Paired_comparisons.html">99 andrew gelman stats-2010-06-19-Paired comparisons</a></p>
<p>15 0.95160192 <a title="690-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-15-Still_more_Mr._P_in_public_health.html">770 andrew gelman stats-2011-06-15-Still more Mr. P in public health</a></p>
<p>16 0.95144856 <a title="690-lda-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>17 0.95136303 <a title="690-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-07-Happy_news_on_happiness%3B_what_can_we_believe%3F.html">1305 andrew gelman stats-2012-05-07-Happy news on happiness; what can we believe?</a></p>
<p>18 0.94990396 <a title="690-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-30-Nooooooooooooooooooo%21.html">934 andrew gelman stats-2011-09-30-Nooooooooooooooooooo!</a></p>
<p>19 0.94960517 <a title="690-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-23-A_statistical_version_of_Arrow%E2%80%99s_paradox.html">586 andrew gelman stats-2011-02-23-A statistical version of Arrow’s paradox</a></p>
<p>20 0.94895059 <a title="690-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-02-Does_a_professor%E2%80%99s_intervention_in_online_discussions_have_the_effect_of_prolonging_discussion_or_cutting_it_off%3F.html">2120 andrew gelman stats-2013-12-02-Does a professor’s intervention in online discussions have the effect of prolonging discussion or cutting it off?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
