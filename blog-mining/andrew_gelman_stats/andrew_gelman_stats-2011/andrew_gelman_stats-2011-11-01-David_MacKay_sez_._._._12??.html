<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>984 andrew gelman stats-2011-11-01-David MacKay sez . . . 12??</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-984" href="#">andrew_gelman_stats-2011-984</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>984 andrew gelman stats-2011-11-01-David MacKay sez . . . 12??</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-984-html" href="http://andrewgelman.com/2011/11/01/d12/">html</a></p><p>Introduction: I’ve recently been reading David MacKay’s 2003  book , Information Theory, Inference, and Learning Algorithms. It’s great background for my Bayesian computation class because he has lots of pictures and detailed discussions of the algorithms.  (Regular readers of this blog will not be surprised to hear that I hate all the  Occam -factor stuff that MacKay talks about, but overall it’s a great book.)
 
Anyway, I happened to notice the following bit, under the heading, “How many samples are needed?”:
  
In many problems, we really only need about twelve independent samples from P(x). Imagine that x is an unknown vector such as the amount of corrosion present in each of 10 000 underground pipelines around Cambridge, and φ(x) is the total cost of repairing those pipelines. The distribution P(x) describes the probability of a state x given the tests that have been carried out on some pipelines and the assumptions about the physics of corrosion. The quantity Φ is the expected cost of the repa</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 (Regular readers of this blog will not be surprised to hear that I hate all the  Occam -factor stuff that MacKay talks about, but overall it’s a great book. [sent-3, score-0.135]
</p><p>2 )   Anyway, I happened to notice the following bit, under the heading, “How many samples are needed? [sent-4, score-0.138]
</p><p>3 ”:    In many problems, we really only need about twelve independent samples from P(x). [sent-5, score-0.36]
</p><p>4 Imagine that x is an unknown vector such as the amount of corrosion present in each of 10 000 underground pipelines around Cambridge, and φ(x) is the total cost of repairing those pipelines. [sent-6, score-0.423]
</p><p>5 The distribution P(x) describes the probability of a state x given the tests that have been carried out on some pipelines and the assumptions about the physics of corrosion. [sent-7, score-0.246]
</p><p>6 The quantity Φ is the expected cost of the repairs. [sent-8, score-0.232]
</p><p>7 The quantity σ^2 is the variance of the cost — σ measures by how much we should expect the actual cost to differ from the expectation Φ. [sent-9, score-0.518]
</p><p>8 I would suggest there is little point in knowing Φ to a precision finer than about σ/3. [sent-11, score-0.18]
</p><p>9 After all, the true cost is likely to differ by ±σ from Φ. [sent-12, score-0.226]
</p><p>10 If we obtain R = 12 independent samples from P(x), we can estimate Φ to a precision of σ/√12 – which is smaller than σ/3. [sent-13, score-0.356]
</p><p>11 By “P(x)”, I think he means p(theta|y), by “10 000″, I think he means 10,000, and by “R” I think he means n_{sims} or something like that. [sent-15, score-0.216]
</p><p>12 Setting aside the “two cultures separated by a common language” thing, and taking MacKay at his word regarding the priorities of pipeline repair managers, I’m in complete agreement with this reasoning. [sent-16, score-0.288]
</p><p>13 I’m always telling people they don’t need as many independent draws as they think they need, and I squirm in frustration when I read a paper where the authors proudly announce that they ran 100,000 simulation draws. [sent-17, score-0.314]
</p><p>14 Accepting MacKay’s stipulation that σ/3 would be enough precision in this sort of example, shouldn’t he have said that 9 random draws would suffice? [sent-24, score-0.268]
</p><p>15 That’s why I inserted the ^ in σ^2 above—the superscript got lost in the copying from pdf. [sent-34, score-0.147]
</p><p>16 On that same page MacKay makes the odd-seeming (to me) remark that the tuning parameters of the Metropolis algorithm “are usually set by trial and error with the rule of thumb being to aim for a rejection frequency of about 0. [sent-38, score-0.35]
</p><p>17 It is not valid to have the width parameters be dynamically updated during the simulation in a way that depends on the history of the simulation. [sent-40, score-0.193]
</p><p>18 Such a modification of the proposal density would violate the detailed balance condition that guarantees that the Markov chain has the correct invariant distribution. [sent-41, score-0.247]
</p><p>19 )  The other part that surprised me was MacKay’s dismissal of adapting the jumping kernel. [sent-50, score-0.182]
</p><p>20 At some level MacKay recognizes this—he talks about tuning “by trial and error”—but he could be misleading people by suggesting that tuning can’t be done more systematically. [sent-54, score-0.474]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mackay', 0.678), ('tuning', 0.158), ('cost', 0.149), ('pipelines', 0.147), ('samples', 0.138), ('twelve', 0.126), ('precision', 0.122), ('adapt', 0.103), ('physics', 0.099), ('independent', 0.096), ('metropolis', 0.093), ('copying', 0.091), ('passage', 0.084), ('quantity', 0.083), ('trial', 0.082), ('draws', 0.079), ('detailed', 0.078), ('differ', 0.077), ('simulation', 0.076), ('talks', 0.076), ('means', 0.072), ('stipulation', 0.067), ('pipeline', 0.067), ('repairing', 0.067), ('suffice', 0.067), ('invariant', 0.063), ('dynamically', 0.063), ('langevin', 0.063), ('dismissal', 0.063), ('proudly', 0.063), ('variance', 0.06), ('adapting', 0.06), ('underground', 0.06), ('occam', 0.06), ('surprised', 0.059), ('finer', 0.058), ('surprises', 0.058), ('separated', 0.058), ('sims', 0.056), ('thumb', 0.056), ('inserted', 0.056), ('repair', 0.056), ('dated', 0.056), ('priorities', 0.054), ('guarantees', 0.054), ('parameters', 0.054), ('cultures', 0.053), ('book', 0.052), ('modification', 0.052), ('cambridge', 0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999976 <a title="984-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-01-David_MacKay_sez_._._._12%3F%3F.html">984 andrew gelman stats-2011-11-01-David MacKay sez . . . 12??</a></p>
<p>Introduction: I’ve recently been reading David MacKay’s 2003  book , Information Theory, Inference, and Learning Algorithms. It’s great background for my Bayesian computation class because he has lots of pictures and detailed discussions of the algorithms.  (Regular readers of this blog will not be surprised to hear that I hate all the  Occam -factor stuff that MacKay talks about, but overall it’s a great book.)
 
Anyway, I happened to notice the following bit, under the heading, “How many samples are needed?”:
  
In many problems, we really only need about twelve independent samples from P(x). Imagine that x is an unknown vector such as the amount of corrosion present in each of 10 000 underground pipelines around Cambridge, and φ(x) is the total cost of repairing those pipelines. The distribution P(x) describes the probability of a state x given the tests that have been carried out on some pipelines and the assumptions about the physics of corrosion. The quantity Φ is the expected cost of the repa</p><p>2 0.26107875 <a title="984-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-04-David_MacKay_and_Occam%E2%80%99s_Razor.html">1041 andrew gelman stats-2011-12-04-David MacKay and Occam’s Razor</a></p>
<p>Introduction: In my comments on David MacKay’s 2003 book on Bayesian inference, I  wrote  that I hate all the Occam-factor stuff that MacKay talks about, and I linked to  this quote  from Radford Neal:
  
Sometimes a simple model will outperform a more complex model . . . Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well.
  
MacKay replied as follows:
  
When you said you disagree with me on Occam factors I think what you meant was that you agree with me on them.  I’ve read your post on the topic and completely agreed with you (and Radford) that we should be using models the size of a  house, models that we believe in, and that anyone who thinks it is a good idea to  bias the model toward</p><p>3 0.24940026 <a title="984-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-01-MacKay_update%3A__where_12_comes_from.html">986 andrew gelman stats-2011-11-01-MacKay update:  where 12 comes from</a></p>
<p>Introduction: In reply to my  question , David MacKay writes:
  
You said that can imagine rounding up 9 to 10 – which would be elegant if we worked in base 10.


But in the UK we haven’t switched to base 10 yet, we still work in  dozens and grosses.  (One gross = 12^2 = 144.)  So I was taught (by John Skilling, probably) “a dozen samples are plenty”. 


Probably in an earlier draft of the book in 2001 I said “a dozen”, rather than “12″.  Then some feedbacker may have written and said “I don’t know what a dozen is”;  so then I sacrificed elegant language and replaced “dozen” by “12″, which  leads to your mystification. 


PS – please send the winner of your competition a free copy of my other book ( sewtha ) too, from me. 


PPS I see that Mikkel Schmidt [in your comments] has diligently found the correct answer, which I guessed above. I suggest you award the prizes to him.
  
OK, we’re just  giving  away books here!
 
P.S.   See here  for my review of MacKay’s book on sustainable energy.</p><p>4 0.16005999 <a title="984-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-23-Learning_Differential_Geometry_for_Hamiltonian_Monte_Carlo.html">1339 andrew gelman stats-2012-05-23-Learning Differential Geometry for Hamiltonian Monte Carlo</a></p>
<p>Introduction: You can get a taste of Hamiltonian Monte Carlo (HMC) by reading the very gentle introduction in David MacKay’s general text on information theory:
  
  MacKay, D.  2003.    Information Theory, Inference, and Learning Algorithms  .  Cambridge University Press.  [see Chapter 31, which is relatively standalone and can be downloaded separately.]
   
Follow this up with Radford Neal’s much more thorough introduction to HMC:
  
 Neal, R. 2011.   MCMC Using Hamiltonian Dynamics .  In Brooks, Gelman, Jones and Meng, eds.,  Handbook of Markov Chain Monte Carlo .  Chapman and Hall/CRC Press.
   
To understand why HMC works and set yourself on the path to understanding generalizations like  Riemann manifold HMC , you’ll need to know a bit about differential geometry.  I really liked the combination of these two books:
  
  Magnus, J. R. and H. Neudecker.  2007.   Matrix Differential Calculus with Application in Statistics and Econometrics .  3rd Edition.  Wiley?
   
and
  
  Leimkuhler, B. and S.</p><p>5 0.12498727 <a title="984-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<p>Introduction: From August 1990.  It was in the form of a note sent to all the people in the statistics group of Bell Labs, where I’d worked that summer.
  
To all:


Here’s the abstract of the work I’ve done this summer.  It’s stored in the file, 
/fs5/gelman/abstract.bell, and copies of the Figures 1-3 are on Trevor’s desk. 
Any comments are of course appreciated; I’m at gelman@stat.berkeley.edu.


On the Routine Use of Markov Chains for Simulation


Andrew Gelman and Donald Rubin, 6 August 1990


corrected version:  8 August 1990
  
  
  
1.  Simulation


In probability and statistics we can often specify multivariate distributions 
many of whose properties we do not fully understand–perhaps, as in the 
Ising model of statistical physics, we can write the joint density function, up 
to a multiplicative constant that cannot be expressed in closed form. 
For an example in statistics, consider the Normal random 
effects model in the analysis of variance, which can be 
easily placed in a Bayesian fram</p><p>6 0.12360671 <a title="984-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-19-Will_Stan_work_well_with_40%C3%9740_matrices%3F.html">861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</a></p>
<p>7 0.1140742 <a title="984-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-21-Readings_for_a_two-week_segment_on_Bayesian_modeling%3F.html">1586 andrew gelman stats-2012-11-21-Readings for a two-week segment on Bayesian modeling?</a></p>
<p>8 0.10753517 <a title="984-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-10-Cross-validation_and_Bayesian_estimation_of_tuning_parameters.html">2129 andrew gelman stats-2013-12-10-Cross-validation and Bayesian estimation of tuning parameters</a></p>
<p>9 0.098985955 <a title="984-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-21-Handbook_of_Markov_Chain_Monte_Carlo.html">674 andrew gelman stats-2011-04-21-Handbook of Markov Chain Monte Carlo</a></p>
<p>10 0.082200751 <a title="984-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-01-My_course_this_fall_on_Bayesian_Computation.html">884 andrew gelman stats-2011-09-01-My course this fall on Bayesian Computation</a></p>
<p>11 0.077484161 <a title="984-tfidf-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-31-Somewhat_Bayesian_multilevel_modeling.html">246 andrew gelman stats-2010-08-31-Somewhat Bayesian multilevel modeling</a></p>
<p>12 0.073937997 <a title="984-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-09-A_tale_of_two_discussion_papers.html">1848 andrew gelman stats-2013-05-09-A tale of two discussion papers</a></p>
<p>13 0.07071241 <a title="984-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<p>14 0.069669425 <a title="984-tfidf-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-07-My_recent_debugging_experience.html">2161 andrew gelman stats-2014-01-07-My recent debugging experience</a></p>
<p>15 0.068474457 <a title="984-tfidf-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-12-More_on_publishing_in_journals.html">2245 andrew gelman stats-2014-03-12-More on publishing in journals</a></p>
<p>16 0.068108335 <a title="984-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-16-Long_discussion_about_causal_inference_and_the_use_of_hierarchical_models_to_bridge_between_different_inferential_settings.html">1418 andrew gelman stats-2012-07-16-Long discussion about causal inference and the use of hierarchical models to bridge between different inferential settings</a></p>
<p>17 0.067614034 <a title="984-tfidf-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-02-Fragment_of_statistical_autobiography.html">390 andrew gelman stats-2010-11-02-Fragment of statistical autobiography</a></p>
<p>18 0.067017302 <a title="984-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-06-Yes%2C_checking_calibration_of_probability_forecasts_is_part_of_Bayesian_statistics.html">1610 andrew gelman stats-2012-12-06-Yes, checking calibration of probability forecasts is part of Bayesian statistics</a></p>
<p>19 0.066676997 <a title="984-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>20 0.0665434 <a title="984-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-16-The_%E2%80%9CWashington_read%E2%80%9D_and_the_algebra_of_conditional_distributions.html">961 andrew gelman stats-2011-10-16-The “Washington read” and the algebra of conditional distributions</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.159), (1, 0.026), (2, -0.008), (3, 0.019), (4, 0.015), (5, 0.014), (6, 0.062), (7, -0.025), (8, 0.02), (9, -0.026), (10, 0.014), (11, -0.031), (12, -0.015), (13, -0.021), (14, 0.032), (15, -0.039), (16, -0.015), (17, 0.009), (18, 0.032), (19, -0.039), (20, 0.027), (21, 0.002), (22, 0.046), (23, 0.013), (24, 0.033), (25, 0.04), (26, -0.015), (27, 0.053), (28, 0.041), (29, 0.02), (30, -0.006), (31, 0.008), (32, 0.011), (33, 0.006), (34, 0.004), (35, -0.022), (36, -0.03), (37, -0.036), (38, -0.003), (39, -0.009), (40, -0.005), (41, -0.035), (42, -0.023), (43, 0.016), (44, 0.005), (45, -0.058), (46, 0.02), (47, 0.011), (48, 0.011), (49, -0.012)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94136268 <a title="984-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-01-David_MacKay_sez_._._._12%3F%3F.html">984 andrew gelman stats-2011-11-01-David MacKay sez . . . 12??</a></p>
<p>Introduction: I’ve recently been reading David MacKay’s 2003  book , Information Theory, Inference, and Learning Algorithms. It’s great background for my Bayesian computation class because he has lots of pictures and detailed discussions of the algorithms.  (Regular readers of this blog will not be surprised to hear that I hate all the  Occam -factor stuff that MacKay talks about, but overall it’s a great book.)
 
Anyway, I happened to notice the following bit, under the heading, “How many samples are needed?”:
  
In many problems, we really only need about twelve independent samples from P(x). Imagine that x is an unknown vector such as the amount of corrosion present in each of 10 000 underground pipelines around Cambridge, and φ(x) is the total cost of repairing those pipelines. The distribution P(x) describes the probability of a state x given the tests that have been carried out on some pipelines and the assumptions about the physics of corrosion. The quantity Φ is the expected cost of the repa</p><p>2 0.78496855 <a title="984-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-05-Monitor_the_efficiency_of_your_Markov_chain_sampler_using_expected_squared_jumped_distance%21.html">650 andrew gelman stats-2011-04-05-Monitor the efficiency of your Markov chain sampler using expected squared jumped distance!</a></p>
<p>Introduction: Marc Tanguay writes in with a specific question that has a very general answer.  First, the question:
  
I [Tanguay] am currently running a MCMC for which I have 3 parameters that are restricted to a specific space. 2 are bounded between 0 and 1 while the third is binary and updated by a Beta-Binomial. Since my priors are also bounded, I notice that, conditional on All the rest (which covers both data and other parameters), the density was not varying a lot within the space of the parameters. As a result, the acceptance rate is high, about 85%, and this despite the fact that all the parameter’s space is explore. Since in your book, the optimal acceptance rates prescribed are lower that 50% (in case of multiple parameters), do you think I should worry about getting 85%.  Or is this normal given the restrictions on the parameters?
  
First off:  Yes, my guess is that you should be taking bigger jumps.  85% seems like too high an acceptance rate for Metropolis jumping.
 
More generally, t</p><p>3 0.78247488 <a title="984-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-21-Handbook_of_Markov_Chain_Monte_Carlo.html">674 andrew gelman stats-2011-04-21-Handbook of Markov Chain Monte Carlo</a></p>
<p>Introduction: Galin Jones, Steve Brooks, Xiao-Li Meng and I edited a handbook of Markov Chain Monte Carlo that has  just been published .  My chapter (with Kenny Shirley) is  here , and it begins like this:
  
Convergence of Markov chain simulations can be monitored by measuring the diffusion and mixing of multiple independently-simulated chains, but different levels of convergence are appropriate for different goals. When considering inference from stochastic simulation, we need to separate two tasks: (1) inference about parameters and functions of parameters based on broad characteristics of their distribution, and (2) more precise computation of expectations and other functions of probability distributions. For the first task, there is a natural limit to precision beyond which additional simulations add essentially nothing; for the second task, the appropriate precision must be decided from external considerations. We illustrate with an example from our current research, a hierarchical model of t</p><p>4 0.76574504 <a title="984-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<p>Introduction: Tomas Iesmantas had  asked me  for advice on a regression problem with 50 parameters, and I’d recommended Hamiltonian Monte Carlo.
 
A few weeks later he reported back: 
  
  
After trying several modifications (HMC for all parameters at once, HMC just for first level parameters and Riemman manifold Hamiltonian Monte Carlo method), I finally got it running with HMC just for first level parameters and for others using direct sampling, since conditional distributions turned out to have closed form.


However, even in this case it is quite tricky, since I had to employ mass matrix and not just diagonal but at the beginning of algorithm generated it randomly (ensuring it is positive definite). Such random generation of mass matrix is quite blind step, but it proved to be quite helpful.


Riemman manifold HMC is quite vagarious, or to be more specific, metric of manifold is very sensitive. In my model log-likelihood I had exponents and values of metrics matrix elements was very large and wh</p><p>5 0.75129426 <a title="984-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<p>Introduction: From August 1990.  It was in the form of a note sent to all the people in the statistics group of Bell Labs, where I’d worked that summer.
  
To all:


Here’s the abstract of the work I’ve done this summer.  It’s stored in the file, 
/fs5/gelman/abstract.bell, and copies of the Figures 1-3 are on Trevor’s desk. 
Any comments are of course appreciated; I’m at gelman@stat.berkeley.edu.


On the Routine Use of Markov Chains for Simulation


Andrew Gelman and Donald Rubin, 6 August 1990


corrected version:  8 August 1990
  
  
  
1.  Simulation


In probability and statistics we can often specify multivariate distributions 
many of whose properties we do not fully understand–perhaps, as in the 
Ising model of statistical physics, we can write the joint density function, up 
to a multiplicative constant that cannot be expressed in closed form. 
For an example in statistics, consider the Normal random 
effects model in the analysis of variance, which can be 
easily placed in a Bayesian fram</p><p>6 0.72627854 <a title="984-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-24-Bleg%3A_Automatic_Differentiation_for_Log_Prob_Gradients%3F.html">535 andrew gelman stats-2011-01-24-Bleg: Automatic Differentiation for Log Prob Gradients?</a></p>
<p>7 0.71952999 <a title="984-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-22-Deviance%2C_DIC%2C_AIC%2C_cross-validation%2C_etc.html">776 andrew gelman stats-2011-06-22-Deviance, DIC, AIC, cross-validation, etc</a></p>
<p>8 0.71449918 <a title="984-lsi-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-12-%E2%80%9CThe_results_%28not_shown%29_._._.%E2%80%9D.html">2332 andrew gelman stats-2014-05-12-“The results (not shown) . . .”</a></p>
<p>9 0.71149909 <a title="984-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-23-Learning_Differential_Geometry_for_Hamiltonian_Monte_Carlo.html">1339 andrew gelman stats-2012-05-23-Learning Differential Geometry for Hamiltonian Monte Carlo</a></p>
<p>10 0.7067275 <a title="984-lsi-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-21-Careers%2C_one-hit_wonders%2C_and_an_offer_of_a_free_book.html">46 andrew gelman stats-2010-05-21-Careers, one-hit wonders, and an offer of a free book</a></p>
<p>11 0.70455599 <a title="984-lsi-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-20-Fooled_by_randomness.html">2297 andrew gelman stats-2014-04-20-Fooled by randomness</a></p>
<p>12 0.68955225 <a title="984-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-27-The_M%C3%B6bius_strip%2C_or%2C_marketing_that_is_impervious_to_criticism.html">1641 andrew gelman stats-2012-12-27-The Möbius strip, or, marketing that is impervious to criticism</a></p>
<p>13 0.68340945 <a title="984-lsi-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-31-Somewhat_Bayesian_multilevel_modeling.html">246 andrew gelman stats-2010-08-31-Somewhat Bayesian multilevel modeling</a></p>
<p>14 0.67780721 <a title="984-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-06-W%E2%80%99man_%3C_W%E2%80%99pedia%2C_again.html">945 andrew gelman stats-2011-10-06-W’man < W’pedia, again</a></p>
<p>15 0.67705238 <a title="984-lsi-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-09-Hermann_Goering_and_Jane_Jacobs%2C_together_at_last%21.html">2164 andrew gelman stats-2014-01-09-Hermann Goering and Jane Jacobs, together at last!</a></p>
<p>16 0.67696369 <a title="984-lsi-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-14-%E2%80%9CThe_subtle_funk_of_just_a_little_poultry_offal%E2%80%9D.html">2334 andrew gelman stats-2014-05-14-“The subtle funk of just a little poultry offal”</a></p>
<p>17 0.67634183 <a title="984-lsi-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-04-26-Prolefeed.html">4 andrew gelman stats-2010-04-26-Prolefeed</a></p>
<p>18 0.67523593 <a title="984-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-02-The_new_Helen_DeWitt_novel.html">886 andrew gelman stats-2011-09-02-The new Helen DeWitt novel</a></p>
<p>19 0.67362893 <a title="984-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-16-The_%E2%80%9CWashington_read%E2%80%9D_and_the_algebra_of_conditional_distributions.html">961 andrew gelman stats-2011-10-16-The “Washington read” and the algebra of conditional distributions</a></p>
<p>20 0.67295164 <a title="984-lsi-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Bayesian_Uncertainty_Quantification_for_Differential_Equations%21.html">2311 andrew gelman stats-2014-04-29-Bayesian Uncertainty Quantification for Differential Equations!</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(5, 0.024), (15, 0.038), (16, 0.043), (21, 0.049), (24, 0.138), (31, 0.011), (42, 0.013), (45, 0.018), (53, 0.013), (56, 0.126), (57, 0.018), (71, 0.016), (79, 0.01), (86, 0.028), (89, 0.049), (95, 0.018), (99, 0.24)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.95544171 <a title="984-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-30-More_bad_news%3A__The_%28mis%29reporting_of_statistical_results_in_psychology_journals.html">933 andrew gelman stats-2011-09-30-More bad news:  The (mis)reporting of statistical results in psychology journals</a></p>
<p>Introduction: Another entry in the growing literature on systematic flaws in the scientific research literature.
 
This time the bad tidings come from Marjan Bakker and Jelte Wicherts, who  write :
  
Around 18% of statistical results in the psychological literature are incorrectly reported. Inconsistencies were more common in low-impact journals than in high-impact journals. Moreover, around 15% of the articles contained at least one statistical conclusion that proved, upon recalculation, to be incorrect; that is, recalculation rendered the previously significant result insignificant, or vice versa. These errors were often in line with researchers’ expectations.
  
Their research also had a qualitative component:
  
To obtain a better understanding of the origins of the errors made in the reporting of statistics, we contacted the authors of the articles with errors in the second study and asked them to send us the raw data. Regrettably, only 24% of the authors shared their data, despite our request</p><p>same-blog 2 0.9477554 <a title="984-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-01-David_MacKay_sez_._._._12%3F%3F.html">984 andrew gelman stats-2011-11-01-David MacKay sez . . . 12??</a></p>
<p>Introduction: I’ve recently been reading David MacKay’s 2003  book , Information Theory, Inference, and Learning Algorithms. It’s great background for my Bayesian computation class because he has lots of pictures and detailed discussions of the algorithms.  (Regular readers of this blog will not be surprised to hear that I hate all the  Occam -factor stuff that MacKay talks about, but overall it’s a great book.)
 
Anyway, I happened to notice the following bit, under the heading, “How many samples are needed?”:
  
In many problems, we really only need about twelve independent samples from P(x). Imagine that x is an unknown vector such as the amount of corrosion present in each of 10 000 underground pipelines around Cambridge, and φ(x) is the total cost of repairing those pipelines. The distribution P(x) describes the probability of a state x given the tests that have been carried out on some pipelines and the assumptions about the physics of corrosion. The quantity Φ is the expected cost of the repa</p><p>3 0.9450807 <a title="984-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-12-More_frustrations_trying_to_replicate_an_analysis_published_in_a_reputable_journal.html">1054 andrew gelman stats-2011-12-12-More frustrations trying to replicate an analysis published in a reputable journal</a></p>
<p>Introduction: The story starts in September, when psychology professor Fred Oswald wrote me:
  
I [Oswald] wanted to point out  this paper  in Science (Ramirez & Beilock, 2010) examining how students’ emotional writing improves their test performance in high-pressure situations.


Although replication is viewed as the hallmark of research, this paper replicates implausibly large d-values and correlations across studies, leading me to be more suspicious of the findings (not less, as is generally the case).
  
   
He also pointed me to this paper:
  
Experimental disclosure and its moderators: A meta-analysis.


Frattaroli, Joanne


Psychological Bulletin, Vol 132(6), Nov 2006, 823-865. 


Disclosing information, thoughts, and feelings about personal and meaningful topics (experimental disclosure) is purported to have various health and psychological consequences (e.g., J. W. Pennebaker, 1993). Although the results of 2 small meta-analyses (P. G. Frisina, J. C. Borod, & S. J. Lepore, 2004; J. M. Smyth</p><p>4 0.94302052 <a title="984-lda-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-01-Imputing_count_data.html">14 andrew gelman stats-2010-05-01-Imputing count data</a></p>
<p>Introduction: Guy asks:
  
I am analyzing an original survey of farmers in Uganda. I am hoping to use a battery of welfare proxy variables to create a single welfare index using PCA. I have quick question which I hope you can find time to address:


How do you recommend treating count data? (for example # of rooms, # of chickens, # of cows, # of radios)? In my dataset these variables are highly skewed with many responses at zero (which makes taking the natural log problematic). In the case of # of cows or chickens several obs have values in the hundreds.
  
My response:  Hereâ&euro;&trade;s what we do in our mi package in R.  We split a variable into two parts:  an indicator for whether it is positive, and the positive part.  That is, y = u*v.  Then u is binary and can be modeled using logisitc regression, and v can be modeled on the log scale.  At the end you can round to the nearest integer  if you want to avoid fractional values.</p><p>5 0.94018489 <a title="984-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-27-Bridges_between_deterministic_and_probabilistic_models_for_binary_data.html">780 andrew gelman stats-2011-06-27-Bridges between deterministic and probabilistic models for binary data</a></p>
<p>Introduction: For the analysis of binary data, various deterministic models have been proposed, which are generally simpler to fit and easier to understand than probabilistic models. We claim that corresponding to any deterministic model is an implicit stochastic model in which the deterministic model fits imperfectly, with errors occurring at random. In the context of binary data, we consider a model in which the probability of error depends on the model prediction. We show how to fit this model using a stochastic modification of deterministic optimization schemes.
 
The advantages of fitting the stochastic model explicitly (rather than implicitly, by simply fitting a deterministic model and accepting the occurrence of errors) include quantification of uncertainty in the deterministic model’s parameter estimates, better estimation of the true model error rate, and the ability to check the fit of the model nontrivially. We illustrate this with a simple theoretical example of item response data and w</p><p>6 0.93948865 <a title="984-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-15-World_record_running_times_vs._distance.html">1011 andrew gelman stats-2011-11-15-World record running times vs. distance</a></p>
<p>7 0.93549287 <a title="984-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-09-This_Friday_afternoon%3A__Applied_Statistics_Center_mini-conference_on_risk_perception.html">267 andrew gelman stats-2010-09-09-This Friday afternoon:  Applied Statistics Center mini-conference on risk perception</a></p>
<p>8 0.9255634 <a title="984-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-07-Martyn_Plummer%E2%80%99s_Secret_JAGS_Blog.html">1045 andrew gelman stats-2011-12-07-Martyn Plummer’s Secret JAGS Blog</a></p>
<p>9 0.92537391 <a title="984-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-22-Americans_think_economy_isn%E2%80%99t_so_bad_in_their_city_but_is_crappy_nationally_and_globally.html">1388 andrew gelman stats-2012-06-22-Americans think economy isn’t so bad in their city but is crappy nationally and globally</a></p>
<p>10 0.92407542 <a title="984-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-07-The_more_likely_it_is_to_be_X%2C_the_more_likely_it_is_to_be_Not_X%3F.html">1158 andrew gelman stats-2012-02-07-The more likely it is to be X, the more likely it is to be Not X?</a></p>
<p>11 0.91962492 <a title="984-lda-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-09-Special_journal_issue_on_statistical_methods_for_the_social_sciences.html">24 andrew gelman stats-2010-05-09-Special journal issue on statistical methods for the social sciences</a></p>
<p>12 0.91919494 <a title="984-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-07-Stereotype_threat%21.html">1929 andrew gelman stats-2013-07-07-Stereotype threat!</a></p>
<p>13 0.9156177 <a title="984-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-11-Adding_an_error_model_to_a_deterministic_model.html">1162 andrew gelman stats-2012-02-11-Adding an error model to a deterministic model</a></p>
<p>14 0.9065001 <a title="984-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-24-Bayes_at_the_end.html">534 andrew gelman stats-2011-01-24-Bayes at the end</a></p>
<p>15 0.89651358 <a title="984-lda-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-15-Problematic_interpretations_of_confidence_intervals.html">2248 andrew gelman stats-2014-03-15-Problematic interpretations of confidence intervals</a></p>
<p>16 0.89646906 <a title="984-lda-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-22-Postdoc_opportunity_here_at_Columbia_%E2%80%94_deadline_soon%21.html">426 andrew gelman stats-2010-11-22-Postdoc opportunity here at Columbia — deadline soon!</a></p>
<p>17 0.89500803 <a title="984-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-23-Traditionalist_claims_that_modern_art_could_just_as_well_be_replaced_by_a_%E2%80%9Cpaint-throwing_chimp%E2%80%9D.html">1390 andrew gelman stats-2012-06-23-Traditionalist claims that modern art could just as well be replaced by a “paint-throwing chimp”</a></p>
<p>18 0.89238429 <a title="984-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-01-Don%E2%80%99t_let_your_standard_errors_drive_your_research_agenda.html">1702 andrew gelman stats-2013-02-01-Don’t let your standard errors drive your research agenda</a></p>
<p>19 0.89221239 <a title="984-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-16-Stantastic%21.html">1580 andrew gelman stats-2012-11-16-Stantastic!</a></p>
<p>20 0.89046234 <a title="984-lda-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-22-Quickies.html">2220 andrew gelman stats-2014-02-22-Quickies</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
