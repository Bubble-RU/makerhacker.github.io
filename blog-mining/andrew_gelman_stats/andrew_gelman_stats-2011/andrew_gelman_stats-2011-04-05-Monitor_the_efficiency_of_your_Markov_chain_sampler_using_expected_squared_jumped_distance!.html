<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>650 andrew gelman stats-2011-04-05-Monitor the efficiency of your Markov chain sampler using expected squared jumped distance!</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-650" href="#">andrew_gelman_stats-2011-650</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>650 andrew gelman stats-2011-04-05-Monitor the efficiency of your Markov chain sampler using expected squared jumped distance!</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-650-html" href="http://andrewgelman.com/2011/04/05/monitor_the_eff/">html</a></p><p>Introduction: Marc Tanguay writes in with a specific question that has a very general answer.  First, the question:
  
I [Tanguay] am currently running a MCMC for which I have 3 parameters that are restricted to a specific space. 2 are bounded between 0 and 1 while the third is binary and updated by a Beta-Binomial. Since my priors are also bounded, I notice that, conditional on All the rest (which covers both data and other parameters), the density was not varying a lot within the space of the parameters. As a result, the acceptance rate is high, about 85%, and this despite the fact that all the parameter’s space is explore. Since in your book, the optimal acceptance rates prescribed are lower that 50% (in case of multiple parameters), do you think I should worry about getting 85%.  Or is this normal given the restrictions on the parameters?
  
First off:  Yes, my guess is that you should be taking bigger jumps.  85% seems like too high an acceptance rate for Metropolis jumping.
 
More generally, t</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Marc Tanguay writes in with a specific question that has a very general answer. [sent-1, score-0.089]
</p><p>2 First, the question:    I [Tanguay] am currently running a MCMC for which I have 3 parameters that are restricted to a specific space. [sent-2, score-0.333]
</p><p>3 2 are bounded between 0 and 1 while the third is binary and updated by a Beta-Binomial. [sent-3, score-0.281]
</p><p>4 Since my priors are also bounded, I notice that, conditional on All the rest (which covers both data and other parameters), the density was not varying a lot within the space of the parameters. [sent-4, score-0.311]
</p><p>5 As a result, the acceptance rate is high, about 85%, and this despite the fact that all the parameter’s space is explore. [sent-5, score-0.456]
</p><p>6 Since in your book, the optimal acceptance rates prescribed are lower that 50% (in case of multiple parameters), do you think I should worry about getting 85%. [sent-6, score-0.433]
</p><p>7 Or is this normal given the restrictions on the parameters? [sent-7, score-0.078]
</p><p>8 First off:  Yes, my guess is that you should be taking bigger jumps. [sent-8, score-0.061]
</p><p>9 85% seems like too high an acceptance rate for Metropolis jumping. [sent-9, score-0.42]
</p><p>10 More generally, though, my recommendation is to monitor expected squared jumped distance (ESJD), which is a cleaner measure than acceptance probability. [sent-10, score-1.183]
</p><p>11 Generally, the higher is ESJD, the happier you should be. [sent-11, score-0.071]
</p><p>12 The short story is that if you maximize ESJD, you’re minimizing the first-order autocorrelation. [sent-13, score-0.328]
</p><p>13 And, within any parameterized family of jumping rules, if you minimize the first-order autocorrelation, I think you’ll pretty much be minimizing all of your autocorrelations and maximizing your efficiency. [sent-14, score-0.741]
</p><p>14 As Cristian and I discuss in the paper, you can use a simple Rao-Blackwellization to compute  expected  squared jumped distance, rather than simply  average  squared jumped distance. [sent-15, score-1.098]
</p><p>15 We develop some tricks based on differentiation and importance sampling to adaptively optimize the algorithm to maximize ESJD, but you can always try the crude approach of trying a few different jumping scales, calculating ESJD for each, and then picking the best to go forward. [sent-16, score-0.862]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('esjd', 0.498), ('acceptance', 0.274), ('jumped', 0.241), ('squared', 0.233), ('tanguay', 0.232), ('cristian', 0.211), ('minimizing', 0.17), ('parameters', 0.17), ('maximize', 0.158), ('bounded', 0.153), ('jumping', 0.147), ('distance', 0.126), ('adaptively', 0.1), ('space', 0.099), ('optimize', 0.095), ('prescribed', 0.092), ('autocorrelations', 0.092), ('expected', 0.091), ('autocorrelation', 0.089), ('maximizing', 0.089), ('specific', 0.089), ('parameterized', 0.087), ('rate', 0.083), ('minimize', 0.082), ('differentiation', 0.082), ('cleaner', 0.082), ('scales', 0.078), ('restrictions', 0.078), ('monitor', 0.078), ('covers', 0.075), ('tricks', 0.075), ('calculating', 0.075), ('within', 0.074), ('generally', 0.074), ('metropolis', 0.074), ('restricted', 0.074), ('marc', 0.073), ('happier', 0.071), ('updated', 0.069), ('crude', 0.068), ('optimal', 0.067), ('density', 0.063), ('mcmc', 0.063), ('high', 0.063), ('picking', 0.062), ('bigger', 0.061), ('since', 0.06), ('binary', 0.059), ('compute', 0.059), ('recommendation', 0.058)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="650-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-05-Monitor_the_efficiency_of_your_Markov_chain_sampler_using_expected_squared_jumped_distance%21.html">650 andrew gelman stats-2011-04-05-Monitor the efficiency of your Markov chain sampler using expected squared jumped distance!</a></p>
<p>Introduction: Marc Tanguay writes in with a specific question that has a very general answer.  First, the question:
  
I [Tanguay] am currently running a MCMC for which I have 3 parameters that are restricted to a specific space. 2 are bounded between 0 and 1 while the third is binary and updated by a Beta-Binomial. Since my priors are also bounded, I notice that, conditional on All the rest (which covers both data and other parameters), the density was not varying a lot within the space of the parameters. As a result, the acceptance rate is high, about 85%, and this despite the fact that all the parameter’s space is explore. Since in your book, the optimal acceptance rates prescribed are lower that 50% (in case of multiple parameters), do you think I should worry about getting 85%.  Or is this normal given the restrictions on the parameters?
  
First off:  Yes, my guess is that you should be taking bigger jumps.  85% seems like too high an acceptance rate for Metropolis jumping.
 
More generally, t</p><p>2 0.2941128 <a title="650-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-28-Understanding_simulations_in_terms_of_predictive_inference%3F.html">1287 andrew gelman stats-2012-04-28-Understanding simulations in terms of predictive inference?</a></p>
<p>Introduction: David Hogg writes:
  
My (now deceased) collaborator and guru in all things inference, Sam Roweis, used to emphasize to me that we should evaluate models in the data space — not the parameter space — because models are always effectively “effective” and not really, fundamentally true. Or, in other words, models should be compared in the space of their predictions, not in the space of their parameters (the  parameters didn’t really “exist” at all for Sam).  In that spirit, when we estimate the effectiveness of a MCMC method or tuning — by autocorrelation time or ESJD or anything else — shouldn’t we be looking at the changes in the model predictions over time, rather than the changes in the parameters over time?  That is, the autocorrelation time should be the autocorrelation time in what the model (at the walker position) predicts for the data, and the ESJD should be the expected squared jump distance in what the model predicts for the data?  This might resolve the concern I expressed a</p><p>3 0.15575053 <a title="650-tfidf-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-12-%E2%80%9CThe_results_%28not_shown%29_._._.%E2%80%9D.html">2332 andrew gelman stats-2014-05-12-“The results (not shown) . . .”</a></p>
<p>Introduction: Pro tip:  Don’t believe any claims about results not shown in a paper.  Even if the paper has been published.  Even if it’s been cited hundreds of times.  If the results aren’t shown, they haven’t been checked.
 
 
 
I learned this the hard way after receiving this note from Bin Liu, who wrote:
  
Today I saw  a paper  [by Ziheng Yang and Carlos Rodríguez] titled “Searching for efficient Markov chain Monte Carlo proposal kernels.”  The authors cited your work: “Gelman A, Roberts GO, Gilks WR (1996) Bayesian Statistics 5, eds Bernardo JM, et al. (Oxford Univ Press, Oxford), Vol 5, pp 599-607″, i.e. ref.6 in the paper.


In the last sentence of pp.19310, the authors write that “… virtually no study has examined alternative kernels; this appears to be due to the influence of ref. 6, which claimed that different kernels had nearly identical performance. This conclusion is incorrect.”
  
 Here’s  our paper, and here’s the offending quote, which appeared after we discussed results for the no</p><p>4 0.14046066 <a title="650-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-17-Jumping_off_the_edge_of_the_world.html">858 andrew gelman stats-2011-08-17-Jumping off the edge of the world</a></p>
<p>Introduction: Tomas Iesmantas writes:
  
I’m facing a problem where parameter space is bounded, e.g. all parameters have to be positive. If in MCMC as proposal distribution I use normal distribution, then at some iterations I get negative proposals. So my question is: should I use recalculation of acceptance probability every time I reject the proposal (something like in delayed rejection method), or I have to use another proposal (like lognormal, truncated normal, etc.)?
  
The simplest solution is to just calculate p(theta)=0 for theta outside the legal region, thus reject those jumps.  This will work fine (just remember that when you reject, you have to stay at the last value for one more iteration), but if you’re doing these rejections all the time, you might want to reparameterize your space, for example using logs for positive parameters, logits for constrained parameters, and softmax for parameters that are constrained to sum to 1.</p><p>5 0.10684743 <a title="650-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-19-Prior_distributions_on_derived_quantities_rather_than_on_parameters_themselves.html">1946 andrew gelman stats-2013-07-19-Prior distributions on derived quantities rather than on parameters themselves</a></p>
<p>Introduction: Following up on our  discussion of the other day , Nick Firoozye writes: 
  
  
One thing I meant by my initial query (but really didn’t manage to get across) was this: I have no idea what my prior would be on many many models, but just like Utility Theory expects ALL consumers to attach a utility to any and all consumption goods (even those I haven’t seen or heard of), Bayesian Stats (almost) expects the same for priors. (Of course it’s not a religious edict much in the way Utility Theory has, since there is no theory of a “modeler” in the Bayesian paradigm—nonetheless there is still an expectation that we should have priors over all sorts of parameters which mean almost nothing to us).


For most models with sufficient complexity, I also have no idea what my informative priors are actually doing and the only way to know anything is through something I can see and experience, through data, not parameters or state variables.


My question was more on the—let’s use the prior to come up</p><p>6 0.098845065 <a title="650-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-25-It%E2%80%99s_binless%21__A_program_for_computing_normalizing_functions.html">1825 andrew gelman stats-2013-04-25-It’s binless!  A program for computing normalizing functions</a></p>
<p>7 0.089790016 <a title="650-tfidf-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-08-Belief_aggregation.html">2162 andrew gelman stats-2014-01-08-Belief aggregation</a></p>
<p>8 0.081290469 <a title="650-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<p>9 0.080950305 <a title="650-tfidf-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-23-Unhappy_with_improvement_by_a_factor_of_10%5E29.html">160 andrew gelman stats-2010-07-23-Unhappy with improvement by a factor of 10^29</a></p>
<p>10 0.077307016 <a title="650-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-05-Xiao-Li_Meng_and_Xianchao_Xie_rethink_asymptotics.html">1406 andrew gelman stats-2012-07-05-Xiao-Li Meng and Xianchao Xie rethink asymptotics</a></p>
<p>11 0.077049479 <a title="650-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-19-Will_Stan_work_well_with_40%C3%9740_matrices%3F.html">861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</a></p>
<p>12 0.072958805 <a title="650-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-10-Cross-validation_and_Bayesian_estimation_of_tuning_parameters.html">2129 andrew gelman stats-2013-12-10-Cross-validation and Bayesian estimation of tuning parameters</a></p>
<p>13 0.069954716 <a title="650-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>14 0.068112418 <a title="650-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-31-Untunable_Metropolis.html">833 andrew gelman stats-2011-07-31-Untunable Metropolis</a></p>
<p>15 0.066827975 <a title="650-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-29-More_by_Berger_and_me_on_weakly_informative_priors.html">1092 andrew gelman stats-2011-12-29-More by Berger and me on weakly informative priors</a></p>
<p>16 0.066582255 <a title="650-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-27-%28R-Py-Cmd%29Stan_2.1.0.html">2150 andrew gelman stats-2013-12-27-(R-Py-Cmd)Stan 2.1.0</a></p>
<p>17 0.06592191 <a title="650-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-24-Bleg%3A_Automatic_Differentiation_for_Log_Prob_Gradients%3F.html">535 andrew gelman stats-2011-01-24-Bleg: Automatic Differentiation for Log Prob Gradients?</a></p>
<p>18 0.065829873 <a title="650-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-29-How_many_parameters_are_in_a_multilevel_model%3F.html">1144 andrew gelman stats-2012-01-29-How many parameters are in a multilevel model?</a></p>
<p>19 0.065628767 <a title="650-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-09-Thomas_Hobbes_would_be_spinning_in_his_grave.html">1715 andrew gelman stats-2013-02-09-Thomas Hobbes would be spinning in his grave</a></p>
<p>20 0.064418107 <a title="650-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-The_reverse-journal-submission_system.html">1393 andrew gelman stats-2012-06-26-The reverse-journal-submission system</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.108), (1, 0.054), (2, 0.028), (3, 0.016), (4, 0.015), (5, -0.007), (6, 0.055), (7, -0.023), (8, -0.036), (9, 0.009), (10, -0.0), (11, -0.012), (12, -0.023), (13, -0.009), (14, -0.002), (15, -0.038), (16, 0.008), (17, 0.019), (18, -0.006), (19, -0.025), (20, 0.007), (21, 0.011), (22, 0.002), (23, -0.001), (24, 0.017), (25, 0.008), (26, -0.03), (27, 0.038), (28, 0.032), (29, 0.009), (30, 0.013), (31, -0.005), (32, -0.01), (33, -0.001), (34, -0.014), (35, -0.009), (36, -0.007), (37, -0.017), (38, 0.0), (39, 0.01), (40, -0.015), (41, 0.012), (42, -0.034), (43, 0.022), (44, -0.025), (45, -0.037), (46, -0.009), (47, 0.016), (48, 0.059), (49, 0.019)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94110858 <a title="650-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-05-Monitor_the_efficiency_of_your_Markov_chain_sampler_using_expected_squared_jumped_distance%21.html">650 andrew gelman stats-2011-04-05-Monitor the efficiency of your Markov chain sampler using expected squared jumped distance!</a></p>
<p>Introduction: Marc Tanguay writes in with a specific question that has a very general answer.  First, the question:
  
I [Tanguay] am currently running a MCMC for which I have 3 parameters that are restricted to a specific space. 2 are bounded between 0 and 1 while the third is binary and updated by a Beta-Binomial. Since my priors are also bounded, I notice that, conditional on All the rest (which covers both data and other parameters), the density was not varying a lot within the space of the parameters. As a result, the acceptance rate is high, about 85%, and this despite the fact that all the parameter’s space is explore. Since in your book, the optimal acceptance rates prescribed are lower that 50% (in case of multiple parameters), do you think I should worry about getting 85%.  Or is this normal given the restrictions on the parameters?
  
First off:  Yes, my guess is that you should be taking bigger jumps.  85% seems like too high an acceptance rate for Metropolis jumping.
 
More generally, t</p><p>2 0.83089298 <a title="650-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<p>Introduction: Tomas Iesmantas had  asked me  for advice on a regression problem with 50 parameters, and I’d recommended Hamiltonian Monte Carlo.
 
A few weeks later he reported back: 
  
  
After trying several modifications (HMC for all parameters at once, HMC just for first level parameters and Riemman manifold Hamiltonian Monte Carlo method), I finally got it running with HMC just for first level parameters and for others using direct sampling, since conditional distributions turned out to have closed form.


However, even in this case it is quite tricky, since I had to employ mass matrix and not just diagonal but at the beginning of algorithm generated it randomly (ensuring it is positive definite). Such random generation of mass matrix is quite blind step, but it proved to be quite helpful.


Riemman manifold HMC is quite vagarious, or to be more specific, metric of manifold is very sensitive. In my model log-likelihood I had exponents and values of metrics matrix elements was very large and wh</p><p>3 0.76224935 <a title="650-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-17-Jumping_off_the_edge_of_the_world.html">858 andrew gelman stats-2011-08-17-Jumping off the edge of the world</a></p>
<p>Introduction: Tomas Iesmantas writes:
  
I’m facing a problem where parameter space is bounded, e.g. all parameters have to be positive. If in MCMC as proposal distribution I use normal distribution, then at some iterations I get negative proposals. So my question is: should I use recalculation of acceptance probability every time I reject the proposal (something like in delayed rejection method), or I have to use another proposal (like lognormal, truncated normal, etc.)?
  
The simplest solution is to just calculate p(theta)=0 for theta outside the legal region, thus reject those jumps.  This will work fine (just remember that when you reject, you have to stay at the last value for one more iteration), but if you’re doing these rejections all the time, you might want to reparameterize your space, for example using logs for positive parameters, logits for constrained parameters, and softmax for parameters that are constrained to sum to 1.</p><p>4 0.75765449 <a title="650-lsi-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-12-%E2%80%9CThe_results_%28not_shown%29_._._.%E2%80%9D.html">2332 andrew gelman stats-2014-05-12-“The results (not shown) . . .”</a></p>
<p>Introduction: Pro tip:  Don’t believe any claims about results not shown in a paper.  Even if the paper has been published.  Even if it’s been cited hundreds of times.  If the results aren’t shown, they haven’t been checked.
 
 
 
I learned this the hard way after receiving this note from Bin Liu, who wrote:
  
Today I saw  a paper  [by Ziheng Yang and Carlos Rodríguez] titled “Searching for efficient Markov chain Monte Carlo proposal kernels.”  The authors cited your work: “Gelman A, Roberts GO, Gilks WR (1996) Bayesian Statistics 5, eds Bernardo JM, et al. (Oxford Univ Press, Oxford), Vol 5, pp 599-607″, i.e. ref.6 in the paper.


In the last sentence of pp.19310, the authors write that “… virtually no study has examined alternative kernels; this appears to be due to the influence of ref. 6, which claimed that different kernels had nearly identical performance. This conclusion is incorrect.”
  
 Here’s  our paper, and here’s the offending quote, which appeared after we discussed results for the no</p><p>5 0.72701657 <a title="650-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<p>Introduction: From August 1990.  It was in the form of a note sent to all the people in the statistics group of Bell Labs, where I’d worked that summer.
  
To all:


Here’s the abstract of the work I’ve done this summer.  It’s stored in the file, 
/fs5/gelman/abstract.bell, and copies of the Figures 1-3 are on Trevor’s desk. 
Any comments are of course appreciated; I’m at gelman@stat.berkeley.edu.


On the Routine Use of Markov Chains for Simulation


Andrew Gelman and Donald Rubin, 6 August 1990


corrected version:  8 August 1990
  
  
  
1.  Simulation


In probability and statistics we can often specify multivariate distributions 
many of whose properties we do not fully understand–perhaps, as in the 
Ising model of statistical physics, we can write the joint density function, up 
to a multiplicative constant that cannot be expressed in closed form. 
For an example in statistics, consider the Normal random 
effects model in the analysis of variance, which can be 
easily placed in a Bayesian fram</p><p>6 0.71837169 <a title="650-lsi-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-23-Unhappy_with_improvement_by_a_factor_of_10%5E29.html">160 andrew gelman stats-2010-07-23-Unhappy with improvement by a factor of 10^29</a></p>
<p>7 0.71208417 <a title="650-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<p>8 0.7021628 <a title="650-lsi-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-Running_into_a_Stan_Reference_by_Accident.html">2231 andrew gelman stats-2014-03-03-Running into a Stan Reference by Accident</a></p>
<p>9 0.69374108 <a title="650-lsi-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-10-R_vs._Stata%2C_or%2C_Different_ways_to_estimate_multilevel_models.html">269 andrew gelman stats-2010-09-10-R vs. Stata, or, Different ways to estimate multilevel models</a></p>
<p>10 0.68513405 <a title="650-lsi-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-20-Thermodynamic_Monte_Carlo%3A__Michael_Betancourt%E2%80%99s_new_method_for_simulating_from_difficult_distributions_and_evaluating_normalizing_constants.html">2340 andrew gelman stats-2014-05-20-Thermodynamic Monte Carlo:  Michael Betancourt’s new method for simulating from difficult distributions and evaluating normalizing constants</a></p>
<p>11 0.67698318 <a title="650-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-01-David_MacKay_sez_._._._12%3F%3F.html">984 andrew gelman stats-2011-11-01-David MacKay sez . . . 12??</a></p>
<p>12 0.6754294 <a title="650-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-21-Handbook_of_Markov_Chain_Monte_Carlo.html">674 andrew gelman stats-2011-04-21-Handbook of Markov Chain Monte Carlo</a></p>
<p>13 0.67232007 <a title="650-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-24-Bleg%3A_Automatic_Differentiation_for_Log_Prob_Gradients%3F.html">535 andrew gelman stats-2011-01-24-Bleg: Automatic Differentiation for Log Prob Gradients?</a></p>
<p>14 0.66468656 <a title="650-lsi-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Bayesian_Uncertainty_Quantification_for_Differential_Equations%21.html">2311 andrew gelman stats-2014-04-29-Bayesian Uncertainty Quantification for Differential Equations!</a></p>
<p>15 0.66167468 <a title="650-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-25-Bayes-respecting_experimental_design_and_other_things.html">1955 andrew gelman stats-2013-07-25-Bayes-respecting experimental design and other things</a></p>
<p>16 0.65080678 <a title="650-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-07-Chi-square_FAIL_when_many_cells_have_small_expected_values.html">996 andrew gelman stats-2011-11-07-Chi-square FAIL when many cells have small expected values</a></p>
<p>17 0.65071064 <a title="650-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>18 0.64741313 <a title="650-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-10-Cross-validation_and_Bayesian_estimation_of_tuning_parameters.html">2129 andrew gelman stats-2013-12-10-Cross-validation and Bayesian estimation of tuning parameters</a></p>
<p>19 0.64567339 <a title="650-lsi-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>20 0.6456216 <a title="650-lsi-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-31-Somewhat_Bayesian_multilevel_modeling.html">246 andrew gelman stats-2010-08-31-Somewhat Bayesian multilevel modeling</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.018), (6, 0.345), (9, 0.026), (16, 0.057), (20, 0.026), (24, 0.137), (42, 0.015), (74, 0.01), (84, 0.014), (86, 0.032), (99, 0.197)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.89984584 <a title="650-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-05-Monitor_the_efficiency_of_your_Markov_chain_sampler_using_expected_squared_jumped_distance%21.html">650 andrew gelman stats-2011-04-05-Monitor the efficiency of your Markov chain sampler using expected squared jumped distance!</a></p>
<p>Introduction: Marc Tanguay writes in with a specific question that has a very general answer.  First, the question:
  
I [Tanguay] am currently running a MCMC for which I have 3 parameters that are restricted to a specific space. 2 are bounded between 0 and 1 while the third is binary and updated by a Beta-Binomial. Since my priors are also bounded, I notice that, conditional on All the rest (which covers both data and other parameters), the density was not varying a lot within the space of the parameters. As a result, the acceptance rate is high, about 85%, and this despite the fact that all the parameter’s space is explore. Since in your book, the optimal acceptance rates prescribed are lower that 50% (in case of multiple parameters), do you think I should worry about getting 85%.  Or is this normal given the restrictions on the parameters?
  
First off:  Yes, my guess is that you should be taking bigger jumps.  85% seems like too high an acceptance rate for Metropolis jumping.
 
More generally, t</p><p>2 0.81121844 <a title="650-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-19-I_got_one_of_these_letters_once_and_was_so_irritated_that_I_wrote_back_to_the_journal_withdrawing_my_paper.html">1069 andrew gelman stats-2011-12-19-I got one of these letters once and was so irritated that I wrote back to the journal withdrawing my paper</a></p>
<p>Introduction: Iâ&euro;&trade;m talkin bout  this .</p><p>3 0.80603844 <a title="650-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-25-Diving_chess.html">1638 andrew gelman stats-2012-12-25-Diving chess</a></p>
<p>Introduction: Knowing of my interest in  Turing run-around-the-house chess , David Lockhart points me to  this :
  
Diving Chess is a chess variant, which is played in a swimming pool. Instead of using chess clocks, each player must submerge themselves underwater during their turn, only to resurface when they are ready to make a move. Players must make a move within 5 seconds of resurfacing (they will receive a warning if not, and three warnings will result in a forfeit). Diving Chess was invented by American Chess Master Etan Ilfeld; the very first exhibition game took place between Ilfeld and former British Chess Champion William Hartston at the Thirdspace gym in Soho on August 2nd, 2011. Hartston won the match which lasted almost two hours such that each player was underwater for an entire hour.</p><p>4 0.8055346 <a title="650-lda-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-21-Busted%21.html">221 andrew gelman stats-2010-08-21-Busted!</a></p>
<p>Introduction: I’m just glad that universities don’t  sanction  professors for publishing false theorems.
 
If the guy really is nailed by the feds for fraud, I hope they don’t throw him in prison.  In general, prison time seems like a brutal, expensive, and inefficient way to punish people.  I’d prefer if the government just took 95% of his salary for several years, made him do community service (cleaning equipment at the local sewage treatment plant, perhaps; a lab scientist should be good at this sort of thing, no?), etc.  If restriction of this dude’s personal freedom is judged be part of the sentence, he could be given some sort of electronic tag that would send a message to the police if he were ever more than 3 miles from his home.  But no need to bill the taxpayers for the cost of keeping him in prison.</p><p>5 0.79959846 <a title="650-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-06-The_new_Stan_1.1.1%2C_featuring_Gaussian_processes%21.html">1710 andrew gelman stats-2013-02-06-The new Stan 1.1.1, featuring Gaussian processes!</a></p>
<p>Introduction: We just released Stan 1.1.1 and RStan 1.1.1
 
As usual, you can find download and install instructions at:
 
http://mc-stan.org/
 
This is a patch release and is fully backward compatible with Stan and RStan 1.1.0.  The main thing you should notice is that the multivariate models should be much faster and all the bugs reported for 1.1.0 have been fixed.  We’ve also added a bit more functionality.  The substantial changes are listed in the following release notes. 
   
v1.1.1 (5 February 2012) 
======================================================================
 
Bug Fixes 
———————————- 
* fixed bug in comparison operators, which swapped operator< with operator<= and swapped operator> with operator>= semantics 
* auto-initialize all variables to prevent segfaults 
* atan2 gradient propagation fixed 
* fixed off-by-one in NUTS treedepth bound so NUTS goes at most to specified tree depth rather than specified depth + 1 
* various compiler compatibility and minor consistency issues 
* f</p><p>6 0.76475662 <a title="650-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-13-Stan_at_NIPS_2012_Workshop_on_Probabilistic_Programming.html">1576 andrew gelman stats-2012-11-13-Stan at NIPS 2012 Workshop on Probabilistic Programming</a></p>
<p>7 0.76226187 <a title="650-lda-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-03-%E2%80%9CThe_graph_clearly_shows_that_mammography_adds_virtually_nothing_to_survival_and_if_anything%2C_decreases_survival_%28and_increases_cost_and_provides_unnecessary_treatment%29%E2%80%9D.html">2316 andrew gelman stats-2014-05-03-“The graph clearly shows that mammography adds virtually nothing to survival and if anything, decreases survival (and increases cost and provides unnecessary treatment)”</a></p>
<p>8 0.74485886 <a title="650-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-19-%E2%80%9CBehind_a_cancer-treatment_firm%E2%80%99s_rosy_survival_claims%E2%80%9D.html">1906 andrew gelman stats-2013-06-19-“Behind a cancer-treatment firm’s rosy survival claims”</a></p>
<p>9 0.73310566 <a title="650-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-16-Gaydar_update%3A__Additional_research_on_estimating_small_fractions_of_the_population.html">150 andrew gelman stats-2010-07-16-Gaydar update:  Additional research on estimating small fractions of the population</a></p>
<p>10 0.71953058 <a title="650-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-24-Don%E2%80%99t_idealize_%E2%80%9Crisk_aversion%E2%80%9D.html">819 andrew gelman stats-2011-07-24-Don’t idealize “risk aversion”</a></p>
<p>11 0.70769316 <a title="650-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-03-Kuhn%2C_1-f_noise%2C_and_the_fractal_nature_of_scientific_revolutions.html">1924 andrew gelman stats-2013-07-03-Kuhn, 1-f noise, and the fractal nature of scientific revolutions</a></p>
<p>12 0.70693672 <a title="650-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-18-Prior_information_._._._about_the_likelihood.html">618 andrew gelman stats-2011-03-18-Prior information . . . about the likelihood</a></p>
<p>13 0.69018388 <a title="650-lda-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-28-On_deck_this_week.html">2310 andrew gelman stats-2014-04-28-On deck this week</a></p>
<p>14 0.67031574 <a title="650-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-08-Is_linear_regression_unethical_in_that_it_gives_more_weight_to_cases_that_are_far_from_the_average%3F.html">1409 andrew gelman stats-2012-07-08-Is linear regression unethical in that it gives more weight to cases that are far from the average?</a></p>
<p>15 0.66009235 <a title="650-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-12-Plaig%21.html">2098 andrew gelman stats-2013-11-12-Plaig!</a></p>
<p>16 0.65575814 <a title="650-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-15-%E2%80%9CI_coach_the_jumpers_here_at_Boise_State_._._.%E2%80%9D.html">1625 andrew gelman stats-2012-12-15-“I coach the jumpers here at Boise State . . .”</a></p>
<p>17 0.65530008 <a title="650-lda-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-09-San_Fernando_Valley_cityscapes%3A__An_example_of_the_benefits_of_fractal_devastation%3F.html">2165 andrew gelman stats-2014-01-09-San Fernando Valley cityscapes:  An example of the benefits of fractal devastation?</a></p>
<p>18 0.65243441 <a title="650-lda-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-12-%E2%80%9CThe_results_%28not_shown%29_._._.%E2%80%9D.html">2332 andrew gelman stats-2014-05-12-“The results (not shown) . . .”</a></p>
<p>19 0.6467061 <a title="650-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-25-Further_evidence_of_a_longstanding_principle_of_statistics.html">1082 andrew gelman stats-2011-12-25-Further evidence of a longstanding principle of statistics</a></p>
<p>20 0.64373213 <a title="650-lda-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-13-CmdStan%2C_RStan%2C_PyStan_v2.2.0.html">2209 andrew gelman stats-2014-02-13-CmdStan, RStan, PyStan v2.2.0</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
