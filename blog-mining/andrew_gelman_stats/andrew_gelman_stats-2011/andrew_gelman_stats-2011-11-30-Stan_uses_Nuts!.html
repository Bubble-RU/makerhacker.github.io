<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1036 andrew gelman stats-2011-11-30-Stan uses Nuts!</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-1036" href="#">andrew_gelman_stats-2011-1036</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1036 andrew gelman stats-2011-11-30-Stan uses Nuts!</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-1036-html" href="http://andrewgelman.com/2011/11/30/stan-uses-nuts/">html</a></p><p>Introduction: We interrupt our usual program of  Ed Wegman   Gregg Easterbrook  Niall Ferguson mockery to deliver a serious update on our statistical computing project.
 
 Stan  (“Sampling Through Adaptive Neighborhoods”) is our new C++ program (written mostly by Bob Carpenter) that draws samples from Bayesian models.  Stan can take different sorts of inputs:  you can write the model in a Bugs-like syntax and it goes from there, or you can write the log-posterior directly as a C++ function.
 
Most of the computation is done using Hamiltonian Monte Carlo.  HMC requires some tuning, so Matt Hoffman up and wrote a new algorithm, Nuts (the “No-U-Turn Sampler”) which optimizes HMC adaptively.  In many settings, Nuts is actually more computationally efficient than the optimal static HMC!
 
 When the  the Nuts paper  appeared on Arxiv, Christian Robert noticed it and had  some reactions .
 
In response to Xian’s comments, Matt writes:
  

Christian writes:

 
I wonder about the computing time (and the “una</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We interrupt our usual program of  Ed Wegman   Gregg Easterbrook  Niall Ferguson mockery to deliver a serious update on our statistical computing project. [sent-1, score-0.289]
</p><p>2 Most of the computation is done using Hamiltonian Monte Carlo. [sent-4, score-0.077]
</p><p>3 In many settings, Nuts is actually more computationally efficient than the optimal static HMC! [sent-6, score-0.13]
</p><p>4 In response to Xian’s comments, Matt writes:     Christian writes:    I wonder about the computing time (and the “unacceptably large amount of memory”, p. [sent-8, score-0.172]
</p><p>5 12) required by the doubling procedure: 2^j is growing fast with j! [sent-9, score-0.188]
</p><p>6 (If my intuition is right, the computing time should increase rather quickly with the dimension. [sent-10, score-0.243]
</p><p>7 And I do not get the argument within the paper that the costly part is the gradient computation: it seems to me the gradient must be computed for all of the 2^j points. [sent-11, score-0.414]
</p><p>8 )     2 j  does grow quickly with j, but so does the length of the trajectory, and it’s impossible to run a Hamiltonian trajectory for a seriously long time without making a U turn and stopping the doubling process. [sent-12, score-0.867]
</p><p>9 (Just like it’s impossible to throw a ball out of an infinitely deep pit with a finite amount of energy. [sent-13, score-0.198]
</p><p>10 As far as memory goes, the “naive” implementation (algorithm 2) has to store all O(2 j ) states it visits, but the more sophisticated implementation (algorithm 3) only needs to store O(j) states. [sent-15, score-0.63]
</p><p>11 Finally, the gradient computations dominate precisely because we must compute 2 j  of them—NUTS introduces O(2 j j) non-gradient overhead, which is usually trivial compared to O(2 j ) gradient computations. [sent-16, score-0.494]
</p><p>12 These costs scale linearly with dimension, like HMC’s costs. [sent-18, score-0.128]
</p><p>13 Trajectory lengths will generally increase faster than linearly with dimension, i. [sent-19, score-0.313]
</p><p>14 But the optimal trajectory length for HMC does as well, and in high dimensions HMC is pretty much the best we’ve got. [sent-22, score-0.553]
</p><p>15 (Unless you can exploit problem structure in some really really clever ways [or for some specific models with lots of independence structure--ed. [sent-23, score-0.129]
</p><p>16 There’s also a c++ implementation in Stan, but it’s not (yet) well documented. [sent-27, score-0.122]
</p><p>17 I guess I [Matt] should also write python and R implementations, although my R expertise doesn’t extend very far beyond what’s needed to use ggplot2. [sent-28, score-0.173]
</p><p>18 And in any case, Stan will have the fast C++ version. [sent-30, score-0.073]
</p><p>19 You’ll be able to run it from R just like you can run Bugs or Jags. [sent-35, score-0.228]
</p><p>20 Compared to Bugs and Jags, Stan should be faster (especially for big models) and should be able to fit a wider range of models (for example, varying-intercept, varying-slope multilevel models with multiple coefficients per group). [sent-36, score-0.334]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hmc', 0.337), ('trajectory', 0.297), ('nuts', 0.282), ('matt', 0.223), ('gradient', 0.207), ('overhead', 0.184), ('stan', 0.178), ('memory', 0.134), ('optimal', 0.13), ('linearly', 0.128), ('faster', 0.126), ('length', 0.126), ('christian', 0.124), ('implementation', 0.122), ('doubling', 0.115), ('matlab', 0.112), ('computing', 0.11), ('algorithm', 0.108), ('python', 0.107), ('xian', 0.102), ('grow', 0.1), ('store', 0.093), ('hamiltonian', 0.089), ('dimension', 0.085), ('bugs', 0.082), ('compared', 0.08), ('run', 0.078), ('impossible', 0.077), ('computation', 0.077), ('bob', 0.075), ('quickly', 0.074), ('fast', 0.073), ('able', 0.072), ('models', 0.068), ('optimizes', 0.068), ('far', 0.066), ('unacceptably', 0.064), ('amount', 0.062), ('exploit', 0.061), ('trajectories', 0.061), ('interrupt', 0.061), ('negligible', 0.061), ('mockery', 0.059), ('infinitely', 0.059), ('program', 0.059), ('increase', 0.059), ('visits', 0.058), ('proprietary', 0.058), ('syntax', 0.058), ('carpenter', 0.058)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="1036-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-30-Stan_uses_Nuts%21.html">1036 andrew gelman stats-2011-11-30-Stan uses Nuts!</a></p>
<p>Introduction: We interrupt our usual program of  Ed Wegman   Gregg Easterbrook  Niall Ferguson mockery to deliver a serious update on our statistical computing project.
 
 Stan  (“Sampling Through Adaptive Neighborhoods”) is our new C++ program (written mostly by Bob Carpenter) that draws samples from Bayesian models.  Stan can take different sorts of inputs:  you can write the model in a Bugs-like syntax and it goes from there, or you can write the log-posterior directly as a C++ function.
 
Most of the computation is done using Hamiltonian Monte Carlo.  HMC requires some tuning, so Matt Hoffman up and wrote a new algorithm, Nuts (the “No-U-Turn Sampler”) which optimizes HMC adaptively.  In many settings, Nuts is actually more computationally efficient than the optimal static HMC!
 
 When the  the Nuts paper  appeared on Arxiv, Christian Robert noticed it and had  some reactions .
 
In response to Xian’s comments, Matt writes:
  

Christian writes:

 
I wonder about the computing time (and the “una</p><p>2 0.23439893 <a title="1036-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<p>Introduction: Tomas Iesmantas had  asked me  for advice on a regression problem with 50 parameters, and I’d recommended Hamiltonian Monte Carlo.
 
A few weeks later he reported back: 
  
  
After trying several modifications (HMC for all parameters at once, HMC just for first level parameters and Riemman manifold Hamiltonian Monte Carlo method), I finally got it running with HMC just for first level parameters and for others using direct sampling, since conditional distributions turned out to have closed form.


However, even in this case it is quite tricky, since I had to employ mass matrix and not just diagonal but at the beginning of algorithm generated it randomly (ensuring it is positive definite). Such random generation of mass matrix is quite blind step, but it proved to be quite helpful.


Riemman manifold HMC is quite vagarious, or to be more specific, metric of manifold is very sensitive. In my model log-likelihood I had exponents and values of metrics matrix elements was very large and wh</p><p>3 0.22002137 <a title="1036-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-04-PyStan%21.html">1748 andrew gelman stats-2013-03-04-PyStan!</a></p>
<p>Introduction: Stan  is written in C++ and can be run from the command line and from R.  We’d like for  Python  users to be able to run Stan as well.  If anyone is interested in doing this, please let us know and we’d be happy to work with you on it.
 
Stan, like Python, is completely free and open-source.
 
P.S.  Because Stan is open-source, it of course would also be possible for people to translate Stan into Python, or to take whatever features they like from Stan and incorporate them into a Python package.  That’s fine too.  But we think it would make sense in addition for users to be able to run Stan directly from Python, in the same way that it can be run from R.</p><p>4 0.18656103 <a title="1036-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-20-Stan_at_Google_this_Thurs_and_at_Berkeley_this_Fri_noon.html">1772 andrew gelman stats-2013-03-20-Stan at Google this Thurs and at Berkeley this Fri noon</a></p>
<p>Introduction: Michael Betancourt  will be speaking at Google and at the University of California, Berkeley.  The Google talk is closed to outsiders (but if you work at Google, you should go!); the Berkeley talk is open to all:
  
Friday March 22, 12:10 pm, Evans Hall 1011.


Title of talk:  Stan : Practical Bayesian Inference with Hamiltonian Monte Carlo


Abstract: Practical implementations of Bayesian inference are often limited to approximation methods that only slowly explore the posterior distribution.  By taking advantage of the curvature of the posterior, however, Hamiltonian Monte Carlo (HMC) efficiently explores even the most highly contorted distributions.  In this talk I will review the foundations of and recent developments within HMC, concluding with a discussion of Stan, a powerful inference engine that utilizes HMC, automatic differentiation, and adaptive methods to minimize user input.
  
This is cool stuff.  And heâ&euro;&trade;ll be showing the whirlpool movie!</p><p>5 0.17835897 <a title="1036-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-17-NUTS_discussed_on_Xi%E2%80%99an%E2%80%99s_Og.html">1809 andrew gelman stats-2013-04-17-NUTS discussed on Xi’an’s Og</a></p>
<p>Introduction: Xi’an’s Og  (aka Christian Robert’s blog) is featuring a very nice  presentation of NUTS  by Marco Banterle, with discussion and some suggestions.
 
I’m not even sure how they found Michael Betancourt’s paper on geometric NUTS — I don’t see it on the arXiv yet, or I’d provide a link.</p><p>6 0.16490771 <a title="1036-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-04-Stan_in_L.A._this_Wed_3%3A30pm.html">1749 andrew gelman stats-2013-03-04-Stan in L.A. this Wed 3:30pm</a></p>
<p>7 0.15606688 <a title="1036-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-26-%E2%80%9CPlease_make_fun_of_this_claim%E2%80%9D.html">2114 andrew gelman stats-2013-11-26-“Please make fun of this claim”</a></p>
<p>8 0.15520316 <a title="1036-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-A_Stan_is_Born.html">1475 andrew gelman stats-2012-08-30-A Stan is Born</a></p>
<p>9 0.14720187 <a title="1036-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-27-%28R-Py-Cmd%29Stan_2.1.0.html">2150 andrew gelman stats-2013-12-27-(R-Py-Cmd)Stan 2.1.0</a></p>
<p>10 0.14078894 <a title="1036-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-16-Stantastic%21.html">1580 andrew gelman stats-2012-11-16-Stantastic!</a></p>
<p>11 0.14002292 <a title="1036-tfidf-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-Running_into_a_Stan_Reference_by_Accident.html">2231 andrew gelman stats-2014-03-03-Running into a Stan Reference by Accident</a></p>
<p>12 0.13850656 <a title="1036-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-07-Job_openings_at_American_University.html">2012 andrew gelman stats-2013-09-07-Job openings at American University</a></p>
<p>13 0.13629042 <a title="1036-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-23-Learning_Differential_Geometry_for_Hamiltonian_Monte_Carlo.html">1339 andrew gelman stats-2012-05-23-Learning Differential Geometry for Hamiltonian Monte Carlo</a></p>
<p>14 0.13513473 <a title="1036-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-04-Handy_Matrix_Cheat_Sheet%2C_with_Gradients.html">555 andrew gelman stats-2011-02-04-Handy Matrix Cheat Sheet, with Gradients</a></p>
<p>15 0.13131899 <a title="1036-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-22-My_talks_that_were_scheduled_for_Tues_at_the_Data_Skeptics_meetup_and_Wed_at_the_Open_Statistical_Programming_meetup.html">1950 andrew gelman stats-2013-07-22-My talks that were scheduled for Tues at the Data Skeptics meetup and Wed at the Open Statistical Programming meetup</a></p>
<p>16 0.12606506 <a title="1036-tfidf-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-24-Bleg%3A_Automatic_Differentiation_for_Log_Prob_Gradients%3F.html">535 andrew gelman stats-2011-01-24-Bleg: Automatic Differentiation for Log Prob Gradients?</a></p>
<p>17 0.12318274 <a title="1036-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-30-Stan_Project%3A__Continuous_Relaxations_for_Discrete_MRFs.html">2003 andrew gelman stats-2013-08-30-Stan Project:  Continuous Relaxations for Discrete MRFs</a></p>
<p>18 0.12243891 <a title="1036-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-06-The_new_Stan_1.1.1%2C_featuring_Gaussian_processes%21.html">1710 andrew gelman stats-2013-02-06-The new Stan 1.1.1, featuring Gaussian processes!</a></p>
<p>19 0.12099776 <a title="1036-tfidf-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-07-My_recent_debugging_experience.html">2161 andrew gelman stats-2014-01-07-My recent debugging experience</a></p>
<p>20 0.11732467 <a title="1036-tfidf-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-14-Transitioning_to_Stan.html">2291 andrew gelman stats-2014-04-14-Transitioning to Stan</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.151), (1, 0.048), (2, -0.048), (3, 0.064), (4, 0.074), (5, 0.084), (6, 0.027), (7, -0.196), (8, -0.07), (9, -0.089), (10, -0.105), (11, -0.028), (12, -0.092), (13, -0.02), (14, 0.061), (15, -0.05), (16, -0.033), (17, 0.012), (18, -0.025), (19, -0.014), (20, -0.028), (21, 0.035), (22, -0.043), (23, -0.004), (24, 0.027), (25, 0.011), (26, -0.021), (27, 0.045), (28, -0.02), (29, 0.016), (30, 0.021), (31, 0.03), (32, 0.007), (33, -0.019), (34, -0.009), (35, 0.002), (36, 0.028), (37, -0.023), (38, -0.013), (39, 0.006), (40, -0.021), (41, 0.008), (42, -0.053), (43, -0.025), (44, 0.022), (45, -0.074), (46, -0.047), (47, 0.007), (48, 0.007), (49, -0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9623087 <a title="1036-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-30-Stan_uses_Nuts%21.html">1036 andrew gelman stats-2011-11-30-Stan uses Nuts!</a></p>
<p>Introduction: We interrupt our usual program of  Ed Wegman   Gregg Easterbrook  Niall Ferguson mockery to deliver a serious update on our statistical computing project.
 
 Stan  (“Sampling Through Adaptive Neighborhoods”) is our new C++ program (written mostly by Bob Carpenter) that draws samples from Bayesian models.  Stan can take different sorts of inputs:  you can write the model in a Bugs-like syntax and it goes from there, or you can write the log-posterior directly as a C++ function.
 
Most of the computation is done using Hamiltonian Monte Carlo.  HMC requires some tuning, so Matt Hoffman up and wrote a new algorithm, Nuts (the “No-U-Turn Sampler”) which optimizes HMC adaptively.  In many settings, Nuts is actually more computationally efficient than the optimal static HMC!
 
 When the  the Nuts paper  appeared on Arxiv, Christian Robert noticed it and had  some reactions .
 
In response to Xian’s comments, Matt writes:
  

Christian writes:

 
I wonder about the computing time (and the “una</p><p>2 0.86931676 <a title="1036-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-27-%28R-Py-Cmd%29Stan_2.1.0.html">2150 andrew gelman stats-2013-12-27-(R-Py-Cmd)Stan 2.1.0</a></p>
<p>Introduction: We’re happy to announce the release of Stan C++, CmdStan, 
RStan, and PyStan 2.1.0.  This is a minor feature release, 
but it is also an important bug fix release.  As always, the 
place to start is the (all new) Stan web pages:
 
 http://mc-stan.org 
 
 
 
 Major Bug in 2.0.0, 2.0.1 
 
Stan 2.0.0 and Stan 2.0.1 introduced a bug in the implementation 
of the NUTS criterion that led to poor tail exploration and 
thus biased the posterior uncertainty downward.  There was no 
bug in NUTS in Stan 1.3 or earlier, and 2.1 has been extensively tested 
and tests put in place so this problem will not recur.
 
If you are using Stan 2.0.0 or 2.0.1, you should switch to 2.1.0 as 
soon as possible and rerun any models you care about.
 
 
 
 New Target Acceptance Rate Default for Stan 2.1.0 
  Another big change aimed at reducing posterior estimation bias 
was an increase in the target acceptance rate during adaptation 
from 0.65 to 0.80.  The bad news is that iterations will take 
around 50% longer</p><p>3 0.81604844 <a title="1036-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-A_Stan_is_Born.html">1475 andrew gelman stats-2012-08-30-A Stan is Born</a></p>
<p>Introduction: Stan 1.0.0 and RStan 1.0.0 
 
It’s official.  The Stan Development Team is happy to announce the first stable versions of Stan and RStan.  
 
 What is (R)Stan? 
 
Stan is an open-source package for obtaining Bayesian inference using the No-U-Turn sampler, a variant of Hamiltonian Monte Carlo.  It’s sort of like BUGS, but with a different language for expressing models and a different sampler for sampling from their posteriors. 
 
RStan is the R interface to Stan.  
 
 Stan Home Page 
 
Stan’s home page is:     http://mc-stan.org/    
 
It links everything you need to get started running Stan from the command line, from R, or from C++, including full step-by-step install instructions, a detailed user’s guide and reference manual for the modeling language, and tested ports of most of the BUGS examples.
 
 Peruse the Manual 
 
If you’d like to learn more, the   Stan User’s Guide and Reference Manual   is the place to start.</p><p>4 0.80751121 <a title="1036-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-16-Stantastic%21.html">1580 andrew gelman stats-2012-11-16-Stantastic!</a></p>
<p>Introduction: Richard McElreath writes:
  
I’ve been translating a few ongoing data analysis projects into  Stan  code, mostly with success. The most important for me right now has been a hierarchical zero-inflated gamma problem. This a “hurdle” model, in which a bernoulli GLM produces zeros/nonzeros, and then a gamma GLM produces the nonzero values, using varying effects correlated with those in the bernoulli process.


The data are 20 years of human foraging returns from a subsistence hunting population in Paraguay (the Ache), comprising about 15k hunts in total (Hill & Kintigh. 2009. Current Anthropology 50:369-377). Observed values are kilograms of meat returned to camp. The more complex models contain a 147-by-9 matrix of varying effects (147 unique hunters), as well as imputation of missing values.


Originally, I had written the sampler myself in raw R code. It was very slow, but I knew what it was doing at least. Just before Stan version 1.0 was released, I had managed to get JAGS to do it a</p><p>5 0.80203974 <a title="1036-lsi-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-07-My_recent_debugging_experience.html">2161 andrew gelman stats-2014-01-07-My recent debugging experience</a></p>
<p>Introduction: OK, so this sort of thing happens sometimes.  I was working on a new idea (still working on it; if it ultimately works out—or if it doesn’t—I’ll let you know) and as part of it I was fitting little models in Stan, in a loop.  I thought it would make sense to start with linear regression with normal priors and known data variance, because then the exact solution is Gaussian and I can also work with the problem analytically.  So I programmed up the algorithm and, no surprise, it didn’t work.  I went through my R code, put in print statements here and there, and cleared out bug after bug until at least it stopped crashing.  But the algorithm still wasn’t doing what it was supposed to do.
 
So I decided to do something simpler, and just check that the Stan linear regression gave the same answer as the analytic posterior distribution:  I ran Stan for tons of iterations, then computed the sample mean and variance of the simulations.  It was an example with two coefficients—I’d originally cho</p><p>6 0.80172795 <a title="1036-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-30-Stan_Project%3A__Continuous_Relaxations_for_Discrete_MRFs.html">2003 andrew gelman stats-2013-08-30-Stan Project:  Continuous Relaxations for Discrete MRFs</a></p>
<p>7 0.80129892 <a title="1036-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-17-Stan_and_RStan_1.1.0.html">1627 andrew gelman stats-2012-12-17-Stan and RStan 1.1.0</a></p>
<p>8 0.79629833 <a title="1036-lsi-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-Running_into_a_Stan_Reference_by_Accident.html">2231 andrew gelman stats-2014-03-03-Running into a Stan Reference by Accident</a></p>
<p>9 0.78824127 <a title="1036-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-13-Stan%21.html">1855 andrew gelman stats-2013-05-13-Stan!</a></p>
<p>10 0.78262997 <a title="1036-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-06-Stan_1.2.0_and_RStan_1.2.0.html">1753 andrew gelman stats-2013-03-06-Stan 1.2.0 and RStan 1.2.0</a></p>
<p>11 0.77963334 <a title="1036-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-12-Samplers_for_Big_Science%3A__emcee_and_BAT.html">2020 andrew gelman stats-2013-09-12-Samplers for Big Science:  emcee and BAT</a></p>
<p>12 0.77877879 <a title="1036-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-06-The_new_Stan_1.1.1%2C_featuring_Gaussian_processes%21.html">1710 andrew gelman stats-2013-02-06-The new Stan 1.1.1, featuring Gaussian processes!</a></p>
<p>13 0.76851451 <a title="1036-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-04-PyStan%21.html">1748 andrew gelman stats-2013-03-04-PyStan!</a></p>
<p>14 0.7638427 <a title="1036-lsi-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-13-CmdStan%2C_RStan%2C_PyStan_v2.2.0.html">2209 andrew gelman stats-2014-02-13-CmdStan, RStan, PyStan v2.2.0</a></p>
<p>15 0.74619114 <a title="1036-lsi-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-04-Stan_%28%26_JAGS%29_Tutorial_on_Linear_Mixed_Models.html">2318 andrew gelman stats-2014-05-04-Stan (& JAGS) Tutorial on Linear Mixed Models</a></p>
<p>16 0.73932421 <a title="1036-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-14-The_joys_of_working_in_the_public_domain.html">712 andrew gelman stats-2011-05-14-The joys of working in the public domain</a></p>
<p>17 0.73910338 <a title="1036-lsi-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-14-Transitioning_to_Stan.html">2291 andrew gelman stats-2014-04-14-Transitioning to Stan</a></p>
<p>18 0.71461219 <a title="1036-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-05-Stan_%28quietly%29_passes_512_people_on_the_users_list.html">2124 andrew gelman stats-2013-12-05-Stan (quietly) passes 512 people on the users list</a></p>
<p>19 0.7130999 <a title="1036-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-12-Stan_1.3.0_and_RStan_1.3.0_Ready_for_Action.html">1799 andrew gelman stats-2013-04-12-Stan 1.3.0 and RStan 1.3.0 Ready for Action</a></p>
<p>20 0.7126509 <a title="1036-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-13-Stan_at_NIPS_2012_Workshop_on_Probabilistic_Programming.html">1576 andrew gelman stats-2012-11-13-Stan at NIPS 2012 Workshop on Probabilistic Programming</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(15, 0.02), (16, 0.073), (21, 0.027), (24, 0.154), (32, 0.012), (55, 0.026), (57, 0.133), (73, 0.033), (79, 0.017), (82, 0.066), (86, 0.029), (87, 0.016), (89, 0.029), (99, 0.206)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94738817 <a title="1036-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-30-Stan_uses_Nuts%21.html">1036 andrew gelman stats-2011-11-30-Stan uses Nuts!</a></p>
<p>Introduction: We interrupt our usual program of  Ed Wegman   Gregg Easterbrook  Niall Ferguson mockery to deliver a serious update on our statistical computing project.
 
 Stan  (“Sampling Through Adaptive Neighborhoods”) is our new C++ program (written mostly by Bob Carpenter) that draws samples from Bayesian models.  Stan can take different sorts of inputs:  you can write the model in a Bugs-like syntax and it goes from there, or you can write the log-posterior directly as a C++ function.
 
Most of the computation is done using Hamiltonian Monte Carlo.  HMC requires some tuning, so Matt Hoffman up and wrote a new algorithm, Nuts (the “No-U-Turn Sampler”) which optimizes HMC adaptively.  In many settings, Nuts is actually more computationally efficient than the optimal static HMC!
 
 When the  the Nuts paper  appeared on Arxiv, Christian Robert noticed it and had  some reactions .
 
In response to Xian’s comments, Matt writes:
  

Christian writes:

 
I wonder about the computing time (and the “una</p><p>2 0.93180716 <a title="1036-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-05-What_are_the_standards_for_reliability_in_experimental_psychology%3F.html">1101 andrew gelman stats-2012-01-05-What are the standards for reliability in experimental psychology?</a></p>
<p>Introduction: An experimental psychologist was wondering about the standards in that field for “acceptable reliability” (when looking at inter-rater reliability in coding data).  He wondered, for example, if some variation on signal detectability theory might be applied to adjust for inter-rater differences in criteria for saying some code is present.
 
What about Cohen’s kappa?  The psychologist wrote:
  
Cohen’s kappa does adjust for “guessing,” but its assumptions are not well motivated, perhaps not any more than adjustments for guessing versus the application of signal detectability theory where that can be applied. But one can’t do a straightforward application of signal detectability theory for reliability in that you don’t know whether the signal is present or not.
  
I think measurement issues are important but I don’t have enough experience in this area to answer the question without knowing more about the problem that this researcher is working on.
 
I’m posting it here because I imagine t</p><p>3 0.91417789 <a title="1036-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-16-%E2%80%9CReal_data_can_be_a_pain%E2%80%9D.html">1460 andrew gelman stats-2012-08-16-“Real data can be a pain”</a></p>
<p>Introduction: Michael McLaughlin sent me the following query with the above title.
  
Some time ago, I [McLaughlin] was handed a dataset that needed to be modeled.  It was generated as follows:


1. Random navigation errors, historically a binary mixture of normal and Laplace with a common mean, were collected by observation.


2. Sadly, these data were recorded with too few decimal places so that the resulting quantization is clearly visible in a scatterplot.


3. The quantized data were then interpolated (to an unobserved location).


The final result looks like fuzzy points (small scale jitter) at quantized intervals spanning a much larger scale (the parent mixture distribution).  This fuzziness, likely ~normal or ~Laplace, results from the interpolation.  Otherwise, the data would look like a discrete analogue of the normal/Laplace mixture.


I would like to characterize the latent normal/Laplace mixture distribution but the quantization is “getting in the way”.  When I tried MCMC on this proble</p><p>4 0.89970344 <a title="1036-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-06-The_K_Foundation_burns_Cosma%E2%80%99s_turkey.html">1044 andrew gelman stats-2011-12-06-The K Foundation burns Cosma’s turkey</a></p>
<p>Introduction: Shalizi  delivers  a slow, drawn-out illustration of the point that economic efficiency is all about who’s got the $, which isn’t always related to what we would usually call “efficiency” in other settings.  (His point is related to my  argument  that the phrase “willingness to pay” should generally be replaced by “ability to pay.”)
 
The basic story is simple:  Good guy  needs  a turkey, bad guy  wants  a turkey.  Bad guy is willing and able to pay more for the turkey than good guy can afford, hence good guy starves to death.
 
The counterargument is that a market in turkeys will motivate producers to breed more turkeys, ultimately saturating the bad guys’ desires and leaving surplus turkeys for the good guys at a reasonable price.
 
I’m sure there’s a counter-counterargument too, but I don’t want to go there.
 
But what really amused me about Cosma’s essay was how he scrambled the usual cultural/political associations.  (I assume he did this on purpose.)  In the standard version of t</p><p>5 0.89634752 <a title="1036-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-06-One_reason_New_York_isn%E2%80%99t_as_rich_as_it_used_to_be%3A__Redistribution_of_federal_tax_money_to_other_states.html">1485 andrew gelman stats-2012-09-06-One reason New York isn’t as rich as it used to be:  Redistribution of federal tax money to other states</a></p>
<p>Introduction: Uberbloggers  Andrew Sullivan  and  Matthew Yglesias  were kind enough to link to my five-year-old post with graphs from Red State Blue State on time trends of average income by state.
 
Here are the  graphs : 
   
 
 
Yglesias’s take-home point:
  
There isn’t that much change over time in states’ economic well-being. All things considered the best predictor of how rich a state was in 2000 was simply how rich it was in 1929…. Massachusetts and Connecticut have always been rich and Arkansas and Mississippi have always been poor.
  
I’d like to point to a different feature of the graphs, which is that, although the  rankings  of the states haven’t changed much (as can be seen from the “2000 compared to 1929″ scale), the relative  values  of the incomes have converged quite a bit—at least, they converged from about 1930 to 1980 before hitting some level of stability.  And the rankings have changed a bit.  My impression (without checking the numbers) is that New York and Connecticut were</p><p>6 0.8902396 <a title="1036-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-19-Will_Stan_work_well_with_40%C3%9740_matrices%3F.html">861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</a></p>
<p>7 0.88782144 <a title="1036-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<p>8 0.88365495 <a title="1036-lda-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-04-Stan_%28%26_JAGS%29_Tutorial_on_Linear_Mixed_Models.html">2318 andrew gelman stats-2014-05-04-Stan (& JAGS) Tutorial on Linear Mixed Models</a></p>
<p>9 0.88331664 <a title="1036-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-15-Fun_fight_over_the_Grover_search_algorithm.html">1120 andrew gelman stats-2012-01-15-Fun fight over the Grover search algorithm</a></p>
<p>10 0.87739897 <a title="1036-lda-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-Stan_Model_of_the_Week%3A_Hierarchical_Modeling_of_Supernovas.html">2299 andrew gelman stats-2014-04-21-Stan Model of the Week: Hierarchical Modeling of Supernovas</a></p>
<p>11 0.87364155 <a title="1036-lda-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-16-Another_update_on_the_spam_email_study.html">35 andrew gelman stats-2010-05-16-Another update on the spam email study</a></p>
<p>12 0.87272757 <a title="1036-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-29-Another_one_of_those_%E2%80%9CPsychological_Science%E2%80%9D_papers_%28this_time_on_biceps_size_and_political_attitudes_among_college_students%29.html">1876 andrew gelman stats-2013-05-29-Another one of those “Psychological Science” papers (this time on biceps size and political attitudes among college students)</a></p>
<p>13 0.87019956 <a title="1036-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-10-The_ethics_of_lying%2C_cheating%2C_and_stealing_with_data%3A__A_case_study.html">2015 andrew gelman stats-2013-09-10-The ethics of lying, cheating, and stealing with data:  A case study</a></p>
<p>14 0.86966944 <a title="1036-lda-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-18-DataMarket.html">215 andrew gelman stats-2010-08-18-DataMarket</a></p>
<p>15 0.86853743 <a title="1036-lda-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-20-A_statistical_model_for_underdispersion.html">1542 andrew gelman stats-2012-10-20-A statistical model for underdispersion</a></p>
<p>16 0.86636728 <a title="1036-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-08-Annals_of_spam.html">1488 andrew gelman stats-2012-09-08-Annals of spam</a></p>
<p>17 0.86504751 <a title="1036-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-31-Response_by_Jessica_Tracy_and_Alec_Beall_to_my_critique_of_the_methods_in_their_paper%2C_%E2%80%9CWomen_Are_More_Likely_to_Wear_Red_or_Pink_at_Peak_Fertility%E2%80%9D.html">1963 andrew gelman stats-2013-07-31-Response by Jessica Tracy and Alec Beall to my critique of the methods in their paper, “Women Are More Likely to Wear Red or Pink at Peak Fertility”</a></p>
<p>18 0.86481655 <a title="1036-lda-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-20-Thermodynamic_Monte_Carlo%3A__Michael_Betancourt%E2%80%99s_new_method_for_simulating_from_difficult_distributions_and_evaluating_normalizing_constants.html">2340 andrew gelman stats-2014-05-20-Thermodynamic Monte Carlo:  Michael Betancourt’s new method for simulating from difficult distributions and evaluating normalizing constants</a></p>
<p>19 0.86221343 <a title="1036-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-22-%E2%80%9CInformation_visualization%E2%80%9D_vs._%E2%80%9CStatistical_graphics%E2%80%9D.html">816 andrew gelman stats-2011-07-22-“Information visualization” vs. “Statistical graphics”</a></p>
<p>20 0.86133444 <a title="1036-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-08-P-values_and_statistical_practice.html">1713 andrew gelman stats-2013-02-08-P-values and statistical practice</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
