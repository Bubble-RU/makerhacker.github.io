<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>979 andrew gelman stats-2011-10-29-Bayesian inference for the parameter of a uniform distribution</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-979" href="#">andrew_gelman_stats-2011-979</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>979 andrew gelman stats-2011-10-29-Bayesian inference for the parameter of a uniform distribution</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-979-html" href="http://andrewgelman.com/2011/10/29/bayesian-inference-for-the-parameter-of-a-uniform-distribution/">html</a></p><p>Introduction: Subhash Lele writes:
  
I was wondering if you might know some good references to Bayesian treatment of parameter estimation for U(0,b) type distributions. I am looking for cases where the parameter is on the boundary. I would appreciate any help and advice you could provide. I am, in particular, looking for an MCMC (preferably in WinBUGS) based approach.  I figured out the WinBUGS part but I am still curious about the theoretical papers, asymptotics etc.
  
I actually can’t think of any examples!  But maybe you, the readers, can.
 
We also should think of the best way to implement this model in Stan.  We like to transform to avoid hard boundary constraints, but it seems a bit tacky to do a data-based transformation (which itself would not work if there are latent variables).
 
P.S.    I actually saw Lele speak at a statistics conference around 20 years ago.  There was a lively exchange between Lele and an older guy who was working on similar problems using a different method.  The oth</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Subhash Lele writes:    I was wondering if you might know some good references to Bayesian treatment of parameter estimation for U(0,b) type distributions. [sent-1, score-0.382]
</p><p>2 I am looking for cases where the parameter is on the boundary. [sent-2, score-0.18]
</p><p>3 I would appreciate any help and advice you could provide. [sent-3, score-0.108]
</p><p>4 I am, in particular, looking for an MCMC (preferably in WinBUGS) based approach. [sent-4, score-0.077]
</p><p>5 I figured out the WinBUGS part but I am still curious about the theoretical papers, asymptotics etc. [sent-5, score-0.292]
</p><p>6 We also should think of the best way to implement this model in Stan. [sent-8, score-0.074]
</p><p>7 We like to transform to avoid hard boundary constraints, but it seems a bit tacky to do a data-based transformation (which itself would not work if there are latent variables). [sent-9, score-0.485]
</p><p>8 I actually saw Lele speak at a statistics conference around 20 years ago. [sent-12, score-0.372]
</p><p>9 There was a lively exchange between Lele and an older guy who was working on similar problems using a different method. [sent-13, score-0.472]
</p><p>10 The other guy  couldn’t stand  what Lele was doing and was upset that at the conference organizers for not disavowing Lele’s talk. [sent-14, score-0.482]
</p><p>11 In retrospect it all seems pretty silly but I imagine it was upsetting to Lele at the time. [sent-16, score-0.228]
</p><p>12 I speak by analogy to my own very disturbing experience having my research loudly denounced by people who worked on similar problems and seemed to think I was a complete idiot. [sent-17, score-0.476]
</p><p>13 (I’m not speaking, by the way, of the people who didn’t want to tenure me at Berkeley. [sent-18, score-0.081]
</p><p>14 Oddly enough, one of them told me they all agreed I was “brilliant. [sent-19, score-0.07]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lele', 0.785), ('winbugs', 0.201), ('conference', 0.143), ('speak', 0.123), ('asymptotics', 0.104), ('parameter', 0.103), ('upsetting', 0.101), ('organizers', 0.098), ('preferably', 0.098), ('guy', 0.096), ('transformation', 0.094), ('tacky', 0.094), ('lively', 0.089), ('disturbing', 0.088), ('transform', 0.085), ('boundary', 0.085), ('upset', 0.083), ('tenure', 0.081), ('oddly', 0.08), ('similar', 0.08), ('figured', 0.077), ('looking', 0.077), ('implement', 0.074), ('retrospect', 0.074), ('older', 0.072), ('latent', 0.072), ('constraints', 0.071), ('mcmc', 0.071), ('odd', 0.07), ('agreed', 0.07), ('exchange', 0.07), ('problems', 0.065), ('references', 0.065), ('analogy', 0.062), ('stand', 0.062), ('curious', 0.06), ('speaking', 0.059), ('complete', 0.058), ('appreciate', 0.057), ('wondering', 0.056), ('estimation', 0.056), ('avoid', 0.055), ('couldn', 0.054), ('saw', 0.054), ('silly', 0.053), ('actually', 0.052), ('advice', 0.051), ('type', 0.051), ('treatment', 0.051), ('theoretical', 0.051)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="979-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-29-Bayesian_inference_for_the_parameter_of_a_uniform_distribution.html">979 andrew gelman stats-2011-10-29-Bayesian inference for the parameter of a uniform distribution</a></p>
<p>Introduction: Subhash Lele writes:
  
I was wondering if you might know some good references to Bayesian treatment of parameter estimation for U(0,b) type distributions. I am looking for cases where the parameter is on the boundary. I would appreciate any help and advice you could provide. I am, in particular, looking for an MCMC (preferably in WinBUGS) based approach.  I figured out the WinBUGS part but I am still curious about the theoretical papers, asymptotics etc.
  
I actually can’t think of any examples!  But maybe you, the readers, can.
 
We also should think of the best way to implement this model in Stan.  We like to transform to avoid hard boundary constraints, but it seems a bit tacky to do a data-based transformation (which itself would not work if there are latent variables).
 
P.S.    I actually saw Lele speak at a statistics conference around 20 years ago.  There was a lively exchange between Lele and an older guy who was working on similar problems using a different method.  The oth</p><p>2 0.091116048 <a title="979-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-11-My_problem_with_the_Lindley_paradox.html">1757 andrew gelman stats-2013-03-11-My problem with the Lindley paradox</a></p>
<p>Introduction: From  a couple years ago but still relevant, I think:
  
To me, the Lindley paradox falls apart because of its noninformative prior distribution on the parameter of interest. If you really think there’s a high probability the parameter is nearly exactly zero, I don’t see the point of the model saying that you have no prior information at all on the parameter. In short: my criticism of so-called Bayesian hypothesis testing is that it’s insufficiently Bayesian.
  
P.S.  To clarify (in response to Bill’s comment below):  I’m speaking of all the examples I’ve ever worked on in social and environmental science, where in some settings I can imagine a parameter being very close to zero and in other settings I can imagine a parameter taking on just about any value in a wide range, but where I’ve never seen an example where a parameter could be  either  right at zero  or  taking on any possible value.  But such examples might occur in areas of application that I haven’t worked on.</p><p>3 0.089680426 <a title="979-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-07-X_on_JLP.html">1792 andrew gelman stats-2013-04-07-X on JLP</a></p>
<p>Introduction: Christian Robert  writes  on the Jeffreys-Lindley paradox.  I have nothing to add to this beyond my recent  comments :
  
To me, the Lindley paradox falls apart because of its noninformative prior distribution on the parameter of interest. If you really think there’s a high probability the parameter is nearly exactly zero, I don’t see the point of the model saying that you have no prior information at all on the parameter. In short: my criticism of so-called Bayesian hypothesis testing is that it’s insufficiently Bayesian.


To clarify, I’m speaking of all the examples I’ve ever worked on in social and environmental science, where in some settings I can imagine a parameter being very close to zero and in other settings I can imagine a parameter taking on just about any value in a wide range, but where I’ve never seen an example where a parameter could be either right at zero or taking on any possible value. But such examples might occur in areas of application that I haven’t worked on.</p><p>4 0.071217865 <a title="979-tfidf-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-07-Valencia%3A___Summer_of_1991.html">72 andrew gelman stats-2010-06-07-Valencia:   Summer of 1991</a></p>
<p>Introduction: With the completion of the last edition of Jose Bernardo’s Valencia (Spain) conference on Bayesian statistics–I didn’t attend, but many of my  friends  were there–I thought I’d share my strongest memory of the Valencia conference that I attended in 1991.  I contributed a poster and a discussion, both on the topic of inference from iterative simulation, but what I remember most vividly, and what bothered me the most, was how little interest there was in checking model fit.  Not only had people mostly not checked the fit of their models to data, and not only did they seem uninterested in such checks, even worse was that many of these Bayesians felt that it was basically illegal to check model fit.
 
I don’t want to get too down on Bayesians for this.  Lots of non-Bayesian statisticians go around not checking their models too.  With Bayes, though, model checking seems particularly important because Bayesians rely on their models so strongly, not just as a way of getting point estimates bu</p><p>5 0.068616547 <a title="979-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-06-Early_stopping_and_penalized_likelihood.html">788 andrew gelman stats-2011-07-06-Early stopping and penalized likelihood</a></p>
<p>Introduction: Maximum likelihood gives the beat fit to the training data but in general overfits, yielding overly-noisy parameter estimates that don’t perform so well when predicting new data.  A popular solution to this overfitting problem takes advantage of the iterative nature of most maximum likelihood algorithms by stopping early.  In general, an iterative optimization algorithm goes from a starting point to the maximum of some objective function.  If the starting point has some good properties, then early stopping can work well, keeping some of the virtues of the starting point while respecting the data.  
 
This trick can be performed the other way, too, starting with the data and then processing it to move it toward a model.  That’s how the iterative proportional fitting algorithm of Deming and Stephan (1940) works to fit multivariate categorical data to known margins.
 
In any case, the trick is to stop at the right point–not so soon that you’re ignoring the data but not so late that you en</p><p>6 0.06622161 <a title="979-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-24-Bell_Labs.html">970 andrew gelman stats-2011-10-24-Bell Labs</a></p>
<p>7 0.062986463 <a title="979-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-20-The_institution_of_tenure.html">2070 andrew gelman stats-2013-10-20-The institution of tenure</a></p>
<p>8 0.062151626 <a title="979-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-08-More_on_the_missing_conservative_psychology_researchers.html">604 andrew gelman stats-2011-03-08-More on the missing conservative psychology researchers</a></p>
<p>9 0.061316699 <a title="979-tfidf-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-31-Type_S_error_rates_for_classical_and_Bayesian_single_and_multiple_comparison_procedures.html">494 andrew gelman stats-2010-12-31-Type S error rates for classical and Bayesian single and multiple comparison procedures</a></p>
<p>10 0.060967155 <a title="979-tfidf-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-23-Bayesian_adaptive_methods_for_clinical_trials.html">427 andrew gelman stats-2010-11-23-Bayesian adaptive methods for clinical trials</a></p>
<p>11 0.059825554 <a title="979-tfidf-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Bayesian_inference_viewed_as_a_computational_approximation_to_classical_calculations.html">254 andrew gelman stats-2010-09-04-Bayesian inference viewed as a computational approximation to classical calculations</a></p>
<p>12 0.05804633 <a title="979-tfidf-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-12-More_on_publishing_in_journals.html">2245 andrew gelman stats-2014-03-12-More on publishing in journals</a></p>
<p>13 0.056959763 <a title="979-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-23-AI_Stats_conference_on_Stan_etc..html">1911 andrew gelman stats-2013-06-23-AI Stats conference on Stan etc.</a></p>
<p>14 0.05663937 <a title="979-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-09-A_tale_of_two_discussion_papers.html">1848 andrew gelman stats-2013-05-09-A tale of two discussion papers</a></p>
<p>15 0.056161325 <a title="979-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-30-Infill_asymptotics_and_sprawl_asymptotics.html">1877 andrew gelman stats-2013-05-30-Infill asymptotics and sprawl asymptotics</a></p>
<p>16 0.055244397 <a title="979-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-08-Robert_Kosara_reviews_Ed_Tufte%E2%80%99s_short_course.html">1451 andrew gelman stats-2012-08-08-Robert Kosara reviews Ed Tufte’s short course</a></p>
<p>17 0.054136828 <a title="979-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-23-Larry_Wasserman%E2%80%99s_statistics_blog.html">1389 andrew gelman stats-2012-06-23-Larry Wasserman’s statistics blog</a></p>
<p>18 0.0538279 <a title="979-tfidf-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-07-Biostatistics_via_Pragmatic_and_Perceptive_Bayes..html">453 andrew gelman stats-2010-12-07-Biostatistics via Pragmatic and Perceptive Bayes.</a></p>
<p>19 0.053671323 <a title="979-tfidf-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-29-Ethics_and_statistics_in_development_research.html">241 andrew gelman stats-2010-08-29-Ethics and statistics in development research</a></p>
<p>20 0.053580105 <a title="979-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-22-The_kluges_of_today_are_the_textbook_solutions_of_tomorrow..html">2143 andrew gelman stats-2013-12-22-The kluges of today are the textbook solutions of tomorrow.</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.114), (1, 0.013), (2, -0.031), (3, 0.02), (4, 0.001), (5, 0.006), (6, 0.029), (7, -0.009), (8, 0.009), (9, 0.016), (10, -0.01), (11, -0.014), (12, 0.015), (13, -0.016), (14, -0.003), (15, -0.007), (16, 0.004), (17, -0.02), (18, -0.004), (19, 0.01), (20, -0.012), (21, -0.005), (22, 0.01), (23, -0.003), (24, -0.011), (25, -0.008), (26, -0.044), (27, -0.017), (28, 0.001), (29, 0.004), (30, 0.026), (31, 0.025), (32, -0.003), (33, 0.004), (34, -0.019), (35, -0.053), (36, -0.002), (37, 0.03), (38, -0.019), (39, 0.027), (40, 0.003), (41, 0.002), (42, -0.005), (43, 0.048), (44, -0.037), (45, -0.02), (46, 0.03), (47, 0.031), (48, -0.017), (49, 0.027)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94368255 <a title="979-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-29-Bayesian_inference_for_the_parameter_of_a_uniform_distribution.html">979 andrew gelman stats-2011-10-29-Bayesian inference for the parameter of a uniform distribution</a></p>
<p>Introduction: Subhash Lele writes:
  
I was wondering if you might know some good references to Bayesian treatment of parameter estimation for U(0,b) type distributions. I am looking for cases where the parameter is on the boundary. I would appreciate any help and advice you could provide. I am, in particular, looking for an MCMC (preferably in WinBUGS) based approach.  I figured out the WinBUGS part but I am still curious about the theoretical papers, asymptotics etc.
  
I actually can’t think of any examples!  But maybe you, the readers, can.
 
We also should think of the best way to implement this model in Stan.  We like to transform to avoid hard boundary constraints, but it seems a bit tacky to do a data-based transformation (which itself would not work if there are latent variables).
 
P.S.    I actually saw Lele speak at a statistics conference around 20 years ago.  There was a lively exchange between Lele and an older guy who was working on similar problems using a different method.  The oth</p><p>2 0.76532102 <a title="979-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-26-Impersonators.html">1639 andrew gelman stats-2012-12-26-Impersonators</a></p>
<p>Introduction: This story  of a Cindy Sherman impersonator reminded me of some graffiti I saw in a bathroom of the Whitney Museum many years ago.  My friend Kenny and I had gone there for the Biennial which had an exhibit featuring Keith Haring and others of the neo-taggers (or whatever they were called).  The bathroom walls were all painted over by Kenny Scharf [no relation to my friend] in his characteristically irritating doodle style.  On top of the ugly stylized graffiti was a Sharpie’d scrawl:  “Kenny Scharf is a pretentious asshole.”
 
I suspected this last bit was added by someone else, but maybe it was Scharf himself?  Ira Glass is a bigshot and can get Cindy Sherman on the phone, but I was just some guy, all I could do was write Scharf a letter, c/o the Whitney Museum.  I described the situation and asked if he was the one who had written, “Kenny Scharf is a pretentious asshole.”  He did not reply.</p><p>3 0.76403272 <a title="979-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-24-Bell_Labs.html">970 andrew gelman stats-2011-10-24-Bell Labs</a></p>
<p>Introduction: Sining Chen told me they’re hiring in the  statistics group at Bell Labs .  I’ll do my bit for economic stimulus by announcing this job (see below).
 
I love Bell Labs.  I worked there for three summers, in a physics lab in 1985-86 under the supervision of Loren Pfeiffer, and by myself in the statistics group in 1990.
 
I learned a lot working for Loren.  He was a really smart and driven guy.  His lab was a small set of rooms—in Bell Labs, everything’s in a small room, as they value the positive externality of close physical proximity of different labs, which you get by making each lab compact—and it was Loren, his assistant (a guy named Ken West who kept everything running in the lab), and three summer students: me, Gowton Achaibar, and a girl whose name I’ve forgotten.  Gowtan and I had a lot of fun chatting in the lab.  One day I made a silly comment about Gowton’s accent—he was from Guyana and pronounced “three” as “tree”—and then I apologized and said:  Hey, here I am making fun o</p><p>4 0.73779362 <a title="979-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-29-What_is_expected_of_a_consultant.html">1597 andrew gelman stats-2012-11-29-What is expected of a consultant</a></p>
<p>Introduction: Robin Hanson  writes  on paid expert consulting (of the sort that I do sometime, and is common among economists and statisticians).  Hanson agrees with Keith Yost, who says:
  
Fellow consultants and associates . . . [said] fifty percent of the job is nodding your head at whatever’s being said, thirty percent of it is just sort of looking good, and the other twenty percent is raising an objection but then if you meet resistance, then dropping it.
  
On the other side is Steven Levitt, who Hanson quotes as saying:
  
My own experience has been that even though I know nothing about an industry, if you give me a week, and you get a bunch of really smart people to explain the industry to me, and to tell me what they do, a lot of times what I’ve learned in economics, what I’ve learned in other places can actually be really helpful in changing the way that they see the world.
  
Perhaps unsurprisingly given my Bayesian attitudes and my preference for  continuity , I’m inclined to split the d</p><p>5 0.73576331 <a title="979-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-30-Using_the_aggregate_of_the_outcome_variable_as_a_group-level_predictor_in_a_hierarchical_model.html">2045 andrew gelman stats-2013-09-30-Using the aggregate of the outcome variable as a group-level predictor in a hierarchical model</a></p>
<p>Introduction: When I was a kid I took a writing class, and one of the assignments was to write a 1-to-2 page story.  I can’t remember what I wrote, but I do remember the following story from one of the other kids.  In its entirety:
  
I snuck into this pay toilet and I can’t get out!
  
In the discussion period, the kid explained that his original idea was a story explaining the character’s situation, how he got into this predicament and how he got stuck.  But then he (the author) realized that the one sentence captured the whole story, there was really no need to elaborate.
 
(To understand the above story, you have to know the following historical fact:  Pay toilets in the U.S., decades ago, were  not  the high-security objects shown (for example) in the picture above.  Rather, they were implemented via coin-operated locks on individual toilet stalls.  So it really would be possible to sneak into certain pay toilets, if you were willing to crawl under the door or climb over it.)
 
Anyway, this is</p><p>6 0.72294843 <a title="979-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-06-Statistical_models_and_actual_models.html">995 andrew gelman stats-2011-11-06-Statistical models and actual models</a></p>
<p>7 0.7201333 <a title="979-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-07-Small_world%3A__MIT%2C_asymptotic_behavior_of_differential-difference_equations%2C_Susan_Assmann%2C_subgroup_analysis%2C_multilevel_modeling.html">507 andrew gelman stats-2011-01-07-Small world:  MIT, asymptotic behavior of differential-difference equations, Susan Assmann, subgroup analysis, multilevel modeling</a></p>
<p>8 0.71848989 <a title="979-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-02-I_just_flew_in_from_the_econ_seminar%2C_and_boy_are_my_arms_tired.html">1039 andrew gelman stats-2011-12-02-I just flew in from the econ seminar, and boy are my arms tired</a></p>
<p>9 0.70860726 <a title="979-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-13-Inventor_of_Connect_Four_dies_at_91.html">763 andrew gelman stats-2011-06-13-Inventor of Connect Four dies at 91</a></p>
<p>10 0.70555681 <a title="979-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-28-Behavioral_economics_doesn%E2%80%99t_seem_to_have_much_to_say_about_marriage.html">594 andrew gelman stats-2011-02-28-Behavioral economics doesn’t seem to have much to say about marriage</a></p>
<p>11 0.70297366 <a title="979-lsi-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-25-Why_I_decided_not_to_be_a_physicist.html">2347 andrew gelman stats-2014-05-25-Why I decided not to be a physicist</a></p>
<p>12 0.69935459 <a title="979-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-31-Untunable_Metropolis.html">833 andrew gelman stats-2011-07-31-Untunable Metropolis</a></p>
<p>13 0.69642925 <a title="979-lsi-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-25-The_von_Neumann_paradox.html">430 andrew gelman stats-2010-11-25-The von Neumann paradox</a></p>
<p>14 0.69291461 <a title="979-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-05-Glenn_Hubbard_and_I_were_on_opposite_sides_of_a_court_case_and_I_didn%E2%80%99t_even_know_it%21.html">1707 andrew gelman stats-2013-02-05-Glenn Hubbard and I were on opposite sides of a court case and I didn’t even know it!</a></p>
<p>15 0.69205159 <a title="979-lsi-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-23-Physics_is_hard.html">626 andrew gelman stats-2011-03-23-Physics is hard</a></p>
<p>16 0.68909049 <a title="979-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-03-The_statistical_properties_of_smart_chains_%28and_referral_chains_more_generally%29.html">1882 andrew gelman stats-2013-06-03-The statistical properties of smart chains (and referral chains more generally)</a></p>
<p>17 0.6878624 <a title="979-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-02-%E2%80%9CThe_sky_is_the_limit%E2%80%9D_isn%E2%80%99t_such_a_good_thing.html">835 andrew gelman stats-2011-08-02-“The sky is the limit” isn’t such a good thing</a></p>
<p>18 0.6861459 <a title="979-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-18-Tibshirani_announces_new_research_result%3A__A_significance_test_for_the_lasso.html">1769 andrew gelman stats-2013-03-18-Tibshirani announces new research result:  A significance test for the lasso</a></p>
<p>19 0.68329954 <a title="979-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-29-The_Great_Race.html">1831 andrew gelman stats-2013-04-29-The Great Race</a></p>
<p>20 0.68165499 <a title="979-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-08-How_to_solve_the_Post_Office%E2%80%99s_problems%3F.html">895 andrew gelman stats-2011-09-08-How to solve the Post Office’s problems?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.014), (9, 0.055), (16, 0.066), (24, 0.168), (34, 0.021), (35, 0.023), (42, 0.031), (45, 0.013), (55, 0.01), (58, 0.147), (61, 0.013), (77, 0.023), (83, 0.023), (99, 0.259)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.93553793 <a title="979-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-30-Why_is_George_Apley_overrated%3F.html">119 andrew gelman stats-2010-06-30-Why is George Apley overrated?</a></p>
<p>Introduction: A  comment  by Mark Palko reminded me that, while I’m a huge  Marquand  fan, I think The Late George Apley is way overrated.  My theory is that Marquand’s best books don’t fit into the modernist way of looking about literature, and that the gatekeepers of the 1930s and 1940s, when judging Marquand by these standards, conveniently labeled Apley has his best book because it had a form–Edith-Wharton-style satire–that they could handle.  In contrast, Point of No Return and all the other classics are a mixture of seriousness and satire that left critics uneasy.
 
Perhaps there’s a way to study this sort of thing more systematically?</p><p>same-blog 2 0.92313349 <a title="979-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-29-Bayesian_inference_for_the_parameter_of_a_uniform_distribution.html">979 andrew gelman stats-2011-10-29-Bayesian inference for the parameter of a uniform distribution</a></p>
<p>Introduction: Subhash Lele writes:
  
I was wondering if you might know some good references to Bayesian treatment of parameter estimation for U(0,b) type distributions. I am looking for cases where the parameter is on the boundary. I would appreciate any help and advice you could provide. I am, in particular, looking for an MCMC (preferably in WinBUGS) based approach.  I figured out the WinBUGS part but I am still curious about the theoretical papers, asymptotics etc.
  
I actually can’t think of any examples!  But maybe you, the readers, can.
 
We also should think of the best way to implement this model in Stan.  We like to transform to avoid hard boundary constraints, but it seems a bit tacky to do a data-based transformation (which itself would not work if there are latent variables).
 
P.S.    I actually saw Lele speak at a statistics conference around 20 years ago.  There was a lively exchange between Lele and an older guy who was working on similar problems using a different method.  The oth</p><p>3 0.91389287 <a title="979-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-07-Robust_logistic_regression.html">1886 andrew gelman stats-2013-06-07-Robust logistic regression</a></p>
<p>Introduction: Corey Yanofsky writes:
  
In your work, you’ve robustificated logistic regression by having the logit function saturate at, e.g., 0.01 and 0.99, instead of  0 and 1. Do you have any thoughts on a sensible setting for the saturation values? My intuition suggests that it has something to do with proportion of outliers expected in the data (assuming a reasonable model fit). 


It would be desirable to have them fit in the model, but my intuition is that integrability of the posterior distribution might become an issue.
  
My reply:  it should be no problem to put these saturation values in the model, I bet it would work fine in Stan if you give them uniform (0,.1) priors or something like that.  Or you could just fit the robit model.
 
And this reminds me . . . I’ve been told that when Stan’s on its optimization setting, it fits generalized linear models just about as fast as regular glm or bayesglm in R.  This suggests to me that we should have some precompiled regression models in Stan,</p><p>4 0.91170782 <a title="979-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-22-Statistical_inference_based_on_the_minimum_description_length_principle.html">815 andrew gelman stats-2011-07-22-Statistical inference based on the minimum description length principle</a></p>
<p>Introduction: Tom Ball writes:
  
Here’s another query to add to the stats backlog…Minimum Description Length (MDL).  I’m  attaching  a 2002 Psych Rev paper on same.  Basically, it’s an approach to model selection that replaces goodness of fit with generalizability or complexity.


Would be great to get your response to this approach.
  
My reply:
 
I’ve heard about the minimum description length principle for a long time but have never really understood it.  So I have nothing to say!  Anyone who has anything useful to say on the topic, feel free to add in the comments.
 
The rest of you might wonder why I posted this.  I just thought it would be good for you to have some sense of the boundaries of my knowledge.</p><p>5 0.91005492 <a title="979-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-14-%E2%80%9CThe_best_data_visualizations_should_stand_on_their_own%E2%80%9D%3F__I_don%E2%80%99t_think_so..html">574 andrew gelman stats-2011-02-14-“The best data visualizations should stand on their own”?  I don’t think so.</a></p>
<p>Introduction: Jimmy pointed me to  this  blog by Drew Conway on word clouds.  I don’t have much to say about Conway’s specifics–word clouds aren’t really my thing, but I’m glad that people are thinking about how to do them better–but I did notice one phrase of his that I’ll dispute.  Conway writes
  
The best data visualizations should stand on their own . . .
  
I disagree.  I prefer the saying, “A picture plus 1000 words is better than two pictures or 2000 words.”  That is, I see a positive interaction between words and pictures or, to put it another way, diminishing returns for words or pictures on their own.  I don’t have any big theory for this, but I think, when expressed as a joint value function, my idea makes sense.  Also, I live this suggestion in my own work.  I typically accompany my graphs with long captions and I try to accompany my words with pictures (although I’m not doing it here, because with the software I use, it’s much easier to type more words than to find, scale, and insert i</p><p>6 0.90821725 <a title="979-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-03-Uncertainty_in_parameter_estimates_using_multilevel_models.html">1966 andrew gelman stats-2013-08-03-Uncertainty in parameter estimates using multilevel models</a></p>
<p>7 0.89359939 <a title="979-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-14-Extra_babies_on_Valentine%E2%80%99s_Day%2C_fewer_on_Halloween%3F.html">1167 andrew gelman stats-2012-02-14-Extra babies on Valentine’s Day, fewer on Halloween?</a></p>
<p>8 0.88794011 <a title="979-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-25-The_problem_with_realistic_advice%3F.html">1428 andrew gelman stats-2012-07-25-The problem with realistic advice?</a></p>
<p>9 0.88499606 <a title="979-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-24-Bell_Labs.html">970 andrew gelman stats-2011-10-24-Bell Labs</a></p>
<p>10 0.88298559 <a title="979-lda-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-22-Beach_reads%2C_Proust%2C_and_income_tax.html">103 andrew gelman stats-2010-06-22-Beach reads, Proust, and income tax</a></p>
<p>11 0.88254255 <a title="979-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-07-Question_28_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1371 andrew gelman stats-2012-06-07-Question 28 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>12 0.88208008 <a title="979-lda-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-25-Revised_statistical_standards_for_evidence_%28comments_to_Val_Johnson%E2%80%99s_comments_on_our_comments_on_Val%E2%80%99s_comments_on_p-values%29.html">2305 andrew gelman stats-2014-04-25-Revised statistical standards for evidence (comments to Val Johnson’s comments on our comments on Val’s comments on p-values)</a></p>
<p>13 0.88142502 <a title="979-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-01-References_on_predicting_elections.html">249 andrew gelman stats-2010-09-01-References on predicting elections</a></p>
<p>14 0.88128579 <a title="979-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-19-Standardized_writing_styles_and_standardized_graphing_styles.html">1176 andrew gelman stats-2012-02-19-Standardized writing styles and standardized graphing styles</a></p>
<p>15 0.88013804 <a title="979-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-R_needs_a_good_function_to_make_line_plots.html">252 andrew gelman stats-2010-09-02-R needs a good function to make line plots</a></p>
<p>16 0.88011748 <a title="979-lda-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-05-Update_on_state_size_and_governors%E2%80%99_popularity.html">187 andrew gelman stats-2010-08-05-Update on state size and governors’ popularity</a></p>
<p>17 0.87927759 <a title="979-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-30-Fixed_effects%2C_followed_by_Bayes_shrinkage%3F.html">1644 andrew gelman stats-2012-12-30-Fixed effects, followed by Bayes shrinkage?</a></p>
<p>18 0.87925673 <a title="979-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-17-Macro_causality.html">807 andrew gelman stats-2011-07-17-Macro causality</a></p>
<p>19 0.8791582 <a title="979-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>20 0.87903422 <a title="979-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-06-Education_and_Poverty.html">560 andrew gelman stats-2011-02-06-Education and Poverty</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
