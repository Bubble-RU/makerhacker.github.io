<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>938 andrew gelman stats-2011-10-03-Comparing prediction errors</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-938" href="#">andrew_gelman_stats-2011-938</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>938 andrew gelman stats-2011-10-03-Comparing prediction errors</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-938-html" href="http://andrewgelman.com/2011/10/03/comparing-prediction-errors/">html</a></p><p>Introduction: Someone named James writes: 
  
  
I’m working on a classification task, sentence segmentation.  The classifier algorithm we use (BoosTexter, a boosted learning algorithm) classifies each word independently conditional on its features, i.e. a bag-of-words model, so any contextual clues need to be encoded into the features.  The feature extraction system I am proposing in my thesis uses a heteroscedastic LDA to transform data to produce the features the classifier runs on. The HLDA system has a couple parameters I’m testing, and I’m running a 3×2 full factorial experiment. That’s the background which may or may not be relevant to the question.


The output of each trial is a class (there are only 2 classes, right now) for every word in the dataset.  Because of the nature of the task, one class strongly predominates, say 90-95% of the data.  My question is this: in terms of overall performance (we use F1 score), many of these trials are pretty close together, which leads me to ask whethe</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Someone named James writes:        I’m working on a classification task, sentence segmentation. [sent-1, score-0.245]
</p><p>2 The classifier algorithm we use (BoosTexter, a boosted learning algorithm) classifies each word independently conditional on its features, i. [sent-2, score-0.611]
</p><p>3 a bag-of-words model, so any contextual clues need to be encoded into the features. [sent-4, score-0.372]
</p><p>4 The feature extraction system I am proposing in my thesis uses a heteroscedastic LDA to transform data to produce the features the classifier runs on. [sent-5, score-1.14]
</p><p>5 The HLDA system has a couple parameters I’m testing, and I’m running a 3×2 full factorial experiment. [sent-6, score-0.239]
</p><p>6 That’s the background which may or may not be relevant to the question. [sent-7, score-0.144]
</p><p>7 The output of each trial is a class (there are only 2 classes, right now) for every word in the dataset. [sent-8, score-0.446]
</p><p>8 Because of the nature of the task, one class strongly predominates, say 90-95% of the data. [sent-9, score-0.152]
</p><p>9 My question is this: in terms of overall performance (we use F1 score), many of these trials are pretty close together, which leads me to ask whether the parameter settings don’t matter, or they do but the performance of the trials just happened to be very similar. [sent-10, score-0.73]
</p><p>10 Is there statistical test to see if two trials (a vector of classes calculated from the same ordered list of data) are significantly different, especially given they will both pick the majority class very often? [sent-11, score-0.984]
</p><p>11 My reply:   I too have found that error rates and even log-likelihoods can be noisy measures of prediction accuracy for discrete-data models. [sent-12, score-0.283]
</p><p>12 One way you could compare two methods would be a head-to-head comparison where you saw which cases were predicted correctly by both, incorrectly by both, or correctly by A and incorrectly by B. [sent-13, score-1.314]
</p><p>13 What’s left should be more informative about the differences; essentially you’re doing something closer to a matched-pairs or two-way comparison which will give you information if the prediction errors are correlated (which I expect they will be). [sent-15, score-0.326]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('classifier', 0.246), ('trials', 0.241), ('correctly', 0.223), ('exclude', 0.215), ('incorrectly', 0.201), ('cases', 0.2), ('task', 0.161), ('class', 0.152), ('classes', 0.147), ('predicted', 0.146), ('algorithm', 0.144), ('features', 0.14), ('encoded', 0.136), ('factorial', 0.136), ('lda', 0.136), ('conditions', 0.135), ('prediction', 0.131), ('clues', 0.129), ('extraction', 0.129), ('word', 0.125), ('performance', 0.124), ('comparison', 0.12), ('proposing', 0.115), ('intercepts', 0.11), ('contextual', 0.107), ('system', 0.103), ('transform', 0.098), ('classification', 0.098), ('independently', 0.096), ('calculated', 0.096), ('ordered', 0.095), ('tricky', 0.095), ('vector', 0.094), ('significantly', 0.086), ('output', 0.085), ('trial', 0.084), ('thesis', 0.082), ('runs', 0.079), ('noisy', 0.079), ('closer', 0.075), ('varying', 0.075), ('produce', 0.075), ('named', 0.074), ('accuracy', 0.073), ('sentence', 0.073), ('majority', 0.073), ('feature', 0.073), ('score', 0.073), ('may', 0.072), ('logistic', 0.07)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="938-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-03-Comparing_prediction_errors.html">938 andrew gelman stats-2011-10-03-Comparing prediction errors</a></p>
<p>Introduction: Someone named James writes: 
  
  
I’m working on a classification task, sentence segmentation.  The classifier algorithm we use (BoosTexter, a boosted learning algorithm) classifies each word independently conditional on its features, i.e. a bag-of-words model, so any contextual clues need to be encoded into the features.  The feature extraction system I am proposing in my thesis uses a heteroscedastic LDA to transform data to produce the features the classifier runs on. The HLDA system has a couple parameters I’m testing, and I’m running a 3×2 full factorial experiment. That’s the background which may or may not be relevant to the question.


The output of each trial is a class (there are only 2 classes, right now) for every word in the dataset.  Because of the nature of the task, one class strongly predominates, say 90-95% of the data.  My question is this: in terms of overall performance (we use F1 score), many of these trials are pretty close together, which leads me to ask whethe</p><p>2 0.11168783 <a title="938-tfidf-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-13-Ethical_concerns_in_medical_trials.html">411 andrew gelman stats-2010-11-13-Ethical concerns in medical trials</a></p>
<p>Introduction: I just read  this article  on the treatment of medical volunteers, written by doctor and bioethicist Carl Ellliott.
 
As a statistician who has done a small amount of consulting for pharmaceutical companies, I have a slightly different perspective.  As a doctor, Elliott focuses on individual patients, whereas, as a statistician, I’ve been trained to focus on the goal of accurately estimate treatment effects.
 
I’ll go through Elliott’s article and give my reactions.
  

 
Elliott:
  
In Miami, investigative reporters for Bloomberg Markets magazine discovered that a contract research organisation called SFBC International was testing drugs on undocumented immigrants in a rundown motel; since that report, the motel has been demolished for fire and safety violations. . . . SFBC had recently been named one of the best small businesses in America by Forbes magazine. The Holiday Inn testing facility was the largest in North America, and had been operating for nearly ten years before inspecto</p><p>3 0.10363732 <a title="938-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-04-Write_This_Book.html">1605 andrew gelman stats-2012-12-04-Write This Book</a></p>
<p>Introduction: This post is by Phil Price.     

 I’ve been preparing a review of a new statistics textbook aimed at students and practitioners in the “physical sciences,” as distinct from the social sciences and also distinct from people who intend to take more statistics courses. I figured that since it’s been years since I looked at an intro stats textbook, I should look at a few others and see how they differ from this one, so in addition to the book I’m reviewing I’ve looked at some other textbooks aimed at similar audiences: Milton and Arnold; Hines, Montgomery, Goldsman, and Borror; and a few others. I also looked at the table of contents of several more. There is a lot of overlap in the coverage of these books — they all have discussions of common discrete and continuous distributions, joint distributions, descriptive statistics, parameter estimation, hypothesis testing, linear regression, ANOVA, factorial experimental design, and a few other topics. 

       

 I can see how, from a statisti</p><p>4 0.097815558 <a title="938-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-20-Prior_beliefs_about_locations_of_decision_boundaries.html">1130 andrew gelman stats-2012-01-20-Prior beliefs about locations of decision boundaries</a></p>
<p>Introduction: Forest Gregg writes: 
  
  
I want to incorporate a prior belief into an estimation of a logistic regression classifier of points distributed in a 2d space. My prior belief is a funny kind of prior though. It’s a belief about where the decision boundary between classes should fall. Over the 2d space, I lay a grid, and I believe that a decision boundary that separates any two classes should fall along any of the grid line with some probablity, and that the decision boundary should fall anywhere except a gridline with a much lower probability.  


For the two class case, and a logistic regression model parameterized by W and data X, my prior could perhaps be expressed  


Pr(W) = (normalizing constant)/exp(d) where d = f(grid,W,X) such that when logistic(W^TX)= .5 and X is ‘far’ from grid lines, then d is large. Have you ever seen a model like this, or do you have any notions about a good avenue to pursue?


My real data consist of geocoded Craigslist’s postings that are labeled with the</p><p>5 0.097707696 <a title="938-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-01-Hoe_noem_je%3F.html">1191 andrew gelman stats-2012-03-01-Hoe noem je?</a></p>
<p>Introduction: Gerrit Storms reports on an interesting linguistic research project in which you can participate!  Here’s the description:
  
Over the past few weeks, we have been trying to set up a scientific study that is important for many researchers interested in words, word meaning, semantics, and cognitive science in general.  It is a huge word association project, in which people are asked to participate in a small task that doesn’t last longer than 5 minutes. Our goal is to build a global word association network that contains connections between about 40,000 words, the size of the lexicon of an average adult. Setting up such a network might learn us a lot about semantic memory, how it develops, and maybe also about how it can deteriorate (like in Alzheimer’s disease). Most people enjoy doing the task, but we need thousands of participants to succeed. Up till today, we found about 53,000 participants willing to do the little task, but we need more subjects. That is why we address you.  Would</p><p>6 0.096038125 <a title="938-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>7 0.09482915 <a title="938-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-22-Handling_multiple_versions_of_an_outcome_variable.html">726 andrew gelman stats-2011-05-22-Handling multiple versions of an outcome variable</a></p>
<p>8 0.09363161 <a title="938-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-19-Will_Stan_work_well_with_40%C3%9740_matrices%3F.html">861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</a></p>
<p>9 0.093567021 <a title="938-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-29-We_go_to_war_with_the_data_we_have%2C_not_the_data_we_want.html">1289 andrew gelman stats-2012-04-29-We go to war with the data we have, not the data we want</a></p>
<p>10 0.092973158 <a title="938-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-27-Three_unblinded_mice.html">2115 andrew gelman stats-2013-11-27-Three unblinded mice</a></p>
<p>11 0.091704935 <a title="938-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-10-Please_send_all_comments_to_-dev-ripley.html">1933 andrew gelman stats-2013-07-10-Please send all comments to -dev-ripley</a></p>
<p>12 0.091302946 <a title="938-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-10-Cross-validation_and_Bayesian_estimation_of_tuning_parameters.html">2129 andrew gelman stats-2013-12-10-Cross-validation and Bayesian estimation of tuning parameters</a></p>
<p>13 0.089595132 <a title="938-tfidf-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Useful_models%2C_model_checking%2C_and_external_validation%3A__a_mini-discussion.html">244 andrew gelman stats-2010-08-30-Useful models, model checking, and external validation:  a mini-discussion</a></p>
<p>14 0.088621438 <a title="938-tfidf-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-04-U-Haul_statistics.html">318 andrew gelman stats-2010-10-04-U-Haul statistics</a></p>
<p>15 0.087805703 <a title="938-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>16 0.085573032 <a title="938-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-03-Uncertainty_in_parameter_estimates_using_multilevel_models.html">1966 andrew gelman stats-2013-08-03-Uncertainty in parameter estimates using multilevel models</a></p>
<p>17 0.084522203 <a title="938-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-06-Question_27_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1368 andrew gelman stats-2012-06-06-Question 27 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>18 0.084412798 <a title="938-tfidf-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-18-Subtle_statistical_issues_to_be_debated_on_TV..html">350 andrew gelman stats-2010-10-18-Subtle statistical issues to be debated on TV.</a></p>
<p>19 0.082832567 <a title="938-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-27-Setting_up_Jitts_online.html">2041 andrew gelman stats-2013-09-27-Setting up Jitts online</a></p>
<p>20 0.082770742 <a title="938-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-11-%E2%80%9CInformative_g-Priors_for_Logistic_Regression%E2%80%9D.html">2017 andrew gelman stats-2013-09-11-“Informative g-Priors for Logistic Regression”</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.177), (1, 0.059), (2, 0.031), (3, -0.015), (4, 0.053), (5, 0.036), (6, 0.033), (7, 0.012), (8, -0.008), (9, 0.01), (10, 0.01), (11, 0.048), (12, -0.014), (13, -0.046), (14, -0.031), (15, -0.021), (16, 0.007), (17, -0.02), (18, 0.009), (19, -0.004), (20, 0.008), (21, 0.031), (22, 0.006), (23, 0.014), (24, -0.001), (25, -0.011), (26, 0.017), (27, -0.015), (28, 0.022), (29, -0.009), (30, 0.019), (31, 0.031), (32, 0.028), (33, 0.027), (34, 0.004), (35, -0.033), (36, -0.031), (37, 0.009), (38, 0.023), (39, -0.006), (40, 0.023), (41, 0.001), (42, -0.032), (43, -0.002), (44, -0.024), (45, 0.0), (46, -0.01), (47, -0.012), (48, 0.031), (49, 0.006)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9663617 <a title="938-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-03-Comparing_prediction_errors.html">938 andrew gelman stats-2011-10-03-Comparing prediction errors</a></p>
<p>Introduction: Someone named James writes: 
  
  
I’m working on a classification task, sentence segmentation.  The classifier algorithm we use (BoosTexter, a boosted learning algorithm) classifies each word independently conditional on its features, i.e. a bag-of-words model, so any contextual clues need to be encoded into the features.  The feature extraction system I am proposing in my thesis uses a heteroscedastic LDA to transform data to produce the features the classifier runs on. The HLDA system has a couple parameters I’m testing, and I’m running a 3×2 full factorial experiment. That’s the background which may or may not be relevant to the question.


The output of each trial is a class (there are only 2 classes, right now) for every word in the dataset.  Because of the nature of the task, one class strongly predominates, say 90-95% of the data.  My question is this: in terms of overall performance (we use F1 score), many of these trials are pretty close together, which leads me to ask whethe</p><p>2 0.7789219 <a title="938-lsi-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-07-There_are_never_70_distinct_parameters.html">327 andrew gelman stats-2010-10-07-There are never 70 distinct parameters</a></p>
<p>Introduction: Sam Seaver writes:
  
I’m a graduate student in computational biology, and I’m relatively new to advanced statistics, and am trying to teach myself how best to approach a problem I have.


My dataset is a small sparse matrix of 150 cases and 70 predictors, it is sparse as in many zeros, not many ‘NA’s.  Each case is a nutrient that is fed into an in silico organism, and its response is whether or not it stimulates growth, and each predictor is one of 70 different pathways that the nutrient may or may not belong to.  Because all of the nutrients do not belong to all of the pathways, there are thus many zeros in my matrix.  My goal is to be able to use the pathways themselves to predict whether or not a nutrient could stimulate growth, thus I wanted to compute regression coefficients for each pathway, with which I could apply to other nutrients for other species.


There are quite a few singularities in the dataset (summary(glm) reports that 14 coefficients are not defined because of sin</p><p>3 0.77418804 <a title="938-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-22-Handling_multiple_versions_of_an_outcome_variable.html">726 andrew gelman stats-2011-05-22-Handling multiple versions of an outcome variable</a></p>
<p>Introduction: Jay Ulfelder asks:
  
I have a question for you about what to do in a situation where you have two measures of your dependent variable and no prior reasons to strongly favor one over the other.


Here’s what brings this up: I’m working on a project with Michael Ross where we’re modeling transitions to and from democracy in countries worldwide since 1960 to estimate the effects of oil income on the likelihood of those events’ occurrence. We’ve got a TSCS data set, and we’re using a discrete-time event history design, splitting the sample by regime type at the start of each year and then using multilevel logistic regression models with parametric measures of time at risk and random intercepts at the country and region levels. (We’re also checking for the usefulness of random slopes for oil wealth at one or the other level and then including them if they improve a model’s goodness of fit.) All of this is being done in Stata with the gllamm module.


Our problem is that we have two plausib</p><p>4 0.76899374 <a title="938-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-29-Difficulties_with_the_1-4-power_transformation.html">1142 andrew gelman stats-2012-01-29-Difficulties with the 1-4-power transformation</a></p>
<p>Introduction: John Hayes writes:
  
I am a fan of the quarter root transform ever since reading about it on your  blog .  However, today my student and I hit a wall that I’m hoping you might have some insight on.


By training, I am a psychophysicist (think SS Stevens), and people in my field often log transform data prior to analysis. However, this data frequently contains zeros, so I’ve tried using quarter root transforms to get around this. But until today, I had never tried to back transform the plot axis for readability. I assumed this would be straightforward – alas it is not.


Specifically, we quarter root transformed our data, performed an ANOVA, got what we thought was a reasonable effect, and then plotted the data. So far so good. However, the LS means in question are below 1, meaning that raising them to the 4th power just makes them smaller, and uninterpretable in the original metric.


Do you have any thoughts or insights you might share?
  
My reply:
 
I don’t see the problem with pre</p><p>5 0.76798129 <a title="938-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>Introduction: A research psychologist writes in with a question that’s so long that I’ll put my answer first, then put the question itself below the fold.
 
Here’s my reply:
 
As I wrote in my Anova paper and in my book with Jennifer Hill, I do think that multilevel models can completely replace Anova.  At the same time, I think the central idea of Anova should persist in our understanding of these models.  To me the central idea of Anova is not F-tests or p-values or sums of squares, but rather the idea of predicting an outcome based on factors with discrete levels, and understanding these factors using variance components.
 
The continuous or categorical response thing doesn’t really matter so much to me.  I have no problem using a normal linear model for continuous outcomes (perhaps suitably transformed) and a logistic model for binary outcomes.
 
I don’t want to throw away interactions just because they’re not statistically significant.  I’d rather partially pool them toward zero using an inform</p><p>6 0.74706638 <a title="938-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-27-Setting_up_Jitts_online.html">2041 andrew gelman stats-2013-09-27-Setting up Jitts online</a></p>
<p>7 0.7466656 <a title="938-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-13-Help_with_this_problem%2C_win_valuable_prizes.html">1164 andrew gelman stats-2012-02-13-Help with this problem, win valuable prizes</a></p>
<p>8 0.74475992 <a title="938-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-02-Interaction-based_feature_selection_and_classification_for_high-dimensional_biological_data.html">1703 andrew gelman stats-2013-02-02-Interaction-based feature selection and classification for high-dimensional biological data</a></p>
<p>9 0.74384534 <a title="938-lsi-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-31-Predicting_marathon_times.html">245 andrew gelman stats-2010-08-31-Predicting marathon times</a></p>
<p>10 0.7433601 <a title="938-lsi-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-03-He_doesn%E2%80%99t_trust_the_fit_._._._r%3D.999.html">315 andrew gelman stats-2010-10-03-He doesn’t trust the fit . . . r=.999</a></p>
<p>11 0.73932683 <a title="938-lsi-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Blending_results_from_two_relatively_independent_multi-level_models.html">250 andrew gelman stats-2010-09-02-Blending results from two relatively independent multi-level models</a></p>
<p>12 0.73818564 <a title="938-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-17-Data_problems%2C_coding_errors%E2%80%A6what_can_be_done%3F.html">1807 andrew gelman stats-2013-04-17-Data problems, coding errors…what can be done?</a></p>
<p>13 0.7381835 <a title="938-lsi-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Bayesian_Uncertainty_Quantification_for_Differential_Equations%21.html">2311 andrew gelman stats-2014-04-29-Bayesian Uncertainty Quantification for Differential Equations!</a></p>
<p>14 0.73488754 <a title="938-lsi-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-17-Futures_contracts%2C_Granger_causality%2C_and_my_preference_for_estimation_to_testing.html">212 andrew gelman stats-2010-08-17-Futures contracts, Granger causality, and my preference for estimation to testing</a></p>
<p>15 0.73298699 <a title="938-lsi-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-19-The_scope_for_snooping.html">1070 andrew gelman stats-2011-12-19-The scope for snooping</a></p>
<p>16 0.72926837 <a title="938-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-29-Going_negative.html">1918 andrew gelman stats-2013-06-29-Going negative</a></p>
<p>17 0.72521317 <a title="938-lsi-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-13-Ross_Ihaka_to_R%3A__Drop_Dead.html">272 andrew gelman stats-2010-09-13-Ross Ihaka to R:  Drop Dead</a></p>
<p>18 0.72503889 <a title="938-lsi-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-31-Somewhat_Bayesian_multilevel_modeling.html">246 andrew gelman stats-2010-08-31-Somewhat Bayesian multilevel modeling</a></p>
<p>19 0.72463506 <a title="938-lsi-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-23-More_on_those_L.A._Times_estimates_of_teacher_effectiveness.html">226 andrew gelman stats-2010-08-23-More on those L.A. Times estimates of teacher effectiveness</a></p>
<p>20 0.7245627 <a title="938-lsi-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-08-Regression_and_causality_and_variable_ordering.html">2364 andrew gelman stats-2014-06-08-Regression and causality and variable ordering</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(9, 0.015), (16, 0.044), (24, 0.614), (99, 0.207)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99720675 <a title="938-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-07-Neutral_noninformative_and_informative_conjugate_beta_and_gamma_prior_distributions.html">1046 andrew gelman stats-2011-12-07-Neutral noninformative and informative conjugate beta and gamma prior distributions</a></p>
<p>Introduction: Jouni Kerman did a cool bit of research justifying the Beta (1/3, 1/3) prior as noninformative for binomial data, and the Gamma (1/3, 0) prior for Poisson data.
 
You probably thought that nothing new could be said about noninformative priors in such basic problems, but you were wrong!
 
Here’s  the story :
  
The conjugate binomial and Poisson models are commonly used for estimating proportions or rates. However, it is not well known that the conventional noninformative conjugate priors tend to shrink the posterior quantiles toward the boundary or toward the middle of the parameter space, making them thus appear excessively informative. The shrinkage is always largest when the number of observed events is small. This behavior persists for all sample sizes and exposures. The effect of the prior is therefore most conspicuous and potentially controversial when analyzing rare events. As alternative default conjugate priors, I [Jouni] introduce Beta(1/3, 1/3) and Gamma(1/3, 0), which I cal</p><p>2 0.99671447 <a title="938-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-29-ARM_solutions.html">240 andrew gelman stats-2010-08-29-ARM solutions</a></p>
<p>Introduction: People sometimes email asking if a solution set is available for the exercises in ARM.  The answer, unfortunately, is no.  Many years ago, I wrote up 50 solutions for BDA and it was a lot of work–really, it was like writing a small book in itself.  The trouble is that, once I started writing them up, I wanted to do it right, to set a good example.  That’s a lot more effort than simply scrawling down some quick answers.</p><p>3 0.9949646 <a title="938-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-31-Paying_survey_respondents.html">1437 andrew gelman stats-2012-07-31-Paying survey respondents</a></p>
<p>Introduction: I  agree  with  Casey Mulligan  that participants in government surveys should be paid, and I think it should be part of the code of ethics for commercial pollsters to compensate their respondents also.
 
As Mulligan points out, if a survey is worth doing, it should be worth compensating the participants for their time and effort.
 
P.S.  Just to clarify, I do  not  recommend that Census surveys be made voluntary, I just think that respondents (who can be required to participate) should be paid a small amount.
 
P.P.S.  More rant  here .</p><p>4 0.99334246 <a title="938-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-30-New_innovations_in_spam.html">545 andrew gelman stats-2011-01-30-New innovations in spam</a></p>
<p>Introduction: I received the following (unsolicited) email today:
  
Hello Andrew,


I’m interested in whether you are accepting guest article submissions for your site Statistical Modeling, Causal Inference, and Social Science? I’m the owner of the recently created nonprofit site OnlineEngineeringDegree.org and am interested in writing / submitting an article for your consideration to be published on your site. Is that something you’d be willing to consider, and if so, what specs in terms of topics or length requirements would you be looking for?


Thanks you for your time, and if you have any questions or are interested, I’d appreciate you letting me know.


Sincerely, 
Samantha Rhodes
  
Huh?
 
P.S.  My vote for most obnoxious spam remains  this one , which does its best to dilute whatever remains of the reputation of Wolfram Research.  Or maybe that particular bit of spam was written by a particularly awesome cellular automaton that Wolfram discovered?  I guess in the world of big-time software</p><p>5 0.99291903 <a title="938-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-02-So-called_Bayesian_hypothesis_testing_is_just_as_bad_as_regular_hypothesis_testing.html">643 andrew gelman stats-2011-04-02-So-called Bayesian hypothesis testing is just as bad as regular hypothesis testing</a></p>
<p>Introduction: Steve Ziliak points me to  this article  by the always-excellent Carl Bialik, slamming hypothesis tests.  I only wish Carl had talked with me before so hastily posting, though!  I would’ve argued with some of the things in the article.  In particular, he writes:
  
Reese and Brad Carlin . . . suggest that Bayesian statistics are a better alternative, because they tackle the probability that the hypothesis is true head-on, and incorporate prior knowledge about the variables involved.
  
Brad Carlin does great work in theory, methods, and applications, and I like the bit about the prior knowledge (although I might prefer the more general phrase “additional information”), but I hate that quote!  
 
My quick response is that the hypothesis of zero effect is almost never true!  The problem with the significance testing framework–Bayesian or otherwise–is in the obsession with the possibility of an exact zero effect.  The real concern is not with zero, it’s with claiming a positive effect whe</p><p>6 0.99281573 <a title="938-lda-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-17-Attractive_models_%28and_data%29_wanted_for_statistical_art_show..html">471 andrew gelman stats-2010-12-17-Attractive models (and data) wanted for statistical art show.</a></p>
<p>7 0.98188722 <a title="938-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-18-Breastfeeding%2C_infant_hyperbilirubinemia%2C_statistical_graphics%2C_and_modern_medicine.html">38 andrew gelman stats-2010-05-18-Breastfeeding, infant hyperbilirubinemia, statistical graphics, and modern medicine</a></p>
<p>same-blog 8 0.9737035 <a title="938-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-03-Comparing_prediction_errors.html">938 andrew gelman stats-2011-10-03-Comparing prediction errors</a></p>
<p>9 0.97277695 <a title="938-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-29-Ethics_and_statistics_in_development_research.html">241 andrew gelman stats-2010-08-29-Ethics and statistics in development research</a></p>
<p>10 0.97128439 <a title="938-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-29-More_by_Berger_and_me_on_weakly_informative_priors.html">1092 andrew gelman stats-2011-12-29-More by Berger and me on weakly informative priors</a></p>
<p>11 0.97119302 <a title="938-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-12-Fixing_the_race%2C_ethnicity%2C_and_national_origin_questions_on_the_U.S._Census.html">1978 andrew gelman stats-2013-08-12-Fixing the race, ethnicity, and national origin questions on the U.S. Census</a></p>
<p>12 0.97035122 <a title="938-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-01-Mothers_and_Moms.html">1479 andrew gelman stats-2012-09-01-Mothers and Moms</a></p>
<p>13 0.96425509 <a title="938-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-30-Extended_Binary_Format_Support_for_Mac_OS_X.html">59 andrew gelman stats-2010-05-30-Extended Binary Format Support for Mac OS X</a></p>
<p>14 0.96405131 <a title="938-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-04-Wanna_be_the_next_Tyler_Cowen%3F__It%E2%80%99s_not_as_easy_as_you_might_think%21.html">1787 andrew gelman stats-2013-04-04-Wanna be the next Tyler Cowen?  It’s not as easy as you might think!</a></p>
<p>15 0.96283305 <a title="938-lda-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-28-God-leaf-tree.html">2229 andrew gelman stats-2014-02-28-God-leaf-tree</a></p>
<p>16 0.96223062 <a title="938-lda-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-23-Capitalism_as_a_form_of_voluntarism.html">482 andrew gelman stats-2010-12-23-Capitalism as a form of voluntarism</a></p>
<p>17 0.95506996 <a title="938-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-04-Too_many_MC%E2%80%99s_not_enough_MIC%E2%80%99s%2C_or_What_principles_should_govern_attempts_to_summarize_bivariate_associations_in_large_multivariate_datasets%3F.html">1706 andrew gelman stats-2013-02-04-Too many MC’s not enough MIC’s, or What principles should govern attempts to summarize bivariate associations in large multivariate datasets?</a></p>
<p>18 0.95173413 <a title="938-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-09-%E2%80%9CHeterogeneity_of_variance_in_experimental_studies%3A__A_challenge_to_conventional_interpretations%E2%80%9D.html">1891 andrew gelman stats-2013-06-09-“Heterogeneity of variance in experimental studies:  A challenge to conventional interpretations”</a></p>
<p>19 0.9512974 <a title="938-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-03-An_argument_that_can%E2%80%99t_possibly_make_sense.html">743 andrew gelman stats-2011-06-03-An argument that can’t possibly make sense</a></p>
<p>20 0.95052004 <a title="938-lda-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-27-It%E2%80%99s_better_than_being_forwarded_the_latest_works_of_you-know-who.html">373 andrew gelman stats-2010-10-27-It’s better than being forwarded the latest works of you-know-who</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
