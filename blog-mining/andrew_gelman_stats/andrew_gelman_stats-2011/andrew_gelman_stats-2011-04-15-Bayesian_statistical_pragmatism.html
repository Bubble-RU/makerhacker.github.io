<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>662 andrew gelman stats-2011-04-15-Bayesian statistical pragmatism</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-662" href="#">andrew_gelman_stats-2011-662</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>662 andrew gelman stats-2011-04-15-Bayesian statistical pragmatism</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-662-html" href="http://andrewgelman.com/2011/04/15/bayesian_statis_1/">html</a></p><p>Introduction: Rob Kass’s  article  on statistical pragmatism is scheduled to appear in  Statistical Science  along with some discussions.  Here are my comments.
 
I agree with Rob Kass’s point that we can and should make use of statistical methods developed under different philosophies, and I am happy to take the opportunity to elaborate on some of his arguments.
 
I’ll discuss the following: 
- Foundations of probability 
- Confidence intervals and hypothesis tests 
- Sampling 
- Subjectivity and belief 
- Different schools of statistics
  

 
 Foundations of probability.   Kass describes probability theory as anchored upon physical randomization (coin flips, die rolls and the like) but being useful more generally as a mathematical model.  I completely agree but would also add another anchoring point:  calibration.  Calibration of probability assessments is an objective, not subjective process, although some subjectivity (or scientific judgment) is necessarily involved in the choice of events used</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I agree with Rob Kass’s point that we can and should make use of statistical methods developed under different philosophies, and I am happy to take the opportunity to elaborate on some of his arguments. [sent-3, score-0.324]
</p><p>2 I’ll discuss the following:  - Foundations of probability  - Confidence intervals and hypothesis tests  - Sampling  - Subjectivity and belief  - Different schools of statistics        Foundations of probability. [sent-4, score-0.543]
</p><p>3 Kass describes probability theory as anchored upon physical randomization (coin flips, die rolls and the like) but being useful more generally as a mathematical model. [sent-5, score-0.447]
</p><p>4 Calibration of probability assessments is an objective, not subjective process, although some subjectivity (or scientific judgment) is necessarily involved in the choice of events used in the calibration. [sent-7, score-0.495]
</p><p>5 In that way, Bayesian probability calibration is closely connected to frequentist probability statements, in that both are conditional on “reference sets” of comparable events. [sent-8, score-0.458]
</p><p>6 I agree with Kass that confidence and statistical significance are “valuable inferential tools. [sent-11, score-0.411]
</p><p>7 In the Neyman-Pearson theory of inference, confidence and statistical significance are two sides of the same coin, with a confidence interval being the set of parameter values not rejected by a significance test. [sent-13, score-0.665]
</p><p>8 In a modern Bayesian approach, confidence intervals and hypothesis testing are both important but are not isomorphic; they represent two different steps of inference. [sent-15, score-0.53]
</p><p>9 Kass discusses the role of sampling as a model for understanding statistical inference. [sent-21, score-0.305]
</p><p>10 Ultimately, sample is just another word for subset, and in both Bayesian and classical inference, appropriate generalization from sample to population depends on a model for the sampling or selection process. [sent-27, score-0.287]
</p><p>11 I have no problem with Kass’s use of sampling as a framework for inference, and I think this will work even better if he emphasizes the generalization from real samples to real populations–not just mathematical constructs–that are central to so much of our applied inferences. [sent-28, score-0.465]
</p><p>12 The only two statements in Kass’s article that I clearly disagree with are the following two claims:  “the only solid foundation for Bayesianism is subjective,” and “the most fundamental belief of any scientist is that the theoretical and real worlds are aligned. [sent-30, score-0.343]
</p><p>13 Claims of the subjectivity of Bayesian inference have been much debated, and I am under no illusion that I can resolve them here. [sent-32, score-0.367]
</p><p>14 To put it another way, I will accept the idea of subjective Bayesianism when this same subjectivity is acknowledged for other methods of inference. [sent-36, score-0.349]
</p><p>15 ”  I agree with Kass that scientists and statisticians can and should feel free to make assumptions without falling into a “solipsistic quagmire. [sent-38, score-0.336]
</p><p>16 ”   Finally, I am surprised to see Kass write that scientists believe that the theoretical and real worlds are aligned. [sent-39, score-0.222]
</p><p>17 It is from acknowledging the discrepancies between these worlds that we can (a) feel free to make assumptions without being paralyzed by fear of making mistakes, and (b) feel free to check the fit of our models (those hypothesis tests again! [sent-40, score-0.682]
</p><p>18 I assume that Kass is using the word “aligned” in a loose sense, to imply that scientists believe that their models are appropriate to reality even if not fully correct. [sent-43, score-0.24]
</p><p>19 Often in my own applied work I have used models that have clear flaws, models that are at best “phenomenological” in the sense of fitting the data rather than corresponding to underlying processes of interest–and often such models don’t fit the data so well either. [sent-45, score-0.503]
</p><p>20 Ideas of sampling, inference, and model checking are important in many different statistical traditions and we are lucky to have so many different ideas on which to draw for inspiration in our applied and methodological research. [sent-50, score-0.361]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kass', 0.554), ('subjectivity', 0.237), ('confidence', 0.171), ('sampling', 0.155), ('rob', 0.15), ('probability', 0.146), ('inference', 0.13), ('models', 0.128), ('statements', 0.125), ('hypothesis', 0.124), ('assumptions', 0.112), ('subjective', 0.112), ('intervals', 0.109), ('calibration', 0.108), ('bayesian', 0.106), ('worlds', 0.106), ('anchored', 0.096), ('significance', 0.091), ('statistical', 0.089), ('coin', 0.081), ('physical', 0.079), ('randomization', 0.074), ('bayesianism', 0.074), ('different', 0.071), ('generalization', 0.071), ('applied', 0.069), ('foundations', 0.066), ('environmental', 0.065), ('repeat', 0.061), ('model', 0.061), ('agree', 0.06), ('discuss', 0.058), ('real', 0.058), ('scientists', 0.058), ('frequentist', 0.058), ('free', 0.056), ('steps', 0.055), ('even', 0.054), ('developed', 0.054), ('belief', 0.054), ('theory', 0.052), ('merely', 0.052), ('schools', 0.052), ('reference', 0.052), ('flips', 0.051), ('intolerant', 0.051), ('isomorphic', 0.051), ('make', 0.05), ('aspects', 0.05), ('fit', 0.05)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="662-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-15-Bayesian_statistical_pragmatism.html">662 andrew gelman stats-2011-04-15-Bayesian statistical pragmatism</a></p>
<p>Introduction: Rob Kass’s  article  on statistical pragmatism is scheduled to appear in  Statistical Science  along with some discussions.  Here are my comments.
 
I agree with Rob Kass’s point that we can and should make use of statistical methods developed under different philosophies, and I am happy to take the opportunity to elaborate on some of his arguments.
 
I’ll discuss the following: 
- Foundations of probability 
- Confidence intervals and hypothesis tests 
- Sampling 
- Subjectivity and belief 
- Different schools of statistics
  

 
 Foundations of probability.   Kass describes probability theory as anchored upon physical randomization (coin flips, die rolls and the like) but being useful more generally as a mathematical model.  I completely agree but would also add another anchoring point:  calibration.  Calibration of probability assessments is an objective, not subjective process, although some subjectivity (or scientific judgment) is necessarily involved in the choice of events used</p><p>2 0.79483277 <a title="662-tfidf-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-04-Rob_Kass_on_statistical_pragmatism%2C_and_my_reactions.html">317 andrew gelman stats-2010-10-04-Rob Kass on statistical pragmatism, and my reactions</a></p>
<p>Introduction: Rob Kass  writes :
  
Statistics has moved beyond the frequentist-Bayesian controversies of the past. Where does this leave our ability to interpret results? I [Kass] suggest that a philosophy compatible with statistical practice, labeled here statistical pragmatism, serves as a foundation for inference. Statistical pragmatism is inclusive and emphasizes the assumptions that connect statistical models with observed data. I argue that introductory courses often mis-characterize the process of statistical inference and I propose an alternative “big picture” depiction.
  
In  my comments , I pretty much agree with everything Rob says, with a few points of elaboration:
  
Kass describes probability theory as anchored upon physical randomization (coin flips, die rolls and the like) but being useful more generally as a mathematical model.  I completely agree but would also add another anchoring point:  calibration.  Calibration of probability assessments is an objective, not subjective proce</p><p>3 0.20460956 <a title="662-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>Introduction: Robert Bell pointed me to  this post  by Brad De Long on Bayesian statistics, and then I also noticed  this  from Noah Smith, who wrote:
  
My impression is that although the Bayesian/Frequentist debate is interesting and intellectually fun, there’s really not much “there” there… despite being so-hip-right-now, Bayesian is not the Statistical Jesus.
  
I’m happy to see the discussion going in this direction.  Twenty-five years ago or so, when I got into this biz, there were some serious anti-Bayesian attitudes floating around in mainstream statistics.  Discussions in the journals sometimes devolved into debates of the form, “Bayesians:  knaves or fools?”.  You’d get all sorts of free-floating skepticism about any prior distribution at all, even while people were accepting without question (and doing theory on) logistic regressions, proportional hazards models, and all sorts of strong strong models.  (In the subfield of survey sampling, various prominent researchers would refuse to mode</p><p>4 0.20342052 <a title="662-tfidf-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-22-Philosophy_of_Bayes_and_non-Bayes%3A__A_dialogue_with_Deborah_Mayo.html">291 andrew gelman stats-2010-09-22-Philosophy of Bayes and non-Bayes:  A dialogue with Deborah Mayo</a></p>
<p>Introduction: I sent Deborah Mayo a link to  my paper  with Cosma Shalizi on the philosophy of statistics, and she sent me the link to this conference which unfortunately already occurred.  (It’s too bad, because I’d have liked to have been there.)  I summarized my philosophy as follows:
  
I am highly sympathetic to the approach of Lakatos (or of Popper, if you consider Lakatos’s “Popper_2″ to be a reasonable simulation of the true Popperism), in that (a) I view statistical models as being built within theoretical structures, and (b) I see the checking and refutation of models to be a key part of scientific progress.  A big problem I have with mainstream Bayesianism is its “inductivist” view that science can operate completely smoothly with posterior updates:  the idea that new data causes us to increase the posterior probability of good models and decrease the posterior probability of bad models.  I don’t buy that:  I see models as ever-changing entities that are flexible and can be patched and ex</p><p>5 0.19331534 <a title="662-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>Introduction: Prasanta Bandyopadhyay and Gordon Brittan  write :
  
We introduce a distinction, unnoticed in the literature, between four varieties of objective Bayesianism. What we call ‘strong objective Bayesianism’ is characterized by two claims, that all scientific inference is ‘logical’ and that, given the same background information two agents will ascribe a unique probability to their priors. We think that neither of these claims can be sustained; in this sense, they are ‘dogmatic’. The first fails to recognize that some scientific inference, in particular that concerning evidential relations, is not (in the appropriate sense) logical, the second fails to provide a non-question-begging account of ‘same background information’. We urge that a suitably objective Bayesian account of scientific inference does not require either of the claims. Finally, we argue that Bayesianism needs to be fine-grained in the same way that Bayesians fine-grain their beliefs.
  
I have not read their paper in detai</p><p>6 0.18984312 <a title="662-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-09-Coming_to_agreement_on_philosophy_of_statistics.html">1205 andrew gelman stats-2012-03-09-Coming to agreement on philosophy of statistics</a></p>
<p>7 0.18892057 <a title="662-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>8 0.18876445 <a title="662-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>9 0.1860294 <a title="662-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-25-Why_it_doesn%E2%80%99t_make_sense_in_general_to_form_confidence_intervals_by_inverting_hypothesis_tests.html">870 andrew gelman stats-2011-08-25-Why it doesn’t make sense in general to form confidence intervals by inverting hypothesis tests</a></p>
<p>10 0.18451029 <a title="662-tfidf-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-05-Does_posterior_predictive_model_checking_fit_with_the_operational_subjective_approach%3F.html">320 andrew gelman stats-2010-10-05-Does posterior predictive model checking fit with the operational subjective approach?</a></p>
<p>11 0.18363446 <a title="662-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-24-Why_it_doesn%E2%80%99t_make_sense_in_general_to_form_confidence_intervals_by_inverting_hypothesis_tests.html">1913 andrew gelman stats-2013-06-24-Why it doesn’t make sense in general to form confidence intervals by inverting hypothesis tests</a></p>
<p>12 0.18188307 <a title="662-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>13 0.17928679 <a title="662-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-11-Why_waste_time_philosophizing%3F.html">1719 andrew gelman stats-2013-02-11-Why waste time philosophizing?</a></p>
<p>14 0.1787506 <a title="662-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-06-Yes%2C_checking_calibration_of_probability_forecasts_is_part_of_Bayesian_statistics.html">1610 andrew gelman stats-2012-12-06-Yes, checking calibration of probability forecasts is part of Bayesian statistics</a></p>
<p>15 0.17189571 <a title="662-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-24-Untangling_the_Jeffreys-Lindley_paradox.html">1182 andrew gelman stats-2012-02-24-Untangling the Jeffreys-Lindley paradox</a></p>
<p>16 0.17131762 <a title="662-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<p>17 0.1667586 <a title="662-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Kind_of_Bayesian.html">811 andrew gelman stats-2011-07-20-Kind of Bayesian</a></p>
<p>18 0.16154593 <a title="662-tfidf-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-21-Instead_of_%E2%80%9Cconfidence_interval%2C%E2%80%9D_let%E2%80%99s_say_%E2%80%9Cuncertainty_interval%E2%80%9D.html">480 andrew gelman stats-2010-12-21-Instead of “confidence interval,” let’s say “uncertainty interval”</a></p>
<p>19 0.1608772 <a title="662-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-16-Long_discussion_about_causal_inference_and_the_use_of_hierarchical_models_to_bridge_between_different_inferential_settings.html">1418 andrew gelman stats-2012-07-16-Long discussion about causal inference and the use of hierarchical models to bridge between different inferential settings</a></p>
<p>20 0.15980619 <a title="662-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-01-Martin_and_Liu%3A__Probabilistic_inference_based_on_consistency_of_model_with_data.html">1095 andrew gelman stats-2012-01-01-Martin and Liu:  Probabilistic inference based on consistency of model with data</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.283), (1, 0.209), (2, -0.072), (3, -0.026), (4, -0.136), (5, 0.001), (6, -0.113), (7, 0.096), (8, 0.07), (9, -0.12), (10, -0.083), (11, -0.015), (12, -0.036), (13, -0.03), (14, -0.046), (15, -0.019), (16, -0.025), (17, -0.036), (18, 0.002), (19, -0.056), (20, 0.045), (21, -0.046), (22, 0.003), (23, 0.03), (24, 0.034), (25, -0.028), (26, 0.01), (27, 0.01), (28, -0.042), (29, 0.056), (30, -0.028), (31, -0.043), (32, -0.018), (33, 0.005), (34, -0.062), (35, 0.025), (36, 0.014), (37, 0.039), (38, -0.007), (39, 0.055), (40, 0.026), (41, -0.036), (42, 0.1), (43, -0.026), (44, 0.009), (45, -0.029), (46, -0.07), (47, 0.024), (48, -0.058), (49, -0.055)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96897227 <a title="662-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-15-Bayesian_statistical_pragmatism.html">662 andrew gelman stats-2011-04-15-Bayesian statistical pragmatism</a></p>
<p>Introduction: Rob Kass’s  article  on statistical pragmatism is scheduled to appear in  Statistical Science  along with some discussions.  Here are my comments.
 
I agree with Rob Kass’s point that we can and should make use of statistical methods developed under different philosophies, and I am happy to take the opportunity to elaborate on some of his arguments.
 
I’ll discuss the following: 
- Foundations of probability 
- Confidence intervals and hypothesis tests 
- Sampling 
- Subjectivity and belief 
- Different schools of statistics
  

 
 Foundations of probability.   Kass describes probability theory as anchored upon physical randomization (coin flips, die rolls and the like) but being useful more generally as a mathematical model.  I completely agree but would also add another anchoring point:  calibration.  Calibration of probability assessments is an objective, not subjective process, although some subjectivity (or scientific judgment) is necessarily involved in the choice of events used</p><p>2 0.95626658 <a title="662-lsi-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-04-Rob_Kass_on_statistical_pragmatism%2C_and_my_reactions.html">317 andrew gelman stats-2010-10-04-Rob Kass on statistical pragmatism, and my reactions</a></p>
<p>Introduction: Rob Kass  writes :
  
Statistics has moved beyond the frequentist-Bayesian controversies of the past. Where does this leave our ability to interpret results? I [Kass] suggest that a philosophy compatible with statistical practice, labeled here statistical pragmatism, serves as a foundation for inference. Statistical pragmatism is inclusive and emphasizes the assumptions that connect statistical models with observed data. I argue that introductory courses often mis-characterize the process of statistical inference and I propose an alternative “big picture” depiction.
  
In  my comments , I pretty much agree with everything Rob says, with a few points of elaboration:
  
Kass describes probability theory as anchored upon physical randomization (coin flips, die rolls and the like) but being useful more generally as a mathematical model.  I completely agree but would also add another anchoring point:  calibration.  Calibration of probability assessments is an objective, not subjective proce</p><p>3 0.84470946 <a title="662-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-01-Martin_and_Liu%3A__Probabilistic_inference_based_on_consistency_of_model_with_data.html">1095 andrew gelman stats-2012-01-01-Martin and Liu:  Probabilistic inference based on consistency of model with data</a></p>
<p>Introduction: What better way to start then new year than with some hard-core statistical theory?
 
Ryan Martin and Chuanhai Liu send along  a new paper  on inferential models:
  
Probability is a useful tool for describing uncertainty, so it is natural to strive for a system of statistical inference based on probabilities for or against various hypotheses. But existing probabilistic inference methods struggle to provide a meaningful interpretation of the probabilities across experiments in sufficient generality. In this paper we further develop a promising new approach based on what are called inferential models (IMs). The fundamental idea behind IMs is that there is an unobservable auxiliary variable that itself describes the inherent uncertainty about the parameter of interest, and that posterior probabilistic inference can be accomplished by predicting this unobserved quantity. We describe a simple and intuitive three-step construction of a random set of candidate parameter values, each being co</p><p>4 0.78518057 <a title="662-lsi-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-22-Philosophy_of_Bayes_and_non-Bayes%3A__A_dialogue_with_Deborah_Mayo.html">291 andrew gelman stats-2010-09-22-Philosophy of Bayes and non-Bayes:  A dialogue with Deborah Mayo</a></p>
<p>Introduction: I sent Deborah Mayo a link to  my paper  with Cosma Shalizi on the philosophy of statistics, and she sent me the link to this conference which unfortunately already occurred.  (It’s too bad, because I’d have liked to have been there.)  I summarized my philosophy as follows:
  
I am highly sympathetic to the approach of Lakatos (or of Popper, if you consider Lakatos’s “Popper_2″ to be a reasonable simulation of the true Popperism), in that (a) I view statistical models as being built within theoretical structures, and (b) I see the checking and refutation of models to be a key part of scientific progress.  A big problem I have with mainstream Bayesianism is its “inductivist” view that science can operate completely smoothly with posterior updates:  the idea that new data causes us to increase the posterior probability of good models and decrease the posterior probability of bad models.  I don’t buy that:  I see models as ever-changing entities that are flexible and can be patched and ex</p><p>5 0.75417316 <a title="662-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-14-Progress%21__%28on_the_understanding_of_the_role_of_randomization_in_Bayesian_inference%29.html">1898 andrew gelman stats-2013-06-14-Progress!  (on the understanding of the role of randomization in Bayesian inference)</a></p>
<p>Introduction: Leading theoretical statistician Larry Wassserman  in 2008 :  
  
Some of the greatest contributions of statistics to science involve adding additional randomness and leveraging that randomness. Examples are randomized experiments, permutation tests, cross-validation and data-splitting. These are unabashedly frequentist ideas and, while one can strain to fit them into a Bayesian framework, they don’t really have a place in Bayesian inference. The fact that Bayesian methods do not naturally accommodate such a powerful set of statistical ideas seems like a serious deficiency.
  
To which I responded on the second-to-last paragraph of page 8  here .
 
Larry Wasserman in  2013 :
  
Some people say that there is no role for randomization in Bayesian inference. In other words, the randomization mechanism plays no role in Bayes’ theorem. But this is not really true. Without randomization, we can indeed derive a posterior for theta but it is highly sensitive to the prior. This is just a restat</p><p>6 0.74746293 <a title="662-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-13-Philosophy_of_Bayesian_statistics%3A__my_reactions_to_Wasserman.html">1165 andrew gelman stats-2012-02-13-Philosophy of Bayesian statistics:  my reactions to Wasserman</a></p>
<p>7 0.73397982 <a title="662-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-24-Untangling_the_Jeffreys-Lindley_paradox.html">1182 andrew gelman stats-2012-02-24-Untangling the Jeffreys-Lindley paradox</a></p>
<p>8 0.72599685 <a title="662-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-10-I_don%E2%80%99t_like_this_cartoon.html">1572 andrew gelman stats-2012-11-10-I don’t like this cartoon</a></p>
<p>9 0.71484065 <a title="662-lsi-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-05-Does_posterior_predictive_model_checking_fit_with_the_operational_subjective_approach%3F.html">320 andrew gelman stats-2010-10-05-Does posterior predictive model checking fit with the operational subjective approach?</a></p>
<p>10 0.71351409 <a title="662-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-09-Coming_to_agreement_on_philosophy_of_statistics.html">1205 andrew gelman stats-2012-03-09-Coming to agreement on philosophy of statistics</a></p>
<p>11 0.71333838 <a title="662-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<p>12 0.70402288 <a title="662-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-08-P-values_and_statistical_practice.html">1713 andrew gelman stats-2013-02-08-P-values and statistical practice</a></p>
<p>13 0.70367497 <a title="662-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>14 0.69394779 <a title="662-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Kind_of_Bayesian.html">811 andrew gelman stats-2011-07-20-Kind of Bayesian</a></p>
<p>15 0.6914109 <a title="662-lsi-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-24-Empirical_implications_of_Empirical_Implications_of_Theoretical_Models.html">2263 andrew gelman stats-2014-03-24-Empirical implications of Empirical Implications of Theoretical Models</a></p>
<p>16 0.68883246 <a title="662-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-17-Christian_Robert_on_the_Jeffreys-Lindley_paradox%3B_more_generally%2C_it%E2%80%99s_good_news_when_philosophical_arguments_can_be_transformed_into_technical_modeling_issues.html">2027 andrew gelman stats-2013-09-17-Christian Robert on the Jeffreys-Lindley paradox; more generally, it’s good news when philosophical arguments can be transformed into technical modeling issues</a></p>
<p>17 0.68735468 <a title="662-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-11-Gelman_on_Hennig_on_Gelman_on_Bayes.html">1208 andrew gelman stats-2012-03-11-Gelman on Hennig on Gelman on Bayes</a></p>
<p>18 0.68374449 <a title="662-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-31-Lindley%E2%80%99s_paradox.html">1355 andrew gelman stats-2012-05-31-Lindley’s paradox</a></p>
<p>19 0.68283594 <a title="662-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-11-Why_waste_time_philosophizing%3F.html">1719 andrew gelman stats-2013-02-11-Why waste time philosophizing?</a></p>
<p>20 0.68122464 <a title="662-lsi-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Useful_models%2C_model_checking%2C_and_external_validation%3A__a_mini-discussion.html">244 andrew gelman stats-2010-08-30-Useful models, model checking, and external validation:  a mini-discussion</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(8, 0.083), (9, 0.024), (15, 0.026), (16, 0.076), (17, 0.018), (18, 0.013), (20, 0.025), (21, 0.017), (24, 0.176), (35, 0.034), (84, 0.027), (86, 0.054), (99, 0.287)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98355305 <a title="662-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-04-Rob_Kass_on_statistical_pragmatism%2C_and_my_reactions.html">317 andrew gelman stats-2010-10-04-Rob Kass on statistical pragmatism, and my reactions</a></p>
<p>Introduction: Rob Kass  writes :
  
Statistics has moved beyond the frequentist-Bayesian controversies of the past. Where does this leave our ability to interpret results? I [Kass] suggest that a philosophy compatible with statistical practice, labeled here statistical pragmatism, serves as a foundation for inference. Statistical pragmatism is inclusive and emphasizes the assumptions that connect statistical models with observed data. I argue that introductory courses often mis-characterize the process of statistical inference and I propose an alternative “big picture” depiction.
  
In  my comments , I pretty much agree with everything Rob says, with a few points of elaboration:
  
Kass describes probability theory as anchored upon physical randomization (coin flips, die rolls and the like) but being useful more generally as a mathematical model.  I completely agree but would also add another anchoring point:  calibration.  Calibration of probability assessments is an objective, not subjective proce</p><p>same-blog 2 0.97191089 <a title="662-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-15-Bayesian_statistical_pragmatism.html">662 andrew gelman stats-2011-04-15-Bayesian statistical pragmatism</a></p>
<p>Introduction: Rob Kass’s  article  on statistical pragmatism is scheduled to appear in  Statistical Science  along with some discussions.  Here are my comments.
 
I agree with Rob Kass’s point that we can and should make use of statistical methods developed under different philosophies, and I am happy to take the opportunity to elaborate on some of his arguments.
 
I’ll discuss the following: 
- Foundations of probability 
- Confidence intervals and hypothesis tests 
- Sampling 
- Subjectivity and belief 
- Different schools of statistics
  

 
 Foundations of probability.   Kass describes probability theory as anchored upon physical randomization (coin flips, die rolls and the like) but being useful more generally as a mathematical model.  I completely agree but would also add another anchoring point:  calibration.  Calibration of probability assessments is an objective, not subjective process, although some subjectivity (or scientific judgment) is necessarily involved in the choice of events used</p><p>3 0.96900487 <a title="662-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-18-Multimodality_in_hierarchical_models.html">916 andrew gelman stats-2011-09-18-Multimodality in hierarchical models</a></p>
<p>Introduction: Jim Hodges posted a note to the Bugs mailing list that I thought could be of more general interest: 
  
  
Is multi-modality a common experience?  I [Hodges] think the answer is “nobody knows in any generality”.  Here are some examples of bimodality that certainly do *not* involve the kind of labeling problems that arise in mixture models.


The only systematic study of multimodality I know of is 


Liu J, Hodges JS (2003).  Posterior bimodality in the balanced one-way random effects model.  J.~Royal Stat.~Soc., Ser.~B, 65:247-255.


The surprise of this paper is that in the simplest possible hierarchical model (analyzed using the standard inverse-gamma priors for the two variances), bimodality occurs quite readily, although it is much less common to have two modes that are big enough so that you’d actually get a noticeable fraction of MCMC draws from both of them.  Because the restricted likelihood (= the marginal posterior for the two variances, if you’ve put flat priors on them) is</p><p>4 0.96434259 <a title="662-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-21-Judea_Pearl_on_why_he_is_%E2%80%9Conly_a_half-Bayesian%E2%80%9D.html">1133 andrew gelman stats-2012-01-21-Judea Pearl on why he is “only a half-Bayesian”</a></p>
<p>Introduction: In  an article  published in 2001, Pearl wrote:
  
I [Pearl] turned Bayesian in 1971, as soon as I began reading Savage’s monograph The Foundations of Statistical Inference [Savage, 1962]. The arguments were unassailable: (i) It is plain silly to ignore what we know, (ii) It is natural and useful to cast what we know in the language of probabilities, and (iii) If our subjective probabilities are erroneous, their impact will get washed out in due time, as the number of observations increases.


Thirty years later, I [Pearl] am still a devout Bayesian in the sense of (i), but I now doubt the wisdom of (ii) and I know that, in general, (iii) is false.
  
He elaborates:
  
The bulk of human knowledge is organized around causal, not probabilistic relationships, and the grammar of probability calculus is insufficient for capturing those relationships. Specifically, the building blocks of our scientific and everyday knowledge are elementary facts such as “mud does not cause rain” and “symptom</p><p>5 0.96283424 <a title="662-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-19-Sharon_Begley%3A__Worse_than_Stephen_Jay_Gould%3F.html">1128 andrew gelman stats-2012-01-19-Sharon Begley:  Worse than Stephen Jay Gould?</a></p>
<p>Introduction: Commenter Tggp  links  to a criticism of science journalist Sharon Begley by science journalist Matthew Hutson.  I learned of this dispute after reporting that Begley had received the American Statistical Association’s Excellence in Statistical Reporting Award, a completely undeserved honor, if Hutson is to believed.
 
The two journalists have somewhat similar profiles: Begley was science editor at Newsweek (she’s now at Reuters) and author of “Train Your Mind, Change Your Brain: How a New Science Reveals Our Extraordinary Potential to Transform Ourselves,” and Hutson was news editor at Psychology Today and wrote the similarly self-helpy-titled, “The 7 Laws of Magical Thinking: How Irrational Beliefs Keep Us Happy, Healthy, and Sane.”
 
Hutson  writes : 
  
  
Psychological Science recently published a fascinating new study on jealousy. I was interested to read Newsweek’s 1300-word article covering the research by their science editor, Sharon Begley. But part-way through the article, I</p><p>6 0.96263218 <a title="662-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-06-Josh_Tenenbaum_presents_._._._a_model_of_folk_physics%21.html">994 andrew gelman stats-2011-11-06-Josh Tenenbaum presents . . . a model of folk physics!</a></p>
<p>7 0.96224797 <a title="662-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-31-Lindley%E2%80%99s_paradox.html">1355 andrew gelman stats-2012-05-31-Lindley’s paradox</a></p>
<p>8 0.95966756 <a title="662-lda-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-06-Bootstrap_averaging%3A_Examples_where_it_works_and_where_it_doesn%E2%80%99t_work.html">2201 andrew gelman stats-2014-02-06-Bootstrap averaging: Examples where it works and where it doesn’t work</a></p>
<p>9 0.95664871 <a title="662-lda-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-Stan_Model_of_the_Week%3A_Hierarchical_Modeling_of_Supernovas.html">2299 andrew gelman stats-2014-04-21-Stan Model of the Week: Hierarchical Modeling of Supernovas</a></p>
<p>10 0.95588905 <a title="662-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-The_statistical_significance_filter.html">899 andrew gelman stats-2011-09-10-The statistical significance filter</a></p>
<p>11 0.95556253 <a title="662-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>12 0.954831 <a title="662-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-04-Interrogating_p-values.html">1883 andrew gelman stats-2013-06-04-Interrogating p-values</a></p>
<p>13 0.95466191 <a title="662-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-What_are_the_trickiest_models_to_fit%3F.html">575 andrew gelman stats-2011-02-15-What are the trickiest models to fit?</a></p>
<p>14 0.95363116 <a title="662-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>15 0.95259273 <a title="662-lda-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-23-Discussion_on_preregistration_of_research_studies.html">2183 andrew gelman stats-2014-01-23-Discussion on preregistration of research studies</a></p>
<p>16 0.95224524 <a title="662-lda-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>17 0.95214438 <a title="662-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-19-Revised_evidence_for_statistical_standards.html">2140 andrew gelman stats-2013-12-19-Revised evidence for statistical standards</a></p>
<p>18 0.95198524 <a title="662-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-13-Drawing_to_Learn_in_Science.html">1056 andrew gelman stats-2011-12-13-Drawing to Learn in Science</a></p>
<p>19 0.95148867 <a title="662-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>20 0.95071965 <a title="662-lda-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-10-95%25_intervals_that_I_don%E2%80%99t_believe%2C_because_they%E2%80%99re_from_a_flat_prior_I_don%E2%80%99t_believe.html">1206 andrew gelman stats-2012-03-10-95% intervals that I don’t believe, because they’re from a flat prior I don’t believe</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
