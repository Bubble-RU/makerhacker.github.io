<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>964 andrew gelman stats-2011-10-19-An interweaving-transformation strategy for boosting MCMC efficiency</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-964" href="#">andrew_gelman_stats-2011-964</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>964 andrew gelman stats-2011-10-19-An interweaving-transformation strategy for boosting MCMC efficiency</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-964-html" href="http://andrewgelman.com/2011/10/19/an-interweaving-transformation-strategy-for-boosting-mcmc-efficiency/">html</a></p><p>Introduction: Yaming Yu and Xiao-Li Meng  write in  with a cool new idea for improving the efficiency of Gibbs and Metropolis in multilevel models:
  
For a broad class of multilevel models, there exist two well-known competing parameterizations, the centered parameterization (CP) and the non-centered parameterization (NCP), for effective MCMC implementation. Much literature has been devoted to the questions of when to use which and how to compromise between them via partial CP/NCP. This article introduces an alternative strategy for boosting MCMC efficiency via simply interweaving—but not alternating—the two parameterizations. This strategy has the surprising property that failure of both the CP and NCP chains to converge geometrically does not prevent the interweaving algorithm from doing so. It achieves this seemingly magical property by taking advantage of the discordance of the two parameterizations, namely, the sufficiency of CP and the ancillarity of NCP, to substantially reduce the Markovian</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Much literature has been devoted to the questions of when to use which and how to compromise between them via partial CP/NCP. [sent-2, score-0.173]
</p><p>2 This article introduces an alternative strategy for boosting MCMC efficiency via simply interweaving—but not alternating—the two parameterizations. [sent-3, score-0.489]
</p><p>3 This strategy has the surprising property that failure of both the CP and NCP chains to converge geometrically does not prevent the interweaving algorithm from doing so. [sent-4, score-0.604]
</p><p>4 It achieves this seemingly magical property by taking advantage of the discordance of the two parameterizations, namely, the sufficiency of CP and the ancillarity of NCP, to substantially reduce the Markovian dependence, especially when the original CP and NCP form a “beauty and beast” pair (i. [sent-5, score-0.725]
</p><p>5 , when one chain mixes far more rapidly than the other). [sent-7, score-0.059]
</p><p>6 The ancillarity–sufficiency reformulation of the CP–NCP dichotomy allows us to borrow insight from the well-known Basu’s theorem on the independence of (complete) sufficient and ancillary statistics, albeit a Bayesian version of Basu’s theorem is currently lacking. [sent-8, score-0.578]
</p><p>7 A bevy of open questions are presented, from the mysterious but exceedingly suggestive connections between ASIS and fiducial/structural inferences to nested ASIS for further boosting MCMC efficiency. [sent-10, score-0.469]
</p><p>8 I’m reminded of  the folk theorem and the Pinocchio principle . [sent-12, score-0.126]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ncp', 0.406), ('cp', 0.349), ('asis', 0.325), ('ancillarity', 0.244), ('interweaving', 0.244), ('sufficiency', 0.244), ('basu', 0.148), ('strategy', 0.146), ('boosting', 0.133), ('mcmc', 0.133), ('yu', 0.129), ('theorem', 0.126), ('parameterizations', 0.125), ('parameterization', 0.109), ('meng', 0.104), ('property', 0.103), ('efficiency', 0.089), ('bevy', 0.074), ('suggestive', 0.074), ('jcgs', 0.074), ('alternating', 0.074), ('competitiveness', 0.074), ('photon', 0.074), ('pinocchio', 0.074), ('reformulation', 0.074), ('ancillary', 0.07), ('achieves', 0.07), ('telescope', 0.07), ('beast', 0.07), ('exceedingly', 0.07), ('dichotomy', 0.067), ('via', 0.065), ('magical', 0.064), ('xl', 0.064), ('multilevel', 0.062), ('mysterious', 0.059), ('nested', 0.059), ('mixes', 0.059), ('borrow', 0.059), ('probit', 0.058), ('lifetime', 0.057), ('albeit', 0.056), ('converge', 0.056), ('introduces', 0.056), ('intensity', 0.055), ('prevent', 0.055), ('compromise', 0.055), ('detecting', 0.054), ('cox', 0.054), ('devoted', 0.053)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="964-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-19-An_interweaving-transformation_strategy_for_boosting_MCMC_efficiency.html">964 andrew gelman stats-2011-10-19-An interweaving-transformation strategy for boosting MCMC efficiency</a></p>
<p>Introduction: Yaming Yu and Xiao-Li Meng  write in  with a cool new idea for improving the efficiency of Gibbs and Metropolis in multilevel models:
  
For a broad class of multilevel models, there exist two well-known competing parameterizations, the centered parameterization (CP) and the non-centered parameterization (NCP), for effective MCMC implementation. Much literature has been devoted to the questions of when to use which and how to compromise between them via partial CP/NCP. This article introduces an alternative strategy for boosting MCMC efficiency via simply interweaving—but not alternating—the two parameterizations. This strategy has the surprising property that failure of both the CP and NCP chains to converge geometrically does not prevent the interweaving algorithm from doing so. It achieves this seemingly magical property by taking advantage of the discordance of the two parameterizations, namely, the sufficiency of CP and the ancillarity of NCP, to substantially reduce the Markovian</p><p>2 0.079401828 <a title="964-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-04-The_Folk_Theorem_of_Statistical_Computing.html">1841 andrew gelman stats-2013-05-04-The Folk Theorem of Statistical Computing</a></p>
<p>Introduction: From an email I received the other day:
  
Things are going much better now — it’s interesting, it feels like with both of my models, parameters are slow to converge or get “stuck” and have trouble mixing when the model is somehow misspecified.
  
See  here  for a statement of the folk theorem.</p><p>3 0.075183101 <a title="964-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-24-F-f-f-fake_data.html">1735 andrew gelman stats-2013-02-24-F-f-f-fake data</a></p>
<p>Introduction: Tiago Fragoso writes:
  
Suppose I fit a two stage regression model


Y = a + bx + e 
a = cw + d + e1


I could fit it all in one step by using MCMC for example (my model is more complicated than that, so I’ll have to do it by MCMC). However, I could fit the first regression only using MCMC because those estimates are hard to obtain and perform the second regression using least squares or a separate MCMC. 


So there’s an ‘one step’ inference based on doing it all at the same time and a ‘two step’ inference by fitting one and using the estimates on the further steps. What is gained or lost between both? Is anything done in this question?
  
My response:
 
Rather than answering your particular question, I’ll give you my generic answer, which is to simulate fake data from your model, then fit your model both ways and see how the results differ.  Repeat the simulation a few thousand times and you can make all the statistical comparisons you like.</p><p>4 0.07258974 <a title="964-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-05-Deadwood_in_the_math_curriculum.html">992 andrew gelman stats-2011-11-05-Deadwood in the math curriculum</a></p>
<p>Introduction: Mark Palko  asks :  What are the worst examples of curriculum dead wood?
 
Here’s the background:
  
One of the first things that hit me [Palko] when I started teaching high school math was how much material there was to cover. . . . The most annoying part, though, was the number of topics that could easily have been cut, thus giving the students the time to master the important skills and concepts.


The example that really stuck with me was synthetic division, a more concise but less intuitive way of performing polynomial long division. Both of these topics are pretty much useless in daily life but polynomial long division does, at least, give the student some insight into the relationship between polynomials and familiar base-ten numbers. Synthetic division has no such value; it’s just a faster but less interesting way of doing something you’ll never have to do.


I started asking hardcore math people — mathematicians, statisticians, physicists, rocket scientists — if they.’d ever u</p><p>5 0.067172565 <a title="964-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-04-Bayesian_Learning_via_Stochastic_Gradient_Langevin_Dynamics.html">1443 andrew gelman stats-2012-08-04-Bayesian Learning via Stochastic Gradient Langevin Dynamics</a></p>
<p>Introduction: Burak Bayramli writes:
  
In  this paper  by Sunjin Ahn, Anoop Korattikara, and Max Welling and  this paper  by Welling and Yee Whye The, there are some arguments on big data and the use of MCMC. Both papers have suggested improvements to speed up MCMC computations. I was wondering what your thoughts were, especially on this paragraph:

 
When a dataset has a billion data-cases (as is not uncommon these days) MCMC algorithms will not even have generated a single (burn-in) sample when a clever learning algorithm based on stochastic gradients may already be making fairly good predictions. In fact, the intriguing results of Bottou and Bousquet (2008) seem to indicate that in terms of “number of bits learned per unit of computation”, an algorithm as simple as stochastic gradient descent is almost optimally efficient. We therefore argue that for Bayesian methods to remain useful in an age when the datasets grow at an exponential rate, they need to embrace the ideas of the stochastic optimiz</p><p>6 0.064719662 <a title="964-tfidf-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-18-Derivative-based_MCMC_as_a_breakthrough_technique_for_implementing_Bayesian_statistics.html">419 andrew gelman stats-2010-11-18-Derivative-based MCMC as a breakthrough technique for implementing Bayesian statistics</a></p>
<p>7 0.064709768 <a title="964-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-05-Xiao-Li_Meng_and_Xianchao_Xie_rethink_asymptotics.html">1406 andrew gelman stats-2012-07-05-Xiao-Li Meng and Xianchao Xie rethink asymptotics</a></p>
<p>8 0.060379546 <a title="964-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>9 0.057709582 <a title="964-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>10 0.055934846 <a title="964-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-19-Will_Stan_work_well_with_40%C3%9740_matrices%3F.html">861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</a></p>
<p>11 0.055254746 <a title="964-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-Another_reason_why_you_can_get_good_inferences_from_a_bad_model.html">1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</a></p>
<p>12 0.0546223 <a title="964-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<p>13 0.053178407 <a title="964-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-28-Understanding_simulations_in_terms_of_predictive_inference%3F.html">1287 andrew gelman stats-2012-04-28-Understanding simulations in terms of predictive inference?</a></p>
<p>14 0.049444251 <a title="964-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-05-An_unexpected_benefit_of_Arrow%E2%80%99s_other_theorem.html">746 andrew gelman stats-2011-06-05-An unexpected benefit of Arrow’s other theorem</a></p>
<p>15 0.048981354 <a title="964-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<p>16 0.047693849 <a title="964-tfidf-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-04-26-Bayes_in_the_news%E2%80%A6in_a_somewhat_frustrating_way.html">3 andrew gelman stats-2010-04-26-Bayes in the news…in a somewhat frustrating way</a></p>
<p>17 0.047027402 <a title="964-tfidf-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-Running_into_a_Stan_Reference_by_Accident.html">2231 andrew gelman stats-2014-03-03-Running into a Stan Reference by Accident</a></p>
<p>18 0.046880096 <a title="964-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-09-A_tale_of_two_discussion_papers.html">1848 andrew gelman stats-2013-05-09-A tale of two discussion papers</a></p>
<p>19 0.046822846 <a title="964-tfidf-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-25-Classics_of_statistics.html">109 andrew gelman stats-2010-06-25-Classics of statistics</a></p>
<p>20 0.045510016 <a title="964-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-03-Advice_that%E2%80%99s_so_eminently_sensible_but_so_difficult_to_follow.html">1520 andrew gelman stats-2012-10-03-Advice that’s so eminently sensible but so difficult to follow</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.076), (1, 0.052), (2, -0.013), (3, 0.015), (4, 0.013), (5, 0.028), (6, -0.017), (7, -0.025), (8, 0.015), (9, 0.017), (10, 0.013), (11, 0.008), (12, -0.028), (13, 0.003), (14, -0.007), (15, -0.014), (16, 0.015), (17, -0.003), (18, -0.013), (19, -0.002), (20, 0.005), (21, -0.007), (22, 0.003), (23, -0.013), (24, -0.002), (25, -0.006), (26, -0.012), (27, 0.013), (28, 0.017), (29, -0.005), (30, -0.022), (31, 0.007), (32, 0.011), (33, -0.001), (34, 0.005), (35, -0.008), (36, -0.02), (37, 0.002), (38, 0.008), (39, 0.002), (40, -0.006), (41, 0.019), (42, -0.007), (43, -0.016), (44, 0.02), (45, -0.024), (46, -0.013), (47, -0.001), (48, -0.003), (49, -0.017)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95867658 <a title="964-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-19-An_interweaving-transformation_strategy_for_boosting_MCMC_efficiency.html">964 andrew gelman stats-2011-10-19-An interweaving-transformation strategy for boosting MCMC efficiency</a></p>
<p>Introduction: Yaming Yu and Xiao-Li Meng  write in  with a cool new idea for improving the efficiency of Gibbs and Metropolis in multilevel models:
  
For a broad class of multilevel models, there exist two well-known competing parameterizations, the centered parameterization (CP) and the non-centered parameterization (NCP), for effective MCMC implementation. Much literature has been devoted to the questions of when to use which and how to compromise between them via partial CP/NCP. This article introduces an alternative strategy for boosting MCMC efficiency via simply interweaving—but not alternating—the two parameterizations. This strategy has the surprising property that failure of both the CP and NCP chains to converge geometrically does not prevent the interweaving algorithm from doing so. It achieves this seemingly magical property by taking advantage of the discordance of the two parameterizations, namely, the sufficiency of CP and the ancillarity of NCP, to substantially reduce the Markovian</p><p>2 0.85021114 <a title="964-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>Introduction: David Duvenaud writes:
  
I’ve been following  your recent discussions  about how an AI could do statistics [see also  here ].  I was especially excited about your suggestion for new statistical methods using “a language-like approach to recursively creating new models from a specified list of distributions and transformations, and an automatic approach to checking model fit.”


Your discussion of these ideas was exciting to me and my colleagues because we recently  did some work  taking a step in this direction, automatically searching through a grammar over Gaussian process regression models.


Roger Grosse previously  did the same thing , but over matrix decomposition models using held-out predictive likelihood to check model fit.


These are both examples of automatic Bayesian model-building by a search over more and more complex models, as you suggested.  One nice thing is that both grammars include lots of standard models for free, and they seem to work pretty well, although the</p><p>3 0.80584359 <a title="964-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-05-Xiao-Li_Meng_and_Xianchao_Xie_rethink_asymptotics.html">1406 andrew gelman stats-2012-07-05-Xiao-Li Meng and Xianchao Xie rethink asymptotics</a></p>
<p>Introduction: In  an article  catchily entitled, “I got more data, my model is more refined, but my estimator is getting worse!  Am I just dumb?”, Meng and Xie write:
  
Possibly, but more likely you are merely a victim of conventional wisdom. More data or better models by no means guarantee better estimators (e.g., with a smaller mean squared error), when you are not following probabilistically principled methods such as MLE (for large samples) or Bayesian approaches. Estimating equations are par- ticularly vulnerable in this regard, almost a necessary price for their robustness. These points will be demonstrated via common tasks of estimating regression parameters and correlations, under simple mod- els such as bivariate normal and ARCH(1). Some general strategies for detecting and avoiding such pitfalls are suggested, including checking for self-efficiency (Meng, 1994, Statistical Science) and adopting a guiding working model.


Using the example of estimating the autocorrelation ρ under a statio</p><p>4 0.79884309 <a title="964-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-14-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">1856 andrew gelman stats-2013-05-14-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>Introduction: I think it’s part of my duty as a blogger to intersperse, along with the steady flow of jokes, rants, and literary criticism, some material that will actually be useful to you.
 
So here goes.
 
Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, and Aki Vehtari  write :
  
The  GPstuff  toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods.
  
We can actually now fit Gaussian processes in  Stan .  But for big problems (or even moderately-sized problems), full Bayes can be slow.  GPstuff uses EP, which is faster.  At some point we’d like to implement EP in Stan.  (Right now we’re working with Dave Blei to implement VB.)
 
GPstuff really works.  I saw Aki use it to fit a nonparametric version of the Bangladesh well-switching example in ARM.  He was sitting in his office and just whip</p><p>5 0.78697115 <a title="964-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>Introduction: Larry Wasserman  refers  to finite mixture models as “beasts” and  writes  jokes that they “should be avoided at all costs.”
 
I’ve thought a lot about mixture models, ever since using them in an  analysis  of voting patterns that was published in 1990.  First off, I’d like to say that our model was useful so I’d prefer not to pay the cost of avoiding it.  For a quick description of our mixture model and its context, see pp. 379-380 of my  article  in the Jim Berger volume).  Actually, our case was particularly difficult because we were not even fitting a mixture model to data, we were fitting it to latent data and using the model to perform partial pooling.  My difficulties in trying to fit this model inspired our discussion of mixture models in Bayesian Data Analysis (page 109 in the second edition, in the section on “Counterexamples to the theorems”).
 
I agree with Larry that if you’re fitting a mixture model, it’s good to be aware of the problems that arise if you try to estimate</p><p>6 0.78424573 <a title="964-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-What_are_the_trickiest_models_to_fit%3F.html">575 andrew gelman stats-2011-02-15-What are the trickiest models to fit?</a></p>
<p>7 0.78085411 <a title="964-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<p>8 0.77940339 <a title="964-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-19-R_package_for_Bayes_factors.html">1682 andrew gelman stats-2013-01-19-R package for Bayes factors</a></p>
<p>9 0.77423364 <a title="964-lsi-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-16-Mandelbrot_and_Akaike%3A__from_taxonomy_to_smooth_runways_%28pioneering_work_in_fractals_and_self-similarity%29.html">346 andrew gelman stats-2010-10-16-Mandelbrot and Akaike:  from taxonomy to smooth runways (pioneering work in fractals and self-similarity)</a></p>
<p>10 0.76816732 <a title="964-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-24-Multilevel_modeling_and_instrumental_variables.html">1468 andrew gelman stats-2012-08-24-Multilevel modeling and instrumental variables</a></p>
<p>11 0.75159049 <a title="964-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-19-Will_Stan_work_well_with_40%C3%9740_matrices%3F.html">861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</a></p>
<p>12 0.74754202 <a title="964-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-04-David_MacKay_and_Occam%E2%80%99s_Razor.html">1041 andrew gelman stats-2011-12-04-David MacKay and Occam’s Razor</a></p>
<p>13 0.74621189 <a title="964-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-21-BDA3_table_of_contents_%28also_a_new_paper_on_visualization%29.html">1991 andrew gelman stats-2013-08-21-BDA3 table of contents (also a new paper on visualization)</a></p>
<p>14 0.74546373 <a title="964-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-24-New_ideas_on_DIC_from_Martyn_Plummer_and_Sumio_Watanabe.html">778 andrew gelman stats-2011-06-24-New ideas on DIC from Martyn Plummer and Sumio Watanabe</a></p>
<p>15 0.74510574 <a title="964-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>16 0.74316502 <a title="964-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>17 0.7415033 <a title="964-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-22-Deviance%2C_DIC%2C_AIC%2C_cross-validation%2C_etc.html">776 andrew gelman stats-2011-06-22-Deviance, DIC, AIC, cross-validation, etc</a></p>
<p>18 0.73996001 <a title="964-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-15-More_on_AIC%2C_WAIC%2C_etc.html">1983 andrew gelman stats-2013-08-15-More on AIC, WAIC, etc</a></p>
<p>19 0.73978478 <a title="964-lsi-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-21-Discussion_of_the_paper_by_Girolami_and_Calderhead_on_Bayesian_computation.html">288 andrew gelman stats-2010-09-21-Discussion of the paper by Girolami and Calderhead on Bayesian computation</a></p>
<p>20 0.73595297 <a title="964-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-04-Model_checking_and_model_understanding_in_machine_learning.html">1482 andrew gelman stats-2012-09-04-Model checking and model understanding in machine learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(5, 0.02), (9, 0.025), (13, 0.015), (15, 0.011), (16, 0.046), (24, 0.122), (30, 0.014), (36, 0.048), (39, 0.012), (43, 0.013), (45, 0.017), (53, 0.013), (63, 0.014), (76, 0.019), (80, 0.31), (86, 0.037), (97, 0.012), (99, 0.15)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.87506157 <a title="964-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-19-An_interweaving-transformation_strategy_for_boosting_MCMC_efficiency.html">964 andrew gelman stats-2011-10-19-An interweaving-transformation strategy for boosting MCMC efficiency</a></p>
<p>Introduction: Yaming Yu and Xiao-Li Meng  write in  with a cool new idea for improving the efficiency of Gibbs and Metropolis in multilevel models:
  
For a broad class of multilevel models, there exist two well-known competing parameterizations, the centered parameterization (CP) and the non-centered parameterization (NCP), for effective MCMC implementation. Much literature has been devoted to the questions of when to use which and how to compromise between them via partial CP/NCP. This article introduces an alternative strategy for boosting MCMC efficiency via simply interweaving—but not alternating—the two parameterizations. This strategy has the surprising property that failure of both the CP and NCP chains to converge geometrically does not prevent the interweaving algorithm from doing so. It achieves this seemingly magical property by taking advantage of the discordance of the two parameterizations, namely, the sufficiency of CP and the ancillarity of NCP, to substantially reduce the Markovian</p><p>2 0.80000597 <a title="964-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-26-%E2%80%9CTo_Rethink_Sprawl%2C_Start_With_Offices%E2%80%9D.html">1029 andrew gelman stats-2011-11-26-“To Rethink Sprawl, Start With Offices”</a></p>
<p>Introduction: According to  this  op-ed by Louise Mozingo, the fashion for suburban corporate parks is seventy years old:
  
In 1942 the AT&T; Bell Telephone Laboratories moved from its offices in Lower Manhattan to a new, custom-designed facility on 213 acres outside Summit, N.J.


The location provided space for laboratories and quiet for acoustical research, and new features: parking lots that allowed scientists and engineers to drive from their nearby suburban homes, a spacious cafeteria and lounge and, most surprisingly, views from every window of a carefully tended pastoral landscape designed by the Olmsted brothers, sons of the designer of Central Park.


Corporate management never saw the city center in the same way again. Bell Labs initiated a tide of migration of white-collar workers, especially as state and federal governments conveniently extended highways into the rural edge.
  
Just to throw some Richard Florida in the mix:  Back in 1990, I  turned down  a job offer from Bell Labs, larg</p><p>3 0.71310818 <a title="964-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-16-%E2%80%9CFor_individuals_with_wine_training%2C_however%2C_we_find__indications_of_a_positive_relationship_between_price_and_enjoyment%E2%80%9D.html">470 andrew gelman stats-2010-12-16-“For individuals with wine training, however, we find  indications of a positive relationship between price and enjoyment”</a></p>
<p>Introduction: The title of this blog post quotes the second line of the abstract of Goldstein et al.’s much ballyhooed 2008 tech report,  Do More Expensive Wines Taste Better?  Evidence from a Large Sample of Blind Tastings .  
 
The first sentence of the abstract is
  

Individuals who are unaware of the price do not derive more enjoyment from more expensive wine.

  
Perhaps not surprisingly, given the easy target wine snobs make, the popular press has picked up on the first sentence of the tech report.  For example, the  Freakonomics blog/radio entry  of the same name quotes the first line, ignores the qualification, then concludes
  

Wishing you the happiest of holiday seasons, and urging you to spend $15 instead of $50 on your next bottle of wine. Go ahead, take the money you save and blow it on the lottery.

  
In case you’re wondering about whether to buy me a cheap or expensive bottle of wine, keep in mind I’ve had classical “wine training”.  After ten minutes of training with some side by</p><p>4 0.70405424 <a title="964-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-25-Rechecking_the_census.html">730 andrew gelman stats-2011-05-25-Rechecking the census</a></p>
<p>Introduction: Sam Roberts  writes :
  
The Census Bureau [reported] that though New York City’s population reached a record high of 8,175,133 in 2010, the gain of 2 percent, or 166,855 people, since 2000 fell about 200,000 short of what the bureau itself had estimated.


Public officials were incredulous that a city that lures tens of thousands of immigrants each year and where a forest of new buildings has sprouted could really have recorded such a puny increase.


How, they wondered, could Queens have grown by only one-tenth of 1 percent since 2000? How, even with a surge in foreclosures, could the number of vacant apartments have soared by nearly 60 percent in Queens and by 66 percent in Brooklyn?
  
That does seem a bit suspicious.  So the newspaper did its own survey:
  
 
Now, a house-to-house New York Times survey of three representative square blocks where the Census Bureau said vacancies had increased and the population had declined since 2000 suggests that the city’s outrage is somewhat ju</p><p>5 0.68960655 <a title="964-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-25-Note_to_student_journalists%3A__Google_is_your_friend.html">1027 andrew gelman stats-2011-11-25-Note to student journalists:  Google is your friend</a></p>
<p>Introduction: A student journalist called me with some questions about when the U.S. would have a female president. At one point she asked if there were any surveys of whether people would vote for a woman. I suggested she try Google. I was by my computer anyway so typed “what percentage of americans would vote for a woman president” (without the quotation marks), and the very first hit was  this  from Gallup, from 2007:
  
The Feb. 9-11, 2007, poll asked Americans whether they would vote for “a generally well-qualified” presidential candidate nominated by their party with each of the following characteristics: Jewish, Catholic, Mormon, an atheist, a woman, black, Hispanic, homosexual, 72 years of age, and someone married for the third time.


 Between now and the 2008 political conventions, there will be discussion about the qualifications of presidential candidates — their education, age, religion, race, and so on. If your party nominated a generally well-qualified person for president who happene</p><p>6 0.67700934 <a title="964-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-13-Watching_the_sharks_jump.html">1494 andrew gelman stats-2012-09-13-Watching the sharks jump</a></p>
<p>7 0.66962457 <a title="964-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-10-Creating_a_good_wager_based_on_probability_estimates.html">138 andrew gelman stats-2010-07-10-Creating a good wager based on probability estimates</a></p>
<p>8 0.65848911 <a title="964-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-16-Suspicious_histogram_bars.html">1063 andrew gelman stats-2011-12-16-Suspicious histogram bars</a></p>
<p>9 0.64434093 <a title="964-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-03-More_research_on_the_role_of_puzzles_in_processing_data_graphics.html">1747 andrew gelman stats-2013-03-03-More research on the role of puzzles in processing data graphics</a></p>
<p>10 0.63288814 <a title="964-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-02-Bill_James_and_the_base-rate_fallacy.html">642 andrew gelman stats-2011-04-02-Bill James and the base-rate fallacy</a></p>
<p>11 0.61711395 <a title="964-lda-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-10-Cost_of_communicating_numbers.html">137 andrew gelman stats-2010-07-10-Cost of communicating numbers</a></p>
<p>12 0.59885693 <a title="964-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-01-Separated_by_a_common_blah_blah_blah.html">2119 andrew gelman stats-2013-12-01-Separated by a common blah blah blah</a></p>
<p>13 0.58554995 <a title="964-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-16-Learning_about_correlations_using_cross-sectional_and_over-time_comparisons_between_and_within_countries.html">1985 andrew gelman stats-2013-08-16-Learning about correlations using cross-sectional and over-time comparisons between and within countries</a></p>
<p>14 0.58110619 <a title="964-lda-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-23-Visualization_magazine.html">227 andrew gelman stats-2010-08-23-Visualization magazine</a></p>
<p>15 0.5806244 <a title="964-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-31-Two_stories_about_the_election_that_I_don%E2%80%99t_believe.html">384 andrew gelman stats-2010-10-31-Two stories about the election that I don’t believe</a></p>
<p>16 0.57636118 <a title="964-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-26-Some_thoughts_on_survey_weighting.html">1430 andrew gelman stats-2012-07-26-Some thoughts on survey weighting</a></p>
<p>17 0.57428324 <a title="964-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-10-Aleks_says_this_is_the_future_of_visualization.html">795 andrew gelman stats-2011-07-10-Aleks says this is the future of visualization</a></p>
<p>18 0.57361174 <a title="964-lda-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-10-SeeThroughNY.html">140 andrew gelman stats-2010-07-10-SeeThroughNY</a></p>
<p>19 0.57187617 <a title="964-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-01-Ice_cream%21_and_temperature.html">1402 andrew gelman stats-2012-07-01-Ice cream! and temperature</a></p>
<p>20 0.55364466 <a title="964-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-24-Chasing_the_noise%3A__W._Edwards_Deming_would_be_spinning_in_his_grave.html">2076 andrew gelman stats-2013-10-24-Chasing the noise:  W. Edwards Deming would be spinning in his grave</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
