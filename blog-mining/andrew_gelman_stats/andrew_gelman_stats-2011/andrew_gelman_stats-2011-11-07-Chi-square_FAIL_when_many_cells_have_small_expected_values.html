<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>996 andrew gelman stats-2011-11-07-Chi-square FAIL when many cells have small expected values</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-996" href="#">andrew_gelman_stats-2011-996</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>996 andrew gelman stats-2011-11-07-Chi-square FAIL when many cells have small expected values</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-996-html" href="http://andrewgelman.com/2011/11/07/chi-square-fail-when-many-cells-have-small-expected-values/">html</a></p><p>Introduction: William Perkins, Mark Tygert, and Rachel Ward  write :
  
If a discrete probability distribution in a model being tested for goodness-of-fit is not close to uniform, then forming the Pearson χ2 statistic can involve division by nearly zero. This often leads to serious trouble in practice — even in the absence of round-off errors . . .
  
The problem is not merely that the chi-squared  statistic  doesn’t have the advertised chi-squared  distribution —a reference distribution can always be computed via simulation, either using the posterior predictive distribution or by conditioning on a point estimate of the cell expectations and then making a degrees-of-freedom sort of adjustment.
 
Rather, the problem is that, when there are lots of cells with near-zero expectation, the chi-squared test is mostly noise.
 
And this is not merely a theoretical problem.  It comes up in real examples.
 
Here’s one, taken from the classic 1992 genetics paper of Guo and Thomspson:
 
   
 
And here are the e</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 William Perkins, Mark Tygert, and Rachel Ward  write :    If a discrete probability distribution in a model being tested for goodness-of-fit is not close to uniform, then forming the Pearson χ2 statistic can involve division by nearly zero. [sent-1, score-0.78]
</p><p>2 This often leads to serious trouble in practice — even in the absence of round-off errors . [sent-2, score-0.077]
</p><p>3 Rather, the problem is that, when there are lots of cells with near-zero expectation, the chi-squared test is mostly noise. [sent-6, score-0.518]
</p><p>4 Here’s one, taken from the classic 1992 genetics paper of Guo and Thomspson:         And here are the expected frequencies from the Guo and Thompson model:         The p-value of the chi-squared test is 0. [sent-9, score-0.633]
</p><p>5 But it turns out that that if you do an equally-weighted mean square test (rather than chi-square, which weights each cell proportional to expected counts), you get a p-value of 0. [sent-12, score-0.984]
</p><p>6 (Perkins, Tygert, and Ward compute the p-value via simulation. [sent-14, score-0.101]
</p><p>7 All those zeroes and near-zeroes in the data give you a chi-squared test that is so noisy as to be useless. [sent-17, score-0.344]
</p><p>8 If people really are going around saying their models fit in such situations, it could be causing real problems. [sent-18, score-0.23]
</p><p>9 In the chi-squared statistic, all that noise in the empty cells is diluting the signal. [sent-21, score-0.533]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cells', 0.348), ('statistic', 0.324), ('expected', 0.296), ('tygert', 0.252), ('perkins', 0.216), ('guo', 0.216), ('ward', 0.194), ('test', 0.17), ('distribution', 0.159), ('square', 0.158), ('cell', 0.149), ('merely', 0.116), ('summed', 0.115), ('zeroes', 0.108), ('diluting', 0.108), ('advertised', 0.104), ('thompson', 0.104), ('via', 0.101), ('rachel', 0.094), ('forming', 0.09), ('frequencies', 0.09), ('discrepancies', 0.089), ('going', 0.088), ('squared', 0.084), ('pearson', 0.083), ('conditioning', 0.078), ('rejection', 0.078), ('absence', 0.077), ('empty', 0.077), ('genetics', 0.077), ('causing', 0.077), ('largest', 0.075), ('proportional', 0.074), ('expectation', 0.074), ('computed', 0.074), ('william', 0.073), ('weights', 0.072), ('expectations', 0.072), ('tested', 0.071), ('weighting', 0.071), ('counts', 0.071), ('division', 0.07), ('signal', 0.069), ('uniform', 0.068), ('noisy', 0.066), ('involve', 0.066), ('simulation', 0.065), ('mean', 0.065), ('real', 0.065), ('situations', 0.065)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="996-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-07-Chi-square_FAIL_when_many_cells_have_small_expected_values.html">996 andrew gelman stats-2011-11-07-Chi-square FAIL when many cells have small expected values</a></p>
<p>Introduction: William Perkins, Mark Tygert, and Rachel Ward  write :
  
If a discrete probability distribution in a model being tested for goodness-of-fit is not close to uniform, then forming the Pearson χ2 statistic can involve division by nearly zero. This often leads to serious trouble in practice — even in the absence of round-off errors . . .
  
The problem is not merely that the chi-squared  statistic  doesn’t have the advertised chi-squared  distribution —a reference distribution can always be computed via simulation, either using the posterior predictive distribution or by conditioning on a point estimate of the cell expectations and then making a degrees-of-freedom sort of adjustment.
 
Rather, the problem is that, when there are lots of cells with near-zero expectation, the chi-squared test is mostly noise.
 
And this is not merely a theoretical problem.  It comes up in real examples.
 
Here’s one, taken from the classic 1992 genetics paper of Guo and Thomspson:
 
   
 
And here are the e</p><p>2 0.1081041 <a title="996-tfidf-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-28-Another_argument_in_favor_of_expressing_conditional_probability_statements_using_the_population_distribution.html">56 andrew gelman stats-2010-05-28-Another argument in favor of expressing conditional probability statements using the population distribution</a></p>
<p>Introduction: Yesterday we had a spirited  discussion  of the following conditional probability puzzle:
  
“I have two children. One is a boy born on a Tuesday. What is the probability I have two boys?”
  
This reminded me of the principle, familiar from  statistics instruction  and the  cognitive psychology  literature, that the best way to teach these sorts of examples is through integers rather than fractions.
 
For example, consider this classic problem:
  
“10% of persons have disease X.  You are tested for the disease and test positive, and the test has 80% accuracy.  What is the probability that you have the disease?”
  
This can be solved directly using conditional probability but it appears to be clearer to do it using integers:
  
Start with 100 people.  10 will have the disease and 90 will not.  Of the 10 with the disease, 8 will test positive and 2 will test negative.  Of the 90 without the disease, 18 will test positive and 72% will test negative.  (72% = 0.8*90.)  So, out of the origin</p><p>3 0.084793083 <a title="996-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>Introduction: Klaas Metselaar writes: 
  
  
I [Metselaar] am currently involved in a discussion about the use of the notion “predictive” as used in “posterior predictive check”. I would argue that the notion “predictive” should be reserved for posterior checks using information not used in the determination of the posterior. 
I quote from the discussion: 
“However, the predictive uncertainty in a Bayesian calculation requires sampling from all the random variables, and this includes both the model parameters and the residual error”.


My [Metselaar's] comment:


This may be exactly the point I am worried about: shouldn’t the predictive uncertainty be defined as sampling from the posterior parameter distribution +  residual error + sampling from the prediction error distribution? 
Residual error reduces to measurement error in the case of a  model which is perfect for the sample of experiments. Measurement error could be reduced to almost zero by ideal and perfect measurement instruments. 
I would h</p><p>4 0.082501985 <a title="996-tfidf-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-28-My_talk_at_American_University.html">376 andrew gelman stats-2010-10-28-My talk at American University</a></p>
<p>Introduction: Red State Blue State:  How Will the U.S. Vote? 
 
It’s the “annual Halloween and pre-election extravaganza” of the Department of Mathematics and Statistics, and they suggested I could talk on the zombies paper (of course), but I thought the material on voting might be of more general interest.
 
The “How will the U.S. vote?” subtitle was not of my choosing, but I suppose I can add a few slides about the forthcoming election.
 
Fri 29 Oct 2010, 7pm in Ward I, in the basement of the Ward Circle building.
 
Should be fun.  I haven’t been to AU since taking a class there, over 30 years ago.
 
P.S.  It was indeed fun.   Here’s  the talk.  I did end up briefly describing my zombie research but it didn’t make it into any of the slides.</p><p>5 0.081748255 <a title="996-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-18-Understanding_posterior_p-values.html">2029 andrew gelman stats-2013-09-18-Understanding posterior p-values</a></p>
<p>Introduction: David Kaplan writes:
  
I came across your  paper  “Understanding Posterior Predictive P-values”, and I have a question regarding your statement “If a posterior predictive p-value is 0.4, say, that means that, if we believe the model, we think there is a 40% chance that tomorrow’s value of T(y_rep) will exceed today’s T(y).” This is perfectly understandable to me and represents the idea of calibration.  However, I am unsure how this relates to statements about fit.  If T is the LR chi-square or Pearson chi-square, then your statement that there is a 40% chance that tomorrows value exceeds today’s value indicates bad fit, I think.  Yet, some literature indicates that high p-values suggest good fit.  Could you clarify this?
  
My reply:
 
I think that “fit” depends on the question being asked.  In this case, I’d say the model fits for this particular purpose, even though it might not fit for other purposes.
 
And here’s the abstract of the paper:
  
Posterior predictive p-values do not i</p><p>6 0.08072561 <a title="996-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-17-The_disappearing_or_non-disappearing_middle_class.html">1767 andrew gelman stats-2013-03-17-The disappearing or non-disappearing middle class</a></p>
<p>7 0.080028705 <a title="996-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>8 0.079294473 <a title="996-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-Another_reason_why_you_can_get_good_inferences_from_a_bad_model.html">1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</a></p>
<p>9 0.078366689 <a title="996-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-09-How_to_model_distributions_that_have_outliers_in_one_direction.html">2128 andrew gelman stats-2013-12-09-How to model distributions that have outliers in one direction</a></p>
<p>10 0.078347065 <a title="996-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>11 0.077515773 <a title="996-tfidf-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-18-%E2%80%9CI_was_finding_the_test_so_irritating_and_boring_that_I_just_started_to_click_through_as_fast_as_I_could%E2%80%9D.html">351 andrew gelman stats-2010-10-18-“I was finding the test so irritating and boring that I just started to click through as fast as I could”</a></p>
<p>12 0.077178508 <a title="996-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-07-Robust_logistic_regression.html">1886 andrew gelman stats-2013-06-07-Robust logistic regression</a></p>
<p>13 0.076671995 <a title="996-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-13-Hypothesis_testing_with_multiple_imputations.html">799 andrew gelman stats-2011-07-13-Hypothesis testing with multiple imputations</a></p>
<p>14 0.075310007 <a title="996-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-04-Write_This_Book.html">1605 andrew gelman stats-2012-12-04-Write This Book</a></p>
<p>15 0.075218603 <a title="996-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>16 0.07450445 <a title="996-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<p>17 0.073928133 <a title="996-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>18 0.073621094 <a title="996-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-27-Visual_diagnostics_for_discrete-data_regressions.html">929 andrew gelman stats-2011-09-27-Visual diagnostics for discrete-data regressions</a></p>
<p>19 0.073531888 <a title="996-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>20 0.072959594 <a title="996-tfidf-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-21-Random_matrices_in_the_news.html">2258 andrew gelman stats-2014-03-21-Random matrices in the news</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.134), (1, 0.072), (2, 0.029), (3, 0.0), (4, 0.02), (5, -0.032), (6, 0.035), (7, 0.011), (8, 0.016), (9, -0.036), (10, 0.001), (11, 0.017), (12, -0.039), (13, -0.038), (14, -0.052), (15, -0.03), (16, 0.033), (17, -0.007), (18, 0.014), (19, -0.048), (20, 0.039), (21, 0.006), (22, 0.016), (23, -0.036), (24, 0.016), (25, 0.02), (26, -0.012), (27, 0.016), (28, 0.03), (29, 0.023), (30, -0.002), (31, 0.019), (32, -0.015), (33, -0.0), (34, 0.002), (35, 0.008), (36, 0.03), (37, -0.006), (38, -0.012), (39, 0.021), (40, 0.013), (41, -0.028), (42, -0.004), (43, -0.028), (44, -0.014), (45, 0.02), (46, 0.01), (47, 0.016), (48, 0.05), (49, -0.03)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97297788 <a title="996-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-07-Chi-square_FAIL_when_many_cells_have_small_expected_values.html">996 andrew gelman stats-2011-11-07-Chi-square FAIL when many cells have small expected values</a></p>
<p>Introduction: William Perkins, Mark Tygert, and Rachel Ward  write :
  
If a discrete probability distribution in a model being tested for goodness-of-fit is not close to uniform, then forming the Pearson χ2 statistic can involve division by nearly zero. This often leads to serious trouble in practice — even in the absence of round-off errors . . .
  
The problem is not merely that the chi-squared  statistic  doesn’t have the advertised chi-squared  distribution —a reference distribution can always be computed via simulation, either using the posterior predictive distribution or by conditioning on a point estimate of the cell expectations and then making a degrees-of-freedom sort of adjustment.
 
Rather, the problem is that, when there are lots of cells with near-zero expectation, the chi-squared test is mostly noise.
 
And this is not merely a theoretical problem.  It comes up in real examples.
 
Here’s one, taken from the classic 1992 genetics paper of Guo and Thomspson:
 
   
 
And here are the e</p><p>2 0.84287447 <a title="996-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-18-Understanding_posterior_p-values.html">2029 andrew gelman stats-2013-09-18-Understanding posterior p-values</a></p>
<p>Introduction: David Kaplan writes:
  
I came across your  paper  “Understanding Posterior Predictive P-values”, and I have a question regarding your statement “If a posterior predictive p-value is 0.4, say, that means that, if we believe the model, we think there is a 40% chance that tomorrow’s value of T(y_rep) will exceed today’s T(y).” This is perfectly understandable to me and represents the idea of calibration.  However, I am unsure how this relates to statements about fit.  If T is the LR chi-square or Pearson chi-square, then your statement that there is a 40% chance that tomorrows value exceeds today’s value indicates bad fit, I think.  Yet, some literature indicates that high p-values suggest good fit.  Could you clarify this?
  
My reply:
 
I think that “fit” depends on the question being asked.  In this case, I’d say the model fits for this particular purpose, even though it might not fit for other purposes.
 
And here’s the abstract of the paper:
  
Posterior predictive p-values do not i</p><p>3 0.78751791 <a title="996-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-28-Another_argument_in_favor_of_expressing_conditional_probability_statements_using_the_population_distribution.html">56 andrew gelman stats-2010-05-28-Another argument in favor of expressing conditional probability statements using the population distribution</a></p>
<p>Introduction: Yesterday we had a spirited  discussion  of the following conditional probability puzzle:
  
“I have two children. One is a boy born on a Tuesday. What is the probability I have two boys?”
  
This reminded me of the principle, familiar from  statistics instruction  and the  cognitive psychology  literature, that the best way to teach these sorts of examples is through integers rather than fractions.
 
For example, consider this classic problem:
  
“10% of persons have disease X.  You are tested for the disease and test positive, and the test has 80% accuracy.  What is the probability that you have the disease?”
  
This can be solved directly using conditional probability but it appears to be clearer to do it using integers:
  
Start with 100 people.  10 will have the disease and 90 will not.  Of the 10 with the disease, 8 will test positive and 2 will test negative.  Of the 90 without the disease, 18 will test positive and 72% will test negative.  (72% = 0.8*90.)  So, out of the origin</p><p>4 0.77715254 <a title="996-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-Another_reason_why_you_can_get_good_inferences_from_a_bad_model.html">1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</a></p>
<p>Introduction: John Cook  considers  how people justify probability distribution assumptions:
  
Sometimes distribution assumptions are not justified.


Sometimes distributions can be derived from fundamental principles [or] . . . on theoretical grounds. For example, large samples and the central limit theorem together may justify assuming that something is normally distributed.


Often the choice of distribution is somewhat arbitrary, chosen by intuition or for convenience, and then empirically shown to work well enough.


Sometimes a distribution can be a bad fit and still work well, depending on what you’re asking of it.
  
Cook continues:
  
The last point is particularly interesting. It’s not hard to imagine that a poor fit would produce poor results. It’s surprising when a poor fit produces good results.
  
And then he gives an example of an effective but inaccurate model used to model survival times in a clinical trial.  Cook explains:
  
The [poorly-fitting] method works well because of the q</p><p>5 0.77552396 <a title="996-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-09-How_to_model_distributions_that_have_outliers_in_one_direction.html">2128 andrew gelman stats-2013-12-09-How to model distributions that have outliers in one direction</a></p>
<p>Introduction: Shravan writes:
  
I have a problem very similar to the one presented chapter 6 of BDA, the speed of light example. You use the distribution of the minimum scores from the posterior predictive distribution, show that it’s not realistic given the data, and suggest that an asymmetric contaminated normal distribution or a symmetric long-tailed distribution would be better.


How does one use such a distribution?
  
My reply:
 
You can actually use a symmetric long-tailed distribution such as t with low degrees of freedom.  One striking feature of symmetric long-tailed distributions is that a small random sample from such a distribution can have outliers on one side or the other and look asymmetric.
 
Just to see this, try the following in R: 
  
par (mfrow=c(3,3), mar=c(1,1,1,1)) 
for (i in 1:9) hist (rt (100, 2), xlab="", ylab="", main="") 
 
 
You’ll see some skewed distributions.  So that’s the message (which I learned from an offhand comment of Rubin, actually):  if you want to model</p><p>6 0.77403027 <a title="996-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>7 0.76295871 <a title="996-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-15-More_on_AIC%2C_WAIC%2C_etc.html">1983 andrew gelman stats-2013-08-15-More on AIC, WAIC, etc</a></p>
<p>8 0.75156933 <a title="996-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-27-Average_predictive_comparisons_when_changing_a_pair_of_variables.html">1346 andrew gelman stats-2012-05-27-Average predictive comparisons when changing a pair of variables</a></p>
<p>9 0.74657172 <a title="996-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-03-Boot.html">1881 andrew gelman stats-2013-06-03-Boot</a></p>
<p>10 0.74013519 <a title="996-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-30-David_Hogg_on_statistics.html">1401 andrew gelman stats-2012-06-30-David Hogg on statistics</a></p>
<p>11 0.7271474 <a title="996-lsi-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-Transformations_for_non-normal_data.html">2176 andrew gelman stats-2014-01-19-Transformations for non-normal data</a></p>
<p>12 0.72403628 <a title="996-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-26-Modeling_probability_data.html">1284 andrew gelman stats-2012-04-26-Modeling probability data</a></p>
<p>13 0.72310996 <a title="996-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-16-%E2%80%9CReal_data_can_be_a_pain%E2%80%9D.html">1460 andrew gelman stats-2012-08-16-“Real data can be a pain”</a></p>
<p>14 0.72026139 <a title="996-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-24-What_is_the_normal_range_of_values_in_a_medical_test%3F.html">923 andrew gelman stats-2011-09-24-What is the normal range of values in a medical test?</a></p>
<p>15 0.71304077 <a title="996-lsi-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-21-Random_matrices_in_the_news.html">2258 andrew gelman stats-2014-03-21-Random matrices in the news</a></p>
<p>16 0.7121594 <a title="996-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-24-Analyzing_photon_counts.html">1509 andrew gelman stats-2012-09-24-Analyzing photon counts</a></p>
<p>17 0.71097642 <a title="996-lsi-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-21-Models_with_constraints.html">2342 andrew gelman stats-2014-05-21-Models with constraints</a></p>
<p>18 0.70808715 <a title="996-lsi-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-14-Confusion_about_continuous_probability_densities.html">341 andrew gelman stats-2010-10-14-Confusion about continuous probability densities</a></p>
<p>19 0.70765263 <a title="996-lsi-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Bayesian_Uncertainty_Quantification_for_Differential_Equations%21.html">2311 andrew gelman stats-2014-04-29-Bayesian Uncertainty Quantification for Differential Equations!</a></p>
<p>20 0.70619649 <a title="996-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-02-Fighting_a_losing_battle.html">1518 andrew gelman stats-2012-10-02-Fighting a losing battle</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.016), (15, 0.02), (16, 0.035), (21, 0.033), (24, 0.153), (31, 0.024), (44, 0.011), (86, 0.056), (89, 0.023), (96, 0.012), (97, 0.247), (98, 0.022), (99, 0.224)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.94900298 <a title="996-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-11-Incredibly_strange_spam.html">1573 andrew gelman stats-2012-11-11-Incredibly strange spam</a></p>
<p>Introduction: Unsolicited (of course) in the email the other day:
  
Just wanted to touch base with you to see if you needed any quotes on 
Parking lot lighting or Garage Lighting? (Induction, LED, Canopy etc…)


We help retrofit 1000′s of garages around the country.


Let me know your specs and ill send you a quote in 24 hours. 


** 
Owner 
Emergency Lights Co.
  
Ill indeed. . . .</p><p>2 0.92810166 <a title="996-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-31-Meanwhile%2C_on_the_sister_blog_._._..html">882 andrew gelman stats-2011-08-31-Meanwhile, on the sister blog . . .</a></p>
<p>Introduction: NYT columnist Douthat asks: Should we be disturbed that a leading presidential candidate endorses a pro-slavery position? 
 
 Who’s on the web? And where are they? 
 
 Sowell, Carlson, Barone: fools, knaves, or simply victims of a cognitive illusion? 
 
 Don’t blame the American public for the D.C. deadlock 
 
 Calvin College update 
 
 Help reform the Institutional Review Board (IRB) system! 
 
 Powerful credit-rating agencies are a creation of the government . . . what does it mean when they bite the hand that feeds them? 
 
 “Waiting for a landslide” 
 
 A simple theory of why Obama didn’t come out fighting in 2009 
 
 A modest proposal 
 
 Noooooooooooooooo!!!!!!!!!!!!!!! 
 
 The Family Research Council and the Barnard Center for Research on Women 
 
 Sleazy data miners 
 
 Genetic essentialism is in our genes 
 
Wow, that was a lot!  No wonder I don’t get any research done…</p><p>3 0.90152955 <a title="996-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-23-Unhappy_with_improvement_by_a_factor_of_10%5E29.html">160 andrew gelman stats-2010-07-23-Unhappy with improvement by a factor of 10^29</a></p>
<p>Introduction: I have an optimization problem:  I have a complicated physical model that predicts energy and thermal behavior of a building, given the values of a slew of parameters, such as insulation effectiveness, window transmissivity, etc.  I’m trying to find the parameter set that best fits several weeks of thermal and energy use data from the real building that we modeled. (Of course I would rather explore parameter space and come up with probability distributions for the parameters, and maybe that will come later, but for now I’m just optimizing).  To do the optimization, colleagues and I implemented a “particle swarm optimization” algorithm on a massively parallel machine. This involves giving each of about 120 “particles” an initial position in parameter space, then letting them move around, trying to move to better positions according to a specific algorithm.  We gave each particle an initial position sampled from our prior distribution for each parameter.  So far we’ve run about 140 itera</p><p>same-blog 4 0.89854753 <a title="996-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-07-Chi-square_FAIL_when_many_cells_have_small_expected_values.html">996 andrew gelman stats-2011-11-07-Chi-square FAIL when many cells have small expected values</a></p>
<p>Introduction: William Perkins, Mark Tygert, and Rachel Ward  write :
  
If a discrete probability distribution in a model being tested for goodness-of-fit is not close to uniform, then forming the Pearson χ2 statistic can involve division by nearly zero. This often leads to serious trouble in practice — even in the absence of round-off errors . . .
  
The problem is not merely that the chi-squared  statistic  doesn’t have the advertised chi-squared  distribution —a reference distribution can always be computed via simulation, either using the posterior predictive distribution or by conditioning on a point estimate of the cell expectations and then making a degrees-of-freedom sort of adjustment.
 
Rather, the problem is that, when there are lots of cells with near-zero expectation, the chi-squared test is mostly noise.
 
And this is not merely a theoretical problem.  It comes up in real examples.
 
Here’s one, taken from the classic 1992 genetics paper of Guo and Thomspson:
 
   
 
And here are the e</p><p>5 0.86937106 <a title="996-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-12-God%2C_Guns%2C_and_Gaydar%3A__The_Laws_of_Probability_Push_You_to_Overestimate_Small_Groups.html">142 andrew gelman stats-2010-07-12-God, Guns, and Gaydar:  The Laws of Probability Push You to Overestimate Small Groups</a></p>
<p>Introduction: Earlier today, Nate  criticized  a U.S. military survey that asks troops the question, “Do you currently serve with a male or female Service member you  believe  to be homosexual.” [emphasis added]  As Nate points out, by asking this question in such a speculative way, “it would seem that you’ll be picking up a tremendous number of false positives–soldiers who are believed to be gay, but aren’t–and that these false positives will swamp any instances in which soldiers (in spite of DADT) are actually somewhat open about their same-sex attractions.”
 
This is a general problem in survey research. In an  article  in Chance magazine in 1997, “The myth of millions of annual self-defense gun uses:  a case study of survey overestimates of rare events” [see  here  for related references], David Hemenway uses the false-positive, false-negative reasoning to explain this bias in terms of probability theory.  Misclassifications that induce seemingly minor biases in estimates of certain small probab</p><p>6 0.85074872 <a title="996-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-03-is_it_possible_to_%E2%80%9Coverstratify%E2%80%9D_when_assigning_a_treatment_in_a_randomized_control_trial%3F.html">553 andrew gelman stats-2011-02-03-is it possible to “overstratify” when assigning a treatment in a randomized control trial?</a></p>
<p>7 0.84743655 <a title="996-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-10-Three_hours_in_the_life_of_a_statistician.html">1001 andrew gelman stats-2011-11-10-Three hours in the life of a statistician</a></p>
<p>8 0.83372498 <a title="996-lda-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-04-30-Things_I_learned_from_the_Mickey_Kaus_for_Senate_campaign.html">13 andrew gelman stats-2010-04-30-Things I learned from the Mickey Kaus for Senate campaign</a></p>
<p>9 0.8325243 <a title="996-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-03-Faculty_Position_in_Visualization%2C_Visual_Analytics%2C_Imaging%2C_and_Human_Centered_Computing.html">1651 andrew gelman stats-2013-01-03-Faculty Position in Visualization, Visual Analytics, Imaging, and Human Centered Computing</a></p>
<p>10 0.80017769 <a title="996-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-19-%E2%80%9CIf_it_saves_the_life_of_a_single_child%E2%80%A6%E2%80%9D_and_other_nonsense.html">526 andrew gelman stats-2011-01-19-“If it saves the life of a single child…” and other nonsense</a></p>
<p>11 0.79789323 <a title="996-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-30-%3F%3F%3F.html">2118 andrew gelman stats-2013-11-30-???</a></p>
<p>12 0.7931264 <a title="996-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-19-Chomsky_chomsky_chomsky_chomsky_furiously.html">1812 andrew gelman stats-2013-04-19-Chomsky chomsky chomsky chomsky furiously</a></p>
<p>13 0.79058695 <a title="996-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-25-Design_of_nonrandomized_cluster_sample_study.html">820 andrew gelman stats-2011-07-25-Design of nonrandomized cluster sample study</a></p>
<p>14 0.78890431 <a title="996-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-26-Reflections_on_ethicsblogging.html">1694 andrew gelman stats-2013-01-26-Reflections on ethicsblogging</a></p>
<p>15 0.76901555 <a title="996-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-06-%2463%2C000_worth_of_abusive_research_._._._or_just_a_really_stupid_waste_of_time%3F.html">18 andrew gelman stats-2010-05-06-$63,000 worth of abusive research . . . or just a really stupid waste of time?</a></p>
<p>16 0.76470488 <a title="996-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-21-Responding_to_a_bizarre_anti-social-science_screed.html">1335 andrew gelman stats-2012-05-21-Responding to a bizarre anti-social-science screed</a></p>
<p>17 0.76129216 <a title="996-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-28-Whassup_with_those_crappy_thrillers%3F.html">115 andrew gelman stats-2010-06-28-Whassup with those crappy thrillers?</a></p>
<p>18 0.75944042 <a title="996-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-02-Fighting_a_losing_battle.html">1518 andrew gelman stats-2012-10-02-Fighting a losing battle</a></p>
<p>19 0.75909162 <a title="996-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-27-Sampling_rate_of_human-scaled_time_series.html">112 andrew gelman stats-2010-06-27-Sampling rate of human-scaled time series</a></p>
<p>20 0.75788748 <a title="996-lda-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-13-An_Economist%E2%80%99s_Guide_to_Visualizing_Data.html">2246 andrew gelman stats-2014-03-13-An Economist’s Guide to Visualizing Data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
