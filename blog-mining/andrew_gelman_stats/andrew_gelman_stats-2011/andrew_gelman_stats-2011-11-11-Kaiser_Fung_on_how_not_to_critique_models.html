<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1004 andrew gelman stats-2011-11-11-Kaiser Fung on how not to critique models</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-1004" href="#">andrew_gelman_stats-2011-1004</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1004 andrew gelman stats-2011-11-11-Kaiser Fung on how not to critique models</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-1004-html" href="http://andrewgelman.com/2011/11/11/kaiser-fung-on-how-not-to-critique-models/">html</a></p><p>Introduction: In the context of a debate between economists Brad DeLong and Tyler Cowen on the “IS-LM model” [no, I don't know what it is, either!], Kaiser  writes :
  
Since a model is an abstraction, a simplification of reality, no model is above critique.


I [Kaiser] consider the following types of critique not deserving:


1) The critique that the modeler makes an assumption 
2) The critique that the modeler makes an assumption for mathematical convenience 
3) The critique that the model omits some feature 
4) The critique that the model doesn’t fit one’s intuition 
5) The critique that the model fails to make a specific prediction


Above all, a serious critique must include an alternative model that is provably better than the one it criticises. It is not enough to show that the alternative solves the problems being pointed out; the alternative must do so while preserving the useful aspects of the model being criticized.
  
I have mixed feelings about Kaiser’s rules.  On one hand, I agree wit</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In the context of a debate between economists Brad DeLong and Tyler Cowen on the “IS-LM model” [no, I don't know what it is, either! [sent-1, score-0.06]
</p><p>2 ], Kaiser  writes :    Since a model is an abstraction, a simplification of reality, no model is above critique. [sent-2, score-0.762]
</p><p>3 It is not enough to show that the alternative solves the problems being pointed out; the alternative must do so while preserving the useful aspects of the model being criticized. [sent-4, score-1.243]
</p><p>4 On one hand, I agree with his point that a model is a practical tool and that an imperfection is no reason to abandon a useful model. [sent-6, score-0.689]
</p><p>5 On the other hand, I think that much can be learned from rejection of a model, even without reference to any alternative. [sent-7, score-0.081]
</p><p>6 Let me put it this way:  That a model makes assumptions, even that a model makes wrong assumptions, is not news. [sent-8, score-0.976]
</p><p>7 If “wrong” is enough to kill, then all our models are dead on arrival anyway. [sent-9, score-0.183]
</p><p>8 But it’s good to understand the ways in which a model disagrees with the data at hand, or with other aspects of reality. [sent-10, score-0.541]
</p><p>9 As Kuhn and Lakatos knew, highlighting, isolating, and exploring anomalies are crucial steps in moving toward improvement—even if no alternative model is currently in the picture. [sent-11, score-0.865]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('critique', 0.558), ('model', 0.329), ('alternative', 0.219), ('modeler', 0.196), ('kaiser', 0.192), ('hand', 0.127), ('makes', 0.122), ('assumption', 0.12), ('imperfection', 0.119), ('abstraction', 0.119), ('isolating', 0.119), ('provably', 0.119), ('aspects', 0.116), ('arrival', 0.112), ('deserving', 0.112), ('omits', 0.108), ('assumptions', 0.104), ('anomalies', 0.104), ('solves', 0.104), ('abandon', 0.104), ('simplification', 0.104), ('delong', 0.101), ('preserving', 0.098), ('highlighting', 0.098), ('disagrees', 0.096), ('kuhn', 0.094), ('lakatos', 0.089), ('fails', 0.086), ('brad', 0.084), ('must', 0.082), ('rejection', 0.081), ('convenience', 0.08), ('kill', 0.079), ('exploring', 0.077), ('useful', 0.076), ('feelings', 0.075), ('wrong', 0.074), ('crucial', 0.072), ('dead', 0.071), ('improvement', 0.069), ('intuition', 0.069), ('mixed', 0.068), ('reality', 0.064), ('steps', 0.064), ('feature', 0.064), ('types', 0.063), ('cowen', 0.062), ('tool', 0.061), ('tyler', 0.061), ('debate', 0.06)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="1004-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-11-Kaiser_Fung_on_how_not_to_critique_models.html">1004 andrew gelman stats-2011-11-11-Kaiser Fung on how not to critique models</a></p>
<p>Introduction: In the context of a debate between economists Brad DeLong and Tyler Cowen on the “IS-LM model” [no, I don't know what it is, either!], Kaiser  writes :
  
Since a model is an abstraction, a simplification of reality, no model is above critique.


I [Kaiser] consider the following types of critique not deserving:


1) The critique that the modeler makes an assumption 
2) The critique that the modeler makes an assumption for mathematical convenience 
3) The critique that the model omits some feature 
4) The critique that the model doesn’t fit one’s intuition 
5) The critique that the model fails to make a specific prediction


Above all, a serious critique must include an alternative model that is provably better than the one it criticises. It is not enough to show that the alternative solves the problems being pointed out; the alternative must do so while preserving the useful aspects of the model being criticized.
  
I have mixed feelings about Kaiser’s rules.  On one hand, I agree wit</p><p>2 0.18428317 <a title="1004-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>Introduction: In response to  my remarks  on his online book, Think Bayes, Allen Downey wrote: 
   
I [Downey] have a question about one of your comments: 
   My [Gelman's] main criticism with both books is that they talk a lot about inference but not so much about model building or model checking (recall the three steps of Bayesian data analysis). I think it’s ok for an introductory book to focus on inference, which of course is central to the data-analytic process—but I’d like them to at least mention that Bayesian ideas arise in model building and model checking as well. 

This sounds like something I agree with, and one of the things I tried to do in the book is to put modeling decisions front and center.  But the word “modeling” is used in lots of ways, so I want to see if we are talking about the same thing.


For example, in many chapters, I start with a simple model of the scenario, do some analysis, then check whether the model is good enough, and iterate.  Here’s the discussion of modeling</p><p>3 0.14520721 <a title="1004-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>Introduction: I’ve been writing a lot about my philosophy of Bayesian statistics and how it fits into Popper’s ideas about falsification and Kuhn’s ideas about scientific revolutions.
 
 Here’s  my long, somewhat technical paper with Cosma Shalizi. 
 Here’s  our shorter overview for the volume on the philosophy of social science. 
 Here’s  my latest try (for an online symposium), focusing on the key issues.
 
I’m pretty happy with my approach–the familiar idea that Bayesian data analysis iterates the three steps of model building, inference, and model checking–but it does have some unresolved (maybe unresolvable) problems.  Here are a couple mentioned in the third of the above links.
 
Consider a simple model with independent data y_1, y_2, .., y_10 ~ N(θ,σ^2), with a prior distribution θ ~ N(0,10^2) and σ known and taking on some value of approximately 10. Inference about μ is straightforward, as is model checking, whether based on graphs or numerical summaries such as the sample variance and skewn</p><p>4 0.14019462 <a title="1004-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-21-More_on_Bayesian_model_selection_in_high-dimensional_settings.html">1817 andrew gelman stats-2013-04-21-More on Bayesian model selection in high-dimensional settings</a></p>
<p>Introduction: David Rossell writes:
  
A friend pointed out that you were having an interesting philosophical  discussion  on my paper with Val Johnson [on Bayesian model selection in high-dimensional settings].


I agree with the view that in almost all practical situations the true model is not in the set under consideration. Still, asking a model choice procedure to be able to pick up the correct model when it is in the set under consideration seems a minimal requirement (though perhaps not sufficient). In other words, if a procedure is unable to pick the data-generating model even when it is one of the models under consideration, I don’t have high hopes for it working well in more realistic scenarios either. 


Most results in the history in statistics seem to have been obtained under an assumed model, e.g. why even do MLE or penalized-likelihood if we don’t trust the model. While unrealistic, these results were useful to help understand important basic principles. In our case Val and I are defe</p><p>5 0.1381516 <a title="1004-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>Introduction: In response to  this article  by Cosma Shalizi and myself on the philosophy of Bayesian statistics, David Hogg writes:
  
I [Hogg] agree–even in physics and astronomy–that the models are not “True” in the God-like sense of being absolute reality (that is, I am not a realist); and I  have argued  (a philosophically very naive 
paper, but hey, I was new to all this) that for pretty fundamental reasons we could never arrive at the True (with a capital “T”) model of the Universe.  The goal of inference is to find the “best” model, where “best” might have something to do with prediction, or explanation, or message length, or (horror!) our utility.  Needless to say, most of my physics friends *are* realists, even in the face of “effective theories” as Newtonian mechanics is an effective theory of GR and GR is an effective theory of “quantum gravity” (this plays to your point, because if you think any theory is possibly an effective theory, how could you ever find Truth?).  I also liked the i</p><p>6 0.13189295 <a title="1004-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>7 0.13035506 <a title="1004-tfidf-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Useful_models%2C_model_checking%2C_and_external_validation%3A__a_mini-discussion.html">244 andrew gelman stats-2010-08-30-Useful models, model checking, and external validation:  a mini-discussion</a></p>
<p>8 0.12918283 <a title="1004-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>9 0.12908533 <a title="1004-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-27-Bridges_between_deterministic_and_probabilistic_models_for_binary_data.html">780 andrew gelman stats-2011-06-27-Bridges between deterministic and probabilistic models for binary data</a></p>
<p>10 0.12438065 <a title="1004-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-13-Flexibility_is_good.html">2133 andrew gelman stats-2013-12-13-Flexibility is good</a></p>
<p>11 0.12033121 <a title="1004-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>12 0.11917171 <a title="1004-tfidf-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-05-Does_posterior_predictive_model_checking_fit_with_the_operational_subjective_approach%3F.html">320 andrew gelman stats-2010-10-05-Does posterior predictive model checking fit with the operational subjective approach?</a></p>
<p>13 0.11602131 <a title="1004-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>14 0.11406988 <a title="1004-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-Another_reason_why_you_can_get_good_inferences_from_a_bad_model.html">1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</a></p>
<p>15 0.11212133 <a title="1004-tfidf-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-26-Infoviz_on_top_of_stat_graphic_on_top_of_spreadsheet.html">2186 andrew gelman stats-2014-01-26-Infoviz on top of stat graphic on top of spreadsheet</a></p>
<p>16 0.11080108 <a title="1004-tfidf-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-01-The_placebo_effect_in_pharma.html">388 andrew gelman stats-2010-11-01-The placebo effect in pharma</a></p>
<p>17 0.10633472 <a title="1004-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-03-Popper_and_Jaynes.html">2007 andrew gelman stats-2013-09-03-Popper and Jaynes</a></p>
<p>18 0.10469859 <a title="1004-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-28-Using_predator-prey_models_on_the_Canadian_lynx_series.html">1141 andrew gelman stats-2012-01-28-Using predator-prey models on the Canadian lynx series</a></p>
<p>19 0.10415211 <a title="1004-tfidf-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-09-Special_journal_issue_on_statistical_methods_for_the_social_sciences.html">24 andrew gelman stats-2010-05-09-Special journal issue on statistical methods for the social sciences</a></p>
<p>20 0.10171846 <a title="1004-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.145), (1, 0.116), (2, 0.005), (3, 0.049), (4, 0.001), (5, -0.019), (6, -0.009), (7, -0.001), (8, 0.149), (9, 0.047), (10, -0.015), (11, 0.062), (12, -0.085), (13, -0.007), (14, -0.135), (15, -0.013), (16, 0.006), (17, 0.065), (18, -0.009), (19, 0.005), (20, 0.015), (21, -0.05), (22, -0.054), (23, -0.102), (24, -0.046), (25, 0.032), (26, -0.007), (27, 0.001), (28, -0.004), (29, 0.027), (30, -0.081), (31, -0.082), (32, 0.002), (33, 0.073), (34, 0.008), (35, -0.015), (36, -0.004), (37, -0.045), (38, 0.007), (39, -0.059), (40, -0.004), (41, 0.019), (42, 0.004), (43, 0.031), (44, 0.022), (45, 0.008), (46, -0.002), (47, -0.039), (48, -0.0), (49, 0.03)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96394962 <a title="1004-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-11-Kaiser_Fung_on_how_not_to_critique_models.html">1004 andrew gelman stats-2011-11-11-Kaiser Fung on how not to critique models</a></p>
<p>Introduction: In the context of a debate between economists Brad DeLong and Tyler Cowen on the “IS-LM model” [no, I don't know what it is, either!], Kaiser  writes :
  
Since a model is an abstraction, a simplification of reality, no model is above critique.


I [Kaiser] consider the following types of critique not deserving:


1) The critique that the modeler makes an assumption 
2) The critique that the modeler makes an assumption for mathematical convenience 
3) The critique that the model omits some feature 
4) The critique that the model doesn’t fit one’s intuition 
5) The critique that the model fails to make a specific prediction


Above all, a serious critique must include an alternative model that is provably better than the one it criticises. It is not enough to show that the alternative solves the problems being pointed out; the alternative must do so while preserving the useful aspects of the model being criticized.
  
I have mixed feelings about Kaiser’s rules.  On one hand, I agree wit</p><p>2 0.8406707 <a title="1004-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-13-Flexibility_is_good.html">2133 andrew gelman stats-2013-12-13-Flexibility is good</a></p>
<p>Introduction: If I made a separate post for each interesting blog discussion, we’d get overwhelmed.  That’s why I often leave detailed responses in the comments section, even though I’m pretty sure that most readers don’t look in the comments at all.
 
Sometimes, though, I think it’s good to bring such discussions to light.  Here’s a recent example.
 
Michael  wrote :
  
Poor predictive performance usually indicates that the model isn’t sufficiently flexible to explain the data, and my understanding of the proper Bayesian strategy is to feed that back into your original model and try again until you achieve better performance.
  
Corey  replied :
  
It was my impression that — in ML at least — poor predictive performance is more often due to the model being too flexible and fitting noise.
  
And Rahul  agreed :
  
Good point. A very flexible model will describe your training data perfectly and then go bonkers when unleashed on wild data.
  
But I  wrote :
  
Overfitting comes from a model being flex</p><p>3 0.81210166 <a title="1004-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-27-Bridges_between_deterministic_and_probabilistic_models_for_binary_data.html">780 andrew gelman stats-2011-06-27-Bridges between deterministic and probabilistic models for binary data</a></p>
<p>Introduction: For the analysis of binary data, various deterministic models have been proposed, which are generally simpler to fit and easier to understand than probabilistic models. We claim that corresponding to any deterministic model is an implicit stochastic model in which the deterministic model fits imperfectly, with errors occurring at random. In the context of binary data, we consider a model in which the probability of error depends on the model prediction. We show how to fit this model using a stochastic modification of deterministic optimization schemes.
 
The advantages of fitting the stochastic model explicitly (rather than implicitly, by simply fitting a deterministic model and accepting the occurrence of errors) include quantification of uncertainty in the deterministic model’s parameter estimates, better estimation of the true model error rate, and the ability to check the fit of the model nontrivially. We illustrate this with a simple theoretical example of item response data and w</p><p>4 0.81154066 <a title="1004-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-21-More_on_Bayesian_model_selection_in_high-dimensional_settings.html">1817 andrew gelman stats-2013-04-21-More on Bayesian model selection in high-dimensional settings</a></p>
<p>Introduction: David Rossell writes:
  
A friend pointed out that you were having an interesting philosophical  discussion  on my paper with Val Johnson [on Bayesian model selection in high-dimensional settings].


I agree with the view that in almost all practical situations the true model is not in the set under consideration. Still, asking a model choice procedure to be able to pick up the correct model when it is in the set under consideration seems a minimal requirement (though perhaps not sufficient). In other words, if a procedure is unable to pick the data-generating model even when it is one of the models under consideration, I don’t have high hopes for it working well in more realistic scenarios either. 


Most results in the history in statistics seem to have been obtained under an assumed model, e.g. why even do MLE or penalized-likelihood if we don’t trust the model. While unrealistic, these results were useful to help understand important basic principles. In our case Val and I are defe</p><p>5 0.81060678 <a title="1004-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>Introduction: In response to  my remarks  on his online book, Think Bayes, Allen Downey wrote: 
   
I [Downey] have a question about one of your comments: 
   My [Gelman's] main criticism with both books is that they talk a lot about inference but not so much about model building or model checking (recall the three steps of Bayesian data analysis). I think it’s ok for an introductory book to focus on inference, which of course is central to the data-analytic process—but I’d like them to at least mention that Bayesian ideas arise in model building and model checking as well. 

This sounds like something I agree with, and one of the things I tried to do in the book is to put modeling decisions front and center.  But the word “modeling” is used in lots of ways, so I want to see if we are talking about the same thing.


For example, in many chapters, I start with a simple model of the scenario, do some analysis, then check whether the model is good enough, and iterate.  Here’s the discussion of modeling</p><p>6 0.80744088 <a title="1004-lsi-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-03-This_is_a_footnote_in_one_of_my_papers.html">448 andrew gelman stats-2010-12-03-This is a footnote in one of my papers</a></p>
<p>7 0.79254282 <a title="1004-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-28-Using_predator-prey_models_on_the_Canadian_lynx_series.html">1141 andrew gelman stats-2012-01-28-Using predator-prey models on the Canadian lynx series</a></p>
<p>8 0.78337938 <a title="1004-lsi-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-09-Special_journal_issue_on_statistical_methods_for_the_social_sciences.html">24 andrew gelman stats-2010-05-09-Special journal issue on statistical methods for the social sciences</a></p>
<p>9 0.78044152 <a title="1004-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>10 0.7793591 <a title="1004-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-04-An_addition_to_the_model-makers%E2%80%99_oath.html">554 andrew gelman stats-2011-02-04-An addition to the model-makers’ oath</a></p>
<p>11 0.77576435 <a title="1004-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-03-Popper_and_Jaynes.html">2007 andrew gelman stats-2013-09-03-Popper and Jaynes</a></p>
<p>12 0.77307332 <a title="1004-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>13 0.77099359 <a title="1004-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-15-Induction_within_a_model%2C_deductive_inference_for_model_evaluation.html">614 andrew gelman stats-2011-03-15-Induction within a model, deductive inference for model evaluation</a></p>
<p>14 0.76145768 <a title="1004-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-11-Adding_an_error_model_to_a_deterministic_model.html">1162 andrew gelman stats-2012-02-11-Adding an error model to a deterministic model</a></p>
<p>15 0.75983524 <a title="1004-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>16 0.75714666 <a title="1004-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-04-Columbo_does_posterior_predictive_checks.html">1521 andrew gelman stats-2012-10-04-Columbo does posterior predictive checks</a></p>
<p>17 0.75269008 <a title="1004-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>18 0.75009692 <a title="1004-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-26-Including_interactions_or_not.html">823 andrew gelman stats-2011-07-26-Including interactions or not</a></p>
<p>19 0.74353456 <a title="1004-lsi-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-19-The_%E2%80%9Ceither-or%E2%80%9D_fallacy_of_believing_in_discrete_models%3A__an_example_of_folk_statistics.html">217 andrew gelman stats-2010-08-19-The “either-or” fallacy of believing in discrete models:  an example of folk statistics</a></p>
<p>20 0.74222344 <a title="1004-lsi-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-08-Displaying_a_fitted_multilevel_model.html">328 andrew gelman stats-2010-10-08-Displaying a fitted multilevel model</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(5, 0.022), (7, 0.014), (9, 0.014), (16, 0.048), (24, 0.241), (33, 0.013), (34, 0.024), (36, 0.011), (72, 0.033), (76, 0.011), (83, 0.013), (86, 0.058), (92, 0.147), (94, 0.014), (95, 0.011), (99, 0.212)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94045615 <a title="1004-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-11-Kaiser_Fung_on_how_not_to_critique_models.html">1004 andrew gelman stats-2011-11-11-Kaiser Fung on how not to critique models</a></p>
<p>Introduction: In the context of a debate between economists Brad DeLong and Tyler Cowen on the “IS-LM model” [no, I don't know what it is, either!], Kaiser  writes :
  
Since a model is an abstraction, a simplification of reality, no model is above critique.


I [Kaiser] consider the following types of critique not deserving:


1) The critique that the modeler makes an assumption 
2) The critique that the modeler makes an assumption for mathematical convenience 
3) The critique that the model omits some feature 
4) The critique that the model doesn’t fit one’s intuition 
5) The critique that the model fails to make a specific prediction


Above all, a serious critique must include an alternative model that is provably better than the one it criticises. It is not enough to show that the alternative solves the problems being pointed out; the alternative must do so while preserving the useful aspects of the model being criticized.
  
I have mixed feelings about Kaiser’s rules.  On one hand, I agree wit</p><p>2 0.90955591 <a title="1004-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-05-Someone_is_wrong_on_the_internet%2C_part_2.html">1563 andrew gelman stats-2012-11-05-Someone is wrong on the internet, part 2</a></p>
<p>Introduction: My coblogger John Sides  feeds  a troll.  It’s a tough call.  Yesterday I gave my  reasoning  for ignoring these provocateurs, but in this case the troll in question is writing for a major newspaper so it makes sense for John to go to the trouble of shooting him down.  Even though I suspect the columnist was trolling for no better reason than . . . he had a deadline and nothing to say so he thought he’d wade into a controversy.
 
On the plus side, as a statistician I’m happy that statistics is considered important enough that it’s worth trolling!  When they start attacking like this, they must feel a bit on the defensive. . . .</p><p>3 0.90852726 <a title="1004-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-20-Paul_Rosenbaum_on_those_annoying_pre-treatment_variables_that_are_sort-of_instruments_and_sort-of_covariates.html">287 andrew gelman stats-2010-09-20-Paul Rosenbaum on those annoying pre-treatment variables that are sort-of instruments and sort-of covariates</a></p>
<p>Introduction: Last year  we discussed  an important challenge in causal inference:  The standard advice (given in many books, including ours) for causal inference is to control for relevant pre-treatment variables as much as possible.  But, as Judea Pearl has pointed out, instruments (as in “instrumental variables”) are pre-treatment variables that we would  not  want to “control for” in a matching or regression sense.
 
At first, this seems like a minor modification, with the new recommendation being to apply instrumental variables estimation using all pre-treatment instruments, and to control for all other pre-treatment variables.  But that can’t really work as general advice.  What about weak instruments or covariates that have some instrumental aspects?
 
I asked Paul Rosenbaum for his thoughts on the matter, and he wrote the following:
  
In section 18.2 of Design of Observational Studies (DOS), I [Rosenbaum] discuss “seemingly innocuous confounding” defined to be a covariate that predicts a su</p><p>4 0.900576 <a title="1004-lda-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-Running_into_a_Stan_Reference_by_Accident.html">2231 andrew gelman stats-2014-03-03-Running into a Stan Reference by Accident</a></p>
<p>Introduction: We were talking about parallelizing MCMC and I came up with what I thought was a neat idea for parallelizing MCMC (sample with fractional prior, average samples on a per-draw basis).  But then I realized this approach could get the right posterior mean or right posterior variance, but not both, depending on how the prior was divided (for a beta-binomial example).  Then  Aki  told me it had already been done in a more general form in a paper of Scott et al.,  Bayes and Big Data , which was then used as the baseline in: 
 
Willie Neiswanger, Chong Wang, and Eric Xing. 2013.   Asymptotically Exact, Embarrassingly Parallel MCMC .  arXiv  1311.4780. 
 
It’s a neat paper, which Xi’an  already blogged  about months ago.  But what really struck me was the following quote:
  

We use Stan, an automated Hamiltonian Monte Carlo (HMC) software package, to perform sampling for both the true posterior (for groundtruth and comparison methods) and for the subposteriors on each machine. One advantage o</p><p>5 0.89687192 <a title="1004-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-02-So_much_artistic_talent.html">1785 andrew gelman stats-2013-04-02-So much artistic talent</a></p>
<p>Introduction: I saw  this excellent art show  the other day, and it reminded me how much artistic talent is out there.  I really have no idea whassup with those all-black canvases and the other stuff you see at modern art museums, given that there’s so much interesting new stuff being created every year.  I see a big difference between art made by people who feel they have something they want to say, compared to art being made by people who feel they are supposed to make art because they’re artists.  And there’s also the internal logic of art responding to other art, as Tom Wolfe discussed in The Painted Word.</p><p>6 0.89541078 <a title="1004-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-11-Steve_Jobs%E2%80%99s_cancer_and_science-based_medicine.html">953 andrew gelman stats-2011-10-11-Steve Jobs’s cancer and science-based medicine</a></p>
<p>7 0.8951146 <a title="1004-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-29-Don%E2%80%99t_try_this_at_home.html">491 andrew gelman stats-2010-12-29-Don’t try this at home</a></p>
<p>8 0.89441121 <a title="1004-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-06-Question_27_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1368 andrew gelman stats-2012-06-06-Question 27 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>9 0.89204466 <a title="1004-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>10 0.89158118 <a title="1004-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-12-Probabilistic_screening_to_get_an_approximate_self-weighted_sample.html">1455 andrew gelman stats-2012-08-12-Probabilistic screening to get an approximate self-weighted sample</a></p>
<p>11 0.89094824 <a title="1004-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-11-%E2%80%9CInformative_g-Priors_for_Logistic_Regression%E2%80%9D.html">2017 andrew gelman stats-2013-09-11-“Informative g-Priors for Logistic Regression”</a></p>
<p>12 0.89061791 <a title="1004-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-12-Simple_graph_WIN%3A__the_example_of_birthday_frequencies.html">1376 andrew gelman stats-2012-06-12-Simple graph WIN:  the example of birthday frequencies</a></p>
<p>13 0.88921368 <a title="1004-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>14 0.88869321 <a title="1004-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-03-Setting_aside_the_politics%2C_the_debate_over_the_new_health-care_study_reveals_that_we%E2%80%99re_moving_to_a_new_high_standard_of_statistical_journalism.html">1838 andrew gelman stats-2013-05-03-Setting aside the politics, the debate over the new health-care study reveals that we’re moving to a new high standard of statistical journalism</a></p>
<p>15 0.88751346 <a title="1004-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-10-The_last_great_essayist%3F.html">197 andrew gelman stats-2010-08-10-The last great essayist?</a></p>
<p>16 0.88703567 <a title="1004-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>17 0.88681412 <a title="1004-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-10-Using_a_%E2%80%9Cpure_infographic%E2%80%9D_to_explore_differences_between_information_visualization_and_statistical_graphics.html">847 andrew gelman stats-2011-08-10-Using a “pure infographic” to explore differences between information visualization and statistical graphics</a></p>
<p>18 0.8862536 <a title="1004-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-05-Question_26_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1367 andrew gelman stats-2012-06-05-Question 26 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>19 0.88601935 <a title="1004-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-16-Mr._Pearson%2C_meet_Mr._Mandelbrot%3A__Detecting_Novel_Associations_in_Large_Data_Sets.html">1062 andrew gelman stats-2011-12-16-Mr. Pearson, meet Mr. Mandelbrot:  Detecting Novel Associations in Large Data Sets</a></p>
<p>20 0.88456947 <a title="1004-lda-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-15-Advice_that_might_make_sense_for_individuals_but_is_negative-sum_overall.html">278 andrew gelman stats-2010-09-15-Advice that might make sense for individuals but is negative-sum overall</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
