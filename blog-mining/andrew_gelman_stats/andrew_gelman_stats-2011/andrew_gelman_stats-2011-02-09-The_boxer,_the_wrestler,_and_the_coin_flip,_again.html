<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>566 andrew gelman stats-2011-02-09-The boxer, the wrestler, and the coin flip, again</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-566" href="#">andrew_gelman_stats-2011-566</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>566 andrew gelman stats-2011-02-09-The boxer, the wrestler, and the coin flip, again</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-566-html" href="http://andrewgelman.com/2011/02/09/the_boxer_the_w_1/">html</a></p><p>Introduction: Mike Grosskopf writes:
  
 
I came across your blog the other day and noticed  your paper  about “The Boxer, the Wrestler, and the Coin Flip” . . . I do not understand the objection to the robust Bayesian inference for conditioning on X=Y in the problem as you describe in the paper.  The paper talks about how using Robust Bayes when conditioning on X=Y “degrades our inference about the coin flip” and “has led us to the claim that we can say nothing at all about the coin ﬂip”. Does that have to be the case however, because while conditioning on X=Y does mean that p({X=1}|{X=Y}I) =  p({Y=1}|{X=Y}I), I don’t see why it has to mean that both have the same π-distribution where Pr(Y = 1) = π.


Which type of inference is being done about Y in the problem?


If you are trying to make an inference on the results of the fight between the boxer and the wrestler that has already happened, in which your friend tells you that either the boxer won and he flipped heads with a coin or the boxer lost a</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Mike Grosskopf writes:      I came across your blog the other day and noticed  your paper  about “The Boxer, the Wrestler, and the Coin Flip” . [sent-1, score-0.037]
</p><p>2 I do not understand the objection to the robust Bayesian inference for conditioning on X=Y in the problem as you describe in the paper. [sent-4, score-0.608]
</p><p>3 The paper talks about how using Robust Bayes when conditioning on X=Y “degrades our inference about the coin flip” and “has led us to the claim that we can say nothing at all about the coin ﬂip”. [sent-5, score-1.105]
</p><p>4 Does that have to be the case however, because while conditioning on X=Y does mean that p({X=1}|{X=Y}I) =  p({Y=1}|{X=Y}I), I don’t see why it has to mean that both have the same π-distribution where Pr(Y = 1) = π. [sent-6, score-0.322]
</p><p>5 Which type of inference is being done about Y in the problem? [sent-7, score-0.131]
</p><p>6 The distribution of π’ defined as (Pr(Y=1|{X=Y},I) = Pr(X=1|{X=Y},I) ~ a delta function at 0. [sent-9, score-0.252]
</p><p>7 π’ ≠ p(π|{X=Y},I) however because p(π|{X=Y},I) = p(p(Y=1|I)|{X=Y},I), which is basically saying how does conditioning on X=Y effect the distribution of possible probabilities for the prior of Y. [sent-12, score-0.519]
</p><p>8 If you are trying to better understand what chance the boxer had going into the fight after conditioning on the information that X=Y, then p(π|{X=Y}I) is the relevant inference instead of π’. [sent-13, score-1.473]
</p><p>9 This is where you would expect no change in uncertainty in π when conditioning on the coin flip. [sent-14, score-0.707]
</p><p>10 As I laid out in the first email (though in a particularly messy and illegible format), this inference is not changed by conditioning on the coin flip in Bayesian analysis. [sent-15, score-1.069]
</p><p>11 We are still just as uncertain about what the boxer’s chances were (π). [sent-16, score-0.12]
</p><p>12 I would think that Bayesian analysis gives the correct analysis in both cases. [sent-17, score-0.059]
</p><p>13 A better way to clarify what I was thinking is considering where, instead of conditioning on the result of the coin flip (Y=X), condition on the result of something essentially certain, like the sun will rise tomorrow (Y=A). [sent-18, score-1.218]
</p><p>14 If someone presented you with the information that “the boxer won just as sure as the sun will rise tomorrow” you would give the same inferential status as certain (p()=1 for both) and the distribution of π” = Pr(Y=1|{Y=A},I) would be basically a delta function at 1. [sent-19, score-1.51]
</p><p>15 However, if you were doing inference on what the boxer’s chances were going into the fight, p(π|{Y=A},I), you would not be certain if the boxer was unlikely to win and pulled an upset or if the boxer was a heavy favorite and easily followed through. [sent-20, score-2.01]
</p><p>16 Your distribution for π would be updated to be p(π|{Y=A},I) = 2*π (from the first email). [sent-21, score-0.18]
</p><p>17 All you would really be certain of is that the boxer had some chance, and you would feel that it was now more likely that he had a good chance to win instead of being the underdog. [sent-22, score-1.11]
</p><p>18 This again seems to work out fine using Bayesian with an uninformative prior. [sent-23, score-0.049]
</p><p>19 My reply:  I’m too tired to think about this, but I’ll post it and then maybe others will have some thoughts. [sent-24, score-0.042]
</p><p>20 The one thing I can tell you is that it’s an old example–I came up with it in discussions with Augustine Kong back around 1988. [sent-25, score-0.037]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('boxer', 0.713), ('coin', 0.326), ('conditioning', 0.322), ('pr', 0.155), ('flip', 0.155), ('inference', 0.131), ('wrestler', 0.118), ('delta', 0.118), ('fight', 0.115), ('certain', 0.096), ('flipped', 0.093), ('sun', 0.082), ('distribution', 0.082), ('tomorrow', 0.081), ('chances', 0.079), ('inferential', 0.077), ('rise', 0.074), ('robust', 0.067), ('chance', 0.066), ('bayesian', 0.061), ('win', 0.06), ('however', 0.06), ('would', 0.059), ('instead', 0.057), ('ip', 0.056), ('kong', 0.056), ('basically', 0.055), ('objection', 0.053), ('function', 0.052), ('heads', 0.051), ('uninformative', 0.049), ('email', 0.049), ('tails', 0.046), ('messy', 0.044), ('pulled', 0.044), ('won', 0.043), ('laid', 0.042), ('tired', 0.042), ('result', 0.042), ('uncertain', 0.041), ('upset', 0.041), ('format', 0.039), ('updated', 0.039), ('heavy', 0.038), ('condition', 0.037), ('came', 0.037), ('unlikely', 0.036), ('understand', 0.035), ('mike', 0.035), ('trying', 0.034)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="566-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-09-The_boxer%2C_the_wrestler%2C_and_the_coin_flip%2C_again.html">566 andrew gelman stats-2011-02-09-The boxer, the wrestler, and the coin flip, again</a></p>
<p>Introduction: Mike Grosskopf writes:
  
 
I came across your blog the other day and noticed  your paper  about “The Boxer, the Wrestler, and the Coin Flip” . . . I do not understand the objection to the robust Bayesian inference for conditioning on X=Y in the problem as you describe in the paper.  The paper talks about how using Robust Bayes when conditioning on X=Y “degrades our inference about the coin flip” and “has led us to the claim that we can say nothing at all about the coin ﬂip”. Does that have to be the case however, because while conditioning on X=Y does mean that p({X=1}|{X=Y}I) =  p({Y=1}|{X=Y}I), I don’t see why it has to mean that both have the same π-distribution where Pr(Y = 1) = π.


Which type of inference is being done about Y in the problem?


If you are trying to make an inference on the results of the fight between the boxer and the wrestler that has already happened, in which your friend tells you that either the boxer won and he flipped heads with a coin or the boxer lost a</p><p>2 0.16305691 <a title="566-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-25-Freakonomics_Experiments.html">1692 andrew gelman stats-2013-01-25-Freakonomics Experiments</a></p>
<p>Introduction: Stephen Dubner  writes :
  
Freakonomics Experiments is a set of simple experiments about complex issues—whether to break up with your significant other, quit your job, or start a diet, just to name a few. . . . a collaboration between researchers at the University of Chicago, Freakonomics, and—we hope!—you. Steve Levitt and John List, of the University of Chicago, run the experimental and statistical side of things. Stephen Dubner, Steve Levitt, and the Freakonomics staff have given these experiments the Freakonomics twist you’re used to. Once you flip the coin, you become a member of the most important part of the collaboration, the Freakonomics Experiments team. Without your participation, we couldn’t complete any of this research. . . .


You’ll choose a question that you are facing today, such as whether to quit your job or buy a house. Then you’ll provide us some background information about yourself. After that, you’ll flip the coin to find out what you should do in your situati</p><p>3 0.095133446 <a title="566-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-11-One_more_time_on_that_ESP_study%3A__The_problem_of_overestimates_and_the_shrinkage_solution.html">511 andrew gelman stats-2011-01-11-One more time on that ESP study:  The problem of overestimates and the shrinkage solution</a></p>
<p>Introduction: Benedict Carey  writes  a follow-up article on ESP studies and Bayesian statistics.  ( See here  for my previous thoughts on the topic.)  Everything Carey writes is fine, and he even uses an example I recommended:
  
The statistical approach that has dominated the social sciences for almost a century is called significance testing. The idea is straightforward. A finding from any well-designed study — say, a correlation between a personality trait and the risk of depression — is considered “significant” if its probability of occurring by chance is less than 5 percent.


This arbitrary cutoff makes sense when the effect being studied is a large one — for example, when measuring the so-called Stroop effect. This effect predicts that naming the color of a word is faster and more accurate when the word and color match (“red” in red letters) than when they do not (“red” in blue letters), and is very strong in almost everyone.


“But if the true effect of what you are measuring is small,” sai</p><p>4 0.088228084 <a title="566-tfidf-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-14-Confusion_about_continuous_probability_densities.html">341 andrew gelman stats-2010-10-14-Confusion about continuous probability densities</a></p>
<p>Introduction: I had the following email exchange with a reader of Bayesian Data Analysis.
  

 
 My correspondent wrote:   Exercise 1(b) involves evaluating the normal pdf at a single point. But p(Y=y|mu,sigma) = 0 (and is not simply N(y|mu,sigma)), since the normal distribution is continuous. So it seems that part (b) of the exercise is inappropriate.
 
The solution does actually evaluate the probability as the value of the pdf at the single point, which is wrong. The probabilities should all be 0, so the answer to (b) is undefined.
 
 I replied:   The pdf is the probability density function, which for a continuous distribution is defined as the derivative of the cumulative density function.  The notation in BDA is rigorous but we do not spell out all the details, so I can see how confusion is possible. 
 
 My correspondent:   I agree that the pdf is the derivative of the cdf. But to compute P(a .lt. Y .lt. b) for a continuous distribution (with support in the real line) requires integrating over t</p><p>5 0.086175174 <a title="566-tfidf-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-15-%3F.html">343 andrew gelman stats-2010-10-15-?</a></p>
<p>Introduction: How am I supposed to handle this sort of thing?  (See below.)  I just stuck it one of my email folders without responding, but then I wondered . . . what’s it all about?  Is there some sort of Glengarry Glen Ross-like parallel world where down-on-their-luck Jack Lemmons of public relations world send out electronic cold calls?  More than anything else, this sort of thing makes me glad I have a steady job.
 
Here’s the (unsolicited) email, which came with the subject line “Please help a reporter do his job”:
  
Dear Andrew,


As an Editor for the Bulldog Reporter (www.bulldogreporter.com/dailydog), a media relations trade publication, my job is to help ensure that my readers have accurate info about you and send you the best quality pitches. By taking five minutes or less to answer my questions (pasted below), you’ll receive targeted PR pitches from our client base that will match your beat and interests. Any help or direction is appreciated. Here are my questions.


We have you listed</p><p>6 0.074047402 <a title="566-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-24-Untangling_the_Jeffreys-Lindley_paradox.html">1182 andrew gelman stats-2012-02-24-Untangling the Jeffreys-Lindley paradox</a></p>
<p>7 0.068712234 <a title="566-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>8 0.067936331 <a title="566-tfidf-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-08-Understanding_Simpson%E2%80%99s_paradox_using_a_graph.html">2286 andrew gelman stats-2014-04-08-Understanding Simpson’s paradox using a graph</a></p>
<p>9 0.067428008 <a title="566-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>10 0.066894189 <a title="566-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>11 0.066104524 <a title="566-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>12 0.063905001 <a title="566-tfidf-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-25-Basketball_Stats%3A__Don%E2%80%99t_model_the_probability_of_win%2C_model_the_expected_score_differential..html">2224 andrew gelman stats-2014-02-25-Basketball Stats:  Don’t model the probability of win, model the expected score differential.</a></p>
<p>13 0.063430138 <a title="566-tfidf-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-30-Silly_baseball_example_illustrates_a_couple_of_key_ideas_they_don%E2%80%99t_usually_teach_you_in_statistics_class.html">171 andrew gelman stats-2010-07-30-Silly baseball example illustrates a couple of key ideas they don’t usually teach you in statistics class</a></p>
<p>14 0.063283592 <a title="566-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-19-Prior_distributions_on_derived_quantities_rather_than_on_parameters_themselves.html">1946 andrew gelman stats-2013-07-19-Prior distributions on derived quantities rather than on parameters themselves</a></p>
<p>15 0.063028172 <a title="566-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-01-Martin_and_Liu%3A__Probabilistic_inference_based_on_consistency_of_model_with_data.html">1095 andrew gelman stats-2012-01-01-Martin and Liu:  Probabilistic inference based on consistency of model with data</a></p>
<p>16 0.062454257 <a title="566-tfidf-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-22-Philosophy_of_Bayes_and_non-Bayes%3A__A_dialogue_with_Deborah_Mayo.html">291 andrew gelman stats-2010-09-22-Philosophy of Bayes and non-Bayes:  A dialogue with Deborah Mayo</a></p>
<p>17 0.061935186 <a title="566-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-15-Bayesian_statistical_pragmatism.html">662 andrew gelman stats-2011-04-15-Bayesian statistical pragmatism</a></p>
<p>18 0.061662268 <a title="566-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<p>19 0.060524378 <a title="566-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-06-Bidding_for_the_kickoff.html">559 andrew gelman stats-2011-02-06-Bidding for the kickoff</a></p>
<p>20 0.060106877 <a title="566-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.109), (1, 0.047), (2, -0.014), (3, 0.02), (4, -0.032), (5, -0.017), (6, 0.018), (7, 0.006), (8, 0.013), (9, -0.049), (10, -0.011), (11, 0.017), (12, 0.03), (13, -0.001), (14, 0.002), (15, 0.019), (16, 0.024), (17, 0.008), (18, 0.007), (19, 0.018), (20, -0.012), (21, 0.025), (22, 0.039), (23, -0.004), (24, 0.034), (25, 0.019), (26, 0.024), (27, 0.007), (28, 0.005), (29, 0.009), (30, 0.038), (31, -0.019), (32, -0.028), (33, 0.004), (34, 0.004), (35, -0.019), (36, 0.004), (37, -0.005), (38, -0.025), (39, 0.028), (40, 0.018), (41, -0.003), (42, -0.012), (43, -0.029), (44, 0.007), (45, -0.007), (46, 0.018), (47, 0.045), (48, 0.005), (49, -0.011)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9527393 <a title="566-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-09-The_boxer%2C_the_wrestler%2C_and_the_coin_flip%2C_again.html">566 andrew gelman stats-2011-02-09-The boxer, the wrestler, and the coin flip, again</a></p>
<p>Introduction: Mike Grosskopf writes:
  
 
I came across your blog the other day and noticed  your paper  about “The Boxer, the Wrestler, and the Coin Flip” . . . I do not understand the objection to the robust Bayesian inference for conditioning on X=Y in the problem as you describe in the paper.  The paper talks about how using Robust Bayes when conditioning on X=Y “degrades our inference about the coin flip” and “has led us to the claim that we can say nothing at all about the coin ﬂip”. Does that have to be the case however, because while conditioning on X=Y does mean that p({X=1}|{X=Y}I) =  p({Y=1}|{X=Y}I), I don’t see why it has to mean that both have the same π-distribution where Pr(Y = 1) = π.


Which type of inference is being done about Y in the problem?


If you are trying to make an inference on the results of the fight between the boxer and the wrestler that has already happened, in which your friend tells you that either the boxer won and he flipped heads with a coin or the boxer lost a</p><p>2 0.73725086 <a title="566-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-20-Problemen_met_het_boek.html">1332 andrew gelman stats-2012-05-20-Problemen met het boek</a></p>
<p>Introduction: Regarding the so-called Dutch Book argument for Bayesian inference (the idea that, if your inferences do not correspond to a Bayesian posterior distribution, you can be forced to make incoherent bets and ultimately become a money pump), I wrote:
 
I have never found this argument appealing, because a bet is a game not a decision.  A bet requires 2 players, and one player has to offer the bets.  I do agree that in some bounded settings (for example, betting on win place show in a horse race), I’d want my bets to be coherent; if they are incoherent (e.g., if my bets correspond to P(A|B)*P(B) not being equal to P(A,B)), then I should be able to do better by examining the incoherence.  But in an “open system” (to borrow some physics jargon), I don’t think coherence is possible.  There is always new information coming in, and there is always additional prior information in reserve that hasn’t entered the model.</p><p>3 0.7097739 <a title="566-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-03-Statistical_methods_that_work_in_some_settings_but_not_others.html">1560 andrew gelman stats-2012-11-03-Statistical methods that work in some settings but not others</a></p>
<p>Introduction: David Hogg pointed me to  this post  by Larry Wasserman:
  
1. The Horwitz-Thompson estimator    satisfies the following condition: for every   ,

  


  where   — the parameter space — is the set of all functions  . (There are practical improvements to the Horwitz-Thompson estimator that we discussed in our earlier posts but we won’t revisit those here.)


2. A Bayes estimator requires a prior   for  . In general, if   is not a function of   then (1) will not hold. . . .


3. If you let   be a function if  , (1) still, in general, does not hold.


4. If you make   a function if   in just the right way, then (1) will hold. . . . There is nothing wrong with doing this, but in our opinion this is not in the spirit of Bayesian inference. . . .


7. This example is only meant to show that Bayesian estimators do not necessarily have good frequentist properties. This should not be surprising. There is no reason why we should in general expect a Bayesian method to have a frequentist property</p><p>4 0.69179094 <a title="566-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-21-Judea_Pearl_on_why_he_is_%E2%80%9Conly_a_half-Bayesian%E2%80%9D.html">1133 andrew gelman stats-2012-01-21-Judea Pearl on why he is “only a half-Bayesian”</a></p>
<p>Introduction: In  an article  published in 2001, Pearl wrote:
  
I [Pearl] turned Bayesian in 1971, as soon as I began reading Savage’s monograph The Foundations of Statistical Inference [Savage, 1962]. The arguments were unassailable: (i) It is plain silly to ignore what we know, (ii) It is natural and useful to cast what we know in the language of probabilities, and (iii) If our subjective probabilities are erroneous, their impact will get washed out in due time, as the number of observations increases.


Thirty years later, I [Pearl] am still a devout Bayesian in the sense of (i), but I now doubt the wisdom of (ii) and I know that, in general, (iii) is false.
  
He elaborates:
  
The bulk of human knowledge is organized around causal, not probabilistic relationships, and the grammar of probability calculus is insufficient for capturing those relationships. Specifically, the building blocks of our scientific and everyday knowledge are elementary facts such as “mud does not cause rain” and “symptom</p><p>5 0.69057405 <a title="566-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-24-Untangling_the_Jeffreys-Lindley_paradox.html">1182 andrew gelman stats-2012-02-24-Untangling the Jeffreys-Lindley paradox</a></p>
<p>Introduction: Ryan Ickert writes:
  
I was wondering if you’d seen  this post , by a particle physicist with some degree of influence.  Dr. Dorigo works at CERN and Fermilab.


The penultimate paragraph is:

 
From the above expression, the Frequentist researcher concludes that the tracker is indeed biased, and rejects the null hypothesis H0, since there is a less-than-2% probability (P’<α) that a result as the one observed could arise by chance! A Frequentist thus draws, strongly, the opposite conclusion than a Bayesian from the same set of data. How to solve the riddle?
 

He goes on to not solve the riddle.  Perhaps you can?


Surely with the large sample size they have (n=10^6), the precision on the frequentist p-value is pretty good, is it not?
  
My reply:
 
The first comment on the site (by Anonymous [who, just to be clear, is not me; I have no idea who wrote that comment], 22 Feb 2012, 21:27pm) pretty much nails it:  In setting up the Bayesian model, Dorigo assumed a silly distribution on th</p><p>6 0.68631929 <a title="566-lsi-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-01-Ratios_where_the_numerator_and_denominator_both_change_signs.html">248 andrew gelman stats-2010-09-01-Ratios where the numerator and denominator both change signs</a></p>
<p>7 0.67721456 <a title="566-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-06-Yes%2C_checking_calibration_of_probability_forecasts_is_part_of_Bayesian_statistics.html">1610 andrew gelman stats-2012-12-06-Yes, checking calibration of probability forecasts is part of Bayesian statistics</a></p>
<p>8 0.66957247 <a title="566-lsi-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-16-Looking_for_Bayesian_expertise_in_India%2C_for_the_purpose_of_analysis_of_sarcoma_trials.html">2293 andrew gelman stats-2014-04-16-Looking for Bayesian expertise in India, for the purpose of analysis of sarcoma trials</a></p>
<p>9 0.6665644 <a title="566-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-31-What_is_a_Bayesian%3F.html">1438 andrew gelman stats-2012-07-31-What is a Bayesian?</a></p>
<p>10 0.66580701 <a title="566-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-25-Bayes-respecting_experimental_design_and_other_things.html">1955 andrew gelman stats-2013-07-25-Bayes-respecting experimental design and other things</a></p>
<p>11 0.66556001 <a title="566-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>12 0.6650148 <a title="566-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-15-Static_sensitivity_analysis.html">804 andrew gelman stats-2011-07-15-Static sensitivity analysis</a></p>
<p>13 0.66476846 <a title="566-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-04-Bayesian_Page_Rank%3F.html">1098 andrew gelman stats-2012-01-04-Bayesian Page Rank?</a></p>
<p>14 0.66386408 <a title="566-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-16-Long_discussion_about_causal_inference_and_the_use_of_hierarchical_models_to_bridge_between_different_inferential_settings.html">1418 andrew gelman stats-2012-07-16-Long discussion about causal inference and the use of hierarchical models to bridge between different inferential settings</a></p>
<p>15 0.66332638 <a title="566-lsi-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-14-Confusion_about_continuous_probability_densities.html">341 andrew gelman stats-2010-10-14-Confusion about continuous probability densities</a></p>
<p>16 0.65712523 <a title="566-lsi-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-11-Bayes_in_the_research_conversation.html">2368 andrew gelman stats-2014-06-11-Bayes in the research conversation</a></p>
<p>17 0.65575522 <a title="566-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-29-Bayes_in_astronomy.html">1091 andrew gelman stats-2011-12-29-Bayes in astronomy</a></p>
<p>18 0.65200067 <a title="566-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-22-Battle_of_the_Repo_Man_quotes%3A__Reid_Hastie%E2%80%99s_turn.html">1336 andrew gelman stats-2012-05-22-Battle of the Repo Man quotes:  Reid Hastie’s turn</a></p>
<p>19 0.65101916 <a title="566-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-26-%E2%80%9CThe_Bayesian_approach_to_forensic_evidence%E2%80%9D.html">2078 andrew gelman stats-2013-10-26-“The Bayesian approach to forensic evidence”</a></p>
<p>20 0.64676088 <a title="566-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-08-The_virtues_of_incoherence%3F.html">792 andrew gelman stats-2011-07-08-The virtues of incoherence?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(16, 0.037), (21, 0.029), (24, 0.11), (35, 0.052), (53, 0.026), (57, 0.011), (72, 0.012), (89, 0.262), (95, 0.011), (99, 0.276)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97538495 <a title="566-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-16-The_%E2%80%9Chot_hand%E2%80%9D_and_problems_with_hypothesis_testing.html">1215 andrew gelman stats-2012-03-16-The “hot hand” and problems with hypothesis testing</a></p>
<p>Introduction: Gur Yaari  writes :
  
Anyone who has ever watched a sports competition is familiar with expressions like “on fire”, “in the zone”, “on a roll”, “momentum” and so on. But what do these expressions really mean? In 1985 when Thomas Gilovich, Robert Vallone and Amos Tversky  studied  this phenomenon for the first time, they defined it as: “. . . these phrases express a belief that the performance of a player during a particular period is significantly better than expected on the basis of the player’s overall record”. Their conclusion was that what people tend to perceive as a “hot hand” is essentially a cognitive illusion caused by a misperception of random sequences. Until recently there was little, if any, evidence to rule out their conclusion. Increased computing power and new data availability from various sports now provide surprising evidence of this phenomenon, thus reigniting the debate.
  
Yaari goes on to some studies that have found time dependence in basketball, baseball, voll</p><p>2 0.9698981 <a title="566-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-10-He_said_he_was_sorry.html">1756 andrew gelman stats-2013-03-10-He said he was sorry</a></p>
<p>Introduction: Yes, it can be  done :
  
Hereby I contact you to clarify the situation that occurred with the publication of the article entitled *** which was published in Volume 11, Issue 3 of *** and I made the mistake of declaring as an author.  This chapter is a plagiarism of . . .


I wish to express and acknowledge that I am solely responsible for this . . . I recognize the gravity of the offense committed, since there is no justification for so doing. Therefore, and as a sign of shame and regret I feel in this situation, I will publish this letter, in order to set an example for other researchers do not engage in a similar error.


No more, and to please accept my apologies,


Sincerely,


***
  
P.S.  Since we’re on Retraction Watch already, I’ll point you to  this unrelated story  featuring a hilarious photo of a fraudster, who in this case was a grad student in psychology who faked his data and “has agreed to submit to a three-year supervisory period for any work involving funding from the</p><p>3 0.96731317 <a title="566-lda-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-11-The_myth_of_the_myth_of_the_myth_of_the_hot_hand.html">2243 andrew gelman stats-2014-03-11-The myth of the myth of the myth of the hot hand</a></p>
<p>Introduction: Phil pointed me to  this paper  so I thought I probably better repeat what I  wrote  a couple years ago:
 
1. The effects are certainly not zero. We are not machines, and anything that can affect our expectations (for example, our success in previous tries) should affect our performance.
 
2. The effects I’ve seen are small, on the order of 2 percentage points (for example, the probability of a success in some sports task might be 45% if you’re “hot” and 43% otherwise).
 
3. There’s a huge amount of variation, not just between but also among players. Sometimes if you succeed you will stay relaxed and focused, other times you can succeed and get overconfidence.
 
4. Whatever the latest results on particular sports, I can’t see anyone overturning the basic finding of Gilovich, Vallone, and Tversky that players and spectators alike will perceive the hot hand even when it does not exist and dramatically overestimate the magnitude and consistency of any hot-hand phenomenon that does exist.</p><p>4 0.96219701 <a title="566-lda-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-09-Solve_mazes_by_starting_at_the_exit.html">459 andrew gelman stats-2010-12-09-Solve mazes by starting at the exit</a></p>
<p>Introduction: It worked on  this one .
 
Good maze designers know this trick and are careful to design multiple branches in each direction. Back when I was in junior high, I used to make huge mazes, and the basic idea was to anticipate what the solver might try to do and to make the maze difficult by postponing the point at which he would realize a path was going nowhere.  For example, you might have 6 branches:  one dead end, two pairs that form loops going back to the start, and one that is the correct solution.  You do this from both directions and add some twists and turns, and there you are.
 
But the maze designer aiming for the naive solver–the sap who starts from the entrance and goes toward the exit–can simplify matters by just having 6 branches:  five dead ends and one winner.  This sort of thing is easy to solve in the reverse direction.  I’m surprised the Times didn’t do better for their special puzzle issue.</p><p>5 0.96161377 <a title="566-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-21-Class_on_computational_social_science_this_semester%2C_Fridays%2C_1%3A00-3%3A40pm.html">1685 andrew gelman stats-2013-01-21-Class on computational social science this semester, Fridays, 1:00-3:40pm</a></p>
<p>Introduction: Sharad Goel, Jake Hofman, and Sergei Vassilvitskii are teaching this awesome class on computational social science this semester in the applied math department at Columbia.   Hereâ&euro;&trade;s the course info .
 
You should take this course.  These guys are amazing.</p><p>6 0.94000626 <a title="566-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-09-Familial_Linkage_between_Neuropsychiatric_Disorders_and_Intellectual_Interests.html">1160 andrew gelman stats-2012-02-09-Familial Linkage between Neuropsychiatric Disorders and Intellectual Interests</a></p>
<p>7 0.93942899 <a title="566-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-31-Untunable_Metropolis.html">833 andrew gelman stats-2011-07-31-Untunable Metropolis</a></p>
<p>8 0.92984879 <a title="566-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-05-Wouldn%E2%80%99t_it_be_cool_if_Glenn_Hubbard_were_consulting_for_Herbalife_and_I_were_on_the_other_side%3F.html">1708 andrew gelman stats-2013-02-05-Wouldn’t it be cool if Glenn Hubbard were consulting for Herbalife and I were on the other side?</a></p>
<p>9 0.92477578 <a title="566-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-13-Stan%21.html">1855 andrew gelman stats-2013-05-13-Stan!</a></p>
<p>10 0.92165315 <a title="566-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-Visualizing_Distributions_of_Covariance_Matrices.html">1477 andrew gelman stats-2012-08-30-Visualizing Distributions of Covariance Matrices</a></p>
<p>11 0.90457177 <a title="566-lda-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-11-Data_Visualization_vs._Statistical_Graphics.html">407 andrew gelman stats-2010-11-11-Data Visualization vs. Statistical Graphics</a></p>
<p>same-blog 12 0.90099567 <a title="566-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-09-The_boxer%2C_the_wrestler%2C_and_the_coin_flip%2C_again.html">566 andrew gelman stats-2011-02-09-The boxer, the wrestler, and the coin flip, again</a></p>
<p>13 0.8832804 <a title="566-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-21-Baseball%E2%80%99s_greatest_fielders.html">623 andrew gelman stats-2011-03-21-Baseball’s greatest fielders</a></p>
<p>14 0.87956864 <a title="566-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-14-Question_4_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1320 andrew gelman stats-2012-05-14-Question 4 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>15 0.87747365 <a title="566-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-31-He%E2%80%99s_getting_ready_to_write_a_book.html">1783 andrew gelman stats-2013-03-31-He’s getting ready to write a book</a></p>
<p>16 0.87507051 <a title="566-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-21-BDA3_table_of_contents_%28also_a_new_paper_on_visualization%29.html">1991 andrew gelman stats-2013-08-21-BDA3 table of contents (also a new paper on visualization)</a></p>
<p>17 0.86993575 <a title="566-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-17-Statistics_in_a_world_where_nothing_is_random.html">1628 andrew gelman stats-2012-12-17-Statistics in a world where nothing is random</a></p>
<p>18 0.86776245 <a title="566-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-11-Understanding_how_estimates_change_when_you_move_to_a_multilevel_model.html">850 andrew gelman stats-2011-08-11-Understanding how estimates change when you move to a multilevel model</a></p>
<p>19 0.86755586 <a title="566-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-24-Yet_another_Bayesian_job_opportunity.html">231 andrew gelman stats-2010-08-24-Yet another Bayesian job opportunity</a></p>
<p>20 0.85517019 <a title="566-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-24-Recently_in_the_sister_blog.html">1953 andrew gelman stats-2013-07-24-Recently in the sister blog</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
