<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>535 andrew gelman stats-2011-01-24-Bleg: Automatic Differentiation for Log Prob Gradients?</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-535" href="#">andrew_gelman_stats-2011-535</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>535 andrew gelman stats-2011-01-24-Bleg: Automatic Differentiation for Log Prob Gradients?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-535-html" href="http://andrewgelman.com/2011/01/24/bleg_automatic/">html</a></p><p>Introduction: We need help picking out an automatic differentiation package for Hamiltonian Monte Carlo sampling from the posterior of a  generalized linear model with deep interactions.  Specifically, we need to compute gradients for log probability functions with thousands of parameters that involve matrix (determinants, eigenvalues, inverses), stats (distributions), and math (log gamma) functions.  Any suggestions?
    
 The Application: Hybrid Monte Carlo for Posteriors 
 
We’re getting serious about implementing posterior sampling using Hamiltonian Monte Carlo.  HMC speeds up mixing by including gradient information to help guide the Metropolis proposals toward areas  high probability.  In practice, the algorithm requires a handful or  of gradient calculations per sample, but there are many dimensions and the functions are hairy enough we don’t want to compute derivaties by hand.
 

 Auto Diff: Perhaps not What you Think 
 
It may not have been clear to readers of this blog that automatic diffe</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We need help picking out an automatic differentiation package for Hamiltonian Monte Carlo sampling from the posterior of a  generalized linear model with deep interactions. [sent-1, score-0.686]
</p><p>2 Specifically, we need to compute gradients for log probability functions with thousands of parameters that involve matrix (determinants, eigenvalues, inverses), stats (distributions), and math (log gamma) functions. [sent-2, score-0.634]
</p><p>3 In practice, the algorithm requires a handful or  of gradient calculations per sample, but there are many dimensions and the functions are hairy enough we don’t want to compute derivaties by hand. [sent-6, score-0.433]
</p><p>4 Auto Diff: Perhaps not What you Think    It may not have been clear to readers of this blog that automatic differentiation isn’t just an automatic procedure for symbolic differentiation or a finite differences method. [sent-7, score-1.164]
</p><p>5 The way auto differentiation works is by applying the chain rule to all the basic expressions and assignments in your source code. [sent-8, score-0.94]
</p><p>6 Wikipedia’s entry for   automatic differentiation article  provides a nice high-level introduction. [sent-9, score-0.582]
</p><p>7 There are two common approaches, operator overloading and source transforming. [sent-11, score-0.595]
</p><p>8 Source transformers read a program and generate a new program to compute derivatives and the  function at the same time. [sent-12, score-0.369]
</p><p>9 Operator overloading uses  feature of C++ and Fortran 90 that lets you redefine  behavior of functions for your types. [sent-13, score-0.326]
</p><p>10 Thus numerical types are replaced with types representing the function value and the derivative value, and operations with updates on both at the same time. [sent-14, score-0.468]
</p><p>11 It seems to me that source transforming would be way more efficient. [sent-15, score-0.319]
</p><p>12 Auto-diff packages almost all handle the forward direction, which is easier. [sent-18, score-0.285]
</p><p>13 By all accounts, the reverse mode is better for gradients, which is  we need. [sent-19, score-0.364]
</p><p>14 But not all packages implement reverse mode at all, and  not as deeply as they implement forward mode. [sent-20, score-0.813]
</p><p>15 If  understanding the doc correctly, some of the packages  investigated, like  Sacado , a part of the Trilinos scientific computing suite, have BLAS/LAPACK matrix library  with their forward mode auto differentiation but not their reverse mode. [sent-23, score-1.379]
</p><p>16 Many of the open source packages are also tied up with larger suites of software of which they are a part, making it rather confusing to install them and tease apart what they do. [sent-24, score-0.448]
</p><p>17 Some of the approaches to automatic differentiation use disk to write out intermediate results. [sent-29, score-0.832]
</p><p>18 For the operator overloading approaches, which seem to be more  used because of the difficulty of source transforming C++, it’s  not quite as simple as pointing an AD program at your code. [sent-32, score-0.781]
</p><p>19 At the  least, everything I’ve looked at requires the numerical types to  replaced with general templated type variables or variables defined within the AD package itself. [sent-33, score-0.445]
</p><p>20 My head’s spinning from trying to sort things out from auto-generated doc and examples that are not geared toward involved reverse-mode gradient computations. [sent-39, score-0.312]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('differentiation', 0.357), ('automatic', 0.225), ('source', 0.219), ('overloading', 0.217), ('reverse', 0.197), ('mode', 0.167), ('packages', 0.163), ('operator', 0.159), ('auto', 0.156), ('gradient', 0.15), ('ad', 0.135), ('forward', 0.122), ('monte', 0.117), ('matrix', 0.117), ('log', 0.116), ('chain', 0.116), ('compute', 0.11), ('functions', 0.109), ('gradients', 0.106), ('types', 0.104), ('package', 0.104), ('nicely', 0.102), ('transforming', 0.1), ('doc', 0.1), ('approaches', 0.097), ('rule', 0.092), ('numerical', 0.09), ('intermediate', 0.087), ('function', 0.087), ('memory', 0.086), ('hamiltonian', 0.086), ('program', 0.086), ('replaced', 0.083), ('carlo', 0.082), ('implement', 0.082), ('stats', 0.076), ('disk', 0.066), ('overload', 0.066), ('tease', 0.066), ('touching', 0.066), ('suggestions', 0.066), ('application', 0.064), ('requires', 0.064), ('play', 0.063), ('reprise', 0.062), ('geared', 0.062), ('speeds', 0.062), ('tapes', 0.062), ('mileage', 0.06), ('diff', 0.06)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="535-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-24-Bleg%3A_Automatic_Differentiation_for_Log_Prob_Gradients%3F.html">535 andrew gelman stats-2011-01-24-Bleg: Automatic Differentiation for Log Prob Gradients?</a></p>
<p>Introduction: We need help picking out an automatic differentiation package for Hamiltonian Monte Carlo sampling from the posterior of a  generalized linear model with deep interactions.  Specifically, we need to compute gradients for log probability functions with thousands of parameters that involve matrix (determinants, eigenvalues, inverses), stats (distributions), and math (log gamma) functions.  Any suggestions?
    
 The Application: Hybrid Monte Carlo for Posteriors 
 
We’re getting serious about implementing posterior sampling using Hamiltonian Monte Carlo.  HMC speeds up mixing by including gradient information to help guide the Metropolis proposals toward areas  high probability.  In practice, the algorithm requires a handful or  of gradient calculations per sample, but there are many dimensions and the functions are hairy enough we don’t want to compute derivaties by hand.
 

 Auto Diff: Perhaps not What you Think 
 
It may not have been clear to readers of this blog that automatic diffe</p><p>2 0.33817959 <a title="535-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-04-Handy_Matrix_Cheat_Sheet%2C_with_Gradients.html">555 andrew gelman stats-2011-02-04-Handy Matrix Cheat Sheet, with Gradients</a></p>
<p>Introduction: This post is an (unpaid) advertisement for the following extremely useful resource:
  
 Petersen, K. B. and M. S. Pedersen. 2008.   The Matrix Cookbook  .  Tehcnical Report, Technical University of Denmark. 
  
It contains 70+ pages of useful relations and derivations involving matrices.  What grabbed my eye was the computation of gradients for matrix operations ranging from eigenvalues and determinants to multivariate normal density functions.   I had no idea the multivariate normal had such a clean gradient (see section 8).
  

 
We’ve been playing around with  Hamiltonian (aka Hybrid) Monte Carlo  for sampling from the posterior of hierarchical generalized linear models with lots of interactions.  HMC speeds up Metropolis sampling by using the gradient of the log probability to drive samples in the direction of higher probability density, which is particularly useful for correlated parameters that mix slowly with standard Gibbs sampling.   Matt “III” Hoffman ‘s already got it workin</p><p>3 0.19387798 <a title="535-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-06-Stan_1.2.0_and_RStan_1.2.0.html">1753 andrew gelman stats-2013-03-06-Stan 1.2.0 and RStan 1.2.0</a></p>
<p>Introduction: Stan 1.2.0 and RStan 1.2.0 are now available for download. See:
  
  http://mc-stan.org/ 
   
Here are the highlights.
  Full Mass Matrix Estimation during Warmup  
Yuanjun Gao, a first-year grad student here at Columbia (!), built a regularized mass-matrix estimator.   This helps for posteriors with high correlation among parameters and varying scales.  We’re still testing this ourselves, so the estimation procedure may change in the future (don’t worry — it satisfies detailed balance as is, but we might be able to make it more computationally efficient in terms of time per effective sample).
 
It’s not the default option.  The major reason is the matrix operations required are expensive, raising the algorithm cost to    , where   is the average number of leapfrog steps,   is the number of iterations, and   is the number of parameters.
 
Yuanjun did a great job with the Cholesky factorizations and implemented this about as efficiently as is possible. (His homework for Andrew’s class w</p><p>4 0.16746351 <a title="535-tfidf-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-18-Derivative-based_MCMC_as_a_breakthrough_technique_for_implementing_Bayesian_statistics.html">419 andrew gelman stats-2010-11-18-Derivative-based MCMC as a breakthrough technique for implementing Bayesian statistics</a></p>
<p>Introduction: John Salvatier pointed me to  this blog  on derivative based MCMC algorithms (also sometimes called “hybrid” or “Hamiltonian” Monte Carlo) and automatic differentiation as the future of MCMC.
 
This all makes sense to me and is consistent both with my mathematical intuition from studying Metropolis algorithms and my experience with Matt using hybrid MCMC when fitting hierarchical spline models. In particular, I agree with Salvatier’s point about the potential for computation of analytic derivatives of the log-density function.  As long as we’re mostly snapping together our models using analytically-simple pieces, the same part of the program that handles the computation of log-posterior densities should also be able to compute derivatives analytically.
 
I’ve been a big fan of automatic derivative-based MCMC methods since I started hearing about them a couple years ago (I’m thinking of the DREAM project and of Mark Girolami’s paper), and I too wonder why they haven’t been used more.  I</p><p>5 0.15528587 <a title="535-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-20-Stan_at_Google_this_Thurs_and_at_Berkeley_this_Fri_noon.html">1772 andrew gelman stats-2013-03-20-Stan at Google this Thurs and at Berkeley this Fri noon</a></p>
<p>Introduction: Michael Betancourt  will be speaking at Google and at the University of California, Berkeley.  The Google talk is closed to outsiders (but if you work at Google, you should go!); the Berkeley talk is open to all:
  
Friday March 22, 12:10 pm, Evans Hall 1011.


Title of talk:  Stan : Practical Bayesian Inference with Hamiltonian Monte Carlo


Abstract: Practical implementations of Bayesian inference are often limited to approximation methods that only slowly explore the posterior distribution.  By taking advantage of the curvature of the posterior, however, Hamiltonian Monte Carlo (HMC) efficiently explores even the most highly contorted distributions.  In this talk I will review the foundations of and recent developments within HMC, concluding with a discussion of Stan, a powerful inference engine that utilizes HMC, automatic differentiation, and adaptive methods to minimize user input.
  
This is cool stuff.  And heâ&euro;&trade;ll be showing the whirlpool movie!</p><p>6 0.14264229 <a title="535-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-15-Forward_causal_reasoning_statements_are_about_estimation%3B_reverse_causal_questions_are_about_model_checking_and_hypothesis_generation.html">1939 andrew gelman stats-2013-07-15-Forward causal reasoning statements are about estimation; reverse causal questions are about model checking and hypothesis generation</a></p>
<p>7 0.13670975 <a title="535-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-06-The_new_Stan_1.1.1%2C_featuring_Gaussian_processes%21.html">1710 andrew gelman stats-2013-02-06-The new Stan 1.1.1, featuring Gaussian processes!</a></p>
<p>8 0.13602212 <a title="535-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-04-Stan_in_L.A._this_Wed_3%3A30pm.html">1749 andrew gelman stats-2013-03-04-Stan in L.A. this Wed 3:30pm</a></p>
<p>9 0.1329211 <a title="535-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-12-Stan_1.3.0_and_RStan_1.3.0_Ready_for_Action.html">1799 andrew gelman stats-2013-04-12-Stan 1.3.0 and RStan 1.3.0 Ready for Action</a></p>
<p>10 0.12857002 <a title="535-tfidf-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-01-%E2%80%9CThough_They_May_Be_Unaware%2C_Newlyweds_Implicitly_Know_Whether_Their_Marriage_Will_Be_Satisfying%E2%80%9D.html">2156 andrew gelman stats-2014-01-01-“Though They May Be Unaware, Newlyweds Implicitly Know Whether Their Marriage Will Be Satisfying”</a></p>
<p>11 0.12606506 <a title="535-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-30-Stan_uses_Nuts%21.html">1036 andrew gelman stats-2011-11-30-Stan uses Nuts!</a></p>
<p>12 0.12103468 <a title="535-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-23-Combining_survey_data_obtained_using_different_modes_of_sampling.html">777 andrew gelman stats-2011-06-23-Combining survey data obtained using different modes of sampling</a></p>
<p>13 0.11973172 <a title="535-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<p>14 0.11954989 <a title="535-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-21-Lessons_learned_from_a_recent_R_package_submission.html">1134 andrew gelman stats-2012-01-21-Lessons learned from a recent R package submission</a></p>
<p>15 0.11418831 <a title="535-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-19-R_package_for_Bayes_factors.html">1682 andrew gelman stats-2013-01-19-R package for Bayes factors</a></p>
<p>16 0.1098533 <a title="535-tfidf-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>17 0.10797872 <a title="535-tfidf-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-17-Getting_arm_and_lme4_running_on_the_Mac.html">347 andrew gelman stats-2010-10-17-Getting arm and lme4 running on the Mac</a></p>
<p>18 0.10661049 <a title="535-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-17-Stan_and_RStan_1.1.0.html">1627 andrew gelman stats-2012-12-17-Stan and RStan 1.1.0</a></p>
<p>19 0.10230149 <a title="535-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-12-Samplers_for_Big_Science%3A__emcee_and_BAT.html">2020 andrew gelman stats-2013-09-12-Samplers for Big Science:  emcee and BAT</a></p>
<p>20 0.10031677 <a title="535-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-23-Learning_Differential_Geometry_for_Hamiltonian_Monte_Carlo.html">1339 andrew gelman stats-2012-05-23-Learning Differential Geometry for Hamiltonian Monte Carlo</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.148), (1, 0.062), (2, -0.013), (3, 0.037), (4, 0.073), (5, 0.044), (6, 0.025), (7, -0.097), (8, -0.04), (9, -0.05), (10, -0.066), (11, -0.037), (12, -0.053), (13, -0.037), (14, 0.013), (15, -0.017), (16, 0.006), (17, 0.019), (18, 0.0), (19, -0.017), (20, 0.031), (21, 0.017), (22, -0.007), (23, 0.03), (24, 0.031), (25, 0.046), (26, -0.003), (27, 0.117), (28, 0.066), (29, -0.005), (30, 0.007), (31, 0.042), (32, 0.03), (33, 0.017), (34, 0.036), (35, -0.084), (36, -0.016), (37, 0.031), (38, -0.023), (39, 0.03), (40, -0.036), (41, -0.018), (42, -0.018), (43, -0.01), (44, 0.026), (45, -0.029), (46, -0.068), (47, 0.043), (48, 0.067), (49, -0.079)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9562102 <a title="535-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-24-Bleg%3A_Automatic_Differentiation_for_Log_Prob_Gradients%3F.html">535 andrew gelman stats-2011-01-24-Bleg: Automatic Differentiation for Log Prob Gradients?</a></p>
<p>Introduction: We need help picking out an automatic differentiation package for Hamiltonian Monte Carlo sampling from the posterior of a  generalized linear model with deep interactions.  Specifically, we need to compute gradients for log probability functions with thousands of parameters that involve matrix (determinants, eigenvalues, inverses), stats (distributions), and math (log gamma) functions.  Any suggestions?
    
 The Application: Hybrid Monte Carlo for Posteriors 
 
We’re getting serious about implementing posterior sampling using Hamiltonian Monte Carlo.  HMC speeds up mixing by including gradient information to help guide the Metropolis proposals toward areas  high probability.  In practice, the algorithm requires a handful or  of gradient calculations per sample, but there are many dimensions and the functions are hairy enough we don’t want to compute derivaties by hand.
 

 Auto Diff: Perhaps not What you Think 
 
It may not have been clear to readers of this blog that automatic diffe</p><p>2 0.85149771 <a title="535-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-04-Handy_Matrix_Cheat_Sheet%2C_with_Gradients.html">555 andrew gelman stats-2011-02-04-Handy Matrix Cheat Sheet, with Gradients</a></p>
<p>Introduction: This post is an (unpaid) advertisement for the following extremely useful resource:
  
 Petersen, K. B. and M. S. Pedersen. 2008.   The Matrix Cookbook  .  Tehcnical Report, Technical University of Denmark. 
  
It contains 70+ pages of useful relations and derivations involving matrices.  What grabbed my eye was the computation of gradients for matrix operations ranging from eigenvalues and determinants to multivariate normal density functions.   I had no idea the multivariate normal had such a clean gradient (see section 8).
  

 
We’ve been playing around with  Hamiltonian (aka Hybrid) Monte Carlo  for sampling from the posterior of hierarchical generalized linear models with lots of interactions.  HMC speeds up Metropolis sampling by using the gradient of the log probability to drive samples in the direction of higher probability density, which is particularly useful for correlated parameters that mix slowly with standard Gibbs sampling.   Matt “III” Hoffman ‘s already got it workin</p><p>3 0.80771118 <a title="535-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<p>Introduction: Tomas Iesmantas had  asked me  for advice on a regression problem with 50 parameters, and I’d recommended Hamiltonian Monte Carlo.
 
A few weeks later he reported back: 
  
  
After trying several modifications (HMC for all parameters at once, HMC just for first level parameters and Riemman manifold Hamiltonian Monte Carlo method), I finally got it running with HMC just for first level parameters and for others using direct sampling, since conditional distributions turned out to have closed form.


However, even in this case it is quite tricky, since I had to employ mass matrix and not just diagonal but at the beginning of algorithm generated it randomly (ensuring it is positive definite). Such random generation of mass matrix is quite blind step, but it proved to be quite helpful.


Riemman manifold HMC is quite vagarious, or to be more specific, metric of manifold is very sensitive. In my model log-likelihood I had exponents and values of metrics matrix elements was very large and wh</p><p>4 0.80355275 <a title="535-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-06-Stan_1.2.0_and_RStan_1.2.0.html">1753 andrew gelman stats-2013-03-06-Stan 1.2.0 and RStan 1.2.0</a></p>
<p>Introduction: Stan 1.2.0 and RStan 1.2.0 are now available for download. See:
  
  http://mc-stan.org/ 
   
Here are the highlights.
  Full Mass Matrix Estimation during Warmup  
Yuanjun Gao, a first-year grad student here at Columbia (!), built a regularized mass-matrix estimator.   This helps for posteriors with high correlation among parameters and varying scales.  We’re still testing this ourselves, so the estimation procedure may change in the future (don’t worry — it satisfies detailed balance as is, but we might be able to make it more computationally efficient in terms of time per effective sample).
 
It’s not the default option.  The major reason is the matrix operations required are expensive, raising the algorithm cost to    , where   is the average number of leapfrog steps,   is the number of iterations, and   is the number of parameters.
 
Yuanjun did a great job with the Cholesky factorizations and implemented this about as efficiently as is possible. (His homework for Andrew’s class w</p><p>5 0.7752862 <a title="535-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-12-Stan_1.3.0_and_RStan_1.3.0_Ready_for_Action.html">1799 andrew gelman stats-2013-04-12-Stan 1.3.0 and RStan 1.3.0 Ready for Action</a></p>
<p>Introduction: The Stan Development Team is happy to announce that Stan 1.3.0 and RStan 1.3.0 are available for download. Follow the links on:
  
 Stan home page:   http://mc-stan.org/ 
   
Please let us know if you have problems updating.
 
Hereâ&euro;&trade;s the full set of release notes.
  
v1.3.0 (12 April 2013)
======================================================================
Enhancements
----------------------------------

Modeling Language
* forward sampling (random draws from distributions)
  in generated quantities
* better error messages in parser
* new distributions: 
    + exp_mod_normal
    + gumbel 
    + skew_normal
* new special functions: 
    + owenst
* new broadcast (repetition) functions for vectors, arrays, matrices
    + rep_arrray
    + rep_matrix
    + rep_row_vector
    + rep_vector    

Command-Line
* added option to display autocorrelations in the command-line program
  to print output
* changed default point estimation routine from the command line to</p><p>6 0.77183956 <a title="535-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-06-The_new_Stan_1.1.1%2C_featuring_Gaussian_processes%21.html">1710 andrew gelman stats-2013-02-06-The new Stan 1.1.1, featuring Gaussian processes!</a></p>
<p>7 0.73568022 <a title="535-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-23-Learning_Differential_Geometry_for_Hamiltonian_Monte_Carlo.html">1339 andrew gelman stats-2012-05-23-Learning Differential Geometry for Hamiltonian Monte Carlo</a></p>
<p>8 0.73270667 <a title="535-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-30-Stan_uses_Nuts%21.html">1036 andrew gelman stats-2011-11-30-Stan uses Nuts!</a></p>
<p>9 0.69459987 <a title="535-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-Visualizing_Distributions_of_Covariance_Matrices.html">1477 andrew gelman stats-2012-08-30-Visualizing Distributions of Covariance Matrices</a></p>
<p>10 0.69189709 <a title="535-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-21-Handbook_of_Markov_Chain_Monte_Carlo.html">674 andrew gelman stats-2011-04-21-Handbook of Markov Chain Monte Carlo</a></p>
<p>11 0.68988889 <a title="535-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-30-Stan_Project%3A__Continuous_Relaxations_for_Discrete_MRFs.html">2003 andrew gelman stats-2013-08-30-Stan Project:  Continuous Relaxations for Discrete MRFs</a></p>
<p>12 0.68664598 <a title="535-lsi-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-Running_into_a_Stan_Reference_by_Accident.html">2231 andrew gelman stats-2014-03-03-Running into a Stan Reference by Accident</a></p>
<p>13 0.68137878 <a title="535-lsi-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-12-%E2%80%9CThe_results_%28not_shown%29_._._.%E2%80%9D.html">2332 andrew gelman stats-2014-05-12-“The results (not shown) . . .”</a></p>
<p>14 0.67630666 <a title="535-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-21-BDA3_table_of_contents_%28also_a_new_paper_on_visualization%29.html">1991 andrew gelman stats-2013-08-21-BDA3 table of contents (also a new paper on visualization)</a></p>
<p>15 0.66298819 <a title="535-lsi-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-18-Derivative-based_MCMC_as_a_breakthrough_technique_for_implementing_Bayesian_statistics.html">419 andrew gelman stats-2010-11-18-Derivative-based MCMC as a breakthrough technique for implementing Bayesian statistics</a></p>
<p>16 0.65718526 <a title="535-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-21-Lessons_learned_from_a_recent_R_package_submission.html">1134 andrew gelman stats-2012-01-21-Lessons learned from a recent R package submission</a></p>
<p>17 0.64923364 <a title="535-lsi-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-21-Random_matrices_in_the_news.html">2258 andrew gelman stats-2014-03-21-Random matrices in the news</a></p>
<p>18 0.63268942 <a title="535-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-19-R_package_for_Bayes_factors.html">1682 andrew gelman stats-2013-01-19-R package for Bayes factors</a></p>
<p>19 0.63012451 <a title="535-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-17-Stan_and_RStan_1.1.0.html">1627 andrew gelman stats-2012-12-17-Stan and RStan 1.1.0</a></p>
<p>20 0.62922186 <a title="535-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-12-Samplers_for_Big_Science%3A__emcee_and_BAT.html">2020 andrew gelman stats-2013-09-12-Samplers for Big Science:  emcee and BAT</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(4, 0.014), (5, 0.014), (13, 0.025), (14, 0.013), (16, 0.062), (21, 0.016), (24, 0.121), (27, 0.051), (29, 0.069), (35, 0.029), (36, 0.016), (45, 0.028), (60, 0.022), (61, 0.018), (82, 0.057), (86, 0.052), (89, 0.021), (99, 0.217)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95999694 <a title="535-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-24-Bleg%3A_Automatic_Differentiation_for_Log_Prob_Gradients%3F.html">535 andrew gelman stats-2011-01-24-Bleg: Automatic Differentiation for Log Prob Gradients?</a></p>
<p>Introduction: We need help picking out an automatic differentiation package for Hamiltonian Monte Carlo sampling from the posterior of a  generalized linear model with deep interactions.  Specifically, we need to compute gradients for log probability functions with thousands of parameters that involve matrix (determinants, eigenvalues, inverses), stats (distributions), and math (log gamma) functions.  Any suggestions?
    
 The Application: Hybrid Monte Carlo for Posteriors 
 
We’re getting serious about implementing posterior sampling using Hamiltonian Monte Carlo.  HMC speeds up mixing by including gradient information to help guide the Metropolis proposals toward areas  high probability.  In practice, the algorithm requires a handful or  of gradient calculations per sample, but there are many dimensions and the functions are hairy enough we don’t want to compute derivaties by hand.
 

 Auto Diff: Perhaps not What you Think 
 
It may not have been clear to readers of this blog that automatic diffe</p><p>2 0.92166615 <a title="535-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-A_poll_that_throws_away_data%3F%3F%3F.html">1940 andrew gelman stats-2013-07-16-A poll that throws away data???</a></p>
<p>Introduction: Mark Blumenthal writes: 
  
  
What do you think about the “random rejection” method used by PPP that was attacked at some length today by a Republican pollster.  Our just published post on the debate  includes all the details as I know them. The  Storify of Martino’s tweets  has some additional data tables linked to toward the end.  


Also, more specifically, setting aside Martino’s suggestion of manipulation (which is also quite possible with post-stratification weights), would the PPP method introduce more potential random error than weighting? 
  
From Blumenthal’s blog:
  
B.J. Martino, a senior vice president at the Republican polling firm The Tarrance Group, went on an 30-minute Twitter rant on Tuesday questioning the unorthodox method used by PPP [Public Policy Polling] to select samples and weight data: “Looking at @ppppolls new VA SW. Wondering how many interviews they discarded to get down to 601 completes? Because @ppppolls discards a LOT of interviews. Of 64,811 conducted</p><p>3 0.91632432 <a title="535-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-10-Chris_Chabris_is_irritated_by_Malcolm_Gladwell.html">2057 andrew gelman stats-2013-10-10-Chris Chabris is irritated by Malcolm Gladwell</a></p>
<p>Introduction: Christopher Chabris  reviewed  the new book by Malcolm Gladwell:
  
One thing “David and Goliath” shows is that Mr. Gladwell has not changed his own strategy, despite serious criticism of his prior work. What he presents are mostly just intriguing possibilities and musings about human behavior, but what his publisher sells them as, and what his readers may incorrectly take them for, are lawful, causal rules that explain how the world really works. Mr. Gladwell should acknowledge when he is speculating or working with thin evidentiary soup. Yet far from abandoning his hand or even standing pat, Mr. Gladwell has doubled down. This will surely bring more success to a Goliath of nonfiction writing, but not to his readers.
  
Afterward he blogged some further thoughts about the popular popular science writer.   Good stuff .  Chabris has a thoughtful explanation of why the “Gladwell is just an entertainer” alibi doesn’t work for him (Chabris).  Some of his discussion reminds me of my  articl</p><p>4 0.91607368 <a title="535-lda-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-21-Workshop_on_science_communication_for_graduate_students.html">1687 andrew gelman stats-2013-01-21-Workshop on science communication for graduate students</a></p>
<p>Introduction: Nathan Sanders writes: 
  
  
Applications are now open for the Communicating Science 2013 workshop (http://workshop.astrobites.com/), to be held in Cambridge, MA on June 13-15th, 2013.  Graduate students at US institutions in all fields of science and engineering are encouraged to apply â&euro;&ldquo; funding is available for travel expenses and accommodations.


The application can be found here: http://workshop.astrobites.org/application


Participants will build the communication skills that technical professionals need to express complex ideas to their peers, experts in other fields, and the general public.  There will be panel discussions on the following topics:


* Engaging Non-Scientific Audiences 
* Science Writing for a Cause 
* Communicating Science Through Fiction 
* Sharing Science with Scientists 
* The World of Non-Academic Publishing 
* Communicating using Multimedia and the Web


In addition to these discussions, ample time is allotted for interacting with the experts and with att</p><p>5 0.91407442 <a title="535-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-30-%3F%3F%3F.html">2118 andrew gelman stats-2013-11-30-???</a></p>
<p>Introduction: I received the following unsolicited email, subject line Technology and Engineering Research:
  
Dear Editor 


We have done research in some of the cutting edge technology and engineering field and would like to if you will be able to write about it in your news section. 
Our Primarily research focus on building high performance systems that are helping in social networks, web, finding disease, cancer and sports using BIG DATA . Hope to hear from you some time soon. 


Thanks, 


***, PhD 
Chartered Scientist 
IBM Corportation 
***@us.ibm.com 
916 *** ****
  
I thought IBM was a professional operation—don’t they have their own public relations department?</p><p>6 0.91349751 <a title="535-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>7 0.91135222 <a title="535-lda-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-13-Test_scores_and_grades_predict_job_performance_%28but_maybe_not_at_Google%29.html">1980 andrew gelman stats-2013-08-13-Test scores and grades predict job performance (but maybe not at Google)</a></p>
<p>8 0.91114676 <a title="535-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-18-You%E2%80%99ll_get_a_high_Type_S_error_rate_if_you_use_classical_statistical_methods_to_analyze_data_from_underpowered_studies.html">1944 andrew gelman stats-2013-07-18-You’ll get a high Type S error rate if you use classical statistical methods to analyze data from underpowered studies</a></p>
<p>9 0.91100943 <a title="535-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-04-Scientific_communication_that_accords_you_%E2%80%9Cthe_basic_human_dignity_of_allowing_you_to_draw_your_own_conclusions%E2%80%9D.html">2051 andrew gelman stats-2013-10-04-Scientific communication that accords you “the basic human dignity of allowing you to draw your own conclusions”</a></p>
<p>10 0.91050333 <a title="535-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-13-Flexibility_is_good.html">2133 andrew gelman stats-2013-12-13-Flexibility is good</a></p>
<p>11 0.90986669 <a title="535-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-31-Bayes%3A_radical%2C_liberal%2C_or_conservative%3F.html">639 andrew gelman stats-2011-03-31-Bayes: radical, liberal, or conservative?</a></p>
<p>12 0.9093107 <a title="535-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-07-Peer_pressure%2C_selection%2C_and_educational_reform.html">326 andrew gelman stats-2010-10-07-Peer pressure, selection, and educational reform</a></p>
<p>13 0.90869057 <a title="535-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-31-Response_by_Jessica_Tracy_and_Alec_Beall_to_my_critique_of_the_methods_in_their_paper%2C_%E2%80%9CWomen_Are_More_Likely_to_Wear_Red_or_Pink_at_Peak_Fertility%E2%80%9D.html">1963 andrew gelman stats-2013-07-31-Response by Jessica Tracy and Alec Beall to my critique of the methods in their paper, “Women Are More Likely to Wear Red or Pink at Peak Fertility”</a></p>
<p>14 0.9085297 <a title="535-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-30-Stan_Project%3A__Continuous_Relaxations_for_Discrete_MRFs.html">2003 andrew gelman stats-2013-08-30-Stan Project:  Continuous Relaxations for Discrete MRFs</a></p>
<p>15 0.90624785 <a title="535-lda-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-10-Update_on_Levitt_paper_on_child_car_seats.html">1491 andrew gelman stats-2012-09-10-Update on Levitt paper on child car seats</a></p>
<p>16 0.90594941 <a title="535-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-31-Dispute_about_ethics_of_data_sharing.html">1238 andrew gelman stats-2012-03-31-Dispute about ethics of data sharing</a></p>
<p>17 0.90570223 <a title="535-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-07-Minor-league_Stats_Predict_Major-league_Performance%2C_Sarah_Palin%2C_and_Some_Differences_Between_Baseball_and_Politics.html">652 andrew gelman stats-2011-04-07-Minor-league Stats Predict Major-league Performance, Sarah Palin, and Some Differences Between Baseball and Politics</a></p>
<p>18 0.90568388 <a title="535-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-17-Macro_causality.html">807 andrew gelman stats-2011-07-17-Macro causality</a></p>
<p>19 0.90507209 <a title="535-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-03-More_on_that_Dartmouth_health_care_study.html">67 andrew gelman stats-2010-06-03-More on that Dartmouth health care study</a></p>
<p>20 0.90482944 <a title="535-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-15-Static_sensitivity_analysis.html">804 andrew gelman stats-2011-07-15-Static sensitivity analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
