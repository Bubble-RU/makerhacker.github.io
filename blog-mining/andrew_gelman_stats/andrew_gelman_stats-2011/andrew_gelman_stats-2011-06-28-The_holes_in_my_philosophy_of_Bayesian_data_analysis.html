<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-781" href="#">andrew_gelman_stats-2011-781</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-781-html" href="http://andrewgelman.com/2011/06/28/the_holes_in_my/">html</a></p><p>Introduction: I’ve been writing a lot about my philosophy of Bayesian statistics and how it fits into Popper’s ideas about falsification and Kuhn’s ideas about scientific revolutions.
 
 Here’s  my long, somewhat technical paper with Cosma Shalizi. 
 Here’s  our shorter overview for the volume on the philosophy of social science. 
 Here’s  my latest try (for an online symposium), focusing on the key issues.
 
I’m pretty happy with my approach–the familiar idea that Bayesian data analysis iterates the three steps of model building, inference, and model checking–but it does have some unresolved (maybe unresolvable) problems.  Here are a couple mentioned in the third of the above links.
 
Consider a simple model with independent data y_1, y_2, .., y_10 ~ N(θ,σ^2), with a prior distribution θ ~ N(0,10^2) and σ known and taking on some value of approximately 10. Inference about μ is straightforward, as is model checking, whether based on graphs or numerical summaries such as the sample variance and skewn</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I’m pretty happy with my approach–the familiar idea that Bayesian data analysis iterates the three steps of model building, inference, and model checking–but it does have some unresolved (maybe unresolvable) problems. [sent-5, score-1.089]
</p><p>2 Consider a simple model with independent data y_1, y_2, . [sent-7, score-0.374]
</p><p>3 , y_10 ~ N(θ,σ^2), with a prior distribution θ ~ N(0,10^2) and σ known and taking on some value of approximately 10. [sent-9, score-0.314]
</p><p>4 Inference about μ is straightforward, as is model checking, whether based on graphs or numerical summaries such as the sample variance and skewness. [sent-10, score-0.356]
</p><p>5 Thus θ = 0 or 1 or 2 or 3 or … or -1 or -2 or -3 or …, and with a discrete prior distribution formed by the discrete approximation to the N(0,10^2) distribution. [sent-12, score-0.782]
</p><p>6 In practice, with the sample size and parameters as defined above, the inferences are essentially unchanged from the continuous case, as we have defined θ on a suitably tight grid. [sent-13, score-0.335]
</p><p>7 But from my philosophical position, the discrete model is completely different: I have already written that I do not like to choose or average over a discrete set of models. [sent-14, score-0.88]
</p><p>8 I understand the distinction in extreme cases–they correspond to the difference between normal science and potential scientific revolutions–but the demarcation does not cleanly align with whether a model is discrete or continuous. [sent-16, score-0.643]
</p><p>9 Another incoherence in Bayesian data analysis, as I practice it, arises after a model check. [sent-17, score-0.544]
</p><p>10 I am deductively Bayesian when performing inference and checking within a model, but I must go outside this framework when making decisions about whether and how to alter my model. [sent-20, score-0.321]
</p><p>11 In my defense, I see comparable incoherence in all other statistical philosophies:   - Subjective Bayesianism appears fully coherent but falls apart when you examine the assumption that your prior distribution can completely reflect prior knowledge. [sent-21, score-0.83]
</p><p>12 If you could really express your uncertainty as a prior distribution, then you could just as well observe data and directly write your subjective posterior distribution, and there would be no need for statistical analysis at all. [sent-23, score-0.73]
</p><p>13 Robust analysis attempts to account for uncertainty about model specification but relies on additional assumptions such as independence. [sent-25, score-0.461]
</p><p>14 My point here is not to say that my preferred methods are better than others but rather to couple my admission of philosophical incoherence with a reminder that there is no available coherent alternative. [sent-27, score-0.388]
</p><p>15 The straight-up subjective Bayes approach requires the AI to already have all possible models specified in its database with appropriate prior probabilities. [sent-29, score-0.639]
</p><p>16 But my approach requires models to be generated on the fly (in response to earlier model checks and the appearance of new data). [sent-31, score-0.519]
</p><p>17 It’s clear enough how an AI could perform inference on a specified graphical model (or on a mixture of such models); it’s not so clear how an AI could do model checking. [sent-32, score-1.023]
</p><p>18 When I do the three steps of Bayesian data analysis, human input is needed to interpret graphs and decide on model improvements. [sent-33, score-0.705]
</p><p>19 Cosma Shalizi writes:    If your graphical model does not have all possible edges, then there are conditional independencies which could be checked mechanically (and there are programs which do things like that). [sent-40, score-0.557]
</p><p>20 But if the program has the ability to check the model (or at least to pass the relevant graphs on to the homunculus), then I would think that working with conditional independence approximations could be a useful way to move forward. [sent-44, score-0.592]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ai', 0.333), ('model', 0.278), ('discrete', 0.234), ('prior', 0.192), ('incoherence', 0.17), ('bayesian', 0.165), ('inference', 0.157), ('philosophical', 0.134), ('distribution', 0.122), ('analysis', 0.121), ('subjective', 0.115), ('steps', 0.113), ('models', 0.106), ('parametric', 0.104), ('defined', 0.1), ('independence', 0.098), ('checking', 0.098), ('data', 0.096), ('cosma', 0.092), ('specified', 0.091), ('consider', 0.087), ('coherent', 0.084), ('graphs', 0.078), ('case', 0.076), ('graphical', 0.075), ('philosophy', 0.075), ('decide', 0.072), ('could', 0.072), ('indexing', 0.071), ('iterates', 0.071), ('unchanged', 0.071), ('fully', 0.07), ('three', 0.068), ('requires', 0.068), ('required', 0.068), ('approach', 0.067), ('cleanly', 0.066), ('mechanically', 0.066), ('zeroes', 0.066), ('invariance', 0.066), ('homunculus', 0.066), ('conditional', 0.066), ('outside', 0.066), ('normal', 0.065), ('classical', 0.065), ('illegitimate', 0.064), ('suitably', 0.064), ('unresolved', 0.064), ('uncertainty', 0.062), ('automating', 0.061)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="781-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>Introduction: I’ve been writing a lot about my philosophy of Bayesian statistics and how it fits into Popper’s ideas about falsification and Kuhn’s ideas about scientific revolutions.
 
 Here’s  my long, somewhat technical paper with Cosma Shalizi. 
 Here’s  our shorter overview for the volume on the philosophy of social science. 
 Here’s  my latest try (for an online symposium), focusing on the key issues.
 
I’m pretty happy with my approach–the familiar idea that Bayesian data analysis iterates the three steps of model building, inference, and model checking–but it does have some unresolved (maybe unresolvable) problems.  Here are a couple mentioned in the third of the above links.
 
Consider a simple model with independent data y_1, y_2, .., y_10 ~ N(θ,σ^2), with a prior distribution θ ~ N(0,10^2) and σ known and taking on some value of approximately 10. Inference about μ is straightforward, as is model checking, whether based on graphs or numerical summaries such as the sample variance and skewn</p><p>2 0.34730765 <a title="781-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-11-Why_waste_time_philosophizing%3F.html">1719 andrew gelman stats-2013-02-11-Why waste time philosophizing?</a></p>
<p>Introduction: I’ll answer the above question after first sharing some background and history on the the philosophy of Bayesian statistics, which appeared at the end of our  rejoinder  to the discussion to which I  linked  the other day:
  
When we were beginning our statistical educations, the word ‘Bayesian’ conveyed membership in an obscure cult. Statisticians who were outside the charmed circle could ignore the Bayesian subfield, while Bayesians themselves tended to be either apologetic or brazenly defiant. These two extremes manifested themselves in ever more elaborate proposals for non-informative priors, on the one hand, and declarations of the purity of subjective probability, on the other.


Much has changed in the past 30 years. ‘Bayesian’ is now often used in casual scientific parlance as a synonym for ‘rational’, the anti-Bayesians have mostly disappeared, and non-Bayesian statisticians feel the need to keep up with developments in Bayesian modelling and computation. Bayesians themselves</p><p>3 0.33337092 <a title="781-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Kind_of_Bayesian.html">811 andrew gelman stats-2011-07-20-Kind of Bayesian</a></p>
<p>Introduction: Astrophysicist Andrew Jaffe pointed me to  this and discussion  of my  philosophy  of statistics (which is, in turn, my rational reconstruction of the statistical practice of Bayesians such as Rubin and Jaynes).  Jaffe’s summary is fair enough and I only disagree in a few points: 
   
1.  Jaffe writes:
  
Subjective probability, at least the way it is actually used by practicing scientists, is a sort of “as-if” subjectivity — how would an agent reason if her beliefs were reflected in a certain set of probability distributions? This is why when I discuss probability I try to make the pedantic point that all probabilities are conditional, at least on some background prior information or context.
  
I agree, and my problem with the usual procedures used for Bayesian model comparison and Bayesian model averaging is not that these approaches are subjective but that the particular models being considered don’t make sense.  I’m thinking of the sorts of models that say the truth is either A or</p><p>4 0.31681457 <a title="781-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>Introduction: Konrad Scheffler writes:
  
I was interested by your  paper  “Induction and deduction in Bayesian data analysis” and was wondering if you would entertain a few questions:
  
  
  
 – Under the banner of objective Bayesianism, I would posit something like this as a description of Bayesian inference:


“Objective Bayesian probability is not a degree of belief (which would necessarily be subjective) but a measure of the plausibility of a hypothesis, conditional on a formally specified information state. One way of specifying a formal information state is to specify a model, which involves specifying both a prior distribution (typically for a set of unobserved variables) and a likelihood function (typically for a set of observed variables, conditioned on the values of the unobserved variables). Bayesian inference involves calculating the objective degree of plausibility of a hypothesis (typically the truth value of the hypothesis is a function of the variables mentioned above) given such a</p><p>5 0.29268578 <a title="781-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-25-Incoherence_of_Bayesian_data_analysis.html">1510 andrew gelman stats-2012-09-25-Incoherence of Bayesian data analysis</a></p>
<p>Introduction: Hogg writes:
  
At the end  this article  you wonder about consistency.  Have you ever considered the possibility that utility might resolve some of the problems?  I have no idea if it 
would—I am not advocating that position—I just get some kind of intuition from phrases like “Judgment is required to decide…”. Perhaps there is a coherent and objective description of what is—or could be—done under a coherent “utility” model (like a utility that could be objectively agreed upon and computed).  Utilities are usually subjective—true—but priors are usually subjective too.
  
My reply:
 
I’m happy to think about utility, for some particular problem or class of problems going to the effort of assigning costs and benefits to different outcomes.  I agree that a utility analysis, even if (necessarily) imperfect, can usefully focus discussion.  For example, if a statistical method for selecting variables is justified on the basis of cost, I like the idea of attempting to quantify the costs of ga</p><p>6 0.28767252 <a title="781-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>7 0.26371762 <a title="781-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-09-Coming_to_agreement_on_philosophy_of_statistics.html">1205 andrew gelman stats-2012-03-09-Coming to agreement on philosophy of statistics</a></p>
<p>8 0.26252368 <a title="781-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>9 0.25563118 <a title="781-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>10 0.25120941 <a title="781-tfidf-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-05-Does_posterior_predictive_model_checking_fit_with_the_operational_subjective_approach%3F.html">320 andrew gelman stats-2010-10-05-Does posterior predictive model checking fit with the operational subjective approach?</a></p>
<p>11 0.24737117 <a title="781-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>12 0.24511315 <a title="781-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>13 0.24331826 <a title="781-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-07-Philosophy_and_the_practice_of_Bayesian_statistics_%28with_all_the_discussions%21%29.html">1712 andrew gelman stats-2013-02-07-Philosophy and the practice of Bayesian statistics (with all the discussions!)</a></p>
<p>14 0.23286332 <a title="781-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>15 0.23018208 <a title="781-tfidf-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Useful_models%2C_model_checking%2C_and_external_validation%3A__a_mini-discussion.html">244 andrew gelman stats-2010-08-30-Useful models, model checking, and external validation:  a mini-discussion</a></p>
<p>16 0.2243019 <a title="781-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>17 0.21734972 <a title="781-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>18 0.21649195 <a title="781-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-24-Untangling_the_Jeffreys-Lindley_paradox.html">1182 andrew gelman stats-2012-02-24-Untangling the Jeffreys-Lindley paradox</a></p>
<p>19 0.21554473 <a title="781-tfidf-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-26-Philosophy_and_the_practice_of_Bayesian_statistics.html">110 andrew gelman stats-2010-06-26-Philosophy and the practice of Bayesian statistics</a></p>
<p>20 0.2123726 <a title="781-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.327), (1, 0.36), (2, -0.077), (3, 0.122), (4, -0.116), (5, -0.014), (6, -0.028), (7, 0.071), (8, 0.044), (9, 0.032), (10, 0.011), (11, 0.01), (12, -0.071), (13, 0.023), (14, -0.05), (15, 0.019), (16, 0.079), (17, -0.036), (18, -0.0), (19, 0.048), (20, -0.006), (21, -0.025), (22, -0.037), (23, -0.061), (24, -0.036), (25, 0.03), (26, -0.003), (27, -0.042), (28, -0.012), (29, 0.006), (30, -0.007), (31, -0.041), (32, -0.023), (33, 0.054), (34, 0.018), (35, 0.066), (36, -0.035), (37, 0.008), (38, -0.032), (39, 0.012), (40, 0.032), (41, -0.02), (42, 0.005), (43, 0.058), (44, 0.022), (45, -0.001), (46, -0.017), (47, -0.027), (48, 0.01), (49, -0.007)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98277527 <a title="781-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>Introduction: I’ve been writing a lot about my philosophy of Bayesian statistics and how it fits into Popper’s ideas about falsification and Kuhn’s ideas about scientific revolutions.
 
 Here’s  my long, somewhat technical paper with Cosma Shalizi. 
 Here’s  our shorter overview for the volume on the philosophy of social science. 
 Here’s  my latest try (for an online symposium), focusing on the key issues.
 
I’m pretty happy with my approach–the familiar idea that Bayesian data analysis iterates the three steps of model building, inference, and model checking–but it does have some unresolved (maybe unresolvable) problems.  Here are a couple mentioned in the third of the above links.
 
Consider a simple model with independent data y_1, y_2, .., y_10 ~ N(θ,σ^2), with a prior distribution θ ~ N(0,10^2) and σ known and taking on some value of approximately 10. Inference about μ is straightforward, as is model checking, whether based on graphs or numerical summaries such as the sample variance and skewn</p><p>2 0.94522583 <a title="781-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Kind_of_Bayesian.html">811 andrew gelman stats-2011-07-20-Kind of Bayesian</a></p>
<p>Introduction: Astrophysicist Andrew Jaffe pointed me to  this and discussion  of my  philosophy  of statistics (which is, in turn, my rational reconstruction of the statistical practice of Bayesians such as Rubin and Jaynes).  Jaffe’s summary is fair enough and I only disagree in a few points: 
   
1.  Jaffe writes:
  
Subjective probability, at least the way it is actually used by practicing scientists, is a sort of “as-if” subjectivity — how would an agent reason if her beliefs were reflected in a certain set of probability distributions? This is why when I discuss probability I try to make the pedantic point that all probabilities are conditional, at least on some background prior information or context.
  
I agree, and my problem with the usual procedures used for Bayesian model comparison and Bayesian model averaging is not that these approaches are subjective but that the particular models being considered don’t make sense.  I’m thinking of the sorts of models that say the truth is either A or</p><p>3 0.94164675 <a title="781-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-25-Incoherence_of_Bayesian_data_analysis.html">1510 andrew gelman stats-2012-09-25-Incoherence of Bayesian data analysis</a></p>
<p>Introduction: Hogg writes:
  
At the end  this article  you wonder about consistency.  Have you ever considered the possibility that utility might resolve some of the problems?  I have no idea if it 
would—I am not advocating that position—I just get some kind of intuition from phrases like “Judgment is required to decide…”. Perhaps there is a coherent and objective description of what is—or could be—done under a coherent “utility” model (like a utility that could be objectively agreed upon and computed).  Utilities are usually subjective—true—but priors are usually subjective too.
  
My reply:
 
I’m happy to think about utility, for some particular problem or class of problems going to the effort of assigning costs and benefits to different outcomes.  I agree that a utility analysis, even if (necessarily) imperfect, can usefully focus discussion.  For example, if a statistical method for selecting variables is justified on the basis of cost, I like the idea of attempting to quantify the costs of ga</p><p>4 0.93589568 <a title="781-lsi-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-05-Does_posterior_predictive_model_checking_fit_with_the_operational_subjective_approach%3F.html">320 andrew gelman stats-2010-10-05-Does posterior predictive model checking fit with the operational subjective approach?</a></p>
<p>Introduction: David Rohde writes:
  
 
I have been thinking a lot lately about your Bayesian model checking approach.  This is in part because I have been working on exploratory data analysis and wishing to avoid controversy and mathematical statistics we omitted model checking from our discussion.  This is something that the refereeing process picked us up on and we ultimately added a critical discussion of null-hypothesis testing to  our paper .  The exploratory technique we discussed was essentially a 2D histogram approach, but we used Polya models as a formal model for the histogram.  We are currently working on a new paper, and we are thinking through how or if we should do “confirmatory analysis” or model checking in the paper.


What I find most admirable about your statistical work is that you clearly use the Bayesian approach to do useful applied statistical analysis.  My own attempts at applied Bayesian analysis makes me greatly admire your applied successes.  On the other hand it may be t</p><p>5 0.931575 <a title="781-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>Introduction: Konrad Scheffler writes:
  
I was interested by your  paper  “Induction and deduction in Bayesian data analysis” and was wondering if you would entertain a few questions:
  
  
  
 – Under the banner of objective Bayesianism, I would posit something like this as a description of Bayesian inference:


“Objective Bayesian probability is not a degree of belief (which would necessarily be subjective) but a measure of the plausibility of a hypothesis, conditional on a formally specified information state. One way of specifying a formal information state is to specify a model, which involves specifying both a prior distribution (typically for a set of unobserved variables) and a likelihood function (typically for a set of observed variables, conditioned on the values of the unobserved variables). Bayesian inference involves calculating the objective degree of plausibility of a hypothesis (typically the truth value of the hypothesis is a function of the variables mentioned above) given such a</p><p>6 0.90527487 <a title="781-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-11-Gelman_on_Hennig_on_Gelman_on_Bayes.html">1208 andrew gelman stats-2012-03-11-Gelman on Hennig on Gelman on Bayes</a></p>
<p>7 0.90436101 <a title="781-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>8 0.90233529 <a title="781-lsi-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-22-Spell-checking_example_demonstrates_key_aspects_of_Bayesian_data_analysis.html">2182 andrew gelman stats-2014-01-22-Spell-checking example demonstrates key aspects of Bayesian data analysis</a></p>
<p>9 0.90127283 <a title="781-lsi-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Useful_models%2C_model_checking%2C_and_external_validation%3A__a_mini-discussion.html">244 andrew gelman stats-2010-08-30-Useful models, model checking, and external validation:  a mini-discussion</a></p>
<p>10 0.89264357 <a title="781-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-04-David_MacKay_and_Occam%E2%80%99s_Razor.html">1041 andrew gelman stats-2011-12-04-David MacKay and Occam’s Razor</a></p>
<p>11 0.89167559 <a title="781-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-24-Untangling_the_Jeffreys-Lindley_paradox.html">1182 andrew gelman stats-2012-02-24-Untangling the Jeffreys-Lindley paradox</a></p>
<p>12 0.87505704 <a title="781-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-11-Why_waste_time_philosophizing%3F.html">1719 andrew gelman stats-2013-02-11-Why waste time philosophizing?</a></p>
<p>13 0.8710621 <a title="781-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-05-Xiao-Li_Meng_and_Xianchao_Xie_rethink_asymptotics.html">1406 andrew gelman stats-2012-07-05-Xiao-Li Meng and Xianchao Xie rethink asymptotics</a></p>
<p>14 0.86991686 <a title="781-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-17-Christian_Robert_on_the_Jeffreys-Lindley_paradox%3B_more_generally%2C_it%E2%80%99s_good_news_when_philosophical_arguments_can_be_transformed_into_technical_modeling_issues.html">2027 andrew gelman stats-2013-09-17-Christian Robert on the Jeffreys-Lindley paradox; more generally, it’s good news when philosophical arguments can be transformed into technical modeling issues</a></p>
<p>15 0.86839664 <a title="781-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>16 0.86752701 <a title="781-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-17-Modeling_group-level_predictors_in_a_multilevel_regression.html">1216 andrew gelman stats-2012-03-17-Modeling group-level predictors in a multilevel regression</a></p>
<p>17 0.86414367 <a title="781-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>18 0.85946709 <a title="781-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>19 0.85788077 <a title="781-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-15-Wacky_priors_can_work_well%3F.html">1723 andrew gelman stats-2013-02-15-Wacky priors can work well?</a></p>
<p>20 0.83867067 <a title="781-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-07-Philosophy_and_the_practice_of_Bayesian_statistics_%28with_all_the_discussions%21%29.html">1712 andrew gelman stats-2013-02-07-Philosophy and the practice of Bayesian statistics (with all the discussions!)</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(14, 0.011), (15, 0.029), (16, 0.082), (21, 0.019), (24, 0.181), (45, 0.02), (53, 0.014), (84, 0.031), (86, 0.179), (89, 0.011), (99, 0.297)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98187315 <a title="781-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-18-Comments_on_%E2%80%9CA_Bayesian_approach_to_complex_clinical_diagnoses%3A_a_case-study_in_child_abuse%E2%80%9D.html">1327 andrew gelman stats-2012-05-18-Comments on “A Bayesian approach to complex clinical diagnoses: a case-study in child abuse”</a></p>
<p>Introduction: I was given the opportunity to briefly comment on the  paper , A Bayesian approach to complex clinical diagnoses: a case-study in child abuse, by Nicky Best, Deborah Ashby, Frank Dunstan, David Foreman, and Neil McIntosh, for the Journal of the Royal Statistical Society.  Here is what I wrote:
  
Best et al. are working on an important applied problem and I have no reason to doubt that their approach is a step forward beyond diagnostic criteria based on point estimation.  An attempt at an accurate assessment of variation is important not just for statistical reasons but also because scientists have the duty to convey their uncertainty to the larger world.  I am thinking, for example, of discredited claims such as that of the mathematician who claimed to predict divorces with 93% accuracy (Abraham, 2010).


Regarding the paper at hand, I thought I would try an experiment in comment-writing.  My usual practice is to read the graphs and then go back and clarify any questions through the t</p><p>2 0.98071527 <a title="781-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-29-%E2%80%9CCommunication_is_a_central_task_of_statistics%2C_and_ideally_a_state-of-the-art_data_analysis_can_have_state-of-the-art_displays_to_match%E2%80%9D.html">1552 andrew gelman stats-2012-10-29-“Communication is a central task of statistics, and ideally a state-of-the-art data analysis can have state-of-the-art displays to match”</a></p>
<p>Introduction: The Journal of the Royal Statistical Society publishes papers followed by discussions.  Lots of discussions, each can be no more than 400 words.  Here’s my most recent discussion:
  
The authors are working on an important applied problem and I have no reason to doubt that their approach is a step forward beyond diagnostic criteria based on point estimation.  An attempt at an accurate assessment of variation is important not just for statistical reasons but also because scientists have the duty to convey their uncertainty to the larger world.  I am thinking, for example, of discredited claims such as that of the mathematician who claimed to predict divorces with 93% accuracy (Abraham, 2010).


Regarding the paper at hand, I thought I would try an experiment in comment-writing.  My usual practice is to read the graphs and then go back and clarify any questions through the text.  So, very quickly:  I would prefer Figure 1 to be displayed in terms of standard deviations, not variances.  I</p><p>3 0.97991586 <a title="781-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-25-College_football%2C_voting%2C_and_the_law_of_large_numbers.html">1547 andrew gelman stats-2012-10-25-College football, voting, and the law of large numbers</a></p>
<p>Introduction: In an article provocatively entitled, “Will Ohio State’s football team decide who wins the White House?”, Tyler Cowen and Kevin Grier  report :
  
It is statistically possible that the outcome of a handful of college football games in the right battleground states could determine the race for the White House.


Economists Andrew Healy, Neil Malhotra, and Cecilia Mo make this argument in a fascinating article in the Proceedings of the National Academy of Science. They examined whether the outcomes of college football games on the eve of elections for presidents, senators, and governors affected the choices voters made. They found that a win by the local team, in the week before an election, raises the vote going to the incumbent by around 1.5 percentage points. When it comes to the 20 highest attendance teams—big athletic programs like the University of Michigan, Oklahoma, and Southern Cal—a victory on the eve of an election pushes the vote for the incumbent up by 3 percentage points. T</p><p>4 0.97988135 <a title="781-lda-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-03-Gladwell_vs_Pinker.html">253 andrew gelman stats-2010-09-03-Gladwell vs Pinker</a></p>
<p>Introduction: I just happened to notice this from last year.  Eric Loken  writes :
  
Steven Pinker reviewed Malcolm Gladwell’s latest book and criticized him rather harshly for several shortcomings. Gladwell appears to have made things worse for himself in a letter to the editor of the NYT by defending a manifestly weak claim from one of his essays – the claim that NFL quarterback performance is unrelated to the order they were drafted out of college. The reason w [Loken and his colleagues] are implicated is that Pinker identified an earlier blog post of ours as one of three sources he used to challenge Gladwell (yay us!). But Gladwell either misrepresented or misunderstood our post in his response, and admonishes Pinker by saying “we should agree that our differences owe less to what can be found in the scientific literature than they do to what can be found on Google.”


Well, here’s what you can find on Google. Follow  this link  to request the data for NFL quarterbacks drafted between 1980 and</p><p>5 0.97489226 <a title="781-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-11-Toward_a_framework_for_automatic_model_building.html">1718 andrew gelman stats-2013-02-11-Toward a framework for automatic model building</a></p>
<p>Introduction: Patrick Caldon writes: 
  
  
I saw your  recent blog post  where you discussed in passing an iterative-chain-of models approach to AI. 


I essentially built such a thing for  my PhD thesis  – not in a Bayesian context, but in a logic programming context – and proved it had a few properties and showed how you could solve some toy problems. The important bit of my framework was that at various points you also go and get more data in the process – in a statistical context this might be seen as building a little univariate model on a subset of the data, then iteratively extending into a better model with more data and more independent variables – a generalized forward stepwise regression if you like.  It wrapped a proper computational framework around E.M. Gold’s identification/learning in the limit based on a logic my advisor (Eric Martin) had invented.


What’s not written up in the thesis is a few months of failed struggle trying to shoehorn some simple statistical inference into this</p><p>6 0.97304094 <a title="781-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-23-%E2%80%9CAny_old_map_will_do%E2%80%9D_meets_%E2%80%9CGod_is_in_every_leaf_of_every_tree%E2%80%9D.html">1278 andrew gelman stats-2012-04-23-“Any old map will do” meets “God is in every leaf of every tree”</a></p>
<p>7 0.97245473 <a title="781-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-29-Decision_science_vs._social_psychology.html">305 andrew gelman stats-2010-09-29-Decision science vs. social psychology</a></p>
<p>8 0.97176266 <a title="781-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-I_doubt_they_cheated.html">1971 andrew gelman stats-2013-08-07-I doubt they cheated</a></p>
<p>9 0.97097051 <a title="781-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-30-Berri_Gladwell_Loken_football_update.html">2082 andrew gelman stats-2013-10-30-Berri Gladwell Loken football update</a></p>
<p>10 0.96944404 <a title="781-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-11-%E2%80%9C2_level_logit_with_2_REs_%26_large_sample._computational_nightmare_%E2%80%93_please_help%E2%80%9D.html">759 andrew gelman stats-2011-06-11-“2 level logit with 2 REs & large sample. computational nightmare – please help”</a></p>
<p>11 0.96693897 <a title="781-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-15-More_on_AIC%2C_WAIC%2C_etc.html">1983 andrew gelman stats-2013-08-15-More on AIC, WAIC, etc</a></p>
<p>same-blog 12 0.96132296 <a title="781-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>13 0.96041429 <a title="781-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-14-Don%E2%80%99t_look_at_just_one_poll_number%E2%80%93unless_you_really_know_what_you%E2%80%99re_doing%21.html">276 andrew gelman stats-2010-09-14-Don’t look at just one poll number–unless you really know what you’re doing!</a></p>
<p>14 0.95748967 <a title="781-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-23-Participate_in_a_research_project_on_combining_information_for_prediction.html">866 andrew gelman stats-2011-08-23-Participate in a research project on combining information for prediction</a></p>
<p>15 0.95487893 <a title="781-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-13-My_wikipedia_edit.html">904 andrew gelman stats-2011-09-13-My wikipedia edit</a></p>
<p>16 0.95287752 <a title="781-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-11-Gladwell_and_Chabris%2C_David_and_Goliath%2C_and_science_writing_as_stone_soup.html">2058 andrew gelman stats-2013-10-11-Gladwell and Chabris, David and Goliath, and science writing as stone soup</a></p>
<p>17 0.95233214 <a title="781-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-16-Another_day%2C_another_plagiarist.html">1266 andrew gelman stats-2012-04-16-Another day, another plagiarist</a></p>
<p>18 0.95076478 <a title="781-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-26-Luck_or_knowledge%3F.html">873 andrew gelman stats-2011-08-26-Luck or knowledge?</a></p>
<p>19 0.94946498 <a title="781-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-09-Both_R_and_Stata.html">76 andrew gelman stats-2010-06-09-Both R and Stata</a></p>
<p>20 0.94641483 <a title="781-lda-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-31-Type_S_error_rates_for_classical_and_Bayesian_single_and_multiple_comparison_procedures.html">494 andrew gelman stats-2010-12-31-Type S error rates for classical and Bayesian single and multiple comparison procedures</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
