<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>803 andrew gelman stats-2011-07-14-Subtleties with measurement-error models for the evaluation of wacky claims</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-803" href="#">andrew_gelman_stats-2011-803</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>803 andrew gelman stats-2011-07-14-Subtleties with measurement-error models for the evaluation of wacky claims</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-803-html" href="http://andrewgelman.com/2011/07/14/subtleties_with/">html</a></p><p>Introduction: A few days ago I  discussed  the evaluation of somewhat-plausible claims that are somewhat supported by theory and somewhat supported by statistical evidence.  One point I raised was that an implausibly large estimate of effect size can be cause for concern:
  
Uri Simonsohn (the author of the recent rebuttal of the name-choice article by Pelham et al.) argued that the implied effects were too large to be believed (just as I was arguing above regarding the July 4th study), which makes more plausible his claims that the results arise from methodological artifacts.


That calculation is straight Bayes: the distribution of systematic errors has much longer tails than the distribution of random errors, so the larger the estimated effect, the more likely it is to be a mistake. This little theoretical result is a bit annoying, because it is the larger effects that are the most interesting!”
  
Larry Bartels notes that my reasoning above is a bit incoherent:
  
I [Bartels] strongly agree with</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A few days ago I  discussed  the evaluation of somewhat-plausible claims that are somewhat supported by theory and somewhat supported by statistical evidence. [sent-1, score-0.494]
</p><p>2 One point I raised was that an implausibly large estimate of effect size can be cause for concern:    Uri Simonsohn (the author of the recent rebuttal of the name-choice article by Pelham et al. [sent-2, score-0.926]
</p><p>3 ) argued that the implied effects were too large to be believed (just as I was arguing above regarding the July 4th study), which makes more plausible his claims that the results arise from methodological artifacts. [sent-3, score-0.485]
</p><p>4 That calculation is straight Bayes: the distribution of systematic errors has much longer tails than the distribution of random errors, so the larger the estimated effect, the more likely it is to be a mistake. [sent-4, score-1.017]
</p><p>5 This little theoretical result is a bit annoying, because it is the larger effects that are the most interesting! [sent-5, score-0.244]
</p><p>6 ”    Larry Bartels notes that my reasoning above is a bit incoherent:    I [Bartels] strongly agree with your bottom line that our main aim should be “understanding effect sizes on a real scale. [sent-6, score-0.456]
</p><p>7 ” However, your paradoxical conclusion (“the larger the estimated effect, the more likely it is to be a mistake”) seems to distract attention from the effect size of primary interest-the magnitude of the “true” (causal) effect. [sent-7, score-1.403]
</p><p>8 But the more important fact would seem to be that your posterior belief regarding the magnitude of the “true” (causal) effect, E(c|b), is also increasing in b (at least for plausible-seeming distributional assumptions). [sent-9, score-0.815]
</p><p>9 Focusing on whether a surprising empirical result is “a mistake” (whatever that means) seems to concede too much to the simple-minded is-there-an-effect-or-isn’t-there perspective, while obscuring your more fundamental interest in “understanding [true] effect sizes on a real scale. [sent-12, score-0.775]
</p><p>10 Maybe a more correct statement would be that, given reasonable models for x, d, and e, if the estimate gets implausibly large, the estimate for x does not increase proportionally. [sent-15, score-0.546]
</p><p>11 I actually think there will be some (non-Gaussian) models for which, as y gets larger, E(x|y) can actually go back toward zero. [sent-16, score-0.073]
</p><p>12 But this will depend on the distributional form. [sent-17, score-0.148]
</p><p>13 I agree that “how likely is it to be a mistake” is the wrong way to look at things. [sent-18, score-0.09]
</p><p>14 No analysis is perfect, so the “mistake” framing is generally not so helpful. [sent-20, score-0.065]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('effect', 0.36), ('estimated', 0.247), ('magnitude', 0.208), ('attribute', 0.184), ('july', 0.181), ('larger', 0.178), ('mistake', 0.171), ('implausibly', 0.158), ('systematic', 0.154), ('causal', 0.15), ('bartels', 0.148), ('distributional', 0.148), ('true', 0.145), ('regarding', 0.145), ('large', 0.14), ('estimate', 0.117), ('posterior', 0.116), ('supported', 0.106), ('larry', 0.106), ('claims', 0.104), ('error', 0.099), ('belief', 0.099), ('increasing', 0.099), ('plausible', 0.096), ('sizes', 0.096), ('concede', 0.094), ('likely', 0.09), ('somewhat', 0.089), ('warranted', 0.088), ('elicited', 0.085), ('distract', 0.085), ('paradoxical', 0.082), ('obscuring', 0.082), ('increase', 0.081), ('pelham', 0.079), ('study', 0.078), ('seems', 0.077), ('size', 0.076), ('rebuttal', 0.075), ('errors', 0.075), ('gets', 0.073), ('incoherent', 0.072), ('tails', 0.072), ('wacky', 0.071), ('random', 0.071), ('understanding', 0.068), ('nevertheless', 0.067), ('result', 0.066), ('distribution', 0.065), ('framing', 0.065)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999952 <a title="803-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-14-Subtleties_with_measurement-error_models_for_the_evaluation_of_wacky_claims.html">803 andrew gelman stats-2011-07-14-Subtleties with measurement-error models for the evaluation of wacky claims</a></p>
<p>Introduction: A few days ago I  discussed  the evaluation of somewhat-plausible claims that are somewhat supported by theory and somewhat supported by statistical evidence.  One point I raised was that an implausibly large estimate of effect size can be cause for concern:
  
Uri Simonsohn (the author of the recent rebuttal of the name-choice article by Pelham et al.) argued that the implied effects were too large to be believed (just as I was arguing above regarding the July 4th study), which makes more plausible his claims that the results arise from methodological artifacts.


That calculation is straight Bayes: the distribution of systematic errors has much longer tails than the distribution of random errors, so the larger the estimated effect, the more likely it is to be a mistake. This little theoretical result is a bit annoying, because it is the larger effects that are the most interesting!”
  
Larry Bartels notes that my reasoning above is a bit incoherent:
  
I [Bartels] strongly agree with</p><p>2 0.36803645 <a title="803-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-11-How_do_we_evaluate_a_new_and_wacky_claim%3F.html">797 andrew gelman stats-2011-07-11-How do we evaluate a new and wacky claim?</a></p>
<p>Introduction: Around these parts we see a continuing flow of unusual claims supported by some statistical evidence.  The claims are varyingly plausible a priori.  Some examples (I won’t bother to supply the links; regular readers will remember these examples and newcomers can find them by searching):
 
- Obesity is contagious 
- People’s names affect where they live, what jobs they take, etc. 
- Beautiful people are more likely to have girl babies 
- More attractive instructors have higher teaching evaluations 
- In a basketball game, it’s better to be behind by a point at halftime than to be ahead by a point 
- Praying for someone without their knowledge improves their recovery from heart attacks 
- A variety of claims about ESP
 
How should we think about these claims?  The usual approach is to evaluate the statistical evidence–in particular, to look for reasons that the claimed results are not really statistically significant.  If nobody can shoot down a claim, it survives.
 
The other part of th</p><p>3 0.20431682 <a title="803-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-01-Why_big_effects_are_more_important_than_small_effects.html">1744 andrew gelman stats-2013-03-01-Why big effects are more important than small effects</a></p>
<p>Introduction: The title of this post is silly but I have an important point to make, regarding an implicit model which I think many people assume even though it does not really make sense.
 
Following a link from Sanjay Srivastava, I came across  a post  from David Funder saying that it’s useful to talk about the sizes of effects (I actually prefer the term “comparisons” so as to avoid the causal baggage) rather than just their signs.  I  agree , and I wanted to elaborate a bit on a point that comes up in Funder’s discussion.  He quotes an (unnamed) prominent social psychologist as writing:
  
The key to our research . . . [is not] to accurately estimate effect size. If I were testing an advertisement for a marketing research firm and wanted to be sure that the cost of the ad would produce enough sales to make it worthwhile, effect size would be crucial. But when I am testing a theory about whether, say, positive mood reduces information processing in comparison with negative mood, I am worried abou</p><p>4 0.20275521 <a title="803-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-18-Question_on_Type_M_errors.html">963 andrew gelman stats-2011-10-18-Question on Type M errors</a></p>
<p>Introduction: Inti Pedroso writes:
  
Today during the group meeting at my new job we were revising a paper whose main conclusions were sustained by an ANOVA.


One of the first observations is that the experiment had a small sample size. Interestingly (may not so), some of the reported effects (most of them interactions) were quite large. One of the experience group members said that “there is a common wisdom that one should not believe effects from small sample sizes but [he thinks] if they [the effects] are large enough to be picked on a small study they must be real large effects”. I argued that if the sample size is small one could incur on a M-type error in which the magnitude of the effect is being over-estimated and that if larger samples are evaluated the magnitude may become smaller and also the confidence intervals. The concept of M-type error is completely new to all other members of the group (on which I am in my second week) and I was given the job of finding a suitable ref to explain</p><p>5 0.19504096 <a title="803-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-05-The_p-value_is_not_._._..html">1607 andrew gelman stats-2012-12-05-The p-value is not . . .</a></p>
<p>Introduction: From a recent email exchange:
 
I agree that you should never compare p-values directly.  The p-value is a strange nonlinear transformation of data that is only interpretable under the null hypothesis. Once you abandon the null (as we do when we observe something with a very low p-value), the p-value itself becomes irrelevant.  To put it another way, the p-value is a measure of evidence, it is not an estimate of effect size (as it is often treated, with the idea that a p=.001 effect is larger than a p=.01 effect, etc).  Even conditional on sample size, the p-value is not a measure of effect size.</p><p>6 0.16940995 <a title="803-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-26-Is_it_plausible_that_1%25_of_people_pick_a_career_based_on_their_first_name%3F.html">629 andrew gelman stats-2011-03-26-Is it plausible that 1% of people pick a career based on their first name?</a></p>
<p>7 0.16511899 <a title="803-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-02-So-called_Bayesian_hypothesis_testing_is_just_as_bad_as_regular_hypothesis_testing.html">643 andrew gelman stats-2011-04-02-So-called Bayesian hypothesis testing is just as bad as regular hypothesis testing</a></p>
<p>8 0.16450399 <a title="803-tfidf-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-31-Type_S_error_rates_for_classical_and_Bayesian_single_and_multiple_comparison_procedures.html">494 andrew gelman stats-2010-12-31-Type S error rates for classical and Bayesian single and multiple comparison procedures</a></p>
<p>9 0.15794986 <a title="803-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>10 0.15759669 <a title="803-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-16-Long_discussion_about_causal_inference_and_the_use_of_hierarchical_models_to_bridge_between_different_inferential_settings.html">1418 andrew gelman stats-2012-07-16-Long discussion about causal inference and the use of hierarchical models to bridge between different inferential settings</a></p>
<p>11 0.14929563 <a title="803-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-29-Decline_Effect_in_Linguistics%3F.html">1400 andrew gelman stats-2012-06-29-Decline Effect in Linguistics?</a></p>
<p>12 0.14899649 <a title="803-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-18-The_estimated_effect_size_is_implausibly_large.__Under_what_models_is_this_a_piece_of_evidence_that_the_true_effect_is_small%3F.html">808 andrew gelman stats-2011-07-18-The estimated effect size is implausibly large.  Under what models is this a piece of evidence that the true effect is small?</a></p>
<p>13 0.14715935 <a title="803-tfidf-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-14-Prior_distribution_for_design_effects.html">85 andrew gelman stats-2010-06-14-Prior distribution for design effects</a></p>
<p>14 0.14321111 <a title="803-tfidf-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-04-27-Should_Mister_P_be_allowed-encouraged_to_reside_in_counter-factual_populations%3F.html">7 andrew gelman stats-2010-04-27-Should Mister P be allowed-encouraged to reside in counter-factual populations?</a></p>
<p>15 0.14110182 <a title="803-tfidf-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-01-The_placebo_effect_in_pharma.html">388 andrew gelman stats-2010-11-01-The placebo effect in pharma</a></p>
<p>16 0.13640732 <a title="803-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-15-Forward_causal_reasoning_statements_are_about_estimation%3B_reverse_causal_questions_are_about_model_checking_and_hypothesis_generation.html">1939 andrew gelman stats-2013-07-15-Forward causal reasoning statements are about estimation; reverse causal questions are about model checking and hypothesis generation</a></p>
<p>17 0.13536428 <a title="803-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-15-The_bias-variance_tradeoff.html">960 andrew gelman stats-2011-10-15-The bias-variance tradeoff</a></p>
<p>18 0.13522655 <a title="803-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-22-Struggles_over_the_criticism_of_the_%E2%80%9Ccannabis_users_and_IQ_change%E2%80%9D_paper.html">1910 andrew gelman stats-2013-06-22-Struggles over the criticism of the “cannabis users and IQ change” paper</a></p>
<p>19 0.13224457 <a title="803-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-25-Bayes-respecting_experimental_design_and_other_things.html">1955 andrew gelman stats-2013-07-25-Bayes-respecting experimental design and other things</a></p>
<p>20 0.13211839 <a title="803-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-11-One_more_time_on_that_ESP_study%3A__The_problem_of_overestimates_and_the_shrinkage_solution.html">511 andrew gelman stats-2011-01-11-One more time on that ESP study:  The problem of overestimates and the shrinkage solution</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.229), (1, 0.076), (2, 0.118), (3, -0.178), (4, -0.039), (5, -0.079), (6, 0.02), (7, 0.034), (8, 0.013), (9, -0.052), (10, -0.105), (11, 0.019), (12, 0.053), (13, -0.042), (14, 0.056), (15, 0.018), (16, -0.054), (17, 0.044), (18, -0.048), (19, 0.058), (20, -0.063), (21, -0.058), (22, 0.049), (23, 0.013), (24, 0.034), (25, 0.099), (26, -0.037), (27, 0.083), (28, -0.021), (29, -0.036), (30, -0.033), (31, 0.034), (32, -0.082), (33, -0.047), (34, -0.009), (35, 0.017), (36, -0.066), (37, -0.117), (38, -0.019), (39, -0.062), (40, -0.018), (41, 0.011), (42, -0.092), (43, -0.013), (44, 0.031), (45, 0.016), (46, -0.035), (47, 0.042), (48, 0.02), (49, 0.034)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99336594 <a title="803-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-14-Subtleties_with_measurement-error_models_for_the_evaluation_of_wacky_claims.html">803 andrew gelman stats-2011-07-14-Subtleties with measurement-error models for the evaluation of wacky claims</a></p>
<p>Introduction: A few days ago I  discussed  the evaluation of somewhat-plausible claims that are somewhat supported by theory and somewhat supported by statistical evidence.  One point I raised was that an implausibly large estimate of effect size can be cause for concern:
  
Uri Simonsohn (the author of the recent rebuttal of the name-choice article by Pelham et al.) argued that the implied effects were too large to be believed (just as I was arguing above regarding the July 4th study), which makes more plausible his claims that the results arise from methodological artifacts.


That calculation is straight Bayes: the distribution of systematic errors has much longer tails than the distribution of random errors, so the larger the estimated effect, the more likely it is to be a mistake. This little theoretical result is a bit annoying, because it is the larger effects that are the most interesting!”
  
Larry Bartels notes that my reasoning above is a bit incoherent:
  
I [Bartels] strongly agree with</p><p>2 0.84117436 <a title="803-lsi-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-04-27-Should_Mister_P_be_allowed-encouraged_to_reside_in_counter-factual_populations%3F.html">7 andrew gelman stats-2010-04-27-Should Mister P be allowed-encouraged to reside in counter-factual populations?</a></p>
<p>Introduction: Lets say you are repeatedly going to recieve unselected sets of well done RCTs on various say medical treatments.
 
One reasonable assumption with all of these treatments is that they are monotonic – either helpful or harmful for all. The treatment effect will (as always) vary for subgroups in the population – these will not be explicitly identified in the studies – but each study very likely will enroll different percentages of the variuos patient subgroups. Being all randomized studies these subgroups will be balanced in the treatment versus control arms – but each study will (as always) be estimating a different – but exchangeable – treatment effect (Exhangeable due to the ignorance about the subgroup memberships of the enrolled patients.) 
 
That reasonable assumption – monotonicity – will be to some extent (as always) wrong, but given that it is a risk believed well worth taking – if the average effect in any population is positive (versus negative) the average effect in any other</p><p>3 0.83497715 <a title="803-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-11-How_do_we_evaluate_a_new_and_wacky_claim%3F.html">797 andrew gelman stats-2011-07-11-How do we evaluate a new and wacky claim?</a></p>
<p>Introduction: Around these parts we see a continuing flow of unusual claims supported by some statistical evidence.  The claims are varyingly plausible a priori.  Some examples (I won’t bother to supply the links; regular readers will remember these examples and newcomers can find them by searching):
 
- Obesity is contagious 
- People’s names affect where they live, what jobs they take, etc. 
- Beautiful people are more likely to have girl babies 
- More attractive instructors have higher teaching evaluations 
- In a basketball game, it’s better to be behind by a point at halftime than to be ahead by a point 
- Praying for someone without their knowledge improves their recovery from heart attacks 
- A variety of claims about ESP
 
How should we think about these claims?  The usual approach is to evaluate the statistical evidence–in particular, to look for reasons that the claimed results are not really statistically significant.  If nobody can shoot down a claim, it survives.
 
The other part of th</p><p>4 0.81482935 <a title="803-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-01-Why_big_effects_are_more_important_than_small_effects.html">1744 andrew gelman stats-2013-03-01-Why big effects are more important than small effects</a></p>
<p>Introduction: The title of this post is silly but I have an important point to make, regarding an implicit model which I think many people assume even though it does not really make sense.
 
Following a link from Sanjay Srivastava, I came across  a post  from David Funder saying that it’s useful to talk about the sizes of effects (I actually prefer the term “comparisons” so as to avoid the causal baggage) rather than just their signs.  I  agree , and I wanted to elaborate a bit on a point that comes up in Funder’s discussion.  He quotes an (unnamed) prominent social psychologist as writing:
  
The key to our research . . . [is not] to accurately estimate effect size. If I were testing an advertisement for a marketing research firm and wanted to be sure that the cost of the ad would produce enough sales to make it worthwhile, effect size would be crucial. But when I am testing a theory about whether, say, positive mood reduces information processing in comparison with negative mood, I am worried abou</p><p>5 0.77762616 <a title="803-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-26-Is_it_plausible_that_1%25_of_people_pick_a_career_based_on_their_first_name%3F.html">629 andrew gelman stats-2011-03-26-Is it plausible that 1% of people pick a career based on their first name?</a></p>
<p>Introduction: In my discussion of dentists-named-Dennis study, I referred to my back-of-the-envelope  calculation  that the effect (if it indeed exists) corresponds to an approximate 1% aggregate chance that you’ll pick a profession based on your first name.  Even if there are nearly twice as many dentist Dennises as would be expected from chance alone, the base rate is so low that a shift of 1% of all Dennises would be enough to do this.  My point was that (a) even a small effect could show up when looking at low-frequency events such as the choice to pick a particular career or live in a particular city, and (b) any small effects will inherently be difficult to detect in any direct way.
 
Uri Simonsohn (the author of the recent rebuttal of the original name-choice article by Brett Pelham et al.) wrote:
  
 
In terms of the effect size. I [Simonsohn] think of it differently and see it as too big to be believable.


I don’t find it plausible that I can double the odds that my daughter will marry an</p><p>6 0.75743335 <a title="803-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-05-The_p-value_is_not_._._..html">1607 andrew gelman stats-2012-12-05-The p-value is not . . .</a></p>
<p>7 0.75391269 <a title="803-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-29-Decline_Effect_in_Linguistics%3F.html">1400 andrew gelman stats-2012-06-29-Decline Effect in Linguistics?</a></p>
<p>8 0.73293257 <a title="803-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-15-Regression_discontinuity_designs%3A__looking_for_the_keys_under_the_lamppost%3F.html">518 andrew gelman stats-2011-01-15-Regression discontinuity designs:  looking for the keys under the lamppost?</a></p>
<p>9 0.73254287 <a title="803-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-Fourteen_magic_words%3A_an_update.html">898 andrew gelman stats-2011-09-10-Fourteen magic words: an update</a></p>
<p>10 0.73112214 <a title="803-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-17-Is_the_internet_causing_half_the_rapes_in_Norway%3F__I_wanna_see_the_scatterplot..html">716 andrew gelman stats-2011-05-17-Is the internet causing half the rapes in Norway?  I wanna see the scatterplot.</a></p>
<p>11 0.71823049 <a title="803-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-18-Question_on_Type_M_errors.html">963 andrew gelman stats-2011-10-18-Question on Type M errors</a></p>
<p>12 0.70609289 <a title="803-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-26-Difficulties_in_making_inferences_about_scientific_truth_from_distributions_of_published_p-values.html">2040 andrew gelman stats-2013-09-26-Difficulties in making inferences about scientific truth from distributions of published p-values</a></p>
<p>13 0.7029016 <a title="803-lsi-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-04-23-Modeling_heterogenous_treatment_effects.html">2 andrew gelman stats-2010-04-23-Modeling heterogenous treatment effects</a></p>
<p>14 0.7004137 <a title="803-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-15-The_bias-variance_tradeoff.html">960 andrew gelman stats-2011-10-15-The bias-variance tradeoff</a></p>
<p>15 0.69628054 <a title="803-lsi-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-24-%E2%80%9CEdlin%E2%80%99s_rule%E2%80%9D_for_routinely_scaling_down_published_estimates.html">2223 andrew gelman stats-2014-02-24-“Edlin’s rule” for routinely scaling down published estimates</a></p>
<p>16 0.69358927 <a title="803-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-27-Confusion_from_illusory_precision.html">1186 andrew gelman stats-2012-02-27-Confusion from illusory precision</a></p>
<p>17 0.69299281 <a title="803-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-02-The_inevitable_problems_with_statistical_significance_and_95%25_intervals.html">1150 andrew gelman stats-2012-02-02-The inevitable problems with statistical significance and 95% intervals</a></p>
<p>18 0.68950987 <a title="803-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-02-So-called_Bayesian_hypothesis_testing_is_just_as_bad_as_regular_hypothesis_testing.html">643 andrew gelman stats-2011-04-02-So-called Bayesian hypothesis testing is just as bad as regular hypothesis testing</a></p>
<p>19 0.68650091 <a title="803-lsi-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-09-San_Fernando_Valley_cityscapes%3A__An_example_of_the_benefits_of_fractal_devastation%3F.html">2165 andrew gelman stats-2014-01-09-San Fernando Valley cityscapes:  An example of the benefits of fractal devastation?</a></p>
<p>20 0.67921281 <a title="803-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-22-Struggles_over_the_criticism_of_the_%E2%80%9Ccannabis_users_and_IQ_change%E2%80%9D_paper.html">1910 andrew gelman stats-2013-06-22-Struggles over the criticism of the “cannabis users and IQ change” paper</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.026), (15, 0.172), (16, 0.03), (21, 0.039), (24, 0.209), (40, 0.016), (43, 0.023), (48, 0.017), (53, 0.011), (55, 0.011), (99, 0.336)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9871242 <a title="803-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-06-W%E2%80%99man_%3C_W%E2%80%99pedia%2C_again.html">945 andrew gelman stats-2011-10-06-W’man < W’pedia, again</a></p>
<p>Introduction: Blogger Deep Climate  looks at  another paper by the 2002 recipient of the American Statistical Association’s Founders award. This time it’s not funny, it’s just sad. 
   
Here’s Wikipedia on simulated annealing:
  
By analogy with this physical process, each step of the SA algorithm replaces the current solution by a random “nearby” solution, chosen with a probability that depends on the difference between the corresponding function values and on a global parameter T (called the temperature), that is gradually decreased during the process. The dependency is such that the current solution changes almost randomly when T is large, but increasingly “downhill” as T goes to zero. The allowance for “uphill” moves saves the method from becoming stuck at local minima—which are the bane of greedier methods.
  
And here’s Wegman:
  
During each step of the algorithm, the variable that will eventually represent the minimum is replaced by a random solution that is chosen according to a temperature</p><p>2 0.98467314 <a title="803-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-08-Gratuitous_use_of_%E2%80%9CBayesian_Statistics%2C%E2%80%9D_a_branding_issue%3F.html">133 andrew gelman stats-2010-07-08-Gratuitous use of “Bayesian Statistics,” a branding issue?</a></p>
<p>Introduction: I’m on an island in Maine for a few weeks (big shout out for North Haven!)  This morning I picked up a copy of “Working Waterfront,” a newspaper that focuses on issues of coastal fishing communities.  I came across  an article  about modeling “fish” populations — actually lobsters, I guess they’re considered “fish” for regulatory purposes.  When I read it, I thought “wow, this article is really well-written, not dumbed down like articles in most newspapers.” I think it’s great that a small coastal newspaper carries reporting like this. (The online version has a few things that I don’t recall in the print version, too, so it’s even better).  But in addition to being struck by finding such a good article in a small newspaper, I was struck by this:
  
According to [University of Maine scientist Yong] Chen, there are four main areas where his model improved on the prior version. “We included the inshore trawl data from Maine and other state surveys, in addition to federal survey data; we h</p><p>3 0.98460668 <a title="803-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-With_a_bit_of_precognition%2C_you%E2%80%99d_have_known_I_was_going_to_post_again_on_this_topic%2C_and_with_a_lot_of_precognition%2C_you%E2%80%99d_have_known_I_was_going_to_post_today.html">576 andrew gelman stats-2011-02-15-With a bit of precognition, you’d have known I was going to post again on this topic, and with a lot of precognition, you’d have known I was going to post today</a></p>
<p>Introduction: Chris Masse points me to  this response  by Daryl Bem and two statisticians (Jessica Utts and Wesley Johnson) to criticisms by Wagenmakers et.al. of Bem’s recent ESP study.  I have nothing to add but would like to repeat a couple bits of my discussions of last month, of  here :
  
Classical statistical methods that work reasonably well when studying moderate or large effects (see the work of Fisher, Snedecor, Cochran, etc.) fall apart in the presence of small effects.


I think it’s naive when people implicitly assume that the study’s claims are correct, or the study’s statistical methods are weak. Generally, the smaller the effects you’re studying, the better the statistics you need. ESP is a field of small effects and so ESP researchers use high-quality statistics.


To put it another way: whatever methodological errors happen to be in the paper in question, probably occur in lots of researcher papers in “legitimate” psychology research. The difference is that when you’re studying a</p><p>same-blog 4 0.98113412 <a title="803-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-14-Subtleties_with_measurement-error_models_for_the_evaluation_of_wacky_claims.html">803 andrew gelman stats-2011-07-14-Subtleties with measurement-error models for the evaluation of wacky claims</a></p>
<p>Introduction: A few days ago I  discussed  the evaluation of somewhat-plausible claims that are somewhat supported by theory and somewhat supported by statistical evidence.  One point I raised was that an implausibly large estimate of effect size can be cause for concern:
  
Uri Simonsohn (the author of the recent rebuttal of the name-choice article by Pelham et al.) argued that the implied effects were too large to be believed (just as I was arguing above regarding the July 4th study), which makes more plausible his claims that the results arise from methodological artifacts.


That calculation is straight Bayes: the distribution of systematic errors has much longer tails than the distribution of random errors, so the larger the estimated effect, the more likely it is to be a mistake. This little theoretical result is a bit annoying, because it is the larger effects that are the most interesting!”
  
Larry Bartels notes that my reasoning above is a bit incoherent:
  
I [Bartels] strongly agree with</p><p>5 0.97914433 <a title="803-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-24-Statistical_ethics_violation.html">1081 andrew gelman stats-2011-12-24-Statistical ethics violation</a></p>
<p>Introduction: A colleague writes:
  
When I was in NYC I went to this party by group of Japanese bio-scientists. There, one guy told me about how the biggest pharmaceutical company in Japan did their statistics. They ran 100 different tests and reported the most significant one. (This was in 2006 and he said they stopped doing this few years back so they were doing this until pretty recently…) I’m not sure if this was 100 multiple comparison or 100 different kinds of test but I’m sure they wouldn’t want to disclose their data…
  
Ouch!</p><p>6 0.97799182 <a title="803-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-09-My_talks_in_DC_and_Baltimore_this_week.html">1794 andrew gelman stats-2013-04-09-My talks in DC and Baltimore this week</a></p>
<p>7 0.9779827 <a title="803-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-19-Statistical_discrimination_again.html">1541 andrew gelman stats-2012-10-19-Statistical discrimination again</a></p>
<p>8 0.96871859 <a title="803-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Interpreting_interactions_in_discrete-data_regression.html">1908 andrew gelman stats-2013-06-21-Interpreting interactions in discrete-data regression</a></p>
<p>9 0.96844459 <a title="803-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>10 0.96768701 <a title="803-lda-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-27-%E2%80%9CDisappointed_with_your_results%3F__Boost_your_scientific_paper%E2%80%9D.html">2188 andrew gelman stats-2014-01-27-“Disappointed with your results?  Boost your scientific paper”</a></p>
<p>11 0.96343571 <a title="803-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-12-Too_tired_to_mock.html">1800 andrew gelman stats-2013-04-12-Too tired to mock</a></p>
<p>12 0.96224999 <a title="803-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-14-Battle_of_the_Americans%3A__Writer_at_the_American_Enterprise_Institute_disparages_the_American_Political_Science_Association.html">274 andrew gelman stats-2010-09-14-Battle of the Americans:  Writer at the American Enterprise Institute disparages the American Political Science Association</a></p>
<p>13 0.96185887 <a title="803-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-08-More_on_those_dudes_who_will_pay_your_professor_%248000_to_assign_a_book_to_your_class%2C_and_related_stories_about_small-time_sleazoids.html">329 andrew gelman stats-2010-10-08-More on those dudes who will pay your professor $8000 to assign a book to your class, and related stories about small-time sleazoids</a></p>
<p>14 0.9600879 <a title="803-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-12-The_importance_of_style_in_academic_writing.html">902 andrew gelman stats-2011-09-12-The importance of style in academic writing</a></p>
<p>15 0.95907831 <a title="803-lda-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-30-I_posted_this_as_a_comment_on_a_sociology_blog.html">2353 andrew gelman stats-2014-05-30-I posted this as a comment on a sociology blog</a></p>
<p>16 0.95858347 <a title="803-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-13-How_should_journals_handle_replication_studies%3F.html">762 andrew gelman stats-2011-06-13-How should journals handle replication studies?</a></p>
<p>17 0.95845807 <a title="803-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-30-rms2.html">981 andrew gelman stats-2011-10-30-rms2</a></p>
<p>18 0.95343053 <a title="803-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-19-%E2%80%9CConfirmation%2C_on_the_other_hand%2C_is_not_sexy%E2%80%9D.html">1683 andrew gelman stats-2013-01-19-“Confirmation, on the other hand, is not sexy”</a></p>
<p>19 0.95339572 <a title="803-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-The_reverse-journal-submission_system.html">1393 andrew gelman stats-2012-06-26-The reverse-journal-submission system</a></p>
<p>20 0.95307708 <a title="803-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-30-%E2%80%9CTragedy_of_the_science-communication_commons%E2%80%9D.html">1833 andrew gelman stats-2013-04-30-“Tragedy of the science-communication commons”</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
