<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>501 andrew gelman stats-2011-01-04-A new R package for fititng multilevel models</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-501" href="#">andrew_gelman_stats-2011-501</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>501 andrew gelman stats-2011-01-04-A new R package for fititng multilevel models</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-501-html" href="http://andrewgelman.com/2011/01/04/a_new_r_package/">html</a></p><p>Introduction: Joscha Legewie points to  this article  by Lars Ronnegard, Xia Shen, and Moudud Alam, “hglm: A Package for Fitting Hierarchical Generalized Linear Models,” which just appeared in the R journal.  This new package has the advantage, compared to lmer(), of allowing non-normal distributions for the varying coefficients.  On the downside, they seem to have reverted to the ugly lme-style syntax (for example, “fixed = y ~ week, random = ~ 1|ID” rather than “y ~ week + (1|D)”).  The old-style syntax has difficulties handling non-nested grouping factors.  They also say they can estimated models with correlated random effects, but isn’t that just the same as varying-intercept, varying-slope models, which lmer (or Stata alternatives such as gllam) can already do?  There’s also a bunch of stuff on H-likelihood theory, which seems pretty pointless to me (although probably it won’t do much harm either).
 
In any case, this package might be useful to some of you, hence this note.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Joscha Legewie points to  this article  by Lars Ronnegard, Xia Shen, and Moudud Alam, “hglm: A Package for Fitting Hierarchical Generalized Linear Models,” which just appeared in the R journal. [sent-1, score-0.165]
</p><p>2 This new package has the advantage, compared to lmer(), of allowing non-normal distributions for the varying coefficients. [sent-2, score-0.784]
</p><p>3 On the downside, they seem to have reverted to the ugly lme-style syntax (for example, “fixed = y ~ week, random = ~ 1|ID” rather than “y ~ week + (1|D)”). [sent-3, score-0.963]
</p><p>4 The old-style syntax has difficulties handling non-nested grouping factors. [sent-4, score-0.832]
</p><p>5 They also say they can estimated models with correlated random effects, but isn’t that just the same as varying-intercept, varying-slope models, which lmer (or Stata alternatives such as gllam) can already do? [sent-5, score-1.137]
</p><p>6 There’s also a bunch of stuff on H-likelihood theory, which seems pretty pointless to me (although probably it won’t do much harm either). [sent-6, score-0.7]
</p><p>7 In any case, this package might be useful to some of you, hence this note. [sent-7, score-0.519]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('syntax', 0.369), ('package', 0.344), ('lmer', 0.332), ('gllam', 0.219), ('grouping', 0.197), ('week', 0.196), ('id', 0.169), ('random', 0.166), ('models', 0.164), ('downside', 0.163), ('pointless', 0.158), ('handling', 0.152), ('harm', 0.147), ('alternatives', 0.141), ('stata', 0.138), ('allowing', 0.137), ('generalized', 0.134), ('ugly', 0.13), ('varying', 0.12), ('difficulties', 0.114), ('correlated', 0.112), ('advantage', 0.109), ('appeared', 0.108), ('hence', 0.105), ('fixed', 0.103), ('fitting', 0.102), ('distributions', 0.097), ('linear', 0.097), ('estimated', 0.096), ('hierarchical', 0.094), ('note', 0.091), ('bunch', 0.086), ('compared', 0.086), ('stuff', 0.084), ('won', 0.079), ('although', 0.077), ('theory', 0.074), ('isn', 0.073), ('either', 0.072), ('useful', 0.07), ('probably', 0.069), ('already', 0.069), ('effects', 0.068), ('also', 0.057), ('seem', 0.057), ('points', 0.057), ('pretty', 0.054), ('case', 0.047), ('rather', 0.045), ('seems', 0.045)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="501-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-04-A_new_R_package_for_fititng_multilevel_models.html">501 andrew gelman stats-2011-01-04-A new R package for fititng multilevel models</a></p>
<p>Introduction: Joscha Legewie points to  this article  by Lars Ronnegard, Xia Shen, and Moudud Alam, “hglm: A Package for Fitting Hierarchical Generalized Linear Models,” which just appeared in the R journal.  This new package has the advantage, compared to lmer(), of allowing non-normal distributions for the varying coefficients.  On the downside, they seem to have reverted to the ugly lme-style syntax (for example, “fixed = y ~ week, random = ~ 1|ID” rather than “y ~ week + (1|D)”).  The old-style syntax has difficulties handling non-nested grouping factors.  They also say they can estimated models with correlated random effects, but isn’t that just the same as varying-intercept, varying-slope models, which lmer (or Stata alternatives such as gllam) can already do?  There’s also a bunch of stuff on H-likelihood theory, which seems pretty pointless to me (although probably it won’t do much harm either).
 
In any case, this package might be useful to some of you, hence this note.</p><p>2 0.32582426 <a title="501-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-19-R_package_for_Bayes_factors.html">1682 andrew gelman stats-2013-01-19-R package for Bayes factors</a></p>
<p>Introduction: Richard Morey writes:
  
You and your blog readers may be interested to know that a we’ve released a major new version of the BayesFactor package to CRAN. The package computes Bayes factors for linear mixed models and regression models. Of course, I’m aware you don’t like point-null model comparisons, but the package does more than that; it also allows sampling from posterior distributions of the compared models, in much the same way that your arm package does with lmer objects. The sampling (both for the Bayes factors and posteriors) is quite fast, since the back end is written in C.


Some basic examples using the package can be found  here , and the CRAN page is  here .
  
Indeed I don’t like point-null model comparisons . . . but maybe this will be useful to some of you!</p><p>3 0.19524369 <a title="501-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-21-Lessons_learned_from_a_recent_R_package_submission.html">1134 andrew gelman stats-2012-01-21-Lessons learned from a recent R package submission</a></p>
<p>Introduction: R has zillions of packages, and people are submitting  new ones each day .  The volunteers who keep R going are doing an incredibly useful service to the profession, and they’re  busy .
 
   
 
A colleague sends in some suugestions based on a recent experience with a package update:
  
 1. Always use the R dev version to write a package.  Not the current stable release.  The R people use the R dev version to check your package anyway.  If you don’t use the R dev version, there is chance that your package won’t pass the check.  In my own experience, every time R has a major change, it tends to have new standards and find new errors in your package with these new standards.  So better use the dev version to find out the potential errors in advance.


 2. After submission, write an email to claim it.   I used to submit the package to the CRAN without writing an email.  This was standard operating procedure, but it has changed. Writing an email to claim about the submission is now a requir</p><p>4 0.15192847 <a title="501-tfidf-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-10-R_vs._Stata%2C_or%2C_Different_ways_to_estimate_multilevel_models.html">269 andrew gelman stats-2010-09-10-R vs. Stata, or, Different ways to estimate multilevel models</a></p>
<p>Introduction: Cyrus writes:
  
I [Cyrus] was teaching a class on multilevel modeling, and we were playing around with different method to fit a random effects logit model with 2 random intercepts—one corresponding to “family” and another corresponding to “community” (labeled “mom” and “cluster” in the data, respectively).  There are also a few regressors at the individual, family, and community level.  We were replicating in part some of the results from the  following paper :  Improved estimation procedures for multilevel models with binary response: a case-study, by G Rodriguez, N Goldman.


(I say “replicating in part” because we didn’t include all the regressors that they use, only a subset.)  We were looking at the performance of estimation via glmer in R’s lme4 package, glmmPQL in R’s MASS package, and Stata’s xtmelogit.  We wanted to study the performance of various estimation methods, including adaptive quadrature methods and penalized quasi-likelihood.


I was shocked to discover that glmer</p><p>5 0.13347904 <a title="501-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-17-Hierarchical-multilevel_modeling_with_%E2%80%9Cbig_data%E2%80%9D.html">1267 andrew gelman stats-2012-04-17-Hierarchical-multilevel modeling with “big data”</a></p>
<p>Introduction: Dean Eckles writes:
  
I make extensive use of random effects models in my academic and industry research, as they are very often appropriate.


However, with very large data sets, I am not sure what to do. Say I have thousands of levels of a grouping factor, and the number of observations totals in the billions. Despite having lots of observations, I am often either dealing with (a) small effects or (b) trying to fit models with many predictors.


So I would really like to use a random effects model to borrow strength across the levels of the grouping factor, but I am not sure how to practically do this. Are you aware of any approaches to fitting random effects models (including approximations) that work for very large data sets? For example, applying a procedure to each group, and then using the results of this to shrink each fit in some appropriate way.


Just to clarify, here I am only worried about the non-crossed and in fact single-level case. I don’t see any easy route for cross</p><p>6 0.11565842 <a title="501-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-19-R_package_for_effect_size_calculations_for_psychology_researchers.html">2069 andrew gelman stats-2013-10-19-R package for effect size calculations for psychology researchers</a></p>
<p>7 0.11533605 <a title="501-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-08-Multilevel_regression_with_shrinkage_for_%E2%80%9Cfixed%E2%80%9D_effects.html">653 andrew gelman stats-2011-04-08-Multilevel regression with shrinkage for “fixed” effects</a></p>
<p>8 0.10988834 <a title="501-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-Multilevel_modeling_even_when_you%E2%80%99re_not_interested_in_predictions_for_new_groups.html">1194 andrew gelman stats-2012-03-04-Multilevel modeling even when you’re not interested in predictions for new groups</a></p>
<p>9 0.10810836 <a title="501-tfidf-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-31-Somewhat_Bayesian_multilevel_modeling.html">246 andrew gelman stats-2010-08-31-Somewhat Bayesian multilevel modeling</a></p>
<p>10 0.10338886 <a title="501-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-30-Fixed_effects%2C_followed_by_Bayes_shrinkage%3F.html">1644 andrew gelman stats-2012-12-30-Fixed effects, followed by Bayes shrinkage?</a></p>
<p>11 0.10172385 <a title="501-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-29-Decline_Effect_in_Linguistics%3F.html">1400 andrew gelman stats-2012-06-29-Decline Effect in Linguistics?</a></p>
<p>12 0.098323032 <a title="501-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-12-year_%2B_%281%7Cyear%29.html">851 andrew gelman stats-2011-08-12-year + (1|year)</a></p>
<p>13 0.097943634 <a title="501-tfidf-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-10-Two_great_tastes_that_taste_great_together.html">25 andrew gelman stats-2010-05-10-Two great tastes that taste great together</a></p>
<p>14 0.0960363 <a title="501-tfidf-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-17-So-called_fixed_and_random_effects.html">472 andrew gelman stats-2010-12-17-So-called fixed and random effects</a></p>
<p>15 0.095098801 <a title="501-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-04-Handy_Matrix_Cheat_Sheet%2C_with_Gradients.html">555 andrew gelman stats-2011-02-04-Handy Matrix Cheat Sheet, with Gradients</a></p>
<p>16 0.094665639 <a title="501-tfidf-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-17-Clutering_and_variance_components.html">417 andrew gelman stats-2010-11-17-Clutering and variance components</a></p>
<p>17 0.091773085 <a title="501-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-03-Uncertainty_in_parameter_estimates_using_multilevel_models.html">1966 andrew gelman stats-2013-08-03-Uncertainty in parameter estimates using multilevel models</a></p>
<p>18 0.089714877 <a title="501-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-02-Fixed_effects_and_identification.html">1241 andrew gelman stats-2012-04-02-Fixed effects and identification</a></p>
<p>19 0.088533685 <a title="501-tfidf-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-04-That_half-Cauchy_prior.html">184 andrew gelman stats-2010-08-04-That half-Cauchy prior</a></p>
<p>20 0.085970446 <a title="501-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-03-How_best_to_compare_effects_measured_in_two_different_time_periods%3F.html">2086 andrew gelman stats-2013-11-03-How best to compare effects measured in two different time periods?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.119), (1, 0.073), (2, 0.019), (3, -0.004), (4, 0.061), (5, 0.019), (6, 0.009), (7, -0.061), (8, 0.042), (9, 0.017), (10, -0.0), (11, -0.019), (12, 0.013), (13, -0.02), (14, 0.036), (15, -0.003), (16, -0.034), (17, 0.028), (18, -0.021), (19, 0.013), (20, -0.009), (21, -0.024), (22, 0.013), (23, 0.031), (24, -0.023), (25, -0.049), (26, -0.073), (27, 0.11), (28, 0.017), (29, -0.015), (30, -0.034), (31, 0.026), (32, -0.01), (33, -0.063), (34, 0.025), (35, -0.033), (36, -0.063), (37, -0.015), (38, -0.045), (39, -0.015), (40, -0.029), (41, 0.017), (42, 0.048), (43, 0.02), (44, 0.018), (45, 0.022), (46, -0.044), (47, 0.001), (48, -0.005), (49, -0.105)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98187494 <a title="501-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-04-A_new_R_package_for_fititng_multilevel_models.html">501 andrew gelman stats-2011-01-04-A new R package for fititng multilevel models</a></p>
<p>Introduction: Joscha Legewie points to  this article  by Lars Ronnegard, Xia Shen, and Moudud Alam, “hglm: A Package for Fitting Hierarchical Generalized Linear Models,” which just appeared in the R journal.  This new package has the advantage, compared to lmer(), of allowing non-normal distributions for the varying coefficients.  On the downside, they seem to have reverted to the ugly lme-style syntax (for example, “fixed = y ~ week, random = ~ 1|ID” rather than “y ~ week + (1|D)”).  The old-style syntax has difficulties handling non-nested grouping factors.  They also say they can estimated models with correlated random effects, but isn’t that just the same as varying-intercept, varying-slope models, which lmer (or Stata alternatives such as gllam) can already do?  There’s also a bunch of stuff on H-likelihood theory, which seems pretty pointless to me (although probably it won’t do much harm either).
 
In any case, this package might be useful to some of you, hence this note.</p><p>2 0.75700641 <a title="501-lsi-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-10-R_vs._Stata%2C_or%2C_Different_ways_to_estimate_multilevel_models.html">269 andrew gelman stats-2010-09-10-R vs. Stata, or, Different ways to estimate multilevel models</a></p>
<p>Introduction: Cyrus writes:
  
I [Cyrus] was teaching a class on multilevel modeling, and we were playing around with different method to fit a random effects logit model with 2 random intercepts—one corresponding to “family” and another corresponding to “community” (labeled “mom” and “cluster” in the data, respectively).  There are also a few regressors at the individual, family, and community level.  We were replicating in part some of the results from the  following paper :  Improved estimation procedures for multilevel models with binary response: a case-study, by G Rodriguez, N Goldman.


(I say “replicating in part” because we didn’t include all the regressors that they use, only a subset.)  We were looking at the performance of estimation via glmer in R’s lme4 package, glmmPQL in R’s MASS package, and Stata’s xtmelogit.  We wanted to study the performance of various estimation methods, including adaptive quadrature methods and penalized quasi-likelihood.


I was shocked to discover that glmer</p><p>3 0.73743725 <a title="501-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-19-R_package_for_Bayes_factors.html">1682 andrew gelman stats-2013-01-19-R package for Bayes factors</a></p>
<p>Introduction: Richard Morey writes:
  
You and your blog readers may be interested to know that a we’ve released a major new version of the BayesFactor package to CRAN. The package computes Bayes factors for linear mixed models and regression models. Of course, I’m aware you don’t like point-null model comparisons, but the package does more than that; it also allows sampling from posterior distributions of the compared models, in much the same way that your arm package does with lmer objects. The sampling (both for the Bayes factors and posteriors) is quite fast, since the back end is written in C.


Some basic examples using the package can be found  here , and the CRAN page is  here .
  
Indeed I don’t like point-null model comparisons . . . but maybe this will be useful to some of you!</p><p>4 0.71151024 <a title="501-lsi-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Computer_models_of_the_oil_spill.html">243 andrew gelman stats-2010-08-30-Computer models of the oil spill</a></p>
<p>Introduction: Chris Wilson points me to  this visualizatio n of three physical models of the oil spill in the Gulf of Mexico.  Cool (and scary) stuff.  Wilson writes:
  
One of the major advantages is that the models are 3D and show the plumes and tails beneath the surface. One of the major disadvantages is that theyâ&euro;&trade;re still just models.</p><p>5 0.70992744 <a title="501-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-04-Handy_Matrix_Cheat_Sheet%2C_with_Gradients.html">555 andrew gelman stats-2011-02-04-Handy Matrix Cheat Sheet, with Gradients</a></p>
<p>Introduction: This post is an (unpaid) advertisement for the following extremely useful resource:
  
 Petersen, K. B. and M. S. Pedersen. 2008.   The Matrix Cookbook  .  Tehcnical Report, Technical University of Denmark. 
  
It contains 70+ pages of useful relations and derivations involving matrices.  What grabbed my eye was the computation of gradients for matrix operations ranging from eigenvalues and determinants to multivariate normal density functions.   I had no idea the multivariate normal had such a clean gradient (see section 8).
  

 
We’ve been playing around with  Hamiltonian (aka Hybrid) Monte Carlo  for sampling from the posterior of hierarchical generalized linear models with lots of interactions.  HMC speeds up Metropolis sampling by using the gradient of the log probability to drive samples in the direction of higher probability density, which is particularly useful for correlated parameters that mix slowly with standard Gibbs sampling.   Matt “III” Hoffman ‘s already got it workin</p><p>6 0.70623916 <a title="501-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-29-The_gradual_transition_to_replicable_science.html">2117 andrew gelman stats-2013-11-29-The gradual transition to replicable science</a></p>
<p>7 0.68949127 <a title="501-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-03-Hierarchical_array_priors_for_ANOVA_decompositions.html">1786 andrew gelman stats-2013-04-03-Hierarchical array priors for ANOVA decompositions</a></p>
<p>8 0.65508324 <a title="501-lsi-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-18-Derivative-based_MCMC_as_a_breakthrough_technique_for_implementing_Bayesian_statistics.html">419 andrew gelman stats-2010-11-18-Derivative-based MCMC as a breakthrough technique for implementing Bayesian statistics</a></p>
<p>9 0.6526255 <a title="501-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-03-How_best_to_compare_effects_measured_in_two_different_time_periods%3F.html">2086 andrew gelman stats-2013-11-03-How best to compare effects measured in two different time periods?</a></p>
<p>10 0.65112841 <a title="501-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-17-Hierarchical-multilevel_modeling_with_%E2%80%9Cbig_data%E2%80%9D.html">1267 andrew gelman stats-2012-04-17-Hierarchical-multilevel modeling with “big data”</a></p>
<p>11 0.64850146 <a title="501-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-Multilevel_modeling_even_when_you%E2%80%99re_not_interested_in_predictions_for_new_groups.html">1194 andrew gelman stats-2012-03-04-Multilevel modeling even when you’re not interested in predictions for new groups</a></p>
<p>12 0.64817309 <a title="501-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-25-Correlation_of_1_._._._too_good_to_be_true%3F.html">1737 andrew gelman stats-2013-02-25-Correlation of 1 . . . too good to be true?</a></p>
<p>13 0.64110821 <a title="501-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-08-Multilevel_regression_with_shrinkage_for_%E2%80%9Cfixed%E2%80%9D_effects.html">653 andrew gelman stats-2011-04-08-Multilevel regression with shrinkage for “fixed” effects</a></p>
<p>14 0.63819975 <a title="501-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-30-Fixed_effects%2C_followed_by_Bayes_shrinkage%3F.html">1644 andrew gelman stats-2012-12-30-Fixed effects, followed by Bayes shrinkage?</a></p>
<p>15 0.62504113 <a title="501-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-21-BDA3_table_of_contents_%28also_a_new_paper_on_visualization%29.html">1991 andrew gelman stats-2013-08-21-BDA3 table of contents (also a new paper on visualization)</a></p>
<p>16 0.61892933 <a title="501-lsi-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-12-Finite-population_standard_deviation_in_a_hierarchical_model.html">464 andrew gelman stats-2010-12-12-Finite-population standard deviation in a hierarchical model</a></p>
<p>17 0.61756009 <a title="501-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-18-What_to_read_to_catch_up_on_multivariate_statistics%3F.html">1726 andrew gelman stats-2013-02-18-What to read to catch up on multivariate statistics?</a></p>
<p>18 0.61498863 <a title="501-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-02-Fixed_effects_and_identification.html">1241 andrew gelman stats-2012-04-02-Fixed effects and identification</a></p>
<p>19 0.60009211 <a title="501-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>20 0.59902728 <a title="501-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-11-%E2%80%9C2_level_logit_with_2_REs_%26_large_sample._computational_nightmare_%E2%80%93_please_help%E2%80%9D.html">759 andrew gelman stats-2011-06-11-“2 level logit with 2 REs & large sample. computational nightmare – please help”</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(16, 0.032), (17, 0.021), (24, 0.11), (43, 0.024), (45, 0.143), (55, 0.047), (72, 0.022), (82, 0.033), (85, 0.047), (86, 0.07), (99, 0.328)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96288025 <a title="501-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-04-A_new_R_package_for_fititng_multilevel_models.html">501 andrew gelman stats-2011-01-04-A new R package for fititng multilevel models</a></p>
<p>Introduction: Joscha Legewie points to  this article  by Lars Ronnegard, Xia Shen, and Moudud Alam, “hglm: A Package for Fitting Hierarchical Generalized Linear Models,” which just appeared in the R journal.  This new package has the advantage, compared to lmer(), of allowing non-normal distributions for the varying coefficients.  On the downside, they seem to have reverted to the ugly lme-style syntax (for example, “fixed = y ~ week, random = ~ 1|ID” rather than “y ~ week + (1|D)”).  The old-style syntax has difficulties handling non-nested grouping factors.  They also say they can estimated models with correlated random effects, but isn’t that just the same as varying-intercept, varying-slope models, which lmer (or Stata alternatives such as gllam) can already do?  There’s also a bunch of stuff on H-likelihood theory, which seems pretty pointless to me (although probably it won’t do much harm either).
 
In any case, this package might be useful to some of you, hence this note.</p><p>2 0.9615941 <a title="501-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-04-A_Wikipedia_whitewash.html">69 andrew gelman stats-2010-06-04-A Wikipedia whitewash</a></p>
<p>Introduction: After hearing a few times about the divorce predictions of researchers John Gottman and James Murray (work that was featured in Blink with a claim that they could predict with 83 percent accuracy whether a couple would be divorced–after meeting with them for 15 minutes) and  feeling some skepticism , I decided to do the Lord’s work and amend Gottman’s wikipedia entry, which had a paragraph saying:
  
Gottman found his methodology predicts with 90% accuracy which newlywed couples will remain married and which will divorce four to six years later. It is also 81% percent accurate in predicting which marriages will survive after seven to nine years.
  
I added the following:
  
Gottman’s claim of 81% or 90% accuracy is misleading, however, because the accuracy is measured only  after  fitting a model to his data. There is no evidence that he can predict the outcome of a marriage with high accuracy in advance. As Laurie Abraham writes, “For the 1998 study, which focused on videotapes of 57</p><p>3 0.96036386 <a title="501-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-17-More_on_the_difficulty_of_%E2%80%9Cpreaching_what_you_practice%E2%80%9D.html">1325 andrew gelman stats-2012-05-17-More on the difficulty of “preaching what you practice”</a></p>
<p>Introduction: A couple months ago, in discussing Charles Murray’s argument that America’s social leaders should “preach what they practice” (Murray argues that they—we!—tend to lead good lives of hard work and moderation but are all too tolerant of antisocial and unproductive behavior among the lower classes), I  wrote : 
  
  
Murray does not consider the case of Joe Paterno, but in many ways the Penn State football coach fits his story well. Paterno was said to live an exemplary personal and professional life, combining traditional morality with football success—but, by his actions, he showed little concern about the morality of his players and coaches. At a professional level, Paterno rose higher and higher, and in his personal life he was a responsible adult. But he had an increasing disconnect with the real world, to the extent that horrible crimes were occurring nearby (in the physical and social senses) but he was completely insulated from the consequences for many years. Paterno’s story is s</p><p>4 0.95458579 <a title="501-lda-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-13-Indiemapper_makes_thematic_mapping_easy.html">206 andrew gelman stats-2010-08-13-Indiemapper makes thematic mapping easy</a></p>
<p>Introduction: Arthur Breitman writes:
  
I had to forward  this  to you when I read about it…
  
My reply:  Interesting; thanks.  Things like this make me feel so computer-incompetent!  The younger generation is passing me by…</p><p>5 0.9545573 <a title="501-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-20-Upper-income_people_still_don%E2%80%99t_realize_they%E2%80%99re_upper-income.html">673 andrew gelman stats-2011-04-20-Upper-income people still don’t realize they’re upper-income</a></p>
<p>Introduction: Catherine Rampell  highlights  this stunning  Gallup Poll  result:
  
6 percent of Americans in households earning over $250,000 a year think their taxes are “too low.” Of that same group, 26 percent said their taxes were “about right,” and a whopping 67 percent said their taxes were “too high.”
  
OK, fine.  Most people don’t like taxes.  No surprise there.  But get this next part:
  
And yet when this same group of high earners was asked whether “upper-income people” paid their fair share in taxes, 30 percent said “upper-income people” paid too little, 30 percent said it was a “fair share,” and 38 percent said it was too much.
  
30 percent of these upper-income people say that upper-income people pay too little, but only 6 percent say that  they personally  pay too little.  38% say that upper-income people pay too much, but 67% say  they personally  pay too much.
  

 
Rampell attributes this to people’s ignorance about population statistics–these 250K+ families just don’t realize t</p><p>6 0.95378113 <a title="501-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-28-New_app_for_learning_intro_statistics.html">735 andrew gelman stats-2011-05-28-New app for learning intro statistics</a></p>
<p>7 0.95242941 <a title="501-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-09-I_was_at_a_meeting_a_couple_months_ago_._._..html">999 andrew gelman stats-2011-11-09-I was at a meeting a couple months ago . . .</a></p>
<p>8 0.95078439 <a title="501-lda-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-22-A_redrawing_of_the_Red-Blue_map_in_November_2010%3F.html">362 andrew gelman stats-2010-10-22-A redrawing of the Red-Blue map in November 2010?</a></p>
<p>9 0.95055759 <a title="501-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-04-Generalized_Method_of_Moments%2C_whatever_that_is.html">449 andrew gelman stats-2010-12-04-Generalized Method of Moments, whatever that is</a></p>
<p>10 0.95044988 <a title="501-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-20-Could_someone_please_lock_this_guy_and_Niall_Ferguson_in_a_room_together%3F.html">1504 andrew gelman stats-2012-09-20-Could someone please lock this guy and Niall Ferguson in a room together?</a></p>
<p>11 0.94774395 <a title="501-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-13-A_Structural_Comparison_of_Conspicuous_Consumption_in_China_and_the_United_States.html">1854 andrew gelman stats-2013-05-13-A Structural Comparison of Conspicuous Consumption in China and the United States</a></p>
<p>12 0.94687462 <a title="501-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-27-Richard_Stallman_and_John_McCarthy.html">1031 andrew gelman stats-2011-11-27-Richard Stallman and John McCarthy</a></p>
<p>13 0.94342899 <a title="501-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-08-Turning_pages_into_data.html">192 andrew gelman stats-2010-08-08-Turning pages into data</a></p>
<p>14 0.94117296 <a title="501-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-16-Blog_bribes%21.html">1012 andrew gelman stats-2011-11-16-Blog bribes!</a></p>
<p>15 0.94088089 <a title="501-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-28-NYT_shills_for_personal_DNA_tests.html">543 andrew gelman stats-2011-01-28-NYT shills for personal DNA tests</a></p>
<p>16 0.94079477 <a title="501-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-07-Free_advice_from_an_academic_writing_coach%21.html">1658 andrew gelman stats-2013-01-07-Free advice from an academic writing coach!</a></p>
<p>17 0.9392485 <a title="501-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-17-The_disappearing_or_non-disappearing_middle_class.html">1767 andrew gelman stats-2013-03-17-The disappearing or non-disappearing middle class</a></p>
<p>18 0.93891519 <a title="501-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-24-A_%28not_quite%29_grand_unified_theory_of_plagiarism%2C_as_applied_to_the_Wegman_case.html">728 andrew gelman stats-2011-05-24-A (not quite) grand unified theory of plagiarism, as applied to the Wegman case</a></p>
<p>19 0.93726051 <a title="501-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-23-More_on_those_divorce_prediction_statistics%2C_including_a_discussion_of_the_innumeracy_of_%28some%29_mathematicians.html">105 andrew gelman stats-2010-06-23-More on those divorce prediction statistics, including a discussion of the innumeracy of (some) mathematicians</a></p>
<p>20 0.93690026 <a title="501-lda-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-28-History_is_too_important_to_be_left_to_the_history_professors.html">2189 andrew gelman stats-2014-01-28-History is too important to be left to the history professors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
