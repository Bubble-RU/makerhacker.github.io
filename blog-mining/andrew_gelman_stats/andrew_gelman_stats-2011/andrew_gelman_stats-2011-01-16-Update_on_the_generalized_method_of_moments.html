<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>519 andrew gelman stats-2011-01-16-Update on the generalized method of moments</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2011" href="../home/andrew_gelman_stats-2011_home.html">andrew_gelman_stats-2011</a> <a title="andrew_gelman_stats-2011-519" href="#">andrew_gelman_stats-2011-519</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>519 andrew gelman stats-2011-01-16-Update on the generalized method of moments</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2011-519-html" href="http://andrewgelman.com/2011/01/16/update_on_the_g/">html</a></p><p>Introduction: After reading all the comments  here  I remembered that I’ve actually written  a paper  on the generalized method of moments–including the bit about maximum likelihood being a special case.  The basic idea is simple enough that it must have been rediscovered dozens of times by different people (sort of like the  trapezoidal rule ).
 
In our case, we were motivated to (independently) develop the (well-known, but not by me) generalized method of moments as a way of specifying an indirectly-parameterized prior distribution, rather than as a way of estimating parameters from direct data.  But the math is the same.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 After reading all the comments  here  I remembered that I’ve actually written  a paper  on the generalized method of moments–including the bit about maximum likelihood being a special case. [sent-1, score-1.699]
</p><p>2 The basic idea is simple enough that it must have been rediscovered dozens of times by different people (sort of like the  trapezoidal rule ). [sent-2, score-1.537]
</p><p>3 In our case, we were motivated to (independently) develop the (well-known, but not by me) generalized method of moments as a way of specifying an indirectly-parameterized prior distribution, rather than as a way of estimating parameters from direct data. [sent-3, score-2.203]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('moments', 0.405), ('generalized', 0.356), ('trapezoidal', 0.29), ('rediscovered', 0.262), ('remembered', 0.229), ('specifying', 0.217), ('method', 0.211), ('independently', 0.205), ('dozens', 0.185), ('maximum', 0.164), ('develop', 0.155), ('motivated', 0.146), ('math', 0.143), ('rule', 0.135), ('estimating', 0.132), ('likelihood', 0.128), ('special', 0.125), ('direct', 0.123), ('basic', 0.117), ('parameters', 0.117), ('written', 0.106), ('distribution', 0.1), ('must', 0.099), ('prior', 0.099), ('reading', 0.094), ('times', 0.093), ('comments', 0.092), ('including', 0.092), ('way', 0.091), ('simple', 0.09), ('enough', 0.072), ('bit', 0.071), ('sort', 0.066), ('idea', 0.065), ('actually', 0.063), ('case', 0.062), ('paper', 0.06), ('rather', 0.06), ('different', 0.057), ('ve', 0.049), ('people', 0.04), ('like', 0.032)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="519-tfidf-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-16-Update_on_the_generalized_method_of_moments.html">519 andrew gelman stats-2011-01-16-Update on the generalized method of moments</a></p>
<p>Introduction: After reading all the comments  here  I remembered that I’ve actually written  a paper  on the generalized method of moments–including the bit about maximum likelihood being a special case.  The basic idea is simple enough that it must have been rediscovered dozens of times by different people (sort of like the  trapezoidal rule ).
 
In our case, we were motivated to (independently) develop the (well-known, but not by me) generalized method of moments as a way of specifying an indirectly-parameterized prior distribution, rather than as a way of estimating parameters from direct data.  But the math is the same.</p><p>2 0.18673417 <a title="519-tfidf-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-02-The_problem_of_overestimation_of_group-level_variance_parameters.html">63 andrew gelman stats-2010-06-02-The problem of overestimation of group-level variance parameters</a></p>
<p>Introduction: John Lawson writes:
  
I have been experimenting using Bayesian Methods to estimate variance components, and I have noticed that even when I use a noninformative prior, my estimates are never close to the method of moments or REML estimates. In every case I have tried, the sum of the Bayesian estimated variance components is always larger than the sum of the estimates obtained by method of moments or REML.
      
For data sets I have used that arise from a simple one-way random effects model, the Bayesian estimates of the between groups variance component is usually larger than the method of moments or REML estimates. When I use a uniform prior on the between standard deviation (as you recommended in  your 2006 paper ) rather than an inverse gamma prior on the between variance component, the between variance component is usually reduced.  However, for the dyestuff data in Davies(1949, p74), the opposite appears to be the case.


I am a worried that the Bayesian estimators of the varian</p><p>3 0.16602357 <a title="519-tfidf-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-04-Generalized_Method_of_Moments%2C_whatever_that_is.html">449 andrew gelman stats-2010-12-04-Generalized Method of Moments, whatever that is</a></p>
<p>Introduction: Xuequn Hu writes:
  
I am an econ doctoral student, trying to do some empirical work using Bayesian methods.  Recently I read a paper(and its discussion) that pitches Bayesian methods against GMM (Generalized Method of Moments), which is quite popular in econometrics for frequentists. I am wondering if you can, here or on your blog, give some insights about these two methods, from the perspective of a Bayesian statistician. I know GMM does not conform to likelihood principle, but Bayesian are often charged with strong distribution assumptions.  
  
I can’t actually help on this, since I don’t know what GMM is.  My guess is that, like other methods that don’t explicitly use prior estimation, this method will work well if sufficient information is included as data.  Which would imply a hierarchical structure.</p><p>4 0.13885976 <a title="519-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>Introduction: For awhile I’ve been fitting most of my multilevel models using lmer/glmer, which gives point estimates of the group-level variance parameters (maximum marginal likelihood estimate for lmer and an approximation for glmer).  I’m usually satisfied with this–sure, point estimation understates the uncertainty in model fitting, but that’s typically the least of our worries. 
 
Sometimes, though, lmer/glmer estimates group-level variances at 0 or estimates group-level correlation parameters at +/- 1.  Typically, when this happens, it’s not that we’re so sure the variance is close to zero or that the correlation is close to 1 or -1; rather, the marginal likelihood does not provide a lot of information about these parameters of the group-level error distribution.
 
I don’t want point estimates on the boundary.  I don’t want to say that the unexplained variance in some dimension is exactly zero.
 
One way to handle this problem is full Bayes:  slap a prior on sigma, do your Gibbs and Metropolis</p><p>5 0.13060451 <a title="519-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>Introduction: Ryan King writes:
  
I was wondering if you have a brief comment on the state of the art for objective priors for hierarchical generalized linear models (generalized linear mixed models).  I have been working off the papers in Bayesian Analysis (2006) 1, Number 3 (Browne and Draper, Kass and Natarajan, Gelman).  There seems to have been continuous work for matching priors in linear mixed models, but GLMMs less so because of the lack of an analytic marginal likelihood for the variance components.  There are a number of additional suggestions in the literature since 2006, but little robust practical guidance.


I’m interested in both mean parameters and the variance components.  I’m almost always concerned with logistic random effect models.  I’m fascinated by the matching-priors idea of higher-order asymptotic improvements to maximum likelihood, and need to make some kind of defensible default recommendation.  Given the massive scale of the datasets (genetics …), extensive sensitivity a</p><p>6 0.12074624 <a title="519-tfidf-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-03-Reinventing_the_wheel%2C_only_more_so..html">447 andrew gelman stats-2010-12-03-Reinventing the wheel, only more so.</a></p>
<p>7 0.11780705 <a title="519-tfidf-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-01-How_does_Bayes_do_it%3F.html">247 andrew gelman stats-2010-09-01-How does Bayes do it?</a></p>
<p>8 0.11758319 <a title="519-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-06-Early_stopping_and_penalized_likelihood.html">788 andrew gelman stats-2011-07-06-Early stopping and penalized likelihood</a></p>
<p>9 0.11426763 <a title="519-tfidf-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>10 0.11000369 <a title="519-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-03-Statistical_methods_that_work_in_some_settings_but_not_others.html">1560 andrew gelman stats-2012-11-03-Statistical methods that work in some settings but not others</a></p>
<p>11 0.10569445 <a title="519-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>12 0.10437253 <a title="519-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>13 0.10106339 <a title="519-tfidf-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-06-Quote_of_the_day.html">398 andrew gelman stats-2010-11-06-Quote of the day</a></p>
<p>14 0.096087664 <a title="519-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-13-Lame_Statistics_Patents.html">1761 andrew gelman stats-2013-03-13-Lame Statistics Patents</a></p>
<p>15 0.095531918 <a title="519-tfidf-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-22-Philosophy_of_Bayes_and_non-Bayes%3A__A_dialogue_with_Deborah_Mayo.html">291 andrew gelman stats-2010-09-22-Philosophy of Bayes and non-Bayes:  A dialogue with Deborah Mayo</a></p>
<p>16 0.094513975 <a title="519-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-25-It%E2%80%99s_binless%21__A_program_for_computing_normalizing_functions.html">1825 andrew gelman stats-2013-04-25-It’s binless!  A program for computing normalizing functions</a></p>
<p>17 0.091550812 <a title="519-tfidf-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-01-bayesglm_in_Stata%3F.html">442 andrew gelman stats-2010-12-01-bayesglm in Stata?</a></p>
<p>18 0.090683162 <a title="519-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-07-Neutral_noninformative_and_informative_conjugate_beta_and_gamma_prior_distributions.html">1046 andrew gelman stats-2011-12-07-Neutral noninformative and informative conjugate beta and gamma prior distributions</a></p>
<p>19 0.088314295 <a title="519-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>20 0.087450162 <a title="519-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-31-Using_sample_size_in_the_prior_distribution.html">547 andrew gelman stats-2011-01-31-Using sample size in the prior distribution</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.118), (1, 0.082), (2, -0.004), (3, 0.031), (4, -0.011), (5, -0.026), (6, 0.093), (7, -0.005), (8, -0.065), (9, 0.015), (10, 0.034), (11, -0.002), (12, 0.002), (13, 0.004), (14, -0.028), (15, -0.018), (16, -0.011), (17, 0.008), (18, 0.02), (19, -0.017), (20, 0.017), (21, -0.035), (22, 0.002), (23, 0.014), (24, 0.034), (25, 0.04), (26, 0.016), (27, 0.044), (28, 0.042), (29, 0.011), (30, -0.003), (31, 0.04), (32, 0.046), (33, 0.012), (34, 0.005), (35, -0.03), (36, 0.009), (37, 0.041), (38, -0.046), (39, 0.011), (40, 0.028), (41, 0.02), (42, -0.014), (43, -0.005), (44, 0.019), (45, -0.021), (46, 0.022), (47, -0.001), (48, 0.009), (49, 0.029)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96768415 <a title="519-lsi-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-16-Update_on_the_generalized_method_of_moments.html">519 andrew gelman stats-2011-01-16-Update on the generalized method of moments</a></p>
<p>Introduction: After reading all the comments  here  I remembered that I’ve actually written  a paper  on the generalized method of moments–including the bit about maximum likelihood being a special case.  The basic idea is simple enough that it must have been rediscovered dozens of times by different people (sort of like the  trapezoidal rule ).
 
In our case, we were motivated to (independently) develop the (well-known, but not by me) generalized method of moments as a way of specifying an indirectly-parameterized prior distribution, rather than as a way of estimating parameters from direct data.  But the math is the same.</p><p>2 0.78883791 <a title="519-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>Introduction: I’ve had a couple of email conversations in the past couple days on dependence in multivariate prior distributions.
 
 Modeling the degrees of freedom and scale parameters in the t distribution 
 
First, in our Stan group we’ve been discussing the choice of priors for the degrees-of-freedom parameter in the t distribution.  I wrote that also there’s the question of parameterization.  It does not necessarily make sense to have independent priors on the df and scale parameters.  In some sense, the meaning of the scale parameter changes with the df.
 
 Prior dependence between correlation and scale parameters in the scaled inverse-Wishart model 
 
The second case of parameterization in prior distribution arose from an email I received from Chris Chatham pointing me to  this exploration  by Matt Simpson of the scaled inverse-Wishart prior distribution for hierarchical covariance matrices.  Simpson writes:
  
A popular prior for Σ is the inverse-Wishart distribution [ not  the same as the</p><p>3 0.76358527 <a title="519-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>Introduction: A student writes:
  
I have a question about an earlier recommendation of yours on the election of the prior distribution for the precision hyperparameter of a normal distribution, and a reference for the recommendation. If I recall correctly I have read that you have suggested to use Gamma(1.4, 0.4) instead of Gamma(0.01,0.01) for the prior distribution of the precision hyper parameter of a normal distribution.


I would very much appreciate if you would have the time to point me to this publication of yours. The reason is that I have used the prior distribution (Gamma(1.4, 0.4)) in a study which we now revise for publication, and where a reviewer question the choice of the distribution (claiming that it is too informative!).


I am well aware of that you in recent publications (Prior distributions for variance parameters in hierarchical models. Bayesian Analysis; Data Analysis using regression and multilevel/hierarchical models) suggest to model the precision as pow(standard deviatio</p><p>4 0.74234247 <a title="519-lsi-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-01-bayesglm_in_Stata%3F.html">442 andrew gelman stats-2010-12-01-bayesglm in Stata?</a></p>
<p>Introduction: Is there an implementation of bayesglm in Stata?  (That is, approximate maximum penalized likelihood estimation with specified normal or t prior distributions on the coefficients.)</p><p>5 0.73259401 <a title="519-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-17-Jumping_off_the_edge_of_the_world.html">858 andrew gelman stats-2011-08-17-Jumping off the edge of the world</a></p>
<p>Introduction: Tomas Iesmantas writes:
  
I’m facing a problem where parameter space is bounded, e.g. all parameters have to be positive. If in MCMC as proposal distribution I use normal distribution, then at some iterations I get negative proposals. So my question is: should I use recalculation of acceptance probability every time I reject the proposal (something like in delayed rejection method), or I have to use another proposal (like lognormal, truncated normal, etc.)?
  
The simplest solution is to just calculate p(theta)=0 for theta outside the legal region, thus reject those jumps.  This will work fine (just remember that when you reject, you have to stay at the last value for one more iteration), but if you’re doing these rejections all the time, you might want to reparameterize your space, for example using logs for positive parameters, logits for constrained parameters, and softmax for parameters that are constrained to sum to 1.</p><p>6 0.71579432 <a title="519-lsi-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-23-Unhappy_with_improvement_by_a_factor_of_10%5E29.html">160 andrew gelman stats-2010-07-23-Unhappy with improvement by a factor of 10^29</a></p>
<p>7 0.71069026 <a title="519-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>8 0.70943105 <a title="519-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-22-The_scaled_inverse_Wishart_prior_distribution_for_a_covariance_matrix_in_a_hierarchical_model.html">1466 andrew gelman stats-2012-08-22-The scaled inverse Wishart prior distribution for a covariance matrix in a hierarchical model</a></p>
<p>9 0.70857042 <a title="519-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-19-Prior_distributions_on_derived_quantities_rather_than_on_parameters_themselves.html">1946 andrew gelman stats-2013-07-19-Prior distributions on derived quantities rather than on parameters themselves</a></p>
<p>10 0.70719129 <a title="519-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>11 0.70663959 <a title="519-lsi-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>12 0.70274353 <a title="519-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-20-Prior_beliefs_about_locations_of_decision_boundaries.html">1130 andrew gelman stats-2012-01-20-Prior beliefs about locations of decision boundaries</a></p>
<p>13 0.6948567 <a title="519-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>14 0.6934942 <a title="519-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-28-Path_sampling_for_models_of_varying_dimension.html">1089 andrew gelman stats-2011-12-28-Path sampling for models of varying dimension</a></p>
<p>15 0.68675882 <a title="519-lsi-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>16 0.67473954 <a title="519-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-31-Using_sample_size_in_the_prior_distribution.html">547 andrew gelman stats-2011-01-31-Using sample size in the prior distribution</a></p>
<p>17 0.66928178 <a title="519-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-10-Cross-validation_and_Bayesian_estimation_of_tuning_parameters.html">2129 andrew gelman stats-2013-12-10-Cross-validation and Bayesian estimation of tuning parameters</a></p>
<p>18 0.66684961 <a title="519-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>19 0.66446871 <a title="519-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-16-The_%E2%80%9CWashington_read%E2%80%9D_and_the_algebra_of_conditional_distributions.html">961 andrew gelman stats-2011-10-16-The “Washington read” and the algebra of conditional distributions</a></p>
<p>20 0.66361898 <a title="519-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-11-%E2%80%9CInformative_g-Priors_for_Logistic_Regression%E2%80%9D.html">2017 andrew gelman stats-2013-09-11-“Informative g-Priors for Logistic Regression”</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(16, 0.021), (24, 0.166), (95, 0.334), (99, 0.331)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97494787 <a title="519-lda-1" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-08-For_chrissake%2C_just_make_up_an_analysis_already%21__We_have_a_lab_here_to_run%2C_y%E2%80%99know%3F.html">1973 andrew gelman stats-2013-08-08-For chrissake, just make up an analysis already!  We have a lab here to run, y’know?</a></p>
<p>Introduction: Ben Hyde sends along  this :
  
Stuck in the middle of the supplemental data, reporting the total workup for their compounds, was this gem:

 
Emma, please insert NMR data here! where are they? and for this compound, just make up an elemental analysis . . .
 
  
I’m reminded of our recent  discussions  of coauthorship, where I argued that I see real advantages to having multiple people taking responsibility for the result.  Jay Verkuilen responded: “On the flipside of collaboration . . . is diffusion of responsibility, where everybody thinks someone else ‘has that problem’ and thus things don’t get solved.”  That’s what seems to have happened (hilariously) here.</p><p>2 0.96006608 <a title="519-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-18-uuuuuuuuuuuuugly.html">1862 andrew gelman stats-2013-05-18-uuuuuuuuuuuuugly</a></p>
<p>Introduction: Hamdan Azhar writes:
  
I came across this graphic of vaccine-attributed decreases in mortality and was curious if you found it as unattractive and unintuitive as I did. Hope all is well with you!
  
My reply:  All’s well with me.  And yes, that’s one horrible graph.  It has all the problems with a bad infographic with none of the virtues.  Compared to this monstrosity, the typical USA Today graph is a stunning, beautiful masterpiece.  I don’t think I want to soil this webpage with the image.  In fact, I don’t even want to link to it.</p><p>3 0.95964575 <a title="519-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-23-Foundation_for_Open_Access_Statistics.html">1820 andrew gelman stats-2013-04-23-Foundation for Open Access Statistics</a></p>
<p>Introduction: Now here’s a foundation I (Bob) can get behind: 
  

 Foundation for Open Access Statistics  (FOAS)

  
Their  mission  is to “promote free software, open access publishing, and reproducible research in statistics.”  To me, that’s like supporting  motherhood and apple pie !
 
FOAS spun out of and is partially designed to support the   Journal of Statistical Software   (aka  JSS , aka  JStatSoft ). I adore  JSS  because it (a) is open access, (b) publishes systems papers on statistical software, (c) has fast reviewing turnaround times, and (d) is free for authors and readers.  One of the next items on my to-do list is to write up the  Stan  modeling language and submit it to  JSS .
 
As a not-for-profit with no visible source of income, they are quite sensibly  asking for donations  (don’t complain — it beats $3K author fees or not being able to read papers).</p><p>4 0.95469743 <a title="519-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-31-Even_a_good_data_display_can_sometimes_be_improved.html">832 andrew gelman stats-2011-07-31-Even a good data display can sometimes be improved</a></p>
<p>Introduction: When I first saw this graphic, I thought “boy, that’s great, sometimes the graphic practically makes itself.” Normally it’s hard to use lots of different colors to differentiate items of interest, because there’s usually not an intuitive mapping between color and item (e.g. for countries, or states, or whatever). But the colors of crayons, what could be more perfect? So this graphic seemed awesome. But, as they discovered after some experimentation at  datapointed.net  there is an even BETTER possibility here. Click the link to see.
 
     Crayola Crayon colors by year</p><p>5 0.9480052 <a title="519-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-09-%E2%80%9CMuch_of_the_recent_reported_drop_in_interstate_migration_is_a_statistical_artifact%E2%80%9D.html">404 andrew gelman stats-2010-11-09-“Much of the recent reported drop in interstate migration is a statistical artifact”</a></p>
<p>Introduction: Greg Kaplan writes:
  
I noticed that you have blogged a little about interstate migration trends in the US, and thought  that you might be interested in  a new working paper  of mine (joint with Sam Schulhofer-Wohl from the Minneapolis Fed) which I have attached.


Briefly, we show that much of the recent reported drop in interstate migration is a statistical artifact: The Census Bureau made an undocumented change in its imputation procedures for missing data in 2006, and this change significantly reduced the number of imputed interstate moves. The change in imputation procedures — not any actual change in migration behavior — explains 90 percent of the reported decrease in interstate migration between the 2005 and 2006 Current Population Surveys, and 42 percent of the decrease between 2000 and 2010.
  
I haven’t had a chance to give a serious look so could only make the quick suggestion to make the graphs smaller and put multiple graphs on a page,  This would allow the reader to bett</p><p>6 0.94689167 <a title="519-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-28-Vaguely_related_to_the_coke-dumping_story.html">876 andrew gelman stats-2011-08-28-Vaguely related to the coke-dumping story</a></p>
<p>7 0.94567859 <a title="519-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-04-30-More_on_problems_with_surveys_estimating_deaths_in_war_zones.html">12 andrew gelman stats-2010-04-30-More on problems with surveys estimating deaths in war zones</a></p>
<p>8 0.94544244 <a title="519-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-08-chartsnthings_%21.html">1308 andrew gelman stats-2012-05-08-chartsnthings !</a></p>
<p>same-blog 9 0.94127685 <a title="519-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-16-Update_on_the_generalized_method_of_moments.html">519 andrew gelman stats-2011-01-16-Update on the generalized method of moments</a></p>
<p>10 0.93614244 <a title="519-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-27-The_most_dangerous_jobs_in_America.html">1086 andrew gelman stats-2011-12-27-The most dangerous jobs in America</a></p>
<p>11 0.93373752 <a title="519-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-13-Help_with_this_problem%2C_win_valuable_prizes.html">1164 andrew gelman stats-2012-02-13-Help with this problem, win valuable prizes</a></p>
<p>12 0.9179306 <a title="519-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-24-How_few_respondents_are_reasonable_to_use_when_calculating_the_average_by_county%3F.html">627 andrew gelman stats-2011-03-24-How few respondents are reasonable to use when calculating the average by county?</a></p>
<p>13 0.89804447 <a title="519-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-09-The_future_of_R.html">266 andrew gelman stats-2010-09-09-The future of R</a></p>
<p>14 0.87973285 <a title="519-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-10-When_you_SHARE_poorly_researched_infographics%E2%80%A6.html">1667 andrew gelman stats-2013-01-10-When you SHARE poorly researched infographics…</a></p>
<p>15 0.87909746 <a title="519-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-11-Yes%2C_the_decision_to_try_%28or_not%29_to_have_a_child_can_be_made_rationally.html">1758 andrew gelman stats-2013-03-11-Yes, the decision to try (or not) to have a child can be made rationally</a></p>
<p>16 0.8767404 <a title="519-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-05-How_accurate_is_your_gaydar%3F.html">944 andrew gelman stats-2011-10-05-How accurate is your gaydar?</a></p>
<p>17 0.87563354 <a title="519-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-25-Correlation_of_1_._._._too_good_to_be_true%3F.html">1737 andrew gelman stats-2013-02-25-Correlation of 1 . . . too good to be true?</a></p>
<p>18 0.87023842 <a title="519-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-01-Back_when_fifty_years_was_a_long_time_ago.html">1646 andrew gelman stats-2013-01-01-Back when fifty years was a long time ago</a></p>
<p>19 0.86904055 <a title="519-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-15-The_UN_Plot_to_Force_Bayesianism_on_Unsuspecting_Americans_%28penalized_B-Spline_edition%29.html">2135 andrew gelman stats-2013-12-15-The UN Plot to Force Bayesianism on Unsuspecting Americans (penalized B-Spline edition)</a></p>
<p>20 0.86754096 <a title="519-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-01-A_graph_at_war_with_its_caption.__Also%2C_how_to_visualize_the_same_numbers_without_giving_the_display_a_misleading_causal_feel%3F.html">1834 andrew gelman stats-2013-05-01-A graph at war with its caption.  Also, how to visualize the same numbers without giving the display a misleading causal feel?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
