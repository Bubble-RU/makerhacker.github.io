<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1197 andrew gelman stats-2012-03-04-“All Models are Right, Most are Useless”</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1197" href="#">andrew_gelman_stats-2012-1197</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1197 andrew gelman stats-2012-03-04-“All Models are Right, Most are Useless”</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1197-html" href="http://andrewgelman.com/2012/03/04/all-models-are-right-most-are-useless/">html</a></p><p>Introduction: The above is the title of  a talk  that Thad Tarpey gave at the Joint Statistical Meetings in 2009.  Here’s the abstract:
  
Students of statistics are often introduced to George Box’s famous quote: “all models are wrong, some are useful.” In this talk I [Tarpey] argue that this quote, although useful, is wrong. A different and more positive perspective is to acknowledge that a model is simply a means of extracting information of interest from data. The truth is infinitely complex and a model is merely an approximation to the truth. If the approximation is poor or misleading, then the model is useless. In this talk I give examples of correct models that are not true models. I illustrate how the notion of a “wrong” model can lead to wrong conclusions.
  
I’m curious what he had to say—maybe he could post the slides?
 
P.S.  And  here they are !</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The above is the title of  a talk  that Thad Tarpey gave at the Joint Statistical Meetings in 2009. [sent-1, score-0.41]
</p><p>2 Here’s the abstract:    Students of statistics are often introduced to George Box’s famous quote: “all models are wrong, some are useful. [sent-2, score-0.451]
</p><p>3 ” In this talk I [Tarpey] argue that this quote, although useful, is wrong. [sent-3, score-0.396]
</p><p>4 A different and more positive perspective is to acknowledge that a model is simply a means of extracting information of interest from data. [sent-4, score-1.02]
</p><p>5 The truth is infinitely complex and a model is merely an approximation to the truth. [sent-5, score-0.977]
</p><p>6 If the approximation is poor or misleading, then the model is useless. [sent-6, score-0.561]
</p><p>7 In this talk I give examples of correct models that are not true models. [sent-7, score-0.622]
</p><p>8 I illustrate how the notion of a “wrong” model can lead to wrong conclusions. [sent-8, score-0.76]
</p><p>9 I’m curious what he had to say—maybe he could post the slides? [sent-9, score-0.168]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tarpey', 0.486), ('approximation', 0.282), ('thad', 0.221), ('talk', 0.221), ('wrong', 0.206), ('quote', 0.196), ('infinitely', 0.193), ('extracting', 0.187), ('model', 0.175), ('meetings', 0.156), ('notion', 0.153), ('acknowledge', 0.149), ('introduced', 0.134), ('slides', 0.132), ('box', 0.131), ('illustrate', 0.125), ('joint', 0.125), ('misleading', 0.118), ('truth', 0.112), ('merely', 0.112), ('george', 0.112), ('models', 0.111), ('curious', 0.111), ('abstract', 0.106), ('poor', 0.104), ('complex', 0.103), ('famous', 0.101), ('lead', 0.101), ('argue', 0.097), ('title', 0.097), ('gave', 0.092), ('correct', 0.087), ('positive', 0.087), ('perspective', 0.085), ('means', 0.08), ('simply', 0.079), ('although', 0.078), ('interest', 0.076), ('examples', 0.076), ('students', 0.073), ('useful', 0.07), ('true', 0.068), ('often', 0.059), ('give', 0.059), ('information', 0.058), ('post', 0.057), ('maybe', 0.049), ('statistics', 0.046), ('different', 0.044), ('statistical', 0.043)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1197-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-%E2%80%9CAll_Models_are_Right%2C_Most_are_Useless%E2%80%9D.html">1197 andrew gelman stats-2012-03-04-“All Models are Right, Most are Useless”</a></p>
<p>Introduction: The above is the title of  a talk  that Thad Tarpey gave at the Joint Statistical Meetings in 2009.  Here’s the abstract:
  
Students of statistics are often introduced to George Box’s famous quote: “all models are wrong, some are useful.” In this talk I [Tarpey] argue that this quote, although useful, is wrong. A different and more positive perspective is to acknowledge that a model is simply a means of extracting information of interest from data. The truth is infinitely complex and a model is merely an approximation to the truth. If the approximation is poor or misleading, then the model is useless. In this talk I give examples of correct models that are not true models. I illustrate how the notion of a “wrong” model can lead to wrong conclusions.
  
I’m curious what he had to say—maybe he could post the slides?
 
P.S.  And  here they are !</p><p>2 0.13108867 <a title="1197-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-01-What_goes_around_._._..html">548 andrew gelman stats-2011-02-01-What goes around . . .</a></p>
<p>Introduction: A few weeks ago I delivered a 10-minute talk on statistical graphics that went so well, it was the best-received talk I’ve ever given.  The crowd was raucous.  Then some poor sap had to go on after me.  He started by saying that my talk was a hard act to follow.  And, indeed, the audience politely listened but did not really get involved in his presentation.  Boy did I feel smug.
 
More recently I gave a talk on Stan, at an entirely different venue.  And this time the story was the exact opposite.  Jim Demmel spoke first and gave a wonderful talk on optimization for linear algebra (it was an applied math conference).  Then I followed, and I never really grabbed the crowd.  My talk was not a disaster but it didn’t really work.  This was particularly frustrating because I’m really excited about Stan and this was a group of researchers I wouldn’t usually have a chance to reach.  It was the plenary session at the conference.
 
Anyway, now I know how that guy felt from last month.  My talk</p><p>3 0.12305146 <a title="1197-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-13-Flexibility_is_good.html">2133 andrew gelman stats-2013-12-13-Flexibility is good</a></p>
<p>Introduction: If I made a separate post for each interesting blog discussion, we’d get overwhelmed.  That’s why I often leave detailed responses in the comments section, even though I’m pretty sure that most readers don’t look in the comments at all.
 
Sometimes, though, I think it’s good to bring such discussions to light.  Here’s a recent example.
 
Michael  wrote :
  
Poor predictive performance usually indicates that the model isn’t sufficiently flexible to explain the data, and my understanding of the proper Bayesian strategy is to feed that back into your original model and try again until you achieve better performance.
  
Corey  replied :
  
It was my impression that — in ML at least — poor predictive performance is more often due to the model being too flexible and fitting noise.
  
And Rahul  agreed :
  
Good point. A very flexible model will describe your training data perfectly and then go bonkers when unleashed on wild data.
  
But I  wrote :
  
Overfitting comes from a model being flex</p><p>4 0.11888925 <a title="1197-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>Introduction: Cosma Shalizi  and  Larry Wasserman  discuss some papers from a conference on Ockham’s Razor.  I don’t have anything new to add on this so let me link to  past blog entries  on the topic and repost the following  from 2004 :
  
A lot has been written in statistics about “parsimony”—that is, the desire to explain phenomena using fewer parameters–but I’ve never seen any good general justification for parsimony.  (I don’t count “Occam’s Razor,” or “Ockham’s Razor,” or whatever, as a justification.  You gotta do better than digging up a 700-year-old quote.)


Maybe it’s because I work in social science, but my feeling is:  if you can approximate reality with just a few parameters, fine.  If you can use more parameters to fold in more information, that’s even better.


In practice, I often use simple models—because they are less effort to fit and, especially, to understand.  But I don’t kid myself that they’re better than more complicated efforts!


My favorite quote on this comes from  Rad</p><p>5 0.1107507 <a title="1197-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-21-More_on_Bayesian_model_selection_in_high-dimensional_settings.html">1817 andrew gelman stats-2013-04-21-More on Bayesian model selection in high-dimensional settings</a></p>
<p>Introduction: David Rossell writes:
  
A friend pointed out that you were having an interesting philosophical  discussion  on my paper with Val Johnson [on Bayesian model selection in high-dimensional settings].


I agree with the view that in almost all practical situations the true model is not in the set under consideration. Still, asking a model choice procedure to be able to pick up the correct model when it is in the set under consideration seems a minimal requirement (though perhaps not sufficient). In other words, if a procedure is unable to pick the data-generating model even when it is one of the models under consideration, I don’t have high hopes for it working well in more realistic scenarios either. 


Most results in the history in statistics seem to have been obtained under an assumed model, e.g. why even do MLE or penalized-likelihood if we don’t trust the model. While unrealistic, these results were useful to help understand important basic principles. In our case Val and I are defe</p><p>6 0.1040854 <a title="1197-tfidf-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-22-Data_Thief.html">290 andrew gelman stats-2010-09-22-Data Thief</a></p>
<p>7 0.10378934 <a title="1197-tfidf-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-31-Just_gave_a_talk.html">2275 andrew gelman stats-2014-03-31-Just gave a talk</a></p>
<p>8 0.10325173 <a title="1197-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-25-Chris_Schmid_on_Evidence_Based_Medicine.html">1138 andrew gelman stats-2012-01-25-Chris Schmid on Evidence Based Medicine</a></p>
<p>9 0.10187779 <a title="1197-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-29-G%2B_%3E_Skype.html">1143 andrew gelman stats-2012-01-29-G+ > Skype</a></p>
<p>10 0.10074092 <a title="1197-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>11 0.095773607 <a title="1197-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-20-Stan_at_Google_this_Thurs_and_at_Berkeley_this_Fri_noon.html">1772 andrew gelman stats-2013-03-20-Stan at Google this Thurs and at Berkeley this Fri noon</a></p>
<p>12 0.094920292 <a title="1197-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>13 0.094805196 <a title="1197-tfidf-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-12-UnConMax_%E2%80%93_uncertainty_consideration_maxims_7_%2B--_2.html">82 andrew gelman stats-2010-06-12-UnConMax – uncertainty consideration maxims 7 +-- 2</a></p>
<p>14 0.094483897 <a title="1197-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>15 0.093538173 <a title="1197-tfidf-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-05-Does_posterior_predictive_model_checking_fit_with_the_operational_subjective_approach%3F.html">320 andrew gelman stats-2010-10-05-Does posterior predictive model checking fit with the operational subjective approach?</a></p>
<p>16 0.090404332 <a title="1197-tfidf-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>17 0.089728057 <a title="1197-tfidf-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-18-The_1.6_rule.html">39 andrew gelman stats-2010-05-18-The 1.6 rule</a></p>
<p>18 0.087914661 <a title="1197-tfidf-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-30-I_just_skyped_in_from_Kentucky%2C_and_boy_are_my_arms_tired.html">438 andrew gelman stats-2010-11-30-I just skyped in from Kentucky, and boy are my arms tired</a></p>
<p>19 0.087012999 <a title="1197-tfidf-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Useful_models%2C_model_checking%2C_and_external_validation%3A__a_mini-discussion.html">244 andrew gelman stats-2010-08-30-Useful models, model checking, and external validation:  a mini-discussion</a></p>
<p>20 0.082295895 <a title="1197-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-27-Bridges_between_deterministic_and_probabilistic_models_for_binary_data.html">780 andrew gelman stats-2011-06-27-Bridges between deterministic and probabilistic models for binary data</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.132), (1, 0.069), (2, -0.03), (3, 0.044), (4, 0.004), (5, 0.028), (6, -0.02), (7, 0.026), (8, 0.049), (9, -0.009), (10, 0.005), (11, 0.073), (12, -0.039), (13, -0.01), (14, -0.042), (15, -0.062), (16, -0.005), (17, -0.025), (18, -0.007), (19, 0.023), (20, -0.047), (21, -0.096), (22, 0.021), (23, -0.054), (24, -0.047), (25, 0.019), (26, -0.069), (27, -0.076), (28, 0.068), (29, -0.043), (30, -0.018), (31, 0.03), (32, 0.001), (33, -0.039), (34, 0.037), (35, 0.029), (36, 0.072), (37, -0.056), (38, 0.035), (39, -0.032), (40, -0.009), (41, -0.044), (42, 0.033), (43, 0.051), (44, 0.028), (45, -0.012), (46, -0.002), (47, -0.0), (48, -0.011), (49, 0.0)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9665094 <a title="1197-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-%E2%80%9CAll_Models_are_Right%2C_Most_are_Useless%E2%80%9D.html">1197 andrew gelman stats-2012-03-04-“All Models are Right, Most are Useless”</a></p>
<p>Introduction: The above is the title of  a talk  that Thad Tarpey gave at the Joint Statistical Meetings in 2009.  Here’s the abstract:
  
Students of statistics are often introduced to George Box’s famous quote: “all models are wrong, some are useful.” In this talk I [Tarpey] argue that this quote, although useful, is wrong. A different and more positive perspective is to acknowledge that a model is simply a means of extracting information of interest from data. The truth is infinitely complex and a model is merely an approximation to the truth. If the approximation is poor or misleading, then the model is useless. In this talk I give examples of correct models that are not true models. I illustrate how the notion of a “wrong” model can lead to wrong conclusions.
  
I’m curious what he had to say—maybe he could post the slides?
 
P.S.  And  here they are !</p><p>2 0.7636258 <a title="1197-lsi-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-09-Removing_the_blindfold%3A_visualising_statistical_models.html">265 andrew gelman stats-2010-09-09-Removing the blindfold: visualising statistical models</a></p>
<p>Introduction: Hadley Wickham’s talk  for Monday 13 Sept at noon in the statistics dept:
  
As the volume of data increases, so to does the complexity of our models. Visualisation is a powerful tool for both understanding how models work, and what they say about a particularly dataset. There are very many well-known techniques for visualising data, but far fewer for visualising models. In this talk I [Wichkam] will discuss three broad strategies for model visualisation: display the model in the data space; look all members of a collection; and explore the process of model fitting, not just the end result. I will demonstrate these techniques with two examples: neural networks, and ensembles of linear 
models.
  
Hey–this is one of my favorite topics!</p><p>3 0.69652545 <a title="1197-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-30-A_graphics_talk_with_no_visuals%21.html">1598 andrew gelman stats-2012-11-30-A graphics talk with no visuals!</a></p>
<p>Introduction: So, I’m at MIT, twenty minutes into my  talk  on tradeoffs in information graphics to the computer scientists, when the power goes out.  They had some dim backup lighting so we weren’t all sitting there in the dark, but the projector wasn’t working.  So I took questions for the remaining 40 minutes.  It went well, perhaps better than the actual talk would’ve gone, even though they didn’t get to see most of my  slides .</p><p>4 0.69162518 <a title="1197-lsi-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-09-Special_journal_issue_on_statistical_methods_for_the_social_sciences.html">24 andrew gelman stats-2010-05-09-Special journal issue on statistical methods for the social sciences</a></p>
<p>Introduction: Last year I  spoke at  a conference celebrating the 10th anniversary of the University of Washington’s Center for Statistics and the Social Sciences, and just today a  special issue  of the journal Statistical Methodology came out in honor of the center’s anniversary.   My article  in the special issue actually has nothing to do with my talk at the conference; rather, it’s an exploration of an idea that Iven Van Mechelen and I had for understanding deterministic models probabilistically:
  
 
For the analysis of binary data, various deterministic models have been proposed, which are generally simpler to fit and easier to understand than probabilistic models. We claim that corresponding to any deterministic model is an implicit stochastic model in which the deterministic model fits imperfectly, with errors occurring at random. In the context of binary data, we consider a model in which the probability of error depends on the model prediction. We show how to fit this model using a stocha</p><p>5 0.67971992 <a title="1197-lsi-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-19-The_%E2%80%9Ceither-or%E2%80%9D_fallacy_of_believing_in_discrete_models%3A__an_example_of_folk_statistics.html">217 andrew gelman stats-2010-08-19-The “either-or” fallacy of believing in discrete models:  an example of folk statistics</a></p>
<p>Introduction: Psychologists talk about “folk psychology”:  ideas that make sense to us about how people think and behave, even if these ideas are not accurate descriptions of reality.  And physicists talk about “folk physics” (for example, the idea that a thrown ball falls in a straight line and then suddenly drops, rather than following an approximate parabola).
 
There’s also “folk statistics.”  Some of the ideas of folk statistics are so strong that even educated people–even well-known researchers–can make these mistakes.
 
One of the ideas of folk statistics that bothers me a lot is what might be called the “either/or fallacy”:  the idea that if there are two possible stories, the truth has to be one or the other.
 
I have often encountered the either/or fallacy in Bayesian statistics, for example the vast literature on “model selection” or “variable selection” or “model averaging” in which it is assumed that one of some pre-specified discrete set of models is the truth, and that this true model</p><p>6 0.6794827 <a title="1197-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-03-Popper_and_Jaynes.html">2007 andrew gelman stats-2013-09-03-Popper and Jaynes</a></p>
<p>7 0.67501074 <a title="1197-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-13-Flexibility_is_good.html">2133 andrew gelman stats-2013-12-13-Flexibility is good</a></p>
<p>8 0.67213535 <a title="1197-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-27-Bridges_between_deterministic_and_probabilistic_models_for_binary_data.html">780 andrew gelman stats-2011-06-27-Bridges between deterministic and probabilistic models for binary data</a></p>
<p>9 0.66456378 <a title="1197-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-11-Adding_an_error_model_to_a_deterministic_model.html">1162 andrew gelman stats-2012-02-11-Adding an error model to a deterministic model</a></p>
<p>10 0.66419703 <a title="1197-lsi-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-07-Valencia%3A___Summer_of_1991.html">72 andrew gelman stats-2010-06-07-Valencia:   Summer of 1991</a></p>
<p>11 0.66308463 <a title="1197-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-29-G%2B_%3E_Skype.html">1143 andrew gelman stats-2012-01-29-G+ > Skype</a></p>
<p>12 0.66017801 <a title="1197-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-17-Ripley_on_model_selection%2C_and_some_links_on_exploratory_model_analysis.html">1066 andrew gelman stats-2011-12-17-Ripley on model selection, and some links on exploratory model analysis</a></p>
<p>13 0.65635902 <a title="1197-lsi-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-31-Just_gave_a_talk.html">2275 andrew gelman stats-2014-03-31-Just gave a talk</a></p>
<p>14 0.65607774 <a title="1197-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>15 0.6519022 <a title="1197-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>16 0.65143371 <a title="1197-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-03-Model_Makers%E2%80%99_Hippocratic_Oath.html">552 andrew gelman stats-2011-02-03-Model Makers’ Hippocratic Oath</a></p>
<p>17 0.64918548 <a title="1197-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-04-Columbo_does_posterior_predictive_checks.html">1521 andrew gelman stats-2012-10-04-Columbo does posterior predictive checks</a></p>
<p>18 0.64913303 <a title="1197-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-21-More_on_Bayesian_model_selection_in_high-dimensional_settings.html">1817 andrew gelman stats-2013-04-21-More on Bayesian model selection in high-dimensional settings</a></p>
<p>19 0.64513081 <a title="1197-lsi-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-11-Data_Visualization_vs._Statistical_Graphics.html">407 andrew gelman stats-2010-11-11-Data Visualization vs. Statistical Graphics</a></p>
<p>20 0.64133835 <a title="1197-lsi-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-03-This_is_a_footnote_in_one_of_my_papers.html">448 andrew gelman stats-2010-12-03-This is a footnote in one of my papers</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(16, 0.097), (24, 0.188), (65, 0.287), (84, 0.041), (99, 0.253)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9582901 <a title="1197-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-07-Whassup_with_phantom-limb_treatment%3F.html">457 andrew gelman stats-2010-12-07-Whassup with phantom-limb treatment?</a></p>
<p>Introduction: OK, here’s something that is completely baffling me.  I read  this article  by John Colapinto on the neuroscientist V. S. Ramachandran, who’s famous for his innovative treatment for “phantom limb” pain:
  
His first subject was a young man who a decade earlier had crashed his motorcycle and torn from his spinal column the nerves supplying the left arm.  After keeping the useless arm in a sling for a year, the man had the arm amputated above the elbow.  Ever since, he had felt unremitting cramping in the phantom limb, as though it were immobilized in an awkward position. . . . Ramachandram positioned a twenty-inch-by-twenty-inch drugstore mirror . . . and told him to place his intact right arm on one side of the mirror and his stump on the other.  He told the man to arrange the mirror so that the reflection created the illusion that his intact arm was the continuation of the amputated one.  The Ramachandran asked the man to move his right and left arms . . . “Oh, my God!” the man began</p><p>2 0.9263649 <a title="1197-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-A_Stan_is_Born.html">1475 andrew gelman stats-2012-08-30-A Stan is Born</a></p>
<p>Introduction: Stan 1.0.0 and RStan 1.0.0 
 
It’s official.  The Stan Development Team is happy to announce the first stable versions of Stan and RStan.  
 
 What is (R)Stan? 
 
Stan is an open-source package for obtaining Bayesian inference using the No-U-Turn sampler, a variant of Hamiltonian Monte Carlo.  It’s sort of like BUGS, but with a different language for expressing models and a different sampler for sampling from their posteriors. 
 
RStan is the R interface to Stan.  
 
 Stan Home Page 
 
Stan’s home page is:     http://mc-stan.org/    
 
It links everything you need to get started running Stan from the command line, from R, or from C++, including full step-by-step install instructions, a detailed user’s guide and reference manual for the modeling language, and tested ports of most of the BUGS examples.
 
 Peruse the Manual 
 
If you’d like to learn more, the   Stan User’s Guide and Reference Manual   is the place to start.</p><p>same-blog 3 0.90983391 <a title="1197-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-%E2%80%9CAll_Models_are_Right%2C_Most_are_Useless%E2%80%9D.html">1197 andrew gelman stats-2012-03-04-“All Models are Right, Most are Useless”</a></p>
<p>Introduction: The above is the title of  a talk  that Thad Tarpey gave at the Joint Statistical Meetings in 2009.  Here’s the abstract:
  
Students of statistics are often introduced to George Box’s famous quote: “all models are wrong, some are useful.” In this talk I [Tarpey] argue that this quote, although useful, is wrong. A different and more positive perspective is to acknowledge that a model is simply a means of extracting information of interest from data. The truth is infinitely complex and a model is merely an approximation to the truth. If the approximation is poor or misleading, then the model is useless. In this talk I give examples of correct models that are not true models. I illustrate how the notion of a “wrong” model can lead to wrong conclusions.
  
I’m curious what he had to say—maybe he could post the slides?
 
P.S.  And  here they are !</p><p>4 0.90640879 <a title="1197-lda-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-22-Improvements_to_Kindle_Version_of_BDA3.html">1993 andrew gelman stats-2013-08-22-Improvements to Kindle Version of BDA3</a></p>
<p>Introduction: I let Andrew know about the comments about the defective Kindle version of BDA2 and he wrote to his editor at Chapman and Hall, Rob Calver, who wrote back with this info:
  

I can guarantee that the Kindle version of the third edition will be a substantial improvement.


We publish all of our mathematics and statistics books through Kindle now as Print Replica. This means that we send the printer pdf to Amazon and they convert into their Print Replica format, which is essentially just a pdf viewer. We have not experienced very many issues at all with this setup.


Unfortunately, there was a period before Amazon launched Print Replica when they converted math/stat books into their Kindle format, and converted them very badly in some cases. Equations were held as images, making them very difficult to read. It appears this was the case with Andrew’s second edition, judging by some of the comments.


The third edition [of BDA] will be available through Kindle with a short delay (for Amazo</p><p>5 0.90555775 <a title="1197-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-23-Special_effects.html">1426 andrew gelman stats-2012-07-23-Special effects</a></p>
<p>Introduction: I just saw L’Age de Glace 4 and boy are my eyes tired.  I’m just glad it wasn’t in 3-D or I probably would’ve thrown up.  The special effects were amazing, way beyond George of the Jungle and that ilk.  Which was good, as I could only understand about 10% of the dialogue.  I’d heard about all this new animation technology but not actually seen it before.</p><p>6 0.8943218 <a title="1197-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-21-Don%E2%80%99t_judge_a_book_by_its_title.html">1021 andrew gelman stats-2011-11-21-Don’t judge a book by its title</a></p>
<p>7 0.88314843 <a title="1197-lda-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-15-Last_word_on_Mister_P_%28for_now%29.html">2062 andrew gelman stats-2013-10-15-Last word on Mister P (for now)</a></p>
<p>8 0.87505233 <a title="1197-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-04-Question_25_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1365 andrew gelman stats-2012-06-04-Question 25 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>9 0.87175071 <a title="1197-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-23-Can%E2%80%99t_Stop_Won%E2%80%99t_Stop_Mister_P_Beatdown.html">2074 andrew gelman stats-2013-10-23-Can’t Stop Won’t Stop Mister P Beatdown</a></p>
<p>10 0.87077868 <a title="1197-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-20-One_more_time-use_graph.html">671 andrew gelman stats-2011-04-20-One more time-use graph</a></p>
<p>11 0.85584509 <a title="1197-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-24-NYT_version_of_birthday_graph.html">2146 andrew gelman stats-2013-12-24-NYT version of birthday graph</a></p>
<p>12 0.85541481 <a title="1197-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-20-Question_10_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1333 andrew gelman stats-2012-05-20-Question 10 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>13 0.85148674 <a title="1197-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-09-Mister_P%3A__What%E2%80%99s_its_secret_sauce%3F.html">2056 andrew gelman stats-2013-10-09-Mister P:  What’s its secret sauce?</a></p>
<p>14 0.83979595 <a title="1197-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-04-At_the_politics_blogs_._._..html">990 andrew gelman stats-2011-11-04-At the politics blogs . . .</a></p>
<p>15 0.83302546 <a title="1197-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-11-Compare_p-values_from_privately_funded_medical_trials_to_those_in_publicly_funded_research%3F.html">463 andrew gelman stats-2010-12-11-Compare p-values from privately funded medical trials to those in publicly funded research?</a></p>
<p>16 0.83068776 <a title="1197-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-11-Weakly_informative_priors_for_Bayesian_nonparametric_models%3F.html">1454 andrew gelman stats-2012-08-11-Weakly informative priors for Bayesian nonparametric models?</a></p>
<p>17 0.81545508 <a title="1197-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-14-More_on_Mister_P_and_how_it_does_what_it_does.html">2061 andrew gelman stats-2013-10-14-More on Mister P and how it does what it does</a></p>
<p>18 0.8107416 <a title="1197-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-11-Hey%2C_good_news%21__Your_p-value_just_passed_the_0.05_threshold%21.html">758 andrew gelman stats-2011-06-11-Hey, good news!  Your p-value just passed the 0.05 threshold!</a></p>
<p>19 0.80508614 <a title="1197-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-07-Is_Felix_Salmon_wrong_on_free_TV%3F.html">1845 andrew gelman stats-2013-05-07-Is Felix Salmon wrong on free TV?</a></p>
<p>20 0.80073136 <a title="1197-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-18-Psychology_experiments_to_understand_what%E2%80%99s_going_on_with_data_graphics%3F.html">1811 andrew gelman stats-2013-04-18-Psychology experiments to understand what’s going on with data graphics?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
