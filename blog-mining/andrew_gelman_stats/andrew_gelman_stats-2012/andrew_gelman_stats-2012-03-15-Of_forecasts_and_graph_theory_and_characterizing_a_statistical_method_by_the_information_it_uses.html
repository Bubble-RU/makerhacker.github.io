<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1214 andrew gelman stats-2012-03-15-Of forecasts and graph theory and characterizing a statistical method by the information it uses</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1214" href="#">andrew_gelman_stats-2012-1214</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1214 andrew gelman stats-2012-03-15-Of forecasts and graph theory and characterizing a statistical method by the information it uses</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1214-html" href="http://andrewgelman.com/2012/03/15/of-forecasts-and-graph-theory-and-characterizing-a-statistical-method-by-the-information-it-uses/">html</a></p><p>Introduction: Wayne Folta points me to  “EigenBracket 2012: Using Graph Theory to Predict NCAA March Madness Basketball”  and writes, “I [Folta] have got to believe that he’s simply re-invented a statistical method in a graph-ish context, but don’t know enough to judge.”
 
I have not looked in detail at the method being presented here—I’m not much of college basketball fan—but I’d like to use this as an excuse to make one of my favorite general point, which is that a good way to characterize any statistical method is by what information it uses. 
   
The basketball ranking method here uses score differentials between teams in the past season.  On the plus side, that is better than simply using one-loss records (which (a) discards score differentials and (b) discards information on who played whom).  On the minus side, the method appears to be discretizing the scores (thus throwing away information on the exact score differential) and doesn’t use any external information such as external ratings.
 
A</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Wayne Folta points me to  “EigenBracket 2012: Using Graph Theory to Predict NCAA March Madness Basketball”  and writes, “I [Folta] have got to believe that he’s simply re-invented a statistical method in a graph-ish context, but don’t know enough to judge. [sent-1, score-0.522]
</p><p>2 ”   I have not looked in detail at the method being presented here—I’m not much of college basketball fan—but I’d like to use this as an excuse to make one of my favorite general point, which is that a good way to characterize any statistical method is by what information it uses. [sent-2, score-1.92]
</p><p>3 The basketball ranking method here uses score differentials between teams in the past season. [sent-3, score-1.435]
</p><p>4 On the plus side, that is better than simply using one-loss records (which (a) discards score differentials and (b) discards information on who played whom). [sent-4, score-1.826]
</p><p>5 On the minus side, the method appears to be discretizing the scores (thus throwing away information on the exact score differential) and doesn’t use any external information such as external ratings. [sent-5, score-1.926]
</p><p>6 Anyway, my point is that the writeup of the method focuses on statistical operations (forming a matrix of a graph, computing eigensomethingorothers), and, sure, something like that is necessary, but to me, what’s interesting is to know what information went into the rankings. [sent-6, score-1.158]
</p><p>7 If I wanted to use the information that this guy was using, I’d probably just fit a simple normal linear model with a latent parameter for each team. [sent-9, score-0.617]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('method', 0.328), ('basketball', 0.307), ('discards', 0.3), ('differentials', 0.283), ('folta', 0.247), ('score', 0.239), ('information', 0.237), ('external', 0.187), ('ncaa', 0.141), ('writeup', 0.135), ('wayne', 0.123), ('side', 0.123), ('forming', 0.118), ('ranking', 0.114), ('simply', 0.107), ('operations', 0.107), ('minus', 0.107), ('graph', 0.104), ('differential', 0.103), ('excuse', 0.101), ('characterize', 0.099), ('march', 0.098), ('focuses', 0.094), ('records', 0.094), ('using', 0.094), ('played', 0.093), ('teams', 0.093), ('latent', 0.091), ('throwing', 0.089), ('matrix', 0.089), ('use', 0.087), ('statistical', 0.087), ('fan', 0.084), ('exact', 0.083), ('scores', 0.081), ('computing', 0.081), ('plus', 0.079), ('detail', 0.078), ('favorite', 0.075), ('uses', 0.071), ('necessary', 0.071), ('predict', 0.07), ('normal', 0.07), ('presented', 0.069), ('team', 0.069), ('linear', 0.067), ('parameter', 0.065), ('appears', 0.064), ('college', 0.063), ('looked', 0.061)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="1214-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-15-Of_forecasts_and_graph_theory_and_characterizing_a_statistical_method_by_the_information_it_uses.html">1214 andrew gelman stats-2012-03-15-Of forecasts and graph theory and characterizing a statistical method by the information it uses</a></p>
<p>Introduction: Wayne Folta points me to  “EigenBracket 2012: Using Graph Theory to Predict NCAA March Madness Basketball”  and writes, “I [Folta] have got to believe that he’s simply re-invented a statistical method in a graph-ish context, but don’t know enough to judge.”
 
I have not looked in detail at the method being presented here—I’m not much of college basketball fan—but I’d like to use this as an excuse to make one of my favorite general point, which is that a good way to characterize any statistical method is by what information it uses. 
   
The basketball ranking method here uses score differentials between teams in the past season.  On the plus side, that is better than simply using one-loss records (which (a) discards score differentials and (b) discards information on who played whom).  On the minus side, the method appears to be discretizing the scores (thus throwing away information on the exact score differential) and doesn’t use any external information such as external ratings.
 
A</p><p>2 0.27673355 <a title="1214-tfidf-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-26-Econometrics%2C_political_science%2C_epidemiology%2C_etc.%3A__Don%E2%80%99t_model_the_probability_of_a_discrete_outcome%2C_model_the_underlying_continuous_variable.html">2226 andrew gelman stats-2014-02-26-Econometrics, political science, epidemiology, etc.:  Don’t model the probability of a discrete outcome, model the underlying continuous variable</a></p>
<p>Introduction: This is an echo of yesterday’s post,  Basketball Stats: Don’t model the probability of win, model the expected score differential .
 
As with basketball, so with baseball:  as the great Bill James wrote, if you want to predict a pitcher’s win-loss record, it’s better to use last year’s ERA than last year’s W-L.
 
As with basketball and baseball, so with epidemiology:  as Joseph Delaney  points out  in my favorite blog that nobody reads, you will see much better prediction if you first model change in the parameter (e.g. blood pressure) and then convert that to the binary disease state (e.g. hypertension) then if you just develop a logistic model for prob(hypertension).
 
As with basketball, baseball, and epidemiology, so with political science:  instead of modeling election winners, better to model vote differential, a point that I made back in 1993 (see page 120  here ) but which seems to continually need  repeating .  A forecasting method should get essentially no credit for correctl</p><p>3 0.24591812 <a title="1214-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-30-Convenient_page_of_data_sources_from_the_Washington_Post.html">1146 andrew gelman stats-2012-01-30-Convenient page of data sources from the Washington Post</a></p>
<p>Introduction: Wayne Folta points us to  this list .</p><p>4 0.19823514 <a title="1214-tfidf-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-23-Win_probabilities_during_a_sporting_event.html">2262 andrew gelman stats-2014-03-23-Win probabilities during a sporting event</a></p>
<p>Introduction: Todd Schneider writes: 
   
 
 Apropos of your recent  blog post about modeling score differential of basketball games , I thought you might enjoy a site I built, gambletron2000.com , that gathers real-time win probabilities from betting markets for most major sports (including NBA and college basketball). 
  
 My original goal was to use the variance of changes in win probabilities to quantify which games were the most exciting, but I got a bit carried away and ended up pursuing a bunch of other ideas, whichÂ  you can read about in the full writeup here  
  
 This particular passage from the anonymous someone in your post: 
  

My idea is for each timestep in a game (a second, 5 seconds, etc), use the Vegas line, the current score differential, who has the ball, and the number of possessions played already (to account for differences in pace) to create a point estimate probability of the home team winning.

  
 reminded me of a graph I made, which shows the mean-reverting tendency of N</p><p>5 0.19669703 <a title="1214-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-03-Bayes_pays%21.html">1923 andrew gelman stats-2013-07-03-Bayes pays!</a></p>
<p>Introduction: Jason Rosenfeld, who has the amazing title of “Manager of Basketball Analytics” at the Charlotte Bobcats, announces the following  jobs :
  
Basketball Operations: Statistics

  Basketball Operations Systems Developer – Charlotte Bobcats (Charlotte, NC)  

   


POSITION OVERVIEW 
The Basketball Operations System Developer will collect and import data to our database, check data, and field requests from the Basketball Operations staff.  This position will be instrumental in molding and improving our database to assist the staff in player personnel and coaching efforts. 
   
ESSENTIAL DUTIES AND RESPONSIBILITIES 
• Respond to data and database requests from the front office. 
• Build user-friendly software tools for use by the basketball operations staff. 
• Accumulate data from various sources to input and organize into our system to assist the basketball operations staff with decisions. 
• Check and clean data for accuracy and import to our database. 
• Provide ideas and play a key ro</p><p>6 0.19365352 <a title="1214-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-05-World_Bank_data_now_online.html">891 andrew gelman stats-2011-09-05-World Bank data now online</a></p>
<p>7 0.18927228 <a title="1214-tfidf-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-25-Basketball_Stats%3A__Don%E2%80%99t_model_the_probability_of_win%2C_model_the_expected_score_differential..html">2224 andrew gelman stats-2014-02-25-Basketball Stats:  Don’t model the probability of win, model the expected score differential.</a></p>
<p>8 0.14366215 <a title="1214-tfidf-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-29-Statistics_and_the_end_of_time.html">306 andrew gelman stats-2010-09-29-Statistics and the end of time</a></p>
<p>9 0.12736434 <a title="1214-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-25-It%E2%80%99s_binless%21__A_program_for_computing_normalizing_functions.html">1825 andrew gelman stats-2013-04-25-It’s binless!  A program for computing normalizing functions</a></p>
<p>10 0.10810835 <a title="1214-tfidf-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-24-On_deck_this_week.html">2222 andrew gelman stats-2014-02-24-On deck this week</a></p>
<p>11 0.10577042 <a title="1214-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-A_poll_that_throws_away_data%3F%3F%3F.html">1940 andrew gelman stats-2013-07-16-A poll that throws away data???</a></p>
<p>12 0.10338566 <a title="1214-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-01-Tukey%E2%80%99s_philosophy.html">496 andrew gelman stats-2011-01-01-Tukey’s philosophy</a></p>
<p>13 0.10150811 <a title="1214-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>14 0.099021085 <a title="1214-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-01-Philosophy_of_Bayesian_statistics%3A__my_reactions_to_Cox_and_Mayo.html">1149 andrew gelman stats-2012-02-01-Philosophy of Bayesian statistics:  my reactions to Cox and Mayo</a></p>
<p>15 0.097321346 <a title="1214-tfidf-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-01-Why_Development_Economics_Needs_Theory%3F.html">309 andrew gelman stats-2010-10-01-Why Development Economics Needs Theory?</a></p>
<p>16 0.095664926 <a title="1214-tfidf-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-A_calibrated_Cook_gives_Dems_the_edge_in_Nov%2C_sez_Sandy.html">300 andrew gelman stats-2010-09-28-A calibrated Cook gives Dems the edge in Nov, sez Sandy</a></p>
<p>17 0.094544552 <a title="1214-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-19-Validation_of_Software_for_Bayesian_Models_Using_Posterior_Quantiles.html">1019 andrew gelman stats-2011-11-19-Validation of Software for Bayesian Models Using Posterior Quantiles</a></p>
<p>18 0.086979114 <a title="1214-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-11-%E2%80%9CInformative_g-Priors_for_Logistic_Regression%E2%80%9D.html">2017 andrew gelman stats-2013-09-11-“Informative g-Priors for Logistic Regression”</a></p>
<p>19 0.083842136 <a title="1214-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-05-The_Grinch_Comes_Back.html">1606 andrew gelman stats-2012-12-05-The Grinch Comes Back</a></p>
<p>20 0.08295732 <a title="1214-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-29-Infovis%2C_infographics%2C_and_data_visualization%3A__Where_I%E2%80%99m_coming_from%2C_and_where_I%E2%80%99d_like_to_go.html">878 andrew gelman stats-2011-08-29-Infovis, infographics, and data visualization:  Where I’m coming from, and where I’d like to go</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.143), (1, 0.056), (2, 0.016), (3, 0.04), (4, 0.073), (5, -0.044), (6, -0.014), (7, 0.018), (8, -0.017), (9, -0.009), (10, 0.006), (11, 0.016), (12, -0.055), (13, -0.027), (14, -0.088), (15, 0.007), (16, 0.049), (17, -0.015), (18, 0.023), (19, -0.051), (20, -0.014), (21, 0.04), (22, 0.012), (23, 0.034), (24, 0.089), (25, 0.033), (26, 0.041), (27, 0.068), (28, -0.031), (29, -0.06), (30, 0.058), (31, 0.024), (32, 0.047), (33, -0.011), (34, 0.016), (35, 0.023), (36, 0.031), (37, 0.012), (38, -0.041), (39, 0.01), (40, 0.04), (41, -0.019), (42, 0.066), (43, 0.024), (44, -0.049), (45, 0.012), (46, -0.01), (47, -0.056), (48, -0.09), (49, 0.037)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96209621 <a title="1214-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-15-Of_forecasts_and_graph_theory_and_characterizing_a_statistical_method_by_the_information_it_uses.html">1214 andrew gelman stats-2012-03-15-Of forecasts and graph theory and characterizing a statistical method by the information it uses</a></p>
<p>Introduction: Wayne Folta points me to  “EigenBracket 2012: Using Graph Theory to Predict NCAA March Madness Basketball”  and writes, “I [Folta] have got to believe that he’s simply re-invented a statistical method in a graph-ish context, but don’t know enough to judge.”
 
I have not looked in detail at the method being presented here—I’m not much of college basketball fan—but I’d like to use this as an excuse to make one of my favorite general point, which is that a good way to characterize any statistical method is by what information it uses. 
   
The basketball ranking method here uses score differentials between teams in the past season.  On the plus side, that is better than simply using one-loss records (which (a) discards score differentials and (b) discards information on who played whom).  On the minus side, the method appears to be discretizing the scores (thus throwing away information on the exact score differential) and doesn’t use any external information such as external ratings.
 
A</p><p>2 0.68787342 <a title="1214-lsi-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-23-Win_probabilities_during_a_sporting_event.html">2262 andrew gelman stats-2014-03-23-Win probabilities during a sporting event</a></p>
<p>Introduction: Todd Schneider writes: 
   
 
 Apropos of your recent  blog post about modeling score differential of basketball games , I thought you might enjoy a site I built, gambletron2000.com , that gathers real-time win probabilities from betting markets for most major sports (including NBA and college basketball). 
  
 My original goal was to use the variance of changes in win probabilities to quantify which games were the most exciting, but I got a bit carried away and ended up pursuing a bunch of other ideas, whichÂ  you can read about in the full writeup here  
  
 This particular passage from the anonymous someone in your post: 
  

My idea is for each timestep in a game (a second, 5 seconds, etc), use the Vegas line, the current score differential, who has the ball, and the number of possessions played already (to account for differences in pace) to create a point estimate probability of the home team winning.

  
 reminded me of a graph I made, which shows the mean-reverting tendency of N</p><p>3 0.62744176 <a title="1214-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-16-Mr._Pearson%2C_meet_Mr._Mandelbrot%3A__Detecting_Novel_Associations_in_Large_Data_Sets.html">1062 andrew gelman stats-2011-12-16-Mr. Pearson, meet Mr. Mandelbrot:  Detecting Novel Associations in Large Data Sets</a></p>
<p>Introduction: Jeremy Fox asks what I think about  this paper  by David N. Reshef, Yakir Reshef, Hilary Finucane, Sharon  Grossman, Gilean McVean, Peter Turnbaugh, Eric Lander, Michael Mitzenmacher, and Pardis Sabeti which proposes a new nonlinear R-squared-like measure.
 
My quick answer is that it looks really cool!
 
From my quick reading of the paper, it appears that the method reduces on average to the usual R-squared when fit to data of the form y = a + bx + error, and that it also has a similar interpretation when “a + bx” is replaced by other continuous functions.
 
Unlike R-squared, the method of Reshef et al. depends on a tuning parameter that controls the level of discretization, in a “How long is the coast of Britain” sort of way.  The dependence on scale is inevitable for such a general method.  Just consider:  if you sample 1000 points from the unit bivariate normal distribution, (x,y) ~ N(0,I), you’ll be able to fit them perfectly by a 999-degree polynomial fit to the data.  So the sca</p><p>4 0.62264937 <a title="1214-lsi-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-25-Basketball_Stats%3A__Don%E2%80%99t_model_the_probability_of_win%2C_model_the_expected_score_differential..html">2224 andrew gelman stats-2014-02-25-Basketball Stats:  Don’t model the probability of win, model the expected score differential.</a></p>
<p>Introduction: Someone who wants to remain anonymous writes:
  
I am working to create a more accurate in-game win probability model for basketball games. My idea is for each timestep in a game (a second, 5 seconds, etc), use the Vegas line, the current score differential, who has the ball, and the number of possessions played already (to account for differences in pace) to create a point estimate probability of the home team winning.


This problem would seem to fit a multi-level model structure well. It seems silly to estimate 2,000 regressions (one for each timestep), but the coefficients should vary at each timestep. Do you have suggestions for what type of model this could/would be? Additionally, I believe this needs to be some form of logit/probit given the binary dependent variable (win or loss).


Finally, do you have suggestions for what package could accomplish this in Stata or R?
  
To answer the questions in reverse order: 
3.  I’d hope this could be done in Stan (which can be run from R)</p><p>5 0.62113136 <a title="1214-lsi-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-14-The_maximal_information_coefficient.html">2247 andrew gelman stats-2014-03-14-The maximal information coefficient</a></p>
<p>Introduction: Justin Kinney writes:
  
I wanted to let you know that the critique Mickey Atwal and I wrote regarding equitability and the maximal information coefficient has just been  published .
  
We discussed this paper last year, under the heading,  Too many MC’s not enough MIC’s, or What principles should govern attempts to summarize bivariate associations in large multivariate datasets? 
 
Kinney and Atwal’s paper is interesting, with my only criticism being that in some places they seem to aim for what might not be possible.  For example, they write that “mutual information is already widely believed to quantify dependencies without bias for relationships of one type or another,” which seems a bit vague to me.  And later they write, “How to compute such an estimate that does not bias the resulting mutual information value remains an open problem,” which seems to me to miss the point in that unbiased statistical estimates are not generally possible and indeed are often not desirable.
 
Their</p><p>6 0.61819178 <a title="1214-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-04-Too_many_MC%E2%80%99s_not_enough_MIC%E2%80%99s%2C_or_What_principles_should_govern_attempts_to_summarize_bivariate_associations_in_large_multivariate_datasets%3F.html">1706 andrew gelman stats-2013-02-04-Too many MC’s not enough MIC’s, or What principles should govern attempts to summarize bivariate associations in large multivariate datasets?</a></p>
<p>7 0.60394263 <a title="1214-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-26-Further_thoughts_on_nonparametric_correlation_measures.html">1230 andrew gelman stats-2012-03-26-Further thoughts on nonparametric correlation measures</a></p>
<p>8 0.60110736 <a title="1214-lsi-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-26-Econometrics%2C_political_science%2C_epidemiology%2C_etc.%3A__Don%E2%80%99t_model_the_probability_of_a_discrete_outcome%2C_model_the_underlying_continuous_variable.html">2226 andrew gelman stats-2014-02-26-Econometrics, political science, epidemiology, etc.:  Don’t model the probability of a discrete outcome, model the underlying continuous variable</a></p>
<p>9 0.5981555 <a title="1214-lsi-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Bayesian_Uncertainty_Quantification_for_Differential_Equations%21.html">2311 andrew gelman stats-2014-04-29-Bayesian Uncertainty Quantification for Differential Equations!</a></p>
<p>10 0.59411585 <a title="1214-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-21-Will_Tiger_Woods_catch_Jack_Nicklaus%3F__And_a_discussion_of_the_virtues_of_using_continuous_data_even_if_your_goal_is_discrete_prediction.html">1387 andrew gelman stats-2012-06-21-Will Tiger Woods catch Jack Nicklaus?  And a discussion of the virtues of using continuous data even if your goal is discrete prediction</a></p>
<p>11 0.58146864 <a title="1214-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-17-Weak_identification_provides_partial_information.html">1903 andrew gelman stats-2013-06-17-Weak identification provides partial information</a></p>
<p>12 0.57331413 <a title="1214-lsi-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-07-Once_more_on_nonparametric_measures_of_mutual_information.html">2324 andrew gelman stats-2014-05-07-Once more on nonparametric measures of mutual information</a></p>
<p>13 0.56980127 <a title="1214-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-06-Bidding_for_the_kickoff.html">559 andrew gelman stats-2011-02-06-Bidding for the kickoff</a></p>
<p>14 0.56592602 <a title="1214-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-21-Baseball%E2%80%99s_greatest_fielders.html">623 andrew gelman stats-2011-03-21-Baseball’s greatest fielders</a></p>
<p>15 0.55920517 <a title="1214-lsi-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-01-Heller%2C_Heller%2C_and_Gorfine_on_univariate_and_multivariate_information_measures.html">2314 andrew gelman stats-2014-05-01-Heller, Heller, and Gorfine on univariate and multivariate information measures</a></p>
<p>16 0.55855322 <a title="1214-lsi-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-30-Silly_baseball_example_illustrates_a_couple_of_key_ideas_they_don%E2%80%99t_usually_teach_you_in_statistics_class.html">171 andrew gelman stats-2010-07-30-Silly baseball example illustrates a couple of key ideas they don’t usually teach you in statistics class</a></p>
<p>17 0.55116707 <a title="1214-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-06-Statistician_cracks_Toronto_lottery.html">562 andrew gelman stats-2011-02-06-Statistician cracks Toronto lottery</a></p>
<p>18 0.55042535 <a title="1214-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-11-Rajiv_Sethi_on_the_interpretation_of_prediction_market_data.html">607 andrew gelman stats-2011-03-11-Rajiv Sethi on the interpretation of prediction market data</a></p>
<p>19 0.54972756 <a title="1214-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-30-Convenient_page_of_data_sources_from_the_Washington_Post.html">1146 andrew gelman stats-2012-01-30-Convenient page of data sources from the Washington Post</a></p>
<p>20 0.5467065 <a title="1214-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-24-Chasing_the_noise%3A__W._Edwards_Deming_would_be_spinning_in_his_grave.html">2076 andrew gelman stats-2013-10-24-Chasing the noise:  W. Edwards Deming would be spinning in his grave</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(9, 0.012), (15, 0.024), (16, 0.069), (21, 0.035), (24, 0.15), (27, 0.013), (41, 0.193), (54, 0.014), (57, 0.046), (59, 0.014), (86, 0.037), (89, 0.023), (97, 0.022), (99, 0.239)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9486227 <a title="1214-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-16-The_lamest%2C_grudgingest%2C_non-retraction_retraction_ever.html">1626 andrew gelman stats-2012-12-16-The lamest, grudgingest, non-retraction retraction ever</a></p>
<p>Introduction: In politics we’re familiar with the non-apology apology (well described in Wikipedia as “a statement that has the form of an apology but does not express the expected contrition”).  Here’s the scientific equivalent:  the non-retraction retraction.
 
Sanjay Srivastava  points  to an amusing yet barfable story of a pair of researchers who (inadvertently, I assume) made a data coding error and were eventually moved to issue a correction notice, but even then refused to fully admit their error.  As Srivastava puts it, the story “ended up with Lew [Goldberg] and colleagues [Kibeom Lee and Michael Ashton] publishing a comment on an erratum – the only time I’ve ever heard of that happening in a scientific journal.”
 
From the  comment  on the erratum:
  
In their “erratum and addendum,” Anderson and Ones (this issue) explained that we had brought their attention to the “potential” of a “possible” misalignment and described the results computed from re-aligned data as being based on a “post-ho</p><p>same-blog 2 0.93576485 <a title="1214-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-15-Of_forecasts_and_graph_theory_and_characterizing_a_statistical_method_by_the_information_it_uses.html">1214 andrew gelman stats-2012-03-15-Of forecasts and graph theory and characterizing a statistical method by the information it uses</a></p>
<p>Introduction: Wayne Folta points me to  “EigenBracket 2012: Using Graph Theory to Predict NCAA March Madness Basketball”  and writes, “I [Folta] have got to believe that he’s simply re-invented a statistical method in a graph-ish context, but don’t know enough to judge.”
 
I have not looked in detail at the method being presented here—I’m not much of college basketball fan—but I’d like to use this as an excuse to make one of my favorite general point, which is that a good way to characterize any statistical method is by what information it uses. 
   
The basketball ranking method here uses score differentials between teams in the past season.  On the plus side, that is better than simply using one-loss records (which (a) discards score differentials and (b) discards information on who played whom).  On the minus side, the method appears to be discretizing the scores (thus throwing away information on the exact score differential) and doesn’t use any external information such as external ratings.
 
A</p><p>3 0.93074191 <a title="1214-lda-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-%E2%80%9CGenomics%E2%80%9D_vs._genetics.html">303 andrew gelman stats-2010-09-28-“Genomics” vs. genetics</a></p>
<p>Introduction: John Cook  and  Joseph Delaney  point to  an article  by Yurii Aulchenko et al., who write:
  
54 loci showing strong statistical evidence for association to human height were described, providing us with potential genomic means of human height prediction. In a population-based study of 5748 people, we find that a 54-loci genomic profile explained 4-6% of the sex- and age-adjusted height variance, and had limited ability to discriminate tall/short people. . . .


In a family-based study of 550 people, with both parents having height measurements, we find that the Galtonian mid-parental prediction method explained 40% of the sex- and age-adjusted height variance, and showed high discriminative accuracy. . . .
  
The message is that the simple approach of predicting child’s height using a regression model given parents’ average height performs much better than the method they have based on combining 54 genes.
 
They also find that, if you start with the prediction based on parents’ heigh</p><p>4 0.89388859 <a title="1214-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-14-A_new_idea_for_a_science_core_course_based_entirely_on_computer_simulation.html">516 andrew gelman stats-2011-01-14-A new idea for a science core course based entirely on computer simulation</a></p>
<p>Introduction: Columbia College has for many years had a Core Curriculum, in which students read classics such as Plato (in translation) etc.  A few years ago they created a Science core course.  There was always some confusion about this idea:  On one hand, how much would college freshmen really learn about science by reading the classic writings of Galileo, Laplace, Darwin, Einstein, etc.?  And they certainly wouldn’t get much out by puzzling over the latest issues of Nature, Cell, and Physical Review Letters.  On the other hand, what’s the point of having them read Dawkins, Gould, or even Brian Greene?  These sorts of popularizations give you a sense of modern science (even to the extent of conveying some of the debates in these fields), but reading them might not give the same intellectual engagement that you’d get from wrestling with the Bible or Shakespeare.
 
I have a different idea.  What about structuring the entire course around computer programming and simulation?  Start with a few weeks t</p><p>5 0.89088005 <a title="1214-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-05-Recently_in_the_sister_blog.html">1300 andrew gelman stats-2012-05-05-Recently in the sister blog</a></p>
<p>Introduction: Culture war: The rules 
 
 You can only accept capital punishment if you’re willing to have innocent people executed every now and then 
 
 The politics of America’s increasing economic inequality</p><p>6 0.88304865 <a title="1214-lda-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-23-Win_probabilities_during_a_sporting_event.html">2262 andrew gelman stats-2014-03-23-Win probabilities during a sporting event</a></p>
<p>7 0.87966114 <a title="1214-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-07-Diabetes_stops_at_the_state_line%3F.html">454 andrew gelman stats-2010-12-07-Diabetes stops at the state line?</a></p>
<p>8 0.87445879 <a title="1214-lda-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-25-Basketball_Stats%3A__Don%E2%80%99t_model_the_probability_of_win%2C_model_the_expected_score_differential..html">2224 andrew gelman stats-2014-02-25-Basketball Stats:  Don’t model the probability of win, model the expected score differential.</a></p>
<p>9 0.87184364 <a title="1214-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-12-The_power_of_the_puzzlegraph.html">1669 andrew gelman stats-2013-01-12-The power of the puzzlegraph</a></p>
<p>10 0.87064993 <a title="1214-lda-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-03-Reinventing_the_wheel%2C_only_more_so..html">447 andrew gelman stats-2010-12-03-Reinventing the wheel, only more so.</a></p>
<p>11 0.86840993 <a title="1214-lda-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Bayesian_Uncertainty_Quantification_for_Differential_Equations%21.html">2311 andrew gelman stats-2014-04-29-Bayesian Uncertainty Quantification for Differential Equations!</a></p>
<p>12 0.86616993 <a title="1214-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-29-Data_mining_and_allergies.html">685 andrew gelman stats-2011-04-29-Data mining and allergies</a></p>
<p>13 0.86611569 <a title="1214-lda-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-09-Keli_Liu_and_Xiao-Li_Meng_on_Simpson%E2%80%99s_paradox.html">2204 andrew gelman stats-2014-02-09-Keli Liu and Xiao-Li Meng on Simpson’s paradox</a></p>
<p>14 0.86376452 <a title="1214-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-12-Peter_Thiel_is_writing_another_book%21.html">1895 andrew gelman stats-2013-06-12-Peter Thiel is writing another book!</a></p>
<p>15 0.86256266 <a title="1214-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-19-Validation_of_Software_for_Bayesian_Models_Using_Posterior_Quantiles.html">1019 andrew gelman stats-2011-11-19-Validation of Software for Bayesian Models Using Posterior Quantiles</a></p>
<p>16 0.85903716 <a title="1214-lda-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-25-Xihong_Lin_on_sparsity_and_density.html">2185 andrew gelman stats-2014-01-25-Xihong Lin on sparsity and density</a></p>
<p>17 0.85579598 <a title="1214-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-03-Bayes_pays%21.html">1923 andrew gelman stats-2013-07-03-Bayes pays!</a></p>
<p>18 0.85462034 <a title="1214-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-24-New_ideas_on_DIC_from_Martyn_Plummer_and_Sumio_Watanabe.html">778 andrew gelman stats-2011-06-24-New ideas on DIC from Martyn Plummer and Sumio Watanabe</a></p>
<p>19 0.85005105 <a title="1214-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-03-New_New_York_data_research_organizations.html">1297 andrew gelman stats-2012-05-03-New New York data research organizations</a></p>
<p>20 0.8490026 <a title="1214-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-16-Mr._Pearson%2C_meet_Mr._Mandelbrot%3A__Detecting_Novel_Associations_in_Large_Data_Sets.html">1062 andrew gelman stats-2011-12-16-Mr. Pearson, meet Mr. Mandelbrot:  Detecting Novel Associations in Large Data Sets</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
