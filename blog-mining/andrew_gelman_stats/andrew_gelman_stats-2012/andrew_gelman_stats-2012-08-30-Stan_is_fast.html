<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1476 andrew gelman stats-2012-08-30-Stan is fast</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1476" href="#">andrew_gelman_stats-2012-1476</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1476 andrew gelman stats-2012-08-30-Stan is fast</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1476-html" href="http://andrewgelman.com/2012/08/30/stan-is-fast/">html</a></p><p>Introduction: 10,000 iterations for 4 chains on the (precompiled) efficiently-parameterized 8-schools model: 
   
 > date () 
[1] "Thu Aug 30 22:12:53 2012" 
> fit3 <- stan (fit=fit2, data = schools_dat, iter = 1e4, n_chains = 4) 
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). 
Iteration: 10000 / 10000 [100%]  (Sampling) 
 
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2). 
Iteration: 10000 / 10000 [100%]  (Sampling)
 
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3). 
Iteration: 10000 / 10000 [100%]  (Sampling)
 
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4). 
Iteration: 10000 / 10000 [100%]  (Sampling)
 
> date () 
[1] "Thu Aug 30 22:12:55 2012" 
> print (fit3) 
Inference for Stan model: anon_model. 
4 chains: each with iter=10000; warmup=5000; thin=1; 10000 iterations saved.
 
         mean se_mean  sd  2.5%  25%  50%  75% 97.5% n_eff Rhat 
mu        8.0     0.1 5.1  -2.0  4.7  8.0 11.3  18.4  4032    1 
tau       6.7     0.1 5.6   0.3  2.5  5.4  9.3  21.2  2958    1 
eta[1]    0.4     0.0 0.9  -1.5 -0</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 10,000 iterations for 4 chains on the (precompiled) efficiently-parameterized 8-schools model:       > date ()  [1] "Thu Aug 30 22:12:53 2012"  > fit3 <- stan (fit=fit2, data = schools_dat, iter = 1e4, n_chains = 4)  SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). [sent-1, score-0.827]
</p><p>2 Iteration: 10000 / 10000 [100%]  (Sampling)   > date ()  [1] "Thu Aug 30 22:12:55 2012"  > print (fit3)  Inference for Stan model: anon_model. [sent-5, score-0.125]
</p><p>3 4 chains: each with iter=10000; warmup=5000; thin=1; 10000 iterations saved. [sent-6, score-0.29]
</p><p>4 For each parameter, n_eff is a crude measure of effective sample size,  and Rhat is the potential scale reduction factor on split chains (at  convergence, Rhat=1). [sent-162, score-0.437]
</p><p>5 And, as you can see from the R-hats and effective sample sizes, 10,000 iterations is overkill here. [sent-165, score-0.476]
</p><p>6 I’ll first simulate 800 schools worth of data, rerun, and see what happens. [sent-169, score-0.216]
</p><p>7 That’s right, 4 chains of 1000 iterations (enough for convergence) for the 800 schools problem, in 10 seconds. [sent-171, score-0.632]
</p><p>8 Well, it’s pretty horrible if you’re planning to do something with a billion data points. [sent-173, score-0.091]
</p><p>9 For now, let me just point out that the 8 schools is not the ideal model to show the strengths of Stan vs. [sent-176, score-0.353]
</p><p>10 The 8 schools model is conditionally conjugate and so Gibbs can work efficiently there. [sent-178, score-0.464]
</p><p>11 Just for laffs, I tried the (nonconjugate) Student-t model (or, as Stan puts it, student_t) with no added parameterizations, I just replaced normal with student_t with 4 df. [sent-181, score-0.131]
</p><p>12 The runs took 3 seconds for the 10,000 iterations of the 8 schools and 34 seconds for the 1000 iterations of the 800 schools. [sent-182, score-1.229]
</p><p>13 But I think the reason it took a bit longer is not the nonconjugacy but just that we haven’t vectorized the student_t model yet. [sent-183, score-0.352]
</p><p>14 That’s just a small implementation detail, nor requiring any tricks or changes to the algorithm. [sent-185, score-0.092]
</p><p>15 These models did take 12 seconds each to compile. [sent-189, score-0.188]
</p><p>16 Once it’s compiled, you can fit it immediately on new data without needing to recompile. [sent-191, score-0.137]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('eta', 0.502), ('theta', 0.324), ('iterations', 0.29), ('sampling', 0.23), ('iteration', 0.225), ('thu', 0.219), ('rhat', 0.2), ('seconds', 0.188), ('chains', 0.174), ('aug', 0.174), ('schools', 0.168), ('chain', 0.156), ('model', 0.131), ('iter', 0.12), ('vectorized', 0.116), ('stan', 0.116), ('took', 0.105), ('convergence', 0.089), ('date', 0.082), ('sample', 0.067), ('nonconjugate', 0.067), ('precompiled', 0.067), ('effective', 0.065), ('laffs', 0.063), ('conditionally', 0.063), ('warmup', 0.063), ('rerun', 0.06), ('tau', 0.056), ('mu', 0.056), ('parameterizations', 0.056), ('conjugate', 0.056), ('compiled', 0.054), ('overkill', 0.054), ('strengths', 0.054), ('thin', 0.05), ('needing', 0.049), ('simulate', 0.048), ('tricks', 0.047), ('efficiently', 0.046), ('billion', 0.046), ('sd', 0.046), ('data', 0.045), ('requiring', 0.045), ('reduction', 0.045), ('gibbs', 0.045), ('damn', 0.044), ('split', 0.043), ('print', 0.043), ('fit', 0.043), ('crude', 0.043)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="1476-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-Stan_is_fast.html">1476 andrew gelman stats-2012-08-30-Stan is fast</a></p>
<p>Introduction: 10,000 iterations for 4 chains on the (precompiled) efficiently-parameterized 8-schools model: 
   
 > date () 
[1] "Thu Aug 30 22:12:53 2012" 
> fit3 <- stan (fit=fit2, data = schools_dat, iter = 1e4, n_chains = 4) 
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). 
Iteration: 10000 / 10000 [100%]  (Sampling) 
 
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2). 
Iteration: 10000 / 10000 [100%]  (Sampling)
 
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3). 
Iteration: 10000 / 10000 [100%]  (Sampling)
 
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4). 
Iteration: 10000 / 10000 [100%]  (Sampling)
 
> date () 
[1] "Thu Aug 30 22:12:55 2012" 
> print (fit3) 
Inference for Stan model: anon_model. 
4 chains: each with iter=10000; warmup=5000; thin=1; 10000 iterations saved.
 
         mean se_mean  sd  2.5%  25%  50%  75% 97.5% n_eff Rhat 
mu        8.0     0.1 5.1  -2.0  4.7  8.0 11.3  18.4  4032    1 
tau       6.7     0.1 5.6   0.3  2.5  5.4  9.3  21.2  2958    1 
eta[1]    0.4     0.0 0.9  -1.5 -0</p><p>2 0.20389777 <a title="1476-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-28-Path_sampling_for_models_of_varying_dimension.html">1089 andrew gelman stats-2011-12-28-Path sampling for models of varying dimension</a></p>
<p>Introduction: Somebody asks: 
  
  
Iâ&euro;&trade;m reading your paper on path sampling. It essentially solves the 
problem of computing the ratio \int q0(omega)d omega/\int q1(omega) d omega. I.e the arguments in q0() and q1() are the same. But this assumption is not always true in Bayesian model selection using Bayes factor.


In general (for BF), we have this problem, t1 and t2 may have no relation at all.


\int f1(y|t1)p1(t1) d t1 / \int f2(y|t2)p2(t2) d t2


As an example, suppose that we want to compare two sets of normally distributed data with known variance whether they have the same mean (H0) or they are not necessarily have the same mean (H1). Then the dummy variable should be mu in H0 (which is the common mean of both set of samples), and should be (mu1, mu2) (which are the means for each set of samples).


One straight method to address my problem is to preform path integration for the numerate and the denominator, as both the numerate and the denominator are integrals. Each integral can be rewrit</p><p>3 0.17568199 <a title="1476-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>Introduction: Nick Firoozye writes:
  
While I am absolutely sympathetic to the Bayesian agenda I am often troubled by the requirement of having priors. We must have priors on the parameter of an infinite number of model we have never seen before and I find this troubling. There is a similarly troubling problem in economics of utility theory. Utility is on consumables. To be complete a consumer must assign utility to all sorts of things they never would have encountered. More recent versions of utility theory instead make consumption goods a portfolio of attributes. Cadillacs are x many units of luxury y of transport etc etc. And we can automatically have personal utilities to all these attributes.  


I don’t ever see parameters. Some model have few and some have hundreds. Instead, I see data. So I don’t know how to have an opinion on parameters themselves. Rather I think it far more natural to have opinions on the behavior of models. The prior predictive density is a good and sensible notion. Also</p><p>4 0.17393009 <a title="1476-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-23-Validation_of_Software_for_Bayesian_Models_Using_Posterior_Quantiles.html">1868 andrew gelman stats-2013-05-23-Validation of Software for Bayesian Models Using Posterior Quantiles</a></p>
<p>Introduction: Every once in awhile I get a question that I can directly answer from my published research.  When that happens it makes me so happy.
 
Here’s an example.  Patrick Lam wrote,
  
Suppose one develops a Bayesian model to estimate a parameter theta.  Now suppose one wants to evaluate the model via simulation by generating fake data where you know the value of theta and see how well you recover theta with your model, assuming that you use the posterior mean as the estimate.  The traditional frequentist way of evaluating it might be to generate many datasets and see how well your estimator performs each time in terms of unbiasedness or mean squared error or something.  But given that unbiasedness means nothing to a Bayesian and there is no repeated sampling interpretation in a Bayesian model, how would you suggest one would evaluate a Bayesian model?
  
My reply:
  
I actually have  a paper on this !  It is by Cook, Gelman, and Rubin.  The idea is to draw theta from the prior distribution.</p><p>5 0.1506522 <a title="1476-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-17-Jumping_off_the_edge_of_the_world.html">858 andrew gelman stats-2011-08-17-Jumping off the edge of the world</a></p>
<p>Introduction: Tomas Iesmantas writes:
  
I’m facing a problem where parameter space is bounded, e.g. all parameters have to be positive. If in MCMC as proposal distribution I use normal distribution, then at some iterations I get negative proposals. So my question is: should I use recalculation of acceptance probability every time I reject the proposal (something like in delayed rejection method), or I have to use another proposal (like lognormal, truncated normal, etc.)?
  
The simplest solution is to just calculate p(theta)=0 for theta outside the legal region, thus reject those jumps.  This will work fine (just remember that when you reject, you have to stay at the last value for one more iteration), but if you’re doing these rejections all the time, you might want to reparameterize your space, for example using logs for positive parameters, logits for constrained parameters, and softmax for parameters that are constrained to sum to 1.</p><p>6 0.14971136 <a title="1476-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-The_statistical_significance_filter.html">899 andrew gelman stats-2011-09-10-The statistical significance filter</a></p>
<p>7 0.14107987 <a title="1476-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-31-Using_sample_size_in_the_prior_distribution.html">547 andrew gelman stats-2011-01-31-Using sample size in the prior distribution</a></p>
<p>8 0.14059834 <a title="1476-tfidf-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-07-My_recent_debugging_experience.html">2161 andrew gelman stats-2014-01-07-My recent debugging experience</a></p>
<p>9 0.13587445 <a title="1476-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>10 0.11867672 <a title="1476-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-16-The_%E2%80%9CWashington_read%E2%80%9D_and_the_algebra_of_conditional_distributions.html">961 andrew gelman stats-2011-10-16-The “Washington read” and the algebra of conditional distributions</a></p>
<p>11 0.11029533 <a title="1476-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-31-No_on_Yes-No_decisions.html">2155 andrew gelman stats-2013-12-31-No on Yes-No decisions</a></p>
<p>12 0.10986167 <a title="1476-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-25-Why_it_doesn%E2%80%99t_make_sense_in_general_to_form_confidence_intervals_by_inverting_hypothesis_tests.html">870 andrew gelman stats-2011-08-25-Why it doesn’t make sense in general to form confidence intervals by inverting hypothesis tests</a></p>
<p>13 0.10690106 <a title="1476-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-29-How_many_parameters_are_in_a_multilevel_model%3F.html">1144 andrew gelman stats-2012-01-29-How many parameters are in a multilevel model?</a></p>
<p>14 0.10672729 <a title="1476-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-22-My_talks_that_were_scheduled_for_Tues_at_the_Data_Skeptics_meetup_and_Wed_at_the_Open_Statistical_Programming_meetup.html">1950 andrew gelman stats-2013-07-22-My talks that were scheduled for Tues at the Data Skeptics meetup and Wed at the Open Statistical Programming meetup</a></p>
<p>15 0.10669054 <a title="1476-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-17-Statistics_in_a_world_where_nothing_is_random.html">1628 andrew gelman stats-2012-12-17-Statistics in a world where nothing is random</a></p>
<p>16 0.10532069 <a title="1476-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>17 0.10336915 <a title="1476-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-24-Why_it_doesn%E2%80%99t_make_sense_in_general_to_form_confidence_intervals_by_inverting_hypothesis_tests.html">1913 andrew gelman stats-2013-06-24-Why it doesn’t make sense in general to form confidence intervals by inverting hypothesis tests</a></p>
<p>18 0.10322066 <a title="1476-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-09-A_tale_of_two_discussion_papers.html">1848 andrew gelman stats-2013-05-09-A tale of two discussion papers</a></p>
<p>19 0.10205717 <a title="1476-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-20-Prior_beliefs_about_locations_of_decision_boundaries.html">1130 andrew gelman stats-2012-01-20-Prior beliefs about locations of decision boundaries</a></p>
<p>20 0.10088801 <a title="1476-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-27-%28R-Py-Cmd%29Stan_2.1.0.html">2150 andrew gelman stats-2013-12-27-(R-Py-Cmd)Stan 2.1.0</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.106), (1, 0.115), (2, 0.018), (3, 0.03), (4, 0.066), (5, 0.047), (6, 0.02), (7, -0.065), (8, -0.021), (9, -0.097), (10, -0.049), (11, 0.001), (12, -0.071), (13, -0.015), (14, -0.046), (15, -0.068), (16, -0.006), (17, 0.022), (18, -0.001), (19, -0.021), (20, 0.026), (21, -0.04), (22, -0.024), (23, -0.032), (24, 0.026), (25, 0.017), (26, -0.049), (27, 0.026), (28, 0.04), (29, 0.036), (30, -0.04), (31, 0.003), (32, -0.044), (33, 0.025), (34, -0.033), (35, 0.084), (36, -0.003), (37, 0.012), (38, -0.052), (39, 0.023), (40, 0.06), (41, 0.052), (42, -0.062), (43, -0.028), (44, -0.026), (45, -0.056), (46, 0.042), (47, 0.004), (48, -0.019), (49, 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92119706 <a title="1476-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-Stan_is_fast.html">1476 andrew gelman stats-2012-08-30-Stan is fast</a></p>
<p>Introduction: 10,000 iterations for 4 chains on the (precompiled) efficiently-parameterized 8-schools model: 
   
 > date () 
[1] "Thu Aug 30 22:12:53 2012" 
> fit3 <- stan (fit=fit2, data = schools_dat, iter = 1e4, n_chains = 4) 
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). 
Iteration: 10000 / 10000 [100%]  (Sampling) 
 
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2). 
Iteration: 10000 / 10000 [100%]  (Sampling)
 
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3). 
Iteration: 10000 / 10000 [100%]  (Sampling)
 
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4). 
Iteration: 10000 / 10000 [100%]  (Sampling)
 
> date () 
[1] "Thu Aug 30 22:12:55 2012" 
> print (fit3) 
Inference for Stan model: anon_model. 
4 chains: each with iter=10000; warmup=5000; thin=1; 10000 iterations saved.
 
         mean se_mean  sd  2.5%  25%  50%  75% 97.5% n_eff Rhat 
mu        8.0     0.1 5.1  -2.0  4.7  8.0 11.3  18.4  4032    1 
tau       6.7     0.1 5.6   0.3  2.5  5.4  9.3  21.2  2958    1 
eta[1]    0.4     0.0 0.9  -1.5 -0</p><p>2 0.69848353 <a title="1476-lsi-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-10-Stan_Model_of_the_Week%3A__PK_Calculation_of_IV_and_Oral_Dosing.html">2242 andrew gelman stats-2014-03-10-Stan Model of the Week:  PK Calculation of IV and Oral Dosing</a></p>
<p>Introduction: [Update: Revised given comments from Wingfeet, Andrew and germo.  Thanks!  I'd mistakenly translated the dlnorm priors in the first version --- amazing what a difference the priors make.  I also escaped the less-than and greater-than signs in the constraints in the model so they're visible.  I also updated to match the thin=2 output of JAGS.]
 
We’re going to be starting a Stan “model of the P” (for some time period P) column, so I thought I’d kick things off with one of my own.  I’ve been following the  Wingvoet blog , the author of which is identified only by the Blogger handle  Wingfeet ;  a couple of days ago this lovely post came out:
  
  PK calculation of IV and oral dosing in JAGS 
   
Wingfeet’s post implemented an answer to question 6 from chapter 6 of problem from Rowland and Tozer’s 2010 book,   Clinical Pharmacokinetics and Pharmacodynamics  , Fourth edition, Lippincott, Williams & Wilkins.   
 
So in the grand tradition of using this blog to procrastinate, I thought I’d t</p><p>3 0.68811518 <a title="1476-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-30-Stan_Project%3A__Continuous_Relaxations_for_Discrete_MRFs.html">2003 andrew gelman stats-2013-08-30-Stan Project:  Continuous Relaxations for Discrete MRFs</a></p>
<p>Introduction: Hamiltonian Monte Carlo (HMC), as used by  Stan , is only defined for continuous parameters.  We’d love to be able to do discrete sampling.  So I was excited when I saw this:
  

Yichuan Zhang, Charles Sutton, Amos J Storkey, and Zoubin Ghahramani.  2012.  Continuous Relaxations for Discrete Hamiltonian Monte Carlo .   NIPS  25.


 Abstract:  Continuous relaxations play an important role in discrete optimization, but have not seen much use in approximate probabilistic inference. Here we show that a general form of the Gaussian Integral Trick makes it possible to transform a wide class of discrete variable undirected models into fully continuous systems. The continuous representation allows the use of gradient-based Hamiltonian Monte Carlo for inference, results in new ways of estimating normalization constants (partition functions), and in general opens up a number of new avenues for inference in difficult discrete systems. We demonstrate some of these continuous relaxation inference a</p><p>4 0.67598951 <a title="1476-lsi-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-Stan_Model_of_the_Week%3A_Hierarchical_Modeling_of_Supernovas.html">2299 andrew gelman stats-2014-04-21-Stan Model of the Week: Hierarchical Modeling of Supernovas</a></p>
<p>Introduction: The Stan Model of the Week showcases research using Stan to push the limits of applied statistics.  If you have a model that you would like to submit for a future post then send us an  email . 
 
Our inaugural post comes from Nathan Sanders, a graduate student finishing up his thesis on astrophysics at Harvard. Nathan writes,
  
“Core-collapse supernovae, the luminous explosions of massive stars, exhibit an expansive and meaningful diversity of behavior in their brightness evolution over time (their “light curves”). Our group discovers and monitors these events using the Pan-STARRS1 telescope in Hawaii, and we’ve collected a dataset of about 20,000 individual photometric observations of about 80 Type IIP supernovae, the class my work has focused on. While this dataset provides one of the best available tools to infer the explosion properties of these supernovae, due to the nature of extragalactic astronomy (observing from distances 
  1 billion light years), these light curves typicall</p><p>5 0.67072415 <a title="1476-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>Introduction: Klaas Metselaar writes: 
  
  
I [Metselaar] am currently involved in a discussion about the use of the notion “predictive” as used in “posterior predictive check”. I would argue that the notion “predictive” should be reserved for posterior checks using information not used in the determination of the posterior. 
I quote from the discussion: 
“However, the predictive uncertainty in a Bayesian calculation requires sampling from all the random variables, and this includes both the model parameters and the residual error”.


My [Metselaar's] comment:


This may be exactly the point I am worried about: shouldn’t the predictive uncertainty be defined as sampling from the posterior parameter distribution +  residual error + sampling from the prediction error distribution? 
Residual error reduces to measurement error in the case of a  model which is perfect for the sample of experiments. Measurement error could be reduced to almost zero by ideal and perfect measurement instruments. 
I would h</p><p>6 0.66865069 <a title="1476-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-17-Jumping_off_the_edge_of_the_world.html">858 andrew gelman stats-2011-08-17-Jumping off the edge of the world</a></p>
<p>7 0.6652382 <a title="1476-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-28-Path_sampling_for_models_of_varying_dimension.html">1089 andrew gelman stats-2011-12-28-Path sampling for models of varying dimension</a></p>
<p>8 0.66221738 <a title="1476-lsi-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>9 0.65340239 <a title="1476-lsi-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-14-Transitioning_to_Stan.html">2291 andrew gelman stats-2014-04-14-Transitioning to Stan</a></p>
<p>10 0.6447798 <a title="1476-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-12-Samplers_for_Big_Science%3A__emcee_and_BAT.html">2020 andrew gelman stats-2013-09-12-Samplers for Big Science:  emcee and BAT</a></p>
<p>11 0.64195329 <a title="1476-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-16-%E2%80%9CReal_data_can_be_a_pain%E2%80%9D.html">1460 andrew gelman stats-2012-08-16-“Real data can be a pain”</a></p>
<p>12 0.64011586 <a title="1476-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-Another_reason_why_you_can_get_good_inferences_from_a_bad_model.html">1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</a></p>
<p>13 0.625009 <a title="1476-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-16-Stantastic%21.html">1580 andrew gelman stats-2012-11-16-Stantastic!</a></p>
<p>14 0.62058228 <a title="1476-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>15 0.6163618 <a title="1476-lsi-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-Running_into_a_Stan_Reference_by_Accident.html">2231 andrew gelman stats-2014-03-03-Running into a Stan Reference by Accident</a></p>
<p>16 0.61595142 <a title="1476-lsi-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-20-Thermodynamic_Monte_Carlo%3A__Michael_Betancourt%E2%80%99s_new_method_for_simulating_from_difficult_distributions_and_evaluating_normalizing_constants.html">2340 andrew gelman stats-2014-05-20-Thermodynamic Monte Carlo:  Michael Betancourt’s new method for simulating from difficult distributions and evaluating normalizing constants</a></p>
<p>17 0.60486245 <a title="1476-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-19-Whassup_with_deviance_having_a_high_posterior_correlation_with_a_parameter_in_the_model%3F.html">1221 andrew gelman stats-2012-03-19-Whassup with deviance having a high posterior correlation with a parameter in the model?</a></p>
<p>18 0.60197401 <a title="1476-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-12-Stan_1.3.0_and_RStan_1.3.0_Ready_for_Action.html">1799 andrew gelman stats-2013-04-12-Stan 1.3.0 and RStan 1.3.0 Ready for Action</a></p>
<p>19 0.60053152 <a title="1476-lsi-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-07-My_recent_debugging_experience.html">2161 andrew gelman stats-2014-01-07-My recent debugging experience</a></p>
<p>20 0.59741199 <a title="1476-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-30-David_Hogg_on_statistics.html">1401 andrew gelman stats-2012-06-30-David Hogg on statistics</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.013), (13, 0.013), (15, 0.016), (16, 0.036), (23, 0.012), (24, 0.158), (27, 0.013), (36, 0.372), (45, 0.012), (54, 0.013), (86, 0.021), (99, 0.172)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98925877 <a title="1476-lda-1" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-10-Stan_Model_of_the_Week%3A__PK_Calculation_of_IV_and_Oral_Dosing.html">2242 andrew gelman stats-2014-03-10-Stan Model of the Week:  PK Calculation of IV and Oral Dosing</a></p>
<p>Introduction: [Update: Revised given comments from Wingfeet, Andrew and germo.  Thanks!  I'd mistakenly translated the dlnorm priors in the first version --- amazing what a difference the priors make.  I also escaped the less-than and greater-than signs in the constraints in the model so they're visible.  I also updated to match the thin=2 output of JAGS.]
 
We’re going to be starting a Stan “model of the P” (for some time period P) column, so I thought I’d kick things off with one of my own.  I’ve been following the  Wingvoet blog , the author of which is identified only by the Blogger handle  Wingfeet ;  a couple of days ago this lovely post came out:
  
  PK calculation of IV and oral dosing in JAGS 
   
Wingfeet’s post implemented an answer to question 6 from chapter 6 of problem from Rowland and Tozer’s 2010 book,   Clinical Pharmacokinetics and Pharmacodynamics  , Fourth edition, Lippincott, Williams & Wilkins.   
 
So in the grand tradition of using this blog to procrastinate, I thought I’d t</p><p>same-blog 2 0.86481804 <a title="1476-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-Stan_is_fast.html">1476 andrew gelman stats-2012-08-30-Stan is fast</a></p>
<p>Introduction: 10,000 iterations for 4 chains on the (precompiled) efficiently-parameterized 8-schools model: 
   
 > date () 
[1] "Thu Aug 30 22:12:53 2012" 
> fit3 <- stan (fit=fit2, data = schools_dat, iter = 1e4, n_chains = 4) 
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). 
Iteration: 10000 / 10000 [100%]  (Sampling) 
 
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2). 
Iteration: 10000 / 10000 [100%]  (Sampling)
 
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3). 
Iteration: 10000 / 10000 [100%]  (Sampling)
 
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4). 
Iteration: 10000 / 10000 [100%]  (Sampling)
 
> date () 
[1] "Thu Aug 30 22:12:55 2012" 
> print (fit3) 
Inference for Stan model: anon_model. 
4 chains: each with iter=10000; warmup=5000; thin=1; 10000 iterations saved.
 
         mean se_mean  sd  2.5%  25%  50%  75% 97.5% n_eff Rhat 
mu        8.0     0.1 5.1  -2.0  4.7  8.0 11.3  18.4  4032    1 
tau       6.7     0.1 5.6   0.3  2.5  5.4  9.3  21.2  2958    1 
eta[1]    0.4     0.0 0.9  -1.5 -0</p><p>3 0.8538276 <a title="1476-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-10-%E2%80%9CProposition_and_experiment%E2%80%9D.html">1797 andrew gelman stats-2013-04-10-“Proposition and experiment”</a></p>
<p>Introduction: Anna Lena Phillips  writes :
  
I. Many people will not, of their own accord, look at a poem.


II. Millions of people will, of their own accord, spend lots and lots of time looking at photographs of cats.


III. Therefore, earlier this year, I concluded that the best strategy for increasing the number of viewers for poems would be to print them on top of photographs of cats.


IV. I happen to like looking at both poems and cats.


V. So this is, for me, a win-win situation.


VI. Fortunately, my own cat is a patient model, and (if I am to be believed) quite photogenic.


VII. The aforementioned cat is Tisko Tansi, small hero.


VII. Thus I present to you (albeit in digital rather than physical form) an Endearments broadside, featuring a poem that originally appeared in BlazeVOX spring 2011.


VIII. If you want to share a copy of this image, please ask first. If you want a real copy, you can ask about that too.
  
She follows up with an image of a cat, on which is superimposed a short</p><p>4 0.84997129 <a title="1476-lda-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-02-Information_is_good.html">176 andrew gelman stats-2010-08-02-Information is good</a></p>
<p>Introduction: Washington Post and Slate reporter Anne Applebaum wrote a  dismissive  column about Wikileaks, saying that they “offer nothing more than raw data.”
 
Applebaum argues that “The notion that the Internet can replace traditional news-gathering has just been revealed to be a myth. . . . without more journalism, more investigation, more work, these documents just don’t matter that much.”
 
Fine.  But don’t undervalue the role of mere data!  The usual story is that we  don’t  get to see the raw data underlying newspaper stories.  Wikileaks and other crowdsourced data can be extremely useful, whether or not they replace “traditional news-gathering.”</p><p>5 0.81868768 <a title="1476-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-31-Watercolor_regression.html">1478 andrew gelman stats-2012-08-31-Watercolor regression</a></p>
<p>Introduction: Solomon Hsiang writes:
  
Two small follow-ups based on the  discussion  (the second/bigger one is to address your comment about the 95% CI edges).


1. I realized that if we plot the confidence intervals as a solid color that fades (eg. using the “fixed ink” scheme from before) we can make sure the regression line also has heightened visual weight where confidence is high by plotting the line white. This makes the contrast (and thus visual weight) between the regression line and the CI highest when the CI is narrow and dark. As the CI fade near the edges, so does the contrast with the regression line. This is a small adjustment, but I like it because it is so simple and it makes the graph much nicer. (see “visually_weighted_fill_reverse” attached). My posted code has been updated to do this automatically.


2. You and your readers didn’t like that the edges of the filled CI were so sharp and arbitrary. But I didn’t like that the contrast between the spaghetti lines and the background</p><p>6 0.75802612 <a title="1476-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-02-Obama_and_Reagan%2C_sitting_in_a_tree%2C_etc..html">551 andrew gelman stats-2011-02-02-Obama and Reagan, sitting in a tree, etc.</a></p>
<p>7 0.71114433 <a title="1476-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-25-Who_gets_wedding_announcements_in_the_Times%3F.html">370 andrew gelman stats-2010-10-25-Who gets wedding announcements in the Times?</a></p>
<p>8 0.70847297 <a title="1476-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-01-Arrow%E2%80%99s_theorem_update.html">883 andrew gelman stats-2011-09-01-Arrow’s theorem update</a></p>
<p>9 0.69628632 <a title="1476-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-26-Graphs_showing_regression_uncertainty%3A__the_code%21.html">1470 andrew gelman stats-2012-08-26-Graphs showing regression uncertainty:  the code!</a></p>
<p>10 0.68427896 <a title="1476-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-08-Of_parsing_and_chess.html">1847 andrew gelman stats-2013-05-08-Of parsing and chess</a></p>
<p>11 0.68284458 <a title="1476-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-17-NSF_program_%E2%80%9Cto_support_analytic_and_methodological_research_in_support_of_its_surveys%E2%80%9D.html">1217 andrew gelman stats-2012-03-17-NSF program “to support analytic and methodological research in support of its surveys”</a></p>
<p>12 0.68108022 <a title="1476-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-20-%E2%80%9CPeople_with_an_itch_to_scratch%E2%80%9D.html">101 andrew gelman stats-2010-06-20-“People with an itch to scratch”</a></p>
<p>13 0.67045128 <a title="1476-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-14-Progress%21__%28on_the_understanding_of_the_role_of_randomization_in_Bayesian_inference%29.html">1898 andrew gelman stats-2013-06-14-Progress!  (on the understanding of the role of randomization in Bayesian inference)</a></p>
<p>14 0.66419429 <a title="1476-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-11-Actually%2C_I_have_no_problem_with_this_graph.html">1851 andrew gelman stats-2013-05-11-Actually, I have no problem with this graph</a></p>
<p>15 0.65885806 <a title="1476-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-18-What%E2%80%99s_my_Kasparov_number%3F.html">2105 andrew gelman stats-2013-11-18-What’s my Kasparov number?</a></p>
<p>16 0.65435016 <a title="1476-lda-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-15-The_two_faces_of_Erving_Goffman%3A__Subtle_observer_of_human_interactions%2C_and_Smug_organzation_man.html">415 andrew gelman stats-2010-11-15-The two faces of Erving Goffman:  Subtle observer of human interactions, and Smug organzation man</a></p>
<p>17 0.6457181 <a title="1476-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-27-In_Linux%2C_use_jags%28%29_to_call_Jags_instead_of_using_bugs%28%29_to_call_OpenBugs.html">55 andrew gelman stats-2010-05-27-In Linux, use jags() to call Jags instead of using bugs() to call OpenBugs</a></p>
<p>18 0.6308006 <a title="1476-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-10-Schiminovich_is_on_The_Simpsons.html">2096 andrew gelman stats-2013-11-10-Schiminovich is on The Simpsons</a></p>
<p>19 0.60675687 <a title="1476-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-10-They%E2%80%99d_rather_be_rigorous_than_right.html">1666 andrew gelman stats-2013-01-10-They’d rather be rigorous than right</a></p>
<p>20 0.5865863 <a title="1476-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-27-%28R-Py-Cmd%29Stan_2.1.0.html">2150 andrew gelman stats-2013-12-27-(R-Py-Cmd)Stan 2.1.0</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
