<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1267 andrew gelman stats-2012-04-17-Hierarchical-multilevel modeling with “big data”</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1267" href="#">andrew_gelman_stats-2012-1267</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1267 andrew gelman stats-2012-04-17-Hierarchical-multilevel modeling with “big data”</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1267-html" href="http://andrewgelman.com/2012/04/17/hierarchicalmultilevel-modeling-with-big-data/">html</a></p><p>Introduction: Dean Eckles writes:
  
I make extensive use of random effects models in my academic and industry research, as they are very often appropriate.


However, with very large data sets, I am not sure what to do. Say I have thousands of levels of a grouping factor, and the number of observations totals in the billions. Despite having lots of observations, I am often either dealing with (a) small effects or (b) trying to fit models with many predictors.


So I would really like to use a random effects model to borrow strength across the levels of the grouping factor, but I am not sure how to practically do this. Are you aware of any approaches to fitting random effects models (including approximations) that work for very large data sets? For example, applying a procedure to each group, and then using the results of this to shrink each fit in some appropriate way.


Just to clarify, here I am only worried about the non-crossed and in fact single-level case. I don’t see any easy route for cross</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Dean Eckles writes:    I make extensive use of random effects models in my academic and industry research, as they are very often appropriate. [sent-1, score-0.651]
</p><p>2 Say I have thousands of levels of a grouping factor, and the number of observations totals in the billions. [sent-3, score-0.587]
</p><p>3 Despite having lots of observations, I am often either dealing with (a) small effects or (b) trying to fit models with many predictors. [sent-4, score-0.403]
</p><p>4 So I would really like to use a random effects model to borrow strength across the levels of the grouping factor, but I am not sure how to practically do this. [sent-5, score-1.201]
</p><p>5 Are you aware of any approaches to fitting random effects models (including approximations) that work for very large data sets? [sent-6, score-0.672]
</p><p>6 For example, applying a procedure to each group, and then using the results of this to shrink each fit in some appropriate way. [sent-7, score-0.172]
</p><p>7 I don’t see any easy route for crossed random effects, which is why we have been content to just get reasonable  estimates uncertainty estimates for means, etc. [sent-9, score-0.9]
</p><p>8 (Some extra details: In one case, I am fitting a propensity score model where there are really more than 2e8 somewhat similar treatments. [sent-13, score-0.328]
</p><p>9 One approach is to go totally unpooled (your secret weapon), but I think variance will be a problem here sense there are so many features. [sent-15, score-0.284]
</p><p>10 Another approach is to use some other kind of shrinkage, like the lasso or the grouped lasso. [sent-16, score-0.286]
</p><p>11 My reply:   I’ve been thinking about this problem for  awhile . [sent-18, score-0.081]
</p><p>12 It seems likely to me that some Gibbs-like and EM-like solutions should be possible. [sent-19, score-0.083]
</p><p>13 (And if there’s an EM solution, there should be a variational Bayes solution too. [sent-20, score-0.211]
</p><p>14 - Speeding things up by analyzing subsets of the data. [sent-22, score-0.099]
</p><p>15 , “California”) but not in the sparser groups (“Rhode Island,” etc. [sent-25, score-0.124]
</p><p>16 This has a bit of the feel of particle filtering. [sent-28, score-0.097]
</p><p>17 - My guess is that the way to go is to get this working for a particular problem of interest, then could think about how to implement it efficiently in Stan etc. [sent-29, score-0.167]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('random', 0.234), ('effects', 0.23), ('grouping', 0.223), ('crossed', 0.215), ('em', 0.182), ('observations', 0.135), ('group', 0.134), ('estimates', 0.127), ('rhode', 0.124), ('sparser', 0.124), ('speeding', 0.124), ('unpooled', 0.124), ('model', 0.122), ('sets', 0.121), ('levels', 0.118), ('factor', 0.117), ('grouped', 0.117), ('fitting', 0.115), ('solution', 0.112), ('totals', 0.111), ('uncertainty', 0.109), ('eckles', 0.108), ('envision', 0.102), ('subsets', 0.099), ('borrow', 0.099), ('variational', 0.099), ('particle', 0.097), ('fed', 0.095), ('weapon', 0.095), ('practically', 0.094), ('island', 0.094), ('extensive', 0.094), ('appreciated', 0.094), ('models', 0.093), ('shrink', 0.092), ('propensity', 0.091), ('lasso', 0.09), ('route', 0.088), ('shrinkage', 0.087), ('approximations', 0.087), ('efficiently', 0.086), ('dean', 0.084), ('gibbs', 0.083), ('solutions', 0.083), ('units', 0.082), ('problem', 0.081), ('strength', 0.081), ('fit', 0.08), ('approach', 0.079), ('including', 0.078)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9999997 <a title="1267-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-17-Hierarchical-multilevel_modeling_with_%E2%80%9Cbig_data%E2%80%9D.html">1267 andrew gelman stats-2012-04-17-Hierarchical-multilevel modeling with “big data”</a></p>
<p>Introduction: Dean Eckles writes:
  
I make extensive use of random effects models in my academic and industry research, as they are very often appropriate.


However, with very large data sets, I am not sure what to do. Say I have thousands of levels of a grouping factor, and the number of observations totals in the billions. Despite having lots of observations, I am often either dealing with (a) small effects or (b) trying to fit models with many predictors.


So I would really like to use a random effects model to borrow strength across the levels of the grouping factor, but I am not sure how to practically do this. Are you aware of any approaches to fitting random effects models (including approximations) that work for very large data sets? For example, applying a procedure to each group, and then using the results of this to shrink each fit in some appropriate way.


Just to clarify, here I am only worried about the non-crossed and in fact single-level case. I don’t see any easy route for cross</p><p>2 0.20441678 <a title="1267-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-30-Fixed_effects%2C_followed_by_Bayes_shrinkage%3F.html">1644 andrew gelman stats-2012-12-30-Fixed effects, followed by Bayes shrinkage?</a></p>
<p>Introduction: Stuart Buck writes:
  
I have a question about  fixed effects vs. random effects . Amongst economists who study teacher value-added, it has become common to see people saying that they estimated teacher fixed effects (via least squares dummy variables, so that there is a parameter for each teacher), but that they then applied empirical Bayes shrinkage so that the teacher effects are brought closer to the mean.  (See  this paper  by Jacob and Lefgren, for example.)


Can that really be what they are doing? Why wouldn’t they just run random (modeled) effects in the first place? I feel like there’s something I’m missing.
  
My reply:  I don’t know the full story here, but I’m thinking there are two goals, first to get an unbiased estimate of an overall treatment effect (and there the econometricians prefer so-called fixed effects; I disagree with them on this but I know where they’re coming from) and second to estimate individual teacher effects (and there it makes sense to use so-called</p><p>3 0.17655671 <a title="1267-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-03-How_best_to_compare_effects_measured_in_two_different_time_periods%3F.html">2086 andrew gelman stats-2013-11-03-How best to compare effects measured in two different time periods?</a></p>
<p>Introduction: I received the following email from someone who wishes to remain anonymous:
  
My colleague and I are trying to understand the best way to approach a problem involving measuring a group of individuals’ abilities across time, and are hoping you can offer some guidance.


We are trying to analyze the combined effect of two distinct groups of people (A and B, with no overlap between A and B) who collaborate to produce a binary outcome, using a mixed logistic regression along the lines of the following.


Outcome ~ (1 | A) + (1 | B) + Other variables


What we’re interested in testing was whether the observed A random effects in period  1 are predictive of the A random effects in the following period 2.  Our idea being create two models, each using a different period’s worth of data, to create two sets of A coefficients, then observe the relationship between the two.  If the A’s have a persistent ability across periods, the coefficients should be correlated or show a linear-ish relationshi</p><p>4 0.16174182 <a title="1267-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-08-Multilevel_regression_with_shrinkage_for_%E2%80%9Cfixed%E2%80%9D_effects.html">653 andrew gelman stats-2011-04-08-Multilevel regression with shrinkage for “fixed” effects</a></p>
<p>Introduction: Dean Eckles writes:
  
I remember reading on your blog that you were working on some tools to fit multilevel models that also include “fixed” effects — such as continuous predictors — that are also estimated with shrinkage (for example, an L1 or L2 penalty). Any new developments on this front?


I often find myself wanting to fit a multilevel model to some data, but also needing to include a number of “fixed” effects, mainly continuous variables. This makes me wary of overfitting to these predictors, so then I’d want to use some kind of shrinkage.


As far as I can tell, the main options for doing this now is by going fully Bayesian and using a Gibbs sampler. With MCMCglmm or BUGS/JAGS I could just specify a prior on the fixed effects that corresponds to a desired penalty. However, this is pretty slow, especially with a large data set and because I’d like to select the penalty parameter by cross-validation (which is where this isn’t very Bayesian I guess?).
  
My reply:
 
We allow info</p><p>5 0.15948689 <a title="1267-tfidf-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-17-So-called_fixed_and_random_effects.html">472 andrew gelman stats-2010-12-17-So-called fixed and random effects</a></p>
<p>Introduction: Someone writes:
  
I am hoping you can give me some advice about when to use fixed and random effects model. I am currently working on a paper that examines the effect of . . . by comparing states . . .


It got reviewed . . . by three economists and all suggest that we run a fixed effects model.  We ran a hierarchial model in the paper that allow the intercept and slope to vary before and after . . . My question is which is correct? We have ran it both ways and really it makes no difference which model you run, the results are very similar. But for my own learning, I would really like to understand which to use under what circumstances.  Is the fact that we use the whole population reason enough to just run a fixed effect model?


Perhaps you can suggest a good reference to this question of when to run a fixed vs. random effects model.
  
I’m not always sure what is meant by a “fixed effects model”; see my paper on Anova for discussion of the problems with this terminology:
 
http://w</p><p>6 0.15276861 <a title="1267-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-03-Hierarchical_array_priors_for_ANOVA_decompositions.html">1786 andrew gelman stats-2013-04-03-Hierarchical array priors for ANOVA decompositions</a></p>
<p>7 0.14985025 <a title="1267-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>8 0.149331 <a title="1267-tfidf-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-10-R_vs._Stata%2C_or%2C_Different_ways_to_estimate_multilevel_models.html">269 andrew gelman stats-2010-09-10-R vs. Stata, or, Different ways to estimate multilevel models</a></p>
<p>9 0.14538337 <a title="1267-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-17-Statistics_in_a_world_where_nothing_is_random.html">1628 andrew gelman stats-2012-12-17-Statistics in a world where nothing is random</a></p>
<p>10 0.14511204 <a title="1267-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-12-year_%2B_%281%7Cyear%29.html">851 andrew gelman stats-2011-08-12-year + (1|year)</a></p>
<p>11 0.14395835 <a title="1267-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-09-%E2%80%9CHeterogeneity_of_variance_in_experimental_studies%3A__A_challenge_to_conventional_interpretations%E2%80%9D.html">1891 andrew gelman stats-2013-06-09-“Heterogeneity of variance in experimental studies:  A challenge to conventional interpretations”</a></p>
<p>12 0.13347904 <a title="1267-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-04-A_new_R_package_for_fititng_multilevel_models.html">501 andrew gelman stats-2011-01-04-A new R package for fititng multilevel models</a></p>
<p>13 0.13212454 <a title="1267-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>14 0.13178524 <a title="1267-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-18-Question_on_Type_M_errors.html">963 andrew gelman stats-2011-10-18-Question on Type M errors</a></p>
<p>15 0.13034311 <a title="1267-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-18-Tibshirani_announces_new_research_result%3A__A_significance_test_for_the_lasso.html">1769 andrew gelman stats-2013-03-18-Tibshirani announces new research result:  A significance test for the lasso</a></p>
<p>16 0.12934241 <a title="1267-tfidf-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-02-The_problem_of_overestimation_of_group-level_variance_parameters.html">63 andrew gelman stats-2010-06-02-The problem of overestimation of group-level variance parameters</a></p>
<p>17 0.12831931 <a title="1267-tfidf-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-21-Random_matrices_in_the_news.html">2258 andrew gelman stats-2014-03-21-Random matrices in the news</a></p>
<p>18 0.12653369 <a title="1267-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-18-What_to_read_to_catch_up_on_multivariate_statistics%3F.html">1726 andrew gelman stats-2013-02-18-What to read to catch up on multivariate statistics?</a></p>
<p>19 0.12479214 <a title="1267-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-03-Uncertainty_in_parameter_estimates_using_multilevel_models.html">1966 andrew gelman stats-2013-08-03-Uncertainty in parameter estimates using multilevel models</a></p>
<p>20 0.12167877 <a title="1267-tfidf-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-31-Somewhat_Bayesian_multilevel_modeling.html">246 andrew gelman stats-2010-08-31-Somewhat Bayesian multilevel modeling</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.243), (1, 0.162), (2, 0.077), (3, -0.065), (4, 0.088), (5, 0.036), (6, 0.023), (7, -0.065), (8, 0.062), (9, 0.049), (10, 0.004), (11, 0.014), (12, 0.008), (13, -0.029), (14, 0.015), (15, -0.009), (16, -0.059), (17, 0.004), (18, -0.016), (19, 0.046), (20, -0.02), (21, -0.042), (22, -0.015), (23, 0.007), (24, -0.029), (25, -0.065), (26, -0.066), (27, 0.079), (28, -0.012), (29, -0.003), (30, -0.026), (31, -0.001), (32, -0.06), (33, 0.007), (34, 0.04), (35, 0.019), (36, -0.043), (37, 0.043), (38, -0.008), (39, -0.022), (40, 0.029), (41, 0.02), (42, 0.048), (43, 0.037), (44, -0.071), (45, 0.04), (46, -0.037), (47, -0.001), (48, 0.008), (49, -0.008)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96274799 <a title="1267-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-17-Hierarchical-multilevel_modeling_with_%E2%80%9Cbig_data%E2%80%9D.html">1267 andrew gelman stats-2012-04-17-Hierarchical-multilevel modeling with “big data”</a></p>
<p>Introduction: Dean Eckles writes:
  
I make extensive use of random effects models in my academic and industry research, as they are very often appropriate.


However, with very large data sets, I am not sure what to do. Say I have thousands of levels of a grouping factor, and the number of observations totals in the billions. Despite having lots of observations, I am often either dealing with (a) small effects or (b) trying to fit models with many predictors.


So I would really like to use a random effects model to borrow strength across the levels of the grouping factor, but I am not sure how to practically do this. Are you aware of any approaches to fitting random effects models (including approximations) that work for very large data sets? For example, applying a procedure to each group, and then using the results of this to shrink each fit in some appropriate way.


Just to clarify, here I am only worried about the non-crossed and in fact single-level case. I don’t see any easy route for cross</p><p>2 0.8861807 <a title="1267-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-03-How_best_to_compare_effects_measured_in_two_different_time_periods%3F.html">2086 andrew gelman stats-2013-11-03-How best to compare effects measured in two different time periods?</a></p>
<p>Introduction: I received the following email from someone who wishes to remain anonymous:
  
My colleague and I are trying to understand the best way to approach a problem involving measuring a group of individuals’ abilities across time, and are hoping you can offer some guidance.


We are trying to analyze the combined effect of two distinct groups of people (A and B, with no overlap between A and B) who collaborate to produce a binary outcome, using a mixed logistic regression along the lines of the following.


Outcome ~ (1 | A) + (1 | B) + Other variables


What we’re interested in testing was whether the observed A random effects in period  1 are predictive of the A random effects in the following period 2.  Our idea being create two models, each using a different period’s worth of data, to create two sets of A coefficients, then observe the relationship between the two.  If the A’s have a persistent ability across periods, the coefficients should be correlated or show a linear-ish relationshi</p><p>3 0.87301767 <a title="1267-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-10-R_vs._Stata%2C_or%2C_Different_ways_to_estimate_multilevel_models.html">269 andrew gelman stats-2010-09-10-R vs. Stata, or, Different ways to estimate multilevel models</a></p>
<p>Introduction: Cyrus writes:
  
I [Cyrus] was teaching a class on multilevel modeling, and we were playing around with different method to fit a random effects logit model with 2 random intercepts—one corresponding to “family” and another corresponding to “community” (labeled “mom” and “cluster” in the data, respectively).  There are also a few regressors at the individual, family, and community level.  We were replicating in part some of the results from the  following paper :  Improved estimation procedures for multilevel models with binary response: a case-study, by G Rodriguez, N Goldman.


(I say “replicating in part” because we didn’t include all the regressors that they use, only a subset.)  We were looking at the performance of estimation via glmer in R’s lme4 package, glmmPQL in R’s MASS package, and Stata’s xtmelogit.  We wanted to study the performance of various estimation methods, including adaptive quadrature methods and penalized quasi-likelihood.


I was shocked to discover that glmer</p><p>4 0.83603084 <a title="1267-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-24-Estimating_and_summarizing_inference_for_hierarchical_variance_parameters_when_the_number_of_groups_is_small.html">2145 andrew gelman stats-2013-12-24-Estimating and summarizing inference for hierarchical variance parameters when the number of groups is small</a></p>
<p>Introduction: Chris Che-Castaldo writes:
  
I am trying to compute variance components for a hierarchical model where the group level has two binary predictors and their interaction. When I model each of these three predictors as N(0, tau) the model will not converge, perhaps because the number of coefficients in each batch is so small (2 for the main effects and 4 for the interaction). Although I could simply leave all these as predictors as unmodeled fixed effects, the last sentence of section 21.2 on page 462 of Gelman and Hill (2007) suggests this would not be a wise course of action:

 
For example, it is not clear how to define the (finite) standard deviation of variables that are included in interactions.
 

I am curious – is there still no clear cut way to directly compute the finite standard deviation for binary unmodeled variables that are also part of an interaction as well as the interaction itself?
  
My reply:  I’d recommend including these in your model (it’s probably easiest to do so</p><p>5 0.81926596 <a title="1267-lsi-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-12-Finite-population_standard_deviation_in_a_hierarchical_model.html">464 andrew gelman stats-2010-12-12-Finite-population standard deviation in a hierarchical model</a></p>
<p>Introduction: Karri Seppa writes:
  
My topic is regional variation in the cause-specific survival of breast cancer patients across the 21 hospital districts in Finland, this component being modeled by random effects. I am interested mainly in the district-specific effects, and with a hierarchical model I can get reasonable estimates also for sparsely populated districts.


Based on the recommendation given in the book by yourself and Dr. Hill (2007) I tend to think that the finite-population variance would be an appropriate measure to summarize the overall variation across the 21 districts. However, I feel it is somewhat incoherent first to assume a Normal distribution for the district effects, involving a “superpopulation” variance parameter, and then to compute the finite-population variance from the estimated district-specific parameters. I wonder whether the finite-population variance were more appropriate in the context of a model with fixed district effects?
  
My reply:
  

 
I agree that th</p><p>6 0.81902272 <a title="1267-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-11-%E2%80%9C2_level_logit_with_2_REs_%26_large_sample._computational_nightmare_%E2%80%93_please_help%E2%80%9D.html">759 andrew gelman stats-2011-06-11-“2 level logit with 2 REs & large sample. computational nightmare – please help”</a></p>
<p>7 0.81653428 <a title="1267-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-03-Uncertainty_in_parameter_estimates_using_multilevel_models.html">1966 andrew gelman stats-2013-08-03-Uncertainty in parameter estimates using multilevel models</a></p>
<p>8 0.81583655 <a title="1267-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-Multilevel_modeling_even_when_you%E2%80%99re_not_interested_in_predictions_for_new_groups.html">1194 andrew gelman stats-2012-03-04-Multilevel modeling even when you’re not interested in predictions for new groups</a></p>
<p>9 0.8130073 <a title="1267-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-08-Multilevel_regression_with_shrinkage_for_%E2%80%9Cfixed%E2%80%9D_effects.html">653 andrew gelman stats-2011-04-08-Multilevel regression with shrinkage for “fixed” effects</a></p>
<p>10 0.80847174 <a title="1267-lsi-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-17-So-called_fixed_and_random_effects.html">472 andrew gelman stats-2010-12-17-So-called fixed and random effects</a></p>
<p>11 0.80077815 <a title="1267-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-12-year_%2B_%281%7Cyear%29.html">851 andrew gelman stats-2011-08-12-year + (1|year)</a></p>
<p>12 0.79610819 <a title="1267-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>13 0.78350842 <a title="1267-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-09-%E2%80%9CHeterogeneity_of_variance_in_experimental_studies%3A__A_challenge_to_conventional_interpretations%E2%80%9D.html">1891 andrew gelman stats-2013-06-09-“Heterogeneity of variance in experimental studies:  A challenge to conventional interpretations”</a></p>
<p>14 0.78201449 <a title="1267-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-03-Hierarchical_array_priors_for_ANOVA_decompositions.html">1786 andrew gelman stats-2013-04-03-Hierarchical array priors for ANOVA decompositions</a></p>
<p>15 0.77624917 <a title="1267-lsi-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-10-Combining_data_from_many_sources.html">948 andrew gelman stats-2011-10-10-Combining data from many sources</a></p>
<p>16 0.77060032 <a title="1267-lsi-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-19-Index_or_indicator_variables.html">2296 andrew gelman stats-2014-04-19-Index or indicator variables</a></p>
<p>17 0.76863533 <a title="1267-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-30-Fixed_effects%2C_followed_by_Bayes_shrinkage%3F.html">1644 andrew gelman stats-2012-12-30-Fixed effects, followed by Bayes shrinkage?</a></p>
<p>18 0.7674607 <a title="1267-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-11-Yes%2C_worry_about_generalizing_from_data_to_population.__But_multilevel_modeling_is_the_solution%2C_not_the_problem.html">1934 andrew gelman stats-2013-07-11-Yes, worry about generalizing from data to population.  But multilevel modeling is the solution, not the problem</a></p>
<p>19 0.76691163 <a title="1267-lsi-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-17-Clutering_and_variance_components.html">417 andrew gelman stats-2010-11-17-Clutering and variance components</a></p>
<p>20 0.76328832 <a title="1267-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Allowing_interaction_terms_to_vary.html">753 andrew gelman stats-2011-06-09-Allowing interaction terms to vary</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(9, 0.025), (15, 0.017), (16, 0.033), (24, 0.159), (45, 0.042), (52, 0.017), (63, 0.024), (68, 0.014), (69, 0.108), (75, 0.016), (85, 0.012), (86, 0.061), (89, 0.013), (99, 0.321)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97791696 <a title="1267-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-10-Translating_into_Votes%3A_The_Electoral_Impact_of_Spanish-Language_Ballots.html">406 andrew gelman stats-2010-11-10-Translating into Votes: The Electoral Impact of Spanish-Language Ballots</a></p>
<p>Introduction: Dan Hopkins sends along  this article :
  
[Hopkins] uses regression discontinuity design to estimate the turnout and election impacts of Spanish-language assistance provided under Section 203 of the Voting Rights Act. Analyses of two different data sets – the Latino National Survey and California 1998 primary election returns – show that Spanish-language assistance increased turnout for citizens who speak little English. The California results also demonstrate that election procedures an influence outcomes, as support for ending bilingual education dropped markedly in heavily Spanish-speaking neighborhoods with Spanish-language assistance. The California analyses find hints of backlash among non-Hispanic white precincts, but not with the same size or certainty. Small changes in election procedures can influence who votes as well as what wins.
  
Beyond the direct relevance of these results, I find this paper interesting as an example of research that is fundamentally quantitative.  Th</p><p>2 0.97123545 <a title="1267-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-24-What_is_the_normal_range_of_values_in_a_medical_test%3F.html">923 andrew gelman stats-2011-09-24-What is the normal range of values in a medical test?</a></p>
<p>Introduction: Geoffrey Sheean writes: 
  
  
I am having trouble thinking Bayesianly about the so-called ‘normal’ or ‘reference’ values that I am supposed to use in some of the tests I perform. These values are obtained from purportedly healthy people. Setting aside concerns about ascertainment bias, non-parametric distributions, and the like, the values are usually obtained by setting the limits at ± 2SD from the mean. In some cases, supposedly because of a non-normal distribution, the third highest and lowest value observed in the healthy group sets the limits, on the assumption that no more than 2 results (out of 20 samples) are allowed to exceed these values: if there are 3 or more, then the test is assumed to be abnormal and the reference range is said to reflect the 90th percentile. The results are binary – normal, abnormal.


The relevance to the diseased state is this. People who are known unequivocally to have condition X show Y abnormalities in these tests. Therefore, when people suspected</p><p>same-blog 3 0.96868396 <a title="1267-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-17-Hierarchical-multilevel_modeling_with_%E2%80%9Cbig_data%E2%80%9D.html">1267 andrew gelman stats-2012-04-17-Hierarchical-multilevel modeling with “big data”</a></p>
<p>Introduction: Dean Eckles writes:
  
I make extensive use of random effects models in my academic and industry research, as they are very often appropriate.


However, with very large data sets, I am not sure what to do. Say I have thousands of levels of a grouping factor, and the number of observations totals in the billions. Despite having lots of observations, I am often either dealing with (a) small effects or (b) trying to fit models with many predictors.


So I would really like to use a random effects model to borrow strength across the levels of the grouping factor, but I am not sure how to practically do this. Are you aware of any approaches to fitting random effects models (including approximations) that work for very large data sets? For example, applying a procedure to each group, and then using the results of this to shrink each fit in some appropriate way.


Just to clarify, here I am only worried about the non-crossed and in fact single-level case. I don’t see any easy route for cross</p><p>4 0.9683708 <a title="1267-lda-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-22-Tenants_and_landlords.html">158 andrew gelman stats-2010-07-22-Tenants and landlords</a></p>
<p>Introduction: Matthew Yglesias  and  Megan McArdle  argue about the economics of landlord/tenant laws in D.C., a topic I know nothing about.  But it did remind me of a few stories . . .
 
1.  In grad school, I shared half of a two-family house with three other students.  At some point, our landlord (who lived in the other half of the house) decided he wanted to sell the place, so he had a real estate agent coming by occasionally to show the house to people.  She was just a flat-out liar (which I guess fits my impression based on screenings of Glengarry Glen Ross).  I could never decide, when I was around and she was lying to a prospective buyer, whether to call her on it.  Sometimes I did, sometimes I didn’t.
 
2.  A year after I graduated, the landlord actually did sell the place but then, when my friends moved out, he refused to pay back their security deposit.  There was some debate about getting the place repainted, I don’t remember the details.  So they sued the landlord in Mass. housing court</p><p>5 0.96215165 <a title="1267-lda-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-18-Tibshirani_announces_new_research_result%3A__A_significance_test_for_the_lasso.html">1769 andrew gelman stats-2013-03-18-Tibshirani announces new research result:  A significance test for the lasso</a></p>
<p>Introduction: Lasso and me 
 
For a long time I was wrong about lasso.
 
Lasso (“least absolute shrinkage and selection operator”) is a regularization procedure that shrinks regression coefficients toward zero, and in its basic form is equivalent to maximum penalized likelihood estimation with a penalty function that is proportional to the sum of the absolute values of the regression coefficients.
 
I first heard about lasso from a talk that  Trevor Hastie  Rob Tibshirani gave at Berkeley in 1994 or 1995.  He demonstrated that it shrunk regression coefficients to zero.  I wasn’t impressed, first because it seemed like no big deal (if that’s the prior you use, that’s the shrinkage you get) and second because, from a Bayesian perspective, I don’t  want  to shrink things all the way to zero.  In the sorts of social and environmental science problems I’ve worked on, just about nothing is zero.  I’d like to control my noisy estimates but there’s nothing special about zero.  At the end of the talk I stood</p><p>6 0.96137863 <a title="1267-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-01-Halloween-Valentine%E2%80%99s_update.html">1357 andrew gelman stats-2012-06-01-Halloween-Valentine’s update</a></p>
<p>7 0.96023697 <a title="1267-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-06-%E2%80%9CSampling%3A__Design_and_Analysis%E2%80%9D%3A__a_course_for_political_science_graduate_students.html">749 andrew gelman stats-2011-06-06-“Sampling:  Design and Analysis”:  a course for political science graduate students</a></p>
<p>8 0.95777428 <a title="1267-lda-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-16-A_historical_perspective_on_financial_bailouts.html">89 andrew gelman stats-2010-06-16-A historical perspective on financial bailouts</a></p>
<p>9 0.95744312 <a title="1267-lda-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-25-Democrats_do_better_among_the_most_and_least_educated_groups.html">678 andrew gelman stats-2011-04-25-Democrats do better among the most and least educated groups</a></p>
<p>10 0.95624197 <a title="1267-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-Varying_treatment_effects%2C_again.html">1310 andrew gelman stats-2012-05-09-Varying treatment effects, again</a></p>
<p>11 0.95347977 <a title="1267-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-15-Regression_discontinuity_designs%3A__looking_for_the_keys_under_the_lamppost%3F.html">518 andrew gelman stats-2011-01-15-Regression discontinuity designs:  looking for the keys under the lamppost?</a></p>
<p>12 0.95008057 <a title="1267-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-11-Jonathan_Chait_and_I_agree_about_the_importance_of_the_fundamentals_in_determining_presidential_elections.html">656 andrew gelman stats-2011-04-11-Jonathan Chait and I agree about the importance of the fundamentals in determining presidential elections</a></p>
<p>13 0.94993848 <a title="1267-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Job_openings_at_conservative_political_analytics_firm%21.html">1909 andrew gelman stats-2013-06-21-Job openings at conservative political analytics firm!</a></p>
<p>14 0.94795096 <a title="1267-lda-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-11-Multilevel_modeling_in_R_on_a_Mac.html">198 andrew gelman stats-2010-08-11-Multilevel modeling in R on a Mac</a></p>
<p>15 0.94714653 <a title="1267-lda-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-14-Extra_babies_on_Valentine%E2%80%99s_Day%2C_fewer_on_Halloween%3F.html">1167 andrew gelman stats-2012-02-14-Extra babies on Valentine’s Day, fewer on Halloween?</a></p>
<p>16 0.94494635 <a title="1267-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-22-Question_12_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1337 andrew gelman stats-2012-05-22-Question 12 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>17 0.94457567 <a title="1267-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Kind_of_Bayesian.html">811 andrew gelman stats-2011-07-20-Kind of Bayesian</a></p>
<p>18 0.94268227 <a title="1267-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-07-Free_advice_from_an_academic_writing_coach%21.html">1658 andrew gelman stats-2013-01-07-Free advice from an academic writing coach!</a></p>
<p>19 0.94259965 <a title="1267-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-27-Setting_up_Jitts_online.html">2041 andrew gelman stats-2013-09-27-Setting up Jitts online</a></p>
<p>20 0.94244552 <a title="1267-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
