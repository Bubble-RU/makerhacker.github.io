<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1506 andrew gelman stats-2012-09-21-Building a regression model . . . with only 27 data points</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1506" href="#">andrew_gelman_stats-2012-1506</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1506 andrew gelman stats-2012-09-21-Building a regression model . . . with only 27 data points</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1506-html" href="http://andrewgelman.com/2012/09/21/building-a-regression-model-with-only-27-data-points/">html</a></p><p>Introduction: Dan Silitonga writes:
  
I was wondering whether you would have any advice on building a regression model on a very small datasets. I’m in the midst of revamping the model to predict tax collections from unincorporated businesses. But I only have 27 data points, 27 years of annual data.  Any advice would be much appreciated.
  
My reply:
 
This sounds tough, especially given that 27 years of annual data isn’t even 27 independent data points.  
 
I have various essentially orthogonal suggestions:
 
1 [added after seeing John Cook's comment below].  Do your best, making as many assumptions as you need.  In a Bayesian context, this means that you’d use a strong and informative prior and let the data update it as appropriate.  In a less formal setting, you’d start with a guess of a model and then alter it to the extent that your data contradict your original guess.
 
2.  Get more data.  Not by getting information on more years (I assume you can’t do that) but by breaking up the data you do</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Dan Silitonga writes:    I was wondering whether you would have any advice on building a regression model on a very small datasets. [sent-1, score-0.434]
</p><p>2 I’m in the midst of revamping the model to predict tax collections from unincorporated businesses. [sent-2, score-0.447]
</p><p>3 But I only have 27 data points, 27 years of annual data. [sent-3, score-0.486]
</p><p>4 My reply:   This sounds tough, especially given that 27 years of annual data isn’t even 27 independent data points. [sent-5, score-0.662]
</p><p>5 I have various essentially orthogonal suggestions:   1 [added after seeing John Cook's comment below]. [sent-6, score-0.152]
</p><p>6 In a Bayesian context, this means that you’d use a strong and informative prior and let the data update it as appropriate. [sent-8, score-0.389]
</p><p>7 In a less formal setting, you’d start with a guess of a model and then alter it to the extent that your data contradict your original guess. [sent-9, score-0.586]
</p><p>8 Not by getting information on more years (I assume you can’t do that) but by breaking up the data you do have, for example by geography, or class of business, or size of business, or some other factor. [sent-12, score-0.569]
</p><p>9 What I’m getting at is, it seems that you must have a lot more than 27 pieces of information you could analyze. [sent-14, score-0.276]
</p><p>10 With a small n and many predictors, you often can’t come to a good story about what is happening but you can still rule out a lot of potential stories. [sent-16, score-0.18]
</p><p>11 For example, suppose you have 20 candidate predictors. [sent-17, score-0.082]
</p><p>12 But you can correlate each of the predictors with the outcome, one at a time, and discover either a very close predictive relation with one or more of the separate predictors, or no such relation. [sent-19, score-0.74]
</p><p>13 It ain’t nothing to know that none of these 20 inputs determines the output all by itself. [sent-21, score-0.344]
</p><p>14 For example, if you have 5 similar predictors, each measuring some aspect of a common input, you can average them (after rescaling, if necessary) and then use that average as a single predictor. [sent-24, score-0.528]
</p><p>15 Bill James did that sort of thing in his baseball analyses. [sent-25, score-0.098]
</p><p>16 Instead of throwing all his variables into a regression, he’d use theory (of a sort) and some data analysis to compute composite scores such as “runs created” and then use these composites in his further analyses. [sent-26, score-0.829]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('predictors', 0.305), ('business', 0.205), ('annual', 0.205), ('data', 0.176), ('orthogonal', 0.152), ('advice', 0.138), ('correlate', 0.137), ('collections', 0.137), ('rescaling', 0.133), ('determines', 0.13), ('composite', 0.13), ('midst', 0.13), ('use', 0.125), ('geography', 0.123), ('alter', 0.116), ('ain', 0.114), ('inputs', 0.113), ('average', 0.11), ('either', 0.107), ('contradict', 0.107), ('breaking', 0.106), ('combine', 0.106), ('years', 0.105), ('regression', 0.104), ('cook', 0.103), ('discover', 0.101), ('output', 0.101), ('input', 0.099), ('baseball', 0.098), ('measuring', 0.098), ('getting', 0.097), ('small', 0.096), ('throwing', 0.096), ('model', 0.096), ('pieces', 0.094), ('runs', 0.094), ('tough', 0.092), ('formal', 0.091), ('relation', 0.09), ('compute', 0.09), ('update', 0.088), ('scores', 0.087), ('created', 0.087), ('dan', 0.086), ('aspect', 0.085), ('information', 0.085), ('tax', 0.084), ('happening', 0.084), ('throw', 0.084), ('candidate', 0.082)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1506-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-21-Building_a_regression_model_._._._with_only_27_data_points.html">1506 andrew gelman stats-2012-09-21-Building a regression model . . . with only 27 data points</a></p>
<p>Introduction: Dan Silitonga writes:
  
I was wondering whether you would have any advice on building a regression model on a very small datasets. I’m in the midst of revamping the model to predict tax collections from unincorporated businesses. But I only have 27 data points, 27 years of annual data.  Any advice would be much appreciated.
  
My reply:
 
This sounds tough, especially given that 27 years of annual data isn’t even 27 independent data points.  
 
I have various essentially orthogonal suggestions:
 
1 [added after seeing John Cook's comment below].  Do your best, making as many assumptions as you need.  In a Bayesian context, this means that you’d use a strong and informative prior and let the data update it as appropriate.  In a less formal setting, you’d start with a guess of a model and then alter it to the extent that your data contradict your original guess.
 
2.  Get more data.  Not by getting information on more years (I assume you can’t do that) but by breaking up the data you do</p><p>2 0.18051054 <a title="1506-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>Introduction: Andreas Graefe writes (see  here   here   here ):
  
The usual procedure for developing linear models to predict any kind of target variable is to identify a subset of most important predictors and to estimate weights that provide the best possible solution for a given sample. The resulting “optimally” weighted linear composite is then used when predicting new data. This approach is useful in situations with large and reliable datasets and few predictor variables. However, a large body of analytical and empirical evidence since the 1970s shows that the weighting of variables is of little, if any, value in situations with small and noisy datasets and a large number of predictor variables. In such situations, including all relevant variables is more important than their weighting. These findings have yet to impact many fields. This study uses data from nine established U.S. election-forecasting models whose forecasts are regularly published in academic journals to demonstrate the value o</p><p>3 0.16991626 <a title="1506-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>Introduction: Robert Bell pointed me to  this post  by Brad De Long on Bayesian statistics, and then I also noticed  this  from Noah Smith, who wrote:
  
My impression is that although the Bayesian/Frequentist debate is interesting and intellectually fun, there’s really not much “there” there… despite being so-hip-right-now, Bayesian is not the Statistical Jesus.
  
I’m happy to see the discussion going in this direction.  Twenty-five years ago or so, when I got into this biz, there were some serious anti-Bayesian attitudes floating around in mainstream statistics.  Discussions in the journals sometimes devolved into debates of the form, “Bayesians:  knaves or fools?”.  You’d get all sorts of free-floating skepticism about any prior distribution at all, even while people were accepting without question (and doing theory on) logistic regressions, proportional hazards models, and all sorts of strong strong models.  (In the subfield of survey sampling, various prominent researchers would refuse to mode</p><p>4 0.15618484 <a title="1506-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-03-Uncertainty_in_parameter_estimates_using_multilevel_models.html">1966 andrew gelman stats-2013-08-03-Uncertainty in parameter estimates using multilevel models</a></p>
<p>Introduction: David Hsu writes: 
   
 I have a (perhaps) simple question about uncertainty in parameter estimates using multilevel models — what is an appropriate threshold for measure parameter uncertainty in a multilevel model? 
 
The reason why I ask is that I set out to do a crossed two-way model with two varying intercepts, similar to your flight simulator example in your 2007 book.  The difference is that I have a lot of predictors specific to each cell (I think equivalent to airport and pilot in your example), and I find after modeling this in JAGS, I happily find that the predictors are much less important than the variability by cell (airport and pilot effects).  Happily because this is what I am writing a paper about.
 
However, I then went to check subsets of predictors using lm() and lmer().  I understand that they all use different estimation methods, but what I can’t figure out is why the errors on all of the coefficient estimates are *so* different.  
 
For example, using JAGS, and th</p><p>5 0.15397744 <a title="1506-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-06-17_groups%2C_6_group-level_predictors%3A__What_to_do%3F.html">1248 andrew gelman stats-2012-04-06-17 groups, 6 group-level predictors:  What to do?</a></p>
<p>Introduction: Yi-Chun Ou writes: 
  
  
I am using a multilevel model with three levels. I read that you wrote a book about multilevel models, and wonder if you can solve the following question.  


The data structure is like this: 


Level one: customer (8444 customers) 
Level two: companys (90 companies) 
Level three: industry (17 industries) 


I use 6 level-three variables (i.e. industry characteristics) to explain the variance of the level-one effect across industries. The question here is whether there is an over-fitting problem since there are only 17 industries. I understand that this must be a problem for non-multilevel models, but is it also a problem for multilevel models?
  
My reply:  Yes, this could be a problem.  I’d suggest combining some of your variables into a common score, or using only some of the variables, or using strong priors to control the inferences.  This is an interesting and important area of statistics research, to do this sort of thing systematically.  There’s lots o</p><p>6 0.14942257 <a title="1506-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-17-Graphical_tools_for_understanding_multilevel_models.html">772 andrew gelman stats-2011-06-17-Graphical tools for understanding multilevel models</a></p>
<p>7 0.14864343 <a title="1506-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-11-%E2%80%9CInformative_g-Priors_for_Logistic_Regression%E2%80%9D.html">2017 andrew gelman stats-2013-09-11-“Informative g-Priors for Logistic Regression”</a></p>
<p>8 0.14029393 <a title="1506-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-24-Estimating_and_summarizing_inference_for_hierarchical_variance_parameters_when_the_number_of_groups_is_small.html">2145 andrew gelman stats-2013-12-24-Estimating and summarizing inference for hierarchical variance parameters when the number of groups is small</a></p>
<p>9 0.13986981 <a title="1506-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-04-What_are_the_key_assumptions_of_linear_regression%3F.html">1967 andrew gelman stats-2013-08-04-What are the key assumptions of linear regression?</a></p>
<p>10 0.13863632 <a title="1506-tfidf-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-19-Analysis_of_survey_data%3A_Design_based_models_vs._hierarchical_modeling%3F.html">352 andrew gelman stats-2010-10-19-Analysis of survey data: Design based models vs. hierarchical modeling?</a></p>
<p>11 0.13664947 <a title="1506-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-07-Prior_distributions_for_regression_coefficients.html">1486 andrew gelman stats-2012-09-07-Prior distributions for regression coefficients</a></p>
<p>12 0.13606669 <a title="1506-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-17-Modeling_group-level_predictors_in_a_multilevel_regression.html">1216 andrew gelman stats-2012-03-17-Modeling group-level predictors in a multilevel regression</a></p>
<p>13 0.13349688 <a title="1506-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>14 0.13209255 <a title="1506-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-27-Why_can%E2%80%99t_I_be_more_like_Bill_James%2C_or%2C_The_use_of_default_and_default-like_models.html">541 andrew gelman stats-2011-01-27-Why can’t I be more like Bill James, or, The use of default and default-like models</a></p>
<p>15 0.13152811 <a title="1506-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>16 0.12792517 <a title="1506-tfidf-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Allowing_interaction_terms_to_vary.html">753 andrew gelman stats-2011-06-09-Allowing interaction terms to vary</a></p>
<p>17 0.12730721 <a title="1506-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-01-Philosophy_of_Bayesian_statistics%3A__my_reactions_to_Cox_and_Mayo.html">1149 andrew gelman stats-2012-02-01-Philosophy of Bayesian statistics:  my reactions to Cox and Mayo</a></p>
<p>18 0.12594542 <a title="1506-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-05-A_statistician_rereads_Bill_James.html">697 andrew gelman stats-2011-05-05-A statistician rereads Bill James</a></p>
<p>19 0.12542516 <a title="1506-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>20 0.12342604 <a title="1506-tfidf-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-Transformations_for_non-normal_data.html">2176 andrew gelman stats-2014-01-19-Transformations for non-normal data</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.255), (1, 0.106), (2, 0.047), (3, 0.034), (4, 0.062), (5, -0.002), (6, 0.036), (7, 0.009), (8, 0.047), (9, 0.102), (10, 0.019), (11, 0.035), (12, 0.015), (13, -0.033), (14, -0.004), (15, 0.037), (16, 0.044), (17, -0.004), (18, 0.024), (19, -0.015), (20, -0.014), (21, 0.061), (22, -0.01), (23, 0.012), (24, -0.038), (25, 0.02), (26, 0.042), (27, -0.066), (28, -0.013), (29, -0.041), (30, 0.052), (31, -0.001), (32, 0.025), (33, 0.026), (34, -0.001), (35, 0.082), (36, 0.001), (37, 0.029), (38, -0.025), (39, -0.027), (40, 0.062), (41, 0.019), (42, -0.015), (43, -0.025), (44, -0.009), (45, 0.055), (46, 0.008), (47, -0.049), (48, 0.012), (49, 0.004)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9658848 <a title="1506-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-21-Building_a_regression_model_._._._with_only_27_data_points.html">1506 andrew gelman stats-2012-09-21-Building a regression model . . . with only 27 data points</a></p>
<p>Introduction: Dan Silitonga writes:
  
I was wondering whether you would have any advice on building a regression model on a very small datasets. I’m in the midst of revamping the model to predict tax collections from unincorporated businesses. But I only have 27 data points, 27 years of annual data.  Any advice would be much appreciated.
  
My reply:
 
This sounds tough, especially given that 27 years of annual data isn’t even 27 independent data points.  
 
I have various essentially orthogonal suggestions:
 
1 [added after seeing John Cook's comment below].  Do your best, making as many assumptions as you need.  In a Bayesian context, this means that you’d use a strong and informative prior and let the data update it as appropriate.  In a less formal setting, you’d start with a guess of a model and then alter it to the extent that your data contradict your original guess.
 
2.  Get more data.  Not by getting information on more years (I assume you can’t do that) but by breaking up the data you do</p><p>2 0.8133961 <a title="1506-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-03-Comparing_prediction_errors.html">938 andrew gelman stats-2011-10-03-Comparing prediction errors</a></p>
<p>Introduction: Someone named James writes: 
  
  
I’m working on a classification task, sentence segmentation.  The classifier algorithm we use (BoosTexter, a boosted learning algorithm) classifies each word independently conditional on its features, i.e. a bag-of-words model, so any contextual clues need to be encoded into the features.  The feature extraction system I am proposing in my thesis uses a heteroscedastic LDA to transform data to produce the features the classifier runs on. The HLDA system has a couple parameters I’m testing, and I’m running a 3×2 full factorial experiment. That’s the background which may or may not be relevant to the question.


The output of each trial is a class (there are only 2 classes, right now) for every word in the dataset.  Because of the nature of the task, one class strongly predominates, say 90-95% of the data.  My question is this: in terms of overall performance (we use F1 score), many of these trials are pretty close together, which leads me to ask whethe</p><p>3 0.81173718 <a title="1506-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-29-Difficulties_with_the_1-4-power_transformation.html">1142 andrew gelman stats-2012-01-29-Difficulties with the 1-4-power transformation</a></p>
<p>Introduction: John Hayes writes:
  
I am a fan of the quarter root transform ever since reading about it on your  blog .  However, today my student and I hit a wall that I’m hoping you might have some insight on.


By training, I am a psychophysicist (think SS Stevens), and people in my field often log transform data prior to analysis. However, this data frequently contains zeros, so I’ve tried using quarter root transforms to get around this. But until today, I had never tried to back transform the plot axis for readability. I assumed this would be straightforward – alas it is not.


Specifically, we quarter root transformed our data, performed an ANOVA, got what we thought was a reasonable effect, and then plotted the data. So far so good. However, the LS means in question are below 1, meaning that raising them to the 4th power just makes them smaller, and uninterpretable in the original metric.


Do you have any thoughts or insights you might share?
  
My reply:
 
I don’t see the problem with pre</p><p>4 0.80602342 <a title="1506-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-15-Still_more_Mr._P_in_public_health.html">770 andrew gelman stats-2011-06-15-Still more Mr. P in public health</a></p>
<p>Introduction: When it  rains  it pours . . .
 
John Transue writes:
  
I saw  a post  on Andrew Sullivan’s blog today about life expectancy in different US counties. With a bunch of the worst counties being in Mississippi, I thought that it might be another case of analysts getting extreme values from small counties.


However, the paper (see  here ) includes a pretty interesting methods section. This is from page 5, “Specifically, we used a mixed-effects Poisson regression with time, geospatial, and covariate components. Poisson regression fits count outcome variables, e.g., death counts, and is preferable to a logistic model because the latter is biased when an outcome is rare (occurring in less than 1% of observations).”


They have downloadable data. I believe that the data are predicted values from the model. A web appendix also gives 90% CIs for their estimates.


Do you think they solved the small county problem and that the worst counties really are where their spreadsheet suggests?
  
My re</p><p>5 0.79033208 <a title="1506-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-31-Using_factor_analysis_or_principal_components_analysis_or_measurement-error_models_for_biological_measurements_in_archaeology%3F.html">1094 andrew gelman stats-2011-12-31-Using factor analysis or principal components analysis or measurement-error models for biological measurements in archaeology?</a></p>
<p>Introduction: Greg Campbell writes:
  
I am a Canadian archaeologist (BSc in Chemistry) researching the past human use of European Atlantic shellfish. After two decades of practice I am finally getting a MA in archaeology at Reading. I am seeing if the habitat or size of harvested mussels (Mytilus edulis) can be reconstructed from measurements of the umbo (the pointy end, and the only bit that survives well in archaeological deposits) using log-transformed measurements (or allometry; relationships between dimensions are more likely exponential than linear). 
Of course multivariate regressions in most statistics packages (Minitab, SPSS, SAS) assume you are trying to predict one variable from all the others (a Model I regression), and use ordinary least squares to fit the regression line. For organismal dimensions this makes little sense, since all the dimensions are (at least in theory) free to change their mutual proportions during growth. So there is no predictor and predicted, mutual variation of</p><p>6 0.78754246 <a title="1506-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-07-Analysis_of_Power_Law_of_Participation.html">946 andrew gelman stats-2011-10-07-Analysis of Power Law of Participation</a></p>
<p>7 0.78415143 <a title="1506-lsi-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-08-Regression_and_causality_and_variable_ordering.html">2364 andrew gelman stats-2014-06-08-Regression and causality and variable ordering</a></p>
<p>8 0.78131443 <a title="1506-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-15-R-squared_for_multilevel_models.html">1121 andrew gelman stats-2012-01-15-R-squared for multilevel models</a></p>
<p>9 0.78069067 <a title="1506-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-19-The_scope_for_snooping.html">1070 andrew gelman stats-2011-12-19-The scope for snooping</a></p>
<p>10 0.78030527 <a title="1506-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-10-Combining_data_from_many_sources.html">948 andrew gelman stats-2011-10-10-Combining data from many sources</a></p>
<p>11 0.77136672 <a title="1506-lsi-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-Transformations_for_non-normal_data.html">2176 andrew gelman stats-2014-01-19-Transformations for non-normal data</a></p>
<p>12 0.7709862 <a title="1506-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-29-Going_negative.html">1918 andrew gelman stats-2013-06-29-Going negative</a></p>
<p>13 0.75962245 <a title="1506-lsi-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-21-Models_with_constraints.html">2342 andrew gelman stats-2014-05-21-Models with constraints</a></p>
<p>14 0.757128 <a title="1506-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-24-Analyzing_photon_counts.html">1509 andrew gelman stats-2012-09-24-Analyzing photon counts</a></p>
<p>15 0.75674838 <a title="1506-lsi-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-12-Get_the_Data.html">569 andrew gelman stats-2011-02-12-Get the Data</a></p>
<p>16 0.75606686 <a title="1506-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-05-Understanding_regression_models_and_regression_coefficients.html">1656 andrew gelman stats-2013-01-05-Understanding regression models and regression coefficients</a></p>
<p>17 0.75455338 <a title="1506-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-07-Prior_distributions_for_regression_coefficients.html">1486 andrew gelman stats-2012-09-07-Prior distributions for regression coefficients</a></p>
<p>18 0.75259435 <a title="1506-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-27-Why_can%E2%80%99t_I_be_more_like_Bill_James%2C_or%2C_The_use_of_default_and_default-like_models.html">541 andrew gelman stats-2011-01-27-Why can’t I be more like Bill James, or, The use of default and default-like models</a></p>
<p>19 0.75223756 <a title="1506-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>20 0.74939078 <a title="1506-lsi-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-07-There_are_never_70_distinct_parameters.html">327 andrew gelman stats-2010-10-07-There are never 70 distinct parameters</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.019), (16, 0.073), (21, 0.039), (24, 0.172), (55, 0.021), (63, 0.135), (68, 0.012), (86, 0.042), (89, 0.021), (99, 0.36)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98866111 <a title="1506-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-05-Two_exciting_movie_ideas%3A__%E2%80%9CSecond_Chance_U%E2%80%9D_and_%E2%80%9CThe_New_Dirty_Dozen%E2%80%9D.html">1484 andrew gelman stats-2012-09-05-Two exciting movie ideas:  “Second Chance U” and “The New Dirty Dozen”</a></p>
<p>Introduction: I have a great idea for a movie.  Actually two movies based on two variants of a similar idea.
 
It all started when I saw this story:
  
Dr. Anil Potti, the controversial cancer researcher whose work at Duke University led to lawsuits from patients, is now a medical oncologist at the Cancer Center of North Dakota in Grand Forks.
  
When asked about Dr. Potti’s controversial appointment, his new boss  said :
  
If a guy can’t get a second chance here in North Dakota, where he trained, man, you can’t get a second chance anywhere.
  
(Link from  Retraction Watch , of course.)
 
Potti’s boss is also quoted as saying, “Most, if not all, his patients have loved him.”  On the other hand, the news article reports: “The North Carolina medical board’s website lists settlements against Potti of at least $75,000.”  I guess there’s no reason you can’t love a guy and still want a juicy malpractice settlement.
 
 Second Chance U 
 
I don’t give two poops about Dr. Anil Potti.  But seeing the above s</p><p>2 0.98830676 <a title="1506-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-02-%E2%80%9CIf_our_product_is_harmful_._._._we%E2%80%99ll_stop_making_it.%E2%80%9D.html">1480 andrew gelman stats-2012-09-02-“If our product is harmful . . . we’ll stop making it.”</a></p>
<p>Introduction: After our  discussion  of the sad case of Darrell Huff, the celebrated “How to Lie with Statistics” guy who had a lucrative side career disparaging the link between smoking and cancer, I was motivated to follow John Mashey’s  recommendation  and read the book, Golden Holocaust: Origins of the Cigarette Catastrophe and the Case for Abolition, by historian Robert Proctor.
 
My first stop upon receiving the book was the index, in particular the entry for  Rubin, Donald B.   I followed the reference to pages 440-442 and found the description of Don’s activities to be accurate, neither diminished nor overstated, to the best of my knowledge.
 
Rubin is the second-most-famous statistician to have been paid by the cigarette industry, but several other big and small names have been on the payroll at one time or another.  Here’s a partial  list .  Just including the people I know or have heard of:
 
Herbert Solomon, Stanford 
Richard Tweedie, Bond U 
Arnold Zellner, U of Chicago 
Paul Switzer, S</p><p>3 0.98512316 <a title="1506-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-13-Puzzles_of_criminal_justice.html">1621 andrew gelman stats-2012-12-13-Puzzles of criminal justice</a></p>
<p>Introduction: Four recent news stories about crime and punishment made me realize, yet again, how little I understand all this.
 
1.   “HSBC to Pay $1.92 Billion to Settle Charges of Money Laundering” :
  
State and federal authorities decided against indicting HSBC in a money-laundering case over concerns that criminal charges could jeopardize one of the world’s largest banks and ultimately destabilize the global financial system.  Instead, HSBC announced on Tuesday that it had agreed to a record $1.92 billion settlement with authorities. . . .
  
I don’t understand this idea of punishing the institution.  I have the same problem when the NCAA punishes a college football program.  These are individual people breaking the law (or the rules), right?  So why not punish them directly?  Giving 40 lashes to a bunch of HSBC executives and garnisheeing their salaries for life, say, that wouldn’t destabilize the global financial system would it?  From the article:  “A money-laundering indictment, or a guilt</p><p>4 0.98463029 <a title="1506-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-29-Putting_together_multinomial_discrete_regressions_by_combining_simple_logits.html">782 andrew gelman stats-2011-06-29-Putting together multinomial discrete regressions by combining simple logits</a></p>
<p>Introduction: When predicting 0/1 data we can use logit (or probit or robit or some other robust model such as invlogit (0.01 + 0.98*X*beta)).  Logit is simple enough and we can use  bayesglm  to regularize and avoid the problem of separation.
 
What if there are more than 2 categories?  If they’re ordered (1, 2, 3, etc), we can do ordered logit (and use bayespolr() to avoid separation).  If the categories are unordered (vanilla, chocolate, strawberry), there are unordered multinomial logit and probit models out there.
 
But it’s not so easy to fit these multinomial model in a multilevel setting (with coefficients that vary by group), especially if the computation is embedded in an iterative routine such as mi where you have real time constraints at each step.
 
So this got me wondering whether we could kluge it with logits.  Here’s the basic idea (in the ordered and unordered forms):
 
- If you have a variable that goes 1, 2, 3, etc., set up a series of logits:  1 vs. 2,3,…; 2 vs. 3,…; and so forth</p><p>5 0.97999859 <a title="1506-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-21-Why_modern_art_is_all_in_the_mind.html">102 andrew gelman stats-2010-06-21-Why modern art is all in the mind</a></p>
<p>Introduction: This  looks cool:
  
Ten years ago researchers in America took two groups of three-year-olds and showed them a blob of paint on a canvas. Children who were told that the marks were the result of an accidental spillage showed little interest. The others, who had been told that the splodge of colour had been carefully created for them, started to refer to it as “a painting”.


Now that experiment . . . has gone on to form part of the foundation of an influential new book that questions the way in which we respond to art. . . . The book, which is subtitled The New Science of Why We Like What We Like, is not an attack on modern or contemporary art and Bloom says fans of more traditional art are not capable of making purely aesthetic judgments either. “I don’t have a strong position about the art itself,” he said this weekend. “But I do have a strong position about why we actually like it.”
  
This sounds fascinating.  But I’m skeptical about this part:
  
Humans are incapable of just getti</p><p>same-blog 6 0.97506469 <a title="1506-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-21-Building_a_regression_model_._._._with_only_27_data_points.html">1506 andrew gelman stats-2012-09-21-Building a regression model . . . with only 27 data points</a></p>
<p>7 0.97425646 <a title="1506-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-23-Lowess_is_great.html">293 andrew gelman stats-2010-09-23-Lowess is great</a></p>
<p>8 0.97402459 <a title="1506-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-16-Objects_of_the_class_%E2%80%9CObjects_of_the_class%E2%80%9D.html">2103 andrew gelman stats-2013-11-16-Objects of the class “Objects of the class”</a></p>
<p>9 0.97369647 <a title="1506-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-20-Are_the_Democrats_avoiding_a_national_campaign%3F.html">286 andrew gelman stats-2010-09-20-Are the Democrats avoiding a national campaign?</a></p>
<p>10 0.97159791 <a title="1506-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-23-When_are_complicated_models_helpful_in_psychology_research_and_when_are_they_overkill%3F.html">1690 andrew gelman stats-2013-01-23-When are complicated models helpful in psychology research and when are they overkill?</a></p>
<p>11 0.97038537 <a title="1506-lda-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-03-A_question_for_psychometricians.html">313 andrew gelman stats-2010-10-03-A question for psychometricians</a></p>
<p>12 0.97000074 <a title="1506-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-22-Philosophy_of_Bayes_and_non-Bayes%3A__A_dialogue_with_Deborah_Mayo.html">291 andrew gelman stats-2010-09-22-Philosophy of Bayes and non-Bayes:  A dialogue with Deborah Mayo</a></p>
<p>13 0.96983308 <a title="1506-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-29-Splitting_the_data.html">544 andrew gelman stats-2011-01-29-Splitting the data</a></p>
<p>14 0.96842545 <a title="1506-lda-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-19-Just_chaid.html">421 andrew gelman stats-2010-11-19-Just chaid</a></p>
<p>15 0.96463203 <a title="1506-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-22-Tables_as_graphs%3A_The_Ramanujan_principle.html">1078 andrew gelman stats-2011-12-22-Tables as graphs: The Ramanujan principle</a></p>
<p>16 0.96077055 <a title="1506-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-25-Spam%21.html">2148 andrew gelman stats-2013-12-25-Spam!</a></p>
<p>17 0.96005565 <a title="1506-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-24-Flawed_visualization_of_U.S._voting_maybe_has_some_good_features.html">428 andrew gelman stats-2010-11-24-Flawed visualization of U.S. voting maybe has some good features</a></p>
<p>18 0.95858419 <a title="1506-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-22-A_Bayesian_model_for_an_increasing_function%2C_in_Stan%21.html">2110 andrew gelman stats-2013-11-22-A Bayesian model for an increasing function, in Stan!</a></p>
<p>19 0.95822179 <a title="1506-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-02-Does_a_professor%E2%80%99s_intervention_in_online_discussions_have_the_effect_of_prolonging_discussion_or_cutting_it_off%3F.html">2120 andrew gelman stats-2013-12-02-Does a professor’s intervention in online discussions have the effect of prolonging discussion or cutting it off?</a></p>
<p>20 0.95815569 <a title="1506-lda-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-07-Inference_%3D_data_%2B_model.html">1201 andrew gelman stats-2012-03-07-Inference = data + model</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
