<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1339 andrew gelman stats-2012-05-23-Learning Differential Geometry for Hamiltonian Monte Carlo</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1339" href="#">andrew_gelman_stats-2012-1339</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1339 andrew gelman stats-2012-05-23-Learning Differential Geometry for Hamiltonian Monte Carlo</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1339-html" href="http://andrewgelman.com/2012/05/23/learning-differential-geometry-for-hamiltonian-monte-carlo-hmc/">html</a></p><p>Introduction: You can get a taste of Hamiltonian Monte Carlo (HMC) by reading the very gentle introduction in David MacKay’s general text on information theory:
  
  MacKay, D.  2003.    Information Theory, Inference, and Learning Algorithms  .  Cambridge University Press.  [see Chapter 31, which is relatively standalone and can be downloaded separately.]
   
Follow this up with Radford Neal’s much more thorough introduction to HMC:
  
 Neal, R. 2011.   MCMC Using Hamiltonian Dynamics .  In Brooks, Gelman, Jones and Meng, eds.,  Handbook of Markov Chain Monte Carlo .  Chapman and Hall/CRC Press.
   
To understand why HMC works and set yourself on the path to understanding generalizations like  Riemann manifold HMC , you’ll need to know a bit about differential geometry.  I really liked the combination of these two books:
  
  Magnus, J. R. and H. Neudecker.  2007.   Matrix Differential Calculus with Application in Statistics and Econometrics .  3rd Edition.  Wiley?
   
and
  
  Leimkuhler, B. and S.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 You can get a taste of Hamiltonian Monte Carlo (HMC) by reading the very gentle introduction in David MacKay’s general text on information theory:      MacKay, D. [sent-1, score-0.497]
</p><p>2 [see Chapter 31, which is relatively standalone and can be downloaded separately. [sent-5, score-0.26]
</p><p>3 ]     Follow this up with Radford Neal’s much more thorough introduction to HMC:     Neal, R. [sent-6, score-0.297]
</p><p>4 To understand why HMC works and set yourself on the path to understanding generalizations like  Riemann manifold HMC , you’ll need to know a bit about differential geometry. [sent-12, score-0.488]
</p><p>5 I really liked the combination of these two books:      Magnus, J. [sent-13, score-0.061]
</p><p>6 As a bonus, Magnus and Neudecker also provide an excellent introduction to matrix algebra and real analysis before mashing them up. [sent-27, score-0.49]
</p><p>7 The question mark after “Wiley” is due to the fact that the preface says that the third-edition is self-published and copyright the authors and and available from the  first author’s home page . [sent-28, score-0.507]
</p><p>8 It’s no longer available on Magnus’s home page, nor is it available for sale by Wiley. [sent-29, score-0.471]
</p><p>9 It can be found in PDF form on the web, though; try Googling [ matrix differential calculus magnus ]. [sent-30, score-1.027]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('magnus', 0.441), ('hmc', 0.311), ('differential', 0.228), ('introduction', 0.217), ('hamiltonian', 0.217), ('matrix', 0.196), ('wiley', 0.186), ('mackay', 0.186), ('cambridge', 0.17), ('neal', 0.167), ('calculus', 0.162), ('dynamics', 0.16), ('carlo', 0.137), ('available', 0.133), ('monte', 0.13), ('manifold', 0.11), ('standalone', 0.11), ('home', 0.106), ('preface', 0.099), ('sale', 0.099), ('gentle', 0.096), ('chapman', 0.093), ('downloaded', 0.091), ('copyright', 0.087), ('handbook', 0.087), ('radford', 0.087), ('jones', 0.087), ('generalizations', 0.085), ('simulating', 0.084), ('page', 0.082), ('thorough', 0.08), ('bonus', 0.079), ('googling', 0.079), ('university', 0.078), ('meng', 0.078), ('algebra', 0.077), ('markov', 0.076), ('theory', 0.075), ('pdf', 0.074), ('brooks', 0.069), ('taste', 0.069), ('econometrics', 0.068), ('mcmc', 0.066), ('algorithms', 0.066), ('chain', 0.065), ('path', 0.065), ('liked', 0.061), ('relatively', 0.059), ('information', 0.058), ('text', 0.057)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="1339-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-23-Learning_Differential_Geometry_for_Hamiltonian_Monte_Carlo.html">1339 andrew gelman stats-2012-05-23-Learning Differential Geometry for Hamiltonian Monte Carlo</a></p>
<p>Introduction: You can get a taste of Hamiltonian Monte Carlo (HMC) by reading the very gentle introduction in David MacKay’s general text on information theory:
  
  MacKay, D.  2003.    Information Theory, Inference, and Learning Algorithms  .  Cambridge University Press.  [see Chapter 31, which is relatively standalone and can be downloaded separately.]
   
Follow this up with Radford Neal’s much more thorough introduction to HMC:
  
 Neal, R. 2011.   MCMC Using Hamiltonian Dynamics .  In Brooks, Gelman, Jones and Meng, eds.,  Handbook of Markov Chain Monte Carlo .  Chapman and Hall/CRC Press.
   
To understand why HMC works and set yourself on the path to understanding generalizations like  Riemann manifold HMC , you’ll need to know a bit about differential geometry.  I really liked the combination of these two books:
  
  Magnus, J. R. and H. Neudecker.  2007.   Matrix Differential Calculus with Application in Statistics and Econometrics .  3rd Edition.  Wiley?
   
and
  
  Leimkuhler, B. and S.</p><p>2 0.32958895 <a title="1339-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<p>Introduction: Tomas Iesmantas had  asked me  for advice on a regression problem with 50 parameters, and I’d recommended Hamiltonian Monte Carlo.
 
A few weeks later he reported back: 
  
  
After trying several modifications (HMC for all parameters at once, HMC just for first level parameters and Riemman manifold Hamiltonian Monte Carlo method), I finally got it running with HMC just for first level parameters and for others using direct sampling, since conditional distributions turned out to have closed form.


However, even in this case it is quite tricky, since I had to employ mass matrix and not just diagonal but at the beginning of algorithm generated it randomly (ensuring it is positive definite). Such random generation of mass matrix is quite blind step, but it proved to be quite helpful.


Riemman manifold HMC is quite vagarious, or to be more specific, metric of manifold is very sensitive. In my model log-likelihood I had exponents and values of metrics matrix elements was very large and wh</p><p>3 0.21947649 <a title="1339-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-20-Stan_at_Google_this_Thurs_and_at_Berkeley_this_Fri_noon.html">1772 andrew gelman stats-2013-03-20-Stan at Google this Thurs and at Berkeley this Fri noon</a></p>
<p>Introduction: Michael Betancourt  will be speaking at Google and at the University of California, Berkeley.  The Google talk is closed to outsiders (but if you work at Google, you should go!); the Berkeley talk is open to all:
  
Friday March 22, 12:10 pm, Evans Hall 1011.


Title of talk:  Stan : Practical Bayesian Inference with Hamiltonian Monte Carlo


Abstract: Practical implementations of Bayesian inference are often limited to approximation methods that only slowly explore the posterior distribution.  By taking advantage of the curvature of the posterior, however, Hamiltonian Monte Carlo (HMC) efficiently explores even the most highly contorted distributions.  In this talk I will review the foundations of and recent developments within HMC, concluding with a discussion of Stan, a powerful inference engine that utilizes HMC, automatic differentiation, and adaptive methods to minimize user input.
  
This is cool stuff.  And heâ&euro;&trade;ll be showing the whirlpool movie!</p><p>4 0.19028232 <a title="1339-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-04-Stan_in_L.A._this_Wed_3%3A30pm.html">1749 andrew gelman stats-2013-03-04-Stan in L.A. this Wed 3:30pm</a></p>
<p>Introduction: Michael Betancourt  will be speaking at UCLA:
  
The location for refreshment is in room 51-254 CHS at 3:00 PM.


The place for the seminar is at CHS 33-105A  at 3:30pm – 4:30pm, Wed 6 Mar.


["CHS" stands for Center for Health Sciences, the building of the UCLA schools of medicine and public health.   Here's a map with directions .]


Title of talk:  Stan : Practical Bayesian Inference with Hamiltonian Monte Carlo


Abstract: Practical implementations of Bayesian inference are often limited to approximation methods that only slowly explore the posterior distribution.  By taking advantage of the curvature of the posterior, however, Hamiltonian Monte Carlo (HMC) efficiently explores even the most highly contorted distributions.  In this talk I will review the foundations of and recent developments within HMC, concluding with a discussion of Stan, a powerful inference engine that utilizes HMC, automatic differentiation, and adaptive methods to minimize user input.
  
This is cool stuff.</p><p>5 0.17121848 <a title="1339-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-07-Update_on_the_new_Handbook_of_MCMC.html">844 andrew gelman stats-2011-08-07-Update on the new Handbook of MCMC</a></p>
<p>Introduction: It’s edited by Steve Brooks, Galin Jones, Xiao-Li Meng, and myself.   Here’s  the information and some sample chapters (including my own chapter with Ken Shirley on inference and monitoring convergence and Radford’s instant classic on Hamiltonian Monte Carlo).
 
Sorry about the $100 price tag–nobody asked me about that!  But if you’re doing these computations as part of your work, I think the book will be well worth it.</p><p>6 0.16005999 <a title="1339-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-01-David_MacKay_sez_._._._12%3F%3F.html">984 andrew gelman stats-2011-11-01-David MacKay sez . . . 12??</a></p>
<p>7 0.14713591 <a title="1339-tfidf-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-Running_into_a_Stan_Reference_by_Accident.html">2231 andrew gelman stats-2014-03-03-Running into a Stan Reference by Accident</a></p>
<p>8 0.13629042 <a title="1339-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-30-Stan_uses_Nuts%21.html">1036 andrew gelman stats-2011-11-30-Stan uses Nuts!</a></p>
<p>9 0.13606007 <a title="1339-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-30-Stan_Project%3A__Continuous_Relaxations_for_Discrete_MRFs.html">2003 andrew gelman stats-2013-08-30-Stan Project:  Continuous Relaxations for Discrete MRFs</a></p>
<p>10 0.11523924 <a title="1339-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-21-Handbook_of_Markov_Chain_Monte_Carlo.html">674 andrew gelman stats-2011-04-21-Handbook of Markov Chain Monte Carlo</a></p>
<p>11 0.11512344 <a title="1339-tfidf-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-26-Econometrics%2C_political_science%2C_epidemiology%2C_etc.%3A__Don%E2%80%99t_model_the_probability_of_a_discrete_outcome%2C_model_the_underlying_continuous_variable.html">2226 andrew gelman stats-2014-02-26-Econometrics, political science, epidemiology, etc.:  Don’t model the probability of a discrete outcome, model the underlying continuous variable</a></p>
<p>12 0.11166175 <a title="1339-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-04-David_MacKay_and_Occam%E2%80%99s_Razor.html">1041 andrew gelman stats-2011-12-04-David MacKay and Occam’s Razor</a></p>
<p>13 0.11034904 <a title="1339-tfidf-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Bayesian_Uncertainty_Quantification_for_Differential_Equations%21.html">2311 andrew gelman stats-2014-04-29-Bayesian Uncertainty Quantification for Differential Equations!</a></p>
<p>14 0.10630602 <a title="1339-tfidf-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-19-Index_or_indicator_variables.html">2296 andrew gelman stats-2014-04-19-Index or indicator variables</a></p>
<p>15 0.10031677 <a title="1339-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-24-Bleg%3A_Automatic_Differentiation_for_Log_Prob_Gradients%3F.html">535 andrew gelman stats-2011-01-24-Bleg: Automatic Differentiation for Log Prob Gradients?</a></p>
<p>16 0.098690853 <a title="1339-tfidf-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-04-Handy_Matrix_Cheat_Sheet%2C_with_Gradients.html">555 andrew gelman stats-2011-02-04-Handy Matrix Cheat Sheet, with Gradients</a></p>
<p>17 0.095606007 <a title="1339-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<p>18 0.094914816 <a title="1339-tfidf-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-25-Basketball_Stats%3A__Don%E2%80%99t_model_the_probability_of_win%2C_model_the_expected_score_differential..html">2224 andrew gelman stats-2014-02-25-Basketball Stats:  Don’t model the probability of win, model the expected score differential.</a></p>
<p>19 0.094442196 <a title="1339-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-21-BDA3_table_of_contents_%28also_a_new_paper_on_visualization%29.html">1991 andrew gelman stats-2013-08-21-BDA3 table of contents (also a new paper on visualization)</a></p>
<p>20 0.093397982 <a title="1339-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-01-My_course_this_fall_on_Bayesian_Computation.html">884 andrew gelman stats-2011-09-01-My course this fall on Bayesian Computation</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.089), (1, 0.026), (2, -0.044), (3, 0.042), (4, 0.031), (5, 0.062), (6, 0.002), (7, -0.069), (8, -0.038), (9, -0.047), (10, -0.025), (11, -0.028), (12, -0.057), (13, 0.002), (14, 0.033), (15, -0.023), (16, -0.007), (17, 0.018), (18, 0.03), (19, -0.041), (20, 0.004), (21, 0.018), (22, 0.05), (23, -0.0), (24, 0.066), (25, 0.063), (26, 0.01), (27, 0.062), (28, 0.041), (29, -0.028), (30, -0.039), (31, 0.012), (32, -0.002), (33, -0.007), (34, 0.02), (35, -0.024), (36, -0.01), (37, -0.047), (38, 0.017), (39, 0.018), (40, -0.039), (41, -0.012), (42, 0.018), (43, -0.019), (44, 0.006), (45, -0.019), (46, -0.046), (47, -0.021), (48, 0.034), (49, -0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95512897 <a title="1339-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-23-Learning_Differential_Geometry_for_Hamiltonian_Monte_Carlo.html">1339 andrew gelman stats-2012-05-23-Learning Differential Geometry for Hamiltonian Monte Carlo</a></p>
<p>Introduction: You can get a taste of Hamiltonian Monte Carlo (HMC) by reading the very gentle introduction in David MacKay’s general text on information theory:
  
  MacKay, D.  2003.    Information Theory, Inference, and Learning Algorithms  .  Cambridge University Press.  [see Chapter 31, which is relatively standalone and can be downloaded separately.]
   
Follow this up with Radford Neal’s much more thorough introduction to HMC:
  
 Neal, R. 2011.   MCMC Using Hamiltonian Dynamics .  In Brooks, Gelman, Jones and Meng, eds.,  Handbook of Markov Chain Monte Carlo .  Chapman and Hall/CRC Press.
   
To understand why HMC works and set yourself on the path to understanding generalizations like  Riemann manifold HMC , you’ll need to know a bit about differential geometry.  I really liked the combination of these two books:
  
  Magnus, J. R. and H. Neudecker.  2007.   Matrix Differential Calculus with Application in Statistics and Econometrics .  3rd Edition.  Wiley?
   
and
  
  Leimkuhler, B. and S.</p><p>2 0.72399199 <a title="1339-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-04-Handy_Matrix_Cheat_Sheet%2C_with_Gradients.html">555 andrew gelman stats-2011-02-04-Handy Matrix Cheat Sheet, with Gradients</a></p>
<p>Introduction: This post is an (unpaid) advertisement for the following extremely useful resource:
  
 Petersen, K. B. and M. S. Pedersen. 2008.   The Matrix Cookbook  .  Tehcnical Report, Technical University of Denmark. 
  
It contains 70+ pages of useful relations and derivations involving matrices.  What grabbed my eye was the computation of gradients for matrix operations ranging from eigenvalues and determinants to multivariate normal density functions.   I had no idea the multivariate normal had such a clean gradient (see section 8).
  

 
We’ve been playing around with  Hamiltonian (aka Hybrid) Monte Carlo  for sampling from the posterior of hierarchical generalized linear models with lots of interactions.  HMC speeds up Metropolis sampling by using the gradient of the log probability to drive samples in the direction of higher probability density, which is particularly useful for correlated parameters that mix slowly with standard Gibbs sampling.   Matt “III” Hoffman ‘s already got it workin</p><p>3 0.7035026 <a title="1339-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-21-Handbook_of_Markov_Chain_Monte_Carlo.html">674 andrew gelman stats-2011-04-21-Handbook of Markov Chain Monte Carlo</a></p>
<p>Introduction: Galin Jones, Steve Brooks, Xiao-Li Meng and I edited a handbook of Markov Chain Monte Carlo that has  just been published .  My chapter (with Kenny Shirley) is  here , and it begins like this:
  
Convergence of Markov chain simulations can be monitored by measuring the diffusion and mixing of multiple independently-simulated chains, but different levels of convergence are appropriate for different goals. When considering inference from stochastic simulation, we need to separate two tasks: (1) inference about parameters and functions of parameters based on broad characteristics of their distribution, and (2) more precise computation of expectations and other functions of probability distributions. For the first task, there is a natural limit to precision beyond which additional simulations add essentially nothing; for the second task, the appropriate precision must be decided from external considerations. We illustrate with an example from our current research, a hierarchical model of t</p><p>4 0.69221622 <a title="1339-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<p>Introduction: Tomas Iesmantas had  asked me  for advice on a regression problem with 50 parameters, and I’d recommended Hamiltonian Monte Carlo.
 
A few weeks later he reported back: 
  
  
After trying several modifications (HMC for all parameters at once, HMC just for first level parameters and Riemman manifold Hamiltonian Monte Carlo method), I finally got it running with HMC just for first level parameters and for others using direct sampling, since conditional distributions turned out to have closed form.


However, even in this case it is quite tricky, since I had to employ mass matrix and not just diagonal but at the beginning of algorithm generated it randomly (ensuring it is positive definite). Such random generation of mass matrix is quite blind step, but it proved to be quite helpful.


Riemman manifold HMC is quite vagarious, or to be more specific, metric of manifold is very sensitive. In my model log-likelihood I had exponents and values of metrics matrix elements was very large and wh</p><p>5 0.67907 <a title="1339-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-21-BDA3_table_of_contents_%28also_a_new_paper_on_visualization%29.html">1991 andrew gelman stats-2013-08-21-BDA3 table of contents (also a new paper on visualization)</a></p>
<p>Introduction: In response to our  recent posting  of Amazon’s offer of Bayesian Data Analysis 3rd edition at 40% off, some people asked what was in this new edition, with more information beyond the beautiful cover image and the  brief paragraph  I’d posted earlier.
 
 Here’s  the table of contents.  The following sections have all-new material:
 
1.4 New introduction of BDA principles using a simple spell checking example 
2.9 Weakly informative prior distributions 
5.7 Weakly informative priors for hierarchical variance parameters 
7.1-7.4 Predictive accuracy for model evaluation and comparison 
10.6 Computing environments 
11.4 Split R-hat 
11.5 New measure of effective number of simulation draws 
13.7 Variational inference 
13.8 Expectation propagation 
13.9 Other approximations 
14.6 Regularization for regression models 
C.1 Getting started with R and Stan 
C.2 Fitting a hierarchical model in Stan 
C.4 Programming Hamiltonian Monte Carlo in R
 
And the new chapters: 
20 Basis function models 
2</p><p>6 0.67901969 <a title="1339-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-24-Bleg%3A_Automatic_Differentiation_for_Log_Prob_Gradients%3F.html">535 andrew gelman stats-2011-01-24-Bleg: Automatic Differentiation for Log Prob Gradients?</a></p>
<p>7 0.66372639 <a title="1339-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-07-Update_on_the_new_Handbook_of_MCMC.html">844 andrew gelman stats-2011-08-07-Update on the new Handbook of MCMC</a></p>
<p>8 0.65348446 <a title="1339-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-Visualizing_Distributions_of_Covariance_Matrices.html">1477 andrew gelman stats-2012-08-30-Visualizing Distributions of Covariance Matrices</a></p>
<p>9 0.63602829 <a title="1339-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-06-The_new_Stan_1.1.1%2C_featuring_Gaussian_processes%21.html">1710 andrew gelman stats-2013-02-06-The new Stan 1.1.1, featuring Gaussian processes!</a></p>
<p>10 0.6261571 <a title="1339-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-06-Stan_1.2.0_and_RStan_1.2.0.html">1753 andrew gelman stats-2013-03-06-Stan 1.2.0 and RStan 1.2.0</a></p>
<p>11 0.62602681 <a title="1339-lsi-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-04-Stan_in_L.A._this_Wed_3%3A30pm.html">1749 andrew gelman stats-2013-03-04-Stan in L.A. this Wed 3:30pm</a></p>
<p>12 0.62118399 <a title="1339-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-30-Stan_uses_Nuts%21.html">1036 andrew gelman stats-2011-11-30-Stan uses Nuts!</a></p>
<p>13 0.59533328 <a title="1339-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-12-Stan_1.3.0_and_RStan_1.3.0_Ready_for_Action.html">1799 andrew gelman stats-2013-04-12-Stan 1.3.0 and RStan 1.3.0 Ready for Action</a></p>
<p>14 0.58905739 <a title="1339-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-30-Stan_Project%3A__Continuous_Relaxations_for_Discrete_MRFs.html">2003 andrew gelman stats-2013-08-30-Stan Project:  Continuous Relaxations for Discrete MRFs</a></p>
<p>15 0.57925189 <a title="1339-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-20-Stan_at_Google_this_Thurs_and_at_Berkeley_this_Fri_noon.html">1772 andrew gelman stats-2013-03-20-Stan at Google this Thurs and at Berkeley this Fri noon</a></p>
<p>16 0.5770672 <a title="1339-lsi-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-Running_into_a_Stan_Reference_by_Accident.html">2231 andrew gelman stats-2014-03-03-Running into a Stan Reference by Accident</a></p>
<p>17 0.56971055 <a title="1339-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-17-Stan_and_RStan_1.1.0.html">1627 andrew gelman stats-2012-12-17-Stan and RStan 1.1.0</a></p>
<p>18 0.54835296 <a title="1339-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-19-R_package_for_Bayes_factors.html">1682 andrew gelman stats-2013-01-19-R package for Bayes factors</a></p>
<p>19 0.54140359 <a title="1339-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<p>20 0.54129595 <a title="1339-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-27-%28R-Py-Cmd%29Stan_2.1.0.html">2150 andrew gelman stats-2013-12-27-(R-Py-Cmd)Stan 2.1.0</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.012), (7, 0.011), (9, 0.011), (15, 0.011), (16, 0.047), (19, 0.013), (21, 0.041), (24, 0.046), (27, 0.069), (29, 0.044), (38, 0.093), (39, 0.032), (40, 0.01), (41, 0.028), (56, 0.035), (68, 0.021), (73, 0.014), (77, 0.031), (82, 0.076), (85, 0.032), (96, 0.026), (98, 0.011), (99, 0.189)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92200428 <a title="1339-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-23-Learning_Differential_Geometry_for_Hamiltonian_Monte_Carlo.html">1339 andrew gelman stats-2012-05-23-Learning Differential Geometry for Hamiltonian Monte Carlo</a></p>
<p>Introduction: You can get a taste of Hamiltonian Monte Carlo (HMC) by reading the very gentle introduction in David MacKay’s general text on information theory:
  
  MacKay, D.  2003.    Information Theory, Inference, and Learning Algorithms  .  Cambridge University Press.  [see Chapter 31, which is relatively standalone and can be downloaded separately.]
   
Follow this up with Radford Neal’s much more thorough introduction to HMC:
  
 Neal, R. 2011.   MCMC Using Hamiltonian Dynamics .  In Brooks, Gelman, Jones and Meng, eds.,  Handbook of Markov Chain Monte Carlo .  Chapman and Hall/CRC Press.
   
To understand why HMC works and set yourself on the path to understanding generalizations like  Riemann manifold HMC , you’ll need to know a bit about differential geometry.  I really liked the combination of these two books:
  
  Magnus, J. R. and H. Neudecker.  2007.   Matrix Differential Calculus with Application in Statistics and Econometrics .  3rd Edition.  Wiley?
   
and
  
  Leimkuhler, B. and S.</p><p>2 0.82105851 <a title="1339-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-04-Estimating_the_effect_of_A_on_B%2C_and_also_the_effect_of_B_on_A.html">393 andrew gelman stats-2010-11-04-Estimating the effect of A on B, and also the effect of B on A</a></p>
<p>Introduction: Lei Liu writes:
  
 
I am working with clinicians in infectious disease and international health to study the (possible causal) relation between malnutrition and virus infection episodes (e.g., diarrhea) in babies in developing countries.


Basically the clinicians are interested in two questions: does malnutrition cause more diarrhea episodes? does diarrhea lead to malnutrition? The malnutrition status is indicated by height and weight (adjusted, HAZ and WAZ measures) observed every 3 months from birth to 1 year. They also recorded the time of each diarrhea episode during the 1 year follow-up period. They have very solid datasets for analysis.


As you can see, this is almost like a chicken and egg problem. I am a layman to causal inference. The method I use is just to do some simple regression. For example, to study the causal relation from malnutrition to diarrhea episodes, I use binary variable (diarrhea yes/no during months 0-3) as response, and use the HAZ at month 0 as covariate</p><p>3 0.81806391 <a title="1339-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-20-Cars_vs._trucks.html">527 andrew gelman stats-2011-01-20-Cars vs. trucks</a></p>
<p>Introduction: Anupam Agrawal writes:
  
I am an Assistant Professor of Operations Management at the University of Illinois. . . . My main work is in supply chain area, and empirical in nature. . . . I am working with a firm that has two separate divisions – one making cars, and the other makes trucks. Four years back, the firm made an interesting organizational change. They created a separate group of ~25 engineers, in their car division (from within their quality and production engineers). This group was focused on improving supplier quality and reported to car plant head . The truck division did not (and still does not) have such an independent “supplier improvement group”. Other than this unit in car, the organizational arrangements in the two divisions mimic each other.  There are many common suppliers to the car and truck division.


Data on quality of components coming from suppliers has been collected (for the last four years). The organizational change happened in January 2007.


My focus is</p><p>4 0.81531221 <a title="1339-lda-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-28-Nostalgia.html">1874 andrew gelman stats-2013-05-28-Nostalgia</a></p>
<p>Introduction: Saw Argo the other day, was impressed by the way it was filmed in such a 70s style, sorta like that movie The Limey or an episode of the Rockford Files.
 
I also felt nostalgia for that relatively nonviolent era.  All those hostages and nobody was killed.  It’s a good thing the Ayatollah didn’t have some fundamentalist Shiite equivalent of John Yoo telling him to waterboard everybody.
 
At the time we were all so angry and upset about the hostage-taking, but from the perspective of our suicide-bomber era, that whole hostage episode seems so comfortingly mild.</p><p>5 0.8127929 <a title="1339-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-31-Editing_and_clutch_hitting.html">173 andrew gelman stats-2010-07-31-Editing and clutch hitting</a></p>
<p>Introduction: Regarding  editing :  The only serious editing I’ve ever received has been for my New York Times op-eds and my article in the American Scientist.  My book editors have all been nice people, and they’ve helped me with many things (including suggestions of what my priorities should be in communicating with readers)–they’ve been great–but they’ve not given (nor have I expected or asked for) serious editing.  Maybe I should’ve asked for it, I don’t know.  I’ve had time-wasting experiences with copy editors and a particularly annoying experience with a production editor (who was so difficult that my coauthors and I actually contacted our agent and a lawyer about the possibility of getting out of our contract), but that’s another story.
 
Regarding  clutch hitting , Bill James once noted that it’s great when a Bucky Dent hits an unexpected home run, but what’s really special is being able to get the big hit when it’s expected of you.  The best players can do their best every time they come t</p><p>6 0.81225097 <a title="1339-lda-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-13-Randomized_experiments%2C_non-randomized_experiments%2C_and_observational_studies.html">340 andrew gelman stats-2010-10-13-Randomized experiments, non-randomized experiments, and observational studies</a></p>
<p>7 0.81038505 <a title="1339-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-06-Another_stereotype_demolished.html">699 andrew gelman stats-2011-05-06-Another stereotype demolished</a></p>
<p>8 0.80988991 <a title="1339-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-16-Choices_in_graphing_parallel_time_series.html">1498 andrew gelman stats-2012-09-16-Choices in graphing parallel time series</a></p>
<p>9 0.80859303 <a title="1339-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-09-Besag.html">193 andrew gelman stats-2010-08-09-Besag</a></p>
<p>10 0.80748069 <a title="1339-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-27-Teaching_is_hard.html">1958 andrew gelman stats-2013-07-27-Teaching is hard</a></p>
<p>11 0.80692893 <a title="1339-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-02-%E2%80%9CA_Christmas_Carol%E2%80%9D_as_applied_to_plagiarism.html">1440 andrew gelman stats-2012-08-02-“A Christmas Carol” as applied to plagiarism</a></p>
<p>12 0.80687827 <a title="1339-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-03-More_on_that_Dartmouth_health_care_study.html">67 andrew gelman stats-2010-06-03-More on that Dartmouth health care study</a></p>
<p>13 0.80644625 <a title="1339-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-03-%28Partisan%29_visualization_of_health_care_legislation.html">178 andrew gelman stats-2010-08-03-(Partisan) visualization of health care legislation</a></p>
<p>14 0.80604649 <a title="1339-lda-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Interactions_of_predictors_in_a_causal_model.html">251 andrew gelman stats-2010-09-02-Interactions of predictors in a causal model</a></p>
<p>15 0.80502021 <a title="1339-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-29-What_are_the_open_problems_in_Bayesian_statistics%3F%3F.html">686 andrew gelman stats-2011-04-29-What are the open problems in Bayesian statistics??</a></p>
<p>16 0.80474722 <a title="1339-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-04-%E2%80%9CSocial_Psychologists_Detect_Liberal_Bias_Within%E2%80%9D.html">600 andrew gelman stats-2011-03-04-“Social Psychologists Detect Liberal Bias Within”</a></p>
<p>17 0.80225611 <a title="1339-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-10-It%E2%80%99s_no_fun_being_graded_on_a_curve.html">606 andrew gelman stats-2011-03-10-It’s no fun being graded on a curve</a></p>
<p>18 0.79906398 <a title="1339-lda-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-07-Peer_pressure%2C_selection%2C_and_educational_reform.html">326 andrew gelman stats-2010-10-07-Peer pressure, selection, and educational reform</a></p>
<p>19 0.79850477 <a title="1339-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-30-Real_rothko%2C_fake_rothko.html">1553 andrew gelman stats-2012-10-30-Real rothko, fake rothko</a></p>
<p>20 0.79770422 <a title="1339-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-31-Using_factor_analysis_or_principal_components_analysis_or_measurement-error_models_for_biological_measurements_in_archaeology%3F.html">1094 andrew gelman stats-2011-12-31-Using factor analysis or principal components analysis or measurement-error models for biological measurements in archaeology?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
