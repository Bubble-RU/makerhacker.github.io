<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1141 andrew gelman stats-2012-01-28-Using predator-prey models on the Canadian lynx series</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1141" href="#">andrew_gelman_stats-2012-1141</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1141 andrew gelman stats-2012-01-28-Using predator-prey models on the Canadian lynx series</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1141-html" href="http://andrewgelman.com/2012/01/28/the-last-word-on-the-canadian-lynx-series/">html</a></p><p>Introduction: The “Canadian lynx data” is one of the famous examples used in time series analysis.  And the usual models that are fit to these data in the statistics time-series literature, don’t work well.  Cavan Reilly and Angelique Zeringue  write :
 
   
 
Reilly and Zeringue then present their analysis.  Their simple little predator-prey model with a weakly informative prior way outperforms the standard big-ass autoregression models.  Check this out:
 
   
 
Or, to put it into numbers, when they fit their model to the first 80 years and predict to the next 34, their root mean square out-of-sample error is 1480 (see scale of data above).  In contrast, the standard model fit to these data (the SETAR model of Tong, 1990) has more than twice as many parameters but gets a worse-performing root mean square error of 1600, even when that model is fit to the entire dataset.  (If you fit the SETAR or any similar autoregressive model to the first 80 years and use it to predict the next 34, the predictions</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The “Canadian lynx data” is one of the famous examples used in time series analysis. [sent-1, score-0.342]
</p><p>2 And the usual models that are fit to these data in the statistics time-series literature, don’t work well. [sent-2, score-0.428]
</p><p>3 Their simple little predator-prey model with a weakly informative prior way outperforms the standard big-ass autoregression models. [sent-4, score-0.779]
</p><p>4 Check this out:         Or, to put it into numbers, when they fit their model to the first 80 years and predict to the next 34, their root mean square out-of-sample error is 1480 (see scale of data above). [sent-5, score-1.223]
</p><p>5 In contrast, the standard model fit to these data (the SETAR model of Tong, 1990) has more than twice as many parameters but gets a worse-performing root mean square error of 1600, even when that model is fit to the entire dataset. [sent-6, score-1.846]
</p><p>6 (If you fit the SETAR or any similar autoregressive model to the first 80 years and use it to predict the next 34, the predictions are a disaster—the predicted values quickly go toward the mean and can’t even attempt to track the curve. [sent-7, score-1.072]
</p><p>7 )   As Reilly and Zeringue note, the above graph shows potential room for improvement in the model, but even as is, it shows the huge benefits that can be obtained by attempting to model the underlying process rather than simply fitting the data using a conventional family of models. [sent-8, score-0.918]
</p><p>8 (It’s funny for me to emphasize this point, given how often I use conventional models such as linear and logistic regression. [sent-9, score-0.352]
</p><p>9 The title and text above have been modified to reflect comments below with reference to models fit to the lynx data in the ecology literature. [sent-12, score-1.277]
</p><p>10 There appears to be not enough communication between ecologists and statisticians. [sent-13, score-0.162]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lynx', 0.342), ('zeringue', 0.342), ('reilly', 0.298), ('autoregression', 0.25), ('setar', 0.25), ('model', 0.225), ('fit', 0.221), ('ecology', 0.17), ('root', 0.168), ('square', 0.157), ('conventional', 0.121), ('models', 0.114), ('angelique', 0.114), ('predict', 0.106), ('ecologists', 0.103), ('cavan', 0.103), ('autoregressive', 0.103), ('shows', 0.098), ('mean', 0.097), ('data', 0.093), ('canadian', 0.092), ('check', 0.091), ('outperforms', 0.09), ('outperform', 0.088), ('disaster', 0.087), ('attempting', 0.084), ('modified', 0.083), ('error', 0.08), ('next', 0.076), ('comments', 0.072), ('standard', 0.072), ('simple', 0.071), ('obtained', 0.071), ('weakly', 0.071), ('holds', 0.069), ('generic', 0.068), ('improvement', 0.066), ('reflect', 0.066), ('track', 0.062), ('twice', 0.062), ('references', 0.062), ('quickly', 0.062), ('room', 0.062), ('predicted', 0.061), ('attempt', 0.059), ('emphasize', 0.059), ('text', 0.059), ('communication', 0.059), ('logistic', 0.058), ('reference', 0.057)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="1141-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-28-Using_predator-prey_models_on_the_Canadian_lynx_series.html">1141 andrew gelman stats-2012-01-28-Using predator-prey models on the Canadian lynx series</a></p>
<p>Introduction: The “Canadian lynx data” is one of the famous examples used in time series analysis.  And the usual models that are fit to these data in the statistics time-series literature, don’t work well.  Cavan Reilly and Angelique Zeringue  write :
 
   
 
Reilly and Zeringue then present their analysis.  Their simple little predator-prey model with a weakly informative prior way outperforms the standard big-ass autoregression models.  Check this out:
 
   
 
Or, to put it into numbers, when they fit their model to the first 80 years and predict to the next 34, their root mean square out-of-sample error is 1480 (see scale of data above).  In contrast, the standard model fit to these data (the SETAR model of Tong, 1990) has more than twice as many parameters but gets a worse-performing root mean square error of 1600, even when that model is fit to the entire dataset.  (If you fit the SETAR or any similar autoregressive model to the first 80 years and use it to predict the next 34, the predictions</p><p>2 0.16717355 <a title="1141-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-20-Amazing_retro_gnu_graphics%21.html">1907 andrew gelman stats-2013-06-20-Amazing retro gnu graphics!</a></p>
<p>Introduction: Bill Harris writes:
  
Speaking of strange graphics, http://makingsense.facilitatedsystems.com/2007/03/making-musical-sense-by-email-part-2.html shows an example of text (gnuplot’s dumb terminal) graphics of data from MCSim (code and other material available from http://makingsense.facilitatedsystems.com/2007/03/making-musical-sense-by-email-table-of.html).


 


At another extreme, slide 20 of https://docs.google.com/viewer?a=v&pid;=sites&srcid;=ZGVmYXVsdGRvbWFpbnx3c2hhcnJpczEzfGd4OjZkNGFjZWZhOTAyYTFkMDg shows a stereogram of more MCSim output (I was a bit more naive back then).  I included the stereogram as a bit of humor just to show what could be done with J graphics.  Surprisingly, one person in the audience focused intently on that slide and, after a moment, said “Got it!”  We spoke afterwards, and it turned out that he was on the board or at least a volunteer at the Portland (OR) 3D Center of Art & Photography (http://www.3dcenterusa.com/index.html).


   
  
Regarding mcsim, the</p><p>3 0.1572741 <a title="1141-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-27-Bridges_between_deterministic_and_probabilistic_models_for_binary_data.html">780 andrew gelman stats-2011-06-27-Bridges between deterministic and probabilistic models for binary data</a></p>
<p>Introduction: For the analysis of binary data, various deterministic models have been proposed, which are generally simpler to fit and easier to understand than probabilistic models. We claim that corresponding to any deterministic model is an implicit stochastic model in which the deterministic model fits imperfectly, with errors occurring at random. In the context of binary data, we consider a model in which the probability of error depends on the model prediction. We show how to fit this model using a stochastic modification of deterministic optimization schemes.
 
The advantages of fitting the stochastic model explicitly (rather than implicitly, by simply fitting a deterministic model and accepting the occurrence of errors) include quantification of uncertainty in the deterministic model’s parameter estimates, better estimation of the true model error rate, and the ability to check the fit of the model nontrivially. We illustrate this with a simple theoretical example of item response data and w</p><p>4 0.15207961 <a title="1141-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>Introduction: In response to  my remarks  on his online book, Think Bayes, Allen Downey wrote: 
   
I [Downey] have a question about one of your comments: 
   My [Gelman's] main criticism with both books is that they talk a lot about inference but not so much about model building or model checking (recall the three steps of Bayesian data analysis). I think it’s ok for an introductory book to focus on inference, which of course is central to the data-analytic process—but I’d like them to at least mention that Bayesian ideas arise in model building and model checking as well. 

This sounds like something I agree with, and one of the things I tried to do in the book is to put modeling decisions front and center.  But the word “modeling” is used in lots of ways, so I want to see if we are talking about the same thing.


For example, in many chapters, I start with a simple model of the scenario, do some analysis, then check whether the model is good enough, and iterate.  Here’s the discussion of modeling</p><p>5 0.14776357 <a title="1141-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-05-Xiao-Li_Meng_and_Xianchao_Xie_rethink_asymptotics.html">1406 andrew gelman stats-2012-07-05-Xiao-Li Meng and Xianchao Xie rethink asymptotics</a></p>
<p>Introduction: In  an article  catchily entitled, “I got more data, my model is more refined, but my estimator is getting worse!  Am I just dumb?”, Meng and Xie write:
  
Possibly, but more likely you are merely a victim of conventional wisdom. More data or better models by no means guarantee better estimators (e.g., with a smaller mean squared error), when you are not following probabilistically principled methods such as MLE (for large samples) or Bayesian approaches. Estimating equations are par- ticularly vulnerable in this regard, almost a necessary price for their robustness. These points will be demonstrated via common tasks of estimating regression parameters and correlations, under simple mod- els such as bivariate normal and ARCH(1). Some general strategies for detecting and avoiding such pitfalls are suggested, including checking for self-efficiency (Meng, 1994, Statistical Science) and adopting a guiding working model.


Using the example of estimating the autocorrelation ρ under a statio</p><p>6 0.1413826 <a title="1141-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>7 0.13613775 <a title="1141-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-24-F-f-f-fake_data.html">1735 andrew gelman stats-2013-02-24-F-f-f-fake data</a></p>
<p>8 0.13400351 <a title="1141-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-13-Flexibility_is_good.html">2133 andrew gelman stats-2013-12-13-Flexibility is good</a></p>
<p>9 0.13235174 <a title="1141-tfidf-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-09-Special_journal_issue_on_statistical_methods_for_the_social_sciences.html">24 andrew gelman stats-2010-05-09-Special journal issue on statistical methods for the social sciences</a></p>
<p>10 0.13197814 <a title="1141-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-11-Adding_an_error_model_to_a_deterministic_model.html">1162 andrew gelman stats-2012-02-11-Adding an error model to a deterministic model</a></p>
<p>11 0.12734659 <a title="1141-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>12 0.12484116 <a title="1141-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>13 0.12355798 <a title="1141-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-29-Difficulties_with_the_1-4-power_transformation.html">1142 andrew gelman stats-2012-01-29-Difficulties with the 1-4-power transformation</a></p>
<p>14 0.12349265 <a title="1141-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>15 0.11155114 <a title="1141-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>16 0.11112433 <a title="1141-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-07-Robust_logistic_regression.html">1886 andrew gelman stats-2013-06-07-Robust logistic regression</a></p>
<p>17 0.11059258 <a title="1141-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>18 0.1069738 <a title="1141-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>19 0.10563438 <a title="1141-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-13-Checking_your_model_using_fake_data.html">852 andrew gelman stats-2011-08-13-Checking your model using fake data</a></p>
<p>20 0.10533426 <a title="1141-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.18), (1, 0.158), (2, 0.022), (3, 0.067), (4, 0.056), (5, -0.02), (6, -0.003), (7, -0.038), (8, 0.066), (9, 0.056), (10, 0.047), (11, 0.065), (12, -0.058), (13, -0.011), (14, -0.089), (15, -0.011), (16, 0.039), (17, -0.029), (18, -0.005), (19, -0.008), (20, 0.021), (21, -0.04), (22, -0.031), (23, -0.076), (24, -0.021), (25, 0.016), (26, -0.026), (27, -0.027), (28, 0.016), (29, -0.027), (30, -0.056), (31, 0.006), (32, -0.031), (33, -0.013), (34, -0.003), (35, 0.038), (36, -0.014), (37, -0.02), (38, 0.014), (39, -0.023), (40, -0.002), (41, 0.002), (42, -0.009), (43, 0.042), (44, 0.001), (45, 0.005), (46, -0.047), (47, -0.06), (48, 0.01), (49, 0.015)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98587036 <a title="1141-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-28-Using_predator-prey_models_on_the_Canadian_lynx_series.html">1141 andrew gelman stats-2012-01-28-Using predator-prey models on the Canadian lynx series</a></p>
<p>Introduction: The “Canadian lynx data” is one of the famous examples used in time series analysis.  And the usual models that are fit to these data in the statistics time-series literature, don’t work well.  Cavan Reilly and Angelique Zeringue  write :
 
   
 
Reilly and Zeringue then present their analysis.  Their simple little predator-prey model with a weakly informative prior way outperforms the standard big-ass autoregression models.  Check this out:
 
   
 
Or, to put it into numbers, when they fit their model to the first 80 years and predict to the next 34, their root mean square out-of-sample error is 1480 (see scale of data above).  In contrast, the standard model fit to these data (the SETAR model of Tong, 1990) has more than twice as many parameters but gets a worse-performing root mean square error of 1600, even when that model is fit to the entire dataset.  (If you fit the SETAR or any similar autoregressive model to the first 80 years and use it to predict the next 34, the predictions</p><p>2 0.92313325 <a title="1141-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-27-Bridges_between_deterministic_and_probabilistic_models_for_binary_data.html">780 andrew gelman stats-2011-06-27-Bridges between deterministic and probabilistic models for binary data</a></p>
<p>Introduction: For the analysis of binary data, various deterministic models have been proposed, which are generally simpler to fit and easier to understand than probabilistic models. We claim that corresponding to any deterministic model is an implicit stochastic model in which the deterministic model fits imperfectly, with errors occurring at random. In the context of binary data, we consider a model in which the probability of error depends on the model prediction. We show how to fit this model using a stochastic modification of deterministic optimization schemes.
 
The advantages of fitting the stochastic model explicitly (rather than implicitly, by simply fitting a deterministic model and accepting the occurrence of errors) include quantification of uncertainty in the deterministic model’s parameter estimates, better estimation of the true model error rate, and the ability to check the fit of the model nontrivially. We illustrate this with a simple theoretical example of item response data and w</p><p>3 0.91605008 <a title="1141-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-09-Special_journal_issue_on_statistical_methods_for_the_social_sciences.html">24 andrew gelman stats-2010-05-09-Special journal issue on statistical methods for the social sciences</a></p>
<p>Introduction: Last year I  spoke at  a conference celebrating the 10th anniversary of the University of Washington’s Center for Statistics and the Social Sciences, and just today a  special issue  of the journal Statistical Methodology came out in honor of the center’s anniversary.   My article  in the special issue actually has nothing to do with my talk at the conference; rather, it’s an exploration of an idea that Iven Van Mechelen and I had for understanding deterministic models probabilistically:
  
 
For the analysis of binary data, various deterministic models have been proposed, which are generally simpler to fit and easier to understand than probabilistic models. We claim that corresponding to any deterministic model is an implicit stochastic model in which the deterministic model fits imperfectly, with errors occurring at random. In the context of binary data, we consider a model in which the probability of error depends on the model prediction. We show how to fit this model using a stocha</p><p>4 0.9107734 <a title="1141-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>Introduction: Ilya Esteban writes:
  
In traditional machine learning and statistical learning techniques, you spend a lot of time selecting your input features, fiddling with model parameter values, etc., all of which leads to the problem of overfitting the data and producing overly optimistic estimates for how good the model really is. You can use techniques such as cross-validation and out-of-sample validation data to try to limit the damage, but they are imperfect solutions at best.


While Bayesian models have the great advantage of not forcing you to manually select among the various weights and input features, you still often end up trying different priors and model structures (especially with hierarchical models), before coming up with a “final” model. When applying Bayesian modeling to real world data sets, how does should you evaluate alternate priors and topologies for the model without falling into the same overfitting trap as you do with non-Bayesian models? If you try several different</p><p>5 0.90884739 <a title="1141-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-13-Flexibility_is_good.html">2133 andrew gelman stats-2013-12-13-Flexibility is good</a></p>
<p>Introduction: If I made a separate post for each interesting blog discussion, we’d get overwhelmed.  That’s why I often leave detailed responses in the comments section, even though I’m pretty sure that most readers don’t look in the comments at all.
 
Sometimes, though, I think it’s good to bring such discussions to light.  Here’s a recent example.
 
Michael  wrote :
  
Poor predictive performance usually indicates that the model isn’t sufficiently flexible to explain the data, and my understanding of the proper Bayesian strategy is to feed that back into your original model and try again until you achieve better performance.
  
Corey  replied :
  
It was my impression that — in ML at least — poor predictive performance is more often due to the model being too flexible and fitting noise.
  
And Rahul  agreed :
  
Good point. A very flexible model will describe your training data perfectly and then go bonkers when unleashed on wild data.
  
But I  wrote :
  
Overfitting comes from a model being flex</p><p>6 0.89179724 <a title="1141-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>7 0.88464165 <a title="1141-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>8 0.88359207 <a title="1141-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-11-Adding_an_error_model_to_a_deterministic_model.html">1162 andrew gelman stats-2012-02-11-Adding an error model to a deterministic model</a></p>
<p>9 0.8804062 <a title="1141-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>10 0.87424982 <a title="1141-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-05-Xiao-Li_Meng_and_Xianchao_Xie_rethink_asymptotics.html">1406 andrew gelman stats-2012-07-05-Xiao-Li Meng and Xianchao Xie rethink asymptotics</a></p>
<p>11 0.86758929 <a title="1141-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-11-Kaiser_Fung_on_how_not_to_critique_models.html">1004 andrew gelman stats-2011-11-11-Kaiser Fung on how not to critique models</a></p>
<p>12 0.86419386 <a title="1141-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-21-More_on_Bayesian_model_selection_in_high-dimensional_settings.html">1817 andrew gelman stats-2013-04-21-More on Bayesian model selection in high-dimensional settings</a></p>
<p>13 0.86273408 <a title="1141-lsi-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-08-Displaying_a_fitted_multilevel_model.html">328 andrew gelman stats-2010-10-08-Displaying a fitted multilevel model</a></p>
<p>14 0.86209494 <a title="1141-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>15 0.86157519 <a title="1141-lsi-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-03-This_is_a_footnote_in_one_of_my_papers.html">448 andrew gelman stats-2010-12-03-This is a footnote in one of my papers</a></p>
<p>16 0.85742968 <a title="1141-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-27-Cross-validation_%28What_is_it_good_for%3F%29.html">1395 andrew gelman stats-2012-06-27-Cross-validation (What is it good for?)</a></p>
<p>17 0.85626453 <a title="1141-lsi-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-07-Bayesian_hierarchical_model_for_the_prediction_of_soccer_results.html">20 andrew gelman stats-2010-05-07-Bayesian hierarchical model for the prediction of soccer results</a></p>
<p>18 0.84622288 <a title="1141-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>19 0.84571207 <a title="1141-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>20 0.84159684 <a title="1141-lsi-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-09-Removing_the_blindfold%3A_visualising_statistical_models.html">265 andrew gelman stats-2010-09-09-Removing the blindfold: visualising statistical models</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(9, 0.02), (16, 0.071), (21, 0.036), (24, 0.189), (29, 0.021), (41, 0.018), (45, 0.014), (53, 0.031), (61, 0.017), (62, 0.012), (74, 0.144), (82, 0.011), (86, 0.04), (99, 0.276)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96404839 <a title="1141-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-10-SeeThroughNY.html">140 andrew gelman stats-2010-07-10-SeeThroughNY</a></p>
<p>Introduction: From  Ira Stoll , a link to this  cool data site , courtesy of the Manhattan Institute, with all sorts of state budget information including the salaries of all city and state employees.</p><p>2 0.95463347 <a title="1141-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-08-The_Case_for_More_False_Positives_in_Anti-doping_Testing.html">1612 andrew gelman stats-2012-12-08-The Case for More False Positives in Anti-doping Testing</a></p>
<p>Introduction: Kaiser Fung was  ahead of the curve  on Lance Armstrong:
  
The media has gotten the statistics totally backwards.


On the one hand, they faithfully report the colorful stories of athletes who fail drug tests pleading their innocence. (I have written about the Spanish cyclist Alberto Contador here.) On the other hand, they unquestioningly report athletes who claim “hundreds of negative tests” prove their honesty. Putting these two together implies that the media believes that negative test results are highly reliable while positive test results are unreliable.


The reality is just the opposite. When an athlete tests positive, it’s almost sure that he/she has doped. Sure, most of the clean athletes will test negative but what is often missed is that the majority of dopers will also test negative.


We don’t need to do any computation to see that this is true. In most major sports competitions, the  proportion of tests declared positive is typically below 1%. If you believe that the pr</p><p>same-blog 3 0.95009089 <a title="1141-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-28-Using_predator-prey_models_on_the_Canadian_lynx_series.html">1141 andrew gelman stats-2012-01-28-Using predator-prey models on the Canadian lynx series</a></p>
<p>Introduction: The “Canadian lynx data” is one of the famous examples used in time series analysis.  And the usual models that are fit to these data in the statistics time-series literature, don’t work well.  Cavan Reilly and Angelique Zeringue  write :
 
   
 
Reilly and Zeringue then present their analysis.  Their simple little predator-prey model with a weakly informative prior way outperforms the standard big-ass autoregression models.  Check this out:
 
   
 
Or, to put it into numbers, when they fit their model to the first 80 years and predict to the next 34, their root mean square out-of-sample error is 1480 (see scale of data above).  In contrast, the standard model fit to these data (the SETAR model of Tong, 1990) has more than twice as many parameters but gets a worse-performing root mean square error of 1600, even when that model is fit to the entire dataset.  (If you fit the SETAR or any similar autoregressive model to the first 80 years and use it to predict the next 34, the predictions</p><p>4 0.94860983 <a title="1141-lda-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-23-Greg_Mankiw%E2%80%99s_utility_function.html">2261 andrew gelman stats-2014-03-23-Greg Mankiw’s utility function</a></p>
<p>Introduction: From 2010 :
  
Greg Mankiw  writes  (link from  Tyler Cowen ):

 
Without any taxes, accepting that editor’s assignment would have yielded my children an extra $10,000. With taxes, it yields only $1,000. In effect, once the entire tax system is taken into account, my family’s marginal tax rate is about 90 percent. Is it any wonder that I [Mankiw] turn down most of the money-making opportunities I am offered?


By contrast, without the tax increases advocated by the Obama administration, the numbers would look quite different. I would face a lower income tax rate, a lower Medicare tax rate, and no deduction phaseout or estate tax. Taking that writing assignment would yield my kids about $2,000. I would have twice the incentive to keep working.
 

 First, the good news 


Obama’s tax rates are much lower than Mankiw had anticipated!  According to the above quote, his marginal tax rate is currently 80% but threatens to rise to 90%.


But, in October 2008, Mankiw  calculated  that Obama’s</p><p>5 0.94570988 <a title="1141-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-11-Mankiw%E2%80%99s_marginal_tax_rate_%28which_declined_from_93%25_to_80%25_in_two_years%29_and_the_difficulty_of_microeconomic_reasoning.html">336 andrew gelman stats-2010-10-11-Mankiw’s marginal tax rate (which declined from 93% to 80% in two years) and the difficulty of microeconomic reasoning</a></p>
<p>Introduction: Greg Mankiw  writes  (link from  Tyler Cowen ):
  
Without any taxes, accepting that editor’s assignment would have yielded my children an extra $10,000. With taxes, it yields only $1,000. In effect, once the entire tax system is taken into account, my family’s marginal tax rate is about 90 percent. Is it any wonder that I [Mankiw] turn down most of the money-making opportunities I am offered?


By contrast, without the tax increases advocated by the Obama administration, the numbers would look quite different. I would face a lower income tax rate, a lower Medicare tax rate, and no deduction phaseout or estate tax. Taking that writing assignment would yield my kids about $2,000. I would have twice the incentive to keep working.
  
 First, the good news 
 
Obama’s tax rates are much lower than Mankiw had anticipated!  According to the above quote, his marginal tax rate is currently 80% but threatens to rise to 90%.
 
But, in October 2008, Mankiw  calculated  that Obama’s would tax his m</p><p>6 0.93766707 <a title="1141-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-28-Racism%21.html">1780 andrew gelman stats-2013-03-28-Racism!</a></p>
<p>7 0.92958689 <a title="1141-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-24-Mankiw_tax_update.html">366 andrew gelman stats-2010-10-24-Mankiw tax update</a></p>
<p>8 0.92172658 <a title="1141-lda-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-28-Correlation%2C_prediction%2C_variation%2C_etc..html">301 andrew gelman stats-2010-09-28-Correlation, prediction, variation, etc.</a></p>
<p>9 0.9215039 <a title="1141-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-12-Update_on_Mankiw%E2%80%99s_work_incentives.html">338 andrew gelman stats-2010-10-12-Update on Mankiw’s work incentives</a></p>
<p>10 0.91825193 <a title="1141-lda-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-09-Reviewing_the_peer_review_process%3F.html">2239 andrew gelman stats-2014-03-09-Reviewing the peer review process?</a></p>
<p>11 0.91715831 <a title="1141-lda-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-18-Fiction_is_not_for_tirades%3F__Tell_that_to_Saul_Bellow%21.html">285 andrew gelman stats-2010-09-18-Fiction is not for tirades?  Tell that to Saul Bellow!</a></p>
<p>12 0.91082299 <a title="1141-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>13 0.90869111 <a title="1141-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-27-Laws_as_expressive.html">1085 andrew gelman stats-2011-12-27-Laws as expressive</a></p>
<p>14 0.90804899 <a title="1141-lda-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-25-Revised_statistical_standards_for_evidence_%28comments_to_Val_Johnson%E2%80%99s_comments_on_our_comments_on_Val%E2%80%99s_comments_on_p-values%29.html">2305 andrew gelman stats-2014-04-25-Revised statistical standards for evidence (comments to Val Johnson’s comments on our comments on Val’s comments on p-values)</a></p>
<p>15 0.90771168 <a title="1141-lda-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-30-Fixed_effects%2C_followed_by_Bayes_shrinkage%3F.html">1644 andrew gelman stats-2012-12-30-Fixed effects, followed by Bayes shrinkage?</a></p>
<p>16 0.90717345 <a title="1141-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-07-X_on_JLP.html">1792 andrew gelman stats-2013-04-07-X on JLP</a></p>
<p>17 0.90649575 <a title="1141-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-The_statistical_significance_filter.html">899 andrew gelman stats-2011-09-10-The statistical significance filter</a></p>
<p>18 0.90641546 <a title="1141-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-03-Boot.html">1881 andrew gelman stats-2013-06-03-Boot</a></p>
<p>19 0.90605623 <a title="1141-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-17-Macro_causality.html">807 andrew gelman stats-2011-07-17-Macro causality</a></p>
<p>20 0.90602988 <a title="1141-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-03-Another_plagiarism_mystery.html">836 andrew gelman stats-2011-08-03-Another plagiarism mystery</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
