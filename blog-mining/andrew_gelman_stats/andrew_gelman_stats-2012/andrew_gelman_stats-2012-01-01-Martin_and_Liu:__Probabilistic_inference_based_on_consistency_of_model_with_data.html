<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1095 andrew gelman stats-2012-01-01-Martin and Liu:  Probabilistic inference based on consistency of model with data</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1095" href="#">andrew_gelman_stats-2012-1095</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1095 andrew gelman stats-2012-01-01-Martin and Liu:  Probabilistic inference based on consistency of model with data</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1095-html" href="http://andrewgelman.com/2012/01/01/martin-and-liu-probabilistic-inference-based-on-consistency-of-model-with-data/">html</a></p><p>Introduction: What better way to start then new year than with some hard-core statistical theory?
 
Ryan Martin and Chuanhai Liu send along  a new paper  on inferential models:
  
Probability is a useful tool for describing uncertainty, so it is natural to strive for a system of statistical inference based on probabilities for or against various hypotheses. But existing probabilistic inference methods struggle to provide a meaningful interpretation of the probabilities across experiments in sufficient generality. In this paper we further develop a promising new approach based on what are called inferential models (IMs). The fundamental idea behind IMs is that there is an unobservable auxiliary variable that itself describes the inherent uncertainty about the parameter of interest, and that posterior probabilistic inference can be accomplished by predicting this unobserved quantity. We describe a simple and intuitive three-step construction of a random set of candidate parameter values, each being co</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 What better way to start then new year than with some hard-core statistical theory? [sent-1, score-0.078]
</p><p>2 Ryan Martin and Chuanhai Liu send along  a new paper  on inferential models:    Probability is a useful tool for describing uncertainty, so it is natural to strive for a system of statistical inference based on probabilities for or against various hypotheses. [sent-2, score-0.719]
</p><p>3 But existing probabilistic inference methods struggle to provide a meaningful interpretation of the probabilities across experiments in sufficient generality. [sent-3, score-0.945]
</p><p>4 In this paper we further develop a promising new approach based on what are called inferential models (IMs). [sent-4, score-0.349]
</p><p>5 The fundamental idea behind IMs is that there is an unobservable auxiliary variable that itself describes the inherent uncertainty about the parameter of interest, and that posterior probabilistic inference can be accomplished by predicting this unobserved quantity. [sent-5, score-1.419]
</p><p>6 We describe a simple and intuitive three-step construction of a random set of candidate parameter values, each being consistent with the model, the observed data, and a auxiliary variable prediction. [sent-6, score-0.798]
</p><p>7 Then prior-free posterior summaries of the available statistical evidence for and against a hypothesis of interest are obtained by calculating the probability that this random set falls completely in and completely out of the hypothesis, respectively. [sent-7, score-1.147]
</p><p>8 We prove that these IM-based measures of evidence are calibrated in a frequentist sense, showing that IMs give easily-interpretable results both within and across experiments. [sent-8, score-0.343]
</p><p>9 I find this stuff difficult to understand, but Chuanhai is a deep thinker and I encourage the theoretically-minded among you to take a look. [sent-9, score-0.181]
</p><p>10 My general impression is that this work is an attempt to provide a foundation for inference based on inverting hypothesis tests—or, to put it another way, inference about the subspace of parameters in which the model is consistent with data. [sent-10, score-1.19]
</p><p>11 As I’ve  discussed  (but unfortunately never written up in any formal way), the naive view of Neyman-Pearson inference by inverting hypothesis tests does not in general work (if by “work” you mean get inferences that make sense in particular cases). [sent-11, score-0.833]
</p><p>12 But I’m sympathetic to the idea that there’s a way to do this right, involving some sort of calibration. [sent-12, score-0.157]
</p><p>13 thesis—I actually had a chapter on it, I think—but I was never satisfied with the results. [sent-15, score-0.09]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ims', 0.352), ('chuanhai', 0.273), ('inference', 0.237), ('auxiliary', 0.21), ('inverting', 0.21), ('hypothesis', 0.201), ('inferential', 0.163), ('probabilistic', 0.156), ('probabilities', 0.125), ('unobservable', 0.124), ('consistent', 0.113), ('uncertainty', 0.109), ('tests', 0.109), ('thinker', 0.108), ('parameter', 0.108), ('strive', 0.102), ('posterior', 0.102), ('variable', 0.101), ('provide', 0.1), ('calibrated', 0.1), ('unobserved', 0.098), ('promising', 0.094), ('random', 0.094), ('accomplished', 0.093), ('completely', 0.092), ('based', 0.092), ('satisfied', 0.09), ('intuitive', 0.089), ('across', 0.088), ('liu', 0.088), ('calculating', 0.088), ('struggle', 0.088), ('ryan', 0.086), ('interest', 0.085), ('construction', 0.083), ('inherent', 0.081), ('summaries', 0.081), ('evidence', 0.079), ('sympathetic', 0.079), ('martin', 0.079), ('probability', 0.079), ('way', 0.078), ('falls', 0.077), ('obtained', 0.077), ('meaningful', 0.076), ('prove', 0.076), ('naive', 0.076), ('sufficient', 0.075), ('thesis', 0.075), ('encourage', 0.073)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999976 <a title="1095-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-01-Martin_and_Liu%3A__Probabilistic_inference_based_on_consistency_of_model_with_data.html">1095 andrew gelman stats-2012-01-01-Martin and Liu:  Probabilistic inference based on consistency of model with data</a></p>
<p>Introduction: What better way to start then new year than with some hard-core statistical theory?
 
Ryan Martin and Chuanhai Liu send along  a new paper  on inferential models:
  
Probability is a useful tool for describing uncertainty, so it is natural to strive for a system of statistical inference based on probabilities for or against various hypotheses. But existing probabilistic inference methods struggle to provide a meaningful interpretation of the probabilities across experiments in sufficient generality. In this paper we further develop a promising new approach based on what are called inferential models (IMs). The fundamental idea behind IMs is that there is an unobservable auxiliary variable that itself describes the inherent uncertainty about the parameter of interest, and that posterior probabilistic inference can be accomplished by predicting this unobserved quantity. We describe a simple and intuitive three-step construction of a random set of candidate parameter values, each being co</p><p>2 0.18720791 <a title="1095-tfidf-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-22-Philosophy_of_Bayes_and_non-Bayes%3A__A_dialogue_with_Deborah_Mayo.html">291 andrew gelman stats-2010-09-22-Philosophy of Bayes and non-Bayes:  A dialogue with Deborah Mayo</a></p>
<p>Introduction: I sent Deborah Mayo a link to  my paper  with Cosma Shalizi on the philosophy of statistics, and she sent me the link to this conference which unfortunately already occurred.  (It’s too bad, because I’d have liked to have been there.)  I summarized my philosophy as follows:
  
I am highly sympathetic to the approach of Lakatos (or of Popper, if you consider Lakatos’s “Popper_2″ to be a reasonable simulation of the true Popperism), in that (a) I view statistical models as being built within theoretical structures, and (b) I see the checking and refutation of models to be a key part of scientific progress.  A big problem I have with mainstream Bayesianism is its “inductivist” view that science can operate completely smoothly with posterior updates:  the idea that new data causes us to increase the posterior probability of good models and decrease the posterior probability of bad models.  I don’t buy that:  I see models as ever-changing entities that are flexible and can be patched and ex</p><p>3 0.15980619 <a title="1095-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-15-Bayesian_statistical_pragmatism.html">662 andrew gelman stats-2011-04-15-Bayesian statistical pragmatism</a></p>
<p>Introduction: Rob Kass’s  article  on statistical pragmatism is scheduled to appear in  Statistical Science  along with some discussions.  Here are my comments.
 
I agree with Rob Kass’s point that we can and should make use of statistical methods developed under different philosophies, and I am happy to take the opportunity to elaborate on some of his arguments.
 
I’ll discuss the following: 
- Foundations of probability 
- Confidence intervals and hypothesis tests 
- Sampling 
- Subjectivity and belief 
- Different schools of statistics
  

 
 Foundations of probability.   Kass describes probability theory as anchored upon physical randomization (coin flips, die rolls and the like) but being useful more generally as a mathematical model.  I completely agree but would also add another anchoring point:  calibration.  Calibration of probability assessments is an objective, not subjective process, although some subjectivity (or scientific judgment) is necessarily involved in the choice of events used</p><p>4 0.13990189 <a title="1095-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-30-David_Hogg_on_statistics.html">1401 andrew gelman stats-2012-06-30-David Hogg on statistics</a></p>
<p>Introduction: Data analysis recipes: Fitting a model to data :
  
We go through the many considerations involved in fitting a model to data, using as an example the fit of a straight line to a set of points in a two-dimensional plane. Standard weighted least-squares fitting is only appropriate when there is a dimension along which the data points have negligible uncertainties, and another along which all the uncertainties can be described by Gaussians of known variance; these conditions are rarely met in practice. We consider cases of general, heterogeneous, and arbitrarily covariant two-dimensional uncertainties, and situations in which there are bad data (large outliers), unknown uncertainties, and unknown but expected intrinsic scatter in the linear relationship being fit. Above all we emphasize the importance of having a “generative model” for the data, even an approximate one. Once there is a generative model, the subsequent fitting is non-arbitrary because the model permits direct computation</p><p>5 0.13927333 <a title="1095-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>Introduction: Konrad Scheffler writes:
  
I was interested by your  paper  “Induction and deduction in Bayesian data analysis” and was wondering if you would entertain a few questions:
  
  
  
 – Under the banner of objective Bayesianism, I would posit something like this as a description of Bayesian inference:


“Objective Bayesian probability is not a degree of belief (which would necessarily be subjective) but a measure of the plausibility of a hypothesis, conditional on a formally specified information state. One way of specifying a formal information state is to specify a model, which involves specifying both a prior distribution (typically for a set of unobserved variables) and a likelihood function (typically for a set of observed variables, conditioned on the values of the unobserved variables). Bayesian inference involves calculating the objective degree of plausibility of a hypothesis (typically the truth value of the hypothesis is a function of the variables mentioned above) given such a</p><p>6 0.13859548 <a title="1095-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-29-Postdocs_in_probabilistic_modeling%21__With_David_Blei%21__And_Stan%21.html">1961 andrew gelman stats-2013-07-29-Postdocs in probabilistic modeling!  With David Blei!  And Stan!</a></p>
<p>7 0.13857383 <a title="1095-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>8 0.13730413 <a title="1095-tfidf-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-23-Of_hypothesis_tests_and_Unitarians.html">1024 andrew gelman stats-2011-11-23-Of hypothesis tests and Unitarians</a></p>
<p>9 0.13702708 <a title="1095-tfidf-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-24-Empirical_implications_of_Empirical_Implications_of_Theoretical_Models.html">2263 andrew gelman stats-2014-03-24-Empirical implications of Empirical Implications of Theoretical Models</a></p>
<p>10 0.13518867 <a title="1095-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-09-Coming_to_agreement_on_philosophy_of_statistics.html">1205 andrew gelman stats-2012-03-09-Coming to agreement on philosophy of statistics</a></p>
<p>11 0.13375673 <a title="1095-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-24-Untangling_the_Jeffreys-Lindley_paradox.html">1182 andrew gelman stats-2012-02-24-Untangling the Jeffreys-Lindley paradox</a></p>
<p>12 0.12710369 <a title="1095-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-31-Lindley%E2%80%99s_paradox.html">1355 andrew gelman stats-2012-05-31-Lindley’s paradox</a></p>
<p>13 0.12324248 <a title="1095-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-25-Why_it_doesn%E2%80%99t_make_sense_in_general_to_form_confidence_intervals_by_inverting_hypothesis_tests.html">870 andrew gelman stats-2011-08-25-Why it doesn’t make sense in general to form confidence intervals by inverting hypothesis tests</a></p>
<p>14 0.12146072 <a title="1095-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Kind_of_Bayesian.html">811 andrew gelman stats-2011-07-20-Kind of Bayesian</a></p>
<p>15 0.12022828 <a title="1095-tfidf-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-04-Rob_Kass_on_statistical_pragmatism%2C_and_my_reactions.html">317 andrew gelman stats-2010-10-04-Rob Kass on statistical pragmatism, and my reactions</a></p>
<p>16 0.11788151 <a title="1095-tfidf-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-18-One-tailed_or_two-tailed%3F.html">2295 andrew gelman stats-2014-04-18-One-tailed or two-tailed?</a></p>
<p>17 0.11720714 <a title="1095-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-24-Why_it_doesn%E2%80%99t_make_sense_in_general_to_form_confidence_intervals_by_inverting_hypothesis_tests.html">1913 andrew gelman stats-2013-06-24-Why it doesn’t make sense in general to form confidence intervals by inverting hypothesis tests</a></p>
<p>18 0.1167383 <a title="1095-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>19 0.11629487 <a title="1095-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-04-Write_This_Book.html">1605 andrew gelman stats-2012-12-04-Write This Book</a></p>
<p>20 0.11542229 <a title="1095-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.204), (1, 0.138), (2, -0.022), (3, -0.028), (4, -0.055), (5, -0.008), (6, -0.042), (7, 0.013), (8, 0.054), (9, -0.065), (10, -0.049), (11, 0.014), (12, -0.028), (13, -0.081), (14, -0.048), (15, -0.011), (16, 0.002), (17, -0.047), (18, -0.009), (19, -0.027), (20, 0.04), (21, -0.028), (22, 0.039), (23, 0.01), (24, 0.036), (25, -0.004), (26, 0.018), (27, 0.015), (28, 0.008), (29, 0.04), (30, 0.01), (31, -0.036), (32, 0.023), (33, 0.01), (34, -0.075), (35, -0.052), (36, 0.07), (37, -0.021), (38, 0.041), (39, 0.051), (40, -0.042), (41, -0.009), (42, 0.027), (43, -0.027), (44, 0.002), (45, -0.014), (46, -0.028), (47, -0.008), (48, 0.001), (49, -0.03)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97807682 <a title="1095-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-01-Martin_and_Liu%3A__Probabilistic_inference_based_on_consistency_of_model_with_data.html">1095 andrew gelman stats-2012-01-01-Martin and Liu:  Probabilistic inference based on consistency of model with data</a></p>
<p>Introduction: What better way to start then new year than with some hard-core statistical theory?
 
Ryan Martin and Chuanhai Liu send along  a new paper  on inferential models:
  
Probability is a useful tool for describing uncertainty, so it is natural to strive for a system of statistical inference based on probabilities for or against various hypotheses. But existing probabilistic inference methods struggle to provide a meaningful interpretation of the probabilities across experiments in sufficient generality. In this paper we further develop a promising new approach based on what are called inferential models (IMs). The fundamental idea behind IMs is that there is an unobservable auxiliary variable that itself describes the inherent uncertainty about the parameter of interest, and that posterior probabilistic inference can be accomplished by predicting this unobserved quantity. We describe a simple and intuitive three-step construction of a random set of candidate parameter values, each being co</p><p>2 0.82385737 <a title="1095-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-15-Bayesian_statistical_pragmatism.html">662 andrew gelman stats-2011-04-15-Bayesian statistical pragmatism</a></p>
<p>Introduction: Rob Kass’s  article  on statistical pragmatism is scheduled to appear in  Statistical Science  along with some discussions.  Here are my comments.
 
I agree with Rob Kass’s point that we can and should make use of statistical methods developed under different philosophies, and I am happy to take the opportunity to elaborate on some of his arguments.
 
I’ll discuss the following: 
- Foundations of probability 
- Confidence intervals and hypothesis tests 
- Sampling 
- Subjectivity and belief 
- Different schools of statistics
  

 
 Foundations of probability.   Kass describes probability theory as anchored upon physical randomization (coin flips, die rolls and the like) but being useful more generally as a mathematical model.  I completely agree but would also add another anchoring point:  calibration.  Calibration of probability assessments is an objective, not subjective process, although some subjectivity (or scientific judgment) is necessarily involved in the choice of events used</p><p>3 0.80020553 <a title="1095-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-04-Rob_Kass_on_statistical_pragmatism%2C_and_my_reactions.html">317 andrew gelman stats-2010-10-04-Rob Kass on statistical pragmatism, and my reactions</a></p>
<p>Introduction: Rob Kass  writes :
  
Statistics has moved beyond the frequentist-Bayesian controversies of the past. Where does this leave our ability to interpret results? I [Kass] suggest that a philosophy compatible with statistical practice, labeled here statistical pragmatism, serves as a foundation for inference. Statistical pragmatism is inclusive and emphasizes the assumptions that connect statistical models with observed data. I argue that introductory courses often mis-characterize the process of statistical inference and I propose an alternative “big picture” depiction.
  
In  my comments , I pretty much agree with everything Rob says, with a few points of elaboration:
  
Kass describes probability theory as anchored upon physical randomization (coin flips, die rolls and the like) but being useful more generally as a mathematical model.  I completely agree but would also add another anchoring point:  calibration.  Calibration of probability assessments is an objective, not subjective proce</p><p>4 0.79520273 <a title="1095-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-31-Lindley%E2%80%99s_paradox.html">1355 andrew gelman stats-2012-05-31-Lindley’s paradox</a></p>
<p>Introduction: Sam Seaver writes:
  
I [Seaver] happened to be reading an ironic  article  by Karl Friston when I learned something new about frequentist vs bayesian, namely Lindley’s paradox, on page 12.  The text is as follows:

 
So why are we worried about trivial effects? They are important because the probability that the true effect size is exactly zero is itself zero and could cause us to reject the null hypothesis inappropriately. This is a fallacy of classical inference and is not unrelated to Lindley’s paradox (Lindley 1957). Lindley’s paradox describes a counterintuitive situation in which Bayesian and frequentist approaches to hypothesis testing give opposite results. It occurs when; (i) a result is significant by a frequentist test, indicating sufficient evidence to reject the null hypothesis d=0 and (ii) priors render the posterior probability of d=0 high, indicating strong evidence that the null hypothesis is true. In his original 
treatment, Lindley (1957) showed that – under a parti</p><p>5 0.74224788 <a title="1095-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-08-P-values_and_statistical_practice.html">1713 andrew gelman stats-2013-02-08-P-values and statistical practice</a></p>
<p>Introduction: From  my new article  in the journal Epidemiology:
  
Sander Greenland and Charles Poole accept that P values are here to stay but recognize that some of their most common interpretations have problems. The casual view of the P value as posterior probability of the truth of the null hypothesis is false and not even close to valid under any reasonable model, yet this misunderstanding persists even in high-stakes settings (as discussed, for example, by Greenland in 2011). The formal view of the P value as a probability conditional on the null is mathematically correct but typically irrelevant to research goals (hence, the popularity of alternative—if wrong—interpretations). A Bayesian interpretation based on a spike-and-slab model makes little sense in applied contexts in epidemiology, political science, and other fields in which true effects are typically nonzero and bounded (thus violating both the “spike” and the “slab” parts of the model).


I find Greenland and Poole’s perspective t</p><p>6 0.74113214 <a title="1095-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-18-Understanding_posterior_p-values.html">2029 andrew gelman stats-2013-09-18-Understanding posterior p-values</a></p>
<p>7 0.73498839 <a title="1095-lsi-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-24-Empirical_implications_of_Empirical_Implications_of_Theoretical_Models.html">2263 andrew gelman stats-2014-03-24-Empirical implications of Empirical Implications of Theoretical Models</a></p>
<p>8 0.73091942 <a title="1095-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-04-Multiple_comparisons_dispute_in_the_tabloids.html">1195 andrew gelman stats-2012-03-04-Multiple comparisons dispute in the tabloids</a></p>
<p>9 0.72943622 <a title="1095-lsi-9" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Ken_Rice_presents_a_unifying_approach_to_statistical_inference_and_hypothesis_testing.html">2312 andrew gelman stats-2014-04-29-Ken Rice presents a unifying approach to statistical inference and hypothesis testing</a></p>
<p>10 0.70292407 <a title="1095-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-26-%E2%80%9CThe_Bayesian_approach_to_forensic_evidence%E2%80%9D.html">2078 andrew gelman stats-2013-10-26-“The Bayesian approach to forensic evidence”</a></p>
<p>11 0.70273679 <a title="1095-lsi-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>12 0.69081992 <a title="1095-lsi-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-22-Philosophy_of_Bayes_and_non-Bayes%3A__A_dialogue_with_Deborah_Mayo.html">291 andrew gelman stats-2010-09-22-Philosophy of Bayes and non-Bayes:  A dialogue with Deborah Mayo</a></p>
<p>13 0.68651778 <a title="1095-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-26-Statistical_evidence_for_revised_standards.html">2149 andrew gelman stats-2013-12-26-Statistical evidence for revised standards</a></p>
<p>14 0.68598694 <a title="1095-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-23-Of_hypothesis_tests_and_Unitarians.html">1024 andrew gelman stats-2011-11-23-Of hypothesis tests and Unitarians</a></p>
<p>15 0.68282467 <a title="1095-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-25-Classical_probability_does_not_apply_to_quantum_systems_%28causal_inference_edition%29.html">2037 andrew gelman stats-2013-09-25-Classical probability does not apply to quantum systems (causal inference edition)</a></p>
<p>16 0.67186296 <a title="1095-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-06-Josh_Tenenbaum_presents_._._._a_model_of_folk_physics%21.html">994 andrew gelman stats-2011-11-06-Josh Tenenbaum presents . . . a model of folk physics!</a></p>
<p>17 0.67129934 <a title="1095-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-17-Where_do_theories_come_from%3F.html">1861 andrew gelman stats-2013-05-17-Where do theories come from?</a></p>
<p>18 0.67004818 <a title="1095-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-12-Thinking_like_a_statistician_%28continuously%29_rather_than_like_a_civilian_%28discretely%29.html">1575 andrew gelman stats-2012-11-12-Thinking like a statistician (continuously) rather than like a civilian (discretely)</a></p>
<p>19 0.66967952 <a title="1095-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-22-Deviance%2C_DIC%2C_AIC%2C_cross-validation%2C_etc.html">776 andrew gelman stats-2011-06-22-Deviance, DIC, AIC, cross-validation, etc</a></p>
<p>20 0.66859823 <a title="1095-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-16-Long_discussion_about_causal_inference_and_the_use_of_hierarchical_models_to_bridge_between_different_inferential_settings.html">1418 andrew gelman stats-2012-07-16-Long discussion about causal inference and the use of hierarchical models to bridge between different inferential settings</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(15, 0.033), (16, 0.058), (21, 0.015), (24, 0.134), (40, 0.027), (76, 0.01), (86, 0.066), (89, 0.016), (99, 0.501)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99666709 <a title="1095-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-01-Martin_and_Liu%3A__Probabilistic_inference_based_on_consistency_of_model_with_data.html">1095 andrew gelman stats-2012-01-01-Martin and Liu:  Probabilistic inference based on consistency of model with data</a></p>
<p>Introduction: What better way to start then new year than with some hard-core statistical theory?
 
Ryan Martin and Chuanhai Liu send along  a new paper  on inferential models:
  
Probability is a useful tool for describing uncertainty, so it is natural to strive for a system of statistical inference based on probabilities for or against various hypotheses. But existing probabilistic inference methods struggle to provide a meaningful interpretation of the probabilities across experiments in sufficient generality. In this paper we further develop a promising new approach based on what are called inferential models (IMs). The fundamental idea behind IMs is that there is an unobservable auxiliary variable that itself describes the inherent uncertainty about the parameter of interest, and that posterior probabilistic inference can be accomplished by predicting this unobserved quantity. We describe a simple and intuitive three-step construction of a random set of candidate parameter values, each being co</p><p>2 0.99408883 <a title="1095-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<p>Introduction: In  this discussion  from last month, computer science student and Judea Pearl collaborator Elias Barenboim expressed an attitude that hierarchical Bayesian methods might be fine in practice but that they lack theory, that Bayesians can’t succeed in toy problems.  I posted a P.S. there which might not have been noticed so I will put it here:
 
I now realize that there is some disagreement about what constitutes a “guarantee.”  In one of his comments, Barenboim writes, “the assurance we have that the result must hold as long as the assumptions in the model are correct should be regarded as a guarantee.”  In that sense, yes, we have guarantees!  It is fundamental to Bayesian inference that the result must hold if the assumptions in the model are correct.  We have lots of that in Bayesian Data Analysis (particularly in the first four chapters but implicitly elsewhere as well), and this is also covered in the classic books by Lindley, Jaynes, and others.  This sort of guarantee is indeed p</p><p>3 0.99366653 <a title="1095-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-05-A_locally_organized_online_BDA_course_on_G%2B_hangout%3F.html">2009 andrew gelman stats-2013-09-05-A locally organized online BDA course on G+ hangout?</a></p>
<p>Introduction: Eoin Lawless wrote me:
  
I’ve been reading your blog (and  John Kruschke ‘s) for several months now, as a result of starting to learn Bayesian methods from Doing Bayesian Data Analysis [I love the title of that book! --- ed.]. More recently I completed a Coursera course on Data Science. 


I found learning through the medium of a online course to be an amazing experience. It does not replace books, but learning new material at the same time as other people and discussing it in the forums is very motivational. Additionally it is much easier to work through exercises and projects when there is a deadline and some element of competition than to plow through the end of chapter exercises in a book. This is especially true, I believe, when the learning is for a long term goal, rather than to be used immediately in work, for example.


My question: you are obviously evangelical about the benefits that Bayesian statistics brings, have you ever considered producing a Coursera (or similar) cour</p><p>4 0.99302822 <a title="1095-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-07-Looking_for_a_purpose_in_life%3A__Update_on_that_underworked_and_overpaid_sociologist_whose_%E2%80%9Cmain_task_as_a_university_professor_was_self-cultivation%E2%80%9D.html">750 andrew gelman stats-2011-06-07-Looking for a purpose in life:  Update on that underworked and overpaid sociologist whose “main task as a university professor was self-cultivation”</a></p>
<p>Introduction: After  posting  on David Rubinstein’s remarks on his “cushy life” as a sociology professor at a public university, I read  these remarks  by some of Rubinstein’s colleagues at the University of Illinois, along with a response from Rubinstein.
 
Before getting to the policy issues, let me first say that I think it must have been so satisfying, first for Rubinstein and then for his colleagues (Barbara Risman, William Bridges, and Anthony Orum) to publish these notes.  We all have people we know and hate, but we rarely have a good excuse for blaring our feelings in public. (I remember when I was up for tenure, I was able to read the outside letters on my case (it’s a public university and they have rules), and one of the letter writers really hated my guts.  I was surprised–I didn’t know the guy well (the letters were anonymized but it was clear from context who the letter writer was) but the few times we’d met, he’d been cordial enough–but there you have it.  He must have been thrilled t</p><p>5 0.99299294 <a title="1095-lda-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-21-Random_matrices_in_the_news.html">2258 andrew gelman stats-2014-03-21-Random matrices in the news</a></p>
<p>Introduction: From 2010 :
  
Mark Buchanan wrote  a cover article  for the New Scientist on random matrices, a heretofore obscure area of probability theory that his headline writer characterizes as “the deep law that shapes our reality.”


It’s interesting stuff, and he gets into some statistical applications at the end, so I’ll give you my take on it.


But first, some background.


About two hundred years ago, the mathematician/physicist Laplace discovered what is now called the central limit theorem, which is that, under certain conditions, the average of a large number of small random variables has an approximate normal (bell-shaped) distribution. A bit over 100 years ago, social scientists such as Galton applied this theorem to all sorts of biological and social phenomena. The central limit theorem, in its generality, is also important in the information that it indirectly conveys when it fails.


For example, the distribution of the heights of adult men or women is nicely bell-shaped, but the</p><p>6 0.99295789 <a title="1095-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-11-Why_waste_time_philosophizing%3F.html">1719 andrew gelman stats-2013-02-11-Why waste time philosophizing?</a></p>
<p>7 0.99271345 <a title="1095-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-10-Controversy_over_the_Christakis-Fowler_findings_on_the_contagion_of_obesity.html">757 andrew gelman stats-2011-06-10-Controversy over the Christakis-Fowler findings on the contagion of obesity</a></p>
<p>8 0.99266416 <a title="1095-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-01-%E2%80%9COn_Inspiring_Students_and_Being_Human%E2%80%9D.html">1517 andrew gelman stats-2012-10-01-“On Inspiring Students and Being Human”</a></p>
<p>9 0.99250436 <a title="1095-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-28-Using_randomized_incentives_as_an_instrument_for_survey_nonresponse%3F.html">2152 andrew gelman stats-2013-12-28-Using randomized incentives as an instrument for survey nonresponse?</a></p>
<p>10 0.99228418 <a title="1095-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-17-Data_problems%2C_coding_errors%E2%80%A6what_can_be_done%3F.html">1807 andrew gelman stats-2013-04-17-Data problems, coding errors…what can be done?</a></p>
<p>11 0.99212134 <a title="1095-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-07-Feedback_on_my_Bayesian_Data_Analysis_class_at_Columbia.html">1611 andrew gelman stats-2012-12-07-Feedback on my Bayesian Data Analysis class at Columbia</a></p>
<p>12 0.99210238 <a title="1095-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-%E2%80%9CIs_machine_learning_a_subset_of_statistics%3F%E2%80%9D.html">1740 andrew gelman stats-2013-02-26-“Is machine learning a subset of statistics?”</a></p>
<p>13 0.99177283 <a title="1095-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-29-We_go_to_war_with_the_data_we_have%2C_not_the_data_we_want.html">1289 andrew gelman stats-2012-04-29-We go to war with the data we have, not the data we want</a></p>
<p>14 0.99164754 <a title="1095-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-21-The_future_%28and_past%29_of_statistical_sciences.html">2072 andrew gelman stats-2013-10-21-The future (and past) of statistical sciences</a></p>
<p>15 0.99143052 <a title="1095-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-10-Bayes_jumps_the_shark.html">331 andrew gelman stats-2010-10-10-Bayes jumps the shark</a></p>
<p>16 0.99113697 <a title="1095-lda-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-22-That_claim_that_students_whose_parents_pay_for_more_of_college_get_worse_grades.html">1688 andrew gelman stats-2013-01-22-That claim that students whose parents pay for more of college get worse grades</a></p>
<p>17 0.99077863 <a title="1095-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-05-Watership_Down%2C_thick_description%2C_applied_statistics%2C_immutability_of_stories%2C_and_playing_tennis_with_a_net.html">1750 andrew gelman stats-2013-03-05-Watership Down, thick description, applied statistics, immutability of stories, and playing tennis with a net</a></p>
<p>18 0.99076438 <a title="1095-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>19 0.99063349 <a title="1095-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-26-Lottery_probability_update.html">731 andrew gelman stats-2011-05-26-Lottery probability update</a></p>
<p>20 0.99057221 <a title="1095-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-14-Statistics_for_firefighters%3A__update.html">1722 andrew gelman stats-2013-02-14-Statistics for firefighters:  update</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
