<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1518 andrew gelman stats-2012-10-02-Fighting a losing battle</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1518" href="#">andrew_gelman_stats-2012-1518</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1518 andrew gelman stats-2012-10-02-Fighting a losing battle</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1518-html" href="http://andrewgelman.com/2012/10/02/fighting-a-losing-battle/">html</a></p><p>Introduction: Following a recent email exchange regarding path sampling and thermodynamic integration (sadly, I’ve gotten rusty and haven’t thought seriously about these challenges for many years), a correspondent referred to the marginal distribution of the data under a model as “the evidence.”
 
I hate that expression!  As we discuss in chapter 6 of BDA, for continuous-parametered models, this quantity can be completely sensitive to aspects of the prior that have essentially no impact on the posterior.  In the examples I’ve seen, this marginal probability is not “evidence” in any useful sense of the term.
 
When I told this to my correspondent, he replied,
  
 I actually don’t find “the evidence” too bothersome.  I don’t have BDA at home where I’m working from at the moment, so I’ll read up on chapter 6 later, but I assume you refer to the problem of the marginal likelihood being strongly sensitive to the prior in a way that the posterior typically isn’t, thereby diminishing the value of the margi</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Following a recent email exchange regarding path sampling and thermodynamic integration (sadly, I’ve gotten rusty and haven’t thought seriously about these challenges for many years), a correspondent referred to the marginal distribution of the data under a model as “the evidence. [sent-1, score-1.613]
</p><p>2 As we discuss in chapter 6 of BDA, for continuous-parametered models, this quantity can be completely sensitive to aspects of the prior that have essentially no impact on the posterior. [sent-3, score-0.822]
</p><p>3 In the examples I’ve seen, this marginal probability is not “evidence” in any useful sense of the term. [sent-4, score-0.376]
</p><p>4 If so, I understand, but I think you might be fighting a losing battle as “the evidence” is seemingly now popular in the stats literature as well as the physics …    I replied that I’ll fight that battle forever. [sent-7, score-1.432]
</p><p>5 I really really hate the use of linguistically-loaded terms such as “bias,” “evidence,” “empirical Bayes,” etc. [sent-8, score-0.178]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('marginal', 0.376), ('battle', 0.243), ('correspondent', 0.225), ('evidence', 0.219), ('bda', 0.214), ('sensitive', 0.212), ('hate', 0.178), ('thermodynamic', 0.162), ('replied', 0.16), ('likelihood', 0.151), ('chapter', 0.145), ('diminishing', 0.141), ('sadly', 0.138), ('thereby', 0.126), ('fighting', 0.126), ('statistic', 0.121), ('losing', 0.117), ('prior', 0.117), ('integration', 0.112), ('fight', 0.111), ('gotten', 0.109), ('quantity', 0.107), ('expression', 0.107), ('referred', 0.106), ('seemingly', 0.104), ('path', 0.101), ('exchange', 0.1), ('stats', 0.099), ('refer', 0.098), ('challenges', 0.096), ('moment', 0.092), ('motivated', 0.086), ('physics', 0.085), ('aspects', 0.084), ('home', 0.083), ('impact', 0.082), ('seriously', 0.081), ('empirical', 0.079), ('ll', 0.078), ('selection', 0.078), ('bias', 0.077), ('strongly', 0.077), ('choose', 0.076), ('bayes', 0.076), ('essentially', 0.075), ('sampling', 0.074), ('etc', 0.073), ('popular', 0.072), ('well', 0.072), ('email', 0.071)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1518-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-02-Fighting_a_losing_battle.html">1518 andrew gelman stats-2012-10-02-Fighting a losing battle</a></p>
<p>Introduction: Following a recent email exchange regarding path sampling and thermodynamic integration (sadly, I’ve gotten rusty and haven’t thought seriously about these challenges for many years), a correspondent referred to the marginal distribution of the data under a model as “the evidence.”
 
I hate that expression!  As we discuss in chapter 6 of BDA, for continuous-parametered models, this quantity can be completely sensitive to aspects of the prior that have essentially no impact on the posterior.  In the examples I’ve seen, this marginal probability is not “evidence” in any useful sense of the term.
 
When I told this to my correspondent, he replied,
  
 I actually don’t find “the evidence” too bothersome.  I don’t have BDA at home where I’m working from at the moment, so I’ll read up on chapter 6 later, but I assume you refer to the problem of the marginal likelihood being strongly sensitive to the prior in a way that the posterior typically isn’t, thereby diminishing the value of the margi</p><p>2 0.2501134 <a title="1518-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-15-More_on_AIC%2C_WAIC%2C_etc.html">1983 andrew gelman stats-2013-08-15-More on AIC, WAIC, etc</a></p>
<p>Introduction: Following up on our  discussion  from the other day, Angelika van der Linde sends along  this paper  from 2012 (link to journal  here ).
 
And Aki pulls out this great quote from Geisser and Eddy (1979):
  
This discussion makes clear that in the nested case this method, as Akaike’s, is not consistent; i.e., even if $M_k$ is true, it will be rejected with probability $\alpha$ as $N\to\infty$. This point is also made by Schwarz (1978).  However, from the point of view of prediction, this is of no great consequence. For large numbers of observations, a prediction based on the falsely assumed $M_k$, will not differ appreciably from one based on the true $M_k$.  For example, if we assert that two normal populations have different means when in fact they have the same mean, then the use of the group mean as opposed to the grand mean for predicting a future observation results in predictors which are asymptotically equivalent and whose predictive variances are $\sigma^2[1 + (1/2n)]$ and $\si</p><p>3 0.2123778 <a title="1518-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>Introduction: For awhile I’ve been fitting most of my multilevel models using lmer/glmer, which gives point estimates of the group-level variance parameters (maximum marginal likelihood estimate for lmer and an approximation for glmer).  I’m usually satisfied with this–sure, point estimation understates the uncertainty in model fitting, but that’s typically the least of our worries. 
 
Sometimes, though, lmer/glmer estimates group-level variances at 0 or estimates group-level correlation parameters at +/- 1.  Typically, when this happens, it’s not that we’re so sure the variance is close to zero or that the correlation is close to 1 or -1; rather, the marginal likelihood does not provide a lot of information about these parameters of the group-level error distribution.
 
I don’t want point estimates on the boundary.  I don’t want to say that the unexplained variance in some dimension is exactly zero.
 
One way to handle this problem is full Bayes:  slap a prior on sigma, do your Gibbs and Metropolis</p><p>4 0.17180885 <a title="1518-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>Introduction: I’ve had a couple of email conversations in the past couple days on dependence in multivariate prior distributions.
 
 Modeling the degrees of freedom and scale parameters in the t distribution 
 
First, in our Stan group we’ve been discussing the choice of priors for the degrees-of-freedom parameter in the t distribution.  I wrote that also there’s the question of parameterization.  It does not necessarily make sense to have independent priors on the df and scale parameters.  In some sense, the meaning of the scale parameter changes with the df.
 
 Prior dependence between correlation and scale parameters in the scaled inverse-Wishart model 
 
The second case of parameterization in prior distribution arose from an email I received from Chris Chatham pointing me to  this exploration  by Matt Simpson of the scaled inverse-Wishart prior distribution for hierarchical covariance matrices.  Simpson writes:
  
A popular prior for Σ is the inverse-Wishart distribution [ not  the same as the</p><p>5 0.15761912 <a title="1518-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>Introduction: In response to  this article  by Cosma Shalizi and myself on the philosophy of Bayesian statistics, David Hogg writes:
  
I [Hogg] agree–even in physics and astronomy–that the models are not “True” in the God-like sense of being absolute reality (that is, I am not a realist); and I  have argued  (a philosophically very naive 
paper, but hey, I was new to all this) that for pretty fundamental reasons we could never arrive at the True (with a capital “T”) model of the Universe.  The goal of inference is to find the “best” model, where “best” might have something to do with prediction, or explanation, or message length, or (horror!) our utility.  Needless to say, most of my physics friends *are* realists, even in the face of “effective theories” as Newtonian mechanics is an effective theory of GR and GR is an effective theory of “quantum gravity” (this plays to your point, because if you think any theory is possibly an effective theory, how could you ever find Truth?).  I also liked the i</p><p>6 0.14841338 <a title="1518-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-14-Progress%21__%28on_the_understanding_of_the_role_of_randomization_in_Bayesian_inference%29.html">1898 andrew gelman stats-2013-06-14-Progress!  (on the understanding of the role of randomization in Bayesian inference)</a></p>
<p>7 0.13274772 <a title="1518-tfidf-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>8 0.13173419 <a title="1518-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>9 0.12490258 <a title="1518-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>10 0.12281869 <a title="1518-tfidf-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-14-Confusion_about_continuous_probability_densities.html">341 andrew gelman stats-2010-10-14-Confusion about continuous probability densities</a></p>
<p>11 0.12203448 <a title="1518-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>12 0.11544949 <a title="1518-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-14-Uh-oh.html">612 andrew gelman stats-2011-03-14-Uh-oh</a></p>
<p>13 0.11511656 <a title="1518-tfidf-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-26-Is_a_steal_really_worth_9_points%3F.html">2267 andrew gelman stats-2014-03-26-Is a steal really worth 9 points?</a></p>
<p>14 0.11338298 <a title="1518-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>15 0.11300824 <a title="1518-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>16 0.10955085 <a title="1518-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-11-%E2%80%9CInformative_g-Priors_for_Logistic_Regression%E2%80%9D.html">2017 andrew gelman stats-2013-09-11-“Informative g-Priors for Logistic Regression”</a></p>
<p>17 0.10919419 <a title="1518-tfidf-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-12-Update_on_Mankiw%E2%80%99s_work_incentives.html">338 andrew gelman stats-2010-10-12-Update on Mankiw’s work incentives</a></p>
<p>18 0.10895004 <a title="1518-tfidf-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-06-Quote_of_the_day.html">398 andrew gelman stats-2010-11-06-Quote of the day</a></p>
<p>19 0.10569572 <a title="1518-tfidf-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-11-Mankiw%E2%80%99s_marginal_tax_rate_%28which_declined_from_93%25_to_80%25_in_two_years%29_and_the_difficulty_of_microeconomic_reasoning.html">336 andrew gelman stats-2010-10-11-Mankiw’s marginal tax rate (which declined from 93% to 80% in two years) and the difficulty of microeconomic reasoning</a></p>
<p>20 0.10521974 <a title="1518-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-04-Insecure_researchers_aren%E2%80%99t_sharing_their_data.html">991 andrew gelman stats-2011-11-04-Insecure researchers aren’t sharing their data</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.173), (1, 0.109), (2, -0.004), (3, 0.037), (4, -0.048), (5, -0.033), (6, 0.094), (7, 0.003), (8, -0.025), (9, -0.012), (10, -0.032), (11, 0.01), (12, 0.0), (13, 0.012), (14, -0.048), (15, -0.038), (16, 0.005), (17, 0.001), (18, -0.005), (19, -0.001), (20, 0.076), (21, -0.049), (22, 0.005), (23, 0.035), (24, -0.023), (25, 0.027), (26, 0.019), (27, 0.01), (28, 0.064), (29, -0.006), (30, -0.053), (31, 0.029), (32, -0.013), (33, 0.039), (34, -0.022), (35, 0.017), (36, 0.048), (37, -0.035), (38, 0.001), (39, 0.0), (40, -0.002), (41, -0.011), (42, 0.024), (43, -0.06), (44, 0.021), (45, -0.01), (46, -0.025), (47, 0.058), (48, 0.027), (49, -0.035)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96060389 <a title="1518-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-02-Fighting_a_losing_battle.html">1518 andrew gelman stats-2012-10-02-Fighting a losing battle</a></p>
<p>Introduction: Following a recent email exchange regarding path sampling and thermodynamic integration (sadly, I’ve gotten rusty and haven’t thought seriously about these challenges for many years), a correspondent referred to the marginal distribution of the data under a model as “the evidence.”
 
I hate that expression!  As we discuss in chapter 6 of BDA, for continuous-parametered models, this quantity can be completely sensitive to aspects of the prior that have essentially no impact on the posterior.  In the examples I’ve seen, this marginal probability is not “evidence” in any useful sense of the term.
 
When I told this to my correspondent, he replied,
  
 I actually don’t find “the evidence” too bothersome.  I don’t have BDA at home where I’m working from at the moment, so I’ll read up on chapter 6 later, but I assume you refer to the problem of the marginal likelihood being strongly sensitive to the prior in a way that the posterior typically isn’t, thereby diminishing the value of the margi</p><p>2 0.80034709 <a title="1518-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-15-More_on_AIC%2C_WAIC%2C_etc.html">1983 andrew gelman stats-2013-08-15-More on AIC, WAIC, etc</a></p>
<p>Introduction: Following up on our  discussion  from the other day, Angelika van der Linde sends along  this paper  from 2012 (link to journal  here ).
 
And Aki pulls out this great quote from Geisser and Eddy (1979):
  
This discussion makes clear that in the nested case this method, as Akaike’s, is not consistent; i.e., even if $M_k$ is true, it will be rejected with probability $\alpha$ as $N\to\infty$. This point is also made by Schwarz (1978).  However, from the point of view of prediction, this is of no great consequence. For large numbers of observations, a prediction based on the falsely assumed $M_k$, will not differ appreciably from one based on the true $M_k$.  For example, if we assert that two normal populations have different means when in fact they have the same mean, then the use of the group mean as opposed to the grand mean for predicting a future observation results in predictors which are asymptotically equivalent and whose predictive variances are $\sigma^2[1 + (1/2n)]$ and $\si</p><p>3 0.75130028 <a title="1518-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-18-Understanding_posterior_p-values.html">2029 andrew gelman stats-2013-09-18-Understanding posterior p-values</a></p>
<p>Introduction: David Kaplan writes:
  
I came across your  paper  “Understanding Posterior Predictive P-values”, and I have a question regarding your statement “If a posterior predictive p-value is 0.4, say, that means that, if we believe the model, we think there is a 40% chance that tomorrow’s value of T(y_rep) will exceed today’s T(y).” This is perfectly understandable to me and represents the idea of calibration.  However, I am unsure how this relates to statements about fit.  If T is the LR chi-square or Pearson chi-square, then your statement that there is a 40% chance that tomorrows value exceeds today’s value indicates bad fit, I think.  Yet, some literature indicates that high p-values suggest good fit.  Could you clarify this?
  
My reply:
 
I think that “fit” depends on the question being asked.  In this case, I’d say the model fits for this particular purpose, even though it might not fit for other purposes.
 
And here’s the abstract of the paper:
  
Posterior predictive p-values do not i</p><p>4 0.73677182 <a title="1518-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-08-P-values_and_statistical_practice.html">1713 andrew gelman stats-2013-02-08-P-values and statistical practice</a></p>
<p>Introduction: From  my new article  in the journal Epidemiology:
  
Sander Greenland and Charles Poole accept that P values are here to stay but recognize that some of their most common interpretations have problems. The casual view of the P value as posterior probability of the truth of the null hypothesis is false and not even close to valid under any reasonable model, yet this misunderstanding persists even in high-stakes settings (as discussed, for example, by Greenland in 2011). The formal view of the P value as a probability conditional on the null is mathematically correct but typically irrelevant to research goals (hence, the popularity of alternative—if wrong—interpretations). A Bayesian interpretation based on a spike-and-slab model makes little sense in applied contexts in epidemiology, political science, and other fields in which true effects are typically nonzero and bounded (thus violating both the “spike” and the “slab” parts of the model).


I find Greenland and Poole’s perspective t</p><p>5 0.71678537 <a title="1518-lsi-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>Introduction: I received the following email:
  
I have an interesting thought on a prior for a logistic regression, and would love your input on how to make it “work.”


Some of my research, two published papers, are on mathematical models of **.  Along those lines, I’m interested in developing more models for **. . . .  Empirical studies show that the public is rather smart and that the wisdom-of-the-crowd is fairly accurate.


So, my thought would be to tread the public’s probability of the event as a prior, and then see how adding data, through a model, would change or perturb our inferred probability of **.  (Similarly, I could envision using previously published epidemiological research as a prior probability of a disease, and then seeing how the addition of new testing protocols would update that belief.)


However, everything I learned about hierarchical Bayesian models has a prior as a distribution on the coefficients.  I don’t know how to start with a prior point estimate for the probabili</p><p>6 0.71193975 <a title="1518-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>7 0.7050581 <a title="1518-lsi-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>8 0.68465114 <a title="1518-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-07-Chi-square_FAIL_when_many_cells_have_small_expected_values.html">996 andrew gelman stats-2011-11-07-Chi-square FAIL when many cells have small expected values</a></p>
<p>9 0.68112993 <a title="1518-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-11-Gelman_on_Hennig_on_Gelman_on_Bayes.html">1208 andrew gelman stats-2012-03-11-Gelman on Hennig on Gelman on Bayes</a></p>
<p>10 0.6776005 <a title="1518-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>11 0.67540497 <a title="1518-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>12 0.66911316 <a title="1518-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-19-Prior_distributions_on_derived_quantities_rather_than_on_parameters_themselves.html">1946 andrew gelman stats-2013-07-19-Prior distributions on derived quantities rather than on parameters themselves</a></p>
<p>13 0.66877407 <a title="1518-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>14 0.66098809 <a title="1518-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-16-Update_on_the_generalized_method_of_moments.html">519 andrew gelman stats-2011-01-16-Update on the generalized method of moments</a></p>
<p>15 0.65815431 <a title="1518-lsi-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-28-Another_argument_in_favor_of_expressing_conditional_probability_statements_using_the_population_distribution.html">56 andrew gelman stats-2010-05-28-Another argument in favor of expressing conditional probability statements using the population distribution</a></p>
<p>16 0.65016162 <a title="1518-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>17 0.6458376 <a title="1518-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>18 0.6451813 <a title="1518-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-15-Wacky_priors_can_work_well%3F.html">1723 andrew gelman stats-2013-02-15-Wacky priors can work well?</a></p>
<p>19 0.64357203 <a title="1518-lsi-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-06-Quote_of_the_day.html">398 andrew gelman stats-2010-11-06-Quote of the day</a></p>
<p>20 0.64085156 <a title="1518-lsi-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-14-Confusion_about_continuous_probability_densities.html">341 andrew gelman stats-2010-10-14-Confusion about continuous probability densities</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(16, 0.078), (21, 0.062), (24, 0.167), (27, 0.064), (48, 0.074), (63, 0.022), (66, 0.012), (86, 0.089), (97, 0.04), (99, 0.293)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97486353 <a title="1518-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-02-Fighting_a_losing_battle.html">1518 andrew gelman stats-2012-10-02-Fighting a losing battle</a></p>
<p>Introduction: Following a recent email exchange regarding path sampling and thermodynamic integration (sadly, I’ve gotten rusty and haven’t thought seriously about these challenges for many years), a correspondent referred to the marginal distribution of the data under a model as “the evidence.”
 
I hate that expression!  As we discuss in chapter 6 of BDA, for continuous-parametered models, this quantity can be completely sensitive to aspects of the prior that have essentially no impact on the posterior.  In the examples I’ve seen, this marginal probability is not “evidence” in any useful sense of the term.
 
When I told this to my correspondent, he replied,
  
 I actually don’t find “the evidence” too bothersome.  I don’t have BDA at home where I’m working from at the moment, so I’ll read up on chapter 6 later, but I assume you refer to the problem of the marginal likelihood being strongly sensitive to the prior in a way that the posterior typically isn’t, thereby diminishing the value of the margi</p><p>2 0.95827413 <a title="1518-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-30-%3F%3F%3F.html">2118 andrew gelman stats-2013-11-30-???</a></p>
<p>Introduction: I received the following unsolicited email, subject line Technology and Engineering Research:
  
Dear Editor 


We have done research in some of the cutting edge technology and engineering field and would like to if you will be able to write about it in your news section. 
Our Primarily research focus on building high performance systems that are helping in social networks, web, finding disease, cancer and sports using BIG DATA . Hope to hear from you some time soon. 


Thanks, 


***, PhD 
Chartered Scientist 
IBM Corportation 
***@us.ibm.com 
916 *** ****
  
I thought IBM was a professional operation—don’t they have their own public relations department?</p><p>3 0.94741279 <a title="1518-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-31-Dispute_about_ethics_of_data_sharing.html">1238 andrew gelman stats-2012-03-31-Dispute about ethics of data sharing</a></p>
<p>Introduction: Several months ago, Sam Behseta, the new editor of Chance magazine, asked me if I’d like to have a column.  I said yes, I’d like to write on ethics and statistics.  My  first column  was called “Open Data and Open Methods” and I discussed the ethical obligation to share data and make our computations transparent wherever possible.  In my column, I recounted a story from a bit over 20 years ago when I noticed a problem in a published analysis (involving electromagnetic fields and calcium flow in chicken brains) and contacted the researcher in charge of the study, who would not share his data with me.
 
Two of the people from that research team—biologist Carl Blackman and statistician Dennis House—saw my Chance column and felt that I had misrepresented the situation and had criticized them unfairly.
 
Blackman and House expressed their concerns in letters to the editor which were just published, along with my reply, in the latest issue of  Chance .
 
Seeing as I posted my article here, I</p><p>4 0.94467318 <a title="1518-lda-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-17-Futures_contracts%2C_Granger_causality%2C_and_my_preference_for_estimation_to_testing.html">212 andrew gelman stats-2010-08-17-Futures contracts, Granger causality, and my preference for estimation to testing</a></p>
<p>Introduction: José Iparraguirre writes:
  
There’s  a letter  in the latest issue of The Economist (July 31st) signed by Sir Richard Branson (Virgin), Michael Masters (Masters Capital Management) and David Frenk (Better Markets) about an    “>OECD report  on speculation and the prices of commodities, which includes the following: “The report uses a Granger causality test to measure the relationship between the level of commodities futures contracts held by swap dealers, and the prices of those commodities. Granger tests, however, are of dubious applicability to extremely volatile variables like commodities prices.”
  
The report says:
  
Granger causality is a standard statistical technique for determining whether one time series is useful in forecasting another. It is important to bear in mind that the term causality is used in a statistical sense, and not in a philosophical one of structural causation. More precisely a variable A is said to Granger cause B if knowing the time paths of B and A toge</p><p>5 0.94409704 <a title="1518-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-23-%E2%80%9CAny_old_map_will_do%E2%80%9D_meets_%E2%80%9CGod_is_in_every_leaf_of_every_tree%E2%80%9D.html">1278 andrew gelman stats-2012-04-23-“Any old map will do” meets “God is in every leaf of every tree”</a></p>
<p>Introduction: As a statistician I am particularly worried about the rhetorical power of anecdotes (even though I use them in my own reasoning; see discussion below).  But much can be learned from a  true  anecdote.  The rough edges—the places where the anecdote doesn’t fit your thesis—these are where you learn.
 
We have recently had a discussion ( here  and  here ) of Karl Weick, a prominent scholar of business management who plagiarized a story and then went on to draw different lessons from the pilfered anecdote in several different publications published over many years.
 
Setting aside an issues of plagiarism and rulebreaking, I argue that, by hiding the source of the story and changing its form, Weick and his management-science audience are losing their ability to get anything out of it beyond empty confirmation.
 
A full discussion follows.
 
 1.  The lost Hungarian soldiers 
 
Thomas Basbøll (who has the unusual (to me) job of “writing consultant” at the Copenhagen Business School) has been</p><p>6 0.94265175 <a title="1518-lda-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>7 0.94177413 <a title="1518-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>8 0.94105846 <a title="1518-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-16-Another_day%2C_another_plagiarist.html">1266 andrew gelman stats-2012-04-16-Another day, another plagiarist</a></p>
<p>9 0.94066274 <a title="1518-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-22-Struggles_over_the_criticism_of_the_%E2%80%9Ccannabis_users_and_IQ_change%E2%80%9D_paper.html">1910 andrew gelman stats-2013-06-22-Struggles over the criticism of the “cannabis users and IQ change” paper</a></p>
<p>10 0.9401018 <a title="1518-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-23-Combining_survey_data_obtained_using_different_modes_of_sampling.html">777 andrew gelman stats-2011-06-23-Combining survey data obtained using different modes of sampling</a></p>
<p>11 0.9396472 <a title="1518-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-09-Does_it_feel_like_cheating_when_I_do_this%3F__Variation_in_ethical_standards_and_expectations.html">605 andrew gelman stats-2011-03-09-Does it feel like cheating when I do this?  Variation in ethical standards and expectations</a></p>
<p>12 0.93896359 <a title="1518-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-09-The_difference_between_significant_and_not_significant%E2%80%A6.html">897 andrew gelman stats-2011-09-09-The difference between significant and not significant…</a></p>
<p>13 0.93728578 <a title="1518-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-12-Finite-population_standard_deviation_in_a_hierarchical_model.html">464 andrew gelman stats-2010-12-12-Finite-population standard deviation in a hierarchical model</a></p>
<p>14 0.93688571 <a title="1518-lda-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-06-Bootstrap_averaging%3A_Examples_where_it_works_and_where_it_doesn%E2%80%99t_work.html">2201 andrew gelman stats-2014-02-06-Bootstrap averaging: Examples where it works and where it doesn’t work</a></p>
<p>15 0.93682837 <a title="1518-lda-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-11-Gladwell_and_Chabris%2C_David_and_Goliath%2C_and_science_writing_as_stone_soup.html">2058 andrew gelman stats-2013-10-11-Gladwell and Chabris, David and Goliath, and science writing as stone soup</a></p>
<p>16 0.93681753 <a title="1518-lda-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-24-Parables_vs._stories.html">2184 andrew gelman stats-2014-01-24-Parables vs. stories</a></p>
<p>17 0.93658757 <a title="1518-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-15-Static_sensitivity_analysis.html">804 andrew gelman stats-2011-07-15-Static sensitivity analysis</a></p>
<p>18 0.9364233 <a title="1518-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-I_doubt_they_cheated.html">1971 andrew gelman stats-2013-08-07-I doubt they cheated</a></p>
<p>19 0.93578178 <a title="1518-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-29-How_to_grab_power_in_a_democracy_%E2%80%93_in_5_easy_non-violent_steps.html">116 andrew gelman stats-2010-06-29-How to grab power in a democracy – in 5 easy non-violent steps</a></p>
<p>20 0.93453133 <a title="1518-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-07-Minor-league_Stats_Predict_Major-league_Performance%2C_Sarah_Palin%2C_and_Some_Differences_Between_Baseball_and_Politics.html">652 andrew gelman stats-2011-04-07-Minor-league Stats Predict Major-league Performance, Sarah Palin, and Some Differences Between Baseball and Politics</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
