<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1521 andrew gelman stats-2012-10-04-Columbo does posterior predictive checks</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1521" href="#">andrew_gelman_stats-2012-1521</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1521 andrew gelman stats-2012-10-04-Columbo does posterior predictive checks</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1521-html" href="http://andrewgelman.com/2012/10/04/columbo-does-posterior-predictive-checks/">html</a></p><p>Introduction: I’m already on record as saying that  Ronald Reagan was a statistician  so I think this is ok too . . .
 
   
 
Here’s what Columbo does.  He hears the killer’s story and he takes it very seriously (it’s murder, and Columbo never jokes about murder), examines all its implications, and finds where it doesn’t fit the data.  Then Columbo carefully examines the discrepancies, tries some model expansion, and eventually concludes that he’s proved there’s a problem.
 
OK, now you’re saying:  Yeah, yeah, sure, but how does that differ from any other fictional detective?  The difference, I think, is that the tradition is for the detective to find clues and use these to come up with hypotheses, or to trap the killer via internal contradictions in his or her statement.  I see Columbo is different—and more in keeping with chapter 6 of Bayesian Data Analysis—in that he is taking the killer’s story seriously and exploring all its implications.  That’s the essence of predictive model checking:  you t</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I’m already on record as saying that  Ronald Reagan was a statistician  so I think this is ok too . [sent-1, score-0.327]
</p><p>2 He hears the killer’s story and he takes it very seriously (it’s murder, and Columbo never jokes about murder), examines all its implications, and finds where it doesn’t fit the data. [sent-5, score-0.785]
</p><p>3 Then Columbo carefully examines the discrepancies, tries some model expansion, and eventually concludes that he’s proved there’s a problem. [sent-6, score-0.763]
</p><p>4 OK, now you’re saying:  Yeah, yeah, sure, but how does that differ from any other fictional detective? [sent-7, score-0.219]
</p><p>5 The difference, I think, is that the tradition is for the detective to find clues and use these to come up with hypotheses, or to trap the killer via internal contradictions in his or her statement. [sent-8, score-1.22]
</p><p>6 I see Columbo is different—and more in keeping with chapter 6 of Bayesian Data Analysis—in that he is taking the killer’s story seriously and exploring all its implications. [sent-9, score-0.518]
</p><p>7 That’s the essence of predictive model checking:  you take advantage of the fact that you’re working with a generative model, and you generate anything and everything you can. [sent-10, score-0.643]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('columbo', 0.559), ('killer', 0.314), ('detective', 0.279), ('examines', 0.25), ('murder', 0.209), ('yeah', 0.162), ('clues', 0.14), ('seriously', 0.139), ('fictional', 0.134), ('trap', 0.117), ('essence', 0.114), ('discrepancies', 0.114), ('generative', 0.112), ('contradictions', 0.109), ('ronald', 0.109), ('jokes', 0.109), ('reagan', 0.102), ('ok', 0.101), ('internal', 0.099), ('tradition', 0.097), ('expansion', 0.097), ('exploring', 0.096), ('finds', 0.094), ('tries', 0.094), ('proved', 0.094), ('saying', 0.09), ('concludes', 0.088), ('model', 0.088), ('keeping', 0.085), ('differ', 0.085), ('hypotheses', 0.085), ('generate', 0.084), ('story', 0.082), ('eventually', 0.081), ('implications', 0.078), ('record', 0.078), ('advantage', 0.074), ('checking', 0.069), ('carefully', 0.068), ('predictive', 0.067), ('via', 0.065), ('takes', 0.063), ('chapter', 0.063), ('statistician', 0.058), ('everything', 0.056), ('taking', 0.053), ('re', 0.053), ('difference', 0.051), ('fact', 0.048), ('fit', 0.048)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1521-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-04-Columbo_does_posterior_predictive_checks.html">1521 andrew gelman stats-2012-10-04-Columbo does posterior predictive checks</a></p>
<p>Introduction: I’m already on record as saying that  Ronald Reagan was a statistician  so I think this is ok too . . .
 
   
 
Here’s what Columbo does.  He hears the killer’s story and he takes it very seriously (it’s murder, and Columbo never jokes about murder), examines all its implications, and finds where it doesn’t fit the data.  Then Columbo carefully examines the discrepancies, tries some model expansion, and eventually concludes that he’s proved there’s a problem.
 
OK, now you’re saying:  Yeah, yeah, sure, but how does that differ from any other fictional detective?  The difference, I think, is that the tradition is for the detective to find clues and use these to come up with hypotheses, or to trap the killer via internal contradictions in his or her statement.  I see Columbo is different—and more in keeping with chapter 6 of Bayesian Data Analysis—in that he is taking the killer’s story seriously and exploring all its implications.  That’s the essence of predictive model checking:  you t</p><p>2 0.10062143 <a title="1521-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-16-The_lamest%2C_grudgingest%2C_non-retraction_retraction_ever.html">1626 andrew gelman stats-2012-12-16-The lamest, grudgingest, non-retraction retraction ever</a></p>
<p>Introduction: In politics we’re familiar with the non-apology apology (well described in Wikipedia as “a statement that has the form of an apology but does not express the expected contrition”).  Here’s the scientific equivalent:  the non-retraction retraction.
 
Sanjay Srivastava  points  to an amusing yet barfable story of a pair of researchers who (inadvertently, I assume) made a data coding error and were eventually moved to issue a correction notice, but even then refused to fully admit their error.  As Srivastava puts it, the story “ended up with Lew [Goldberg] and colleagues [Kibeom Lee and Michael Ashton] publishing a comment on an erratum – the only time I’ve ever heard of that happening in a scientific journal.”
 
From the  comment  on the erratum:
  
In their “erratum and addendum,” Anderson and Ones (this issue) explained that we had brought their attention to the “potential” of a “possible” misalignment and described the results computed from re-aligned data as being based on a “post-ho</p><p>3 0.093819223 <a title="1521-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-03-Web_equation.html">1152 andrew gelman stats-2012-02-03-Web equation</a></p>
<p>Introduction: Aleks sends along this  app  which, while cute, is not quite “killer” for me.  I find it more difficult to write the equation using the trackpad than to simply type it in using Latex!  But I suppose it could be useful to beginners who want their papers to  look more like science .</p><p>4 0.086745091 <a title="1521-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>Introduction: In response to  my remarks  on his online book, Think Bayes, Allen Downey wrote: 
   
I [Downey] have a question about one of your comments: 
   My [Gelman's] main criticism with both books is that they talk a lot about inference but not so much about model building or model checking (recall the three steps of Bayesian data analysis). I think it’s ok for an introductory book to focus on inference, which of course is central to the data-analytic process—but I’d like them to at least mention that Bayesian ideas arise in model building and model checking as well. 

This sounds like something I agree with, and one of the things I tried to do in the book is to put modeling decisions front and center.  But the word “modeling” is used in lots of ways, so I want to see if we are talking about the same thing.


For example, in many chapters, I start with a simple model of the scenario, do some analysis, then check whether the model is good enough, and iterate.  Here’s the discussion of modeling</p><p>5 0.086606666 <a title="1521-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-10-Amtrak_sucks.html">1255 andrew gelman stats-2012-04-10-Amtrak sucks</a></p>
<p>Introduction: Couldn’t they at least let me buy my tickets from Amazon so I wouldn’t have to re-enter the credit card information each time?
 
Yeah, yeah, I know it’s no big deal.  It just seems so silly.</p><p>6 0.077955753 <a title="1521-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-04-Model_checking_and_model_understanding_in_machine_learning.html">1482 andrew gelman stats-2012-09-04-Model checking and model understanding in machine learning</a></p>
<p>7 0.073816679 <a title="1521-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>8 0.071861207 <a title="1521-tfidf-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-17-So-called_fixed_and_random_effects.html">472 andrew gelman stats-2010-12-17-So-called fixed and random effects</a></p>
<p>9 0.071655847 <a title="1521-tfidf-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>10 0.070535913 <a title="1521-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-05-High_temperatures_cause_violent_crime_and_implications_for_climate_change%2C_also_some_suggestions_about_how_to_better_summarize_these_claims.html">1522 andrew gelman stats-2012-10-05-High temperatures cause violent crime and implications for climate change, also some suggestions about how to better summarize these claims</a></p>
<p>11 0.069993079 <a title="1521-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-07-That_last_satisfaction_at_the_end_of_the_career.html">1568 andrew gelman stats-2012-11-07-That last satisfaction at the end of the career</a></p>
<p>12 0.063125916 <a title="1521-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-19-Data_exploration_and_multiple_comparisons.html">524 andrew gelman stats-2011-01-19-Data exploration and multiple comparisons</a></p>
<p>13 0.062975824 <a title="1521-tfidf-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-24-Empirical_implications_of_Empirical_Implications_of_Theoretical_Models.html">2263 andrew gelman stats-2014-03-24-Empirical implications of Empirical Implications of Theoretical Models</a></p>
<p>14 0.062708899 <a title="1521-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-01-Separated_by_a_common_blah_blah_blah.html">2119 andrew gelman stats-2013-12-01-Separated by a common blah blah blah</a></p>
<p>15 0.055822298 <a title="1521-tfidf-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-10-Bayes_jumps_the_shark.html">331 andrew gelman stats-2010-10-10-Bayes jumps the shark</a></p>
<p>16 0.055141203 <a title="1521-tfidf-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-10-The_last_great_essayist%3F.html">197 andrew gelman stats-2010-08-10-The last great essayist?</a></p>
<p>17 0.054901954 <a title="1521-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-03-Popper_and_Jaynes.html">2007 andrew gelman stats-2013-09-03-Popper and Jaynes</a></p>
<p>18 0.054865841 <a title="1521-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-20-A_statistical_model_for_underdispersion.html">1542 andrew gelman stats-2012-10-20-A statistical model for underdispersion</a></p>
<p>19 0.053598914 <a title="1521-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-13-Checking_your_model_using_fake_data.html">852 andrew gelman stats-2011-08-13-Checking your model using fake data</a></p>
<p>20 0.053065322 <a title="1521-tfidf-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-11-Incumbency_advantage_in_2010.html">408 andrew gelman stats-2010-11-11-Incumbency advantage in 2010</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.09), (1, 0.037), (2, -0.008), (3, 0.031), (4, -0.019), (5, -0.002), (6, 0.002), (7, -0.002), (8, 0.066), (9, 0.012), (10, -0.009), (11, 0.023), (12, -0.031), (13, -0.013), (14, -0.019), (15, -0.015), (16, 0.029), (17, -0.02), (18, -0.007), (19, -0.012), (20, -0.008), (21, -0.007), (22, -0.033), (23, -0.028), (24, -0.029), (25, -0.012), (26, -0.029), (27, -0.011), (28, 0.009), (29, -0.0), (30, -0.003), (31, 0.013), (32, -0.001), (33, 0.018), (34, 0.014), (35, 0.027), (36, -0.01), (37, -0.026), (38, 0.005), (39, -0.003), (40, 0.021), (41, -0.003), (42, 0.001), (43, 0.015), (44, 0.012), (45, -0.017), (46, 0.019), (47, -0.012), (48, -0.02), (49, 0.01)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97458607 <a title="1521-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-04-Columbo_does_posterior_predictive_checks.html">1521 andrew gelman stats-2012-10-04-Columbo does posterior predictive checks</a></p>
<p>Introduction: I’m already on record as saying that  Ronald Reagan was a statistician  so I think this is ok too . . .
 
   
 
Here’s what Columbo does.  He hears the killer’s story and he takes it very seriously (it’s murder, and Columbo never jokes about murder), examines all its implications, and finds where it doesn’t fit the data.  Then Columbo carefully examines the discrepancies, tries some model expansion, and eventually concludes that he’s proved there’s a problem.
 
OK, now you’re saying:  Yeah, yeah, sure, but how does that differ from any other fictional detective?  The difference, I think, is that the tradition is for the detective to find clues and use these to come up with hypotheses, or to trap the killer via internal contradictions in his or her statement.  I see Columbo is different—and more in keeping with chapter 6 of Bayesian Data Analysis—in that he is taking the killer’s story seriously and exploring all its implications.  That’s the essence of predictive model checking:  you t</p><p>2 0.83715302 <a title="1521-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>Introduction: In response to  my remarks  on his online book, Think Bayes, Allen Downey wrote: 
   
I [Downey] have a question about one of your comments: 
   My [Gelman's] main criticism with both books is that they talk a lot about inference but not so much about model building or model checking (recall the three steps of Bayesian data analysis). I think it’s ok for an introductory book to focus on inference, which of course is central to the data-analytic process—but I’d like them to at least mention that Bayesian ideas arise in model building and model checking as well. 

This sounds like something I agree with, and one of the things I tried to do in the book is to put modeling decisions front and center.  But the word “modeling” is used in lots of ways, so I want to see if we are talking about the same thing.


For example, in many chapters, I start with a simple model of the scenario, do some analysis, then check whether the model is good enough, and iterate.  Here’s the discussion of modeling</p><p>3 0.82429677 <a title="1521-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-07-Valencia%3A___Summer_of_1991.html">72 andrew gelman stats-2010-06-07-Valencia:   Summer of 1991</a></p>
<p>Introduction: With the completion of the last edition of Jose Bernardo’s Valencia (Spain) conference on Bayesian statistics–I didn’t attend, but many of my  friends  were there–I thought I’d share my strongest memory of the Valencia conference that I attended in 1991.  I contributed a poster and a discussion, both on the topic of inference from iterative simulation, but what I remember most vividly, and what bothered me the most, was how little interest there was in checking model fit.  Not only had people mostly not checked the fit of their models to data, and not only did they seem uninterested in such checks, even worse was that many of these Bayesians felt that it was basically illegal to check model fit.
 
I don’t want to get too down on Bayesians for this.  Lots of non-Bayesian statisticians go around not checking their models too.  With Bayes, though, model checking seems particularly important because Bayesians rely on their models so strongly, not just as a way of getting point estimates bu</p><p>4 0.81804425 <a title="1521-lsi-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-05-Does_posterior_predictive_model_checking_fit_with_the_operational_subjective_approach%3F.html">320 andrew gelman stats-2010-10-05-Does posterior predictive model checking fit with the operational subjective approach?</a></p>
<p>Introduction: David Rohde writes:
  
 
I have been thinking a lot lately about your Bayesian model checking approach.  This is in part because I have been working on exploratory data analysis and wishing to avoid controversy and mathematical statistics we omitted model checking from our discussion.  This is something that the refereeing process picked us up on and we ultimately added a critical discussion of null-hypothesis testing to  our paper .  The exploratory technique we discussed was essentially a 2D histogram approach, but we used Polya models as a formal model for the histogram.  We are currently working on a new paper, and we are thinking through how or if we should do “confirmatory analysis” or model checking in the paper.


What I find most admirable about your statistical work is that you clearly use the Bayesian approach to do useful applied statistical analysis.  My own attempts at applied Bayesian analysis makes me greatly admire your applied successes.  On the other hand it may be t</p><p>5 0.81493855 <a title="1521-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>Introduction: Cosma Shalizi  and  Larry Wasserman  discuss some papers from a conference on Ockham’s Razor.  I don’t have anything new to add on this so let me link to  past blog entries  on the topic and repost the following  from 2004 :
  
A lot has been written in statistics about “parsimony”—that is, the desire to explain phenomena using fewer parameters–but I’ve never seen any good general justification for parsimony.  (I don’t count “Occam’s Razor,” or “Ockham’s Razor,” or whatever, as a justification.  You gotta do better than digging up a 700-year-old quote.)


Maybe it’s because I work in social science, but my feeling is:  if you can approximate reality with just a few parameters, fine.  If you can use more parameters to fold in more information, that’s even better.


In practice, I often use simple models—because they are less effort to fit and, especially, to understand.  But I don’t kid myself that they’re better than more complicated efforts!


My favorite quote on this comes from  Rad</p><p>6 0.80714136 <a title="1521-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-03-Popper_and_Jaynes.html">2007 andrew gelman stats-2013-09-03-Popper and Jaynes</a></p>
<p>7 0.80448633 <a title="1521-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-04-David_MacKay_and_Occam%E2%80%99s_Razor.html">1041 andrew gelman stats-2011-12-04-David MacKay and Occam’s Razor</a></p>
<p>8 0.80280972 <a title="1521-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-15-Induction_within_a_model%2C_deductive_inference_for_model_evaluation.html">614 andrew gelman stats-2011-03-15-Induction within a model, deductive inference for model evaluation</a></p>
<p>9 0.80144674 <a title="1521-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-13-Flexibility_is_good.html">2133 andrew gelman stats-2013-12-13-Flexibility is good</a></p>
<p>10 0.80100137 <a title="1521-lsi-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-03-This_is_a_footnote_in_one_of_my_papers.html">448 andrew gelman stats-2010-12-03-This is a footnote in one of my papers</a></p>
<p>11 0.80030686 <a title="1521-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>12 0.79839569 <a title="1521-lsi-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-19-The_%E2%80%9Ceither-or%E2%80%9D_fallacy_of_believing_in_discrete_models%3A__an_example_of_folk_statistics.html">217 andrew gelman stats-2010-08-19-The “either-or” fallacy of believing in discrete models:  an example of folk statistics</a></p>
<p>13 0.79396105 <a title="1521-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-25-Incoherence_of_Bayesian_data_analysis.html">1510 andrew gelman stats-2012-09-25-Incoherence of Bayesian data analysis</a></p>
<p>14 0.78298223 <a title="1521-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-11-Kaiser_Fung_on_how_not_to_critique_models.html">1004 andrew gelman stats-2011-11-11-Kaiser Fung on how not to critique models</a></p>
<p>15 0.78205317 <a title="1521-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>16 0.78187513 <a title="1521-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-05-Xiao-Li_Meng_and_Xianchao_Xie_rethink_asymptotics.html">1406 andrew gelman stats-2012-07-05-Xiao-Li Meng and Xianchao Xie rethink asymptotics</a></p>
<p>17 0.77457547 <a title="1521-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-28-Using_predator-prey_models_on_the_Canadian_lynx_series.html">1141 andrew gelman stats-2012-01-28-Using predator-prey models on the Canadian lynx series</a></p>
<p>18 0.77265364 <a title="1521-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-27-Bridges_between_deterministic_and_probabilistic_models_for_binary_data.html">780 andrew gelman stats-2011-06-27-Bridges between deterministic and probabilistic models for binary data</a></p>
<p>19 0.77096707 <a title="1521-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>20 0.76816225 <a title="1521-lsi-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-09-Special_journal_issue_on_statistical_methods_for_the_social_sciences.html">24 andrew gelman stats-2010-05-09-Special journal issue on statistical methods for the social sciences</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.028), (15, 0.028), (16, 0.09), (21, 0.028), (23, 0.016), (24, 0.123), (28, 0.035), (51, 0.031), (53, 0.017), (63, 0.045), (64, 0.229), (72, 0.013), (81, 0.026), (99, 0.172)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.87037778 <a title="1521-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-04-Columbo_does_posterior_predictive_checks.html">1521 andrew gelman stats-2012-10-04-Columbo does posterior predictive checks</a></p>
<p>Introduction: I’m already on record as saying that  Ronald Reagan was a statistician  so I think this is ok too . . .
 
   
 
Here’s what Columbo does.  He hears the killer’s story and he takes it very seriously (it’s murder, and Columbo never jokes about murder), examines all its implications, and finds where it doesn’t fit the data.  Then Columbo carefully examines the discrepancies, tries some model expansion, and eventually concludes that he’s proved there’s a problem.
 
OK, now you’re saying:  Yeah, yeah, sure, but how does that differ from any other fictional detective?  The difference, I think, is that the tradition is for the detective to find clues and use these to come up with hypotheses, or to trap the killer via internal contradictions in his or her statement.  I see Columbo is different—and more in keeping with chapter 6 of Bayesian Data Analysis—in that he is taking the killer’s story seriously and exploring all its implications.  That’s the essence of predictive model checking:  you t</p><p>2 0.85568964 <a title="1521-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-09-Google_correlate_links_statistics_with_minorities.html">1109 andrew gelman stats-2012-01-09-Google correlate links statistics with minorities</a></p>
<p>Introduction: John Eppley asks what I make of  this :
 
   
 
Eppley is guessing the negative spikes are searches getting swamped by holiday season shoppers.</p><p>3 0.85032421 <a title="1521-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-04-Census_dotmap.html">1653 andrew gelman stats-2013-01-04-Census dotmap</a></p>
<p>Introduction: Andrew Vande Moere points to  this  impressive interactive map from Brandon Martin-Anderson showing the locations of all the residents of the United States and Canada.
 
It says, “The map has 341,817,095 dots – one for each person.”  Not quite . . . I was hoping to zoom into my building (approximately 10 people live on our floor, I say approximately because two of the apartments are split between two floors and I’m not sure how they would assign the residents), but unfortunately our entire block is just a solid mass of black.  Also, they put a few dots in the park and in the river by accident (presumably because the borders of the census blocks were specified only approximately).  But, hey, no algorithm is perfect.
 
 
 
It’s hard to know what to do about this.  The idea of mapping every person is cool, but you’ll always run into trouble displaying densely populated areas.  Smaller dots might work, but then that might depend on the screen being used for display.</p><p>4 0.8500098 <a title="1521-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-01-Doug_Schoen_has_2_poll_reports.html">985 andrew gelman stats-2011-11-01-Doug Schoen has 2 poll reports</a></p>
<p>Introduction: According to  Chris Wilson , there are two versions of the report of the Occupy Wall Street poll from  so-called  hack pollster Doug Schoen.
 
 Here’s  the report that Azi Paybarah says that Schoen sent to him, and here’s the final question from the poll:
 
   
 
And here’s what’s on Schoen’s  own  website: 
   
 
Very similar, except for that last phrase, “no matter what the cost.”  I have no idea which was actually asked to the survey participants, but it’s a reminder of the difficulties of public opinion research—sometimes you don’t even know what question was asked!
 
I’m not implying anything sinister on Schoen’s part, it’s just interesting to see these two documents floating around.
 
P.S.   More here  from Kaiser Fung on fundamental flaws with Schoen’s poll.</p><p>5 0.82174879 <a title="1521-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-28-What_Zombies_see_in_Scatterplots.html">595 andrew gelman stats-2011-02-28-What Zombies see in Scatterplots</a></p>
<p>Introduction: This video caught my interest –  news video clip   
(from this 
 post2 )
 
http://www.stat.columbia.edu/~cook/movabletype/archives/2011/02/on_summarizing.html
 
The news commentator did seem to be trying to point out what a couple of states had to say about the claimed relationship – almost on their own. 
 
Some  methods  have been worked out for zombies to do just this!
 
So I grabbed the data as close as I quickly could, modified the code slightly and here’s the zombie veiw of it.
 
 PoliticInt.pdf 
 
North Carolina is the bolded red curve, Idaho the bolded green curve. 
Missisipi and New York are the bolded blue.
 
As ugly as it is this is the Bayasian marginal picture – exactly (given MCMC errror).
 
K? 
p.s. you will get a very confusing picture if you forget to centre the x (i.e. see chapter 4 of Gelman and Hill book)</p><p>6 0.80460441 <a title="1521-lda-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-30-Question_%26_Answer_Communities.html">118 andrew gelman stats-2010-06-30-Question & Answer Communities</a></p>
<p>7 0.77906054 <a title="1521-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-21-New_search_engine_for_data_%26_statistics.html">724 andrew gelman stats-2011-05-21-New search engine for data & statistics</a></p>
<p>8 0.760077 <a title="1521-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-14-Higgs_bozos%3A__Rosencrantz_and_Guildenstern_are_spinning_in_their_graves.html">1058 andrew gelman stats-2011-12-14-Higgs bozos:  Rosencrantz and Guildenstern are spinning in their graves</a></p>
<p>9 0.74817443 <a title="1521-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-04-29-Auto-Gladwell%2C_or_Can_fractals_be_used_to_predict_human_history%3F.html">11 andrew gelman stats-2010-04-29-Auto-Gladwell, or Can fractals be used to predict human history?</a></p>
<p>10 0.74498433 <a title="1521-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-24-Textbook_for_data_visualization%3F.html">1637 andrew gelman stats-2012-12-24-Textbook for data visualization?</a></p>
<p>11 0.72534209 <a title="1521-lda-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-15-Recently_in_the_sister_blog.html">2249 andrew gelman stats-2014-03-15-Recently in the sister blog</a></p>
<p>12 0.70053023 <a title="1521-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-25-Why_it_doesn%E2%80%99t_make_sense_in_general_to_form_confidence_intervals_by_inverting_hypothesis_tests.html">870 andrew gelman stats-2011-08-25-Why it doesn’t make sense in general to form confidence intervals by inverting hypothesis tests</a></p>
<p>13 0.70040566 <a title="1521-lda-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-24-Why_it_doesn%E2%80%99t_make_sense_in_general_to_form_confidence_intervals_by_inverting_hypothesis_tests.html">1913 andrew gelman stats-2013-06-24-Why it doesn’t make sense in general to form confidence intervals by inverting hypothesis tests</a></p>
<p>14 0.69185871 <a title="1521-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-27-Hack_pollster_Doug_Schoen_illustrates_a_general_point%3A__The_%231_way_to_lie_with_statistics_is_._._._to_just_lie%21.html">977 andrew gelman stats-2011-10-27-Hack pollster Doug Schoen illustrates a general point:  The #1 way to lie with statistics is . . . to just lie!</a></p>
<p>15 0.69143689 <a title="1521-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-03-Two_interesting_posts_elsewhere_on_graphics.html">599 andrew gelman stats-2011-03-03-Two interesting posts elsewhere on graphics</a></p>
<p>16 0.68446004 <a title="1521-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-13-Hypothesis_testing_with_multiple_imputations.html">799 andrew gelman stats-2011-07-13-Hypothesis testing with multiple imputations</a></p>
<p>17 0.68207884 <a title="1521-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-06-Research_Directions_for_Machine_Learning_and_Algorithms.html">747 andrew gelman stats-2011-06-06-Research Directions for Machine Learning and Algorithms</a></p>
<p>18 0.68105096 <a title="1521-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-23-A_statistical_version_of_Arrow%E2%80%99s_paradox.html">586 andrew gelman stats-2011-02-23-A statistical version of Arrow’s paradox</a></p>
<p>19 0.68071872 <a title="1521-lda-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-20-The_AAA_Tranche_of_Subprime_Science.html">2179 andrew gelman stats-2014-01-20-The AAA Tranche of Subprime Science</a></p>
<p>20 0.68039405 <a title="1521-lda-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-13-Ethical_concerns_in_medical_trials.html">411 andrew gelman stats-2010-11-13-Ethical concerns in medical trials</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
