<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1486 andrew gelman stats-2012-09-07-Prior distributions for regression coefficients</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1486" href="#">andrew_gelman_stats-2012-1486</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1486 andrew gelman stats-2012-09-07-Prior distributions for regression coefficients</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1486-html" href="http://andrewgelman.com/2012/09/07/prior-distributions-for-regression-coefficients/">html</a></p><p>Introduction: Eric Brown writes:
  
I have come across a number of recommendations over the years about best practices for multilevel regression modeling.  For example, the use of t-distributed priors for coefficients in logistic regression and standardizing input variables from one of your 2008 Annals of Applied Statistics papers; or recommendations for priors on variance parameters from your 2006 Bayesian Analysis paper.  I understand that these are often of varied opinion of people in the field, but I was wondering if you have a reference that you point people to for a place to get started?  I’ve tried looking through your blog posts but couldn’t find any summaries.


For example, what are some examples of when I should use more than a two-level hierarchical model?  Can I use a spike-slab coefficient model with a t-distributed prior for the slab rather than a normal? If I assume that my model is a priori wrong (but still useful), what are some recommended ways to choose how many interactions to u</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Eric Brown writes:    I have come across a number of recommendations over the years about best practices for multilevel regression modeling. [sent-1, score-0.262]
</p><p>2 For example, the use of t-distributed priors for coefficients in logistic regression and standardizing input variables from one of your 2008 Annals of Applied Statistics papers; or recommendations for priors on variance parameters from your 2006 Bayesian Analysis paper. [sent-2, score-1.374]
</p><p>3 I understand that these are often of varied opinion of people in the field, but I was wondering if you have a reference that you point people to for a place to get started? [sent-3, score-0.09]
</p><p>4 For example, what are some examples of when I should use more than a two-level hierarchical model? [sent-5, score-0.119]
</p><p>5 Can I use a spike-slab coefficient model with a t-distributed prior for the slab rather than a normal? [sent-6, score-0.723]
</p><p>6 If I assume that my model is a priori wrong (but still useful), what are some recommended ways to choose how many interactions to use in the model? [sent-7, score-0.457]
</p><p>7 Finally, how would you recommend handling correlated / collinear variables (such as daily average temperature and daily average morning temperature)? [sent-8, score-1.073]
</p><p>8 As you can probably tell, this isn’t my area of expertise (I’m an ophthalmologist with a PhD in biochemistry) but I like to be able to justify my decisions on how to analyze data — “because its easy” or “because thats how others do it” isn’t enough. [sent-9, score-0.085]
</p><p>9 I’m quite comfortable with programming, R, and BUGS/JAGS so that shouldn’t get in the way. [sent-10, score-0.155]
</p><p>10 The biggest issue with regression is not the priors (once you do the basic step of assigning  some  weakly informative prior to deal with separation and near-separation) but what variables to include in the model in the first place. [sent-12, score-1.286]
</p><p>11 That said, the priors you use can influence what variables you feel comfortable including in the model. [sent-13, score-0.74]
</p><p>12 I’ve long thought that a lot of the fuss about variable selection arose because of the traditional insistence on flat priors within a model. [sent-14, score-0.509]
</p><p>13 Thus, rather than spike-and-slab, I prefer a prior that is not so restrictive at 0 (that is, spreading the “spike”) and also does partial pooling away from zero (that is, replacing the “slab” with a proper density). [sent-15, score-0.725]
</p><p>14 In that sense the t prior is a replacement for the entire spike-and-slab, not just for the slab. [sent-16, score-0.434]
</p><p>15 It seems to make sense to have a prior that partially pools interactions more strongly toward zero if the main effects are small. [sent-18, score-0.748]
</p><p>16 We’ve played around with such models for awhile but with no particularly clean results. [sent-19, score-0.076]
</p><p>17 One of our motivations for building Stan was in fact to enable us to experiment more systematically with such models. [sent-20, score-0.248]
</p><p>18 If we are going to assign independent or nearly independent prior distributions on the coefficients, we should put some effort into transforming so that independence makes some sense. [sent-22, score-0.744]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('priors', 0.287), ('prior', 0.251), ('slab', 0.232), ('variables', 0.179), ('comfortable', 0.155), ('temperature', 0.151), ('recommendations', 0.144), ('daily', 0.144), ('interactions', 0.127), ('coefficients', 0.124), ('collinear', 0.123), ('model', 0.121), ('use', 0.119), ('independent', 0.118), ('regression', 0.118), ('standardizing', 0.116), ('insistence', 0.111), ('fuss', 0.111), ('restrictive', 0.107), ('spreading', 0.101), ('pools', 0.099), ('finally', 0.099), ('zero', 0.098), ('spike', 0.097), ('replacement', 0.095), ('annals', 0.095), ('transforming', 0.093), ('varied', 0.09), ('assigning', 0.09), ('priori', 0.09), ('parameterization', 0.09), ('enable', 0.089), ('replacing', 0.088), ('separation', 0.088), ('sense', 0.088), ('independence', 0.086), ('handling', 0.086), ('justify', 0.085), ('partially', 0.085), ('average', 0.084), ('isn', 0.083), ('brown', 0.081), ('motivations', 0.081), ('pooling', 0.08), ('systematically', 0.078), ('assign', 0.078), ('morning', 0.078), ('played', 0.076), ('biggest', 0.076), ('weakly', 0.076)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="1486-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-07-Prior_distributions_for_regression_coefficients.html">1486 andrew gelman stats-2012-09-07-Prior distributions for regression coefficients</a></p>
<p>Introduction: Eric Brown writes:
  
I have come across a number of recommendations over the years about best practices for multilevel regression modeling.  For example, the use of t-distributed priors for coefficients in logistic regression and standardizing input variables from one of your 2008 Annals of Applied Statistics papers; or recommendations for priors on variance parameters from your 2006 Bayesian Analysis paper.  I understand that these are often of varied opinion of people in the field, but I was wondering if you have a reference that you point people to for a place to get started?  I’ve tried looking through your blog posts but couldn’t find any summaries.


For example, what are some examples of when I should use more than a two-level hierarchical model?  Can I use a spike-slab coefficient model with a t-distributed prior for the slab rather than a normal? If I assume that my model is a priori wrong (but still useful), what are some recommended ways to choose how many interactions to u</p><p>2 0.30675375 <a title="1486-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-29-More_by_Berger_and_me_on_weakly_informative_priors.html">1092 andrew gelman stats-2011-12-29-More by Berger and me on weakly informative priors</a></p>
<p>Introduction: A couple days ago we  discussed  some remarks by Tony O’Hagan and Jim Berger on weakly informative priors.  Jim  followed up  on Deborah Mayo’s blog with this:
  
Objective Bayesian priors are often improper (i.e., have infinite total mass), but this is not a problem when they are developed correctly. But not every improper prior is satisfactory. For instance, the constant prior is known to be unsatisfactory in many situations. The ‘solution’ pseudo-Bayesians often use is to choose a constant prior over a large but bounded set (a ‘weakly informative’ prior), saying it is now proper and so all is well. This is not true; if the constant prior on the whole parameter space is bad, so will be the constant prior over the bounded set. The problem is, in part, that some people confuse proper priors with subjective priors and, having learned that true subjective priors are fine, incorrectly presume that weakly informative proper priors are fine.
  
I have a few reactions to this:
 
1.  I agree</p><p>3 0.27354062 <a title="1486-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>Introduction: Nick Firoozye writes:
  
While I am absolutely sympathetic to the Bayesian agenda I am often troubled by the requirement of having priors. We must have priors on the parameter of an infinite number of model we have never seen before and I find this troubling. There is a similarly troubling problem in economics of utility theory. Utility is on consumables. To be complete a consumer must assign utility to all sorts of things they never would have encountered. More recent versions of utility theory instead make consumption goods a portfolio of attributes. Cadillacs are x many units of luxury y of transport etc etc. And we can automatically have personal utilities to all these attributes.  


I don’t ever see parameters. Some model have few and some have hundreds. Instead, I see data. So I don’t know how to have an opinion on parameters themselves. Rather I think it far more natural to have opinions on the behavior of models. The prior predictive density is a good and sensible notion. Also</p><p>4 0.24711576 <a title="1486-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>Introduction: Following up on Christian’s  post  [link fixed] on the topic, I’d like to offer a few thoughts of my own.
 
In BDA, we express the idea that a noninformative prior is a placeholder:  you can use the noninformative prior to get the analysis started, then if your posterior distribution is less informative than you would like, or if it does not make sense, you can go back and add prior information.
 
Same thing for the data model (the “likelihood”), for that matter:  it often makes sense to start with something simple and conventional and then go from there.
 
So, in that sense, noninformative priors are no big deal, they’re just a way to get started.  Just don’t take them too seriously.
 
Traditionally in statistics we’ve worked with the paradigm of a single highly informative dataset with only weak external information.  But if the data are sparse and prior information is strong, we have to think differently.  And, when you increase the dimensionality of a problem, both these things hap</p><p>5 0.24505137 <a title="1486-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-19-Prior_distributions_on_derived_quantities_rather_than_on_parameters_themselves.html">1946 andrew gelman stats-2013-07-19-Prior distributions on derived quantities rather than on parameters themselves</a></p>
<p>Introduction: Following up on our  discussion of the other day , Nick Firoozye writes: 
  
  
One thing I meant by my initial query (but really didn’t manage to get across) was this: I have no idea what my prior would be on many many models, but just like Utility Theory expects ALL consumers to attach a utility to any and all consumption goods (even those I haven’t seen or heard of), Bayesian Stats (almost) expects the same for priors. (Of course it’s not a religious edict much in the way Utility Theory has, since there is no theory of a “modeler” in the Bayesian paradigm—nonetheless there is still an expectation that we should have priors over all sorts of parameters which mean almost nothing to us).


For most models with sufficient complexity, I also have no idea what my informative priors are actually doing and the only way to know anything is through something I can see and experience, through data, not parameters or state variables.


My question was more on the—let’s use the prior to come up</p><p>6 0.24432303 <a title="1486-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>7 0.24248692 <a title="1486-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>8 0.22342227 <a title="1486-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-21-D._Buggin.html">1465 andrew gelman stats-2012-08-21-D. Buggin</a></p>
<p>9 0.22049421 <a title="1486-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-27-%E2%80%9CKeeping_things_unridiculous%E2%80%9D%3A__Berger%2C_O%E2%80%99Hagan%2C_and_me_on_weakly_informative_priors.html">1087 andrew gelman stats-2011-12-27-“Keeping things unridiculous”:  Berger, O’Hagan, and me on weakly informative priors</a></p>
<p>10 0.21092448 <a title="1486-tfidf-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>11 0.20186748 <a title="1486-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>12 0.19322641 <a title="1486-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>13 0.18395516 <a title="1486-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-17-Christian_Robert_on_the_Jeffreys-Lindley_paradox%3B_more_generally%2C_it%E2%80%99s_good_news_when_philosophical_arguments_can_be_transformed_into_technical_modeling_issues.html">2027 andrew gelman stats-2013-09-17-Christian Robert on the Jeffreys-Lindley paradox; more generally, it’s good news when philosophical arguments can be transformed into technical modeling issues</a></p>
<p>14 0.18300554 <a title="1486-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>15 0.18244223 <a title="1486-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-15-Reputations_changeable%2C_situations_tolerable.html">1858 andrew gelman stats-2013-05-15-Reputations changeable, situations tolerable</a></p>
<p>16 0.17392649 <a title="1486-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-11-%E2%80%9CInformative_g-Priors_for_Logistic_Regression%E2%80%9D.html">2017 andrew gelman stats-2013-09-11-“Informative g-Priors for Logistic Regression”</a></p>
<p>17 0.17363903 <a title="1486-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-13-On_the_half-Cauchy_prior_for_a_global_scale_parameter.html">801 andrew gelman stats-2011-07-13-On the half-Cauchy prior for a global scale parameter</a></p>
<p>18 0.17314535 <a title="1486-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>19 0.17260015 <a title="1486-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-24-Estimating_and_summarizing_inference_for_hierarchical_variance_parameters_when_the_number_of_groups_is_small.html">2145 andrew gelman stats-2013-12-24-Estimating and summarizing inference for hierarchical variance parameters when the number of groups is small</a></p>
<p>20 0.16692446 <a title="1486-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-07-Neutral_noninformative_and_informative_conjugate_beta_and_gamma_prior_distributions.html">1046 andrew gelman stats-2011-12-07-Neutral noninformative and informative conjugate beta and gamma prior distributions</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.255), (1, 0.25), (2, 0.034), (3, 0.075), (4, 0.022), (5, -0.033), (6, 0.155), (7, -0.054), (8, -0.12), (9, 0.167), (10, 0.041), (11, 0.046), (12, 0.083), (13, 0.032), (14, 0.024), (15, 0.003), (16, -0.02), (17, 0.011), (18, 0.031), (19, 0.019), (20, -0.065), (21, -0.009), (22, -0.049), (23, 0.037), (24, -0.033), (25, -0.011), (26, 0.038), (27, -0.079), (28, -0.068), (29, 0.005), (30, 0.057), (31, -0.017), (32, 0.008), (33, -0.009), (34, -0.04), (35, -0.012), (36, 0.023), (37, 0.031), (38, -0.006), (39, 0.01), (40, -0.018), (41, -0.01), (42, 0.044), (43, -0.006), (44, 0.026), (45, 0.004), (46, -0.05), (47, -0.033), (48, -0.03), (49, 0.033)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97409034 <a title="1486-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-07-Prior_distributions_for_regression_coefficients.html">1486 andrew gelman stats-2012-09-07-Prior distributions for regression coefficients</a></p>
<p>Introduction: Eric Brown writes:
  
I have come across a number of recommendations over the years about best practices for multilevel regression modeling.  For example, the use of t-distributed priors for coefficients in logistic regression and standardizing input variables from one of your 2008 Annals of Applied Statistics papers; or recommendations for priors on variance parameters from your 2006 Bayesian Analysis paper.  I understand that these are often of varied opinion of people in the field, but I was wondering if you have a reference that you point people to for a place to get started?  I’ve tried looking through your blog posts but couldn’t find any summaries.


For example, what are some examples of when I should use more than a two-level hierarchical model?  Can I use a spike-slab coefficient model with a t-distributed prior for the slab rather than a normal? If I assume that my model is a priori wrong (but still useful), what are some recommended ways to choose how many interactions to u</p><p>2 0.8834523 <a title="1486-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-21-D._Buggin.html">1465 andrew gelman stats-2012-08-21-D. Buggin</a></p>
<p>Introduction: Joe Zhao writes:
  
I am trying to fit my data using the scaled inverse wishart model you mentioned in your book, Data analysis using regression and hierarchical models. Instead of using a uniform prior on the scale parameters, I try to use a log-normal distribution prior. However, I found that the individual coefficients don’t shrink much to a certain value even a highly informative prior (with extremely low variance) is considered. The coefficients are just very close to their least-squares estimations. Is it because of the log-normal prior I’m using or I’m wrong somewhere?
  
My reply:  If your priors are concentrated enough at zero variance, then yeah, the posterior estimates of the parameters should be pulled (almost) all the way to zero.  If this isn’t happening, you got a problem.  So as a start I’d try putting in some really strong priors concentrated at 0 (for example, N(0,.1^2)) and checking that you get a sensible answer.  If not, you might well have a bug.  You can also try</p><p>3 0.88141596 <a title="1486-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-11-%E2%80%9CInformative_g-Priors_for_Logistic_Regression%E2%80%9D.html">2017 andrew gelman stats-2013-09-11-“Informative g-Priors for Logistic Regression”</a></p>
<p>Introduction: Tim Hanson sends along  this paper  (coauthored with Adam Branscum and Wesley Johnson):
  
Eliciting information from experts for use in constructing prior distributions for logistic regression coefficients can be challenging. The task is especially difficult when the model contains many predictor variables, because the expert is asked to provide summary information about the probability of “success” for many subgroups of the population. Often, however, experts are confident only in their assessment of the population as a whole. This paper is about incorporating such overall, marginal or averaged, information easily into a logistic regression data analysis by using g-priors. We present a version of the g-prior such that the prior distribution on the probability of success can be set to closely match a beta dis- tribution, when averaged over the set of predictors in a logistic regression. A simple data augmentation formulation that can be implemented in standard statistical software pac</p><p>4 0.8773976 <a title="1486-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>Introduction: Ryan King writes:
  
I was wondering if you have a brief comment on the state of the art for objective priors for hierarchical generalized linear models (generalized linear mixed models).  I have been working off the papers in Bayesian Analysis (2006) 1, Number 3 (Browne and Draper, Kass and Natarajan, Gelman).  There seems to have been continuous work for matching priors in linear mixed models, but GLMMs less so because of the lack of an analytic marginal likelihood for the variance components.  There are a number of additional suggestions in the literature since 2006, but little robust practical guidance.


I’m interested in both mean parameters and the variance components.  I’m almost always concerned with logistic random effect models.  I’m fascinated by the matching-priors idea of higher-order asymptotic improvements to maximum likelihood, and need to make some kind of defensible default recommendation.  Given the massive scale of the datasets (genetics …), extensive sensitivity a</p><p>5 0.87624645 <a title="1486-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-19-Prior_distributions_on_derived_quantities_rather_than_on_parameters_themselves.html">1946 andrew gelman stats-2013-07-19-Prior distributions on derived quantities rather than on parameters themselves</a></p>
<p>Introduction: Following up on our  discussion of the other day , Nick Firoozye writes: 
  
  
One thing I meant by my initial query (but really didn’t manage to get across) was this: I have no idea what my prior would be on many many models, but just like Utility Theory expects ALL consumers to attach a utility to any and all consumption goods (even those I haven’t seen or heard of), Bayesian Stats (almost) expects the same for priors. (Of course it’s not a religious edict much in the way Utility Theory has, since there is no theory of a “modeler” in the Bayesian paradigm—nonetheless there is still an expectation that we should have priors over all sorts of parameters which mean almost nothing to us).


For most models with sufficient complexity, I also have no idea what my informative priors are actually doing and the only way to know anything is through something I can see and experience, through data, not parameters or state variables.


My question was more on the—let’s use the prior to come up</p><p>6 0.87376434 <a title="1486-lsi-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-29-More_by_Berger_and_me_on_weakly_informative_priors.html">1092 andrew gelman stats-2011-12-29-More by Berger and me on weakly informative priors</a></p>
<p>7 0.85328847 <a title="1486-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-27-%E2%80%9CKeeping_things_unridiculous%E2%80%9D%3A__Berger%2C_O%E2%80%99Hagan%2C_and_me_on_weakly_informative_priors.html">1087 andrew gelman stats-2011-12-27-“Keeping things unridiculous”:  Berger, O’Hagan, and me on weakly informative priors</a></p>
<p>8 0.85096991 <a title="1486-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>9 0.84797347 <a title="1486-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>10 0.8399657 <a title="1486-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>11 0.83786362 <a title="1486-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-11-Weakly_informative_priors_for_Bayesian_nonparametric_models%3F.html">1454 andrew gelman stats-2012-08-11-Weakly informative priors for Bayesian nonparametric models?</a></p>
<p>12 0.83479822 <a title="1486-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-13-On_the_half-Cauchy_prior_for_a_global_scale_parameter.html">801 andrew gelman stats-2011-07-13-On the half-Cauchy prior for a global scale parameter</a></p>
<p>13 0.8317222 <a title="1486-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-22-The_scaled_inverse_Wishart_prior_distribution_for_a_covariance_matrix_in_a_hierarchical_model.html">1466 andrew gelman stats-2012-08-22-The scaled inverse Wishart prior distribution for a covariance matrix in a hierarchical model</a></p>
<p>14 0.82806391 <a title="1486-lsi-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-15-Weakly_informative_priors_and_imprecise_probabilities.html">468 andrew gelman stats-2010-12-15-Weakly informative priors and imprecise probabilities</a></p>
<p>15 0.82739204 <a title="1486-lsi-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>16 0.81268376 <a title="1486-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-15-Reputations_changeable%2C_situations_tolerable.html">1858 andrew gelman stats-2013-05-15-Reputations changeable, situations tolerable</a></p>
<p>17 0.79141074 <a title="1486-lsi-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-07-There_are_never_70_distinct_parameters.html">327 andrew gelman stats-2010-10-07-There are never 70 distinct parameters</a></p>
<p>18 0.78894562 <a title="1486-lsi-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>19 0.78480369 <a title="1486-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>20 0.78387928 <a title="1486-lsi-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.025), (16, 0.05), (21, 0.018), (24, 0.246), (47, 0.144), (86, 0.073), (99, 0.293)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97202182 <a title="1486-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-07-Prior_distributions_for_regression_coefficients.html">1486 andrew gelman stats-2012-09-07-Prior distributions for regression coefficients</a></p>
<p>Introduction: Eric Brown writes:
  
I have come across a number of recommendations over the years about best practices for multilevel regression modeling.  For example, the use of t-distributed priors for coefficients in logistic regression and standardizing input variables from one of your 2008 Annals of Applied Statistics papers; or recommendations for priors on variance parameters from your 2006 Bayesian Analysis paper.  I understand that these are often of varied opinion of people in the field, but I was wondering if you have a reference that you point people to for a place to get started?  I’ve tried looking through your blog posts but couldn’t find any summaries.


For example, what are some examples of when I should use more than a two-level hierarchical model?  Can I use a spike-slab coefficient model with a t-distributed prior for the slab rather than a normal? If I assume that my model is a priori wrong (but still useful), what are some recommended ways to choose how many interactions to u</p><p>2 0.95456666 <a title="1486-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-14-Data_visualization_at_the_American_Evaluation_Association.html">275 andrew gelman stats-2010-09-14-Data visualization at the American Evaluation Association</a></p>
<p>Introduction: Stephanie Evergreen writes:
  
 
Media, web design, and marketing have all created an environment where stakeholders – clients, program participants, funders – all expect high quality graphics and reporting that effectively conveys the valuable insights from evaluation work. Some in statistics and mathematics have used data visualization strategies to support more useful reporting of complex ideas. Global growing interest in improving communications has begun to take root in the evaluation field as well. But as anyone who has sat through a day’s worth of a conference or had to endure a dissertation-worthy evaluation report knows, evaluators still have a long way to go. To support the development of researchers and evaluators, some members of the American Evaluation Association are proposing a new TIG (Topical Interest Group) on Data Visualization and Reporting.  If you are a member of AEA (or want to be) and you are interested in joining this TIG,  contact  Stephanie Evergreen.</p><p>3 0.94898409 <a title="1486-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-01-What_goes_around_._._..html">548 andrew gelman stats-2011-02-01-What goes around . . .</a></p>
<p>Introduction: A few weeks ago I delivered a 10-minute talk on statistical graphics that went so well, it was the best-received talk I’ve ever given.  The crowd was raucous.  Then some poor sap had to go on after me.  He started by saying that my talk was a hard act to follow.  And, indeed, the audience politely listened but did not really get involved in his presentation.  Boy did I feel smug.
 
More recently I gave a talk on Stan, at an entirely different venue.  And this time the story was the exact opposite.  Jim Demmel spoke first and gave a wonderful talk on optimization for linear algebra (it was an applied math conference).  Then I followed, and I never really grabbed the crowd.  My talk was not a disaster but it didn’t really work.  This was particularly frustrating because I’m really excited about Stan and this was a group of researchers I wouldn’t usually have a chance to reach.  It was the plenary session at the conference.
 
Anyway, now I know how that guy felt from last month.  My talk</p><p>4 0.94765735 <a title="1486-lda-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-11-My_talk_at_the_NY_data_visualization_meetup_this_Monday%21.html">1668 andrew gelman stats-2013-01-11-My talk at the NY data visualization meetup this Monday!</a></p>
<p>Introduction: It’s in midtown at 7pm  (on Mon 14 Jan 2013).
 
Last time I talked for this group, I spoke on  Infovis vs. Statistical Graphics .  This time I plan to just go thru the choices involved in a few zillion graphs I’ve published over the years, to give a sense of the options and choices involved in graphical communication. For this talk there will be no single theme (except, perhaps, my usual “Graphs as comparisons,” “All of statistics as comparisons,” and “Exploratory data analysis as hypothesis testing”), just a bunch of open discussion about what I tried, why I tried it, what worked and what didn’t work, etc. I’ve discussed these sorts of decisions on occasion (and am now writing a paper with Yair about some of this for our voting models), but I’ve never tried to make a talk out of it before. Could be fun.</p><p>5 0.94506377 <a title="1486-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>Introduction: Some recent blog discussion revealed some confusion that I’ll try to resolve here.
 
I  wrote  that I’m not a big fan of subjective priors.  Various commenters had difficulty with this point, and I think the issue was most clearly stated by Bill Jeff re erys, who  wrote :
  
It seems to me that your prior has to reflect your subjective information before you look at the data. How can it not?


But this does not mean that the (subjective) prior that you choose is irrefutable; Surely a prior that reflects prior information just does not have to be inconsistent with that information. But that still leaves a range of priors that are consistent with it, the sort of priors that one would use in a sensitivity analysis, for example.
  
I think I see what Bill is getting at.  A prior represents your subjective belief, or some approximation to your subjective belief, even if it’s not perfect.  That sounds reasonable but I don’t think it works.  Or, at least, it often doesn’t work.
 
Let’s start</p><p>6 0.94089687 <a title="1486-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-27-%E2%80%9CHow_to_Lie_with_Statistics%E2%80%9D_guy_worked_for_the_tobacco_industry_to_mock_studies_of_the_risks_of_smoking_statistics.html">1285 andrew gelman stats-2012-04-27-“How to Lie with Statistics” guy worked for the tobacco industry to mock studies of the risks of smoking statistics</a></p>
<p>7 0.93776816 <a title="1486-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>8 0.93735975 <a title="1486-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-13-When%E2%80%99s_that_next_gamma-ray_blast_gonna_come%2C_already%3F.html">1897 andrew gelman stats-2013-06-13-When’s that next gamma-ray blast gonna come, already?</a></p>
<p>9 0.93551129 <a title="1486-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-18-Check_your_missing-data_imputations_using_cross-validation.html">1218 andrew gelman stats-2012-03-18-Check your missing-data imputations using cross-validation</a></p>
<p>10 0.93339753 <a title="1486-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-05-Question_26_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1367 andrew gelman stats-2012-06-05-Question 26 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>11 0.93254715 <a title="1486-lda-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Ken_Rice_presents_a_unifying_approach_to_statistical_inference_and_hypothesis_testing.html">2312 andrew gelman stats-2014-04-29-Ken Rice presents a unifying approach to statistical inference and hypothesis testing</a></p>
<p>12 0.93049955 <a title="1486-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-31-Type_S_error_rates_for_classical_and_Bayesian_single_and_multiple_comparison_procedures.html">494 andrew gelman stats-2010-12-31-Type S error rates for classical and Bayesian single and multiple comparison procedures</a></p>
<p>13 0.92948145 <a title="1486-lda-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-23-Discussion_on_preregistration_of_research_studies.html">2183 andrew gelman stats-2014-01-23-Discussion on preregistration of research studies</a></p>
<p>14 0.92885458 <a title="1486-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-11-Steve_Jobs%E2%80%99s_cancer_and_science-based_medicine.html">953 andrew gelman stats-2011-10-11-Steve Jobs’s cancer and science-based medicine</a></p>
<p>15 0.92876053 <a title="1486-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>16 0.92807412 <a title="1486-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>17 0.92772233 <a title="1486-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-02-Blogads_update.html">1240 andrew gelman stats-2012-04-02-Blogads update</a></p>
<p>18 0.92672253 <a title="1486-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>19 0.9251371 <a title="1486-lda-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-12-The_Naval_Research_Lab.html">1261 andrew gelman stats-2012-04-12-The Naval Research Lab</a></p>
<p>20 0.92487371 <a title="1486-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-03-Setting_aside_the_politics%2C_the_debate_over_the_new_health-care_study_reveals_that_we%E2%80%99re_moving_to_a_new_high_standard_of_statistical_journalism.html">1838 andrew gelman stats-2013-05-03-Setting aside the politics, the debate over the new health-care study reveals that we’re moving to a new high standard of statistical journalism</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
