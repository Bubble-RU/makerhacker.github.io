<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1270 andrew gelman stats-2012-04-19-Demystifying Blup</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1270" href="#">andrew_gelman_stats-2012-1270</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1270 andrew gelman stats-2012-04-19-Demystifying Blup</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1270-html" href="http://andrewgelman.com/2012/04/19/demystifying-blup/">html</a></p><p>Introduction: In our recent thread on computing hierarchical models with big datasets, someone  brought up  Blup.  I thought it might be worth explaining what Blup is and how it relates to hierarchical models.
 
Blup stands for Best Linear Unbiased Prediction, but in my terminology it’s just hierarchical modeling.  Let me break it down: 
- “Best” doesn’t really matter.  What’s important is that our estimates and predictions make sense and are as accurate as possible. 
- “Linear” isn’t so important.  Statistical predictions are linear for Gaussian linear models, otherwise not.  We can and do perform hierarchical generalized linear models all the time. 
- “Unbiased” doesn’t really matter (see discussion of “Best,” above). 
- “Prediction” is the key word for relating Blup and hierarchical modeling to classical statistical terminology.  In classical statistics, “estimation” of a parameter theta is evaluated conditional on the true value of theta, whereas “prediction” of a predictive quantity phi is eval</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In our recent thread on computing hierarchical models with big datasets, someone  brought up  Blup. [sent-1, score-0.567]
</p><p>2 I thought it might be worth explaining what Blup is and how it relates to hierarchical models. [sent-2, score-0.389]
</p><p>3 Blup stands for Best Linear Unbiased Prediction, but in my terminology it’s just hierarchical modeling. [sent-3, score-0.409]
</p><p>4 Let me break it down:  - “Best” doesn’t really matter. [sent-4, score-0.047]
</p><p>5 What’s important is that our estimates and predictions make sense and are as accurate as possible. [sent-5, score-0.124]
</p><p>6 Statistical predictions are linear for Gaussian linear models, otherwise not. [sent-7, score-0.453]
</p><p>7 We can and do perform hierarchical generalized linear models all the time. [sent-8, score-0.654]
</p><p>8 - “Prediction” is the key word for relating Blup and hierarchical modeling to classical statistical terminology. [sent-10, score-0.588]
</p><p>9 In classical statistics, “estimation” of a parameter theta is evaluated conditional on the true value of theta, whereas “prediction” of a predictive quantity phi is evaluated  unconditional  on phi, but conditional on theta. [sent-11, score-0.967]
</p><p>10 “Prediction” is a way to do Bayesian inference in a classical setting. [sent-12, score-0.156]
</p><p>11 In the classical “empirical Bayes” framework, some of the unknowns are called “parameters” and some are called “predictive quantities” or missing data. [sent-13, score-0.355]
</p><p>12 ” We discuss this briefly in BDA (maybe in a footnote somewhere). [sent-15, score-0.107]
</p><p>13 For the purposes of modeling and data analysis, Blup is hierarchical regression. [sent-16, score-0.429]
</p><p>14 Any computational method for Blup can be ported directly into computation for hierarchical modeling, and vice-versa. [sent-18, score-0.529]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('blup', 0.737), ('hierarchical', 0.289), ('unbiased', 0.218), ('linear', 0.186), ('prediction', 0.161), ('classical', 0.156), ('phi', 0.146), ('evaluated', 0.107), ('theta', 0.102), ('datasets', 0.099), ('computation', 0.095), ('modeling', 0.087), ('models', 0.084), ('ported', 0.084), ('predictions', 0.081), ('unknowns', 0.079), ('estimation', 0.079), ('conditional', 0.079), ('predictive', 0.076), ('best', 0.068), ('terminology', 0.063), ('unconditional', 0.063), ('method', 0.061), ('called', 0.06), ('footnote', 0.058), ('stands', 0.057), ('relating', 0.056), ('mainstream', 0.055), ('quantities', 0.054), ('relates', 0.054), ('purposes', 0.053), ('brought', 0.052), ('quantity', 0.052), ('bda', 0.052), ('thread', 0.052), ('generalized', 0.051), ('gaussian', 0.051), ('briefly', 0.049), ('relevance', 0.047), ('break', 0.047), ('explaining', 0.046), ('users', 0.045), ('computing', 0.045), ('machine', 0.045), ('big', 0.045), ('developed', 0.044), ('perform', 0.044), ('accurate', 0.043), ('framework', 0.043), ('somewhere', 0.043)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1270-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-19-Demystifying_Blup.html">1270 andrew gelman stats-2012-04-19-Demystifying Blup</a></p>
<p>Introduction: In our recent thread on computing hierarchical models with big datasets, someone  brought up  Blup.  I thought it might be worth explaining what Blup is and how it relates to hierarchical models.
 
Blup stands for Best Linear Unbiased Prediction, but in my terminology it’s just hierarchical modeling.  Let me break it down: 
- “Best” doesn’t really matter.  What’s important is that our estimates and predictions make sense and are as accurate as possible. 
- “Linear” isn’t so important.  Statistical predictions are linear for Gaussian linear models, otherwise not.  We can and do perform hierarchical generalized linear models all the time. 
- “Unbiased” doesn’t really matter (see discussion of “Best,” above). 
- “Prediction” is the key word for relating Blup and hierarchical modeling to classical statistical terminology.  In classical statistics, “estimation” of a parameter theta is evaluated conditional on the true value of theta, whereas “prediction” of a predictive quantity phi is eval</p><p>2 0.15798391 <a title="1270-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-23-Examples_of_the_use_of_hierarchical_modeling_to_generalize_to_new_settings.html">1425 andrew gelman stats-2012-07-23-Examples of the use of hierarchical modeling to generalize to new settings</a></p>
<p>Introduction: In a link to our  back-and-forth  on causal inference and the use of hierarchical models to bridge between different inferential settings, Elias Bareinboim (a computer scientist who is working with Judea Pearl)  writes :
  
In the past week, I have been engaged in a discussion with Andrew Gelman and his blog readers regarding causal inference, selection bias, confounding, and generalizability. I was trying to understand how his method which he calls “hierarchical modeling” would handle these issues and what guarantees it provides. . . . If anyone understands how “hierarchical modeling” can solve a simple toy problem (e.g., M-bias, control of confounding, mediation, generalizability), please share with us.
  
In his post, Bareinboim raises a direct question about hierarchical modeling and also indirectly brings up larger questions about what is convincing evidence when evaluating a statistical method.  As I wrote earlier, Bareinboim believes that “The only way investigators can decide w</p><p>3 0.14347577 <a title="1270-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>Introduction: Nick Firoozye writes:
  
I had a question about BMA [Bayesian model averaging] and model combinations in general, and direct it to you since they are a basic form of hierarchical model, albeit in the simplest of forms. I wanted to ask what the underlying assumptions are that could lead to BMA improving on a larger model. 


I know model combination is a topic of interest in the (frequentist) econometrics community (e.g., Bates & Granger, http://www.jstor.org/discover/10.2307/3008764?uid=3738032&uid;=2&uid;=4&sid;=21101948653381) but at the time it was considered a bit of a puzzle. Perhaps small models combined outperform a big model due to standard errors, insufficient data, etc. But I haven’t seen much in way of Bayesian justification.


In simplest terms, you might have a joint density P(Y,theta_1,theta_2) from which you could use the two marginals P(Y,theta_1) and P(Y,theta_2) to derive two separate forecasts. A BMA-er would do a weighted average of the two forecast densities, having p</p><p>4 0.13639006 <a title="1270-tfidf-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-29-References_%28with_code%29_for_Bayesian_hierarchical_%28multilevel%29_modeling_and_structural_equation_modeling.html">2273 andrew gelman stats-2014-03-29-References (with code) for Bayesian hierarchical (multilevel) modeling and structural equation modeling</a></p>
<p>Introduction: A student writes:
  
I am new to Bayesian methods.  While I am reading your book, I have some questions for you.  I am interested in doing Bayesian hierarchical (multi-level) linear regression (e.g., random-intercept model) and Bayesian structural equation modeling (SEM)—for causality.  Do you happen to know if I could find some articles, where authors could provide data w/ R and/or BUGS codes that I could replicate them?
  
My reply:  For Bayesian hierarchical (multi-level) linear regression and causal inference, see my book with Jennifer Hill.  For Bayesian structural equation modeling, try google and you’ll find some good stuff.  Also, I recommend Stan (http://mc-stan.org/) rather than Bugs.</p><p>5 0.12874353 <a title="1270-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-14-Everyone%E2%80%99s_trading_bias_for_variance_at_some_point%2C_it%E2%80%99s_just_done_at_different_places_in_the_analyses.html">1763 andrew gelman stats-2013-03-14-Everyone’s trading bias for variance at some point, it’s just done at different places in the analyses</a></p>
<p>Introduction: Some things I respect 
 
When it comes to meta-models of statistics, here are two philosophies that I respect:
 
1.  (My) Bayesian approach, which I associate with E. T. Jaynes, in which you construct models with strong assumptions, ride your models hard, check their fit to data, and then scrap them and improve them as necessary.
 
2.  At the other extreme, model-free statistical procedures that are designed to work well under very weak assumptions—for example, instead of assuming a distribution is Gaussian, you would just want the procedure to work well under some conditions on the smoothness of the second derivative of the log density function.
 
Both the above philosophies recognize that (almost) all important assumptions will be wrong, and they resolve this concern via aggressive model checking or via robustness.  And of course there are intermediate positions, such as working with Bayesian models that have been shown to be robust, and then still checking them.  Or, to flip it arou</p><p>6 0.11656293 <a title="1270-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-18-Hierarchical_modeling_as_a_framework_for_extrapolation.html">1383 andrew gelman stats-2012-06-18-Hierarchical modeling as a framework for extrapolation</a></p>
<p>7 0.10562662 <a title="1270-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-09-Default_priors_update%3F.html">846 andrew gelman stats-2011-08-09-Default priors update?</a></p>
<p>8 0.095654309 <a title="1270-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-16-Long_discussion_about_causal_inference_and_the_use_of_hierarchical_models_to_bridge_between_different_inferential_settings.html">1418 andrew gelman stats-2012-07-16-Long discussion about causal inference and the use of hierarchical models to bridge between different inferential settings</a></p>
<p>9 0.094870158 <a title="1270-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-15-The_bias-variance_tradeoff.html">960 andrew gelman stats-2011-10-15-The bias-variance tradeoff</a></p>
<p>10 0.094544142 <a title="1270-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-03-Hierarchical_array_priors_for_ANOVA_decompositions.html">1786 andrew gelman stats-2013-04-03-Hierarchical array priors for ANOVA decompositions</a></p>
<p>11 0.09276282 <a title="1270-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>12 0.092441924 <a title="1270-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-13-On_the_half-Cauchy_prior_for_a_global_scale_parameter.html">801 andrew gelman stats-2011-07-13-On the half-Cauchy prior for a global scale parameter</a></p>
<p>13 0.089740932 <a title="1270-tfidf-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-01-Two_Postdoc_Positions_Available_on_Bayesian_Hierarchical_Modeling.html">62 andrew gelman stats-2010-06-01-Two Postdoc Positions Available on Bayesian Hierarchical Modeling</a></p>
<p>14 0.089527339 <a title="1270-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<p>15 0.086089157 <a title="1270-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-The_statistical_significance_filter.html">899 andrew gelman stats-2011-09-10-The statistical significance filter</a></p>
<p>16 0.084803037 <a title="1270-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-15-More_on_AIC%2C_WAIC%2C_etc.html">1983 andrew gelman stats-2013-08-15-More on AIC, WAIC, etc</a></p>
<p>17 0.08462131 <a title="1270-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-23-Scalable_Stan.html">2035 andrew gelman stats-2013-09-23-Scalable Stan</a></p>
<p>18 0.08302062 <a title="1270-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-06-Yes%2C_checking_calibration_of_probability_forecasts_is_part_of_Bayesian_statistics.html">1610 andrew gelman stats-2012-12-06-Yes, checking calibration of probability forecasts is part of Bayesian statistics</a></p>
<p>19 0.08138974 <a title="1270-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-16-The_%E2%80%9CWashington_read%E2%80%9D_and_the_algebra_of_conditional_distributions.html">961 andrew gelman stats-2011-10-16-The “Washington read” and the algebra of conditional distributions</a></p>
<p>20 0.077652067 <a title="1270-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-02-Flame_bait.html">1880 andrew gelman stats-2013-06-02-Flame bait</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.113), (1, 0.123), (2, -0.016), (3, 0.025), (4, -0.002), (5, 0.023), (6, -0.023), (7, -0.02), (8, 0.014), (9, 0.024), (10, -0.016), (11, 0.001), (12, 0.001), (13, 0.001), (14, -0.02), (15, 0.008), (16, -0.038), (17, -0.001), (18, 0.022), (19, -0.029), (20, 0.015), (21, 0.004), (22, 0.03), (23, 0.026), (24, 0.02), (25, -0.017), (26, -0.029), (27, 0.029), (28, 0.006), (29, 0.012), (30, 0.009), (31, 0.028), (32, 0.003), (33, -0.032), (34, -0.018), (35, -0.001), (36, -0.014), (37, -0.01), (38, -0.037), (39, 0.022), (40, -0.015), (41, 0.024), (42, -0.025), (43, -0.008), (44, -0.037), (45, -0.041), (46, 0.026), (47, 0.014), (48, -0.02), (49, -0.061)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97355658 <a title="1270-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-19-Demystifying_Blup.html">1270 andrew gelman stats-2012-04-19-Demystifying Blup</a></p>
<p>Introduction: In our recent thread on computing hierarchical models with big datasets, someone  brought up  Blup.  I thought it might be worth explaining what Blup is and how it relates to hierarchical models.
 
Blup stands for Best Linear Unbiased Prediction, but in my terminology it’s just hierarchical modeling.  Let me break it down: 
- “Best” doesn’t really matter.  What’s important is that our estimates and predictions make sense and are as accurate as possible. 
- “Linear” isn’t so important.  Statistical predictions are linear for Gaussian linear models, otherwise not.  We can and do perform hierarchical generalized linear models all the time. 
- “Unbiased” doesn’t really matter (see discussion of “Best,” above). 
- “Prediction” is the key word for relating Blup and hierarchical modeling to classical statistical terminology.  In classical statistics, “estimation” of a parameter theta is evaluated conditional on the true value of theta, whereas “prediction” of a predictive quantity phi is eval</p><p>2 0.6983602 <a title="1270-lsi-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-25-Correlation_of_1_._._._too_good_to_be_true%3F.html">1737 andrew gelman stats-2013-02-25-Correlation of 1 . . . too good to be true?</a></p>
<p>Introduction: Alex Hoffman points me to  this interview  by Dylan Matthews of education researcher Thomas Kane, who at one point says,
  
Once you corrected for measurement error, a teacher’s score on their chosen videos and on their unchosen videos were correlated at 1. They were perfectly correlated.
  
Hoffman asks, “What do you think? Do you think that just maybe, perhaps, it’s possible we aught to consider, I’m just throwing out the possibility that it might be that the procedure for correcting measurement error might, you now, be a little too strong?”
 
I don’t know exactly what’s happening here, but it might be something that I’ve seen on occasion when fitting multilevel models using a point estimate for the group-level variance.  It goes like this:  measurement-error models are multilevel models, they involve the estimation of a distribution of a latent variable.  When fitting multilevel models, it is possible to estimate the group-level variance to be zero, even though the group-level varia</p><p>3 0.69486374 <a title="1270-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-10-Cross-validation_and_Bayesian_estimation_of_tuning_parameters.html">2129 andrew gelman stats-2013-12-10-Cross-validation and Bayesian estimation of tuning parameters</a></p>
<p>Introduction: Ilya Lipkovich writes:
  
I read with great interest your 2008  paper  [with Aleks Jakulin, Grazia Pittau, and Yu-Sung Su] on weakly informative priors for logistic regression and also followed an interesting discussion on your blog. This discussion was within Bayesian community in relation to the validity of priors. However i would like to approach it rather from a more broad perspective on predictive modeling bringing in the ideas from machine/statistical learning approach”.  Actually you were the first to bring it up by mentioning in your paper “borrowing ideas from computer science” on cross-validation when comparing predictive ability of your proposed priors with other choices.


However, using cross-validation for comparing method performance is not the only or primary use of CV in machine-learning. Most of machine learning methods have some “meta” or complexity parameters and use cross-validation to tune them up. For example, one of your comparison methods is BBR which actually</p><p>4 0.69486332 <a title="1270-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-23-Examples_of_the_use_of_hierarchical_modeling_to_generalize_to_new_settings.html">1425 andrew gelman stats-2012-07-23-Examples of the use of hierarchical modeling to generalize to new settings</a></p>
<p>Introduction: In a link to our  back-and-forth  on causal inference and the use of hierarchical models to bridge between different inferential settings, Elias Bareinboim (a computer scientist who is working with Judea Pearl)  writes :
  
In the past week, I have been engaged in a discussion with Andrew Gelman and his blog readers regarding causal inference, selection bias, confounding, and generalizability. I was trying to understand how his method which he calls “hierarchical modeling” would handle these issues and what guarantees it provides. . . . If anyone understands how “hierarchical modeling” can solve a simple toy problem (e.g., M-bias, control of confounding, mediation, generalizability), please share with us.
  
In his post, Bareinboim raises a direct question about hierarchical modeling and also indirectly brings up larger questions about what is convincing evidence when evaluating a statistical method.  As I wrote earlier, Bareinboim believes that “The only way investigators can decide w</p><p>5 0.69396186 <a title="1270-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-14-Everyone%E2%80%99s_trading_bias_for_variance_at_some_point%2C_it%E2%80%99s_just_done_at_different_places_in_the_analyses.html">1763 andrew gelman stats-2013-03-14-Everyone’s trading bias for variance at some point, it’s just done at different places in the analyses</a></p>
<p>Introduction: Some things I respect 
 
When it comes to meta-models of statistics, here are two philosophies that I respect:
 
1.  (My) Bayesian approach, which I associate with E. T. Jaynes, in which you construct models with strong assumptions, ride your models hard, check their fit to data, and then scrap them and improve them as necessary.
 
2.  At the other extreme, model-free statistical procedures that are designed to work well under very weak assumptions—for example, instead of assuming a distribution is Gaussian, you would just want the procedure to work well under some conditions on the smoothness of the second derivative of the log density function.
 
Both the above philosophies recognize that (almost) all important assumptions will be wrong, and they resolve this concern via aggressive model checking or via robustness.  And of course there are intermediate positions, such as working with Bayesian models that have been shown to be robust, and then still checking them.  Or, to flip it arou</p><p>6 0.68304616 <a title="1270-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-13-Philosophy_of_Bayesian_statistics%3A__my_reactions_to_Wasserman.html">1165 andrew gelman stats-2012-02-13-Philosophy of Bayesian statistics:  my reactions to Wasserman</a></p>
<p>7 0.68145502 <a title="1270-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-03-Hierarchical_array_priors_for_ANOVA_decompositions.html">1786 andrew gelman stats-2013-04-03-Hierarchical array priors for ANOVA decompositions</a></p>
<p>8 0.67550886 <a title="1270-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-What_are_the_trickiest_models_to_fit%3F.html">575 andrew gelman stats-2011-02-15-What are the trickiest models to fit?</a></p>
<p>9 0.67516297 <a title="1270-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>10 0.66729885 <a title="1270-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<p>11 0.66408342 <a title="1270-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Adding_more_information_can_make_the_variance_go_up_%28depending_on_your_model%29.html">810 andrew gelman stats-2011-07-20-Adding more information can make the variance go up (depending on your model)</a></p>
<p>12 0.66367972 <a title="1270-lsi-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-31-Somewhat_Bayesian_multilevel_modeling.html">246 andrew gelman stats-2010-08-31-Somewhat Bayesian multilevel modeling</a></p>
<p>13 0.65934962 <a title="1270-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<p>14 0.6589942 <a title="1270-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-19-Data_exploration_and_multiple_comparisons.html">524 andrew gelman stats-2011-01-19-Data exploration and multiple comparisons</a></p>
<p>15 0.65770876 <a title="1270-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-21-The_future_%28and_past%29_of_statistical_sciences.html">2072 andrew gelman stats-2013-10-21-The future (and past) of statistical sciences</a></p>
<p>16 0.65764046 <a title="1270-lsi-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-10-R_vs._Stata%2C_or%2C_Different_ways_to_estimate_multilevel_models.html">269 andrew gelman stats-2010-09-10-R vs. Stata, or, Different ways to estimate multilevel models</a></p>
<p>17 0.65512323 <a title="1270-lsi-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-26-WAIC_and_cross-validation_in_Stan%21.html">2349 andrew gelman stats-2014-05-26-WAIC and cross-validation in Stan!</a></p>
<p>18 0.65021026 <a title="1270-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-23-More_on_Bayesian_methods_and_multilevel_modeling.html">2033 andrew gelman stats-2013-09-23-More on Bayesian methods and multilevel modeling</a></p>
<p>19 0.64893991 <a title="1270-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-05-Xiao-Li_Meng_and_Xianchao_Xie_rethink_asymptotics.html">1406 andrew gelman stats-2012-07-05-Xiao-Li Meng and Xianchao Xie rethink asymptotics</a></p>
<p>20 0.64636719 <a title="1270-lsi-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-21-Random_matrices_in_the_news.html">2258 andrew gelman stats-2014-03-21-Random matrices in the news</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.01), (16, 0.082), (20, 0.152), (21, 0.04), (24, 0.162), (28, 0.013), (84, 0.016), (86, 0.071), (99, 0.286)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97903323 <a title="1270-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-20-WWJD%3F__U_can_find_out%21.html">479 andrew gelman stats-2010-12-20-WWJD?  U can find out!</a></p>
<p>Introduction: Two positions open  in the statistics group at the NYU education school.  If you get the job, you get to work with Jennifer HIll!
 
One position is a postdoctoral fellowship, and the other is a visiting professorship.  The latter position requires “the demonstrated ability to develop a nationally recognized research program,” which seems like a lot to ask for a visiting professor.  Do they expect the visiting prof to develop a nationally recognized research program and then leave it there at NYU after the visit is over?
 
In any case, Jennifer and her colleagues are doing excellent work, both applied and methodological, and this seems like a great opportunity.</p><p>same-blog 2 0.95426184 <a title="1270-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-19-Demystifying_Blup.html">1270 andrew gelman stats-2012-04-19-Demystifying Blup</a></p>
<p>Introduction: In our recent thread on computing hierarchical models with big datasets, someone  brought up  Blup.  I thought it might be worth explaining what Blup is and how it relates to hierarchical models.
 
Blup stands for Best Linear Unbiased Prediction, but in my terminology it’s just hierarchical modeling.  Let me break it down: 
- “Best” doesn’t really matter.  What’s important is that our estimates and predictions make sense and are as accurate as possible. 
- “Linear” isn’t so important.  Statistical predictions are linear for Gaussian linear models, otherwise not.  We can and do perform hierarchical generalized linear models all the time. 
- “Unbiased” doesn’t really matter (see discussion of “Best,” above). 
- “Prediction” is the key word for relating Blup and hierarchical modeling to classical statistical terminology.  In classical statistics, “estimation” of a parameter theta is evaluated conditional on the true value of theta, whereas “prediction” of a predictive quantity phi is eval</p><p>3 0.93977249 <a title="1270-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-18-The_treatment%2C_the_intermediate_outcome%2C_and_the_ultimate_outcome%3A__Leverage_and_the_financial_crisis.html">1420 andrew gelman stats-2012-07-18-The treatment, the intermediate outcome, and the ultimate outcome:  Leverage and the financial crisis</a></p>
<p>Introduction: Gur Huberman points to an article on the financial crisis by Bethany McLean, who  writes :
  
lthough our understanding of what instigated the 2008 global financial crisis remains at best incomplete, there are a few widely agreed upon contributing factors. One of them is a 2004 rule change by the U.S. Securities and Exchange Commission that allowed investment banks to load up on leverage.


This disastrous decision has been cited by a host of prominent economists, including Princeton professor and former Federal Reserve Vice-Chairman Alan Blinder and Nobel laureate Joseph Stiglitz. It has even been immortalized in Hollywood, figuring into the dark financial narrative that propelled the Academy Award-winning film Inside Job. . . .


Here’s just one problem with this story line: It’s not true. Nor is it hard to prove that. Look at the historical leverage of the big five investment banks — Bear Stearns, Lehman Brothers, Merrill Lynch, Goldman Sachs and Morgan Stanley. The Government Accou</p><p>4 0.93584895 <a title="1270-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-10-95%25_intervals_that_I_don%E2%80%99t_believe%2C_because_they%E2%80%99re_from_a_flat_prior_I_don%E2%80%99t_believe.html">1206 andrew gelman stats-2012-03-10-95% intervals that I don’t believe, because they’re from a flat prior I don’t believe</a></p>
<p>Introduction: Arnaud Trolle (no  relation ) writes:
  
I have a question about the interpretation of (non-)overlapping of 95% credibility intervals. In a Bayesian ANOVA (a within-subjects one), I computed 95% credibility intervals about the main effects of a factor. I’d like to compare two by two the main effects across the different conditions of the factor. Can I directly interpret the (non-)overlapping of these credibility intervals and make the following statements: “As the 95% credibility intervals do not overlap, both conditions have significantly different main effects” or conversely “As the 95% credibility intervals overlap, the main effects of both conditions are not significantly different, i.e. equivalent”? 
I heard that, in the case of classical confidence intervals, the second statement is false, but what happens when working within a Bayesian framework?
  
My reply:
 
I think it makes more sense to directly look at inference for the difference.  Also, your statements about equivalence</p><p>5 0.93146515 <a title="1270-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-28-Understanding_simulations_in_terms_of_predictive_inference%3F.html">1287 andrew gelman stats-2012-04-28-Understanding simulations in terms of predictive inference?</a></p>
<p>Introduction: David Hogg writes:
  
My (now deceased) collaborator and guru in all things inference, Sam Roweis, used to emphasize to me that we should evaluate models in the data space — not the parameter space — because models are always effectively “effective” and not really, fundamentally true. Or, in other words, models should be compared in the space of their predictions, not in the space of their parameters (the  parameters didn’t really “exist” at all for Sam).  In that spirit, when we estimate the effectiveness of a MCMC method or tuning — by autocorrelation time or ESJD or anything else — shouldn’t we be looking at the changes in the model predictions over time, rather than the changes in the parameters over time?  That is, the autocorrelation time should be the autocorrelation time in what the model (at the walker position) predicts for the data, and the ESJD should be the expected squared jump distance in what the model predicts for the data?  This might resolve the concern I expressed a</p><p>6 0.92766291 <a title="1270-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-17-I_got_99_comparisons_but_multiplicity_ain%E2%80%99t_one.html">1016 andrew gelman stats-2011-11-17-I got 99 comparisons but multiplicity ain’t one</a></p>
<p>7 0.92661703 <a title="1270-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-21-Instead_of_%E2%80%9Cconfidence_interval%2C%E2%80%9D_let%E2%80%99s_say_%E2%80%9Cuncertainty_interval%E2%80%9D.html">480 andrew gelman stats-2010-12-21-Instead of “confidence interval,” let’s say “uncertainty interval”</a></p>
<p>8 0.92540634 <a title="1270-lda-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-06-Bootstrap_averaging%3A_Examples_where_it_works_and_where_it_doesn%E2%80%99t_work.html">2201 andrew gelman stats-2014-02-06-Bootstrap averaging: Examples where it works and where it doesn’t work</a></p>
<p>9 0.9165554 <a title="1270-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-03-Boot.html">1881 andrew gelman stats-2013-06-03-Boot</a></p>
<p>10 0.91454339 <a title="1270-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-24-Why_it_doesn%E2%80%99t_make_sense_in_general_to_form_confidence_intervals_by_inverting_hypothesis_tests.html">1913 andrew gelman stats-2013-06-24-Why it doesn’t make sense in general to form confidence intervals by inverting hypothesis tests</a></p>
<p>11 0.91316867 <a title="1270-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-25-Why_it_doesn%E2%80%99t_make_sense_in_general_to_form_confidence_intervals_by_inverting_hypothesis_tests.html">870 andrew gelman stats-2011-08-25-Why it doesn’t make sense in general to form confidence intervals by inverting hypothesis tests</a></p>
<p>12 0.91313136 <a title="1270-lda-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-15-Problematic_interpretations_of_confidence_intervals.html">2248 andrew gelman stats-2014-03-15-Problematic interpretations of confidence intervals</a></p>
<p>13 0.91310018 <a title="1270-lda-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-09-I_hate_polynomials.html">2365 andrew gelman stats-2014-06-09-I hate polynomials</a></p>
<p>14 0.91166931 <a title="1270-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-11-Symptomatic_innumeracy.html">900 andrew gelman stats-2011-09-11-Symptomatic innumeracy</a></p>
<p>15 0.91140598 <a title="1270-lda-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>16 0.9089216 <a title="1270-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-15-Bayesian_statistical_pragmatism.html">662 andrew gelman stats-2011-04-15-Bayesian statistical pragmatism</a></p>
<p>17 0.90870386 <a title="1270-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-15-Google_Refine.html">910 andrew gelman stats-2011-09-15-Google Refine</a></p>
<p>18 0.90690124 <a title="1270-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-10-The_statistical_significance_filter.html">899 andrew gelman stats-2011-09-10-The statistical significance filter</a></p>
<p>19 0.90678453 <a title="1270-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-03-Some_thoughts_on_election_forecasting.html">391 andrew gelman stats-2010-11-03-Some thoughts on election forecasting</a></p>
<p>20 0.90669632 <a title="1270-lda-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-29-Decline_Effect_in_Linguistics%3F.html">1400 andrew gelman stats-2012-06-29-Decline Effect in Linguistics?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
