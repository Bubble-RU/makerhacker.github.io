<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1149 andrew gelman stats-2012-02-01-Philosophy of Bayesian statistics:  my reactions to Cox and Mayo</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1149" href="#">andrew_gelman_stats-2012-1149</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1149 andrew gelman stats-2012-02-01-Philosophy of Bayesian statistics:  my reactions to Cox and Mayo</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1149-html" href="http://andrewgelman.com/2012/02/01/philosophy-of-bayesian-statistics-my-reactions-to-cox-and-mayo/">html</a></p><p>Introduction: The journal Rationality, Markets and Morals has finally posted all the articles in their  special issue  on the philosophy of Bayesian statistics.
 
 My contribution  is called Induction and Deduction in Bayesian Data Analysis.  I’ll also post my reactions to the other articles.  I wrote these notes a few weeks ago and could post them all at once, but I think it will be easier if I post my reactions to each article separately.
 
  
 
To start with my best material, here’s my reaction to David Cox and Deborah Mayo, “A Statistical Scientist Meets a Philosopher of Science.”  I recommend you read all the way through my long note below; there’s good stuff throughout:
 
1.  Cox:  “[Philosophy] forces us to say what it is that we really want to know when we analyze a situation statistically.”
 
This reminds me of a standard question that Don Rubin (who, unlike me, has little use for philosophy in his research) asks in virtually any situation:  “What would you do if you had all the data?”  For</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Mayo defines scientific objectivity as concerning “the goal of using data to distinguish correct from incorrect claims about the world” and contrasts this with so-called objective Bayesian statistics. [sent-11, score-0.519]
</p><p>2 Cox discusses Fisher’s rule that it’s ok to use prior information in design of data collection but not in data analysis. [sent-16, score-0.808]
</p><p>3 But in this case we have good prior information suggesting that the difference in sex ratios in the population, comparing beautiful to less-beautiful parents, is less than 1 percentage point. [sent-21, score-0.739]
</p><p>4 A classical design analysis reveals that, with this level of true difference, any statistically-significant oberved difference in the sample is likely to be noise. [sent-22, score-0.329]
</p><p>5 (Even conditional on statistical significance, the observed difference has an over 40% chance of being in the wrong direction and will overestimate the population difference by an order of magnitude. [sent-23, score-0.348]
</p><p>6 )  At this point, you might well say that the original analysis should never have been done at all---but, given that it has been done, it is essential to use prior information to interpret the data and generalize from sample to population. [sent-24, score-0.96]
</p><p>7 We’re in a setting where the prior information is much stronger than the data. [sent-27, score-0.572]
</p><p>8 One is trying to extract information from the data, while the other, personalistic theory, is trying to indicate what you should believe, with regard to information from the data and other, prior, information treated equally seriously. [sent-31, score-1.273]
</p><p>9 We have a lot of prior information on the processes under which tree rings grow and how they are measured. [sent-47, score-0.771]
</p><p>10 You need some scientific knowledge and prior information on where these measurements came from. [sent-50, score-0.657]
</p><p>11 In my example, I assume he’d allow prior information on the tree-ring measurement process—I don’t see how you can get anywhere otherwise—but he’d rather not combine with external estimates of the temperature series. [sent-55, score-0.699]
</p><p>12 I’ve followed this approach in much of my own applied work, using noninformative priors and carefully avoiding the use of prior information in the final stages a statistical analysis. [sent-58, score-0.822]
</p><p>13 Sometimes (as in the sex ratio example above), the data are just too weak—and a classical textbook data analysis can be misleading. [sent-60, score-0.373]
</p><p>14 Imagine a Venn diagram, where one circle is “Topics that are so controversial that we want to avoid using prior information in the statistical analysis” and the other circle is “Problems where the data are weak compared to prior information. [sent-61, score-1.527]
</p><p>15 More generally, there is a Bayesian solution to the problem of sensitivity to prior assumptions. [sent-63, score-0.471]
</p><p>16 Make more explicit the mapping from prior and data to conclusions. [sent-65, score-0.448]
</p><p>17 And, if you’re going that route, I’d also like to see some analysis of sensitivity to assumptions that are not conventionally classified as “prior. [sent-67, score-0.341]
</p><p>18 For example, Cox regression is great, but additivity is a prior assumption too! [sent-69, score-0.414]
</p><p>19 , are exempt from Fisher’s strictures by virtue of being default assumptions rather than being based on prior information—but I certainly don’t think Mayo would take that position, given her strong feelings on Bayesian default priors. [sent-71, score-0.622]
</p><p>20 If you don’t want your choices to be based on prior information, what other options do you have? [sent-74, score-0.488]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cox', 0.414), ('prior', 0.33), ('personalistic', 0.281), ('information', 0.242), ('sensitivity', 0.141), ('assumptions', 0.134), ('data', 0.118), ('objective', 0.117), ('difference', 0.106), ('choices', 0.105), ('rings', 0.102), ('mayo', 0.101), ('tree', 0.097), ('fisher', 0.094), ('methods', 0.089), ('sample', 0.086), ('scientific', 0.085), ('additivity', 0.084), ('subjective', 0.084), ('philosophy', 0.081), ('bayesian', 0.08), ('population', 0.077), ('defines', 0.075), ('trying', 0.074), ('circle', 0.072), ('priors', 0.072), ('classical', 0.071), ('avoid', 0.07), ('theory', 0.069), ('analysis', 0.066), ('generalize', 0.065), ('using', 0.064), ('external', 0.064), ('temperature', 0.063), ('inside', 0.061), ('controversial', 0.061), ('beautiful', 0.061), ('goal', 0.06), ('climate', 0.06), ('statistical', 0.059), ('reactions', 0.057), ('weak', 0.056), ('final', 0.055), ('re', 0.055), ('parents', 0.055), ('conventional', 0.054), ('interpret', 0.053), ('default', 0.053), ('want', 0.053), ('take', 0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="1149-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-01-Philosophy_of_Bayesian_statistics%3A__my_reactions_to_Cox_and_Mayo.html">1149 andrew gelman stats-2012-02-01-Philosophy of Bayesian statistics:  my reactions to Cox and Mayo</a></p>
<p>Introduction: The journal Rationality, Markets and Morals has finally posted all the articles in their  special issue  on the philosophy of Bayesian statistics.
 
 My contribution  is called Induction and Deduction in Bayesian Data Analysis.  I’ll also post my reactions to the other articles.  I wrote these notes a few weeks ago and could post them all at once, but I think it will be easier if I post my reactions to each article separately.
 
  
 
To start with my best material, here’s my reaction to David Cox and Deborah Mayo, “A Statistical Scientist Meets a Philosopher of Science.”  I recommend you read all the way through my long note below; there’s good stuff throughout:
 
1.  Cox:  “[Philosophy] forces us to say what it is that we really want to know when we analyze a situation statistically.”
 
This reminds me of a standard question that Don Rubin (who, unlike me, has little use for philosophy in his research) asks in virtually any situation:  “What would you do if you had all the data?”  For</p><p>2 0.33021793 <a title="1149-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>Introduction: Nick Firoozye writes:
  
While I am absolutely sympathetic to the Bayesian agenda I am often troubled by the requirement of having priors. We must have priors on the parameter of an infinite number of model we have never seen before and I find this troubling. There is a similarly troubling problem in economics of utility theory. Utility is on consumables. To be complete a consumer must assign utility to all sorts of things they never would have encountered. More recent versions of utility theory instead make consumption goods a portfolio of attributes. Cadillacs are x many units of luxury y of transport etc etc. And we can automatically have personal utilities to all these attributes.  


I don’t ever see parameters. Some model have few and some have hundreds. Instead, I see data. So I don’t know how to have an opinion on parameters themselves. Rather I think it far more natural to have opinions on the behavior of models. The prior predictive density is a good and sensible notion. Also</p><p>3 0.32048529 <a title="1149-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>Introduction: Following up on Christian’s  post  [link fixed] on the topic, I’d like to offer a few thoughts of my own.
 
In BDA, we express the idea that a noninformative prior is a placeholder:  you can use the noninformative prior to get the analysis started, then if your posterior distribution is less informative than you would like, or if it does not make sense, you can go back and add prior information.
 
Same thing for the data model (the “likelihood”), for that matter:  it often makes sense to start with something simple and conventional and then go from there.
 
So, in that sense, noninformative priors are no big deal, they’re just a way to get started.  Just don’t take them too seriously.
 
Traditionally in statistics we’ve worked with the paradigm of a single highly informative dataset with only weak external information.  But if the data are sparse and prior information is strong, we have to think differently.  And, when you increase the dimensionality of a problem, both these things hap</p><p>4 0.3141143 <a title="1149-tfidf-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>Introduction: Robert Bell pointed me to  this post  by Brad De Long on Bayesian statistics, and then I also noticed  this  from Noah Smith, who wrote:
  
My impression is that although the Bayesian/Frequentist debate is interesting and intellectually fun, there’s really not much “there” there… despite being so-hip-right-now, Bayesian is not the Statistical Jesus.
  
I’m happy to see the discussion going in this direction.  Twenty-five years ago or so, when I got into this biz, there were some serious anti-Bayesian attitudes floating around in mainstream statistics.  Discussions in the journals sometimes devolved into debates of the form, “Bayesians:  knaves or fools?”.  You’d get all sorts of free-floating skepticism about any prior distribution at all, even while people were accepting without question (and doing theory on) logistic regressions, proportional hazards models, and all sorts of strong strong models.  (In the subfield of survey sampling, various prominent researchers would refuse to mode</p><p>5 0.31135833 <a title="1149-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>Introduction: Some recent blog discussion revealed some confusion that I’ll try to resolve here.
 
I  wrote  that I’m not a big fan of subjective priors.  Various commenters had difficulty with this point, and I think the issue was most clearly stated by Bill Jeff re erys, who  wrote :
  
It seems to me that your prior has to reflect your subjective information before you look at the data. How can it not?


But this does not mean that the (subjective) prior that you choose is irrefutable; Surely a prior that reflects prior information just does not have to be inconsistent with that information. But that still leaves a range of priors that are consistent with it, the sort of priors that one would use in a sensitivity analysis, for example.
  
I think I see what Bill is getting at.  A prior represents your subjective belief, or some approximation to your subjective belief, even if it’s not perfect.  That sounds reasonable but I don’t think it works.  Or, at least, it often doesn’t work.
 
Let’s start</p><p>6 0.26729867 <a title="1149-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-29-More_by_Berger_and_me_on_weakly_informative_priors.html">1092 andrew gelman stats-2011-12-29-More by Berger and me on weakly informative priors</a></p>
<p>7 0.24689433 <a title="1149-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-27-%E2%80%9CKeeping_things_unridiculous%E2%80%9D%3A__Berger%2C_O%E2%80%99Hagan%2C_and_me_on_weakly_informative_priors.html">1087 andrew gelman stats-2011-12-27-“Keeping things unridiculous”:  Berger, O’Hagan, and me on weakly informative priors</a></p>
<p>8 0.22911973 <a title="1149-tfidf-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>9 0.22790435 <a title="1149-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>10 0.21660931 <a title="1149-tfidf-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>11 0.21595471 <a title="1149-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-11-%E2%80%9CInformative_g-Priors_for_Logistic_Regression%E2%80%9D.html">2017 andrew gelman stats-2013-09-11-“Informative g-Priors for Logistic Regression”</a></p>
<p>12 0.21349858 <a title="1149-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-29-More_on_scaled-inverse_Wishart_and_prior_independence.html">1474 andrew gelman stats-2012-08-29-More on scaled-inverse Wishart and prior independence</a></p>
<p>13 0.20754558 <a title="1149-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-19-Prior_distributions_on_derived_quantities_rather_than_on_parameters_themselves.html">1946 andrew gelman stats-2013-07-19-Prior distributions on derived quantities rather than on parameters themselves</a></p>
<p>14 0.20108655 <a title="1149-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>15 0.19980417 <a title="1149-tfidf-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-30-Articles_on_the_philosophy_of_Bayesian_statistics_by_Cox%2C_Mayo%2C_Senn%2C_and_others%21.html">932 andrew gelman stats-2011-09-30-Articles on the philosophy of Bayesian statistics by Cox, Mayo, Senn, and others!</a></p>
<p>16 0.18447457 <a title="1149-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-03-Philosophy_of_Bayesian_statistics%3A__my_reactions_to_Senn.html">1151 andrew gelman stats-2012-02-03-Philosophy of Bayesian statistics:  my reactions to Senn</a></p>
<p>17 0.18345273 <a title="1149-tfidf-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-14-Trying_to_be_precise_about_vagueness.html">342 andrew gelman stats-2010-10-14-Trying to be precise about vagueness</a></p>
<p>18 0.18214536 <a title="1149-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-11-Why_waste_time_philosophizing%3F.html">1719 andrew gelman stats-2013-02-11-Why waste time philosophizing?</a></p>
<p>19 0.18049397 <a title="1149-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-16-Long_discussion_about_causal_inference_and_the_use_of_hierarchical_models_to_bridge_between_different_inferential_settings.html">1418 andrew gelman stats-2012-07-16-Long discussion about causal inference and the use of hierarchical models to bridge between different inferential settings</a></p>
<p>20 0.17502521 <a title="1149-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-24-Untangling_the_Jeffreys-Lindley_paradox.html">1182 andrew gelman stats-2012-02-24-Untangling the Jeffreys-Lindley paradox</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.332), (1, 0.206), (2, -0.014), (3, -0.0), (4, -0.104), (5, -0.084), (6, 0.039), (7, 0.108), (8, -0.154), (9, 0.003), (10, -0.022), (11, -0.028), (12, 0.07), (13, 0.034), (14, -0.009), (15, 0.018), (16, -0.028), (17, -0.021), (18, 0.045), (19, 0.007), (20, -0.027), (21, -0.009), (22, -0.009), (23, 0.053), (24, -0.008), (25, -0.007), (26, 0.058), (27, -0.054), (28, -0.03), (29, 0.061), (30, 0.053), (31, -0.049), (32, 0.027), (33, -0.005), (34, 0.021), (35, 0.079), (36, -0.022), (37, 0.036), (38, -0.006), (39, 0.02), (40, 0.041), (41, -0.013), (42, 0.035), (43, -0.019), (44, 0.031), (45, 0.049), (46, -0.04), (47, -0.081), (48, -0.035), (49, 0.016)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9733721 <a title="1149-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-01-Philosophy_of_Bayesian_statistics%3A__my_reactions_to_Cox_and_Mayo.html">1149 andrew gelman stats-2012-02-01-Philosophy of Bayesian statistics:  my reactions to Cox and Mayo</a></p>
<p>Introduction: The journal Rationality, Markets and Morals has finally posted all the articles in their  special issue  on the philosophy of Bayesian statistics.
 
 My contribution  is called Induction and Deduction in Bayesian Data Analysis.  I’ll also post my reactions to the other articles.  I wrote these notes a few weeks ago and could post them all at once, but I think it will be easier if I post my reactions to each article separately.
 
  
 
To start with my best material, here’s my reaction to David Cox and Deborah Mayo, “A Statistical Scientist Meets a Philosopher of Science.”  I recommend you read all the way through my long note below; there’s good stuff throughout:
 
1.  Cox:  “[Philosophy] forces us to say what it is that we really want to know when we analyze a situation statistically.”
 
This reminds me of a standard question that Don Rubin (who, unlike me, has little use for philosophy in his research) asks in virtually any situation:  “What would you do if you had all the data?”  For</p><p>2 0.87815171 <a title="1149-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-27-%E2%80%9CKeeping_things_unridiculous%E2%80%9D%3A__Berger%2C_O%E2%80%99Hagan%2C_and_me_on_weakly_informative_priors.html">1087 andrew gelman stats-2011-12-27-“Keeping things unridiculous”:  Berger, O’Hagan, and me on weakly informative priors</a></p>
<p>Introduction: Deborah Mayo sent me  this quote  from Jim Berger:
  
Too often I see people pretending to be subjectivists, and then using “weakly informative” priors that the objective Bayesian community knows are terrible and will give ridiculous answers; subjectivism is then being used as a shield to hide ignorance. . . . In my own more provocative moments, I claim that the only true subjectivists are the objective Bayesians, because they refuse to use subjectivism as a shield against criticism of sloppy pseudo-Bayesian practice.
  
This caught my attention because I’ve become more and more convinced that weakly informative priors are  the right way to go  in many different situations.  I don’t think Berger was talking about  me , though, as the above quote came from a publication in 2006, at which time I’d only started writing about weakly informative priors.
 
Going back to Berger’s  article , I see that his “weakly informative priors” remark was aimed at  this article  by Anthony O’Hagan, who w</p><p>3 0.87703824 <a title="1149-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>Introduction: Robert Bell pointed me to  this post  by Brad De Long on Bayesian statistics, and then I also noticed  this  from Noah Smith, who wrote:
  
My impression is that although the Bayesian/Frequentist debate is interesting and intellectually fun, there’s really not much “there” there… despite being so-hip-right-now, Bayesian is not the Statistical Jesus.
  
I’m happy to see the discussion going in this direction.  Twenty-five years ago or so, when I got into this biz, there were some serious anti-Bayesian attitudes floating around in mainstream statistics.  Discussions in the journals sometimes devolved into debates of the form, “Bayesians:  knaves or fools?”.  You’d get all sorts of free-floating skepticism about any prior distribution at all, even while people were accepting without question (and doing theory on) logistic regressions, proportional hazards models, and all sorts of strong strong models.  (In the subfield of survey sampling, various prominent researchers would refuse to mode</p><p>4 0.86490804 <a title="1149-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>Introduction: Following up on Christian’s  post  [link fixed] on the topic, I’d like to offer a few thoughts of my own.
 
In BDA, we express the idea that a noninformative prior is a placeholder:  you can use the noninformative prior to get the analysis started, then if your posterior distribution is less informative than you would like, or if it does not make sense, you can go back and add prior information.
 
Same thing for the data model (the “likelihood”), for that matter:  it often makes sense to start with something simple and conventional and then go from there.
 
So, in that sense, noninformative priors are no big deal, they’re just a way to get started.  Just don’t take them too seriously.
 
Traditionally in statistics we’ve worked with the paradigm of a single highly informative dataset with only weak external information.  But if the data are sparse and prior information is strong, we have to think differently.  And, when you increase the dimensionality of a problem, both these things hap</p><p>5 0.84497368 <a title="1149-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-29-More_by_Berger_and_me_on_weakly_informative_priors.html">1092 andrew gelman stats-2011-12-29-More by Berger and me on weakly informative priors</a></p>
<p>Introduction: A couple days ago we  discussed  some remarks by Tony O’Hagan and Jim Berger on weakly informative priors.  Jim  followed up  on Deborah Mayo’s blog with this:
  
Objective Bayesian priors are often improper (i.e., have infinite total mass), but this is not a problem when they are developed correctly. But not every improper prior is satisfactory. For instance, the constant prior is known to be unsatisfactory in many situations. The ‘solution’ pseudo-Bayesians often use is to choose a constant prior over a large but bounded set (a ‘weakly informative’ prior), saying it is now proper and so all is well. This is not true; if the constant prior on the whole parameter space is bad, so will be the constant prior over the bounded set. The problem is, in part, that some people confuse proper priors with subjective priors and, having learned that true subjective priors are fine, incorrectly presume that weakly informative proper priors are fine.
  
I have a few reactions to this:
 
1.  I agree</p><p>6 0.84347868 <a title="1149-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>7 0.82881856 <a title="1149-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-19-Prior_distributions_on_derived_quantities_rather_than_on_parameters_themselves.html">1946 andrew gelman stats-2013-07-19-Prior distributions on derived quantities rather than on parameters themselves</a></p>
<p>8 0.82845622 <a title="1149-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>9 0.82362205 <a title="1149-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-07-Philosophy_of_Bayesian_statistics%3A_my_reactions_to_Hendry.html">1157 andrew gelman stats-2012-02-07-Philosophy of Bayesian statistics: my reactions to Hendry</a></p>
<p>10 0.78797525 <a title="1149-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-11-%E2%80%9CInformative_g-Priors_for_Logistic_Regression%E2%80%9D.html">2017 andrew gelman stats-2013-09-11-“Informative g-Priors for Logistic Regression”</a></p>
<p>11 0.78777748 <a title="1149-lsi-11" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-15-Weakly_informative_priors_and_imprecise_probabilities.html">468 andrew gelman stats-2010-12-15-Weakly informative priors and imprecise probabilities</a></p>
<p>12 0.78661424 <a title="1149-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-13-%E2%80%9CWhat_are_some_situations_in_which_the_classical_approach_%28or_a_naive_implementation_of_it%2C_based_on_cookbook_recipes%29_gives_worse_results_than_a_Bayesian_approach%2C_results_that_actually_impeded_the_science%3F%E2%80%9D.html">2099 andrew gelman stats-2013-11-13-“What are some situations in which the classical approach (or a naive implementation of it, based on cookbook recipes) gives worse results than a Bayesian approach, results that actually impeded the science?”</a></p>
<p>13 0.78294039 <a title="1149-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-15-Reputations_changeable%2C_situations_tolerable.html">1858 andrew gelman stats-2013-05-15-Reputations changeable, situations tolerable</a></p>
<p>14 0.77713025 <a title="1149-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-10-Cross-validation_and_Bayesian_estimation_of_tuning_parameters.html">2129 andrew gelman stats-2013-12-10-Cross-validation and Bayesian estimation of tuning parameters</a></p>
<p>15 0.75853294 <a title="1149-lsi-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-13-On_the_half-Cauchy_prior_for_a_global_scale_parameter.html">801 andrew gelman stats-2011-07-13-On the half-Cauchy prior for a global scale parameter</a></p>
<p>16 0.75592053 <a title="1149-lsi-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-14-Trying_to_be_precise_about_vagueness.html">342 andrew gelman stats-2010-10-14-Trying to be precise about vagueness</a></p>
<p>17 0.75167584 <a title="1149-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-15-Static_sensitivity_analysis.html">804 andrew gelman stats-2011-07-15-Static sensitivity analysis</a></p>
<p>18 0.75154471 <a title="1149-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-24-Untangling_the_Jeffreys-Lindley_paradox.html">1182 andrew gelman stats-2012-02-24-Untangling the Jeffreys-Lindley paradox</a></p>
<p>19 0.75139582 <a title="1149-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-11-Weakly_informative_priors_for_Bayesian_nonparametric_models%3F.html">1454 andrew gelman stats-2012-08-11-Weakly informative priors for Bayesian nonparametric models?</a></p>
<p>20 0.75135386 <a title="1149-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-11-Gelman_on_Hennig_on_Gelman_on_Bayes.html">1208 andrew gelman stats-2012-03-11-Gelman on Hennig on Gelman on Bayes</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.013), (15, 0.054), (16, 0.075), (17, 0.045), (21, 0.018), (24, 0.178), (27, 0.021), (30, 0.031), (55, 0.018), (65, 0.012), (72, 0.011), (76, 0.013), (84, 0.017), (95, 0.023), (99, 0.35)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98646748 <a title="1149-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-20-Likelihood_thresholds_and_decisions.html">1422 andrew gelman stats-2012-07-20-Likelihood thresholds and decisions</a></p>
<p>Introduction: David Hogg points me to  this  discussion:
  
Martin Strasbourg and I [Hogg] discussed his project to detect new satellites of M31 in the PAndAS survey. He can construct a likelihood ratio (possibly even a marginalized likelihood ratio) at every position in the M31 imaging, between the best-fit satellite-plus-background model and the best nothing-plus-background model. He can make a two-dimensional map of these likelihood ratios and show a the histogram of them. Looking at this histogram, which has a tail to very large ratios, he asked me, where should I put my cut? That is, at what likelihood ratio does a candidate deserve follow-up? Here’s my unsatisfying answer:


To a statistician, the distribution of likelihood ratios is interesting and valuable to study. To an astronomer, it is uninteresting. You don’t want to know the distribution of likelihoods, you want to find satellites . . .
  
I wrote that I think this makes sense and that it would actualy be an interesting and useful rese</p><p>same-blog 2 0.98466289 <a title="1149-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-01-Philosophy_of_Bayesian_statistics%3A__my_reactions_to_Cox_and_Mayo.html">1149 andrew gelman stats-2012-02-01-Philosophy of Bayesian statistics:  my reactions to Cox and Mayo</a></p>
<p>Introduction: The journal Rationality, Markets and Morals has finally posted all the articles in their  special issue  on the philosophy of Bayesian statistics.
 
 My contribution  is called Induction and Deduction in Bayesian Data Analysis.  I’ll also post my reactions to the other articles.  I wrote these notes a few weeks ago and could post them all at once, but I think it will be easier if I post my reactions to each article separately.
 
  
 
To start with my best material, here’s my reaction to David Cox and Deborah Mayo, “A Statistical Scientist Meets a Philosopher of Science.”  I recommend you read all the way through my long note below; there’s good stuff throughout:
 
1.  Cox:  “[Philosophy] forces us to say what it is that we really want to know when we analyze a situation statistically.”
 
This reminds me of a standard question that Don Rubin (who, unlike me, has little use for philosophy in his research) asks in virtually any situation:  “What would you do if you had all the data?”  For</p><p>3 0.98391783 <a title="1149-lda-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-11-What_if_I_were_to_stop_publishing_in_journals%3F.html">2244 andrew gelman stats-2014-03-11-What if I were to stop publishing in journals?</a></p>
<p>Introduction: In our recent discussion of modes of publication, Joseph Wilson wrote, “The single best reform science can make right now is to decouple publication from career advancement, thereby reducing the number of publications by an order of magnitude and then move to an entirely disjointed, informal, online free-for-all communication system for research results.”
 
My first thought on this was: Sure, yeah, that makes sense. But then I got to thinking: what would it  really  mean to decouple publication from career advancement? This is too late for me—I’m middle-aged and have no career advancement in my future—but it got me thinking more carefully about the role of publication in the research process, and this seemed worth a blog (the simplest sort of publication available to me).
 
However, somewhere between writing the above paragraphs and writing the blog entry, I forgot exactly what I was going to say!  I guess I should’ve just typed it all in then.  In the old days I just wouldn’t run this</p><p>4 0.98062074 <a title="1149-lda-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-%E2%80%9CThe_British_amateur_who_debunked_the_mathematics_of_happiness%E2%80%9D.html">2177 andrew gelman stats-2014-01-19-“The British amateur who debunked the mathematics of happiness”</a></p>
<p>Introduction: Andrew Anthony  tells the excellent story  of how Nick Brown, Alan Sokal, and Harris Friedman shot down some particularly silly work in psychology.  (“According to the graph, it all came down to a specific ratio of positive emotions to negative emotions. If your ratio was greater than 2.9013 positive emotions to 1 negative emotion you were flourishing in life. If your ratio was less than that number you were languishing.”  And, yes, the work they were shooting down really is that bad.)
 
If you want to see what the fuss is about, just google “2.9013.”   Here’s  an example (from 2012) of an uncritical reporting of the claim,  here’s  another one from 2010,  here’s  one from 2011 . . . well, you get the idea.
 
And  here’s a quick summary  posted by Rolf Zwaan after Brown et al. came out with their paper.
 
I know Sokal and Brown and so this story was not news to me.  I didn’t post anything about it on this blog because it seemed like it was getting enough coverage elsewhere.  I think Ni</p><p>5 0.98012245 <a title="1149-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-26-Politics_as_an_escape_hatch.html">1591 andrew gelman stats-2012-11-26-Politics as an escape hatch</a></p>
<p>Introduction: Reading  these  news articles that slam more and more nails into the (perhaps unfairly) already-dead reputation of Hewlett Packard executive Meg Whitman, I keep thinking:  what if she’d won her election a couple years ago and was now governor or senator or whatever she was running for?  Then nobody would care that her company was falling apart!
 
Conversely, when Jon Corzine lost his reelection and reentered the business world, he left himself open to charges of acts of corruption that wouldn’t have been possible in congress or from the governor’s office.
 
But sometimes the immunity can go the other way.  Jack Welch still has the street-cred to  write  Wall Street Journal editorials despite his  history  of data manipulation, but it’s hard to imagine he could be elected to public office, even if he wanted to.  For another example, Al Sharpton was caught out on his lies in a well-publicized court case but that does not stop him from being bankrolled as a quasi-public figure.
 
Big name</p><p>6 0.98002434 <a title="1149-lda-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-23-Bayesian_adaptive_methods_for_clinical_trials.html">427 andrew gelman stats-2010-11-23-Bayesian adaptive methods for clinical trials</a></p>
<p>7 0.97976363 <a title="1149-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-11-One_more_time_on_that_ESP_study%3A__The_problem_of_overestimates_and_the_shrinkage_solution.html">511 andrew gelman stats-2011-01-11-One more time on that ESP study:  The problem of overestimates and the shrinkage solution</a></p>
<p>8 0.97958601 <a title="1149-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-23-Fight%21__%28also_a_bit_of_reminiscence_at_the_end%29.html">1136 andrew gelman stats-2012-01-23-Fight!  (also a bit of reminiscence at the end)</a></p>
<p>9 0.97941208 <a title="1149-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-09-A_tale_of_two_discussion_papers.html">1848 andrew gelman stats-2013-05-09-A tale of two discussion papers</a></p>
<p>10 0.978001 <a title="1149-lda-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-27-%E2%80%9CWhat_Can_we_Learn_from_the_Many_Labs_Replication_Project%3F%E2%80%9D.html">2227 andrew gelman stats-2014-02-27-“What Can we Learn from the Many Labs Replication Project?”</a></p>
<p>11 0.97788715 <a title="1149-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-03-Statistical_methods_that_work_in_some_settings_but_not_others.html">1560 andrew gelman stats-2012-11-03-Statistical methods that work in some settings but not others</a></p>
<p>12 0.97760379 <a title="1149-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-28-Writing_for_free.html">2080 andrew gelman stats-2013-10-28-Writing for free</a></p>
<p>13 0.97717124 <a title="1149-lda-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-30-I_posted_this_as_a_comment_on_a_sociology_blog.html">2353 andrew gelman stats-2014-05-30-I posted this as a comment on a sociology blog</a></p>
<p>14 0.9771595 <a title="1149-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-12-The_importance_of_style_in_academic_writing.html">902 andrew gelman stats-2011-09-12-The importance of style in academic writing</a></p>
<p>15 0.97691107 <a title="1149-lda-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-11-Adding_an_error_model_to_a_deterministic_model.html">1162 andrew gelman stats-2012-02-11-Adding an error model to a deterministic model</a></p>
<p>16 0.97658229 <a title="1149-lda-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-07-Jenny_Davidson_wins_Mark_Van_Doren_Award%2C_also_some_reflections_on_the_continuity_of_work_within_literary_criticism_or_statistics.html">22 andrew gelman stats-2010-05-07-Jenny Davidson wins Mark Van Doren Award, also some reflections on the continuity of work within literary criticism or statistics</a></p>
<p>17 0.97634667 <a title="1149-lda-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-01-How_does_Bayes_do_it%3F.html">247 andrew gelman stats-2010-09-01-How does Bayes do it?</a></p>
<p>18 0.97615713 <a title="1149-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-01-%E2%80%98Researcher_Degrees_of_Freedom%E2%80%99.html">1557 andrew gelman stats-2012-11-01-‘Researcher Degrees of Freedom’</a></p>
<p>19 0.97597581 <a title="1149-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-29-Infovis%2C_infographics%2C_and_data_visualization%3A__Where_I%E2%80%99m_coming_from%2C_and_where_I%E2%80%99d_like_to_go.html">878 andrew gelman stats-2011-08-29-Infovis, infographics, and data visualization:  Where I’m coming from, and where I’d like to go</a></p>
<p>20 0.97587395 <a title="1149-lda-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-30-You_can%E2%80%99t_put_Pandora_back_in_the_box.html">120 andrew gelman stats-2010-06-30-You can’t put Pandora back in the box</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
