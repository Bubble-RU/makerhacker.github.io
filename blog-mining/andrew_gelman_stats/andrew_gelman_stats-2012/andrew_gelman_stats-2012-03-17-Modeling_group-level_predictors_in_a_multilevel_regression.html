<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1216 andrew gelman stats-2012-03-17-Modeling group-level predictors in a multilevel regression</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1216" href="#">andrew_gelman_stats-2012-1216</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1216 andrew gelman stats-2012-03-17-Modeling group-level predictors in a multilevel regression</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1216-html" href="http://andrewgelman.com/2012/03/17/modeling-group-level-predictors-in-a-multilevel-regression/">html</a></p><p>Introduction: Trey Causey writes:
  
Do you have suggestions as to model selection strategies akin to Bayesian model averaging for multilevel models when level-2 inputs are of substantive interest? I [Causey] have seen plenty of R packages and procedures for non-multilevel models, and tried the glmulti package but found that it did not perform well with more than a few level-2 variables.
  
My quick answer is:  with a name like that, you should really be fitting three-level models!
 
My longer answer is:  regular readers will be unsurprised to hear that I’m no fan of  Bayesian model averaging .  Instead I’d prefer to bite the bullet and assign an informative prior distribution on these coefficients.  I don’t have a great example of such an analysis but I’m more and more thinking that this is the way to go.  I don’t see the point in aiming for the intermediate goal of pruning the predictors; I’d rather have a procedure that includes prior information on the predictors and their interactions.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Trey Causey writes:    Do you have suggestions as to model selection strategies akin to Bayesian model averaging for multilevel models when level-2 inputs are of substantive interest? [sent-1, score-1.566]
</p><p>2 I [Causey] have seen plenty of R packages and procedures for non-multilevel models, and tried the glmulti package but found that it did not perform well with more than a few level-2 variables. [sent-2, score-0.85]
</p><p>3 My quick answer is:  with a name like that, you should really be fitting three-level models! [sent-3, score-0.413]
</p><p>4 My longer answer is:  regular readers will be unsurprised to hear that I’m no fan of  Bayesian model averaging . [sent-4, score-1.238]
</p><p>5 Instead I’d prefer to bite the bullet and assign an informative prior distribution on these coefficients. [sent-5, score-0.899]
</p><p>6 I don’t have a great example of such an analysis but I’m more and more thinking that this is the way to go. [sent-6, score-0.127]
</p><p>7 I don’t see the point in aiming for the intermediate goal of pruning the predictors; I’d rather have a procedure that includes prior information on the predictors and their interactions. [sent-7, score-1.014]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('causey', 0.432), ('averaging', 0.269), ('trey', 0.216), ('predictors', 0.204), ('unsurprised', 0.195), ('bite', 0.183), ('bullet', 0.17), ('akin', 0.17), ('models', 0.162), ('aiming', 0.159), ('inputs', 0.151), ('prior', 0.147), ('plenty', 0.146), ('intermediate', 0.143), ('strategies', 0.14), ('assign', 0.137), ('answer', 0.137), ('packages', 0.133), ('model', 0.128), ('fan', 0.122), ('substantive', 0.122), ('procedures', 0.118), ('package', 0.113), ('bayesian', 0.112), ('perform', 0.112), ('interactions', 0.112), ('procedure', 0.112), ('regular', 0.11), ('suggestions', 0.108), ('includes', 0.107), ('longer', 0.102), ('fitting', 0.101), ('informative', 0.099), ('selection', 0.098), ('hear', 0.098), ('tried', 0.093), ('multilevel', 0.09), ('quick', 0.088), ('prefer', 0.088), ('name', 0.087), ('goal', 0.085), ('readers', 0.077), ('seen', 0.076), ('distribution', 0.075), ('interest', 0.074), ('instead', 0.07), ('thinking', 0.065), ('great', 0.062), ('found', 0.059), ('information', 0.057)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="1216-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-17-Modeling_group-level_predictors_in_a_multilevel_regression.html">1216 andrew gelman stats-2012-03-17-Modeling group-level predictors in a multilevel regression</a></p>
<p>Introduction: Trey Causey writes:
  
Do you have suggestions as to model selection strategies akin to Bayesian model averaging for multilevel models when level-2 inputs are of substantive interest? I [Causey] have seen plenty of R packages and procedures for non-multilevel models, and tried the glmulti package but found that it did not perform well with more than a few level-2 variables.
  
My quick answer is:  with a name like that, you should really be fitting three-level models!
 
My longer answer is:  regular readers will be unsurprised to hear that I’m no fan of  Bayesian model averaging .  Instead I’d prefer to bite the bullet and assign an informative prior distribution on these coefficients.  I don’t have a great example of such an analysis but I’m more and more thinking that this is the way to go.  I don’t see the point in aiming for the intermediate goal of pruning the predictors; I’d rather have a procedure that includes prior information on the predictors and their interactions.</p><p>2 0.17550065 <a title="1216-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>Introduction: In response to  this article  by Cosma Shalizi and myself on the philosophy of Bayesian statistics, David Hogg writes:
  
I [Hogg] agree–even in physics and astronomy–that the models are not “True” in the God-like sense of being absolute reality (that is, I am not a realist); and I  have argued  (a philosophically very naive 
paper, but hey, I was new to all this) that for pretty fundamental reasons we could never arrive at the True (with a capital “T”) model of the Universe.  The goal of inference is to find the “best” model, where “best” might have something to do with prediction, or explanation, or message length, or (horror!) our utility.  Needless to say, most of my physics friends *are* realists, even in the face of “effective theories” as Newtonian mechanics is an effective theory of GR and GR is an effective theory of “quantum gravity” (this plays to your point, because if you think any theory is possibly an effective theory, how could you ever find Truth?).  I also liked the i</p><p>3 0.15661263 <a title="1216-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>Introduction: Nick Firoozye writes:
  
I had a question about BMA [Bayesian model averaging] and model combinations in general, and direct it to you since they are a basic form of hierarchical model, albeit in the simplest of forms. I wanted to ask what the underlying assumptions are that could lead to BMA improving on a larger model. 


I know model combination is a topic of interest in the (frequentist) econometrics community (e.g., Bates & Granger, http://www.jstor.org/discover/10.2307/3008764?uid=3738032&uid;=2&uid;=4&sid;=21101948653381) but at the time it was considered a bit of a puzzle. Perhaps small models combined outperform a big model due to standard errors, insufficient data, etc. But I haven’t seen much in way of Bayesian justification.


In simplest terms, you might have a joint density P(Y,theta_1,theta_2) from which you could use the two marginals P(Y,theta_1) and P(Y,theta_2) to derive two separate forecasts. A BMA-er would do a weighted average of the two forecast densities, having p</p><p>4 0.15541835 <a title="1216-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-05-What_is_a_prior_distribution%3F.html">1155 andrew gelman stats-2012-02-05-What is a prior distribution?</a></p>
<p>Introduction: Some recent blog discussion revealed some confusion that I’ll try to resolve here.
 
I  wrote  that I’m not a big fan of subjective priors.  Various commenters had difficulty with this point, and I think the issue was most clearly stated by Bill Jeff re erys, who  wrote :
  
It seems to me that your prior has to reflect your subjective information before you look at the data. How can it not?


But this does not mean that the (subjective) prior that you choose is irrefutable; Surely a prior that reflects prior information just does not have to be inconsistent with that information. But that still leaves a range of priors that are consistent with it, the sort of priors that one would use in a sensitivity analysis, for example.
  
I think I see what Bill is getting at.  A prior represents your subjective belief, or some approximation to your subjective belief, even if it’s not perfect.  That sounds reasonable but I don’t think it works.  Or, at least, it often doesn’t work.
 
Let’s start</p><p>5 0.14797856 <a title="1216-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Kind_of_Bayesian.html">811 andrew gelman stats-2011-07-20-Kind of Bayesian</a></p>
<p>Introduction: Astrophysicist Andrew Jaffe pointed me to  this and discussion  of my  philosophy  of statistics (which is, in turn, my rational reconstruction of the statistical practice of Bayesians such as Rubin and Jaynes).  Jaffe’s summary is fair enough and I only disagree in a few points: 
   
1.  Jaffe writes:
  
Subjective probability, at least the way it is actually used by practicing scientists, is a sort of “as-if” subjectivity — how would an agent reason if her beliefs were reflected in a certain set of probability distributions? This is why when I discuss probability I try to make the pedantic point that all probabilities are conditional, at least on some background prior information or context.
  
I agree, and my problem with the usual procedures used for Bayesian model comparison and Bayesian model averaging is not that these approaches are subjective but that the particular models being considered don’t make sense.  I’m thinking of the sorts of models that say the truth is either A or</p><p>6 0.14338465 <a title="1216-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-11-%E2%80%9CInformative_g-Priors_for_Logistic_Regression%E2%80%9D.html">2017 andrew gelman stats-2013-09-11-“Informative g-Priors for Logistic Regression”</a></p>
<p>7 0.14210062 <a title="1216-tfidf-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-14-Trying_to_be_precise_about_vagueness.html">342 andrew gelman stats-2010-10-14-Trying to be precise about vagueness</a></p>
<p>8 0.13606669 <a title="1216-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-21-Building_a_regression_model_._._._with_only_27_data_points.html">1506 andrew gelman stats-2012-09-21-Building a regression model . . . with only 27 data points</a></p>
<p>9 0.1318993 <a title="1216-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-17-Graphical_tools_for_understanding_multilevel_models.html">772 andrew gelman stats-2011-06-17-Graphical tools for understanding multilevel models</a></p>
<p>10 0.13125615 <a title="1216-tfidf-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>11 0.12951054 <a title="1216-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>12 0.12921685 <a title="1216-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-21-Hidden_dangers_of_noninformative_priors.html">2109 andrew gelman stats-2013-11-21-Hidden dangers of noninformative priors</a></p>
<p>13 0.12876827 <a title="1216-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-08-Multilevel_regression_with_shrinkage_for_%E2%80%9Cfixed%E2%80%9D_effects.html">653 andrew gelman stats-2011-04-08-Multilevel regression with shrinkage for “fixed” effects</a></p>
<p>14 0.12675901 <a title="1216-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>15 0.12584373 <a title="1216-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>16 0.12582488 <a title="1216-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>17 0.12512653 <a title="1216-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>18 0.12484969 <a title="1216-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>19 0.12306624 <a title="1216-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-29-More_by_Berger_and_me_on_weakly_informative_priors.html">1092 andrew gelman stats-2011-12-29-More by Berger and me on weakly informative priors</a></p>
<p>20 0.1229834 <a title="1216-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.152), (1, 0.205), (2, 0.002), (3, 0.08), (4, 0.004), (5, -0.006), (6, 0.035), (7, -0.003), (8, 0.017), (9, 0.091), (10, 0.049), (11, 0.017), (12, 0.006), (13, 0.022), (14, 0.006), (15, 0.0), (16, 0.008), (17, -0.008), (18, 0.019), (19, 0.026), (20, -0.033), (21, 0.008), (22, -0.02), (23, -0.031), (24, -0.043), (25, -0.045), (26, -0.02), (27, -0.038), (28, -0.021), (29, -0.014), (30, -0.029), (31, -0.007), (32, 0.017), (33, 0.004), (34, 0.002), (35, 0.017), (36, 0.019), (37, 0.021), (38, -0.008), (39, -0.01), (40, 0.004), (41, -0.004), (42, 0.048), (43, -0.01), (44, 0.013), (45, 0.026), (46, -0.019), (47, 0.003), (48, 0.012), (49, -0.023)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97645551 <a title="1216-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-17-Modeling_group-level_predictors_in_a_multilevel_regression.html">1216 andrew gelman stats-2012-03-17-Modeling group-level predictors in a multilevel regression</a></p>
<p>Introduction: Trey Causey writes:
  
Do you have suggestions as to model selection strategies akin to Bayesian model averaging for multilevel models when level-2 inputs are of substantive interest? I [Causey] have seen plenty of R packages and procedures for non-multilevel models, and tried the glmulti package but found that it did not perform well with more than a few level-2 variables.
  
My quick answer is:  with a name like that, you should really be fitting three-level models!
 
My longer answer is:  regular readers will be unsurprised to hear that I’m no fan of  Bayesian model averaging .  Instead I’d prefer to bite the bullet and assign an informative prior distribution on these coefficients.  I don’t have a great example of such an analysis but I’m more and more thinking that this is the way to go.  I don’t see the point in aiming for the intermediate goal of pruning the predictors; I’d rather have a procedure that includes prior information on the predictors and their interactions.</p><p>2 0.85334027 <a title="1216-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-07-Prior_distributions_for_regression_coefficients.html">1486 andrew gelman stats-2012-09-07-Prior distributions for regression coefficients</a></p>
<p>Introduction: Eric Brown writes:
  
I have come across a number of recommendations over the years about best practices for multilevel regression modeling.  For example, the use of t-distributed priors for coefficients in logistic regression and standardizing input variables from one of your 2008 Annals of Applied Statistics papers; or recommendations for priors on variance parameters from your 2006 Bayesian Analysis paper.  I understand that these are often of varied opinion of people in the field, but I was wondering if you have a reference that you point people to for a place to get started?  I’ve tried looking through your blog posts but couldn’t find any summaries.


For example, what are some examples of when I should use more than a two-level hierarchical model?  Can I use a spike-slab coefficient model with a t-distributed prior for the slab rather than a normal? If I assume that my model is a priori wrong (but still useful), what are some recommended ways to choose how many interactions to u</p><p>3 0.83978373 <a title="1216-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>Introduction: Larry Wasserman  refers  to finite mixture models as “beasts” and  writes  jokes that they “should be avoided at all costs.”
 
I’ve thought a lot about mixture models, ever since using them in an  analysis  of voting patterns that was published in 1990.  First off, I’d like to say that our model was useful so I’d prefer not to pay the cost of avoiding it.  For a quick description of our mixture model and its context, see pp. 379-380 of my  article  in the Jim Berger volume).  Actually, our case was particularly difficult because we were not even fitting a mixture model to data, we were fitting it to latent data and using the model to perform partial pooling.  My difficulties in trying to fit this model inspired our discussion of mixture models in Bayesian Data Analysis (page 109 in the second edition, in the section on “Counterexamples to the theorems”).
 
I agree with Larry that if you’re fitting a mixture model, it’s good to be aware of the problems that arise if you try to estimate</p><p>4 0.82725906 <a title="1216-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>Introduction: Nick Firoozye writes:
  
I had a question about BMA [Bayesian model averaging] and model combinations in general, and direct it to you since they are a basic form of hierarchical model, albeit in the simplest of forms. I wanted to ask what the underlying assumptions are that could lead to BMA improving on a larger model. 


I know model combination is a topic of interest in the (frequentist) econometrics community (e.g., Bates & Granger, http://www.jstor.org/discover/10.2307/3008764?uid=3738032&uid;=2&uid;=4&sid;=21101948653381) but at the time it was considered a bit of a puzzle. Perhaps small models combined outperform a big model due to standard errors, insufficient data, etc. But I haven’t seen much in way of Bayesian justification.


In simplest terms, you might have a joint density P(Y,theta_1,theta_2) from which you could use the two marginals P(Y,theta_1) and P(Y,theta_2) to derive two separate forecasts. A BMA-er would do a weighted average of the two forecast densities, having p</p><p>5 0.80543602 <a title="1216-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>Introduction: Ilya Esteban writes:
  
In traditional machine learning and statistical learning techniques, you spend a lot of time selecting your input features, fiddling with model parameter values, etc., all of which leads to the problem of overfitting the data and producing overly optimistic estimates for how good the model really is. You can use techniques such as cross-validation and out-of-sample validation data to try to limit the damage, but they are imperfect solutions at best.


While Bayesian models have the great advantage of not forcing you to manually select among the various weights and input features, you still often end up trying different priors and model structures (especially with hierarchical models), before coming up with a “final” model. When applying Bayesian modeling to real world data sets, how does should you evaluate alternate priors and topologies for the model without falling into the same overfitting trap as you do with non-Bayesian models? If you try several different</p><p>6 0.80110407 <a title="1216-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-23-More_on_Bayesian_methods_and_multilevel_modeling.html">2033 andrew gelman stats-2013-09-23-More on Bayesian methods and multilevel modeling</a></p>
<p>7 0.79943341 <a title="1216-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>8 0.79616952 <a title="1216-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-11-Weakly_informative_priors_for_Bayesian_nonparametric_models%3F.html">1454 andrew gelman stats-2012-08-11-Weakly informative priors for Bayesian nonparametric models?</a></p>
<p>9 0.79237276 <a title="1216-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-24-Estimating_and_summarizing_inference_for_hierarchical_variance_parameters_when_the_number_of_groups_is_small.html">2145 andrew gelman stats-2013-12-24-Estimating and summarizing inference for hierarchical variance parameters when the number of groups is small</a></p>
<p>10 0.79028732 <a title="1216-lsi-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>11 0.78537363 <a title="1216-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-21-D._Buggin.html">1465 andrew gelman stats-2012-08-21-D. Buggin</a></p>
<p>12 0.78078467 <a title="1216-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-15-Wacky_priors_can_work_well%3F.html">1723 andrew gelman stats-2013-02-15-Wacky priors can work well?</a></p>
<p>13 0.77450389 <a title="1216-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>14 0.77032512 <a title="1216-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>15 0.76873368 <a title="1216-lsi-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-08-Multilevel_regression_with_shrinkage_for_%E2%80%9Cfixed%E2%80%9D_effects.html">653 andrew gelman stats-2011-04-08-Multilevel regression with shrinkage for “fixed” effects</a></p>
<p>16 0.76635921 <a title="1216-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-25-Incoherence_of_Bayesian_data_analysis.html">1510 andrew gelman stats-2012-09-25-Incoherence of Bayesian data analysis</a></p>
<p>17 0.76627916 <a title="1216-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-04-David_MacKay_and_Occam%E2%80%99s_Razor.html">1041 andrew gelman stats-2011-12-04-David MacKay and Occam’s Razor</a></p>
<p>18 0.76239085 <a title="1216-lsi-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<p>19 0.76051337 <a title="1216-lsi-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-14-Trying_to_be_precise_about_vagueness.html">342 andrew gelman stats-2010-10-14-Trying to be precise about vagueness</a></p>
<p>20 0.75907719 <a title="1216-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-24-Multilevel_modeling_and_instrumental_variables.html">1468 andrew gelman stats-2012-08-24-Multilevel modeling and instrumental variables</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.019), (16, 0.02), (21, 0.025), (22, 0.273), (24, 0.162), (63, 0.015), (86, 0.049), (89, 0.016), (99, 0.302)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.95971149 <a title="1216-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-03-This_is_a_footnote_in_one_of_my_papers.html">448 andrew gelman stats-2010-12-03-This is a footnote in one of my papers</a></p>
<p>Introduction: In the annals of hack literature, it is sometimes said that if you aim to write best-selling crap, all you’ll end up with is crap.  To truly produce best-selling crap, you have to have a conviction, perhaps misplaced, that your writing has integrity.  Whether or not this is a good generalization about writing, I have seen an analogous phenomenon in statistics:  If you try to do nothing but model the data, you can be in for a wild and unpleasant ride:  real data always seem to have one more twist beyond our ability to model (von Neumann’s elephant’s trunk notwithstanding).  But if you model the underlying process, sometimes your model can fit surprisingly well as well as inviting openings for future research progress.</p><p>2 0.93220425 <a title="1216-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-28-Every_time_you_take_a_sample%2C_you%E2%80%99ll_have_to_pay_this_guy_a_quarter.html">1398 andrew gelman stats-2012-06-28-Every time you take a sample, you’ll have to pay this guy a quarter</a></p>
<p>Introduction: Roy Mendelssohn pointed me to  this  heartwarming story of Jay Vadiveloo, an actuary who got a patent for the idea of statistical sampling.  Vadiveloo writes, “the results were astounding: statistical sampling worked.”
 
You may laugh, but wait till Albedo Man buys the patent and makes everybody do his bidding.  They’re gonna dig up Laplace and make him pay retroactive royalties.  And somehow Clippy will get involved in all this.
 
P.S.  Mendelssohn writes:  “Yes, I felt it was a heartwarming story also.  Perhaps we can get a patent for regression.”
 
I say, forget a patent for regression.  I want a patent for the sample mean.  That’s where the real money is.  You can’t charge a lot for each use, but consider the volume!</p><p>3 0.92389739 <a title="1216-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-01-Lamentably_common_misunderstanding_of_meritocracy.html">1037 andrew gelman stats-2011-12-01-Lamentably common misunderstanding of meritocracy</a></p>
<p>Introduction: Tyler Cowen  pointed  to  an article  by business-school professor Luigi Zingales about meritocracy.  I’d expect a b-school prof to support the idea of meritocracy, and Zingales does not disappoint.
 
But he says a bunch of other things that to me represent a confused conflation of ideas.  Here’s Zingales:
  
America became known as a land of opportunity—a place whose capitalist system benefited  the hardworking and the virtuous  [emphasis added]. In a word, it was a meritocracy.
  
That’s interesting—and revealing.  Here’s what I get when I look up “meritocracy” in the  dictionary :
  
1 : a system in which the talented are chosen and moved ahead on the basis of their achievement 
2 : leadership selected on the basis of intellectual criteria
  
Nothing here about “hardworking” or “virtuous.”  In a meritocracy, you can be as hardworking as John Kruk or as virtuous as Kobe Bryant and you’ll still get ahead—if you have the talent and achievement.  Throwing in “hardworking” and “virtuous”</p><p>same-blog 4 0.91924226 <a title="1216-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-17-Modeling_group-level_predictors_in_a_multilevel_regression.html">1216 andrew gelman stats-2012-03-17-Modeling group-level predictors in a multilevel regression</a></p>
<p>Introduction: Trey Causey writes:
  
Do you have suggestions as to model selection strategies akin to Bayesian model averaging for multilevel models when level-2 inputs are of substantive interest? I [Causey] have seen plenty of R packages and procedures for non-multilevel models, and tried the glmulti package but found that it did not perform well with more than a few level-2 variables.
  
My quick answer is:  with a name like that, you should really be fitting three-level models!
 
My longer answer is:  regular readers will be unsurprised to hear that I’m no fan of  Bayesian model averaging .  Instead I’d prefer to bite the bullet and assign an informative prior distribution on these coefficients.  I don’t have a great example of such an analysis but I’m more and more thinking that this is the way to go.  I don’t see the point in aiming for the intermediate goal of pruning the predictors; I’d rather have a procedure that includes prior information on the predictors and their interactions.</p><p>5 0.90575361 <a title="1216-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-13-Statistical_controversy_regarding_human_rights_violations_in_Colomnbia.html">145 andrew gelman stats-2010-07-13-Statistical controversy regarding human rights violations in Colomnbia</a></p>
<p>Introduction: Megan Price wrote in that she and Daniel Guzmán of the Benetech Human Rights Program released a paper today entitled  “Comments to the article ‘Is Violence Against Union Members in Colombia Systematic and Targeted?’”  (o  aqui  en español), which examines an article written by Colombian academics Daniel Mejía and María José Uribe.  Price writes [in the third person]:
  
 
The paper reviewed by Price and Guzmán concluded that “. . . on average, violence against unionists in Colombia is neither systematic nor targeted.” However, in their response, Price and Guzmán present – in technical and methodological detail – the reasons they find the conclusions in Mejía and Uribe’s study to be overstated. Price and Guzmán believe that weaknesses in the data, in the choice of the statistical model, and the interpretation of the model used in Mejía and Uribe’s study, all raise serious questions about the authors’ strong causal conclusions.


Price and Guzmán point out that unchecked, those conclusio</p><p>6 0.90392077 <a title="1216-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-31-Snotty_reviewers.html">1700 andrew gelman stats-2013-01-31-Snotty reviewers</a></p>
<p>7 0.89883077 <a title="1216-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-20-Costless_false_beliefs.html">477 andrew gelman stats-2010-12-20-Costless false beliefs</a></p>
<p>8 0.89172214 <a title="1216-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-05-For_those_of_you_in_the_U.K.%2C_also_an_amusing_paradox_involving_the_infamous_hookah_story.html">504 andrew gelman stats-2011-01-05-For those of you in the U.K., also an amusing paradox involving the infamous hookah story</a></p>
<p>9 0.8860842 <a title="1216-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-31-Wacky_surveys_where_they_don%E2%80%99t_tell_you_the_questions_they_asked.html">385 andrew gelman stats-2010-10-31-Wacky surveys where they don’t tell you the questions they asked</a></p>
<p>10 0.87392616 <a title="1216-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-04-Tesla_fires%21.html">2123 andrew gelman stats-2013-12-04-Tesla fires!</a></p>
<p>11 0.87156481 <a title="1216-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-01-Non-topical_blogging.html">1964 andrew gelman stats-2013-08-01-Non-topical blogging</a></p>
<p>12 0.8685351 <a title="1216-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-17-Drug_testing_for_recipents_of_NSF_and_NIH_grants%3F.html">92 andrew gelman stats-2010-06-17-Drug testing for recipents of NSF and NIH grants?</a></p>
<p>13 0.86820674 <a title="1216-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-10-If_an_entire_article_in_Computational_Statistics_and_Data_Analysis_were_put_together_from_other%2C_unacknowledged%2C_sources%2C_would_that_be_a_work_of_art%3F.html">1161 andrew gelman stats-2012-02-10-If an entire article in Computational Statistics and Data Analysis were put together from other, unacknowledged, sources, would that be a work of art?</a></p>
<p>14 0.84886563 <a title="1216-lda-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-15-How_effective_are_football_coaches%3F.html">1804 andrew gelman stats-2013-04-15-How effective are football coaches?</a></p>
<p>15 0.84667784 <a title="1216-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-29-New_journal_on_causal_inference.html">879 andrew gelman stats-2011-08-29-New journal on causal inference</a></p>
<p>16 0.84593445 <a title="1216-lda-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-11-News_flash%3A__Probability_and_statistics_are_hard_to_understand.html">1413 andrew gelman stats-2012-07-11-News flash:  Probability and statistics are hard to understand</a></p>
<p>17 0.83704996 <a title="1216-lda-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-04-Honored_oldsters_write_about_statistics.html">2317 andrew gelman stats-2014-05-04-Honored oldsters write about statistics</a></p>
<p>18 0.83145654 <a title="1216-lda-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-10-Do_you_believe_that_%E2%80%9Chumans_and_other_living_things_have_evolved_over_time%E2%80%9D%3F.html">2167 andrew gelman stats-2014-01-10-Do you believe that “humans and other living things have evolved over time”?</a></p>
<p>19 0.82744104 <a title="1216-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-16-BDA_at_40%25_off%21.html">1984 andrew gelman stats-2013-08-16-BDA at 40% off!</a></p>
<p>20 0.82317287 <a title="1216-lda-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-20-Thermodynamic_Monte_Carlo%3A__Michael_Betancourt%E2%80%99s_new_method_for_simulating_from_difficult_distributions_and_evaluating_normalizing_constants.html">2340 andrew gelman stats-2014-05-20-Thermodynamic Monte Carlo:  Michael Betancourt’s new method for simulating from difficult distributions and evaluating normalizing constants</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
